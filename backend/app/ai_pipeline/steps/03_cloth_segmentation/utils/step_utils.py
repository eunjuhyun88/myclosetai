#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Step Utils
=====================================================================

ClothSegmentationStep ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 1.0
"""

import logging
import os
import platform
import subprocess
import gc
import threading
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

try:
    import numpy as np
    import cv2
    NUMPY_AVAILABLE = True
    CV2_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    CV2_AVAILABLE = False
    np = None
    cv2 = None

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
        import_paths = [
            'app.ai_pipeline.steps.base_step_mixin',
            '.base_step_mixin',
            'backend.app.ai_pipeline.steps.base_step_mixin'
        ]
        
        for import_path in import_paths:
            try:
                import importlib
                if import_path.startswith('.'):
                    module = importlib.import_module(import_path, package='app.ai_pipeline.steps')
                else:
                    module = importlib.import_module(import_path)
                base_step_mixin = getattr(module, 'BaseStepMixin', None)
                if base_step_mixin:
                    return base_step_mixin
            except ImportError:
                continue
        
        return None
    except Exception as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_central_hub_container():
    """Central Hub Container ê°€ì ¸ì˜¤ê¸°"""
    try:
        from app.ai_pipeline.core.di_container import get_central_hub_container as get_container
        return get_container()
    except ImportError:
        try:
            from ..core.di_container import get_central_hub_container as get_container
            return get_container()
        except ImportError:
            logger.warning("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None

def inject_dependencies_safe(step_instance):
    """ì˜ì¡´ì„± ì•ˆì „ ì£¼ì…"""
    try:
        container = get_central_hub_container()
        if container:
            # ì˜ì¡´ì„± ì£¼ì… ë¡œì§
            if hasattr(step_instance, 'set_model_loader'):
                model_loader = container.get_service('model_loader')
                if model_loader:
                    step_instance.set_model_loader(model_loader)
            
            if hasattr(step_instance, 'set_memory_manager'):
                memory_manager = container.get_service('memory_manager')
                if memory_manager:
                    step_instance.set_memory_manager(memory_manager)
            
            logger.info("âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ Central Hub Containerê°€ ì—†ì–´ ì˜ì¡´ì„± ì£¼ì…ì„ ê±´ë„ˆëœë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

def get_service_from_central_hub(service_key: str):
    """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    try:
        container = get_central_hub_container()
        if container:
            return container.get_service(service_key)
        return None
    except Exception as e:
        logger.error(f"âŒ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
        if TORCH_AVAILABLE:
            if hasattr(torch, 'mps') and torch.mps.is_available():
                torch.mps.empty_cache()
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def safe_torch_operation(operation_func, *args, **kwargs):
    """ì•ˆì „í•œ PyTorch ì—°ì‚° ì‹¤í–‰"""
    try:
        if not TORCH_AVAILABLE:
            logger.warning("âš ï¸ PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            return None
        
        # ë©”ëª¨ë¦¬ ì•ˆì „ì„± ì²´í¬
        try:
            import psutil
            memory_usage = psutil.virtual_memory().percent
            if memory_usage > 90:
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}%")
                cleanup_memory()
        except ImportError:
            pass
        
        # ì—°ì‚° ì‹¤í–‰
        result = operation_func(*args, **kwargs)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ PyTorch ì—°ì‚° ì‹¤íŒ¨: {e}")
        cleanup_memory()
        return None

def create_cloth_segmentation_step(**kwargs) -> 'ClothSegmentationStep':
    """ClothSegmentationStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    try:
        from .step_core import ClothSegmentationStepCore
        step = ClothSegmentationStepCore(**kwargs)
        inject_dependencies_safe(step)
        return step
    except Exception as e:
        logger.error(f"âŒ ClothSegmentationStep ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def create_m3_max_segmentation_step(**kwargs) -> 'ClothSegmentationStep':
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    try:
        # M3 Max íŠ¹í™” ì„¤ì •
        m3_max_config = {
            'device': 'mps' if TORCH_AVAILABLE and hasattr(torch, 'mps') and torch.mps.is_available() else 'cpu',
            'max_workers': 4,
            'memory_limit': 0.8,  # 80% ë©”ëª¨ë¦¬ ì‚¬ìš© ì œí•œ
            **kwargs
        }
        
        step = create_cloth_segmentation_step(**m3_max_config)
        return step
        
    except Exception as e:
        logger.error(f"âŒ M3 Max ClothSegmentationStep ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def test_cloth_segmentation_ai():
    """ClothSegmentationStep AI í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª ClothSegmentationStep AI í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # Step ìƒì„±
        step = create_cloth_segmentation_step()
        if not step:
            logger.error("âŒ Step ìƒì„± ì‹¤íŒ¨")
            return False
        
        # ì´ˆê¸°í™”
        if not step.initialize():
            logger.error("âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = step.process(image=test_image)
        
        if result and result.get('success', False):
            logger.info("âœ… ClothSegmentationStep AI í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
        else:
            logger.error("âŒ ClothSegmentationStep AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ClothSegmentationStep AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_central_hub_compatibility():
    """Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # Central Hub Container í…ŒìŠ¤íŠ¸
        container = get_central_hub_container()
        if not container:
            logger.warning("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° í…ŒìŠ¤íŠ¸
        services = ['model_loader', 'memory_manager', 'data_converter']
        for service_key in services:
            service = get_service_from_central_hub(service_key)
            if service:
                logger.info(f"âœ… {service_key} ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            else:
                logger.warning(f"âš ï¸ {service_key} ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
        
        logger.info("âœ… Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def validate_step_configuration(config: Dict[str, Any]) -> Dict[str, Any]:
    """Step ì„¤ì • ê²€ì¦"""
    try:
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': []
        }
        
        # í•„ìˆ˜ ì„¤ì • ê²€ì¦
        required_configs = ['method', 'quality_level', 'input_size']
        for config_key in required_configs:
            if config_key not in config:
                validation_result['valid'] = False
                validation_result['errors'].append(f"í•„ìˆ˜ ì„¤ì • ëˆ„ë½: {config_key}")
        
        # ì„¤ì •ê°’ ê²€ì¦
        if 'method' in config:
            valid_methods = ['u2net_cloth', 'sam_huge', 'deeplabv3_plus', 'hybrid_ai']
            if config['method'] not in valid_methods:
                validation_result['valid'] = False
                validation_result['errors'].append(f"ì˜ëª»ëœ method: {config['method']}")
        
        if 'quality_level' in config:
            valid_quality_levels = ['fast', 'balanced', 'high', 'ultra']
            if config['quality_level'] not in valid_quality_levels:
                validation_result['valid'] = False
                validation_result['errors'].append(f"ì˜ëª»ëœ quality_level: {config['quality_level']}")
        
        if 'input_size' in config:
            input_size = config['input_size']
            if not isinstance(input_size, (list, tuple)) or len(input_size) != 2:
                validation_result['valid'] = False
                validation_result['errors'].append(f"ì˜ëª»ëœ input_size: {input_size}")
            else:
                width, height = input_size
                if width <= 0 or height <= 0:
                    validation_result['valid'] = False
                    validation_result['errors'].append(f"input_sizeëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤: {input_size}")
        
        return validation_result
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'valid': False,
            'errors': [f"ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {e}"],
            'warnings': []
        }

def get_step_requirements() -> Dict[str, Any]:
    """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
    return {
        'python_version': '3.8+',
        'required_packages': [
            'torch>=1.9.0',
            'torchvision>=0.10.0',
            'numpy>=1.21.0',
            'opencv-python>=4.5.0',
            'Pillow>=8.0.0'
        ],
        'optional_packages': [
            'psutil',
            'scikit-image',
            'scipy'
        ],
        'hardware_requirements': {
            'min_memory_gb': 8,
            'recommended_memory_gb': 16,
            'gpu_support': 'optional',
            'mps_support': 'optional'
        },
        'model_requirements': {
            'u2net_cloth': '168.1MB',
            'sam_huge': '2445.7MB',
            'deeplabv3_plus': '233.3MB'
        }
    }

def create_step_documentation() -> Dict[str, Any]:
    """Step ë¬¸ì„œí™” ìƒì„±"""
    return {
        'name': 'ClothSegmentationStep',
        'description': 'ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ AI Step',
        'version': '1.0',
        'author': 'MyCloset AI Team',
        'methods': {
            'u2net_cloth': {
                'description': 'U2Net ê¸°ë°˜ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜',
                'size': '168.1MB',
                'speed': 'fast',
                'accuracy': 'high'
            },
            'sam_huge': {
                'description': 'SAM ViT-Huge ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜',
                'size': '2445.7MB',
                'speed': 'slow',
                'accuracy': 'very_high'
            },
            'deeplabv3_plus': {
                'description': 'DeepLabV3+ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜',
                'size': '233.3MB',
                'speed': 'medium',
                'accuracy': 'high'
            },
            'hybrid_ai': {
                'description': 'ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•œ ì•™ìƒë¸”',
                'size': 'variable',
                'speed': 'slow',
                'accuracy': 'very_high'
            }
        },
        'configuration': {
            'method': 'ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì„ íƒ',
            'quality_level': 'í’ˆì§ˆ ë ˆë²¨ (fast/balanced/high/ultra)',
            'input_size': 'ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°',
            'confidence_threshold': 'ì‹ ë¢°ë„ ì„ê³„ê°’'
        },
        'output_format': {
            'masks': 'ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë“¤',
            'confidence': 'ì‹ ë¢°ë„ ì ìˆ˜',
            'processing_time': 'ì²˜ë¦¬ ì‹œê°„',
            'method_used': 'ì‚¬ìš©ëœ ë°©ë²•'
        }
    }
