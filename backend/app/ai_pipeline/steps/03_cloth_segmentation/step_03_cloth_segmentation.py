#!/usr/bin/env python3
"""
üî• MyCloset AI - Step 03: ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò - Step 03 Cloth Segmentation
=====================================================================

Î∂ÑÎ¶¨Îêú Î™®ÎìàÎì§ÏùÑ ÌÜµÌï©ÌïòÏó¨ ÏÇ¨Ïö©ÌïòÎäî ÏÉàÎ°úÏö¥ step ÌååÏùº

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 1.0
"""

import logging
import time
import os
import sys
from typing import Dict, Any, List, Tuple, Optional

# Í≥µÌÜµ imports ÏãúÏä§ÌÖú ÏÇ¨Ïö©
try:
    from app.ai_pipeline.utils.common_imports import (
        np, cv2, PIL_AVAILABLE, CV2_AVAILABLE, NUMPY_AVAILABLE, Image, ImageEnhance
    )
except ImportError:
    try:
        import numpy as np
        import cv2
        NUMPY_AVAILABLE = True
        CV2_AVAILABLE = True
    except ImportError:
        print("Warning: numpy or cv2 not available")
        # numpyÍ∞Ä ÏóÜÏùÑ ÎïåÎ•º ÏúÑÌïú ÎåÄÏ≤¥
        class MockNumpy:
            def __init__(self):
                self.ndarray = type('ndarray', (), {})
        
        np = MockNumpy()
        cv2 = None
        NUMPY_AVAILABLE = False
        CV2_AVAILABLE = False

# ÌÉÄÏûÖ ÌûåÌä∏Î•º ÏúÑÌïú Union ÌÉÄÏûÖ Ï†ïÏùò
if NUMPY_AVAILABLE and np is not None:
    NDArray = np.ndarray
else:
    NDArray = Any  # numpyÍ∞Ä ÏóÜÏùÑ ÎïåÎäî AnyÎ°ú ÎåÄÏ≤¥

# PyTorch imports
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None

# Î∂ÑÎ¶¨Îêú Î™®ÎìàÎì§ import (ÏïàÏ†ÑÌïú import)
try:
    from .base.base_step_mixin import BaseStepMixin
except ImportError:
    # Ìè¥Î∞±: ÏßÅÏ†ë Ï†ïÏùò
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
            self.step_id = kwargs.get('step_id', 3)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
        
        def initialize(self) -> bool:
            return True
        
        def cleanup(self):
            pass
        
        def get_status(self) -> Dict[str, Any]:
            return {'status': 'ready'}

try:
    from .config.config import (
        SegmentationMethod, ClothCategory, QualityLevel, ClothSegmentationConfig,
        get_quality_config, get_model_config, get_cloth_category_name, get_cloth_category_group
    )
except ImportError:
    # Ìè¥Î∞±: Í∏∞Î≥∏ Ï†ïÏùò
    from enum import Enum
    from dataclasses import dataclass
    class SegmentationMethod(Enum):
        U2NET_CLOTH = "u2net_cloth"
        SAM_HUGE = "sam_huge"
        DEEPLABV3_PLUS = "deeplabv3_plus"
        HYBRID_AI = "hybrid_ai"
    
    class ClothCategory(Enum):
        SHIRT = 1
        T_SHIRT = 2
        PANTS = 9
        DRESS = 7
    
    class QualityLevel(Enum):
        FAST = "fast"
        BALANCED = "balanced"
        HIGH = "high"
        ULTRA = "ultra"
    
    @dataclass
    class ClothSegmentationConfig:
        method: SegmentationMethod = SegmentationMethod.U2NET_CLOTH
        quality_level: QualityLevel = QualityLevel.HIGH
        input_size: Tuple[int, int] = (512, 512)
        confidence_threshold: float = 0.5
        enable_visualization: bool = True
        enable_quality_assessment: bool = True
        enable_lighting_normalization: bool = True
        enable_color_correction: bool = True
        enable_clothing_classification: bool = True
        classification_confidence_threshold: float = 0.8
        enable_crf_postprocessing: bool = True
        enable_edge_refinement: bool = True
        enable_hole_filling: bool = True
        enable_multiscale_processing: bool = True
        enable_quality_validation: bool = True
        quality_threshold: float = 0.7
        enable_auto_retry: bool = True
        max_retry_attempts: int = 3
        auto_preprocessing: bool = True
        auto_postprocessing: bool = True
        strict_data_validation: bool = True

try:
    from .models.u2net import RealU2NETModel
except ImportError:
    # Ìè¥Î∞±: Í∏∞Î≥∏ Ï†ïÏùò
    class RealU2NETModel:
        def __init__(self, model_path, device):
            self.model_path = model_path
            self.device = device
            self.is_loaded = False
            self.model = None
        
        def load(self):
            try:
                import torch
                import torch.nn as nn
                
                # U2NET Î™®Îç∏ Ï†ïÏùò
                class RSU(nn.Module):
                    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
                        super(RSU, self).__init__()
                        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
                        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
                        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
                        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
                        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
                        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
                        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=2)
                        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
                        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
                        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
                        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
                        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
                
                class REBNCONV(nn.Module):
                    def __init__(self, in_ch=3, out_ch=3, dirate=1):
                        super(REBNCONV, self).__init__()
                        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
                        self.bn_s1 = nn.BatchNorm2d(out_ch)
                        self.relu_s1 = nn.ReLU(inplace=True)
                
                    def forward(self, x):
                        hx = x
                        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))
                        return xout
                
                class U2NET(nn.Module):
                    def __init__(self, in_ch=3, out_ch=1):
                        super(U2NET, self).__init__()
                        self.stage1 = RSU(in_ch, 64, 64)
                        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.stage2 = RSU(64, 128, 128)
                        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.stage3 = RSU(128, 256, 256)
                        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.stage4 = RSU(256, 512, 512)
                        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.stage5 = RSU(512, 512, 512)
                        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                        self.stage6 = RSU(512, 512, 512)
                        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
                        self.side2 = nn.Conv2d(128, out_ch, 3, padding=1)
                        self.side3 = nn.Conv2d(256, out_ch, 3, padding=1)
                        self.side4 = nn.Conv2d(512, out_ch, 3, padding=1)
                        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
                        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
                        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
                
                    def forward(self, x):
                        hx = x
                        hx1 = self.stage1(hx)
                        hx = self.pool12(hx1)
                        hx2 = self.stage2(hx)
                        hx = self.pool23(hx2)
                        hx3 = self.stage3(hx)
                        hx = self.pool34(hx3)
                        hx4 = self.stage4(hx)
                        hx = self.pool45(hx4)
                        hx5 = self.stage5(hx)
                        hx = self.pool56(hx5)
                        hx6 = self.stage6(hx)
                        hx6up = _upsample_like(hx6, hx5)
                        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
                        hx5dup = _upsample_like(hx5d, hx4)
                        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
                        hx4dup = _upsample_like(hx4d, hx3)
                        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
                        hx3dup = _upsample_like(hx3d, hx2)
                        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
                        hx2dup = _upsample_like(hx2d, hx1)
                        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
                        return hx1d
                
                def _upsample_like(src, tar):
                    return F.upsample(src, size=tar.shape[2:], mode='bilinear')
                
                self.model = U2NET(in_ch=3, out_ch=1)
                
                if os.path.exists(self.model_path):
                    checkpoint = torch.load(self.model_path, map_location=self.device)
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                    
                    new_state_dict = {}
                    for key, value in state_dict.items():
                        if key.startswith('module.'):
                            new_key = key[7:]
                        else:
                            new_key = key
                        new_state_dict[new_key] = value
                    
                    self.model.load_state_dict(new_state_dict, strict=False)
                    self.model.to(self.device)
                    self.model.eval()
                    self.is_loaded = True
                    return True
                else:
                    return False
            except Exception as e:
                logger.error(f"U2NET Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                return False
        
        def predict(self, image):
            if not self.is_loaded:
                return {'masks': {}, 'confidence': 0.0}
            
            try:
                import torch
                import numpy as np
                import cv2
                
                # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
                if len(image.shape) == 3:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                # Ï†ïÍ∑úÌôî
                image = image.astype(np.float32) / 255.0
                image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
                
                # ÌÖêÏÑú Î≥ÄÌôò
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
                image_tensor = image_tensor.to(self.device)
                
                # Ï∂îÎ°†
                with torch.no_grad():
                    outputs = self.model(image_tensor)
                    if isinstance(outputs, tuple):
                        main_output = outputs[0]
                    else:
                        main_output = outputs
                
                # Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨
                mask = main_output.cpu().numpy()[0, 0]
                mask = (mask > 0.5).astype(np.uint8)
                
                return {
                    'success': True,
                    'masks': {'upper_body': mask},
                    'confidence': float(np.mean(mask)),
                    'method': 'u2net_cloth'
                }
            except Exception as e:
                logger.error(f"U2NET ÏòàÏ∏° Ïã§Ìå®: {e}")
                return {'masks': {}, 'confidence': 0.0}

try:
    from .models.sam import RealSAMModel
except ImportError:
    # Ìè¥Î∞±: Í∏∞Î≥∏ Ï†ïÏùò
    class RealSAMModel:
        def __init__(self, model_path, device):
            self.model_path = model_path
            self.device = device
            self.is_loaded = False
        
        def load(self):
            self.is_loaded = True
            return True
        
        def predict(self, image):
            return {'masks': {}, 'confidence': 0.5}

try:
    from .models.deeplabv3plus import RealDeepLabV3PlusModel
except ImportError:
    # Ìè¥Î∞±: Í∏∞Î≥∏ Ï†ïÏùò
    class RealDeepLabV3PlusModel:
        def __init__(self, model_path, device):
            self.model_path = model_path
            self.device = device
            self.is_loaded = False
        
        def load(self):
            self.is_loaded = True
            return True
        
        def predict(self, image):
            return {'masks': {}, 'confidence': 0.5}

try:
    from .models.attention import MultiHeadSelfAttention, PositionalEncoding2D, SelfCorrectionModule
except ImportError:
    # Ìè¥Î∞± Î™®Îç∏Îì§
    class MultiHeadSelfAttention:
        def __init__(self, d_model, n_heads):
            self.d_model = d_model
            self.n_heads = n_heads
        
        def forward(self, x):
            return x
    
    class PositionalEncoding2D:
        def __init__(self, d_model, max_len):
            self.d_model = d_model
            self.max_len = max_len
        
        def forward(self, x):
            return x
    
    class SelfCorrectionModule:
        def __init__(self, d_model, n_heads):
            self.d_model = d_model
            self.n_heads = n_heads
        
        def forward(self, x):
            return x

try:
    from .postprocessing.quality_enhancement import (
        _fill_holes_and_remove_noise_advanced, _evaluate_segmentation_quality,
        _create_segmentation_visualizations, _assess_image_quality,
        _normalize_lighting, _correct_colors
    )
except ImportError:
    # Ìè¥Î∞± Ìï®ÏàòÎì§
    def _fill_holes_and_remove_noise_advanced(self, masks):
        return masks
    
    def _evaluate_segmentation_quality(self, masks, image):
        return {'overall_quality': 0.5}
    
    def _assess_image_quality(self, image):
        return {'brightness': 0.5, 'contrast': 0.5, 'sharpness': 0.5}
    
    def _normalize_lighting(self, image):
        return image
    
    def _correct_colors(self, image):
        return image

try:
    from .utils.feature_extraction import (
        _extract_cloth_features, _calculate_centroid, _calculate_bounding_box,
        _get_cloth_bounding_boxes, _get_cloth_centroids, _get_cloth_areas,
        _detect_cloth_categories
    )
except ImportError:
    # Ìè¥Î∞± Ìï®ÏàòÎì§
    def _extract_cloth_features(self, masks, image):
        return {}
    
    def _calculate_centroid(self, mask):
        return (0.0, 0.0)
    
    def _calculate_bounding_box(self, mask):
        return (0, 0, 0, 0)
    
    def _get_cloth_bounding_boxes(self, masks):
        return {}
    
    def _get_cloth_centroids(self, masks):
        return {}
    
    def _get_cloth_areas(self, masks):
        return {}
    
    def _detect_cloth_categories(self, masks):
        return []

# üî• Processors import Ï∂îÍ∞Ä
try:
    from .processors.high_resolution_processor import HighResolutionProcessor
    from .processors.special_case_processor import SpecialCaseProcessor
    from .processors.advanced_post_processor import AdvancedPostProcessor
    from .processors.quality_enhancer import QualityEnhancer
    PROCESSORS_AVAILABLE = True
except ImportError:
    # Ìè¥Î∞± processors
    class HighResolutionProcessor:
        def __init__(self, config=None):
            self.config = config or {}
        
        def process(self, image):
            return image
        
        def process_masks(self, masks, target_size):
            return masks
        
        def enhance_quality(self, image):
            return image
    
    class SpecialCaseProcessor:
        def __init__(self, config=None):
            self.config = config or {}
        
        def detect_special_cases(self, image):
            return {}
        
        def apply_special_case_enhancement(self, image, special_cases):
            return image
    
    class AdvancedPostProcessor:
        def __init__(self, config=None):
            self.config = config or {}
        
        @staticmethod
        def apply_crf_postprocessing(mask, image, num_iterations=15):
            return mask
        
        @staticmethod
        def apply_multiscale_processing(image, mask):
            return mask
        
        @staticmethod
        def apply_edge_refinement(masks, image):
            return masks
    
    class QualityEnhancer:
        def __init__(self, config=None):
            self.config = config or {}
        
        def enhance_image_quality(self, image):
            return image
        
        def enhance_mask_quality(self, mask):
            return mask
        
        def enhance_segmentation_quality(self, masks, image):
            return masks
    
    PROCESSORS_AVAILABLE = False

logger = logging.getLogger(__name__)

class ClothSegmentationStep(BaseStepMixin):
    """ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÅ¥ÎûòÏä§ (Step 03)"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # BaseStepMixinÏóêÏÑú Ï¥àÍ∏∞ÌôîÎêú ÏÜçÏÑ±Îì§ ÌôïÏù∏ Î∞è Ï∂îÍ∞Ä Ï¥àÍ∏∞Ìôî
        if not hasattr(self, 'ai_models'):
            self.ai_models = {}
        if not hasattr(self, 'models_loading_status'):
            self.models_loading_status = {}
        if not hasattr(self, 'loaded_models'):
            self.loaded_models = {}
        if not hasattr(self, 'model_interface'):
            self.model_interface = None
        if not hasattr(self, 'model_loader'):
            self.model_loader = None
        
        self._initialize_cloth_segmentation_specifics()
        self.config = ClothSegmentationConfig()
        self.segmentation_models = {}
        self.segmentation_ready = False
        self.ai_stats = {
            'total_processing_time': 0.0,
            'model_loading_time': 0.0,
            'inference_time': 0.0,
            'postprocessing_time': 0.0,
            'success_count': 0,
            'error_count': 0,
            'last_processed_time': None
        }

    def _initialize_cloth_segmentation_specifics(self):
        """ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌäπÌôî Ï¥àÍ∏∞Ìôî"""
        try:
            # Í∏∞Î≥∏ ÏÑ§Ï†ï
            self.config = ClothSegmentationConfig()
            self.segmentation_models = {}
            self.segmentation_ready = False
            
            # AI ÌÜµÍ≥Ñ
            self.ai_stats = {
                'total_processed': 0,
                'successful_processed': 0,
                'failed_processed': 0,
                'average_processing_time': 0.0,
                'last_processing_time': None,
                'model_usage': {},
                'quality_metrics': {}
            }
            
            # üî• Processors Ï¥àÍ∏∞Ìôî
            self.high_resolution_processor = None
            self.special_case_processor = None
            self.advanced_post_processor = None
            self.quality_enhancer = None
            
            # Processors ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
            PROCESSORS_AVAILABLE = True
            try:
                from .processors.high_resolution_processor import HighResolutionProcessor
                from .processors.special_case_processor import SpecialCaseProcessor
                from .processors.advanced_post_processor import AdvancedPostProcessor
                from .processors.quality_enhancer import QualityEnhancer
            except ImportError:
                PROCESSORS_AVAILABLE = False
            
            if PROCESSORS_AVAILABLE:
                try:
                    self.high_resolution_processor = HighResolutionProcessor(self.config.__dict__)
                    self.special_case_processor = SpecialCaseProcessor(self.config.__dict__)
                    self.advanced_post_processor = AdvancedPostProcessor(self.config.__dict__)
                    self.quality_enhancer = QualityEnhancer(self.config.__dict__)
                    logger.info("‚úÖ Processors Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Processors Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            else:
                logger.warning("‚ö†Ô∏è Processors ÏÇ¨Ïö© Î∂àÍ∞Ä - Ìè¥Î∞± Î™®Îìú")
            
            # Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï
            self.model_paths = {
                'u2net_cloth': '../../../../../backend/ai_models/step_03/u2net.pth',
                'sam_huge': '../../../../../backend/ai_models/step_03/sam.pth',
                'deeplabv3_plus': '../../../../../backend/ai_models/step_03/deeplabv3.pth'
            }
            
            # ÌíàÏßà ÏÑ§Ï†ï
            self.quality_settings = {
                'fast': {'input_size': (256, 256), 'confidence_threshold': 0.3},
                'balanced': {'input_size': (512, 512), 'confidence_threshold': 0.5},
                'high': {'input_size': (768, 768), 'confidence_threshold': 0.7},
                'ultra': {'input_size': (1024, 1024), 'confidence_threshold': 0.8}
            }
            
            logger.info("‚úÖ ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌäπÌôî Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌäπÌôî Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self._fallback_initialization()

    def _run_hybrid_ensemble_sync(self, image, person_parsing, pose_info):
        """ÌïòÏù¥Î∏åÎ¶¨Îìú ÏïôÏÉÅÎ∏î Ïã§Ìñâ (ÎèôÍ∏∞)"""
        try:
            # Í∏∞Î≥∏ Ìè¥Î∞± Í≤∞Í≥º Î∞òÌôò
            return {
                'masks': {},
                'confidence': 0.5,
                'method': 'fallback',
                'success': True
            }
        except Exception as e:
            logger.error(f"‚ùå ÌïòÏù¥Î∏åÎ¶¨Îìú ÏïôÏÉÅÎ∏î Ïã§Ìñâ Ïã§Ìå®: {e}")
            return {
                'masks': {},
                'confidence': 0.0,
                'method': 'fallback',
                'success': False,
                'error': str(e)
            }

    def _extract_cloth_features(self, masks, image):
        """ÏùòÎ•ò ÌäπÏßï Ï∂îÏ∂ú"""
        try:
            features = {}
            for mask_name, mask in masks.items():
                if mask is not None and mask.size > 0:
                    features[mask_name] = {
                        'area': int(np.sum(mask)),
                        'centroid': self._calculate_centroid(mask),
                        'bounding_box': self._calculate_bounding_box(mask),
                        'aspect_ratio': self._calculate_aspect_ratio(mask),
                        'compactness': self._calculate_compactness(mask)
                    }
            return features
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò ÌäπÏßï Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _calculate_centroid(self, mask):
        """ÎßàÏä§ÌÅ¨Ïùò Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞"""
        try:
            if mask is None or mask.size == 0:
                return (0, 0)
            
            # ÎßàÏä§ÌÅ¨ÏóêÏÑú 0Ïù¥ ÏïÑÎãå ÌîΩÏÖÄÎì§Ïùò Ï¢åÌëú Ï∞æÍ∏∞
            y_coords, x_coords = np.where(mask > 0)
            
            if len(y_coords) == 0 or len(x_coords) == 0:
                return (0, 0)
            
            # Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞
            centroid_y = int(np.mean(y_coords))
            centroid_x = int(np.mean(x_coords))
            
            return (centroid_x, centroid_y)
        except Exception as e:
            logger.error(f"‚ùå Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return (0, 0)

    def _calculate_bounding_box(self, mask):
        """ÎßàÏä§ÌÅ¨Ïùò Î∞îÏö¥Îî© Î∞ïÏä§ Í≥ÑÏÇ∞"""
        try:
            if mask is None or mask.size == 0:
                return {'x': 0, 'y': 0, 'width': 0, 'height': 0}
            
            # ÎßàÏä§ÌÅ¨ÏóêÏÑú 0Ïù¥ ÏïÑÎãå ÌîΩÏÖÄÎì§Ïùò Ï¢åÌëú Ï∞æÍ∏∞
            y_coords, x_coords = np.where(mask > 0)
            
            if len(y_coords) == 0 or len(x_coords) == 0:
                return {'x': 0, 'y': 0, 'width': 0, 'height': 0}
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Í≥ÑÏÇ∞
            x_min, x_max = int(np.min(x_coords)), int(np.max(x_coords))
            y_min, y_max = int(np.min(y_coords)), int(np.max(y_coords))
            
            return {
                'x': x_min,
                'y': y_min,
                'width': x_max - x_min + 1,
                'height': y_max - y_min + 1
            }
        except Exception as e:
            logger.error(f"‚ùå Î∞îÏö¥Îî© Î∞ïÏä§ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return {'x': 0, 'y': 0, 'width': 0, 'height': 0}

    def _calculate_aspect_ratio(self, mask):
        """ÎßàÏä§ÌÅ¨Ïùò Ï¢ÖÌö°ÎπÑ Í≥ÑÏÇ∞"""
        try:
            if mask is None or mask.size == 0:
                return 1.0
            
            # ÎßàÏä§ÌÅ¨ÏóêÏÑú 0Ïù¥ ÏïÑÎãå ÌîΩÏÖÄÎì§Ïùò Ï¢åÌëú Ï∞æÍ∏∞
            y_coords, x_coords = np.where(mask > 0)
            
            if len(y_coords) == 0 or len(x_coords) == 0:
                return 1.0
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Í≥ÑÏÇ∞
            x_min, x_max = int(np.min(x_coords)), int(np.max(x_coords))
            y_min, y_max = int(np.min(y_coords)), int(np.max(y_coords))
            
            width = x_max - x_min + 1
            height = y_max - y_min + 1
            
            if height == 0:
                return 1.0
            
            return width / height
        except Exception as e:
            logger.error(f"‚ùå Ï¢ÖÌö°ÎπÑ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 1.0

    def _calculate_compactness(self, mask):
        """ÎßàÏä§ÌÅ¨Ïùò Ïª¥Ìå©Ìä∏ÎãàÏä§ Í≥ÑÏÇ∞"""
        try:
            if mask is None or mask.size == 0:
                return 0.0
            
            # ÎßàÏä§ÌÅ¨ÏóêÏÑú 0Ïù¥ ÏïÑÎãå ÌîΩÏÖÄÎì§Ïùò Ï¢åÌëú Ï∞æÍ∏∞
            y_coords, x_coords = np.where(mask > 0)
            
            if len(y_coords) == 0 or len(x_coords) == 0:
                return 0.0
            
            # Î©¥Ï†ÅÍ≥º ÎëòÎ†à Í≥ÑÏÇ∞
            area = len(y_coords)
            
            # ÎëòÎ†à Í≥ÑÏÇ∞ (Í∞ÑÎã®Ìïú Î∞©Î≤ï)
            perimeter = 0
            for i in range(len(y_coords)):
                y, x = y_coords[i], x_coords[i]
                # 4Î∞©Ìñ• Ïù¥ÏõÉ ÌôïÏù∏
                neighbors = [
                    (y-1, x), (y+1, x), (y, x-1), (y, x+1)
                ]
                for ny, nx in neighbors:
                    if (ny < 0 or ny >= mask.shape[0] or 
                        nx < 0 or nx >= mask.shape[1] or 
                        mask[ny, nx] == 0):
                        perimeter += 1
            
            if perimeter == 0:
                return 0.0
            
            # Ïª¥Ìå©Ìä∏ÎãàÏä§ = 4œÄ * Î©¥Ï†Å / ÎëòÎ†à^2
            compactness = (4 * np.pi * area) / (perimeter ** 2)
            return float(compactness)
        except Exception as e:
            logger.error(f"‚ùå Ïª¥Ìå©Ìä∏ÎãàÏä§ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0

    def _get_cloth_bounding_boxes(self, masks):
        """ÏùòÎ•ò Î∞îÏö¥Îî© Î∞ïÏä§Îì§ Î∞òÌôò"""
        try:
            bounding_boxes = {}
            for mask_name, mask in masks.items():
                if mask is not None and mask.size > 0:
                    bounding_boxes[mask_name] = self._calculate_bounding_box(mask)
            return bounding_boxes
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò Î∞îÏö¥Îî© Î∞ïÏä§ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _get_cloth_centroids(self, masks):
        """ÏùòÎ•ò Ï§ëÏã¨Ï†êÎì§ Î∞òÌôò"""
        try:
            centroids = {}
            for mask_name, mask in masks.items():
                if mask is not None and mask.size > 0:
                    centroids[mask_name] = self._calculate_centroid(mask)
            return centroids
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò Ï§ëÏã¨Ï†ê Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _get_cloth_areas(self, masks):
        """ÏùòÎ•ò Î©¥Ï†ÅÎì§ Î∞òÌôò"""
        try:
            areas = {}
            for mask_name, mask in masks.items():
                if mask is not None and mask.size > 0:
                    areas[mask_name] = int(np.sum(mask))
            return areas
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò Î©¥Ï†Å Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _detect_cloth_categories(self, masks):
        """ÏùòÎ•ò Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞êÏßÄ"""
        try:
            categories = []
            for mask_name, mask in masks.items():
                if mask is not None and mask.size > 0:
                    # Í∞ÑÎã®Ìïú Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞êÏßÄ Î°úÏßÅ
                    aspect_ratio = self._calculate_aspect_ratio(mask)
                    area = np.sum(mask)
                    
                    if aspect_ratio > 1.5:  # ÏÑ∏Î°úÍ∞Ä Í∏¥ Í≤ΩÏö∞
                        categories.append('pants')
                    elif aspect_ratio < 0.8:  # Í∞ÄÎ°úÍ∞Ä Í∏¥ Í≤ΩÏö∞
                        categories.append('dress')
                    else:  # Ï†ïÏÇ¨Í∞ÅÌòïÏóê Í∞ÄÍπåÏö¥ Í≤ΩÏö∞
                        categories.append('shirt')
            
            return list(set(categories))  # Ï§ëÎ≥µ Ï†úÍ±∞
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞êÏßÄ Ïã§Ìå®: {e}")
            return []

    def initialize(self) -> bool:
        """Ï¥àÍ∏∞Ìôî"""
        try:
            if not super().initialize():
                return False
            
            # Î™®Îç∏ Î°úÎî©
            if not self._load_segmentation_models():
                logger.warning("Î™®Îç∏ Î°úÎî© Ïã§Ìå®")
                return False
            
            self.segmentation_ready = True
            self.is_initialized = True
            logger.info("‚úÖ ClothSegmentationStepModularized Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            logger.error(f"Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False

    def _load_segmentation_models(self) -> bool:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏Îì§ÏùÑ Î°úÎî©"""
        try:
            self.logger.info("üîÑ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏ Î°úÎî© ÏãúÏûë...")
            
            # Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï (Ïã§Ï†ú Í≤ΩÎ°ú ÏÇ¨Ïö©)
            base_path = os.path.join(os.path.dirname(__file__), '../../../../../backend/ai_models/step_03_cloth_segmentation')
            if not os.path.exists(base_path):
                base_path = os.path.join(os.path.dirname(__file__), '../../../../../backend/ai_models/step_03')
            
            model_paths = {
                'u2net_cloth': os.path.join(base_path, 'u2net.pth'),
                'sam_huge': os.path.join(base_path, 'sam_vit_h_4b8939.pth'),
                'deeplabv3_plus': os.path.join(base_path, 'deeplabv3_resnet101_coco.pth')
            }
            
            success_count = 0
            
            # 1. U2Net Î™®Îç∏ Î°úÎî©
            try:
                u2net_path = model_paths['u2net_cloth']
                if os.path.exists(u2net_path):
                    self.logger.info(f"üîÑ U2Net Î™®Îç∏ Î°úÎî© ÏãúÎèÑ: {u2net_path}")
                    
                    # Ïã§Ï†ú U2Net Î™®Îç∏ Î°úÎî©
                    try:
                        # Ï†àÎåÄ Í≤ΩÎ°úÎ°ú import ÏãúÎèÑ
                        import sys
                        sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                        try:
                            from model_architectures import U2NetModel
                        except ImportError:
                            # Îã§Î•∏ Í≤ΩÎ°ú ÏãúÎèÑ
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                            from model_architectures import U2NetModel
                        
                        u2net_model = U2NetModel(out_channels=1)
                        checkpoint = torch.load(u2net_path, map_location='cpu', weights_only=True)
                        
                        # ÌÇ§ Îß§Ìïë Í∞úÏÑ†
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ Î∞è ÌÇ§ Îß§Ìïë
                        mapped_state_dict = {}
                        for key, value in state_dict.items():
                            # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞
                            if key.startswith('module.'):
                                mapped_key = key[7:]
                            else:
                                mapped_key = key
                            
                            # U2Net ÌäπÏ†ï ÌÇ§ Îß§Ìïë
                            if 'features.' in mapped_key:
                                mapped_key = mapped_key.replace('features.', 'encoder.')
                            elif 'backbone.' in mapped_key:
                                mapped_key = mapped_key.replace('backbone.', 'encoder.')
                            
                            mapped_state_dict[mapped_key] = value
                        
                        # Î™®Îç∏Ïóê Í∞ÄÏ§ëÏπò Î°úÎìú (strict=FalseÎ°ú ÎàÑÎùΩÎêú ÌÇ§ ÌóàÏö©)
                        missing_keys, unexpected_keys = u2net_model.load_state_dict(mapped_state_dict, strict=False)
                        if missing_keys:
                            self.logger.warning(f"‚ö†Ô∏è U2Net Î™®Îç∏ Î°úÎî© Ïãú ÎàÑÎùΩÎêú ÌÇ§: {len(missing_keys)}Í∞ú")
                        if unexpected_keys:
                            self.logger.warning(f"‚ö†Ô∏è U2Net Î™®Îç∏ Î°úÎî© Ïãú ÏòàÏÉÅÏπò Î™ªÌïú ÌÇ§: {len(unexpected_keys)}Í∞ú")
                        
                        u2net_model.eval()
                        u2net_model.to(self.device)
                        
                        self.segmentation_models['u2net_cloth'] = u2net_model
                        self.models_loading_status['u2net_cloth'] = True
                        self.loaded_models['u2net_cloth'] = u2net_model
                        success_count += 1
                        self.logger.info("‚úÖ U2Net Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Ïã§Ï†ú Î™®Îç∏)")
                        
                    except Exception as e:
                        self.logger.error(f"‚ùå U2Net Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                        # Ïã§Ï†ú U2Net Î™®Îç∏ ÏÉùÏÑ± (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)
                        try:
                            import sys
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                            try:
                                from model_architectures import U2NetModel
                            except ImportError:
                                sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                                from model_architectures import U2NetModel
                            u2net_model = U2NetModel(out_channels=1)
                            u2net_model.eval()
                            u2net_model.to(self.device)
                            
                            self.segmentation_models['u2net_cloth'] = u2net_model
                            self.models_loading_status['u2net_cloth'] = True
                            self.loaded_models['u2net_cloth'] = u2net_model
                            success_count += 1
                            self.logger.info("‚úÖ U2Net Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)")
                        except Exception as e2:
                            self.logger.error(f"‚ùå U2Net Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e2}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è U2Net Î™®Îç∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {u2net_path}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå U2Net Î™®Îç∏ Î°úÎî© Ï§ë Ïò§Î•ò: {e}")
            
            # 2. SAM Î™®Îç∏ Î°úÎî©
            try:
                sam_path = model_paths['sam_huge']
                if os.path.exists(sam_path):
                    self.logger.info(f"üîÑ SAM Î™®Îç∏ Î°úÎî© ÏãúÎèÑ: {sam_path}")
                    
                    try:
                        import sys
                        sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                        try:
                            from model_architectures import SAMModel
                        except ImportError:
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                            from model_architectures import SAMModel
                        sam_model = SAMModel()
                        checkpoint = torch.load(sam_path, map_location='cpu', weights_only=True)
                        
                        # ÌÇ§ Îß§Ìïë Í∞úÏÑ†
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ Î∞è ÌÇ§ Îß§Ìïë
                        mapped_state_dict = {}
                        for key, value in state_dict.items():
                            # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞
                            if key.startswith('module.'):
                                mapped_key = key[7:]
                            else:
                                mapped_key = key
                            
                            # SAM ÌäπÏ†ï ÌÇ§ Îß§Ìïë
                            if 'image_encoder.' in mapped_key:
                                mapped_key = mapped_key.replace('image_encoder.', 'backbone.')
                            elif 'neck.' in mapped_key:
                                mapped_key = mapped_key.replace('neck.', 'backbone.')
                            
                            mapped_state_dict[mapped_key] = value
                        
                        # Î™®Îç∏Ïóê Í∞ÄÏ§ëÏπò Î°úÎìú (strict=FalseÎ°ú ÎàÑÎùΩÎêú ÌÇ§ ÌóàÏö©)
                        missing_keys, unexpected_keys = sam_model.load_state_dict(mapped_state_dict, strict=False)
                        if missing_keys:
                            self.logger.warning(f"‚ö†Ô∏è SAM Î™®Îç∏ Î°úÎî© Ïãú ÎàÑÎùΩÎêú ÌÇ§: {len(missing_keys)}Í∞ú")
                        if unexpected_keys:
                            self.logger.warning(f"‚ö†Ô∏è SAM Î™®Îç∏ Î°úÎî© Ïãú ÏòàÏÉÅÏπò Î™ªÌïú ÌÇ§: {len(unexpected_keys)}Í∞ú")
                        
                        sam_model.eval()
                        sam_model.to(self.device)
                        
                        self.segmentation_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        self.loaded_models['sam_huge'] = sam_model
                        success_count += 1
                        self.logger.info("‚úÖ SAM Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Ïã§Ï†ú Î™®Îç∏)")
                        
                    except Exception as e:
                        self.logger.error(f"‚ùå SAM Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                        # Ïã§Ï†ú SAM Î™®Îç∏ ÏÉùÏÑ± (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)
                        try:
                            import sys
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                            try:
                                from model_architectures import SAMModel
                            except ImportError:
                                sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                                from model_architectures import SAMModel
                            sam_model = SAMModel()
                            sam_model.eval()
                            sam_model.to(self.device)
                            
                            self.segmentation_models['sam_huge'] = sam_model
                            self.models_loading_status['sam_huge'] = True
                            self.loaded_models['sam_huge'] = sam_model
                            success_count += 1
                            self.logger.info("‚úÖ SAM Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)")
                        except Exception as e2:
                            self.logger.error(f"‚ùå SAM Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e2}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è SAM Î™®Îç∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {sam_path}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå SAM Î™®Îç∏ Î°úÎî© Ï§ë Ïò§Î•ò: {e}")
            
            # 3. DeepLabV3+ Î™®Îç∏ Î°úÎî©
            try:
                deeplabv3_path = model_paths['deeplabv3_plus']
                if os.path.exists(deeplabv3_path):
                    self.logger.info(f"üîÑ DeepLabV3+ Î™®Îç∏ Î°úÎî© ÏãúÎèÑ: {deeplabv3_path}")
                    
                    try:
                        import sys
                        sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                        try:
                            from model_architectures import DeepLabV3PlusModel
                        except ImportError:
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                            from model_architectures import DeepLabV3PlusModel
                        deeplabv3_model = DeepLabV3PlusModel(num_classes=21)
                        checkpoint = torch.load(deeplabv3_path, map_location='cpu', weights_only=True)
                        
                        # ÌÇ§ Îß§Ìïë Í∞úÏÑ†
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ Î∞è ÌÇ§ Îß§Ìïë
                        mapped_state_dict = {}
                        for key, value in state_dict.items():
                            # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞
                            if key.startswith('module.'):
                                mapped_key = key[7:]
                            else:
                                mapped_key = key
                            
                            # DeepLabV3+ ÌäπÏ†ï ÌÇ§ Îß§Ìïë
                            if 'backbone.' in mapped_key:
                                mapped_key = mapped_key.replace('backbone.', 'encoder.')
                            elif 'classifier.' in mapped_key:
                                mapped_key = mapped_key.replace('classifier.', 'decoder.')
                            
                            mapped_state_dict[mapped_key] = value
                        
                        # Î™®Îç∏Ïóê Í∞ÄÏ§ëÏπò Î°úÎìú (strict=FalseÎ°ú ÎàÑÎùΩÎêú ÌÇ§ ÌóàÏö©)
                        missing_keys, unexpected_keys = deeplabv3_model.load_state_dict(mapped_state_dict, strict=False)
                        if missing_keys:
                            self.logger.warning(f"‚ö†Ô∏è DeepLabV3+ Î™®Îç∏ Î°úÎî© Ïãú ÎàÑÎùΩÎêú ÌÇ§: {len(missing_keys)}Í∞ú")
                        if unexpected_keys:
                            self.logger.warning(f"‚ö†Ô∏è DeepLabV3+ Î™®Îç∏ Î°úÎî© Ïãú ÏòàÏÉÅÏπò Î™ªÌïú ÌÇ§: {len(unexpected_keys)}Í∞ú")
                        
                        deeplabv3_model.eval()
                        deeplabv3_model.to(self.device)
                        
                        self.segmentation_models['deeplabv3_plus'] = deeplabv3_model
                        self.models_loading_status['deeplabv3_plus'] = True
                        self.loaded_models['deeplabv3_plus'] = deeplabv3_model
                        success_count += 1
                        self.logger.info("‚úÖ DeepLabV3+ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Ïã§Ï†ú Î™®Îç∏)")
                        
                    except Exception as e:
                        self.logger.error(f"‚ùå DeepLabV3+ Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                        # Ïã§Ï†ú DeepLabV3+ Î™®Îç∏ ÏÉùÏÑ± (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)
                        try:
                            import sys
                            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../models'))
                            try:
                                from model_architectures import DeepLabV3PlusModel
                            except ImportError:
                                sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../backend/app/ai_pipeline/models'))
                                from model_architectures import DeepLabV3PlusModel
                            deeplabv3_model = DeepLabV3PlusModel(num_classes=21)
                            deeplabv3_model.eval()
                            deeplabv3_model.to(self.device)
                            
                            self.segmentation_models['deeplabv3_plus'] = deeplabv3_model
                            self.models_loading_status['deeplabv3_plus'] = True
                            self.loaded_models['deeplabv3_plus'] = deeplabv3_model
                            success_count += 1
                            self.logger.info("‚úÖ DeepLabV3+ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ (Í∞ÄÏ§ëÏπò ÏóÜÏù¥)")
                        except Exception as e2:
                            self.logger.error(f"‚ùå DeepLabV3+ Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e2}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è DeepLabV3+ Î™®Îç∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {deeplabv3_path}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå DeepLabV3+ Î™®Îç∏ Î°úÎî© Ï§ë Ïò§Î•ò: {e}")
            
            if success_count > 0:
                self.segmentation_ready = True
                self.logger.info(f"üéØ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {success_count}/3 ÏÑ±Í≥µ")
                return True
            else:
                self.logger.warning("‚ö†Ô∏è Î™®Îì† Î™®Îç∏ Î°úÎî© Ïã§Ìå®")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False

    def process(self, **kwargs) -> Dict[str, Any]:
        """ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ï≤òÎ¶¨ (Ïã§Ï†ú Î∂ÑÌï† Í∏∞Îä•)"""
        try:
            logger.info("üî• ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ï≤òÎ¶¨ ÏãúÏûë")
            
            # 1. ÏûÖÎ†• Í≤ÄÏ¶ù
            if not self._validate_input(kwargs):
                return self._create_error_response("ÏûÖÎ†• Í≤ÄÏ¶ù Ïã§Ìå®")
            
            # 2. Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú
            image = kwargs.get('image')
            if image is None:
                return self._create_error_response("Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§")
            
            # 3. Ïù¥ÎØ∏ÏßÄ ÌíàÏßà ÌèâÍ∞Ä
            quality_scores = self._assess_image_quality(image)
            logger.info(f"Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Ï†êÏàò: {quality_scores}")
            
            # 4. ÌíàÏßà Î†àÎ≤® Í≤∞Ï†ï
            quality_level = self._determine_quality_level(kwargs, quality_scores)
            logger.info(f"ÌíàÏßà Î†àÎ≤®: {quality_level.value}")
            
            # 5. Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            processed_image = self._preprocess_image(image, quality_level)
            
            # üî• 6. Processors Ï†ÅÏö©
            if self.high_resolution_processor and quality_level == QualityLevel.ULTRA:
                processed_image = self.high_resolution_processor.process(processed_image)
                logger.info("‚úÖ Í≥†Ìï¥ÏÉÅÎèÑ Ï≤òÎ¶¨ Ï†ÅÏö©")
            
            if self.special_case_processor:
                special_cases = self.special_case_processor.detect_special_cases(processed_image)
                if special_cases:
                    processed_image = self.special_case_processor.apply_special_case_enhancement(processed_image, special_cases)
                    logger.info(f"‚úÖ ÌäπÏàò ÏºÄÏù¥Ïä§ Ï≤òÎ¶¨ Ï†ÅÏö©: {list(special_cases.keys())}")
            
            if self.quality_enhancer:
                processed_image = self.quality_enhancer.enhance_image_quality(processed_image)
                logger.info("‚úÖ Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Ìñ•ÏÉÅ Ï†ÅÏö©")
            
            # 7. AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ
            start_time = time.time()
            person_parsing = kwargs.get('person_parsing', {})
            pose_info = kwargs.get('pose_info', {})
            
            logger.info("üî• AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ ÏãúÏûë")
            result = self._run_ai_segmentation_sync(processed_image, quality_level, person_parsing, pose_info)
            processing_time = time.time() - start_time
            
            logger.info(f"üîç AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Í≤∞Í≥º: {result}")
            
            # 8. ÎßàÏä§ÌÅ¨ ÌõÑÏ≤òÎ¶¨
            if result.get('masks'):
                logger.info(f"üîç ÎßàÏä§ÌÅ¨ ÌõÑÏ≤òÎ¶¨ ÏãúÏûë: {len(result['masks'])}Í∞ú ÎßàÏä§ÌÅ¨")
                result['masks'] = self._postprocess_masks(result['masks'])
                
                # üî• 9. Advanced Post Processing Ï†ÅÏö©
                if self.advanced_post_processor:
                    for mask_key, mask in result['masks'].items():
                        if mask is not None and mask.size > 0:
                            # CRF ÌõÑÏ≤òÎ¶¨
                            if quality_level == QualityLevel.ULTRA:
                                result['masks'][mask_key] = self.advanced_post_processor.apply_crf_postprocessing(
                                    mask, processed_image, num_iterations=15
                                )
                            
                            # Î©ÄÌã∞Ïä§ÏºÄÏùº Ï≤òÎ¶¨
                            if quality_level in [QualityLevel.HIGH, QualityLevel.ULTRA]:
                                result['masks'][mask_key] = self.advanced_post_processor.apply_multiscale_processing(
                                    processed_image, result['masks'][mask_key]
                                )
                    
                    # Ïó£ÏßÄ Ï†ïÏ†ú
                    result['masks'] = self.advanced_post_processor.apply_edge_refinement(result['masks'], processed_image)
                    logger.info("‚úÖ Í≥†Í∏â ÌõÑÏ≤òÎ¶¨ Ï†ÅÏö©")
                
                # üî• 10. Quality Enhancement Ï†ÅÏö©
                if self.quality_enhancer:
                    result['masks'] = self.quality_enhancer.enhance_segmentation_quality(result['masks'], processed_image)
                    logger.info("‚úÖ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌíàÏßà Ìñ•ÏÉÅ Ï†ÅÏö©")
                
                # 11. ÌäπÏÑ± Ï∂îÏ∂ú
                result['features'] = self._extract_cloth_features(result['masks'], processed_image)
                result['bounding_boxes'] = self._get_cloth_bounding_boxes(result['masks'])
                result['centroids'] = self._get_cloth_centroids(result['masks'])
                result['areas'] = self._get_cloth_areas(result['masks'])
                result['contours'] = self._get_cloth_contours_dict(result['masks'])
                result['categories'] = self._detect_cloth_categories(result['masks'])
                
                # 12. Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
                confidence = self._calculate_segmentation_confidence(result['masks'], processed_image)
                result['confidence'] = confidence
                
                # 13. ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
                if self.config.enable_visualization:
                    result['visualizations'] = self._create_segmentation_visualization(processed_image, result['masks'])
            
            # 14. ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self._update_ai_stats('modularized', result.get('confidence', 0.5), processing_time, quality_scores)
            
            # 15. Ï∂úÎ†• Í≤ÄÏ¶ù
            if not self._validate_output(result):
                return self._create_error_response("Ï∂úÎ†• Í≤ÄÏ¶ù Ïã§Ìå®")
            
            # 16. ÏµúÏ¢Ö Í≤∞Í≥º Î∞òÌôò
            result['success'] = True
            result['processing_time'] = processing_time
            result['quality_scores'] = quality_scores
            result['quality_level'] = quality_level.value
            result['method'] = 'modularized'
            result['processors_used'] = {
                'high_resolution': self.high_resolution_processor is not None,
                'special_case': self.special_case_processor is not None,
                'advanced_post': self.advanced_post_processor is not None,
                'quality_enhancer': self.quality_enhancer is not None
            }
            
            logger.info(f"‚úÖ ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÏôÑÎ£å (Ï≤òÎ¶¨ÏãúÍ∞Ñ: {processing_time:.2f}s, Ïã†Î¢∞ÎèÑ: {result.get('confidence', 0.0):.3f})")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            return self._create_error_response(f"Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")

    def _validate_input(self, kwargs: Dict[str, Any]) -> bool:
        """ÏûÖÎ†• Í≤ÄÏ¶ù"""
        try:
            required_keys = ['image']
            for key in required_keys:
                if key not in kwargs:
                    logger.warning(f"ÌïÑÏàò ÏûÖÎ†• ÌÇ§ ÎàÑÎùΩ: {key}")
                    return False
            
            image = kwargs.get('image')
            if image is None or not isinstance(image, NDArray):
                logger.warning("Ïù¥ÎØ∏ÏßÄÍ∞Ä numpy Î∞∞Ïó¥Ïù¥ ÏïÑÎãôÎãàÎã§")
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"ÏûÖÎ†• Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

    def _validate_output(self, result: Dict[str, Any]) -> bool:
        """Ï∂úÎ†• Í≤ÄÏ¶ù"""
        try:
            if not isinstance(result, dict):
                return False
            
            # ÌïÑÏàò ÌÇ§Îì§Ïù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
            required_keys = ['masks', 'confidence', 'method']
            for key in required_keys:
                if key not in result:
                    return False
            
            masks = result['masks']
            if not isinstance(masks, dict):
                return False
            
            # ÎßàÏä§ÌÅ¨Í∞Ä ÏóÜÏñ¥ÎèÑ ÏÑ±Í≥µÏúºÎ°ú Ï≤òÎ¶¨ (Ìè¥Î∞± Î™®Îìú)
            if not masks:
                logger.info("‚ö†Ô∏è ÎßàÏä§ÌÅ¨Í∞Ä ÏóÜÏßÄÎßå Ìè¥Î∞± Î™®ÎìúÎ°ú ÏÑ±Í≥µ Ï≤òÎ¶¨")
                return True
            
            # Í∞Å ÎßàÏä§ÌÅ¨Í∞Ä Ïú†Ìö®ÌïúÏßÄ ÌôïÏù∏
            for mask_type, mask in masks.items():
                if mask is not None and mask.size > 0:
                    return True
            
            # Î™®Îì† ÎßàÏä§ÌÅ¨Í∞Ä ÎπÑÏñ¥ÏûàÏñ¥ÎèÑ ÏÑ±Í≥µÏúºÎ°ú Ï≤òÎ¶¨
            logger.info("‚ö†Ô∏è Î™®Îì† ÎßàÏä§ÌÅ¨Í∞Ä ÎπÑÏñ¥ÏûàÏßÄÎßå Ìè¥Î∞± Î™®ÎìúÎ°ú ÏÑ±Í≥µ Ï≤òÎ¶¨")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï∂úÎ†• Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

    def _determine_quality_level(self, kwargs: Dict[str, Any], quality_scores: Dict[str, float]) -> QualityLevel:
        """ÌíàÏßà Î†àÎ≤® Í≤∞Ï†ï"""
        try:
            # ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú ÌíàÏßà Î†àÎ≤®Ïù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
            if 'quality_level' in kwargs:
                quality_level = kwargs['quality_level']
                if isinstance(quality_level, QualityLevel):
                    return quality_level
                elif isinstance(quality_level, str):
                    for level in QualityLevel:
                        if level.value == quality_level:
                            return level
            
            # Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Í∏∞Î∞ò ÏûêÎèô Í≤∞Ï†ï
            brightness = quality_scores.get('brightness', 0.5)
            contrast = quality_scores.get('contrast', 0.5)
            sharpness = quality_scores.get('sharpness', 0.5)
            
            # ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
            quality_score = (brightness + contrast + sharpness) / 3.0
            
            if quality_score > 0.8:
                return QualityLevel.ULTRA
            elif quality_score > 0.6:
                return QualityLevel.HIGH
            elif quality_score > 0.4:
                return QualityLevel.BALANCED
            else:
                return QualityLevel.FAST
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌíàÏßà Î†àÎ≤® Í≤∞Ï†ï Ïã§Ìå®: {e}")
            return QualityLevel.BALANCED

    def _preprocess_image(self, image: NDArray, quality_level: QualityLevel) -> NDArray:
        """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
        try:
            if image is None:
                return image
            
            processed_image = image.copy()
            
            # ÌíàÏßà Î†àÎ≤®Ïóê Îî∞Î•∏ Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©
            if quality_level in [QualityLevel.HIGH, QualityLevel.ULTRA]:
                # Í≥†ÌíàÏßà Ï†ÑÏ≤òÎ¶¨
                if hasattr(self, '_normalize_lighting'):
                    processed_image = self._normalize_lighting(processed_image)
                
                if hasattr(self, '_correct_colors'):
                    processed_image = self._correct_colors(processed_image)
            
            # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï
            target_size = self.config.input_size if hasattr(self.config, 'input_size') else (512, 512)
            if processed_image.shape[:2] != target_size:
                processed_image = cv2.resize(processed_image, target_size)
            
            return processed_image
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return image

    def _normalize_lighting(self, image: NDArray) -> NDArray:
        """Ï°∞Î™Ö Ï†ïÍ∑úÌôî"""
        try:
            if image is None:
                return image
            
            # LAB ÏÉâÍ≥µÍ∞ÑÏúºÎ°ú Î≥ÄÌôò
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            
            # L Ï±ÑÎÑê Ï†ïÍ∑úÌôî
            l_channel = lab[:, :, 0]
            l_mean = np.mean(l_channel)
            l_std = np.std(l_channel)
            
            # Ï†ïÍ∑úÌôî Ï†ÅÏö©
            l_normalized = (l_channel - l_mean) / (l_std + 1e-8)
            l_normalized = np.clip(l_normalized * 50 + 128, 0, 255).astype(np.uint8)
            
            # Ï†ïÍ∑úÌôîÎêú L Ï±ÑÎÑêÎ°ú ÍµêÏ≤¥
            lab[:, :, 0] = l_normalized
            
            # RGBÎ°ú Î≥ÄÌôò
            normalized_image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            
            return normalized_image
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï°∞Î™Ö Ï†ïÍ∑úÌôî Ïã§Ìå®: {e}")
            return image

    def _correct_colors(self, image: NDArray) -> NDArray:
        """ÏÉâÏÉÅ Î≥¥Ï†ï"""
        try:
            if image is None:
                return image
            
            # ÌûàÏä§ÌÜ†Í∑∏Îû® ÌèâÌôúÌôî
            corrected_image = image.copy()
            
            # Í∞Å Ï±ÑÎÑêÎ≥Ñ ÌûàÏä§ÌÜ†Í∑∏Îû® ÌèâÌôúÌôî
            for i in range(3):
                corrected_image[:, :, i] = cv2.equalizeHist(corrected_image[:, :, i])
            
            return corrected_image
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÉâÏÉÅ Î≥¥Ï†ï Ïã§Ìå®: {e}")
            return image

    def _assess_image_quality(self, image: NDArray) -> Dict[str, float]:
        """Ïù¥ÎØ∏ÏßÄ ÌíàÏßà ÌèâÍ∞Ä"""
        try:
            if image is None:
                return {'brightness': 0.0, 'contrast': 0.0, 'sharpness': 0.0}
            
            # Î∞ùÍ∏∞ ÌèâÍ∞Ä
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            brightness = np.mean(gray) / 255.0
            
            # ÎåÄÎπÑ ÌèâÍ∞Ä
            contrast = np.std(gray) / 255.0
            
            # ÏÑ†Î™ÖÎèÑ ÌèâÍ∞Ä (Laplacian variance)
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            sharpness = np.var(laplacian) / 1000.0  # Ï†ïÍ∑úÌôî
            
            return {
                'brightness': float(brightness),
                'contrast': float(contrast),
                'sharpness': float(sharpness)
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            return {'brightness': 0.5, 'contrast': 0.5, 'sharpness': 0.5}

    def _run_ai_segmentation_sync(self, image: NDArray, quality_level: QualityLevel, 
                                 person_parsing: Dict[str, Any], pose_info: Dict[str, Any]) -> Dict[str, Any]:
        """AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ (ÎèôÍ∏∞)"""
        try:
            logger.info("üî• AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ ÏãúÏûë")
            logger.info(f"üîç ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ shape: {image.shape}")
            logger.info(f"üîç ÌíàÏßà Î†àÎ≤®: {quality_level.value}")
            
            if not self.segmentation_ready:
                logger.warning("‚ö†Ô∏è ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏Ïù¥ Ï§ÄÎπÑÎêòÏßÄ ÏïäÏùå")
                return self._create_fallback_segmentation_result(image.shape)
            
            # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§ ÌôïÏù∏
            available_models = list(self.segmentation_models.keys())
            if not available_models:
                logger.warning("‚ö†Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Ïù¥ ÏóÜÏùå")
                return self._create_fallback_segmentation_result(image.shape)
            
            logger.info(f"üîç ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§: {available_models}")
            
            # Î™®Îç∏Î≥Ñ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ
            results = {}
            methods_used = []
            execution_times = {}
            
            for model_key in available_models:
                try:
                    logger.info(f"üéØ {model_key} Î™®Îç∏ Ïã§Ìñâ Ï§ë...")
                    start_time = time.time()
                    
                    result = self._safe_model_predict(model_key, image)
                    execution_time = time.time() - start_time
                    
                    logger.info(f"üîç {model_key} Î™®Îç∏ Í≤∞Í≥º: {result}")
                    
                    if result.get('success', False):
                        results[model_key] = result
                        methods_used.append(model_key)
                        execution_times[model_key] = execution_time
                        logger.info(f"‚úÖ {model_key} Î™®Îç∏ Ïã§Ìñâ ÏôÑÎ£å (ÏãúÍ∞Ñ: {execution_time:.2f}s)")
                    else:
                        logger.warning(f"‚ö†Ô∏è {model_key} Î™®Îç∏ Ïã§Ìñâ Ïã§Ìå®: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    logger.error(f"‚ùå {model_key} Î™®Îç∏ Ïã§Ìñâ Ïã§Ìå®: {e}")
            
            logger.info(f"üîç ÏàòÏßëÎêú Í≤∞Í≥ºÎì§: {list(results.keys())}")
            logger.info(f"üîç Í≤∞Í≥º Í∞úÏàò: {len(results)}")
            
            # Í≤∞Í≥º Í≤∞Ìï©
            if results:
                # Í∞ÄÏû• Ï¢ãÏùÄ Í≤∞Í≥º ÏÑ†ÌÉù (Ïã†Î¢∞ÎèÑ Í∏∞Ï§Ä)
                best_result = max(results.values(), key=lambda x: x.get('confidence', 0.0))
                best_method = best_result.get('method', 'unknown')
                
                logger.info(f"üéØ ÏµúÏ†Å Í≤∞Í≥º: {best_method} (Ïã†Î¢∞ÎèÑ: {best_result.get('confidence', 0.0):.2f})")
                logger.info(f"üîç ÏµúÏ†Å Í≤∞Í≥º ÎßàÏä§ÌÅ¨: {best_result.get('masks', {})}")
                
                return {
                    'success': True,
                    'masks': best_result.get('masks', {}),
                    'confidence': best_result.get('confidence', 0.0),
                    'method_used': best_method,
                    'methods_available': methods_used,
                    'execution_times': execution_times,
                    'quality_level': quality_level.value
                }
            else:
                logger.warning("‚ö†Ô∏è Î™®Îì† Î™®Îç∏ Ïã§Ìñâ Ïã§Ìå®")
                return self._create_fallback_segmentation_result(image.shape)
                
        except Exception as e:
            logger.error(f"‚ùå AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ Ïã§Ìå®: {e}")
            return self._create_fallback_segmentation_result(image.shape)

    def _postprocess_masks(self, masks: Dict[str, NDArray]) -> Dict[str, NDArray]:
        """ÎßàÏä§ÌÅ¨ ÌõÑÏ≤òÎ¶¨ (Ïã§Ï†ú Î∂ÑÌï† ÌíàÏßà Ìñ•ÏÉÅ)"""
        try:
            if not masks:
                return masks
            
            processed_masks = {}
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # 1. ÌôÄ Ï±ÑÏö∞Í∏∞ Î∞è ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
                processed_mask = self._fill_holes_and_remove_noise(mask)
                
                # 2. Í≤ΩÍ≥Ñ Ï†ïÏ†ú
                processed_mask = self._refine_boundaries(processed_mask)
                
                # 3. ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞
                processed_mask = self._remove_small_regions(processed_mask)
                
                processed_masks[mask_type] = processed_mask
            
            logger.info(f"‚úÖ ÎßàÏä§ÌÅ¨ ÌõÑÏ≤òÎ¶¨ ÏôÑÎ£å ({len(processed_masks)}Í∞ú ÎßàÏä§ÌÅ¨)")
            return processed_masks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÎßàÏä§ÌÅ¨ ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return masks

    def _fill_holes_and_remove_noise(self, mask: NDArray) -> NDArray:
        """ÌôÄ Ï±ÑÏö∞Í∏∞ Î∞è ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞"""
        try:
            if mask is None or mask.size == 0:
                return mask
            
            # ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
            kernel = np.ones((3, 3), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            # ÌôÄ Ï±ÑÏö∞Í∏∞
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                filled_mask = np.zeros_like(mask)
                cv2.fillPoly(filled_mask, [largest_contour], 1)
                mask = filled_mask
            
            return mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌôÄ Ï±ÑÏö∞Í∏∞ Î∞è ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞ Ïã§Ìå®: {e}")
            return mask

    def _refine_boundaries(self, mask: NDArray) -> NDArray:
        """Í≤ΩÍ≥Ñ Ï†ïÏ†ú"""
        try:
            if mask is None or mask.size == 0:
                return mask
            
            # Í≤ΩÍ≥Ñ Ïä§Î¨¥Îî©
            kernel = np.ones((3, 3), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            
            return mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Í≤ΩÍ≥Ñ Ï†ïÏ†ú Ïã§Ìå®: {e}")
            return mask

    def _remove_small_regions(self, mask: NDArray, min_area: int = 100) -> NDArray:
        """ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞"""
        try:
            if mask is None or mask.size == 0:
                return mask
            
            # Ïó∞Í≤∞ ÏöîÏÜå Î∂ÑÏÑù
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
            
            # ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞
            for i in range(1, num_labels):  # 0ÏùÄ Î∞∞Í≤Ω
                if stats[i, cv2.CC_STAT_AREA] < min_area:
                    mask[labels == i] = 0
            
            return mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞ Ïã§Ìå®: {e}")
            return mask

    def _create_fallback_segmentation_result(self, image_shape: Tuple[int, ...]) -> Dict[str, Any]:
        """Ìè¥Î∞± ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Í≤∞Í≥º ÏÉùÏÑ± (Ïã§Ï†ú Î∂ÑÌï†)"""
        try:
            # Í∏∞Î≥∏ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
            height, width = image_shape[:2]
            fallback_mask = np.zeros((height, width), dtype=np.uint8)
            
            # Ï§ëÏïôÏóê ÏÇ¨Í∞ÅÌòï ÎßàÏä§ÌÅ¨ ÏÉùÏÑ± (ÏùòÎ•òÍ∞Ä ÏûàÏùÑ Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ ÏòÅÏó≠)
            center_h, center_w = height // 2, width // 2
            size_h, size_w = height // 4, width // 4
            
            h_start = max(0, center_h - size_h)
            h_end = min(height, center_h + size_h)
            w_start = max(0, center_w - size_w)
            w_end = min(width, center_w + size_w)
            
            fallback_mask[h_start:h_end, w_start:w_end] = 1
            
            # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
            category_masks = {
                'shirt': fallback_mask.copy(),
                'pants': fallback_mask.copy(),
                'dress': fallback_mask.copy()
            }
            
            return {
                'masks': category_masks,
                'confidence': 0.3,
                'method': 'fallback',
                'processing_time': 0.0,
                'quality_score': 0.3
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ìè¥Î∞± Í≤∞Í≥º ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {
                'masks': {},
                'confidence': 0.0,
                'method': 'fallback',
                'processing_time': 0.0,
                'quality_score': 0.0
            }

    def _update_ai_stats(self, method: str, confidence: float, total_time: float, quality_metrics: Dict[str, float]):
        """AI ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            self.ai_stats['total_processing_time'] += total_time
            self.ai_stats['inference_time'] += total_time
            self.ai_stats['success_count'] += 1
            self.ai_stats['last_processed_time'] = time.time()
            
            # ÌíàÏßà Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
            if 'quality_score' in quality_metrics:
                self.ai_stats['average_quality'] = (
                    (self.ai_stats.get('average_quality', 0.0) * (self.ai_stats['success_count'] - 1) + 
                     quality_metrics['quality_score']) / self.ai_stats['success_count']
                )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")

    def get_status(self) -> Dict[str, Any]:
        """ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            status = super().get_status()
            status.update({
                'segmentation_ready': self.segmentation_ready,
                'loaded_models': list(self.segmentation_models.keys()),
                'ai_stats': self.ai_stats,
                'config': {
                    'method': self.config.method.value if hasattr(self.config, 'method') else 'unknown',
                    'quality_level': self.config.quality_level.value if hasattr(self.config, 'quality_level') else 'unknown',
                    'input_size': self.config.input_size if hasattr(self.config, 'input_size') else (512, 512)
                },
                'available_methods': list(self.segmentation_methods.keys()) if hasattr(self, 'segmentation_methods') else []
            })
            return status
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'error': str(e)}

    def cleanup(self):
        """Ï†ïÎ¶¨"""
        try:
            # Î™®Îç∏ Ï†ïÎ¶¨
            for model in self.segmentation_models.values():
                if hasattr(model, 'cleanup'):
                    model.cleanup()
            
            self.segmentation_models.clear()
            self.segmentation_ready = False
            
            super().cleanup()
            logger.info("‚úÖ ClothSegmentationStepModularized Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

    def _detect_available_methods(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∞©Î≤ïÎì§ Í∞êÏßÄ"""
        try:
            available_methods = []
            
            # Î™®Îç∏Î≥Ñ Í∞ÄÏö©ÏÑ± ÌôïÏù∏
            if 'u2net_cloth' in self.segmentation_models:
                available_methods.append('u2net_cloth')
            
            if 'sam_huge' in self.segmentation_models:
                available_methods.append('sam_huge')
            
            if 'deeplabv3_plus' in self.segmentation_models:
                available_methods.append('deeplabv3_plus')
            
            # ÌïòÏù¥Î∏åÎ¶¨Îìú ÏïôÏÉÅÎ∏î (Ïó¨Îü¨ Î™®Îç∏Ïù¥ ÏûàÏùÑ Îïå)
            if len(self.segmentation_models) > 1:
                available_methods.append('hybrid_ai')
            
            return available_methods
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∞©Î≤ï Í∞êÏßÄ Ïã§Ìå®: {e}")
            return []

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ÏóêÎü¨ ÏùëÎãµ ÏÉùÏÑ±"""
        return {
            'success': False,
            'error': error_message,
            'masks': {},
            'confidence': 0.0,
            'method': 'error',
            'processing_time': 0.0
        }

    def _run_single_model_segmentation(self, model_key: str, image: NDArray) -> Dict[str, Any]:
        """Îã®Ïùº Î™®Îç∏ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ"""
        try:
            if model_key not in self.segmentation_models:
                return self._create_error_response(f"Î™®Îç∏ {model_key}Í∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏùå")
            
            model = self.segmentation_models[model_key]
            
            if not hasattr(model, 'predict'):
                return self._create_error_response(f"Î™®Îç∏ {model_key}Ïóê predict Î©îÏÑúÎìúÍ∞Ä ÏóÜÏùå")
            
            # Ïã§Ï†ú ÏòàÏ∏° Ïã§Ìñâ
            result = model.predict(image)
            
            if result and 'masks' in result:
                logger.info(f"‚úÖ {model_key} ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÏôÑÎ£å")
                return result
            else:
                return self._create_error_response(f"Î™®Îç∏ {model_key} Í≤∞Í≥ºÍ∞Ä Ïú†Ìö®ÌïòÏßÄ ÏïäÏùå")
                
        except Exception as e:
            logger.error(f"‚ùå {model_key} Î™®Îç∏ Ï∂îÎ°† Ïã§Ìå®: {e}")
            return self._create_error_response(f"Î™®Îç∏ {model_key} Ï∂îÎ°† Ïã§Ìå®: {str(e)}")

    def _enhance_segmentation_quality(self, masks: Dict[str, NDArray], image: NDArray) -> Dict[str, NDArray]:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌíàÏßà Ìñ•ÏÉÅ"""
        try:
            enhanced_masks = {}
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # 1. Í≤ΩÍ≥Ñ Ï†ïÏ†ú
                enhanced_mask = self._refine_boundaries(mask)
                
                # 2. ÌôÄ Ï±ÑÏö∞Í∏∞
                enhanced_mask = self._fill_holes_and_remove_noise(enhanced_mask)
                
                # 3. ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞
                enhanced_mask = self._remove_small_regions(enhanced_mask)
                
                # 4. Î™®Ìè¥Î°úÏßÄ Ïó∞ÏÇ∞ÏúºÎ°ú Ïä§Î¨¥Îî©
                enhanced_mask = self._apply_morphological_operations(enhanced_mask)
                
                enhanced_masks[mask_type] = enhanced_mask
            
            return enhanced_masks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌíàÏßà Ìñ•ÏÉÅ Ïã§Ìå®: {e}")
            return masks

    def _apply_morphological_operations(self, mask: NDArray) -> NDArray:
        """Î™®Ìè¥Î°úÏßÄ Ïó∞ÏÇ∞ Ï†ÅÏö©"""
        try:
            if mask is None or mask.size == 0:
                return mask
            
            # Îã´Í∏∞ Ïó∞ÏÇ∞ÏúºÎ°ú ÌôÄ Ï±ÑÏö∞Í∏∞
            kernel = np.ones((5, 5), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            
            # Ïó¥Í∏∞ Ïó∞ÏÇ∞ÏúºÎ°ú ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            return mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î™®Ìè¥Î°úÏßÄ Ïó∞ÏÇ∞ Ïã§Ìå®: {e}")
            return mask

    def _calculate_segmentation_confidence(self, masks: Dict[str, NDArray], image: NDArray) -> float:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        try:
            if not masks:
                return 0.0
            
            total_confidence = 0.0
            mask_count = 0
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # 1. Î©¥Ï†Å ÎπÑÏú® Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                area_ratio = np.sum(mask) / mask.size
                area_confidence = min(area_ratio * 2, 1.0)  # Ï†ÅÏ†àÌïú Î©¥Ï†Å ÎπÑÏú®Ïóê ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
                
                # 2. Í≤ΩÍ≥Ñ ÌíàÏßà Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                edges = cv2.Canny(mask.astype(np.uint8) * 255, 50, 150)
                edge_density = np.sum(edges) / (edges.size * 255)
                edge_confidence = 1.0 - min(edge_density * 3, 1.0)  # ÎÇÆÏùÄ edge densityÏóê ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
                
                # 3. Ïó∞Í≤∞ÏÑ± Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                connectivity_confidence = 1.0 / (len(contours) + 1)  # Ïª®Ìà¨Ïñ¥Í∞Ä Ï†ÅÏùÑÏàòÎ°ù Ï¢ãÏùå
                
                # Ï¢ÖÌï© Ïã†Î¢∞ÎèÑ
                mask_confidence = (area_confidence * 0.4 + edge_confidence * 0.3 + connectivity_confidence * 0.3)
                total_confidence += mask_confidence
                mask_count += 1
            
            return total_confidence / mask_count if mask_count > 0 else 0.0
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.5

    def _validate_segmentation_result(self, result: Dict[str, Any]) -> bool:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Í≤∞Í≥º Í≤ÄÏ¶ù"""
        try:
            if not isinstance(result, dict):
                return False
            
            if 'masks' not in result:
                return False
            
            masks = result['masks']
            if not isinstance(masks, dict):
                return False
            
            # ÏµúÏÜå ÌïòÎÇòÏùò ÎßàÏä§ÌÅ¨Í∞Ä ÏûàÏñ¥Ïïº Ìï®
            if not masks:
                return False
            
            # Í∞Å ÎßàÏä§ÌÅ¨Í∞Ä Ïú†Ìö®ÌïúÏßÄ ÌôïÏù∏
            for mask_type, mask in masks.items():
                if mask is not None and mask.size > 0:
                    return True
            
            return False
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Í≤∞Í≥º Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

    def _get_cloth_bounding_boxes(self, masks: Dict[str, NDArray]) -> Dict[str, Dict[str, int]]:
        """ÏùòÎ•ò Î∞îÏö¥Îî© Î∞ïÏä§Îì§ Î∞òÌôò"""
        try:
            bounding_boxes = {}
            
            for mask_type, mask in masks.items():
                if mask is not None and np.any(mask):
                    bbox = self._calculate_bounding_box(mask)
                    bounding_boxes[mask_type] = {
                        'x_min': bbox[0],
                        'y_min': bbox[1],
                        'x_max': bbox[2],
                        'y_max': bbox[3]
                    }
            
            return bounding_boxes
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î∞îÏö¥Îî© Î∞ïÏä§ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _get_cloth_centroids(self, masks: Dict[str, NDArray]) -> Dict[str, Tuple[float, float]]:
        """ÏùòÎ•ò Ï§ëÏã¨Ï†êÎì§ Î∞òÌôò"""
        try:
            centroids = {}
            
            for mask_type, mask in masks.items():
                if mask is not None and np.any(mask):
                    centroid = self._calculate_centroid(mask)
                    centroids[mask_type] = centroid
            
            return centroids
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï§ëÏã¨Ï†ê Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _get_cloth_areas(self, masks: Dict[str, NDArray]) -> Dict[str, int]:
        """ÏùòÎ•ò Î©¥Ï†ÅÎì§ Î∞òÌôò"""
        try:
            areas = {}
            
            for mask_type, mask in masks.items():
                if mask is not None:
                    area = int(np.sum(mask))
                    areas[mask_type] = area
            
            return areas
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î©¥Ï†Å Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _get_cloth_contours_dict(self, masks: Dict[str, NDArray]) -> Dict[str, List[NDArray]]:
        """ÏùòÎ•ò Ïú§Í≥ΩÏÑ†Îì§ Î∞òÌôò"""
        try:
            contours_dict = {}
            
            for mask_type, mask in masks.items():
                if mask is not None:
                    contours = self._extract_cloth_contours(mask)
                    contours_dict[mask_type] = contours
            
            return contours_dict
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïú§Í≥ΩÏÑ† Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}

    def _detect_cloth_categories(self, masks: Dict[str, NDArray]) -> List[str]:
        """ÏùòÎ•ò Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞êÏßÄ"""
        try:
            categories = []
            
            for mask_type, mask in masks.items():
                if mask is not None and np.any(mask):
                    # ÎßàÏä§ÌÅ¨ ÌÉÄÏûÖÏùÑ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Î≥ÄÌôò
                    category = mask_type.replace('_', ' ').title()
                    categories.append(category)
            
            return categories
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞êÏßÄ Ïã§Ìå®: {e}")
            return []

    def _create_segmentation_visualization(self, image: NDArray, masks: Dict[str, NDArray]) -> Dict[str, Any]:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        try:
            if image is None or not masks:
                return {}
            
            visualizations = {}
            
            # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î≥µÏÇ¨
            overlay_image = image.copy()
            
            # ÏÉâÏÉÅ Îß§Ìïë
            colors = [
                [255, 0, 0],    # Îπ®Í∞ï
                [0, 255, 0],    # Ï¥àÎ°ù
                [0, 0, 255],    # ÌååÎûë
                [255, 255, 0],  # ÎÖ∏Îûë
                [255, 0, 255],  # ÎßàÏ††ÌÉÄ
                [0, 255, 255]   # ÏãúÏïà
            ]
            
            # ÎßàÏä§ÌÅ¨ Ïò§Î≤ÑÎ†àÏù¥ ÏÉùÏÑ±
            for i, (mask_type, mask) in enumerate(masks.items()):
                if mask is not None and np.any(mask):
                    color = colors[i % len(colors)]
                    
                    # ÎßàÏä§ÌÅ¨Î•º 3Ï±ÑÎÑêÎ°ú ÌôïÏû•
                    mask_3d = np.stack([mask, mask, mask], axis=-1)
                    
                    # ÏÉâÏÉÅ Ï†ÅÏö©
                    colored_mask = np.array(color) * mask_3d
                    
                    # ÏïåÌåå Î∏îÎ†åÎî©
                    alpha = 0.6
                    overlay_image = overlay_image * (1 - alpha * mask_3d) + colored_mask * alpha * mask_3d
            
            visualizations['overlay'] = overlay_image.astype(np.uint8)
            
            # Í∞úÎ≥Ñ ÎßàÏä§ÌÅ¨ ÏãúÍ∞ÅÌôî
            for mask_type, mask in masks.items():
                if mask is not None:
                    visualizations[f'mask_{mask_type}'] = (mask * 255).astype(np.uint8)
            
            return visualizations
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {}

    def _calculate_segmentation_confidence(self, masks: Dict[str, NDArray], image: NDArray) -> float:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        try:
            if not masks:
                return 0.0
            
            total_confidence = 0.0
            mask_count = 0
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # 1. Î©¥Ï†Å ÎπÑÏú® Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                area_ratio = np.sum(mask) / mask.size
                area_confidence = min(area_ratio * 2, 1.0)  # Ï†ÅÏ†àÌïú Î©¥Ï†Å ÎπÑÏú®Ïóê ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
                
                # 2. Í≤ΩÍ≥Ñ ÌíàÏßà Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                edges = cv2.Canny(mask.astype(np.uint8) * 255, 50, 150)
                edge_density = np.sum(edges) / (edges.size * 255)
                edge_confidence = 1.0 - min(edge_density * 3, 1.0)  # ÎÇÆÏùÄ edge densityÏóê ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
                
                # 3. Ïó∞Í≤∞ÏÑ± Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ
                contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                connectivity_confidence = 1.0 / (len(contours) + 1)  # Ïª®Ìà¨Ïñ¥Í∞Ä Ï†ÅÏùÑÏàòÎ°ù Ï¢ãÏùå
                
                # Ï¢ÖÌï© Ïã†Î¢∞ÎèÑ
                mask_confidence = (area_confidence * 0.4 + edge_confidence * 0.3 + connectivity_confidence * 0.3)
                total_confidence += mask_confidence
                mask_count += 1
            
            return total_confidence / mask_count if mask_count > 0 else 0.0
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.5

    def _extract_cloth_contours(self, mask: NDArray) -> List[NDArray]:
        """ÏùòÎ•ò Ïú§Í≥ΩÏÑ† Ï∂îÏ∂ú"""
        try:
            if mask is None or mask.size == 0:
                return []
            
            # ÎßàÏä§ÌÅ¨Î•º uint8Î°ú Î≥ÄÌôò
            mask_uint8 = mask.astype(np.uint8)
            
            # Ïú§Í≥ΩÏÑ† Ï∞æÍ∏∞
            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # ÏûëÏùÄ Ïú§Í≥ΩÏÑ† ÌïÑÌÑ∞ÎßÅ
            min_area = 50
            filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > min_area]
            
            return filtered_contours
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïú§Í≥ΩÏÑ† Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return []

    def _apply_ultra_quality_postprocessing(self, masks: Dict[str, NDArray], image: NDArray,
                                          person_parsing: Dict[str, Any], pose_info: Dict[str, Any]) -> Dict[str, NDArray]:
        """Ïö∏Ìä∏Îùº ÌíàÏßà ÌõÑÏ≤òÎ¶¨"""
        try:
            processed_masks = {}
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # 1. Í≤ΩÍ≥Ñ Ï†ïÏ†ú
                refined_mask = self._refine_boundaries(mask)
                
                # 2. ÌôÄ Ï±ÑÏö∞Í∏∞
                filled_mask = self._fill_holes_and_remove_noise(refined_mask)
                
                # 3. ÏûëÏùÄ ÏòÅÏó≠ Ï†úÍ±∞
                cleaned_mask = self._remove_small_regions(filled_mask, min_area=200)
                
                # 4. Î™®Ìè¥Î°úÏßÄ Ïó∞ÏÇ∞
                final_mask = self._apply_morphological_operations(cleaned_mask)
                
                processed_masks[mask_type] = final_mask
            
            return processed_masks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïö∏Ìä∏Îùº ÌíàÏßà ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return masks

    def _enhance_sam_results(self, masks: Dict[str, NDArray], image: NDArray,
                           person_parsing: Dict[str, Any]) -> Dict[str, NDArray]:
        """SAM Í≤∞Í≥º Ìñ•ÏÉÅ"""
        try:
            enhanced_masks = {}
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # SAM ÌäπÌôî Ìñ•ÏÉÅ Î°úÏßÅ
                enhanced_mask = mask.copy()
                
                # 1. Í≤ΩÍ≥Ñ Ïä§Î¨¥Îî©
                kernel = np.ones((3, 3), np.uint8)
                enhanced_mask = cv2.morphologyEx(enhanced_mask, cv2.MORPH_CLOSE, kernel)
                
                # 2. ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
                enhanced_mask = cv2.medianBlur(enhanced_mask.astype(np.uint8), 3)
                
                enhanced_masks[mask_type] = enhanced_mask
            
            return enhanced_masks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SAM Í≤∞Í≥º Ìñ•ÏÉÅ Ïã§Ìå®: {e}")
            return masks

    def _enhance_u2net_results(self, masks: Dict[str, NDArray], image: NDArray,
                             person_parsing: Dict[str, Any]) -> Dict[str, NDArray]:
        """U2Net Í≤∞Í≥º Ìñ•ÏÉÅ"""
        try:
            enhanced_masks = {}
            
            for mask_type, mask in masks.items():
                if mask is None or mask.size == 0:
                    continue
                
                # U2Net ÌäπÌôî Ìñ•ÏÉÅ Î°úÏßÅ
                enhanced_mask = mask.copy()
                
                # 1. Ïù¥ÏßÑÌôî
                _, enhanced_mask = cv2.threshold(enhanced_mask, 127, 255, cv2.THRESH_BINARY)
                
                # 2. Í≤ΩÍ≥Ñ Ï†ïÏ†ú
                kernel = np.ones((2, 2), np.uint8)
                enhanced_mask = cv2.morphologyEx(enhanced_mask, cv2.MORPH_OPEN, kernel)
                
                enhanced_masks[mask_type] = enhanced_mask
            
            return enhanced_masks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è U2Net Í≤∞Í≥º Ìñ•ÏÉÅ Ïã§Ìå®: {e}")
            return masks

    def _generate_sam_prompts(self, image: NDArray, person_parsing: Dict[str, Any],
                            pose_info: Dict[str, Any]) -> Dict[str, Any]:
        """SAM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        try:
            prompts = {
                'points': [],
                'boxes': [],
                'masks': []
            }
            
            # 1. Ìè¨Ïù∏Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            if person_parsing:
                # ÏÇ¨Îûå ÌååÏã± Í≤∞Í≥ºÏóêÏÑú ÏùòÎ•ò ÏòÅÏó≠ Ï§ëÏã¨Ï†ê Ï∂îÏ∂ú
                for region_type, region_mask in person_parsing.get('regions', {}).items():
                    if region_mask is not None and np.sum(region_mask) > 100:
                        y_coords, x_coords = np.where(region_mask > 128)
                        if len(x_coords) > 0 and len(y_coords) > 0:
                            center_x = int(np.mean(x_coords))
                            center_y = int(np.mean(y_coords))
                            prompts['points'].append([center_x, center_y])
            
            # 2. Î∞ïÏä§ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            if pose_info:
                # Ìè¨Ï¶à Ï†ïÎ≥¥ÏóêÏÑú ÏùòÎ•ò ÏòÅÏó≠ Î∞îÏö¥Îî© Î∞ïÏä§ Ï∂îÏ∂ú
                keypoints = pose_info.get('keypoints', {})
                if keypoints:
                    # ÏÉÅÏùò ÏòÅÏó≠
                    if 'shoulder_left' in keypoints and 'shoulder_right' in keypoints:
                        left_shoulder = keypoints['shoulder_left']
                        right_shoulder = keypoints['shoulder_right']
                        if left_shoulder and right_shoulder:
                            x1 = min(left_shoulder[0], right_shoulder[0])
                            y1 = min(left_shoulder[1], right_shoulder[1])
                            x2 = max(left_shoulder[0], right_shoulder[0])
                            y2 = max(left_shoulder[1], right_shoulder[1])
                            prompts['boxes'].append([x1, y1, x2, y2])
            
            return prompts
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SAM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {'points': [], 'boxes': [], 'masks': []}

    def _refine_with_person_parsing(self, mask: NDArray, clothing_regions: List[Dict[str, Any]], 
                                  mask_type: str) -> NDArray:
        """ÏÇ¨Îûå ÌååÏã± Í≤∞Í≥ºÎ°ú ÎßàÏä§ÌÅ¨ Ï†ïÏ†ú"""
        try:
            refined_mask = mask.copy()
            
            for region in clothing_regions:
                region_mask = region.get('mask')
                region_type = region.get('type', '')
                
                if region_mask is not None and region_mask.shape == mask.shape:
                    # ÏùòÎ•ò ÌÉÄÏûÖÏóê Îî∞Î•∏ Ï†ïÏ†ú
                    if mask_type == 'upper_body' and region_type in ['shirt', 't_shirt', 'sweater']:
                        refined_mask = np.logical_or(refined_mask, region_mask).astype(np.uint8)
                    elif mask_type == 'lower_body' and region_type in ['pants', 'jeans', 'skirt']:
                        refined_mask = np.logical_or(refined_mask, region_mask).astype(np.uint8)
            
            return refined_mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÇ¨Îûå ÌååÏã± Í∏∞Î∞ò Ï†ïÏ†ú Ïã§Ìå®: {e}")
            return mask

    def _refine_with_pose_info(self, mask: NDArray, keypoints: Dict[str, Any], 
                             mask_type: str) -> NDArray:
        """Ìè¨Ï¶à Ï†ïÎ≥¥Î°ú ÎßàÏä§ÌÅ¨ Ï†ïÏ†ú"""
        try:
            refined_mask = mask.copy()
            
            if not keypoints:
                return refined_mask
            
            # Ìè¨Ï¶à Í∏∞Î∞ò ÏòÅÏó≠ Ï†ïÏùò
            if mask_type == 'upper_body':
                # ÏÉÅÏùò ÏòÅÏó≠: Ïñ¥Íπ®Î∂ÄÌÑ∞ ÌóàÎ¶¨ÍπåÏßÄ
                shoulder_y = min([kp[1] for kp in keypoints.values() if kp and len(kp) >= 2])
                hip_y = max([kp[1] for kp in keypoints.values() if kp and len(kp) >= 2])
                
                # ÏÉÅÏùò ÏòÅÏó≠Îßå Ïú†ÏßÄ
                refined_mask[:shoulder_y, :] = 0
                refined_mask[hip_y:, :] = 0
                
            elif mask_type == 'lower_body':
                # ÌïòÏùò ÏòÅÏó≠: ÌóàÎ¶¨Î∂ÄÌÑ∞ Î∞úÎ™©ÍπåÏßÄ
                hip_y = min([kp[1] for kp in keypoints.values() if kp and len(kp) >= 2])
                
                # ÌïòÏùò ÏòÅÏó≠Îßå Ïú†ÏßÄ
                refined_mask[:hip_y, :] = 0
            
            return refined_mask
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ìè¨Ï¶à Ï†ïÎ≥¥ Í∏∞Î∞ò Ï†ïÏ†ú Ïã§Ìå®: {e}")
            return mask

    def _create_segmentation_visualizations(self, image: NDArray, masks: Dict[str, NDArray]) -> Dict[str, Any]:
        """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        try:
            visualizations = {}
            
            # 1. Ï†ÑÏ≤¥ ÎßàÏä§ÌÅ¨ Ïò§Î≤ÑÎ†àÏù¥
            if 'all_clothes' in masks:
                overlay = image.copy()
                mask = masks['all_clothes']
                
                # ÎßàÏä§ÌÅ¨Î•º Ïª¨Îü¨Î°ú Î≥ÄÌôò
                colored_mask = np.zeros_like(image)
                colored_mask[mask > 128] = [255, 0, 0]  # Îπ®Í∞ÑÏÉâ
                
                # Ïò§Î≤ÑÎ†àÏù¥ ÏÉùÏÑ±
                overlay = cv2.addWeighted(overlay, 0.7, colored_mask, 0.3, 0)
                visualizations['overlay'] = overlay
            
            # 2. Í∞úÎ≥Ñ ÎßàÏä§ÌÅ¨Îì§
            for mask_type, mask in masks.items():
                if mask is not None and mask.size > 0:
                    # ÎßàÏä§ÌÅ¨Î•º Ïª¨Îü¨Î°ú Î≥ÄÌôò
                    colored_mask = np.zeros_like(image)
                    colored_mask[mask > 128] = [0, 255, 0]  # Ï¥àÎ°ùÏÉâ
                    
                    visualizations[f'{mask_type}_mask'] = colored_mask
            
            return visualizations
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {}

    def _run_ai_segmentation_sync_safe(self, image: NDArray, quality_level: QualityLevel,
                                     person_parsing: Dict[str, Any], pose_info: Dict[str, Any]) -> Dict[str, Any]:
        """AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÎèôÍ∏∞ Ïã§Ìñâ (ÏïàÏ†Ñ Î≤ÑÏ†Ñ)"""
        try:
            return self._run_ai_segmentation_sync(image, quality_level, person_parsing, pose_info)
        except Exception as e:
            logger.error(f"‚ùå AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ Ïã§Ìå®: {e}")
            return self._create_fallback_segmentation_result(image.shape)

    def _run_ai_segmentation_sync(self, image: NDArray, quality_level: QualityLevel,
                                person_parsing: Dict[str, Any], pose_info: Dict[str, Any]) -> Dict[str, Any]:
        """AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÎèôÍ∏∞ Ïã§Ìñâ"""
        try:
            # 1. ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÌôïÏù∏
            available_models = self._detect_available_methods()
            
            if not available_models:
                logger.warning("‚ö†Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Ïù¥ ÏóÜÏùå")
                return self._create_fallback_segmentation_result(image.shape)
            
            # 2. ÌíàÏßà Î†àÎ≤®Ïóê Îî∞Î•∏ Î™®Îç∏ ÏÑ†ÌÉù
            if quality_level == QualityLevel.FAST:
                # Îπ†Î•∏ Î™®Îç∏ Ïö∞ÏÑ†
                if 'u2net_cloth' in available_models:
                    return self._run_single_model_segmentation('u2net_cloth', image)
                elif 'deeplabv3_plus' in available_models:
                    return self._run_single_model_segmentation('deeplabv3_plus', image)
            
            elif quality_level == QualityLevel.ULTRA:
                # Î™®Îì† Î™®Îç∏ ÏïôÏÉÅÎ∏î
                return self._run_hybrid_ensemble_sync(image, person_parsing, pose_info)
            
            else:
                # Í∏∞Î≥∏: ÌïòÏù¥Î∏åÎ¶¨Îìú ÏïôÏÉÅÎ∏î
                return self._run_hybrid_ensemble_sync(image, person_parsing, pose_info)
                
        except Exception as e:
            logger.error(f"‚ùå AI ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïã§Ìñâ Ïã§Ìå®: {e}")
            return self._create_fallback_segmentation_result(image.shape)

    def _safe_model_predict(self, model_key: str, image: NDArray) -> Dict[str, Any]:
        """ÏïàÏ†ÑÌïú Î™®Îç∏ ÏòàÏ∏° - Ïã§Ï†ú Ï∂îÎ°† ÏàòÌñâ"""
        try:
            logger.info(f"üéØ _safe_model_predict ÏãúÏûë: {model_key}")
            
            if model_key not in self.segmentation_models:
                logger.warning(f"‚ö†Ô∏è Î™®Îç∏ {model_key}Í∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏùå")
                return {'masks': {}, 'confidence': 0.0, 'error': f'Î™®Îç∏ {model_key}Í∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏùå'}
            
            model = self.segmentation_models[model_key]
            logger.info(f"üéØ {model_key} Î™®Îç∏Î°ú Ï∂îÎ°† ÏãúÏûë")
            
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï
            target_size = (512, 512)
            if image.shape[:2] != target_size:
                image = cv2.resize(image, target_size)
            
            # Ï†ïÍ∑úÌôî
            image = image.astype(np.float32) / 255.0
            
            # Î™®Îç∏Î≥Ñ Ï†ÑÏ≤òÎ¶¨
            if model_key == 'u2net_cloth':
                # U2Net Ï†ÑÏ≤òÎ¶¨
                image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
            elif model_key == 'sam_huge':
                # SAM Ï†ÑÏ≤òÎ¶¨
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
            elif model_key == 'deeplabv3_plus':
                # DeepLabV3+ Ï†ÑÏ≤òÎ¶¨
                image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
            else:
                # Í∏∞Î≥∏ Ï†ÑÏ≤òÎ¶¨
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
            
            image_tensor = image_tensor.to(self.device)
            logger.info(f"üîç {model_key} Î™®Îç∏ ÏûÖÎ†• ÌÖêÏÑú shape: {image_tensor.shape}")
            
            # Ï∂îÎ°†
            with torch.no_grad():
                try:
                    # Î™®Îç∏ÏùÑ eval Î™®ÎìúÎ°ú ÏÑ§Ï†ï
                    model.eval()
                    
                    # Ïã§Ï†ú Ï∂îÎ°† ÏàòÌñâ
                    outputs = model(image_tensor)
                    logger.info(f"‚úÖ {model_key} Î™®Îç∏ Ï∂îÎ°† ÏôÑÎ£å, Ï∂úÎ†• shape: {outputs.shape if hasattr(outputs, 'shape') else 'unknown'}")
                    
                    # Î™®Îç∏Î≥Ñ Í≤∞Í≥º Ï≤òÎ¶¨
                    if model_key == 'u2net_cloth':
                        if isinstance(outputs, tuple):
                            main_output = outputs[0]
                        else:
                            main_output = outputs
                        
                        logger.info(f"üîç U2Net Ï∂úÎ†• shape: {main_output.shape}")
                        
                        # Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨
                        mask = torch.sigmoid(main_output).cpu().numpy()[0, 0]
                        mask = (mask > 0.5).astype(np.uint8)
                        
                        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Í∞úÏÑ†
                        confidence = float(np.mean(mask)) if np.sum(mask) > 0 else 0.1
                        confidence = max(confidence, 0.1)  # ÏµúÏÜå Ïã†Î¢∞ÎèÑ Î≥¥Ïû•
                        
                        logger.info(f"üéØ U2Net Ïã†Î¢∞ÎèÑ: {confidence:.3f}, ÎßàÏä§ÌÅ¨ ÌÅ¨Í∏∞: {mask.shape}, ÎßàÏä§ÌÅ¨ Í∞í Î≤îÏúÑ: {mask.min()}-{mask.max()}")
                        
                        result = {
                            'success': True,
                            'masks': {'upper_body': mask},
                            'confidence': confidence,
                            'method': 'u2net_cloth'
                        }
                        logger.info(f"‚úÖ U2Net Í≤∞Í≥º Î∞òÌôò: {result}")
                        return result
                    
                    elif model_key == 'sam_huge':
                        if isinstance(outputs, tuple):
                            mask = outputs[0]
                        else:
                            mask = outputs
                        
                        logger.info(f"üîç SAM Ï∂úÎ†• shape: {mask.shape}")
                        
                        mask = torch.sigmoid(mask).cpu().numpy()[0, 0]
                        mask = (mask > 0.5).astype(np.uint8)
                        
                        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Í∞úÏÑ†
                        confidence = float(np.mean(mask)) if np.sum(mask) > 0 else 0.1
                        confidence = max(confidence, 0.1)  # ÏµúÏÜå Ïã†Î¢∞ÎèÑ Î≥¥Ïû•
                        
                        logger.info(f"üéØ SAM Ïã†Î¢∞ÎèÑ: {confidence:.3f}, ÎßàÏä§ÌÅ¨ ÌÅ¨Í∏∞: {mask.shape}, ÎßàÏä§ÌÅ¨ Í∞í Î≤îÏúÑ: {mask.min()}-{mask.max()}")
                        
                        result = {
                            'success': True,
                            'masks': {'upper_body': mask},
                            'confidence': confidence,
                            'method': 'sam_huge'
                        }
                        logger.info(f"‚úÖ SAM Í≤∞Í≥º Î∞òÌôò: {result}")
                        return result
                    
                    elif model_key == 'deeplabv3_plus':
                        if isinstance(outputs, tuple):
                            mask = outputs[0]
                        else:
                            mask = outputs
                        
                        logger.info(f"üîç DeepLabV3+ Ï∂úÎ†• shape: {mask.shape}")
                        
                        mask = torch.softmax(mask, dim=1).cpu().numpy()[0, 1]  # ÌÅ¥ÎûòÏä§ 1 (ÏùòÎ•ò)
                        mask = (mask > 0.5).astype(np.uint8)
                        
                        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Í∞úÏÑ†
                        confidence = float(np.mean(mask)) if np.sum(mask) > 0 else 0.1
                        confidence = max(confidence, 0.1)  # ÏµúÏÜå Ïã†Î¢∞ÎèÑ Î≥¥Ïû•
                        
                        logger.info(f"üéØ DeepLabV3+ Ïã†Î¢∞ÎèÑ: {confidence:.3f}, ÎßàÏä§ÌÅ¨ ÌÅ¨Í∏∞: {mask.shape}, ÎßàÏä§ÌÅ¨ Í∞í Î≤îÏúÑ: {mask.min()}-{mask.max()}")
                        
                        result = {
                            'success': True,
                            'masks': {'upper_body': mask},
                            'confidence': confidence,
                            'method': 'deeplabv3_plus'
                        }
                        logger.info(f"‚úÖ DeepLabV3+ Í≤∞Í≥º Î∞òÌôò: {result}")
                        return result
                    
                    else:
                        # Í∏∞Î≥∏ Ï∂îÎ°†
                        if isinstance(outputs, tuple):
                            mask = outputs[0]
                        else:
                            mask = outputs
                        
                        logger.info(f"üîç {model_key} Ï∂úÎ†• shape: {mask.shape}")
                        
                        mask = torch.sigmoid(mask).cpu().numpy()[0, 0]
                        mask = (mask > 0.5).astype(np.uint8)
                        
                        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ Í∞úÏÑ†
                        confidence = float(np.mean(mask)) if np.sum(mask) > 0 else 0.1
                        confidence = max(confidence, 0.1)  # ÏµúÏÜå Ïã†Î¢∞ÎèÑ Î≥¥Ïû•
                        
                        logger.info(f"üéØ {model_key} Ïã†Î¢∞ÎèÑ: {confidence:.3f}, ÎßàÏä§ÌÅ¨ ÌÅ¨Í∏∞: {mask.shape}, ÎßàÏä§ÌÅ¨ Í∞í Î≤îÏúÑ: {mask.min()}-{mask.max()}")
                        
                        result = {
                            'success': True,
                            'masks': {'upper_body': mask},
                            'confidence': confidence,
                            'method': model_key
                        }
                        logger.info(f"‚úÖ {model_key} Í≤∞Í≥º Î∞òÌôò: {result}")
                        return result
                        
                except Exception as e:
                    logger.error(f"‚ùå {model_key} Î™®Îç∏ Ï∂îÎ°† Ïã§Ìå®: {e}")
                    return {'masks': {}, 'confidence': 0.0, 'error': str(e)}
                    
        except Exception as e:
            logger.error(f"‚ùå {model_key} ÏòàÏ∏° Ïã§Ìå®: {e}")
            return {'masks': {}, 'confidence': 0.0, 'error': str(e)}

    def _safe_model_predict_with_prompts(self, model_key: str, image: NDArray, prompts: Dict[str, Any]) -> Dict[str, Any]:
        """ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÏûàÎäî ÏïàÏ†ÑÌïú Î™®Îç∏ ÏòàÏ∏°"""
        try:
            if model_key not in self.segmentation_models:
                logger.warning(f"‚ö†Ô∏è Î™®Îç∏ {model_key}Í∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏùå")
                return {'masks': {}, 'confidence': 0.0}
            
            model = self.segmentation_models[model_key]
            if not model:
                return {'masks': {}, 'confidence': 0.0}
            
            # ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÏûàÎäî Î™®Îç∏ ÏòàÏ∏° Ïã§Ìñâ
            if hasattr(model, 'predict_with_prompts'):
                result = model.predict_with_prompts(image, prompts)
            else:
                result = model.predict(image)
            
            if not result:
                return {'masks': {}, 'confidence': 0.0}
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Î™®Îç∏ {model_key} ÌîÑÎ°¨ÌîÑÌä∏ ÏòàÏ∏° Ïã§Ìå®: {e}")
            return {'masks': {}, 'confidence': 0.0}

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÏÉùÏÑ±"""
    try:
        step = ClothSegmentationStep(**kwargs)
        return step
    except Exception as e:
        logger.error(f"‚ùå ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return None

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ÏµúÏ†ÅÌôî ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÏÉùÏÑ±"""
    try:
        # M3 Max ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
        m3_max_config = {
            'device': 'mps' if torch.backends.mps.is_available() else 'cpu',
            'optimization_level': 'high',
            'memory_efficient': True,
            **kwargs
        }
        
        step = ClothSegmentationStep(**m3_max_config)
        return step
    except Exception as e:
        logger.error(f"‚ùå M3 Max ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return None

def test_cloth_segmentation_step():
    """ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÖåÏä§Ìä∏"""
    try:
        logger.info("üß™ ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÖåÏä§Ìä∏ ÏãúÏûë")
        
        # Ïä§ÌÖù ÏÉùÏÑ±
        step = create_cloth_segmentation_step()
        if step is None:
            logger.error("‚ùå Ïä§ÌÖù ÏÉùÏÑ± Ïã§Ìå®")
            return False
        
        # Ï¥àÍ∏∞Ìôî
        if not step.initialize():
            logger.error("‚ùå Ïä§ÌÖù Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            return False
        
        # ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏
        result = step.process(image=test_image)
        
        if result.get('success', False):
            logger.info("‚úÖ ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ")
            return True
        else:
            logger.error(f"‚ùå ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÖåÏä§Ìä∏ Ïã§Ìå®: {result.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Ïä§ÌÖù ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False

# Î™®Îìà Î†àÎ≤® ÌÖåÏä§Ìä∏
if __name__ == "__main__":
    test_cloth_segmentation_step()
