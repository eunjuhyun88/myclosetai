#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Hybrid Ensemble
=====================================================================

ì•™ìƒë¸” ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ë¶„ë¦¬í•œ ëª¨ë“ˆ

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 1.0
"""

import logging
import time
import numpy as np
from typing import Dict, Any, List, Tuple, Optional

# ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš©
try:
    from app.ai_pipeline.utils.common_imports import (
        np, cv2, PIL_AVAILABLE, CV2_AVAILABLE, NUMPY_AVAILABLE
    )
except ImportError:
    import numpy as np
    import cv2

logger = logging.getLogger(__name__)

def _run_hybrid_ensemble_sync(
    self, 
    image: np.ndarray, 
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë™ê¸°)
    
    Args:
        image: ì…ë ¥ ì´ë¯¸ì§€
        person_parsing: ì¸ì²´ íŒŒì‹± ê²°ê³¼
        pose_info: í¬ì¦ˆ ì •ë³´
        
    Returns:
        ì•™ìƒë¸” ê²°ê³¼
    """
    try:
        logger.info("ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ í™•ì¸
        available_models = self._detect_available_methods()
        if not available_models:
            logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŒ")
            return self._create_fallback_segmentation_result(image.shape)
        
        # ì•™ìƒë¸” ì‹¤í–‰
        ensemble_results = []
        methods_used = []
        execution_times = []
        
        for method in available_models[:3]:  # ìµœëŒ€ 3ê°œ ëª¨ë¸ë§Œ ì‚¬ìš©
            try:
                start_time = time.time()
                
                if method == SegmentationMethod.U2NET_CLOTH:
                    result = self._run_u2net_segmentation(image, person_parsing, pose_info)
                elif method == SegmentationMethod.SAM_HUGE:
                    result = self._run_sam_segmentation(image, person_parsing, pose_info)
                elif method == SegmentationMethod.DEEPLABV3_PLUS:
                    result = self._run_deeplabv3plus_segmentation(image, person_parsing, pose_info)
                else:
                    continue
                
                execution_time = time.time() - start_time
                
                if result and result.get('success', False):
                    ensemble_results.append(result)
                    methods_used.append(method.value)
                    execution_times.append(execution_time)
                    logger.info(f"âœ… {method.value} ì™„ë£Œ ({execution_time:.2f}s)")
                
            except Exception as e:
                logger.error(f"âŒ {method.value} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                continue
        
        if not ensemble_results:
            logger.warning("âš ï¸ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return self._create_fallback_segmentation_result(image.shape)
        
        # ì•™ìƒë¸” ê²°ê³¼ ê²°í•©
        final_result = self._combine_ensemble_results(
            ensemble_results, methods_used, execution_times, image, person_parsing
        )
        
        logger.info(f"ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì™„ë£Œ (ì‚¬ìš©ëœ ëª¨ë¸: {methods_used})")
        return final_result
        
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
        return self._create_fallback_segmentation_result(image.shape)

def _combine_ensemble_results(
    self,
    results: List[Dict[str, Any]],
    methods_used: List[str],
    execution_times: List[float],
    image: np.ndarray,
    person_parsing: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ì•™ìƒë¸” ê²°ê³¼ ê²°í•© (ê°œì„ ëœ ë²„ì „)
    
    Args:
        results: ê°œë³„ ëª¨ë¸ ê²°ê³¼ë“¤
        methods_used: ì‚¬ìš©ëœ ëª¨ë¸ë“¤
        execution_times: ì‹¤í–‰ ì‹œê°„ë“¤
        image: ì›ë³¸ ì´ë¯¸ì§€
        person_parsing: ì¸ì²´ íŒŒì‹± ê²°ê³¼
        
    Returns:
        ê²°í•©ëœ ê²°ê³¼
    """
    try:
        logger.info("ğŸ”¥ ì•™ìƒë¸” ê²°ê³¼ ê²°í•© ì‹œì‘")
        
        if len(results) == 1:
            # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ ë°˜í™˜
            result = results[0]
            result['ensemble_methods'] = methods_used
            result['ensemble_times'] = execution_times
            return result
        
        # ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ ê²°í•© (ê°œì„ ëœ ê°€ì¤‘ì¹˜ ê³„ì‚°)
        combined_masks = {}
        combined_confidence = 0.0
        total_weight = 0.0
        
        # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)
        model_weights = []
        for i, (result, method, exec_time) in enumerate(zip(results, methods_used, execution_times)):
            # 1. ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.3)
            confidence = result.get('confidence', 0.5)
            confidence_weight = confidence * 0.3
            
            # 2. ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.2) - ë¹ ë¥¸ ëª¨ë¸ì— ë³´ë„ˆìŠ¤
            time_weight = 1.0 / (exec_time + 1e-6) * 0.2
            
            # 3. ëª¨ë¸ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ (0.3)
            type_weight = 0.0
            if 'deeplabv3' in method.lower():
                type_weight = 1.0 * 0.3  # ìµœê³  ê°€ì¤‘ì¹˜
            elif 'sam' in method.lower():
                type_weight = 0.9 * 0.3  # ë†’ì€ ê°€ì¤‘ì¹˜
            elif 'u2net' in method.lower():
                type_weight = 0.8 * 0.3  # ì¤‘ê°„ ê°€ì¤‘ì¹˜
            else:
                type_weight = 0.5 * 0.3  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            
            # 4. ê²°ê³¼ í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.2)
            quality_weight = 0.0
            if 'masks' in result and result['masks']:
                # ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€
                mask_quality = self._evaluate_mask_quality(result['masks'], image)
                quality_weight = mask_quality * 0.2
            
            # ì´ ê°€ì¤‘ì¹˜ ê³„ì‚°
            total_model_weight = confidence_weight + time_weight + type_weight + quality_weight
            model_weights.append(total_model_weight)
            total_weight += total_model_weight
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if total_weight > 0:
            normalized_weights = [w / total_weight for w in model_weights]
        else:
            normalized_weights = [1.0 / len(results)] * len(results)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°ê³¼ ê²°í•©
        for i, result in enumerate(results):
            weight = normalized_weights[i]
            
            if 'masks' in result:
                for mask_type, mask in result['masks'].items():
                    if mask_type not in combined_masks:
                        combined_masks[mask_type] = np.zeros_like(mask, dtype=np.float32)
                    combined_masks[mask_type] += mask.astype(np.float32) * weight
            
            if 'confidence' in result:
                combined_confidence += result['confidence'] * weight
        
        # ë§ˆìŠ¤í¬ ì •ê·œí™” ë° ì„ê³„ê°’ ì ìš©
        for mask_type in combined_masks:
            combined_masks[mask_type] = (combined_masks[mask_type] > 0.5).astype(np.uint8)
        
        # í›„ì²˜ë¦¬ ì ìš©
        refined_masks = self._apply_ensemble_postprocessing(combined_masks, image)
        
        # ìµœì¢… ê²°ê³¼ ìƒì„±
        final_result = {
            'success': True,
            'masks': refined_masks,
            'confidence': combined_confidence,
            'ensemble_methods': methods_used,
            'ensemble_times': execution_times,
            'ensemble_weights': normalized_weights,
            'ensemble_count': len(results),
            'method': 'hybrid_ensemble'
        }
        
        logger.info(f"ğŸ”¥ ì•™ìƒë¸” ê²°ê³¼ ê²°í•© ì™„ë£Œ (ê°€ì¤‘ì¹˜: {normalized_weights})")
        return final_result
        
    except Exception as e:
        logger.error(f"âŒ ì•™ìƒë¸” ê²°ê³¼ ê²°í•© ì‹¤íŒ¨: {e}")
        return self._create_fallback_segmentation_result(image.shape)

def _calculate_adaptive_threshold(
    self, 
    ensemble_mask: np.ndarray, 
    image: np.ndarray
) -> float:
    """
    ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚°
    
    Args:
        ensemble_mask: ì•™ìƒë¸” ë§ˆìŠ¤í¬
        image: ì›ë³¸ ì´ë¯¸ì§€
        
    Returns:
        ì ì‘í˜• ì„ê³„ê°’
    """
    try:
        # ì´ë¯¸ì§€ í†µê³„ ê³„ì‚°
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
        mean_intensity = np.mean(gray)
        std_intensity = np.std(gray)
        
        # ë§ˆìŠ¤í¬ í†µê³„ ê³„ì‚°
        mask_mean = np.mean(ensemble_mask) if ensemble_mask.size > 0 else 0.5
        mask_std = np.std(ensemble_mask) if ensemble_mask.size > 0 else 0.1
        
        # ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚°
        base_threshold = 0.5
        intensity_factor = (mean_intensity - 128) / 128  # -1 to 1
        contrast_factor = (std_intensity - 50) / 50      # -1 to 1
        
        adaptive_threshold = base_threshold + intensity_factor * 0.1 + contrast_factor * 0.05
        
        # ë²”ìœ„ ì œí•œ
        adaptive_threshold = np.clip(adaptive_threshold, 0.3, 0.7)
        
        logger.debug(f"ì ì‘í˜• ì„ê³„ê°’: {adaptive_threshold:.3f}")
        return adaptive_threshold
        
    except Exception as e:
        logger.warning(f"ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def _apply_ensemble_postprocessing(
    self, 
    mask: np.ndarray, 
    image: np.ndarray
) -> np.ndarray:
    """
    ì•™ìƒë¸” í›„ì²˜ë¦¬ ì ìš©
    
    Args:
        mask: ì›ë³¸ ë§ˆìŠ¤í¬
        image: ì›ë³¸ ì´ë¯¸ì§€
        
    Returns:
        í›„ì²˜ë¦¬ëœ ë§ˆìŠ¤í¬
    """
    try:
        logger.debug("ì•™ìƒë¸” í›„ì²˜ë¦¬ ì‹œì‘")
        
        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = np.ones((3, 3), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # ê²½ê³„ ì •ì œ
        mask = cv2.GaussianBlur(mask.astype(np.float32), (3, 3), 0)
        mask = (mask > 0.5).astype(np.uint8)
        
        # í™€ ì±„ìš°ê¸°
        mask = cv2.fillPoly(mask, [cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0][0]], 1)
        
        logger.debug("ì•™ìƒë¸” í›„ì²˜ë¦¬ ì™„ë£Œ")
        return mask
        
    except Exception as e:
        logger.warning(f"ì•™ìƒë¸” í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return mask

def _run_u2net_segmentation(
    self,
    image: np.ndarray,
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """U2Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
    try:
        if 'u2net_cloth' not in self.segmentation_models:
            return None
        
        model = self.segmentation_models['u2net_cloth']
        result = model.predict(image)
        
        if result and result.get('success', False):
            result['method'] = 'u2net_cloth'
            return result
        
        return None
        
    except Exception as e:
        logger.error(f"U2Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
        return None

def _run_sam_segmentation(
    self,
    image: np.ndarray,
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
    try:
        if 'sam_huge' not in self.segmentation_models:
            return None
        
        model = self.segmentation_models['sam_huge']
        
        # SAM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts = self._generate_sam_prompts(image, person_parsing, pose_info)
        
        result = model.predict(image, prompts)
        
        if result and result.get('success', False):
            result['method'] = 'sam_huge'
            return result
        
        return None
        
    except Exception as e:
        logger.error(f"SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
        return None

def _run_deeplabv3plus_segmentation(
    self,
    image: np.ndarray,
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """DeepLabV3+ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
    try:
        if 'deeplabv3plus' not in self.segmentation_models:
            return None
        
        model = self.segmentation_models['deeplabv3plus']
        result = model.predict(image)
        
        if result and result.get('success', False):
            result['method'] = 'deeplabv3plus'
            return result
        
        return None
        
    except Exception as e:
        logger.error(f"DeepLabV3+ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
        return None

def _generate_sam_prompts(
    self,
    image: np.ndarray,
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """SAM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    try:
        prompts = {
            'points': [],
            'boxes': [],
            'masks': []
        }
        
        # ì¸ì²´ íŒŒì‹±ì—ì„œ ì˜ë¥˜ ì˜ì—­ ì¶”ì¶œ
        if 'parsing_map' in person_parsing:
            parsing_map = person_parsing['parsing_map']
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë“¤ (ìƒì˜, í•˜ì˜, ì „ì‹ )
            clothing_categories = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]
            
            for category in clothing_categories:
                if category in parsing_map:
                    mask = (parsing_map == category).astype(np.uint8)
                    
                    # ì»¨íˆ¬ì–´ ì°¾ê¸°
                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    
                    for contour in contours:
                        if cv2.contourArea(contour) > 100:  # ìµœì†Œ ë©´ì 
                            # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ê°€
                            x, y, w, h = cv2.boundingRect(contour)
                            prompts['boxes'].append([x, y, x + w, y + h])
                            
                            # ì¤‘ì‹¬ì  ì¶”ê°€
                            M = cv2.moments(contour)
                            if M["m00"] != 0:
                                cx = int(M["m10"] / M["m00"])
                                cy = int(M["m01"] / M["m00"])
                                prompts['points'].append([cx, cy])
        
        return prompts
        
    except Exception as e:
        logger.warning(f"SAM í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return {'points': [], 'boxes': [], 'masks': []}

def _evaluate_mask_quality(self, masks: Dict[str, np.ndarray], image: np.ndarray) -> float:
    """ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€"""
    try:
        if not masks:
            return 0.5
        
        total_quality = 0.0
        mask_count = 0
        
        for mask_type, mask in masks.items():
            if mask is None or mask.size == 0:
                continue
            
            # 1. ë©´ì  ë¹„ìœ¨ í‰ê°€
            area_ratio = np.sum(mask) / mask.size
            area_score = min(area_ratio * 10, 1.0)  # ì ì ˆí•œ ë©´ì  ë¹„ìœ¨ì— ë†’ì€ ì ìˆ˜
            
            # 2. ê²½ê³„ í’ˆì§ˆ í‰ê°€
            edges = cv2.Canny(mask.astype(np.uint8) * 255, 50, 150)
            edge_density = np.sum(edges) / (edges.size * 255)
            edge_score = 1.0 - min(edge_density * 5, 1.0)  # ë‚®ì€ edge densityì— ë†’ì€ ì ìˆ˜
            
            # 3. ì—°ê²°ì„± í‰ê°€
            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            connectivity_score = 1.0 / (len(contours) + 1)  # ì»¨íˆ¬ì–´ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            
            # 4. ì›í˜•ë„ í‰ê°€
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                contour_area = cv2.contourArea(largest_contour)
                contour_perimeter = cv2.arcLength(largest_contour, True)
                
                if contour_perimeter > 0:
                    circularity = 4 * np.pi * contour_area / (contour_perimeter ** 2)
                else:
                    circularity = 0.0
            else:
                circularity = 0.0
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            quality_score = (area_score * 0.3 + edge_score * 0.3 + 
                           connectivity_score * 0.2 + circularity * 0.2)
            
            total_quality += quality_score
            mask_count += 1
        
        return total_quality / mask_count if mask_count > 0 else 0.5
        
    except Exception as e:
        logger.warning(f"ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5
