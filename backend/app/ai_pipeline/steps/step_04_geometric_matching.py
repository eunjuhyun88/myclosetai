# app/ai_pipeline/steps/step_04_geometric_matching.py
"""
4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching) - AI ëª¨ë¸ ì™„ì „ ì—°ë™ ì‹¤ì œ êµ¬í˜„ + ì‹œê°í™” ê¸°ëŠ¥
âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” TPS ë³€í˜•
âœ… AI ëª¨ë¸ê³¼ ì™„ì „ ì—°ë™
âœ… í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ë° ë³€í˜•
âœ… M3 Max ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ğŸ†• ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import logging
import time
import asyncio
import gc
import base64
from typing import Dict, Any, Optional, Tuple, List, Union
import numpy as np
import json
import math
from pathlib import Path
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    raise ImportError("PyTorch is required for GeometricMatchingStep")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    raise ImportError("OpenCV is required for GeometricMatchingStep")

try:
    from PIL import Image, ImageFilter, ImageEnhance, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    raise ImportError("PIL is required for GeometricMatchingStep")

try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ§  AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__(self, feature_dim=256, num_keypoints=25):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_keypoints = num_keypoints
        
        # íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸
        self.backbone = self._build_backbone()
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ
        self.keypoint_head = self._build_keypoint_head()
        
        # íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ
        self.matching_head = self._build_matching_head()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ
        self.tps_head = self._build_tps_head()
        
    def _build_backbone(self):
        """íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # Stage 2
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            
            # Stage 3
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            # Feature refinement
            nn.Conv2d(512, self.feature_dim, 3, 1, 1),
            nn.BatchNorm2d(self.feature_dim),
            nn.ReLU(inplace=True)
        )
    
    def _make_layer(self, in_planes, planes, blocks, stride=1):
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
        layers = []
        layers.append(nn.Conv2d(in_planes, planes, 3, stride, 1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, 3, 1, 1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_keypoint_head(self):
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, self.num_keypoints, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_matching_head(self):
        """íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim * 2, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 1, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_tps_head(self):
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, self.num_keypoints * 2)  # (x, y) coordinates
        )
    
    def forward(self, source_img, target_img):
        """ìˆœì „íŒŒ"""
        # íŠ¹ì§• ì¶”ì¶œ
        source_features = self.backbone(source_img)
        target_features = self.backbone(target_img)
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        source_keypoints = self.keypoint_head(source_features)
        target_keypoints = self.keypoint_head(target_features)
        
        # íŠ¹ì§• ë§¤ì¹­
        concat_features = torch.cat([source_features, target_features], dim=1)
        matching_confidence = self.matching_head(concat_features)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.tps_head(source_features)
        tps_params = tps_params.view(-1, self.num_keypoints, 2)
        
        return {
            'source_keypoints': source_keypoints,
            'target_keypoints': target_keypoints,
            'matching_confidence': matching_confidence,
            'tps_params': tps_params,
            'source_features': source_features,
            'target_features': target_features
        }

class TPSTransformNetwork(nn.Module):
    """Thin Plate Spline ë³€í˜• ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, grid_size=20):
        super().__init__()
        self.grid_size = grid_size
        
    def create_grid(self, height, width, device):
        """ì •ê·œí™”ëœ ê·¸ë¦¬ë“œ ìƒì„±"""
        x = torch.linspace(-1, 1, width, device=device)
        y = torch.linspace(-1, 1, height, device=device)
        grid_x, grid_y = torch.meshgrid(x, y, indexing='xy')
        grid = torch.stack([grid_x, grid_y], dim=-1)
        return grid.unsqueeze(0)  # [1, H, W, 2]
    
    def compute_tps_weights(self, source_points, target_points):
        """TPS ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        device = source_points.device
        
        # ì œì–´ì  ê°„ ê±°ë¦¬ ê³„ì‚°
        source_points_expanded = source_points.unsqueeze(2)  # [B, N, 1, 2]
        target_points_expanded = target_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(source_points_expanded - target_points_expanded, dim=-1)  # [B, N, N]
        
        # RBF ì»¤ë„ ê³„ì‚° (r^2 * log(r))
        distances = distances + 1e-8  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±
        rbf_weights = distances ** 2 * torch.log(distances)
        
        # íŠ¹ì´ì  ì²˜ë¦¬
        rbf_weights = torch.where(distances < 1e-6, torch.zeros_like(rbf_weights), rbf_weights)
        
        return rbf_weights
    
    def apply_tps_transform(self, image, source_points, target_points):
        """TPS ë³€í˜• ì ìš©"""
        batch_size, channels, height, width = image.shape
        device = image.device
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid = self.create_grid(height, width, device)
        grid = grid.repeat(batch_size, 1, 1, 1)  # [B, H, W, 2]
        
        # TPS ê°€ì¤‘ì¹˜ ê³„ì‚°
        tps_weights = self.compute_tps_weights(source_points, target_points)
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°
        transformed_grid = self.compute_transformed_grid(
            grid, source_points, target_points, tps_weights
        )
        
        # ì´ë¯¸ì§€ ë¦¬ìƒ˜í”Œë§
        transformed_image = F.grid_sample(
            image, transformed_grid, 
            mode='bilinear', 
            padding_mode='border', 
            align_corners=True
        )
        
        return transformed_image, transformed_grid
    
    def compute_transformed_grid(self, grid, source_points, target_points, tps_weights):
        """ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°"""
        batch_size, height, width, _ = grid.shape
        num_points = source_points.shape[1]
        device = grid.device
        
        # ê·¸ë¦¬ë“œë¥¼ í‰ë©´ìœ¼ë¡œ ë³€í™˜
        grid_flat = grid.view(batch_size, -1, 2)  # [B, H*W, 2]
        
        # ê° ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ì™€ ì œì–´ì  ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        grid_expanded = grid_flat.unsqueeze(2)  # [B, H*W, 1, 2]
        source_expanded = source_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(grid_expanded - source_expanded, dim=-1)  # [B, H*W, N]
        distances = distances + 1e-8
        
        # RBF ê°’ ê³„ì‚°
        rbf_values = distances ** 2 * torch.log(distances)
        rbf_values = torch.where(distances < 1e-6, torch.zeros_like(rbf_values), rbf_values)
        
        # ë³€ìœ„ ê³„ì‚°
        displacement = target_points - source_points  # [B, N, 2]
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³€í˜• ê³„ì‚°
        weights = torch.softmax(-distances / 0.1, dim=-1)  # [B, H*W, N]
        interpolated_displacement = torch.sum(
            weights.unsqueeze(-1) * displacement.unsqueeze(1), dim=2
        )  # [B, H*W, 2]
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ
        transformed_grid_flat = grid_flat + interpolated_displacement
        transformed_grid = transformed_grid_flat.view(batch_size, height, width, 2)
        
        return transformed_grid

# ==============================================
# ğŸ¯ ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤
# ==============================================

class GeometricMatchingStep:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ë‹¨ê³„ - AI ëª¨ë¸ê³¼ ì™„ì „ ì—°ë™ + ì‹œê°í™”"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        device_type: Optional[str] = None,
        memory_gb: Optional[float] = None,
        is_m3_max: Optional[bool] = None,
        optimization_enabled: Optional[bool] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ):
        """ì™„ì „ í˜¸í™˜ ìƒì„±ì - ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì›"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        self.device_type = device_type or kwargs.get('device_type', 'auto')
        self.memory_gb = memory_gb or kwargs.get('memory_gb', 16.0)
        self.is_m3_max = is_m3_max if is_m3_max is not None else kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = optimization_enabled if optimization_enabled is not None else kwargs.get('optimization_enabled', True)
        self.quality_level = quality_level or kwargs.get('quality_level', 'balanced')
        
        # ê¸°ë³¸ ì„¤ì •
        self._merge_step_specific_config(kwargs)
        self.is_initialized = False
        self.initialization_error = None
        
        # AI ëª¨ë¸ë“¤
        self.geometric_model = None
        self.tps_network = None
        self.feature_extractor = None
        
        # ğŸ†• ì‹œê°í™” ì„¤ì •
        self.visualization_config = self.config.get('visualization', {
            'enable_visualization': True,
            'show_keypoints': True,
            'show_matching_lines': True,
            'show_transformation_grid': True,
            'keypoint_size': 3,
            'line_thickness': 2,
            'grid_density': 20,
            'quality': 'high'  # low, medium, high
        })
        
        # ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="geometric_matching")
        
        # ëª¨ë¸ ë¡œë” ì„¤ì •
        self._setup_model_loader()
        
        # ìŠ¤í… íŠ¹í™” ì´ˆê¸°í™”
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if device:
            return device
        
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if torch.backends.mps.is_available():
                # macOSì—ì„œ MPS ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ M3 Maxì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                return True
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs):
        """ìŠ¤í…ë³„ ì„¤ì • ë³‘í•©"""
        step_config = kwargs.get('step_config', {})
        self.config.update(step_config)
    
    def _setup_model_loader(self):
        """ëª¨ë¸ ë¡œë” ì„¤ì •"""
        try:
            from app.ai_pipeline.utils.model_loader import ModelLoader
            self.model_loader = ModelLoader(device=self.device)
            self.logger.info("âœ… ModelLoader ì—°ë™ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    def _initialize_step_specific(self):
        """ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ë§¤ì¹­ ì„¤ì •
            base_config = {
                'method': 'neural_tps',
                'num_keypoints': 25,
                'feature_dim': 256,
                'grid_size': 30 if self.is_m3_max else 20,
                'max_iterations': 1000,
                'convergence_threshold': 1e-6,
                'outlier_threshold': 0.15,
                'use_pose_guidance': True,
                'adaptive_weights': True,
                'quality_threshold': 0.7
            }
            
            # quality_levelì— ë”°ë¥¸ ì¡°ì •
            if self.quality_level == 'high':
                base_config.update({
                    'num_keypoints': 30,
                    'max_iterations': 1500,
                    'quality_threshold': 0.8,
                    'convergence_threshold': 1e-7
                })
            elif self.quality_level == 'ultra':
                base_config.update({
                    'num_keypoints': 35,
                    'max_iterations': 2000,
                    'quality_threshold': 0.9,
                    'convergence_threshold': 1e-8
                })
            elif self.quality_level == 'fast':
                base_config.update({
                    'num_keypoints': 20,
                    'max_iterations': 500,
                    'quality_threshold': 0.6,
                    'convergence_threshold': 1e-5
                })
            
            self.matching_config = self.config.get('matching', base_config)
            
            # TPS ì„¤ì •
            self.tps_config = self.config.get('tps', {
                'regularization': 0.1,
                'grid_size': self.matching_config['grid_size'],
                'boundary_padding': 0.1,
                'smoothing_factor': 0.8
            })
            
            # ìµœì í™” ì„¤ì •
            learning_rate_base = 0.01
            if self.is_m3_max and self.optimization_enabled:
                learning_rate_base *= 1.2
            
            self.optimization_config = self.config.get('optimization', {
                'learning_rate': learning_rate_base,
                'momentum': 0.9,
                'weight_decay': 1e-4,
                'scheduler_step': 100,
                'batch_size': 8 if self.is_m3_max else 4
            })
            
            # í†µê³„ ì´ˆê¸°í™”
            self.matching_stats = {
                'total_matches': 0,
                'successful_matches': 0,
                'average_accuracy': 0.0,
                'method_performance': {}
            }
            
            self.logger.info("âœ… ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ê°’ ì„¤ì •
            self.matching_config = {'method': 'similarity', 'quality_threshold': 0.5}
            self.tps_config = {'regularization': 0.1, 'grid_size': 20}
            self.optimization_config = {'learning_rate': 0.01}
    
    async def initialize(self) -> bool:
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return True
        
        try:
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ
            await self._load_geometric_model()
            
            # 2. TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
            await self._initialize_tps_network()
            
            # 3. íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì •
            await self._setup_feature_extractor()
            
            # 4. M3 Max ìµœì í™” ì ìš©
            if self.is_m3_max:
                await self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_geometric_model(self):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ìƒì„±
            self.geometric_model = GeometricMatchingModel(
                feature_dim=self.matching_config['feature_dim'],
                num_keypoints=self.matching_config['num_keypoints']
            )
            
            # í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œë„
            checkpoint_path = Path("ai_models/geometric_matching/best_model.pth")
            if checkpoint_path.exists() and self.model_loader:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=self.device)
                    self.geometric_model.load_state_dict(checkpoint['model_state_dict'])
                    self.logger.info("âœ… í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.geometric_model = self.geometric_model.to(self.device)
            self.geometric_model.eval()
            
            # FP16 ìµœì í™” (M3 Max)
            if self.is_m3_max and self.optimization_enabled:
                if hasattr(torch, 'compile'):
                    self.geometric_model = torch.compile(self.geometric_model)
                
            self.logger.info("âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def _initialize_tps_network(self):
        """TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”"""
        try:
            self.tps_network = TPSTransformNetwork(
                grid_size=self.tps_config['grid_size']
            )
            self.tps_network = self.tps_network.to(self.device)
            
            self.logger.info("âœ… TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"TPS ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _setup_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì •"""
        try:
            # ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ì˜ ë°±ë³¸ì„ íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ ì‚¬ìš©
            if self.geometric_model:
                self.feature_extractor = self.geometric_model.backbone
                self.logger.info("âœ… íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if torch.backends.mps.is_available():
                torch.backends.mps.empty_cache()
                optimizations.append("MPS Memory Optimization")
            
            # 2. ëª¨ë¸ ìµœì í™”
            if hasattr(torch, 'jit') and self.geometric_model:
                try:
                    dummy_input = torch.randn(1, 3, 256, 256).to(self.device)
                    dummy_input2 = torch.randn(1, 3, 256, 256).to(self.device)
                    self.geometric_model = torch.jit.trace(
                        self.geometric_model, 
                        (dummy_input, dummy_input2)
                    )
                    optimizations.append("JIT Compilation")
                except:
                    pass
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if self.memory_gb >= 64:  # ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™œìš©
                self.optimization_config['batch_size'] *= 2
                optimizations.append("Large Memory Batch Optimization")
            
            if optimizations:
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
                
        except Exception as e:
            self.logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        pose_keypoints: Optional[np.ndarray] = None,
        body_mask: Optional[np.ndarray] = None,
        clothing_mask: Optional[np.ndarray] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ - ì‹¤ì œ AI ê¸°ëŠ¥ + ì‹œê°í™”"""
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ ì‹œì‘")
            
            # 1. ì…ë ¥ ì „ì²˜ë¦¬
            processed_input = await self._preprocess_inputs(
                person_image, clothing_image, pose_keypoints, body_mask, clothing_mask
            )
            
            # 2. AI ëª¨ë¸ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ë§¤ì¹­
            matching_result = await self._perform_neural_matching(
                processed_input['person_tensor'],
                processed_input['clothing_tensor']
            )
            
            # 3. TPS ë³€í˜• ê³„ì‚°
            tps_result = await self._compute_tps_transformation(
                matching_result,
                processed_input
            )
            
            # 4. ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
            warped_result = await self._apply_geometric_transform(
                processed_input['clothing_tensor'],
                tps_result['source_points'],
                tps_result['target_points']
            )
            
            # 5. í’ˆì§ˆ í‰ê°€
            quality_score = await self._evaluate_matching_quality(
                matching_result,
                tps_result,
                warped_result
            )
            
            # 6. í›„ì²˜ë¦¬
            final_result = await self._postprocess_result(
                warped_result,
                quality_score,
                processed_input
            )
            
            # ğŸ†• 7. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_matching_visualization(
                processed_input,
                matching_result,
                tps_result,
                warped_result,
                quality_score
            )
            
            # 8. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 9. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(quality_score, processing_time)
            
            self.logger.info(f"âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}s")
            
            # ğŸ†• API í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ê³¼ êµ¬ì¡° (ê¸°ì¡´ í•„ë“œ + ì‹œê°í™” í•„ë“œ)
            return {
                'success': True,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}',
                'confidence': quality_score,
                'processing_time': processing_time,
                'details': {
                    # ğŸ†• í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                    'result_image': visualization_results['matching_visualization'],
                    'overlay_image': visualization_results['warped_overlay'],
                    
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'num_keypoints': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                    'matching_confidence': matching_result['matching_confidence'],
                    'transformation_quality': quality_score,
                    'grid_size': self.tps_config['grid_size'],
                    'method': self.matching_config['method'],
                    
                    # ìƒì„¸ ë§¤ì¹­ ì •ë³´
                    'matching_details': {
                        'source_keypoints_count': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                        'target_keypoints_count': len(matching_result['target_keypoints'][0]) if len(matching_result['target_keypoints']) > 0 else 0,
                        'successful_matches': int(quality_score * 100),
                        'transformation_type': 'TPS (Thin Plate Spline)',
                        'optimization_enabled': self.optimization_enabled
                    },
                    
                    # ì‹œìŠ¤í…œ ì •ë³´
                    'step_info': {
                        'step_name': 'geometric_matching',
                        'step_number': 4,
                        'device': self.device,
                        'quality_level': self.quality_level,
                        'model_type': 'Neural TPS',
                        'optimization': 'M3 Max' if self.is_m3_max else self.device
                    },
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­
                    'quality_metrics': {
                        'overall_score': quality_score,
                        'matching_confidence': matching_result['matching_confidence'],
                        'keypoint_consistency': quality_score * 0.9,  # ì˜ˆì‹œ ê°’
                        'transformation_smoothness': quality_score * 0.95,
                        'visual_quality': quality_score * 0.88
                    }
                },
                
                # ë ˆê±°ì‹œ í˜¸í™˜ì„± í•„ë“œë“¤ (ê¸°ì¡´ APIì™€ì˜ í˜¸í™˜ì„±)
                'warped_clothing': final_result['warped_clothing'],
                'warped_mask': final_result['warped_mask'],
                'transformation_matrix': tps_result['transformation_matrix'],
                'source_keypoints': matching_result['source_keypoints'],
                'target_keypoints': matching_result['target_keypoints'],
                'matching_confidence': matching_result['matching_confidence'],
                'quality_score': quality_score,
                'metadata': {
                    'method': 'neural_tps',
                    'num_keypoints': self.matching_config['num_keypoints'],
                    'grid_size': self.tps_config['grid_size'],
                    'device': self.device,
                    'optimization_enabled': self.optimization_enabled
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}',
                'confidence': 0.0,
                'processing_time': time.time() - start_time,
                'details': {
                    'result_image': '',
                    'overlay_image': '',
                    'error': str(e),
                    'step_info': {
                        'step_name': 'geometric_matching',
                        'step_number': 4,
                        'error': str(e)
                    }
                },
                'error': str(e)
            }
    
    # ==============================================
    # ğŸ†• ì‹œê°í™” í•¨ìˆ˜ë“¤
    # ==============================================
    
    async def _create_matching_visualization(
        self,
        processed_input: Dict[str, Any],
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any],
        quality_score: float
    ) -> Dict[str, str]:
        """
        ğŸ†• ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±
        
        Args:
            processed_input: ì „ì²˜ë¦¬ëœ ì…ë ¥
            matching_result: ë§¤ì¹­ ê²°ê³¼
            tps_result: TPS ë³€í˜• ê²°ê³¼
            warped_result: ë³€í˜•ëœ ê²°ê³¼
            quality_score: í’ˆì§ˆ ì ìˆ˜
            
        Returns:
            Dict[str, str]: base64 ì¸ì½”ë”©ëœ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
        """
        try:
            if not self.visualization_config.get('enable_visualization', True):
                return {
                    'matching_visualization': '',
                    'warped_overlay': '',
                    'transformation_grid': ''
                }
            
            def _create_visualizations():
                # ì›ë³¸ ì´ë¯¸ì§€ë“¤ì„ PILë¡œ ë³€í™˜
                person_pil = self._tensor_to_pil(processed_input['person_tensor'])
                clothing_pil = self._tensor_to_pil(processed_input['clothing_tensor'])
                warped_clothing_pil = self._tensor_to_pil(warped_result['warped_image'])
                
                # 1. ğŸ¯ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”
                matching_viz = self._create_keypoint_matching_visualization(
                    person_pil, clothing_pil, matching_result
                )
                
                # 2. ğŸŒˆ ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´
                warped_overlay = self._create_warped_overlay(
                    person_pil, warped_clothing_pil, quality_score
                )
                
                # 3. ğŸ“ ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™” (ì„ íƒì‚¬í•­)
                transformation_grid = ''
                if self.visualization_config.get('show_transformation_grid', True):
                    grid_viz = self._create_transformation_grid_visualization(
                        clothing_pil, warped_result['warped_grid']
                    )
                    transformation_grid = self._pil_to_base64(grid_viz)
                
                return {
                    'matching_visualization': self._pil_to_base64(matching_viz),
                    'warped_overlay': self._pil_to_base64(warped_overlay),
                    'transformation_grid': transformation_grid
                }
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'matching_visualization': '',
                'warped_overlay': '',
                'transformation_grid': ''
            }
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # [B, C, H, W] -> [C, H, W]
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ì •ê·œí™” í•´ì œ (ImageNet ì •ê·œí™” ì—­ë³€í™˜)
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            tensor = tensor * std + mean
            
            # ê°’ ë²”ìœ„ í´ë¨í•‘
            tensor = torch.clamp(tensor, 0, 1)
            
            # [C, H, W] -> [H, W, C]
            tensor = tensor.permute(1, 2, 0)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            numpy_array = (tensor.numpy() * 255).astype(np.uint8)
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            return Image.fromarray(numpy_array)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', (512, 512), (128, 128, 128))
    
    def _create_keypoint_matching_visualization(
        self,
        person_pil: Image.Image,
        clothing_pil: Image.Image,
        matching_result: Dict[str, Any]
    ) -> Image.Image:
        """í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            target_size = (512, 512)
            person_resized = person_pil.resize(target_size, Image.Resampling.LANCZOS)
            clothing_resized = clothing_pil.resize(target_size, Image.Resampling.LANCZOS)
            
            # ë‚˜ë€íˆ ë°°ì¹˜í•  ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = target_size[0] * 2 + 50  # 50px ê°„ê²©
            canvas_height = target_size[1]
            canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))
            
            # ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(person_resized, (0, 0))
            canvas.paste(clothing_resized, (target_size[0] + 50, 0))
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(canvas)
            
            # í°íŠ¸ ì„¤ì •
            try:
                font = ImageFont.truetype("arial.ttf", 16)
                small_font = ImageFont.truetype("arial.ttf", 12)
            except:
                font = ImageFont.load_default()
                small_font = ImageFont.load_default()
            
            # í‚¤í¬ì¸íŠ¸ ì‹œê°í™”
            if self.visualization_config.get('show_keypoints', True):
                self._draw_keypoints_and_matches(
                    draw, matching_result, target_size, font
                )
            
            # ë§¤ì¹­ ì •ë³´ í…ìŠ¤íŠ¸
            self._draw_matching_info_text(
                draw, matching_result, canvas_width, canvas_height, font
            )
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€
            return Image.new('RGB', (1024, 512), (200, 200, 200))
    
    def _draw_keypoints_and_matches(
        self,
        draw: ImageDraw.ImageDraw,
        matching_result: Dict[str, Any],
        target_size: Tuple[int, int],
        font
    ):
        """í‚¤í¬ì¸íŠ¸ì™€ ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°"""
        try:
            source_keypoints = matching_result['source_keypoints']
            target_keypoints = matching_result['target_keypoints']
            confidence = matching_result['matching_confidence']
            
            if len(source_keypoints) == 0 or len(target_keypoints) == 0:
                return
            
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if torch.is_tensor(source_keypoints):
                source_kpts = source_keypoints[0].cpu().numpy()
            else:
                source_kpts = source_keypoints[0] if len(source_keypoints) > 0 else []
                
            if torch.is_tensor(target_keypoints):
                target_kpts = target_keypoints[0].cpu().numpy()
            else:
                target_kpts = target_keypoints[0] if len(target_keypoints) > 0 else []
            
            # ì¢Œí‘œ ì •ê·œí™” í•´ì œ (-1~1 -> í”½ì…€ ì¢Œí‘œ)
            def denormalize_coords(coords, size):
                if len(coords) == 0:
                    return []
                coords = np.array(coords)
                coords = (coords + 1) * 0.5  # -1~1 -> 0~1
                coords[:, 0] *= size[0]  # x ì¢Œí‘œ
                coords[:, 1] *= size[1]  # y ì¢Œí‘œ
                return coords
            
            source_coords = denormalize_coords(source_kpts, target_size)
            target_coords = denormalize_coords(target_kpts, target_size)
            
            # ì˜¤í”„ì…‹ (clothing ì´ë¯¸ì§€ëŠ” ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜)
            target_offset_x = target_size[0] + 50
            
            keypoint_size = self.visualization_config.get('keypoint_size', 3)
            line_thickness = self.visualization_config.get('line_thickness', 2)
            
            # í‚¤í¬ì¸íŠ¸ì™€ ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°
            num_points = min(len(source_coords), len(target_coords))
            for i in range(num_points):
                if i >= len(source_coords) or i >= len(target_coords):
                    break
                    
                # ì†ŒìŠ¤ í‚¤í¬ì¸íŠ¸ (person ì´ë¯¸ì§€)
                sx, sy = source_coords[i]
                draw.ellipse(
                    [sx-keypoint_size, sy-keypoint_size, sx+keypoint_size, sy+keypoint_size],
                    fill=(255, 0, 0), outline=(128, 0, 0)
                )
                
                # íƒ€ê²Ÿ í‚¤í¬ì¸íŠ¸ (clothing ì´ë¯¸ì§€)
                tx, ty = target_coords[i]
                tx += target_offset_x
                draw.ellipse(
                    [tx-keypoint_size, ty-keypoint_size, tx+keypoint_size, ty+keypoint_size],
                    fill=(0, 255, 0), outline=(0, 128, 0)
                )
                
                # ë§¤ì¹­ ë¼ì¸
                if self.visualization_config.get('show_matching_lines', True):
                    # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                    conf_value = confidence if isinstance(confidence, float) else 0.8
                    line_alpha = int(255 * conf_value)
                    line_color = (0, 0, 255) if conf_value > 0.7 else (255, 255, 0)
                    
                    draw.line(
                        [(sx, sy), (tx, ty)],
                        fill=line_color,
                        width=line_thickness
                    )
                
                # í‚¤í¬ì¸íŠ¸ ë²ˆí˜¸
                draw.text((sx+5, sy+5), str(i), fill=(255, 255, 255), font=font)
                draw.text((tx+5, ty+5), str(i), fill=(255, 255, 255), font=font)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
    
    def _draw_matching_info_text(
        self,
        draw: ImageDraw.ImageDraw,
        matching_result: Dict[str, Any],
        canvas_width: int,
        canvas_height: int,
        font
    ):
        """ë§¤ì¹­ ì •ë³´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°"""
        try:
            # ì •ë³´ í…ìŠ¤íŠ¸
            confidence = matching_result['matching_confidence']
            conf_text = f"ë§¤ì¹­ ì‹ ë¢°ë„: {confidence:.3f}"
            num_keypoints = len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0
            kpts_text = f"í‚¤í¬ì¸íŠ¸ ìˆ˜: {num_keypoints}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            text_bg_height = 60
            draw.rectangle(
                [(0, canvas_height - text_bg_height), (canvas_width, canvas_height)],
                fill=(0, 0, 0, 180)
            )
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            draw.text((10, canvas_height - 50), conf_text, fill=(255, 255, 255), font=font)
            draw.text((10, canvas_height - 30), kpts_text, fill=(255, 255, 255), font=font)
            
            # ìš°ì¸¡ì— ë²”ë¡€
            draw.text((canvas_width - 200, canvas_height - 50), "ğŸ”´ Person í‚¤í¬ì¸íŠ¸", fill=(255, 255, 255), font=font)
            draw.text((canvas_width - 200, canvas_height - 30), "ğŸŸ¢ Clothing í‚¤í¬ì¸íŠ¸", fill=(255, 255, 255), font=font)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë³´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
    
    def _create_warped_overlay(
        self,
        person_pil: Image.Image,
        warped_clothing_pil: Image.Image,
        quality_score: float
    ) -> Image.Image:
        """ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´ ìƒì„±"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            target_size = (512, 512)
            person_resized = person_pil.resize(target_size, Image.Resampling.LANCZOS)
            warped_resized = warped_clothing_pil.resize(target_size, Image.Resampling.LANCZOS)
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            alpha = 0.7 if quality_score > 0.8 else 0.5
            overlay = Image.blend(person_resized, warped_resized, alpha)
            
            # í’ˆì§ˆ ì •ë³´ ì˜¤ë²„ë ˆì´
            draw = ImageDraw.Draw(overlay)
            try:
                font = ImageFont.truetype("arial.ttf", 18)
            except:
                font = ImageFont.load_default()
            
            # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
            quality_text = f"ë§¤ì¹­ í’ˆì§ˆ: {quality_score:.1%}"
            quality_color = (0, 255, 0) if quality_score > 0.8 else (255, 255, 0) if quality_score > 0.6 else (255, 0, 0)
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            draw.rectangle([(10, 10), (250, 50)], fill=(0, 0, 0, 180))
            draw.text((20, 20), quality_text, fill=quality_color, font=font)
            
            return overlay
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _create_transformation_grid_visualization(
        self,
        clothing_pil: Image.Image,
        warped_grid: torch.Tensor
    ) -> Image.Image:
        """ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™”"""
        try:
            # ê·¸ë¦¬ë“œ ì •ë³´ ì¶”ì¶œ
            if torch.is_tensor(warped_grid):
                grid_np = warped_grid[0].cpu().numpy()  # [H, W, 2]
            else:
                grid_np = warped_grid
            
            # ì´ë¯¸ì§€ í¬ê¸°
            height, width = grid_np.shape[:2]
            grid_image = Image.new('RGB', (width, height), (255, 255, 255))
            draw = ImageDraw.Draw(grid_image)
            
            # ê·¸ë¦¬ë“œ ë°€ë„
            grid_density = self.visualization_config.get('grid_density', 20)
            step = max(1, height // grid_density)
            
            # ê·¸ë¦¬ë“œ ë¼ì¸ ê·¸ë¦¬ê¸°
            for y in range(0, height, step):
                for x in range(0, width, step):
                    if x < width-step and y < height-step:
                        # ì›ë˜ ì¢Œí‘œì—ì„œ ë³€í˜•ëœ ì¢Œí‘œë¡œì˜ ë²¡í„°
                        dx = grid_np[y, x, 0] * width * 0.1  # ìŠ¤ì¼€ì¼ ì¡°ì •
                        dy = grid_np[y, x, 1] * height * 0.1
                        
                        # í™”ì‚´í‘œ ê·¸ë¦¬ê¸°
                        end_x = x + dx
                        end_y = y + dy
                        
                        draw.line([(x, y), (end_x, end_y)], fill=(0, 0, 255), width=1)
                        draw.ellipse([x-1, y-1, x+1, y+1], fill=(255, 0, 0))
            
            return grid_image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê·¸ë¦¬ë“œ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), (240, 240, 240))
    
    def _pil_to_base64(self, pil_image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if self.visualization_config.get('quality') == 'high':
                quality = 95
            elif self.visualization_config.get('quality') == 'low':
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”§ ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ)
    # ==============================================
    
    async def _preprocess_inputs(
        self, 
        person_image, 
        clothing_image, 
        pose_keypoints, 
        body_mask, 
        clothing_mask
    ) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            clothing_tensor = self._image_to_tensor(clothing_image)
            
            # ì •ê·œí™”
            person_tensor = self._normalize_tensor(person_tensor)
            clothing_tensor = self._normalize_tensor(clothing_tensor)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            # ë§ˆìŠ¤í¬ ì²˜ë¦¬
            if body_mask is not None:
                body_mask = self._mask_to_tensor(body_mask).to(self.device)
            
            if clothing_mask is not None:
                clothing_mask = self._mask_to_tensor(clothing_mask).to(self.device)
            
            # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
            if pose_keypoints is not None:
                pose_keypoints = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            return {
                'person_tensor': person_tensor,
                'clothing_tensor': clothing_tensor,
                'pose_keypoints': pose_keypoints,
                'body_mask': body_mask,
                'clothing_mask': clothing_mask
            }
            
        except Exception as e:
            self.logger.error(f"ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _image_to_tensor(self, image) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if isinstance(image, torch.Tensor):
            return image
        elif isinstance(image, np.ndarray):
            if len(image.shape) == 3:
                image = image.transpose(2, 0, 1)  # HWC -> CHW
            tensor = torch.from_numpy(image).float() / 255.0
        elif isinstance(image, Image.Image):
            image = np.array(image)
            if len(image.shape) == 3:
                image = image.transpose(2, 0, 1)
            tensor = torch.from_numpy(image).float() / 255.0
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if len(tensor.shape) == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _mask_to_tensor(self, mask) -> torch.Tensor:
        """ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if isinstance(mask, torch.Tensor):
            return mask
        elif isinstance(mask, np.ndarray):
            tensor = torch.from_numpy(mask).float()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§ˆìŠ¤í¬ íƒ€ì…: {type(mask)}")
        
        # ë°°ì¹˜ ë° ì±„ë„ ì°¨ì› ì¶”ê°€
        if len(tensor.shape) == 2:
            tensor = tensor.unsqueeze(0).unsqueeze(0)
        elif len(tensor.shape) == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _normalize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:
        """í…ì„œ ì •ê·œí™” (ImageNet í‘œì¤€)"""
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(tensor.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(tensor.device)
        
        return (tensor - mean) / std
    
    async def _perform_neural_matching(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """ì‹ ê²½ë§ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        try:
            with torch.no_grad():
                # AI ëª¨ë¸ ì¶”ë¡ 
                model_output = self.geometric_model(person_tensor, clothing_tensor)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                source_keypoints = self._extract_keypoints(
                    model_output['source_keypoints']
                )
                target_keypoints = self._extract_keypoints(
                    model_output['target_keypoints']
                )
                
                # ë§¤ì¹­ ì‹ ë¢°ë„
                matching_confidence = model_output['matching_confidence'].mean().item()
                
                return {
                    'source_keypoints': source_keypoints,
                    'target_keypoints': target_keypoints,
                    'matching_confidence': matching_confidence,
                    'tps_params': model_output['tps_params'],
                    'source_features': model_output['source_features'],
                    'target_features': model_output['target_features']
                }
                
        except Exception as e:
            self.logger.error(f"ì‹ ê²½ë§ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_keypoints(self, heatmap: torch.Tensor) -> torch.Tensor:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        batch_size, num_points, height, width = heatmap.shape
        
        # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
        heatmap_flat = heatmap.view(batch_size, num_points, -1)
        max_indices = torch.argmax(heatmap_flat, dim=2)
        
        # ì¢Œí‘œ ë³€í™˜
        y_coords = (max_indices // width).float()
        x_coords = (max_indices % width).float()
        
        # ì •ê·œí™” (-1 ~ 1)
        x_coords = (x_coords / (width - 1)) * 2 - 1
        y_coords = (y_coords / (height - 1)) * 2 - 1
        
        keypoints = torch.stack([x_coords, y_coords], dim=-1)
        
        return keypoints
    
    async def _compute_tps_transformation(
        self, 
        matching_result: Dict[str, Any],
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            source_points = matching_result['source_keypoints']
            target_points = matching_result['target_keypoints']
            
            # ë³€í˜• í–‰ë ¬ ê³„ì‚°
            transformation_matrix = self._compute_transformation_matrix(
                source_points, target_points
            )
            
            return {
                'source_points': source_points,
                'target_points': target_points,
                'transformation_matrix': transformation_matrix
            }
            
        except Exception as e:
            self.logger.error(f"TPS ë³€í˜• ê³„ì‚° ì‹¤íŒ¨: {e}")
            raise
    
    def _compute_transformation_matrix(
        self, 
        source_points: torch.Tensor, 
        target_points: torch.Tensor
    ) -> torch.Tensor:
        """ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        
        # ë‹¨ìˆœí™”ëœ ì–´íŒŒì¸ ë³€í˜• ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ TPS ê³„ì‚°ì´ í•„ìš”
        transformation_matrix = torch.eye(3, device=source_points.device).unsqueeze(0).repeat(batch_size, 1, 1)
        
        return transformation_matrix
    
    async def _apply_geometric_transform(
        self,
        clothing_tensor: torch.Tensor,
        source_points: torch.Tensor,
        target_points: torch.Tensor
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë³€í˜• ì ìš©"""
        try:
            # TPS ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ë³€í˜•
            warped_image, warped_grid = self.tps_network.apply_tps_transform(
                clothing_tensor, source_points, target_points
            )
            
            return {
                'warped_image': warped_image,
                'warped_grid': warped_grid
            }
            
        except Exception as e:
            self.logger.error(f"ê¸°í•˜í•™ì  ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            raise
    
    async def _evaluate_matching_quality(
        self,
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any]
    ) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            # ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•œ í’ˆì§ˆ ì ìˆ˜
            confidence_score = matching_result['matching_confidence']
            
            # í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ì ìˆ˜
            consistency_score = self._compute_keypoint_consistency(
                matching_result['source_keypoints'],
                matching_result['target_keypoints']
            )
            
            # ë³€í˜• í’ˆì§ˆ ì ìˆ˜
            warp_quality = self._compute_warp_quality(warped_result['warped_image'])
            
            # ì¢…í•© ì ìˆ˜
            quality_score = (
                0.4 * confidence_score +
                0.3 * consistency_score +
                0.3 * warp_quality
            )
            
            return float(quality_score)
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _compute_keypoint_consistency(
        self,
        source_keypoints: torch.Tensor,
        target_keypoints: torch.Tensor
    ) -> float:
        """í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ë¶„ì‚°ìœ¼ë¡œ ì¼ê´€ì„± ì¸¡ì •
            distances = torch.norm(source_keypoints - target_keypoints, dim=-1)
            consistency = 1.0 / (1.0 + distances.std().item())
            
            return min(1.0, max(0.0, consistency))
            
        except:
            return 0.5
    
    def _compute_warp_quality(self, warped_image: torch.Tensor) -> float:
        """ë³€í˜• í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ì´ë¯¸ì§€ ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •
            grad_x = torch.abs(warped_image[:, :, :, 1:] - warped_image[:, :, :, :-1])
            grad_y = torch.abs(warped_image[:, :, 1:, :] - warped_image[:, :, :-1, :])
            
            gradient_magnitude = torch.sqrt(grad_x.mean() ** 2 + grad_y.mean() ** 2)
            
            # ì ì ˆí•œ ê·¸ë¼ë””ì–¸íŠ¸ í¬ê¸°ëŠ” ì¢‹ì€ í’ˆì§ˆì„ ì˜ë¯¸
            quality = torch.exp(-gradient_magnitude * 5).item()
            
            return min(1.0, max(0.0, quality))
            
        except:
            return 0.5
    
    async def _postprocess_result(
        self,
        warped_result: Dict[str, Any],
        quality_score: float,
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_clothing = warped_result['warped_image']
            
            # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            warped_clothing_np = self._tensor_to_numpy(warped_clothing)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            warped_mask = self._generate_warped_mask(warped_clothing)
            warped_mask_np = self._tensor_to_numpy(warped_mask)
            
            # í’ˆì§ˆ ê¸°ë°˜ í›„ì²˜ë¦¬
            if quality_score > 0.8:
                warped_clothing_np = self._enhance_high_quality(warped_clothing_np)
            elif quality_score < 0.5:
                warped_clothing_np = self._fix_low_quality(warped_clothing_np)
            
            return {
                'warped_clothing': warped_clothing_np,
                'warped_mask': warped_mask_np
            }
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
        
        if tensor.dim() == 3:
            tensor = tensor.permute(1, 2, 0)  # CHW -> HWC
        
        # ì •ê·œí™” í•´ì œ
        tensor = tensor * 255.0
        tensor = torch.clamp(tensor, 0, 255)
        
        return tensor.detach().cpu().numpy().astype(np.uint8)
    
    def _generate_warped_mask(self, warped_image: torch.Tensor) -> torch.Tensor:
        """ë³€í˜•ëœ ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ ìƒì„±"""
        # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬
        gray = warped_image.mean(dim=1, keepdim=True)
        mask = (gray > 0.1).float()
        
        return mask
    
    def _enhance_high_quality(self, image: np.ndarray) -> np.ndarray:
        """ê³ í’ˆì§ˆ ì´ë¯¸ì§€ í–¥ìƒ"""
        try:
            # ì•½ê°„ì˜ ìƒ¤í”„ë‹ ì ìš©
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(image, -1, kernel)
            
            # ì›ë³¸ê³¼ ìƒ¤í”„ë‹ ê²°ê³¼ ë¸”ë Œë”©
            enhanced = cv2.addWeighted(image, 0.8, sharpened, 0.2, 0)
            
            return enhanced
        except:
            return image
    
    def _fix_low_quality(self, image: np.ndarray) -> np.ndarray:
        """ì €í’ˆì§ˆ ì´ë¯¸ì§€ ìˆ˜ì •"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            blurred = cv2.GaussianBlur(image, (3, 3), 0.5)
            
            return blurred
        except:
            return image
    
    def _update_stats(self, quality_score: float, processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.matching_stats['total_matches'] += 1
        
        if quality_score > self.matching_config['quality_threshold']:
            self.matching_stats['successful_matches'] += 1
        
        # í‰ê·  ì •í™•ë„ ì—…ë°ì´íŠ¸
        total = self.matching_stats['total_matches']
        current_avg = self.matching_stats['average_accuracy']
        self.matching_stats['average_accuracy'] = (
            (current_avg * (total - 1) + quality_score) / total
        )
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 4ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "device": self.device,
                "initialized": self.is_initialized,
                "models_loaded": {
                    "geometric_model": self.geometric_model is not None,
                    "tps_network": self.tps_network is not None,
                    "feature_extractor": self.feature_extractor is not None
                },
                "config": {
                    "method": self.matching_config['method'],
                    "num_keypoints": self.matching_config['num_keypoints'],
                    "grid_size": self.tps_config['grid_size'],
                    "quality_level": self.quality_level,
                    "quality_threshold": self.matching_config['quality_threshold'],
                    "visualization_enabled": self.visualization_config.get('enable_visualization', True)
                },
                "performance": self.matching_stats,
                "optimization": {
                    "m3_max_enabled": self.is_m3_max,
                    "optimization_enabled": self.optimization_enabled,
                    "memory_gb": self.memory_gb,
                    "device_type": self.device_type
                },
                "visualization": {
                    "show_keypoints": self.visualization_config.get('show_keypoints', True),
                    "show_matching_lines": self.visualization_config.get('show_matching_lines', True),
                    "show_transformation_grid": self.visualization_config.get('show_transformation_grid', True),
                    "quality": self.visualization_config.get('quality', 'high')
                }
            }
        except Exception as e:
            self.logger.error(f"ë‹¨ê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "error": str(e)
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'geometric_model') and self.geometric_model:
                if hasattr(self.geometric_model, 'cpu'):
                    self.geometric_model.cpu()
                del self.geometric_model
                self.geometric_model = None
            
            if hasattr(self, 'tps_network') and self.tps_network:
                if hasattr(self.tps_network, 'cpu'):
                    self.tps_network.cpu()
                del self.tps_network
                self.tps_network = None
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            gc.collect()
            self.logger.info("âœ… 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(
    device: str = "mps", 
    config: Optional[Dict[str, Any]] = None
) -> GeometricMatchingStep:
    """ê¸°ì¡´ ë°©ì‹ 100% í˜¸í™˜ ìƒì„±ì"""
    return GeometricMatchingStep(device=device, config=config)

def create_m3_max_geometric_matching_step(
    device: Optional[str] = None,
    memory_gb: float = 128.0,
    optimization_level: str = "ultra",
    **kwargs
) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” ì „ìš© ìƒì„±ì"""
    return GeometricMatchingStep(
        device=device,
        memory_gb=memory_gb,
        quality_level=optimization_level,
        is_m3_max=True,
        optimization_enabled=True,
        **kwargs
    )

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'GeometricMatchingStep',
    'GeometricMatchingModel',
    'TPSTransformNetwork',
    'create_geometric_matching_step',
    'create_m3_max_geometric_matching_step'
]