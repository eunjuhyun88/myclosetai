# app/ai_pipeline/steps/step_04_geometric_matching.py
"""
4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching) - AI ëª¨ë¸ ì™„ì „ ì—°ë™ ì‹¤ì œ êµ¬í˜„
âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” TPS ë³€í˜•
âœ… AI ëª¨ë¸ê³¼ ì™„ì „ ì—°ë™
âœ… í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ë° ë³€í˜•
âœ… M3 Max ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
"""

import os
import logging
import time
import asyncio
import gc
from typing import Dict, Any, Optional, Tuple, List, Union
import numpy as np
import json
import math
from pathlib import Path

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    raise ImportError("PyTorch is required for GeometricMatchingStep")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    raise ImportError("OpenCV is required for GeometricMatchingStep")

try:
    from PIL import Image, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    raise ImportError("PIL is required for GeometricMatchingStep")

try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ§  AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__(self, feature_dim=256, num_keypoints=25):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_keypoints = num_keypoints
        
        # íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸
        self.backbone = self._build_backbone()
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ
        self.keypoint_head = self._build_keypoint_head()
        
        # íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ
        self.matching_head = self._build_matching_head()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ
        self.tps_head = self._build_tps_head()
        
    def _build_backbone(self):
        """íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # Stage 2
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            
            # Stage 3
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            # Feature refinement
            nn.Conv2d(512, self.feature_dim, 3, 1, 1),
            nn.BatchNorm2d(self.feature_dim),
            nn.ReLU(inplace=True)
        )
    
    def _make_layer(self, in_planes, planes, blocks, stride=1):
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
        layers = []
        layers.append(nn.Conv2d(in_planes, planes, 3, stride, 1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, 3, 1, 1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_keypoint_head(self):
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, self.num_keypoints, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_matching_head(self):
        """íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim * 2, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 1, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_tps_head(self):
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, self.num_keypoints * 2)  # (x, y) coordinates
        )
    
    def forward(self, source_img, target_img):
        """ìˆœì „íŒŒ"""
        # íŠ¹ì§• ì¶”ì¶œ
        source_features = self.backbone(source_img)
        target_features = self.backbone(target_img)
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        source_keypoints = self.keypoint_head(source_features)
        target_keypoints = self.keypoint_head(target_features)
        
        # íŠ¹ì§• ë§¤ì¹­
        concat_features = torch.cat([source_features, target_features], dim=1)
        matching_confidence = self.matching_head(concat_features)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.tps_head(source_features)
        tps_params = tps_params.view(-1, self.num_keypoints, 2)
        
        return {
            'source_keypoints': source_keypoints,
            'target_keypoints': target_keypoints,
            'matching_confidence': matching_confidence,
            'tps_params': tps_params,
            'source_features': source_features,
            'target_features': target_features
        }

class TPSTransformNetwork(nn.Module):
    """Thin Plate Spline ë³€í˜• ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, grid_size=20):
        super().__init__()
        self.grid_size = grid_size
        
    def create_grid(self, height, width, device):
        """ì •ê·œí™”ëœ ê·¸ë¦¬ë“œ ìƒì„±"""
        x = torch.linspace(-1, 1, width, device=device)
        y = torch.linspace(-1, 1, height, device=device)
        grid_x, grid_y = torch.meshgrid(x, y, indexing='xy')
        grid = torch.stack([grid_x, grid_y], dim=-1)
        return grid.unsqueeze(0)  # [1, H, W, 2]
    
    def compute_tps_weights(self, source_points, target_points):
        """TPS ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        device = source_points.device
        
        # ì œì–´ì  ê°„ ê±°ë¦¬ ê³„ì‚°
        source_points_expanded = source_points.unsqueeze(2)  # [B, N, 1, 2]
        target_points_expanded = target_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(source_points_expanded - target_points_expanded, dim=-1)  # [B, N, N]
        
        # RBF ì»¤ë„ ê³„ì‚° (r^2 * log(r))
        distances = distances + 1e-8  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±
        rbf_weights = distances ** 2 * torch.log(distances)
        
        # íŠ¹ì´ì  ì²˜ë¦¬
        rbf_weights = torch.where(distances < 1e-6, torch.zeros_like(rbf_weights), rbf_weights)
        
        return rbf_weights
    
    def apply_tps_transform(self, image, source_points, target_points):
        """TPS ë³€í˜• ì ìš©"""
        batch_size, channels, height, width = image.shape
        device = image.device
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid = self.create_grid(height, width, device)
        grid = grid.repeat(batch_size, 1, 1, 1)  # [B, H, W, 2]
        
        # TPS ê°€ì¤‘ì¹˜ ê³„ì‚°
        tps_weights = self.compute_tps_weights(source_points, target_points)
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°
        transformed_grid = self.compute_transformed_grid(
            grid, source_points, target_points, tps_weights
        )
        
        # ì´ë¯¸ì§€ ë¦¬ìƒ˜í”Œë§
        transformed_image = F.grid_sample(
            image, transformed_grid, 
            mode='bilinear', 
            padding_mode='border', 
            align_corners=True
        )
        
        return transformed_image, transformed_grid
    
    def compute_transformed_grid(self, grid, source_points, target_points, tps_weights):
        """ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°"""
        batch_size, height, width, _ = grid.shape
        num_points = source_points.shape[1]
        device = grid.device
        
        # ê·¸ë¦¬ë“œë¥¼ í‰ë©´ìœ¼ë¡œ ë³€í™˜
        grid_flat = grid.view(batch_size, -1, 2)  # [B, H*W, 2]
        
        # ê° ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ì™€ ì œì–´ì  ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        grid_expanded = grid_flat.unsqueeze(2)  # [B, H*W, 1, 2]
        source_expanded = source_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(grid_expanded - source_expanded, dim=-1)  # [B, H*W, N]
        distances = distances + 1e-8
        
        # RBF ê°’ ê³„ì‚°
        rbf_values = distances ** 2 * torch.log(distances)
        rbf_values = torch.where(distances < 1e-6, torch.zeros_like(rbf_values), rbf_values)
        
        # ë³€ìœ„ ê³„ì‚°
        displacement = target_points - source_points  # [B, N, 2]
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³€í˜• ê³„ì‚°
        weights = torch.softmax(-distances / 0.1, dim=-1)  # [B, H*W, N]
        interpolated_displacement = torch.sum(
            weights.unsqueeze(-1) * displacement.unsqueeze(1), dim=2
        )  # [B, H*W, 2]
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ
        transformed_grid_flat = grid_flat + interpolated_displacement
        transformed_grid = transformed_grid_flat.view(batch_size, height, width, 2)
        
        return transformed_grid

# ==============================================
# ğŸ¯ ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤
# ==============================================

class GeometricMatchingStep:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ë‹¨ê³„ - AI ëª¨ë¸ê³¼ ì™„ì „ ì—°ë™"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        device_type: Optional[str] = None,
        memory_gb: Optional[float] = None,
        is_m3_max: Optional[bool] = None,
        optimization_enabled: Optional[bool] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ):
        """ì™„ì „ í˜¸í™˜ ìƒì„±ì - ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì›"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        self.device_type = device_type or kwargs.get('device_type', 'auto')
        self.memory_gb = memory_gb or kwargs.get('memory_gb', 16.0)
        self.is_m3_max = is_m3_max if is_m3_max is not None else kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = optimization_enabled if optimization_enabled is not None else kwargs.get('optimization_enabled', True)
        self.quality_level = quality_level or kwargs.get('quality_level', 'balanced')
        
        # ê¸°ë³¸ ì„¤ì •
        self._merge_step_specific_config(kwargs)
        self.is_initialized = False
        self.initialization_error = None
        
        # AI ëª¨ë¸ë“¤
        self.geometric_model = None
        self.tps_network = None
        self.feature_extractor = None
        
        # ëª¨ë¸ ë¡œë” ì„¤ì •
        self._setup_model_loader()
        
        # ìŠ¤í… íŠ¹í™” ì´ˆê¸°í™”
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if device:
            return device
        
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if torch.backends.mps.is_available():
                # macOSì—ì„œ MPS ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ M3 Maxì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                return True
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs):
        """ìŠ¤í…ë³„ ì„¤ì • ë³‘í•©"""
        step_config = kwargs.get('step_config', {})
        self.config.update(step_config)
    
    def _setup_model_loader(self):
        """ëª¨ë¸ ë¡œë” ì„¤ì •"""
        try:
            from app.ai_pipeline.utils.model_loader import ModelLoader
            self.model_loader = ModelLoader(device=self.device)
            self.logger.info("âœ… ModelLoader ì—°ë™ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    def _initialize_step_specific(self):
        """ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ë§¤ì¹­ ì„¤ì •
            base_config = {
                'method': 'neural_tps',
                'num_keypoints': 25,
                'feature_dim': 256,
                'grid_size': 30 if self.is_m3_max else 20,
                'max_iterations': 1000,
                'convergence_threshold': 1e-6,
                'outlier_threshold': 0.15,
                'use_pose_guidance': True,
                'adaptive_weights': True,
                'quality_threshold': 0.7
            }
            
            # quality_levelì— ë”°ë¥¸ ì¡°ì •
            if self.quality_level == 'high':
                base_config.update({
                    'num_keypoints': 30,
                    'max_iterations': 1500,
                    'quality_threshold': 0.8,
                    'convergence_threshold': 1e-7
                })
            elif self.quality_level == 'ultra':
                base_config.update({
                    'num_keypoints': 35,
                    'max_iterations': 2000,
                    'quality_threshold': 0.9,
                    'convergence_threshold': 1e-8
                })
            elif self.quality_level == 'fast':
                base_config.update({
                    'num_keypoints': 20,
                    'max_iterations': 500,
                    'quality_threshold': 0.6,
                    'convergence_threshold': 1e-5
                })
            
            self.matching_config = self.config.get('matching', base_config)
            
            # TPS ì„¤ì •
            self.tps_config = self.config.get('tps', {
                'regularization': 0.1,
                'grid_size': self.matching_config['grid_size'],
                'boundary_padding': 0.1,
                'smoothing_factor': 0.8
            })
            
            # ìµœì í™” ì„¤ì •
            learning_rate_base = 0.01
            if self.is_m3_max and self.optimization_enabled:
                learning_rate_base *= 1.2
            
            self.optimization_config = self.config.get('optimization', {
                'learning_rate': learning_rate_base,
                'momentum': 0.9,
                'weight_decay': 1e-4,
                'scheduler_step': 100,
                'batch_size': 8 if self.is_m3_max else 4
            })
            
            # í†µê³„ ì´ˆê¸°í™”
            self.matching_stats = {
                'total_matches': 0,
                'successful_matches': 0,
                'average_accuracy': 0.0,
                'method_performance': {}
            }
            
            self.logger.info("âœ… ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤í…ë³„ íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ê°’ ì„¤ì •
            self.matching_config = {'method': 'similarity', 'quality_threshold': 0.5}
            self.tps_config = {'regularization': 0.1, 'grid_size': 20}
            self.optimization_config = {'learning_rate': 0.01}
    
    async def initialize(self) -> bool:
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return True
        
        try:
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ
            await self._load_geometric_model()
            
            # 2. TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
            await self._initialize_tps_network()
            
            # 3. íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì •
            await self._setup_feature_extractor()
            
            # 4. M3 Max ìµœì í™” ì ìš©
            if self.is_m3_max:
                await self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_geometric_model(self):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ìƒì„±
            self.geometric_model = GeometricMatchingModel(
                feature_dim=self.matching_config['feature_dim'],
                num_keypoints=self.matching_config['num_keypoints']
            )
            
            # í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œë„
            checkpoint_path = Path("ai_models/geometric_matching/best_model.pth")
            if checkpoint_path.exists() and self.model_loader:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=self.device)
                    self.geometric_model.load_state_dict(checkpoint['model_state_dict'])
                    self.logger.info("âœ… í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"í”„ë¦¬íŠ¸ë ˆì¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.geometric_model = self.geometric_model.to(self.device)
            self.geometric_model.eval()
            
            # FP16 ìµœì í™” (M3 Max)
            if self.is_m3_max and self.optimization_enabled:
                if hasattr(torch, 'compile'):
                    self.geometric_model = torch.compile(self.geometric_model)
                
            self.logger.info("âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def _initialize_tps_network(self):
        """TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”"""
        try:
            self.tps_network = TPSTransformNetwork(
                grid_size=self.tps_config['grid_size']
            )
            self.tps_network = self.tps_network.to(self.device)
            
            self.logger.info("âœ… TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"TPS ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _setup_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì •"""
        try:
            # ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ì˜ ë°±ë³¸ì„ íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ ì‚¬ìš©
            if self.geometric_model:
                self.feature_extractor = self.geometric_model.backbone
                self.logger.info("âœ… íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì§• ì¶”ì¶œê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if torch.backends.mps.is_available():
                torch.backends.mps.empty_cache()
                optimizations.append("MPS Memory Optimization")
            
            # 2. ëª¨ë¸ ìµœì í™”
            if hasattr(torch, 'jit') and self.geometric_model:
                try:
                    dummy_input = torch.randn(1, 3, 256, 256).to(self.device)
                    dummy_input2 = torch.randn(1, 3, 256, 256).to(self.device)
                    self.geometric_model = torch.jit.trace(
                        self.geometric_model, 
                        (dummy_input, dummy_input2)
                    )
                    optimizations.append("JIT Compilation")
                except:
                    pass
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if self.memory_gb >= 64:  # ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™œìš©
                self.optimization_config['batch_size'] *= 2
                optimizations.append("Large Memory Batch Optimization")
            
            if optimizations:
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
                
        except Exception as e:
            self.logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        pose_keypoints: Optional[np.ndarray] = None,
        body_mask: Optional[np.ndarray] = None,
        clothing_mask: Optional[np.ndarray] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ - ì‹¤ì œ AI ê¸°ëŠ¥"""
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ ì‹œì‘")
            
            # 1. ì…ë ¥ ì „ì²˜ë¦¬
            processed_input = await self._preprocess_inputs(
                person_image, clothing_image, pose_keypoints, body_mask, clothing_mask
            )
            
            # 2. AI ëª¨ë¸ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ë§¤ì¹­
            matching_result = await self._perform_neural_matching(
                processed_input['person_tensor'],
                processed_input['clothing_tensor']
            )
            
            # 3. TPS ë³€í˜• ê³„ì‚°
            tps_result = await self._compute_tps_transformation(
                matching_result,
                processed_input
            )
            
            # 4. ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
            warped_result = await self._apply_geometric_transform(
                processed_input['clothing_tensor'],
                tps_result['source_points'],
                tps_result['target_points']
            )
            
            # 5. í’ˆì§ˆ í‰ê°€
            quality_score = await self._evaluate_matching_quality(
                matching_result,
                tps_result,
                warped_result
            )
            
            # 6. í›„ì²˜ë¦¬
            final_result = await self._postprocess_result(
                warped_result,
                quality_score,
                processed_input
            )
            
            # 7. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(quality_score, processing_time)
            
            self.logger.info(f"âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}s")
            
            return {
                'success': True,
                'warped_clothing': final_result['warped_clothing'],
                'warped_mask': final_result['warped_mask'],
                'transformation_matrix': tps_result['transformation_matrix'],
                'source_keypoints': matching_result['source_keypoints'],
                'target_keypoints': matching_result['target_keypoints'],
                'matching_confidence': matching_result['matching_confidence'],
                'quality_score': quality_score,
                'processing_time': processing_time,
                'metadata': {
                    'method': 'neural_tps',
                    'num_keypoints': self.matching_config['num_keypoints'],
                    'grid_size': self.tps_config['grid_size'],
                    'device': self.device,
                    'optimization_enabled': self.optimization_enabled
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    async def _preprocess_inputs(
        self, 
        person_image, 
        clothing_image, 
        pose_keypoints, 
        body_mask, 
        clothing_mask
    ) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            clothing_tensor = self._image_to_tensor(clothing_image)
            
            # ì •ê·œí™”
            person_tensor = self._normalize_tensor(person_tensor)
            clothing_tensor = self._normalize_tensor(clothing_tensor)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            # ë§ˆìŠ¤í¬ ì²˜ë¦¬
            if body_mask is not None:
                body_mask = self._mask_to_tensor(body_mask).to(self.device)
            
            if clothing_mask is not None:
                clothing_mask = self._mask_to_tensor(clothing_mask).to(self.device)
            
            # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
            if pose_keypoints is not None:
                pose_keypoints = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            return {
                'person_tensor': person_tensor,
                'clothing_tensor': clothing_tensor,
                'pose_keypoints': pose_keypoints,
                'body_mask': body_mask,
                'clothing_mask': clothing_mask
            }
            
        except Exception as e:
            self.logger.error(f"ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _image_to_tensor(self, image) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if isinstance(image, torch.Tensor):
            return image
        elif isinstance(image, np.ndarray):
            if len(image.shape) == 3:
                image = image.transpose(2, 0, 1)  # HWC -> CHW
            tensor = torch.from_numpy(image).float() / 255.0
        elif isinstance(image, Image.Image):
            image = np.array(image)
            if len(image.shape) == 3:
                image = image.transpose(2, 0, 1)
            tensor = torch.from_numpy(image).float() / 255.0
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if len(tensor.shape) == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _mask_to_tensor(self, mask) -> torch.Tensor:
        """ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if isinstance(mask, torch.Tensor):
            return mask
        elif isinstance(mask, np.ndarray):
            tensor = torch.from_numpy(mask).float()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§ˆìŠ¤í¬ íƒ€ì…: {type(mask)}")
        
        # ë°°ì¹˜ ë° ì±„ë„ ì°¨ì› ì¶”ê°€
        if len(tensor.shape) == 2:
            tensor = tensor.unsqueeze(0).unsqueeze(0)
        elif len(tensor.shape) == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _normalize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:
        """í…ì„œ ì •ê·œí™” (ImageNet í‘œì¤€)"""
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(tensor.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(tensor.device)
        
        return (tensor - mean) / std
    
    async def _perform_neural_matching(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """ì‹ ê²½ë§ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        try:
            with torch.no_grad():
                # AI ëª¨ë¸ ì¶”ë¡ 
                model_output = self.geometric_model(person_tensor, clothing_tensor)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                source_keypoints = self._extract_keypoints(
                    model_output['source_keypoints']
                )
                target_keypoints = self._extract_keypoints(
                    model_output['target_keypoints']
                )
                
                # ë§¤ì¹­ ì‹ ë¢°ë„
                matching_confidence = model_output['matching_confidence'].mean().item()
                
                return {
                    'source_keypoints': source_keypoints,
                    'target_keypoints': target_keypoints,
                    'matching_confidence': matching_confidence,
                    'tps_params': model_output['tps_params'],
                    'source_features': model_output['source_features'],
                    'target_features': model_output['target_features']
                }
                
        except Exception as e:
            self.logger.error(f"ì‹ ê²½ë§ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_keypoints(self, heatmap: torch.Tensor) -> torch.Tensor:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        batch_size, num_points, height, width = heatmap.shape
        
        # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
        heatmap_flat = heatmap.view(batch_size, num_points, -1)
        max_indices = torch.argmax(heatmap_flat, dim=2)
        
        # ì¢Œí‘œ ë³€í™˜
        y_coords = (max_indices // width).float()
        x_coords = (max_indices % width).float()
        
        # ì •ê·œí™” (-1 ~ 1)
        x_coords = (x_coords / (width - 1)) * 2 - 1
        y_coords = (y_coords / (height - 1)) * 2 - 1
        
        keypoints = torch.stack([x_coords, y_coords], dim=-1)
        
        return keypoints
    
    async def _compute_tps_transformation(
        self, 
        matching_result: Dict[str, Any],
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            source_points = matching_result['source_keypoints']
            target_points = matching_result['target_keypoints']
            
            # ë³€í˜• í–‰ë ¬ ê³„ì‚°
            transformation_matrix = self._compute_transformation_matrix(
                source_points, target_points
            )
            
            return {
                'source_points': source_points,
                'target_points': target_points,
                'transformation_matrix': transformation_matrix
            }
            
        except Exception as e:
            self.logger.error(f"TPS ë³€í˜• ê³„ì‚° ì‹¤íŒ¨: {e}")
            raise
    
    def _compute_transformation_matrix(
        self, 
        source_points: torch.Tensor, 
        target_points: torch.Tensor
    ) -> torch.Tensor:
        """ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        
        # ë‹¨ìˆœí™”ëœ ì–´íŒŒì¸ ë³€í˜• ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ TPS ê³„ì‚°ì´ í•„ìš”
        transformation_matrix = torch.eye(3, device=source_points.device).unsqueeze(0).repeat(batch_size, 1, 1)
        
        return transformation_matrix
    
    async def _apply_geometric_transform(
        self,
        clothing_tensor: torch.Tensor,
        source_points: torch.Tensor,
        target_points: torch.Tensor
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë³€í˜• ì ìš©"""
        try:
            # TPS ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ë³€í˜•
            warped_image, warped_grid = self.tps_network.apply_tps_transform(
                clothing_tensor, source_points, target_points
            )
            
            return {
                'warped_image': warped_image,
                'warped_grid': warped_grid
            }
            
        except Exception as e:
            self.logger.error(f"ê¸°í•˜í•™ì  ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            raise
    
    async def _evaluate_matching_quality(
        self,
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any]
    ) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            # ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•œ í’ˆì§ˆ ì ìˆ˜
            confidence_score = matching_result['matching_confidence']
            
            # í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ì ìˆ˜
            consistency_score = self._compute_keypoint_consistency(
                matching_result['source_keypoints'],
                matching_result['target_keypoints']
            )
            
            # ë³€í˜• í’ˆì§ˆ ì ìˆ˜
            warp_quality = self._compute_warp_quality(warped_result['warped_image'])
            
            # ì¢…í•© ì ìˆ˜
            quality_score = (
                0.4 * confidence_score +
                0.3 * consistency_score +
                0.3 * warp_quality
            )
            
            return float(quality_score)
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _compute_keypoint_consistency(
        self,
        source_keypoints: torch.Tensor,
        target_keypoints: torch.Tensor
    ) -> float:
        """í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ë¶„ì‚°ìœ¼ë¡œ ì¼ê´€ì„± ì¸¡ì •
            distances = torch.norm(source_keypoints - target_keypoints, dim=-1)
            consistency = 1.0 / (1.0 + distances.std().item())
            
            return min(1.0, max(0.0, consistency))
            
        except:
            return 0.5
    
    def _compute_warp_quality(self, warped_image: torch.Tensor) -> float:
        """ë³€í˜• í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ì´ë¯¸ì§€ ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •
            grad_x = torch.abs(warped_image[:, :, :, 1:] - warped_image[:, :, :, :-1])
            grad_y = torch.abs(warped_image[:, :, 1:, :] - warped_image[:, :, :-1, :])
            
            gradient_magnitude = torch.sqrt(grad_x.mean() ** 2 + grad_y.mean() ** 2)
            
            # ì ì ˆí•œ ê·¸ë¼ë””ì–¸íŠ¸ í¬ê¸°ëŠ” ì¢‹ì€ í’ˆì§ˆì„ ì˜ë¯¸
            quality = torch.exp(-gradient_magnitude * 5).item()
            
            return min(1.0, max(0.0, quality))
            
        except:
            return 0.5
    
    async def _postprocess_result(
        self,
        warped_result: Dict[str, Any],
        quality_score: float,
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_clothing = warped_result['warped_image']
            
            # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            warped_clothing_np = self._tensor_to_numpy(warped_clothing)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            warped_mask = self._generate_warped_mask(warped_clothing)
            warped_mask_np = self._tensor_to_numpy(warped_mask)
            
            # í’ˆì§ˆ ê¸°ë°˜ í›„ì²˜ë¦¬
            if quality_score > 0.8:
                warped_clothing_np = self._enhance_high_quality(warped_clothing_np)
            elif quality_score < 0.5:
                warped_clothing_np = self._fix_low_quality(warped_clothing_np)
            
            return {
                'warped_clothing': warped_clothing_np,
                'warped_mask': warped_mask_np
            }
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
        
        if tensor.dim() == 3:
            tensor = tensor.permute(1, 2, 0)  # CHW -> HWC
        
        # ì •ê·œí™” í•´ì œ
        tensor = tensor * 255.0
        tensor = torch.clamp(tensor, 0, 255)
        
        return tensor.detach().cpu().numpy().astype(np.uint8)
    
    def _generate_warped_mask(self, warped_image: torch.Tensor) -> torch.Tensor:
        """ë³€í˜•ëœ ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ ìƒì„±"""
        # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬
        gray = warped_image.mean(dim=1, keepdim=True)
        mask = (gray > 0.1).float()
        
        return mask
    
    def _enhance_high_quality(self, image: np.ndarray) -> np.ndarray:
        """ê³ í’ˆì§ˆ ì´ë¯¸ì§€ í–¥ìƒ"""
        try:
            # ì•½ê°„ì˜ ìƒ¤í”„ë‹ ì ìš©
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(image, -1, kernel)
            
            # ì›ë³¸ê³¼ ìƒ¤í”„ë‹ ê²°ê³¼ ë¸”ë Œë”©
            enhanced = cv2.addWeighted(image, 0.8, sharpened, 0.2, 0)
            
            return enhanced
        except:
            return image
    
    def _fix_low_quality(self, image: np.ndarray) -> np.ndarray:
        """ì €í’ˆì§ˆ ì´ë¯¸ì§€ ìˆ˜ì •"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            blurred = cv2.GaussianBlur(image, (3, 3), 0.5)
            
            return blurred
        except:
            return image
    
    def _update_stats(self, quality_score: float, processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.matching_stats['total_matches'] += 1
        
        if quality_score > self.matching_config['quality_threshold']:
            self.matching_stats['successful_matches'] += 1
        
        # í‰ê·  ì •í™•ë„ ì—…ë°ì´íŠ¸
        total = self.matching_stats['total_matches']
        current_avg = self.matching_stats['average_accuracy']
        self.matching_stats['average_accuracy'] = (
            (current_avg * (total - 1) + quality_score) / total
        )
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            gc.collect()
            self.logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(
    device: str = "mps", 
    config: Optional[Dict[str, Any]] = None
) -> GeometricMatchingStep:
    """ê¸°ì¡´ ë°©ì‹ 100% í˜¸í™˜ ìƒì„±ì"""
    return GeometricMatchingStep(device=device, config=config)

def create_m3_max_geometric_matching_step(
    device: Optional[str] = None,
    memory_gb: float = 128.0,
    optimization_level: str = "ultra",
    **kwargs
) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” ì „ìš© ìƒì„±ì"""
    return GeometricMatchingStep(
        device=device,
        memory_gb=memory_gb,
        quality_level=optimization_level,
        is_m3_max=True,
        optimization_enabled=True,
        **kwargs
    )

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'GeometricMatchingStep',
    'GeometricMatchingModel',
    'TPSTransformNetwork',
    'create_geometric_matching_step',
    'create_m3_max_geometric_matching_step'
]