# app/ai_pipeline/steps/step_04_geometric_matching.py
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ (ì™„ì „ í†µí•© ë²„ì „)
âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… GeometricMatchingMixin ì™„ì „ ìƒì†
âœ… ModelLoader ì™„ë²½ ì—°ë™  
âœ… BaseStepMixin ì™„ì „ ìƒì†
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©
âœ… PyTorch 2.1 ì™„ì „ í˜¸í™˜
âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„
"""

import os
import gc
import cv2
import time
import torch
import logging
import asyncio
import traceback
import numpy as np
import base64
import json
import math
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import torch.nn as nn
import torch.nn.functional as F
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO

# ğŸ”¥ BaseStepMixin ë° GeometricMatchingMixin ì„í¬íŠ¸
try:
    from .base_step_mixin import BaseStepMixin, GeometricMatchingMixin
    MIXIN_AVAILABLE = True
except ImportError:
    # í´ë°±: ê¸°ë³¸ Mixin í´ë˜ìŠ¤ ì •ì˜
    
    def _setup_model_precision:
    
        """M3 Max í˜¸í™˜ ì •ë°€ë„ ì„¤ì •"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float()
            elif self.device == "cuda" and hasattr(model, 'half'):
                return model.half()
            else:
                return model.float()
        except:
            self.logger.warning(f"âš ï¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model.float()

class BaseStepMixin:

    def __init__:

        if:

            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
    
class GeometricMatchingMixin:
    
    def __init__:
    
    super().__init__(*args, **kwargs)
    self.step_number = 4
    self.step_type = "geometric_matching"
    self.num_control_points = 25
    self.output_format = "transformation_matrix"
    
MIXIN_AVAILABLE = False

# ModelLoader ì—°ë™
try:
    from ..utils.model_loader import ModelLoader, get_global_model_loader
    from ..utils.step_model_requests import get_step_request, StepModelRequestAnalyzer
    MODEL_LOADER_AVAILABLE = True
except:
    MODEL_LOADER_AVAILABLE = False

# ì„¤ì • ë° ì½”ì–´ ëª¨ë“ˆ
try:
    from ...core.config import MODEL_CONFIG
    from ...core.gpu_config import GPUConfig
    from ...core.m3_optimizer import M3MaxOptimizer
    CORE_AVAILABLE = True
except:
    CORE_AVAILABLE = False

# scipy ì¶”ê°€ ì§€ì›
try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata
    SCIPY_AVAILABLE = True
except:
    SCIPY_AVAILABLE = False

# ==============================================
# ğŸ”¥ PyTorch 2.1 í˜¸í™˜ì„± ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
# ==============================================

def safe_mps_memory_cleanup:

    """PyTorch 2.1 í˜¸í™˜ ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬"""
    result = {
        "success": False,
        "method": "none",
        "device": device,
        "pytorch_version": torch.__version__
    }
    
    try:
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        if device == "mps" and torch.backends.mps.is_available():
            # PyTorch 2.1 ë²„ì „ë³„ MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                # Method 1: torch.mps.empty_cache() (PyTorch 2.1+)
                if:
                    torch.mps.empty_cache()
                    result.update({
                        "success": True,
                        "method": "torch.mps.empty_cache"
                    })
                
                # Method 2: torch.mps.synchronize() (fallback)
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result.update({
                        "success": True,
                        "method": "torch.mps.synchronize"
                    })
                
                # Method 3: Manual cleanup for older versions
                else:
                    result.update({
                        "success": True,
                        "method": "manual_gc_cleanup"
                    })
                    
            except:
                    
                result.update({
                    "success": True,  # GCëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ True
                    "method": "gc_fallback",
                    "warning": str(e)
                })
        
        elif device == "cuda" and torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                result.update({
                    "success": True,
                    "method": "torch.cuda.empty_cache"
                })
            except:
                result.update({
                    "success": True,
                    "method": "gc_fallback",
                    "warning": str(e)
                })
        
        else:
        
            result.update({
                "success": True,
                "method": "gc_only"
            })
        
        return result
        
    except:
        
        return {
            "success": False,
            "method": "error",
            "device": device,
            "error": str(e)
        }

# ==============================================
# ğŸ§  AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class GeometricMatchingModel:

    """ê¸°í•˜í•™ì  ë§¤ì¹­ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__:
    
        super().__init__()
        self.feature_dim = feature_dim
        self.num_keypoints = num_keypoints
        
        # íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸
        self.backbone = self._build_backbone()
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ
        self.keypoint_head = self._build_keypoint_head()
        
        # íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ
        self.matching_head = self._build_matching_head()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ
        self.tps_head = self._build_tps_head()
        
    def _build_backbone:
        
        """íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # Stage 2
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            
            # Stage 3
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            # Feature refinement
            nn.Conv2d(512, self.feature_dim, 3, 1, 1),
            nn.BatchNorm2d(self.feature_dim),
            nn.ReLU(inplace=True)
        )
    
    def _make_layer:
    
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
        layers = []
        layers.append(nn.Conv2d(in_planes, planes, 3, stride, 1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, 3, 1, 1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_keypoint_head:
    
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, self.num_keypoints, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_matching_head:
    
        """íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim * 2, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 1, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_tps_head:
    
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, self.num_keypoints * 2)  # (x, y) coordinates
        )
    
    def forward:
    
        """ìˆœì „íŒŒ"""
        # íŠ¹ì§• ì¶”ì¶œ
        source_features = self.backbone(source_img)
        target_features = self.backbone(target_img)
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        source_keypoints = self.keypoint_head(source_features)
        target_keypoints = self.keypoint_head(target_features)
        
        # íŠ¹ì§• ë§¤ì¹­
        concat_features = torch.cat([source_features, target_features], dim=1)
        matching_confidence = self.matching_head(concat_features)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.tps_head(source_features)
        tps_params = tps_params.view(-1, self.num_keypoints, 2)
        
        return {
            'source_keypoints': source_keypoints,
            'target_keypoints': target_keypoints,
            'matching_confidence': matching_confidence,
            'tps_params': tps_params,
            'source_features': source_features,
            'target_features': target_features
        }

class TPSTransformNetwork:

    """Thin Plate Spline ë³€í˜• ë„¤íŠ¸ì›Œí¬"""
    
    def __init__:
    
        super().__init__()
        self.grid_size = grid_size
        
    def create_grid:
        
        """ì •ê·œí™”ëœ ê·¸ë¦¬ë“œ ìƒì„±"""
        x = torch.linspace(-1, 1, width, device=device)
        y = torch.linspace(-1, 1, height, device=device)
        grid_x, grid_y = torch.meshgrid(x, y, indexing='xy')
        grid = torch.stack([grid_x, grid_y], dim=-1)
        return grid.unsqueeze(0)  # [1, H, W, 2]
    
    def compute_tps_weights:
    
        """TPS ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        device = source_points.device
        
        # ì œì–´ì  ê°„ ê±°ë¦¬ ê³„ì‚°
        source_points_expanded = source_points.unsqueeze(2)  # [B, N, 1, 2]
        target_points_expanded = target_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(source_points_expanded - target_points_expanded, dim=-1)  # [B, N, N]
        
        # RBF ì»¤ë„ ê³„ì‚° (r^2 * log(r))
        distances = distances + 1e-8  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±
        rbf_weights = distances ** 2 * torch.log(distances)
        
        # íŠ¹ì´ì  ì²˜ë¦¬
        rbf_weights = torch.where(distances < 1e-6, torch.zeros_like(rbf_weights), rbf_weights)
        
        return rbf_weights
    
    def apply_tps_transform:
    
        """TPS ë³€í˜• ì ìš©"""
        batch_size, channels, height, width = image.shape
        device = image.device
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid = self.create_grid(height, width, device)
        grid = grid.repeat(batch_size, 1, 1, 1)  # [B, H, W, 2]
        
        # TPS ê°€ì¤‘ì¹˜ ê³„ì‚°
        tps_weights = self.compute_tps_weights(source_points, target_points)
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°
        transformed_grid = self.compute_transformed_grid(
            grid, source_points, target_points, tps_weights
        )
        
        # ì´ë¯¸ì§€ ë¦¬ìƒ˜í”Œë§
        transformed_image = F.grid_sample(
            image, transformed_grid, 
            mode='bilinear', 
            padding_mode='border', 
            align_corners=True
        )
        
        return transformed_image, transformed_grid
    
    def compute_transformed_grid:
    
        """ë³€í˜•ëœ ê·¸ë¦¬ë“œ ê³„ì‚°"""
        batch_size, height, width, _ = grid.shape
        num_points = source_points.shape[1]
        device = grid.device
        
        # ê·¸ë¦¬ë“œë¥¼ í‰ë©´ìœ¼ë¡œ ë³€í™˜
        grid_flat = grid.view(batch_size, -1, 2)  # [B, H*W, 2]
        
        # ê° ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ì™€ ì œì–´ì  ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        grid_expanded = grid_flat.unsqueeze(2)  # [B, H*W, 1, 2]
        source_expanded = source_points.unsqueeze(1)  # [B, 1, N, 2]
        
        distances = torch.norm(grid_expanded - source_expanded, dim=-1)  # [B, H*W, N]
        distances = distances + 1e-8
        
        # RBF ê°’ ê³„ì‚°
        rbf_values = distances ** 2 * torch.log(distances)
        rbf_values = torch.where(distances < 1e-6, torch.zeros_like(rbf_values), rbf_values)
        
        # ë³€ìœ„ ê³„ì‚°
        displacement = target_points - source_points  # [B, N, 2]
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³€í˜• ê³„ì‚°
        weights = torch.softmax(-distances / 0.1, dim=-1)  # [B, H*W, N]
        interpolated_displacement = torch.sum(
            weights.unsqueeze(-1) * displacement.unsqueeze(1), dim=2
        )  # [B, H*W, 2]
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ
        transformed_grid_flat = grid_flat + interpolated_displacement
        transformed_grid = transformed_grid_flat.view(batch_size, height, width, 2)
        
        return transformed_grid

# ==============================================
# ğŸ¯ ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤
# ==============================================

class GeometricMatchingStep:

    """
    ğŸ”¥ Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ - ì™„ì „ í†µí•© ë²„ì „
    âœ… GeometricMatchingMixin ì™„ì „ ìƒì† (logger ì†ì„± ë³´ì¥)
    âœ… ModelLoader ì™„ë²½ ì—°ë™
    âœ… M3 Max 128GB ìµœì í™”
    âœ… ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©
    âœ… PyTorch 2.1 ì™„ì „ í˜¸í™˜
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        device_type: Optional[str] = None,
        memory_gb: Optional[float] = None,
        is_m3_max: Optional[bool] = None,
        optimization_enabled: Optional[bool] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ):
        """ì™„ì „ í˜¸í™˜ ìƒì„±ì - ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì›"""
        
        # ğŸ”¥ GeometricMatchingMixin ì´ˆê¸°í™” (logger ì†ì„± ìë™ ë³´ì¥)
        super().__init__(**kwargs)
        
        # ğŸ”¥ Logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì¶”ê°€ ë³´ì¥
        if:
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        self.logger.info("ğŸ”¥ GeometricMatchingStep ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # ê¸°ë³¸ ì†ì„± ì„¤ì •
            self.device = self._auto_detect_device(device)
            self.config = config or {}
            self.step_name = self.__class__.__name__
            self.is_initialized = False
            self.models_loaded = False
            self.initialization_error = None
            
            # M3 Max ìµœì í™” ì„¤ì •
            self.device_type = device_type or "auto"
            self.memory_gb = memory_gb or 128.0
            self.is_m3_max = is_m3_max if is_m3_max is not None else self._detect_m3_max()
            self.optimization_enabled = optimization_enabled if optimization_enabled is not None else True
            self.quality_level = quality_level or "high"
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
            self.model_interface = None
            self.model_loader = None
            
            # AI ëª¨ë¸ë“¤
            self.geometric_model = None
            self.tps_network = None
            self.feature_extractor = None
            
            # ì„¤ì • ì´ˆê¸°í™”
            self._setup_configs()
            
            # í†µê³„ ë° ì„±ëŠ¥
            self._setup_stats()
            
            # ìŠ¤ë ˆë“œ í’€
            self.executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="geometric_matching")
            
            self.logger.info("âœ… GeometricMatchingStep ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ GeometricMatchingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _auto_detect_device:
    
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if:
            return device
            
        if:
            
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max:
    
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            if:
                return True
        except:
            pass
        return False
    
    def _setup_configs:
    
        """ì„¤ì • ì´ˆê¸°í™”"""
        # ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •
        base_config = {
            'method': 'neural_tps',
            'num_keypoints': 25,
            'feature_dim': 256,
            'grid_size': 30 if self.is_m3_max else 20,
            'max_iterations': 1000,
            'convergence_threshold': 1e-6,
            'outlier_threshold': 0.15,
            'use_pose_guidance': True,
            'adaptive_weights': True,
            'quality_threshold': 0.7
        }
        
        # quality_levelì— ë”°ë¥¸ ì¡°ì •
        if:
            base_config.update({
                'num_keypoints': 30,
                'max_iterations': 1500,
                'quality_threshold': 0.8,
                'convergence_threshold': 1e-7
            })
        elif self.quality_level == 'ultra':
            base_config.update({
                'num_keypoints': 35,
                'max_iterations': 2000,
                'quality_threshold': 0.9,
                'convergence_threshold': 1e-8
            })
        elif self.quality_level == 'fast':
            base_config.update({
                'num_keypoints': 20,
                'max_iterations': 500,
                'quality_threshold': 0.6,
                'convergence_threshold': 1e-5
            })
        
        self.matching_config = self.config.get('matching', base_config)
        
        # TPS ì„¤ì •
        self.tps_config = self.config.get('tps', {
            'regularization': 0.1,
            'grid_size': self.matching_config['grid_size'],
            'boundary_padding': 0.1,
            'smoothing_factor': 0.8
        })
        
        # ğŸ†• ì‹œê°í™” ì„¤ì •
        self.visualization_config = self.config.get('visualization', {
            'enable_visualization': True,
            'show_keypoints': True,
            'show_matching_lines': True,
            'show_transformation_grid': True,
            'keypoint_size': 3,
            'line_thickness': 2,
            'grid_density': 20,
            'quality': 'high'  # low, medium, high
        })
        
        # M3 Max ìµœì í™” ì ìš©
        if:
            self._apply_m3_max_optimization()
    
    def _apply_m3_max_optimization:
    
        """M3 Max ìµœì í™” ì ìš©"""
        try:
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            
            # ìŠ¤ë ˆë“œ ìµœì í™”
            torch.set_num_threads(16)  # M3 Max 16ì½”ì–´
            
            # ë°°ì¹˜ í¬ê¸° ìµœì í™”
            if:
                self.matching_config['batch_size'] = 8
            
            self.logger.info("ğŸ M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
            
        except:
            
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _setup_stats:
    
        """í†µê³„ ì´ˆê¸°í™”"""
        self.matching_stats = {
            'total_matches': 0,
            'successful_matches': 0,
            'average_accuracy': 0.0,
            'total_processing_time': 0.0,
            'memory_usage': {},
            'error_count': 0,
            'last_error': None
        }
    
    async def initialize(self) -> bool:
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        if:
            return True
        
        try:
        
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ModelLoader ì—°ë™
            await self._setup_model_interface()
            
            # AI ëª¨ë¸ ë¡œë“œ
            await self._load_models()
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            await self._setup_device()
            
            self.is_initialized = True
            self.logger.info("âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except:
            
            self.initialization_error = str(e)
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.matching_stats['error_count'] += 1
            self.matching_stats['last_error'] = str(e)
            return False
    
    async def _setup_model_interface(self):
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if MODEL_LOADER_AVAILABLE:
                # ì „ì—­ ModelLoader ì‚¬ìš©
                self.model_loader = get_global_model_loader()
                
                # Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                self.model_interface = self.model_loader.create_step_interface(self.step_name)
                
                self.logger.info("ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€ - Mock ëª¨ë“œë¡œ ì „í™˜")
                
        except:
                
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def _load_models(self):
        """AI ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_interface:
                # Step ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                step_request = StepModelRequestAnalyzer.get_step_request_info(self.step_name)
                
                if step_request:
                    # ê¶Œì¥ ëª¨ë¸ ë¡œë“œ
                    self.geometric_model = await self.model_interface.get_model(
                        step_request['model_name']
                    )
                    
                    # TPS ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
                    self.tps_network = await self.model_interface.get_model('tps_network')
                    
                    self.logger.info("ğŸ§  AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    self.logger.warning("âš ï¸ Step ìš”ì²­ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ ìƒì„±")
                    await self._create_default_models()
            else:
                # Mock ëª¨ë¸ ìƒì„±
                await self._create_mock_models()
                
            self.models_loaded = True
            
        except:
            
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            await self._create_mock_models()
    
    async def _create_default_models(self):
        """ê¸°ë³¸ ëª¨ë¸ ìƒì„±"""
        try:
            # ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ìƒì„±
            self.geometric_model = GeometricMatchingModel(
                feature_dim=self.matching_config['feature_dim'],
                num_keypoints=self.matching_config['num_keypoints']
            ).to(self.device)
            
            # TPS ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.tps_network = TPSTransformNetwork(
                grid_size=self.tps_config['grid_size']
            ).to(self.device)
            
            # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.geometric_model.eval()
            self.tps_network.eval()
            
            self.logger.info("ğŸ”§ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            await self._create_mock_models()
    
    async def _create_mock_models(self):
        """Mock ëª¨ë¸ ìƒì„± (í´ë°±)"""
        self.geometric_model = lambda x, y: {
            'source_keypoints': torch.randn(1, 25, 64, 64),
            'target_keypoints': torch.randn(1, 25, 64, 64),
            'matching_confidence': torch.randn(1, 1, 64, 64),
            'tps_params': torch.randn(1, 25, 2)
        }
        self.tps_network = lambda: None
        self.feature_extractor = lambda x: torch.randn(1, 256)
        
        self.logger.info("ğŸ­ Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    async def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        try:
            if:
                torch.mps.empty_cache()
                self.logger.info("ğŸ MPS ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            elif self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                self.logger.info("ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            else:
                self.logger.info("ğŸ’» CPU ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                
        except:
                
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        pose_keypoints: Optional[np.ndarray] = None,
        body_mask: Optional[np.ndarray] = None,
        clothing_mask: Optional[np.ndarray] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰ (ê¸°ì¡´ API í˜¸í™˜)"""
        
        start_time = time.time()
        
        try:
            # ì´ˆê¸°í™” í™•ì¸
            if:
                await self.initialize()
            
            self.logger.info("ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ ì‹œì‘...")
            
            # ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            processed_input = await self._preprocess_inputs(
                person_image, clothing_image, pose_keypoints, body_mask, clothing_mask
            )
            
            # AI ëª¨ë¸ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ë§¤ì¹­
            matching_result = await self._perform_neural_matching(
                processed_input['person_tensor'],
                processed_input['clothing_tensor']
            )
            
            # TPS ë³€í˜• ê³„ì‚°
            tps_result = await self._compute_tps_transformation(
                matching_result,
                processed_input
            )
            
            # ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
            warped_result = await self._apply_geometric_transform(
                processed_input['clothing_tensor'],
                tps_result['source_points'],
                tps_result['target_points']
            )
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = await self._evaluate_matching_quality(
                matching_result,
                tps_result,
                warped_result
            )
            
            # í›„ì²˜ë¦¬
            final_result = await self._postprocess_result(
                warped_result,
                quality_score,
                processed_input
            )
            
            # ğŸ†• ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_matching_visualization(
                processed_input,
                matching_result,
                tps_result,
                warped_result,
                quality_score
            )
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_cleanup = safe_mps_memory_cleanup(self.device)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_stats(quality_score, processing_time)
            
            self.logger.info(f"âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}s")
            
            # ğŸ†• API í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ê³¼ êµ¬ì¡° (ê¸°ì¡´ í•„ë“œ + ì‹œê°í™” í•„ë“œ)
            return {
                'success': True,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}',
                'confidence': quality_score,
                'processing_time': processing_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'details': {
                    # ğŸ†• í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                    'result_image': visualization_results['matching_visualization'],
                    'overlay_image': visualization_results['warped_overlay'],
                    
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'num_keypoints': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                    'matching_confidence': matching_result['matching_confidence'],
                    'transformation_quality': quality_score,
                    'grid_size': self.tps_config['grid_size'],
                    'method': self.matching_config['method'],
                    
                    # ìƒì„¸ ë§¤ì¹­ ì •ë³´
                    'matching_details': {
                        'source_keypoints_count': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                        'target_keypoints_count': len(matching_result['target_keypoints'][0]) if len(matching_result['target_keypoints']) > 0 else 0,
                        'successful_matches': int(quality_score * 100),
                        'transformation_type': 'TPS (Thin Plate Spline)',
                        'optimization_enabled': self.optimization_enabled
                    }
                },
                
                # ë ˆê±°ì‹œ í˜¸í™˜ì„± í•„ë“œë“¤ (ê¸°ì¡´ APIì™€ì˜ í˜¸í™˜ì„±)
                'warped_clothing': final_result['warped_clothing'],
                'warped_mask': final_result.get('warped_mask', np.zeros((384, 512), dtype=np.uint8)),
                'transformation_matrix': tps_result.get('transformation_matrix'),
                'source_keypoints': matching_result['source_keypoints'],
                'target_keypoints': matching_result['target_keypoints'],
                'matching_confidence': matching_result['matching_confidence'],
                'quality_score': quality_score,
                'metadata': {
                    'method': 'neural_tps',
                    'num_keypoints': self.matching_config['num_keypoints'],
                    'grid_size': self.tps_config['grid_size'],
                    'device': self.device,
                    'optimization_enabled': self.optimization_enabled,
                    'pytorch_version': torch.__version__,
                    'memory_management': memory_cleanup
                }
            }
            
        except:
            
            self.logger.error(f"âŒ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            self.matching_stats['error_count'] += 1
            self.matching_stats['last_error'] = str(e)
            
            return {
                'success': False,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}',
                'confidence': 0.0,
                'processing_time': time.time() - start_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'error': str(e),
                'details': {
                    'result_image': '',
                    'overlay_image': '',
                    'error_type': type(e).__name__,
                    'error_count': self.matching_stats['error_count'],
                    'traceback': traceback.format_exc()
                }
            }
    
    # ==============================================
    # ğŸ†• ì‹œê°í™” í•¨ìˆ˜ë“¤
    # ==============================================
    
    async def _create_matching_visualization(
        self,
        processed_input: Dict[str, Any],
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any],
        quality_score: float
    ) -> Dict[str, str]:
        """
        ğŸ†• ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±
        
        Returns:
            Dict[str, str]: base64 ì¸ì½”ë”©ëœ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
        """
        try:
            if:
                return {
                    'matching_visualization': '',
                    'warped_overlay': '',
                    'transformation_grid': ''
                }
            
            def _create_visualizations():
                # ì›ë³¸ ì´ë¯¸ì§€ë“¤ì„ PILë¡œ ë³€í™˜
                person_pil = self._tensor_to_pil(processed_input['person_tensor'])
                clothing_pil = self._tensor_to_pil(processed_input['clothing_tensor'])
                warped_clothing_pil = self._tensor_to_pil(warped_result['warped_image'])
                
                # 1. ğŸ¯ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”
                matching_viz = self._create_keypoint_matching_visualization(
                    person_pil, clothing_pil, matching_result
                )
                
                # 2. ğŸŒˆ ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´
                warped_overlay = self._create_warped_overlay(
                    person_pil, warped_clothing_pil, quality_score
                )
                
                # 3. ğŸ“ ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™” (ì„ íƒì‚¬í•­)
                transformation_grid = ''
                if:
                    grid_viz = self._create_transformation_grid_visualization(
                        clothing_pil, warped_result.get('warped_grid')
                    )
                    transformation_grid = self._pil_to_base64(grid_viz)
                
                return {
                    'matching_visualization': self._pil_to_base64(matching_viz),
                    'warped_overlay': self._pil_to_base64(warped_overlay),
                    'transformation_grid': transformation_grid
                }
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except:
            
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'matching_visualization': '',
                'warped_overlay': '',
                'transformation_grid': ''
            }
    
    def _tensor_to_pil:
    
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # [B, C, H, W] -> [C, H, W]
            if:
                tensor = tensor.squeeze(0)
            
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ì •ê·œí™” í•´ì œ (ImageNet ì •ê·œí™” ì—­ë³€í™˜)
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            tensor = tensor * std + mean
            
            # ê°’ ë²”ìœ„ í´ë¨í•‘
            tensor = torch.clamp(tensor, 0, 1)
            
            # [C, H, W] -> [H, W, C]
            tensor = tensor.permute(1, 2, 0)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            numpy_array = (tensor.numpy() * 255).astype(np.uint8)
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            return Image.fromarray(numpy_array)
            
        except:
            
            self.logger.warning(f"âš ï¸ í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', (512, 512), (128, 128, 128))
    
    def _create_keypoint_matching_visualization(
        self,
        person_pil: Image.Image,
        clothing_pil: Image.Image,
        matching_result: Dict[str, Any]
    ) -> Image.Image:
        """í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            target_size = (512, 512)
            person_resized = person_pil.resize(target_size, Image.Resampling.LANCZOS)
            clothing_resized = clothing_pil.resize(target_size, Image.Resampling.LANCZOS)
            
            # ë‚˜ë€íˆ ë°°ì¹˜í•  ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = target_size[0] * 2 + 50  # 50px ê°„ê²©
            canvas_height = target_size[1]
            canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))
            
            # ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(person_resized, (0, 0))
            canvas.paste(clothing_resized, (target_size[0] + 50, 0))
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(canvas)
            
            # í°íŠ¸ ì„¤ì •
            try:
                font = ImageFont.truetype("arial.ttf", 16)
                small_font = ImageFont.truetype("arial.ttf", 12)
            except:
                font = ImageFont.load_default()
                small_font = ImageFont.load_default()
            
            # í‚¤í¬ì¸íŠ¸ ì‹œê°í™”
            if:
                self._draw_keypoints_and_matches(
                    draw, matching_result, target_size, font
                )
            
            # ë§¤ì¹­ ì •ë³´ í…ìŠ¤íŠ¸
            self._draw_matching_info_text(
                draw, matching_result, canvas_width, canvas_height, font
            )
            
            return canvas
            
        except:
            
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€
            return Image.new('RGB', (1024, 512), (200, 200, 200))
    
    def _draw_keypoints_and_matches(
        self,
        draw: ImageDraw.ImageDraw,
        matching_result: Dict[str, Any],
        target_size: Tuple[int, int],
        font
    ):
        """í‚¤í¬ì¸íŠ¸ì™€ ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°"""
        try:
            source_keypoints = matching_result.get('source_keypoints')
            target_keypoints = matching_result.get('target_keypoints')
            confidence = matching_result.get('matching_confidence', 0.8)
            
            if:
            
                return
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            source_coords = self._extract_coordinates_from_heatmap(source_keypoints, target_size)
            target_coords = self._extract_coordinates_from_heatmap(target_keypoints, target_size)
            
            if:
            
                return
            
            # ì˜¤í”„ì…‹ (clothing ì´ë¯¸ì§€ëŠ” ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜)
            target_offset_x = target_size[0] + 50
            
            keypoint_size = self.visualization_config.get('keypoint_size', 3)
            line_thickness = self.visualization_config.get('line_thickness', 2)
            
            # í‚¤í¬ì¸íŠ¸ì™€ ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°
            num_points = min(len(source_coords), len(target_coords))
            for i in range(num_points):
                if:
                    break
                    
                # ì†ŒìŠ¤ í‚¤í¬ì¸íŠ¸ (person ì´ë¯¸ì§€)
                sx, sy = source_coords[i]
                draw.ellipse(
                    [sx-keypoint_size, sy-keypoint_size, sx+keypoint_size, sy+keypoint_size],
                    fill=(255, 0, 0), outline=(128, 0, 0)
                )
                
                # íƒ€ê²Ÿ í‚¤í¬ì¸íŠ¸ (clothing ì´ë¯¸ì§€)
                tx, ty = target_coords[i]
                tx += target_offset_x
                draw.ellipse(
                    [tx-keypoint_size, ty-keypoint_size, tx+keypoint_size, ty+keypoint_size],
                    fill=(0, 255, 0), outline=(0, 128, 0)
                )
                
                # ë§¤ì¹­ ë¼ì¸
                if:
                    conf_value = confidence if isinstance(confidence, float) else 0.8
                    line_color = (0, 0, 255) if conf_value > 0.7 else (255, 255, 0)
                    
                    draw.line(
                        [(sx, sy), (tx, ty)],
                        fill=line_color,
                        width=line_thickness
                    )
                
                # í‚¤í¬ì¸íŠ¸ ë²ˆí˜¸
                draw.text((sx+5, sy+5), str(i), fill=(255, 255, 255), font=font)
                draw.text((tx+5, ty+5), str(i), fill=(255, 255, 255), font=font)
                
        except:
                
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
    
    def _extract_coordinates_from_heatmap:
    
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ"""
        try:
            if torch.is_tensor(heatmap):
                # [B, N, H, W] í˜•íƒœì˜ íˆíŠ¸ë§µì—ì„œ ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                if:
                    batch_size, num_points, height, width = heatmap.shape
                    heatmap = heatmap[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©
                else:
                    num_points, height, width = heatmap.shape
                
                coords = []
                for i in range(num_points):
                    # ê° í‚¤í¬ì¸íŠ¸ë³„ íˆíŠ¸ë§µì—ì„œ ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                    point_heatmap = heatmap[i]
                    max_val, max_idx = torch.max(point_heatmap.view(-1), 0)
                    
                    if max_val > 0.1:  # ì„ê³„ê°’ë³´ë‹¤ í° ê²½ìš°ë§Œ
                        y = max_idx // width
                        x = max_idx % width
                        
                        # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
                        scaled_x = int(x * target_size[0] / width)
                        scaled_y = int(y * target_size[1] / height)
                        
                        coords.append((scaled_x, scaled_y))
                
                return coords
            else:
                # ì´ë¯¸ ì¢Œí‘œ í˜•íƒœì¸ ê²½ìš°
                return [(int(x), int(y)) for x, y in heatmap[:10]]  # ìµœëŒ€ 10ê°œë§Œ
                
        except:
                
            self.logger.warning(f"âš ï¸ ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _draw_matching_info_text(
        self,
        draw: ImageDraw.ImageDraw,
        matching_result: Dict[str, Any],
        canvas_width: int,
        canvas_height: int,
        font
    ):
        """ë§¤ì¹­ ì •ë³´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°"""
        try:
            # ì •ë³´ í…ìŠ¤íŠ¸
            confidence = matching_result.get('matching_confidence', 0.5)
            conf_text = f"ë§¤ì¹­ ì‹ ë¢°ë„: {confidence:.3f}"
            
            source_keypoints = matching_result.get('source_keypoints')
            num_keypoints = 0
            if:
                if torch.is_tensor(source_keypoints):
                    num_keypoints = source_keypoints.shape[1] if source_keypoints.dim() > 1 else 0
                else:
                    num_keypoints = len(source_keypoints[0]) if len(source_keypoints) > 0 else 0
            
            kpts_text = f"í‚¤í¬ì¸íŠ¸ ìˆ˜: {num_keypoints}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            text_bg_height = 60
            draw.rectangle(
                [(0, canvas_height - text_bg_height), (canvas_width, canvas_height)],
                fill=(0, 0, 0, 180)
            )
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            draw.text((10, canvas_height - 50), conf_text, fill=(255, 255, 255), font=font)
            draw.text((10, canvas_height - 30), kpts_text, fill=(255, 255, 255), font=font)
            
            # ìš°ì¸¡ì— ë²”ë¡€
            draw.text((canvas_width - 200, canvas_height - 50), "ğŸ”´ Person í‚¤í¬ì¸íŠ¸", fill=(255, 255, 255), font=font)
            draw.text((canvas_width - 200, canvas_height - 30), "ğŸŸ¢ Clothing í‚¤í¬ì¸íŠ¸", fill=(255, 255, 255), font=font)
            
        except:
            
            self.logger.warning(f"âš ï¸ ì •ë³´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
    
    def _create_warped_overlay(
        self,
        person_pil: Image.Image,
        warped_clothing_pil: Image.Image,
        quality_score: float
    ) -> Image.Image:
        """ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´ ìƒì„±"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            target_size = (512, 512)
            person_resized = person_pil.resize(target_size, Image.Resampling.LANCZOS)
            warped_resized = warped_clothing_pil.resize(target_size, Image.Resampling.LANCZOS)
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            alpha = 0.7 if quality_score > 0.8 else 0.5
            overlay = Image.blend(person_resized, warped_resized, alpha)
            
            # í’ˆì§ˆ ì •ë³´ ì˜¤ë²„ë ˆì´
            draw = ImageDraw.Draw(overlay)
            try:
                font = ImageFont.truetype("arial.ttf", 18)
            except:
                font = ImageFont.load_default()
            
            # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
            quality_text = f"ë§¤ì¹­ í’ˆì§ˆ: {quality_score:.1%}"
            quality_color = (0, 255, 0) if quality_score > 0.8 else (255, 255, 0) if quality_score > 0.6 else (255, 0, 0)
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            draw.rectangle([(10, 10), (250, 50)], fill=(0, 0, 0, 180))
            draw.text((20, 20), quality_text, fill=quality_color, font=font)
            
            return overlay
            
        except:
            
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _create_transformation_grid_visualization(
        self,
        clothing_pil: Image.Image,
        warped_grid: Optional[torch.Tensor]
    ) -> Image.Image:
        """ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™”"""
        try:
            if:
                return Image.new('RGB', (512, 512), (240, 240, 240))
            
            # ê·¸ë¦¬ë“œ ì •ë³´ ì¶”ì¶œ
            if:
                grid_np = warped_grid[0].cpu().numpy()  # [H, W, 2]
            else:
                grid_np = warped_grid
            
            # ì´ë¯¸ì§€ í¬ê¸°
            height, width = grid_np.shape[:2]
            grid_image = Image.new('RGB', (width, height), (255, 255, 255))
            draw = ImageDraw.Draw(grid_image)
            
            # ê·¸ë¦¬ë“œ ë°€ë„
            grid_density = self.visualization_config.get('grid_density', 20)
            step = max(1, height // grid_density)
            
            # ê·¸ë¦¬ë“œ ë¼ì¸ ê·¸ë¦¬ê¸°
            for y in range(0, height, step):
                for x in range(0, width, step):
                    if x < width-step and y < height-step:
                        # ì›ë˜ ì¢Œí‘œì—ì„œ ë³€í˜•ëœ ì¢Œí‘œë¡œì˜ ë²¡í„°
                        dx = grid_np[y, x, 0] * width * 0.1  # ìŠ¤ì¼€ì¼ ì¡°ì •
                        dy = grid_np[y, x, 1] * height * 0.1
                        
                        # í™”ì‚´í‘œ ê·¸ë¦¬ê¸°
                        end_x = x + dx
                        end_y = y + dy
                        
                        draw.line([(x, y), (end_x, end_y)], fill=(0, 0, 255), width=1)
                        draw.ellipse([x-1, y-1, x+1, y+1], fill=(255, 0, 0))
            
            return grid_image
            
        except:
            
            self.logger.warning(f"âš ï¸ ê·¸ë¦¬ë“œ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), (240, 240, 240))
    
    def _pil_to_base64:
    
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if:
                quality = 95
            elif self.visualization_config.get('quality') == 'low':
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except:
            
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”§ ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ì…ë ¥ ì²˜ë¦¬ ë“±)
    # ==============================================
    
    async def _preprocess_inputs(
        self, 
        person_image, 
        clothing_image, 
        pose_keypoints, 
        body_mask, 
        clothing_mask
    ) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            clothing_tensor = self._image_to_tensor(clothing_image)
            
            # ì •ê·œí™”
            person_tensor = self._normalize_tensor(person_tensor)
            clothing_tensor = self._normalize_tensor(clothing_tensor)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            # ë§ˆìŠ¤í¬ ì²˜ë¦¬
            if:
                body_mask = self._mask_to_tensor(body_mask).to(self.device)
            
            if:
            
                clothing_mask = self._mask_to_tensor(clothing_mask).to(self.device)
            
            # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
            if:
                pose_keypoints = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            return {
                'person_tensor': person_tensor,
                'clothing_tensor': clothing_tensor,
                'pose_keypoints': pose_keypoints,
                'body_mask': body_mask,
                'clothing_mask': clothing_mask
            }
            
        except:
            
            self.logger.error(f"ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _image_to_tensor:
    
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if:
            return image
        elif isinstance(image, np.ndarray):
            if:
                image = image.transpose(2, 0, 1)  # HWC -> CHW
            tensor = torch.from_numpy(image).float() / 255.0
        elif isinstance(image, Image.Image):
            image = np.array(image)
            if:
                image = image.transpose(2, 0, 1)
            tensor = torch.from_numpy(image).float() / 255.0
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _mask_to_tensor:
    
        """ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if:
            return mask
        elif isinstance(mask, np.ndarray):
            tensor = torch.from_numpy(mask).float()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§ˆìŠ¤í¬ íƒ€ì…: {type(mask)}")
        
        # ë°°ì¹˜ ë° ì±„ë„ ì°¨ì› ì¶”ê°€
        if:
            tensor = tensor.unsqueeze(0).unsqueeze(0)
        elif len(tensor.shape) == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def _normalize_tensor:
    
        """í…ì„œ ì •ê·œí™” (ImageNet í‘œì¤€)"""
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(tensor.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(tensor.device)
        
        return (tensor - mean) / std
    
    async def _perform_neural_matching(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """ì‹ ê²½ë§ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        try:
            with torch.no_grad():
                # AI ëª¨ë¸ ì¶”ë¡ 
                model_output = self.geometric_model(person_tensor, clothing_tensor)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                source_keypoints = self._extract_keypoints(
                    model_output['source_keypoints']
                )
                target_keypoints = self._extract_keypoints(
                    model_output['target_keypoints']
                )
                
                # ë§¤ì¹­ ì‹ ë¢°ë„
                matching_confidence = model_output['matching_confidence'].mean().item()
                
                return {
                    'source_keypoints': source_keypoints,
                    'target_keypoints': target_keypoints,
                    'matching_confidence': matching_confidence,
                    'tps_params': model_output['tps_params'],
                    'source_features': model_output['source_features'],
                    'target_features': model_output['target_features']
                }
                
        except:
                
            self.logger.error(f"ì‹ ê²½ë§ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_keypoints:
    
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        batch_size, num_points, height, width = heatmap.shape
        
        # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
        heatmap_flat = heatmap.view(batch_size, num_points, -1)
        max_indices = torch.argmax(heatmap_flat, dim=2)
        
        # ì¢Œí‘œ ë³€í™˜
        y_coords = (max_indices // width).float()
        x_coords = (max_indices % width).float()
        
        # ì •ê·œí™” (-1 ~ 1)
        x_coords = (x_coords / (width - 1)) * 2 - 1
        y_coords = (y_coords / (height - 1)) * 2 - 1
        
        keypoints = torch.stack([x_coords, y_coords], dim=-1)
        
        return keypoints
    
    async def _compute_tps_transformation(
        self, 
        matching_result: Dict[str, Any],
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            source_points = matching_result['source_keypoints']
            target_points = matching_result['target_keypoints']
            
            # ë³€í˜• í–‰ë ¬ ê³„ì‚°
            transformation_matrix = self._compute_transformation_matrix(
                source_points, target_points
            )
            
            return {
                'source_points': source_points,
                'target_points': target_points,
                'transformation_matrix': transformation_matrix
            }
            
        except:
            
            self.logger.error(f"TPS ë³€í˜• ê³„ì‚° ì‹¤íŒ¨: {e}")
            raise
    
    def _compute_transformation_matrix(
        self, 
        source_points: torch.Tensor, 
        target_points: torch.Tensor
    ) -> torch.Tensor:
        """ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size, num_points, _ = source_points.shape
        
        # ë‹¨ìˆœí™”ëœ ì–´íŒŒì¸ ë³€í˜• ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ TPS ê³„ì‚°ì´ í•„ìš”
        transformation_matrix = torch.eye(3, device=source_points.device).unsqueeze(0).repeat(batch_size, 1, 1)
        
        return transformation_matrix
    
    async def _apply_geometric_transform(
        self,
        clothing_tensor: torch.Tensor,
        source_points: torch.Tensor,
        target_points: torch.Tensor
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë³€í˜• ì ìš©"""
        try:
            # TPS ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ë³€í˜•
            warped_image, warped_grid = self.tps_network.apply_tps_transform(
                clothing_tensor, source_points, target_points
            )
            
            return {
                'warped_image': warped_image,
                'warped_grid': warped_grid
            }
            
        except:
            
            self.logger.error(f"ê¸°í•˜í•™ì  ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            raise
    
    async def _evaluate_matching_quality(
        self,
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any]
    ) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            # ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•œ í’ˆì§ˆ ì ìˆ˜
            confidence_score = matching_result['matching_confidence']
            
            # í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ì ìˆ˜
            consistency_score = self._compute_keypoint_consistency(
                matching_result['source_keypoints'],
                matching_result['target_keypoints']
            )
            
            # ë³€í˜• í’ˆì§ˆ ì ìˆ˜
            warp_quality = self._compute_warp_quality(warped_result['warped_image'])
            
            # ì¢…í•© ì ìˆ˜
            quality_score = (
                0.4 * confidence_score +
                0.3 * consistency_score +
                0.3 * warp_quality
            )
            
            return float(quality_score)
            
        except:
            
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _compute_keypoint_consistency(
        self,
        source_keypoints: torch.Tensor,
        target_keypoints: torch.Tensor
    ) -> float:
        """í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ë¶„ì‚°ìœ¼ë¡œ ì¼ê´€ì„± ì¸¡ì •
            distances = torch.norm(source_keypoints - target_keypoints, dim=-1)
            consistency = 1.0 / (1.0 + distances.std().item())
            
            return min(1.0, max(0.0, consistency))
            
        except:
            
            return 0.5
    
    def _compute_warp_quality:
    
        """ë³€í˜• í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ì´ë¯¸ì§€ ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •
            grad_x = torch.abs(warped_image[:, :, :, 1:] - warped_image[:, :, :, :-1])
            grad_y = torch.abs(warped_image[:, :, 1:, :] - warped_image[:, :, :-1, :])
            
            gradient_magnitude = torch.sqrt(grad_x.mean() ** 2 + grad_y.mean() ** 2)
            
            # ì ì ˆí•œ ê·¸ë¼ë””ì–¸íŠ¸ í¬ê¸°ëŠ” ì¢‹ì€ í’ˆì§ˆì„ ì˜ë¯¸
            quality = torch.exp(-gradient_magnitude * 5).item()
            
            return min(1.0, max(0.0, quality))
            
        except:
            
            return 0.5
    
    async def _postprocess_result(
        self,
        warped_result: Dict[str, Any],
        quality_score: float,
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_clothing = warped_result['warped_image']
            
            # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            warped_clothing_np = self._tensor_to_numpy(warped_clothing)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            warped_mask = self._generate_warped_mask(warped_clothing)
            warped_mask_np = self._tensor_to_numpy(warped_mask)
            
            # í’ˆì§ˆ ê¸°ë°˜ í›„ì²˜ë¦¬
            if:
                warped_clothing_np = self._enhance_high_quality(warped_clothing_np)
            elif quality_score < 0.5:
                warped_clothing_np = self._fix_low_quality(warped_clothing_np)
            
            return {
                'warped_clothing': warped_clothing_np,
                'warped_mask': warped_mask_np
            }
            
        except:
            
            self.logger.error(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_numpy:
    
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        if:
            tensor = tensor.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
        
        if:
        
            tensor = tensor.permute(1, 2, 0)  # CHW -> HWC
        
        # ì •ê·œí™” í•´ì œ
        tensor = tensor * 255.0
        tensor = torch.clamp(tensor, 0, 255)
        
        return tensor.detach().cpu().numpy().astype(np.uint8)
    
    def _generate_warped_mask:
    
        """ë³€í˜•ëœ ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ ìƒì„±"""
        # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬
        gray = warped_image.mean(dim=1, keepdim=True)
        mask = (gray > 0.1).float()
        
        return mask
    
    def _enhance_high_quality:
    
        """ê³ í’ˆì§ˆ ì´ë¯¸ì§€ í–¥ìƒ"""
        try:
            # ì•½ê°„ì˜ ìƒ¤í”„ë‹ ì ìš©
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(image, -1, kernel)
            
            # ì›ë³¸ê³¼ ìƒ¤í”„ë‹ ê²°ê³¼ ë¸”ë Œë”©
            enhanced = cv2.addWeighted(image, 0.8, sharpened, 0.2, 0)
            
            return enhanced
        except:
            return image
    
    def _fix_low_quality:
    
        """ì €í’ˆì§ˆ ì´ë¯¸ì§€ ìˆ˜ì •"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            blurred = cv2.GaussianBlur(image, (3, 3), 0.5)
            
            return blurred
        except:
            return image
    
    def _update_stats:
    
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.matching_stats['total_matches'] += 1
        self.matching_stats['total_processing_time'] += processing_time
        
        if:
        
            self.matching_stats['successful_matches'] += 1
        
        # í‰ê·  ì •í™•ë„ ì—…ë°ì´íŠ¸
        total = self.matching_stats['total_matches']
        current_avg = self.matching_stats['average_accuracy']
        self.matching_stats['average_accuracy'] = (
            (current_avg * (total - 1) + quality_score) / total
        )
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 4ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "device": self.device,
                "initialized": self.is_initialized,
                "models_loaded": self.models_loaded,
                "model_interface_available": self.model_interface is not None,
                "models_loaded": {
                    "geometric_model": self.geometric_model is not None,
                    "tps_network": self.tps_network is not None,
                    "feature_extractor": self.feature_extractor is not None
                },
                "config": {
                    "method": self.matching_config['method'],
                    "num_keypoints": self.matching_config['num_keypoints'],
                    "grid_size": self.tps_config['grid_size'],
                    "quality_level": self.quality_level,
                    "quality_threshold": self.matching_config['quality_threshold'],
                    "visualization_enabled": self.visualization_config.get('enable_visualization', True)
                },
                "performance": self.matching_stats,
                "optimization": {
                    "m3_max_enabled": self.is_m3_max,
                    "optimization_enabled": self.optimization_enabled,
                    "memory_gb": self.memory_gb,
                    "device_type": self.device_type,
                    "pytorch_version": torch.__version__
                },
                "visualization": {
                    "show_keypoints": self.visualization_config.get('show_keypoints', True),
                    "show_matching_lines": self.visualization_config.get('show_matching_lines', True),
                    "show_transformation_grid": self.visualization_config.get('show_transformation_grid', True),
                    "quality": self.visualization_config.get('quality', 'high')
                }
            }
        except:
            self.logger.error(f"ë‹¨ê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "error": str(e),
                "pytorch_version": torch.__version__
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - PyTorch 2.1 í˜¸í™˜"""
        try:
            self.logger.info("ğŸ§¹ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë¸ ì •ë¦¬
            if:
                if hasattr(self.geometric_model, 'cpu'):
                    self.geometric_model.cpu()
                del self.geometric_model
                self.geometric_model = None
            
            if:
            
                if hasattr(self.tps_network, 'cpu'):
                    self.tps_network.cpu()
                del self.tps_network
                self.tps_network = None
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if:
                self.executor.shutdown(wait=True)
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if:
                self.model_interface.unload_models()
            
            # ğŸ”¥ PyTorch 2.1 í˜¸í™˜ ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_result = safe_mps_memory_cleanup(self.device)
            
            gc.collect()
            
            self.logger.info(f"âœ… 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ - ë©”ëª¨ë¦¬ ì •ë¦¬: {memory_result['method']}")
            
        except:
            
            self.logger.warning(f"âš ï¸ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__:
    
        """ì†Œë©¸ì"""
        try:
            if:
                asyncio.create_task(self.cleanup())
        except:
            pass

# ==============================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(
    device: str = "mps", 
    config: Optional[Dict[str, Any]] = None
) -> GeometricMatchingStep:
    """ê¸°ì¡´ ë°©ì‹ 100% í˜¸í™˜ ìƒì„±ì"""
    return GeometricMatchingStep(device=device, config=config)

def create_m3_max_geometric_matching_step(
    device: Optional[str] = None,
    memory_gb: float = 128.0,
    optimization_level: str = "ultra",
    **kwargs
) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” ì „ìš© ìƒì„±ì"""
    return GeometricMatchingStep(
        device=device,
        memory_gb=memory_gb,
        quality_level=optimization_level,
        is_m3_max=True,
        optimization_enabled=True,
        **kwargs
    )

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'GeometricMatchingStep',
    'GeometricMatchingModel',
    'TPSTransformNetwork',
    'create_geometric_matching_step',
    'create_m3_max_geometric_matching_step',
    'safe_mps_memory_cleanup'  # ğŸ†• PyTorch 2.1 í˜¸í™˜ ìœ í‹¸ë¦¬í‹°
]

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.info("âœ… GeometricMatchingStep v3.0 ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— GeometricMatchingMixin ì™„ì „ ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°")
logger.info("ğŸ”— ModelLoader ì™„ë²½ ì—°ë™")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ¨ ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©")
logger.info("ğŸ”¥ PyTorch 2.1 ì™„ì „ í˜¸í™˜")
logger.info("ğŸ¯ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„")