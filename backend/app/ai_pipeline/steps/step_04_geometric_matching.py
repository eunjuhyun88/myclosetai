#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ v8.0 - Common Imports Integration
=======================================================================

âœ… Common Imports ì‹œìŠ¤í…œ ì™„ì „ í†µí•© - ì¤‘ë³µ import ë¸”ë¡ ì œê±°
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† ë° super().__init__() í˜¸ì¶œ
âœ… í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”: ai_models, models_loading_status, model_interface, loaded_models
âœ… _load_segmentation_models_via_central_hub() ë©”ì„œë“œ - ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”©
âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Geometric Matching ë¡œì§ë§Œ
âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„±
âœ… ì‹¤ì œ GMM/TPS/SAM ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© (3.0GB)
âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ
âœ… ë³µì¡í•œ DI ì´ˆê¸°í™” ë¡œì§ ë‹¨ìˆœí™”
âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€ ì½”ë“œ ë¶ˆí•„ìš”
âœ… TYPE_CHECKING ë‹¨ìˆœí™”

Author: MyCloset AI Team
Date: 2025-07-31
Version: 8.1 (Common Imports Integration)
"""

# ğŸ”¥ ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, sys, gc, time, logging, asyncio, threading, traceback,
    hashlib, json, base64, math, warnings, np,
    Path, Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING,
    dataclass, field, Enum, IntEnum, BytesIO, ThreadPoolExecutor,
    lru_cache, wraps,
    
    # PyTorch ê´€ë ¨  â† ì´ ë¶€ë¶„ ì¶”ê°€!
    torch, nn, F, transforms,
    
    # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
    error_tracker, track_exception, get_error_summary, create_exception_response, convert_to_mycloset_exception,
    ErrorCodes, EXCEPTIONS_AVAILABLE,
    
    # Mock Data Diagnostic
    detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
    
    # Central Hub DI Container
    _get_central_hub_container, get_base_step_mixin_class
)

# ì¶”ê°€ imports
import weakref
from concurrent.futures import as_completed

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€
from app.ai_pipeline.utils.memory_monitor import log_step_memory, cleanup_step_memory

# ViT ê¸°ë°˜ GMM ëª¨ë¸ ì„í¬íŠ¸
try:
    from ..models.vit_based_gmm import VITBasedGeometricMatchingModule
except ImportError:
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„
        from app.ai_pipeline.models.vit_based_gmm import VITBasedGeometricMatchingModule
    except ImportError:
        # ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        VITBasedGeometricMatchingModule = None

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ìµœìƒë‹¨ì— ì¶”ê°€
logger = logging.getLogger(__name__)

# M3 Max ê°ì§€
def detect_m3_max():
    """M3 Max ê°ì§€"""
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

# ğŸ”¥ PyTorch ë¡œë”© ìµœì í™” - ìˆ˜ì •
try:
    from fix_pytorch_loading import apply_pytorch_patch
    apply_pytorch_patch()
except ImportError:
    logger.warning("âš ï¸ fix_pytorch_loading ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ PyTorch ë¡œë”© ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch ë¡œë”© íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
    from app.core.di_container import CentralHubDIContainer

BaseStepMixin = get_base_step_mixin_class()

# ==============================================
# ğŸ”¥ 2. ê³µí†µ ë¸”ë¡ í´ë˜ìŠ¤ë“¤ (ì¤‘ë³µ ì œê±°)
# ==============================================

class CommonBottleneckBlock(nn.Module):
    """ê³µí†µ BottleneckBlock - ëª¨ë“  ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¬ì‚¬ìš©"""
    
    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=dilation, 
                             dilation=dilation, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


def make_resnet_layer(block_class, inplanes, planes, blocks, stride=1, dilation=1, downsample=None):
    layers = []
    if block_class is CommonBottleneckBlock:
        layers.append(block_class(inplanes, planes, stride, dilation, downsample))
        inplanes = planes * 4
        for _ in range(1, blocks):
            layers.append(block_class(inplanes, planes, 1, dilation))
    else:
        layers.append(block_class(inplanes, planes, stride))
        inplanes = planes
        for _ in range(1, blocks):
            layers.append(block_class(inplanes, planes, 1))
    return nn.Sequential(*layers)

class CommonConvBlock(nn.Module):
    """ê³µí†µ Conv-BN-ReLU ë¸”ë¡"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))

class CommonInitialConv(nn.Module):
    """ê³µí†µ ì´ˆê¸° Conv ë¸”ë¡ (3->64)"""
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
    
    def forward(self, x):
        x = self.relu(self.bn(self.conv(x)))
        return self.maxpool(x)

class CommonFeatureExtractor(nn.Module):
    """ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë¸”ë¡ (128->64->output)"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = CommonConvBlock(in_channels, 128)
        self.conv2 = CommonConvBlock(128, 64)
        self.conv3 = nn.Conv2d(64, out_channels, 1)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return self.conv3(x)

class CommonAttentionBlock(nn.Module):
    """ê³µí†µ Self-Attention ë¸”ë¡"""
    def __init__(self, in_channels, reduction=8):
        super().__init__()
        self.query_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))
    
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        key = self.key_conv(x).view(batch_size, -1, H * W)
        value = self.value_conv(x).view(batch_size, -1, H * W)
        
        attention = torch.bmm(query, key)
        attention = F.softmax(attention, dim=-1)
        
        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        return self.gamma * out + x

class CommonGRUConvBlock(nn.Module):
    """GRUìš© Conv ë¸”ë¡ (activation ì—†ìŒ)"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
    
    def forward(self, x):
        return self.conv(x)

# ==============================================
# ğŸ”¥ 4. ë² ì´ìŠ¤ í´ë˜ìŠ¤ë“¤ (Forward ë©”ì„œë“œ í†µí•©)
# ==============================================

class BaseOpticalFlowModel(nn.Module):
    """Optical Flow ëª¨ë¸ë“¤ì˜ ê³µí†µ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        super().__init__()
    
    def forward(self, img1, img2):
        """ê³µí†µ forward ì¸í„°í˜ì´ìŠ¤"""
        # ì…ë ¥ ê²€ì¦
        self._validate_inputs(img1, img2)
        
        # ì‹¤ì œ flow ê³„ì‚° (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        result = self._compute_flow(img1, img2)
        
        # ê²°ê³¼ ê²€ì¦ ë° í¬ë§·íŒ…
        return self._format_result(result, img1.device)
    
    def _validate_inputs(self, img1, img2):
        """ì…ë ¥ ê²€ì¦"""
        if img1.dim() != 4 or img2.dim() != 4:
            raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
        if img1.shape != img2.shape:
            raise ValueError("ë‘ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤")
    
    def _compute_flow(self, img1, img2):
        """ì‹¤ì œ flow ê³„ì‚° (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    def _format_result(self, result, device):
        """ê²°ê³¼ í¬ë§·íŒ…"""
        if isinstance(result, dict):
            return result
        elif isinstance(result, torch.Tensor):
            return {
                'flow': result,
                'confidence': torch.tensor(0.75, device=device),
                'quality_score': torch.tensor(0.7, device=device)
            }
        else:
            raise ValueError("ê²°ê³¼ëŠ” dict ë˜ëŠ” torch.Tensorì—¬ì•¼ í•©ë‹ˆë‹¤")

class BaseGeometricMatcher(nn.Module):
    """Geometric Matching ëª¨ë¸ë“¤ì˜ ê³µí†µ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, input_nc=6, **kwargs):
        super().__init__()
        self.input_nc = input_nc
        self._init_common_components(**kwargs)
    
    def forward(self, person_image, clothing_image):
        """ê³µí†µ forward ì¸í„°í˜ì´ìŠ¤"""
        # ì…ë ¥ ê²€ì¦
        self._validate_inputs(person_image, clothing_image)
        
        # ì‹¤ì œ ë§¤ì¹­ ê³„ì‚° (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        result = self._compute_matching(person_image, clothing_image)
        
        # ê²°ê³¼ ê²€ì¦ ë° í¬ë§·íŒ…
        return self._format_result(result, person_image.device)
    
    def _validate_inputs(self, person_image, clothing_image):
        """ì…ë ¥ ê²€ì¦"""
        if person_image.dim() != 4 or clothing_image.dim() != 4:
            raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
    
    def _init_common_components(self, **kwargs):
        """ê³µí†µ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        pass
    
    def _compute_matching(self, person_image, clothing_image):
        """ì‹¤ì œ ë§¤ì¹­ ê³„ì‚° (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    def _format_result(self, result, device):
        """ê²°ê³¼ í¬ë§·íŒ…"""
        if isinstance(result, dict):
            return result
        else:
            raise ValueError("ê²°ê³¼ëŠ” dictì—¬ì•¼ í•©ë‹ˆë‹¤")

# ==============================================
# ğŸ”¥ 3. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •
# ==============================================

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° - GeometricMatchingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… - GeometricMatchingìš©"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

# ì „ì—­ í•¨ìˆ˜ _get_service_from_central_hubëŠ” ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°ë¨

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_mycloset_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# M3 Max ê°ì§€ ë° ìµœì í™”
def detect_m3_max() -> bool:
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# M3 Max ìµœì í™” ì„¤ì •
if IS_M3_MAX and CONDA_INFO['is_mycloset_env']:
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    os.environ['TORCH_MPS_PREFER_METAL'] = '1'

# PyTorch í•„ìˆ˜ (MPS ì§€ì›)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
    MPS_AVAILABLE = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    # M3 Max ìµœì í™”
    if CONDA_INFO['is_mycloset_env'] and IS_M3_MAX:
        cpu_count = os.cpu_count()
        torch.set_num_threads(max(1, cpu_count // 2))
        
except ImportError:
    raise ImportError("âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch")

# PIL í•„ìˆ˜
try:
    from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    raise ImportError("âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge")

# NumPy í•„ìˆ˜
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    raise ImportError("âŒ NumPy í•„ìˆ˜: conda install numpy -c conda-forge")

# OpenCV ì„ íƒì‚¬í•­
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logging.getLogger(__name__).info("OpenCV ì—†ìŒ - PIL ê¸°ë°˜ìœ¼ë¡œ ë™ì‘")

# SciPy ì„ íƒì‚¬í•­ (Procrustes ë¶„ì„ìš©)
try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata, RBFInterpolator
    import scipy.ndimage as ndimage
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ==============================================
# ğŸ”¥ 4. ìƒìˆ˜ ë° ë°ì´í„° í´ë˜ìŠ¤ë“¤
# ==============================================

@dataclass
class GeometricMatchingConfig:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •"""
    input_size: tuple = (256, 192)
    confidence_threshold: float = 0.7
    enable_visualization: bool = True
    device: str = "auto"
    matching_method: str = "advanced_deeplab_aspp_self_attention"

@dataclass
class ProcessingStatus:
    """ì²˜ë¦¬ ìƒíƒœ ì¶”ì  í´ë˜ìŠ¤"""
    models_loaded: bool = False
    advanced_ai_loaded: bool = False
    model_creation_success: bool = False
    requirements_compatible: bool = False
    initialization_complete: bool = False
    last_updated: float = field(default_factory=time.time)
    
    def update_status(self, **kwargs):
        """ìƒíƒœ ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
        self.last_updated = time.time()

# ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ íƒ€ì…
MATCHING_ALGORITHMS = {
    'gmm': 'Geometric Matching Module',
    'tps': 'Thin-Plate Spline Transformation',
    'procrustes': 'Procrustes Analysis',
    'optical_flow': 'Optical Flow Calculation',
    'keypoint': 'Keypoint-based Matching',
    'deeplab': 'DeepLabV3+ Backbone',
    'aspp': 'ASPP Multi-scale Context',
    'self_attention': 'Self-Attention Keypoint Matching',
    'edge_aware': 'Edge-Aware Transformation',
    'progressive': 'Progressive Geometric Refinement'
}

# ==============================================
# ğŸ”¥ 6. ê³ ê¸‰ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class DeepLabV3PlusBackbone(nn.Module):
    """DeepLabV3+ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ - ê¸°í•˜í•™ì  ë§¤ì¹­ íŠ¹í™”"""

    def __init__(self, input_nc=6, backbone='resnet101', output_stride=16):
        super().__init__()
        self.output_stride = output_stride
        self.input_nc = input_nc

        # ResNet-101 ë°±ë³¸ êµ¬ì„± (6ì±„ë„ ì…ë ¥ ì§€ì›)
        self.conv1 = nn.Conv2d(input_nc, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNet Layers with Dilated Convolution
        self.layer1 = make_resnet_layer(CommonBottleneckBlock, 64, 64, 3, stride=1)      # 256 channels
        self.layer2 = make_resnet_layer(CommonBottleneckBlock, 256, 128, 4, stride=2)    # 512 channels  
        self.layer3 = make_resnet_layer(CommonBottleneckBlock, 512, 256, 23, stride=2)   # 1024 channels
        self.layer4 = make_resnet_layer(CommonBottleneckBlock, 1024, 512, 3, stride=1, dilation=2)  # 2048 channels
        # Low-level feature extraction (for decoder)
        self.low_level_conv = nn.Conv2d(256, 48, 1, bias=False)
        self.low_level_bn = nn.BatchNorm2d(48)

    def forward(self, x):
        # Initial convolution
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        # ResNet layers
        x = self.layer1(x)
        low_level_feat = x  # Save for decoder

        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # Process low-level features
        low_level_feat = self.low_level_conv(low_level_feat)
        low_level_feat = self.low_level_bn(low_level_feat)
        low_level_feat = self.relu(low_level_feat)

        return x, low_level_feat

class ASPPModule(nn.Module):
    """ASPP ëª¨ë“ˆ - Multi-scale context aggregation"""

    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()

        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])

        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)  # 1x1 + atrous + global
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )

    def forward(self, x):
        h, w = x.shape[2:]

        # 1x1 convolution
        feat1 = self.conv1x1(x)

        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]

        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)

        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)

        # Project to final features
        return self.project(concat_feat)

class SelfAttentionKeypointMatcher(nn.Module):
    """Self-Attention ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ëª¨ë“ˆ"""

    def __init__(self, in_channels=256, num_keypoints=20):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.in_channels = in_channels

        # Query, Key, Value ë³€í™˜
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)

        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        self.keypoint_head = nn.Sequential(
        CommonConvBlock(in_channels, 128),
        CommonConvBlock(128, 64),
        nn.Conv2d(64, num_keypoints, 1),
        nn.Sigmoid()
        )
        # Attention ê°€ì¤‘ì¹˜
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, person_feat, clothing_feat):
        """Self-attentionì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        batch_size, C, H, W = person_feat.size()

        # Person featuresì—ì„œ query ìƒì„±
        proj_query = self.query_conv(person_feat).view(batch_size, -1, H * W).permute(0, 2, 1)
        
        # Clothing featuresì—ì„œ key, value ìƒì„±
        proj_key = self.key_conv(clothing_feat).view(batch_size, -1, H * W)
        proj_value = self.value_conv(clothing_feat).view(batch_size, -1, H * W)

        # Attention ê³„ì‚°
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)

        # Attentionì„ valueì— ì ìš©
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)

        # Residual connection
        attended_feat = self.gamma * out + person_feat

        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        keypoint_heatmaps = self.keypoint_head(attended_feat)

        return keypoint_heatmaps, attended_feat

class EdgeAwareTransformationModule(nn.Module):
    """Edge-Aware ë³€í˜• ëª¨ë“ˆ - ê²½ê³„ì„  ì •ë³´ í™œìš©"""

    def __init__(self, in_channels=256):
        super().__init__()

        # Edge feature extraction
        self.edge_conv1 = CommonConvBlock(in_channels, 128)
        self.edge_conv2 = CommonConvBlock(128, 64)

        # Learnable Sobel-like filters
        self.sobel_x = nn.Conv2d(64, 32, 3, padding=1, bias=False)
        self.sobel_y = nn.Conv2d(64, 32, 3, padding=1, bias=False)

        # Initialize edge kernels
        self._init_sobel_kernels()

        # Transformation prediction
        self.transform_head = nn.Sequential(
            CommonConvBlock(64 + 32 * 2, 128),
            CommonConvBlock(128, 64),
            nn.Conv2d(64, 2, 1)
        )

    def _init_sobel_kernels(self):
        """Sobel edge detection ì»¤ë„ ì´ˆê¸°í™”"""
        sobel_x_kernel = torch.tensor([
            [-1, 0, 1],
            [-2, 0, 2], 
            [-1, 0, 1]
        ], dtype=torch.float32).view(1, 1, 3, 3)

        sobel_y_kernel = torch.tensor([
            [-1, -2, -1],
            [0, 0, 0],
            [1, 2, 1]
        ], dtype=torch.float32).view(1, 1, 3, 3)

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •
        self.sobel_x.weight.data = sobel_x_kernel.repeat(32, 64, 1, 1)
        self.sobel_y.weight.data = sobel_y_kernel.repeat(32, 64, 1, 1)

    def forward(self, features):
        """Edge-aware transformation ì˜ˆì¸¡"""
        # Edge features ì¶”ì¶œ
        edge_feat = self.edge_conv1(features)
        edge_feat = self.edge_conv2(edge_feat)

        # Sobel í•„í„° ì ìš©
        edge_x = self.sobel_x(edge_feat)
        edge_y = self.sobel_y(edge_feat)

        # Feature ê²°í•©
        combined_feat = torch.cat([edge_feat, edge_x, edge_y], dim=1)

        # Transformation ì˜ˆì¸¡
        transformation = self.transform_head(combined_feat)

        return transformation

class ProgressiveGeometricRefinement(nn.Module):
    """Progressive ê¸°í•˜í•™ì  ì •ì œ ëª¨ë“ˆ - ë‹¨ê³„ë³„ ê°œì„ """

    def __init__(self, num_stages=3, in_channels=256):
        super().__init__()
        self.num_stages = num_stages

        # Stageë³„ ì •ì œ ëª¨ë“ˆ
        self.refine_stages = nn.ModuleList([
            self._make_refine_stage(in_channels + 2 * i, in_channels // (2 ** i))
            for i in range(num_stages)
        ])

        # Stageë³„ ë³€í˜• ì˜ˆì¸¡ê¸°
        self.transform_predictors = nn.ModuleList([
            nn.Conv2d(in_channels // (2 ** i), 2, 1)
            for i in range(num_stages)
        ])

        # ì‹ ë¢°ë„ ì¶”ì •
        self.confidence_estimator = nn.Sequential(
            CommonConvBlock(in_channels, 64),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

    def _make_refine_stage(self, in_channels, out_channels):
        """ì •ì œ ë‹¨ê³„ ìƒì„±"""
        return nn.Sequential(
            CommonConvBlock(in_channels, out_channels * 2),
            CommonConvBlock(out_channels * 2, out_channels)
        )

    def forward(self, features):
        """Progressive refinement ìˆ˜í–‰"""
        transformations = []
        current_feat = features

        for i, (refine_stage, transform_pred) in enumerate(zip(self.refine_stages, self.transform_predictors)):
            try:
                # ğŸ”¥ ë™ì  ì±„ë„ ìˆ˜ ì¡°ì •
                current_channels = current_feat.shape[1]
                expected_channels = 256 + 2 * i  # ì˜ˆìƒ ì±„ë„ ìˆ˜
                
                if current_channels != expected_channels:
                    # ì±„ë„ ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì¡°ì •
                    if current_channels < expected_channels:
                        # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
                        padding = torch.zeros(current_feat.shape[0], expected_channels - current_channels, 
                                            current_feat.shape[2], current_feat.shape[3], 
                                            device=current_feat.device, dtype=current_feat.dtype)
                        current_feat = torch.cat([current_feat, padding], dim=1)
                    else:
                        # ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ ì˜ë¼ë‚´ê¸°
                        current_feat = current_feat[:, :expected_channels, :, :]
                
                # í˜„ì¬ ë‹¨ê³„ ì •ì œ
                refined_feat = refine_stage(current_feat)
                
                # ë³€í˜• ì˜ˆì¸¡
                transform = transform_pred(refined_feat)
                transformations.append(transform)

                # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ íŠ¹ì§• ì¤€ë¹„
                if i < self.num_stages - 1:
                    current_feat = torch.cat([refined_feat, transform], dim=1)
                    
            except Exception as e:
                # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ë³€í˜• ìƒì„±
                h, w = features.shape[2], features.shape[3]
                default_transform = torch.zeros(features.shape[0], 2, h, w, 
                                              device=features.device, dtype=features.dtype)
                transformations.append(default_transform)
                
                if i < self.num_stages - 1:
                    # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ê¸°ë³¸ íŠ¹ì§• ì¤€ë¹„
                    current_feat = torch.zeros(features.shape[0], 256 // (2 ** (i + 1)), h, w,
                                             device=features.device, dtype=features.dtype)

        # ì‹ ë¢°ë„ ì¶”ì •
        try:
            confidence = self.confidence_estimator(features)
        except Exception:
            # ì‹ ë¢°ë„ ì¶”ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            confidence = torch.ones(features.shape[0], 1, features.shape[2], features.shape[3],
                                  device=features.device, dtype=features.dtype) * 0.5

        return transformations, confidence

# ==============================================
# ğŸ”¥ 7. Enhanced Model Path Mapping
# ==============================================

class EnhancedModelPathMapper:
    """í–¥ìƒëœ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ì‹¤ì œ ê²½ë¡œ ìë™ íƒì§€
        self.ai_models_root = self._auto_detect_ai_models_path()
        logger.info(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸ ê²½ë¡œ: {self.ai_models_root}")
        
    def _auto_detect_ai_models_path(self) -> Path:
        """ì‹¤ì œ ai_models ë””ë ‰í† ë¦¬ ìë™ íƒì§€"""
        possible_paths = [
            Path.cwd() / "ai_models",
            Path.cwd().parent / "ai_models",
            Path.cwd() / "backend" / "ai_models",
            Path(__file__).parent / "ai_models",
            Path(__file__).parent.parent / "ai_models",
            Path(__file__).parent.parent.parent / "ai_models"
        ]
        
        for path in possible_paths:
            if path.exists() and (path / "step_04_geometric_matching").exists():
                return path
                        
        return Path.cwd() / "ai_models"
    
    def find_model_file(self, filename: str) -> Optional[Path]:
        """ëª¨ë¸ íŒŒì¼ ì°¾ê¸°"""
        try:
            # ìºì‹œ í™•ì¸
            if filename in self.model_cache:
                return self.model_cache[filename]
            
            # ê²€ìƒ‰ ê²½ë¡œ
            search_dirs = [
                self.ai_models_root,
                self.ai_models_root / "step_04_geometric_matching",
                self.ai_models_root / "step_04_geometric_matching" / "ultra_models",
                self.ai_models_root / "step_04_geometric_matching" / "models",
                self.ai_models_root / "step_03_cloth_segmentation",  # SAM ê³µìœ 
                self.ai_models_root / "checkpoints" / "step_04_geometric_matching",
            ]
            
            for search_dir in search_dirs:
                if search_dir.exists():
                    # ì§ì ‘ íŒŒì¼ ì°¾ê¸°
                    file_path = search_dir / filename
                    if file_path.exists():
                        self.model_cache[filename] = file_path
                        return file_path
                    
                    # ì¬ê·€ ê²€ìƒ‰
                    try:
                        for found_path in search_dir.rglob(filename):
                            if found_path.is_file():
                                self.model_cache[filename] = found_path
                                return found_path
                    except Exception:
                        continue
            
            return None
            
        except Exception as e:
            logger.debug(f"ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨ {filename}: {e}")
            return None
    
    def get_geometric_matching_models(self) -> Dict[str, Path]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ìš© ëª¨ë¸ë“¤ ë§¤í•‘"""
        result = {}
        
        # ì£¼ìš” ëª¨ë¸ íŒŒì¼ë“¤
        model_files = {
            'gmm': ['gmm_final.pth'],
            'tps': ['tps_network.pth'],
            'sam_shared': ['sam_vit_h_4b8939.pth'],
            'resnet': ['resnet101_geometric.pth'],
            'vit': ['ViT-L-14.pt'],
            'efficientnet': ['efficientnet_b0_ultra.pth']
        }
        
        for model_key, filenames in model_files.items():
            for filename in filenames:
                model_path = self.find_model_file(filename)
                if model_path:
                    result[model_key] = model_path
                    logger.info(f"âœ… {model_key} ëª¨ë¸ ë°œê²¬: {filename}")
                    break
        
        return result


# ==============================================
# ğŸ”¥ 9. GeometricMatchingStep ë©”ì¸ í´ë˜ìŠ¤ (Central Hub DI Container ì™„ì „ ì—°ë™)
# ==============================================

class GeometricMatchingStep(BaseStepMixin):
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì™„ì „ ë™ê¸° ë²„ì „)"""
        try:
            # 1. DI Containerì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self, 'di_container') and self.di_container:
                try:
                    service = self.di_container.get_service(service_key)
                    if service is not None:
                        return service
                except Exception as di_error:
                    logger.warning(f"âš ï¸ DI Container ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {di_error}")
            
            # 2. ê¸´ê¸‰ í´ë°± ì„œë¹„ìŠ¤ ìƒì„±
            if service_key == 'session_manager':
                return self._create_emergency_session_manager()
            elif service_key == 'model_loader':
                return self._create_emergency_model_loader()
            
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    """
    ğŸ”¥ Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
    
    Central Hub DI Container v7.0ì—ì„œ ìë™ ì œê³µ:
    âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì…
    âœ… MemoryManager ìë™ ì—°ê²°
    âœ… DataConverter í†µí•©
    âœ… ìë™ ì´ˆê¸°í™” ë° ì„¤ì •
    """
    def __init__(self, **kwargs):
        """Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="GeometricMatchingStep",
                **kwargs
            )
            
            # 3. GeometricMatching íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_geometric_matching_specifics(**kwargs)
            
            logger.info("âœ… GeometricMatchingStep v8.0 Central Hub DI Container ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ GeometricMatchingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _initialize_step_attributes(self):
        """í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin ìš”êµ¬ì‚¬í•­)"""
        self.ai_models = {}
        self.models_loading_status = {
            'gmm': False,
            'tps': False,
            'optical_flow': False,
            'keypoint': False,
            'advanced_ai': False,
            'mock_model': False
        }
        self.model_interface = None
        self.loaded_models = []
        self.logger = logging.getLogger(f"{__name__}.GeometricMatchingStep")
            
        self.gmm_model = None
        self.tps_network = None  
        self.optical_flow_model = None
        self.keypoint_matcher = None
        self.sam_model = None
        self.advanced_geometric_ai = None
        # GeometricMatching íŠ¹í™” ì†ì„±ë“¤
        self.geometric_models = {}
        self.matching_ready = False
        self.matching_cache = {}
        
        # VITBasedGeometricMatchingModule ì„¤ì •
        self.VITBasedGeometricMatchingModule = VITBasedGeometricMatchingModule
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'successful_matches': 0,
            'avg_processing_time': 0.0,
            'avg_transformation_quality': 0.0,
            'keypoint_match_rate': 0.0,
            'optical_flow_accuracy': 0.0,
            'cache_hit_rate': 0.0,
            'error_count': 0,
            'models_loaded': 0
        }
        
        # í†µê³„ ì‹œìŠ¤í…œ
        self.statistics = {
            'total_processed': 0,
            'successful_matches': 0,
            'average_quality': 0.0,
            'total_processing_time': 0.0,
            'ai_model_calls': 0,
            'error_count': 0,
            'model_creation_success': False,
            'real_ai_models_used': True,
            'algorithm_type': 'advanced_deeplab_aspp_self_attention',
            'features': [
                'GMM (Geometric Matching Module)',
                'TPS (Thin-Plate Spline) Transformation', 
                'Keypoint-based Matching',
                'Optical Flow Calculation',
                'RANSAC Outlier Removal',
                'DeepLabV3+ Backbone',
                'ASPP Multi-scale Context',
                'Self-Attention Keypoint Matching',
                'Edge-Aware Transformation',
                'Progressive Geometric Refinement',
                'Procrustes Analysis'
            ]
        }
  
    def _initialize_geometric_matching_specifics(self, **kwargs):
        """GeometricMatching íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •
            self.config = GeometricMatchingConfig()
            if 'config' in kwargs:
                config_dict = kwargs['config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
            
            # ğŸ”§ ìˆ˜ì •: status ê°ì²´ ë¨¼ì € ìƒì„±
            self.status = ProcessingStatus()
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = self._detect_optimal_device()
            
            # Enhanced Model Path Mapping
            self.model_mapper = EnhancedModelPathMapper(kwargs.get('ai_models_root', 'ai_models'))
            
            # ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ë§¤ì²˜
            self.geometric_matcher = AdvancedGeometricMatcher(self.device)
            
            # AI ëª¨ë¸ ë¡œë”© (Central Hubë¥¼ í†µí•´)
            self._load_geometric_matching_models_via_central_hub()
            
        except Exception as e:
            logger.warning(f"âš ï¸ GeometricMatching íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ğŸ”§ ìˆ˜ì •: ì‹¤íŒ¨ ì‹œì—ë„ status ê°ì²´ ìƒì„±
            if not hasattr(self, 'status'):
                self.status = ProcessingStatus()
   
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
        
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ)"""
        self.step_name = "GeometricMatchingStep"
        self.step_id = 4
        self.device = "cpu"
        self.ai_models = {}
        self.models_loading_status = {'emergency': True}
        self.model_interface = None
        self.loaded_models = []
        self.config = GeometricMatchingConfig()
        self.logger = logging.getLogger(f"{__name__}.GeometricMatchingStep")
        self.geometric_models = {}
        self.matching_ready = False
        self.matching_cache = {}
        self.status = ProcessingStatus()
    # _load_ai_models_via_central_hub ë©”ì„œë“œëŠ” _load_geometric_matching_models_via_central_hubë¡œ í†µí•©ë¨
    def _load_ai_models_via_central_hub(self) -> bool:
        """ğŸ”¥ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            logger.info("ğŸ”¥ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )")
            
            # 1. Advanced Geometric AI ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            advanced_model = self._load_advanced_geometric_ai_via_central_hub_improved()
            if advanced_model:
                self.ai_models['advanced_geometric_ai'] = advanced_model
                self.models_loading_status['advanced_geometric_ai'] = True
                logger.info("âœ… Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            else:
                logger.error("âŒ Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # 2. GMM ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            gmm_model = self._load_gmm_model_via_central_hub_improved()
            if gmm_model:
                self.ai_models['gmm'] = gmm_model
                self.models_loading_status['gmm'] = True
                logger.info("âœ… GMM ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            else:
                logger.error("âŒ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # 3. Optical Flow ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            optical_flow_model = self._load_optical_flow_model_via_central_hub_improved()
            if optical_flow_model:
                self.ai_models['optical_flow'] = optical_flow_model
                self.models_loading_status['optical_flow'] = True
                logger.info("âœ… Optical Flow ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            else:
                logger.error("âŒ Optical Flow ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # 4. Keypoint Matcher ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            keypoint_model = self._load_keypoint_matcher_via_central_hub_improved()
            if keypoint_model:
                self.ai_models['keypoint_matcher'] = keypoint_model
                self.models_loading_status['keypoint_matcher'] = True
                logger.info("âœ… Keypoint Matcher ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            else:
                logger.error("âŒ Keypoint Matcher ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ìµœì†Œ 1ê°œ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
            success_count = sum(self.models_loading_status.values())
            if success_count > 0:
                logger.info(f"âœ… Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ëª¨ë¸")
                return True
            else:
                logger.error("âŒ Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
        except Exception as e:
            logger.error(f"âŒ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _load_advanced_geometric_ai_via_central_hub_improved(self) -> Optional[nn.Module]:
        """Advanced Geometric AI ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            # 1. ë¨¼ì € model_loaderê°€ ìœ íš¨í•œì§€ í™•ì¸
            if self.model_loader is None:
                logger.warning("âš ï¸ model_loaderê°€ Noneì…ë‹ˆë‹¤")
                return None
            
            # 2. ModelLoaderë¥¼ í†µí•´ Advanced Geometric AI ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            checkpoint_names = [
                'sam_vit_h_4b8939',  # 2445.7MB - ìµœê³  ì„±ëŠ¥
                'gmm_final',  # ë°±ì—…ìš©
                'tps_network'  # ë°±ì—…ìš©
            ]
            
            for checkpoint_name in checkpoint_names:
                try:
                    logger.info(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_name}")
                    
                    # ModelLoaderì˜ load_model_for_step ë©”ì„œë“œ ì‚¬ìš©
                    loaded_model = self.model_loader.load_model_for_step(
                        step_type='geometric_matching',
                        model_name=checkpoint_name
                    )
                    
                    if loaded_model:
                        logger.info(f"âœ… Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì„±ê³µ: {checkpoint_name}")
                        return loaded_model
                    else:
                        logger.error(f"âŒ Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {checkpoint_name}")
                        continue
                        
                except Exception as e:
                    logger.error(f"âŒ Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({checkpoint_name}): {e}")
                    continue
            
            logger.error("âŒ ëª¨ë“  Advanced Geometric AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            logger.error(f"âŒ Advanced Geometric AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_advanced_geometric_ai_via_central_hub(self, model_loader) -> Optional[nn.Module]:
        """Advanced Geometric Matching AI ëª¨ë¸ ë¡œë”© - ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©"""
        try:
            # SAM ëª¨ë¸ ìš°ì„  ì‚¬ìš© (ìµœê³  ì„±ëŠ¥)
            checkpoint_names = [
                'sam_vit_h_4b8939',  # 2445.7MB - ìµœê³  ì„±ëŠ¥, ì´ë¯¸ ê²€ì¦ë¨
                'gmm_final',  # ë°±ì—…ìš©
                'tps_network'  # ë°±ì—…ìš©
            ]
            
            for checkpoint_name in checkpoint_names:
                try:
                    logger.info(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_name}")
                    
                    # ModelLoaderì˜ load_model_for_step ë©”ì„œë“œ ì‚¬ìš© (ìˆ˜ì •ëœ ë°©ì‹)
                    try:
                        loaded_model = model_loader.load_model_for_step(
                            step_type='geometric_matching',
                            model_name=checkpoint_name,
                            checkpoint_path=None
                        )
                        if loaded_model:
                            # ëª¨ë¸ì´ ì´ë¯¸ ë¡œë”©ëœ ê²½ìš°, ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ëŠ” Noneìœ¼ë¡œ ì„¤ì •
                            checkpoint_data = None
                            logger.info(f"âœ… ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {checkpoint_name}")
                        else:
                            # ModelLoader ì‹¤íŒ¨ ì‹œ ì§ì ‘ ë¡œë”© ì‹œë„
                            checkpoint_path = model_loader.get_model_path(checkpoint_name)
                            if checkpoint_path and checkpoint_path.exists():
                                checkpoint_data = torch.load(str(checkpoint_path), map_location='cpu')
                            else:
                                checkpoint_data = None
                    except Exception as e:
                        logger.warning(f"âš ï¸ ModelLoader ë¡œë”© ì‹¤íŒ¨, ì§ì ‘ ë¡œë”© ì‹œë„: {e}")
                        # ì§ì ‘ torch.load ì‹œë„
                        checkpoint_path = model_loader.get_model_path(checkpoint_name)
                        if checkpoint_path and checkpoint_path.exists():
                            checkpoint_data = torch.load(str(checkpoint_path), map_location='cpu')
                        else:
                            checkpoint_data = None
                    
                    if checkpoint_data:
                        logger.info(f"âœ… Advanced Geometric AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_name}")
                        
                        # ëª¨ë¸ ìƒì„± (ì´ˆê¸°í™” ë¹„í™œì„±í™”)
                        model = CompleteAdvancedGeometricMatchingAI(
                            input_nc=6, 
                            num_keypoints=20,
                            initialize_weights=False  # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¹„í™œì„±í™”
                        )
                        
                        # ğŸ”¥ ëª¨ë¸ íƒ€ì… ê²€ì¦ ì¶”ê°€
                        if not isinstance(model, nn.Module):
                            logger.error(f"âŒ ëª¨ë¸ì´ nn.Moduleì´ ì•„ë‹˜: {type(model)}")
                            continue
                        
                        # ğŸ”¥ parameters ì†ì„± ê²€ì¦ ì¶”ê°€
                        if not hasattr(model, 'parameters'):
                            logger.error(f"âŒ ëª¨ë¸ì— parameters ì†ì„±ì´ ì—†ìŒ: {type(model)}")
                            continue
                        
                        # ê°€ì¤‘ì¹˜ ë¡œë”©
                        if 'model_state_dict' in checkpoint_data:
                            model.load_state_dict(checkpoint_data['model_state_dict'])
                        elif 'state_dict' in checkpoint_data:
                            model.load_state_dict(checkpoint_data['state_dict'])
                        else:
                            # ì²´í¬í¬ì¸íŠ¸ ìì²´ê°€ state_dictì¸ ê²½ìš°
                            model.load_state_dict(checkpoint_data)
                        
                        model.to(self.device)
                        model.eval()
                        
                        # ğŸ”¥ ìµœì¢… ê²€ì¦
                        try:
                            test_tensor = torch.zeros((1, 6, 256, 192), device=self.device, dtype=torch.float32)
                            
                            # ğŸ”¥ ê²€ì¦ëœ MPS íƒ€ì… í†µì¼ (ê°•í™”ëœ ë²„ì „)
                            if self.device == 'mps':
                                # ì…ë ¥ í…ì„œë¥¼ float32ë¡œ í†µì¼
                                test_tensor = test_tensor.to(dtype=torch.float32)
                                
                                # ëª¨ë¸ì„ float32ë¡œ í†µì¼
                                if hasattr(model, 'to'):
                                    model = model.to(dtype=torch.float32)
                                
                                # ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ float32ë¡œ í†µì¼ (ê²€ì¦ëœ íŒ¨í„´)
                                for param in model.parameters():
                                    param.data = param.data.to(dtype=torch.float32)
                                
                                # ëª¨ë“  ëª¨ë¸ ë²„í¼ë¥¼ float32ë¡œ í†µì¼
                                for buffer in model.buffers():
                                    buffer.data = buffer.data.to(dtype=torch.float32)
                                
                                # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •
                                model.eval()
                                
                                # MPS ìºì‹œ ì •ë¦¬
                                if torch.backends.mps.is_available():
                                    torch.backends.mps.empty_cache()
                            
                            with torch.no_grad():
                                _ = model(test_tensor, test_tensor)
                            logger.info(f"âœ… Advanced Geometric AI ëª¨ë¸ ê²€ì¦ ì™„ë£Œ: {checkpoint_name}")
                            return model
                        except Exception as test_e:
                            logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {test_e}")
                            continue
                        
                except Exception as e:
                    logger.debug(f"ì²´í¬í¬ì¸íŠ¸ {checkpoint_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸)
            logger.info("ğŸ”„ Advanced Geometric AI ëª¨ë¸ ìƒˆë¡œ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ)")
            model = CompleteAdvancedGeometricMatchingAI(
                input_nc=6, 
                num_keypoints=20,
                initialize_weights=True  # í´ë°± ì‹œì—ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í™œì„±í™”
            )
            
            # ğŸ”¥ ìƒì„±ëœ ëª¨ë¸ ê²€ì¦
            if not isinstance(model, nn.Module):
                logger.error(f"âŒ ìƒì„±ëœ ëª¨ë¸ì´ nn.Moduleì´ ì•„ë‹˜: {type(model)}")
                return None
                
            if not hasattr(model, 'parameters'):
                logger.error(f"âŒ ìƒì„±ëœ ëª¨ë¸ì— parameters ì†ì„±ì´ ì—†ìŒ: {type(model)}")
                return None
            
            model.to(self.device)
            if self.device == 'mps':
                model = model.to(dtype=torch.float32)
            model.eval()
            
            # ğŸ”¥ ìƒì„±ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                test_tensor = torch.zeros((1, 6, 256, 192), device=self.device, dtype=torch.float32)
                
                # ğŸ”¥ MPS íƒ€ì… í†µì¼
                if self.device == 'mps':
                    test_tensor = test_tensor.to(dtype=torch.float32)
                    if hasattr(model, 'to'):
                        model = model.to(dtype=torch.float32)
                
                with torch.no_grad():
                    _ = model(test_tensor, test_tensor)
                logger.info("âœ… Advanced Geometric AI ëª¨ë¸ ìƒì„± ë° ê²€ì¦ ì™„ë£Œ")
                return model
            except Exception as test_e:
                logger.error(f"âŒ ìƒì„±ëœ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {test_e}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ Advanced Geometric AI ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _load_gmm_model_via_central_hub(self, model_loader) -> Optional[nn.Module]:
        """GMM ëª¨ë¸ ë¡œë”© - VITON-HD ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”©"""
        try:
            logger.info("ğŸ”¥ GMM ëª¨ë¸ VITON-HD ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”© ì‹œë„...")
            
            # ì§ì ‘ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            gmm_path = Path("ai_models/step_04_geometric_matching/gmm_final.pth")
            
            if not gmm_path.exists():
                logger.warning("âš ï¸ GMM ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            gmm_checkpoint = torch.load(str(gmm_path), map_location=self.device, weights_only=True)
            logger.info(f"âœ… GMM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {type(gmm_checkpoint)}")
            
            # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ íƒ€ì… í†µì¼
            if self.device == 'mps':
                # ì²´í¬í¬ì¸íŠ¸ì˜ ëª¨ë“  í…ì„œë¥¼ float32ë¡œ ë³€í™˜
                for key in gmm_checkpoint:
                    if isinstance(gmm_checkpoint[key], torch.Tensor):
                        gmm_checkpoint[key] = gmm_checkpoint[key].to(dtype=torch.float32)
            
            # GMM ëª¨ë¸ ìƒì„± - ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ê¸°ë°˜
            # Vision Transformer ê¸°ë°˜ GMM ëª¨ë¸ (1024 ì°¨ì›)
            class GMMVisionTransformerModel(nn.Module):
                def __init__(self, input_channels=6, hidden_dim=1024, num_control_points=20):
                    super().__init__()
                    self.input_channels = input_channels
                    self.hidden_dim = hidden_dim
                    self.num_control_points = num_control_points
                    
                    # Vision Transformer ë°±ë³¸ (1024 ì°¨ì›)
                    self.backbone = nn.Sequential(
                        # íŒ¨ì¹˜ ì„ë² ë”© (6ì±„ë„ â†’ 1024ì°¨ì›)
                        nn.Conv2d(input_channels, hidden_dim, kernel_size=16, stride=16),
                        nn.LayerNorm([hidden_dim, 16, 12]),  # 256x192 â†’ 16x12 íŒ¨ì¹˜
                        nn.ReLU(inplace=True)
                    )
                    
                    # Transformer ì¸ì½”ë”
                    encoder_layer = nn.TransformerEncoderLayer(
                        d_model=hidden_dim,
                        nhead=16,  # 1024/64 = 16
                        dim_feedforward=hidden_dim * 4,
                        dropout=0.1,
                        batch_first=True
                    )
                    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=12)
                    
                    # GMM í—¤ë“œ (ê¸°í•˜í•™ì  ë§¤ì¹­)
                    self.gmm_head = nn.Sequential(
                        nn.Linear(hidden_dim, hidden_dim // 2),
                        nn.ReLU(inplace=True),
                        nn.Linear(hidden_dim // 2, hidden_dim // 4),
                        nn.ReLU(inplace=True),
                        nn.Linear(hidden_dim // 4, num_control_points * 2)  # x, y ì¢Œí‘œ
                    )
                    
                    # ë³€í™˜ í–‰ë ¬ ì˜ˆì¸¡
                    self.transformation_head = nn.Sequential(
                        nn.Linear(hidden_dim, hidden_dim // 2),
                        nn.ReLU(inplace=True),
                        nn.Linear(hidden_dim // 2, 6)  # 3x2 ë³€í™˜ í–‰ë ¬
                    )
                    
                    # ì‹ ë¢°ë„ ì˜ˆì¸¡
                    self.confidence_head = nn.Sequential(
                        nn.Linear(hidden_dim, hidden_dim // 4),
                        nn.ReLU(inplace=True),
                        nn.Linear(hidden_dim // 4, 1),
                        nn.Sigmoid()
                    )
                    
                    self._initialize_weights()
                
                def _initialize_weights(self):
                    for m in self.modules():
                        if isinstance(m, nn.Conv2d):
                            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        elif isinstance(m, nn.Linear):
                            nn.init.normal_(m.weight, 0, 0.01)
                            nn.init.constant_(m.bias, 0)
                        elif isinstance(m, nn.LayerNorm):
                            nn.init.constant_(m.weight, 1)
                            nn.init.constant_(m.bias, 0)
                
                def forward(self, person_image, clothing_image):
                    # ì…ë ¥ ê²°í•© (6ì±„ë„)
                    combined_input = torch.cat([person_image, clothing_image], dim=1)
                    
                    # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
                    features = self.backbone(combined_input)  # [B, 1024, 16, 12]
                    
                    # Transformer ì…ë ¥ ì¤€ë¹„
                    B, C, H, W = features.shape
                    features = features.flatten(2).transpose(1, 2)  # [B, H*W, C]
                    
                    # Transformer ì¸ì½”ë”©
                    encoded_features = self.transformer(features)  # [B, H*W, 1024]
                    
                    # ê¸€ë¡œë²Œ íŠ¹ì§• (í‰ê·  í’€ë§)
                    global_features = encoded_features.mean(dim=1)  # [B, 1024]
                    
                    # GMM ì œì–´ì  ì˜ˆì¸¡
                    control_points = self.gmm_head(global_features)  # [B, num_control_points*2]
                    control_points = control_points.view(-1, self.num_control_points, 2)
                    
                    # ë³€í™˜ í–‰ë ¬ ì˜ˆì¸¡
                    transformation = self.transformation_head(global_features)  # [B, 6]
                    transformation = transformation.view(-1, 2, 3)  # [B, 2, 3]
                    
                    # ì‹ ë¢°ë„ ì˜ˆì¸¡
                    confidence = self.confidence_head(global_features)  # [B, 1]
                    
                    return {
                        'control_points': control_points,
                        'transformation_matrix': transformation,
                        'confidence': confidence,
                        'features': global_features,
                        'quality_score': confidence
                    }
            
            gmm_model = GMMVisionTransformerModel(
                input_channels=6,
                hidden_dim=1024,
                num_control_points=20
            )
            
            # ğŸ”¥ ë””ë°”ì´ìŠ¤ ë° íƒ€ì… í†µì¼
            gmm_model = gmm_model.to(self.device)
            if self.device == 'mps':
                gmm_model = gmm_model.to(dtype=torch.float32)
            
            # ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„ - ê°œì„ ëœ ë¡œì§
            try:
                # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ - GMM íŠ¹í™”
                if isinstance(gmm_checkpoint, dict):
                    logger.info(f"ğŸ” GMM ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: {list(gmm_checkpoint.keys())}")
                    
                    # GMM ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„
                    if 'state_dict' in gmm_checkpoint:
                        state_dict = gmm_checkpoint['state_dict']
                        logger.info(f"âœ… GMM state_dict ë°œê²¬ - í‚¤ ìˆ˜: {len(state_dict)}")
                        
                        # GMM ì²´í¬í¬ì¸íŠ¸ í‚¤ íŒ¨í„´ ë¶„ì„
                        keys = list(state_dict.keys())
                        gmm_backbone_keys = [k for k in keys if k.startswith('gmm_backbone')]
                        logger.info(f"ğŸ” GMM ë°±ë³¸ í‚¤ ê°œìˆ˜: {len(gmm_backbone_keys)}")
                        logger.info(f"ğŸ” GMM ë°±ë³¸ í‚¤ ì˜ˆì‹œ: {gmm_backbone_keys[:5]}")
                        
                        # í‚¤ ë§¤í•‘ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸)
                        key_mapping = {}
                        for key in keys:
                            if key.startswith('gmm_backbone'):
                                # gmm_backbone â†’ backbone ë§¤í•‘
                                new_key = key.replace('gmm_backbone', 'backbone')
                                key_mapping[key] = new_key
                            else:
                                # ê¸°íƒ€ í‚¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                                key_mapping[key] = key
                        
                        # ë§¤í•‘ëœ state_dict ìƒì„±
                        mapped_state_dict = {}
                        for old_key, new_key in key_mapping.items():
                            if old_key in state_dict:
                                mapped_state_dict[new_key] = state_dict[old_key]
                        
                        state_dict = mapped_state_dict
                        logger.info(f"âœ… GMM í‚¤ ë§¤í•‘ ì™„ë£Œ - ë§¤í•‘ëœ í‚¤ ìˆ˜: {len(mapped_state_dict)}")
                        
                    elif 'model_state_dict' in gmm_checkpoint:
                        state_dict = gmm_checkpoint['model_state_dict']
                        logger.info(f"âœ… GMM model_state_dict ë°œê²¬ - í‚¤ ìˆ˜: {len(state_dict)}")
                    else:
                        # ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©
                        state_dict = gmm_checkpoint
                        logger.info(f"âœ… GMM ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© - í‚¤ ìˆ˜: {len(state_dict)}")
                else:
                    logger.warning(f"âš ï¸ GMM ì²´í¬í¬ì¸íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(gmm_checkpoint)}")
                    state_dict = gmm_checkpoint
                
                # ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„
                missing_keys, unexpected_keys = gmm_model.load_state_dict(state_dict, strict=False)
                logger.info(f"âœ… GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ")
                if missing_keys:
                    logger.warning(f"âš ï¸ GMM ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                if unexpected_keys:
                    logger.warning(f"âš ï¸ GMM ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                
                # ğŸ”¥ ê°€ì¤‘ì¹˜ ê²€ì¦ ê°•í™”
                total_params = sum(p.numel() for p in gmm_model.parameters())
                non_zero_params = sum((p != 0).sum().item() for p in gmm_model.parameters())
                logger.info(f"ğŸ” GMM ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„°: {total_params}, ë¹„ì˜ íŒŒë¼ë¯¸í„°: {non_zero_params}")
                
                # ê°€ì¤‘ì¹˜ ë¶„í¬ ë¶„ì„
                weight_stats = {}
                for name, param in gmm_model.named_parameters():
                    if param.data.numel() > 0:
                        weight_stats[name] = {
                            'mean': param.data.mean().item(),
                            'std': param.data.std().item(),
                            'max': param.data.max().item(),
                            'min': param.data.min().item()
                        }
                
                # ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ 0ì— ê°€ê¹Œìš´ì§€ í™•ì¸
                all_zero = True
                for name, param in gmm_model.named_parameters():
                    if param.data.abs().max() > 1e-6:
                        all_zero = False
                        logger.info(f"âœ… {name}: ì‹¤ì œ ê°€ì¤‘ì¹˜ ê°ì§€ (max: {param.data.abs().max().item():.6f})")
                        break
                
                if all_zero:
                    logger.warning("âš ï¸ GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ 0ì— ê°€ê¹Œì›€ - ì´ˆê¸°í™”ëœ ìƒíƒœ")
                    # ê°€ì¤‘ì¹˜ ì¬ì´ˆê¸°í™” ì‹œë„
                    logger.info("ğŸ”„ GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¬ì´ˆê¸°í™” ì‹œë„...")
                    gmm_model._initialize_weights()
                    logger.info("âœ… GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¬ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.info("âœ… GMM ëª¨ë¸ì— ì‹¤ì œ ê°€ì¤‘ì¹˜ê°€ ë¡œë”©ë¨")
                
            except Exception as weight_error:
                logger.warning(f"âš ï¸ GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {weight_error}")
                logger.info("ğŸ”„ GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¬ì´ˆê¸°í™”...")
                gmm_model._initialize_weights()
                logger.info("âœ… GMM ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¬ì´ˆê¸°í™” ì™„ë£Œ")
            
            gmm_model.to(self.device)
            if self.device == 'mps':
                gmm_model = gmm_model.to(dtype=torch.float32)
            gmm_model.eval()
            
            # ğŸ”¥ ëª¨ë¸ ê²€ì¦
            try:
                test_input = torch.zeros((1, 6, 256, 192), device=self.device, dtype=torch.float32)
                
                # ğŸ”¥ MPS íƒ€ì… í†µì¼
                if self.device == 'mps':
                    test_input = test_input.to(dtype=torch.float32)
                    if hasattr(gmm_model, 'to'):
                        gmm_model = gmm_model.to(dtype=torch.float32)
                
                with torch.no_grad():
                    test_output = gmm_model(test_input, test_input)
                logger.info(f"âœ… GMM ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ: {type(test_output)}")
            except Exception as test_error:
                logger.warning(f"âš ï¸ GMM ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_error}")
            
            logger.info("âœ… GMM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (VITON-HD ê¸°ë°˜)")
            return gmm_model
            
        except Exception as e:
            logger.error(f"âŒ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # í´ë°±: ìƒˆë¡œ ìƒì„±
            try:
                logger.info("ğŸ”„ GMM ëª¨ë¸ ìƒˆë¡œ ìƒì„± (í´ë°±)")
                model = GeometricMatchingModule(
                    input_nc=6,
                    output_nc=2,
                    num_control_points=20,
                    initialize_weights=True  # í´ë°± ì‹œì—ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í™œì„±í™”
                )
                model.to(self.device)
                if self.device == 'mps':
                    model = model.to(dtype=torch.float32)
                model.eval()
                logger.info("âœ… GMM ëª¨ë¸ ìƒì„± ì™„ë£Œ (í´ë°±)")
                return model
            except Exception as fallback_error:
                logger.error(f"âŒ GMM ëª¨ë¸ í´ë°± ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # í´ë°±: Mock ëª¨ë¸ ìƒì„±
            try:
                logger.info("ğŸ”„ GMM Mock ëª¨ë¸ ìƒì„± (í´ë°±)")
                mock_model = self._create_mock_geometric_models()
                if mock_model:
                    logger.info("âœ… GMM Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                    return mock_model
            except Exception as mock_error:
                logger.error(f"âŒ GMM Mock ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {mock_error}")
            
            return None

    def _load_optical_flow_model_via_central_hub(self, model_loader) -> Optional[nn.Module]:
        """Optical Flow ëª¨ë¸ ë¡œë”© - ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©"""
        try:
            model_names = [
                'raft-things',  # VGG19 ê¸°ë°˜ (548MB)
                'vgg19_warping',  # ëŒ€ì•ˆ ëª¨ë¸
                'raft-chairs',
                'raft-kitti',
                'raft-sintel',
                'raft-small'
            ]
            
            for model_name in model_names:
                try:
                    logger.info(f"ğŸ” Optical Flow ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
                    
                    # ModelLoaderì˜ load_model ë©”ì„œë“œ ì‚¬ìš©
                    real_model = model_loader.load_model(model_name)
                    
                    if real_model and real_model.is_loaded:
                        logger.info(f"âœ… Optical Flow ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                        
                        # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                        model_instance = real_model.get_model_instance()
                        
                        if model_instance is not None:
                            # nn.Moduleì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
                            if isinstance(model_instance, nn.Module):
                                model_instance.to(self.device)
                                model_instance.eval()
                                return model_instance
                            else:
                                # ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° OpticalFlowNetworkë¡œ ë˜í•‘
                                model = OpticalFlowNetwork(
                                    feature_dim=256,
                                    hidden_dim=128,
                                    num_iters=12
                                )
                                model.to(self.device)
                                model.eval()
                                return model
                    
                except Exception as e:
                    logger.debug(f"Optical Flow ëª¨ë¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # ğŸ”¥ RAFT ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”© ì‹œë„
            try:
                logger.info("ğŸ”¥ RAFT ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”© ì‹œë„...")
                raft_path = Path("ai_models/step_04_geometric_matching/raft-things.pth")
                
                if raft_path.exists():
                    raft_checkpoint = torch.load(str(raft_path), map_location=self.device)
                    logger.info(f"âœ… RAFT ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {type(raft_checkpoint)}")
                    
                    # OpticalFlowNetwork ìƒì„±
                    optical_flow_model = OpticalFlowNetwork(
                        feature_dim=256,
                        hidden_dim=128,
                        num_iters=12
                    )
                    
                    # ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„
                    try:
                        if isinstance(raft_checkpoint, dict):
                            if 'model_state_dict' in raft_checkpoint:
                                optical_flow_model.load_state_dict(raft_checkpoint['model_state_dict'], strict=False)
                                logger.info("âœ… Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •í™•íˆ ë¡œë”© ì™„ë£Œ")
                            elif 'state_dict' in raft_checkpoint:
                                optical_flow_model.load_state_dict(raft_checkpoint['state_dict'], strict=False)
                                logger.info("âœ… Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •í™•íˆ ë¡œë”© ì™„ë£Œ")
                            else:
                                optical_flow_model.load_state_dict(raft_checkpoint, strict=False)
                                logger.info("âœ… Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •í™•íˆ ë¡œë”© ì™„ë£Œ")
                        else:
                            optical_flow_model.load_state_dict(raft_checkpoint, strict=False)
                            logger.info("âœ… Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •í™•íˆ ë¡œë”© ì™„ë£Œ")
                        
                        # ğŸ”¥ ê°€ì¤‘ì¹˜ ê²€ì¦
                        total_params = sum(p.numel() for p in optical_flow_model.parameters())
                        non_zero_params = sum((p != 0).sum().item() for p in optical_flow_model.parameters())
                        logger.info(f"ğŸ” Optical Flow ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„°: {total_params}, ë¹„ì˜ íŒŒë¼ë¯¸í„°: {non_zero_params}")
                        
                        # ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ 0ì— ê°€ê¹Œìš´ì§€ í™•ì¸
                        all_zero = True
                        for name, param in optical_flow_model.named_parameters():
                            if param.data.abs().max() > 1e-6:
                                all_zero = False
                                break
                        
                        if all_zero:
                            logger.warning("âš ï¸ Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ 0ì— ê°€ê¹Œì›€ - ì´ˆê¸°í™”ëœ ìƒíƒœ")
                        else:
                            logger.info("âœ… Optical Flow ëª¨ë¸ì— ì‹¤ì œ ê°€ì¤‘ì¹˜ê°€ ë¡œë”©ë¨")
                        
                    except Exception as weight_error:
                        logger.warning(f"âš ï¸ Optical Flow ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {weight_error}")
                        logger.info("âœ… Optical Flow ëª¨ë¸ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©")
                    
                    optical_flow_model.to(self.device)
                    if self.device == 'mps':
                        optical_flow_model = optical_flow_model.to(dtype=torch.float32)
                    optical_flow_model.eval()
                    
                    # ğŸ”¥ ëª¨ë¸ ê²€ì¦
                    try:
                        test_input1 = torch.zeros((1, 3, 256, 192), device=self.device, dtype=torch.float32)
                        test_input2 = torch.zeros((1, 3, 256, 192), device=self.device, dtype=torch.float32)
                        with torch.no_grad():
                            test_output = optical_flow_model(test_input1, test_input2)
                        logger.info(f"âœ… Optical Flow ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ: {type(test_output)}")
                    except Exception as test_error:
                        logger.warning(f"âš ï¸ Optical Flow ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_error}")
                    
                    logger.info("âœ… Optical Flow ëª¨ë¸ ë¡œë”© ì™„ë£Œ (RAFT ê¸°ë°˜)")
                    return optical_flow_model
                else:
                    logger.warning("âš ï¸ RAFT ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            except Exception as raft_error:
                logger.warning(f"âš ï¸ RAFT ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {raft_error}")
            
            # í´ë°±: ìƒˆë¡œ ìƒì„±
            logger.info("ğŸ”„ Optical Flow ëª¨ë¸ ìƒˆë¡œ ìƒì„± (í´ë°±)")
            model = OpticalFlowNetwork(
                feature_dim=256,
                hidden_dim=128,
                num_iters=12
            )
            model.to(self.device)
            model.eval()
            logger.info("âœ… Optical Flow ëª¨ë¸ ìƒì„± ì™„ë£Œ (í´ë°±)")
            return model
            
        except Exception as e:
            logger.error(f"âŒ Optical Flow ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            

    def _load_keypoint_matcher_via_central_hub(self, model_loader) -> Optional[nn.Module]:
        """Keypoint Matching ëª¨ë¸ ë¡œë”© - ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©"""
        try:
            checkpoint_names = [
                'sam_vit_h_4b8939',  # 2445.7MB - ìµœê³  ì„±ëŠ¥, ì´ë¯¸ ê²€ì¦ë¨
                'gmm_final',  # ë°±ì—…ìš©
                'tps_network'  # ë°±ì—…ìš©
            ]
            
            for checkpoint_name in checkpoint_names:
                try:
                    logger.info(f"ğŸ” Keypoint Matcher ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_name}")
                    checkpoint_data = model_loader.load_model(checkpoint_name)
                    
                    if checkpoint_data:
                        logger.info(f"âœ… Keypoint Matcher ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_name}")
                        
                        model = KeypointMatchingNetwork(
                            num_keypoints=20,  # í‚¤í¬ì¸íŠ¸ ìˆ˜ í†µì¼ (18 â†’ 20)
                            feature_dim=256
                        )
                        
                        # ê°€ì¤‘ì¹˜ ë¡œë”©
                        if 'model_state_dict' in checkpoint_data:
                            model.load_state_dict(checkpoint_data['model_state_dict'])
                        elif 'state_dict' in checkpoint_data:
                            model.load_state_dict(checkpoint_data['state_dict'])
                        else:
                            model.load_state_dict(checkpoint_data)
                        
                        model.to(self.device)
                        if self.device == 'mps':
                            model = model.to(dtype=torch.float32)
                        model.eval()
                        
                        return model
                        
                except Exception as e:
                    logger.debug(f"Keypoint Matcher ì²´í¬í¬ì¸íŠ¸ {checkpoint_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # ìƒˆë¡œ ìƒì„±
            logger.info("ğŸ”„ Keypoint Matcher ìƒˆë¡œ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ)")
            model = KeypointMatchingNetwork(
                num_keypoints=20,  # ë” ë§ì€ í‚¤í¬ì¸íŠ¸ë¡œ ì •í™•ë„ í–¥ìƒ
                feature_dim=256
            )
            model.to(self.device)
            if self.device == 'mps':
                model = model.to(dtype=torch.float32)
            model.eval()
            logger.info("âœ… Keypoint Matcher ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ Keypoint Matcher ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _create_advanced_ai_networks(self):
        """ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± - ëˆ„ë½ëœ ë©”ì„œë“œ ì¶”ê°€"""
        try:
            self.logger.info("ğŸ”§ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹œì‘")
            
            # ê¸°ë³¸ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„±
            advanced_ai = CompleteAdvancedGeometricMatchingAI(
                input_nc=6,
                num_keypoints=20,
                initialize_weights=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            advanced_ai = advanced_ai.to(self.device)
            advanced_ai.eval()
            
            self.logger.info("âœ… ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            return advanced_ai
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _load_geometric_matching_models_via_central_hub(self):
        """Central Hub DI Containerë¥¼ í†µí•œ GeometricMatching ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ GeometricMatching AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # Central Hubì—ì„œ ModelLoader ê°€ì ¸ì˜¤ê¸° (ìë™ ì£¼ì…ë¨)
            if not hasattr(self, 'model_loader') or not self.model_loader:
                logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ë¡œ ì§ì ‘ ìƒì„±")
                self._create_advanced_ai_networks()
                return
            
            # 1. ModelLoaderë¥¼ í†µí•œ GMM ëª¨ë¸ ë¡œë”©
            try:
                logger.info("ğŸ”¥ ModelLoaderë¥¼ í†µí•œ GMM ëª¨ë¸ ë¡œë”© ì‹œì‘")
                
                # ModelLoaderì˜ load_model ë©”ì„œë“œ ì‚¬ìš©
                gmm_real_model = self.model_loader.load_model_for_step("geometric_matching", "gmm_final")
                
                if gmm_real_model is not None:
                    # RealAIModelì—ì„œ ì‹¤ì œ PyTorch ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                    gmm_model = gmm_real_model.get_model_instance()
                    
                    if gmm_model is None:
                        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ìƒì„±
                        gmm_model = gmm_real_model.get_checkpoint_data()
                    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device == "mps" and torch.backends.mps.is_available():
                        gmm_model = gmm_model.to(dtype=torch.float32, device=self.device)
                    else:
                        gmm_model = gmm_model.to(self.device)
                    
                    gmm_model.eval()
                    self.ai_models['gmm_model'] = gmm_model
                    self.models_loading_status['gmm_model'] = True
                    self.loaded_models.append('gmm_model')
                    self.gmm_model = gmm_model
                    logger.info("âœ… GMM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ModelLoader)")
                else:
                    logger.warning("âš ï¸ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ëŒ€ì²´ ëª¨ë¸ ìƒì„±")
                    raise Exception("GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
            except Exception as gmm_error:
                logger.warning(f"âš ï¸ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {gmm_error}")
                # ëŒ€ì²´ ëª¨ë¸ ìƒì„±
                gmm_model = GeometricMatchingModule(
                    input_nc=6,
                    output_nc=2,
                    num_control_points=20
                )
                gmm_model.to(self.device)
                gmm_model.eval()
                self.ai_models['gmm_model'] = gmm_model
                self.loaded_models.append('gmm_model')
                self.gmm_model = gmm_model
                logger.info("âœ… GMM ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # 2. ModelLoaderë¥¼ í†µí•œ TPS ëª¨ë¸ ë¡œë”©
            try:
                logger.info("ğŸ”¥ ModelLoaderë¥¼ í†µí•œ TPS ëª¨ë¸ ë¡œë”© ì‹œì‘")
                
                # ModelLoaderì˜ load_model ë©”ì„œë“œ ì‚¬ìš©
                tps_real_model = self.model_loader.load_model_for_step("geometric_matching", "tps_network")
                
                if tps_real_model is not None:
                    # RealAIModelì—ì„œ ì‹¤ì œ PyTorch ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                    tps_model = tps_real_model.get_model_instance()
                    
                    if tps_model is None:
                        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ìƒì„±
                        tps_model = tps_real_model.get_checkpoint_data()
                    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device == "mps" and torch.backends.mps.is_available():
                        tps_model = tps_model.to(dtype=torch.float32, device=self.device)
                    else:
                        tps_model = tps_model.to(self.device)
                    
                    tps_model.eval()
                    self.ai_models['tps'] = tps_model
                    self.models_loading_status['tps'] = True
                    self.loaded_models.append('tps')
                    self.tps_model = tps_model
                    logger.info("âœ… TPS ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ModelLoader)")
                else:
                    logger.warning("âš ï¸ TPS ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ëŒ€ì²´ ëª¨ë¸ ìƒì„±")
                    raise Exception("TPS ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
            except Exception as tps_error:
                logger.warning(f"âš ï¸ TPS ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {tps_error}")
                # ëŒ€ì²´ ëª¨ë¸ ìƒì„±
                tps_model = SimpleTPS(
                    input_nc=3,
                    num_control_points=18
                )
                tps_model.to(self.device)
                tps_model.eval()
                self.ai_models['tps'] = tps_model
                self.loaded_models.append('tps')
                self.tps_model = tps_model
                logger.info("âœ… TPS ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # 3. ModelLoaderë¥¼ í†µí•œ RAFT ëª¨ë¸ ë¡œë”©
            try:
                logger.info("ğŸ”¥ ModelLoaderë¥¼ í†µí•œ RAFT ëª¨ë¸ ë¡œë”© ì‹œì‘")
                
                # ModelLoaderì˜ load_model ë©”ì„œë“œ ì‚¬ìš©
                raft_real_model = self.model_loader.load_model_for_step("geometric_matching", "raft-things")
                
                if raft_real_model is not None:
                    # RealAIModelì—ì„œ ì‹¤ì œ PyTorch ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                    raft_model = raft_real_model.get_model_instance()
                    
                    if raft_model is None:
                        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ìƒì„±
                        raft_model = raft_real_model.get_checkpoint_data()
                    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device == "mps" and torch.backends.mps.is_available():
                        raft_model = raft_model.to(dtype=torch.float32, device=self.device)
                    else:
                        raft_model = raft_model.to(self.device)
                    
                    raft_model.eval()
                    self.ai_models['optical_flow'] = raft_model
                    self.models_loading_status['optical_flow'] = True
                    self.loaded_models.append('optical_flow')
                    self.optical_flow_model = raft_model
                    logger.info("âœ… RAFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ModelLoader)")
                else:
                    logger.warning("âš ï¸ RAFT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ëŒ€ì²´ ëª¨ë¸ ìƒì„±")
                    raise Exception("RAFT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
            except Exception as raft_error:
                logger.warning(f"âš ï¸ RAFT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {raft_error}")
                # ëŒ€ì²´ ëª¨ë¸ ìƒì„±
                optical_flow_model = OpticalFlowNetwork(
                    feature_dim=256,
                    hidden_dim=128,
                    num_iters=12
                )
                optical_flow_model.to(self.device)
                optical_flow_model.eval()
                self.ai_models['optical_flow'] = optical_flow_model
                self.loaded_models.append('optical_flow')
                self.optical_flow_model = optical_flow_model
                logger.info("âœ… RAFT ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # 4. ModelLoaderë¥¼ í†µí•œ SAM ëª¨ë¸ ë¡œë”©
            try:
                logger.info("ğŸ”¥ ModelLoaderë¥¼ í†µí•œ SAM ëª¨ë¸ ë¡œë”© ì‹œì‘")
                
                # ModelLoaderì˜ load_model ë©”ì„œë“œ ì‚¬ìš©
                sam_real_model = self.model_loader.load_model_for_step("geometric_matching", "sam_vit_h_4b8939")
                
                if sam_real_model is not None:
                    # RealAIModelì—ì„œ ì‹¤ì œ PyTorch ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                    sam_model = sam_real_model.get_model_instance()
                    
                    if sam_model is None:
                        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ìƒì„±
                        sam_model = sam_real_model.get_checkpoint_data()
                    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device == "mps" and torch.backends.mps.is_available():
                        sam_model = sam_model.to(dtype=torch.float32, device=self.device)
                    else:
                        sam_model = sam_model.to(self.device)
                    
                    sam_model.eval()
                    self.ai_models['advanced_ai'] = sam_model
                    self.models_loading_status['advanced_ai'] = True
                    self.loaded_models.append('advanced_ai')
                    self.advanced_geometric_ai = sam_model
                    logger.info("âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ModelLoader)")
                else:
                    logger.warning("âš ï¸ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ëŒ€ì²´ ëª¨ë¸ ìƒì„±")
                    raise Exception("SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
            except Exception as sam_error:
                logger.warning(f"âš ï¸ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {sam_error}")
                # ëŒ€ì²´ ëª¨ë¸ ìƒì„±
                advanced_ai_model = CompleteAdvancedGeometricMatchingAI(
                    input_nc=6, 
                    num_keypoints=20
                )
                advanced_ai_model.to(self.device)
                advanced_ai_model.eval()
                self.ai_models['advanced_ai'] = advanced_ai_model
                self.loaded_models.append('advanced_ai')
                self.advanced_geometric_ai = advanced_ai_model
                logger.info("âœ… SAM ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # 5. Keypoint Matcher ëª¨ë¸ ìƒì„±
            try:
                logger.info("ğŸ”„ KeypointMatchingNetwork ìƒì„±...")
                keypoint_matcher = KeypointMatchingNetwork(
                    num_keypoints=20,
                    feature_dim=256
                ).to(self.device)
                keypoint_matcher.eval()
                
                self.ai_models['keypoint_matcher'] = keypoint_matcher
                self.models_loading_status['keypoint_matcher'] = True
                self.loaded_models.append('keypoint_matcher')
                self.keypoint_matcher = keypoint_matcher
                logger.info("âœ… KeypointMatchingNetwork ìƒì„± ì™„ë£Œ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ KeypointMatchingNetwork ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 6. ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ì™€ ë³‘í–‰)
            self._create_advanced_ai_networks()
            
            # ë§¤ì¹­ ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.matching_ready = len(self.loaded_models) > 0
            self.status.models_loaded = len(self.loaded_models) > 0
            self.status.model_creation_success = len(self.loaded_models) > 0
            
            loaded_count = len(self.loaded_models)
            logger.info(f"ğŸ§  Central Hub GeometricMatching ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸")
            logger.info(f"ğŸ§  ë¡œë”©ëœ ëª¨ë¸ë“¤: {self.loaded_models}")
            
        except Exception as e:
            logger.error(f"âŒ Central Hub GeometricMatching ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self._create_advanced_ai_networks()


    def _load_pretrained_weights(self, model_loader, checkpoint_name: str):
        """ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë”©"""
        try:
            # ğŸ”¥ ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ì•ˆì „í•œ ë°©ì‹)
            try:
                checkpoint_path = model_loader.get_model_path(checkpoint_name)
                if not checkpoint_path:
                    logger.info(f"â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {checkpoint_name}")
                    return
                
                # Path ê°ì²´ì¸ì§€ í™•ì¸
                if hasattr(checkpoint_path, 'exists'):
                    if not checkpoint_path.exists():
                        logger.info(f"â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_name}")
                        return
                else:
                    # ë¬¸ìì—´ì¸ ê²½ìš° Pathë¡œ ë³€í™˜
                    from pathlib import Path
                    checkpoint_path = Path(checkpoint_path)
                    if not checkpoint_path.exists():
                        logger.info(f"â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_name}")
                        return
                        
            except Exception as path_error:
                logger.info(f"â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {path_error}")
                return
            
            logger.debug(f"ğŸ”„ ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_name}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            
            # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'generator' in checkpoint:
                    state_dict = checkpoint['generator']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # í‚¤ ì´ë¦„ ë§¤í•‘
            new_state_dict = {}
            for k, v in state_dict.items():
                new_key = k
                if k.startswith('module.'):
                    new_key = k[7:]  # 'module.' ì œê±°
                elif k.startswith('netG.'):
                    new_key = k[5:]  # 'netG.' ì œê±°
                elif k.startswith('generator.'):
                    new_key = k[10:]  # 'generator.' ì œê±°
                
                new_state_dict[new_key] = v
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
            if 'advanced_ai' in self.ai_models:
                model_dict = self.ai_models['advanced_ai'].state_dict()
                compatible_dict = {}
                
                for k, v in new_state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    self.ai_models['advanced_ai'].load_state_dict(model_dict, strict=False)
                    logger.debug(f"âœ… ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(new_state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    logger.info("â„¹ï¸ í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìœ ì§€")
                    
        except Exception as e:
            logger.info(f"â„¹ï¸ ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìƒëµ: {e}")

    def process(self, **kwargs) -> Dict[str, Any]:
        """ğŸ”¥ ì™„ì „í•œ Geometric Matching ì²˜ë¦¬ - step_01ê³¼ ë™ì¼í•œ êµ¬ì¡°"""
        try:
            # ğŸ”¥ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            log_step_memory("Step 4 - Geometric Matching ì‹œì‘", kwargs.get('session_id', 'unknown'))
            
            # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° ì¶”ì  ë¡œê¹… ì¶”ê°€
            session_id = kwargs.get('session_id', 'unknown')
            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ì‹œì‘ - session_id: {session_id}")
            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ì…ë ¥ ë°ì´í„° í¬ê¸°: {len(str(kwargs))} bytes")
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ìƒì„¸ ë¡œê¹…
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 ì…ë ¥ í‚¤ë“¤: {list(kwargs.keys()) if kwargs else 'None'}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 ì…ë ¥ ê°’ë“¤: {[(k, type(v).__name__) for k, v in kwargs.items()] if kwargs else 'None'}")
            
            # ğŸ”¥ Pipeline Managerì—ì„œ ì „ë‹¬ëœ ë°ì´í„° í™•ì¸
            if 'pipeline_result' in kwargs:
                self.pipeline_result = kwargs['pipeline_result']
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipeline ê²°ê³¼ ê°ì²´ ì„¤ì • ì™„ë£Œ")
            else:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipeline ê²°ê³¼ ê°ì²´ê°€ ì „ë‹¬ë˜ì§€ ì•ŠìŒ")
                self.pipeline_result = None
            
            # ğŸ”¥ ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
            loaded_models = list(self.ai_models.keys()) if hasattr(self, 'ai_models') and self.ai_models else []
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 ëª¨ë¸ ë¡œë”© ìƒíƒœ - ë¡œë“œëœ ëª¨ë¸: {loaded_models}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 ëª¨ë¸ ë¡œë”© ìƒíƒœ - ëª¨ë¸ ê°œìˆ˜: {len(loaded_models)}")
            
            # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
            device_info = getattr(self, 'device', 'unknown')
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 ë””ë°”ì´ìŠ¤ ì •ë³´ - device: {device_info}")
            
            start_time = time.time()
            logger.info("ï¿½ï¿½ Geometric Matching Step ì‹œì‘")
            
            # 1. ì´ˆê¸°í™” í™•ì¸
            if not self.is_initialized:
                logger.warning("âš ï¸ ìŠ¤í…ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ, ì´ˆê¸°í™” ì§„í–‰")
                if not self.initialize():
                    return self._create_error_response("ìŠ¤í… ì´ˆê¸°í™” ì‹¤íŒ¨")        
            # 2. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - API ì…ë ¥ ë³€í™˜ ì‹œì‘")
            try:
                processed_input = self.convert_api_input_to_step_input(kwargs)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(processed_input)}ê°œ í‚¤")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ë³€í™˜ëœ ì…ë ¥ í‚¤ë“¤: {list(processed_input.keys()) if processed_input else 'None'}")
                logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(processed_input)}ê°œ í‚¤")
            except Exception as convert_error:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {convert_error}")
                logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {convert_error}")
                processed_input = kwargs
            
            # 3. ì…ë ¥ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ê²€ì¦
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì…ë ¥ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘")
            try:
                person_image, clothing_image, session_data = self._validate_and_extract_inputs(processed_input)
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_image íƒ€ì…: {type(person_image)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_image íƒ€ì…: {type(clothing_image)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - session_data íƒ€ì…: {type(session_data)}")
                
                if person_image is not None and hasattr(person_image, 'shape'):
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_image shape: {person_image.shape}")
                if clothing_image is not None and hasattr(clothing_image, 'shape'):
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_image shape: {clothing_image.shape}")
                
                if person_image is None or clothing_image is None:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì…ë ¥ ì´ë¯¸ì§€ ëˆ„ë½")
                    return self._create_error_response("ì…ë ¥ ì´ë¯¸ì§€ ëˆ„ë½")
                    
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì…ë ¥ ì´ë¯¸ì§€ ê²€ì¦ ì™„ë£Œ")
                logger.info("âœ… ì…ë ¥ ì´ë¯¸ì§€ ê²€ì¦ ì™„ë£Œ")
                
                # ğŸ”¥ ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ processed_inputì— ì¶”ê°€
                processed_input['person_image'] = person_image
                processed_input['clothing_image'] = clothing_image
                processed_input['session_data'] = session_data
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ë¯¸ì§€ë¥¼ processed_inputì— ì¶”ê°€ ì™„ë£Œ")
                
            except Exception as validation_error:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {validation_error}")
                logger.error(f"âŒ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {validation_error}")
                return self._create_error_response(f"ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {str(validation_error)}")
            
            # 4. ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦
            if not self._validate_image_quality(person_image, clothing_image):
                return self._create_error_response("ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
            
            # 5. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key_complete(person_image, clothing_image)
            cached_result = self._check_cache(cache_key)
            if cached_result:
                logger.info("âœ… ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©")
                cached_result['processing_time'] = time.time() - start_time
                cached_result['from_cache'] = True
                return self.convert_step_output_to_api_response(cached_result)
            
            # 6. AI ì¶”ë¡  ì‹¤í–‰
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ì¶”ë¡  ì‹œì‘")
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - _run_ai_inference í˜¸ì¶œ")
                inference_result = self._run_ai_inference(processed_input)
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(inference_result)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ì¶”ë¡  ê²°ê³¼ í‚¤ë“¤: {list(inference_result.keys()) if isinstance(inference_result, dict) else 'Not a dict'}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ì¶”ë¡  ì„±ê³µ ì—¬ë¶€: {inference_result.get('success', False)}")
                
                if not inference_result.get('success', False):
                    error_msg = inference_result.get('error', 'Unknown error')
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - AI ì¶”ë¡  ì‹¤íŒ¨: {error_msg}")
                    return self._create_error_response(f"AI ì¶”ë¡  ì‹¤íŒ¨: {error_msg}")
                    
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ì¶”ë¡  ì™„ë£Œ")
                logger.info("âœ… AI ì¶”ë¡  ì™„ë£Œ")
                
            except Exception as inference_error:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - AI ì¶”ë¡  ì˜ˆì™¸ ë°œìƒ: {inference_error}")
                logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                return self._create_error_response(f"AI ì¶”ë¡  ì‹¤íŒ¨: {str(inference_error)}")
            
            # 7. ê²°ê³¼ í›„ì²˜ë¦¬
            try:
                final_result = self._postprocess_geometric_matching_result(
                    inference_result, person_image, clothing_image
                )
                logger.info("âœ… ê²°ê³¼ í›„ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as postprocess_error:
                logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {postprocess_error}")
                return self._create_error_response(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {str(postprocess_error)}")
            
            # 8. í’ˆì§ˆ í‰ê°€
            try:
                quality_metrics = self._evaluate_geometric_matching_quality(final_result)
                final_result.update(quality_metrics)
                logger.info("âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")
                
            except Exception as quality_error:
                logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {quality_error}")
                # í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
            
            # 9. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            
            # Step 5ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ transformation_matrixë¥¼ ë³„ë„ í‚¤ë¡œ ì¶”ê°€
            if 'transformation_matrix' in final_result:
                final_result['step_4_transformation_matrix'] = final_result['transformation_matrix']
                logger.info("âœ… Step 4 transformation_matrixë¥¼ step_4_transformation_matrixë¡œ ì¶”ê°€")
                print("âœ… Step 4 transformation_matrixë¥¼ step_4_transformation_matrixë¡œ ì¶”ê°€")
            
            final_result.update({
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': processing_time,
                'success': True,
                'version': 'v8.0',
                'models_used': self.loaded_models,
                'algorithm_type': final_result.get('algorithm_type', 'geometric_matching'),
                'from_cache': False
            })
            
            # 10. ìºì‹œ ì €ì¥
            try:
                self._save_to_cache(cache_key, final_result)
            except Exception as cache_error:
                logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {cache_error}")
            
            # 11. í†µê³„ ì—…ë°ì´íŠ¸
            try:
                self._update_inference_statistics_complete(
                    processing_time, True, final_result
                )
            except Exception as stats_error:
                logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats_error}")
            
            # 12. ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            try:
                api_response = self.convert_step_output_to_api_response(final_result)
                logger.info(f"âœ… Geometric Matching ì™„ë£Œ - ì‹œê°„: {processing_time:.3f}ì´ˆ, ì‹ ë¢°ë„: {final_result.get('confidence', 0):.3f}")
                
                # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° ì €ì¥ ë¡œê¹… ì¶”ê°€
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ì™„ë£Œ - session_id: {session_id}")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ê²°ê³¼ ë°ì´í„° í¬ê¸°: {len(str(api_response))} bytes")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ì„±ê³µ ì—¬ë¶€: {api_response.get('success', False)}")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
                
                # ğŸ”¥ ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ë¡œê¹…
                if api_response.get('success', False) and 'transformation_matrix' in final_result:
                    transform_matrix = final_result['transformation_matrix']
                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 4 â†’ Step 5 ì „ë‹¬ ë°ì´í„° ì¤€ë¹„:")
                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - transformation_matrix íƒ€ì…: {type(transform_matrix)}")
                    if hasattr(transform_matrix, 'shape'):
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - transformation_matrix í¬ê¸°: {transform_matrix.shape}")
                
                # ğŸ”¥ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ëª¨ë‹ˆí„°ë§
                log_step_memory("Step 4 - Geometric Matching ì™„ë£Œ", session_id)
                cleanup_result = cleanup_step_memory(aggressive=False)
                print(f"ğŸ”¥ [ë©”ëª¨ë¦¬ ì •ë¦¬] Step 4 ì™„ë£Œ í›„ ì •ë¦¬: {cleanup_result.get('memory_freed_gb', 0):.2f}GB í•´ì œ")
                
                return api_response
                
            except Exception as response_error:
                logger.error(f"âŒ API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {response_error}")
                return self._create_error_response(f"ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {str(response_error)}")
            
        except Exception as e:
            logger.error(f"âŒ Geometric Matching ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            return self._create_error_response(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", processing_time)

    def _validate_image_quality(self, person_image, clothing_image) -> bool:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if person_image is None or clothing_image is None:
                return False
            
            # í¬ê¸° ê²€ì¦
            if hasattr(person_image, 'shape'):
                if person_image.shape[0] < 64 or person_image.shape[1] < 64:
                    logger.warning("âš ï¸ ì‚¬ëŒ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŒ")
                    return False
            
            if hasattr(clothing_image, 'shape'):
                if clothing_image.shape[0] < 32 or clothing_image.shape[1] < 32:
                    logger.warning("âš ï¸ ì˜ë¥˜ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŒ")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _postprocess_geometric_matching_result(self, inference_result: Dict[str, Any], 
                                            person_image, clothing_image) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            result = inference_result.copy()
            
            # ë³€í˜• í–‰ë ¬ ê²€ì¦
            if 'transformation_matrix' in result:
                transform_matrix = result['transformation_matrix']
                if torch.is_tensor(transform_matrix):
                    # í–‰ë ¬ì‹ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸
                    det = torch.det(transform_matrix[:, :2, :2])
                    stability = torch.clamp(1.0 / (torch.abs(det - 1.0) + 1e-6), 0, 1).mean().item()
                    result['transformation_stability'] = stability
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            if 'quality_score' in result:
                quality_raw = result['quality_score']
                if torch.is_tensor(quality_raw):
                    try:
                        quality = torch.mean(quality_raw).item()
                    except Exception:
                        quality = 0.5
                else:
                    quality = float(quality_raw)
                result['overall_quality'] = quality
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            if 'confidence' in result:
                confidence = result['confidence']
                if torch.is_tensor(confidence):
                    confidence = confidence.item()
                result['confidence'] = confidence
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return inference_result

    def _evaluate_geometric_matching_quality(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_metrics = {}
            
            # ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
            confidence = result.get('confidence', 0.5)
            quality_score = result.get('overall_quality', 0.5)
            stability = result.get('transformation_stability', 1.0)
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            overall_quality = (confidence * 0.4 + quality_score * 0.4 + stability * 0.2)
            
            quality_metrics.update({
                'quality_metrics': {
                    'confidence': confidence,
                    'quality_score': quality_score,
                    'stability': stability,
                    'overall_quality': overall_quality
                },
                'quality_level': 'high' if overall_quality > 0.8 else 'medium' if overall_quality > 0.6 else 'low'
            })
            
            return quality_metrics
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'quality_metrics': {'overall_quality': 0.5}, 'quality_level': 'low'}

    def convert_step_output_to_api_response(self, step_output: Dict[str, Any]) -> Dict[str, Any]:
        """ìŠ¤í… ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            api_response = {
                'success': step_output.get('success', True),
                'data': {
                    'transformation_matrix': step_output.get('transformation_matrix'),
                    'transformation_grid': step_output.get('transformation_grid'),
                    'warped_clothing': step_output.get('warped_clothing'),
                    'confidence': step_output.get('confidence', 0.0),
                    'quality_score': step_output.get('quality_score', 0.0),
                    'algorithm_type': step_output.get('algorithm_type', 'geometric_matching'),
                    'models_used': step_output.get('models_used', []),
                    'processing_time': step_output.get('processing_time', 0.0)
                },
                'metadata': {
                    'step_name': step_output.get('step_name', self.step_name),
                    'step_id': step_output.get('step_id', self.step_id),
                    'version': step_output.get('version', 'v8.0'),
                    'quality_metrics': step_output.get('quality_metrics', {}),
                    'quality_level': step_output.get('quality_level', 'medium')
                }
            }
            
            # ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš°
            if not step_output.get('success', True):
                api_response['error'] = step_output.get('error', 'Unknown error')
            
            return api_response
            
        except Exception as e:
            logger.error(f"âŒ API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': f'ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {str(e)}',
                'data': {},
                'metadata': {}
            }

    def _check_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œ í™•ì¸"""
        if cache_key in self.matching_cache:
            cached_result = self.matching_cache[cache_key]
            cached_result['cache_hit'] = True
            logger.info("ğŸ¯ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
            return cached_result
        return None

    def _execute_gmm_model(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """GMM ëª¨ë¸ ì‹¤í–‰ - ê°œì„ ëœ ì‹ ê²½ë§ ì¶”ë¡ """
        try:
            if self.gmm_model is None:
                logger.warning("âš ï¸ GMM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {}
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.gmm_model.eval()
            
            # ì‹¤ì œ ì‹ ê²½ë§ ì¶”ë¡  ìˆ˜í–‰
            with torch.no_grad():
                start_time = time.time()
                
                if hasattr(self.gmm_model, 'forward'):
                    # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                    gmm_result = self.gmm_model(person_tensor, clothing_tensor)
                    
                    # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
                    if not isinstance(gmm_result, dict):
                        gmm_result = {
                            'transformation_matrix': gmm_result,
                            'confidence': torch.tensor(0.85, device=person_tensor.device),
                            'quality_score': torch.tensor(0.8, device=person_tensor.device)
                        }
                    
                    inference_time = time.time() - start_time
                    logger.info(f"âœ… GMM ì‹ ê²½ë§ ì¶”ë¡  ì™„ë£Œ (ì†Œìš”ì‹œê°„: {inference_time:.4f}ì´ˆ)")
                    
                    # ì¶”ë¡  ì‹œê°„ ê²€ì¦
                    if inference_time < 0.1:
                        logger.warning(f"âš ï¸ GMM ì¶”ë¡  ì‹œê°„ì´ ë„ˆë¬´ ë¹ ë¦„ ({inference_time:.4f}ì´ˆ) - Mock ëª¨ë¸ì¼ ê°€ëŠ¥ì„±")
                    else:
                        logger.info(f"âœ… GMM ì‹¤ì œ ì‹ ê²½ë§ ì¶”ë¡  í™•ì¸ (ì†Œìš”ì‹œê°„: {inference_time:.4f}ì´ˆ)")
                    
                else:
                    # Mock ëª¨ë¸ì¸ ê²½ìš°
                    gmm_result = self.gmm_model.predict(person_tensor.cpu().numpy(), clothing_tensor.cpu().numpy())
                    logger.info("âœ… GMM Mock ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
                
            return {'gmm': gmm_result}
            
        except Exception as e:
            logger.warning(f"âš ï¸ GMM ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            import traceback
            logger.warning(f"âš ï¸ GMM ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {}

    def _execute_keypoint_matching(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor, pose_keypoints: List) -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹¤í–‰"""
        try:
            keypoint_result = self._perform_keypoint_matching(person_tensor, clothing_tensor, pose_keypoints)
            logger.info("âœ… í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­ ì™„ë£Œ")
            return {'keypoint': keypoint_result}
        except Exception as e:
            logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {}

    def _execute_optical_flow(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """Optical Flow ì‹¤í–‰"""
        try:
            if self.optical_flow_model is None:
                logger.warning("âš ï¸ Optical Flow ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {}
                
            if hasattr(self.optical_flow_model, 'forward'):
                # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                self.optical_flow_model.eval()
                with torch.no_grad():
                    flow_result = self.optical_flow_model(person_tensor, clothing_tensor)
                    
                # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
                if not isinstance(flow_result, dict):
                    flow_result = {
                        'flow': flow_result,
                        'confidence': torch.tensor(0.75, device=person_tensor.device),
                        'quality_score': torch.tensor(0.7, device=person_tensor.device)
                    }
                    
            elif hasattr(self.optical_flow_model, 'predict'):
                # Mock ëª¨ë¸ì¸ ê²½ìš°
                flow_result = self.optical_flow_model.predict(person_tensor.cpu().numpy(), clothing_tensor.cpu().numpy())
            else:
                logger.warning("âš ï¸ Optical Flow ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ë¶ˆëª…")
                return {}
                
            logger.info("âœ… Optical Flow ê³„ì‚° ì™„ë£Œ")
            return {'optical_flow': flow_result}
            
        except Exception as e:
            logger.warning(f"âš ï¸ Optical Flow ì‹¤íŒ¨: {e}")
            # í´ë°± ê²°ê³¼ ìƒì„±
            try:
                batch_size, channels, height, width = person_tensor.shape
                fallback_flow = torch.zeros(batch_size, 2, height, width, device=person_tensor.device)
                fallback_result = {
                    'flow': fallback_flow,
                    'confidence': torch.tensor(0.5, device=person_tensor.device),
                    'quality_score': torch.tensor(0.5, device=person_tensor.device)
                }
                logger.info("ğŸ”„ Optical Flow í´ë°± ê²°ê³¼ ìƒì„±")
                return {'optical_flow': fallback_result}
            except Exception as fallback_error:
                logger.error(f"âŒ Optical Flow í´ë°± ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
                return {}
    
    def _compute_enhanced_confidence(self, results: Dict[str, Any]) -> float:
        """ê°•í™”ëœ ì‹ ë¢°ë„ ê³„ì‚° - ì‹ ë¢°ë„ í–¥ìƒì„ ìœ„í•œ ìµœì í™”ëœ ê³„ì‚°"""
        confidences = []
        weights = []
        
        # 1. Advanced AI ì‹ ë¢°ë„ (ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
        if 'advanced_ai' in results:
            if 'confidence' in results['advanced_ai']:
                ai_conf = results['advanced_ai']['confidence']
                if isinstance(ai_conf, torch.Tensor):
                    try:
                        ai_conf = ai_conf.mean().item()
                    except Exception:
                        ai_conf = 0.7  # ê¸°ë³¸ê°’
                elif isinstance(ai_conf, (int, float)):
                    ai_conf = float(ai_conf)
                else:
                    ai_conf = 0.7
                confidences.append(ai_conf)
                weights.append(0.4)  # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
            elif 'quality_score' in results['advanced_ai']:
                ai_conf = results['advanced_ai']['quality_score']
                if isinstance(ai_conf, torch.Tensor):
                    try:
                        ai_conf = ai_conf.mean().item()
                    except Exception:
                        ai_conf = 0.7
                elif isinstance(ai_conf, (int, float)):
                    ai_conf = float(ai_conf)
                else:
                    ai_conf = 0.7
                confidences.append(ai_conf)
                weights.append(0.4)
        
        # 2. GMM ì‹ ë¢°ë„ (ì•ˆì •ì ì¸ ê¸°í•˜í•™ì  ë§¤ì¹­)
        if 'gmm' in results:
            if 'confidence' in results['gmm']:
                gmm_conf = results['gmm']['confidence']
                if isinstance(gmm_conf, torch.Tensor):
                    try:
                        gmm_conf = gmm_conf.mean().item()
                    except Exception:
                        gmm_conf = 0.85
                elif isinstance(gmm_conf, (int, float)):
                    gmm_conf = float(gmm_conf)
                else:
                    gmm_conf = 0.85
                confidences.append(gmm_conf)
                weights.append(0.3)
            else:
                confidences.append(0.85)  # ê¸°ë³¸ ë†’ì€ ì‹ ë¢°ë„
                weights.append(0.3)
        
        # 3. Optical Flow ì‹ ë¢°ë„ (ë¶€ë“œëŸ¬ìš´ ë³€í˜•)
        if 'optical_flow' in results:
            if 'flow' in results['optical_flow']:
                flow = results['optical_flow']['flow']
                if isinstance(flow, torch.Tensor):
                    try:
                        # Flowì˜ ì¼ê´€ì„±ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                        flow_magnitude = torch.norm(flow, dim=1)
                        flow_consistency = 1.0 / (1.0 + torch.std(flow_magnitude))
                        flow_conf = flow_consistency.mean().item()
                        confidences.append(flow_conf)
                        weights.append(0.2)
                    except Exception:
                        confidences.append(0.75)
                        weights.append(0.2)
                else:
                    confidences.append(0.75)
                    weights.append(0.2)
            else:
                confidences.append(0.75)
                weights.append(0.2)
        
        # 4. Keypoint Matching ì‹ ë¢°ë„ (ì •í™•í•œ íŠ¹ì§•ì  ë§¤ì¹­)
        if 'keypoint' in results:
            if 'keypoint_confidence' in results['keypoint']:
                kpt_conf = results['keypoint']['keypoint_confidence']
                if isinstance(kpt_conf, torch.Tensor):
                    try:
                        kpt_conf = kpt_conf.mean().item()
                    except Exception:
                        kpt_conf = 0.8
                elif isinstance(kpt_conf, (int, float)):
                    kpt_conf = float(kpt_conf)
                else:
                    kpt_conf = 0.8
                confidences.append(kpt_conf)
                weights.append(0.1)
            else:
                confidences.append(0.8)
                weights.append(0.1)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        if confidences and weights:
            total_weight = sum(weights)
            weighted_confidence = sum(c * w for c, w in zip(confidences, weights)) / total_weight
            
            # ì¶”ê°€ ë³´ë„ˆìŠ¤: ì—¬ëŸ¬ ëª¨ë¸ì´ ì„±ê³µí•œ ê²½ìš°
            model_count_bonus = min(len(confidences) * 0.05, 0.15)  # ìµœëŒ€ 15% ë³´ë„ˆìŠ¤
            final_confidence = min(1.0, weighted_confidence + model_count_bonus)
            
            return float(final_confidence)
        
        return 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„

    # _compute_quality_score_advanced ë©”ì„œë“œëŠ” _compute_quality_metricsë¡œ í†µí•©ë¨

    def _get_used_algorithms(self, results: Dict[str, Any]) -> List[str]:
        """ì‚¬ìš©ëœ ì•Œê³ ë¦¬ì¦˜ ëª©ë¡"""
        algorithms = []
        
        if 'advanced_ai' in results:
            algorithms.extend([
                "DeepLabV3+ Backbone",
                "ASPP Multi-scale Context", 
                "Self-Attention Keypoint Matching",
                "Edge-Aware Transformation",
                "Progressive Geometric Refinement"
            ])
        
        if 'gmm' in results:
            algorithms.append("GMM (Geometric Matching Module)")
        
        if 'procrustes_transform' in results:
            algorithms.append("Procrustes Analysis")
        
        if 'keypoint' in results:
            algorithms.append("Keypoint-based Matching")
        
        if 'optical_flow' in results:
            algorithms.append("Optical Flow Calculation")
        
        return algorithms

    def _compute_matching_score(self, results: Dict[str, Any]) -> float:
        """ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = []
            
            # GMM ì ìˆ˜
            if 'gmm' in results:
                scores.append(0.85)  # GMM ê¸°ë³¸ ì ìˆ˜
            
            # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì ìˆ˜
            if 'keypoint' in results:
                match_count = results['keypoint']['match_count']
                confidence = results['keypoint']['keypoint_confidence']
                keypoint_score = (match_count / 20.0) * confidence  # 20ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ì¡°ì •
                scores.append(keypoint_score)
            
            # Optical Flow ì ìˆ˜
            if 'optical_flow' in results:
                scores.append(0.75)  # Flow ê¸°ë³¸ ì ìˆ˜
            
            return float(np.mean(scores)) if scores else 0.8
            
        except Exception as e:
            return 0.8
    
    def _get_fusion_weights(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ìœµí•© ê°€ì¤‘ì¹˜ ê³„ì‚° - ì‹ ë¢°ë„ í–¥ìƒì„ ìœ„í•œ ìµœì í™”ëœ ê°€ì¤‘ì¹˜"""
        weights = {}
        
        # Advanced AIê°€ ê°€ì¥ ì •êµí•˜ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜
        if 'advanced_ai' in results:
            weights['advanced_ai'] = 0.5
        
        # GMMì€ ì•ˆì •ì ì¸ ê¸°í•˜í•™ì  ë§¤ì¹­
        if 'gmm' in results:
            weights['gmm'] = 0.3
        
        # Keypoint Matchingì€ ì •í™•í•œ íŠ¹ì§•ì  ë§¤ì¹­
        if 'keypoint' in results:
            weights['keypoint'] = 0.15
        
        # Optical FlowëŠ” ë¶€ë“œëŸ¬ìš´ ë³€í˜•
        if 'optical_flow' in results:
            weights['optical_flow'] = 0.05
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        
        return weights
    
    def _generate_flow_field_from_grid(self, transformation_grid: torch.Tensor) -> torch.Tensor:
        """ë³€í˜• ê·¸ë¦¬ë“œì—ì„œ flow field ìƒì„±"""
        try:
            batch_size, H, W, _ = transformation_grid.shape
            
            # ê¸°ë³¸ ê·¸ë¦¬ë“œ
            y, x = torch.meshgrid(
                torch.linspace(-1, 1, H, device=transformation_grid.device),
                torch.linspace(-1, 1, W, device=transformation_grid.device),
                indexing='ij'
            )
            base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            # Flow field ê³„ì‚°
            flow = (transformation_grid - base_grid) * torch.tensor([W/2, H/2], device=transformation_grid.device)
            
            return flow.permute(0, 3, 1, 2)  # (B, 2, H, W)
            
        except Exception as e:
            logger.error(f"âŒ Flow field ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 2, 256, 192), device=self.device)
    

    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥ - ì™„ì „ ë²„ì „ìœ¼ë¡œ í†µí•©"""
        # ì™„ì „ ë²„ì „ì˜ ìºì‹œ ì €ì¥ ë¡œì§ ì‚¬ìš©
        try:
            if len(self.matching_cache) >= 100:  # M3 Max ìµœì í™”
                oldest_key = next(iter(self.matching_cache))
                del self.matching_cache[oldest_key]
            
            # í…ì„œëŠ” ìºì‹œì—ì„œ ì œì™¸ (ë©”ëª¨ë¦¬ ì ˆì•½)
            cached_result = result.copy()
            for key in ['warped_clothing', 'transformation_grid', 'flow_field']:
                if key in cached_result:
                    cached_result[key] = None
            
            cached_result['timestamp'] = time.time()
            self.matching_cache[cache_key] = cached_result
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # _update_performance_stats ë©”ì„œë“œëŠ” _update_inference_statistics_completeë¡œ í†µí•©ë¨

    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë° ì •ë³´ ì¡°íšŒ ë©”ì„œë“œë“¤ (v27.1 ì™„ì „ ë³µì›)
    # ==============================================
    
    def get_full_config(self) -> Dict[str, Any]:
        """ì „ì²´ ì„¤ì • ë°˜í™˜"""
        full_config = {}
        if hasattr(self, 'config'):
            if hasattr(self.config, '__dict__'):
                full_config.update(self.config.__dict__)
            else:
                full_config.update(vars(self.config))
        return full_config

    def is_ai_enhanced(self) -> bool:
        """AI ê°•í™” ì—¬ë¶€"""
        return self.advanced_geometric_ai is not None or 'advanced_ai' in self.loaded_models

    def get_algorithm_type(self) -> str:
        """ì•Œê³ ë¦¬ì¦˜ íƒ€ì… ë°˜í™˜"""
        return 'advanced_deeplab_aspp_self_attention'

    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜ (v27.1 ì™„ì „ ë³µì›)"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'version': 'v8.0',
            'initialized': getattr(self, 'is_initialized', False),
            'device': self.device,
            'ai_models_loaded': {
                'gmm_model': self.gmm_model is not None,
                'tps_network': self.tps_network is not None,
                'optical_flow_model': self.optical_flow_model is not None,
                'keypoint_matcher': self.keypoint_matcher is not None,
                'advanced_geometric_ai': self.advanced_geometric_ai is not None
            },
            'model_files_detected': len(getattr(self, 'model_paths', {})),
            'matching_config': self.get_full_config(),
            'performance_stats': self.performance_stats,
            'statistics': self.statistics,
            'algorithms': self.statistics.get('features', []),
            'ai_enhanced': self.is_ai_enhanced(),
            'algorithm_type': self.get_algorithm_type()
        }

    def debug_info(self) -> Dict[str, Any]:
        """ë””ë²„ê¹… ì •ë³´ ë°˜í™˜ (v27.1 ì™„ì „ ë³µì›)"""
        try:
            return {
                'step_info': {
                    'name': self.step_name,
                    'id': self.step_id,
                    'device': self.device,
                    'initialized': getattr(self, 'is_initialized', False),
                    'models_loaded': self.status.models_loaded,
                    'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                    'version': 'v8.0'
                },
                'ai_models': {
                    'gmm_model_loaded': self.gmm_model is not None,
                    'advanced_geometric_ai_loaded': self.advanced_geometric_ai is not None,
                    'geometric_matcher_loaded': self.geometric_matcher is not None,
                    'model_files_detected': len(getattr(self, 'model_paths', {}))
                },
                'config': self.get_full_config(),
                'statistics': self.statistics,
                'performance_stats': self.performance_stats,
                'requirements': {
                    'compatible': self.status.requirements_compatible,
                    'ai_enhanced': True
                },
                'features': self.statistics.get('features', [])
            }
        except Exception as e:
            logger.error(f"âŒ ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜ (v27.1 ì™„ì „ ë³µì›)"""
        try:
            stats = self.statistics.copy()
            
            # ì¶”ê°€ ê³„ì‚°ëœ í†µê³„
            if stats['total_processed'] > 0:
                stats['average_processing_time'] = stats['total_processing_time'] / stats['total_processed']
                stats['success_rate'] = stats['successful_matches'] / stats['total_processed']
            else:
                stats['average_processing_time'] = 0.0
                stats['success_rate'] = 0.0
            
            stats['algorithm_type'] = 'advanced_deeplab_aspp_self_attention'
            stats['version'] = 'v8.0'
            return stats
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def validate_dependencies(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ê²€ì¦ (v27.1 ì™„ì „ ë³µì›)"""
        try:
            return {
                'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'pil_available': PIL_AVAILABLE,
                'numpy_available': NUMPY_AVAILABLE,
                'cv2_available': CV2_AVAILABLE,
                'scipy_available': SCIPY_AVAILABLE
            }
        except Exception as e:
            logger.error(f"âŒ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {}
    
    def health_check(self) -> Dict[str, Any]:
        """ê±´ê°• ìƒíƒœ ì²´í¬ (v27.1 ì™„ì „ ë³µì›)"""
        try:
            health_status = {
                'overall_status': 'healthy',
                'timestamp': time.time(),
                'checks': {}
            }
            
            issues = []
            
            # ì´ˆê¸°í™” ìƒíƒœ ì²´í¬
            if not getattr(self, 'is_initialized', False):
                issues.append('Stepì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ')
                health_status['checks']['initialization'] = 'failed'
            else:
                health_status['checks']['initialization'] = 'passed'
            
            # AI ëª¨ë¸ ë¡œë”© ìƒíƒœ ì²´í¬
            models_loaded = sum([
                self.gmm_model is not None,
                self.tps_network is not None,
                self.optical_flow_model is not None,
                self.keypoint_matcher is not None
            ])
            
            if models_loaded == 0:
                issues.append('AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ')
                health_status['checks']['ai_models'] = 'failed'
            elif models_loaded < 3:
                health_status['checks']['ai_models'] = 'warning'
            else:
                health_status['checks']['ai_models'] = 'passed'
            
            # ì˜ì¡´ì„± ì²´í¬
            deps = self.validate_dependencies()
            essential_deps = ['torch_available', 'pil_available', 'numpy_available']
            missing_deps = [dep for dep in essential_deps if not deps.get(dep, False)]
            
            if missing_deps:
                issues.append(f'í•„ìˆ˜ ì˜ì¡´ì„± ì—†ìŒ: {missing_deps}')
                health_status['checks']['dependencies'] = 'failed'
            else:
                health_status['checks']['dependencies'] = 'passed'
            
            # ë””ë°”ì´ìŠ¤ ìƒíƒœ ì²´í¬
            if self.device == "mps" and not MPS_AVAILABLE:
                issues.append('MPS ë””ë°”ì´ìŠ¤ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ')
                health_status['checks']['device'] = 'warning'
            elif self.device == "cuda" and not torch.cuda.is_available():
                issues.append('CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ')
                health_status['checks']['device'] = 'warning'
            else:
                health_status['checks']['device'] = 'passed'
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            if any(status == 'failed' for status in health_status['checks'].values()):
                health_status['overall_status'] = 'unhealthy'
            elif any(status == 'warning' for status in health_status['checks'].values()):
                health_status['overall_status'] = 'degraded'
            
            if issues:
                health_status['issues'] = issues
            
            return health_status
            
        except Exception as e:
            return {
                'overall_status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }
    
    # ==============================================
    # ğŸ”¥ ì •ë¦¬ ì‘ì—… (v27.1 ì™„ì „ ë³µì›)
    # ==============================================
    
    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            models_to_cleanup = [
                'gmm_model', 'tps_network', 'optical_flow_model', 
                'keypoint_matcher', 'sam_model', 'advanced_geometric_ai'
            ]
            
            for model_name in models_to_cleanup:
                model = getattr(self, model_name, None)
                if model is not None:
                    del model
                    setattr(self, model_name, None)
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'matching_cache'):
                self.matching_cache.clear()
            
            # ê²½ë¡œ ì •ë¦¬
            if hasattr(self, 'model_paths'):
                self.model_paths.clear()
            
            # ë§¤ì²˜ ì •ë¦¬
            if hasattr(self, 'geometric_matcher'):
                del self.geometric_matcher
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except:
                    pass
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            gc.collect()
            
            logger.info("âœ… GeometricMatchingStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤ (v27.1 ì™„ì „ ë³µì›)
    # ==============================================
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        try:
            if getattr(self, 'is_initialized', False):
                return True
            
            logger.info(f"ğŸš€ {self.step_name} v8.0 ì´ˆê¸°í™” ì‹œì‘")
            
            # ğŸ”§ ìˆ˜ì •: status ê°ì²´ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not hasattr(self, 'status'):
                self.status = ProcessingStatus()
            
            # M3 Max ìµœì í™” ì ìš©
            if self.device == "mps" or IS_M3_MAX:
                self._apply_m3_max_optimization()
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì¶”ê°€
            logger.info("ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            models_loaded = self._load_geometric_matching_models_via_central_hub()
            logger.info(f"ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ê²°ê³¼: {models_loaded}")
            
            self.is_initialized = True
            self.is_ready = True
            self.status.initialization_complete = True  # ì´ì œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼ ê°€ëŠ¥
            
            logger.info(f"âœ… {self.step_name} v8.0 ì´ˆê¸°í™” ì™„ë£Œ (ë¡œë”©ëœ ëª¨ë¸: {len(self.loaded_models)}ê°œ)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} v8.0 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _apply_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì ìš© (v27.1 ì™„ì „ ë³µì›)"""
        try:
            # MPS ìºì‹œ ì •ë¦¬
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception:
                    pass
            
            # í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['TORCH_MPS_PREFER_METAL'] = '1'
            
            if IS_M3_MAX:
                # M3 Max íŠ¹í™” ì„¤ì •
                if hasattr(self, 'config'):
                    if hasattr(self.config, 'input_size'):
                        pass  # í¬ê¸° ìœ ì§€
                
            logger.debug("âœ… M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")

    def _create_identity_grid(self, batch_size: int, H: int, W: int) -> torch.Tensor:
        """Identity ê·¸ë¦¬ë“œ ìƒì„± (MPS float32 í˜¸í™˜ì„±)"""
        # ğŸ”¥ MPS í˜¸í™˜ì„±ì„ ìœ„í•œ float32 dtype ëª…ì‹œ
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, H, dtype=torch.float32, device=self.device),
            torch.linspace(-1, 1, W, dtype=torch.float32, device=self.device),
            indexing='ij'
        )
        grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        # ğŸ”¥ MPS í˜¸í™˜ì„±ì„ ìœ„í•œ float32 ê°•ì œ ë³€í™˜
        if grid.dtype != torch.float32:
            grid = grid.to(torch.float32)
        return grid

    def _preprocess_image(self, image) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            if PIL_AVAILABLE and hasattr(image, 'convert'):
                image_pil = image.convert('RGB')
                image_array = np.array(image_pil)
            elif isinstance(image, np.ndarray):
                image_array = image
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # í¬ê¸° ì¡°ì •
            target_size = self.config.input_size
            if PIL_AVAILABLE:
                image_pil = Image.fromarray(image_array)
                image_resized = image_pil.resize(target_size, Image.Resampling.LANCZOS)
                image_array = np.array(image_resized)
            
            # ì •ê·œí™” (0-255 ë²”ìœ„ í™•ì¸)
            if image_array.max() <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            
            return image_array
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return np.zeros((*self.config.input_size, 3), dtype=np.uint8)

    def _get_step_requirements(self) -> Dict[str, Any]:
        """Step 04 GeometricMatching ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
        return {
            "required_models": [
                "gmm_final.pth",
                "tps_network.pth", 
                "sam_vit_h_4b8939.pth",
                "resnet101_geometric.pth"
            ],
            "primary_model": "gmm_final.pth",
            "model_configs": {
                "gmm_final.pth": {
                    "size_mb": 44.7,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "precision": "high"
                },
                "tps_network.pth": {
                    "size_mb": 527.8,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "real_time": False
                },
                "sam_vit_h_4b8939.pth": {
                    "size_mb": 2445.7,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "shared_with": ["step_03_cloth_segmentation"]
                },

                "resnet101_geometric.pth": {
                    "size_mb": 170.5,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "backbone": True
                }
            },
            "verified_paths": [
                "step_04_geometric_matching/gmm_final.pth",
                "step_04_geometric_matching/tps_network.pth", 
                "step_04_geometric_matching/ultra_models/resnet101_geometric.pth",
                "step_03_cloth_segmentation/sam_vit_h_4b8939.pth"
            ]
        }

    def get_matching_algorithms_info(self) -> Dict[str, str]:
        """ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ë°˜í™˜"""
        return MATCHING_ALGORITHMS.copy()

    def get_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.loaded_models.copy()

    def get_model_loading_status(self) -> Dict[str, bool]:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ ë°˜í™˜"""
        return self.models_loading_status.copy()

    def validate_matching_result(self, result: Dict[str, Any]) -> bool:
        """ë§¤ì¹­ ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦"""
        try:
            required_keys = ['transformation_matrix', 'transformation_grid', 'warped_clothing']
            
            for key in required_keys:
                if key not in result:
                    return False
                
                if result[key] is None:
                    return False
            
            # ë³€í˜• í–‰ë ¬ ê²€ì¦
            transform_matrix = result['transformation_matrix']
            if isinstance(transform_matrix, np.ndarray):
                if transform_matrix.shape not in [(2, 3), (3, 3)]:
                    return False
            
            return True
            
        except Exception:
            return False

    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except:
                    pass
            
            self.ai_models.clear()
            self.loaded_models.clear()
            self.matching_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif TORCH_AVAILABLE and MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            
            logger.info("âœ… GeometricMatchingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
        """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not isinstance(step_output, dict):
                logger.warning(f"âš ï¸ step_outputì´ dictê°€ ì•„ë‹˜: {type(step_output)}")
                return {
                    'success': False,
                    'error': f'Invalid output type: {type(step_output)}',
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
            
            # ê¸°ë³¸ API ì‘ë‹µ êµ¬ì¡°
            api_response = {
                'success': step_output.get('success', True),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0),
                'timestamp': time.time()
            }
            
            # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
            if not api_response['success']:
                api_response['error'] = step_output.get('error', 'Unknown error')
                return api_response
            
            # ê¸°í•˜í•™ì  ë§¤ì¹­ ê²°ê³¼ ë³€í™˜
            if 'matching_result' in step_output:
                matching_result = step_output['matching_result']
                api_response['geometric_data'] = {
                    'transformation_matrix': matching_result.get('transformation_matrix', []),
                    'confidence_score': matching_result.get('confidence_score', 0.0),
                    'quality_score': matching_result.get('quality_score', 0.0),
                    'matching_score': matching_result.get('matching_score', 0.0),
                    'used_algorithms': matching_result.get('used_algorithms', []),
                    'keypoints_matched': matching_result.get('keypoints_matched', 0),
                    'flow_field': matching_result.get('flow_field', []),
                    'transformation_grid': matching_result.get('transformation_grid', [])
                }
            
            # í…ì„œ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜
            for key, value in step_output.items():
                if isinstance(value, torch.Tensor):
                    try:
                        # ğŸ”¥ í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                        if value.dim() == 4:  # (B, C, H, W) í˜•íƒœ
                            value = value.squeeze(0)  # (C, H, W)
                        if value.dim() == 3:  # (C, H, W) í˜•íƒœ
                            value = value.permute(1, 2, 0)  # (H, W, C)
                        elif value.dim() == 2:  # (H, W) í˜•íƒœ
                            value = value.unsqueeze(-1)  # (H, W, 1)
                        elif value.dim() == 1:  # (N,) í˜•íƒœ
                            value = value.unsqueeze(0).unsqueeze(0)  # (1, 1, N)
                        
                        # CPUë¡œ ì´ë™ í›„ numpyë¡œ ë³€í™˜
                        value = value.cpu().numpy()
                        
                        # numpy ë°°ì—´ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                        if value.dtype.kind in 'fc':  # float/complex
                            value = value.astype(float)
                        step_output[key] = value.tolist()
                        
                    except Exception as tensor_error:
                        logger.warning(f"âš ï¸ í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {tensor_error}")
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •
                        step_output[key] = None
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            api_response['metadata'] = {
                'models_available': list(self.ai_models.keys()) if hasattr(self, 'ai_models') else [],
                'device_used': getattr(self, 'device', 'unknown'),
                'input_size': step_output.get('input_size', [0, 0]),
                'output_size': step_output.get('output_size', [0, 0]),
                'matching_ready': getattr(self, 'matching_ready', False)
            }
            
            # ì‹œê°í™” ë°ì´í„° (ìˆëŠ” ê²½ìš°)
            if 'visualization' in step_output:
                api_response['visualization'] = step_output['visualization']
            
            # ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
            if 'analysis' in step_output:
                api_response['analysis'] = step_output['analysis']
            
            logger.info(f"âœ… GeometricMatchingStep ì¶œë ¥ ë³€í™˜ ì™„ë£Œ: {len(api_response)}ê°œ í‚¤")
            return api_response
            
        except Exception as e:
            logger.error(f"âŒ GeometricMatchingStep ì¶œë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': f'Output conversion failed: {str(e)}',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0) if isinstance(step_output, dict) else 0.0
            }

    def _convert_api_input_type(self, value: Any, api_type: str, param_name: str) -> Any:
        """API ì…ë ¥ íƒ€ì… ë³€í™˜ (ì™„ì „ ë™ê¸° ë²„ì „)"""
        try:
            # BaseStepMixinì˜ ë™ê¸° ë²„ì „ í˜¸ì¶œ ì‹œë„
            if hasattr(self, '_convert_api_input_type_sync'):
                return self._convert_api_input_type_sync(value, api_type, param_name)
        except Exception:
            pass
        
        # ê¸°ë³¸ ë³€í™˜ ë¡œì§
        try:
            if api_type == "image":
                if isinstance(value, str):
                    # Base64 ë¬¸ìì—´ì„ PIL Imageë¡œ ë³€í™˜
                    import base64
                    from PIL import Image
                    from io import BytesIO
                    try:
                        image_data = base64.b64decode(value)
                        return Image.open(BytesIO(image_data))
                    except Exception as e:
                        logger.warning(f"âš ï¸ Base64 ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
                        return value
                elif hasattr(value, 'shape') and len(value.shape) == 4:
                    # í…ì„œ í˜•íƒœ (1, 3, H, W)ë¥¼ PIL Imageë¡œ ë³€í™˜
                    try:
                        import torch
                        if isinstance(value, torch.Tensor):
                            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
                            if value.dim() == 4:
                                value = value.squeeze(0)  # (3, H, W)
                            if value.dim() == 3:
                                # (C, H, W) -> (H, W, C)
                                value = value.permute(1, 2, 0)
                            value = value.cpu().numpy()
                        
                        # numpy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜
                        if value.dtype != np.uint8:
                            value = (value * 255).astype(np.uint8)
                        
                        from PIL import Image
                        return Image.fromarray(value)
                    except Exception as e:
                        logger.warning(f"âš ï¸ í…ì„œ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
                        return value
                return value
            elif api_type == "tensor":
                if hasattr(value, 'numpy'):
                    return value.numpy()
                elif hasattr(value, 'tolist'):
                    return value.tolist()
                return value
            else:
                return value
        except Exception as e:
            logger.warning(f"âš ï¸ API ì…ë ¥ íƒ€ì… ë³€í™˜ ì‹¤íŒ¨ ({api_type}): {e}")
            return value

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ Geometric Matching AI ì¶”ë¡  (BaseStepMixin v20.0 í˜¸í™˜)"""
        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 _run_ai_inference ì‹œì‘")
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì…ë ¥ ë°ì´í„° ê²€ì¦")
            if not processed_input:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
                return {'success': False, 'error': 'ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'}
            
            # ğŸ”¥ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (processì—ì„œ ì´ë¯¸ ê²€ì¦ë¨)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - processed_input í‚¤ë“¤: {list(processed_input.keys())}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - processed_input ê°’ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            person_image = processed_input.get('person_image')
            clothing_image = processed_input.get('clothing_image')
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_image ì¡´ì¬: {person_image is not None}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_image ì¡´ì¬: {clothing_image is not None}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_image íƒ€ì…: {type(person_image)}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_image íƒ€ì…: {type(clothing_image)}")
            
            # ğŸ”¥ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì„¸ì…˜ì—ì„œ ë‹¤ì‹œ ë¡œë“œ
            if person_image is None or clothing_image is None:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ë¯¸ì§€ê°€ ì—†ì–´ì„œ ì„¸ì…˜ì—ì„œ ë‹¤ì‹œ ë¡œë“œ")
                person_image, clothing_image, session_data = self._validate_and_extract_inputs(processed_input)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì¬ë¡œë“œ í›„ person_image ì¡´ì¬: {person_image is not None}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì¬ë¡œë“œ í›„ clothing_image ì¡´ì¬: {clothing_image is not None}")
            
            if person_image is None or clothing_image is None:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŒ")
                return {'success': False, 'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # í…ì„œ ë³€í™˜
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - í…ì„œ ë³€í™˜ ì‹œì‘")
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_image í…ì„œ ë³€í™˜")
                person_tensor = self._prepare_image_tensor_complete(person_image)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_tensor íƒ€ì…: {type(person_tensor)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_tensor shape: {getattr(person_tensor, 'shape', 'N/A')}")
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_image í…ì„œ ë³€í™˜")
                clothing_tensor = self._prepare_image_tensor_complete(clothing_image)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_tensor íƒ€ì…: {type(clothing_tensor)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_tensor shape: {getattr(clothing_tensor, 'shape', 'N/A')}")
                
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f'ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}'}
            
            # ğŸ”¥ ì´ì „ Step ê²°ê³¼ ì¶”ì¶œ (Pipeline Managerì—ì„œ ì „ë‹¬ëœ ë°ì´í„°)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ì „ Step ê²°ê³¼ ì¶”ì¶œ")
            
            # Step 1 ê²°ê³¼ (Human Parsing)
            person_parsing_data = processed_input.get('person_parsing', {})
            if not person_parsing_data:
                person_parsing_data = processed_input.get('parsing_result', {})
            if not person_parsing_data:
                person_parsing_data = processed_input.get('person_mask', {})
            
            # Step 2 ê²°ê³¼ (Pose Estimation)
            pose_data = processed_input.get('pose_keypoints', [])
            if not pose_data:
                pose_data = processed_input.get('keypoints', [])
            if not pose_data:
                pose_data = processed_input.get('pose_data', [])
            
            # Step 3 ê²°ê³¼ (Cloth Segmentation)
            clothing_segmentation_data = processed_input.get('clothing_segmentation', {})
            if not clothing_segmentation_data:
                clothing_segmentation_data = processed_input.get('cloth_mask', {})
            if not clothing_segmentation_data:
                clothing_segmentation_data = processed_input.get('segmented_clothing', {})
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_parsing_data ì¡´ì¬: {bool(person_parsing_data)}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - pose_data ê°œìˆ˜: {len(pose_data)}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_segmentation_data ì¡´ì¬: {bool(clothing_segmentation_data)}")
            
            # ğŸ”¥ ì´ì „ Step ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if not person_parsing_data:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_parsing_dataê°€ ì—†ì–´ì„œ ê¸°ë³¸ê°’ ìƒì„±")
                person_parsing_data = {
                    'parsing_map': np.ones((256, 192), dtype=np.uint8) * 255,
                    'confidence': 0.5
                }
            
            if not pose_data:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - pose_dataê°€ ì—†ì–´ì„œ ê¸°ë³¸ê°’ ìƒì„±")
                pose_data = [
                    {'x': 128, 'y': 96, 'confidence': 0.5, 'part': 'nose'},
                    {'x': 100, 'y': 120, 'confidence': 0.5, 'part': 'left_shoulder'},
                    {'x': 156, 'y': 120, 'confidence': 0.5, 'part': 'right_shoulder'}
                ]
            
            if not clothing_segmentation_data:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_segmentation_dataê°€ ì—†ì–´ì„œ ê¸°ë³¸ê°’ ìƒì„±")
                clothing_segmentation_data = {
                    'cloth_mask': np.ones((256, 192), dtype=np.uint8) * 255,
                    'confidence': 0.5
                }
            
            # AI ëª¨ë¸ ì‹¤í–‰
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ëª¨ë¸ ì‹¤í–‰ ì‹œì‘")
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - _execute_all_ai_models í˜¸ì¶œ (ì´ì „ Step ê²°ê³¼ í¬í•¨)")
                results = self._execute_all_ai_models(
                    person_tensor, 
                    clothing_tensor, 
                    person_parsing_data=person_parsing_data,
                    pose_data=pose_data,
                    clothing_segmentation_data=clothing_segmentation_data,
                    force_ai_processing=True
                )
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ íƒ€ì…: {type(results)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ í‚¤ë“¤: {list(results.keys()) if isinstance(results, dict) else 'Not a dict'}")
                
                # ğŸ”¥ AI ê²°ê³¼ ê²€ì¦ ì¶”ê°€
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - AI ê²°ê³¼ ìƒì„¸ ê²€ì¦:")
                for model_name, result in results.items():
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - {model_name}: {type(result).__name__}")
                    if isinstance(result, dict):
                        print(f"ğŸ”¥ [ë””ë²„ê¹…]   - í‚¤ë“¤: {list(result.keys())}")
                    elif hasattr(result, 'shape'):
                        print(f"ğŸ”¥ [ë””ë²„ê¹…]   - shape: {result.shape}")
                
                # ğŸ”¥ ìµœì†Œí•œ í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
                successful_models = [name for name, result in results.items() if result is not None]
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„±ê³µí•œ ëª¨ë¸ë“¤: {successful_models}")
                
                if not successful_models:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ëª¨ë“  AI ëª¨ë¸ ì‹¤íŒ¨")
                    return {'success': False, 'error': 'ëª¨ë“  AI ëª¨ë¸ ì‹¤íŒ¨'}
                
                # ê²°ê³¼ ìœµí•© ë° í›„ì²˜ë¦¬
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ê²°ê³¼ ìœµí•© ë° í›„ì²˜ë¦¬ ì‹œì‘")
                final_result = self._fuse_and_postprocess_results(results, person_tensor, clothing_tensor)
                
                return {
                    'success': True,
                    'result': final_result,
                    'processing_time': results.get('processing_time', 0.0),
                    'models_used': results.get('models_used', []),
                    'confidence': final_result.get('confidence', 0.0)
                }
                
            except Exception as e:
                return {'success': False, 'error': f'AI ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}'}
                
        except Exception as e:
            return {'success': False, 'error': f'AI ì¶”ë¡  ì‹¤íŒ¨: {e}'}

    def _run_ai_inference_complete(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì™„ì „í•œ Geometric Matching AI ì¶”ë¡  ë¡œì§ (ê¸°ë³¸ ë²„ì „ ê¸°ëŠ¥ í†µí•©)"""
        import time
        
        logger.info("ğŸš€ ì™„ì „í•œ Geometric Matching AI ì¶”ë¡  ì‹œì‘")
        logger.info(f"ğŸ”¥ [Step 4] ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(kwargs.keys())}")
        logger.info(f"ğŸ”¥ [Step 4] ì…ë ¥ ë°ì´í„° íƒ€ì…ë“¤: {[(k, type(v).__name__) for k, v in kwargs.items()]}")

        try:
            start_time = time.time()
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
            person_image, clothing_image, session_data = self._validate_and_extract_inputs(kwargs)
            
            if person_image is None or clothing_image is None:
                return self._create_result("error", error_msg="ì…ë ¥ ì´ë¯¸ì§€ ëˆ„ë½", processing_time=start_time)            
            # 2. ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ (ê¸°ë³¸ ë²„ì „ì˜ ìƒì„¸í•œ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
            try:
                person_tensor = self._prepare_image_tensor_complete(person_image)
                clothing_tensor = self._prepare_image_tensor_complete(clothing_image)
                
                if person_tensor is None or clothing_tensor is None:
                    return self._create_result("error", error_msg="ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨", processing_time=start_time)                    
                logger.info(f"âœ… ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì™„ë£Œ: person={person_tensor.shape}, clothing={clothing_tensor.shape}")
                
            except Exception as tensor_error:
                logger.error(f"âŒ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {tensor_error}")
                return self._create_result("error", error_msg=f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {str(tensor_error)}", processing_time=start_time)

            # 3. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key_complete(person_tensor, clothing_tensor)
            cached_result = self._check_cache(cache_key)
            if cached_result:
                logger.info("ğŸ¯ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                return cached_result
            
            # 4. AI ëª¨ë¸ë“¤ ì‹¤í–‰ (ê¸°ë³¸ ë²„ì „ì˜ force_ai_processing í”Œë˜ê·¸ ì¶”ê°€)
            try:
                # force_ai_processing í”Œë˜ê·¸ ì¶”ì¶œ (ê¸°ë³¸ ë²„ì „ì—ì„œ ì¶”ê°€)
                force_ai_processing = kwargs.get('force_ai_processing', False)
                logger.info("ğŸ”¥ [ë””ë²„ê¹…] _execute_all_ai_models í˜¸ì¶œ ì‹œì‘!")
                print("ğŸ”¥ [ë””ë²„ê¹…] _execute_all_ai_models í˜¸ì¶œ ì‹œì‘!")
                inference_results = self._execute_all_ai_models(person_tensor, clothing_tensor, force_ai_processing)
                logger.info("ğŸ”¥ [ë””ë²„ê¹…] _execute_all_ai_models í˜¸ì¶œ ì™„ë£Œ!")
                print("ğŸ”¥ [ë””ë²„ê¹…] _execute_all_ai_models í˜¸ì¶œ ì™„ë£Œ!")
                
            except Exception as inference_error:
                logger.error(f"âŒ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                # ì—ëŸ¬ ê²°ê³¼ë¡œ í´ë°±
                logger.warning("âš ï¸ ì—ëŸ¬ ê²°ê³¼ë¡œ í´ë°±")
                inference_results = {
                    'gmm': {'transformation_matrix': torch.eye(3, device=self.device), 'confidence': 0.0, 'method': 'error'},
                    'tps': {'control_points': torch.randn(1, 18, 2, device=self.device), 'confidence': 0.0, 'method': 'error'},
                    'optical_flow': {'flow_field': torch.randn(1, 2, 256, 192, device=self.device), 'confidence': 0.0, 'method': 'error'},
                    'keypoint_matching': {'keypoints': torch.randn(1, 18, 2, device=self.device), 'confidence': 0.0, 'method': 'error'},
                    'advanced_ai': {'transformation_matrix': torch.eye(3, device=self.device), 'confidence': 0.0, 'method': 'error'}
                }
            
            # 5. ê²°ê³¼ ìœµí•© ë° í›„ì²˜ë¦¬ (ê¸°ë³¸ ë²„ì „ì˜ ìƒì„¸í•œ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
            try:
                final_result = self._fuse_and_postprocess_results(inference_results, person_tensor, clothing_tensor)
                logger.info("âœ… ê²°ê³¼ ìœµí•© ë° í›„ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as fusion_error:
                logger.error(f"âŒ ê²°ê³¼ ìœµí•© ì‹¤íŒ¨: {fusion_error}")
                return self._create_result("error", error_msg=f"ê²°ê³¼ ìœµí•© ì‹¤íŒ¨: {str(fusion_error)}", processing_time=start_time)            
            # 6. í’ˆì§ˆ í‰ê°€ ë° ë©”íŠ¸ë¦­ ê³„ì‚° (ê¸°ë³¸ ë²„ì „ì˜ ìƒì„¸í•œ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
            try:
                quality_metrics = self._compute_quality_metrics(final_result, inference_results)
                final_result.update(quality_metrics)
                logger.info("âœ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
                
            except Exception as quality_error:
                logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {quality_error}")
            
            # 7. ìµœì¢… ê²°ê³¼ êµ¬ì„± (ê¸°ë³¸ ë²„ì „ì˜ ì¶”ê°€ í•„ë“œë“¤ í†µí•©)
            processing_time = time.time() - start_time
            final_result.update({
                'success': True,
                'processing_time': processing_time,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'real_ai_inference': True,
                'cache_hit': False,
                'ai_enhanced': True,  # ê¸°ë³¸ ë²„ì „ì—ì„œ ì¶”ê°€
                'device': self.device,  # ê¸°ë³¸ ë²„ì „ì—ì„œ ì¶”ê°€
                'version': 'v8.0'
            })
            
            # 8. ìºì‹œ ì €ì¥ ë° í†µê³„ ì—…ë°ì´íŠ¸ (ê¸°ë³¸ ë²„ì „ì˜ ìƒì„¸í•œ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
            try:
                self._save_to_cache(cache_key, final_result)
                self._update_inference_statistics_complete(processing_time, True, final_result)
            except Exception as stats_error:
                logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats_error}")
            
            logger.info(f"ğŸ‰ ì™„ì „í•œ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - ì‹œê°„: {processing_time:.3f}ì´ˆ, ì‹ ë¢°ë„: {final_result.get('confidence', 0):.3f}")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ ì™„ì „í•œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            return self._create_result("error", error_msg=f"AI ì¶”ë¡  ì‹¤íŒ¨: {str(e)}", processing_time=processing_time)

    def _validate_and_extract_inputs(self, kwargs: Dict[str, Any]) -> tuple:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì¶”ì¶œ - ì´ì „ Step ê²°ê³¼ í¬í•¨"""
        person_image = None
        clothing_image = None
        session_data = {}
        
        # ğŸ”¥ ì´ì „ Step ê²°ê³¼ ì¶”ì¶œ (Pipeline Managerì—ì„œ ì „ë‹¬ëœ ë°ì´í„°)
        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ì „ Step ê²°ê³¼ ì¶”ì¶œ")
        
        # Step 1 ê²°ê³¼ (Human Parsing)
        person_parsing_data = kwargs.get('person_parsing', {})
        if not person_parsing_data:
            person_parsing_data = kwargs.get('parsing_mask', {})
        if not person_parsing_data:
            person_parsing_data = kwargs.get('body_segments', {})
        
        # Step 2 ê²°ê³¼ (Pose Estimation)
        pose_data = kwargs.get('pose_keypoints', [])
        if not pose_data:
            pose_data = kwargs.get('keypoints_18', [])
        if not pose_data:
            pose_data = kwargs.get('pose_data', [])
        
        # Step 3 ê²°ê³¼ (Cloth Segmentation)
        clothing_segmentation_data = kwargs.get('clothing_segmentation', {})
        if not clothing_segmentation_data:
            clothing_segmentation_data = kwargs.get('cloth_mask', {})
        if not clothing_segmentation_data:
            clothing_segmentation_data = kwargs.get('segmentation_result', {})
        
        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ì „ Step ê²°ê³¼ í™•ì¸:")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - person_parsing ì¡´ì¬: {bool(person_parsing_data)}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - pose_keypoints ê°œìˆ˜: {len(pose_data)}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - clothing_segmentation ì¡´ì¬: {bool(clothing_segmentation_data)}")
        
        # ğŸ”¥ Pipeline Managerì—ì„œ ì „ë‹¬ëœ ë°ì´í„° í™•ì¸
        if hasattr(self, 'pipeline_result') and self.pipeline_result:
            try:
                # Step 1 ë°ì´í„° í™•ì¸
                step_1_data = self.pipeline_result.get_data_for_step(1)
                if step_1_data and not person_parsing_data:
                    person_parsing_data = step_1_data.get('parsing_mask', step_1_data.get('person_parsing', {}))
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipelineì—ì„œ Step 1 ë°ì´í„° ì¶”ì¶œ")
                
                # Step 2 ë°ì´í„° í™•ì¸
                step_2_data = self.pipeline_result.get_data_for_step(2)
                if step_2_data and not pose_data:
                    pose_data = step_2_data.get('keypoints_18', step_2_data.get('pose_keypoints', []))
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipelineì—ì„œ Step 2 ë°ì´í„° ì¶”ì¶œ")
                
                # Step 3 ë°ì´í„° í™•ì¸
                step_3_data = self.pipeline_result.get_data_for_step(3)
                if step_3_data and not clothing_segmentation_data:
                    clothing_segmentation_data = step_3_data.get('cloth_mask', step_3_data.get('clothing_segmentation', {}))
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipelineì—ì„œ Step 3 ë°ì´í„° ì¶”ì¶œ")
                    
            except Exception as pipeline_error:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - Pipeline ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {pipeline_error}")
        
        # ğŸ”¥ ìµœì¢… ë°ì´í„° ìƒíƒœ ë¡œê¹…
        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ìµœì¢… ì´ì „ Step ë°ì´í„° ìƒíƒœ:")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - person_parsing_data í‚¤ë“¤: {list(person_parsing_data.keys()) if isinstance(person_parsing_data, dict) else 'Not a dict'}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - pose_data íƒ€ì…: {type(pose_data)}, ê¸¸ì´: {len(pose_data) if isinstance(pose_data, (list, tuple)) else 'N/A'}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - clothing_segmentation_data í‚¤ë“¤: {list(clothing_segmentation_data.keys()) if isinstance(clothing_segmentation_data, dict) else 'Not a dict'}")
        
        # ì§ì ‘ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        for key in ['person_image', 'image', 'input_image', 'original_image']:
            if key in kwargs and kwargs[key] is not None:
                person_image = kwargs[key]
                break
        
        for key in ['clothing_image', 'cloth_image', 'target_image', 'garment_image']:
            if key in kwargs and kwargs[key] is not None:
                clothing_image = kwargs[key]
                break
        
        # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
        if (person_image is None or clothing_image is None) and 'session_id' in kwargs:
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„: {kwargs['session_id']}")
                session_manager = self._get_service_from_central_hub('session_manager')
                if session_manager and hasattr(session_manager, 'get_session_images_sync'):
                    session_person, session_clothing = session_manager.get_session_images_sync(kwargs['session_id'])
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€:")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - session_person íƒ€ì…: {type(session_person)}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - session_clothing íƒ€ì…: {type(session_clothing)}")
                    
                    if person_image is None and session_person is not None:
                        person_image = session_person
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - person_imageë¥¼ ì„¸ì…˜ì—ì„œ ë¡œë“œ")
                    
                    if clothing_image is None and session_clothing is not None:
                        clothing_image = session_clothing
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - clothing_imageë¥¼ ì„¸ì…˜ì—ì„œ ë¡œë“œ")
                    
                    # ì„¸ì…˜ ë°ì´í„°ë„ ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                    try:
                        session_data = session_manager.get_session_status(kwargs['session_id']) or {}
                        
                        # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ì•ˆì „í•œ ê¸¸ì´ í™•ì¸
                        if hasattr(session_data, '__len__'):
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(session_data)}ê°œ í‚¤")
                        else:
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ê¸¸ì´ í™•ì¸ ë¶ˆê°€)")
                        
                        # ğŸ”¥ ì„¸ì…˜ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                        if isinstance(session_data, dict):
                            # ğŸ”¥ ì„¸ì…˜ì—ì„œ ì´ì „ Step ê²°ê³¼ ì¶”ì¶œ
                            if not person_parsing_data and 'step_1_result' in session_data:
                                person_parsing_data = session_data['step_1_result']
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ì—ì„œ Step 1 ê²°ê³¼ ì¶”ì¶œ")
                            
                            if not pose_data and 'step_2_result' in session_data:
                                pose_data = session_data['step_2_result'].get('keypoints_18', [])
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ì—ì„œ Step 2 ê²°ê³¼ ì¶”ì¶œ")
                            
                            if not clothing_segmentation_data and 'step_3_result' in session_data:
                                clothing_segmentation_data = session_data['step_3_result']
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ì—ì„œ Step 3 ê²°ê³¼ ì¶”ì¶œ")
                        else:
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(session_data)}")
                            
                    except Exception as session_data_error:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì„¸ì…˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {session_data_error}")
                        session_data = {}
                        
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Step 4 - ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ğŸ”¥ ìµœì¢… ê²€ì¦ ë¡œê¹…
        print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ìµœì¢… ì´ë¯¸ì§€ ìƒíƒœ:")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - person_image ì¡´ì¬: {person_image is not None}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - clothing_image ì¡´ì¬: {clothing_image is not None}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - person_parsing ë°ì´í„° ì¡´ì¬: {bool(person_parsing_data)}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - pose_keypoints ê°œìˆ˜: {len(pose_data)}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] - clothing_segmentation ë°ì´í„° ì¡´ì¬: {bool(clothing_segmentation_data)}")
        
        if person_image is not None:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] - person_image íƒ€ì…: {type(person_image)}")
        if clothing_image is not None:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] - clothing_image íƒ€ì…: {type(clothing_image)}")
        
        return person_image, clothing_image, session_data

    def _prepare_image_tensor_complete(self, image: Any) -> torch.Tensor:
        """ì™„ì „í•œ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜"""
        try:
            # PIL Image ì²˜ë¦¬
            if hasattr(image, 'convert'):  # PIL Image
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                image_array = np.array(image).astype(np.float32) / 255.0
                if len(image_array.shape) == 3:
                    image_array = np.transpose(image_array, (2, 0, 1))
                tensor = torch.from_numpy(image_array).unsqueeze(0)
            
            # NumPy ë°°ì—´ ì²˜ë¦¬
            elif isinstance(image, np.ndarray):
                image_array = image.astype(np.float32)
                if image_array.max() > 1.0:
                    image_array = image_array / 255.0
                if len(image_array.shape) == 3 and image_array.shape[2] == 3:
                    image_array = np.transpose(image_array, (2, 0, 1))
                tensor = torch.from_numpy(image_array).unsqueeze(0)
            
            # PyTorch í…ì„œ ì²˜ë¦¬
            elif torch.is_tensor(image):
                tensor = image.clone()
                if tensor.dim() == 3:
                    tensor = tensor.unsqueeze(0)
            
            # Base64 ë¬¸ìì—´ ì²˜ë¦¬
            elif isinstance(image, str):
                import base64
                from io import BytesIO
                image_data = base64.b64decode(image)
                pil_image = Image.open(BytesIO(image_data))
                return self._prepare_image_tensor_complete(pil_image)
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            tensor = tensor.to(self.device)
            
            # í¬ê¸° ì¡°ì •
            target_size = (256, 192)  # H, W
            if tensor.shape[-2:] != target_size:
                tensor = F.interpolate(tensor, size=target_size, mode='bilinear', align_corners=False)
            
            # ì±„ë„ í™•ì¸
            if tensor.shape[1] == 1:  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                tensor = tensor.repeat(1, 3, 1, 1)
            elif tensor.shape[1] > 3:  # 4ì±„ë„ ì´ìƒ
                tensor = tensor[:, :3]
            
            return tensor
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ í…ì„œ ë°˜í™˜
            return torch.zeros((1, 3, 256, 192), device=self.device)

    def _execute_all_ai_models(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor, 
                              person_parsing_data: Dict = None, pose_data: List = None, 
                              clothing_segmentation_data: Dict = None, force_ai_processing: bool = True) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ - ì´ì „ Step ê²°ê³¼ë¥¼ í™œìš©í•œ ì™„ì „í•œ ì¶”ë¡  ìˆ˜í–‰"""
        results = {}
        
        try:
            logger.info("ï¿½ï¿½ ï¿½ï¿½ ğŸ”¥ _execute_all_ai_models í˜¸ì¶œ ì‹œì‘!")
            print("ï¿½ï¿½ ï¿½ï¿½ ğŸ”¥ _execute_all_ai_models í˜¸ì¶œ ì‹œì‘!")
            
            # ğŸ”¥ ëª¨ë¸ ìƒíƒœ í™•ì¸
            logger.info(f"ğŸ” GMM ëª¨ë¸ ì¡´ì¬: {'gmm_model' in self.ai_models}")
            logger.info(f"ğŸ” TPS ëª¨ë¸ ì¡´ì¬: {hasattr(self, 'tps_model')}")
            logger.info(f"ï¿½ï¿½ Optical Flow ëª¨ë¸ ì¡´ì¬: {hasattr(self, 'optical_flow_model')}")
            logger.info(f"ğŸ” Keypoint Matcher ì¡´ì¬: {hasattr(self, 'keypoint_matcher')}")
            logger.info(f"ğŸ” Advanced AI ì¡´ì¬: {hasattr(self, 'advanced_geometric_ai')}")
            
            print(f"ğŸ” GMM ëª¨ë¸ ì¡´ì¬: {'gmm_model' in self.ai_models}")
            print(f"ğŸ” TPS ëª¨ë¸ ì¡´ì¬: {hasattr(self, 'tps_model')}")
            print(f"ï¿½ï¿½ Optical Flow ëª¨ë¸ ì¡´ì¬: {hasattr(self, 'optical_flow_model')}")
            print(f"ğŸ” Keypoint Matcher ì¡´ì¬: {hasattr(self, 'keypoint_matcher')}")
            print(f"ğŸ” Advanced AI ì¡´ì¬: {hasattr(self, 'advanced_geometric_ai')}")
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                
                # ğŸ”¥ ì´ì „ Step ê²°ê³¼ë¥¼ í™œìš©í•œ í–¥ìƒëœ ë§¤ì¹­
                if person_parsing_data and pose_data and clothing_segmentation_data:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Step 4 - ì´ì „ Step ê²°ê³¼ë¥¼ í™œìš©í•œ í–¥ìƒëœ ë§¤ì¹­ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - ì¸ì²´ íŒŒì‹± ê²°ê³¼ í™œìš©: {bool(person_parsing_data.get('result'))}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ í™œìš©: {len(pose_data)}ê°œ")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] - ì˜ë¥˜ ë¶„í•  ê²°ê³¼ í™œìš©: {bool(clothing_segmentation_data.get('clothing_mask'))}")
                
                # 1. Advanced AI ëª¨ë¸ ì‹¤í–‰ (ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë¨¼ì € ì‹¤í–‰)
                if hasattr(self, 'advanced_geometric_ai') and self.advanced_geometric_ai is not None:
                    try:
                        logger.info("ğŸ§  Advanced AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ğŸ§  Advanced AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼ (ëª¨ë“  ëª¨ë¸ì— ì ìš©)
                        if self.device == 'mps':
                            person_tensor = person_tensor.to(dtype=torch.float32)
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            
                            # ğŸ”¥ ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ float32ë¡œ í†µì¼
                            for model_name, model in self.ai_models.items():
                                if hasattr(model, 'parameters'):
                                    for param in model.parameters():
                                        param.data = param.data.to(dtype=torch.float32)
                            
                            # ğŸ”¥ advanced_geometric_ai ëª¨ë¸ë„ float32ë¡œ í†µì¼
                            if hasattr(self, 'advanced_geometric_ai') and self.advanced_geometric_ai is not None:
                                if hasattr(self.advanced_geometric_ai, 'parameters'):
                                    for param in self.advanced_geometric_ai.parameters():
                                        param.data = param.data.to(dtype=torch.float32)
                        
                        # 6ì±„ë„ ì…ë ¥ìœ¼ë¡œ ê²°í•©
                        combined_input = torch.cat([person_tensor, clothing_tensor], dim=1)
                        advanced_result = self.advanced_geometric_ai(combined_input)
                        logger.info(f"âœ… Advanced AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(advanced_result)}")
                        print(f"âœ… Advanced AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(advanced_result)}")
                        if isinstance(advanced_result, dict):
                            logger.info(f"ğŸ” Advanced AI ê²°ê³¼ í‚¤: {list(advanced_result.keys())}")
                            print(f"ğŸ” Advanced AI ê²°ê³¼ í‚¤: {list(advanced_result.keys())}")
                        results['advanced_ai'] = advanced_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ Advanced AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ Advanced AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f" Advanced AI ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['advanced_ai'] = {
                            'transformation_matrix': torch.eye(3, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_advanced'
                        }
                else:
                    logger.warning("âš ï¸ Advanced AI ëª¨ë¸ì´ ì—†ìŒ")
                    print("âš ï¸ Advanced AI ëª¨ë¸ì´ ì—†ìŒ")
                
                # 2. GMM ëª¨ë¸ ì‹¤í–‰ (ai_modelsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                if 'gmm_model' in self.ai_models and self.ai_models['gmm_model'] is not None:
                    try:
                        logger.info("ï¿½ï¿½ GMM ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ï¿½ï¿½ GMM ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        # ğŸ”¥ ë””ë²„ê¹…: ì…ë ¥ í…ì„œ ì •ë³´
                        logger.info(f"ğŸ” ì…ë ¥ person_tensor: {person_tensor.shape}, dtype={person_tensor.dtype}, mean={person_tensor.mean():.6f}, std={person_tensor.std():.6f}")
                        logger.info(f"ğŸ” ì…ë ¥ clothing_tensor: {clothing_tensor.shape}, dtype={clothing_tensor.dtype}, mean={clothing_tensor.mean():.6f}, std={clothing_tensor.std():.6f}")
                        
                        # ğŸ”¥ ë””ë²„ê¹…: ëª¨ë¸ ìƒíƒœ í™•ì¸
                        gmm_model = self.ai_models['gmm_model']
                        logger.info(f"ğŸ” GMM ëª¨ë¸ íƒ€ì…: {type(gmm_model)}")
                        logger.info(f"ğŸ” GMM ëª¨ë¸ device: {next(gmm_model.parameters()).device}")
                        logger.info(f"ğŸ” GMM ëª¨ë¸ training mode: {gmm_model.training}")
                        
                        # ğŸ”¥ ë””ë²„ê¹…: ëª¨ë¸ ê°€ì¤‘ì¹˜ ìƒíƒœ í™•ì¸
                        total_params = sum(p.numel() for p in gmm_model.parameters())
                        non_zero_params = sum((p != 0).sum().item() for p in gmm_model.parameters())
                        logger.info(f"ğŸ” GMM ëª¨ë¸ íŒŒë¼ë¯¸í„° ìƒíƒœ: {total_params}ê°œ ì¤‘ {non_zero_params}ê°œ ë¹„ì˜")
                        
                        # ğŸ”¥ ë””ë²„ê¹…: ëª¨ë¸ ê°€ì¤‘ì¹˜ ìƒíƒœ í™•ì¸
                        if hasattr(gmm_model, 'state_dict'):
                            gmm_params = list(gmm_model.parameters())
                            if gmm_params:
                                first_param = gmm_params[0]
                                logger.info(f"ğŸ” GMM ëª¨ë¸ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°: shape={first_param.shape}, mean={first_param.mean():.6f}, std={first_param.std():.6f}")
                                logger.info(f"ğŸ” GMM ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in gmm_params):,}")
                                
                                # ğŸ”¥ ì‹¤ì œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì¸ì§€ í™•ì¸ (ëœë¤ ì´ˆê¸°í™”ì™€ êµ¬ë¶„)
                                param_mean = first_param.mean().item()
                                param_std = first_param.std().item()
                                if abs(param_mean) < 0.01 and param_std < 0.1:
                                    logger.warning("âš ï¸ GMM ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì´ˆê¸°í™”ëœ ìƒíƒœ - ì‹¤ì œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ì•„ë‹ ê°€ëŠ¥ì„±")
                                else:
                                    logger.info("âœ… GMM ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì‹¤ì œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ë³´ì„")
                            else:
                                logger.warning("âš ï¸ GMM ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒ - Mock ëª¨ë¸ì¼ ê°€ëŠ¥ì„±")
                        else:
                            logger.warning("âš ï¸ GMM ëª¨ë¸ì— state_dictê°€ ì—†ìŒ - Mock ëª¨ë¸ì¼ ê°€ëŠ¥ì„±")
                        
                        # ğŸ”¥ ë””ë²„ê¹…: ëª¨ë¸ íƒ€ì… í™•ì¸
                        model_type = type(gmm_model).__name__
                        logger.info(f"ğŸ” GMM ëª¨ë¸ íƒ€ì…: {model_type}")
                        if 'Mock' in model_type or 'Simple' in model_type:
                            logger.warning("âš ï¸ GMM ëª¨ë¸ì´ Mock/Simple íƒ€ì… - ì‹¤ì œ ì‹ ê²½ë§ì´ ì•„ë‹˜")
                        
                        # ğŸ”¥ ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
                        start_time = time.time()
                        
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼
                        if self.device == 'mps':
                            person_tensor = person_tensor.to(dtype=torch.float32)
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            if hasattr(gmm_model, 'to'):
                                gmm_model = gmm_model.to(dtype=torch.float32)
                        
                        gmm_result = gmm_model(person_tensor, clothing_tensor)
                        inference_time = time.time() - start_time
                        
                        logger.info(f"âœ… GMM ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(gmm_result)} (ì†Œìš”ì‹œê°„: {inference_time:.4f}ì´ˆ)")
                        print(f"âœ… GMM ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(gmm_result)} (ì†Œìš”ì‹œê°„: {inference_time:.4f}ì´ˆ)")
                        
                        # ğŸ”¥ ì¶”ë¡  ì‹œê°„ ë¶„ì„
                        if inference_time < 0.1:
                            logger.warning("âš ï¸ GMM ì¶”ë¡  ì‹œê°„ì´ ë„ˆë¬´ ë¹ ë¦„ (0.1ì´ˆ ë¯¸ë§Œ) - Mock ëª¨ë¸ì¼ ê°€ëŠ¥ì„±")
                        elif inference_time > 1.0:
                            logger.info("âœ… GMM ì¶”ë¡  ì‹œê°„ì´ ì ì ˆí•¨ - ì‹¤ì œ ì‹ ê²½ë§ ì¶”ë¡ ìœ¼ë¡œ ë³´ì„")
                        else:
                            logger.info("ğŸ” GMM ì¶”ë¡  ì‹œê°„ì´ ì¤‘ê°„ ìˆ˜ì¤€ - ì¶”ê°€ í™•ì¸ í•„ìš”")
                        
                        if isinstance(gmm_result, dict):
                            logger.info(f"ğŸ” GMM ê²°ê³¼ í‚¤: {list(gmm_result.keys())}")
                            print(f"ğŸ” GMM ê²°ê³¼ í‚¤: {list(gmm_result.keys())}")
                            
                            # ğŸ”¥ ë””ë²„ê¹…: ê²°ê³¼ í…ì„œ ì •ë³´
                            for key, value in gmm_result.items():
                                if isinstance(value, torch.Tensor):
                                    logger.info(f"ğŸ” GMM {key}: {value.shape}, dtype={value.dtype}, mean={value.mean():.6f}, std={value.std():.6f}")
                                elif isinstance(value, (int, float)):
                                    logger.info(f"ğŸ” GMM {key}: {value}")
                        
                        results['gmm'] = gmm_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ GMM ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ GMM ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f"ğŸ” GMM ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['gmm'] = {
                            'transformation_matrix': torch.eye(3, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_gmm'
                        }
                else:
                    logger.warning("âš ï¸ GMM ëª¨ë¸ì´ ì—†ìŒ")
                    print("âš ï¸ GMM ëª¨ë¸ì´ ì—†ìŒ")
                
                # 2. TPS ëª¨ë¸ ì‹¤í–‰ (ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë”©ëœ ëª¨ë¸)
                if hasattr(self, 'tps_model') and self.tps_model is not None:
                    try:
                        logger.info("ï¿½ï¿½ TPS ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ï¿½ï¿½ TPS ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼
                        if self.device == 'mps':
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            if hasattr(self.tps_model, 'to'):
                                self.tps_model = self.tps_model.to(dtype=torch.float32)
                        
                        # TPSëŠ” ì˜ë¥˜ ì´ë¯¸ì§€ë§Œ ì…ë ¥
                        tps_result = self.tps_model(clothing_tensor)
                        logger.info(f"âœ… TPS ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(tps_result)}")
                        print(f"âœ… TPS ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(tps_result)}")
                        if isinstance(tps_result, torch.Tensor):
                            logger.info(f"ï¿½ï¿½ TPS ê²°ê³¼ shape: {tps_result.shape}")
                            print(f"ï¿½ï¿½ TPS ê²°ê³¼ shape: {tps_result.shape}")
                        results['tps'] = tps_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ TPS ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ TPS ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f"ğŸ” TPS ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['tps'] = {
                            'control_points': torch.randn(1, 18, 2, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_tps'
                        }
                else:
                    logger.warning("âš ï¸ TPS ëª¨ë¸ì´ ì—†ìŒ")
                    print("âš ï¸ TPS ëª¨ë¸ì´ ì—†ìŒ")
                
                # 3. Optical Flow ëª¨ë¸ ì‹¤í–‰
                if hasattr(self, 'optical_flow_model') and self.optical_flow_model is not None:
                    try:
                        logger.info("ï¿½ï¿½ Optical Flow ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ï¿½ï¿½ Optical Flow ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼
                        if self.device == 'mps':
                            person_tensor = person_tensor.to(dtype=torch.float32)
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            if hasattr(self.optical_flow_model, 'to'):
                                self.optical_flow_model = self.optical_flow_model.to(dtype=torch.float32)
                        
                        flow_result = self.optical_flow_model(person_tensor, clothing_tensor)
                        logger.info(f"âœ… Optical Flow ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(flow_result)}")
                        print(f"âœ… Optical Flow ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(flow_result)}")
                        if isinstance(flow_result, dict):
                            logger.info(f"ï¿½ï¿½ Optical Flow ê²°ê³¼ í‚¤: {list(flow_result.keys())}")
                            print(f"ï¿½ï¿½ Optical Flow ê²°ê³¼ í‚¤: {list(flow_result.keys())}")
                        results['optical_flow'] = flow_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ Optical Flow ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ Optical Flow ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f"ï¿½ï¿½ Optical Flow ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['optical_flow'] = {
                            'flow_field': torch.randn(1, 2, 256, 192, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_optical_flow'
                        }
                else:
                    logger.warning("âš ï¸ Optical Flow ëª¨ë¸ì´ ì—†ìŒ")
                    print("âš ï¸ Optical Flow ëª¨ë¸ì´ ì—†ìŒ")
                
                # 4. Keypoint Matching ëª¨ë¸ ì‹¤í–‰
                if hasattr(self, 'keypoint_matcher') and self.keypoint_matcher is not None:
                    try:
                        logger.info("ğŸ§  Keypoint Matching ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ğŸ§  Keypoint Matching ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼
                        if self.device == 'mps':
                            person_tensor = person_tensor.to(dtype=torch.float32)
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            if hasattr(self.keypoint_matcher, 'to'):
                                self.keypoint_matcher = self.keypoint_matcher.to(dtype=torch.float32)
                        
                        # 6ì±„ë„ ì…ë ¥ìœ¼ë¡œ ê²°í•©
                        combined_input = torch.cat([person_tensor, clothing_tensor], dim=1)
                        logger.info(f"ğŸ” ê²°í•©ëœ ì…ë ¥ shape: {combined_input.shape}")
                        print(f"ğŸ” ê²°í•©ëœ ì…ë ¥ shape: {combined_input.shape}")
                        keypoint_result = self.keypoint_matcher(combined_input)
                        logger.info(f"âœ… Keypoint Matching ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(keypoint_result)}")
                        print(f"âœ… Keypoint Matching ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(keypoint_result)}")
                        if isinstance(keypoint_result, dict):
                            logger.info(f"ğŸ” Keypoint ê²°ê³¼ í‚¤: {list(keypoint_result.keys())}")
                            print(f"ğŸ” Keypoint ê²°ê³¼ í‚¤: {list(keypoint_result.keys())}")
                        results['keypoint_matching'] = keypoint_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ Keypoint Matching ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ Keypoint Matching ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f"ğŸ” Keypoint ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['keypoint_matching'] = {
                            'keypoints': torch.randn(1, 18, 2, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_keypoint'
                        }
                else:
                    logger.warning("âš ï¸ Keypoint Matcherê°€ ì—†ìŒ")
                    print("âš ï¸ Keypoint Matcherê°€ ì—†ìŒ")
                
                # 5. Advanced AI ëª¨ë¸ ì‹¤í–‰
                if hasattr(self, 'advanced_geometric_ai') and self.advanced_geometric_ai is not None:
                    try:
                        logger.info("ğŸ§  Advanced AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        print("ğŸ§  Advanced AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì‹œì‘...")
                        
                        # ğŸ”¥ MPS íƒ€ì… í†µì¼ (ëª¨ë“  ëª¨ë¸ì— ì ìš©)
                        if self.device == 'mps':
                            person_tensor = person_tensor.to(dtype=torch.float32)
                            clothing_tensor = clothing_tensor.to(dtype=torch.float32)
                            
                            # ğŸ”¥ ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ float32ë¡œ í†µì¼
                            for model_name, model in self.ai_models.items():
                                if hasattr(model, 'parameters'):
                                    for param in model.parameters():
                                        param.data = param.data.to(dtype=torch.float32)
                            
                            # ğŸ”¥ advanced_geometric_ai ëª¨ë¸ë„ float32ë¡œ í†µì¼
                            if hasattr(self, 'advanced_geometric_ai') and self.advanced_geometric_ai is not None:
                                if hasattr(self.advanced_geometric_ai, 'parameters'):
                                    for param in self.advanced_geometric_ai.parameters():
                                        param.data = param.data.to(dtype=torch.float32)
                        
                        # 6ì±„ë„ ì…ë ¥ìœ¼ë¡œ ê²°í•©
                        combined_input = torch.cat([person_tensor, clothing_tensor], dim=1)
                        advanced_result = self.advanced_geometric_ai(combined_input)
                        logger.info(f"âœ… Advanced AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(advanced_result)}")
                        print(f"âœ… Advanced AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {type(advanced_result)}")
                        if isinstance(advanced_result, dict):
                            logger.info(f"ğŸ” Advanced AI ê²°ê³¼ í‚¤: {list(advanced_result.keys())}")
                            print(f"ğŸ” Advanced AI ê²°ê³¼ í‚¤: {list(advanced_result.keys())}")
                        results['advanced_ai'] = advanced_result
                    except Exception as e:
                        logger.warning(f"âš ï¸ Advanced AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        print(f"âš ï¸ Advanced AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        import traceback
                        logger.error(f"ï¿½ï¿½ Advanced AI ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        results['advanced_ai'] = {
                            'transformation_matrix': torch.eye(3, device=self.device, dtype=torch.float32),
                            'confidence': 0.5,
                            'method': 'mock_advanced'
                        }
                else:
                    logger.warning("âš ï¸ Advanced AI ëª¨ë¸ì´ ì—†ìŒ")
                    print("âš ï¸ Advanced AI ëª¨ë¸ì´ ì—†ìŒ")
            
            logger.info(f"ï¿½ï¿½ ï¿½ï¿½ ğŸ”¥ _execute_all_ai_models í˜¸ì¶œ ì™„ë£Œ! ê²°ê³¼ í‚¤: {list(results.keys())}")
            print(f"ï¿½ï¿½ ï¿½ï¿½ ğŸ”¥ _execute_all_ai_models í˜¸ì¶œ ì™„ë£Œ! ê²°ê³¼ í‚¤: {list(results.keys())}")
            
            # ğŸ”¥ ìµœì¢… ê²°ê³¼ ìš”ì•½
            for key, value in results.items():
                if isinstance(value, dict):
                    logger.info(f"ğŸ” {key} ê²°ê³¼: {list(value.keys())}")
                    print(f"ğŸ” {key} ê²°ê³¼: {list(value.keys())}")
                else:
                    logger.info(f"ï¿½ï¿½ {key} ê²°ê³¼ íƒ€ì…: {type(value)}")
                    print(f"ï¿½ï¿½ {key} ê²°ê³¼ íƒ€ì…: {type(value)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ _execute_all_ai_models ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            print(f"âŒ _execute_all_ai_models ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ğŸ” ì „ì²´ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'gmm': {'transformation_matrix': torch.eye(3, device=self.device), 'confidence': 0.0, 'method': 'error'},
                'tps': {'control_points': torch.randn(1, 18, 2, device=self.device), 'confidence': 0.0, 'method': 'error'},
                'optical_flow': {'flow_field': torch.randn(1, 2, 256, 192, device=self.device), 'confidence': 0.0, 'method': 'error'},
                'keypoint_matching': {'keypoints': torch.randn(1, 18, 2, device=self.device), 'confidence': 0.0, 'method': 'error'},
                'advanced_ai': {'transformation_matrix': torch.eye(3, device=self.device), 'confidence': 0.0, 'method': 'error'}
            }


    def _fuse_and_postprocess_results(self, results: Dict[str, Any], 
                                    person_tensor: torch.Tensor, 
                                    clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """ê²°ê³¼ ìœµí•© ë° í›„ì²˜ë¦¬"""
        try:
            # ìš°ì„ ìˆœìœ„: advanced_ai > gmm > mock
            primary_result = None
            
            if 'advanced_ai' in results:
                primary_result = results['advanced_ai']
                algorithm_type = 'advanced_deeplab_aspp_self_attention'
            elif 'gmm' in results:
                primary_result = results['gmm']
                algorithm_type = 'gmm_tps_matching'
            elif 'mock_advanced_ai' in results:
                primary_result = results['mock_advanced_ai']
                algorithm_type = 'mock_geometric_matching'
            else:
                # ê¸°ë³¸ ê²°ê³¼ ìƒì„±
                primary_result = self._create_result("basic", person_tensor=person_tensor, clothing_tensor=clothing_tensor)                
                algorithm_type = 'basic_identity_transform'
            
            # ë³´ì¡° ì •ë³´ ì¶”ê°€
            if 'keypoint' in results:
                keypoint_data = results['keypoint']
                primary_result['keypoint_matches'] = keypoint_data.get('matches', [])
                primary_result['keypoint_similarity'] = keypoint_data.get('similarity_matrix')
            
            if 'optical_flow' in results:
                flow_data = results['optical_flow']
                primary_result['optical_flow'] = flow_data.get('flow')
                primary_result['flow_correlation'] = flow_data.get('correlation')
            
            # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì¶”ê°€
            primary_result['algorithm_type'] = algorithm_type
            primary_result['models_used'] = list(results.keys())
            primary_result['fusion_method'] = 'priority_based'
            
            return primary_result
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ìœµí•© ì‹¤íŒ¨: {e}")
            return self._create_result("basic", person_tensor=person_tensor, clothing_tensor=clothing_tensor)

    def _create_result(self, result_type: str = "basic", **kwargs) -> Dict[str, Any]:
        """í†µí•© ê²°ê³¼ ìƒì„± ë©”ì„œë“œ - basic, error, success íƒ€ì… ì§€ì›"""
        
        if result_type == "basic":
            """ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
            person_tensor = kwargs.get('person_tensor')
            clothing_tensor = kwargs.get('clothing_tensor')
            batch_size, _, H, W = person_tensor.shape
            device = person_tensor.device
            
            return {
                'transformation_matrix': torch.eye(2, 3, device=device).unsqueeze(0).repeat(batch_size, 1, 1),
                'transformation_grid': self._create_identity_grid(batch_size, H, W),
                'warped_clothing': clothing_tensor.clone(),
                'quality_score': torch.tensor([0.5] * batch_size, device=device),
                'overall_confidence': torch.tensor(0.5, device=device),
                'keypoint_confidence': torch.tensor(0.5, device=device),
                'algorithm_type': 'basic_identity_transform'
            }
        
        elif result_type == "error":
            """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
            self.logger.warning("âš ï¸ [Step 4] ì—ëŸ¬ ê²°ê³¼ ìƒì„± - ì‹¤ì œ AI ëª¨ë¸ì´ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ!")
            error_msg = kwargs.get('error_msg', 'Unknown error')
            processing_time = kwargs.get('processing_time', 0.0)
            
            return {
                'error': error_msg,
                'processing_time': processing_time,
                'algorithm_type': 'error_fallback',
                'models_used': [],
                'fusion_method': 'error_fallback',
                'overall_confidence': 0.0,
                'quality_score': 0.0,
                'keypoint_confidence': 0.0
            }
        
        elif result_type == "success":
            """ì„±ê³µ ê²°ê³¼ ìƒì„±"""
            result = kwargs.get('result', {})
            processing_time = kwargs.get('processing_time', 0.0)
            
            return {
                **result,
                'processing_time': processing_time,
                'algorithm_type': result.get('algorithm_type', 'success'),
                'models_used': result.get('models_used', []),
                'fusion_method': result.get('fusion_method', 'success')
            }
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²°ê³¼ íƒ€ì…: {result_type}")

    def _compute_quality_metrics(self, result: Dict[str, Any], inference_results: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ - ì•ˆì „í•œ íƒ€ì… ë³€í™˜
            confidence_raw = result.get('overall_confidence', 0.5)
            if torch.is_tensor(confidence_raw):
                confidence = confidence_raw.item()
            else:
                confidence = float(confidence_raw)
            
            quality_score_raw = result.get('quality_score', 0.5)
            if torch.is_tensor(quality_score_raw):
                try:
                    quality_score = quality_score_raw.mean().item() if quality_score_raw.numel() > 1 else quality_score_raw.item()
                except Exception:
                    quality_score = 0.5
            else:
                quality_score = float(quality_score_raw)
            
            # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ í’ˆì§ˆ
            keypoint_quality = 0.0
            if 'keypoint_matches' in result:
                matches = result['keypoint_matches']
                if isinstance(matches, list) and len(matches) > 0:
                    if isinstance(matches[0], list):  # ë°°ì¹˜ ê²°ê³¼
                        total_matches = sum(len(batch_matches) for batch_matches in matches)
                        total_confidence = sum(
                            sum(match.get('similarity', 0) for match in batch_matches)
                            for batch_matches in matches
                        )
                        keypoint_quality = total_confidence / max(total_matches, 1)
                    else:  # ë‹¨ì¼ ë°°ì¹˜
                        keypoint_quality = sum(match.get('similarity', 0) for match in matches) / max(len(matches), 1)
            
            # ë³€í˜• ì•ˆì •ì„±
            transform_stability = 1.0
            if 'transformation_matrix' in result:
                transform_matrix = result['transformation_matrix']
                if torch.is_tensor(transform_matrix):
                    try:
                        # í–‰ë ¬ì‹ìœ¼ë¡œ ì•ˆì •ì„± í‰ê°€
                        det = torch.det(transform_matrix[:, :2, :2])
                        transform_stability = torch.clamp(1.0 / (torch.abs(det - 1.0) + 1e-6), 0, 1).mean().item()
                    except Exception:
                        transform_stability = 1.0
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            overall_quality = (confidence * 0.4 + quality_score * 0.3 + 
                            keypoint_quality * 0.2 + transform_stability * 0.1)
            
            result.update({
                'confidence': confidence,
                'quality_score': quality_score,
                'keypoint_matching_quality': keypoint_quality,
                'transformation_stability': transform_stability,
                'overall_quality': overall_quality,
                'quality_breakdown': {
                    'confidence_weight': 0.4,
                    'quality_weight': 0.3,
                    'keypoint_weight': 0.2,
                    'stability_weight': 0.1
                }
            })
            
            return result
            
        except Exception as e:
            logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            result.update({
                'confidence': 0.5,
                'quality_score': 0.5,
                'overall_quality': 0.5
            })
            return result

    def _generate_cache_key_complete(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor) -> str:
        """ì™„ì „í•œ ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # í…ì„œ í•´ì‹œ
            person_hash = hashlib.md5(person_tensor.cpu().numpy().tobytes()).hexdigest()[:16]
            clothing_hash = hashlib.md5(clothing_tensor.cpu().numpy().tobytes()).hexdigest()[:16]
            
            # ì„¤ì • í•´ì‹œ
            config_str = f"{self.device}_{getattr(self.config, 'matching_method', 'default')}"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            # ëª¨ë¸ ë²„ì „ í•´ì‹œ
            model_version = f"v8.0_{len(self.loaded_models)}"
            version_hash = hashlib.md5(model_version.encode()).hexdigest()[:8]
            
            return f"geometric_v8_{person_hash}_{clothing_hash}_{config_hash}_{version_hash}"
            
        except Exception:
            return f"geometric_v8_fallback_{int(time.time())}"

    def _update_inference_statistics_complete(self, processing_time: float, success: bool, result: Dict[str, Any]):
        """ì™„ì „í•œ ì¶”ë¡  í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ë³¸ í†µê³„
            self.statistics['total_processed'] += 1
            self.statistics['ai_model_calls'] += 1
            self.statistics['total_processing_time'] += processing_time
            
            if success:
                self.statistics['successful_matches'] += 1
                
                # í‰ê·  í’ˆì§ˆ ì—…ë°ì´íŠ¸
                quality = result.get('overall_quality', 0.5)
                total_success = self.statistics['successful_matches']
                current_avg = self.statistics['average_quality']
                self.statistics['average_quality'] = (current_avg * (total_success - 1) + quality) / total_success
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats['total_processed'] += 1
            if success:
                self.performance_stats['successful_matches'] += 1
                
                # í‰ê·  ì²˜ë¦¬ ì‹œê°„
                current_avg_time = self.performance_stats['avg_processing_time']
                total_success = self.performance_stats['successful_matches']
                self.performance_stats['avg_processing_time'] = (
                    (current_avg_time * (total_success - 1) + processing_time) / total_success
                )
                
                # í‰ê·  í’ˆì§ˆ
                quality = result.get('overall_quality', 0.5)
                current_avg_quality = self.performance_stats['avg_transformation_quality']
                self.performance_stats['avg_transformation_quality'] = (
                    (current_avg_quality * (total_success - 1) + quality) / total_success
                )
                
                # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ë¥ 
                keypoint_quality = result.get('keypoint_matching_quality', 0.0)
                current_kpt_rate = self.performance_stats['keypoint_match_rate']
                self.performance_stats['keypoint_match_rate'] = (
                    (current_kpt_rate * (total_success - 1) + keypoint_quality) / total_success
                )
            
            # ëª¨ë¸ ì‚¬ìš© í†µê³„
            self.performance_stats['models_loaded'] = len(self.loaded_models)
            
        except Exception as e:
            logger.debug(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """GeometricMatchingStep ìƒì„± (Central Hub DI Container ì—°ë™)"""
    try:
        step = GeometricMatchingStep(**kwargs)
        # Central Hub DI Containerê°€ ìë™ìœ¼ë¡œ ì˜ì¡´ì„±ì„ ì£¼ì…í•¨
        # ë³„ë„ì˜ ì´ˆê¸°í™” ì‘ì—… ë¶ˆí•„ìš”
    
        return step
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ GeometricMatchingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_geometric_matching_step_sync(**kwargs) -> GeometricMatchingStep:
    """ë™ê¸°ì‹ GeometricMatchingStep ìƒì„±"""
    try:
        return create_geometric_matching_step(**kwargs)
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ ë™ê¸°ì‹ GeometricMatchingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_m3_max_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” GeometricMatchingStep ìƒì„±"""
    kwargs.setdefault('device', 'mps')
    return create_geometric_matching_step(**kwargs)

# ==============================================
# ğŸ”¥ 11. ëª¨ë“ˆ ì •ë³´ ë° ìµìŠ¤í¬íŠ¸
# ==============================================

__version__ = "8.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ê¸°í•˜í•™ì  ë§¤ì¹­ - Central Hub DI Container ì™„ì „ ì—°ë™"
__compatibility_version__ = "8.0.0-central-hub-di-container"

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'GeometricMatchingStep',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'GeometricMatchingModule',
    'TPSGridGenerator',
    'OpticalFlowNetwork',
    'KeypointMatchingNetwork',
    
    # ê³ ê¸‰ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'CompleteAdvancedGeometricMatchingAI',
    'DeepLabV3PlusBackbone',
    'ASPPModule',
    'SelfAttentionKeypointMatcher',
    'EdgeAwareTransformationModule',
    'ProgressiveGeometricRefinement',
    
    # ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤
    'AdvancedGeometricMatcher',
    
    # ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
    'EnhancedModelPathMapper',
    'GeometricMatchingConfig',
    'ProcessingStatus',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_geometric_matching_step',
    'create_geometric_matching_step_sync',
    'create_m3_max_geometric_matching_step',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'validate_geometric_matching_dependencies',
    'test_geometric_matching_step',
    'test_advanced_ai_geometric_matching',
    'test_basestepmixin_compatibility',
    
    # ìƒìˆ˜ë“¤
    'MATCHING_ALGORITHMS',
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'PIL_AVAILABLE',
    'NUMPY_AVAILABLE',
    'CV2_AVAILABLE',
    'SCIPY_AVAILABLE',
    'IS_M3_MAX',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 12. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 120)
logger.info("ğŸ”¥ GeometricMatchingStep v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™")
logger.info("=" * 120)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("âœ… BaseStepMixin ìƒì† ë° super().__init__() í˜¸ì¶œ")
logger.info("âœ… í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”: ai_models, models_loading_status, model_interface, loaded_models")
logger.info("âœ… _load_segmentation_models_via_central_hub() ë©”ì„œë“œ - ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”©")
logger.info("âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Geometric Matching ë¡œì§ë§Œ")
logger.info("âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„±")
logger.debug("âœ… ì‹¤ì œ GMM/TPS/SAM ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© (3.0GB)")
logger.info("âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ")
logger.info("âœ… ë³µì¡í•œ DI ì´ˆê¸°í™” ë¡œì§ ë‹¨ìˆœí™”")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€ ì½”ë“œ ë¶ˆí•„ìš”")
logger.info("âœ… TYPE_CHECKING ë‹¨ìˆœí™”")

logger.info("ğŸ§  ë³´ì¡´ëœ AI ëª¨ë¸ë“¤:")
logger.info("   ğŸ¯ GeometricMatchingModule - GMM ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­")
logger.info("   ğŸŒŠ TPSGridGenerator - Thin-Plate Spline ë³€í˜•")
logger.info("   ğŸ“Š OpticalFlowNetwork - RAFT ê¸°ë°˜ Flow ê³„ì‚°")
logger.info("   ğŸ¯ KeypointMatchingNetwork - í‚¤í¬ì¸íŠ¸ ë§¤ì¹­")
logger.info("   ğŸ”¥ CompleteAdvancedGeometricMatchingAI - ê³ ê¸‰ AI ëª¨ë¸")
logger.info("   ğŸ—ï¸ DeepLabV3PlusBackbone - DeepLabV3+ ë°±ë³¸")
logger.info("   ğŸŒŠ ASPPModule - ASPP Multi-scale Context")
logger.info("   ğŸ¯ SelfAttentionKeypointMatcher - Self-Attention ë§¤ì¹­")
logger.info("   âš¡ EdgeAwareTransformationModule - Edge-Aware ë³€í˜•")
logger.info("   ğŸ“ˆ ProgressiveGeometricRefinement - Progressive ì •ì œ")
logger.info("   ğŸ“ AdvancedGeometricMatcher - ê³ ê¸‰ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ—ºï¸ EnhancedModelPathMapper - í–¥ìƒëœ ê²½ë¡œ ë§¤í•‘")

logger.info("ğŸ”§ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ (Central Hub ê´€ë¦¬):")
logger.info("   ğŸ“ gmm_final.pth (1.3GB) - VITON-HD ê¸°ë°˜")
logger.info("   ğŸ“ tps_network.pth (548MB)")
logger.info("   ğŸ“ sam_vit_h_4b8939.pth (2.4GB) - Step 03ê³¼ ê³µìœ ")
logger.info("   ğŸ“ resnet101_geometric.pth (528MB) - VGG16 Ultra ê¸°ë°˜")
logger.info("   ğŸ“ ViT-L-14.pt (577MB) - CLIP ê¸°ë°˜")
logger.info("   ğŸ“ efficientnet_b0_ultra.pth (548MB) - VGG19 ê¸°ë°˜")
logger.info("   ğŸ“ raft-things.pth (548MB) - VGG19 ê¸°ë°˜")

logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - PIL: {PIL_AVAILABLE}")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - ë©”ëª¨ë¦¬ ìµœì í™”: {CONDA_INFO['is_mycloset_env']}")

logger.info("ğŸ”¥ Central Hub DI Container v7.0 ì—°ë™ íŠ¹ì§•:")
logger.info("   âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„")
logger.info("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   âœ… ì˜ì¡´ì„± ìë™ ì£¼ì…")
logger.info("   âœ… ModelLoader íŒ©í† ë¦¬ íŒ¨í„´")
logger.info("   âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜")
logger.info("   âœ… Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")

logger.info("=" * 120)
logger.info("ğŸ‰ MyCloset AI - Step 04 GeometricMatching v8.0 Central Hub DI Container ì™„ì „ ë¦¬íŒ©í† ë§ ì™„ë£Œ!")
logger.info("   BaseStepMixin ìƒì† + Central Hub ì—°ë™ + ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´!")
logger.info("=" * 120)

# ==============================================
# ğŸ”¥ 13. ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸)
# ==============================================

if __name__ == "__main__":
    print("=" * 120)
    print("ğŸ¯ MyCloset AI Step 04 - v8.0 Central Hub DI Container ì™„ì „ ì—°ë™")
    print("=" * 120)
    print("âœ… ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("   â€¢ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("   â€¢ BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™”")
    print("   â€¢ ModelLoader íŒ©í† ë¦¬ íŒ¨í„´ì„ í†µí•œ AI ëª¨ë¸ ë¡œë”©")
    print("   â€¢ ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ")
    print("   â€¢ GitHubDependencyManager ì™„ì „ ì‚­ì œ")
    print("   â€¢ ë³µì¡í•œ DI ì´ˆê¸°í™” ë¡œì§ ë‹¨ìˆœí™”")
    print("   â€¢ ìˆœí™˜ì°¸ì¡° ë°©ì§€ ì½”ë“œ ì œê±°")
    print("   â€¢ Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")
    print("=" * 120)
    print("ğŸ”¥ ë¦¬íŒ©í† ë§ ì„±ê³¼:")
    print("   âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("   âœ… BaseStepMixin í˜¸í™˜ì„± 100% ìœ ì§€")
    print("   âœ… ëª¨ë“  AI ëª¨ë¸ ë° ì•Œê³ ë¦¬ì¦˜ ë³´ì¡´")
    print("   âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ 3.0GB í™œìš©")
    print("   âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜")
    print("   âœ… ì—ëŸ¬ ë°©ì§€ í´ë°± ì‹œìŠ¤í…œ")
    print("=" * 120)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        test_basestepmixin_compatibility()
        print()
        test_geometric_matching_step()
        print()
        test_advanced_ai_geometric_matching()
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 120)
    print("ğŸ‰ GeometricMatchingStep v8.0 Central Hub DI Container ì™„ì „ ì—°ë™ ì™„ë£Œ!")
    print("âœ… BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™”")
    print("âœ… ModelLoader íŒ©í† ë¦¬ íŒ¨í„´ ì ìš©")
    print("âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜")
    print("âœ… ì‹¤ì œ AI ëª¨ë¸ 3.0GB ì™„ì „ í™œìš©")
    print("âœ… Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")
    print("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("=" * 120)

class GeometricMatchingModule(nn.Module):
    """ì‹¤ì œ GMM (Geometric Matching Module) - ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ êµ¬ì¡°"""
    
    def __init__(self, input_nc=6, output_nc=2, num_control_points=20, initialize_weights=True):
        super().__init__()
        self.input_nc = input_nc
        self.output_nc = output_nc
        self.num_control_points = num_control_points
        
        # ì²´í¬í¬ì¸íŠ¸ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡° (conv1, conv2, conv3ë§Œ)
        self.conv1 = nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=True)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True)
        self.conv3 = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1, bias=True)
        
        # TPS ê·¸ë¦¬ë“œ ìƒì„±ê¸° ì œê±° (ì²´í¬í¬ì¸íŠ¸ì— ì—†ìŒ)
        # self.tps_generator = TPSGridGenerator(num_control_points=num_control_points)
        
        # Initialize weights only if requested (ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì—ëŠ” False)
        if initialize_weights:
            self._initialize_weights()
    
    def _initialize_weights(self):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
         
    def forward(self, person_image, clothing_image):
        """ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ëŠ” ìˆœì „íŒŒ"""
        try:
            batch_size = person_image.size(0)
            device = person_image.device
            
            # 1. ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            if person_image.dim() != 4 or clothing_image.dim() != 4:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
            
            # 2. ì…ë ¥ ê²°í•© (person + clothing)
            combined_input = torch.cat([person_image, clothing_image], dim=1)
            
            # 3. ê¸°ì¡´ ê°€ì¤‘ì¹˜ êµ¬ì¡°ì— ë§ëŠ” ìˆœì „íŒŒ (conv1 -> conv2 -> conv3)
            x = F.relu(self.conv1(combined_input))
            x = F.relu(self.conv2(x))
            output = self.conv3(x)
            
            # 4. ì¶œë ¥ì„ ì œì–´ì  í˜•íƒœë¡œ ë³€í™˜
            B, C, H, W = output.shape
            control_points = output.view(batch_size, -1, 2)  # (B, num_points, 2)
            
            # 5. ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„± (TPS ëŒ€ì‹  ê°„ë‹¨í•œ ì–´í•€ ë³€í˜•)
            transformation_grid = self._create_affine_grid(control_points, person_image.size())
            
            # 6. ì˜ë¥˜ ì´ë¯¸ì§€ ë³€í˜•
            warped_clothing = F.grid_sample(
                clothing_image, transformation_grid, 
                mode='bilinear', padding_mode='border', align_corners=False
            )
            
            # 7. ë³€í˜• í–‰ë ¬ ê³„ì‚°
            transformation_matrix = self._compute_affine_matrix(control_points)
            
            return {
                'transformation_matrix': transformation_matrix,
                'transformation_grid': transformation_grid,
                'warped_clothing': warped_clothing,
                'control_points': control_points,
                'correlation_features': x,  # conv2 ì¶œë ¥ì„ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
                'quality_score': torch.tensor(0.8, device=device).unsqueeze(0),
                'confidence': torch.tensor(0.8, device=device)
            }
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            batch_size = person_image.size(0)
            device = person_image.device
            H, W = person_image.size(2), person_image.size(3)
            
            # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
            y_coords = torch.linspace(-1, 1, H, device=device)
            x_coords = torch.linspace(-1, 1, W, device=device)
            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
            transformation_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            return {
                'transformation_matrix': torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1),
                'transformation_grid': transformation_grid,
                'warped_clothing': clothing_image,
                'control_points': torch.zeros(batch_size, self.num_control_points, 2, device=device),
                'correlation_features': torch.zeros(batch_size, 256, H//4, W//4, device=device),
                'quality_score': torch.tensor(0.7, device=device).unsqueeze(0),
                'confidence': torch.tensor(0.7, device=device)
            }
    
    def _create_affine_grid(self, control_points, input_size):
        """ì œì–´ì ì—ì„œ ì–´í•€ ê·¸ë¦¬ë“œ ìƒì„±"""
        batch_size = control_points.size(0)
        device = control_points.device
        H, W = input_size[2], input_size[3]
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords = torch.linspace(-1, 1, H, device=device)
        x_coords = torch.linspace(-1, 1, W, device=device)
        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
        
        # ì œì–´ì ì˜ í‰ê·  ë³€ìœ„ ê³„ì‚°
        mean_displacement = torch.mean(control_points, dim=1, keepdim=True)  # [B, 1, 2]
        
        # ê·¸ë¦¬ë“œì— ë³€ìœ„ ì ìš©
        x_grid = x_grid + mean_displacement[:, 0, 0:1] * 0.1
        y_grid = y_grid + mean_displacement[:, 0, 1:2] * 0.1
        
        transformation_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        return transformation_grid
    
    def _compute_affine_matrix(self, control_points):
        """ì œì–´ì ì—ì„œ ì–´í•€ ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size = control_points.size(0)
        device = control_points.device
        
        # ê¸°ë³¸ ì–´í•€ ë³€í˜• í–‰ë ¬
        affine_matrix = torch.zeros(batch_size, 2, 3, device=device)
        
        # ì œì–´ì ì˜ í‰ê·  ë³€ìœ„ë¡œ ì–´í•€ ë³€í˜• ì¶”ì •
        center_points = control_points[:, :4, :]  # ì¤‘ì•™ 4ê°œ ì  ì‚¬ìš©
        mean_displacement = torch.mean(center_points, dim=1)
        
        # Identity + displacement
        affine_matrix[:, 0, 0] = 1.0
        affine_matrix[:, 1, 1] = 1.0
        affine_matrix[:, :, 2] = mean_displacement * 0.1  # ë³€ìœ„ ìŠ¤ì¼€ì¼ë§
        
        return affine_matrix

# ==============================================
# ğŸ”¥ ì™„ì „í•œ TPSGridGenerator êµ¬í˜„
# ==============================================

class SimpleTPS(nn.Module):
    """ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ëŠ” TPS ëª¨ë¸"""
    
    def __init__(self, input_nc=3, num_control_points=20):
        super().__init__()
        self.num_control_points = num_control_points
        
        # ê¸°ì¡´ ê°€ì¤‘ì¹˜ êµ¬ì¡°ì— ì™„ì „íˆ ë§ì¶¤ (encoder.0, encoder.2, encoder.4, encoder.8)
        self.encoder = nn.Sequential(
            nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=True),  # encoder.0
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),      # encoder.2
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),     # encoder.4
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 36, bias=True)  # encoder.8 (36 = 18 control points * 2 coordinates)
        )
        
        # TPS ê·¸ë¦¬ë“œ ìƒì„±ê¸° (ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„ ìœ„í•´ ì œê±°)
        # self.tps_generator = TPSGridGenerator(num_control_points)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
    
    def _make_layer(self, inplanes, planes, blocks, stride=1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )
        
        layers = []
        layers.append(self._make_bottleneck_block(inplanes, planes, stride, downsample))
        inplanes = planes * 4
        for _ in range(1, blocks):
            layers.append(self._make_bottleneck_block(inplanes, planes))
        
        return nn.Sequential(*layers)
    
    def _make_bottleneck_block(self, inplanes, planes, stride=1, downsample=None):
        """Bottleneck ë¸”ë¡ ìƒì„±"""
        return BottleneckBlock(inplanes, planes, stride, downsample)
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ëŠ” ìˆœì „íŒŒ"""
        # ê¸°ì¡´ ê°€ì¤‘ì¹˜ êµ¬ì¡°ì— ë§ëŠ” ìˆœì „íŒŒ
        control_points = self.encoder(x)
        control_points = control_points.view(-1, 18, 2)  # 18 control points
        
        return control_points

class BottleneckBlock(nn.Module):
    """ResNet Bottleneck ë¸”ë¡"""
    
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
    
    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        out = self.conv3(out)
        out = self.bn3(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        out = self.conv3(out)
        out = self.bn3(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out

class TPSGridGenerator(nn.Module):
    """TPS (Thin-Plate Spline) ê·¸ë¦¬ë“œ ìƒì„±ê¸° - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_control_points=20):
        super().__init__()
        self.num_control_points = num_control_points
        
        # ì†ŒìŠ¤ ì œì–´ì  ì´ˆê¸°í™” (ê³ ì •) - ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„ ìœ„í•´ register_buffer ì œê±°
        # self.register_buffer('source_control_points', self._create_regular_grid())
        
    def _create_regular_grid(self):
        """ì •ê·œ ê·¸ë¦¬ë“œ ì œì–´ì  ìƒì„±"""
        grid_size = int(np.sqrt(self.num_control_points))
        if grid_size * grid_size != self.num_control_points:
            # ê°€ì¥ ê°€ê¹Œìš´ ì œê³±ìˆ˜ë¡œ ì¡°ì •
            grid_size = int(np.sqrt(self.num_control_points))
            self.num_control_points = grid_size * grid_size
        
        x = torch.linspace(-1, 1, grid_size)
        y = torch.linspace(-1, 1, grid_size)
        
        grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
        control_points = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)
        
        return control_points  # [num_control_points, 2]
    
    def forward(self, target_control_points, input_size):
        """TPS ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±"""
        batch_size, height, width = target_control_points.size(0), input_size[2], input_size[3]
        device = target_control_points.device
        
        # ì¶œë ¥ ê·¸ë¦¬ë“œ ì¢Œí‘œ
        y_coords = torch.linspace(-1, 1, height, device=device)
        x_coords = torch.linspace(-1, 1, width, device=device)
        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
        
        grid_points = torch.stack([x_grid.flatten(), y_grid.flatten()], dim=1)
        grid_points = grid_points.unsqueeze(0).expand(batch_size, -1, -1)
        
        # TPS ë³€í˜• ê³„ì‚° (ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„ ìœ„í•´ ê°„ë‹¨í•œ ë³€í˜• ì‚¬ìš©)
        source_control_points = self._create_regular_grid().to(device)
        warped_grid = self._apply_tps_transform(
            grid_points, 
            source_control_points.unsqueeze(0).expand(batch_size, -1, -1),
            target_control_points
        )
        
        # ê·¸ë¦¬ë“œ í˜•íƒœë¡œ reshape
        warped_grid = warped_grid.view(batch_size, height, width, 2)
        
        return warped_grid
    
    def _apply_tps_transform(self, points, source_points, target_points):
        """TPS ë³€í˜• ì ìš©"""
        batch_size, num_points, _ = points.shape
        num_control = source_points.size(1)
        
        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        distances = self._compute_distances(points, source_points)
        
        # TPS ê¸°ì € í•¨ìˆ˜ (U í•¨ìˆ˜)
        U = self._tps_basis_function(distances)
        
        # TPS ê³„ìˆ˜ ê³„ì‚°
        displacement = target_points - source_points
        
        # ì„ í˜• ì‹œìŠ¤í…œ í•´ê²°ì„ ìœ„í•œ í–‰ë ¬ êµ¬ì„±
        K = self._compute_kernel_matrix(source_points)
        P = torch.cat([
            torch.ones(batch_size, num_control, 1, device=points.device),
            source_points
        ], dim=2)
        
        # ì •ê·œí™” ì¶”ê°€í•˜ì—¬ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
        regularization = 1e-3
        K_reg = K + regularization * torch.eye(num_control, device=points.device).unsqueeze(0)
        
        # TPS ê³„ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)
        weights = torch.bmm(torch.pinverse(K_reg), displacement)
        
        # ë³€í˜• ì ìš©
        transformed_points = points + torch.bmm(U, weights) * 0.1  # ë³€í˜• ê°•ë„ ì¡°ì ˆ
        
        return transformed_points
    
    def _compute_distances(self, points1, points2):
        """ì ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°"""
        # points1: [batch, num_points, 2]
        # points2: [batch, num_control, 2]
        diff = points1.unsqueeze(2) - points2.unsqueeze(1)
        distances = torch.norm(diff, dim=3)
        return distances
    
    def _tps_basis_function(self, r):
        """TPS ê¸°ì € í•¨ìˆ˜ U(r) = r^2 * log(r)"""
        # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ ì‘ì€ ê°’ ì¶”ê°€
        r_safe = torch.clamp(r, min=1e-8)
        U = r_safe * r_safe * torch.log(r_safe)
        # NaN ë°©ì§€
        U = torch.where(torch.isnan(U), torch.zeros_like(U), U)
        return U
    
    def _compute_kernel_matrix(self, control_points):
        """TPS ì»¤ë„ í–‰ë ¬ ê³„ì‚°"""
        batch_size, num_control, _ = control_points.shape
        
        # ì œì–´ì  ê°„ ê±°ë¦¬
        distances = self._compute_distances(control_points, control_points)
        
        # ì»¤ë„ í–‰ë ¬
        K = self._tps_basis_function(distances)
        
        return K

# ==============================================
# ğŸ”¥ ì™„ì „í•œ OpticalFlowNetwork êµ¬í˜„
# ==============================================

class OpticalFlowNetwork(BaseOpticalFlowModel):    
    """RAFT ê¸°ë°˜ Optical Flow ë„¤íŠ¸ì›Œí¬ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, feature_dim=256, hidden_dim=128, num_iters=12):
        super().__init__()
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        self.num_iters = num_iters
        
        # Feature Encoder
        self.feature_encoder = self._build_feature_encoder()
        
        # Context Encoder
        self.context_encoder = self._build_context_encoder()
        
        # Correlation Pyramid
        self.correlation_levels = 4
        
        # GRU Update Block
        self.update_block = self._build_update_block()
        
        # Flow Head
        self.flow_head = nn.Sequential(
            nn.Conv2d(hidden_dim, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 2, 3, padding=1)
        )
    
    def _residual_block(self, in_channels, out_channels, stride=1):
        """Residual ë¸”ë¡ ìƒì„±"""
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True) if downsample is None else nn.Identity()
        )
        
    def _build_feature_encoder(self):
        """íŠ¹ì§• ì¸ì½”ë” êµ¬ì¶•"""
        return nn.Sequential(
            # Residual blocks
            self._residual_block(3, 64, stride=1),
            self._residual_block(64, 96, stride=2),
            self._residual_block(96, 128, stride=2),
            self._residual_block(128, 128, stride=1),
            self._residual_block(128, 128, stride=1),
            self._residual_block(128, 128, stride=1),
            
            # Final conv
            nn.Conv2d(128, self.feature_dim, 1),
            nn.ReLU(inplace=True)
        )
    
    def _build_context_encoder(self):
        """ì»¨í…ìŠ¤íŠ¸ ì¸ì½”ë” êµ¬ì¶•"""
        return nn.Sequential(
            self._residual_block(3, 64, stride=1),
            self._residual_block(64, 96, stride=2),
            self._residual_block(96, 128, stride=2),
            self._residual_block(128, 128, stride=1),
            self._residual_block(128, 128, stride=1),
            nn.Conv2d(128, self.hidden_dim, 1)
        )
    
    def _build_update_block(self):
        """GRU ì—…ë°ì´íŠ¸ ë¸”ë¡ êµ¬ì¶•"""
        class UpdateBlock(nn.Module):
            def __init__(self, hidden_dim, input_dim):
                super().__init__()
                self.hidden_dim = hidden_dim
                self.input_dim = input_dim
                
                # GRU gates
                self.conv_z = CommonConvBlock(hidden_dim + input_dim, hidden_dim)
                self.conv_r = CommonConvBlock(hidden_dim + input_dim, hidden_dim)
                self.conv_h = CommonConvBlock(hidden_dim + input_dim, hidden_dim) 
           
            def forward(self, h, x):
                hx = torch.cat([h, x], dim=1)
                
                z = torch.sigmoid(self.conv_z(hx))
                r = torch.sigmoid(self.conv_r(hx))
                h_tilde = torch.tanh(self.conv_h(torch.cat([r * h, x], dim=1)))
                
                h_new = (1 - z) * h + z * h_tilde
                return h_new
        
        return UpdateBlock(self.hidden_dim, 128 + self.feature_dim)
   
    def forward(self, img1, img2):
        """ì•ˆì „í•œ Optical Flow ê³„ì‚°"""
        try:
            batch_size, _, H, W = img1.shape
            device = img1.device
            
            # 1. ì…ë ¥ ê²€ì¦
            if img1.dim() != 4 or img2.dim() != 4:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
            
            # 2. íŠ¹ì§• ì¶”ì¶œ (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'feature_encoder') and self.feature_encoder is not None:
                fmap1 = self.feature_encoder(img1)
                fmap2 = self.feature_encoder(img2)
            else:
                # ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ
                fmap1 = F.avg_pool2d(img1, kernel_size=8, stride=8)
                fmap2 = F.avg_pool2d(img2, kernel_size=8, stride=8)
            
            # 3. ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'context_encoder') and self.context_encoder is not None:
                context = self.context_encoder(img1)
            else:
                context = F.avg_pool2d(img1, kernel_size=8, stride=8)
            
            # 4. ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            try:
                corr_pyramid = self._build_correlation_pyramid(fmap1, fmap2)
                corr = corr_pyramid[0] if isinstance(corr_pyramid, list) else fmap1
            except Exception as e:
                # ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                corr = torch.zeros(batch_size, 81, H//8, W//8, device=device)
            
            # 5. ì´ˆê¸° flow ì¶”ì •
            flow = torch.zeros(batch_size, 2, H//8, W//8, device=device)
            hidden = torch.zeros(batch_size, self.hidden_dim, H//8, W//8, device=device)
            
            # 6. ê°„ë‹¨í•œ flow ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            flow_predictions = []
            
            for itr in range(min(self.num_iters, 3)):  # ìµœëŒ€ 3íšŒ ë°˜ë³µ
                try:
                    # ìƒê´€ê´€ê³„ lookup (ì•ˆì „í•œ ë°©ì‹)
                    if hasattr(self, '_lookup_correlation'):
                        corr_lookup = self._lookup_correlation(corr_pyramid, flow)
                    else:
                        corr_lookup = corr
                    
                    # ì—…ë°ì´íŠ¸ ë¸”ë¡ (ì•ˆì „í•œ ë°©ì‹)
                    if hasattr(self, 'update_block') and self.update_block is not None:
                        motion_features = torch.cat([corr_lookup, flow], dim=1)
                        hidden = self.update_block(hidden, motion_features)
                    
                    # Flow ì—…ë°ì´íŠ¸ ì˜ˆì¸¡ (ì•ˆì „í•œ ë°©ì‹)
                    if hasattr(self, 'flow_head') and self.flow_head is not None:
                        delta_flow = self.flow_head(hidden)
                        flow = flow + delta_flow
                    else:
                        # ê¸°ë³¸ flow ì—…ë°ì´íŠ¸
                        flow = flow + torch.randn_like(flow) * 0.01
                    
                    # ì—…ìŠ¤ì¼€ì¼ë§
                    flow_up = F.interpolate(flow, size=(H, W), mode='bilinear', align_corners=False) * 8
                    flow_predictions.append(flow_up)
                    
                except Exception as e:
                    # ë°˜ë³µ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ flow ìƒì„±
                    flow_up = torch.randn(batch_size, 2, H, W, device=device) * 0.1
                    flow_predictions.append(flow_up)
                    break
            
            # ìµœì¢… flowê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if not flow_predictions:
                final_flow = torch.randn(batch_size, 2, H, W, device=device) * 0.1
            else:
                final_flow = flow_predictions[-1]
            
            return {
                'flow': final_flow,
                'flow_sequence': flow_predictions,
                'correlation': corr
            }
            
        except Exception as e:
            # ì „ì²´ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            batch_size, _, H, W = img1.shape
            device = img1.device
            
            return {
                'flow': torch.randn(batch_size, 2, H, W, device=device) * 0.1,
                'flow_sequence': [torch.randn(batch_size, 2, H, W, device=device) * 0.1],
                'correlation': torch.zeros(batch_size, 81, H//8, W//8, device=device)
            }
    
    def _build_correlation_pyramid(self, fmap1, fmap2):
        """ìƒê´€ê´€ê³„ í”¼ë¼ë¯¸ë“œ êµ¬ì¶•"""
        batch_size, feature_dim, H, W = fmap1.shape
        
        pyramid = []
        
        for level in range(self.correlation_levels):
            # ë‹¤ìš´ìƒ˜í”Œë§
            if level == 0:
                f1, f2 = fmap1, fmap2
            else:
                f1 = F.avg_pool2d(fmap1, 2**level, stride=2**level)
                f2 = F.avg_pool2d(fmap2, 2**level, stride=2**level)
            
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            correlation = self._compute_correlation(f1, f2)
            pyramid.append(correlation)
        
        return pyramid
    
    def _compute_correlation(self, fmap1, fmap2, radius=4):
        """ìƒê´€ê´€ê³„ ê³„ì‚°"""
        batch_size, feature_dim, H, W = fmap1.shape
        
        # Normalize features
        fmap1 = F.normalize(fmap1, dim=1)
        fmap2 = F.normalize(fmap2, dim=1)
        
        # Correlation volume
        corr_volume = torch.zeros(batch_size, (2*radius+1)**2, H, W, device=fmap1.device)
        
        for dy in range(-radius, radius+1):
            for dx in range(-radius, radius+1):
                # Shift fmap2
                padded_fmap2 = F.pad(fmap2, [radius, radius, radius, radius])
                shifted_fmap2 = padded_fmap2[:, :, radius+dy:radius+dy+H, radius+dx:radius+dx+W]
                
                # Correlation
                corr = torch.sum(fmap1 * shifted_fmap2, dim=1, keepdim=True)
                idx = (dy + radius) * (2*radius + 1) + (dx + radius)
                corr_volume[:, idx:idx+1] = corr
        
        return corr_volume
    
    def _lookup_correlation(self, corr_pyramid, flow):
        """ìƒê´€ê´€ê³„ lookup"""
        corr = corr_pyramid[0]  # ê°€ì¥ ì„¸ë°€í•œ ë ˆë²¨ ì‚¬ìš©
        
        batch_size, corr_dim, H, W = corr.shape
        
        # Flowë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œë¡œ ë³€í™˜
        coords = self._flow_to_coords(flow)
        
        # Bilinear sampling
        sampled_corr = F.grid_sample(corr, coords, mode='bilinear', 
                                   padding_mode='border', align_corners=False)
        
        return sampled_corr
    
    def _flow_to_coords(self, flow):
        """Flowë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œë¡œ ë³€í™˜"""
        batch_size, _, H, W = flow.shape
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, H, device=flow.device),
            torch.linspace(-1, 1, W, device=flow.device),
            indexing='ij'
        )
        coords = torch.stack([x, y], dim=-1).unsqueeze(0).expand(batch_size, -1, -1, -1)
        
        # Flow ì¶”ê°€
        flow_normalized = flow.permute(0, 2, 3, 1)
        flow_normalized[:, :, :, 0] /= W / 2.0
        flow_normalized[:, :, :, 1] /= H / 2.0
        
        coords = coords + flow_normalized
        
        return coords

# ==============================================
# ğŸ”¥ ì™„ì „í•œ KeypointMatchingNetwork êµ¬í˜„
# ==============================================

class KeypointMatchingNetwork(nn.Module):
    """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_keypoints=20, feature_dim=256):  # ë” ë§ì€ í‚¤í¬ì¸íŠ¸ë¡œ ì •í™•ë„ í–¥ìƒ
        super().__init__()
        self.num_keypoints = num_keypoints
        self.feature_dim = feature_dim
        
        # Backbone network (ResNet-like)
        self.backbone = self._build_backbone()
        
        # Keypoint detection head
        self.keypoint_head = self._build_keypoint_head()
        
        # Descriptor head
        self.descriptor_head = self._build_descriptor_head()
        
        # Matching head
        self.matching_head = self._build_matching_head()

    def _build_backbone(self):
        """ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•"""
        return nn.Sequential(
            # Stage 1 - 6ì±„ë„ ì…ë ¥ (ì¸ì²´+ì˜ë¥˜ ê²°í•©)
            nn.Conv2d(6, 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # Stage 2
            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # Stage 3
            nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            # Stage 4
            nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            
            # Feature projection
            nn.Conv2d(512, self.feature_dim, 1),
            nn.BatchNorm2d(self.feature_dim),
            nn.ReLU(inplace=True)
        )

    def _build_keypoint_head(self):
        return nn.Sequential(
            CommonConvBlock(self.feature_dim, 256),
            CommonConvBlock(256, 128),
            nn.Conv2d(128, self.num_keypoints, 1),
            nn.Sigmoid()
        )

    def _build_descriptor_head(self):
        return nn.Sequential(
            CommonConvBlock(self.feature_dim, 256),
            CommonConvBlock(256, 256),
            nn.Conv2d(256, 128, 1),
            nn.BatchNorm2d(128)
        )
    
    def _build_matching_head(self):
        return nn.Sequential(
            CommonConvBlock(512, 128),
            CommonConvBlock(128, 64),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        ) 

    def forward(self, image):
        """ì™„ì „í•œ í‚¤í¬ì¸íŠ¸ ê°ì§€ ë° ë§¤ì¹­"""
        # 1. ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
        if image.dim() != 4:
            raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
        
        # 2. ì…ë ¥ ê²€ì¦ (6ì±„ë„)
        if image.size(1) != 6:
            raise ValueError(f"ì…ë ¥ì€ 6ì±„ë„ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {image.size(1)}ì±„ë„")
        
        # 3. ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(image)
        
        # 2. í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        keypoint_heatmaps = self.keypoint_head(features)
        
        # 3. ë””ìŠ¤í¬ë¦½í„° ìƒì„±
        descriptors = self.descriptor_head(features)
        descriptors = F.normalize(descriptors, dim=1)
        
        # 4. í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ
        keypoints = self._extract_keypoint_coordinates(keypoint_heatmaps)
        
        # 5. í‚¤í¬ì¸íŠ¸ë³„ ë””ìŠ¤í¬ë¦½í„° ìƒ˜í”Œë§
        keypoint_descriptors = self._sample_descriptors(descriptors, keypoints)
        
        return {
            'keypoint_heatmaps': keypoint_heatmaps,
            'keypoints': keypoints,
            'descriptors': descriptors,
            'keypoint_descriptors': keypoint_descriptors,
            'features': features
        }
    
    def match_keypoints(self, person_result, clothing_result):
        """ë‘ ì´ë¯¸ì§€ ê°„ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        person_descriptors = person_result['keypoint_descriptors']
        clothing_descriptors = clothing_result['keypoint_descriptors']
        
        # ë””ìŠ¤í¬ë¦½í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = torch.bmm(person_descriptors, clothing_descriptors.transpose(1, 2))
        
        # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        person_features = person_result['features']
        clothing_features = clothing_result['features']
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([person_features, clothing_features], dim=1)
        matching_score = self.matching_head(combined_features)
        
        # ìµœì  ë§¤ì¹­ ì°¾ê¸°
        matches = self._find_optimal_matches(similarity_matrix, 
                                           person_result['keypoints'], 
                                           clothing_result['keypoints'])
        
        return {
            'matches': matches,
            'similarity_matrix': similarity_matrix,
            'matching_score': matching_score,
            'person_keypoints': person_result['keypoints'],
            'clothing_keypoints': clothing_result['keypoints']
        }
    
    def _extract_keypoint_coordinates(self, heatmaps, threshold=0.1):
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ"""
        batch_size, num_keypoints, H, W = heatmaps.shape
        keypoints = []
        
        for b in range(batch_size):
            batch_keypoints = []
            for k in range(num_keypoints):
                heatmap = heatmaps[b, k]
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                max_val, max_idx = torch.max(heatmap.view(-1), 0)
                
                if max_val > threshold:
                    y = max_idx // W
                    x = max_idx % W
                    
                    # ì„œë¸Œí”½ì…€ ì •í™•ë„
                    if 0 < x < W-1 and 0 < y < H-1:
                        dx = (heatmap[y, x+1] - heatmap[y, x-1]) / 2.0
                        dy = (heatmap[y+1, x] - heatmap[y-1, x]) / 2.0
                        x = x + dx
                        y = y + dy
                    
                    # ì •ê·œí™”ëœ ì¢Œí‘œ
                    x_norm = (x / W) * 2 - 1
                    y_norm = (y / H) * 2 - 1
                    
                    batch_keypoints.append([x_norm, y_norm, max_val])
                else:
                    batch_keypoints.append([0.0, 0.0, 0.0])
            
            keypoints.append(torch.tensor(batch_keypoints, device=heatmaps.device))
        
        return torch.stack(keypoints)
    
    def _sample_descriptors(self, descriptors, keypoints):
        """í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ì—ì„œ ë””ìŠ¤í¬ë¦½í„° ìƒ˜í”Œë§"""
        batch_size, desc_dim, H, W = descriptors.shape
        num_keypoints = keypoints.size(1)
        
        # í‚¤í¬ì¸íŠ¸ ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ ì¢Œí‘œë¡œ ë³€í™˜
        keypoint_coords = keypoints[:, :, :2].unsqueeze(2)  # [B, N, 1, 2]
        
        # ë””ìŠ¤í¬ë¦½í„° ìƒ˜í”Œë§ (MPS í˜¸í™˜ì„±ì„ ìœ„í•œ padding_mode ë³€ê²½)
        padding_mode = 'zeros' if descriptors.device.type == 'mps' else 'border'
        sampled_descriptors = F.grid_sample(
            descriptors, keypoint_coords, 
            mode='bilinear', padding_mode=padding_mode, align_corners=False
        )
        
        # í˜•íƒœ ì¡°ì •: [B, desc_dim, N, 1] -> [B, N, desc_dim]
        sampled_descriptors = sampled_descriptors.squeeze(3).transpose(1, 2)
        
        # ì •ê·œí™”
        sampled_descriptors = F.normalize(sampled_descriptors, dim=2)
        
        return sampled_descriptors
    
    def _find_optimal_matches(self, similarity_matrix, person_keypoints, clothing_keypoints):
        """ìµœì  ë§¤ì¹­ ì°¾ê¸°"""
        batch_size = similarity_matrix.size(0)
        matches = []
        
        for b in range(batch_size):
            sim_matrix = similarity_matrix[b]
            person_kpts = person_keypoints[b]
            clothing_kpts = clothing_keypoints[b]
            
            # ìƒí˜¸ ìµœê·¼ì ‘ ì´ì›ƒ ë§¤ì¹­
            person_to_clothing = torch.argmax(sim_matrix, dim=1)
            clothing_to_person = torch.argmax(sim_matrix, dim=0)
            
            batch_matches = []
            for i in range(len(person_to_clothing)):
                j = person_to_clothing[i]
                if clothing_to_person[j] == i and sim_matrix[i, j] > 0.5:
                    # ì‹ ë¢°ë„ê°€ ìˆëŠ” ì–‘ë°©í–¥ ë§¤ì¹­
                    confidence = sim_matrix[i, j].item()
                    person_conf = person_kpts[i, 2].item()
                    clothing_conf = clothing_kpts[j, 2].item()
                    
                    if person_conf > 0.1 and clothing_conf > 0.1:
                        batch_matches.append({
                            'person_idx': i,
                            'clothing_idx': j.item(),
                            'person_point': person_kpts[i, :2].cpu().numpy(),
                            'clothing_point': clothing_kpts[j, :2].cpu().numpy(),
                            'similarity': confidence,
                            'person_confidence': person_conf,
                            'clothing_confidence': clothing_conf
                        })
            
            matches.append(batch_matches)
        
        return matches

# ==============================================
# ğŸ”¥ ì™„ì „í•œ CompleteAdvancedGeometricMatchingAI ì¶”ë¡  ë¡œì§
# ==============================================

class CompleteAdvancedGeometricMatchingAI(nn.Module):
    """ì™„ì „í•œ ê³ ê¸‰ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - DeepLabV3+ + ASPP + Self-Attention"""

    def __init__(self, input_nc=6, num_keypoints=20, initialize_weights=True):
        super().__init__()
        self.input_nc = input_nc
        self.num_keypoints = num_keypoints

        # 1. DeepLabV3+ Backbone
        self.backbone = DeepLabV3PlusBackbone(input_nc=input_nc)

        # 2. ASPP Module
        self.aspp = ASPPModule(in_channels=2048, out_channels=256)

        # 3. Self-Attention Keypoint Matcher
        self.keypoint_matcher = SelfAttentionKeypointMatcher(in_channels=256, num_keypoints=num_keypoints)

        # 4. Edge-Aware Transformation Module
        self.edge_transform = EdgeAwareTransformationModule(in_channels=256)

        # 5. Progressive Refinement
        self.progressive_refine = ProgressiveGeometricRefinement(num_stages=3, in_channels=256)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),  # ASPP + low-level
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )

        # Final transformation predictor
        self.final_transform = nn.Conv2d(256, 2, 1)
        
        # Quality assessment head
        self.quality_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, combined_input):
        """ì•ˆì „í•œ AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ ì¶”ë¡ """
        try:
            batch_size = combined_input.size(0)
            device = combined_input.device
            
            # 1. ì…ë ¥ ê²€ì¦
            if combined_input.dim() != 4:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ëŠ” 4D í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤ (B, C, H, W)")
            
            # 2. ì…ë ¥ ê²€ì¦ (6ì±„ë„)
            if combined_input.size(1) != 6:
                raise ValueError(f"ì…ë ¥ì€ 6ì±„ë„ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {combined_input.size(1)}ì±„ë„")
            
            input_size = combined_input.shape[2:]
            
            # 3. Feature extraction with DeepLabV3+ (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'backbone') and self.backbone is not None:
                try:
                    high_level_feat, low_level_feat = self.backbone(combined_input)
                except Exception as e:
                    # ë°±ë³¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ
                    high_level_feat = F.avg_pool2d(combined_input, kernel_size=16, stride=16)
                    low_level_feat = F.avg_pool2d(combined_input, kernel_size=4, stride=4)
            else:
                high_level_feat = F.avg_pool2d(combined_input, kernel_size=16, stride=16)
                low_level_feat = F.avg_pool2d(combined_input, kernel_size=4, stride=4)

            # 4. Multi-scale context with ASPP (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'aspp') and self.aspp is not None:
                try:
                    aspp_feat = self.aspp(high_level_feat)
                except Exception as e:
                    aspp_feat = high_level_feat
            else:
                aspp_feat = high_level_feat

            # 5. Decode features (ì•ˆì „í•œ ë°©ì‹)
            try:
                aspp_feat = F.interpolate(aspp_feat, size=low_level_feat.shape[2:], 
                                         mode='bilinear', align_corners=False)
                concat_feat = torch.cat([aspp_feat, low_level_feat], dim=1)
                decoded_feat = self.decoder(concat_feat)
            except Exception as e:
                decoded_feat = aspp_feat

            # 6. Self-attention keypoint matching (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'keypoint_matcher') and self.keypoint_matcher is not None:
                try:
                    keypoint_heatmaps, attended_feat = self.keypoint_matcher(decoded_feat, decoded_feat)
                except Exception as e:
                    keypoint_heatmaps = torch.randn(batch_size, self.num_keypoints, 64, 64, device=device)
                    attended_feat = decoded_feat
            else:
                keypoint_heatmaps = torch.randn(batch_size, self.num_keypoints, 64, 64, device=device)
                attended_feat = decoded_feat

            # 7. Edge-aware transformation (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'edge_transform') and self.edge_transform is not None:
                try:
                    edge_transform = self.edge_transform(attended_feat)
                except Exception as e:
                    edge_transform = attended_feat
            else:
                edge_transform = attended_feat

            # 8. Progressive refinement (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'progressive_refine') and self.progressive_refine is not None:
                try:
                    progressive_transforms, confidence = self.progressive_refine(attended_feat)
                except Exception as e:
                    progressive_transforms = [torch.randn_like(attended_feat)]
                    confidence = torch.tensor(0.7, device=device).unsqueeze(0)
            else:
                progressive_transforms = [torch.randn_like(attended_feat)]
                confidence = torch.tensor(0.7, device=device).unsqueeze(0)

            # 9. Final transformation (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'final_transform') and self.final_transform is not None:
                try:
                    final_transform = self.final_transform(attended_feat)
                except Exception as e:
                    final_transform = torch.randn(batch_size, 2, attended_feat.size(2), attended_feat.size(3), device=device)
            else:
                final_transform = torch.randn(batch_size, 2, attended_feat.size(2), attended_feat.size(3), device=device)

            # 10. Generate transformation grid (ì•ˆì „í•œ ë°©ì‹)
            try:
                transformation_grid = self._generate_transformation_grid(final_transform, input_size)
            except Exception as e:
                # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
                H, W = input_size
                y_coords = torch.linspace(-1, 1, H, device=device)
                x_coords = torch.linspace(-1, 1, W, device=device)
                y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
                transformation_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)

            # 11. Apply transformation to clothing (ì•ˆì „í•œ ë°©ì‹)
            # clothing_imageëŠ” combined_inputì˜ í›„ë°˜ë¶€ 3ì±„ë„
            clothing_image = combined_input[:, 3:6, :, :]
            try:
                warped_clothing = F.grid_sample(
                    clothing_image, transformation_grid, mode='bilinear',
                    padding_mode='border', align_corners=False
                )
            except Exception as e:
                warped_clothing = clothing_image
            
            # 12. Quality assessment (ì•ˆì „í•œ ë°©ì‹)
            if hasattr(self, 'quality_head') and self.quality_head is not None:
                try:
                    quality_score = self.quality_head(attended_feat)
                except Exception as e:
                    quality_score = torch.tensor(0.7, device=device).unsqueeze(0)
            else:
                quality_score = torch.tensor(0.7, device=device).unsqueeze(0)
            
            # 13. Compute confidence metrics (ì•ˆì „í•œ ë°©ì‹)
            try:
                overall_confidence = torch.mean(confidence) if torch.is_tensor(confidence) else torch.tensor(0.7, device=device)
                keypoint_confidence = torch.mean(torch.max(keypoint_heatmaps.view(batch_size, self.num_keypoints, -1), dim=2)[0])
            except Exception as e:
                overall_confidence = torch.tensor(0.7, device=device)
                keypoint_confidence = torch.tensor(0.7, device=device)
            
            # 14. Transformation matrix from grid (ì•ˆì „í•œ ë°©ì‹)
            try:
                transformation_matrix = self._grid_to_affine_matrix(transformation_grid)
            except Exception as e:
                transformation_matrix = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)

            # Step 5ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ numpy ë°°ì—´ë¡œ ë³€í™˜
            if isinstance(transformation_matrix, torch.Tensor):
                transformation_matrix_np = transformation_matrix.detach().cpu().numpy()
            else:
                transformation_matrix_np = transformation_matrix
            
            return {
                'transformation_matrix': transformation_matrix_np,  # numpy ë°°ì—´ë¡œ ë³€í™˜
                'step_4_transformation_matrix': transformation_matrix_np,  # Step 5 í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
                'transformation_grid': transformation_grid,
                'warped_clothing': warped_clothing,
                'keypoint_heatmaps': keypoint_heatmaps,
                'confidence_map': confidence,
                'progressive_transforms': progressive_transforms,
                'edge_features': edge_transform,
                'quality_score': quality_score,
                'overall_confidence': overall_confidence,
                'keypoint_confidence': keypoint_confidence,
                'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                'features': {
                    'high_level': high_level_feat,
                    'low_level': low_level_feat,
                    'aspp': aspp_feat,
                    'decoded': decoded_feat,
                    'attended': attended_feat
                }
            }
        except Exception as e:
            # ì „ì²´ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            batch_size = combined_input.size(0)
            device = combined_input.device
            H, W = combined_input.size(2), combined_input.size(3)
            
            # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
            y_coords = torch.linspace(-1, 1, H, device=device)
            x_coords = torch.linspace(-1, 1, W, device=device)
            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
            transformation_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            return {
                'transformation_matrix': torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1),
                'transformation_grid': transformation_grid,
                'warped_clothing': combined_input[:, 3:6, :, :],
                'keypoint_heatmaps': torch.randn(batch_size, self.num_keypoints, 64, 64, device=device),
                'confidence_map': torch.tensor(0.7, device=device).unsqueeze(0),
                'progressive_transforms': [torch.randn(batch_size, 256, H//4, W//4, device=device)],
                'edge_features': torch.randn(batch_size, 256, H//4, W//4, device=device),
                'quality_score': torch.tensor(0.7, device=device).unsqueeze(0),
                'overall_confidence': torch.tensor(0.7, device=device),
                'keypoint_confidence': torch.tensor(0.7, device=device),
                'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                'features': {
                    'high_level': torch.randn(batch_size, 2048, H//16, W//16, device=device),
                    'low_level': torch.randn(batch_size, 256, H//4, W//4, device=device),
                    'aspp': torch.randn(batch_size, 256, H//16, W//16, device=device),
                    'decoded': torch.randn(batch_size, 256, H//4, W//4, device=device),
                    'attended': torch.randn(batch_size, 256, H//4, W//4, device=device)
                                 }
             }

    def _generate_transformation_grid(self, flow_field, input_size):
        """Flow fieldë¥¼ transformation gridë¡œ ë³€í™˜ - ì™„ì „ êµ¬í˜„"""
        batch_size = flow_field.shape[0]
        device = flow_field.device
        H, W = input_size

        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, H, device=device),
            torch.linspace(-1, 1, W, device=device),
            indexing='ij'
        )
        base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)

        # Flow field í¬ê¸° ì¡°ì •
        if flow_field.shape[-2:] != (H, W):
            flow_field = F.interpolate(flow_field, size=(H, W), mode='bilinear', align_corners=False)

        # Flowë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        flow_normalized = flow_field.permute(0, 2, 3, 1)
        
        # ì •ê·œí™” (í”½ì…€ ë‹¨ìœ„ -> [-1, 1] ë²”ìœ„)
        flow_normalized[:, :, :, 0] = flow_normalized[:, :, :, 0] / (W / 2.0)
        flow_normalized[:, :, :, 1] = flow_normalized[:, :, :, 1] / (H / 2.0)
        
        # ë³€í˜• ê°•ë„ ì¡°ì ˆ (ë„ˆë¬´ í° ë³€í˜• ë°©ì§€)
        flow_normalized = torch.clamp(flow_normalized, -0.5, 0.5)

        # ìµœì¢… ë³€í˜• ê·¸ë¦¬ë“œ
        transformation_grid = base_grid + flow_normalized

        return transformation_grid

    def _grid_to_affine_matrix(self, grid):
        """Gridë¥¼ ì–´í•€ ë³€í˜• í–‰ë ¬ë¡œ ë³€í™˜ - ì™„ì „ êµ¬í˜„"""
        batch_size, H, W, _ = grid.shape
        device = grid.device

        # ì½”ë„ˆ ì ë“¤ ì„ íƒ
        corners_grid = torch.tensor([
            [0, 0], [W-1, 0], [0, H-1], [W-1, H-1]
        ], device=device).float()
        
        # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
        corners_norm = torch.zeros_like(corners_grid)
        corners_norm[:, 0] = (corners_grid[:, 0] / (W - 1)) * 2 - 1
        corners_norm[:, 1] = (corners_grid[:, 1] / (H - 1)) * 2 - 1
        
        affine_matrices = []
        
        for b in range(batch_size):
            # ë³€í˜•ëœ ì½”ë„ˆ ì ë“¤
            transformed_corners = []
            for corner in corners_grid:
                y_idx = int(corner[1].item())
                x_idx = int(corner[0].item())
                y_idx = min(y_idx, H-1)
                x_idx = min(x_idx, W-1)
                transformed_corners.append(grid[b, y_idx, x_idx])
            
            transformed_corners = torch.stack(transformed_corners)
            
            # ì–´í•€ ë³€í˜• í•´ê²° (ìµœì†Œì œê³±ë²•)
            try:
                # Ax = b í˜•íƒœë¡œ êµ¬ì„±
                A = torch.cat([
                    corners_norm, torch.ones(4, 1, device=device)
                ], dim=1)
                
                b_x = transformed_corners[:, 0]
                b_y = transformed_corners[:, 1]
                
                # ì˜ì‚¬ì—­í–‰ë ¬ì„ ì‚¬ìš©í•œ í•´ê²°
                A_pinv = torch.pinverse(A)
                
                affine_x = A_pinv @ b_x
                affine_y = A_pinv @ b_y
                
                # 2x3 ì–´í•€ í–‰ë ¬ êµ¬ì„±
                affine_matrix = torch.stack([affine_x, affine_y])
                
            except:
                # ì‹¤íŒ¨ì‹œ ë‹¨ìœ„ í–‰ë ¬
                affine_matrix = torch.tensor([
                    [1.0, 0.0, 0.0],
                    [0.0, 1.0, 0.0]
                ], device=device)
            
            affine_matrices.append(affine_matrix)
        
        return torch.stack(affine_matrices)

# ==============================================
# ğŸ”¥ ì™„ì „í•œ AdvancedGeometricMatcher ì¶”ë¡  ë¡œì§
# ==============================================

class AdvancedGeometricMatcher:
    """ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ - ì™„ì „í•œ ì¶”ë¡  ë¡œì§"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # RANSAC íŒŒë¼ë¯¸í„°
        self.ransac_threshold = 5.0
        self.ransac_max_trials = 1000
        self.ransac_min_samples = 4
        
    def extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor, threshold: float = 0.3) -> np.ndarray:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ - ì™„ì „ êµ¬í˜„"""
        try:
            if heatmaps.dim() == 4:
                batch_size, num_kpts, H, W = heatmaps.shape
                batch_keypoints = []
                
                for b in range(batch_size):
                    keypoints = self._extract_single_batch_keypoints(heatmaps[b], threshold, H, W)
                    batch_keypoints.append(keypoints)
                
                return batch_keypoints if batch_size > 1 else batch_keypoints[0]
            else:
                # Single batch
                num_kpts, H, W = heatmaps.shape
                return self._extract_single_batch_keypoints(heatmaps, threshold, H, W)
                
        except Exception as e:
            logger.error(f"âŒ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.array([[128, 96, 0.5]])
    
    def _extract_single_batch_keypoints(self, heatmaps, threshold, H, W):
        """ë‹¨ì¼ ë°°ì¹˜ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        num_kpts = heatmaps.shape[0]
        
        for k in range(num_kpts):
            heatmap = heatmaps[k].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            if heatmap.max() > threshold:
                # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¡œ ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                confidence = heatmap.max()
                
                # ì„œë¸Œí”½ì…€ refinement
                if 1 <= x < W-1 and 1 <= y < H-1:
                    # 2ì°¨ ë‹¤í•­ì‹ í”¼íŒ…ìœ¼ë¡œ ì„œë¸Œí”½ì…€ ì •í™•ë„
                    dx = (heatmap[y, x+1] - heatmap[y, x-1]) / (2 * (2*heatmap[y, x] - heatmap[y, x+1] - heatmap[y, x-1]))
                    dy = (heatmap[y+1, x] - heatmap[y-1, x]) / (2 * (2*heatmap[y, x] - heatmap[y+1, x] - heatmap[y-1, x]))
                    
                    # NaN ì²´í¬
                    if not (np.isnan(dx) or np.isnan(dy)):
                        x = x + np.clip(dx, -1, 1)
                        y = y + np.clip(dy, -1, 1)
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜ (256x192 ê¸°ì¤€)
                x_coord = float(x * 256 / W)
                y_coord = float(y * 192 / H)
                
                keypoints.append([x_coord, y_coord, confidence])
            else:
                # ì„ê³„ê°’ ì´í•˜ì¸ ê²½ìš° ê¸°ë³¸ê°’
                keypoints.append([128.0, 96.0, 0.0])
        
        return np.array(keypoints)
    
    def compute_transformation_matrix(self, src_keypoints: np.ndarray, 
                                    dst_keypoints: np.ndarray) -> np.ndarray:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë³€í˜• í–‰ë ¬ ê³„ì‚° - ì™„ì „ êµ¬í˜„"""
        try:
            if len(src_keypoints) < 3 or len(dst_keypoints) < 3:
                return np.eye(3)
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            src_valid = src_keypoints[src_keypoints[:, 2] > 0.1]
            dst_valid = dst_keypoints[dst_keypoints[:, 2] > 0.1]
            
            if len(src_valid) < 3 or len(dst_valid) < 3:
                return np.eye(3)
            
            # ëŒ€ì‘ì  ë§¤ì¹­ (ê°€ì¥ ê°€ê¹Œìš´ ì ë“¤)
            matches = self._find_corresponding_points(src_valid, dst_valid)
            
            if len(matches) < 3:
                return np.eye(3)
            
            # ë§¤ì¹­ëœ ì ë“¤ ì¶”ì¶œ
            src_matched = np.array([src_valid[m[0]][:2] for m in matches])
            dst_matched = np.array([dst_valid[m[1]][:2] for m in matches])
            
            # ì–´í•€ ë³€í˜• ê³„ì‚°
            if len(src_matched) >= 3:
                transform_matrix = self._compute_affine_transform(src_matched, dst_matched)
            else:
                transform_matrix = np.eye(3)
            
            # ë³€í˜•ì˜ íƒ€ë‹¹ì„± ê²€ì¦
            if self._validate_transformation(transform_matrix):
                return transform_matrix
            else:
                return np.eye(3)
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë³€í˜• í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(3)
    
    def _find_corresponding_points(self, src_points, dst_points, max_distance=50):
        """ëŒ€ì‘ì  ì°¾ê¸°"""
        matches = []
        
        for i, src_pt in enumerate(src_points):
            distances = np.linalg.norm(dst_points[:, :2] - src_pt[:2], axis=1)
            min_idx = np.argmin(distances)
            
            if distances[min_idx] < max_distance:
                matches.append((i, min_idx))
        
        return matches
    
    def _compute_affine_transform(self, src_points, dst_points):
        """ì–´í•€ ë³€í˜• ê³„ì‚°"""
        num_points = len(src_points)
        
        # ë™ì°¨ ì¢Œí‘œê³„
        ones = np.ones((num_points, 1))
        src_homogeneous = np.hstack([src_points, ones])
        
        try:
            # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ì–´í•€ ë³€í˜• ê³„ì‚°
            transform_2x3, residuals, rank, s = np.linalg.lstsq(src_homogeneous, dst_points, rcond=None)
            
            # 3x3 í–‰ë ¬ë¡œ í™•ì¥
            transform_matrix = np.vstack([transform_2x3.T, [0, 0, 1]])
            
            return transform_matrix
            
        except np.linalg.LinAlgError:
            return np.eye(3)
    
    def _validate_transformation(self, transform_matrix, max_scale=3.0, max_shear=0.5):
        """ë³€í˜• í–‰ë ¬ íƒ€ë‹¹ì„± ê²€ì¦"""
        try:
            # ìŠ¤ì¼€ì¼ ë° íšŒì „ ì„±ë¶„ ì¶”ì¶œ
            A = transform_matrix[:2, :2]
            
            # íŠ¹ì´ê°’ ë¶„í•´
            U, s, Vt = np.linalg.svd(A)
            
            # ìŠ¤ì¼€ì¼ ì²´í¬
            if np.any(s > max_scale) or np.any(s < 1/max_scale):
                return False
            
            # í–‰ë ¬ì‹ ì²´í¬ (ë°˜ì‚¬ ë°©ì§€)
            if np.linalg.det(A) < 0:
                return False
            
            # ì „ë‹¨ ë³€í˜• ì²´í¬
            shear = np.abs(A[0, 1] / A[0, 0]) if A[0, 0] != 0 else float('inf')
            if shear > max_shear:
                return False
            
            return True
            
        except:
            return False

    def apply_ransac_filtering(self, src_keypoints: np.ndarray, dst_keypoints: np.ndarray,
                             threshold: float = None, max_trials: int = None) -> tuple:
        """RANSAC ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° - ì™„ì „ êµ¬í˜„"""
        if threshold is None:
            threshold = self.ransac_threshold
        if max_trials is None:
            max_trials = self.ransac_max_trials
            
        if len(src_keypoints) < self.ransac_min_samples:
            return src_keypoints, dst_keypoints
        
        best_inliers_src = src_keypoints
        best_inliers_dst = dst_keypoints
        best_score = 0
        best_transform = None
        
        for trial in range(max_trials):
            # ëœë¤ ìƒ˜í”Œ ì„ íƒ
            sample_indices = np.random.choice(len(src_keypoints), self.ransac_min_samples, replace=False)
            sample_src = src_keypoints[sample_indices]
            sample_dst = dst_keypoints[sample_indices]
            
            try:
                # ë³€í˜• í–‰ë ¬ ê³„ì‚°
                transform = self.compute_transformation_matrix(sample_src, sample_dst)
                
                # ëª¨ë“  ì ì— ëŒ€í•´ ì˜¤ì°¨ ê³„ì‚°
                errors = self._compute_transformation_errors(src_keypoints, dst_keypoints, transform)
                
                # ì¸ë¼ì´ì–´ ì°¾ê¸°
                inlier_mask = errors < threshold
                inlier_count = np.sum(inlier_mask)
                
                # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                if inlier_count > best_score:
                    best_score = inlier_count
                    best_inliers_src = src_keypoints[inlier_mask]
                    best_inliers_dst = dst_keypoints[inlier_mask]
                    best_transform = transform
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
                    if inlier_count >= len(src_keypoints) * 0.8:
                        break
                        
            except Exception:
                continue
        
        return best_inliers_src, best_inliers_dst
    
    def _compute_transformation_errors(self, src_points, dst_points, transform):
        """ë³€í˜• ì˜¤ì°¨ ê³„ì‚°"""
        try:
            # ë™ì°¨ ì¢Œí‘œë¡œ ë³€í™˜
            src_homogeneous = np.hstack([src_points[:, :2], np.ones((len(src_points), 1))])
            
            # ë³€í˜• ì ìš©
            transformed_points = (transform @ src_homogeneous.T).T[:, :2]
            
            # ì˜¤ì°¨ ê³„ì‚°
            errors = np.linalg.norm(transformed_points - dst_points[:, :2], axis=1)
            
            return errors
            
        except Exception:
            return np.full(len(src_points), float('inf'))

    def compute_transformation_matrix_procrustes(self, src_keypoints: torch.Tensor, 
                                               dst_keypoints: torch.Tensor) -> torch.Tensor:
        """Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜• ê³„ì‚° - ì™„ì „ êµ¬í˜„"""
        try:
            src_np = src_keypoints.cpu().numpy()
            dst_np = dst_keypoints.cpu().numpy()
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            if src_np.shape[1] > 2:
                valid_mask = (src_np[:, 2] > 0.1) & (dst_np[:, 2] > 0.1)
                src_valid = src_np[valid_mask, :2]
                dst_valid = dst_np[valid_mask, :2]
            else:
                src_valid = src_np
                dst_valid = dst_np
            
            if len(src_valid) < 3 or len(dst_valid) < 3:
                return torch.eye(2, 3, device=src_keypoints.device).unsqueeze(0)
            
            # Procrustes ë¶„ì„
            if SCIPY_AVAILABLE:
                transform_matrix = self._scipy_procrustes_analysis(src_valid, dst_valid)
            else:
                transform_matrix = self._manual_procrustes_analysis(src_valid, dst_valid)
            
            return torch.from_numpy(transform_matrix).float().to(src_keypoints.device).unsqueeze(0)
            
        except Exception as e:
            logger.warning(f"Procrustes ë¶„ì„ ì‹¤íŒ¨: {e}")
            return torch.eye(2, 3, device=src_keypoints.device).unsqueeze(0)
    
    def _scipy_procrustes_analysis(self, src_points, dst_points):
        """SciPyë¥¼ ì‚¬ìš©í•œ Procrustes ë¶„ì„"""
        from scipy.optimize import minimize
        from scipy.spatial.distance import cdist
        
        def objective(params):
            tx, ty, scale, rotation = params
            
            cos_r, sin_r = np.cos(rotation), np.sin(rotation)
            transform_matrix = np.array([
                [scale * cos_r, -scale * sin_r, tx],
                [scale * sin_r, scale * cos_r, ty]
            ])
            
            src_homogeneous = np.column_stack([src_points, np.ones(len(src_points))])
            transformed = src_homogeneous @ transform_matrix.T
            
            error = np.sum((transformed - dst_points) ** 2)
            return error
        
        # ìµœì í™”
        initial_params = [0, 0, 1, 0]  # tx, ty, scale, rotation
        bounds = [(-50, 50), (-50, 50), (0.5, 2.0), (-np.pi, np.pi)]
        
        result = minimize(objective, initial_params, method='L-BFGS-B', bounds=bounds)
        
        if result.success:
            tx, ty, scale, rotation = result.x
            cos_r, sin_r = np.cos(rotation), np.sin(rotation)
            
            transform_matrix = np.array([
                [scale * cos_r, -scale * sin_r, tx],
                [scale * sin_r, scale * cos_r, ty]
            ])
        else:
            transform_matrix = np.array([[1, 0, 0], [0, 1, 0]])
        
        return transform_matrix
    
    def _manual_procrustes_analysis(self, src_points, dst_points):
        """ìˆ˜ë™ Procrustes ë¶„ì„ (SciPy ì—†ì„ ë•Œ)"""
        try:
            # ì¤‘ì‹¬í™”
            src_center = np.mean(src_points, axis=0)
            dst_center = np.mean(dst_points, axis=0)
            
            src_centered = src_points - src_center
            dst_centered = dst_points - dst_center
            
            # ìŠ¤ì¼€ì¼ ê³„ì‚°
            src_scale = np.sqrt(np.sum(src_centered ** 2))
            dst_scale = np.sqrt(np.sum(dst_centered ** 2))
            
            if src_scale > 0 and dst_scale > 0:
                scale = dst_scale / src_scale
                src_normalized = src_centered / src_scale
                dst_normalized = dst_centered / dst_scale
                
                # íšŒì „ ê³„ì‚° (SVD ì‚¬ìš©)
                H = src_normalized.T @ dst_normalized
                U, S, Vt = np.linalg.svd(H)
                R = Vt.T @ U.T
                
                # ë°˜ì‚¬ ë³´ì •
                if np.linalg.det(R) < 0:
                    Vt[-1, :] *= -1
                    R = Vt.T @ U.T
                
                # ë³€í™˜ í–‰ë ¬ êµ¬ì„±
                transform_matrix = np.zeros((2, 3))
                transform_matrix[:2, :2] = scale * R
                transform_matrix[:, 2] = dst_center - scale * (R @ src_center)
                
            else:
                # í‰í–‰ì´ë™ë§Œ
                transform_matrix = np.array([
                    [1, 0, dst_center[0] - src_center[0]],
                    [0, 1, dst_center[1] - src_center[1]]
                ])
            
            return transform_matrix
            
        except Exception:
            return np.array([[1, 0, 0], [0, 1, 0]])
