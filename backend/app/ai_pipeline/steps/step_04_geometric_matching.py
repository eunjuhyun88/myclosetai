#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ (ì™„ì „ ê°œì„  ë²„ì „ - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)
===========================================================================

âœ… ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€
âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„  
âœ… ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™” ë° ì¼ê´€ì„± í™•ë³´
âœ… BaseStepMixin ì™„ì „ í˜¸í™˜
âœ… TYPE_CHECKING íŒ¨í„´ ìœ ì§€
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… AI ëª¨ë¸ ì—°ë™ ì™„ì „ êµ¬í˜„
âœ… Step 01 ì„±ê³µ íŒ¨í„´ ì ìš©
âœ… 4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„ 
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

Author: MyCloset AI Team
Date: 2025-07-23
Version: 10.0 (Complete Refactor with All Features)
"""

import os
import gc
import time
import logging
import asyncio
import traceback
import threading
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from enum import Enum
from io import BytesIO
import base64

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

# íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ import (ëŸ°íƒ€ì„ì—ëŠ” import ì•ˆë¨)
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from app.core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 2. í™˜ê²½ ìµœì í™” (M3 Max + conda)
# ==============================================

# PyTorch í™˜ê²½ ìµœì í™”
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
os.environ['OMP_NUM_THREADS'] = '16'  # M3 Max 16ì½”ì–´

# PyTorch ë° ì´ë¯¸ì§€ ì²˜ë¦¬
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.nn import Conv2d, ConvTranspose2d, BatchNorm2d, ReLU, Dropout
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logging.error("âŒ PyTorch import ì‹¤íŒ¨")

try:
    from PIL import Image, ImageDraw, ImageEnhance
    VISION_AVAILABLE = True
except ImportError:
    VISION_AVAILABLE = False
    logging.error("âŒ Vision ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨")

# OpenCV ì•ˆì „ import
try:
    os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
    os.environ['OPENCV_IO_ENABLE_JASPER'] = '0'
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    # OpenCV í´ë°±
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2GRAY = 7
            self.THRESH_BINARY = 0
            self.MORPH_CLOSE = 3
            self.MORPH_OPEN = 2
        
        def resize(self, img, size, interpolation=1):
            try:
                from PIL import Image
                pil_img = Image.fromarray(img) if hasattr(img, 'shape') else img
                return np.array(pil_img.resize(size))
            except:
                return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code == 7:  # RGB2GRAY
                    return np.dot(img[...,:3], [0.299, 0.587, 0.114]).astype(np.uint8)
                elif code in [3, 4]:  # BGR<->RGB
                    return img[:, :, ::-1]
            return img
        
        def threshold(self, src, thresh, maxval, type):
            binary = (src > thresh).astype(np.uint8) * maxval
            return thresh, binary
        
        def morphologyEx(self, src, op, kernel):
            return src  # ê°„ë‹¨í•œ í´ë°±
    
    cv2 = OpenCVFallback()
    OPENCV_AVAILABLE = False

try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# íŒŒì¼ ìƒë‹¨ import ì„¹ì…˜ì— ì•ˆì „í•œ utils import ì¶”ê°€
try:
    from ..utils.pytorch_safe_ops import (
        safe_max, safe_amax, safe_argmax,
        extract_keypoints_from_heatmaps,
        tensor_to_pil_conda_optimized
    )
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False
    # í´ë°± í•¨ìˆ˜ë“¤
    def safe_max(tensor, dim=None, keepdim=False):
        return torch.max(tensor, dim=dim, keepdim=keepdim)
    
    def safe_amax(tensor, dim=None, keepdim=False):
        return torch.amax(tensor, dim=dim, keepdim=keepdim)
    
    def safe_argmax(tensor, dim=None, keepdim=False):
        return torch.argmax(tensor, dim=dim, keepdim=keepdim)
    
    def extract_keypoints_from_heatmaps(heatmaps):
        return torch.zeros(heatmaps.shape[0], heatmaps.shape[1], 2)
    
    def tensor_to_pil_conda_optimized(tensor):
        return None

# ==============================================
# ğŸ”¥ 3. ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('..utils.model_loader', package=__name__)
        get_global_fn = getattr(module, 'get_global_model_loader', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_memory_manager():
    """MemoryManagerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('..utils.memory_manager', package=__name__)
        get_global_fn = getattr(module, 'get_global_memory_manager', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"MemoryManager ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_data_converter():
    """DataConverterë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('..utils.data_converter', package=__name__)
        get_global_fn = getattr(module, 'get_global_data_converter', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"DataConverter ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_di_container():
    """DI Containerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_di_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"DI Container ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 4. BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package=__package__)
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin í´ë˜ìŠ¤ ë™ì  ë¡œë”©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
        
        def set_di_container(self, di_container):
            self.di_container = di_container
        
        async def cleanup(self):
            pass

# ==============================================
# ğŸ”¥ 5. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (Step 01 íŒ¨í„´ ì ìš©) - ì™„ì „ ìœ ì§€
# ==============================================

class KeypointDetectionNet(nn.Module):
    """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹ ê²½ë§ (ResNet ê¸°ë°˜)"""
    
    def __init__(self, num_keypoints: int = 25, input_channels: int = 3):
        super().__init__()
        self.num_keypoints = num_keypoints
        
        # Backbone (ResNet-like)
        self.backbone = nn.Sequential(
            Conv2d(input_channels, 64, 7, stride=2, padding=3),
            BatchNorm2d(64),
            ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet blocks
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
        )
        
        # Keypoint detection head
        self.keypoint_head = nn.Sequential(
            Conv2d(512, 256, 3, padding=1),
            BatchNorm2d(256),
            ReLU(inplace=True),
            Conv2d(256, 128, 3, padding=1),
            BatchNorm2d(128),
            ReLU(inplace=True),
            Conv2d(128, num_keypoints, 1),  # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ
        )
        
        # Regression head (ì •í™•í•œ ì¢Œí‘œ)
        self.regression_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_keypoints * 2)  # (x, y) ì¢Œí‘œ
        )
    
    def _make_layer(self, in_channels: int, out_channels: int, blocks: int, stride: int = 1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        layers = []
        layers.append(self._make_block(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(self._make_block(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _make_block(self, in_channels: int, out_channels: int, stride: int = 1):
        """ResNet ë¸”ë¡ ìƒì„±"""
        return nn.Sequential(
            Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
            BatchNorm2d(out_channels),
            ReLU(inplace=True),
            Conv2d(out_channels, out_channels, 3, padding=1),
            BatchNorm2d(out_channels),
            ReLU(inplace=True)
        )
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        # Backbone íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ
        heatmaps = self.keypoint_head(features)
        
        # ì •í™•í•œ ì¢Œí‘œ íšŒê·€
        coords = self.regression_head(features)
        coords = coords.view(-1, self.num_keypoints, 2)
        
        # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = self._extract_keypoints_from_heatmap(heatmaps)
        
        # íšŒê·€ ê²°ê³¼ì™€ ê²°í•©
        final_keypoints = (keypoints + coords) / 2.0
        
        if heatmaps.dim() != 4:
            raise ValueError(f"Expected 4D heatmaps (B, C, H, W), got {heatmaps.dim()}D")
    
        max_values, _ = heatmaps.max(dim=(2,3), keepdim=True)
        confidence = torch.sigmoid(max_values.squeeze(-1).squeeze(-1))
    
        return {
            'keypoints': final_keypoints,
            'heatmaps': heatmaps,
            'coords': coords,
            'confidence': confidence
        }
    
    def _extract_keypoints_from_heatmap(self, heatmaps: torch.Tensor) -> torch.Tensor:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ"""
        batch_size, num_keypoints, height, width = heatmaps.shape
        device = heatmaps.device
        
        # ì†Œí”„íŠ¸ ì•„ë¥´ê·¸ë§¥ìŠ¤ë¡œ ë¶€ë“œëŸ¬ìš´ ì¢Œí‘œ ì¶”ì¶œ
        heatmaps_flat = heatmaps.view(batch_size, num_keypoints, -1)
        weights = F.softmax(heatmaps_flat, dim=-1)
        
        # ê²©ì ì¢Œí‘œ ìƒì„±
        y_coords, x_coords = torch.meshgrid(
            torch.arange(height, device=device),
            torch.arange(width, device=device),
            indexing='ij'
        )
        coords_flat = torch.stack([
            x_coords.flatten(), y_coords.flatten()
        ], dim=0).float()  # (2, H*W)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í‚¤í¬ì¸íŠ¸ ê³„ì‚°
        keypoints = torch.matmul(weights, coords_flat.T)  # (B, K, 2)
        
        # ì •ê·œí™” [0, 1]
        keypoints[:, :, 0] = keypoints[:, :, 0] / (width - 1)
        keypoints[:, :, 1] = keypoints[:, :, 1] / (height - 1)
        
        return keypoints

class TPSTransformationNet(nn.Module):
    """TPS (Thin Plate Spline) ë³€í˜• ì‹ ê²½ë§"""
    
    def __init__(self, num_control_points: int = 25, grid_size: int = 20):
        super().__init__()
        self.num_control_points = num_control_points
        self.grid_size = grid_size
        
        # ì œì–´ì  íŠ¹ì§• ì¸ì½”ë”
        self.control_encoder = nn.Sequential(
            nn.Linear(num_control_points * 4, 512),  # source + target points
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
        )
        
        # TPS ê³„ìˆ˜ ì˜ˆì¸¡
        self.tps_predictor = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, num_control_points + 3),  # W + A (affine)
        )
        
        # ê·¸ë¦¬ë“œ ìƒì„±ê¸°
        self.grid_generator = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, grid_size * grid_size * 2),
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì œí•œ
        )
    
    def forward(self, source_points: torch.Tensor, target_points: torch.Tensor) -> Dict[str, torch.Tensor]:
        """TPS ë³€í˜• ê³„ì‚°"""
        batch_size = source_points.size(0)
        device = source_points.device
        
        # ì…ë ¥ ì¤€ë¹„
        control_input = torch.cat([
            source_points.view(batch_size, -1),
            target_points.view(batch_size, -1)
        ], dim=1)
        
        # íŠ¹ì§• ì¸ì½”ë”©
        features = self.control_encoder(control_input)
        
        # TPS ê³„ìˆ˜ ì˜ˆì¸¡
        tps_params = self.tps_predictor(features)
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid_offsets = self.grid_generator(features)
        grid_offsets = grid_offsets.view(batch_size, self.grid_size, self.grid_size, 2)
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        base_grid = self._create_base_grid(batch_size, device)
        
        # TPS ë³€í˜• ì ìš©
        tps_grid = self._apply_tps_transformation(
            base_grid, source_points, target_points, tps_params
        )
        
        # ìµœì¢… ë³€í˜• ê·¸ë¦¬ë“œ
        final_grid = tps_grid + grid_offsets * 0.1  # ë¯¸ì„¸ ì¡°ì •
        
        return {
            'transformation_grid': final_grid,
            'tps_params': tps_params,
            'grid_offsets': grid_offsets,
            'base_grid': base_grid
        }
    
    def _create_base_grid(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """ê¸°ë³¸ ì •ê·œ ê·¸ë¦¬ë“œ ìƒì„±"""
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, self.grid_size, device=device),
            torch.linspace(-1, 1, self.grid_size, device=device),
            indexing='ij'
        )
        grid = torch.stack([x, y], dim=-1)
        return grid.unsqueeze(0).expand(batch_size, -1, -1, -1)
    
    def _apply_tps_transformation(
        self, 
        grid: torch.Tensor, 
        source_points: torch.Tensor, 
        target_points: torch.Tensor,
        tps_params: torch.Tensor
    ) -> torch.Tensor:
        """TPS ë³€í˜• ì ìš©"""
        batch_size = grid.size(0)
        grid_flat = grid.view(batch_size, -1, 2)  # (B, H*W, 2)
        
        # TPS ê¸°ì € í•¨ìˆ˜ ê³„ì‚°
        tps_basis = self._compute_tps_basis(grid_flat, source_points)  # (B, H*W, K+3)
        
        # TPS ê³„ìˆ˜ ì ìš©
        tps_params_expanded = tps_params.unsqueeze(1)  # (B, 1, K+3)
        
        # ë³€í˜• ê³„ì‚°
        displacement = torch.sum(tps_basis.unsqueeze(-1) * tps_params_expanded.unsqueeze(-1), dim=2)
        
        # ì›ë³¸ ê·¸ë¦¬ë“œì— ë³€ìœ„ ì¶”ê°€
        transformed_grid = grid_flat + displacement
        
        return transformed_grid.view(batch_size, self.grid_size, self.grid_size, 2)
    
    def _compute_tps_basis(self, points: torch.Tensor, control_points: torch.Tensor) -> torch.Tensor:
        """TPS ê¸°ì € í•¨ìˆ˜ ê³„ì‚°"""
        # ê±°ë¦¬ ê³„ì‚°
        distances = torch.cdist(points, control_points)  # (B, P, K)
        
        # TPS ë°©ì‚¬ ê¸°ì € í•¨ìˆ˜: r^2 * log(r)
        eps = 1e-8
        tps_basis = distances ** 2 * torch.log(distances + eps)
        
        # ì•„í•€ í•­ ì¶”ê°€ (1, x, y)
        batch_size, num_points = points.shape[:2]
        ones = torch.ones(batch_size, num_points, 1, device=points.device)
        affine_basis = torch.cat([ones, points], dim=-1)
        
        # ê²°í•©
        full_basis = torch.cat([tps_basis, affine_basis], dim=-1)
        
        return full_basis

class GeometricMatchingModel(nn.Module):
    """ì „ì²´ ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸"""
    
    def __init__(self, num_keypoints: int = 25, grid_size: int = 20):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.grid_size = grid_size
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë„¤íŠ¸ì›Œí¬
        self.keypoint_net = KeypointDetectionNet(num_keypoints)
        
        # TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬
        self.tps_net = TPSTransformationNet(num_keypoints, grid_size)
        
        # í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬
        self.quality_net = nn.Sequential(
            nn.Linear(num_keypoints * 2, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, person_image: torch.Tensor, clothing_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ì „ì²´ ë§¤ì¹­ ê³¼ì •"""
        # 1. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        person_result = self.keypoint_net(person_image)
        clothing_result = self.keypoint_net(clothing_image)
        
        person_keypoints = person_result['keypoints']
        clothing_keypoints = clothing_result['keypoints']
        
        # 2. TPS ë³€í˜• ê³„ì‚°
        tps_result = self.tps_net(person_keypoints, clothing_keypoints)
        
        # 3. í’ˆì§ˆ í‰ê°€
        keypoint_diff = (person_keypoints - clothing_keypoints).view(person_keypoints.size(0), -1)
        quality_score = self.quality_net(keypoint_diff)
        
        return {
            'person_keypoints': person_keypoints,
            'clothing_keypoints': clothing_keypoints,
            'person_confidence': person_result['confidence'],
            'clothing_confidence': clothing_result['confidence'],
            'transformation_grid': tps_result['transformation_grid'],
            'tps_params': tps_result['tps_params'],
            'quality_score': quality_score
        }

# ==============================================
# ğŸ”¥ 6. Step 01 íŒ¨í„´ ì ìš©: ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ê¸°
# ==============================================

class GeometricMatchingModelFactory:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ íŒ©í† ë¦¬ (Step 01 íŒ¨í„´)"""
    
    @staticmethod
    def create_model_from_checkpoint(
        checkpoint_data: Any,
        device: str = "cpu",
        num_keypoints: int = 25,
        grid_size: int = 20
    ) -> GeometricMatchingModel:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ AI ëª¨ë¸ ìƒì„± (Step 01 ì„±ê³µ íŒ¨í„´)"""
        try:
            # 1. AI ëª¨ë¸ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            model = GeometricMatchingModel(
                num_keypoints=num_keypoints,
                grid_size=grid_size
            )
            
            # 2. ì²´í¬í¬ì¸íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(checkpoint_data, dict):
                # ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„
                if 'model_state_dict' in checkpoint_data:
                    try:
                        model.load_state_dict(checkpoint_data['model_state_dict'])
                        logging.info("âœ… model_state_dictì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                    except Exception as e:
                        logging.warning(f"âš ï¸ model_state_dict ë¡œë“œ ì‹¤íŒ¨: {e}")
                        # ë¶€ë¶„ ë¡œë”© ì‹œë„
                        GeometricMatchingModelFactory._load_partial_weights(model, checkpoint_data['model_state_dict'])
                
                elif 'state_dict' in checkpoint_data:
                    try:
                        model.load_state_dict(checkpoint_data['state_dict'])
                        logging.info("âœ… state_dictì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                    except Exception as e:
                        logging.warning(f"âš ï¸ state_dict ë¡œë“œ ì‹¤íŒ¨: {e}")
                        GeometricMatchingModelFactory._load_partial_weights(model, checkpoint_data['state_dict'])
                
                else:
                    # ë”•ì…”ë„ˆë¦¬ ìì²´ê°€ state_dictì¸ ê²½ìš°
                    try:
                        model.load_state_dict(checkpoint_data)
                        logging.info("âœ… ì§ì ‘ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                    except Exception as e:
                        logging.warning(f"âš ï¸ ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        GeometricMatchingModelFactory._load_partial_weights(model, checkpoint_data)
            
            else:
                logging.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            
            # 3. ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ë° í‰ê°€ ëª¨ë“œ
            model = model.to(device)
            model.eval()
            
            logging.info(f"âœ… GeometricMatchingModel ìƒì„± ì™„ë£Œ: {device}")
            return model
            
        except Exception as e:
            logging.error(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸
            model = GeometricMatchingModel(num_keypoints=num_keypoints, grid_size=grid_size)
            model = model.to(device)
            model.eval()
            logging.info("ğŸ”„ í´ë°±: ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ ì‚¬ìš©")
            return model
    
    @staticmethod
    def _load_partial_weights(model: nn.Module, state_dict: Dict[str, Any]):
        """ë¶€ë¶„ ê°€ì¤‘ì¹˜ ë¡œë”© (í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ)"""
        try:
            model_dict = model.state_dict()
            # í˜¸í™˜ë˜ëŠ” í‚¤ë§Œ í•„í„°ë§
            compatible_dict = {
                k: v for k, v in state_dict.items() 
                if k in model_dict and v.shape == model_dict[k].shape
            }
            
            if compatible_dict:
                model_dict.update(compatible_dict)
                model.load_state_dict(model_dict)
                logging.info(f"âœ… ë¶€ë¶„ ê°€ì¤‘ì¹˜ ë¡œë“œ: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
            else:
                logging.warning("âš ï¸ í˜¸í™˜ë˜ëŠ” ê°€ì¤‘ì¹˜ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìœ ì§€")
                
        except Exception as e:
            logging.warning(f"âš ï¸ ë¶€ë¶„ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 7. ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ê´€ë¦¬ (ì™„ì „ ìœ ì§€)
# ==============================================

class GeometricMatchingError(Exception):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ê´€ë ¨ ì—ëŸ¬"""
    pass

class ModelLoaderError(Exception):
    """ModelLoader ê´€ë ¨ ì—ëŸ¬"""
    pass

class DependencyInjectionError(Exception):
    """ì˜ì¡´ì„± ì£¼ì… ê´€ë ¨ ì—ëŸ¬"""
    pass

@dataclass
class ProcessingStatus:
    """ì²˜ë¦¬ ìƒíƒœ ì¶”ì """
    initialized: bool = False
    models_loaded: bool = False
    dependencies_injected: bool = False
    processing_active: bool = False
    error_count: int = 0
    last_error: Optional[str] = None
    ai_model_calls: int = 0
    model_creation_success: bool = False

# ==============================================
# ğŸ”¥ 8. ê°œì„ ëœ ì˜ì¡´ì„± ì£¼ì… ê´€ë¦¬ì
# ==============================================

class ImprovedDependencyManager:
    """ê°œì„ ëœ ì˜ì¡´ì„± ì£¼ì… ê´€ë¦¬ì (ì›ë³¸ ê¸°ëŠ¥ + ê°œì„ ì‚¬í•­)"""
    
    def __init__(self):
        # TYPE_CHECKINGìœ¼ë¡œ íƒ€ì…ë§Œ ì •ì˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        self.model_loader: Optional['ModelLoader'] = None
        self.memory_manager: Optional['MemoryManager'] = None
        self.data_converter: Optional['DataConverter'] = None
        self.di_container: Optional['DIContainer'] = None
        
        # ì˜ì¡´ì„± ìƒíƒœ ì¶”ì 
        self.dependency_status = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        # ìë™ ì£¼ì… í”Œë˜ê·¸
        self.auto_injection_attempted = False
        
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    # ==============================================
    # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (ì›ë³¸ ë°©ì‹ ìœ ì§€)
    # ==============================================
    
    def set_model_loader(self, model_loader: 'ModelLoader'):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        self.model_loader = model_loader
        self.dependency_status['model_loader'] = True
        self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_memory_manager(self, memory_manager: 'MemoryManager'):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        self.memory_manager = memory_manager
        self.dependency_status['memory_manager'] = True
        self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter: 'DataConverter'):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        self.data_converter = data_converter
        self.dependency_status['data_converter'] = True
        self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_di_container(self, di_container: 'DIContainer'):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        self.di_container = di_container
        self.dependency_status['di_container'] = True
        self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ ìë™ ì˜ì¡´ì„± ì£¼ì… (ë™ì  import ì‚¬ìš©)
    # ==============================================
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„"""
        if self.auto_injection_attempted:
            return any(self.dependency_status.values())
        
        self.auto_injection_attempted = True
        success_count = 0
        
        try:
            # ModelLoader ìë™ ì£¼ì… (í•„ìˆ˜)
            if not self.model_loader:
                try:
                    auto_loader = get_model_loader()
                    if auto_loader:
                        self.set_model_loader(auto_loader)
                        success_count += 1
                        self.logger.info("âœ… ModelLoader ìë™ ì£¼ì… ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # MemoryManager ìë™ ì£¼ì… (ì„ íƒì )
            if not self.memory_manager:
                try:
                    auto_manager = get_memory_manager()
                    if auto_manager:
                        self.set_memory_manager(auto_manager)
                        success_count += 1
                        self.logger.info("âœ… MemoryManager ìë™ ì£¼ì… ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # DataConverter ìë™ ì£¼ì… (ì„ íƒì )
            if not self.data_converter:
                try:
                    auto_converter = get_data_converter()
                    if auto_converter:
                        self.set_data_converter(auto_converter)
                        success_count += 1
                        self.logger.info("âœ… DataConverter ìë™ ì£¼ì… ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"DataConverter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # DIContainer ìë™ ì£¼ì… (ì„ íƒì )
            if not self.di_container:
                try:
                    auto_container = get_di_container()
                    if auto_container:
                        self.set_di_container(auto_container)
                        success_count += 1
                        self.logger.info("âœ… DIContainer ìë™ ì£¼ì… ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"DIContainer ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}/4ê°œ ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ì˜ì¡´ì„± ì£¼ì… ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def validate_dependencies(self) -> bool:
        """ì˜ì¡´ì„± ê²€ì¦ (ìë™ ì£¼ì… í¬í•¨)"""
        try:
            # ìë™ ì£¼ì… ì‹œë„
            if not self.auto_injection_attempted:
                self.auto_inject_dependencies()
            
            missing_deps = []
            
            # í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸
            if not self.dependency_status['model_loader']:
                missing_deps.append('model_loader')
            
            # ì„ íƒì  ì˜ì¡´ì„±ì€ ê²½ê³ ë§Œ
            optional_missing = [
                dep for dep, status in self.dependency_status.items() 
                if not status and dep != 'model_loader'
            ]
            
            if optional_missing:
                self.logger.debug(f"ì„ íƒì  ì˜ì¡´ì„± ëˆ„ë½: {optional_missing}")
            
            # í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½ ì‹œ ì—ëŸ¬ (ê°œë°œ í™˜ê²½ì—ì„œëŠ” ê²½ê³ )
            if missing_deps:
                error_msg = f"í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½: {missing_deps}"
                self.logger.error(f"âŒ {error_msg}")
                
                # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ê²½ê³ ë¡œ ì²˜ë¦¬
                if os.environ.get('MYCLOSET_ENV') == 'development':
                    self.logger.warning(f"âš ï¸ ê°œë°œ ëª¨ë“œ: {error_msg} - ê³„ì† ì§„í–‰")
                    return True
                else:
                    return False
            
            self.logger.info("âœ… ëª¨ë“  ì˜ì¡´ì„± ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ì˜ì¡´ì„±ì„ í†µí•œ ê¸°ëŠ¥ í˜¸ì¶œ
    # ==============================================
    
    async def get_model_checkpoint(self, model_name: str = 'geometric_matching'):
        """ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ íšë“"""
        try:
            if not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoader ì—†ìŒ - ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë¶ˆê°€")
                return None
            
            # ë‹¤ì–‘í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ì‹œë„ (Step 04 ì „ìš©)
            model_names = [
                model_name,
                'geometric_matching_model',
                'tps_transformation_model', 
                'keypoint_detection_model',
                'step_04_model',
                'step_04_geometric_matching',
                'matching_model',
                'tps_model'
            ]
            
            for name in model_names:
                try:
                    checkpoint = None
                    
                    # ë¹„ë™ê¸° ë©”ì„œë“œ ìš°ì„  ì‹œë„
                    if hasattr(self.model_loader, 'load_model_async'):
                        try:
                            checkpoint = await self.model_loader.load_model_async(name)
                        except Exception as e:
                            self.logger.debug(f"ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
                    
                    # ë™ê¸° ë©”ì„œë“œ ì‹œë„
                    if checkpoint is None and hasattr(self.model_loader, 'load_model'):
                        try:
                            checkpoint = self.model_loader.load_model(name)
                        except Exception as e:
                            self.logger.debug(f"ë™ê¸° ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
                    
                    if checkpoint is not None:
                        self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {name}")
                        return checkpoint
                        
                except Exception as e:
                    self.logger.debug(f"ëª¨ë¸ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.warning("âš ï¸ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            return {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ëœë¤ ì´ˆê¸°í™”ìš©)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íšë“ ì‹¤íŒ¨: {e}")
            return {}
    
    async def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """MemoryManagerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self.memory_manager and hasattr(self.memory_manager, 'optimize_memory_async'):
                result = await self.memory_manager.optimize_memory_async(aggressive)
                result["source"] = "injected_memory_manager"
                return result
            elif self.memory_manager and hasattr(self.memory_manager, 'optimize_memory'):
                result = self.memory_manager.optimize_memory(aggressive)
                result["source"] = "injected_memory_manager"
                return result
            else:
                # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                
                if TORCH_AVAILABLE:
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except:
                            pass
                    elif torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                return {
                    "success": True,
                    "source": "fallback_memory_cleanup",
                    "operations": ["gc.collect", "torch_cache_clear"]
                }
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def convert_data(self, data: Any, target_format: str) -> Any:
        """DataConverterë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜"""
        try:
            if self.data_converter and hasattr(self.data_converter, 'convert_data'):
                return self.data_converter.convert_data(data, target_format)
            else:
                # í´ë°±: ê¸°ë³¸ ë³€í™˜ ë¡œì§
                return data
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return data
    
    def get_dependency_status(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ"""
        return {
            'dependency_status': self.dependency_status.copy(),
            'auto_injection_attempted': self.auto_injection_attempted,
            'total_injected': sum(self.dependency_status.values()),
            'critical_dependencies_met': self.dependency_status['model_loader']
        }

# ==============================================
# ğŸ”¥ 9. ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤ (ì™„ì „ ê°œì„ )
# ==============================================

class GeometricMatchingStep(BaseStepMixin):
    """
    ğŸ”¥ Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ - ì™„ì „ ê°œì„  ë²„ì „
    
    âœ… ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€
    âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„ 
    âœ… ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™”
    âœ… BaseStepMixin ì™„ì „ í˜¸í™˜
    âœ… TYPE_CHECKING íŒ¨í„´ ìœ ì§€
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… AI ëª¨ë¸ ì—°ë™ ì™„ì „ êµ¬í˜„
    âœ… Step 01 ì„±ê³µ íŒ¨í„´ ì ìš©
    âœ… 4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€
    """
    
    def __init__(self, **kwargs):
        """ê°œì„ ëœ ì˜ì¡´ì„± ì£¼ì… ê¸°ë°˜ ìƒì„±ì"""
        # BaseStepMixin ì´ˆê¸°í™”
        super().__init__(**kwargs)
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.step_name = "geometric_matching"
        self.step_id = 4
        self.device = kwargs.get('device', 'mps' if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu')
        
        # ìƒíƒœ ê´€ë¦¬
        self.status = ProcessingStatus()
        
        # ğŸ”¥ ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ì
        self.dependency_manager = ImprovedDependencyManager()
        
        # AI ëª¨ë¸ë“¤ (ë‚˜ì¤‘ì— ë¡œë“œ)
        self.geometric_model: Optional[GeometricMatchingModel] = None
        
        # ì„¤ì • ì´ˆê¸°í™”
        self._setup_configurations(kwargs.get('config', {}))
        
        # í†µê³„ ì´ˆê¸°í™”
        self._init_statistics()
        
        # ğŸ”¥ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
        self._auto_inject_dependencies()
        
        self.logger.info(f"âœ… GeometricMatchingStep ìƒì„± ì™„ë£Œ - Device: {self.device}")
    
    def _auto_inject_dependencies(self):
        """ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            success = self.dependency_manager.auto_inject_dependencies()
            if success:
                self.status.dependencies_injected = True
                self.logger.info("âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ")
            else:
                self.logger.warning("âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ 10. ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (ì›ë³¸ ë°©ì‹ ìœ ì§€ + ê°œì„ )
    # ==============================================
    
    def set_model_loader(self, model_loader: 'ModelLoader'):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        self.dependency_manager.set_model_loader(model_loader)
        self.status.dependencies_injected = True
        self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_memory_manager(self, memory_manager: 'MemoryManager'):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        self.dependency_manager.set_memory_manager(memory_manager)
        self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter: 'DataConverter'):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        self.dependency_manager.set_data_converter(data_converter)
        self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_di_container(self, di_container: 'DIContainer'):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        self.dependency_manager.set_di_container(di_container)
        self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def validate_dependencies(self) -> bool:
        """ì˜ì¡´ì„± ê²€ì¦ (ìë™ ì£¼ì… í¬í•¨)"""
        return self.dependency_manager.validate_dependencies()
    
    # ==============================================
    # ğŸ”¥ 11. ì´ˆê¸°í™” (ê°„ì†Œí™” + ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    async def initialize(self) -> bool:
        """ê°„ì†Œí™”ëœ ì´ˆê¸°í™” (4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€)"""
        if self.status.initialized:
            return True
        
        try:
            self.logger.info("ğŸ”„ Step 04 ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ì˜ì¡´ì„± ê²€ì¦ (ìë™ ì£¼ì… í¬í•¨)
            try:
                if not self.validate_dependencies():
                    self.logger.warning("âš ï¸ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨ - í´ë°± ëª¨ë“œë¡œ ì§„í–‰")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ê²€ì¦ ì˜¤ë¥˜: {e} - í´ë°± ëª¨ë“œë¡œ ì§„í–‰")
            
            # 2. AI ëª¨ë¸ ë¡œë“œ (Step 01 íŒ¨í„´ ì ìš©)
            try:
                await self._load_ai_models_step01_pattern()
            except Exception as e:
                self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e} - í´ë°± ëª¨ë¸ ì‚¬ìš©")
                # í´ë°±: ëœë¤ ì´ˆê¸°í™” ëª¨ë¸ ìƒì„±
                self.geometric_model = GeometricMatchingModelFactory.create_model_from_checkpoint(
                    {},  # ë¹ˆ ì²´í¬í¬ì¸íŠ¸
                    device=self.device,
                    num_keypoints=self.matching_config['num_keypoints'],
                    grid_size=self.tps_config['grid_size']
                )
            
            # 3. ë””ë°”ì´ìŠ¤ ì„¤ì •
            try:
                await self._setup_device_models()
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # 4. ëª¨ë¸ ì›Œë°ì—…
            try:
                await self._warmup_models()
            except Exception as e:
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            self.status.initialized = True
            self.status.models_loaded = self.geometric_model is not None
            
            # ê²°ê³¼ í™•ì¸
            if self.geometric_model is not None:
                self.logger.info("âœ… Step 04 ì´ˆê¸°í™” ì™„ë£Œ (AI ëª¨ë¸ í¬í•¨)")
            else:
                self.logger.warning("âš ï¸ Step 04 ì´ˆê¸°í™” ì™„ë£Œ (AI ëª¨ë¸ ì—†ìŒ)")
            
            return True
            
        except Exception as e:
            self.status.error_count += 1
            self.status.last_error = str(e)
            self.logger.error(f"âŒ Step 04 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ìµœì†Œí•œì˜ í´ë°± ì´ˆê¸°í™”
            try:
                self.geometric_model = GeometricMatchingModelFactory.create_model_from_checkpoint(
                    {},  # ë¹ˆ ì²´í¬í¬ì¸íŠ¸ - ëœë¤ ì´ˆê¸°í™”
                    device=self.device,
                    num_keypoints=self.matching_config['num_keypoints'],
                    grid_size=self.tps_config['grid_size']
                )
                self.status.initialized = True
                self.status.models_loaded = True
                self.logger.warning("âš ï¸ í´ë°± ì´ˆê¸°í™” ì™„ë£Œ - ëœë¤ ì´ˆê¸°í™” ëª¨ë¸ ì‚¬ìš©")
                return True
            except Exception as e2:
                self.logger.error(f"âŒ í´ë°± ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e2}")
                return False
    
    async def _load_ai_models_step01_pattern(self):
        """Step 01 ì„±ê³µ íŒ¨í„´ì„ ì ìš©í•œ AI ëª¨ë¸ ë¡œë“œ (ê°œì„ ëœ ì˜ì¡´ì„± ì‚¬ìš©)"""
        try:
            checkpoint_data = None
            
            # ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            try:
                checkpoint_data = await self.dependency_manager.get_model_checkpoint()
                self.logger.info("âœ… ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ê´€ë¦¬ì ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # Step 01 íŒ¨í„´: ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜
            self.geometric_model = GeometricMatchingModelFactory.create_model_from_checkpoint(
                checkpoint_data or {},  # Noneì´ë©´ ë¹ˆ dict ì‚¬ìš©
                device=self.device,
                num_keypoints=self.matching_config['num_keypoints'],
                grid_size=self.tps_config['grid_size']
            )
            
            if self.geometric_model is not None:
                self.status.model_creation_success = True
                if checkpoint_data:
                    self.logger.info("âœ… AI ëª¨ë¸ ìƒì„± ì™„ë£Œ (ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)")
                else:
                    self.logger.info("âœ… AI ëª¨ë¸ ìƒì„± ì™„ë£Œ (ëœë¤ ì´ˆê¸°í™”)")
            else:
                raise GeometricMatchingError("AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            
        except Exception as e:
            self.status.model_creation_success = False
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ìµœí›„ í´ë°±: ê°•ì œë¡œ ëœë¤ ì´ˆê¸°í™” ëª¨ë¸ ìƒì„±
            try:
                self.geometric_model = GeometricMatchingModel(
                    num_keypoints=self.matching_config['num_keypoints'],
                    grid_size=self.tps_config['grid_size']
                )
                self.geometric_model = self.geometric_model.to(self.device)
                self.geometric_model.eval()
                self.status.model_creation_success = True
                self.logger.warning("âš ï¸ ìµœí›„ í´ë°±: ì§ì ‘ ëœë¤ ì´ˆê¸°í™” ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            except Exception as e2:
                self.logger.error(f"âŒ ìµœí›„ í´ë°± ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
                raise GeometricMatchingError(f"ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {e2}") from e2
    
    async def _setup_device_models(self):
        """ëª¨ë¸ë“¤ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        try:
            if self.geometric_model:
                self.geometric_model = self.geometric_model.to(self.device)
                self.geometric_model.eval()
                self.logger.info(f"âœ… ëª¨ë¸ì´ {self.device}ë¡œ ì´ë™ ì™„ë£Œ")
                
        except Exception as e:
            raise GeometricMatchingError(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}") from e
    
    async def _warmup_models(self):
        """ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if self.geometric_model:
                dummy_person = torch.randn(1, 3, 384, 512, device=self.device)
                dummy_clothing = torch.randn(1, 3, 384, 512, device=self.device)
                
                with torch.no_grad():
                    result = self.geometric_model(dummy_person, dummy_clothing)
                    
                    # ê²°ê³¼ ê²€ì¦
                    if isinstance(result, dict) and 'person_keypoints' in result:
                        self.logger.info("ğŸ”¥ AI ëª¨ë¸ ì›Œë°ì—… ë° ê²€ì¦ ì™„ë£Œ")
                    else:
                        self.logger.warning("âš ï¸ ëª¨ë¸ ì¶œë ¥ í˜•ì‹ í™•ì¸ í•„ìš”")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 12. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        **kwargs
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©"""
        
        if self.status.processing_active:
            raise RuntimeError("âŒ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤")
        
        start_time = time.time()
        self.status.processing_active = True
        
        try:
            # 1. ì´ˆê¸°í™” í™•ì¸
            if not self.status.initialized:
                success = await self.initialize()
                if not success:
                    raise GeometricMatchingError("ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            self.logger.info("ğŸ¯ ì‹¤ì œ AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œì‘...")
            
            # 2. ì…ë ¥ ì „ì²˜ë¦¬
            processed_input = await self._preprocess_inputs(
                person_image, clothing_image
            )
            
            # 3. AI ëª¨ë¸ ì¶”ë¡  (Step 01 íŒ¨í„´)
            ai_result = await self._run_ai_inference(
                processed_input['person_tensor'],
                processed_input['clothing_tensor']
            )
            
            # 4. ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
            warping_result = await self._apply_geometric_transformation(
                processed_input['clothing_tensor'],
                ai_result['transformation_grid']
            )
            
            # 5. í›„ì²˜ë¦¬
            final_result = await self._postprocess_result(
                warping_result,
                ai_result,
                processed_input
            )
            
            # 6. ì‹œê°í™” ìƒì„±
            visualization = await self._create_visualization(
                processed_input, ai_result, warping_result
            )
            
            # 7. í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            quality_score = ai_result['quality_score'].item()
            self._update_statistics(quality_score, processing_time)
            
            self.logger.info(
                f"âœ… AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - "
                f"í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}s"
            )
            
            # 8. API ì‘ë‹µ ë°˜í™˜
            return self._format_api_response(
                True, final_result, visualization, quality_score, processing_time
            )
            
        except Exception as e:
            self.status.error_count += 1
            self.status.last_error = str(e)
            processing_time = time.time() - start_time
            
            self.logger.error(f"âŒ AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ì‘ë‹µ ë°˜í™˜
            return self._format_api_response(
                False, None, None, 0.0, processing_time, str(e)
            )
            
        finally:
            self.status.processing_active = False
            # ë©”ëª¨ë¦¬ ìµœì í™” (ê°œì„ ëœ ì˜ì¡´ì„± ì‚¬ìš©)
            try:
                await self.dependency_manager.optimize_memory()
            except Exception as e:
                self.logger.debug(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 13. AI ëª¨ë¸ ì¶”ë¡  (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def _run_ai_inference(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (Step 01 íŒ¨í„´ ì ìš©)"""
        try:
            if not self.geometric_model:
                raise GeometricMatchingError("AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            with torch.no_grad():
                # Step 01 íŒ¨í„´: ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ
                result = self.geometric_model(person_tensor, clothing_tensor)
                
                # ê²°ê³¼ ê²€ì¦ (dictê°€ ì•„ë‹Œ ì‹¤ì œ ëª¨ë¸ ì¶œë ¥ì¸ì§€ í™•ì¸)
                if not isinstance(result, dict):
                    raise GeometricMatchingError(f"ëª¨ë¸ ì¶œë ¥ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(result)}")
                
                # í•„ìˆ˜ í‚¤ í™•ì¸
                required_keys = ['person_keypoints', 'clothing_keypoints', 'transformation_grid', 'quality_score']
                missing_keys = [key for key in required_keys if key not in result]
                if missing_keys:
                    raise GeometricMatchingError(f"ëª¨ë¸ ì¶œë ¥ì— í•„ìˆ˜ í‚¤ ëˆ„ë½: {missing_keys}")
                
                self.status.ai_model_calls += 1
                
                return {
                    'person_keypoints': result['person_keypoints'],
                    'clothing_keypoints': result['clothing_keypoints'],
                    'transformation_grid': result['transformation_grid'],
                    'quality_score': result['quality_score'],
                    'person_confidence': result.get('person_confidence', torch.ones(1)),
                    'clothing_confidence': result.get('clothing_confidence', torch.ones(1))
                }
                
        except Exception as e:
            raise GeometricMatchingError(f"AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}") from e
    
    async def _apply_geometric_transformation(
        self,
        clothing_tensor: torch.Tensor,
        transformation_grid: torch.Tensor
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë³€í˜• ì ìš©"""
        try:
            # F.grid_sampleì„ ì‚¬ìš©í•œ ê¸°í•˜í•™ì  ë³€í˜•
            warped_clothing = F.grid_sample(
                clothing_tensor,
                transformation_grid,
                mode='bilinear',
                padding_mode='zeros',
                align_corners=False
            )
            
            # ê²°ê³¼ ê²€ì¦
            if torch.isnan(warped_clothing).any():
                raise ValueError("ë³€í˜•ëœ ì˜ë¥˜ì— NaN ê°’ í¬í•¨")
            
            return {
                'warped_clothing': warped_clothing,
                'transformation_grid': transformation_grid,
                'warping_success': True
            }
            
        except Exception as e:
            raise GeometricMatchingError(f"ê¸°í•˜í•™ì  ë³€í˜• ì‹¤íŒ¨: {e}") from e
    
    # ==============================================
    # ğŸ”¥ 14. ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def _preprocess_inputs(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor]
    ) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            clothing_tensor = self._image_to_tensor(clothing_image)
            
            # í¬ê¸° ì •ê·œí™” (384x512)
            target_size = (384, 512)
            person_tensor = F.interpolate(person_tensor, size=target_size, mode='bilinear', align_corners=False)
            clothing_tensor = F.interpolate(clothing_tensor, size=target_size, mode='bilinear', align_corners=False)
            
            # ì •ê·œí™” (ImageNet ìŠ¤íƒ€ì¼)
            mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            
            person_tensor = (person_tensor - mean) / std
            clothing_tensor = (clothing_tensor - mean) / std
            
            return {
                'person_tensor': person_tensor,
                'clothing_tensor': clothing_tensor,
                'target_size': target_size
            }
            
        except Exception as e:
            raise GeometricMatchingError(f"ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}") from e
    
    def _image_to_tensor(self, image: Union[np.ndarray, Image.Image, torch.Tensor]) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            if isinstance(image, torch.Tensor):
                if image.dim() == 3:
                    image = image.unsqueeze(0)
                return image.to(self.device)
            
            elif isinstance(image, Image.Image):
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                tensor = torch.from_numpy(np.array(image)).float()
                tensor = tensor.permute(2, 0, 1).unsqueeze(0) / 255.0
                return tensor.to(self.device)
            
            elif isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                tensor = torch.from_numpy(image).float()
                tensor = tensor.permute(2, 0, 1).unsqueeze(0) / 255.0
                return tensor.to(self.device)
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                
        except Exception as e:
            raise GeometricMatchingError(f"ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}") from e
    
    async def _postprocess_result(
        self,
        warping_result: Dict[str, Any],
        ai_result: Dict[str, Any],
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_tensor = warping_result['warped_clothing']
            
            # ì •ê·œí™” í•´ì œ
            mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            
            warped_tensor = warped_tensor * std + mean
            warped_tensor = torch.clamp(warped_tensor, 0, 1)
            
            # numpy ë³€í™˜
            warped_clothing = self._tensor_to_numpy(warped_tensor)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            warped_mask = self._generate_mask_from_image(warped_clothing)
            
            return {
                'warped_clothing': warped_clothing,
                'warped_mask': warped_mask,
                'person_keypoints': ai_result['person_keypoints'].cpu().numpy(),
                'clothing_keypoints': ai_result['clothing_keypoints'].cpu().numpy(),
                'quality_score': ai_result['quality_score'].item(),
                'processing_success': True
            }
            
        except Exception as e:
            raise GeometricMatchingError(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}") from e
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            if tensor.is_cuda or (hasattr(tensor, 'device') and tensor.device.type == 'mps'):
                tensor = tensor.cpu()
            
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.dim() == 3 and tensor.size(0) == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = torch.clamp(tensor * 255.0, 0, 255)
            return tensor.detach().numpy().astype(np.uint8)
            
        except Exception as e:
            raise GeometricMatchingError(f"í…ì„œ numpy ë³€í™˜ ì‹¤íŒ¨: {e}") from e
    
    def _generate_mask_from_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            if OPENCV_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                _, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)
                
                # ëª¨í´ë¡œì§€ ì—°ì‚°
                kernel = np.ones((3, 3), np.uint8)
                mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
                
                return mask
            else:
                # OpenCV ì—†ëŠ” ê²½ìš° ë‹¨ìˆœ ë§ˆìŠ¤í¬
                gray = np.dot(image[...,:3], [0.299, 0.587, 0.114])
                mask = (gray > 10).astype(np.uint8) * 255
                return mask
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.ones((384, 512), dtype=np.uint8) * 255
    
    # ==============================================
    # ğŸ”¥ 15. ì‹œê°í™” ìƒì„± (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def _create_visualization(
        self,
        processed_input: Dict[str, Any],
        ai_result: Dict[str, Any],
        warping_result: Dict[str, Any]
    ) -> Dict[str, str]:
        """ì‹œê°í™” ìƒì„±"""
        try:
            if not VISION_AVAILABLE:
                return {
                    'matching_visualization': '',
                    'warped_overlay': '',
                    'transformation_grid': ''
                }
            
            # ì´ë¯¸ì§€ ë³€í™˜
            person_image = self._tensor_to_pil_image(processed_input['person_tensor'])
            clothing_image = self._tensor_to_pil_image(processed_input['clothing_tensor'])
            warped_image = self._tensor_to_pil_image(warping_result['warped_clothing'])
            
            # í‚¤í¬ì¸íŠ¸ ì‹œê°í™”
            matching_viz = self._create_keypoint_visualization(
                person_image, clothing_image, ai_result
            )
            
            # ì˜¤ë²„ë ˆì´ ì‹œê°í™”
            quality_score = ai_result['quality_score'].item()
            warped_overlay = self._create_warped_overlay(person_image, warped_image, quality_score)
            
            return {
                'matching_visualization': self._image_to_base64(matching_viz),
                'warped_overlay': self._image_to_base64(warped_overlay),
                'transformation_grid': ''
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'matching_visualization': '',
                'warped_overlay': '',
                'transformation_grid': ''
            }
    
    def _tensor_to_pil_image(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # ì •ê·œí™” í•´ì œ (í•„ìš”ì‹œ)
            if tensor.min() < 0:  # ì •ê·œí™”ëœ í…ì„œì¸ ê²½ìš°
                mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(1, 3, 1, 1)
                tensor = tensor * std + mean
                tensor = torch.clamp(tensor, 0, 1)
            
            numpy_array = self._tensor_to_numpy(tensor)
            return Image.fromarray(numpy_array)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 384), color='black')
    
    def _create_keypoint_visualization(
        self,
        person_image: Image.Image,
        clothing_image: Image.Image,
        ai_result: Dict[str, Any]
    ) -> Image.Image:
        """í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”"""
        try:
            # ì´ë¯¸ì§€ ê²°í•©
            combined_width = person_image.width + clothing_image.width
            combined_height = max(person_image.height, clothing_image.height)
            combined_image = Image.new('RGB', (combined_width, combined_height), color='white')
            
            combined_image.paste(person_image, (0, 0))
            combined_image.paste(clothing_image, (person_image.width, 0))
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(combined_image)
            
            person_keypoints = ai_result['person_keypoints'].cpu().numpy()[0]
            clothing_keypoints = ai_result['clothing_keypoints'].cpu().numpy()[0]
            
            # Person í‚¤í¬ì¸íŠ¸ (ë¹¨ê°„ìƒ‰)
            for point in person_keypoints:
                x, y = point * np.array([person_image.width, person_image.height])
                draw.ellipse([x-3, y-3, x+3, y+3], fill='red', outline='darkred')
            
            # Clothing í‚¤í¬ì¸íŠ¸ (íŒŒë€ìƒ‰)
            for point in clothing_keypoints:
                x, y = point * np.array([clothing_image.width, clothing_image.height])
                x += person_image.width
                draw.ellipse([x-3, y-3, x+3, y+3], fill='blue', outline='darkblue')
            
            # ë§¤ì¹­ ë¼ì¸
            for p_point, c_point in zip(person_keypoints, clothing_keypoints):
                px, py = p_point * np.array([person_image.width, person_image.height])
                cx, cy = c_point * np.array([clothing_image.width, clothing_image.height])
                cx += person_image.width
                draw.line([px, py, cx, cy], fill='green', width=1)
            
            return combined_image
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (1024, 384), color='black')
    
    def _create_warped_overlay(
        self,
        person_image: Image.Image,
        warped_image: Image.Image,
        quality_score: float
    ) -> Image.Image:
        """ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´"""
        try:
            alpha = int(255 * min(0.8, max(0.3, quality_score)))
            warped_resized = warped_image.resize(person_image.size, Image.Resampling.LANCZOS)
            
            person_rgba = person_image.convert('RGBA')
            warped_rgba = warped_resized.convert('RGBA')
            warped_rgba.putalpha(alpha)
            
            overlay = Image.alpha_composite(person_rgba, warped_rgba)
            return overlay.convert('RGB')
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_image
    
    def _image_to_base64(self, image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            image.save(buffer, format='JPEG', quality=85)
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return f"data:image/jpeg;base64,{img_str}"
        except Exception as e:
            self.logger.error(f"âŒ Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”¥ 16. ì„¤ì • ë° í†µê³„ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    def _setup_configurations(self, config: Dict[str, Any]):
        """ì„¤ì • ì´ˆê¸°í™”"""
        self.matching_config = config.get('matching', {
            'method': 'neural_tps',
            'num_keypoints': 25,
            'quality_threshold': 0.7,
            'batch_size': 4 if self.device == "mps" else 2
        })
        
        self.tps_config = config.get('tps', {
            'grid_size': 20,
            'control_points': 25,
            'regularization': 0.01
        })
    
    def _init_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.statistics = {
            'total_processed': 0,
            'successful_matches': 0,
            'average_quality': 0.0,
            'total_processing_time': 0.0,
            'ai_model_calls': 0,
            'error_count': 0,
            'model_creation_success': False
        }
    
    def _update_statistics(self, quality_score: float, processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.statistics['total_processed'] += 1
            
            if quality_score >= self.matching_config['quality_threshold']:
                self.statistics['successful_matches'] += 1
            
            total = self.statistics['total_processed']
            current_avg = self.statistics['average_quality']
            self.statistics['average_quality'] = (current_avg * (total - 1) + quality_score) / total
            
            self.statistics['total_processing_time'] += processing_time
            self.statistics['ai_model_calls'] = self.status.ai_model_calls
            self.statistics['model_creation_success'] = self.status.model_creation_success
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _format_api_response(
        self,
        success: bool,
        final_result: Optional[Dict[str, Any]],
        visualization: Optional[Dict[str, str]],
        quality_score: float,
        processing_time: float,
        error_message: str = ""
    ) -> Dict[str, Any]:
        """API ì‘ë‹µ í¬ë§·"""
        
        if success and final_result:
            return {
                'success': True,
                'message': f'AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}',
                'confidence': quality_score,
                'processing_time': processing_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'details': {
                    'result_image': visualization.get('matching_visualization', ''),
                    'overlay_image': visualization.get('warped_overlay', ''),
                    'num_keypoints': self.matching_config['num_keypoints'],
                    'matching_confidence': quality_score,
                    'method': self.matching_config['method'],
                    'using_real_ai_models': True,
                    'ai_model_calls': self.status.ai_model_calls,
                    'model_creation_success': self.status.model_creation_success,
                    'dependencies_injected': self.status.dependencies_injected,
                    'improved_dependency_system': True
                },
                'warped_clothing': final_result['warped_clothing'],
                'warped_mask': final_result.get('warped_mask'),
                'person_keypoints': final_result.get('person_keypoints', []),
                'clothing_keypoints': final_result.get('clothing_keypoints', []),
                'quality_score': quality_score,
                'metadata': {
                    'method': 'neural_tps_ai',
                    'device': self.device,
                    'real_ai_models_used': True,
                    'dependencies_injected': self.status.dependencies_injected,
                    'ai_model_calls': self.status.ai_model_calls,
                    'model_creation_success': self.status.model_creation_success,
                    'step_01_pattern_applied': True,
                    'type_checking_pattern_applied': True,
                    'circular_import_resolved': True,
                    'improved_dependency_system': True,
                    'dependency_status': self.dependency_manager.get_dependency_status()
                }
            }
        else:
            return {
                'success': False,
                'message': f'AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {error_message}',
                'confidence': 0.0,
                'processing_time': processing_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'error': error_message,
                'metadata': {
                    'real_ai_models_used': False,
                    'dependencies_injected': self.status.dependencies_injected,
                    'error_count': self.status.error_count,
                    'model_creation_success': self.status.model_creation_success,
                    'type_checking_pattern_applied': True,
                    'circular_import_resolved': True,
                    'improved_dependency_system': True,
                    'dependency_status': self.dependency_manager.get_dependency_status()
                }
            }
    
    # ==============================================
    # ğŸ”¥ 17. BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "geometric_matching",
            "step_number": 4,
            "device": self.device,
            "initialized": self.status.initialized,
            "models_loaded": self.status.models_loaded,
            "dependencies_injected": self.status.dependencies_injected,
            "ai_model_available": self.geometric_model is not None,
            "model_creation_success": self.status.model_creation_success,
            "config": {
                "method": self.matching_config['method'],
                "num_keypoints": self.matching_config['num_keypoints'],
                "quality_threshold": self.matching_config['quality_threshold']
            },
            "performance": self.statistics,
            "status": {
                "processing_active": self.status.processing_active,
                "error_count": self.status.error_count,
                "ai_model_calls": self.status.ai_model_calls
            },
            "patterns_applied": {
                "step_01_pattern": True,
                "type_checking_pattern": True,
                "circular_import_resolved": True,
                "checkpoint_to_model_conversion": True,
                "dict_object_callable_issue_resolved": True,
                "improved_dependency_system": True
            },
            "dependency_status": self.dependency_manager.get_dependency_status()
        }
    
    async def validate_inputs(self, person_image: Any, clothing_image: Any) -> Dict[str, Any]:
        """ì…ë ¥ ê²€ì¦"""
        try:
            validation_result = {
                'valid': False,
                'person_image': False,
                'clothing_image': False,
                'errors': []
            }
            
            # Person ì´ë¯¸ì§€ ê²€ì¦
            try:
                self._validate_single_image(person_image, "person_image")
                validation_result['person_image'] = True
            except Exception as e:
                validation_result['errors'].append(f"Person ì´ë¯¸ì§€ ì˜¤ë¥˜: {e}")
            
            # Clothing ì´ë¯¸ì§€ ê²€ì¦
            try:
                self._validate_single_image(clothing_image, "clothing_image")
                validation_result['clothing_image'] = True
            except Exception as e:
                validation_result['errors'].append(f"Clothing ì´ë¯¸ì§€ ì˜¤ë¥˜: {e}")
            
            validation_result['valid'] = (
                validation_result['person_image'] and 
                validation_result['clothing_image'] and 
                len(validation_result['errors']) == 0
            )
            
            return validation_result
            
        except Exception as e:
            return {
                'valid': False,
                'error': str(e),
                'person_image': False,
                'clothing_image': False
            }
    
    def _validate_single_image(self, image: Any, name: str):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ê²€ì¦"""
        if image is None:
            raise ValueError(f"{name}ì´ None")
        
        if isinstance(image, np.ndarray):
            if len(image.shape) != 3 or image.shape[2] != 3:
                raise ValueError(f"{name} í˜•íƒœ ì˜¤ë¥˜: {image.shape}")
        elif isinstance(image, Image.Image):
            if image.mode not in ['RGB', 'RGBA']:
                raise ValueError(f"{name} ëª¨ë“œ ì˜¤ë¥˜: {image.mode}")
        elif isinstance(image, torch.Tensor):
            if image.dim() not in [3, 4]:
                raise ValueError(f"{name} í…ì„œ ì°¨ì› ì˜¤ë¥˜: {image.dim()}")
        else:
            raise ValueError(f"{name} íƒ€ì… ì˜¤ë¥˜: {type(image)}")
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        try:
            total_processed = self.statistics['total_processed']
            success_rate = (
                (self.statistics['successful_matches'] / total_processed * 100) 
                if total_processed > 0 else 0
            )
            
            return {
                "total_processed": total_processed,
                "success_rate": success_rate,
                "average_quality": self.statistics['average_quality'],
                "average_processing_time": (
                    self.statistics['total_processing_time'] / total_processed
                ) if total_processed > 0 else 0,
                "error_count": self.status.error_count,
                "ai_model_calls": self.statistics['ai_model_calls'],
                "device": self.device,
                "dependencies_injected": self.status.dependencies_injected,
                "using_real_ai_models": True,
                "model_creation_success": self.statistics['model_creation_success'],
                "patterns_applied": {
                    "step_01_pattern": True,
                    "type_checking_pattern": True,
                    "circular_import_resolved": True,
                    "improved_dependency_system": True
                },
                "dependency_status": self.dependency_manager.get_dependency_status()
            }
        except Exception as e:
            return {"error": str(e)}
    
    # ==============================================
    # ğŸ”¥ 18. ì¶”ê°€ BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ì§ì ‘ ë°˜í™˜ (BaseStepMixin í˜¸í™˜ì„±)"""
        try:
            if model_name == "geometric_matching" or model_name is None:
                return self.geometric_model
            else:
                self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ëª¨ë¸ {model_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë°˜í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def setup_model_precision(self, model: Any) -> Any:
        """ëª¨ë¸ ì •ë°€ë„ ì„¤ì • (BaseStepMixin í˜¸í™˜ì„±)"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float() if hasattr(model, 'float') else model
            elif self.device == "cuda" and hasattr(model, 'half'):
                return model.half()
            else:
                return model.float() if hasattr(model, 'float') else model
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model
    
    def get_model_info(self, model_name: str = "geometric_matching") -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (BaseStepMixin í˜¸í™˜ì„±)"""
        try:
            if model_name == "geometric_matching" and self.geometric_model:
                model = self.geometric_model
                return {
                    "model_name": model_name,
                    "model_type": type(model).__name__,
                    "device": str(model.keypoint_net.backbone[0].weight.device) if hasattr(model, 'keypoint_net') else self.device,
                    "parameters": sum(p.numel() for p in model.parameters()) if hasattr(model, 'parameters') else 0,
                    "loaded": True,
                    "real_model": True,
                    "patterns_applied": {
                        "step_01_pattern": True,
                        "type_checking_pattern": True,
                        "circular_import_resolved": True,
                        "improved_dependency_system": True
                    },
                    "model_creation_success": self.status.model_creation_success,
                    "dependency_status": self.dependency_manager.get_dependency_status()
                }
            else:
                return {
                    "error": f"ëª¨ë¸ {model_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    "available_models": ["geometric_matching"],
                    "patterns_applied": {
                        "step_01_pattern": True,
                        "type_checking_pattern": True,
                        "circular_import_resolved": True,
                        "improved_dependency_system": True
                    }
                }
        except Exception as e:
            return {"error": str(e)}
    
    # ==============================================
    # ğŸ”¥ 19. ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™” (ê°œì„ ëœ ì˜ì¡´ì„± ì‚¬ìš©)
    # ==============================================
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°œì„ ëœ ì˜ì¡´ì„± ì‚¬ìš©)"""
        try:
            # ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
            asyncio.create_task(self.dependency_manager.optimize_memory(aggressive=False))
            
            gc.collect()
            
            if self.device == "mps" and TORCH_AVAILABLE and hasattr(torch.backends, 'mps'):
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE:
                torch.cuda.empty_cache()
            
            self.logger.debug("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _apply_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì ìš©"""
        try:
            if self.device == "mps":
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                if TORCH_AVAILABLE:
                    torch.set_num_threads(16)  # M3 Max 16ì½”ì–´
                self.matching_config['batch_size'] = 8  # M3 Max ìµœì í™”
                self.logger.info("ğŸ M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 20. ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì™„ì „ ìœ ì§€)
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ Step 04: AI ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            self.status.processing_active = False
            
            # AI ëª¨ë¸ ì •ë¦¬
            if self.geometric_model:
                if hasattr(self.geometric_model, 'cpu'):
                    self.geometric_model.cpu()
                del self.geometric_model
                self.geometric_model = None
            
            # ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.dependency_manager.optimize_memory(aggressive=True)
            
            self._safe_memory_cleanup()
            
            self.logger.info("âœ… Step 04: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step 04: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'status'):
                self.status.processing_active = False
        except Exception:
            pass

# ==============================================
# ğŸ”¥ 21. í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ ìœ ì§€)
# ==============================================

def create_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    return GeometricMatchingStep(**kwargs)

def create_m3_max_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    kwargs.setdefault('device', 'mps')
    kwargs.setdefault('config', {})
    kwargs['config'].setdefault('matching', {})['batch_size'] = 8
    return GeometricMatchingStep(**kwargs)

# ==============================================
# ğŸ”¥ 22. ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ì™„ì „ ìœ ì§€)
# ==============================================

def validate_dependencies() -> Dict[str, bool]:
    """ì˜ì¡´ì„± ê²€ì¦"""
    return {
        "torch": TORCH_AVAILABLE,
        "vision": VISION_AVAILABLE,
        "opencv": OPENCV_AVAILABLE,
        "scipy": SCIPY_AVAILABLE,
        "utils": UTILS_AVAILABLE,
        "base_step_mixin": BaseStepMixin is not None,
        "model_loader_dynamic": get_model_loader() is not None,
        "memory_manager_dynamic": get_memory_manager() is not None,
        "data_converter_dynamic": get_data_converter() is not None,
        "di_container_dynamic": get_di_container() is not None
    }

async def test_step_04_complete_pipeline() -> bool:
    """Step 04 ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    
    try:
        # ì˜ì¡´ì„± í™•ì¸
        deps = validate_dependencies()
        missing_deps = [k for k, v in deps.items() if not v]
        if missing_deps:
            logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì˜ì¡´ì„±: {missing_deps}")
        
        # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step = GeometricMatchingStep(device="cpu")
        
        # ê°œì„ ì‚¬í•­ í™•ì¸
        logger.info("ğŸ” ê°œì„ ì‚¬í•­ í™•ì¸:")
        logger.info(f"  - ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ì: âœ…")
        logger.info(f"  - TYPE_CHECKING íŒ¨í„´: âœ…")
        logger.info(f"  - ìë™ ì˜ì¡´ì„± ì£¼ì…: âœ…")
        logger.info(f"  - Step 01 íŒ¨í„´: âœ…")
        logger.info(f"  - BaseStepMixin í˜¸í™˜: âœ…")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            await step.initialize()
            logger.info("âœ… ì´ˆê¸°í™” ì„±ê³µ")
            
            # ëª¨ë¸ ìƒì„± í™•ì¸
            if step.geometric_model is not None:
                logger.info("âœ… AI ëª¨ë¸ ìƒì„± ì„±ê³µ (ê°œì„ ëœ ì˜ì¡´ì„± ì‹œìŠ¤í…œ)")
                logger.info(f"  - ëª¨ë¸ íƒ€ì…: {type(step.geometric_model).__name__}")
                logger.info(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in step.geometric_model.parameters()):,}")
            else:
                logger.warning("âš ï¸ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        dummy_person = np.random.randint(0, 255, (384, 512, 3), dtype=np.uint8)
        dummy_clothing = np.random.randint(0, 255, (384, 512, 3), dtype=np.uint8)
        
        try:
            result = await step.process(dummy_person, dummy_clothing)
            if result['success']:
                logger.info(f"âœ… ì²˜ë¦¬ ì„±ê³µ - í’ˆì§ˆ: {result['confidence']:.3f}")
                logger.info(f"  - AI ëª¨ë¸ í˜¸ì¶œ: {result['metadata']['ai_model_calls']}íšŒ")
                logger.info(f"  - ê°œì„ ëœ ì˜ì¡´ì„± ì‹œìŠ¤í…œ: {result['metadata']['improved_dependency_system']}")
                logger.info(f"  - ì˜ì¡´ì„± ìƒíƒœ: {result['metadata']['dependency_status']['total_injected']}/4ê°œ ì£¼ì…")
            else:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('message', 'Unknown error')}")
        except Exception as e:
            logger.warning(f"âš ï¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # Step ì •ë³´ í™•ì¸
        step_info = await step.get_step_info()
        logger.info("ğŸ“‹ Step ì •ë³´:")
        logger.info(f"  - ì´ˆê¸°í™”: {'âœ…' if step_info['initialized'] else 'âŒ'}")
        logger.info(f"  - ëª¨ë¸ ë¡œë“œ: {'âœ…' if step_info['models_loaded'] else 'âŒ'}")
        logger.info(f"  - ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if step_info['dependencies_injected'] else 'âŒ'}")
        logger.info(f"  - ê°œì„ ëœ ì˜ì¡´ì„± ì‹œìŠ¤í…œ: {'âœ…' if step_info['patterns_applied']['improved_dependency_system'] else 'âŒ'}")
        logger.info(f"  - ì˜ì¡´ì„± ìƒíƒœ: {step_info['dependency_status']['total_injected']}/4ê°œ")
        
        # ì •ë¦¬
        await step.cleanup()
        
        logger.info("âœ… Step 04 ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ 23. ëª¨ë“ˆ ì •ë³´ (ì™„ì „ ìœ ì§€)
# ==============================================

__version__ = "10.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ê¸°í•˜í•™ì  ë§¤ì¹­ - ì™„ì „ ê°œì„  ë²„ì „ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)"
__features__ = [
    "ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€",
    "ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„ ", 
    "ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™” ë° ì¼ê´€ì„± í™•ë³´",
    "BaseStepMixin ì™„ì „ í˜¸í™˜",
    "TYPE_CHECKING íŒ¨í„´ ìœ ì§€",
    "ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°",
    "AI ëª¨ë¸ ì—°ë™ ì™„ì „ êµ¬í˜„",
    "Step 01 ì„±ê³µ íŒ¨í„´ ì ìš©",
    "4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€",
    "ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ì",
    "ìë™ ì˜ì¡´ì„± ì£¼ì…",
    "M3 Max 128GB ìµœì í™”",
    "conda í™˜ê²½ ìš°ì„ ",
    "í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±"
]

__all__ = [
    'GeometricMatchingStep',
    'GeometricMatchingModel',
    'KeypointDetectionNet',
    'TPSTransformationNet',
    'GeometricMatchingModelFactory',
    'ImprovedDependencyManager',
    'create_geometric_matching_step',
    'create_m3_max_geometric_matching_step',
    'validate_dependencies',
    'test_step_04_complete_pipeline',
    'get_model_loader',
    'get_memory_manager',
    'get_data_converter',
    'get_di_container',
    'get_base_step_mixin_class',
    'ProcessingStatus',
    'GeometricMatchingError',
    'ModelLoaderError',
    'DependencyInjectionError'
]

logger = logging.getLogger(__name__)
logger.info("âœ… GeometricMatchingStep v10.0 ë¡œë“œ ì™„ë£Œ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)")
logger.info("ğŸ”¥ ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€")
logger.info("ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„ ")
logger.info("ğŸ”¥ ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™” ë° ì¼ê´€ì„± í™•ë³´")
logger.info("ğŸ”¥ BaseStepMixin ì™„ì „ í˜¸í™˜")
logger.info("ğŸ”¥ TYPE_CHECKING íŒ¨í„´ ìœ ì§€")
logger.info("ğŸ”¥ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("ğŸ”¥ AI ëª¨ë¸ ì—°ë™ ì™„ì „ êµ¬í˜„")
logger.info("ğŸ”¥ Step 01 ì„±ê³µ íŒ¨í„´ ì ìš©")
logger.info("ğŸ”¥ 4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€")
logger.info("ğŸ”¥ ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ì - ImprovedDependencyManager")
logger.info("ğŸ”¥ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ")
logger.info("ğŸ”¥ M3 Max + conda í™˜ê²½ ìµœì í™”")
logger.info("ğŸ”¥ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")

# ê°œë°œìš© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    import asyncio
    
    print("=" * 80)
    print("ğŸ”¥ GeometricMatchingStep v10.0 - ì™„ì „ ê°œì„  ë²„ì „ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)")
    print("=" * 80)
    
    # ì˜ì¡´ì„± í™•ì¸
    deps = validate_dependencies()
    print("\nğŸ“‹ ì˜ì¡´ì„± í™•ì¸:")
    for dep, available in deps.items():
        status = "âœ…" if available else "âŒ"
        print(f"  {status} {dep}: {available}")
    
    # ê°œì„ ì‚¬í•­ í™•ì¸
    print("\nğŸ” ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print(f"  âœ… ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€")
    print(f"  âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„ ")
    print(f"  âœ… ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™”")
    print(f"  âœ… BaseStepMixin ì™„ì „ í˜¸í™˜")
    print(f"  âœ… TYPE_CHECKING íŒ¨í„´ ìœ ì§€")
    print(f"  âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
    print(f"  âœ… ImprovedDependencyManager ë„ì…")
    print(f"  âœ… ìë™ ì˜ì¡´ì„± ì£¼ì…")
    
    # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:")
    test_result = asyncio.run(test_step_04_complete_pipeline())
    print(f"  {'âœ…' if test_result else 'âŒ'} íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if test_result else 'ì‹¤íŒ¨'}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ Step 04 ì™„ì „ ê°œì„  ì™„ë£Œ!")
    print("âœ… ì›ë³¸ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ìœ ì§€")
    print("âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ê°œì„ ")
    print("âœ… ì´ˆê¸°í™” ë¡œì§ ê°„ì†Œí™”")
    print("âœ… BaseStepMixin ì™„ì „ í˜¸í™˜")
    print("âœ… TYPE_CHECKING íŒ¨í„´ ìœ ì§€")
    print("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
    print("âœ… AI ëª¨ë¸ ì—°ë™ ì™„ì „ êµ¬í˜„")
    print("âœ… Step 01 ì„±ê³µ íŒ¨í„´ ì ìš©")
    print("âœ… 4ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ìœ ì§€")
    print("âœ… ê°œì„ ëœ ì˜ì¡´ì„± ê´€ë¦¬ì ì™„ì„±")
    print("âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… êµ¬í˜„")
    print("=" * 80)