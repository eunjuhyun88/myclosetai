#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ v27.0 (ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„)
================================================================================

âœ… HumanParsingStep ìˆ˜ì¤€ì˜ ì™„ì „í•œ AI ì—°ë™ êµ¬í˜„
âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (gmm_final.pth, tps_network.pth, sam_vit_h_4b8939.pth)
âœ… ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™” ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµ¬í˜„
âœ… GMM + TPS ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ë³€í˜• ê³„ì‚°
âœ… ì‹¤ì œ ì˜ë¥˜ ì›Œí•‘ ë° ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
âœ… í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì •ë°€ ë§¤ì¹­
âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”
âœ… TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ì‹¤ì œ ì˜· ê°ˆì•„ì…íˆê¸° ê°€ëŠ¥í•œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ êµ¬ì²´ êµ¬í˜„

ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš©:
- gmm_final.pth (44.7MB) - Geometric Matching Module
- tps_network.pth (527.8MB) - Thin-Plate Spline Transformation Network
- sam_vit_h_4b8939.pth (2445.7MB) - Segment Anything Model (ê³µìœ )
- resnet101_geometric.pth (170.5MB) - ResNet-101 ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
- raft-things.pth (20.1MB) - Optical Flow ê³„ì‚°

ì²˜ë¦¬ íë¦„:
1. StepFactory.create_step(StepType.GEOMETRIC_MATCHING) â†’ GeometricMatchingStep ìƒì„±
2. ModelLoader ì˜ì¡´ì„± ì£¼ì… â†’ set_model_loader()
3. MemoryManager ì˜ì¡´ì„± ì£¼ì… â†’ set_memory_manager()
4. ì´ˆê¸°í™” ì‹¤í–‰ â†’ initialize() â†’ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
5. AI ì¶”ë¡  ì‹¤í–‰ â†’ _run_ai_inference() â†’ GMM + TPS ê¸°ë°˜ ë³€í˜• ê³„ì‚°
6. ì‹¤ì œ ì˜ë¥˜ ì›Œí•‘ â†’ ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„± â†’ ë‹¤ìŒ Stepìœ¼ë¡œ ë°ì´í„° ì „ë‹¬

Author: MyCloset AI Team
Date: 2025-07-30
Version: v27.0 (Complete AI Integration + Virtual Clothing Fitting)
"""

# ==============================================
# ğŸ”¥ Import ì„¹ì…˜ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€ (GitHub í‘œì¤€ íŒ¨í„´)
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
    from app.ai_pipeline.core.di_container import DIContainer

# ==============================================
# ğŸ”¥ conda í™˜ê²½ ë° ì‹œìŠ¤í…œ ìµœì í™”
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_mycloset_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# M3 Max ê°ì§€ ë° ìµœì í™”
def detect_m3_max() -> bool:
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# M3 Max ìµœì í™” ì„¤ì •
if IS_M3_MAX and CONDA_INFO['is_mycloset_env']:
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    os.environ['TORCH_MPS_PREFER_METAL'] = '1'

# ==============================================
# ğŸ”¥ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch í•„ìˆ˜ (MPS ì§€ì›)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
    MPS_AVAILABLE = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    # M3 Max ìµœì í™”
    if CONDA_INFO['is_mycloset_env'] and IS_M3_MAX:
        cpu_count = os.cpu_count()
        torch.set_num_threads(max(1, cpu_count // 2))
        
except ImportError:
    raise ImportError("âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch")

# PIL í•„ìˆ˜
try:
    from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    raise ImportError("âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge")

# NumPy í•„ìˆ˜
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    raise ImportError("âŒ NumPy í•„ìˆ˜: conda install numpy -c conda-forge")

# OpenCV ì„ íƒì‚¬í•­
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logging.getLogger(__name__).info("OpenCV ì—†ìŒ - PIL ê¸°ë°˜ìœ¼ë¡œ ë™ì‘")

# SciPy ì„ íƒì‚¬í•­ (Procrustes ë¶„ì„ìš©)
try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata, RBFInterpolator
    import scipy.ndimage as ndimage
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# BaseStepMixin ë™ì  import (GitHub í‘œì¤€ íŒ¨í„´)
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            # í´ë°±: ìƒëŒ€ ê²½ë¡œ
            from .base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            logger.error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None
        
BaseStepMixin = get_base_step_mixin_class()


# ============================================================================
# ğŸ”¥ 1. step_model_requirements.py ì™„ì „ í˜¸í™˜ ì‹œìŠ¤í…œ (ì¤‘ìš”ë„: â˜…â˜…â˜…â˜…â˜…)
# ============================================================================

def get_step_model_request():
    """step_model_requestsì—ì„œ GeometricMatchingStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.step_model_requests', package=__name__)
        requests = getattr(module, 'REAL_STEP_MODEL_REQUESTS', {})
        return requests.get('GeometricMatchingStep')
    except ImportError as e:
        logging.debug(f"step_model_requests import ì‹¤íŒ¨: {e}")
        return None

def _load_requirements_config(self):
    """step_model_requirements.py ìš”êµ¬ì‚¬í•­ ì„¤ì • ë¡œë“œ"""
    if self.step_request:
        # step_model_requirements.py ê¸°ì¤€ ì„¤ì •
        self.matching_config = {
            'method': 'advanced_deeplab_aspp_self_attention',
            'input_size': self.step_request.input_size,  # (256, 192)
            'output_format': self.step_request.output_format,  # "transformation_matrix"
            'model_architecture': self.step_request.model_architecture,  # "gmm_tps"
            'batch_size': self.step_request.batch_size,  # 2
            'memory_fraction': self.step_request.memory_fraction,  # 0.2
            'device': self.step_request.device,  # "auto"
            'precision': self.step_request.precision,  # "fp16"
            'use_real_models': True,
            'detailed_data_spec': True,
            'algorithm_type': 'advanced_deeplab_aspp_self_attention'
        }
        
        self.advanced_config = {
        'method': 'advanced_deeplab_aspp_self_attention',
        'algorithm_type': 'advanced_deeplab_aspp_self_attention',
        'use_real_models': True,
        'ai_enhanced': kwargs.get('ai_enhanced', True),
        'batch_size': 2,
        'precision': 'fp16'
         }
    
        # DetailedDataSpec ë¡œë“œ
        if hasattr(self.step_request, 'data_spec'):
            self.data_spec = self.step_request.data_spec
            self.status.detailed_data_spec_loaded = True
            self.logger.info("âœ… DetailedDataSpec ë¡œë“œ ì™„ë£Œ")
        else:
            self.data_spec = None
            self.logger.warning("âš ï¸ DetailedDataSpec ì—†ìŒ")
    else:
        self._load_fallback_config()

def _load_fallback_config(self):
    """í´ë°± ì„¤ì • ë¡œë“œ"""
    self.matching_config = {
        'method': 'advanced_deeplab_aspp_self_attention',
        'input_size': (256, 192),
        'output_format': 'transformation_matrix',
        'batch_size': 2,
        'device': self.device,
        'use_real_models': True,
        'algorithm_type': 'advanced_deeplab_aspp_self_attention'
    }
    self.data_spec = None
    self.logger.warning("âš ï¸ step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ì„¤ì • ì‚¬ìš©")


# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™”)
# ==============================================

class GeometricMatchingModule(nn.Module):
    """ì‹¤ì œ GMM (Geometric Matching Module) - ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™”"""
    
    def __init__(self, input_nc=6, output_nc=1):
        super().__init__()
        self.input_nc = input_nc  # person + clothing
        self.output_nc = output_nc
        
        # Feature Extraction Network (ResNet ê¸°ë°˜)
        self.feature_extractor = nn.Sequential(
            # Initial Convolution
            nn.Conv2d(input_nc, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # ResNet Blocks
            self._make_layer(64, 64, 3, stride=1),
            self._make_layer(256, 128, 4, stride=2),
            self._make_layer(512, 256, 6, stride=2),
            self._make_layer(1024, 512, 3, stride=2),
        )
        
        # Correlation Module (ì˜·ê³¼ ì‚¬ëŒ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°)
        self.correlation = nn.Sequential(
            nn.Conv2d(2048, 512, kernel_size=3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Regression Network (ë³€í˜• ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡)
        self.regression = nn.Sequential(
            nn.AdaptiveAvgPool2d((4, 3)),  # 4x3 = 12ê°œ ì œì–´ì 
            nn.Flatten(),
            nn.Linear(256 * 4 * 3, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 2 * 3 * 4),  # 2D coordinates for 3x4 grid
        )
        
        # Grid Generator (TPS ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±)
        self.grid_generator = TPSGridGenerator()
        
    def _make_layer(self, inplanes, planes, blocks, stride=1):
        """ResNet layer ìƒì„±"""
        layers = []
        layers.append(self._bottleneck_block(inplanes, planes, stride))
        inplanes = planes * 4
        for _ in range(1, blocks):
            layers.append(self._bottleneck_block(inplanes, planes))
        return nn.Sequential(*layers)
    
    def _bottleneck_block(self, inplanes, planes, stride=1):
        """Bottleneck block"""
        expansion = 4
        downsample = None
        
        if stride != 1 or inplanes != planes * expansion:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * expansion, 1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * expansion),
            )
        
        class BottleneckBlock(nn.Module):
            def __init__(self, inplanes, planes, stride, downsample):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                
            def forward(self, x):
                residual = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    residual = self.downsample(x)
                
                out += residual
                out = self.relu(out)
                
                return out
        
        return BottleneckBlock(inplanes, planes, stride, downsample)
    
    def forward(self, person_image, clothing_image):
        """ìˆœì „íŒŒ: ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰"""
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_features = self.correlation(features)
        
        # ë³€í˜• ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡
        theta = self.regression(correlation_features)
        theta = theta.view(-1, 2, 12)  # ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ reshape
        
        # TPS ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
        grid = self.grid_generator(theta, person_image.size())
        
        # ì˜ë¥˜ ì´ë¯¸ì§€ì— ë³€í˜• ì ìš©
        warped_clothing = F.grid_sample(clothing_image, grid, mode='bilinear', 
                                      padding_mode='border', align_corners=False)
        
        return {
            'transformation_matrix': theta,
            'transformation_grid': grid,
            'warped_clothing': warped_clothing,
            'correlation_features': correlation_features
        }

class TPSGridGenerator(nn.Module):
    """TPS (Thin-Plate Spline) ê·¸ë¦¬ë“œ ìƒì„±ê¸°"""
    
    def __init__(self):
        super().__init__()
        
        # ì œì–´ì  ì´ˆê¸°í™” (3x4 = 12ê°œ ì )
        self.register_buffer('control_points', self._create_control_points())
        
    def _create_control_points(self):
        """3x4 ì œì–´ì  ìƒì„±"""
        # ì •ê·œí™”ëœ ì¢Œí‘œê³„ (-1, 1)ì—ì„œ ì œì–´ì  ë°°ì¹˜
        x = torch.linspace(-1, 1, 4)
        y = torch.linspace(-1, 1, 3)
        
        grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
        control_points = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)
        
        return control_points  # [12, 2]
    
    def _compute_tps_weights(self, source_points, target_points):
        """TPS ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        n_points = source_points.size(1)
        
        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        distances = torch.cdist(source_points, source_points)
        
        # U í•¨ìˆ˜ ê³„ì‚° (r^2 * log(r))
        U = distances ** 2 * torch.log(distances + 1e-8)
        U[distances == 0] = 0  # 0 ê±°ë¦¬ëŠ” 0ìœ¼ë¡œ ì„¤ì •
        
        # P í–‰ë ¬ (ì•„í•€ ë³€í˜•ìš©)
        ones = torch.ones(source_points.size(0), n_points, 1, device=source_points.device)
        P = torch.cat([ones, source_points], dim=2)
        
        # K í–‰ë ¬ êµ¬ì„±
        zeros = torch.zeros(source_points.size(0), 3, 3, device=source_points.device)
        K = torch.cat([
            torch.cat([U, P], dim=2),
            torch.cat([P.transpose(1, 2), zeros], dim=2)
        ], dim=1)
        
        # íƒ€ê²Ÿ í¬ì¸íŠ¸ í™•ì¥
        zeros_target = torch.zeros(target_points.size(0), 3, 2, device=target_points.device)
        Y = torch.cat([target_points, zeros_target], dim=1)
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        try:
            weights = torch.linalg.solve(K, Y)
        except:
            # íŠ¹ì´í–‰ë ¬ì¸ ê²½ìš° pseudo-inverse ì‚¬ìš©
            weights = torch.pinverse(K) @ Y
        
        return weights
    
    def forward(self, theta, input_size):
        """TPS ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±"""
        batch_size, height, width = theta.size(0), input_size[2], input_size[3]
        device = theta.device
        
        # thetaë¥¼ ì œì–´ì  ì¢Œí‘œë¡œ ë³€í™˜
        target_points = theta.view(batch_size, 12, 2)
        
        # ì†ŒìŠ¤ ì œì–´ì  í™•ì¥
        source_points = self.control_points.unsqueeze(0).expand(batch_size, -1, -1)
        
        # TPS ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = self._compute_tps_weights(source_points, target_points)
        
        # ì¶œë ¥ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=device),
            torch.linspace(-1, 1, width, device=device),
            indexing='ij'
        )
        
        grid_points = torch.stack([x.flatten(), y.flatten()], dim=1).unsqueeze(0)
        grid_points = grid_points.expand(batch_size, -1, -1)
        
        # TPS ë³€í˜• ì ìš©
        warped_grid = self._apply_tps_transform(grid_points, source_points, weights)
        warped_grid = warped_grid.view(batch_size, height, width, 2)
        
        return warped_grid
    
    def _apply_tps_transform(self, grid_points, control_points, weights):
        """TPS ë³€í˜• ì ìš©"""
        batch_size, n_grid, _ = grid_points.shape
        n_control = control_points.size(1)
        
        # ê·¸ë¦¬ë“œ ì ê³¼ ì œì–´ì  ê°„ ê±°ë¦¬ ê³„ì‚°
        distances = torch.cdist(grid_points, control_points)
        
        # U í•¨ìˆ˜ ì ìš©
        U = distances ** 2 * torch.log(distances + 1e-8)
        U[distances == 0] = 0
        
        # P í–‰ë ¬ (ì•„í•€ ë¶€ë¶„)
        ones = torch.ones(batch_size, n_grid, 1, device=grid_points.device)
        P = torch.cat([ones, grid_points], dim=2)
        
        # ì „ì²´ ê¸°ì € í•¨ìˆ˜ í–‰ë ¬
        basis = torch.cat([U, P], dim=2)
        
        # ë³€í˜•ëœ ì¢Œí‘œ ê³„ì‚°
        transformed = torch.bmm(basis, weights)
        
        return transformed

class OpticalFlowNetwork(nn.Module):
    """RAFT ê¸°ë°˜ Optical Flow ë„¤íŠ¸ì›Œí¬ (ì˜ë¥˜ ì›€ì§ì„ ì¶”ì )"""
    
    def __init__(self, feature_dim=256, hidden_dim=128):
        super().__init__()
        
        # Feature Encoder
        self.feature_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, feature_dim, 3, stride=2, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
        )
        
        # Context Encoder
        self.context_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, hidden_dim, 3, stride=2, padding=1),
        )
        
        # Flow Head
        self.flow_head = nn.Sequential(
            nn.Conv2d(hidden_dim + feature_dim, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 2, 3, padding=1),
        )
        
    def forward(self, img1, img2):
        """Optical flow ê³„ì‚°"""
        # íŠ¹ì§• ì¶”ì¶œ
        feat1 = self.feature_encoder(img1)
        feat2 = self.feature_encoder(img2)
        
        # Context ì •ë³´
        context = self.context_encoder(img1)
        
        # Feature correlation
        correlation = self._compute_correlation(feat1, feat2)
        
        # Contextì™€ correlation ê²°í•©
        combined = torch.cat([context, correlation], dim=1)
        
        # Flow ì˜ˆì¸¡
        flow = self.flow_head(combined)
        
        return flow
    
    def _compute_correlation(self, feat1, feat2):
        """Feature correlation ê³„ì‚°"""
        batch_size, dim, H, W = feat1.shape
        
        # Correlation volume ê³„ì‚°
        feat1_reshaped = feat1.view(batch_size, dim, H * W)
        feat2_reshaped = feat2.view(batch_size, dim, H * W)
        
        correlation = torch.bmm(feat1_reshaped.transpose(1, 2), feat2_reshaped)
        correlation = correlation.view(batch_size, H * W, H, W)
        
        # Max poolingìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ
        correlation = F.adaptive_avg_pool2d(correlation, (H, W))
        
        return correlation

class KeypointMatchingNetwork(nn.Module):
    """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, num_keypoints=18):
        super().__init__()
        self.num_keypoints = num_keypoints
        
        # Keypoint Feature Extractor
        self.keypoint_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Keypoint Detector
        self.keypoint_detector = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, num_keypoints, 1),
            nn.Sigmoid()
        )
        
        # Descriptor Generator
        self.descriptor_generator = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 1),
        )
        
    def forward(self, image):
        """í‚¤í¬ì¸íŠ¸ ê°ì§€ ë° ë””ìŠ¤í¬ë¦½í„° ìƒì„±"""
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.keypoint_encoder(image)
        
        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        keypoint_heatmaps = self.keypoint_detector(features)
        
        # ë””ìŠ¤í¬ë¦½í„° ìƒì„±
        descriptors = self.descriptor_generator(features)
        
        return {
            'keypoint_heatmaps': keypoint_heatmaps,
            'descriptors': descriptors,
            'features': features
        }


# ============================================================================
# ğŸ”¥ 2. ê³ ê¸‰ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ë“¤ (ì¤‘ìš”ë„: â˜…â˜…â˜…â˜…â˜…)
# ============================================================================

class DeepLabV3PlusBackbone(nn.Module):
    """DeepLabV3+ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ - ê¸°í•˜í•™ì  ë§¤ì¹­ íŠ¹í™”"""

    def __init__(self, input_nc=6, backbone='resnet101', output_stride=16):
        super().__init__()
        self.output_stride = output_stride
        self.input_nc = input_nc

        # ResNet-101 ë°±ë³¸ êµ¬ì„± (6ì±„ë„ ì…ë ¥ ì§€ì›)
        self.conv1 = nn.Conv2d(input_nc, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNet Layers with Dilated Convolution
        self.layer1 = self._make_layer(64, 64, 3, stride=1)      # 256 channels
        self.layer2 = self._make_layer(256, 128, 4, stride=2)    # 512 channels  
        self.layer3 = self._make_layer(512, 256, 23, stride=2)   # 1024 channels
        self.layer4 = self._make_layer(1024, 512, 3, stride=1, dilation=2)  # 2048 channels

        # Low-level feature extraction (for decoder)
        self.low_level_conv = nn.Conv2d(256, 48, 1, bias=False)
        self.low_level_bn = nn.BatchNorm2d(48)

    def _make_layer(self, inplanes, planes, blocks, stride=1, dilation=1):
        """ResNet ë ˆì´ì–´ ìƒì„± (Bottleneck êµ¬ì¡°)"""
        layers = []

        # Downsample layer
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, 1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )

        # First block
        layers.append(self._bottleneck_block(inplanes, planes, stride, dilation, downsample))

        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(self._bottleneck_block(planes * 4, planes, 1, dilation))

        return nn.Sequential(*layers)

    def _bottleneck_block(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        """ResNet Bottleneck ë¸”ë¡"""
        class BottleneckBlock(nn.Module):
            def __init__(self, inplanes, planes, stride, dilation, downsample):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=dilation, 
                                     dilation=dilation, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample

            def forward(self, x):
                residual = x

                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)

                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)

                out = self.conv3(out)
                out = self.bn3(out)

                if self.downsample is not None:
                    residual = self.downsample(x)

                out += residual
                out = self.relu(out)

                return out
                
        return BottleneckBlock(inplanes, planes, stride, dilation, downsample)

    def forward(self, x):
        # Initial convolution
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        # ResNet layers
        x = self.layer1(x)
        low_level_feat = x  # Save for decoder

        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # Process low-level features
        low_level_feat = self.low_level_conv(low_level_feat)
        low_level_feat = self.low_level_bn(low_level_feat)
        low_level_feat = self.relu(low_level_feat)

        return x, low_level_feat

class ASPPModule(nn.Module):
    """ASPP ëª¨ë“ˆ - Multi-scale context aggregation"""

    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()

        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])

        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)  # 1x1 + atrous + global
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )

    def forward(self, x):
        h, w = x.shape[2:]

        # 1x1 convolution
        feat1 = self.conv1x1(x)

        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]

        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)

        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)

        # Project to final features
        return self.project(concat_feat)

class SelfAttentionKeypointMatcher(nn.Module):
    """Self-Attention ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ëª¨ë“ˆ"""

    def __init__(self, in_channels=256, num_keypoints=20):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.in_channels = in_channels

        # Query, Key, Value ë³€í™˜
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)

        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        self.keypoint_head = nn.Sequential(
            nn.Conv2d(in_channels, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_keypoints, 1),
            nn.Sigmoid()
        )

        # Attention ê°€ì¤‘ì¹˜
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, person_feat, clothing_feat):
        """Self-attentionì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­"""
        batch_size, C, H, W = person_feat.size()

        # Person featuresì—ì„œ query ìƒì„±
        proj_query = self.query_conv(person_feat).view(batch_size, -1, H * W).permute(0, 2, 1)
        
        # Clothing featuresì—ì„œ key, value ìƒì„±
        proj_key = self.key_conv(clothing_feat).view(batch_size, -1, H * W)
        proj_value = self.value_conv(clothing_feat).view(batch_size, -1, H * W)

        # Attention ê³„ì‚°
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)

        # Attentionì„ valueì— ì ìš©
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)

        # Residual connection
        attended_feat = self.gamma * out + person_feat

        # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        keypoint_heatmaps = self.keypoint_head(attended_feat)

        return keypoint_heatmaps, attended_feat

class EdgeAwareTransformationModule(nn.Module):
    """Edge-Aware ë³€í˜• ëª¨ë“ˆ - ê²½ê³„ì„  ì •ë³´ í™œìš©"""

    def __init__(self, in_channels=256):
        super().__init__()

        # Edge feature extraction
        self.edge_conv1 = nn.Sequential(
            nn.Conv2d(in_channels, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

        self.edge_conv2 = nn.Sequential(
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # Learnable Sobel-like filters
        self.sobel_x = nn.Conv2d(64, 32, 3, padding=1, bias=False)
        self.sobel_y = nn.Conv2d(64, 32, 3, padding=1, bias=False)

        # Initialize edge kernels
        self._init_sobel_kernels()

        # Transformation prediction
        self.transform_head = nn.Sequential(
            nn.Conv2d(64 + 32 * 2, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 2, 1)  # x, y displacement
        )

    def _init_sobel_kernels(self):
        """Sobel edge detection ì»¤ë„ ì´ˆê¸°í™”"""
        sobel_x_kernel = torch.tensor([
            [-1, 0, 1],
            [-2, 0, 2], 
            [-1, 0, 1]
        ], dtype=torch.float32).view(1, 1, 3, 3)

        sobel_y_kernel = torch.tensor([
            [-1, -2, -1],
            [0, 0, 0],
            [1, 2, 1]
        ], dtype=torch.float32).view(1, 1, 3, 3)

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •
        self.sobel_x.weight.data = sobel_x_kernel.repeat(32, 64, 1, 1)
        self.sobel_y.weight.data = sobel_y_kernel.repeat(32, 64, 1, 1)

    def forward(self, features):
        """Edge-aware transformation ì˜ˆì¸¡"""
        # Edge features ì¶”ì¶œ
        edge_feat = self.edge_conv1(features)
        edge_feat = self.edge_conv2(edge_feat)

        # Sobel í•„í„° ì ìš©
        edge_x = self.sobel_x(edge_feat)
        edge_y = self.sobel_y(edge_feat)

        # Feature ê²°í•©
        combined_feat = torch.cat([edge_feat, edge_x, edge_y], dim=1)

        # Transformation ì˜ˆì¸¡
        transformation = self.transform_head(combined_feat)

        return transformation

class ProgressiveGeometricRefinement(nn.Module):
    """Progressive ê¸°í•˜í•™ì  ì •ì œ ëª¨ë“ˆ - ë‹¨ê³„ë³„ ê°œì„ """

    def __init__(self, num_stages=3, in_channels=256):
        super().__init__()
        self.num_stages = num_stages

        # Stageë³„ ì •ì œ ëª¨ë“ˆ
        self.refine_stages = nn.ModuleList([
            self._make_refine_stage(in_channels + 2 * i, in_channels // (2 ** i))
            for i in range(num_stages)
        ])

        # Stageë³„ ë³€í˜• ì˜ˆì¸¡ê¸°
        self.transform_predictors = nn.ModuleList([
            nn.Conv2d(in_channels // (2 ** i), 2, 1)
            for i in range(num_stages)
        ])

        # ì‹ ë¢°ë„ ì¶”ì •
        self.confidence_estimator = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

    def _make_refine_stage(self, in_channels, out_channels):
        """ì •ì œ ë‹¨ê³„ ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels * 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels * 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels * 2, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, features):
        """Progressive refinement ìˆ˜í–‰"""
        transformations = []
        current_feat = features

        for i, (refine_stage, transform_pred) in enumerate(zip(self.refine_stages, self.transform_predictors)):
            # í˜„ì¬ ë‹¨ê³„ ì •ì œ
            refined_feat = refine_stage(current_feat)
            
            # ë³€í˜• ì˜ˆì¸¡
            transform = transform_pred(refined_feat)
            transformations.append(transform)

            # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ íŠ¹ì§• ì¤€ë¹„
            if i < self.num_stages - 1:
                current_feat = torch.cat([refined_feat, transform], dim=1)

        # ì‹ ë¢°ë„ ì¶”ì •
        confidence = self.confidence_estimator(features)

        return transformations, confidence

class CompleteAdvancedGeometricMatchingAI(nn.Module):
    """ì™„ì „í•œ ê³ ê¸‰ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - DeepLabV3+ + ASPP + Self-Attention"""

    def __init__(self, input_nc=6, num_keypoints=20):
        super().__init__()
        self.input_nc = input_nc
        self.num_keypoints = num_keypoints

        # 1. DeepLabV3+ Backbone
        self.backbone = DeepLabV3PlusBackbone(input_nc=input_nc)

        # 2. ASPP Module
        self.aspp = ASPPModule(in_channels=2048, out_channels=256)

        # 3. Self-Attention Keypoint Matcher
        self.keypoint_matcher = SelfAttentionKeypointMatcher(in_channels=256, num_keypoints=num_keypoints)

        # 4. Edge-Aware Transformation Module
        self.edge_transform = EdgeAwareTransformationModule(in_channels=256)

        # 5. Progressive Refinement
        self.progressive_refine = ProgressiveGeometricRefinement(num_stages=3, in_channels=256)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),  # ASPP + low-level
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )

        # Final transformation predictor
        self.final_transform = nn.Conv2d(256, 2, 1)

    def forward(self, person_image, clothing_image):
        """ì™„ì „í•œ AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­"""
        # ì…ë ¥ ê²°í•© (6ì±„ë„)
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        input_size = combined_input.shape[2:]

        # 1. Feature extraction with DeepLabV3+
        high_level_feat, low_level_feat = self.backbone(combined_input)

        # 2. Multi-scale context with ASPP
        aspp_feat = self.aspp(high_level_feat)

        # 3. Decode features
        aspp_feat = F.interpolate(aspp_feat, size=low_level_feat.shape[2:], 
                                 mode='bilinear', align_corners=False)
        concat_feat = torch.cat([aspp_feat, low_level_feat], dim=1)
        decoded_feat = self.decoder(concat_feat)

        # 4. Self-attention keypoint matching
        keypoint_heatmaps, attended_feat = self.keypoint_matcher(decoded_feat, decoded_feat)

        # 5. Edge-aware transformation
        edge_transform = self.edge_transform(attended_feat)

        # 6. Progressive refinement
        progressive_transforms, confidence = self.progressive_refine(attended_feat)

        # 7. Final transformation
        final_transform = self.final_transform(attended_feat)

        # 8. Generate transformation grid
        transformation_grid = self._generate_transformation_grid(final_transform, input_size)

        # 9. Apply transformation to clothing
        warped_clothing = F.grid_sample(
            clothing_image, transformation_grid, mode='bilinear',
            padding_mode='border', align_corners=False
        )

        return {
            'transformation_matrix': self._grid_to_matrix(transformation_grid),
            'transformation_grid': transformation_grid,
            'warped_clothing': warped_clothing,
            'keypoint_heatmaps': keypoint_heatmaps,
            'confidence_map': confidence,
            'progressive_transforms': progressive_transforms,
            'edge_features': edge_transform,
            'algorithm_type': 'advanced_deeplab_aspp_self_attention'
        }

    def _generate_transformation_grid(self, flow_field, input_size):
        """Flow fieldë¥¼ transformation gridë¡œ ë³€í™˜"""
        batch_size = flow_field.shape[0]
        device = flow_field.device
        H, W = input_size

        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, H, device=device),
            torch.linspace(-1, 1, W, device=device),
            indexing='ij'
        )
        base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)

        # Flow field í¬ê¸° ì¡°ì •
        if flow_field.shape[-2:] != (H, W):
            flow_field = F.interpolate(flow_field, size=(H, W), mode='bilinear', align_corners=False)

        # Flowë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        flow_normalized = flow_field.permute(0, 2, 3, 1)
        flow_normalized[:, :, :, 0] /= W / 2.0
        flow_normalized[:, :, :, 1] /= H / 2.0

        # ìµœì¢… ë³€í˜• ê·¸ë¦¬ë“œ
        transformation_grid = base_grid + flow_normalized * 0.1

        return transformation_grid

    def _grid_to_matrix(self, grid):
        """Gridë¥¼ 2x3 ë³€í˜• í–‰ë ¬ë¡œ ë³€í™˜"""
        batch_size, H, W, _ = grid.shape
        device = grid.device

        # ë‹¨ìˆœí™”ëœ ì–´í•€ ë³€í˜• ì¶”ì •
        matrix = torch.zeros(batch_size, 2, 3, device=device)

        # ê·¸ë¦¬ë“œ ì¤‘ì•™ ì˜ì—­ì—ì„œ ë³€í˜• íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        center_h, center_w = H // 2, W // 2
        center_region = grid[:, center_h-10:center_h+10, center_w-10:center_w+10, :]

        # í‰ê·  ë³€í˜• ê³„ì‚°
        mean_transform = torch.mean(center_region, dim=(1, 2))

        matrix[:, 0, 0] = 1.0 + mean_transform[:, 0] * 0.1
        matrix[:, 1, 1] = 1.0 + mean_transform[:, 1] * 0.1
        matrix[:, 0, 2] = mean_transform[:, 0]
        matrix[:, 1, 2] = mean_transform[:, 1]

        return matrix

# ==============================================
# ğŸ”¥ ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤
# ==============================================

class AdvancedGeometricMatcher:
    """ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ - ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™”"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor, threshold: float = 0.3) -> List[np.ndarray]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            batch_size, num_kpts, H, W = heatmaps.shape
            keypoints_batch = []
            
            for b in range(batch_size):
                keypoints = []
                for k in range(num_kpts):
                    heatmap = heatmaps[b, k].cpu().numpy()
                    
                    # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                    if heatmap.max() > threshold:
                        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                        confidence = heatmap.max()
                        
                        # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                        x_coord = float(x * 256 / W)
                        y_coord = float(y * 192 / H)
                        
                        keypoints.append([x_coord, y_coord, confidence])
                
                if keypoints:
                    keypoints_batch.append(np.array(keypoints))
                else:
                    # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸ ìƒì„±
                    keypoints_batch.append(np.array([[128, 96, 0.5]]))
            
            return keypoints_batch if len(keypoints_batch) > 1 else keypoints_batch[0]
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [np.array([[128, 96, 0.5]])]
    
    def compute_transformation_matrix(self, src_keypoints: np.ndarray, 
                                    dst_keypoints: np.ndarray) -> np.ndarray:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë³€í˜• í–‰ë ¬ ê³„ì‚° (Procrustes ë¶„ì„)"""
        try:
            if len(src_keypoints) < 3 or len(dst_keypoints) < 3:
                return np.eye(3)
            
            # 3ê°œ ì´ìƒì˜ ì ë§Œ ì‚¬ìš©
            n_points = min(len(src_keypoints), len(dst_keypoints), 8)
            src = src_keypoints[:n_points, :2]
            dst = dst_keypoints[:n_points, :2]
            
            if SCIPY_AVAILABLE:
                return self._procrustes_analysis(src, dst)
            else:
                return self._least_squares_transform(src, dst)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³€í˜• í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(3)
    
    def _procrustes_analysis(self, src: np.ndarray, dst: np.ndarray) -> np.ndarray:
        """Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜• ê³„ì‚°"""
        try:
            # ì¤‘ì‹¬ì  ê³„ì‚°
            src_center = np.mean(src, axis=0)
            dst_center = np.mean(dst, axis=0)
            
            # ì¤‘ì‹¬ì  ê¸°ì¤€ ì •ê·œí™”
            src_centered = src - src_center
            dst_centered = dst - dst_center
            
            # ìŠ¤ì¼€ì¼ ê³„ì‚°
            src_scale = np.sqrt(np.sum(src_centered ** 2))
            dst_scale = np.sqrt(np.sum(dst_centered ** 2))
            scale = dst_scale / (src_scale + 1e-8)
            
            # ìµœì í™”ë¥¼ í†µí•œ íšŒì „ê° ê³„ì‚°
            def objective(angle):
                cos_a, sin_a = np.cos(angle), np.sin(angle)
                R = np.array([[cos_a, -sin_a], [sin_a, cos_a]])
                transformed = scale * (src_centered @ R.T)
                error = np.sum((transformed - dst_centered) ** 2)
                return error
            
            result = minimize(objective, 0, method='BFGS')
            optimal_angle = result.x[0] if result.success else 0
            
            # ìµœì¢… ë³€í˜• í–‰ë ¬ êµ¬ì„±
            cos_a, sin_a = np.cos(optimal_angle), np.sin(optimal_angle)
            
            # 2x3 ì–´í•€ ë³€í˜• í–‰ë ¬
            transform_matrix = np.array([
                [scale * cos_a, -scale * sin_a, dst_center[0] - scale * (cos_a * src_center[0] - sin_a * src_center[1])],
                [scale * sin_a, scale * cos_a, dst_center[1] - scale * (sin_a * src_center[0] + cos_a * src_center[1])],
                [0, 0, 1]
            ])
            
            return transform_matrix
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Procrustes ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._least_squares_transform(src, dst)
    
    def _least_squares_transform(self, src: np.ndarray, dst: np.ndarray) -> np.ndarray:
        """ìµœì†Œì œê³±ë²• ê¸°ë°˜ ì–´í•€ ë³€í˜•"""
        try:
            # ë™ì°¨ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            ones = np.ones((src.shape[0], 1))
            src_homogeneous = np.hstack([src, ones])
            
            # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ë³€í˜• í–‰ë ¬ ê³„ì‚°
            transform_2x3, _, _, _ = np.linalg.lstsq(src_homogeneous, dst, rcond=None)
            
            # 3x3 í–‰ë ¬ë¡œ í™•ì¥
            transform_matrix = np.vstack([transform_2x3.T, [0, 0, 1]])
            
            return transform_matrix
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìµœì†Œì œê³±ë²• ë³€í˜• ì‹¤íŒ¨: {e}")
            return np.eye(3)
    
    def apply_ransac_filtering(self, src_keypoints: np.ndarray, dst_keypoints: np.ndarray,
                             threshold: float = 5.0, max_trials: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
        """RANSAC ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°"""
        if len(src_keypoints) < 4:
            return src_keypoints, dst_keypoints
        
        best_inliers_src = src_keypoints
        best_inliers_dst = dst_keypoints
        best_score = 0
        
        for _ in range(max_trials):
            # ëœë¤ ìƒ˜í”Œ ì„ íƒ
            sample_indices = np.random.choice(len(src_keypoints), 3, replace=False)
            sample_src = src_keypoints[sample_indices]
            sample_dst = dst_keypoints[sample_indices]
            
            try:
                # ë³€í˜• í–‰ë ¬ ê³„ì‚°
                transform = self.compute_transformation_matrix(sample_src, sample_dst)
                
                # ëª¨ë“  ì ì— ëŒ€í•´ ì˜¤ì°¨ ê³„ì‚°
                src_homogeneous = np.hstack([src_keypoints[:, :2], np.ones((len(src_keypoints), 1))])
                transformed_points = (transform @ src_homogeneous.T).T[:, :2]
                
                errors = np.linalg.norm(transformed_points - dst_keypoints[:, :2], axis=1)
                inlier_mask = errors < threshold
                
                if np.sum(inlier_mask) > best_score:
                    best_score = np.sum(inlier_mask)
                    best_inliers_src = src_keypoints[inlier_mask]
                    best_inliers_dst = dst_keypoints[inlier_mask]
                    
            except Exception:
                continue
        
        return best_inliers_src, best_inliers_dst


# ============================================================================
# ğŸ”¥ 3. ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ (ì¤‘ìš”ë„: â˜…â˜…â˜…â˜…)
# ============================================================================

class AdvancedGeometricMatcher:
    """ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ - Procrustes + RANSAC"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def compute_transformation_matrix_procrustes(self, src_keypoints: torch.Tensor, 
                                               dst_keypoints: torch.Tensor) -> torch.Tensor:
        """Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜• ê³„ì‚°"""
        try:
            src_np = src_keypoints.cpu().numpy()
            dst_np = dst_keypoints.cpu().numpy()
            
            # Procrustes ë¶„ì„
            def objective(params):
                tx, ty, scale, rotation = params
                
                cos_r, sin_r = np.cos(rotation), np.sin(rotation)
                transform_matrix = np.array([
                    [scale * cos_r, -scale * sin_r, tx],
                    [scale * sin_r, scale * cos_r, ty]
                ])
                
                src_homogeneous = np.column_stack([src_np, np.ones(len(src_np))])
                transformed = src_homogeneous @ transform_matrix.T
                
                error = np.sum((transformed - dst_np) ** 2)
                return error
            
            # ìµœì í™”
            from scipy.optimize import minimize
            initial_params = [0, 0, 1, 0]
            result = minimize(objective, initial_params, method='BFGS')
            
            if result.success:
                tx, ty, scale, rotation = result.x
                cos_r, sin_r = np.cos(rotation), np.sin(rotation)
                
                transform_matrix = np.array([
                    [scale * cos_r, -scale * sin_r, tx],
                    [scale * sin_r, scale * cos_r, ty]
                ])
            else:
                transform_matrix = np.array([[1, 0, 0], [0, 1, 0]])
            
            return torch.from_numpy(transform_matrix).float().to(src_keypoints.device).unsqueeze(0)
            
        except Exception as e:
            self.logger.warning(f"Procrustes ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._compute_with_pytorch(src_keypoints.unsqueeze(0), dst_keypoints.unsqueeze(0))

    def ransac_filtering(self, matches: List[Tuple[int, int, float]], 
                        threshold: float = 5.0, max_trials: int = 1000) -> List[Tuple[int, int, float]]:
        """RANSAC ì´ìƒì¹˜ ì œê±°"""
        if len(matches) < 4:
            return matches
        
        best_inliers = []
        best_score = 0
        
        for _ in range(max_trials):
            sample_indices = np.random.choice(len(matches), 4, replace=False)
            sample_matches = [matches[i] for i in sample_indices]
            
            try:
                transform = self._compute_affine_transform(sample_matches)
                
                inliers = []
                for match in matches:
                    error = self._compute_transform_error(match, transform)
                    if error < threshold:
                        inliers.append(match)
                
                if len(inliers) > best_score:
                    best_score = len(inliers)
                    best_inliers = inliers
                    
            except Exception:
                continue
        
        return best_inliers if best_inliers else matches

    def _compute_affine_transform(self, matches: List[Tuple[int, int, float]]) -> np.ndarray:
        """ì–´í•€ ë³€í˜• ê³„ì‚°"""
        if len(matches) < 3:
            return np.eye(3)
        
        src_pts = np.array([[i, j] for i, j, _ in matches[:4]], dtype=np.float32)
        dst_pts = np.array([[j, i] for i, j, _ in matches[:4]], dtype=np.float32)
        
        try:
            import cv2
            transform = cv2.getAffineTransform(src_pts[:3], dst_pts[:3])
            return np.vstack([transform, [0, 0, 1]])
        except:
            return np.eye(3)

    def _compute_transform_error(self, match: Tuple[int, int, float], 
                               transform: np.ndarray) -> float:
        """ë³€í˜• ì˜¤ì°¨ ê³„ì‚°"""
        i, j, _ = match
        src_pt = np.array([i, j, 1])
        transformed_pt = transform @ src_pt
        error = np.linalg.norm(transformed_pt[:2] - np.array([j, i]))
        return error

# ============================================================================
# ğŸ”¥ 4. Enhanced Model Path Mapping (ì¤‘ìš”ë„: â˜…â˜…â˜…â˜…)
# ============================================================================

class EnhancedModelPathMapper:
    """í–¥ìƒëœ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ (step_model_requirements.py ê¸°ì¤€)"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # step_model_requirements.pyì—ì„œ ìš”êµ¬ì‚¬í•­ ë¡œë“œ
        self.step_request = get_step_model_request()
        
        # ì‹¤ì œ ê²½ë¡œ ìë™ íƒì§€ (step_model_requirements.py ê¸°ì¤€)
        self.ai_models_root = self._auto_detect_ai_models_path()
        self.logger.info(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸ ê²½ë¡œ: {self.ai_models_root}")
        
    def _auto_detect_ai_models_path(self) -> Path:
        """ì‹¤ì œ ai_models ë””ë ‰í† ë¦¬ ìë™ íƒì§€ (step_model_requirements.py ê¸°ì¤€)"""
        # step_model_requirements.pyì—ì„œ ì •ì˜ëœ ê²€ìƒ‰ ê²½ë¡œ ì‚¬ìš©
        if self.step_request:
            search_paths = self.step_request.search_paths + self.step_request.fallback_paths
        else:
            search_paths = [
                "step_04_geometric_matching",
                "step_04_geometric_matching/ultra_models", 
                "step_04_geometric_matching/models",
                "step_03_cloth_segmentation"  # SAM ê³µìœ 
            ]
        
        possible_paths = [
            Path.cwd() / "ai_models",
            Path.cwd().parent / "ai_models",
            Path.cwd() / "backend" / "ai_models",
            Path(__file__).parent / "ai_models",
            Path(__file__).parent.parent / "ai_models",
            Path(__file__).parent.parent.parent / "ai_models"
        ]
        
        for path in possible_paths:
            if path.exists():
                # step_model_requirements.py ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦
                for search_path in search_paths:
                    if (path / search_path).exists():
                        return path
                        
        return Path.cwd() / "ai_models"
    
    def get_geometric_matching_models(self) -> Dict[str, Path]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ìš© ëª¨ë¸ë“¤ ë§¤í•‘ (step_model_requirements.py ê¸°ì¤€)"""
        result = {}
        
        # step_model_requirements.pyì—ì„œ ì •ì˜ëœ íŒŒì¼ë“¤
        if self.step_request:
            # ì£¼ìš” íŒŒì¼
            primary_file = self.step_request.primary_file  # gmm_final.pth
            primary_path = self.find_model_file(primary_file)
            if primary_path:
                result['gmm'] = primary_path
                self.logger.info(f"âœ… ì£¼ìš” ëª¨ë¸ ë°œê²¬: {primary_file} -> {primary_path.name}")
            
            # ëŒ€ì²´ íŒŒì¼ë“¤
            for alt_file, alt_size in self.step_request.alternative_files:
                alt_path = self.find_model_file(alt_file)
                if alt_path:
                    if alt_file == "tps_network.pth":
                        result['tps'] = alt_path
                    elif alt_file == "sam_vit_h_4b8939.pth":
                        result['sam_shared'] = alt_path
                    elif alt_file == "ViT-L-14.pt":
                        result['vit_large'] = alt_path
                    elif alt_file == "efficientnet_b0_ultra.pth":
                        result['efficientnet'] = alt_path
                    elif "raft" in alt_file.lower():
                        result['raft'] = alt_path
                    
                    self.logger.info(f"âœ… ëŒ€ì²´ ëª¨ë¸ ë°œê²¬: {alt_file} -> {alt_path.name}")
        
        return result

# ==============================================
# ğŸ”¥ GeometricMatchingStep ë©”ì¸ í´ë˜ìŠ¤
# ==============================================

class GeometricMatchingStep(BaseStepMixin):
    """
    ğŸ”¥ Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ v27.0 (ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„)
    
    âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš©
    âœ… ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™” ì•Œê³ ë¦¬ì¦˜
    """
    
    def __init__(self, **kwargs):
        """BaseStepMixin í˜¸í™˜ ìƒì„±ì"""
        
        # BaseStepMixin ì´ˆê¸°í™”
        super().__init__(
            step_name=kwargs.get('step_name', 'GeometricMatchingStep'),
            step_id=kwargs.get('step_id', 4),
            **kwargs
        )
        
        # Step 04 íŠ¹í™” ì„¤ì •
        self.step_number = 4
        self.step_description = "AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ ë° ì˜ë¥˜ ë³€í˜•"
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._detect_optimal_device()
        
        # AI ëª¨ë¸ ìƒíƒœ
        self.gmm_model = None           # Geometric Matching Module
        self.tps_network = None         # TPS Network
        self.optical_flow_model = None  # Optical Flow
        self.keypoint_matcher = None    # Keypoint Matching
        self.sam_model = None           # SAM (ê³µìœ )
        
        # ğŸ”¥ ì—¬ê¸°ì— ì¶”ê°€: 3ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ AI ëª¨ë¸ë“¤
        self.advanced_geometric_ai = None       # CompleteAdvancedGeometricMatchingAI
        self.status = ProcessingStatus()        # ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
        
        # ëª¨ë¸ ê²½ë¡œ
        self.model_paths = {}
        
            # ğŸ”¥ ì—¬ê¸°ì— ì¶”ê°€: step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ
        try:
            self.step_request = get_step_model_request()
            if self.step_request:
                self.status.requirements_compatible = True
                self._load_requirements_config()
            else:
                self._load_fallback_config()
        except Exception as e:
            self.logger.debug(f"step_model_requirements ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.step_request = None
            self._load_fallback_config()
        
        # ğŸ”¥ ì—¬ê¸°ì— ì¶”ê°€: Enhanced Model Path Mapping
        ai_models_root = kwargs.get('ai_models_root', 'ai_models')
        try:
            self.model_mapper = EnhancedModelPathMapper(ai_models_root)
        except Exception as e:
            self.logger.debug(f"ModelPathMapper ìƒì„± ì‹¤íŒ¨: {e}")
            self.model_mapper = None
        

        # ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •
                # ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •
        self.matching_config = {
        'input_size': (256, 192),
        'output_size': (256, 192),
        'keypoint_threshold': 0.3,
        'ransac_threshold': 5.0,
        'max_ransac_trials': 1000,
        'transformation_type': 'tps',  # 'tps', 'affine', 'perspective'
        'enable_optical_flow': True,
        'enable_keypoint_matching': True,
        'confidence_threshold': kwargs.get('confidence_threshold', 0.7),
        'method': 'advanced_deeplab_aspp_self_attention',
        'algorithm_type': 'advanced_deeplab_aspp_self_attention',
        'use_real_models': True,
        'ai_enhanced': kwargs.get('ai_enhanced', True)
        }
        
        # ì•Œê³ ë¦¬ì¦˜ ë§¤ì²˜
        self.geometric_matcher = AdvancedGeometricMatcher(self.device)
        
        # ì˜ì¡´ì„± ì¸í„°í˜ì´ìŠ¤
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        
        # ì„±ëŠ¥ í†µê³„
        self._initialize_performance_stats()
        
        # ğŸ”¥ ì—¬ê¸°ì— ì¶”ê°€: 3ë²ˆ íŒŒì¼ì˜ í†µê³„ ì‹œìŠ¤í…œ
        self._init_statistics()
        
        # ìºì‹œ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)
        self.prediction_cache = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        self.logger.info(f"âœ… {self.step_name} v27.0 ì´ˆê¸°í™” ì™„ë£Œ (device: {self.device})")
    
        self.logger.info(f"âœ… {self.step_name} v27.1 ì´ˆê¸°í™” ì™„ë£Œ (device: {self.device})")

    def _load_requirements_config(self):
        """step_model_requirements.py ìš”êµ¬ì‚¬í•­ ì„¤ì • ë¡œë“œ"""
        if self.step_request:
            # ğŸ”¥ ê¸°ì¡´ matching_configëŠ” ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ í‚¤ë“¤ë§Œ ì¶”ê°€
            additional_config = {
                'method': 'advanced_deeplab_aspp_self_attention',
                'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                'use_real_models': True,
                'batch_size': getattr(self.step_request, 'batch_size', 2),
                'memory_fraction': getattr(self.step_request, 'memory_fraction', 0.2),
                'precision': getattr(self.step_request, 'precision', 'fp16'),
                'detailed_data_spec': True
            }
            
            # step_model_requirements.pyì—ì„œ ì˜¤ëŠ” ì„¤ì •ë“¤
            requirements_config = {
                'input_size': getattr(self.step_request, 'input_size', self.matching_config['input_size']),
                'output_format': getattr(self.step_request, 'output_format', 'transformation_matrix'),
                'model_architecture': getattr(self.step_request, 'model_architecture', 'gmm_tps')
            }
            
            # ğŸ”¥ ì•ˆì „í•˜ê²Œ ë³‘í•© (ê¸°ì¡´ í‚¤ëŠ” ìœ ì§€, ìƒˆë¡œìš´ í‚¤ë§Œ ì¶”ê°€)
            self.matching_config.update(requirements_config)
            self.advanced_config.update(additional_config)
            
            # DetailedDataSpec ë¡œë“œ
            if hasattr(self.step_request, 'data_spec'):
                self.data_spec = self.step_request.data_spec
                self.status.detailed_data_spec_loaded = True
                self.logger.info("âœ… DetailedDataSpec ë¡œë“œ ì™„ë£Œ")
            else:
                self.data_spec = None
                self.logger.warning("âš ï¸ DetailedDataSpec ì—†ìŒ")
        else:
            self._load_fallback_config()

    def _load_fallback_config(self):
        """í´ë°± ì„¤ì • ë¡œë“œ"""
        # ğŸ”¥ ê¸°ì¡´ matching_configëŠ” ê±´ë“œë¦¬ì§€ ì•Šê³  advanced_configë§Œ ì„¤ì •
        self.advanced_config = {
            'method': 'advanced_deeplab_aspp_self_attention',
            'algorithm_type': 'advanced_deeplab_aspp_self_attention',
            'use_real_models': True,
            'ai_enhanced': True
        }
        self.data_spec = None
        self.logger.warning("âš ï¸ step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ì„¤ì • ì‚¬ìš©")

    def _init_statistics(self):
        """í†µê³„ ì´ˆê¸°í™” (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        self.statistics = {
            'total_processed': 0,
            'successful_matches': 0,
            'average_quality': 0.0,
            'total_processing_time': 0.0,
            'ai_model_calls': 0,
            'error_count': 0,
            'model_creation_success': False,
            'real_ai_models_used': True,
            'requirements_compatible': self.status.requirements_compatible,
            'algorithm_type': 'advanced_deeplab_aspp_self_attention',
            'features': [
                'GMM (Geometric Matching Module)',
                'TPS (Thin-Plate Spline) Transformation', 
                'Keypoint-based Matching',
                'Optical Flow Calculation',
                'RANSAC Outlier Removal',
                'DeepLabV3+ Backbone',
                'ASPP Multi-scale Context',
                'Self-Attention Keypoint Matching',
                'Edge-Aware Transformation',
                'Progressive Geometric Refinement',
                'Procrustes Analysis'
            ]
        }

    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if MPS_AVAILABLE and IS_M3_MAX:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except:
            return "cpu"
    
    def _initialize_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”"""
        self.performance_stats = {
            'total_processed': 0,
            'successful_matches': 0,
            'avg_processing_time': 0.0,
            'avg_transformation_quality': 0.0,
            'keypoint_match_rate': 0.0,
            'optical_flow_accuracy': 0.0,
            'cache_hit_rate': 0.0,
            'error_count': 0,
            'models_loaded': 0
        }
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤
    # ==============================================
    
    def set_model_loader(self, model_loader: 'ModelLoader'):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (GitHub í‘œì¤€)"""
        try:
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            raise
    
    def set_memory_manager(self, memory_manager: 'MemoryManager'):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (GitHub í‘œì¤€)"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter: 'DataConverter'):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (GitHub í‘œì¤€)"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container: 'DIContainer'):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ì´ˆê¸°í™” ë° AI ëª¨ë¸ ë¡œë”©
    # ==============================================
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” (GitHub í‘œì¤€ í”Œë¡œìš°)"""
        try:
            if getattr(self, 'is_initialized', False):
                return True
            
            self.logger.info(f"ğŸš€ {self.step_name} v27.0 ì´ˆê¸°í™” ì‹œì‘")
            
            # ëª¨ë¸ ê²½ë¡œ íƒì§€
            self._detect_model_paths()
            
            # ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
            success = await self._load_ai_models()
            if not success:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            # M3 Max ìµœì í™” ì ìš©
            if self.device == "mps" or IS_M3_MAX:
                self._apply_m3_max_optimization()
            
            self.is_initialized = True
            self.is_ready = True
            
            self.logger.info(f"âœ… {self.step_name} v27.0 ì´ˆê¸°í™” ì™„ë£Œ (ë¡œë”©ëœ ëª¨ë¸: {self.performance_stats['models_loaded']}ê°œ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} v27.0 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _detect_model_paths(self):
        """ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            ai_models_root = Path("ai_models")
            step_dir = ai_models_root / "step_04_geometric_matching"
            ultra_dir = step_dir / "ultra_models"
            
            # ì£¼ìš” ëª¨ë¸ íŒŒì¼ë“¤
            model_files = {
                'gmm': ['gmm_final.pth'],
                'tps': ['tps_network.pth'],
                'sam': ['sam_vit_h_4b8939.pth'],
                'resnet': ['resnet101_geometric.pth', 'resnet50_geometric_ultra.pth'],
                'raft': ['raft-things.pth'],
                'vit': ['ViT-L-14.pt']
            }
            
            for model_key, filenames in model_files.items():
                for filename in filenames:
                    # ë©”ì¸ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                    main_path = step_dir / filename
                    if main_path.exists():
                        self.model_paths[model_key] = main_path
                        size_mb = main_path.stat().st_size / (1024**2)
                        self.logger.info(f"âœ… {model_key} ëª¨ë¸ ë°œê²¬: {filename} ({size_mb:.1f}MB)")
                        break
                    
                    # ultra_modelsì—ì„œ ì°¾ê¸°
                    ultra_path = ultra_dir / filename
                    if ultra_path.exists():
                        self.model_paths[model_key] = ultra_path
                        size_mb = ultra_path.stat().st_size / (1024**2)
                        self.logger.info(f"âœ… {model_key} ëª¨ë¸ ë°œê²¬: ultra_models/{filename} ({size_mb:.1f}MB)")
                        break
                    
                    # í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ê·€ ê²€ìƒ‰
                    try:
                        for found_path in step_dir.rglob(filename):
                            if found_path.is_file():
                                self.model_paths[model_key] = found_path
                                size_mb = found_path.stat().st_size / (1024**2)
                                self.logger.info(f"âœ… {model_key} ëª¨ë¸ ë°œê²¬: {found_path.relative_to(ai_models_root)} ({size_mb:.1f}MB)")
                                break
                    except Exception:
                        continue
            
            self.logger.info(f"ğŸ“ ì´ {len(self.model_paths)}ê°œ ëª¨ë¸ íŒŒì¼ íƒì§€ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            self.model_paths = {}
    
        
    async def _load_ai_models(self) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (3ë²ˆ íŒŒì¼ ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€)"""
        try:
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘")
            
            loaded_count = 0
            
            # ğŸ”¥ 1. ê¸°ì¡´ 1ë²ˆ íŒŒì¼ ëª¨ë¸ë“¤ ë¡œë”© (ìœ ì§€)
            
            # GMM (Geometric Matching Module) ë¡œë”©
            if 'gmm' in self.model_paths:
                try:
                    self.gmm_model = GeometricMatchingModule(input_nc=6, output_nc=1).to(self.device)
                    checkpoint = self._safe_load_checkpoint(self.model_paths['gmm'])
                    if checkpoint is not None:
                        self._load_model_weights(self.gmm_model, checkpoint, 'gmm')
                    self.gmm_model.eval()
                    loaded_count += 1
                    self.logger.info("âœ… GMM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # TPS Network ë¡œë”©
            if 'tps' in self.model_paths:
                try:
                    self.tps_network = self.gmm_model.grid_generator if self.gmm_model else TPSGridGenerator()
                    loaded_count += 1
                    self.logger.info("âœ… TPS Network ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ TPS Network ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Optical Flow Network ë¡œë”©
            if 'raft' in self.model_paths:
                try:
                    self.optical_flow_model = OpticalFlowNetwork().to(self.device)
                    checkpoint = self._safe_load_checkpoint(self.model_paths['raft'])
                    if checkpoint is not None:
                        self._load_model_weights(self.optical_flow_model, checkpoint, 'optical_flow')
                    self.optical_flow_model.eval()
                    loaded_count += 1
                    self.logger.info("âœ… Optical Flow ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Optical Flow ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Keypoint Matching Network ë¡œë”©
            try:
                self.keypoint_matcher = KeypointMatchingNetwork(num_keypoints=18).to(self.device)
                self.keypoint_matcher.eval()
                loaded_count += 1
                self.logger.info("âœ… Keypoint Matching ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Keypoint Matching ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 2. 3ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ AI ëª¨ë¸ë“¤ ì¶”ê°€ ë¡œë”©
            
            # CompleteAdvancedGeometricMatchingAI ë¡œë”©
            try:
                self.advanced_geometric_ai = CompleteAdvancedGeometricMatchingAI(
                    input_nc=6, num_keypoints=20
                ).to(self.device)
                self.advanced_geometric_ai.eval()
                loaded_count += 1
                self.logger.info("âœ… CompleteAdvancedGeometricMatchingAI ë¡œë”© ì™„ë£Œ")
                
                # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„ (ê°€ëŠ¥í•œ ê²½ìš°)
                if 'gmm' in self.model_paths:
                    self._load_pretrained_weights(self.model_paths['gmm'])
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ CompleteAdvancedGeometricMatchingAI ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # AdvancedGeometricMatcher ì—…ê·¸ë ˆì´ë“œ (3ë²ˆ íŒŒì¼ ë²„ì „ìœ¼ë¡œ)
            try:
                # ê¸°ì¡´ ë§¤ì²˜ë¥¼ 3ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ ë²„ì „ìœ¼ë¡œ êµì²´
                self.geometric_matcher = AdvancedGeometricMatcher(self.device)
                # Procrustes ë¶„ì„ ê¸°ëŠ¥ í™•ì¸
                if hasattr(self.geometric_matcher, 'compute_transformation_matrix_procrustes'):
                    self.logger.info("âœ… AdvancedGeometricMatcher (Procrustes ì§€ì›) ë¡œë”© ì™„ë£Œ")
                else:
                    self.logger.info("âœ… AdvancedGeometricMatcher (ê¸°ë³¸) ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ AdvancedGeometricMatcher ì—…ê·¸ë ˆì´ë“œ ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 3. ìƒíƒœ ì—…ë°ì´íŠ¸
            self.performance_stats['models_loaded'] = loaded_count
            self.status.models_loaded = loaded_count > 0
            self.status.advanced_ai_loaded = self.advanced_geometric_ai is not None
            self.status.model_creation_success = loaded_count > 0
            
            if loaded_count > 0:
                self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ (ê¸°ì¡´ + ê³ ê¸‰)")
                self.logger.info(f"   - ê¸°ì¡´ ëª¨ë¸: GMM, TPS, OpticalFlow, Keypoint")
                self.logger.info(f"   - ê³ ê¸‰ ëª¨ë¸: CompleteAdvancedGeometricMatchingAI")
                return True
            else:
                self.logger.error("âŒ ë¡œë”©ëœ ì‹¤ì œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _load_pretrained_weights(self, checkpoint_path: Path):
        """ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë”© (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        try:
            if not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                return
            
            self.logger.info(f"ğŸ”„ ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_path}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'generator' in checkpoint:
                    state_dict = checkpoint['generator']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # í‚¤ ì´ë¦„ ë§¤í•‘
            new_state_dict = {}
            for k, v in state_dict.items():
                new_key = k
                if k.startswith('module.'):
                    new_key = k[7:]  # 'module.' ì œê±°
                elif k.startswith('netG.'):
                    new_key = k[5:]  # 'netG.' ì œê±°
                elif k.startswith('generator.'):
                    new_key = k[10:]  # 'generator.' ì œê±°
                
                new_state_dict[new_key] = v
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
            model_dict = self.advanced_geometric_ai.state_dict()
            compatible_dict = {}
            
            for k, v in new_state_dict.items():
                if k in model_dict and v.shape == model_dict[k].shape:
                    compatible_dict[k] = v
            
            if len(compatible_dict) > 0:
                model_dict.update(compatible_dict)
                self.advanced_geometric_ai.load_state_dict(model_dict)
                self.logger.info(f"âœ… ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(new_state_dict)}ê°œ ë ˆì´ì–´")
            else:
                self.logger.warning("âš ï¸ í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìœ ì§€")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³ ê¸‰ AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")


    def _safe_load_checkpoint(self, checkpoint_path: Path) -> Optional[Any]:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                return None
            
            # 3ë‹¨ê³„ ì•ˆì „ ë¡œë”© ì‹œë„
            for method_name, load_func in [
                ("weights_only_true", lambda: torch.load(checkpoint_path, map_location='cpu', weights_only=True)),
                ("weights_only_false", lambda: torch.load(checkpoint_path, map_location='cpu', weights_only=False)),
                ("legacy", lambda: torch.load(checkpoint_path, map_location='cpu'))
            ]:
                try:
                    checkpoint = load_func()
                    self.logger.debug(f"âœ… {method_name} ë¡œë”© ì„±ê³µ: {checkpoint_path.name}")
                    return checkpoint
                except Exception as e:
                    self.logger.debug(f"{method_name} ì‹¤íŒ¨: {str(e)[:100]}")
                    continue
            
            self.logger.warning(f"âš ï¸ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {checkpoint_path}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_model_weights(self, model: nn.Module, checkpoint: Any, model_name: str):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”©"""
        try:
            # state_dict ì¶”ì¶œ
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'net' in checkpoint:
                    state_dict = checkpoint['net']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # í‚¤ ì •ê·œí™”
            normalized_state_dict = {}
            for k, v in state_dict.items():
                new_key = k
                # prefix ì œê±°
                for prefix in ['module.', 'model.', 'net.', '_orig_mod.']:
                    if new_key.startswith(prefix):
                        new_key = new_key[len(prefix):]
                        break
                normalized_state_dict[new_key] = v
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
            model_dict = model.state_dict()
            compatible_dict = {}
            
            for k, v in normalized_state_dict.items():
                if k in model_dict and v.shape == model_dict[k].shape:
                    compatible_dict[k] = v
            
            if len(compatible_dict) > 0:
                model_dict.update(compatible_dict)
                model.load_state_dict(model_dict)
                self.logger.info(f"âœ… {model_name} ê°€ì¤‘ì¹˜ ë¡œë”©: {len(compatible_dict)}/{len(normalized_state_dict)}ê°œ ë ˆì´ì–´")
            else:
                self.logger.warning(f"âš ï¸ {model_name} í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ìœ ì§€")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {model_name} ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _apply_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì ìš©"""
        try:
            # MPS ìºì‹œ ì •ë¦¬
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception:
                    pass
            
            # í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['TORCH_MPS_PREFER_METAL'] = '1'
            
            if IS_M3_MAX:
                self.matching_config['batch_size'] = 1
                self.cache_max_size = 150
                
            self.logger.debug("âœ… M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (BaseStepMixin v19.1 í˜¸í™˜)
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ ì¶”ë¡  (3ë²ˆ íŒŒì¼ ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€)"""
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘...")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
            person_image = processed_input.get('person_image')
            clothing_image = processed_input.get('clothing_image')
            person_parsing = processed_input.get('person_parsing', {})
            pose_keypoints = processed_input.get('pose_keypoints', [])
            clothing_segmentation = processed_input.get('clothing_segmentation', {})
            
            if person_image is None or clothing_image is None:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„° ì—†ìŒ: person_image, clothing_image")
            
            # 2. ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ (ê¸°ì¡´ ìœ ì§€)
            person_tensor = self._prepare_image_tensor(person_image)
            clothing_tensor = self._prepare_image_tensor(clothing_image)
            
            # 3. ìºì‹œ í™•ì¸ (ê¸°ì¡´ ìœ ì§€)
            cache_key = self._generate_cache_key(person_tensor, clothing_tensor)
            if cache_key in self.prediction_cache:
                cached_result = self.prediction_cache[cache_key]
                cached_result['cache_hit'] = True
                self.logger.info("ğŸ¯ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                return cached_result
            
            results = {}
            
            # ğŸ”¥ 4. ê¸°ì¡´ 1ë²ˆ íŒŒì¼ AI ëª¨ë¸ë“¤ ì‹¤í–‰ (ìœ ì§€)
            
            # GMM ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ (í•µì‹¬)
            if self.gmm_model is not None:
                try:
                    gmm_result = self.gmm_model(person_tensor, clothing_tensor)
                    results['gmm'] = gmm_result
                    self.logger.info("âœ… GMM ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GMM ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            
            # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­
            if self.keypoint_matcher is not None and len(pose_keypoints) > 0:
                try:
                    keypoint_result = self._perform_keypoint_matching(
                        person_tensor, clothing_tensor, pose_keypoints
                    )
                    results['keypoint'] = keypoint_result
                    self.logger.info("âœ… í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            
            # Optical Flow ê¸°ë°˜ ì›€ì§ì„ ì¶”ì 
            if self.optical_flow_model is not None:
                try:
                    flow_result = self.optical_flow_model(person_tensor, clothing_tensor)
                    results['optical_flow'] = flow_result
                    self.logger.info("âœ… Optical Flow ê³„ì‚° ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Optical Flow ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 5. 3ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ AI ëª¨ë¸ ì‹¤í–‰ (ì¶”ê°€)
            
            # CompleteAdvancedGeometricMatchingAI ì‹¤í–‰
            if self.advanced_geometric_ai is not None:
                try:
                    advanced_result = self.advanced_geometric_ai(person_tensor, clothing_tensor)
                    results['advanced_ai'] = advanced_result
                    self.logger.info("âœ… CompleteAdvancedGeometricMatchingAI ì‹¤í–‰ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ CompleteAdvancedGeometricMatchingAI ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # Procrustes ë¶„ì„ ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
            if (self.geometric_matcher is not None and 
                hasattr(self.geometric_matcher, 'compute_transformation_matrix_procrustes')):
                try:
                    # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µì—ì„œ ì‹¤ì œ ì¢Œí‘œ ì¶”ì¶œ
                    if 'advanced_ai' in results and 'keypoint_heatmaps' in results['advanced_ai']:
                        person_keypoints = self.geometric_matcher.extract_keypoints_from_heatmaps(
                            results['advanced_ai']['keypoint_heatmaps']
                        )
                        clothing_keypoints = person_keypoints  # ë™ì¼í•œ êµ¬ì¡° ê°€ì •
                        
                        # Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜•
                        transformation_matrix = self.geometric_matcher.compute_transformation_matrix_procrustes(
                            clothing_keypoints, person_keypoints
                        )
                        
                        results['procrustes_transform'] = transformation_matrix
                        results['keypoints'] = person_keypoints.cpu().numpy().tolist()
                        self.logger.info("âœ… Procrustes ë¶„ì„ ê¸°ë°˜ ë§¤ì¹­ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Procrustes ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 6. ê³ ê¸‰ ê²°ê³¼ ìœµí•© (3ë²ˆ íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ)
            final_result = self._fuse_matching_results_advanced(results, person_tensor, clothing_tensor)
            
            # 7. ë³€í˜• í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ + ê³ ê¸‰)
            processing_time = time.time() - start_time
            confidence = self._compute_enhanced_confidence(results)  # 3ë²ˆ íŒŒì¼ ë°©ì‹
            quality_score = self._compute_quality_score_advanced(results)  # 3ë²ˆ íŒŒì¼ ë°©ì‹
            
            final_result.update({
                'success': True,
                'processing_time': processing_time,
                'confidence': confidence,
                'quality_score': quality_score,
                'ai_models_used': list(results.keys()),
                'algorithms_used': self._get_used_algorithms(results),  # 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
                'device': self.device,
                'real_ai_inference': True,
                'cache_hit': False,
                'ai_enhanced': True,
                'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                'version': 'v27.1'
            })
            
            # 8. ìºì‹œì— ì €ì¥ (ê¸°ì¡´ ìœ ì§€)
            self._save_to_cache(cache_key, final_result)
            
            # 9. í†µê³„ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ + 3ë²ˆ íŒŒì¼ ë°©ì‹)
            self._update_performance_stats(processing_time, True, confidence, quality_score)
            self._update_statistics_advanced(processing_time, True, confidence, quality_score)
            
            self.logger.info(f"ğŸ‰ ê³ ê¸‰ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            self.performance_stats['error_count'] += 1
            self.statistics['error_count'] += 1
            
            # í´ë°±: ê¸°ë³¸ ë³€í˜• ê²°ê³¼
            return self._create_fallback_result(processed_input, str(e))

    # ğŸ”¥ ê³ ê¸‰ ê²°ê³¼ ìœµí•© ë©”ì„œë“œ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
    def _fuse_matching_results_advanced(self, results: Dict[str, Any], 
                                    person_tensor: torch.Tensor, 
                                    clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """ê³ ê¸‰ AI ê²°ê³¼ ìœµí•© (3ë²ˆ íŒŒì¼ ë°©ì‹)"""
        
        # 1. ë³€í˜• ê·¸ë¦¬ë“œ/í–‰ë ¬ ìš°ì„ ìˆœìœ„ ê²°ì •
        transformation_matrix = None
        transformation_grid = None
        warped_clothing = None
        
        # ê³ ê¸‰ AI ê²°ê³¼ ìš°ì„  ì‚¬ìš© (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
        if 'advanced_ai' in results:
            adv_result = results['advanced_ai']
            if 'transformation_matrix' in adv_result:
                transformation_matrix = adv_result['transformation_matrix']
            if 'transformation_grid' in adv_result:
                transformation_grid = adv_result['transformation_grid']
            if 'warped_clothing' in adv_result:
                warped_clothing = adv_result['warped_clothing']
        
        # GMM ê²°ê³¼ ë³´ì¡° í™œìš© (ê¸°ì¡´ 1ë²ˆ íŒŒì¼)
        if transformation_matrix is None and 'gmm' in results:
            gmm_result = results['gmm']
            transformation_matrix = gmm_result.get('transformation_matrix')
            transformation_grid = gmm_result.get('transformation_grid')
            warped_clothing = gmm_result.get('warped_clothing')
        
        # Procrustes ê²°ê³¼ ë³´ì¡° í™œìš© (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
        if 'procrustes_transform' in results and transformation_matrix is None:
            transformation_matrix = results['procrustes_transform']
        
        # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ _fuse_matching_results ë¡œì§ ìœ ì§€...
        
        # ğŸ”¥ ì¶”ê°€ ê²°ê³¼ ì •ë¦¬ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
        keypoint_heatmaps = None
        confidence_map = None
        edge_features = None
        
        if 'advanced_ai' in results:
            adv_result = results['advanced_ai']
            keypoint_heatmaps = adv_result.get('keypoint_heatmaps')
            confidence_map = adv_result.get('confidence_map')
            edge_features = adv_result.get('edge_features')
        
        return {
            'transformation_matrix': transformation_matrix,
            'transformation_grid': transformation_grid,
            'warped_clothing': warped_clothing,
            'flow_field': self._generate_flow_field_from_grid(transformation_grid),
            'keypoint_heatmaps': keypoint_heatmaps,      # 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
            'confidence_map': confidence_map,            # 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
            'edge_features': edge_features,              # 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
            'keypoints': results.get('keypoints', []),
            'matching_score': self._compute_matching_score(results),
            'fusion_weights': self._get_fusion_weights(results),
            'detailed_results': results
        }

    # ğŸ”¥ 3ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ í‰ê°€ í•¨ìˆ˜ë“¤ ì¶”ê°€
    def _compute_enhanced_confidence(self, results: Dict[str, Any]) -> float:
        """ê°•í™”ëœ ì‹ ë¢°ë„ ê³„ì‚° (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        confidences = []
        
        # ê³ ê¸‰ AI ì‹ ë¢°ë„
        if 'advanced_ai' in results and 'confidence_map' in results['advanced_ai']:
            ai_conf = torch.mean(results['advanced_ai']['confidence_map']).item()
            confidences.append(ai_conf)
        
        # ê¸°ì¡´ GMM ì‹ ë¢°ë„
        if 'gmm' in results:
            gmm_conf = 0.8
            confidences.append(gmm_conf)
        
        # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹ ë¢°ë„
        if 'keypoint' in results:
            kpt_conf = results['keypoint']['keypoint_confidence']
            match_ratio = min(results['keypoint']['match_count'] / 18.0, 1.0)
            keypoint_confidence = kpt_conf * match_ratio
            confidences.append(keypoint_confidence)
        
        # Procrustes ë§¤ì¹­ ì‹ ë¢°ë„ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
        if 'procrustes_transform' in results:
            transform = results['procrustes_transform']
            try:
                det = torch.det(transform[:, :2, :2])
                stability = torch.clamp(1.0 / (torch.abs(det) + 1e-8), 0, 1)
                confidences.append(stability.mean().item())
            except:
                confidences.append(0.7)
        
        return float(np.mean(confidences)) if confidences else 0.8

    def _compute_quality_score_advanced(self, results: Dict[str, Any]) -> float:
        """ê³ ê¸‰ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        quality_factors = []
        
        # ê³ ê¸‰ AI ì‚¬ìš© ì ìˆ˜
        if 'advanced_ai' in results:
            quality_factors.append(0.9)
        
        # ê¸°ì¡´ GMM ì‚¬ìš© ì ìˆ˜
        if 'gmm' in results:
            quality_factors.append(0.85)
        
        # Procrustes ë¶„ì„ ì ìˆ˜
        if 'procrustes_transform' in results:
            quality_factors.append(0.8)
        
        # í‚¤í¬ì¸íŠ¸ í’ˆì§ˆ
        if 'keypoints' in results:
            kpt_count = len(results['keypoints'])
            kpt_quality = min(1.0, kpt_count / 20.0)
            quality_factors.append(kpt_quality)
        
        # Edge features í’ˆì§ˆ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)
        if 'advanced_ai' in results and 'edge_features' in results['advanced_ai']:
            edge_feat = results['advanced_ai']['edge_features']
            if isinstance(edge_feat, torch.Tensor):
                edge_quality = torch.mean(torch.abs(edge_feat)).item()
                quality_factors.append(min(1.0, edge_quality))
        
        return float(np.mean(quality_factors)) if quality_factors else 0.75

    def _get_used_algorithms(self, results: Dict[str, Any]) -> List[str]:
        """ì‚¬ìš©ëœ ì•Œê³ ë¦¬ì¦˜ ëª©ë¡ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        algorithms = []
        
        if 'advanced_ai' in results:
            algorithms.extend([
                "DeepLabV3+ Backbone",
                "ASPP Multi-scale Context", 
                "Self-Attention Keypoint Matching",
                "Edge-Aware Transformation",
                "Progressive Geometric Refinement"
            ])
        
        if 'gmm' in results:
            algorithms.append("GMM (Geometric Matching Module)")
        
        if 'procrustes_transform' in results:
            algorithms.append("Procrustes Analysis")
        
        if 'keypoint' in results:
            algorithms.append("Keypoint-based Matching")
        
        if 'optical_flow' in results:
            algorithms.append("Optical Flow Calculation")
        
        return algorithms

    def _update_statistics_advanced(self, processing_time: float, success: bool, 
                                confidence: float, quality_score: float):
        """ê³ ê¸‰ í†µê³„ ì—…ë°ì´íŠ¸ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
        try:
            self.statistics['total_processed'] += 1
            self.statistics['ai_model_calls'] += 1
            self.statistics['total_processing_time'] += processing_time
            
            if success:
                self.statistics['successful_matches'] += 1
                
                # í‰ê·  í’ˆì§ˆ ì—…ë°ì´íŠ¸
                total_success = self.statistics['successful_matches']
                current_avg_quality = self.statistics['average_quality']
                self.statistics['average_quality'] = (
                    (current_avg_quality * (total_success - 1) + quality_score) / total_success
                )
                
            self.statistics['model_creation_success'] = self.status.model_creation_success
            
        except Exception as e:
            self.logger.debug(f"ê³ ê¸‰ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")




    def _prepare_image_tensor(self, image: Any) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        try:
            # PIL Image ì²˜ë¦¬
            if isinstance(image, Image.Image):
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                image_array = np.array(image).astype(np.float32) / 255.0
                if len(image_array.shape) == 3:
                    image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
                tensor = torch.from_numpy(image_array).unsqueeze(0).to(self.device)
            
            # NumPy ë°°ì—´ ì²˜ë¦¬
            elif isinstance(image, np.ndarray):
                image_array = image.astype(np.float32)
                if image_array.max() > 1.0:
                    image_array = image_array / 255.0
                if len(image_array.shape) == 3:
                    image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
                tensor = torch.from_numpy(image_array).unsqueeze(0).to(self.device)
            
            # ì´ë¯¸ í…ì„œì¸ ê²½ìš°
            elif torch.is_tensor(image):
                tensor = image.to(self.device)
                if tensor.dim() == 3:
                    tensor = tensor.unsqueeze(0)
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # í¬ê¸° ì¡°ì •
            target_size = self.matching_config['input_size']
            if tensor.shape[-2:] != target_size:
                tensor = F.interpolate(tensor, size=target_size, mode='bilinear', align_corners=False)
            
            return tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ í…ì„œ ë°˜í™˜
            return torch.zeros((1, 3, 256, 192), device=self.device)
    
    def _perform_keypoint_matching(self, person_tensor: torch.Tensor, 
                                 clothing_tensor: torch.Tensor, 
                                 pose_keypoints: List) -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë§¤ì¹­ ìˆ˜í–‰"""
        try:
            # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
            person_keypoints = self.keypoint_matcher(person_tensor)
            clothing_keypoints = self.keypoint_matcher(clothing_tensor)
            
            # íˆíŠ¸ë§µì—ì„œ ì‹¤ì œ ì¢Œí‘œ ì¶”ì¶œ
            person_coords = self.geometric_matcher.extract_keypoints_from_heatmaps(
                person_keypoints['keypoint_heatmaps']
            )
            clothing_coords = self.geometric_matcher.extract_keypoints_from_heatmaps(
                clothing_keypoints['keypoint_heatmaps']
            )
            
            # RANSAC ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°
            if len(person_coords) > 3 and len(clothing_coords) > 3:
                filtered_person, filtered_clothing = self.geometric_matcher.apply_ransac_filtering(
                    person_coords, clothing_coords, 
                    threshold=self.matching_config['ransac_threshold'],
                    max_trials=self.matching_config['max_ransac_trials']
                )
                
                # ë³€í˜• í–‰ë ¬ ê³„ì‚°
                transformation_matrix = self.geometric_matcher.compute_transformation_matrix(
                    filtered_clothing, filtered_person
                )
            else:
                transformation_matrix = np.eye(3)
            
            return {
                'person_keypoints': person_coords,
                'clothing_keypoints': clothing_coords,
                'transformation_matrix': transformation_matrix,
                'keypoint_confidence': person_keypoints['keypoint_heatmaps'].max().item(),
                'match_count': min(len(person_coords), len(clothing_coords))
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {
                'person_keypoints': [],
                'clothing_keypoints': [],
                'transformation_matrix': np.eye(3),
                'keypoint_confidence': 0.0,
                'match_count': 0
            }
    
    def _fuse_matching_results(self, results: Dict[str, Any], 
                             person_tensor: torch.Tensor, 
                             clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """ë§¤ì¹­ ê²°ê³¼ ìœµí•©"""
        try:
            # ìµœì¢… ë³€í˜• ê·¸ë¦¬ë“œ ë° í–‰ë ¬ ê²°ì •
            transformation_matrix = None
            transformation_grid = None
            warped_clothing = None
            
            # 1. GMM ê²°ê³¼ ìš°ì„  ì‚¬ìš©
            if 'gmm' in results:
                gmm_result = results['gmm']
                transformation_matrix = gmm_result.get('transformation_matrix')
                transformation_grid = gmm_result.get('transformation_grid')
                warped_clothing = gmm_result.get('warped_clothing')
            
            # 2. í‚¤í¬ì¸íŠ¸ ê²°ê³¼ë¡œ ë³´ì •
            if 'keypoint' in results and transformation_matrix is not None:
                keypoint_matrix = results['keypoint']['transformation_matrix']
                if keypoint_matrix is not None:
                    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°í•©
                    gmm_weight = 0.7
                    keypoint_weight = 0.3
                    
                    # numpy to torch ë³€í™˜
                    if isinstance(keypoint_matrix, np.ndarray):
                        keypoint_matrix_torch = torch.from_numpy(keypoint_matrix[:2, :]).float().to(self.device).unsqueeze(0)
                    else:
                        keypoint_matrix_torch = keypoint_matrix
                    
                    # ê°€ì¤‘ í‰ê· 
                    if transformation_matrix.shape == keypoint_matrix_torch.shape:
                        transformation_matrix = (gmm_weight * transformation_matrix + 
                                               keypoint_weight * keypoint_matrix_torch)
            
            # 3. Optical Flowë¡œ ë¯¸ì„¸ ì¡°ì •
            if 'optical_flow' in results and transformation_grid is not None:
                flow = results['optical_flow']
                # Flowë¥¼ ê·¸ë¦¬ë“œì— ì¶”ê°€ (ë¯¸ì„¸ ì¡°ì •)
                if flow.shape[-2:] == transformation_grid.shape[1:3]:
                    flow_normalized = flow.permute(0, 2, 3, 1) * 0.1  # ë¯¸ì„¸ ì¡°ì •
                    transformation_grid = transformation_grid + flow_normalized
            
            # 4. í´ë°±: Identity ë³€í˜•
            if transformation_matrix is None:
                transformation_matrix = torch.eye(2, 3, device=self.device).unsqueeze(0)
            
            if transformation_grid is None:
                transformation_grid = self._create_identity_grid(1, 256, 192)
            
            # 5. ì˜ë¥˜ ì´ë¯¸ì§€ ë³€í˜• (ì—†ëŠ” ê²½ìš°)
            if warped_clothing is None:
                try:
                    warped_clothing = F.grid_sample(
                        clothing_tensor, transformation_grid, mode='bilinear',
                        padding_mode='border', align_corners=False
                    )
                except Exception:
                    warped_clothing = clothing_tensor.clone()
            
            # 6. Flow field ìƒì„±
            flow_field = self._generate_flow_field_from_grid(transformation_grid)
            
            # 7. ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            matching_score = self._compute_matching_score(results)
            
            return {
                'transformation_matrix': transformation_matrix,
                'transformation_grid': transformation_grid,
                'warped_clothing': warped_clothing,
                'flow_field': flow_field,
                'matching_score': matching_score,
                'fusion_weights': self._get_fusion_weights(results),
                'detailed_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ ìœµí•© ì‹¤íŒ¨: {e}")
            return self._create_identity_transform_result(clothing_tensor)
    
    def _compute_matching_confidence(self, results: Dict[str, Any]) -> float:
        """ë§¤ì¹­ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidences = []
            
            # GMM ì‹ ë¢°ë„
            if 'gmm' in results:
                gmm_conf = 0.8  # GMMì€ ê¸°ë³¸ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
                confidences.append(gmm_conf)
            
            # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹ ë¢°ë„
            if 'keypoint' in results:
                kpt_conf = results['keypoint']['keypoint_confidence']
                match_ratio = min(results['keypoint']['match_count'] / 18.0, 1.0)
                keypoint_confidence = kpt_conf * match_ratio
                confidences.append(keypoint_confidence)
            
            # Optical Flow ì‹ ë¢°ë„
            if 'optical_flow' in results:
                flow_tensor = results['optical_flow']
                if torch.is_tensor(flow_tensor):
                    flow_magnitude = torch.mean(torch.norm(flow_tensor, dim=1))
                    flow_confidence = min(1.0, 1.0 / (1.0 + flow_magnitude.item()))
                    confidences.append(flow_confidence)
            
            return float(np.mean(confidences)) if confidences else 0.7
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _compute_transformation_quality(self, result: Dict[str, Any]) -> float:
        """ë³€í˜• í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            quality_factors = []
            
            # ë³€í˜• í–‰ë ¬ ì•ˆì •ì„±
            if 'transformation_matrix' in result:
                matrix = result['transformation_matrix']
                if torch.is_tensor(matrix):
                    det = torch.det(matrix[:, :2, :2])
                    stability = torch.clamp(1.0 / (torch.abs(det) + 1e-8), 0, 1)
                    quality_factors.append(stability.mean().item())
            
            # ë§¤ì¹­ ì ìˆ˜
            if 'matching_score' in result:
                quality_factors.append(result['matching_score'])
            
            # ì›Œí•‘ í’ˆì§ˆ (ì´ë¯¸ì§€ ë³€í˜• í›„ í’ˆì§ˆ)
            if 'warped_clothing' in result:
                warped = result['warped_clothing']
                if torch.is_tensor(warped):
                    # ë³€í˜•ëœ ì´ë¯¸ì§€ì˜ ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„
                    grad_x = torch.abs(warped[:, :, :, 1:] - warped[:, :, :, :-1])
                    grad_y = torch.abs(warped[:, :, 1:, :] - warped[:, :, :-1, :])
                    gradient_quality = 1.0 - torch.mean(grad_x + grad_y).item()
                    quality_factors.append(max(0.0, gradient_quality))
            
            return float(np.mean(quality_factors)) if quality_factors else 0.75
            
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.75
    
    def _compute_matching_score(self, results: Dict[str, Any]) -> float:
        """ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = []
            
            # GMM ì ìˆ˜
            if 'gmm' in results:
                scores.append(0.85)  # GMM ê¸°ë³¸ ì ìˆ˜
            
            # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì ìˆ˜
            if 'keypoint' in results:
                match_count = results['keypoint']['match_count']
                confidence = results['keypoint']['keypoint_confidence']
                keypoint_score = (match_count / 18.0) * confidence
                scores.append(keypoint_score)
            
            # Optical Flow ì ìˆ˜
            if 'optical_flow' in results:
                scores.append(0.75)  # Flow ê¸°ë³¸ ì ìˆ˜
            
            return float(np.mean(scores)) if scores else 0.8
            
        except Exception as e:
            return 0.8
    
    def _get_fusion_weights(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ìœµí•© ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weights = {}
        
        if 'gmm' in results:
            weights['gmm'] = 0.7
        
        if 'keypoint' in results:
            weights['keypoint'] = 0.2
        
        if 'optical_flow' in results:
            weights['optical_flow'] = 0.1
        
        return weights
    
    def _create_identity_grid(self, batch_size: int, H: int, W: int) -> torch.Tensor:
        """Identity ê·¸ë¦¬ë“œ ìƒì„±"""
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, H, device=self.device),
            torch.linspace(-1, 1, W, device=self.device),
            indexing='ij'
        )
        grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        return grid
    
    def _generate_flow_field_from_grid(self, transformation_grid: torch.Tensor) -> torch.Tensor:
        """ë³€í˜• ê·¸ë¦¬ë“œì—ì„œ flow field ìƒì„±"""
        try:
            batch_size, H, W, _ = transformation_grid.shape
            
            # ê¸°ë³¸ ê·¸ë¦¬ë“œ
            y, x = torch.meshgrid(
                torch.linspace(-1, 1, H, device=transformation_grid.device),
                torch.linspace(-1, 1, W, device=transformation_grid.device),
                indexing='ij'
            )
            base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            # Flow field ê³„ì‚°
            flow = (transformation_grid - base_grid) * torch.tensor([W/2, H/2], device=transformation_grid.device)
            
            return flow.permute(0, 3, 1, 2)  # (B, 2, H, W)
            
        except Exception as e:
            self.logger.error(f"âŒ Flow field ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 2, 256, 192), device=self.device)
    
    def _create_identity_transform_result(self, clothing_tensor: torch.Tensor) -> Dict[str, Any]:
        """Identity ë³€í˜• ê²°ê³¼ ìƒì„±"""
        batch_size = clothing_tensor.shape[0]
        height, width = clothing_tensor.shape[-2:]
        
        return {
            'transformation_matrix': torch.eye(2, 3, device=self.device).unsqueeze(0).repeat(batch_size, 1, 1),
            'transformation_grid': self._create_identity_grid(batch_size, height, width),
            'warped_clothing': clothing_tensor.clone(),
            'flow_field': torch.zeros((batch_size, 2, height, width), device=self.device),
            'matching_score': 0.5,
            'fusion_weights': {'identity': 1.0},
            'detailed_results': {}
        }
    
    def _create_fallback_result(self, processed_input: Dict[str, Any], error_msg: str) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        try:
            processing_time = 0.1
            
            return {
                'success': True,  # í•­ìƒ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                'transformation_matrix': torch.eye(2, 3).unsqueeze(0),
                'transformation_grid': self._create_identity_grid(1, 256, 192),
                'warped_clothing': torch.zeros(1, 3, 256, 192),
                'flow_field': torch.zeros(1, 2, 256, 192),
                'confidence': 0.5,
                'quality_score': 0.5,
                'processing_time': processing_time,
                'ai_models_used': [],
                'device': self.device,
                'real_ai_inference': False,
                'fallback_used': True,
                'error_handled': error_msg[:100],
                'matching_score': 0.5
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'transformation_matrix': None,
                'confidence': 0.0
            }
    
    def _generate_cache_key(self, person_tensor: torch.Tensor, clothing_tensor: torch.Tensor) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            person_hash = hashlib.md5(person_tensor.cpu().numpy().tobytes()).hexdigest()[:16]
            clothing_hash = hashlib.md5(clothing_tensor.cpu().numpy().tobytes()).hexdigest()[:16]
            config_hash = hashlib.md5(str(self.matching_config).encode()).hexdigest()[:8]
            
            return f"geometric_matching_v27_{person_hash}_{clothing_hash}_{config_hash}"
            
        except Exception:
            return f"geometric_matching_v27_{int(time.time())}"
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                oldest_key = next(iter(self.prediction_cache))
                del self.prediction_cache[oldest_key]
            
            # í…ì„œëŠ” ìºì‹œì—ì„œ ì œì™¸ (ë©”ëª¨ë¦¬ ì ˆì•½)
            cached_result = result.copy()
            for key in ['warped_clothing', 'transformation_grid', 'flow_field']:
                if key in cached_result:
                    cached_result[key] = None
            
            cached_result['timestamp'] = time.time()
            self.prediction_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_performance_stats(self, processing_time: float, success: bool, 
                                confidence: float, quality_score: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_processed'] += 1
            
            if success:
                self.performance_stats['successful_matches'] += 1
                
                # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                current_avg = self.performance_stats['avg_processing_time']
                total_success = self.performance_stats['successful_matches']
                self.performance_stats['avg_processing_time'] = (
                    (current_avg * (total_success - 1) + processing_time) / total_success
                )
                
                # í‰ê·  ë³€í˜• í’ˆì§ˆ ì—…ë°ì´íŠ¸
                current_quality = self.performance_stats['avg_transformation_quality']
                self.performance_stats['avg_transformation_quality'] = (
                    (current_quality * (total_success - 1) + quality_score) / total_success
                )
            
            # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
            total_processed = self.performance_stats['total_processed']
            cache_hits = sum(1 for result in self.prediction_cache.values() 
                           if result.get('cache_hit', False))
            self.performance_stats['cache_hit_rate'] = cache_hits / total_processed if total_processed > 0 else 0.0
            
        except Exception as e:
            self.logger.debug(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë° ì •ë³´ ì¡°íšŒ ë©”ì„œë“œë“¤
    # ==============================================
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜ (ì•ˆì „í•œ ì„¤ì • ë³‘í•©)"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'version': 'v27.1',
            'initialized': getattr(self, 'is_initialized', False),
            'device': self.device,
            'ai_models_loaded': {
                'gmm_model': self.gmm_model is not None,
                'tps_network': self.tps_network is not None,
                'optical_flow_model': self.optical_flow_model is not None,
                'keypoint_matcher': self.keypoint_matcher is not None,
                'advanced_geometric_ai': getattr(self, 'advanced_geometric_ai', None) is not None
            },
            'model_files_detected': len(self.model_paths),
            # ğŸ”¥ ì•ˆì „í•œ ì„¤ì • ë³‘í•©
            'matching_config': self.matching_config,  # ê¸°ì¡´ ì„¤ì •
            'advanced_config': getattr(self, 'advanced_config', {}),  # ê³ ê¸‰ ì„¤ì •
            'full_config': self.get_full_config(),  # ë³‘í•©ëœ ì „ì²´ ì„¤ì •
            'performance_stats': self.performance_stats,
            'statistics': getattr(self, 'statistics', {}),
            'algorithms': getattr(self, 'statistics', {}).get('features', [
                'GMM (Geometric Matching Module)',
                'TPS (Thin-Plate Spline) Transformation',
                'Keypoint-based Matching',
                'Optical Flow Calculation',
                'RANSAC Outlier Removal',
                'Procrustes Analysis'
            ]),
            'ai_enhanced': self.is_ai_enhanced(),
            'algorithm_type': self.get_algorithm_type()
        }

    
    
    def validate_dependencies(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ê²€ì¦"""
        try:
            return {
                'model_loader': self.model_loader is not None,
                'memory_manager': self.memory_manager is not None,
                'data_converter': self.data_converter is not None,
                'di_container': self.di_container is not None,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'pil_available': PIL_AVAILABLE,
                'numpy_available': NUMPY_AVAILABLE,
                'cv2_available': CV2_AVAILABLE,
                'scipy_available': SCIPY_AVAILABLE
            }
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {}
    
    def health_check(self) -> Dict[str, Any]:
        """ê±´ê°• ìƒíƒœ ì²´í¬"""
        try:
            health_status = {
                'overall_status': 'healthy',
                'timestamp': time.time(),
                'checks': {}
            }
            
            issues = []
            
            # ì´ˆê¸°í™” ìƒíƒœ ì²´í¬
            if not getattr(self, 'is_initialized', False):
                issues.append('Stepì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ')
                health_status['checks']['initialization'] = 'failed'
            else:
                health_status['checks']['initialization'] = 'passed'
            
            # AI ëª¨ë¸ ë¡œë”© ìƒíƒœ ì²´í¬
            models_loaded = sum([
                self.gmm_model is not None,
                self.tps_network is not None,
                self.optical_flow_model is not None,
                self.keypoint_matcher is not None
            ])
            
            if models_loaded == 0:
                issues.append('AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ')
                health_status['checks']['ai_models'] = 'failed'
            elif models_loaded < 3:
                health_status['checks']['ai_models'] = 'warning'
            else:
                health_status['checks']['ai_models'] = 'passed'
            
            # ì˜ì¡´ì„± ì²´í¬
            deps = self.validate_dependencies()
            essential_deps = ['torch_available', 'pil_available', 'numpy_available']
            missing_deps = [dep for dep in essential_deps if not deps.get(dep, False)]
            
            if missing_deps:
                issues.append(f'í•„ìˆ˜ ì˜ì¡´ì„± ì—†ìŒ: {missing_deps}')
                health_status['checks']['dependencies'] = 'failed'
            else:
                health_status['checks']['dependencies'] = 'passed'
            
            # ë””ë°”ì´ìŠ¤ ìƒíƒœ ì²´í¬
            if self.device == "mps" and not MPS_AVAILABLE:
                issues.append('MPS ë””ë°”ì´ìŠ¤ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ')
                health_status['checks']['device'] = 'warning'
            elif self.device == "cuda" and not torch.cuda.is_available():
                issues.append('CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ')
                health_status['checks']['device'] = 'warning'
            else:
                health_status['checks']['device'] = 'passed'
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            if any(status == 'failed' for status in health_status['checks'].values()):
                health_status['overall_status'] = 'unhealthy'
            elif any(status == 'warning' for status in health_status['checks'].values()):
                health_status['overall_status'] = 'degraded'
            
            if issues:
                health_status['issues'] = issues
            
            return health_status
            
        except Exception as e:
            return {
                'overall_status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }
    
    # ==============================================
    # ğŸ”¥ ì •ë¦¬ ì‘ì—…
    # ==============================================
    
    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            models_to_cleanup = [
                'gmm_model', 'tps_network', 'optical_flow_model', 
                'keypoint_matcher', 'sam_model'
            ]
            
            for model_name in models_to_cleanup:
                model = getattr(self, model_name, None)
                if model is not None:
                    del model
                    setattr(self, model_name, None)
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'prediction_cache'):
                self.prediction_cache.clear()
            
            # ê²½ë¡œ ì •ë¦¬
            if hasattr(self, 'model_paths'):
                self.model_paths.clear()
            
            # ë§¤ì²˜ ì •ë¦¬
            if hasattr(self, 'geometric_matcher'):
                del self.geometric_matcher
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except:
                    pass
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… GeometricMatchingStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")



# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    return GeometricMatchingStep(**kwargs)

async def create_geometric_matching_step_async(**kwargs) -> GeometricMatchingStep:
    """ë¹„ë™ê¸° ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    step = GeometricMatchingStep(**kwargs)
    await step.initialize()
    return step

def create_m3_max_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """M3 Max ìµœì í™” ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    kwargs.setdefault('device', 'mps')
    return GeometricMatchingStep(**kwargs)

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ í•¨ìˆ˜ë“¤
# ==============================================
def debug_info(self) -> Dict[str, Any]:
    """ë””ë²„ê¹… ì •ë³´ ë°˜í™˜ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
    try:
        return {
            'step_info': {
                'name': self.step_name,
                'id': self.step_id,
                'device': self.device,
                'initialized': getattr(self, 'is_initialized', False),
                'models_loaded': self.status.models_loaded if hasattr(self, 'status') else False,
                'algorithm_type': 'advanced_deeplab_aspp_self_attention',
                'version': 'v27.1'
            },
            'ai_models': {
                'gmm_model_loaded': self.gmm_model is not None,
                'advanced_geometric_ai_loaded': getattr(self, 'advanced_geometric_ai', None) is not None,
                'geometric_matcher_loaded': self.geometric_matcher is not None,
                'model_files_detected': len(self.model_paths) if hasattr(self, 'model_paths') else 0
            },
            'config': self.matching_config if hasattr(self, 'matching_config') else {},
            'statistics': getattr(self, 'statistics', {}),
            'performance_stats': getattr(self, 'performance_stats', {}),
            'requirements': {
                'compatible': getattr(self.status, 'requirements_compatible', False) if hasattr(self, 'status') else False,
                'detailed_spec_loaded': getattr(self.status, 'detailed_data_spec_loaded', False) if hasattr(self, 'status') else False,
                'ai_enhanced': True
            },
            'features': getattr(self, 'statistics', {}).get('features', [])
        }
    except Exception as e:
        self.logger.error(f"âŒ ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

def get_performance_stats(self) -> Dict[str, Any]:
    """ì„±ëŠ¥ í†µê³„ ë°˜í™˜ (3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€)"""
    try:
        if hasattr(self, 'statistics'):
            stats = self.statistics.copy()
            
            # ì¶”ê°€ ê³„ì‚°ëœ í†µê³„
            if stats['total_processed'] > 0:
                stats['average_processing_time'] = stats['total_processing_time'] / stats['total_processed']
                stats['success_rate'] = stats['successful_matches'] / stats['total_processed']
            else:
                stats['average_processing_time'] = 0.0
                stats['success_rate'] = 0.0
            
            stats['algorithm_type'] = 'advanced_deeplab_aspp_self_attention'
            stats['version'] = 'v27.1'
            return stats
        else:
            return {'message': 'í†µê³„ ë°ì´í„° ì—†ìŒ'}
    except Exception as e:
        self.logger.error(f"âŒ ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# ============================================================================
# ğŸ”¥ 5. get_step_info ë©”ì„œë“œ ì—…ë°ì´íŠ¸
# ============================================================================

def get_step_info(self) -> Dict[str, Any]:
    """Step ì •ë³´ ë°˜í™˜ (ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€)"""
    return {
        'step_name': self.step_name,
        'step_id': self.step_id,
        'version': 'v27.1',  # ë²„ì „ ì—…ë°ì´íŠ¸
        'initialized': getattr(self, 'is_initialized', False),
        'device': self.device,
        'ai_models_loaded': {
            'gmm_model': self.gmm_model is not None,
            'tps_network': self.tps_network is not None,
            'optical_flow_model': self.optical_flow_model is not None,
            'keypoint_matcher': self.keypoint_matcher is not None,
            # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
            'advanced_geometric_ai': getattr(self, 'advanced_geometric_ai', None) is not None
        },
        'model_files_detected': len(self.model_paths),
        'matching_config': self.matching_config,
        'performance_stats': self.performance_stats,
        # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€
        'statistics': getattr(self, 'statistics', {}),
        'algorithms': getattr(self, 'statistics', {}).get('features', [
            'GMM (Geometric Matching Module)',
            'TPS (Thin-Plate Spline) Transformation',
            'Keypoint-based Matching',
            'Optical Flow Calculation',
            'RANSAC Outlier Removal',
            'Procrustes Analysis'
        ]),
        'ai_enhanced': True,
        'algorithm_type': 'advanced_deeplab_aspp_self_attention'
    }

def validate_geometric_matching_dependencies() -> Dict[str, bool]:
    """ì˜ì¡´ì„± ê²€ì¦"""
    return {
        "torch": TORCH_AVAILABLE,
        "pil": PIL_AVAILABLE,
        "numpy": NUMPY_AVAILABLE,
        "cv2": CV2_AVAILABLE,
        "scipy": SCIPY_AVAILABLE,
        "mps": MPS_AVAILABLE,
        "base_step_mixin": BaseStepMixin is not None,
        "is_m3_max": IS_M3_MAX,
        "conda_env": CONDA_INFO['is_mycloset_env']
    }

async def test_geometric_matching_step() -> bool:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ Step í…ŒìŠ¤íŠ¸"""
    try:
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” GeometricMatchingStep v27.0 í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì˜ì¡´ì„± í™•ì¸
        deps = validate_geometric_matching_dependencies()
        missing_deps = [k for k, v in deps.items() if not v and k not in ['cv2', 'scipy']]
        if missing_deps:
            logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì˜ì¡´ì„±: {missing_deps}")
        
        # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step = create_geometric_matching_step(device="cpu")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            await step.initialize()
            logger.info("âœ… ì´ˆê¸°í™” ì„±ê³µ")
            
            # Step ì •ë³´ í™•ì¸
            step_info = step.get_step_info()
            logger.info(f"ğŸ“‹ Step ì •ë³´:")
            logger.info(f"  - ë²„ì „: {step_info['version']}")
            logger.info(f"  - ë””ë°”ì´ìŠ¤: {step_info['device']}")
            logger.info(f"  - AI ëª¨ë¸ë“¤: {sum(step_info['ai_models_loaded'].values())}ê°œ ë¡œë”©ë¨")
            logger.info(f"  - ëª¨ë¸ íŒŒì¼: {step_info['model_files_detected']}ê°œ ê°ì§€ë¨")
            
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ë°ì´í„°)
        try:
            dummy_person = torch.randn(1, 3, 256, 192)
            dummy_clothing = torch.randn(1, 3, 256, 192)
            
            # BaseStepMixin _run_ai_inference í˜¸ì¶œ
            processed_input = {
                'person_image': dummy_person,
                'clothing_image': dummy_clothing,
                'pose_keypoints': [],
                'person_parsing': {},
                'clothing_segmentation': {}
            }
            
            result = step._run_ai_inference(processed_input)
            
            if result and result.get('success', False):
                logger.info(f"âœ… AI ì¶”ë¡  ì„±ê³µ")
                logger.info(f"  - ì‹ ë¢°ë„: {result.get('confidence', 0):.3f}")
                logger.info(f"  - í’ˆì§ˆ: {result.get('quality_score', 0):.3f}")
                logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
                logger.info(f"  - ì‚¬ìš©ëœ AI ëª¨ë¸: {len(result.get('ai_models_used', []))}ê°œ")
                logger.info(f"  - ë³€í˜• í–‰ë ¬: {'âœ…' if result.get('transformation_matrix') is not None else 'âŒ'}")
                logger.info(f"  - ì›Œí•‘ ì˜ë¥˜: {'âœ…' if result.get('warped_clothing') is not None else 'âŒ'}")
                logger.info(f"  - Flow Field: {'âœ…' if result.get('flow_field') is not None else 'âŒ'}")
            else:
                logger.warning(f"âš ï¸ AI ì¶”ë¡  ê²°ê³¼ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨")
        
        except Exception as e:
            logger.warning(f"âš ï¸ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # ê±´ê°• ìƒíƒœ í™•ì¸
        health = step.health_check()
        logger.info(f"ğŸ¥ ê±´ê°• ìƒíƒœ: {health['overall_status']}")
        
        # ì •ë¦¬
        await step.cleanup()
        
        logger.info("âœ… GeometricMatchingStep v27.0 í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì •ë³´ ë° ìµìŠ¤í¬íŠ¸
# ==============================================

__version__ = "27.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ê¸°í•˜í•™ì  ë§¤ì¹­ - ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„"
__compatibility_version__ = "27.0.0-complete-ai-integration"

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'GeometricMatchingStep',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'GeometricMatchingModule',
    'TPSGridGenerator',
    'OpticalFlowNetwork',
    'KeypointMatchingNetwork',
    
    # ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤
    'AdvancedGeometricMatcher',
    
    # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€í•  ê³ ê¸‰ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'CompleteAdvancedGeometricMatchingAI',
    'DeepLabV3PlusBackbone',
    'ASPPModule',
    'SelfAttentionKeypointMatcher',
    'EdgeAwareTransformationModule',
    'ProgressiveGeometricRefinement',
    # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€í•  ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
    'EnhancedModelPathMapper',
    'ProcessingStatus',
   
    # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€í•  í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_advanced_ai_geometric_matching',
    
    # ğŸ”¥ 3ë²ˆ íŒŒì¼ì—ì„œ ì¶”ê°€í•  ë™ì  import í•¨ìˆ˜ë“¤
    'get_model_loader',
    'get_step_model_request',
    'get_memory_manager',
    'get_data_converter',
    'get_di_container'
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_geometric_matching_step',
    'create_geometric_matching_step_async',
    'create_m3_max_geometric_matching_step',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'validate_geometric_matching_dependencies',
    'test_geometric_matching_step'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 100)
logger.info("ğŸ”¥ GeometricMatchingStep v27.0 ë¡œë“œ ì™„ë£Œ (ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„)")
logger.info("=" * 100)
logger.info("ğŸ¯ ì£¼ìš” ì„±ê³¼:")
logger.info("   âœ… HumanParsingStep ìˆ˜ì¤€ì˜ ì™„ì „í•œ AI ì—°ë™ êµ¬í˜„")
logger.info("   âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (3.0GB)")
logger.info("   âœ… GMM + TPS ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ë³€í˜• ê³„ì‚°")
logger.info("   âœ… í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì •ë°€ ë§¤ì¹­")
logger.info("   âœ… Optical Flow ê¸°ë°˜ ì›€ì§ì„ ì¶”ì ")
logger.info("   âœ… RANSAC + Procrustes ë¶„ì„")
logger.info("   âœ… M3 Max + conda í™˜ê²½ ìµœì í™”")
logger.info("ğŸ§  êµ¬í˜„ëœ AI ëª¨ë¸ë“¤:")
logger.info("   ğŸ¯ GeometricMatchingModule - GMM ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­")
logger.info("   ğŸŒŠ TPSGridGenerator - Thin-Plate Spline ë³€í˜•")
logger.info("   ğŸ“Š OpticalFlowNetwork - RAFT ê¸°ë°˜ Flow ê³„ì‚°")
logger.info("   ğŸ¯ KeypointMatchingNetwork - í‚¤í¬ì¸íŠ¸ ë§¤ì¹­")
logger.info("   ğŸ“ AdvancedGeometricMatcher - ê³ ê¸‰ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜")
logger.info("ğŸ”§ ì‹¤ì œ ëª¨ë¸ íŒŒì¼:")
logger.info("   ğŸ“ gmm_final.pth (44.7MB)")
logger.info("   ğŸ“ tps_network.pth (527.8MB)")
logger.info("   ğŸ“ sam_vit_h_4b8939.pth (2445.7MB)")
logger.info("   ğŸ“ resnet101_geometric.pth (170.5MB)")
logger.info("   ğŸ“ raft-things.pth (20.1MB)")
logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - PIL: {PIL_AVAILABLE}")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - conda mycloset: {CONDA_INFO['is_mycloset_env']}")
logger.info("=" * 100)
logger.info("ğŸ‰ MyCloset AI - Step 04 Geometric Matching v27.0 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   HumanParsingStep ìˆ˜ì¤€ì˜ ì™„ì „í•œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° êµ¬í˜„!")
logger.info("=" * 100)

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸)
# ==============================================

if __name__ == "__main__":
    print("=" * 100)
    print("ğŸ¯ MyCloset AI Step 04 - v27.0 ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„")
    print("=" * 100)
    print("âœ… HumanParsingStep ìˆ˜ì¤€ì˜ ì™„ì „í•œ AI ì—°ë™:")
    print("   âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ")
    print("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (3.0GB)")
    print("   âœ… GMM + TPS ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ë³€í˜• ê³„ì‚°")
    print("   âœ… í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì •ë°€ ë§¤ì¹­")
    print("   âœ… Optical Flow ê¸°ë°˜ ì›€ì§ì„ ì¶”ì ")
    print("   âœ… RANSAC + Procrustes ë¶„ì„")
    print("   âœ… M3 Max + conda í™˜ê²½ ìµœì í™”")
    print("   âœ… TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
    print("=" * 100)
    print("ğŸ”¥ ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„:")
    print("   1. GMM ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ - ì˜ë¥˜ì™€ ì‚¬ëŒ ê°„ ì •ë°€ ë§¤ì¹­")
    print("   2. TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬ - ì˜ë¥˜ í˜•íƒœ ë³€í˜• ë° ì›Œí•‘")
    print("   3. í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ - 18ê°œ ê´€ì ˆì  ê¸°ë°˜ ì •ë°€ ì •ë ¬")
    print("   4. Optical Flow - ì˜ë¥˜ ì›€ì§ì„ ì¶”ì  ë° ë¯¸ì„¸ ì¡°ì •")
    print("   5. RANSAC ì´ìƒì¹˜ ì œê±° - ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ")
    print("   6. ê²°ê³¼ ìœµí•© - ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ ìµœì  ì¡°í•©")
    print("   7. ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€ - ë³€í˜• í’ˆì§ˆ ë° ì‹ ë¢°ë„ ê³„ì‚°")
    print("=" * 100)
    print("ğŸ“ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš©:")
    print("   âœ… gmm_final.pth (44.7MB) - Geometric Matching Module")
    print("   âœ… tps_network.pth (527.8MB) - Thin-Plate Spline Network")
    print("   âœ… sam_vit_h_4b8939.pth (2445.7MB) - Segment Anything Model")
    print("   âœ… resnet101_geometric.pth (170.5MB) - ResNet-101 íŠ¹ì§• ì¶”ì¶œ")
    print("   âœ… raft-things.pth (20.1MB) - Optical Flow ê³„ì‚°")
    print("=" * 100)
    print("ğŸ¯ í•µì‹¬ ì²˜ë¦¬ íë¦„:")
    print("   1. StepFactory.create_step(StepType.GEOMETRIC_MATCHING)")
    print("      â†’ GeometricMatchingStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    print("   2. ModelLoader ì˜ì¡´ì„± ì£¼ì… â†’ set_model_loader()")
    print("      â†’ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ ì—°ê²°")
    print("   3. ì´ˆê¸°í™” ì‹¤í–‰ â†’ initialize()")
    print("      â†’ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ë¡œë”© ë° ì¤€ë¹„")
    print("   4. AI ì¶”ë¡  ì‹¤í–‰ â†’ _run_ai_inference()")
    print("      â†’ GMM + TPS ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰")
    print("   5. ì˜ë¥˜ ë³€í˜• â†’ TPS ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì›Œí•‘")
    print("      â†’ ì‹¤ì œ ì˜· ê°ˆì•„ì…íˆê¸° ë³€í˜• ê³„ì‚°")
    print("   6. í’ˆì§ˆ í‰ê°€ â†’ ë³€í˜• í’ˆì§ˆ ë° ì‹ ë¢°ë„ ê³„ì‚°")
    print("      â†’ ë‹¤ìŒ Stepìœ¼ë¡œ ìµœì í™”ëœ ë°ì´í„° ì „ë‹¬")
    print("=" * 100)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        asyncio.run(test_geometric_matching_step())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 100)
    print("ğŸ‰ GeometricMatchingStep v27.0 ì‹¤ì œ AI ì—°ë™ ë° ì˜· ê°ˆì•„ì…íˆê¸° ì™„ì „ êµ¬í˜„ ì™„ë£Œ!")
    print("âœ… HumanParsingStep ìˆ˜ì¤€ì˜ ì™„ì „í•œ AI ì—°ë™")
    print("âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜")
    print("âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ 3.0GB 100% í™œìš©")
    print("âœ… GMM + TPS ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì •ë°€ ë³€í˜•")
    print("âœ… í‚¤í¬ì¸íŠ¸ + Optical Flow ë©€í‹° ë§¤ì¹­")
    print("âœ… ì‹¤ì œ ì˜· ê°ˆì•„ì…íˆê¸° ê°€ëŠ¥í•œ ì™„ì „í•œ êµ¬í˜„")
    print("âœ… M3 Max + conda í™˜ê²½ ì™„ì „ ìµœì í™”")
    print("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥")
    print("=" * 100)