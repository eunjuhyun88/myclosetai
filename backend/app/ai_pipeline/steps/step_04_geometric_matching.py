# backend/app/ai_pipeline/steps/step_04_geometric_matching.py
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ (ì™„ì „ ê°œì„  ë²„ì „)
âœ… ìˆœí™˜ ì°¸ì¡° ì™„ì „ í•´ê²° - í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡°
âœ… ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±)
âœ… ModelLoader ì™„ë²½ ì—°ë™ - ì§ì ‘ ëª¨ë¸ í˜¸ì¶œ ì œê±°
âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©
âœ… PyTorch 2.1 ì™„ì „ í˜¸í™˜
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  AI ëª¨ë¸ í´ë˜ìŠ¤ í¬í•¨

ğŸ¯ ModelLoader í˜‘ì—… êµ¬ì¡°:
- ModelLoader: AI ëª¨ë¸ ê´€ë¦¬ ë° ì œê³µ
- Step íŒŒì¼: ì‹¤ì œ AI ì¶”ë¡  ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
"""

import os
import gc
import cv2
import time
import torch
import logging
import asyncio
import traceback
import numpy as np
import base64
import json
import math
import weakref
import threading
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import torch.nn as nn
import torch.nn.functional as F
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
from dataclasses import dataclass, field
from enum import Enum

# ==============================================
# ğŸ”¥ í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡° - ìˆœí™˜ ì°¸ì¡° í•´ê²°
# ==============================================

# 1. BaseStepMixin ë° GeometricMatchingMixin ì„í¬íŠ¸
try:
    from .base_step_mixin import BaseStepMixin, GeometricMatchingMixin
    MIXIN_AVAILABLE = True
except ImportError as e:
    logging.warning(f"BaseStepMixin ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MIXIN_AVAILABLE = False

# 2. ModelLoader ì„í¬íŠ¸
try:
    from ..utils.model_loader import ModelLoader, get_global_model_loader
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ModelLoader ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MODEL_LOADER_AVAILABLE = False

# 3. ì„¤ì • ë° ì½”ì–´ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from ...core.config import MODEL_CONFIG
    from ...core.gpu_config import GPUConfig
    from ...core.m3_optimizer import M3MaxOptimizer
    CORE_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Core ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    CORE_AVAILABLE = False

# 4. ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# 5. Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì„í¬íŠ¸ (ì˜¬ë°”ë¥¸ íŒŒì¼ëª…)
try:
    from ..utils.step_model_requests import get_step_request, StepModelRequestAnalyzer
    STEP_REQUESTS_AVAILABLE = True
except ImportError:
    STEP_REQUESTS_AVAILABLE = False

# 6. ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
try:
    from ..utils.image_utils import preprocess_image, postprocess_segmentation
    IMAGE_UTILS_AVAILABLE = True
except ImportError:
    IMAGE_UTILS_AVAILABLE = False

# ==============================================
# ğŸ”¥ MRO ì•ˆì „í•œ í´ë°± í´ë˜ìŠ¤ ì •ì˜ (import ì‹¤íŒ¨ ì‹œ)
# ==============================================

if not MIXIN_AVAILABLE:
    class BaseStepMixin:
        """MRO ì•ˆì „í•œ í´ë°± BaseStepMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.step_name = "geometric_matching"
            self.device = "mps" if torch.backends.mps.is_available() else "cpu"
            self.is_initialized = False
    
    class GeometricMatchingMixin(BaseStepMixin):
        """MRO ì•ˆì „í•œ í´ë°± GeometricMatchingMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.step_number = 4
            self.step_type = "geometric_matching"

# ==============================================
# ğŸ”¥ PyTorch 2.1 í˜¸í™˜ì„± ë©”ëª¨ë¦¬ ê´€ë¦¬
# ==============================================

def safe_mps_memory_cleanup(device: str = "mps") -> Dict[str, Any]:
    """PyTorch 2.1 í˜¸í™˜ ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬"""
    result = {
        "success": False,
        "method": "none",
        "device": device,
        "pytorch_version": torch.__version__
    }
    
    try:
        gc.collect()
        
        if device == "mps" and torch.backends.mps.is_available():
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    result.update({
                        "success": True,
                        "method": "torch.mps.empty_cache"
                    })
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result.update({
                        "success": True,
                        "method": "torch.mps.synchronize"
                    })
                else:
                    result.update({
                        "success": True,
                        "method": "manual_gc_cleanup"
                    })
            except Exception as e:
                result.update({
                    "success": True,
                    "method": "gc_fallback",
                    "warning": str(e)
                })
        
        elif device == "cuda" and torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                result.update({
                    "success": True,
                    "method": "torch.cuda.empty_cache"
                })
            except Exception as e:
                result.update({
                    "success": True,
                    "method": "gc_fallback",
                    "warning": str(e)
                })
        
        else:
            result.update({
                "success": True,
                "method": "gc_only"
            })
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "method": "error",
            "device": device,
            "error": str(e)
        }

# ==============================================
# ğŸ§  AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ModelLoaderê°€ ê´€ë¦¬í•  ëª¨ë¸ë“¤)
# ==============================================

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__(self, feature_dim: int = 256, num_keypoints: int = 25):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_keypoints = num_keypoints
        
        # íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸
        self.backbone = self._build_backbone()
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ
        self.keypoint_head = self._build_keypoint_head()
        
        # íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ
        self.matching_head = self._build_matching_head()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ
        self.tps_head = self._build_tps_head()
        
    def _build_backbone(self):
        """íŠ¹ì§• ì¶”ì¶œ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # Stage 2
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            
            # Stage 3
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            # Feature refinement
            nn.Conv2d(512, self.feature_dim, 3, 1, 1),
            nn.BatchNorm2d(self.feature_dim),
            nn.ReLU(inplace=True)
        )
    
    def _make_layer(self, in_planes: int, planes: int, blocks: int, stride: int = 1):
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
        layers = []
        layers.append(nn.Conv2d(in_planes, planes, 3, stride, 1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, 3, 1, 1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_keypoint_head(self):
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, self.num_keypoints, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_matching_head(self):
        """íŠ¹ì§• ë§¤ì¹­ í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(self.feature_dim * 2, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 1, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def _build_tps_head(self):
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ í—¤ë“œ"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, self.num_keypoints * 2)  # x, y coordinates
        )
    
    def forward(self, person_image: torch.Tensor, clothing_image: torch.Tensor = None):
        """ìˆœì „íŒŒ"""
        # Person ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        person_features = self.backbone(person_image)
        person_keypoints_heatmap = self.keypoint_head(person_features)
        person_keypoints = self.tps_head(person_features)
        person_keypoints = person_keypoints.view(-1, self.num_keypoints, 2)
        
        if clothing_image is not None:
            # Clothing ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
            clothing_features = self.backbone(clothing_image)
            clothing_keypoints_heatmap = self.keypoint_head(clothing_features)
            clothing_keypoints = self.tps_head(clothing_features)
            clothing_keypoints = clothing_keypoints.view(-1, self.num_keypoints, 2)
            
            # íŠ¹ì§• ë§¤ì¹­
            combined_features = torch.cat([person_features, clothing_features], dim=1)
            matching_map = self.matching_head(combined_features)
            
            return {
                'person_keypoints': person_keypoints,
                'clothing_keypoints': clothing_keypoints,
                'person_heatmap': person_keypoints_heatmap,
                'clothing_heatmap': clothing_keypoints_heatmap,
                'matching_map': matching_map
            }
        else:
            # Person ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
            return {
                'keypoints': person_keypoints,
                'heatmap': person_keypoints_heatmap
            }

class TPSTransformNetwork(nn.Module):
    """TPS(Thin Plate Spline) ë³€í˜• ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, control_points: int = 25, grid_size: int = 20):
        super().__init__()
        self.control_points = control_points
        self.grid_size = grid_size
        
        # TPS íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬
        self.tps_predictor = nn.Sequential(
            nn.Linear(control_points * 4, 512),  # source + target points
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, control_points * 2)  # TPS coefficients
        )
        
        # ê·¸ë¦¬ë“œ ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
        self.register_buffer('base_grid', self._create_base_grid())
    
    def _create_base_grid(self):
        """ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±"""
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, self.grid_size),
            torch.linspace(-1, 1, self.grid_size),
            indexing='ij'
        )
        return torch.stack([x, y], dim=-1)
    
    def forward(self, source_points: torch.Tensor, target_points: torch.Tensor, grid_size: int = None):
        """TPS ë³€í˜• ì ìš©"""
        if grid_size is None:
            grid_size = self.grid_size
        
        batch_size = source_points.size(0)
        device = source_points.device
        
        # ì…ë ¥ íŠ¹ì§• ìƒì„± (source + target points)
        input_features = torch.cat([
            source_points.view(batch_size, -1),
            target_points.view(batch_size, -1)
        ], dim=1)
        
        # TPS ê³„ìˆ˜ ì˜ˆì¸¡
        tps_coefficients = self.tps_predictor(input_features)
        tps_coefficients = tps_coefficients.view(batch_size, self.control_points, 2)
        
        # ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
        transformation_grid = self._generate_transformation_grid(
            source_points, target_points, tps_coefficients, grid_size, device
        )
        
        return transformation_grid
    
    def _generate_transformation_grid(
        self,
        source_points: torch.Tensor,
        target_points: torch.Tensor,
        tps_coefficients: torch.Tensor,
        grid_size: int,
        device: torch.device
    ) -> torch.Tensor:
        """ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±"""
        batch_size = source_points.size(0)
        height, width = grid_size, grid_size
        
        # ì •ê·œ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=device),
            torch.linspace(-1, 1, width, device=device),
            indexing='ij'
        )
        grid_flat = torch.stack([x, y], dim=-1).view(-1, 2)
        grid_flat = grid_flat.unsqueeze(0).repeat(batch_size, 1, 1)
        
        # ê±°ë¦¬ ê³„ì‚° (RBF ê¸°ë°˜)
        distances = torch.cdist(grid_flat, source_points)  # [B, H*W, N]
        
        # RBF ê°’ ê³„ì‚° (r^2 * log(r))
        rbf_values = distances ** 2 * torch.log(distances + 1e-6)
        rbf_values = torch.where(distances < 1e-6, torch.zeros_like(rbf_values), rbf_values)
        
        # ë³€ìœ„ ê³„ì‚°
        displacement = target_points - source_points  # [B, N, 2]
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³€í˜• ê³„ì‚°
        weights = torch.softmax(-distances / 0.1, dim=-1)  # [B, H*W, N]
        interpolated_displacement = torch.sum(
            weights.unsqueeze(-1) * displacement.unsqueeze(1), dim=2
        )  # [B, H*W, 2]
        
        # ë³€í˜•ëœ ê·¸ë¦¬ë“œ
        transformed_grid_flat = grid_flat + interpolated_displacement
        transformed_grid = transformed_grid_flat.view(batch_size, height, width, 2)
        
        return transformed_grid

class FeatureExtractor(nn.Module):
    """íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_channels: int = 3, feature_dim: int = 256):
        super().__init__()
        self.feature_dim = feature_dim
        
        # ì¸ì½”ë”
        self.encoder = nn.Sequential(
            # Block 1
            nn.Conv2d(input_channels, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Feature refinement
            nn.Conv2d(256, feature_dim, 3, 1, 1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """íŠ¹ì§• ì¶”ì¶œ"""
        return self.encoder(x)

# ==============================================
# ğŸ¯ ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤
# ==============================================

class GeometricMatchingStep(GeometricMatchingMixin):
    """
    ğŸ”¥ Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ - ModelLoader ì™„ë²½ ì—°ë™ ë²„ì „
    âœ… MRO(Method Resolution Order) ì™„ì „ ì•ˆì „
    âœ… ìˆœí™˜ ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
    âœ… logger ì†ì„± ìë™ ë³´ì¥
    âœ… ModelLoader ì™„ë²½ ì—°ë™ - ì§ì ‘ AI ëª¨ë¸ í˜¸ì¶œ ì œê±°
    âœ… M3 Max 128GB ìµœì í™”
    âœ… ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©
    âœ… PyTorch 2.1 ì™„ì „ í˜¸í™˜
    âœ… ëª¨ë“  AI ëª¨ë¸ í´ë˜ìŠ¤ í¬í•¨
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        device_type: Optional[str] = None,
        memory_gb: Optional[float] = None,
        is_m3_max: Optional[bool] = None,
        optimization_enabled: Optional[bool] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ):
        """MRO ì•ˆì „í•œ ì™„ì „ í˜¸í™˜ ìƒì„±ì"""
        
        # ğŸ”¥ MRO ì•ˆì „: kwargs í•„í„°ë§
        safe_kwargs = {k: v for k, v in kwargs.items() 
                      if k not in ['step_number', 'step_type', 'num_control_points', 'output_format']}
        
        # ğŸ”¥ GeometricMatchingMixin ì´ˆê¸°í™” (MRO ì•ˆì „)
        try:
            super().__init__(**safe_kwargs)
        except TypeError:
            super().__init__()
        
        # ğŸ”¥ logger ì†ì„± ì¶”ê°€ ë³´ì¥
        if not hasattr(self, 'logger') or self.logger is None:
            self.logger = logging.getLogger(f"pipeline.geometric_matching")
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì • (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
        self.step_name = "geometric_matching"
        self.step_number = 4
        self.device = device or ("mps" if torch.backends.mps.is_available() else "cpu")
        self.device_type = device_type or self.device
        self.memory_gb = memory_gb or 128.0
        self.is_m3_max = is_m3_max or (self.device == "mps")
        self.optimization_enabled = optimization_enabled or True
        self.quality_level = quality_level or "ultra"
        
        # AI ëª¨ë¸ ê´€ë ¨ ì†ì„± ì´ˆê¸°í™”
        self.is_initialized = False
        self.models_loaded = False
        self.initialization_error = None
        
        # ModelLoader ì¸í„°í˜ì´ìŠ¤
        self.model_loader = None
        self.model_interface = None
        
        # ğŸ”¥ AI ëª¨ë¸ë“¤ (ModelLoaderë¥¼ í†µí•´ ë¡œë“œ)
        self.geometric_model = None
        self.tps_network = None
        self.feature_extractor = None
        
        # ìŠ¤ë ˆë“œ í’€ ì‹¤í–‰ì
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ì„¤ì • ì´ˆê¸°í™”
        self._setup_configurations(config)
        
        # M3 Max ìµœì í™”
        if self.is_m3_max:
            self._apply_m3_max_optimization()
        
        # í†µê³„ ì´ˆê¸°í™”
        self._setup_stats()
        
        self.logger.info(f"âœ… GeometricMatchingStep ì´ˆê¸°í™” ì™„ë£Œ - Device: {self.device}")
    
    def _setup_configurations(self, config: Optional[Dict[str, Any]] = None):
        """ì„¤ì • ì´ˆê¸°í™”"""
        base_config = config or {}
        
        # ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •
        self.matching_config = base_config.get('matching', {
            'method': 'neural_tps',
            'num_keypoints': 25,
            'quality_threshold': 0.7,
            'batch_size': 4 if self.memory_gb >= 128 else 2,
            'max_iterations': 100
        })
        
        # TPS ë³€í˜• ì„¤ì •
        self.tps_config = base_config.get('tps', {
            'grid_size': 20,
            'control_points': 25,
            'regularization': 0.01,
            'interpolation_mode': 'bilinear'
        })
        
        # ì‹œê°í™” ì„¤ì •
        self.visualization_config = base_config.get('visualization', {
            'enable_visualization': True,
            'show_keypoints': True,
            'show_matching_lines': True,
            'show_transformation_grid': True,
            'keypoint_size': 3,
            'line_thickness': 2,
            'grid_density': 20,
            'quality': 'high'
        })
        
        # M3 Max ìµœì í™” ì ìš©
        if self.is_m3_max:
            self._apply_m3_max_optimization()
    
    def _apply_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì ìš©"""
        try:
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            
            # ìŠ¤ë ˆë“œ ìµœì í™”
            torch.set_num_threads(16)  # M3 Max 16ì½”ì–´
            
            # ë°°ì¹˜ í¬ê¸° ìµœì í™”
            if self.memory_gb >= 128:
                self.matching_config['batch_size'] = 8
            
            self.logger.info("ğŸ M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _setup_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.matching_stats = {
            'total_matches': 0,
            'successful_matches': 0,
            'average_accuracy': 0.0,
            'total_processing_time': 0.0,
            'memory_usage': {},
            'error_count': 0,
            'last_error': None
        }
    
    async def initialize(self) -> bool:
        """ğŸ”¥ AI ëª¨ë¸ ì´ˆê¸°í™” - ModelLoader ì™„ë²½ ì—°ë™"""
        if self.is_initialized:
            return True
        
        try:
            self.logger.info("ğŸ”„ ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
            await self._setup_model_interface()
            
            # 2. AI ëª¨ë¸ ë¡œë“œ (ModelLoaderë¥¼ í†µí•´)
            await self._load_models_via_model_loader()
            
            # 3. ë””ë°”ì´ìŠ¤ ì„¤ì •
            await self._setup_device()
            
            self.is_initialized = True
            self.models_loaded = True
            self.logger.info("âœ… ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.matching_stats['error_count'] += 1
            self.matching_stats['last_error'] = str(e)
            
            # í´ë°±: ê¸°ë³¸ ëª¨ë¸ ìƒì„±
            await self._create_fallback_models()
            return False
    
    async def _setup_model_interface(self):
        """ğŸ”¥ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if MODEL_LOADER_AVAILABLE:
                # ì „ì—­ ModelLoader ì‚¬ìš©
                self.model_loader = get_global_model_loader()
                
                # Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoaderì˜ ë©”ì„œë“œ ì‚¬ìš©)
                if self.model_loader and hasattr(self.model_loader, 'create_step_interface'):
                    self.model_interface = self.model_loader.create_step_interface(self.step_name)
                    self.logger.info("ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                else:
                    self.logger.warning("âš ï¸ ModelLoader create_step_interface ë©”ì„œë“œ ì—†ìŒ")
                    
            else:
                self.logger.warning("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€ - í´ë°± ëª¨ë“œë¡œ ì „í™˜")
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def _load_models_via_model_loader(self):
        """ğŸ”¥ ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_interface:
                # Step ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                if STEP_REQUESTS_AVAILABLE:
                    step_request = StepModelRequestAnalyzer.get_step_request_info(self.step_name)
                    
                    if step_request:
                        self.logger.info(f"ğŸ§  Step ìš”ì²­ ì •ë³´: {step_request}")
                        
                        # 1. ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ
                        try:
                            self.geometric_model = await self.model_interface.get_model(
                                step_request.get('model_name', 'geometric_matching_base')
                            )
                            if self.geometric_model:
                                self.logger.info("âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        
                        # 2. TPS ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
                        try:
                            self.tps_network = await self.model_interface.get_model('tps_network')
                            if self.tps_network:
                                self.logger.info("âœ… TPS ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ TPS ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        
                        # 3. íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ (ì„ íƒì )
                        try:
                            self.feature_extractor = await self.model_interface.get_model('feature_extractor')
                            if self.feature_extractor:
                                self.logger.info("âœ… íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.debug(f"íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ ê±´ë„ˆëœ€: {e}")
                        
                        # ëª¨ë¸ ë¡œë“œ ì„±ê³µ í™•ì¸
                        if self.geometric_model or self.tps_network:
                            self.logger.info("ğŸ§  ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        else:
                            self.logger.warning("âš ï¸ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ëª¨ë¸ ìƒì„±")
                            await self._create_fallback_models()
                    else:
                        self.logger.warning("âš ï¸ Step ìš”ì²­ ì •ë³´ ì—†ìŒ - í´ë°± ëª¨ë¸ ìƒì„±")
                        await self._create_fallback_models()
                else:
                    self.logger.warning("âš ï¸ Step ìš”ì²­ì‚¬í•­ ëª¨ë“ˆ ì—†ìŒ - í´ë°± ëª¨ë¸ ìƒì„±")
                    await self._create_fallback_models()
            else:
                # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—†ìŒ - í´ë°± ëª¨ë¸ ìƒì„±
                await self._create_fallback_models()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            await self._create_fallback_models()
    
    async def _create_fallback_models(self):
        """ğŸ”§ í´ë°± ëª¨ë¸ ìƒì„± (ModelLoader ì‹¤íŒ¨ ì‹œ)"""
        try:
            self.logger.info("ğŸ”§ í´ë°± AI ëª¨ë¸ ìƒì„± ì¤‘...")
            
            # ê°„ë‹¨í•œ ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸
            class SimpleGeometricModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.feature_extractor = nn.Sequential(
                        nn.Conv2d(3, 64, 3, 1, 1),
                        nn.ReLU(),
                        nn.Conv2d(64, 128, 3, 2, 1),
                        nn.ReLU(),
                        nn.Conv2d(128, 256, 3, 2, 1),
                        nn.ReLU(),
                        nn.AdaptiveAvgPool2d((8, 8))
                    )
                    self.keypoint_detector = nn.Sequential(
                        nn.Linear(256 * 8 * 8, 512),
                        nn.ReLU(),
                        nn.Linear(512, 50)  # 25 keypoints * 2 coordinates
                    )
                
                def forward(self, x):
                    features = self.feature_extractor(x)
                    features = features.view(features.size(0), -1)
                    keypoints = self.keypoint_detector(features)
                    return keypoints.view(-1, 25, 2)
            
            # ê°„ë‹¨í•œ TPS ë³€í˜• ë„¤íŠ¸ì›Œí¬
            class SimpleTPS(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.control_points = 25
                
                def forward(self, source_points, target_points, grid_size=20):
                    # ê°„ë‹¨í•œ TPS ë³€í˜• êµ¬í˜„
                    batch_size = source_points.size(0)
                    device = source_points.device
                    
                    # ì •ê·œ ê·¸ë¦¬ë“œ ìƒì„±
                    y, x = torch.meshgrid(
                        torch.linspace(-1, 1, grid_size, device=device),
                        torch.linspace(-1, 1, grid_size, device=device),
                        indexing='ij'
                    )
                    grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
                    
                    return grid
            
            # í´ë°± ëª¨ë¸ ìƒì„±
            self.geometric_model = SimpleGeometricModel().to(self.device)
            self.tps_network = SimpleTPS().to(self.device)
            
            # ì •ë°€ë„ ì„¤ì •
            self.geometric_model = self._setup_model_precision(self.geometric_model)
            self.tps_network = self._setup_model_precision(self.tps_network)
            
            self.logger.info("âœ… í´ë°± AI ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _setup_model_precision(self, model: nn.Module) -> nn.Module:
        """M3 Max í˜¸í™˜ ì •ë°€ë„ ì„¤ì •"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float()
            elif self.device == "cuda" and hasattr(model, 'half'):
                # CUDAì—ì„œëŠ” Float16 ì‚¬ìš© ê°€ëŠ¥
                return model.half()
            else:
                return model.float()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model
    
    async def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        try:
            # ëª¨ë¸ë“¤ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.geometric_model:
                self.geometric_model = self.geometric_model.to(self.device)
                self.geometric_model.eval()
            
            if self.tps_network:
                self.tps_network = self.tps_network.to(self.device)
                self.tps_network.eval()
            
            if self.feature_extractor:
                self.feature_extractor = self.feature_extractor.to(self.device)
                self.feature_extractor.eval()
            
            self.logger.info(f"âœ… ëª¨ë“  ëª¨ë¸ì´ {self.device}ë¡œ ì´ë™ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        pose_keypoints: Optional[np.ndarray] = None,
        body_mask: Optional[np.ndarray] = None,
        clothing_mask: Optional[np.ndarray] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰ (ê¸°ì¡´ API í˜¸í™˜)"""
        
        start_time = time.time()
        
        try:
            # ì´ˆê¸°í™” í™•ì¸
            if not self.is_initialized:
                await self.initialize()
            
            self.logger.info("ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ ì‹œì‘...")
            
            # ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            processed_input = await self._preprocess_inputs(
                person_image, clothing_image, pose_keypoints, body_mask, clothing_mask
            )
            
            # ğŸ”¥ AI ëª¨ë¸ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ë§¤ì¹­ (ModelLoader ì œê³µ ëª¨ë¸ ì‚¬ìš©)
            matching_result = await self._perform_neural_matching(
                processed_input['person_tensor'],
                processed_input['clothing_tensor']
            )
            
            # TPS ë³€í˜• ê³„ì‚°
            tps_result = await self._compute_tps_transformation(
                matching_result,
                processed_input
            )
            
            # ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
            warped_result = await self._apply_geometric_transform(
                processed_input['clothing_tensor'],
                tps_result['source_points'],
                tps_result['target_points']
            )
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = await self._evaluate_matching_quality(
                matching_result,
                tps_result,
                warped_result
            )
            
            # í›„ì²˜ë¦¬
            final_result = await self._postprocess_result(
                warped_result,
                quality_score,
                processed_input
            )
            
            # ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_matching_visualization(
                processed_input,
                matching_result,
                tps_result,
                warped_result,
                quality_score
            )
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_cleanup = safe_mps_memory_cleanup(self.device)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_stats(quality_score, processing_time)
            
            self.logger.info(f"âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}s")
            
            # API í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ê³¼ êµ¬ì¡°
            return {
                'success': True,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}',
                'confidence': quality_score,
                'processing_time': processing_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'details': {
                    # í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                    'result_image': visualization_results['matching_visualization'],
                    'overlay_image': visualization_results['warped_overlay'],
                    
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'num_keypoints': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                    'matching_confidence': matching_result['matching_confidence'],
                    'transformation_quality': quality_score,
                    'grid_size': self.tps_config['grid_size'],
                    'method': self.matching_config['method'],
                    
                    # ìƒì„¸ ë§¤ì¹­ ì •ë³´
                    'matching_details': {
                        'source_keypoints_count': len(matching_result['source_keypoints'][0]) if len(matching_result['source_keypoints']) > 0 else 0,
                        'target_keypoints_count': len(matching_result['target_keypoints'][0]) if len(matching_result['target_keypoints']) > 0 else 0,
                        'successful_matches': int(quality_score * 100),
                        'transformation_type': 'TPS (Thin Plate Spline)',
                        'optimization_enabled': self.optimization_enabled
                    }
                },
                
                # ë ˆê±°ì‹œ í˜¸í™˜ì„± í•„ë“œë“¤
                'warped_clothing': final_result['warped_clothing'],
                'warped_mask': final_result.get('warped_mask', np.zeros((384, 512), dtype=np.uint8)),
                'transformation_matrix': tps_result.get('transformation_matrix'),
                'source_keypoints': matching_result['source_keypoints'],
                'target_keypoints': matching_result['target_keypoints'],
                'matching_confidence': matching_result['matching_confidence'],
                'quality_score': quality_score,
                'metadata': {
                    'method': 'neural_tps',
                    'num_keypoints': self.matching_config['num_keypoints'],
                    'grid_size': self.tps_config['grid_size'],
                    'device': self.device,
                    'optimization_enabled': self.optimization_enabled,
                    'pytorch_version': torch.__version__,
                    'memory_management': memory_cleanup
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            self.matching_stats['error_count'] += 1
            self.matching_stats['last_error'] = str(e)
            
            return {
                'success': False,
                'message': f'ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}',
                'confidence': 0.0,
                'processing_time': time.time() - start_time,
                'step_name': 'geometric_matching',
                'step_number': 4,
                'error': str(e),
                'details': {
                    'result_image': '',
                    'overlay_image': '',
                    'error_type': type(e).__name__,
                    'error_count': self.matching_stats['error_count'],
                    'traceback': traceback.format_exc()
                }
            }
    
    async def _preprocess_inputs(
        self,
        person_image: Union[np.ndarray, Image.Image, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
        pose_keypoints: Optional[np.ndarray] = None,
        body_mask: Optional[np.ndarray] = None,
        clothing_mask: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            clothing_tensor = self._image_to_tensor(clothing_image)
            
            # í¬ê¸° ì •ê·œí™” (512x384)
            person_tensor = F.interpolate(person_tensor, size=(384, 512), mode='bilinear', align_corners=False)
            clothing_tensor = F.interpolate(clothing_tensor, size=(384, 512), mode='bilinear', align_corners=False)
            
            return {
                'person_tensor': person_tensor,
                'clothing_tensor': clothing_tensor,
                'pose_keypoints': pose_keypoints,
                'body_mask': body_mask,
                'clothing_mask': clothing_mask
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _image_to_tensor(self, image: Union[np.ndarray, Image.Image, torch.Tensor]) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            if isinstance(image, torch.Tensor):
                if image.dim() == 3:
                    return image.unsqueeze(0)
                return image
            elif isinstance(image, Image.Image):
                transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                return transform(image).unsqueeze(0)
            elif isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                pil_image = Image.fromarray(image)
                return self._image_to_tensor(pil_image)
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    async def _perform_neural_matching(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """ğŸ”¥ ì‹ ê²½ë§ ê¸°ë°˜ ë§¤ì¹­ (ModelLoader ì œê³µ ëª¨ë¸ ì‚¬ìš©)"""
        try:
            with torch.no_grad():
                # 1. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (ModelLoader ì œê³µ ëª¨ë¸ ì‚¬ìš©)
                if self.geometric_model:
                    person_keypoints = self.geometric_model(person_tensor.to(self.device))
                    clothing_keypoints = self.geometric_model(clothing_tensor.to(self.device))
                else:
                    # í´ë°±: ë‹¨ìˆœ í‚¤í¬ì¸íŠ¸ ìƒì„±
                    person_keypoints = self._generate_fallback_keypoints(person_tensor)
                    clothing_keypoints = self._generate_fallback_keypoints(clothing_tensor)
                
                # 2. í‚¤í¬ì¸íŠ¸ ë§¤ì¹­
                matching_confidence = self._compute_matching_confidence(
                    person_keypoints, clothing_keypoints
                )
                
                return {
                    'source_keypoints': person_keypoints,
                    'target_keypoints': clothing_keypoints,
                    'matching_confidence': matching_confidence
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ê²½ë§ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_fallback_keypoints(self, image_tensor: torch.Tensor) -> torch.Tensor:
        """í´ë°± í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        try:
            batch_size = image_tensor.size(0)
            device = image_tensor.device
            
            # ê· ë“±í•˜ê²Œ ë¶„í¬ëœ í‚¤í¬ì¸íŠ¸ ìƒì„±
            y_coords = torch.linspace(0.1, 0.9, 5, device=device)
            x_coords = torch.linspace(0.1, 0.9, 5, device=device)
            
            keypoints = []
            for y in y_coords:
                for x in x_coords:
                    keypoints.append([x.item(), y.item()])
            
            keypoints_tensor = torch.tensor(keypoints, device=device, dtype=torch.float32)
            return keypoints_tensor.unsqueeze(0).repeat(batch_size, 1, 1)
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± í‚¤í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _compute_matching_confidence(
        self,
        source_keypoints: torch.Tensor,
        target_keypoints: torch.Tensor
    ) -> float:
        """ë§¤ì¹­ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ê³„ì‚°
            distances = torch.norm(source_keypoints - target_keypoints, dim=-1)
            avg_distance = distances.mean().item()
            
            # ì‹ ë¢°ë„ëŠ” ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ë†’ìŒ
            confidence = max(0.0, 1.0 - avg_distance)
            
            return confidence
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§¤ì¹­ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    async def _compute_tps_transformation(
        self,
        matching_result: Dict[str, Any],
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            source_points = matching_result['source_keypoints']
            target_points = matching_result['target_keypoints']
            
            # TPS ë³€í˜• ê³„ì‚° (ModelLoader ì œê³µ ëª¨ë¸ ì‚¬ìš©)
            if self.tps_network:
                with torch.no_grad():
                    transformation_grid = self.tps_network(
                        source_points, 
                        target_points, 
                        self.tps_config['grid_size']
                    )
            else:
                # í´ë°±: ë‹¨ìˆœ ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
                transformation_grid = self._generate_fallback_grid(source_points, target_points)
            
            return {
                'source_points': source_points,
                'target_points': target_points,
                'transformation_grid': transformation_grid,
                'transformation_matrix': None  # ë ˆê±°ì‹œ í˜¸í™˜ì„±
            }
            
        except Exception as e:
            self.logger.error(f"âŒ TPS ë³€í˜• ê³„ì‚° ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_fallback_grid(
        self,
        source_points: torch.Tensor,
        target_points: torch.Tensor
    ) -> torch.Tensor:
        """í´ë°± ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±"""
        try:
            batch_size = source_points.size(0)
            device = source_points.device
            grid_size = self.tps_config['grid_size']
            
            # ì •ê·œ ê·¸ë¦¬ë“œ ìƒì„±
            y, x = torch.meshgrid(
                torch.linspace(-1, 1, grid_size, device=device),
                torch.linspace(-1, 1, grid_size, device=device),
                indexing='ij'
            )
            grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            return grid
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ê·¸ë¦¬ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _apply_geometric_transform(
        self,
        clothing_tensor: torch.Tensor,
        source_points: torch.Tensor,
        target_points: torch.Tensor
    ) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë³€í˜• ì ìš©"""
        try:
            # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ì„ í†µí•œ ë³€í˜• ì ìš©
            grid_size = self.tps_config['grid_size']
            
            # ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
            transformation_grid = self._generate_transformation_grid(
                source_points, target_points, grid_size
            )
            
            # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ ì ìš©
            warped_clothing = F.grid_sample(
                clothing_tensor.to(self.device),
                transformation_grid,
                mode='bilinear',
                padding_mode='zeros',
                align_corners=False
            )
            
            return {
                'warped_image': warped_clothing,
                'transformation_grid': transformation_grid
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°í•˜í•™ì  ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_transformation_grid(
        self,
        source_points: torch.Tensor,
        target_points: torch.Tensor,
        grid_size: int
    ) -> torch.Tensor:
        """ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„± (ë‹¨ìˆœí™”ëœ TPS)"""
        try:
            batch_size = source_points.size(0)
            device = source_points.device
            height, width = grid_size, grid_size
            
            # ì •ê·œ ê·¸ë¦¬ë“œ ìƒì„±
            y, x = torch.meshgrid(
                torch.linspace(-1, 1, height, device=device),
                torch.linspace(-1, 1, width, device=device),
                indexing='ij'
            )
            grid_flat = torch.stack([x, y], dim=-1).view(-1, 2)
            grid_flat = grid_flat.unsqueeze(0).repeat(batch_size, 1, 1)
            
            # ê±°ë¦¬ ê¸°ë°˜ ë³´ê°„
            distances = torch.cdist(grid_flat, source_points)  # [B, H*W, N]
            weights = torch.softmax(-distances / 0.1, dim=-1)  # [B, H*W, N]
            
            # ë³€ìœ„ ê³„ì‚°
            displacement = target_points - source_points  # [B, N, 2]
            interpolated_displacement = torch.sum(
                weights.unsqueeze(-1) * displacement.unsqueeze(1), dim=2
            )  # [B, H*W, 2]
            
            # ë³€í˜•ëœ ê·¸ë¦¬ë“œ
            transformed_grid_flat = grid_flat + interpolated_displacement
            transformed_grid = transformed_grid_flat.view(batch_size, height, width, 2)
            
            return transformed_grid
            
        except Exception as e:
            self.logger.error(f"âŒ ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _evaluate_matching_quality(
        self,
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any]
    ) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            # 1. ë§¤ì¹­ ì‹ ë¢°ë„
            matching_confidence = matching_result['matching_confidence']
            
            # 2. ë³€í˜• í’ˆì§ˆ (ê°„ë‹¨í•œ ë©”íŠ¸ë¦­)
            transformation_quality = 0.8  # ê¸°ë³¸ê°’
            
            # 3. ìµœì¢… í’ˆì§ˆ ì ìˆ˜
            quality_score = (matching_confidence + transformation_quality) / 2.0
            
            return min(1.0, max(0.0, quality_score))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    async def _postprocess_result(
        self,
        warped_result: Dict[str, Any],
        quality_score: float,
        processed_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_image = warped_result['warped_image']
            
            # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            warped_clothing = self._tensor_to_numpy(warped_image)
            
            # ë§ˆìŠ¤í¬ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            warped_mask = np.ones((384, 512), dtype=np.uint8) * 255
            
            return {
                'warped_clothing': warped_clothing,
                'warped_mask': warped_mask,
                'quality_score': quality_score
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # GPU í…ì„œë¥¼ CPUë¡œ ì´ë™
            if tensor.is_cuda or (hasattr(tensor, 'device') and tensor.device.type == 'mps'):
                tensor = tensor.cpu()
            
            # ì •ê·œí™” í•´ì œ
            tensor = tensor.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
            if tensor.size(0) == 3:  # CHW -> HWC
                tensor = tensor.permute(1, 2, 0)
            
            # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            tensor = torch.clamp(tensor, 0, 1)
            
            # numpy ë³€í™˜
            numpy_array = tensor.detach().numpy()
            
            # uint8ë¡œ ë³€í™˜
            numpy_array = (numpy_array * 255).astype(np.uint8)
            
            return numpy_array
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return np.zeros((384, 512, 3), dtype=np.uint8)
    
    async def _create_matching_visualization(
        self,
        processed_input: Dict[str, Any],
        matching_result: Dict[str, Any],
        tps_result: Dict[str, Any],
        warped_result: Dict[str, Any],
        quality_score: float
    ) -> Dict[str, str]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±"""
        try:
            if not self.visualization_config.get('enable_visualization', True):
                return {
                    'matching_visualization': '',
                    'warped_overlay': '',
                    'transformation_grid': ''
                }
            
            def _create_visualizations():
                # ì›ë³¸ ì´ë¯¸ì§€ë“¤ì„ PILë¡œ ë³€í™˜
                person_pil = self._tensor_to_pil(processed_input['person_tensor'])
                clothing_pil = self._tensor_to_pil(processed_input['clothing_tensor'])
                warped_clothing_pil = self._tensor_to_pil(warped_result['warped_image'])
                
                # 1. í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”
                matching_viz = self._create_keypoint_matching_visualization(
                    person_pil, clothing_pil, matching_result
                )
                
                # 2. ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´
                warped_overlay = self._create_warped_overlay(
                    person_pil, warped_clothing_pil, quality_score
                )
                
                # 3. ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™”
                transformation_grid = self._create_transformation_grid_visualization(
                    tps_result.get('transformation_grid')
                )
                
                return {
                    'matching_visualization': self._pil_to_base64(matching_viz),
                    'warped_overlay': self._pil_to_base64(warped_overlay),
                    'transformation_grid': self._pil_to_base64(transformation_grid)
                }
            
            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹œê°í™” ìƒì„±
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_create_visualizations)
                return future.result()
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'matching_visualization': '',
                'warped_overlay': '',
                'transformation_grid': ''
            }
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            numpy_array = self._tensor_to_numpy(tensor)
            if numpy_array.ndim == 3:
                return Image.fromarray(numpy_array)
            else:
                return Image.fromarray(numpy_array, mode='L')
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 384), color='black')
    
    def _create_keypoint_matching_visualization(
        self,
        person_image: Image.Image,
        clothing_image: Image.Image,
        matching_result: Dict[str, Any]
    ) -> Image.Image:
        """í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì‹œê°í™”"""
        try:
            # ì´ë¯¸ì§€ ë‚˜ë€íˆ ë°°ì¹˜
            combined_width = person_image.width + clothing_image.width
            combined_height = max(person_image.height, clothing_image.height)
            combined_image = Image.new('RGB', (combined_width, combined_height), color='white')
            
            combined_image.paste(person_image, (0, 0))
            combined_image.paste(clothing_image, (person_image.width, 0))
            
            # í‚¤í¬ì¸íŠ¸ ë° ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(combined_image)
            
            # í‚¤í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            source_keypoints = matching_result['source_keypoints']
            target_keypoints = matching_result['target_keypoints']
            
            if isinstance(source_keypoints, torch.Tensor):
                source_keypoints = source_keypoints.cpu().numpy()
            if isinstance(target_keypoints, torch.Tensor):
                target_keypoints = target_keypoints.cpu().numpy()
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            keypoint_size = self.visualization_config.get('keypoint_size', 3)
            
            # Person í‚¤í¬ì¸íŠ¸ (ë¹¨ê°„ìƒ‰)
            for point in source_keypoints[0]:  # ì²« ë²ˆì§¸ ë°°ì¹˜
                x, y = point * np.array([person_image.width, person_image.height])
                draw.ellipse([x-keypoint_size, y-keypoint_size, x+keypoint_size, y+keypoint_size], 
                           fill='red', outline='darkred')
            
            # Clothing í‚¤í¬ì¸íŠ¸ (íŒŒë€ìƒ‰)
            for point in target_keypoints[0]:  # ì²« ë²ˆì§¸ ë°°ì¹˜
                x, y = point * np.array([clothing_image.width, clothing_image.height])
                x += person_image.width  # ì˜¤í”„ì…‹ ì ìš©
                draw.ellipse([x-keypoint_size, y-keypoint_size, x+keypoint_size, y+keypoint_size], 
                           fill='blue', outline='darkblue')
            
            # ë§¤ì¹­ ë¼ì¸ ê·¸ë¦¬ê¸°
            if self.visualization_config.get('show_matching_lines', True):
                for i, (src_point, tgt_point) in enumerate(zip(source_keypoints[0], target_keypoints[0])):
                    src_x, src_y = src_point * np.array([person_image.width, person_image.height])
                    tgt_x, tgt_y = tgt_point * np.array([clothing_image.width, clothing_image.height])
                    tgt_x += person_image.width  # ì˜¤í”„ì…‹ ì ìš©
                    
                    draw.line([src_x, src_y, tgt_x, tgt_y], fill='green', width=1)
            
            return combined_image
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (1024, 384), color='black')
    
    def _create_warped_overlay(
        self,
        person_image: Image.Image,
        warped_clothing: Image.Image,
        quality_score: float
    ) -> Image.Image:
        """ë³€í˜•ëœ ì˜ë¥˜ ì˜¤ë²„ë ˆì´ ì‹œê°í™”"""
        try:
            # íˆ¬ëª…ë„ ì„¤ì • (í’ˆì§ˆì— ë”°ë¼)
            alpha = int(255 * min(0.8, quality_score))
            
            # ì˜¤ë²„ë ˆì´ ìƒì„±
            overlay = Image.alpha_composite(
                person_image.convert('RGBA'),
                warped_clothing.convert('RGBA').resize(person_image.size)
            )
            
            return overlay.convert('RGB')
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_image
    
    def _create_transformation_grid_visualization(
        self,
        transformation_grid: Optional[torch.Tensor]
    ) -> Image.Image:
        """ë³€í˜• ê·¸ë¦¬ë“œ ì‹œê°í™”"""
        try:
            if transformation_grid is None:
                return Image.new('RGB', (400, 400), color='black')
            
            # ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„±
            grid_image = Image.new('RGB', (400, 400), color='white')
            draw = ImageDraw.Draw(grid_image)
            
            # ê·¸ë¦¬ë“œ ë¼ì¸ ê·¸ë¦¬ê¸°
            grid_size = transformation_grid.size(1)
            cell_width = 400 // grid_size
            cell_height = 400 // grid_size
            
            for i in range(grid_size + 1):
                # ì„¸ë¡œì„ 
                x = i * cell_width
                draw.line([x, 0, x, 400], fill='gray', width=1)
                
                # ê°€ë¡œì„ 
                y = i * cell_height
                draw.line([0, y, 400, y], fill='gray', width=1)
            
            return grid_image
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¸ë¦¬ë“œ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (400, 400), color='black')
    
    def _pil_to_base64(self, pil_image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            pil_image.save(buffer, format='JPEG', quality=85)
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return f"data:image/jpeg;base64,{img_str}"
        except Exception as e:
            self.logger.error(f"âŒ Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    def _update_stats(self, quality_score: float, processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.matching_stats['total_matches'] += 1
            if quality_score >= self.matching_config['quality_threshold']:
                self.matching_stats['successful_matches'] += 1
            
            # í‰ê·  ì •í™•ë„ ì—…ë°ì´íŠ¸
            total = self.matching_stats['total_matches']
            current_avg = self.matching_stats['average_accuracy']
            self.matching_stats['average_accuracy'] = (current_avg * (total - 1) + quality_score) / total
            
            # ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.matching_stats['total_processing_time'] += processing_time
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def validate_inputs(
        self,
        person_image: Any,
        clothing_image: Any
    ) -> Dict[str, Any]:
        """ì…ë ¥ ê²€ì¦"""
        try:
            validation_results = {
                'valid': False,
                'person_image': False,
                'clothing_image': False,
                'errors': [],
                'image_sizes': {}
            }
            
            # Person ì´ë¯¸ì§€ ê²€ì¦
            if person_image is not None:
                if isinstance(person_image, (np.ndarray, Image.Image, torch.Tensor)):
                    validation_results['person_image'] = True
                    if hasattr(person_image, 'shape'):
                        validation_results['image_sizes']['person'] = person_image.shape
                    elif hasattr(person_image, 'size'):
                        validation_results['image_sizes']['person'] = person_image.size
                else:
                    validation_results['errors'].append("Person ì´ë¯¸ì§€ íƒ€ì…ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ")
            else:
                validation_results['errors'].append("Person ì´ë¯¸ì§€ê°€ None")
            
            # Clothing ì´ë¯¸ì§€ ê²€ì¦
            if clothing_image is not None:
                if isinstance(clothing_image, (np.ndarray, Image.Image, torch.Tensor)):
                    validation_results['clothing_image'] = True
                    if hasattr(clothing_image, 'shape'):
                        validation_results['image_sizes']['clothing'] = clothing_image.shape
                    elif hasattr(clothing_image, 'size'):
                        validation_results['image_sizes']['clothing'] = clothing_image.size
                else:
                    validation_results['errors'].append("Clothing ì´ë¯¸ì§€ íƒ€ì…ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ")
            else:
                validation_results['errors'].append("Clothing ì´ë¯¸ì§€ê°€ None")
            
            # ì „ì²´ ê²€ì¦ ê²°ê³¼
            validation_results['valid'] = (
                validation_results['person_image'] and 
                validation_results['clothing_image'] and 
                len(validation_results['errors']) == 0
            )
            
            return validation_results
            
        except Exception as e:
            return {
                'valid': False,
                'error': str(e),
                'person_image': False,
                'clothing_image': False
            }
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 4ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "device": self.device,
                "initialized": self.is_initialized,
                "models_loaded": self.models_loaded,
                "model_interface_available": self.model_interface is not None,
                "models": {
                    "geometric_model": self.geometric_model is not None,
                    "tps_network": self.tps_network is not None,
                    "feature_extractor": self.feature_extractor is not None
                },
                "config": {
                    "method": self.matching_config['method'],
                    "num_keypoints": self.matching_config['num_keypoints'],
                    "grid_size": self.tps_config['grid_size'],
                    "quality_level": self.quality_level,
                    "quality_threshold": self.matching_config['quality_threshold'],
                    "visualization_enabled": self.visualization_config.get('enable_visualization', True)
                },
                "performance": self.matching_stats,
                "optimization": {
                    "m3_max_enabled": self.is_m3_max,
                    "optimization_enabled": self.optimization_enabled,
                    "memory_gb": self.memory_gb,
                    "device_type": self.device_type,
                    "pytorch_version": torch.__version__
                },
                "visualization": {
                    "show_keypoints": self.visualization_config.get('show_keypoints', True),
                    "show_matching_lines": self.visualization_config.get('show_matching_lines', True),
                    "show_transformation_grid": self.visualization_config.get('show_transformation_grid', True),
                    "quality": self.visualization_config.get('quality', 'high')
                }
            }
        except Exception as e:
            self.logger.error(f"ë‹¨ê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "step_name": "geometric_matching",
                "step_number": 4,
                "error": str(e),
                "pytorch_version": torch.__version__
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - PyTorch 2.1 í˜¸í™˜"""
        try:
            self.logger.info("ğŸ§¹ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë¸ ì •ë¦¬
            if self.geometric_model is not None:
                if hasattr(self.geometric_model, 'cpu'):
                    self.geometric_model.cpu()
                del self.geometric_model
                self.geometric_model = None
            
            if self.tps_network is not None:
                if hasattr(self.tps_network, 'cpu'):
                    self.tps_network.cpu()
                del self.tps_network
                self.tps_network = None
            
            if self.feature_extractor is not None:
                if hasattr(self.feature_extractor, 'cpu'):
                    self.feature_extractor.cpu()
                del self.feature_extractor
                self.feature_extractor = None
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'executor') and self.executor:
                self.executor.shutdown(wait=True)
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if self.model_interface and hasattr(self.model_interface, 'unload_models'):
                await self.model_interface.unload_models()
            
            # PyTorch 2.1 í˜¸í™˜ ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_result = safe_mps_memory_cleanup(self.device)
            
            gc.collect()
            
            self.logger.info(f"âœ… 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ - ë©”ëª¨ë¦¬ ì •ë¦¬: {memory_result['method']}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ 4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """MRO ì•ˆì „í•œ ì†Œë©¸ì"""
        try:
            # hasattrë¡œ ì•ˆì „ì„± í™•ë³´
            if hasattr(self, 'executor') and self.executor:
                self.executor.shutdown(wait=False)
        except Exception:
            # ì†Œë©¸ìì—ì„œëŠ” ë¡œê¹…ë„ ì•ˆì „í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            pass

# ==============================================
# ğŸ”„ MRO ì•ˆì „í•œ í•˜ìœ„ í˜¸í™˜ì„± ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_geometric_matching_step(
    device: str = "mps", 
    config: Optional[Dict[str, Any]] = None
) -> GeometricMatchingStep:
    """MRO ì•ˆì „í•œ ê¸°ì¡´ ë°©ì‹ 100% í˜¸í™˜ ìƒì„±ì"""
    try:
        return GeometricMatchingStep(device=device, config=config)
    except Exception as e:
        # MRO ì˜¤ë¥˜ ì‹œ í´ë°±
        logging.warning(f"GeometricMatchingStep ìƒì„± ì‹¤íŒ¨: {e}, ê¸°ë³¸ ìƒì„±ì ì‚¬ìš©")
        return GeometricMatchingStep()

def create_m3_max_geometric_matching_step(
    device: Optional[str] = None,
    memory_gb: float = 128.0,
    optimization_level: str = "ultra",
    **kwargs
) -> GeometricMatchingStep:
    """MRO ì•ˆì „í•œ M3 Max ìµœì í™” ì „ìš© ìƒì„±ì"""
    try:
        return GeometricMatchingStep(
            device=device,
            memory_gb=memory_gb,
            quality_level=optimization_level,
            is_m3_max=True,
            optimization_enabled=True,
            **kwargs
        )
    except Exception as e:
        # MRO ì˜¤ë¥˜ ì‹œ í´ë°±
        logging.warning(f"M3 Max GeometricMatchingStep ìƒì„± ì‹¤íŒ¨: {e}, ê¸°ë³¸ ìƒì„±ì ì‚¬ìš©")
        return GeometricMatchingStep(device=device or "mps")

# ==============================================
# ğŸ¯ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def optimize_geometric_matching_for_m3_max():
    """M3 Max ì „ìš© ìµœì í™” ì„¤ì •"""
    try:
        # PyTorch ì„¤ì •
        torch.set_num_threads(16)  # M3 Max 16ì½”ì–´
        torch.backends.mps.set_per_process_memory_fraction(0.8)  # ë©”ëª¨ë¦¬ 80% ì‚¬ìš©
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        os.environ['OMP_NUM_THREADS'] = '16'
        
        return True
    except Exception as e:
        logging.warning(f"M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

def get_geometric_matching_benchmarks() -> Dict[str, Any]:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ë²¤ì¹˜ë§ˆí¬ ì •ë³´"""
    return {
        "m3_max_128gb": {
            "expected_processing_time": "2-5ì´ˆ",
            "memory_usage": "8-16GB",
            "batch_size": 8,
            "quality_threshold": 0.85
        },
        "standard": {
            "expected_processing_time": "5-10ì´ˆ",
            "memory_usage": "4-8GB", 
            "batch_size": 4,
            "quality_threshold": 0.75
        }
    }

# ==============================================
# ğŸ”¥ MRO ê²€ì¦ í•¨ìˆ˜
# ==============================================

def validate_mro() -> bool:
    """MRO(Method Resolution Order) ê²€ì¦"""
    try:
        # í´ë˜ìŠ¤ MRO í™•ì¸
        mro = GeometricMatchingStep.__mro__
        mro_names = [cls.__name__ for cls in mro]
        
        logger.info(f"âœ… GeometricMatchingStep MRO: {' -> '.join(mro_names)}")
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        test_instance = GeometricMatchingStep(device="cpu")
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attrs = ['logger', 'step_name', 'device', 'is_initialized']
        for attr in required_attrs:
            if not hasattr(test_instance, attr):
                logger.error(f"âŒ í•„ìˆ˜ ì†ì„± ëˆ„ë½: {attr}")
                return False
        
        logger.info("âœ… MRO ê²€ì¦ í†µê³¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ MRO ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

async def test_geometric_matching_pipeline():
    """ê¸°í•˜í•™ì  ë§¤ì¹­ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    try:
        # í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step = GeometricMatchingStep(device="cpu")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        init_result = await step.initialize()
        assert init_result, "ì´ˆê¸°í™” ì‹¤íŒ¨"
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_person = np.random.randint(0, 255, (384, 512, 3), dtype=np.uint8)
        dummy_clothing = np.random.randint(0, 255, (384, 512, 3), dtype=np.uint8)
        
        # ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = await step.process(dummy_person, dummy_clothing)
        assert result['success'], f"ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('message', 'Unknown error')}"
        
        # ì •ë¦¬
        await step.cleanup()
        
        logger.info("âœ… ê¸°í•˜í•™ì  ë§¤ì¹­ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'GeometricMatchingStep',
    'GeometricMatchingModel', 
    'TPSTransformNetwork',
    'FeatureExtractor',
    'create_geometric_matching_step',
    'create_m3_max_geometric_matching_step',
    'safe_mps_memory_cleanup',
    'validate_mro',
    'optimize_geometric_matching_for_m3_max',
    'get_geometric_matching_benchmarks',
    'test_geometric_matching_pipeline'
]

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.info("âœ… GeometricMatchingStep v5.0 ë¡œë“œ ì™„ë£Œ - ModelLoader ì™„ë²½ ì—°ë™")
logger.info("ğŸ”— ìˆœí™˜ ì°¸ì¡° ì™„ì „ í•´ê²° - í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡°")
logger.info("ğŸ”— ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±)")
logger.info("ğŸ”— MRO(Method Resolution Order) ì™„ì „ ì•ˆì „")
logger.info("ğŸ”— logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("ğŸ§  ModelLoader ì™„ë²½ ì—°ë™ - ì§ì ‘ AI ëª¨ë¸ í˜¸ì¶œ ì œê±°")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ¨ ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ í†µí•©")
logger.info("ğŸ”¥ PyTorch 2.1 ì™„ì „ í˜¸í™˜")
logger.info("ğŸ¯ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„ (AI ëª¨ë¸ í´ë˜ìŠ¤ í¬í•¨)")
logger.info("ğŸ conda í™˜ê²½ ì™„ë²½ ìµœì í™”")

# MRO ê²€ì¦ ì‹¤í–‰
if __name__ == "__main__":
    validate_mro()
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_geometric_matching_pipeline())