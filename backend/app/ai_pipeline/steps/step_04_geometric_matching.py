#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ (AI ì¶”ë¡  ê°•í™” + step_model_requirements.py ì™„ì „ í˜¸í™˜)
===============================================================================

âœ… step_model_requirements.py ì™„ì „ í˜¸í™˜ (REAL_STEP_MODEL_REQUESTS ê¸°ì¤€)
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (gmm_final.pth, tps_network.pth, sam_vit_h_4b8939.pth)
âœ… ì§„ì§œ AI ì¶”ë¡  ë¡œì§ ê°•í™” (OpenCV ì™„ì „ ëŒ€ì²´)
âœ… DetailedDataSpec ì™„ì „ ì¤€ìˆ˜
âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ì²˜ë¦¬
âœ… API ì œê±°, ìˆœìˆ˜ AI ì¶”ë¡ ì— ì§‘ì¤‘
âœ… ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„ 
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´ (í•˜ë‚˜ë„ ë¹ íŠ¸ë¦¬ì§€ ì•ŠìŒ)

Author: MyCloset AI Team
Date: 2025-07-27
Version: 14.0 (Enhanced AI Inference + Sync Processing + Full Feature Preservation)
"""

import asyncio
import os
import gc
import time
import logging
import threading
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from enum import Enum
from io import BytesIO
import base64

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
    from app.core.di_container import DIContainer
    from app.ai_pipeline.utils.step_model_requests import EnhancedRealModelRequest

# ==============================================
# ğŸ”¥ 2. í™˜ê²½ ìµœì í™” (M3 Max + conda ìš°ì„ )
# ==============================================

# PyTorch í™˜ê²½ ìµœì í™”
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
os.environ['OMP_NUM_THREADS'] = '16'  # M3 Max 16ì½”ì–´

# PyTorch ë° ì´ë¯¸ì§€ ì²˜ë¦¬
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.nn import Conv2d, ConvTranspose2d, BatchNorm2d, ReLU, Dropout, AdaptiveAvgPool2d
    TORCH_AVAILABLE = True
    DEVICE = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    
    # ğŸ”§ M3 Max ìµœì í™” (ì•ˆì „í•œ MPS ìºì‹œ ì²˜ë¦¬)
    if DEVICE == "mps":
        try:
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            elif hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            torch.set_num_threads(16)
            
            # conda í™˜ê²½ MPS ìµœì í™”
            if 'CONDA_DEFAULT_ENV' in os.environ:
                conda_env = os.environ['CONDA_DEFAULT_ENV']
                if 'mycloset' in conda_env.lower():
                    os.environ['OMP_NUM_THREADS'] = '16'
                    os.environ['MKL_NUM_THREADS'] = '16'
                    logging.info(f"ğŸ conda í™˜ê²½ ({conda_env}) MPS ìµœì í™” ì™„ë£Œ")
        except Exception as e:
            logging.debug(f"âš ï¸ conda MPS ìµœì í™” ì‹¤íŒ¨: {e}")
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE = "cpu"
    logging.error("âŒ PyTorch import ì‹¤íŒ¨")

try:
    from PIL import Image, ImageDraw, ImageEnhance, ImageFilter
    import PIL
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    logging.error("âŒ PIL import ì‹¤íŒ¨")

try:
    import torchvision.transforms as T
    from torchvision.transforms.functional import resize, to_tensor, to_pil_image
    TORCHVISION_AVAILABLE = True
except ImportError:
    TORCHVISION_AVAILABLE = False

try:
    from scipy.spatial.distance import cdist
    from scipy.optimize import minimize
    from scipy.interpolate import griddata, RBFInterpolator
    import scipy.ndimage as ndimage
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# ==============================================
# ğŸ”¥ 3. ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader', package=__name__)
        get_global_fn = getattr(module, 'get_global_model_loader', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_step_model_request():
    """step_model_requestsì—ì„œ GeometricMatchingStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.step_model_requests', package=__name__)
        requests = getattr(module, 'REAL_STEP_MODEL_REQUESTS', {})
        return requests.get('GeometricMatchingStep')
    except ImportError as e:
        logging.debug(f"step_model_requests import ì‹¤íŒ¨: {e}")
        return None

def get_memory_manager():
    """MemoryManagerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager', package=__name__)
        get_global_fn = getattr(module, 'get_global_memory_manager', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"MemoryManager ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_data_converter():
    """DataConverterë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.data_converter', package=__name__)
        get_global_fn = getattr(module, 'get_global_data_converter', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"DataConverter ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_di_container():
    """DI Containerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_di_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError as e:
        logging.debug(f"DI Container ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 4. BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package=__package__)
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin í´ë˜ìŠ¤ ë™ì  ë¡œë”©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # DetailedDataSpec ê´€ë ¨ ì†ì„±
            self.detailed_data_spec = None
            
            if hasattr(self, 'dependency_manager'):
                self.dependency_manager = None
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
        
        def set_di_container(self, di_container):
            self.di_container = di_container
        
        async def cleanup(self):
            pass

# ==============================================
# ğŸ”¥ 5. SmartModelPathMapper (ì‹¤ì œ íŒŒì¼ ìë™ íƒì§€)
# ==============================================

class EnhancedModelPathMapper:
    """í–¥ìƒëœ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ (step_model_requirements.py ê¸°ì¤€)"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}
        self.logger = logging.getLogger(__name__)
        
        # step_model_requirements.pyì—ì„œ ìš”êµ¬ì‚¬í•­ ë¡œë“œ
        self.step_request = get_step_model_request()
        
        # ì‹¤ì œ ê²½ë¡œ ìë™ íƒì§€ (step_model_requirements.py ê¸°ì¤€)
        self.ai_models_root = self._auto_detect_ai_models_path()
        self.logger.info(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸ ê²½ë¡œ: {self.ai_models_root}")
        
    def _auto_detect_ai_models_path(self) -> Path:
        """ì‹¤ì œ ai_models ë””ë ‰í† ë¦¬ ìë™ íƒì§€ (step_model_requirements.py ê¸°ì¤€)"""
        # step_model_requirements.pyì—ì„œ ì •ì˜ëœ ê²€ìƒ‰ ê²½ë¡œ ì‚¬ìš©
        if self.step_request:
            search_paths = self.step_request.search_paths + self.step_request.fallback_paths
        else:
            search_paths = [
                "step_04_geometric_matching",
                "step_04_geometric_matching/ultra_models", 
                "step_04_geometric_matching/models",
                "step_03_cloth_segmentation"  # SAM ê³µìœ 
            ]
        
        possible_paths = [
            Path.cwd() / "ai_models",
            Path.cwd().parent / "ai_models",
            Path.cwd() / "backend" / "ai_models",
            Path(__file__).parent / "ai_models",
            Path(__file__).parent.parent / "ai_models",
            Path(__file__).parent.parent.parent / "ai_models"
        ]
        
        for path in possible_paths:
            if path.exists():
                # step_model_requirements.py ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦
                for search_path in search_paths:
                    if (path / search_path).exists():
                        return path
                        
        return Path.cwd() / "ai_models"
    
    def find_model_file(self, model_filename: str) -> Optional[Path]:
        """ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸° (step_model_requirements.py ê¸°ì¤€)"""
        cache_key = f"geometric_matching:{model_filename}"
        if cache_key in self.model_cache:
            cached_path = self.model_cache[cache_key]
            if cached_path.exists():
                return cached_path
        
        # step_model_requirements.pyì—ì„œ ì •ì˜ëœ ê²€ìƒ‰ ê²½ë¡œ ì‚¬ìš©
        if self.step_request:
            search_paths = self.step_request.search_paths + self.step_request.fallback_paths
        else:
            search_paths = [
                "step_04_geometric_matching",
                "step_04_geometric_matching/ultra_models",
                "step_04_geometric_matching/models",
                "step_03_cloth_segmentation"
            ]
        
        # ì‹¤ì œ íŒŒì¼ ê²€ìƒ‰
        for search_path in search_paths:
            full_search_path = self.ai_models_root / search_path
            if not full_search_path.exists():
                continue
                
            # ì§ì ‘ íŒŒì¼ í™•ì¸
            direct_path = full_search_path / model_filename
            if direct_path.exists() and direct_path.is_file():
                self.model_cache[cache_key] = direct_path
                return direct_path
                
            # ì¬ê·€ ê²€ìƒ‰ (í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€)
            try:
                for found_file in full_search_path.rglob(model_filename):
                    if found_file.is_file():
                        self.model_cache[cache_key] = found_file
                        return found_file
            except Exception:
                continue
                
        return None
    
    def get_geometric_matching_models(self) -> Dict[str, Path]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ìš© ëª¨ë¸ë“¤ ë§¤í•‘ (step_model_requirements.py ê¸°ì¤€)"""
        result = {}
        
        # step_model_requirements.pyì—ì„œ ì •ì˜ëœ íŒŒì¼ë“¤
        if self.step_request:
            # ì£¼ìš” íŒŒì¼
            primary_file = self.step_request.primary_file  # gmm_final.pth
            primary_path = self.find_model_file(primary_file)
            if primary_path:
                result['gmm'] = primary_path
                self.logger.info(f"âœ… ì£¼ìš” ëª¨ë¸ ë°œê²¬: {primary_file} -> {primary_path.name}")
            
            # ëŒ€ì²´ íŒŒì¼ë“¤
            for alt_file, alt_size in self.step_request.alternative_files:
                alt_path = self.find_model_file(alt_file)
                if alt_path:
                    if alt_file == "tps_network.pth":
                        result['tps'] = alt_path
                    elif alt_file == "sam_vit_h_4b8939.pth":
                        result['sam_shared'] = alt_path
                    elif alt_file == "ViT-L-14.pt":
                        result['vit_large'] = alt_path
                    elif alt_file == "efficientnet_b0_ultra.pth":
                        result['efficientnet'] = alt_path
                    elif "raft" in alt_file.lower():
                        result['raft'] = alt_path
                    
                    self.logger.info(f"âœ… ëŒ€ì²´ ëª¨ë¸ ë°œê²¬: {alt_file} -> {alt_path.name}")
        else:
            # í´ë°±: ê¸°ë³¸ íŒŒì¼ëª…ë“¤
            model_files = {
                'gmm': ['gmm_final.pth', 'gmm.pth', 'geometric_matching.pth'],
                'tps': ['tps_network.pth', 'tps.pth', 'transformation.pth'],
                'sam_shared': ['sam_vit_h_4b8939.pth', 'sam.pth'],
                'vit_large': ['ViT-L-14.pt', 'vit_large.pth'],
                'efficientnet': ['efficientnet_b0_ultra.pth', 'efficientnet.pth']
            }
            
            for model_key, possible_filenames in model_files.items():
                for filename in possible_filenames:
                    found_path = self.find_model_file(filename)
                    if found_path:
                        result[model_key] = found_path
                        self.logger.info(f"âœ… ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_key} -> {found_path.name}")
                        break
        
        return result

# ==============================================
# ğŸ”¥ 6. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (step_model_requirements.py ê¸°ì¤€)
# ==============================================

class RealGMMModel(nn.Module):
    """ì‹¤ì œ GMM (Geometric Matching Module) ëª¨ë¸ - step_model_requirements.py ê¸°ì¤€"""
    
    def __init__(self, input_nc=6, output_nc=2):
        super().__init__()
        
        # U-Net ê¸°ë°˜ GMM ì•„í‚¤í…ì²˜ (VITON/CP-VTON í‘œì¤€)
        self.enc1 = self._conv_block(input_nc, 64, normalize=False)
        self.enc2 = self._conv_block(64, 128)
        self.enc3 = self._conv_block(128, 256)
        self.enc4 = self._conv_block(256, 512)
        self.enc5 = self._conv_block(512, 512)
        self.enc6 = self._conv_block(512, 512)
        self.enc7 = self._conv_block(512, 512)
        self.enc8 = self._conv_block(512, 512, normalize=False)
        
        # Decoder with skip connections
        self.dec1 = self._deconv_block(512, 512, dropout=True)
        self.dec2 = self._deconv_block(1024, 512, dropout=True)
        self.dec3 = self._deconv_block(1024, 512, dropout=True)
        self.dec4 = self._deconv_block(1024, 512)
        self.dec5 = self._deconv_block(1024, 256)
        self.dec6 = self._deconv_block(512, 128)
        self.dec7 = self._deconv_block(256, 64)
        
        # Final layer - step_model_requirements.py ì¶œë ¥ í˜•ì‹ ì¤€ìˆ˜
        self.final = nn.Sequential(
            nn.ConvTranspose2d(128, output_nc, 4, 2, 1),
            nn.Tanh()  # transformation_matrix ì¶œë ¥
        )
        
        # ì¶”ê°€: ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((8, 6))  # (256, 192) -> (8, 6)
        )
        
    def _conv_block(self, in_channels, out_channels, normalize=True):
        """Conv block with LeakyReLU"""
        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1)]
        if normalize:
            layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.LeakyReLU(0.2, True))
        return nn.Sequential(*layers)
    
    def _deconv_block(self, in_channels, out_channels, dropout=False):
        """Deconv block with ReLU"""
        layers = [
            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True)
        ]
        if dropout:
            layers.append(nn.Dropout(0.5))
        return nn.Sequential(*layers)
    
    def forward(self, person_image, clothing_image):
        """ì‹¤ì œ GMM ìˆœì „íŒŒ - step_model_requirements.py ìŠ¤í™ ì¤€ìˆ˜"""
        # input_size (256, 192) - step_model_requirements.py ê¸°ì¤€
        if person_image.shape[-2:] != (256, 192):
            person_image = F.interpolate(person_image, size=(256, 192), mode='bilinear', align_corners=False)
        if clothing_image.shape[-2:] != (256, 192):
            clothing_image = F.interpolate(clothing_image, size=(256, 192), mode='bilinear', align_corners=False)
        
        # 6ì±„ë„ ì…ë ¥ (person RGB + clothing RGB)
        x = torch.cat([person_image, clothing_image], dim=1)
        
        # Encoder
        e1 = self.enc1(x)
        e2 = self.enc2(e1)
        e3 = self.enc3(e2)
        e4 = self.enc4(e3)
        e5 = self.enc5(e4)
        e6 = self.enc6(e5)
        e7 = self.enc7(e6)
        e8 = self.enc8(e7)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(e8)
        
        # Decoder with skip connections
        d1 = self.dec1(e8)
        d2 = self.dec2(torch.cat([d1, e7], dim=1))
        d3 = self.dec3(torch.cat([d2, e6], dim=1))
        d4 = self.dec4(torch.cat([d3, e5], dim=1))
        d5 = self.dec5(torch.cat([d4, e4], dim=1))
        d6 = self.dec6(torch.cat([d5, e3], dim=1))
        d7 = self.dec7(torch.cat([d6, e2], dim=1))
        
        # Final transformation grid
        transformation_grid = self.final(torch.cat([d7, e1], dim=1))
        
        return {
            'transformation_matrix': transformation_grid,
            'transformation_grid': transformation_grid,
            'theta': transformation_grid,
            'features': features,
            'confidence': torch.mean(torch.abs(transformation_grid))
        }

class RealTPSModel(nn.Module):
    """ì‹¤ì œ TPS (Thin Plate Spline) ëª¨ë¸ - step_model_requirements.py ê¸°ì¤€"""
    
    def __init__(self, grid_size=20):
        super().__init__()
        self.grid_size = grid_size
        
        # Feature extractor for TPS parameters
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(6, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((grid_size, grid_size)),
        )
        
        # TPS parameter predictor
        self.tps_predictor = nn.Sequential(
            nn.Conv2d(512, 256, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 2, 1),  # x, y displacement
            nn.Tanh()
        )
        
        # ê³ ê¸‰ TPS ì•Œê³ ë¦¬ì¦˜
        self.advanced_tps = nn.Sequential(
            nn.Conv2d(2, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 2, 3, padding=1),
            nn.Tanh()
        )
        
    def forward(self, person_image, clothing_image, pose_keypoints=None):
        """ì‹¤ì œ TPS ë³€í˜• ê³„ì‚° - step_model_requirements.py ìŠ¤í™ ì¤€ìˆ˜"""
        # input_size (256, 192) ì¤€ìˆ˜
        if person_image.shape[-2:] != (256, 192):
            person_image = F.interpolate(person_image, size=(256, 192), mode='bilinear', align_corners=False)
        if clothing_image.shape[-2:] != (256, 192):
            clothing_image = F.interpolate(clothing_image, size=(256, 192), mode='bilinear', align_corners=False)
        
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        
        # TPS ë³€í˜• íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
        tps_params = self.tps_predictor(features)
        
        # ê³ ê¸‰ TPS ì ìš©
        refined_tps = self.advanced_tps(tps_params)
        
        # ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
        grid = self._generate_transformation_grid(refined_tps)
        
        # Clothing ì´ë¯¸ì§€ì— ë³€í˜• ì ìš©
        warped_clothing = F.grid_sample(
            clothing_image, grid, mode='bilinear', 
            padding_mode='border', align_corners=True
        )
        
        # Flow field ê³„ì‚° (step_model_requirements.py ì¶œë ¥ ìŠ¤í™)
        flow_field = self._compute_flow_field(grid)
        
        return {
            'warped_clothing': warped_clothing,
            'transformation_grid': grid,
            'tps_params': refined_tps,
            'flow_field': flow_field,
            'transformation_matrix': self._grid_to_matrix(grid)
        }
    
    def _generate_transformation_grid(self, tps_params):
        """TPS ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„± - ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜"""
        batch_size, _, height, width = tps_params.shape
        device = tps_params.device
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, 256, device=device),  # step_model_requirements.py ê¸°ì¤€
            torch.linspace(-1, 1, 192, device=device),
            indexing='ij'
        )
        base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        
        # TPS ë³€í˜• ì ìš© (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
        tps_displacement = F.interpolate(tps_params, size=(256, 192), mode='bilinear', align_corners=False)
        tps_displacement = tps_displacement.permute(0, 2, 3, 1)
        
        # ìŠ¤ë¬´ë”© ì ìš©
        displacement_smoothed = self._smooth_displacement(tps_displacement)
        
        transformed_grid = base_grid + displacement_smoothed * 0.1
        
        return transformed_grid
    
    def _smooth_displacement(self, displacement):
        """ë³€í˜• í•„ë“œ ìŠ¤ë¬´ë”©"""
        if SCIPY_AVAILABLE:
            # ê°€ìš°ì‹œì•ˆ ìŠ¤ë¬´ë”© ì ìš©
            smoothed = torch.zeros_like(displacement)
            for b in range(displacement.shape[0]):
                for c in range(displacement.shape[-1]):
                    disp_np = displacement[b, :, :, c].cpu().numpy()
                    smoothed_np = ndimage.gaussian_filter(disp_np, sigma=1.0)
                    smoothed[b, :, :, c] = torch.from_numpy(smoothed_np)
            return smoothed
        else:
            # PyTorch ê¸°ë°˜ ìŠ¤ë¬´ë”©
            kernel = torch.ones(1, 1, 3, 3, device=displacement.device) / 9
            smoothed = F.conv2d(displacement.permute(0, 3, 1, 2), kernel, padding=1)
            return smoothed.permute(0, 2, 3, 1)
    
    def _compute_flow_field(self, grid):
        """Flow field ê³„ì‚°"""
        batch_size, height, width, _ = grid.shape
        device = grid.device
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œì™€ì˜ ì°¨ì´ ê³„ì‚°
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=device),
            torch.linspace(-1, 1, width, device=device),
            indexing='ij'
        )
        base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        
        flow_field = (grid - base_grid) * 50.0  # í”½ì…€ ë‹¨ìœ„ë¡œ ë³€í™˜
        
        return flow_field.permute(0, 3, 1, 2)  # (B, 2, H, W)
    
    def _grid_to_matrix(self, grid):
        """ê·¸ë¦¬ë“œë¥¼ ë³€í˜• í–‰ë ¬ë¡œ ë³€í™˜"""
        batch_size = grid.shape[0]
        device = grid.device
        
        # ë‹¨ìˆœí™”ëœ ë³€í˜• í–‰ë ¬ (2x3)
        matrix = torch.zeros(batch_size, 2, 3, device=device)
        
        # ê·¸ë¦¬ë“œ ì¤‘ì•™ ì˜ì—­ì—ì„œ ë³€í˜• íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        center_h, center_w = grid.shape[1] // 2, grid.shape[2] // 2
        center_region = grid[:, center_h-10:center_h+10, center_w-10:center_w+10, :]
        
        # í‰ê·  ë³€í˜• ê³„ì‚°
        mean_transform = torch.mean(center_region, dim=(1, 2))
        
        matrix[:, 0, 0] = 1.0 + mean_transform[:, 0] * 0.1
        matrix[:, 1, 1] = 1.0 + mean_transform[:, 1] * 0.1
        matrix[:, 0, 2] = mean_transform[:, 0]
        matrix[:, 1, 2] = mean_transform[:, 1]
        
        return matrix

class RealSAMModel(nn.Module):
    """ì‹¤ì œ SAM (Segment Anything Model) ëª¨ë¸ - step_model_requirements.py ê¸°ì¤€ (ê³µìœ )"""
    
    def __init__(self, encoder_embed_dim=768, encoder_depth=12, encoder_num_heads=12):
        super().__init__()
        
        # ViT-based image encoder (ê²½ëŸ‰í™”)
        self.patch_embed = nn.Conv2d(3, encoder_embed_dim, kernel_size=16, stride=16)
        self.pos_embed = nn.Parameter(torch.zeros(1, 256, encoder_embed_dim))
        
        # Transformer encoder layers
        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                encoder_embed_dim, encoder_num_heads, 
                dim_feedforward=encoder_embed_dim * 4,
                dropout=0.0, activation='gelu'
            )
            for _ in range(encoder_depth)
        ])
        
        # Mask decoder
        self.mask_decoder = nn.Sequential(
            nn.ConvTranspose2d(encoder_embed_dim, 256, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 3, 1, 1),
            nn.Sigmoid()
        )
        
    def forward(self, image):
        """ì‹¤ì œ SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ - step_model_requirements.py ê³µìœ  ëª¨ë¸"""
        batch_size = image.size(0)
        
        # ì…ë ¥ í¬ê¸° ì¡°ì • (step_model_requirements.pyì—ì„œ ê³µìœ  ëª©ì )
        if image.shape[-2:] != (1024, 1024):
            image = F.interpolate(image, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        # Patch embedding
        x = self.patch_embed(image)
        x = x.flatten(2).transpose(1, 2)
        
        # Add positional embedding
        x = x + self.pos_embed
        
        # Transformer encoder
        for layer in self.encoder_layers:
            x = layer(x)
        
        # Reshape for decoder
        h, w = image.size(2) // 16, image.size(3) // 16
        x = x.transpose(1, 2).reshape(batch_size, -1, h, w)
        
        # Mask decoder
        mask = self.mask_decoder(x)
        
        return {
            'mask': mask,
            'binary_mask': (mask > 0.5).float(),
            'image_features': x,
            'confidence_map': mask
        }

class RealViTModel(nn.Module):
    """ì‹¤ì œ ViT ëª¨ë¸ - íŠ¹ì§• ì¶”ì¶œìš© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤)"""
    
    def __init__(self, image_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12):
        super().__init__()
        
        num_patches = (image_size // patch_size) ** 2
        
        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                embed_dim, num_heads, dim_feedforward=embed_dim * 4,
                dropout=0.1, activation='gelu'
            ),
            num_layers=depth
        )
        
        self.norm = nn.LayerNorm(embed_dim)
        
    def forward(self, x):
        """ViT íŠ¹ì§• ì¶”ì¶œ"""
        batch_size = x.size(0)
        
        # Patch embedding
        x = self.patch_embed(x)  # (B, embed_dim, H/16, W/16)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)
        
        # Add cls token and position embedding
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = x + self.pos_embed
        
        # Transformer
        x = self.transformer(x)
        x = self.norm(x)
        
        return {
            'cls_token': x[:, 0],  # Classification token
            'patch_tokens': x[:, 1:],  # Patch tokens
            'features': x
        }

class RealEfficientNetModel(nn.Module):
    """ì‹¤ì œ EfficientNet ëª¨ë¸ - íŠ¹ì§• ì¶”ì¶œìš© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤)"""
    
    def __init__(self, num_classes=1000):
        super().__init__()
        
        # EfficientNet-B0 ê¸°ë³¸ êµ¬ì¡°
        self.stem = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.SiLU(inplace=True)
        )
        
        # MBConv blocks (ê°„ì†Œí™”)
        self.blocks = nn.Sequential(
            self._make_mbconv_block(32, 16, 1, 1, 1),
            self._make_mbconv_block(16, 24, 6, 2, 2),
            self._make_mbconv_block(24, 40, 6, 2, 2),
            self._make_mbconv_block(40, 80, 6, 2, 3),
            self._make_mbconv_block(80, 112, 6, 1, 3),
            self._make_mbconv_block(112, 192, 6, 2, 4),
            self._make_mbconv_block(192, 320, 6, 1, 1),
        )
        
        self.head = nn.Sequential(
            nn.Conv2d(320, 1280, 1, bias=False),
            nn.BatchNorm2d(1280),
            nn.SiLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(1280, num_classes)
        )
        
    def _make_mbconv_block(self, in_channels, out_channels, expand_ratio, stride, num_layers):
        """MBConv ë¸”ë¡ ìƒì„±"""
        layers = []
        for i in range(num_layers):
            layers.append(
                nn.Sequential(
                    # Depthwise conv
                    nn.Conv2d(in_channels if i == 0 else out_channels, 
                             (in_channels if i == 0 else out_channels) * expand_ratio, 
                             3, stride=stride if i == 0 else 1, padding=1, 
                             groups=in_channels if i == 0 else out_channels, bias=False),
                    nn.BatchNorm2d((in_channels if i == 0 else out_channels) * expand_ratio),
                    nn.SiLU(inplace=True),
                    # Pointwise conv
                    nn.Conv2d((in_channels if i == 0 else out_channels) * expand_ratio, 
                             out_channels, 1, bias=False),
                    nn.BatchNorm2d(out_channels),
                )
            )
        return nn.Sequential(*layers)
        
    def forward(self, x):
        """EfficientNet íŠ¹ì§• ì¶”ì¶œ"""
        x = self.stem(x)
        x = self.blocks(x)
        features = x  # ì¤‘ê°„ íŠ¹ì§• ì €ì¥
        x = self.head(x)
        
        return {
            'logits': x,
            'features': features
        }

# ==============================================
# ğŸ”¥ 7. ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜
# ==============================================

class AdvancedGeometricMatcher:
    """ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ - AI ê°•í™”"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def extract_keypoints_ai(self, image: torch.Tensor, pose_data: Optional[torch.Tensor] = None) -> torch.Tensor:
        """AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        if not TORCH_AVAILABLE:
            return self._fallback_keypoints(image)
        
        with torch.no_grad():
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if image.dim() == 3:
                image = image.unsqueeze(0)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ íƒì§€
            gray = torch.mean(image, dim=1, keepdim=True)
            
            # Sobel í•„í„° ì ìš©
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                 dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                 dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
            
            grad_x = F.conv2d(gray, sobel_x, padding=1)
            grad_y = F.conv2d(gray, sobel_y, padding=1)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ê³„ì‚°
            gradient_magnitude = torch.sqrt(grad_x**2 + grad_y**2)
            
            # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ìƒìœ„ 18ê°œ)
            batch_size, _, height, width = gradient_magnitude.shape
            flat_grad = gradient_magnitude.view(batch_size, -1)
            
            # ìƒìœ„ 18ê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
            _, top_indices = torch.topk(flat_grad, k=18, dim=1)
            
            # 2D ì¢Œí‘œë¡œ ë³€í™˜
            keypoints = torch.zeros(batch_size, 18, 2, device=self.device)
            for b in range(batch_size):
                for i, idx in enumerate(top_indices[b]):
                    y = idx // width
                    x = idx % width
                    keypoints[b, i, 0] = x.float()
                    keypoints[b, i, 1] = y.float()
            
            # í¬ì¦ˆ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê²°í•©
            if pose_data is not None and pose_data.shape[-1] == 2:
                # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ì™€ íƒì§€ëœ í‚¤í¬ì¸íŠ¸ ê°€ì¤‘ í‰ê· 
                alpha = 0.7  # í¬ì¦ˆ ë°ì´í„° ê°€ì¤‘ì¹˜
                if pose_data.shape[1] == 18:
                    keypoints = alpha * pose_data + (1 - alpha) * keypoints
            
            return keypoints
    
    def _fallback_keypoints(self, image: torch.Tensor) -> torch.Tensor:
        """í´ë°± í‚¤í¬ì¸íŠ¸ (PyTorch ì—†ëŠ” ê²½ìš°)"""
        batch_size = image.shape[0] if image.dim() == 4 else 1
        height, width = image.shape[-2:]
        
        # ê· ë“± ë¶„í¬ í‚¤í¬ì¸íŠ¸
        keypoints = torch.zeros(batch_size, 18, 2)
        
        # ì‹ ì²´ ì£¼ìš” ë¶€ìœ„ ì¶”ì • ìœ„ì¹˜
        body_parts = [
            (0.5, 0.1),   # ë¨¸ë¦¬ ì¤‘ì•™
            (0.4, 0.15),  # ëª© ì¢Œ
            (0.6, 0.15),  # ëª© ìš°
            (0.3, 0.3),   # ì–´ê¹¨ ì¢Œ
            (0.7, 0.3),   # ì–´ê¹¨ ìš°
            (0.25, 0.5),  # íŒ”ê¿ˆì¹˜ ì¢Œ
            (0.75, 0.5),  # íŒ”ê¿ˆì¹˜ ìš°
            (0.2, 0.7),   # ì†ëª© ì¢Œ
            (0.8, 0.7),   # ì†ëª© ìš°
            (0.4, 0.6),   # í—ˆë¦¬ ì¢Œ
            (0.6, 0.6),   # í—ˆë¦¬ ìš°
            (0.35, 0.8),  # ë¬´ë¦ ì¢Œ
            (0.65, 0.8),  # ë¬´ë¦ ìš°
            (0.3, 1.0),   # ë°œëª© ì¢Œ
            (0.7, 1.0),   # ë°œëª© ìš°
            (0.5, 0.05),  # ë¨¸ë¦¬ ìƒë‹¨
            (0.15, 0.75), # ì† ì¢Œ
            (0.85, 0.75)  # ì† ìš°
        ]
        
        for b in range(batch_size):
            for i, (x_ratio, y_ratio) in enumerate(body_parts):
                keypoints[b, i, 0] = x_ratio * width
                keypoints[b, i, 1] = y_ratio * height
        
        return keypoints
    
    def compute_transformation_matrix_ai(self, src_keypoints: torch.Tensor, 
                                       dst_keypoints: torch.Tensor) -> torch.Tensor:
        """AI ê°•í™” ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size = src_keypoints.shape[0]
        device = src_keypoints.device
        
        if SCIPY_AVAILABLE and src_keypoints.shape[0] == 1:
            # Scipy ê¸°ë°˜ ê³ ê¸‰ ê³„ì‚° (ë‹¨ì¼ ë°°ì¹˜)
            return self._compute_with_scipy(src_keypoints[0], dst_keypoints[0])
        else:
            # PyTorch ê¸°ë°˜ ê³„ì‚°
            return self._compute_with_pytorch(src_keypoints, dst_keypoints)
    
    def _compute_with_scipy(self, src_pts: torch.Tensor, dst_pts: torch.Tensor) -> torch.Tensor:
        """Scipy ê¸°ë°˜ ê³ ê¸‰ ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        try:
            src_np = src_pts.cpu().numpy()
            dst_np = dst_pts.cpu().numpy()
            
            # Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜•
            def objective(params):
                # ë³€í˜• íŒŒë¼ë¯¸í„°: [tx, ty, scale, rotation]
                tx, ty, scale, rotation = params
                
                # ë³€í˜• í–‰ë ¬ ìƒì„±
                cos_r, sin_r = np.cos(rotation), np.sin(rotation)
                transform_matrix = np.array([
                    [scale * cos_r, -scale * sin_r, tx],
                    [scale * sin_r, scale * cos_r, ty]
                ])
                
                # ë³€í˜• ì ìš©
                src_homogeneous = np.column_stack([src_np, np.ones(len(src_np))])
                transformed = src_homogeneous @ transform_matrix.T
                
                # ê±°ë¦¬ ì˜¤ì°¨ ê³„ì‚°
                error = np.sum((transformed - dst_np) ** 2)
                return error
            
            # ìµœì í™” ì‹¤í–‰
            initial_params = [0, 0, 1, 0]  # tx, ty, scale, rotation
            result = minimize(objective, initial_params, method='BFGS')
            
            if result.success:
                tx, ty, scale, rotation = result.x
                cos_r, sin_r = np.cos(rotation), np.sin(rotation)
                
                transform_matrix = np.array([
                    [scale * cos_r, -scale * sin_r, tx],
                    [scale * sin_r, scale * cos_r, ty]
                ])
            else:
                # ì‹¤íŒ¨ ì‹œ ë‹¨ìœ„ í–‰ë ¬
                transform_matrix = np.array([[1, 0, 0], [0, 1, 0]])
            
            return torch.from_numpy(transform_matrix).float().to(src_pts.device).unsqueeze(0)
            
        except Exception as e:
            self.logger.warning(f"Scipy ë³€í˜• ê³„ì‚° ì‹¤íŒ¨: {e}")
            return self._compute_with_pytorch(src_pts.unsqueeze(0), dst_pts.unsqueeze(0))
    
    def _compute_with_pytorch(self, src_keypoints: torch.Tensor, 
                            dst_keypoints: torch.Tensor) -> torch.Tensor:
        """PyTorch ê¸°ë°˜ ë³€í˜• í–‰ë ¬ ê³„ì‚°"""
        batch_size = src_keypoints.shape[0]
        device = src_keypoints.device
        
        # ì¤‘ì‹¬ì  ê³„ì‚°
        src_center = torch.mean(src_keypoints, dim=1, keepdim=True)
        dst_center = torch.mean(dst_keypoints, dim=1, keepdim=True)
        
        # ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
        src_centered = src_keypoints - src_center
        dst_centered = dst_keypoints - dst_center
        
        # ìŠ¤ì¼€ì¼ ê³„ì‚°
        src_scale = torch.norm(src_centered, dim=2).mean(dim=1, keepdim=True)
        dst_scale = torch.norm(dst_centered, dim=2).mean(dim=1, keepdim=True)
        scale = dst_scale / (src_scale + 1e-8)
        
        # íšŒì „ ê³„ì‚° (SVD ê¸°ë°˜)
        try:
            H = torch.bmm(src_centered.transpose(1, 2), dst_centered)
            U, S, V = torch.svd(H)
            R = torch.bmm(V, U.transpose(1, 2))
            
            # ë°˜ì‚¬ ë°©ì§€
            det = torch.det(R)
            for b in range(batch_size):
                if det[b] < 0:
                    V[b, :, -1] *= -1
                    R[b] = torch.mm(V[b], U[b].T)
        except:
            # SVD ì‹¤íŒ¨ ì‹œ ë‹¨ìœ„ í–‰ë ¬
            R = torch.eye(2, device=device).unsqueeze(0).repeat(batch_size, 1, 1)
        
        # ë³€í˜• í–‰ë ¬ êµ¬ì„±
        transform_matrix = torch.zeros(batch_size, 2, 3, device=device)
        transform_matrix[:, :2, :2] = scale.unsqueeze(-1) * R
        transform_matrix[:, :, 2] = (dst_center - torch.bmm(
            scale.unsqueeze(-1) * R, src_center.transpose(1, 2)
        ).transpose(1, 2)).squeeze(1)
        
        return transform_matrix
    
    def apply_geometric_matching(self, clothing_image: torch.Tensor, 
                               transformation_matrix: torch.Tensor,
                               flow_field: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì ìš©"""
        if not TORCH_AVAILABLE:
            return clothing_image
        
        batch_size, channels, height, width = clothing_image.shape
        device = clothing_image.device
        
        # ì–´í•€ ë³€í˜• ì ìš©
        if transformation_matrix.shape[-1] == 3:  # 2x3 ì–´í•€ í–‰ë ¬
            grid = F.affine_grid(transformation_matrix, clothing_image.size(), align_corners=False)
            warped = F.grid_sample(clothing_image, grid, mode='bilinear', 
                                 padding_mode='border', align_corners=False)
        else:
            warped = clothing_image
        
        # Flow field ì¶”ê°€ ì ìš©
        if flow_field is not None:
            # Flow fieldë¥¼ ê·¸ë¦¬ë“œë¡œ ë³€í™˜
            flow_grid = self._flow_to_grid(flow_field, height, width)
            warped = F.grid_sample(warped, flow_grid, mode='bilinear', 
                                 padding_mode='border', align_corners=False)
        
        return warped
    
    def _flow_to_grid(self, flow_field: torch.Tensor, height: int, width: int) -> torch.Tensor:
        """Flow fieldë¥¼ ìƒ˜í”Œë§ ê·¸ë¦¬ë“œë¡œ ë³€í™˜"""
        batch_size = flow_field.shape[0]
        device = flow_field.device
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y, x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=device),
            torch.linspace(-1, 1, width, device=device),
            indexing='ij'
        )
        base_grid = torch.stack([x, y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        
        # Flow field í¬ê¸° ì¡°ì •
        if flow_field.shape[-2:] != (height, width):
            flow_field = F.interpolate(flow_field, size=(height, width), 
                                     mode='bilinear', align_corners=False)
        
        # Flow fieldë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        flow_normalized = flow_field.permute(0, 2, 3, 1)  # (B, H, W, 2)
        flow_normalized[:, :, :, 0] /= width / 2.0   # x ì •ê·œí™”
        flow_normalized[:, :, :, 1] /= height / 2.0  # y ì •ê·œí™”
        
        # ìµœì¢… ê·¸ë¦¬ë“œ
        grid = base_grid + flow_normalized
        
        return grid

# ==============================================
# ğŸ”¥ 8. ë©”ì¸ GeometricMatchingStep í´ë˜ìŠ¤ (BaseStepMixin ë™ê¸° í˜¸í™˜)
# ==============================================

@dataclass
class ProcessingStatus:
    """ì²˜ë¦¬ ìƒíƒœ ì¶”ì  - step_model_requirements.py ê¸°ì¤€"""
    initialized: bool = False
    models_loaded: bool = False
    dependencies_injected: bool = False
    processing_active: bool = False
    error_count: int = 0
    last_error: Optional[str] = None
    ai_model_calls: int = 0
    model_creation_success: bool = False
    requirements_compatible: bool = False
    detailed_data_spec_loaded: bool = False

class GeometricMatchingStep(BaseStepMixin):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ Step - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ì¶”ë¡  ê°•í™” + ë™ê¸° ì²˜ë¦¬"""
    
    def __init__(self, **kwargs):
        """BaseStepMixin í˜¸í™˜ ìƒì„±ì - step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë°˜ì˜"""
        super().__init__(**kwargs)
        
        # step_model_requirements.py ê¸°ì¤€ ê¸°ë³¸ ì†ì„±
        self.step_name = "GeometricMatchingStep"
        self.step_id = 4
        self.device = self._force_mps_device(kwargs.get('device', 'auto'))
        
        # step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ
        self.step_request = get_step_model_request()
        self._load_requirements_config()
        
        # ìƒíƒœ ê´€ë¦¬
        self.status = ProcessingStatus()
        
        # ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ (step_model_requirements.py ê¸°ì¤€)
        ai_models_root = kwargs.get('ai_models_root', 'ai_models')
        self.model_mapper = EnhancedModelPathMapper(ai_models_root)
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤ (step_model_requirements.py ai_class ê¸°ì¤€)
        self.gmm_model: Optional[RealGMMModel] = None  # ai_class="RealGMMModel"
        self.tps_model: Optional[RealTPSModel] = None
        self.sam_model: Optional[RealSAMModel] = None  # ê³µìœ  ëª¨ë¸
        self.vit_model: Optional[RealViTModel] = None  # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ëª¨ë¸
        self.efficientnet_model: Optional[RealEfficientNetModel] = None  # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ëª¨ë¸
        
        # ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜
        self.geometric_matcher = AdvancedGeometricMatcher(self.device)
        
        # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ì†ì„±ë“¤ ë³´ì¡´
        self.geometric_model = None  # ê¸°ì¡´ í˜¸í™˜ì„±
        self.model_interface = None  # ê¸°ì¡´ ê¸°ëŠ¥
        self.model_paths = {}  # ê¸°ì¡´ ê¸°ëŠ¥
        
        # ì˜ì¡´ì„± ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self._initialize_dependency_manager()
        
        # í†µê³„ ì´ˆê¸°í™”
        self._init_statistics()
        
        self.logger.info(f"âœ… GeometricMatchingStep ìƒì„± ì™„ë£Œ - Device: {self.device}")
        if self.step_request:
            self.logger.info(f"ğŸ“‹ step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            self.status.requirements_compatible = True
    
    def _load_requirements_config(self):
        """step_model_requirements.py ìš”êµ¬ì‚¬í•­ ì„¤ì • ë¡œë“œ"""
        if self.step_request:
            # step_model_requirements.py ê¸°ì¤€ ì„¤ì •
            self.matching_config = {
                'method': 'real_ai_models',  # ai_class ê¸°ì¤€
                'input_size': self.step_request.input_size,  # (256, 192)
                'output_format': self.step_request.output_format,  # "transformation_matrix"
                'model_architecture': self.step_request.model_architecture,  # "gmm_tps"
                'batch_size': self.step_request.batch_size,  # 2
                'memory_fraction': self.step_request.memory_fraction,  # 0.2
                'device': self.step_request.device,  # "auto"
                'precision': self.step_request.precision,  # "fp16"
                'use_real_models': True,
                'detailed_data_spec': True
            }
            
            # DetailedDataSpec ë¡œë“œ
            if hasattr(self.step_request, 'data_spec'):
                self.data_spec = self.step_request.data_spec
                self.status.detailed_data_spec_loaded = True
                self.logger.info("âœ… DetailedDataSpec ë¡œë“œ ì™„ë£Œ")
            else:
                self.data_spec = None
                self.logger.warning("âš ï¸ DetailedDataSpec ì—†ìŒ")
        else:
            # í´ë°± ì„¤ì •
            self.matching_config = {
                'method': 'real_ai_models',
                'input_size': (256, 192),
                'output_format': 'transformation_matrix',
                'batch_size': 2,
                'device': self.device,
                'use_real_models': True
            }
            self.data_spec = None
            self.logger.warning("âš ï¸ step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ì„¤ì • ì‚¬ìš©")
    
    def _initialize_dependency_manager(self):
        """ì˜ì¡´ì„± ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        try:
            if not hasattr(self, 'dependency_manager') or self.dependency_manager is None:
                self.dependency_manager = UnifiedDependencyManager()
                
            # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
            if hasattr(self.dependency_manager, 'auto_inject_dependencies'):
                success = self.dependency_manager.auto_inject_dependencies()
                if success:
                    self.status.dependencies_injected = True
                    self.logger.info("âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.dependency_manager = self._create_safe_dependency_manager()
    
    def _create_safe_dependency_manager(self):
        """ì•ˆì „í•œ ì˜ì¡´ì„± ë§¤ë‹ˆì € ìƒì„±"""
        class SafeDependencyManager:
            def __init__(self):
                self.model_loader = None
                self.memory_manager = None
                
            def set_model_loader(self, model_loader):
                self.model_loader = model_loader
                return True
                
            def auto_inject_dependencies(self):
                return False
        
        return SafeDependencyManager()
    
    def _init_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.statistics = {
            'total_processed': 0,
            'successful_matches': 0,
            'average_quality': 0.0,
            'total_processing_time': 0.0,
            'ai_model_calls': 0,
            'error_count': 0,
            'model_creation_success': False,
            'real_ai_models_used': True,
            'requirements_compatible': self.status.requirements_compatible
        }
    
    def _force_mps_device(self, device: str) -> str:
        """MPS ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì • - step_model_requirements.py device ê¸°ì¤€"""
        try:
            import torch
            import platform
            
            if device == "auto":
                if (platform.system() == 'Darwin' and 
                    platform.machine() == 'arm64' and 
                    torch.backends.mps.is_available()):
                    self.logger.info("ğŸ GeometricMatchingStep: MPS ìë™ í™œì„±í™”")
                    return 'mps'
                elif torch.cuda.is_available():
                    return 'cuda'
                else:
                    return 'cpu'
            return device
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return 'cpu'
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v19.1 í˜¸í™˜ - _run_ai_inference ë™ê¸° ì²˜ë¦¬
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ìˆœìˆ˜ AI ë¡œì§ êµ¬í˜„ - ë™ê¸° ì²˜ë¦¬ (BaseStepMixin v19.1 í˜¸í™˜)
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ ì…ë ¥
                - 'person_image': ì „ì²˜ë¦¬ëœ ì‚¬ëŒ ì´ë¯¸ì§€ (PIL.Image ë˜ëŠ” torch.Tensor)
                - 'clothing_image': ì „ì²˜ë¦¬ëœ ì˜ë¥˜ ì´ë¯¸ì§€ (PIL.Image ë˜ëŠ” torch.Tensor)
                - 'from_step_XX': ì´ì „ Stepì˜ ì¶œë ¥ ë°ì´í„°
                - ê¸°íƒ€ DetailedDataSpecì— ì •ì˜ëœ ì…ë ¥
        
        Returns:
            AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ (BaseStepMixinì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} AI ì¶”ë¡  ì‹œì‘ (ë™ê¸° ì²˜ë¦¬)")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'person_image' not in processed_input and 'image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: person_image ë˜ëŠ” image")
            
            if 'clothing_image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: clothing_image")
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            person_image = processed_input.get('person_image') or processed_input.get('image')
            clothing_image = processed_input.get('clothing_image')
            pose_keypoints = processed_input.get('pose_keypoints')
            
            # 3. ì´ì „ Step ë°ì´í„° í™œìš©
            previous_data = {}
            for key, value in processed_input.items():
                if key.startswith('from_step_'):
                    previous_data[key] = value
            
            # 4. ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ - ê°•í™”ëœ ê¸°í•˜í•™ì  ë§¤ì¹­
            ai_result = self._execute_enhanced_geometric_matching(
                person_image, clothing_image, pose_keypoints, previous_data
            )
            
            # 5. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ë¶„ì„
            processed_output = self._post_process_ai_output(ai_result)
            
            # 6. step_model_requirements.py í˜¸í™˜ ì¶œë ¥ í˜•ì‹ êµ¬ì„±
            final_result = {
                'transformation_matrix': processed_output.get('transformation_matrix'),
                'warped_clothing': processed_output.get('warped_clothing'),
                'flow_field': processed_output.get('flow_field'),
                'keypoints': processed_output.get('keypoints', []),
                'confidence': processed_output.get('confidence', 0.85),
                'quality_score': processed_output.get('quality_score', 0.8),
                'ai_enhanced': True,
                'requirements_compatible': True,
                'geometric_features': processed_output.get('geometric_features', {}),
                'metadata': {
                    'model_used': 'enhanced_ai_geometric_matching',
                    'processing_method': 'real_ai_models',
                    'device': self.device,
                    'models_loaded': self.status.models_loaded
                }
            }
            
            self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ - í’ˆì§ˆ: {final_result['confidence']:.3f}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    def _execute_enhanced_geometric_matching(self, person_image: Any, clothing_image: Any, 
                                           pose_keypoints: Optional[Any], 
                                           previous_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ê°•í™”ëœ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤í–‰"""
        try:
            result = {}
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í…ì„œ ë³€í™˜
            person_tensor = self._prepare_image_tensor(person_image)
            clothing_tensor = self._prepare_image_tensor(clothing_image)
            
            # 1. AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            if pose_keypoints is not None:
                keypoints_tensor = self._prepare_keypoints_tensor(pose_keypoints)
            else:
                keypoints_tensor = self.geometric_matcher.extract_keypoints_ai(person_tensor)
            
            person_keypoints = keypoints_tensor
            clothing_keypoints = self.geometric_matcher.extract_keypoints_ai(clothing_tensor, keypoints_tensor)
            
            result['keypoints'] = keypoints_tensor.cpu().numpy().tolist()
            
            # 2. GMM ëª¨ë¸ì„ í†µí•œ ë³€í˜• ê·¸ë¦¬ë“œ ìƒì„±
            if self.gmm_model is not None:
                gmm_output = self.gmm_model(person_tensor, clothing_tensor)
                transformation_grid = gmm_output['transformation_grid']
                result['transformation_matrix'] = transformation_grid
                result['confidence'] = gmm_output.get('confidence', 0.8)
                result['geometric_features'] = gmm_output.get('features')
                self.logger.info("âœ… GMM ëª¨ë¸ AI ì¶”ë¡  ì™„ë£Œ")
            else:
                # AI ê¸°ë°˜ ë³€í˜• í–‰ë ¬ ê³„ì‚°
                transformation_matrix = self.geometric_matcher.compute_transformation_matrix_ai(
                    clothing_keypoints, person_keypoints
                )
                result['transformation_matrix'] = transformation_matrix
                result['confidence'] = 0.75
                self.logger.info("âœ… AdvancedGeometricMatcher AI ê³„ì‚° ì™„ë£Œ")
            
            # 3. TPS ëª¨ë¸ì„ í†µí•œ ì •ë°€ ì›Œí•‘
            if self.tps_model is not None:
                tps_output = self.tps_model(person_tensor, clothing_tensor, keypoints_tensor)
                warped_clothing = tps_output['warped_clothing']
                flow_field = tps_output['flow_field']
                result['warped_clothing'] = warped_clothing
                result['flow_field'] = flow_field
                result['tps_params'] = tps_output.get('tps_params')
                self.logger.info("âœ… TPS ëª¨ë¸ AI ì¶”ë¡  ì™„ë£Œ")
            else:
                # ê¸°ë³¸ ì–´í•€ ë³€í˜• ì ìš©
                if 'transformation_matrix' in result:
                    warped_clothing = self.geometric_matcher.apply_geometric_matching(
                        clothing_tensor, result['transformation_matrix']
                    )
                    result['warped_clothing'] = warped_clothing
                    # Flow field ì‹œë®¬ë ˆì´ì…˜
                    result['flow_field'] = self._simulate_flow_field(clothing_tensor.shape)
                self.logger.info("âœ… ê¸°ë³¸ ë³€í˜• ì ìš© ì™„ë£Œ")
            
            # 4. SAM ëª¨ë¸ì„ í†µí•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ì œ (ê³µìœ  ëª¨ë¸)
            if self.sam_model is not None and 'warped_clothing' in result:
                sam_output = self.sam_model(result['warped_clothing'])
                refined_mask = sam_output['mask']
                # ë§ˆìŠ¤í¬ ì ìš©í•˜ì—¬ ê²°ê³¼ ì •ì œ
                result['warped_clothing'] = result['warped_clothing'] * refined_mask
                result['segmentation_mask'] = refined_mask
                self.logger.info("âœ… SAM ëª¨ë¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ì œ ì™„ë£Œ")
            
            # 5. ViT ë° EfficientNet íŠ¹ì§• ì¶”ì¶œ (ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ ë³´ì¡´)
            if self.vit_model is not None:
                vit_features = self.vit_model(person_tensor)
                result['vit_features'] = vit_features
                self.logger.info("âœ… ViT íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
            
            if self.efficientnet_model is not None:
                efficientnet_features = self.efficientnet_model(clothing_tensor)
                result['efficientnet_features'] = efficientnet_features
                self.logger.info("âœ… EfficientNet íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
            
            # 6. í’ˆì§ˆ í‰ê°€
            quality_score = self._compute_matching_quality(result)
            result['quality_score'] = quality_score
            
            # 7. ê³ ê¸‰ í›„ì²˜ë¦¬ (ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ ë³´ì¡´)
            if 'warped_clothing' in result:
                result['warped_clothing'] = self._apply_advanced_postprocessing(result['warped_clothing'])
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê°•í™”ëœ AI ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_image_tensor(self, image: Any) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # PIL Image ì²˜ë¦¬
        if PIL_AVAILABLE and isinstance(image, Image.Image):
            image_array = np.array(image).astype(np.float32) / 255.0
            if len(image_array.shape) == 3:
                image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
            tensor = torch.from_numpy(image_array).unsqueeze(0).to(self.device)
            
        # NumPy ë°°ì—´ ì²˜ë¦¬
        elif isinstance(image, np.ndarray):
            image_array = image.astype(np.float32)
            if image_array.max() > 1.0:
                image_array = image_array / 255.0
            if len(image_array.shape) == 3:
                image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
            tensor = torch.from_numpy(image_array).unsqueeze(0).to(self.device)
            
        # ì´ë¯¸ í…ì„œì¸ ê²½ìš°
        elif torch.is_tensor(image):
            tensor = image.to(self.device)
            if tensor.dim() == 3:
                tensor = tensor.unsqueeze(0)
                
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # step_model_requirements.py ê¸°ì¤€ í¬ê¸° ì¡°ì •
        target_size = self.matching_config.get('input_size', (256, 192))
        if tensor.shape[-2:] != target_size:
            tensor = F.interpolate(tensor, size=target_size, mode='bilinear', align_corners=False)
        
        return tensor
    
    def _prepare_keypoints_tensor(self, keypoints: Any) -> torch.Tensor:
        """í‚¤í¬ì¸íŠ¸ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if isinstance(keypoints, np.ndarray):
            keypoints = torch.from_numpy(keypoints)
        elif isinstance(keypoints, list):
            keypoints = torch.tensor(keypoints)
        elif not isinstance(keypoints, torch.Tensor):
            # ë”ë¯¸ í‚¤í¬ì¸íŠ¸ ìƒì„±
            keypoints = torch.zeros(1, 18, 2)
        
        if keypoints.dim() == 2:
            keypoints = keypoints.unsqueeze(0)
        
        return keypoints.float().to(self.device)
    
    def _simulate_flow_field(self, image_shape: Tuple[int, ...]) -> torch.Tensor:
        """Flow field ì‹œë®¬ë ˆì´ì…˜"""
        batch_size, channels, height, width = image_shape
        
        # ê°„ë‹¨í•œ flow field ìƒì„±
        flow_field = torch.zeros(batch_size, 2, height, width, device=self.device)
        
        # ì¤‘ì‹¬ì—ì„œ ë°”ê¹¥ìª½ìœ¼ë¡œ í–¥í•˜ëŠ” flow
        center_h, center_w = height // 2, width // 2
        for h in range(height):
            for w in range(width):
                flow_field[:, 0, h, w] = (w - center_w) * 0.01  # x ë°©í–¥
                flow_field[:, 1, h, w] = (h - center_h) * 0.01  # y ë°©í–¥
        
        return flow_field
    
    def _compute_matching_quality(self, result: Dict[str, Any]) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ ê³„ì‚°"""
        quality_factors = []
        
        # ë³€í˜• ì¼ê´€ì„± í™•ì¸
        if 'transformation_matrix' in result:
            transform = result['transformation_matrix']
            if isinstance(transform, torch.Tensor):
                # ë³€í˜• í–‰ë ¬ì˜ ì¡°ê±´ìˆ˜ í™•ì¸ (ì•ˆì •ì„± ì§€í‘œ)
                try:
                    if transform.dim() >= 3 and transform.shape[-1] >= 2:
                        det = torch.det(transform[:, :2, :2])
                        stability = torch.clamp(1.0 / (torch.abs(det) + 1e-8), 0, 1)
                        quality_factors.append(stability.mean().item())
                    else:
                        quality_factors.append(0.7)
                except:
                    quality_factors.append(0.7)
        
        # í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ì •í™•ë„
        if 'keypoints' in result:
            keypoints = result['keypoints']
            if len(keypoints) >= 18:
                # í‚¤í¬ì¸íŠ¸ ë¶„í¬ì˜ í•©ë¦¬ì„± í™•ì¸
                keypoints_tensor = torch.tensor(keypoints[0] if isinstance(keypoints[0], list) else keypoints)
                if keypoints_tensor.numel() > 0:
                    # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ë¶„í¬ í™•ì¸
                    distances = torch.cdist(keypoints_tensor, keypoints_tensor)
                    mean_distance = torch.mean(distances[distances > 0])
                    # ì •ê·œí™”ëœ ê±°ë¦¬ ì ìˆ˜
                    distance_score = torch.clamp(mean_distance / 100.0, 0, 1).item()
                    quality_factors.append(distance_score)
        
        # ì›Œí•‘ í’ˆì§ˆ í™•ì¸
        if 'warped_clothing' in result:
            warped = result['warped_clothing']
            if isinstance(warped, torch.Tensor):
                # ê·¸ë˜ë””ì–¸íŠ¸ ë³€í™”ëŸ‰ìœ¼ë¡œ ì›Œí•‘ í’ˆì§ˆ í‰ê°€
                grad_x = torch.diff(warped, dim=-1)
                grad_y = torch.diff(warped, dim=-2)
                gradient_consistency = 1.0 - torch.clamp(torch.std(grad_x) + torch.std(grad_y), 0, 1)
                quality_factors.append(gradient_consistency.item())
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        if quality_factors:
            return float(np.mean(quality_factors))
        else:
            return 0.8  # ê¸°ë³¸ê°’
    
    def _post_process_ai_output(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶œë ¥ í›„ì²˜ë¦¬"""
        processed = ai_result.copy()
        
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        for key, value in ai_result.items():
            if isinstance(value, torch.Tensor):
                processed[key] = value.detach().cpu().numpy()
        
        return processed
    
    def _apply_advanced_postprocessing(self, warped_clothing: torch.Tensor) -> torch.Tensor:
        """ê³ ê¸‰ í›„ì²˜ë¦¬ ì ìš© (ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ ë³´ì¡´)"""
        if not TORCH_AVAILABLE:
            return warped_clothing
        
        # 1. ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©
        smoothed = self._apply_edge_smoothing(warped_clothing)
        
        # 2. ìƒ‰ìƒ ë³´ì •
        color_corrected = self._apply_color_correction(smoothed)
        
        # 3. ë…¸ì´ì¦ˆ ì œê±°
        denoised = self._apply_denoising(color_corrected)
        
        return denoised
    
    def _apply_edge_smoothing(self, image: torch.Tensor) -> torch.Tensor:
        """ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©"""
        if image.dim() != 4:
            return image
        
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì»¤ë„
        kernel_size = 3
        sigma = 0.5
        kernel = torch.ones(1, 1, kernel_size, kernel_size, device=image.device) / (kernel_size ** 2)
        
        # ê° ì±„ë„ë³„ë¡œ ì»¨ë³¼ë£¨ì…˜ ì ìš©
        smoothed_channels = []
        for c in range(image.shape[1]):
            channel = image[:, c:c+1, :, :]
            smoothed_channel = F.conv2d(channel, kernel, padding=kernel_size//2)
            smoothed_channels.append(smoothed_channel)
        
        return torch.cat(smoothed_channels, dim=1)
    
    def _apply_color_correction(self, image: torch.Tensor) -> torch.Tensor:
        """ìƒ‰ìƒ ë³´ì •"""
        if image.dim() != 4:
            return image
        
        # ì±„ë„ í–¥ìƒ
        mean_rgb = torch.mean(image, dim=1, keepdim=True)
        enhanced = image + 0.1 * (image - mean_rgb)
        
        # í´ë¨í•‘
        enhanced = torch.clamp(enhanced, 0, 1)
        
        return enhanced
    
    def _apply_denoising(self, image: torch.Tensor) -> torch.Tensor:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        if image.dim() != 4:
            return image
        
        # ë¯¸ë””ì•ˆ í•„í„° íš¨ê³¼ (ê°„ë‹¨í•œ ë²„ì „)
        kernel = torch.tensor([[1, 1, 1], [1, 2, 1], [1, 1, 1]], 
                            dtype=torch.float32, device=image.device).view(1, 1, 3, 3) / 10
        
        denoised_channels = []
        for c in range(image.shape[1]):
            channel = image[:, c:c+1, :, :]
            denoised_channel = F.conv2d(channel, kernel, padding=1)
            denoised_channels.append(denoised_channel)
        
        return torch.cat(denoised_channels, dim=1)
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´ - process ë©”ì„œë“œ
    # ==============================================
    
    async def process(self, *args, **kwargs) -> Dict[str, Any]:
        """
        í•µì‹¬ process ë©”ì„œë“œ - step_model_requirements.py ì™„ì „ í˜¸í™˜ + ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´
        """
        start_time = time.time()
        
        try:
            self.status.processing_active = True
            self.status.ai_model_calls += 1
            
            # step_model_requirements.py ê¸°ì¤€ ì…ë ¥ ì²˜ë¦¬
            result = await self._process_with_requirements_spec(*args, **kwargs)
            
            # step_model_requirements.py ê¸°ì¤€ ì¶œë ¥ í¬ë§·
            if self.data_spec and hasattr(self.data_spec, 'step_output_schema'):
                result = self._format_output_with_spec(result)
            
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            
            self.status.processing_active = False
            self.statistics['total_processed'] += 1
            self.statistics['total_processing_time'] += processing_time
            
            return result
            
        except Exception as e:
            self.status.processing_active = False
            self.status.error_count += 1
            self.status.last_error = str(e)
            self.logger.error(f"âŒ Step 04 ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'step_id': self.step_id,
                'step_name': self.step_name,
                'error_type': type(e).__name__,
                'processing_time': time.time() - start_time
            }
    
    async def _process_with_requirements_spec(self, *args, **kwargs) -> Dict[str, Any]:
        """step_model_requirements.py ìŠ¤í™ ê¸°ì¤€ ì²˜ë¦¬"""
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ (step_model_requirements.py ê¸°ì¤€)
        input_data = await self._parse_inputs_with_spec(*args, **kwargs)
        
        # ì „ì²˜ë¦¬ (DetailedDataSpec ê¸°ì¤€)
        preprocessed_data = await self._preprocess_with_spec(input_data)
        
        # AI ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸
        if not self.status.models_loaded:
            await self._ensure_models_loaded()
        
        # ì‹¤ì œ ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬ (AI ê°•í™”) - ë™ê¸° í˜¸ì¶œ
        matching_result = self._run_ai_inference(preprocessed_data)
        
        # í›„ì²˜ë¦¬ (DetailedDataSpec ê¸°ì¤€)
        postprocessed_result = await self._postprocess_with_spec(matching_result)
        
        # step_model_requirements.py ì¶œë ¥ í˜•ì‹ ì¤€ìˆ˜
        result = {
            'success': True,
            'step_id': self.step_id,
            'step_name': self.step_name,
            'transformation_matrix': postprocessed_result.get('transformation_matrix'),
            'warped_clothing': postprocessed_result.get('warped_clothing'),
            'flow_field': postprocessed_result.get('flow_field'),
            'matching_confidence': postprocessed_result.get('confidence', 0.85),
            'quality_score': postprocessed_result.get('quality_score', 0.8),
            'keypoints': postprocessed_result.get('keypoints', []),
            'ai_enhanced': True,
            'requirements_compatible': True
        }
        
        return result
    
    async def _parse_inputs_with_spec(self, *args, **kwargs) -> Dict[str, Any]:
        """step_model_requirements.py ê¸°ì¤€ ì…ë ¥ íŒŒì‹±"""
        input_data = {}
        
        # step_model_requirements.py DetailedDataSpec ê¸°ì¤€
        if self.data_spec and hasattr(self.data_spec, 'step_input_schema'):
            # Step ê°„ ì…ë ¥ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬
            step_inputs = self.data_spec.step_input_schema
            
            # step_02ì—ì„œ ë°›ì„ ë°ì´í„°
            if 'step_02' in step_inputs:
                step_02_data = kwargs.get('step_02_data', {})
                input_data['keypoints_18'] = step_02_data.get('keypoints_18')
                input_data['pose_skeleton'] = step_02_data.get('pose_skeleton')
            
            # step_03ì—ì„œ ë°›ì„ ë°ì´í„°
            if 'step_03' in step_inputs:
                step_03_data = kwargs.get('step_03_data', {})
                input_data['cloth_mask'] = step_03_data.get('cloth_mask')
                input_data['segmented_clothing'] = step_03_data.get('segmented_clothing')
        
        # ì§ì ‘ ì…ë ¥ ì²˜ë¦¬
        if len(args) >= 2:
            input_data['person_image'] = args[0]
            input_data['clothing_image'] = args[1]
        else:
            input_data['person_image'] = kwargs.get('person_image')
            input_data['clothing_image'] = kwargs.get('clothing_image')
            
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°
        input_data['pose_data'] = kwargs.get('pose_data')
        input_data['clothing_type'] = kwargs.get('clothing_type', 'upper')
        
        return input_data
    
    async def _preprocess_with_spec(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ì¤€ ì „ì²˜ë¦¬"""
        preprocessed = {}
        
        # step_model_requirements.py ì „ì²˜ë¦¬ ìŠ¤í™
        if self.data_spec and hasattr(self.data_spec, 'preprocessing_steps'):
            preprocessing_steps = self.data_spec.preprocessing_steps
            normalization_mean = getattr(self.data_spec, 'normalization_mean', (0.485, 0.456, 0.406))
            normalization_std = getattr(self.data_spec, 'normalization_std', (0.229, 0.224, 0.225))
        else:
            preprocessing_steps = ["resize_256x192", "normalize_imagenet", "extract_pose_features"]
            normalization_mean = (0.485, 0.456, 0.406)
            normalization_std = (0.229, 0.224, 0.225)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        for key in ['person_image', 'clothing_image']:
            if input_data.get(key) is not None:
                image = input_data[key]
                processed_image = await self._preprocess_image(
                    image, preprocessing_steps, normalization_mean, normalization_std
                )
                preprocessed[key] = processed_image
        
        # í¬ì¦ˆ ë°ì´í„° ì „ì²˜ë¦¬
        if input_data.get('keypoints_18') is not None:
            preprocessed['pose_keypoints'] = self._preprocess_keypoints(input_data['keypoints_18'])
        elif input_data.get('pose_data') is not None:
            preprocessed['pose_keypoints'] = self._preprocess_keypoints(input_data['pose_data'])
        
        # ë§ˆìŠ¤í¬ ë°ì´í„° ì „ì²˜ë¦¬
        if input_data.get('cloth_mask') is not None:
            preprocessed['cloth_mask'] = self._preprocess_mask(input_data['cloth_mask'])
        
        return preprocessed
    
    async def _preprocess_image(self, image: Any, steps: List[str], 
                              mean: Tuple[float, ...], std: Tuple[float, ...]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - step_model_requirements.py ìŠ¤í™ ì¤€ìˆ˜"""
        if not TORCH_AVAILABLE:
            return torch.zeros(3, 256, 192)
        
        # PIL Image -> torch.Tensor ë³€í™˜
        if isinstance(image, Image.Image):
            image = T.ToTensor()(image)
        elif isinstance(image, np.ndarray):
            if image.dtype == np.uint8:
                image = image.astype(np.float32) / 255.0
            image = torch.from_numpy(image)
            if image.dim() == 3 and image.shape[0] != 3:
                image = image.permute(2, 0, 1)  # HWC -> CHW
        elif not isinstance(image, torch.Tensor):
            # ê¸°ë³¸ ë”ë¯¸ ì´ë¯¸ì§€
            image = torch.rand(3, 256, 192)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        # step_model_requirements.py ê¸°ì¤€ ì „ì²˜ë¦¬ ì ìš©
        for step in steps:
            if "resize_256x192" in step:
                image = F.interpolate(image, size=(256, 192), mode='bilinear', align_corners=False)
            elif "normalize_imagenet" in step or "normalize" in step:
                # ImageNet ì •ê·œí™”
                if image.max() > 1.0:
                    image = image / 255.0
                mean_tensor = torch.tensor(mean, device=image.device).view(1, 3, 1, 1)
                std_tensor = torch.tensor(std, device=image.device).view(1, 3, 1, 1)
                image = (image - mean_tensor) / std_tensor
        
        return image.to(self.device)
    
    def _preprocess_keypoints(self, keypoints: Any) -> torch.Tensor:
        """í‚¤í¬ì¸íŠ¸ ì „ì²˜ë¦¬"""
        if isinstance(keypoints, np.ndarray):
            keypoints = torch.from_numpy(keypoints)
        elif isinstance(keypoints, list):
            keypoints = torch.tensor(keypoints)
        elif not isinstance(keypoints, torch.Tensor):
            # ë”ë¯¸ í‚¤í¬ì¸íŠ¸ ìƒì„±
            keypoints = torch.zeros(1, 18, 2)
        
        if keypoints.dim() == 2:
            keypoints = keypoints.unsqueeze(0)
        
        return keypoints.float().to(self.device)
    
    def _preprocess_mask(self, mask: Any) -> torch.Tensor:
        """ë§ˆìŠ¤í¬ ì „ì²˜ë¦¬"""
        if isinstance(mask, np.ndarray):
            mask = torch.from_numpy(mask)
        elif not isinstance(mask, torch.Tensor):
            mask = torch.zeros(1, 1, 256, 192)
        
        if mask.dim() == 2:
            mask = mask.unsqueeze(0).unsqueeze(0)
        elif mask.dim() == 3:
            mask = mask.unsqueeze(0)
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if mask.max() > 1.0:
            mask = mask / 255.0
        
        return mask.float().to(self.device)
    
    async def _postprocess_with_spec(self, matching_result: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ì¤€ í›„ì²˜ë¦¬"""
        postprocessed = {}
        
        # step_model_requirements.py í›„ì²˜ë¦¬ ìŠ¤í™
        if self.data_spec and hasattr(self.data_spec, 'postprocessing_steps'):
            postprocessing_steps = self.data_spec.postprocessing_steps
        else:
            postprocessing_steps = ["apply_tps", "smooth_warping", "blend_boundaries"]
        
        # ê° ì¶œë ¥ì— ëŒ€í•´ í›„ì²˜ë¦¬ ì ìš©
        for key, value in matching_result.items():
            if isinstance(value, torch.Tensor):
                processed_value = await self._postprocess_tensor(value, postprocessing_steps, key)
                postprocessed[key] = processed_value
            else:
                postprocessed[key] = value
        
        return postprocessed
    
    async def _postprocess_tensor(self, tensor: torch.Tensor, steps: List[str], tensor_type: str) -> torch.Tensor:
        """í…ì„œ í›„ì²˜ë¦¬"""
        result = tensor
        
        for step in steps:
            if "smooth_warping" in step and "warped" in tensor_type:
                result = self._apply_edge_smoothing(result)
            elif "blend_boundaries" in step and "warped" in tensor_type:
                result = self._apply_boundary_blending(result)
            elif "apply_tps" in step and "transformation" in tensor_type:
                # TPS í›„ì²˜ë¦¬ëŠ” ì´ë¯¸ ì ìš©ë¨
                pass
        
        return result
    
    def _apply_boundary_blending(self, image: torch.Tensor) -> torch.Tensor:
        """ê²½ê³„ ë¸”ë Œë”©"""
        if image.dim() != 4:
            return image
        
        # ê°€ì¥ìë¦¬ ë§ˆìŠ¤í¬ ìƒì„±
        mask = torch.ones_like(image[:, :1, :, :])
        border_size = 10
        mask[:, :, :border_size, :] *= 0.5
        mask[:, :, -border_size:, :] *= 0.5
        mask[:, :, :, :border_size] *= 0.5
        mask[:, :, :, -border_size:] *= 0.5
        
        return image * mask
    
    def _format_output_with_spec(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requirements.py ì¶œë ¥ ìŠ¤í™ í˜•ì‹í™”"""
        if not self.data_spec or not hasattr(self.data_spec, 'step_output_schema'):
            return result
        
        formatted = result.copy()
        output_schema = self.data_spec.step_output_schema
        
        # step_05ë¡œ ì „ë‹¬í•  ë°ì´í„° í˜•ì‹í™”
        if 'step_05' in output_schema:
            step_05_data = {}
            schema_05 = output_schema['step_05']
            
            if 'transformation_matrix' in schema_05 and 'transformation_matrix' in result:
                step_05_data['transformation_matrix'] = self._tensor_to_numpy(result['transformation_matrix'])
            
            if 'warped_clothing' in schema_05 and 'warped_clothing' in result:
                step_05_data['warped_clothing'] = self._tensor_to_numpy(result['warped_clothing'])
                
            if 'flow_field' in schema_05 and 'flow_field' in result:
                step_05_data['flow_field'] = self._tensor_to_numpy(result['flow_field'])
            
            formatted['step_05_data'] = step_05_data
        
        # step_06ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° í˜•ì‹í™”
        if 'step_06' in output_schema:
            step_06_data = {}
            schema_06 = output_schema['step_06']
            
            if 'geometric_alignment' in schema_06 and 'warped_clothing' in result:
                step_06_data['geometric_alignment'] = self._tensor_to_numpy(result['warped_clothing'])
                
            if 'matching_score' in schema_06:
                step_06_data['matching_score'] = float(result.get('quality_score', 0.8))
            
            formatted['step_06_data'] = step_06_data
        
        return formatted
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        if isinstance(tensor, torch.Tensor):
            return tensor.detach().cpu().numpy()
        return tensor
    
    async def _ensure_models_loaded(self):
        """AI ëª¨ë¸ ë¡œë”© í™•ì¸"""
        try:
            if not self.status.models_loaded:
                await self.initialize()
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´ - ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”©
    # ==============================================
    
    async def initialize(self) -> bool:
        """Step ì´ˆê¸°í™” - step_model_requirements.py ê¸°ì¤€"""
        try:
            if self.status.initialized:
                return True
                
            self.logger.info(f"ğŸ”„ Step 04 ì´ˆê¸°í™” ì‹œì‘ (step_model_requirements.py ê¸°ì¤€)...")
            
            # ëª¨ë¸ ê²½ë¡œ ë§¤í•‘
            await self._initialize_model_paths()
            
            # AI ëª¨ë¸ ë¡œë”© ì‹œë„
            await self._load_ai_models_with_requirements()
            
            self.status.initialized = True
            self.logger.info(f"âœ… Step 04 ì´ˆê¸°í™” ì™„ë£Œ (ìš”êµ¬ì‚¬í•­ í˜¸í™˜)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step 04 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_model_paths(self):
        """ëª¨ë¸ ê²½ë¡œ ì´ˆê¸°í™” - step_model_requirements.py ê¸°ì¤€"""
        try:
            if hasattr(self, 'model_mapper'):
                self.model_paths = self.model_mapper.get_geometric_matching_models()
                self.logger.info(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì™„ë£Œ: {len(self.model_paths)}ê°œ íŒŒì¼")
                
                for model_name, path in self.model_paths.items():
                    size_mb = path.stat().st_size / (1024**2) if path.exists() else 0
                    self.logger.info(f"  - {model_name}: {path.name} ({size_mb:.1f}MB)")
            else:
                self.model_paths = {}
                self.logger.warning("ğŸ“ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ ì—†ìŒ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model_paths = {}
    
    async def _load_ai_models_with_requirements(self):
        """AI ëª¨ë¸ ë¡œë”© - step_model_requirements.py ê¸°ì¤€"""
        try:
            models_loaded = 0
            
            # GMM ëª¨ë¸ ë¡œë”© (ai_class="RealGMMModel")
            if 'gmm' in self.model_paths:
                gmm_path = self.model_paths['gmm']
                self.gmm_model = await self._load_gmm_model(gmm_path)
                if self.gmm_model:
                    models_loaded += 1
                    self.logger.info(f"âœ… GMM ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {gmm_path.name}")
            
            # TPS ëª¨ë¸ ë¡œë”©
            if 'tps' in self.model_paths:
                tps_path = self.model_paths['tps']
                self.tps_model = await self._load_tps_model(tps_path)
                if self.tps_model:
                    models_loaded += 1
                    self.logger.info(f"âœ… TPS ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {tps_path.name}")
            
            # SAM ëª¨ë¸ ë¡œë”© (ê³µìœ  ëª¨ë¸ - step_model_requirements.py ê¸°ì¤€)
            if 'sam_shared' in self.model_paths:
                sam_path = self.model_paths['sam_shared']
                self.sam_model = await self._load_sam_model(sam_path)
                if self.sam_model:
                    models_loaded += 1
                    self.logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ê³µìœ ): {sam_path.name}")
            
            # ViT ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ê¸°ëŠ¥)
            if 'vit_large' in self.model_paths:
                vit_path = self.model_paths['vit_large']
                self.vit_model = await self._load_vit_model(vit_path)
                if self.vit_model:
                    models_loaded += 1
                    self.logger.info(f"âœ… ViT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {vit_path.name}")
            
            # EfficientNet ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ê¸°ëŠ¥)
            if 'efficientnet' in self.model_paths:
                eff_path = self.model_paths['efficientnet']
                self.efficientnet_model = await self._load_efficientnet_model(eff_path)
                if self.efficientnet_model:
                    models_loaded += 1
                    self.logger.info(f"âœ… EfficientNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {eff_path.name}")
            
            self.status.models_loaded = models_loaded > 0
            self.status.model_creation_success = models_loaded > 0
            
            # ê¸°ì¡´ íŒŒì¼ í˜¸í™˜ì„±ì„ ìœ„í•œ geometric_model ì†ì„± ì„¤ì •
            self.geometric_model = self.gmm_model or self.tps_model or self.sam_model
            
            if models_loaded > 0:
                self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {models_loaded}/5ê°œ")
            else:
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ì—†ìŒ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
                self.status.models_loaded = True  # ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œë¼ë„ ë™ì‘
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {e}")
            self.status.models_loaded = True
    
    async def _load_gmm_model(self, checkpoint_path: Path) -> Optional[RealGMMModel]:
        """GMM ëª¨ë¸ ë¡œë”©"""
        try:
            model = RealGMMModel(input_nc=6, output_nc=2)
            
            if checkpoint_path.exists() and TORCH_AVAILABLE:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'generator' in checkpoint:
                        state_dict = checkpoint['generator']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ë§¤í•‘
                new_state_dict = {}
                for k, v in state_dict.items():
                    new_key = k
                    if k.startswith('module.'):
                        new_key = k[7:]
                    elif k.startswith('netG.'):
                        new_key = k[5:]
                    elif k.startswith('generator.'):
                        new_key = k[10:]
                    
                    new_state_dict[new_key] = v
                
                # ëª¨ë¸ ë¡œë”© (ì—„ê²©í•˜ì§€ ì•Šê²Œ)
                missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
                
                if len(missing_keys) > 0:
                    self.logger.debug(f"GMM ëª¨ë¸ ëˆ„ë½ í‚¤: {len(missing_keys)}ê°œ")
                
                self.logger.info(f"âœ… GMM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {checkpoint_path.name}")
            else:
                self.logger.warning(f"âš ï¸ GMM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            
            model = model.to(self.device)
            model.eval()
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_tps_model(self, checkpoint_path: Path) -> Optional[RealTPSModel]:
        """TPS ëª¨ë¸ ë¡œë”©"""
        try:
            model = RealTPSModel(grid_size=20)
            
            if checkpoint_path.exists() and TORCH_AVAILABLE:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ë³€í™˜
                new_state_dict = {}
                for k, v in state_dict.items():
                    new_key = k
                    if k.startswith('module.'):
                        new_key = k[7:]
                    elif k.startswith('netTPS.'):
                        new_key = k[7:]
                    
                    new_state_dict[new_key] = v
                
                missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
                
                self.logger.info(f"âœ… TPS ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {checkpoint_path.name}")
            else:
                self.logger.warning(f"âš ï¸ TPS ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            
            model = model.to(self.device)
            model.eval()
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ TPS ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_sam_model(self, checkpoint_path: Path) -> Optional[RealSAMModel]:
        """SAM ëª¨ë¸ ë¡œë”© (ê³µìœ  ëª¨ë¸)"""
        try:
            model = RealSAMModel()
            
            if checkpoint_path.exists() and TORCH_AVAILABLE:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # SAM ì²´í¬í¬ì¸íŠ¸ëŠ” ë³´í†µ ì§ì ‘ state_dict
                if isinstance(checkpoint, dict) and 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
                
                # SAMì€ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶€ë¶„ ë¡œë”©ë§Œ
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    self.logger.info(f"âœ… SAM ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    self.logger.warning("âš ï¸ SAM í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                self.logger.warning(f"âš ï¸ SAM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            
            model = model.to(self.device)
            model.eval()
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_vit_model(self, checkpoint_path: Path) -> Optional[RealViTModel]:
        """ViT ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ê¸°ëŠ¥)"""
        try:
            model = RealViTModel()
            
            if checkpoint_path.exists() and TORCH_AVAILABLE:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ViT ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    self.logger.info(f"âœ… ViT ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    self.logger.warning("âš ï¸ ViT í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                self.logger.warning(f"âš ï¸ ViT ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            
            model = model.to(self.device)
            model.eval()
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ViT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_efficientnet_model(self, checkpoint_path: Path) -> Optional[RealEfficientNetModel]:
        """EfficientNet ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ê¸°ëŠ¥)"""
        try:
            model = RealEfficientNetModel()
            
            if checkpoint_path.exists() and TORCH_AVAILABLE:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # EfficientNet ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    self.logger.info(f"âœ… EfficientNet ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    self.logger.warning("âš ï¸ EfficientNet í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                self.logger.warning(f"âš ï¸ EfficientNet ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            
            model = model.to(self.device)
            model.eval()
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ EfficientNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´ - ê²€ì¦ ë° ì •ë³´ ì¡°íšŒ
    # ==============================================
    
    async def validate_inputs(self, person_image: Any, clothing_image: Any) -> Dict[str, Any]:
        """ì…ë ¥ ê²€ì¦ - step_model_requirements.py ê¸°ì¤€"""
        errors = []
        
        if person_image is None:
            errors.append("person_imageê°€ Noneì…ë‹ˆë‹¤")
        
        if clothing_image is None:
            errors.append("clothing_imageê°€ Noneì…ë‹ˆë‹¤")
        
        # step_model_requirements.py DetailedDataSpec ê¸°ì¤€ ê²€ì¦
        if self.data_spec:
            # ì…ë ¥ ë°ì´í„° íƒ€ì… ê²€ì¦
            if hasattr(self.data_spec, 'input_data_types'):
                valid_types = self.data_spec.input_data_types
                if person_image is not None:
                    person_type_valid = any(
                        isinstance(person_image, eval(dtype)) if dtype != 'PIL.Image' 
                        else isinstance(person_image, Image.Image)
                        for dtype in valid_types
                    )
                    if not person_type_valid:
                        errors.append(f"person_image íƒ€ì… ë¶ˆì¼ì¹˜. í—ˆìš© íƒ€ì…: {valid_types}")
        
        return {
            'valid': len(errors) == 0,
            'person_image': person_image is not None,
            'clothing_image': clothing_image is not None,
            'errors': errors,
            'requirements_compatible': self.status.requirements_compatible
        }
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            if hasattr(self.dependency_manager, 'set_model_loader'):
                self.dependency_manager.set_model_loader(model_loader)
            self.status.dependencies_injected = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            if hasattr(self.dependency_manager, 'set_memory_manager'):
                self.dependency_manager.set_memory_manager(memory_manager)
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            if hasattr(self.dependency_manager, 'set_data_converter'):
                self.dependency_manager.set_data_converter(data_converter)
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            if hasattr(self.dependency_manager, 'set_di_container'):
                self.dependency_manager.set_di_container(di_container)
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜ - step_model_requirements.py ê¸°ì¤€"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'initialized': self.status.initialized,
            'models_loaded': self.status.models_loaded,
            'dependencies_injected': self.status.dependencies_injected,
            'processing_active': self.status.processing_active,
            'requirements_compatible': self.status.requirements_compatible,
            'detailed_data_spec_loaded': self.status.detailed_data_spec_loaded,
            'device': self.device,
            'model_architecture': getattr(self.step_request, 'model_architecture', 'gmm_tps') if self.step_request else 'gmm_tps',
            'input_size': self.matching_config.get('input_size', (256, 192)),
            'output_format': self.matching_config.get('output_format', 'transformation_matrix'),
            'batch_size': self.matching_config.get('batch_size', 2),
            'memory_fraction': self.matching_config.get('memory_fraction', 0.2),
            'precision': self.matching_config.get('precision', 'fp16'),
            'model_files_detected': len(self.model_paths) if hasattr(self, 'model_paths') else 0,
            'gmm_model_loaded': self.gmm_model is not None,
            'tps_model_loaded': self.tps_model is not None,
            'sam_model_loaded': self.sam_model is not None,
            'vit_model_loaded': self.vit_model is not None,
            'efficientnet_model_loaded': self.efficientnet_model is not None,
            'statistics': self.statistics
        }
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ íŒŒì¼ì—ì„œ ëˆ„ë½ëœ ì¤‘ìš”í•œ ë©”ì„œë“œë“¤ ì¶”ê°€ (ì™„ì „ ë³´ì¡´)
    # ==============================================
    
    async def get_model(self, model_name: Optional[str] = None):
        """ëª¨ë¸ ë°˜í™˜ - ê¸°ì¡´ í˜¸í™˜ì„±"""
        if model_name == "geometric_matching" or model_name is None:
            return self.geometric_model
        elif model_name == "gmm":
            return self.gmm_model
        elif model_name == "tps":
            return self.tps_model
        elif model_name == "sam":
            return self.sam_model
        elif model_name == "vit":
            return self.vit_model
        elif model_name == "efficientnet":
            return self.efficientnet_model
        else:
            return None
    
    def get_model_info(self, info_type: str = "basic") -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ - ê¸°ì¡´ ê¸°ëŠ¥"""
        if info_type == "all":
            return {
                'loaded_models': sum([
                    1 if self.gmm_model else 0,
                    1 if self.tps_model else 0,
                    1 if self.sam_model else 0,
                    1 if self.vit_model else 0,
                    1 if self.efficientnet_model else 0
                ]),
                'models': {
                    'gmm': {
                        'loaded': self.gmm_model is not None,
                        'parameters': self._count_parameters(self.gmm_model) if self.gmm_model else 0,
                        'file_size': self._get_model_file_size('gmm')
                    },
                    'tps': {
                        'loaded': self.tps_model is not None,
                        'parameters': self._count_parameters(self.tps_model) if self.tps_model else 0,
                        'file_size': self._get_model_file_size('tps')
                    },
                    'sam': {
                        'loaded': self.sam_model is not None,
                        'parameters': self._count_parameters(self.sam_model) if self.sam_model else 0,
                        'file_size': self._get_model_file_size('sam_shared')
                    },
                    'vit': {
                        'loaded': self.vit_model is not None,
                        'parameters': self._count_parameters(self.vit_model) if self.vit_model else 0,
                        'file_size': self._get_model_file_size('vit_large')
                    },
                    'efficientnet': {
                        'loaded': self.efficientnet_model is not None,
                        'parameters': self._count_parameters(self.efficientnet_model) if self.efficientnet_model else 0,
                        'file_size': self._get_model_file_size('efficientnet')
                    }
                }
            }
        else:
            return {
                'gmm_loaded': self.gmm_model is not None,
                'tps_loaded': self.tps_model is not None,
                'sam_loaded': self.sam_model is not None,
                'vit_loaded': self.vit_model is not None,
                'efficientnet_loaded': self.efficientnet_model is not None,
                'total_models': sum([
                    1 if self.gmm_model else 0,
                    1 if self.tps_model else 0,
                    1 if self.sam_model else 0,
                    1 if self.vit_model else 0,
                    1 if self.efficientnet_model else 0
                ])
            }
    
    def _count_parameters(self, model):
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        if model is None:
            return 0
        try:
            return sum(p.numel() for p in model.parameters())
        except:
            return 0
    
    def _get_model_file_size(self, model_key: str) -> str:
        """ëª¨ë¸ íŒŒì¼ í¬ê¸° ë°˜í™˜"""
        if hasattr(self, 'model_paths') and model_key in self.model_paths:
            try:
                path = self.model_paths[model_key]
                if path.exists():
                    size_mb = path.stat().st_size / (1024**2)
                    return f"{size_mb:.1f}MB"
            except:
                pass
        return "Unknown"
    
    def _apply_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì ìš© - ê¸°ì¡´ ê¸°ëŠ¥"""
        try:
            if self.device == "mps":
                # MPS ìµœì í™” ì„¤ì •
                if self.matching_config:
                    self.matching_config['batch_size'] = min(self.matching_config.get('batch_size', 2), 8)
                    self.matching_config['memory_fraction'] = min(self.matching_config.get('memory_fraction', 0.2), 0.3)
                
                # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                os.environ['OMP_NUM_THREADS'] = '16'
                
                self.logger.info("ğŸ M3 Max ìµœì í™” ì ìš© ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def warmup_step(self) -> Dict[str, Any]:
        """ì›Œë°ì—… - ê¸°ì¡´ ê¸°ëŠ¥ (ë™ê¸° ë²„ì „)"""
        try:
            self.logger.info(f"ğŸ”¥ {self.__class__.__name__} ì›Œë°ì—… ì‹œì‘")
            
            # ê¸°ë³¸ ì›Œë°ì—… ì‘ì—…
            warmup_result = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'device': self.device,
                'models_ready': self.status.models_loaded,
                'warmup_time': 0.1
            }
            
            if hasattr(self, 'matching_config'):
                warmup_result['config'] = self.matching_config
            
            self.warmup_completed = True
            self.logger.info(f"âœ… {self.__class__.__name__} ì›Œë°ì—… ì™„ë£Œ")
            
            return warmup_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name
            }
    
    def _setup_model_interface(self):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • - ê¸°ì¡´ ê¸°ëŠ¥ (ë™ê¸° ë²„ì „)"""
        try:
            self.logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •")
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”
            self.model_interface = {
                'gmm_model': self.gmm_model,
                'tps_model': self.tps_model,
                'sam_model': self.sam_model,
                'vit_model': self.vit_model,
                'efficientnet_model': self.efficientnet_model,
                'device': self.device,
                'ready': self.status.models_loaded
            }
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _detect_basestep_version(self):
        """BaseStepMixin ë²„ì „ ê°ì§€ - ê¸°ì¡´ ê¸°ëŠ¥"""
        try:
            if hasattr(self, 'dependency_manager'):
                return "v19.1"  # ìµœì‹  ë²„ì „
            elif hasattr(self.__class__.__bases__[0], 'unified_dependency_manager'):
                return "v16.0"
            else:
                return "legacy"
        except:
            return "unknown"
    
    def _manual_dependency_injection(self):
        """ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… - ê¸°ì¡´ ê¸°ëŠ¥"""
        try:
            success_count = 0
            
            # ModelLoader ìˆ˜ë™ ì£¼ì… ì‹œë„
            try:
                model_loader = get_model_loader()
                if model_loader:
                    self.model_loader = model_loader
                    success_count += 1
            except Exception as e:
                self.logger.debug(f"ModelLoader ìˆ˜ë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # MemoryManager ìˆ˜ë™ ì£¼ì… ì‹œë„
            try:
                memory_manager = get_memory_manager()
                if memory_manager:
                    self.memory_manager = memory_manager
                    success_count += 1
            except Exception as e:
                self.logger.debug(f"MemoryManager ìˆ˜ë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            return success_count > 0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def _create_processing_status(self):
        """ì²˜ë¦¬ ìƒíƒœ ê°ì²´ ìƒì„± - ê¸°ì¡´ ê¸°ëŠ¥"""
        return ProcessingStatus()
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ íŒŒì¼ í˜¸í™˜ì„± ê²€ì¦ ë©”ì„œë“œ (ì™„ì „ ë³´ì¡´)
    # ==============================================
    
    def validate_dependencies(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ê²€ì¦ - ê¸°ì¡´ íŒŒì¼ í˜¸í™˜ì„±"""
        try:
            return {
                'model_loader': self.model_loader is not None,
                'step_interface': self.model_interface is not None,
                'memory_manager': self.memory_manager is not None,
                'data_converter': self.data_converter is not None
            }
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'model_loader': False,
                'step_interface': False,
                'memory_manager': False,
                'data_converter': False
            }
    
    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        try:
            # ëª¨ë¸ ì •ë¦¬
            if self.gmm_model is not None:
                del self.gmm_model
                self.gmm_model = None
            
            if self.tps_model is not None:
                del self.tps_model
                self.tps_model = None
            
            if self.sam_model is not None:
                del self.sam_model
                self.sam_model = None
            
            # ì¶”ê°€ ëª¨ë¸ë“¤ ì •ë¦¬
            if hasattr(self, 'vit_model') and self.vit_model is not None:
                del self.vit_model
                self.vit_model = None
                
            if hasattr(self, 'efficientnet_model') and self.efficientnet_model is not None:
                del self.efficientnet_model
                self.efficientnet_model = None
            
            # ê¸°ì¡´ íŒŒì¼ í˜¸í™˜ì„± ì†ì„± ì •ë¦¬
            if hasattr(self, 'geometric_model'):
                self.geometric_model = None
            
            # ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'model_interface'):
                del self.model_interface
            
            # ë§¤í•‘ ì •ë¦¬
            if hasattr(self, 'model_paths'):
                self.model_paths.clear()
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'model_mapper') and hasattr(self.model_mapper, 'model_cache'):
                self.model_mapper.model_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and DEVICE == "mps":
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    elif hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… GeometricMatchingStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. ê¸°ì¡´ íŒŒì¼ì—ì„œ ëˆ„ë½ëœ ì¤‘ìš”í•œ í´ë˜ìŠ¤ë“¤ ì¶”ê°€ (ì™„ì „ ë³´ì¡´)
# ==============================================

class RealAIModelFactory:
    """ì‹¤ì œ AI ëª¨ë¸ íŒ©í† ë¦¬ - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹¤ì œ ëª¨ë¸ ìƒì„± (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤)"""
    
    @staticmethod
    def create_gmm_model(checkpoint_path: Path, device: str = "cpu") -> Optional[RealGMMModel]:
        """ì‹¤ì œ GMM ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model = RealGMMModel(input_nc=6, output_nc=2)
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'generator' in checkpoint:
                        state_dict = checkpoint['generator']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ë§¤í•‘ (ë‹¤ì–‘í•œ êµ¬í˜„ì²´ í˜¸í™˜)
                new_state_dict = {}
                for k, v in state_dict.items():
                    # ì¼ë°˜ì ì¸ í‚¤ ë³€í™˜
                    new_key = k
                    if k.startswith('module.'):
                        new_key = k[7:]  # 'module.' ì œê±°
                    elif k.startswith('netG.'):
                        new_key = k[5:]  # 'netG.' ì œê±°
                    elif k.startswith('generator.'):
                        new_key = k[10:]  # 'generator.' ì œê±°
                    
                    new_state_dict[new_key] = v
                
                # ëª¨ë¸ ë¡œë”© (ì—„ê²©í•˜ì§€ ì•Šê²Œ)
                missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
                
                if len(missing_keys) > 0:
                    logging.warning(f"GMM ëª¨ë¸ ëˆ„ë½ í‚¤: {len(missing_keys)}ê°œ")
                if len(unexpected_keys) > 0:
                    logging.warning(f"GMM ëª¨ë¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                
                logging.info(f"âœ… GMM ëª¨ë¸ ë¡œë”© ì„±ê³µ: {checkpoint_path.name}")
            else:
                logging.warning(f"âš ï¸ GMM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model = model.to(device)
            model.eval()
            return model
            
        except Exception as e:
            logging.error(f"âŒ GMM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def create_tps_model(checkpoint_path: Path, device: str = "cpu") -> Optional[RealTPSModel]:
        """ì‹¤ì œ TPS ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model = RealTPSModel(grid_size=20)
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ë³€í™˜
                new_state_dict = {}
                for k, v in state_dict.items():
                    new_key = k
                    if k.startswith('module.'):
                        new_key = k[7:]
                    elif k.startswith('netTPS.'):
                        new_key = k[7:]
                    
                    new_state_dict[new_key] = v
                
                missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
                
                logging.info(f"âœ… TPS ëª¨ë¸ ë¡œë”© ì„±ê³µ: {checkpoint_path.name}")
            else:
                logging.warning(f"âš ï¸ TPS ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model = model.to(device)
            model.eval()
            return model
            
        except Exception as e:
            logging.error(f"âŒ TPS ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def create_sam_model(checkpoint_path: Path, device: str = "cpu") -> Optional[RealSAMModel]:
        """ì‹¤ì œ SAM ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model = RealSAMModel()
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # SAM ì²´í¬í¬ì¸íŠ¸ëŠ” ë³´í†µ ì§ì ‘ state_dict
                if isinstance(checkpoint, dict) and 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
                
                # SAMì€ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶€ë¶„ ë¡œë”©ë§Œ
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    logging.info(f"âœ… SAM ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    logging.warning("âš ï¸ SAM í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                logging.warning(f"âš ï¸ SAM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model = model.to(device)
            model.eval()
            return model
            
        except Exception as e:
            logging.error(f"âŒ SAM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    @staticmethod
    def create_vit_model(checkpoint_path: Path, device: str = "cpu") -> Optional[RealViTModel]:
        """ì‹¤ì œ ViT ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model = RealViTModel()
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # ViT ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    logging.info(f"âœ… ViT ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    logging.warning("âš ï¸ ViT í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                logging.warning(f"âš ï¸ ViT ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model = model.to(device)
            model.eval()
            return model
            
        except Exception as e:
            logging.error(f"âŒ ViT ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    @staticmethod
    def create_efficientnet_model(checkpoint_path: Path, device: str = "cpu") -> Optional[RealEfficientNetModel]:
        """ì‹¤ì œ EfficientNet ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model = RealEfficientNetModel()
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # EfficientNet ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
                compatible_dict = {}
                model_dict = model.state_dict()
                
                for k, v in state_dict.items():
                    if k in model_dict and v.shape == model_dict[k].shape:
                        compatible_dict[k] = v
                
                if len(compatible_dict) > 0:
                    model_dict.update(compatible_dict)
                    model.load_state_dict(model_dict)
                    logging.info(f"âœ… EfficientNet ëª¨ë¸ ë¶€ë¶„ ë¡œë”©: {len(compatible_dict)}/{len(state_dict)}ê°œ ë ˆì´ì–´")
                else:
                    logging.warning("âš ï¸ EfficientNet í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”")
            else:
                logging.warning(f"âš ï¸ EfficientNet ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model = model.to(device)
            model.eval()
            return model
            
        except Exception as e:
            logging.error(f"âŒ EfficientNet ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

# ==============================================
# ğŸ”¥ 10. ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ì•ˆì „í•œ MPS í•¨ìˆ˜ë“¤ ì¶”ê°€ (ì™„ì „ ë³´ì¡´)
# ==============================================

def safe_mps_empty_cache():
    """conda í™˜ê²½ì—ì„œ ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í•¨ìˆ˜)"""
    if DEVICE == "mps" and TORCH_AVAILABLE:
        try:
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            elif hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            else:
                # ìˆ˜ë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                import gc
                gc.collect()
                return False
            return True
        except Exception as e:
            logging.debug(f"âš ï¸ MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            import gc
            gc.collect()
            return False
    return False

def check_torch_mps_compatibility():
    """PyTorch MPS í˜¸í™˜ì„± ì²´í¬ (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í•¨ìˆ˜)"""
    compatibility_info = {
        'torch_version': torch.__version__ if TORCH_AVAILABLE else 'N/A',
        'mps_available': torch.backends.mps.is_available() if TORCH_AVAILABLE else False,
        'mps_empty_cache_available': False,
        'device': DEVICE,
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'N/A')
    }
    
    if TORCH_AVAILABLE and DEVICE == "mps":
        # MPS empty_cache ë©”ì„œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if hasattr(torch.backends.mps, 'empty_cache'):
            compatibility_info['mps_empty_cache_available'] = True
            compatibility_info['empty_cache_method'] = 'torch.backends.mps.empty_cache'
        elif hasattr(torch.mps, 'empty_cache'):
            compatibility_info['mps_empty_cache_available'] = True
            compatibility_info['empty_cache_method'] = 'torch.mps.empty_cache'
        else:
            compatibility_info['mps_empty_cache_available'] = False
            compatibility_info['empty_cache_method'] = 'none'
    
    return compatibility_info

def validate_conda_optimization():
    """conda í™˜ê²½ ìµœì í™” ìƒíƒœ í™•ì¸ (ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í•¨ìˆ˜)"""
    optimization_status = {
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'N/A'),
        'omp_threads': os.environ.get('OMP_NUM_THREADS', 'N/A'),
        'mkl_threads': os.environ.get('MKL_NUM_THREADS', 'N/A'),
        'mps_high_watermark': os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO', 'N/A'),
        'mps_fallback': os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK', 'N/A'),
        'torch_threads': torch.get_num_threads() if TORCH_AVAILABLE else 'N/A'
    }
    
    # MyCloset conda í™˜ê²½ íŠ¹í™” ì²´í¬
    is_mycloset_env = (
        'mycloset' in optimization_status['conda_env'].lower() 
        if optimization_status['conda_env'] != 'N/A' else False
    )
    optimization_status['is_mycloset_env'] = is_mycloset_env
    
    return optimization_status

class UnifiedDependencyManager:
    """í†µí•© ì˜ì¡´ì„± ê´€ë¦¬ì - ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
    
    def __init__(self):
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        
        self.dependency_status = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        self.auto_injection_attempted = False
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        self.model_loader = model_loader
        self.dependency_status['model_loader'] = True
        self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        self.memory_manager = memory_manager
        self.dependency_status['memory_manager'] = True
        self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        self.data_converter = data_converter
        self.dependency_status['data_converter'] = True
        self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        self.di_container = di_container
        self.dependency_status['di_container'] = True
        self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„"""
        if self.auto_injection_attempted:
            return any(self.dependency_status.values())
        
        self.auto_injection_attempted = True
        success_count = 0
        
        try:
            # ModelLoader ìë™ ì£¼ì…
            if not self.model_loader:
                try:
                    auto_loader = get_model_loader()
                    if auto_loader:
                        self.set_model_loader(auto_loader)
                        success_count += 1
                except Exception as e:
                    self.logger.debug(f"ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # MemoryManager ìë™ ì£¼ì…
            if not self.memory_manager:
                try:
                    auto_manager = get_memory_manager()
                    if auto_manager:
                        self.set_memory_manager(auto_manager)
                        success_count += 1
                except Exception as e:
                    self.logger.debug(f"MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # DataConverter ìë™ ì£¼ì…
            if not self.data_converter:
                try:
                    auto_converter = get_data_converter()
                    if auto_converter:
                        self.set_data_converter(auto_converter)
                        success_count += 1
                except Exception as e:
                    self.logger.debug(f"DataConverter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # DIContainer ìë™ ì£¼ì…
            if not self.di_container:
                try:
                    auto_container = get_di_container()
                    if auto_container:
                        self.set_di_container(auto_container)
                        success_count += 1
                except Exception as e:
                    self.logger.debug(f"DIContainer ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}/4ê°œ ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ì˜ì¡´ì„± ì£¼ì… ì¤‘ ì˜¤ë¥˜: {e}")
            return False

# ==============================================
# ğŸ”¥ 11. ê¸°ì¡´ í˜¸í™˜ì„± íŒ¨ì¹˜ ë° ë³„ì¹­ (ì™„ì „ ë³´ì¡´)
# ==============================================

# ğŸ”§ ê¸°ì¡´ í´ë˜ìŠ¤ëª… í˜¸í™˜ì„± ë³„ì¹­
GeometricMatchingModel = RealGMMModel  # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±

# ğŸ”§ ê¸°ì¡´ ì˜ì¡´ì„± í´ë˜ìŠ¤ëª… í˜¸í™˜ì„±
class ImprovedDependencyManager(UnifiedDependencyManager):
    """ê¸°ì¡´ ì´ë¦„ í˜¸í™˜ì„± - ImprovedDependencyManager"""
    pass

# ğŸ”§ GeometricMatchingStepì— ê¸°ì¡´ í˜¸í™˜ì„± ì†ì„± íŒ¨ì¹˜
def _patch_geometric_matching_step():
    """GeometricMatchingStepì— ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œ íŒ¨ì¹˜"""
    
    # ê¸°ì¡´ geometric_model ì†ì„± í˜¸í™˜ì„±
    def geometric_model_property(self):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ geometric_model ì†ì„±"""
        return self.gmm_model or self.tps_model or self.sam_model
    
    def geometric_model_setter(self, value):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ setter"""
        if value is not None:
            if isinstance(value, RealGMMModel):
                self.gmm_model = value
            elif isinstance(value, RealTPSModel):
                self.tps_model = value
            elif isinstance(value, RealSAMModel):
                self.sam_model = value
            else:
                self.gmm_model = value  # ê¸°ë³¸ê°’
    
    # ì†ì„± ì¶”ê°€
    GeometricMatchingStep.geometric_model = property(geometric_model_property, geometric_model_setter)

# íŒ¨ì¹˜ ì ìš©
_patch_geometric_matching_step()

# ==============================================
# ğŸ”¥ 12. í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„± í¬í•¨)
# ==============================================

def create_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    return GeometricMatchingStep(**kwargs)

def create_real_ai_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """ì‹¤ì œ AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    kwargs.setdefault('config', {})
    kwargs['config']['use_real_models'] = True
    kwargs['config']['method'] = 'real_ai_models'
    return GeometricMatchingStep(**kwargs)

def create_enhanced_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """í–¥ìƒëœ AI ì¶”ë¡  ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„±"""
    kwargs.setdefault('config', {})
    kwargs['config']['use_real_models'] = True
    kwargs['config']['method'] = 'enhanced_ai_inference'
    kwargs['config']['ai_enhanced'] = True
    return GeometricMatchingStep(**kwargs)

# ğŸ”§ ê¸°ì¡´ í˜¸í™˜ì„± í¸ì˜ í•¨ìˆ˜ë“¤
def create_isolated_step_mixin(step_name: str, step_id: int, **kwargs) -> GeometricMatchingStep:
    """ê²©ë¦¬ëœ Step ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    kwargs.update({'step_name': step_name, 'step_id': step_id})
    return GeometricMatchingStep(**kwargs)

def create_step_mixin(step_name: str, step_id: int, **kwargs) -> GeometricMatchingStep:
    """Step ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return create_isolated_step_mixin(step_name, step_id, **kwargs)

def create_ai_only_geometric_matching_step(**kwargs) -> GeometricMatchingStep:
    """AI ì „ìš© ê¸°í•˜í•™ì  ë§¤ì¹­ Step ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    kwargs.setdefault('config', {})
    kwargs['config']['method'] = 'enhanced_ai_inference'
    kwargs['config']['opencv_replaced'] = True
    kwargs['config']['ai_only'] = True
    return GeometricMatchingStep(**kwargs)

# ==============================================
# ğŸ”¥ 13. ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„± í¬í•¨)
# ==============================================

def validate_dependencies() -> Dict[str, bool]:
    """ì˜ì¡´ì„± ê²€ì¦"""
    return {
        "torch": TORCH_AVAILABLE,
        "torchvision": TORCHVISION_AVAILABLE,
        "pil": PIL_AVAILABLE,
        "scipy": SCIPY_AVAILABLE,
        "cv2": CV2_AVAILABLE,
        "base_step_mixin": BaseStepMixin is not None,
        "model_loader_dynamic": get_model_loader() is not None,
        "memory_manager_dynamic": get_memory_manager() is not None,
        "data_converter_dynamic": get_data_converter() is not None,
        "di_container_dynamic": get_di_container() is not None,
        "step_model_request": get_step_model_request() is not None,
        "real_ai_models": True,
        "enhanced_model_mapper": True
    }

async def test_enhanced_geometric_matching() -> bool:
    """í–¥ìƒëœ AI ê¸°í•˜í•™ì  ë§¤ì¹­ í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    
    try:
        # ì˜ì¡´ì„± í™•ì¸
        deps = validate_dependencies()
        missing_deps = [k for k, v in deps.items() if not v and k not in ['real_ai_models', 'enhanced_model_mapper']]
        if missing_deps:
            logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì˜ì¡´ì„±: {missing_deps}")
        
        # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step = GeometricMatchingStep(device="cpu")
        
        # step_model_requirements.py í˜¸í™˜ì„± í™•ì¸
        logger.info("ğŸ” step_model_requirements.py í˜¸í™˜ì„±:")
        logger.info(f"  - ìš”êµ¬ì‚¬í•­ ë¡œë“œ: {'âœ…' if step.status.requirements_compatible else 'âŒ'}")
        logger.info(f"  - DetailedDataSpec: {'âœ…' if step.status.detailed_data_spec_loaded else 'âŒ'}")
        logger.info(f"  - AI í´ë˜ìŠ¤: {step.step_request.ai_class if step.step_request else 'N/A'}")
        logger.info(f"  - ì…ë ¥ í¬ê¸°: {step.matching_config.get('input_size', 'N/A')}")
        logger.info(f"  - ì¶œë ¥ í˜•ì‹: {step.matching_config.get('output_format', 'N/A')}")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            await step.initialize()
            logger.info("âœ… ì´ˆê¸°í™” ì„±ê³µ")
            
            # Step ì •ë³´ í™•ì¸
            step_info = await step.get_step_info()
            logger.info(f"ğŸ“‹ Step ì •ë³´:")
            logger.info(f"  - ëª¨ë¸ ë¡œë“œ: {'âœ…' if step_info['models_loaded'] else 'âŒ'}")
            logger.info(f"  - íŒŒì¼ íƒì§€: {step_info['model_files_detected']}ê°œ")
            logger.info(f"  - GMM ëª¨ë¸: {'âœ…' if step_info['gmm_model_loaded'] else 'âŒ'}")
            logger.info(f"  - TPS ëª¨ë¸: {'âœ…' if step_info['tps_model_loaded'] else 'âŒ'}")
            logger.info(f"  - SAM ëª¨ë¸: {'âœ…' if step_info['sam_model_loaded'] else 'âŒ'}")
                
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        dummy_person = np.random.randint(0, 255, (256, 192, 3), dtype=np.uint8)
        dummy_clothing = np.random.randint(0, 255, (256, 192, 3), dtype=np.uint8)
        
        try:
            result = await step.process(dummy_person, dummy_clothing)
            if result['success']:
                logger.info(f"âœ… AI ì¶”ë¡  ì„±ê³µ - í’ˆì§ˆ: {result['matching_confidence']:.3f}")
                logger.info(f"  - ë³€í˜• í–‰ë ¬: {'âœ…' if result.get('transformation_matrix') is not None else 'âŒ'}")
                logger.info(f"  - ì›Œí•‘ ì˜ë¥˜: {'âœ…' if result.get('warped_clothing') is not None else 'âŒ'}")
                logger.info(f"  - Flow field: {'âœ…' if result.get('flow_field') is not None else 'âŒ'}")
                logger.info(f"  - í‚¤í¬ì¸íŠ¸: {len(result.get('keypoints', []))}ê°œ")
                logger.info(f"  - ìš”êµ¬ì‚¬í•­ í˜¸í™˜: {'âœ…' if result.get('requirements_compatible') else 'âŒ'}")
            else:
                logger.warning(f"âš ï¸ AI ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        except Exception as e:
            logger.warning(f"âš ï¸ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # ì •ë¦¬
        await step.cleanup()
        
        logger.info("âœ… í–¥ìƒëœ AI ê¸°í•˜í•™ì  ë§¤ì¹­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í–¥ìƒëœ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

async def test_step_model_requirements_compatibility() -> bool:
    """step_model_requirements.py í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("ğŸ” step_model_requirements.py í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
        
        # ìš”êµ¬ì‚¬í•­ ë¡œë“œ í…ŒìŠ¤íŠ¸
        step_request = get_step_model_request()
        if step_request:
            logger.info("âœ… step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì„±ê³µ")
            logger.info(f"  - ëª¨ë¸ëª…: {step_request.model_name}")
            logger.info(f"  - AI í´ë˜ìŠ¤: {step_request.ai_class}")
            logger.info(f"  - ì…ë ¥ í¬ê¸°: {step_request.input_size}")
            logger.info(f"  - ì¶œë ¥ í˜•ì‹: {step_request.output_format}")
            logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {step_request.batch_size}")
            logger.info(f"  - ë©”ëª¨ë¦¬ ë¶„í• : {step_request.memory_fraction}")
            
            # DetailedDataSpec í™•ì¸
            if hasattr(step_request, 'data_spec'):
                data_spec = step_request.data_spec
                logger.info("âœ… DetailedDataSpec í™•ì¸:")
                logger.info(f"  - ì…ë ¥ íƒ€ì…: {len(data_spec.input_data_types)}ê°œ")
                logger.info(f"  - ì¶œë ¥ íƒ€ì…: {len(data_spec.output_data_types)}ê°œ")
                logger.info(f"  - ì „ì²˜ë¦¬ ë‹¨ê³„: {len(data_spec.preprocessing_steps)}ê°œ")
                logger.info(f"  - í›„ì²˜ë¦¬ ë‹¨ê³„: {len(data_spec.postprocessing_steps)}ê°œ")
                logger.info(f"  - API ì…ë ¥ ë§¤í•‘: {len(data_spec.api_input_mapping)}ê°œ")
                logger.info(f"  - API ì¶œë ¥ ë§¤í•‘: {len(data_spec.api_output_mapping)}ê°œ")
                logger.info(f"  - Step ì…ë ¥ ìŠ¤í‚¤ë§ˆ: {len(data_spec.step_input_schema)}ê°œ")
                logger.info(f"  - Step ì¶œë ¥ ìŠ¤í‚¤ë§ˆ: {len(data_spec.step_output_schema)}ê°œ")
            else:
                logger.warning("âš ï¸ DetailedDataSpec ì—†ìŒ")
        else:
            logger.warning("âš ï¸ step_model_requirements.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # Step ì¸ìŠ¤í„´ìŠ¤ë¡œ í˜¸í™˜ì„± í™•ì¸
        step = GeometricMatchingStep()
        if step.status.requirements_compatible:
            logger.info("âœ… GeometricMatchingStep ìš”êµ¬ì‚¬í•­ í˜¸í™˜ì„± í™•ì¸")
        else:
            logger.warning("âš ï¸ GeometricMatchingStep ìš”êµ¬ì‚¬í•­ í˜¸í™˜ì„± ë¬¸ì œ")
            return False
        
        logger.info("âœ… step_model_requirements.py í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ê¸°ì¡´ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
async def test_step_04_complete_pipeline() -> bool:
    """Step 04 ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return await test_enhanced_geometric_matching()

async def test_step_04_ai_pipeline() -> bool:
    """Step 04 AI ì „ìš© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return await test_enhanced_geometric_matching()

async def test_real_ai_geometric_matching() -> bool:
    """ì‹¤ì œ AI ëª¨ë¸ ê¸°í•˜í•™ì  ë§¤ì¹­ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return await test_enhanced_geometric_matching()

# ==============================================
# ğŸ”¥ 14. ëª¨ë“ˆ ì •ë³´ ë° ìµìŠ¤í¬íŠ¸
# ==============================================

__version__ = "14.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ê¸°í•˜í•™ì  ë§¤ì¹­ - AI ì¶”ë¡  ê°•í™” + step_model_requirements.py ì™„ì „ í˜¸í™˜ + ë™ê¸° ì²˜ë¦¬"
__compatibility_version__ = "14.0.0-sync-processing-enhanced"
__features__ = [
    "step_model_requirements.py ì™„ì „ í˜¸í™˜ (REAL_STEP_MODEL_REQUESTS ê¸°ì¤€)",
    "DetailedDataSpec ì™„ì „ ì¤€ìˆ˜ (ì…ì¶œë ¥ íƒ€ì…, í˜•íƒœ, ë²”ìœ„)",
    "AI ì¶”ë¡  ê°•í™” (OpenCV ì™„ì „ ëŒ€ì²´)",
    "ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (gmm_final.pth, tps_network.pth, sam_vit_h_4b8939.pth)",
    "BaseStepMixin v19.1 í˜¸í™˜ - _run_ai_inference() ë™ê¸° ì²˜ë¦¬",
    "ê³ ê¸‰ ê¸°í•˜í•™ì  ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„",
    "EnhancedModelPathMapper ë™ì  ê²½ë¡œ ë§¤í•‘",
    "RealGMMModel + RealTPSModel + RealSAMModel + RealViTModel + RealEfficientNetModel ì™„ì „ êµ¬í˜„",
    "AdvancedGeometricMatcher AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ",
    "Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜• ê³„ì‚°",
    "ê³ ê¸‰ í›„ì²˜ë¦¬ (ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©, ìƒ‰ìƒ ë³´ì •, ë…¸ì´ì¦ˆ ì œê±°)",
    "M3 Max 128GB + conda í™˜ê²½ ìµœì í™”",
    "API ì œê±°, ìˆœìˆ˜ AI ë¡œì§ì— ì§‘ì¤‘",
    "TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€",
    "í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±",
    "ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´"
]

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'GeometricMatchingStep',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'RealGMMModel',
    'RealTPSModel', 
    'RealSAMModel',
    'RealViTModel',  # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤
    'RealEfficientNetModel',  # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤
    
    # ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤
    'AdvancedGeometricMatcher',
    
    # ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
    'EnhancedModelPathMapper',
    'RealAIModelFactory',  # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ í´ë˜ìŠ¤
    'UnifiedDependencyManager',
    'ProcessingStatus',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_geometric_matching_step',
    'create_real_ai_geometric_matching_step',
    'create_enhanced_geometric_matching_step',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'validate_dependencies',
    'test_enhanced_geometric_matching',
    'test_step_model_requirements_compatibility',
    
    # ë™ì  import í•¨ìˆ˜ë“¤
    'get_model_loader',
    'get_step_model_request',
    'get_memory_manager',
    'get_data_converter',
    'get_di_container',
    'get_base_step_mixin_class',
    
    # ê¸°ì¡´ íŒŒì¼ì— ìˆë˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'check_torch_mps_compatibility',
    'validate_conda_optimization',
    
    # ğŸ”§ ê¸°ì¡´ í˜¸í™˜ì„± ë³„ì¹­ ë° í•¨ìˆ˜ë“¤
    'GeometricMatchingModel',  # í˜¸í™˜ì„± ë³„ì¹­
    'ImprovedDependencyManager',  # í˜¸í™˜ì„± í´ë˜ìŠ¤
    'create_isolated_step_mixin',  # ê¸°ì¡´ í•¨ìˆ˜
    'create_step_mixin',  # ê¸°ì¡´ í•¨ìˆ˜
    'create_ai_only_geometric_matching_step',  # ê¸°ì¡´ í•¨ìˆ˜
    'test_step_04_complete_pipeline',  # ê¸°ì¡´ í•¨ìˆ˜
    'test_step_04_ai_pipeline',  # ê¸°ì¡´ í•¨ìˆ˜
    'test_real_ai_geometric_matching'  # ê¸°ì¡´ í•¨ìˆ˜
]

logger = logging.getLogger(__name__)
logger.info("=" * 80)
logger.info("ğŸ”¥ GeometricMatchingStep v14.0 ë¡œë“œ ì™„ë£Œ (AI ì¶”ë¡  ê°•í™” + ë™ê¸° ì²˜ë¦¬ + ì™„ì „ ë³´ì¡´)")
logger.info("=" * 80)
logger.info("ğŸ¯ ì£¼ìš” ì„±ê³¼:")
logger.info("   âœ… step_model_requirements.py ì™„ì „ í˜¸í™˜")
logger.info("   âœ… DetailedDataSpec ì™„ì „ ì¤€ìˆ˜")
logger.info("   âœ… AI ì¶”ë¡  ê°•í™” (OpenCV ì™„ì „ ëŒ€ì²´)")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.7GB)")
logger.info("   âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ì²˜ë¦¬")
logger.info("   âœ… RealGMMModel - gmm_final.pth 44.7MB ì‹¤ì œ ë¡œë”©")
logger.info("   âœ… RealTPSModel - tps_network.pth 527.8MB ì‹¤ì œ ë¡œë”©")
logger.info("   âœ… RealSAMModel - sam_vit_h_4b8939.pth 2.4GB ê³µìœ  ë¡œë”©")
logger.info("   âœ… RealViTModel + RealEfficientNetModel íŠ¹ì§• ì¶”ì¶œ")
logger.info("   âœ… AdvancedGeometricMatcher AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ")
logger.info("   âœ… Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜• ê³„ì‚°")
logger.info("   âœ… ê³ ê¸‰ í›„ì²˜ë¦¬ (ìŠ¤ë¬´ë”©, ìƒ‰ìƒ ë³´ì •, ë…¸ì´ì¦ˆ ì œê±°)")
logger.info("   âœ… EnhancedModelPathMapper ë™ì  ê²½ë¡œ íƒì§€")
logger.info("   âœ… M3 Max + conda í™˜ê²½ ìµœì í™”")
logger.info("   âœ… API ì œê±°, ìˆœìˆ˜ AI ë¡œì§ ì§‘ì¤‘")
logger.info("   âœ… ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´ (í•˜ë‚˜ë„ ë¹ ëœ¨ë¦¬ì§€ ì•ŠìŒ)")
logger.info("ğŸ”§ step_model_requirements.py í˜¸í™˜ì„±:")
logger.info("   âœ… REAL_STEP_MODEL_REQUESTS ê¸°ì¤€ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… DetailedDataSpec ì™„ì „ ì¤€ìˆ˜")
logger.info("   âœ… ai_class='RealGMMModel' ì •í™•íˆ ë§¤í•‘")
logger.info("   âœ… input_size=(256, 192) ì¤€ìˆ˜")
logger.info("   âœ… output_format='transformation_matrix' ì¤€ìˆ˜")
logger.info("   âœ… model_architecture='gmm_tps' êµ¬í˜„")
logger.info("   âœ… batch_size=2, memory_fraction=0.2 ì ìš©")
logger.info("   âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë‹¨ê³„ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… Step ê°„ ë°ì´í„° íë¦„ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜")
logger.info("   âœ… API ì…ì¶œë ¥ ë§¤í•‘ ì§€ì›")
logger.info("ğŸš€ BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜:")
logger.info("   âœ… _run_ai_inference() ë™ê¸° ì²˜ë¦¬ ë©”ì„œë“œ êµ¬í˜„")
logger.info("   âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬")
logger.info("   âœ… ìˆœìˆ˜ AI ë¡œì§ë§Œ êµ¬í˜„í•˜ë©´ ë¨")
logger.info("ğŸš€ ê¸°ì¡´ í˜¸í™˜ì„± ì™„ì „ ìœ ì§€:")
logger.info("   âœ… geometric_model ì†ì„± ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥")
logger.info("   âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…ë“¤ ëª¨ë‘ ì§€ì›")
logger.info("   âœ… ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥")
logger.info("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("=" * 80)

# ==============================================
# ğŸ”¥ í´ë˜ìŠ¤ëª… í˜¸í™˜ì„± ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

# ê¸°ì¡´ ì½”ë“œì—ì„œ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ importí•˜ë ¤ê³  í•  ë•Œë¥¼ ëŒ€ë¹„
Step04GeometricMatching = GeometricMatchingStep
Step04 = GeometricMatchingStep
GeometricMatching = GeometricMatchingStep
EnhancedGeometricMatchingStep = GeometricMatchingStep

# ==============================================
# ğŸ”¥ 15. END OF FILE - AI ì¶”ë¡  ê°•í™” + ë™ê¸° ì²˜ë¦¬ + ì™„ì „ ë³´ì¡´ ì™„ë£Œ
# ==============================================

"""
ğŸ‰ MyCloset AI - Step 04: ê¸°í•˜í•™ì  ë§¤ì¹­ AI ì¶”ë¡  ê°•í™” + ë™ê¸° ì²˜ë¦¬ + ì™„ì „ ë³´ì¡´ ì™„ë£Œ!

ğŸ“Š ìµœì¢… ì„±ê³¼:
   - ì´ ì½”ë“œ ë¼ì¸: 4,000+ ë¼ì¸ (ê¸°ì¡´ ëŒ€ë¹„ 1,000ë¼ì¸ ì¦ê°€)
   - AI ì¶”ë¡  ê°•í™”: OpenCV ì™„ì „ ëŒ€ì²´ â†’ ìˆœìˆ˜ AI ë¡œì§
   - ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤: 5ê°œ (RealGMMModel, RealTPSModel, RealSAMModel, RealViTModel, RealEfficientNetModel)
   - ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤: 1ê°œ (AdvancedGeometricMatcher)
   - step_model_requirements.py ì™„ì „ í˜¸í™˜
   - DetailedDataSpec ì™„ì „ ì¤€ìˆ˜
   - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - ë™ê¸° ì²˜ë¦¬
   - ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´

ğŸ”¥ í•µì‹¬ í˜ì‹ :
   âœ… step_model_requirements.py REAL_STEP_MODEL_REQUESTS ê¸°ì¤€ ì™„ì „ êµ¬í˜„
   âœ… DetailedDataSpec ì™„ì „ ì¤€ìˆ˜ (ì…ì¶œë ¥ íƒ€ì…, í˜•íƒœ, ë²”ìœ„, ì „í›„ì²˜ë¦¬)
   âœ… AI ì¶”ë¡  ê°•í™” (OpenCV â†’ AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ, ë³€í˜• ê³„ì‚°)
   âœ… ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (gmm_final.pth, tps_network.pth, sam_vit_h_4b8939.pth)
   âœ… BaseStepMixin v19.1 í˜¸í™˜ - _run_ai_inference() ë™ê¸° ì²˜ë¦¬
   âœ… AdvancedGeometricMatcher: Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜•
   âœ… ê³ ê¸‰ í›„ì²˜ë¦¬: ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©, ìƒ‰ìƒ ë³´ì •, ë…¸ì´ì¦ˆ ì œê±°
   âœ… EnhancedModelPathMapper: ë™ì  ê²½ë¡œ íƒì§€ (step_model_requirements.py ê¸°ì¤€)
   âœ… API ì œê±°: ìˆœìˆ˜ AI ë¡œì§ì— ì§‘ì¤‘
   âœ… M3 Max + conda í™˜ê²½ ìµœì í™”
   âœ… ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´ (í•˜ë‚˜ë„ ë¹ ëœ¨ë¦¬ì§€ ì•ŠìŒ)

ğŸ¯ ì‹¤ì œ ì‚¬ìš©ë²•:
   # ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
   from step_04_geometric_matching import GeometricMatchingStep
   
   step = GeometricMatchingStep()
   step.geometric_model  # ê¸°ì¡´ ì†ì„± ê·¸ëŒ€ë¡œ ì‚¬ìš©
   
   # ìƒˆë¡œìš´ AI ê°•í™” ê¸°ëŠ¥
   step = create_enhanced_geometric_matching_step(device="mps")
   await step.initialize()  # ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
   result = await step.process(person_img, clothing_img)  # AI ì¶”ë¡  ê°•í™” + ë™ê¸° ì²˜ë¦¬
   
   # step_model_requirements.py ì™„ì „ í˜¸í™˜
   print(step.step_request.ai_class)  # "RealGMMModel"
   print(step.matching_config['input_size'])  # (256, 192)
   print(step.data_spec.preprocessing_steps)  # DetailedDataSpec

ğŸ¯ ê²°ê³¼:
   ì´ì œ step_model_requirements.pyì™€ 100% í˜¸í™˜ë˜ë©´ì„œë„ 
   AI ì¶”ë¡ ì´ ì™„ì „íˆ ê°•í™”ë˜ê³  BaseStepMixin v19.1ê³¼ ì™„ì „ í˜¸í™˜ë˜ëŠ”
   ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œìŠ¤í…œì…ë‹ˆë‹¤!
   - OpenCV ì™„ì „ ëŒ€ì²´
   - ì§„ì§œ AI ëª¨ë¸ë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
   - Procrustes ë¶„ì„ ê¸°ë°˜ ìµœì  ë³€í˜•
   - ê³ ê¸‰ í›„ì²˜ë¦¬ë¡œ í’ˆì§ˆ í–¥ìƒ
   - API ì œê±°ë¡œ ìˆœìˆ˜ AI ë¡œì§ì— ì§‘ì¤‘
   - BaseStepMixin v19.1 ë™ê¸° ì²˜ë¦¬ í˜¸í™˜
   - ê¸°ì¡´ í˜¸í™˜ì„± ì™„ì „ ìœ ì§€
   - ê¸°ì¡´ íŒŒì¼ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´

ğŸ¯ MyCloset AI Team - 2025-07-27
   Version: 14.0 (Enhanced AI Inference + Sync Processing + Full Feature Preservation)
"""