#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Common Imports Integration
=====================================================================

âœ… Common Imports ì‹œìŠ¤í…œ ì™„ì „ í†µí•© - ì¤‘ë³µ import ë¸”ë¡ ì œê±°
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net, Mask R-CNN ì§€ì›
âœ… ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ 100% ìœ ì§€ - ASPP, Self-Correction, Progressive Parsing
âœ… 50% ì½”ë“œ ë‹¨ì¶• - 2000ì¤„ â†’ 1000ì¤„ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)
âœ… ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ - Mock ì œê±°í•˜ê³  ì§„ì§œ ëª¨ë¸ ì‚¬ìš©
âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›
âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 33.1 (Common Imports Integration)
"""

# ğŸ”¥ ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, gc, time, logging, threading, math, hashlib, json, base64, warnings, np,
    Path, Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING,
    dataclass, field, Enum, BytesIO, ThreadPoolExecutor,
    
    # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
    error_tracker, track_exception, get_error_summary, create_exception_response, convert_to_mycloset_exception,
    ErrorCodes, EXCEPTIONS_AVAILABLE,
    
    # Mock Data Diagnostic
    detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    cv2, PIL_AVAILABLE, CV2_AVAILABLE, NUMPY_AVAILABLE, Image, ImageEnhance
)

# PIL Image import ì¶”ê°€
if PIL_AVAILABLE:
    from PIL import Image as PILImage

# ì¶”ê°€ imports
import weakref
from abc import ABC, abstractmethod

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ìµœìƒë‹¨ì— ì¶”ê°€
logger = logging.getLogger(__name__)

# ğŸ”¥ PyTorch ë¡œë”© ìµœì í™” - ìˆ˜ì •
try:
    from fix_pytorch_loading import apply_pytorch_patch
    apply_pytorch_patch()
except ImportError:
    logger.warning("âš ï¸ fix_pytorch_loading ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ PyTorch ë¡œë”© ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch ë¡œë”© íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# ğŸ”¥ PyTorch í†µí•© import - ì¤‘ë³µ ì œê±°
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("âŒ PyTorch í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    if EXCEPTIONS_AVAILABLE:
        error = ModelLoadingError("PyTorch í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨", ErrorCodes.MODEL_LOADING_FAILED)
        track_exception(error, {'library': 'torch'}, 3)
        raise error
    else:
        raise

# model_architecturesì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ë“¤ ì„í¬íŠ¸
try:
    from ..utils.model_architectures import (
        SAMModel, U2NetModel, DeepLabV3PlusModel
    )
    MODEL_ARCHITECTURES_AVAILABLE = True
except ImportError:
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„
        from app.ai_pipeline.utils.model_architectures import (
            SAMModel, U2NetModel, DeepLabV3PlusModel
        )
        MODEL_ARCHITECTURES_AVAILABLE = True
    except ImportError:
        # ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        MODEL_ARCHITECTURES_AVAILABLE = False
        SAMModel = None
        U2NetModel = None
        DeepLabV3PlusModel = None

def detect_m3_max():
    """M3 Max ê°ì§€"""
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€) - ClothSegmentationìš©
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - ClothSegmentationìš©"""
    try:
        # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
        import_paths = [
            'app.ai_pipeline.steps.base_step_mixin',
            '.base_step_mixin',
            'backend.app.ai_pipeline.steps.base_step_mixin'
        ]
        
        for import_path in import_paths:
            try:
                import importlib
                if import_path.startswith('.'):
                    module = importlib.import_module(import_path, package='app.ai_pipeline.steps')
                else:
                    module = importlib.import_module(import_path)
                base_step_mixin = getattr(module, 'BaseStepMixin', None)
                if base_step_mixin:
                    return base_step_mixin
            except ImportError:
                continue
        
        return None
    except Exception as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (ClothSegmentation íŠ¹í™”)
if BaseStepMixin is None:
    class BaseStepMixin:
        """ClothSegmentationStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤"""
        
        def __init__(self, **kwargs):
            print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin __init__ ì‹œì‘")
            
            # ê¸°ë³¸ ì†ì„±ë“¤ (ì•ˆì „í•œ ì´ˆê¸°í™”)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Logger ì´ˆê¸°í™” ì‹œì‘")
                self.logger = logging.getLogger(self.__class__.__name__)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Logger ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Logger ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.logger = None
            
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê¸°ë³¸ ì†ì„± ì„¤ì • ì‹œì‘")
                self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
                self.step_id = kwargs.get('step_id', 3)
                self.device = kwargs.get('device', 'cpu')
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê¸°ë³¸ ì†ì„± ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê¸°ë³¸ ì†ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
                self.step_name = 'ClothSegmentationStep'
                self.step_id = 3
                self.device = 'cpu'
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (ClothSegmentationì´ í•„ìš”ë¡œ í•˜ëŠ”)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
                self.ai_models = {}
                self.models_loading_status = {
                    'deeplabv3plus': False,
                    'maskrcnn': False,
                    'sam_huge': False,
                    'u2net_cloth': False,
                    'total_loaded': 0,
                    'loading_errors': []
                }
                self.model_interface = None
                self.loaded_models = {}
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ì†ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_models = {}
                self.models_loading_status = {'loading_errors': []}
                self.model_interface = None
                self.loaded_models = {}
            
            # ClothSegmentation íŠ¹í™” ì†ì„±ë“¤
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentation ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
                self.segmentation_models = {}
                self.segmentation_ready = False
                self.cloth_cache = {}
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentation ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentation ì†ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.segmentation_models = {}
                self.segmentation_ready = False
                self.cloth_cache = {}
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜ ì‹œì‘")
                self.cloth_categories = {
                    0: 'background',
                    1: 'shirt', 2: 't_shirt', 3: 'sweater', 4: 'hoodie',
                    5: 'jacket', 6: 'coat', 7: 'dress', 8: 'skirt',
                    9: 'pants', 10: 'jeans', 11: 'shorts',
                    12: 'shoes', 13: 'boots', 14: 'sneakers',
                    15: 'bag', 16: 'hat', 17: 'glasses', 18: 'scarf', 19: 'belt'
                }
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜ ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜ ì‹¤íŒ¨: {e}")
                self.cloth_categories = {}
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒíƒœ ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
                self.is_initialized = False
                self.is_ready = False
                self.has_model = False
                self.model_loaded = False
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒíƒœ ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒíƒœ ì†ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.is_initialized = False
                self.is_ready = False
                self.has_model = False
                self.model_loaded = False
            
            # Central Hub DI Container ê´€ë ¨
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
                self.model_loader = None
                self.memory_manager = None
                self.data_converter = None
                self.di_container = None
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ì†ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.memory_manager = None
                self.data_converter = None
                self.di_container = None
            
            # í†µê³„
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] í†µê³„ ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
                self.ai_stats = {
                    'total_processed': 0,
                    'deeplabv3_calls': 0,
                    'sam_calls': 0,
                    'u2net_calls': 0,
                    'average_confidence': 0.0
                }
                print(f"ğŸ”¥ [ë””ë²„ê¹…] í†µê³„ ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] í†µê³„ ì†ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ai_stats = {'total_processed': 0}
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin ì´ˆê¸°í™” ì™„ë£Œ - step_name: {self.step_name}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] logger íƒ€ì…: {type(self.logger)}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] logger ì´ë¦„: {self.logger.name}")
            
            try:
                self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ë¡œê·¸ ì¶œë ¥ ì„±ê³µ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ë¡œê·¸ ì¶œë ¥ ì‹¤íŒ¨: {e}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
                import traceback
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        
 
        def process(self, **kwargs) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ (ë™ê¸° ë²„ì „) - ì¤‘ë³µ ë³€í™˜ ì œê±°"""
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentationStep.process() ì§„ì…!")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs í‚¤ë“¤: {list(kwargs.keys()) if kwargs else 'None'}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs ê°’ë“¤: {[(k, type(v).__name__) for k, v in kwargs.items()] if kwargs else 'None'}")
            
            # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° ì¶”ì  ë¡œê¹… ì¶”ê°€
            session_id = kwargs.get('session_id', 'unknown')
            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ì‹œì‘ - session_id: {session_id}")
            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ì…ë ¥ ë°ì´í„° í¬ê¸°: {len(str(kwargs))} bytes")
            
            try:
                start_time = time.time()
                
                # ğŸ”¥ ìƒì„¸ ë¡œê¹…: process ë©”ì„œë“œ ì‹œì‘
                self.logger.info(f"ğŸ”„ {self.step_name} process ì‹œì‘ (Central Hub)")
                self.logger.info(f"ğŸ” process kwargs í‚¤ë“¤: {list(kwargs.keys())}")
                self.logger.info(f"ğŸ” process kwargs íƒ€ì…: {type(kwargs)}")
                
                # ğŸ”¥ ì…ë ¥ ë°ì´í„°ëŠ” ì´ë¯¸ step_service.pyì—ì„œ ë³€í™˜ë¨ (ì¤‘ë³µ ë³€í™˜ ì™„ì „ ì œê±°)
                processed_input = kwargs
                
                # ğŸ”¥ ìƒì„¸ ë¡œê¹…: processed_input í™•ì¸
                self.logger.info(f"ğŸ” processed_input í‚¤ë“¤: {list(processed_input.keys())}")
                self.logger.info(f"ğŸ” processed_input íƒ€ì…: {type(processed_input)}")
                
                # session_data í™•ì¸
                if 'session_data' in processed_input:
                    session_data = processed_input['session_data']
                    self.logger.info(f"âœ… session_data ìˆìŒ: íƒ€ì…={type(session_data)}, í‚¤ë“¤={list(session_data.keys()) if isinstance(session_data, dict) else 'N/A'}")
                else:
                    self.logger.warning("âš ï¸ session_data ì—†ìŒ")
                
                # ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸
                image_keys = ['image', 'clothing_image', 'cloth_image', 'person_image']
                found_images = []
                for key in image_keys:
                    if key in processed_input and processed_input[key] is not None:
                        found_images.append(key)
                self.logger.info(f"ğŸ” ë°œê²¬ëœ ì´ë¯¸ì§€ í‚¤ë“¤: {found_images}")
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ (ë™ê¸°ì ìœ¼ë¡œ)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference ë©”ì„œë“œ í™•ì¸")
                if hasattr(self, '_run_ai_inference'):
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference ë©”ì„œë“œ ë°œê²¬ - í˜¸ì¶œ ì‹œì‘")
                    try:
                        result = self._run_ai_inference(processed_input)
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference í˜¸ì¶œ ì™„ë£Œ")
                        
                        # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                        if isinstance(result, dict):
                            result['processing_time'] = time.time() - start_time
                            result['step_name'] = self.step_name
                            result['step_id'] = self.step_id
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
                        
                        # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° ì €ì¥ ë¡œê¹… ì¶”ê°€
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ì™„ë£Œ - session_id: {session_id}")
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ê²°ê³¼ ë°ì´í„° í¬ê¸°: {len(str(result))} bytes")
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ì„±ê³µ ì—¬ë¶€: {result.get('success', False)}")
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
                        
                        # ğŸ”¥ ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ë¡œê¹…
                        if result.get('success', False) and 'segmentation_result' in result:
                            seg_data = result['segmentation_result']
                            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 3 â†’ Step 4 ì „ë‹¬ ë°ì´í„° ì¤€ë¹„:")
                            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - segmentation_result íƒ€ì…: {type(seg_data)}")
                            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - segmentation_result í‚¤ë“¤: {list(seg_data.keys()) if isinstance(seg_data, dict) else 'N/A'}")
                            if isinstance(seg_data, dict) and 'masks' in seg_data:
                                masks = seg_data['masks']
                                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - masks íƒ€ì…: {type(masks)}")
                                if isinstance(masks, dict):
                                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - masks í‚¤ ê°œìˆ˜: {len(masks)}")
                        
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] process() ë©”ì„œë“œ ì™„ë£Œ - ê²°ê³¼ ë°˜í™˜")
                        return result
                    except Exception as e:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
                        import traceback
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                        raise
                else:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference ë©”ì„œë“œ ì—†ìŒ - ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                if EXCEPTIONS_AVAILABLE:
                    error = convert_to_mycloset_exception(e, {
                        'step_name': self.step_name,
                        'step_id': self.step_id,
                        'operation': 'process'
                    })
                    track_exception(error, {
                        'step_name': self.step_name,
                        'step_id': self.step_id,
                        'operation': 'process'
                    }, self.step_id)
                    
                    return create_exception_response(
                        error,
                        self.step_name,
                        self.step_id,
                        "unknown"
                    )
                else:
                    return {
                        'success': False,
                        'error': str(e),
                        'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
        
        def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
                
                # Central Hubë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                injected_count = _inject_dependencies_safe(self)
                if injected_count > 0:
                    self.logger.info(f"âœ… Central Hub ì˜ì¡´ì„± ì£¼ì…: {injected_count}ê°œ")
                
                # Cloth Segmentation ëª¨ë¸ë“¤ ë¡œë”© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” _load_segmentation_models_via_central_hub í˜¸ì¶œ)
                if hasattr(self, '_load_segmentation_models_via_central_hub'):
                    self._load_segmentation_models_via_central_hub()
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                if EXCEPTIONS_AVAILABLE:
                    error = convert_to_mycloset_exception(e, {
                        'step_name': self.step_name,
                        'step_id': self.step_id,
                        'operation': 'initialize'
                    })
                    track_exception(error, {
                        'step_name': self.step_name,
                        'step_id': self.step_id,
                        'operation': 'initialize'
                    }, self.step_id)
                return False
        
        def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
                
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
                
                # ìºì‹œ ì •ë¦¬
                self.ai_models.clear()
                if hasattr(self, 'segmentation_models'):
                    self.segmentation_models.clear()
                if hasattr(self, 'cloth_cache'):
                    self.cloth_cache.clear()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    if TORCH_AVAILABLE:
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        elif MPS_AVAILABLE:
                            torch.mps.empty_cache()
                except:
                    pass
                
                import gc
                gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'segmentation_ready': getattr(self, 'segmentation_ready', False),
                'models_loaded': len(getattr(self, 'loaded_models', {})),
                'segmentation_models': list(getattr(self, 'segmentation_models', {}).keys()),
                'cloth_categories': len(getattr(self, 'cloth_categories', {})),
                'fallback_mode': True
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.model_loader = model_loader
                self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
                # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                        self.model_interface = model_loader
                else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.data_converter = data_converter
                self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            try:
                self.di_container = di_container
                self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

        def _get_step_requirements(self) -> Dict[str, Any]:
            """Step 03 ClothSegmentation ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
            return {
                "required_models": [
                    "deeplabv3plus_resnet101.pth",
                    "sam_vit_h_4b8939.pth",
                    "u2net.pth",
                    "maskrcnn_resnet50_fpn.pth"
                ],
                "primary_model": "deeplabv3plus_resnet101.pth",
                "model_configs": {
                    "deeplabv3plus_resnet101.pth": {
                        "size_mb": 233.3,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "precision": "high",
                        "num_classes": 20
                    },
                    "sam_vit_h_4b8939.pth": {
                        "size_mb": 2445.7,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "shared_with": ["step_04_geometric_matching"]
                    },
                    "u2net.pth": {
                        "size_mb": 168.1,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "cloth_specialized": True
                    },
                    "maskrcnn_resnet50_fpn.pth": {
                        "size_mb": 328.4,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "instance_segmentation": True
                    }
                },
                "verified_paths": [
                    "step_03_cloth_segmentation/deeplabv3plus_resnet101.pth",
                    "step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                    "step_03_cloth_segmentation/u2net.pth",
                    "step_03_cloth_segmentation/maskrcnn_resnet50_fpn.pth"
                ],
                "cloth_categories": {
                    0: "background", 1: "shirt", 2: "t_shirt", 3: "sweater", 4: "hoodie",
                    5: "jacket", 6: "coat", 7: "dress", 8: "skirt", 9: "pants",
                    10: "jeans", 11: "shorts", 12: "shoes", 13: "boots", 14: "sneakers",
                    15: "bag", 16: "hat", 17: "glasses", 18: "scarf", 19: "belt"
                }
            }
        
# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - ClothSegmentation íŠ¹í™”
# ==============================================

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° - ClothSegmentationìš©"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… - ClothSegmentationìš©"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ - ClothSegmentationìš©"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None
    
# ==============================================
# ğŸ”¥ ì„¹ì…˜ 3: ì‹œìŠ¤í…œ í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================


# PyTorch (í•„ìˆ˜)
# ğŸ”¥ PyTorch í†µí•© import - ì¤‘ë³µ ì œê±°
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("âŒ PyTorch í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    if EXCEPTIONS_AVAILABLE:
        error = ModelLoadingError("PyTorch í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨", ErrorCodes.MODEL_LOADING_FAILED)
        track_exception(error, {'library': 'torch'}, 3)
        raise error
    else:
        raise

# PIL (í•„ìˆ˜) - common_importsì—ì„œ ì´ë¯¸ ë¡œë“œë¨
if not PIL_AVAILABLE:
    try:
        from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw
        PIL_AVAILABLE = True
        logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        logger.error("âŒ PIL í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
        if EXCEPTIONS_AVAILABLE:
            error = ModelLoadingError("PIL í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨", ErrorCodes.MODEL_LOADING_FAILED)
            track_exception(error, {'library': 'pil'}, 3)
            raise error
        else:
            raise

# NumPy (í•„ìˆ˜) - common_importsì—ì„œ ì´ë¯¸ ë¡œë“œë¨
if not NUMPY_AVAILABLE:
    try:
        import numpy as np
        NUMPY_AVAILABLE = True
        logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        logger.error("âŒ NumPy í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
        if EXCEPTIONS_AVAILABLE:
            error = ModelLoadingError("NumPy í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨", ErrorCodes.MODEL_LOADING_FAILED)
            track_exception(error, {'library': 'numpy'}, 3)
            raise error
        else:
            raise

# SAM (ì„ íƒì )
SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")



# SciPy (ê³ ê¸‰ í›„ì²˜ë¦¬ìš©) - ìˆ˜ì •
SCIPY_AVAILABLE = False
try:
    from scipy import ndimage
    SCIPY_AVAILABLE = True
    logger.info("ğŸ”¬ SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SciPy ì—†ìŒ - ê³ ê¸‰ í›„ì²˜ë¦¬ ì œí•œ")
except Exception as e:
    logger.warning(f"âš ï¸ SciPy ë¡œë“œ ì‹¤íŒ¨: {e}")

# DenseCRF (CRF í›„ì²˜ë¦¬ìš©) - ì¶”ê°€
DENSECRF_AVAILABLE = False
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
    logger.info("ğŸ”¥ DenseCRF ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.info("â„¹ï¸ DenseCRF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - CRF í›„ì²˜ë¦¬ ê¸°ëŠ¥ ì œí•œ (ì„ íƒì  ê¸°ëŠ¥)")
except Exception as e:
    logger.info(f"â„¹ï¸ DenseCRF ë¡œë“œ ì‹¤íŒ¨: {e} (ì„ íƒì  ê¸°ëŠ¥)")

# Scikit-image (ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬) - ìˆ˜ì •
SKIMAGE_AVAILABLE = False
try:
    from skimage import measure, morphology, segmentation, filters
    SKIMAGE_AVAILABLE = True
    logger.info("ğŸ”¬ Scikit-image ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Scikit-image ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
except Exception as e:
    logger.warning(f"âš ï¸ Scikit-image ë¡œë“œ ì‹¤íŒ¨: {e}")

# Torchvision - PyTorch importì—ì„œ ì´ë¯¸ ë¡œë“œë¨
TORCHVISION_AVAILABLE = TORCH_AVAILABLE
if TORCHVISION_AVAILABLE:
    try:
        import torchvision
        from torchvision import models
        logger.info("ğŸ¤– Torchvision ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        logger.warning("âš ï¸ Torchvision ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
        TORCHVISION_AVAILABLE = False

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 4: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„° êµ¬ì¡°
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    U2NET_CLOTH = "u2net_cloth"         # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB) - ìš°ì„ ìˆœìœ„ 1 (M3 Max ì•ˆì „)
    SAM_HUGE = "sam_huge"               # SAM ViT-Huge (2445.7MB) - ìš°ì„ ìˆœìœ„ 2 (ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ)
    DEEPLABV3_PLUS = "deeplabv3_plus"   # DeepLabV3+ (233.3MB) - ìš°ì„ ìˆœìœ„ 3 (ë‚˜ì¤‘ì—)
    MASK_RCNN = "mask_rcnn"             # Mask R-CNN (í´ë°±)
    HYBRID_AI = "hybrid_ai"             # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”

class ClothCategory(Enum):
    """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (ë‹¤ì¤‘ í´ë˜ìŠ¤)"""
    BACKGROUND = 0
    SHIRT = 1           # ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤
    T_SHIRT = 2         # í‹°ì…”ì¸ 
    SWEATER = 3         # ìŠ¤ì›¨í„°/ë‹ˆíŠ¸
    HOODIE = 4          # í›„ë“œí‹°
    JACKET = 5          # ì¬í‚·/ì•„ìš°í„°
    COAT = 6            # ì½”íŠ¸
    DRESS = 7           # ì›í”¼ìŠ¤
    SKIRT = 8           # ìŠ¤ì»¤íŠ¸
    PANTS = 9           # ë°”ì§€
    JEANS = 10          # ì²­ë°”ì§€
    SHORTS = 11         # ë°˜ë°”ì§€
    SHOES = 12          # ì‹ ë°œ
    BOOTS = 13          # ë¶€ì¸ 
    SNEAKERS = 14       # ìš´ë™í™”
    BAG = 15            # ê°€ë°©
    HAT = 16            # ëª¨ì
    GLASSES = 17        # ì•ˆê²½
    SCARF = 18          # ìŠ¤ì¹´í”„
    BELT = 19           # ë²¨íŠ¸
    ACCESSORY = 20      # ì•¡ì„¸ì„œë¦¬

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # ë¹ ë¥¸ ì²˜ë¦¬
    BALANCED = "balanced"   # ê· í˜•
    HIGH = "high"          # ê³ í’ˆì§ˆ
    ULTRA = "ultra"        # ìµœê³ í’ˆì§ˆ

@dataclass
class ClothSegmentationConfig:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.U2NET_CLOTH  # M3 Max ì•ˆì „ ëª¨ë“œ
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (512, 512)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    
    # ì˜ë¥˜ ë¶„ë¥˜ ì„¤ì •
    enable_clothing_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # í›„ì²˜ë¦¬ ì„¤ì •
    enable_crf_postprocessing: bool = True  # ğŸ”¥ CRF í›„ì²˜ë¦¬ ë³µì›
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True  # ğŸ”¥ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ë³µì›
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    confidence_threshold: float = 0.5
    enable_visualization: bool = True
    
    # ìë™ ì „ì²˜ë¦¬ ì„¤ì •
    auto_preprocessing: bool = True
    
    # ìë™ í›„ì²˜ë¦¬ ì„¤ì •
    auto_postprocessing: bool = True
    
    # ë°ì´í„° ê²€ì¦ ì„¤ì •
    strict_data_validation: bool = True


# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
# ==============================================

def _run_hybrid_ensemble_sync(
    self, 
    image: np.ndarray, 
    person_parsing: Dict[str, Any],
    pose_info: Dict[str, Any]
) -> Dict[str, Any]:
    """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ (ë™ê¸°) - ì™„ì „ êµ¬í˜„"""
    try:
        results = []
        methods_used = []
        execution_times = []
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰
        for model_key, model in self.ai_models.items():
            model_start_time = time.time()
            
            try:
                if model_key.startswith('deeplabv3'):
                    result = model.predict(image)
                    if result.get('masks'):
                        results.append(result)
                        methods_used.append(model_key)
                        execution_times.append(time.time() - model_start_time)
                        
                elif model_key.startswith('sam'):
                    prompts = self._generate_sam_prompts(image, person_parsing, pose_info)
                    result = model.predict(image, prompts)
                    if result.get('masks'):
                        results.append(result)
                        methods_used.append(model_key)
                        execution_times.append(time.time() - model_start_time)
                        
                elif model_key.startswith('u2net'):
                    result = model.predict(image)
                    if result.get('masks'):
                        results.append(result)
                        methods_used.append(model_key)
                        execution_times.append(time.time() - model_start_time)
                        
            except Exception as e:
                logger.warning(f"âš ï¸ {model_key} ì•™ìƒë¸” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì•™ìƒë¸” ê²°í•©
        if len(results) >= 2:
            ensemble_result = self._combine_ensemble_results(
                results, methods_used, execution_times, image, person_parsing
            )
            ensemble_result['method_used'] = f"hybrid_ensemble_{'+'.join(methods_used)}"
            return ensemble_result
            
        elif len(results) == 1:
            # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼
            results[0]['method_used'] = methods_used[0]
            return results[0]
        
        # ì‹¤íŒ¨
        return {"masks": {}, "confidence": 0.0, "method_used": "ensemble_failed"}
        
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {"masks": {}, "confidence": 0.0, "method_used": "ensemble_error"}

def _combine_ensemble_results(
    self,
    results: List[Dict[str, Any]],
    methods_used: List[str],
    execution_times: List[float],
    image: np.ndarray,
    person_parsing: Dict[str, Any]
) -> Dict[str, Any]:
    """ì•™ìƒë¸” ê²°ê³¼ ê²°í•© - ì™„ì „ êµ¬í˜„"""
    try:
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        confidences = [r.get('confidence', 0.0) for r in results]
        
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ íŒ¨ë„í‹° (ë¹ ë¥¸ ëª¨ë¸ì— ì•½ê°„ì˜ ë³´ë„ˆìŠ¤)
        time_weights = [1.0 / (1.0 + t) for t in execution_times]
        
        # ëª¨ë¸ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        type_weights = []
        for method in methods_used:
            if 'deeplabv3' in method:
                type_weights.append(1.0)  # ìµœê³  ê°€ì¤‘ì¹˜
            elif 'sam' in method:
                type_weights.append(0.8)
            elif 'u2net' in method:
                type_weights.append(0.7)
            else:
                type_weights.append(0.5)
        
        # ì´ ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_weights = []
        for conf, time_w, type_w in zip(confidences, time_weights, type_weights):
            total_weight = conf * type_w * time_w
            total_weights.append(total_weight)
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_sum = sum(total_weights)
        if total_sum > 0:
            normalized_weights = [w / total_sum for w in total_weights]
        else:
            normalized_weights = [1.0 / len(results)] * len(results)
        
        # ë§ˆìŠ¤í¬ ì•™ìƒë¸”
        ensemble_masks = {}
        mask_keys = set()
        
        # ëª¨ë“  ë§ˆìŠ¤í¬ í‚¤ ìˆ˜ì§‘
        for result in results:
            if 'masks' in result:
                mask_keys.update(result['masks'].keys())
        
        # ê° ë§ˆìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì•™ìƒë¸”
        for mask_key in mask_keys:
            mask_list = []
            weight_list = []
            
            for result, weight in zip(results, normalized_weights):
                if mask_key in result.get('masks', {}):
                    mask = result['masks'][mask_key]
                    if mask is not None and mask.size > 0:
                        mask_normalized = mask.astype(np.float32) / 255.0
                        mask_list.append(mask_normalized)
                        weight_list.append(weight)
            
            if mask_list:
                # ê°€ì¤‘ í‰ê· 
                ensemble_mask = np.zeros_like(mask_list[0])
                total_weight = sum(weight_list)
                
                for mask, weight in zip(mask_list, weight_list):
                    ensemble_mask += mask * (weight / total_weight)
                
                # ì ì‘ì  ì„ê³„ê°’ ì ìš©
                threshold = self._calculate_adaptive_threshold(ensemble_mask, image)
                final_mask = (ensemble_mask > threshold).astype(np.uint8) * 255
                
                # í›„ì²˜ë¦¬
                final_mask = self._apply_ensemble_postprocessing(final_mask, image)
                
                ensemble_masks[mask_key] = final_mask
        
        # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        ensemble_confidence = np.average(confidences, weights=normalized_weights)
        
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        ensemble_metadata = {
            'individual_confidences': confidences,
            'model_weights': normalized_weights,
            'execution_times': execution_times,
            'methods_combined': methods_used,
            'ensemble_type': 'weighted_average'
        }
        
        return {
            'masks': ensemble_masks,
            'confidence': float(ensemble_confidence),
            'ensemble_metadata': ensemble_metadata
        }
        
    except Exception as e:
        logger.error(f"âŒ ì•™ìƒë¸” ê²°ê³¼ ê²°í•© ì‹¤íŒ¨: {e}")
        # í´ë°±: ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
        if results:
            return results[0]
        return {"masks": {}, "confidence": 0.0}

def _calculate_adaptive_threshold(
    self, 
    ensemble_mask: np.ndarray, 
    image: np.ndarray
) -> float:
    """ì ì‘ì  ì„ê³„ê°’ ê³„ì‚°"""
    try:
        # Otsu's method ì‹œë„
        if SKIMAGE_AVAILABLE:
            try:
                threshold = filters.threshold_otsu(ensemble_mask)
                return threshold
            except:
                pass
        
        # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì„ê³„ê°’
        hist, bins = np.histogram(ensemble_mask.flatten(), bins=256, range=[0, 1])
        
        # ë‘ ë²ˆì§¸ í”¼í¬ ì°¾ê¸° (ë°°ê²½ê³¼ ì „ê²½ êµ¬ë¶„)
        peaks = []
        for i in range(1, len(hist) - 1):
            if hist[i] > hist[i-1] and hist[i] > hist[i+1] and hist[i] > np.max(hist) * 0.1:
                peaks.append(i)
        
        if len(peaks) >= 2:
            # ë‘ í”¼í¬ ì‚¬ì´ì˜ ìµœì†Œê°’ì„ ì„ê³„ê°’ìœ¼ë¡œ
            peak1, peak2 = sorted(peaks)[:2]
            valley_idx = np.argmin(hist[peak1:peak2]) + peak1
            threshold = bins[valley_idx]
        else:
            # ê¸°ë³¸ ì„ê³„ê°’
            threshold = 0.5
        
        return float(threshold)
        
    except Exception as e:
        logger.warning(f"ì ì‘ì  ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def _apply_ensemble_postprocessing(
    self, 
    mask: np.ndarray, 
    image: np.ndarray
) -> np.ndarray:
    """ì•™ìƒë¸” í›„ì²˜ë¦¬ ì ìš©"""
    try:
        processed_mask = mask.copy()
        
        # 1. ëª¨í´ë¡œì§€ ì—°ì‚°
        if SCIPY_AVAILABLE:
            # Opening (ë…¸ì´ì¦ˆ ì œê±°)
            structure = np.ones((2, 2))
            opened = ndimage.binary_opening(processed_mask > 128, structure=structure)
            
            # Closing (í™€ ì±„ìš°ê¸°)
            structure = np.ones((3, 3))
            closed = ndimage.binary_closing(opened, structure=structure)
            
            processed_mask = (closed * 255).astype(np.uint8)
        
        # 2. ì—°ê²° êµ¬ì„±ìš”ì†Œ í•„í„°ë§
        if SKIMAGE_AVAILABLE:
            labeled = measure.label(processed_mask > 128)
            regions = measure.regionprops(labeled)
            
            if regions:
                # ë©´ì  ê¸°ë°˜ í•„í„°ë§
                min_area = processed_mask.size * 0.005  # ì „ì²´ì˜ 0.5% ì´ìƒ
                max_area = processed_mask.size * 0.8    # ì „ì²´ì˜ 80% ì´í•˜
                
                filtered_mask = np.zeros_like(processed_mask)
                for region in regions:
                    if min_area <= region.area <= max_area:
                        filtered_mask[labeled == region.label] = 255
                
                processed_mask = filtered_mask
        
        # 3. ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©
        if SCIPY_AVAILABLE:
            # ì•½ê°„ì˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ê°€ì¥ìë¦¬ ìŠ¤ë¬´ë”©
            smoothed = ndimage.gaussian_filter(
                processed_mask.astype(np.float32), sigma=0.5
            )
            processed_mask = (smoothed > 127).astype(np.uint8) * 255
        
        return processed_mask
        
    except Exception as e:
        logger.warning(f"ì•™ìƒë¸” í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return mask

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 8: ClothSegmentationStep ë©”ì¸ í´ë˜ìŠ¤ (Central Hub DI Container v7.0 ì—°ë™)
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - 50% ì½”ë“œ ë‹¨ì¶•
    âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net ì§€ì›
    âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›
    âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬
    """
    def __init__(self, **kwargs):
        """Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™” - ìˆ˜ì •"""
        try:
            # ğŸ”¥ 1. í•„ìˆ˜ ì†ì„±ë“¤ ìš°ì„  ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
            self._initialize_critical_attributes()
            
            # ğŸ”¥ 2. BaseStepMixin ì´ˆê¸°í™” (ì•ˆì „í•œ í˜¸ì¶œ)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin ì´ˆê¸°í™” ì‹œì‘...")
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] super().__init__() í˜¸ì¶œ ì „")
                super().__init__(step_name="ClothSegmentationStep", **kwargs)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] super().__init__() í˜¸ì¶œ ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
                import traceback
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                self.logger.warning(f"âš ï¸ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨, í´ë°± ëª¨ë“œ: {e}")
                self._fallback_initialization(**kwargs)
            
            # ğŸ”¥ 3. Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™”
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™” ì‹œì‘")
            self._initialize_cloth_segmentation_specifics()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentationStep ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"âœ… {self.step_name} Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            self.logger.error(f"âŒ ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)

    def _initialize_critical_attributes(self):
        """ì¤‘ìš” ì†ì„±ë“¤ ìš°ì„  ì´ˆê¸°í™”"""
        # Logger ë¨¼ì € ì„¤ì •
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # í•„ìˆ˜ ì†ì„±ë“¤
        self.step_name = "ClothSegmentationStep"
        self.step_id = 3
        self.device = "cpu"
        self.is_initialized = False
        self.is_ready = False
        
        # ğŸ”¥ ëˆ„ë½ë˜ì—ˆë˜ ì†ì„±ë“¤ ì¶”ê°€ (ì˜¤ë¥˜ í•´ê²°)
        self.segmentation_models = {}
        self.segmentation_ready = False
        self.cloth_cache = {}
        
        # í•µì‹¬ ì»¨í…Œì´ë„ˆë“¤
        self.ai_models = {}
        self.model_paths = {}
        self.loaded_models = {}
        self.models_loading_status = {
            'deeplabv3plus': False,
            'maskrcnn': False,
            'sam_huge': False,
            'u2net_cloth': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        
        # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì¶”ê°€)
        self.cloth_categories = {
            0: 'background',
            1: 'shirt', 2: 't_shirt', 3: 'sweater', 4: 'hoodie',
            5: 'jacket', 6: 'coat', 7: 'dress', 8: 'skirt',
            9: 'pants', 10: 'jeans', 11: 'shorts',
            12: 'shoes', 13: 'boots', 14: 'sneakers',
            15: 'bag', 16: 'hat', 17: 'glasses', 18: 'scarf', 19: 'belt'
        }
        
        # í†µê³„ (ì¶”ê°€)
        self.ai_stats = {
            'total_processed': 0,
            'deeplabv3_calls': 0,
            'sam_calls': 0,
            'u2net_calls': 0,
            'average_confidence': 0.0
        }
        
        # ì˜ì¡´ì„± ì£¼ì… ê´€ë ¨
        self.model_loader = None
        self.model_interface = None
        
    def _fallback_initialization(self, **kwargs):
        """BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ í´ë°±"""
        self.logger.warning("âš ï¸ í´ë°± ì´ˆê¸°í™” ëª¨ë“œ")
        self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
        self.step_id = kwargs.get('step_id', 3)
        self.device = kwargs.get('device', 'cpu')


    def _initialize_step_attributes(self):
        """Step í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        self.ai_models = {}
        self.models_loading_status = {
            'deeplabv3plus': False,
            'maskrcnn': False,
            'sam_huge': False,
            'u2net_cloth': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        self.model_interface = None
        self.loaded_models = {}
        
        # Cloth Segmentation íŠ¹í™” ì†ì„±ë“¤
        self.segmentation_models = {}
        self.segmentation_ready = False
        self.cloth_cache = {}
        
        # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
        self.cloth_categories = {category.value: category.name.lower() 
                                for category in ClothCategory}
        
        # í†µê³„
        self.ai_stats = {
            'total_processed': 0,
            'deeplabv3_calls': 0,
            'sam_calls': 0,
            'u2net_calls': 0,
            'average_confidence': 0.0
        }
    
    def _initialize_cloth_segmentation_specifics(self):
        """Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _initialize_cloth_segmentation_specifics ì‹œì‘")
            
            # ì„¤ì •
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentationConfig ìƒì„± ì‹œì‘")
            self.config = ClothSegmentationConfig()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ClothSegmentationConfig ìƒì„± ì™„ë£Œ")
            
            # ğŸ”§ í•µì‹¬ ì†ì„±ë“¤ ì•ˆì „ ì´ˆê¸°í™”
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í•µì‹¬ ì†ì„± ì´ˆê¸°í™” ì‹œì‘")
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            if not hasattr(self, 'ai_models'):
                self.ai_models = {}
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í•µì‹¬ ì†ì„± ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹œìŠ¤í…œ ìµœì í™”
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì • ì‹œì‘")
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì • ì™„ë£Œ - M3 Max: {self.is_m3_max}, ë©”ëª¨ë¦¬: {self.memory_gb}GB")
            
            # ì„±ëŠ¥ ë° ìºì‹±
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ThreadPoolExecutor ìƒì„± ì‹œì‘")
            try:
                self.executor = ThreadPoolExecutor(
                    max_workers=4 if self.is_m3_max else 2,
                    thread_name_prefix="cloth_seg"
                )
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ThreadPoolExecutor ìƒì„± ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ThreadPoolExecutor ìƒì„± ì‹¤íŒ¨: {e}")
                self.executor = None
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìºì‹œ ì´ˆê¸°í™” ì‹œì‘")
            self.segmentation_cache = {}
            self.cache_lock = threading.RLock()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì´ˆê¸°í™”
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì´ˆê¸°í™” ì‹œì‘")
            self.available_methods = []
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì´ˆê¸°í™” ì™„ë£Œ")
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _initialize_cloth_segmentation_specifics ì™„ë£Œ")
            self.logger.debug(f"âœ… {self.step_name} íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ğŸ”§ ìµœì†Œí•œì˜ ì†ì„±ë“¤ ë³´ì¥
            self.model_paths = {}
            self.ai_models = {}
            self.available_methods = []
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì •"""
        try:
            self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ")
            self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
            self.step_id = kwargs.get('step_id', 3)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.ai_models = {}
            self.model_paths = {}  # ğŸ”§ model_paths ê¸´ê¸‰ ì´ˆê¸°í™”
            self.ai_stats = {'total_processed': 0}
            self.config = ClothSegmentationConfig()
            self.cache_lock = threading.RLock()
            self.cloth_categories = {category.value: category.name.lower() 
                                    for category in ClothCategory}
        except Exception as e:
            print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
            # ğŸ†˜ ìµœí›„ì˜ ìˆ˜ë‹¨
            self.model_paths = {}
    
    def initialize(self) -> bool:
        """Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™” + ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] initialize() ë©”ì„œë“œ ì‹œì‘")
            
            if self.is_initialized:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì´ë¯¸ ì´ˆê¸°í™”ë¨ - True ë°˜í™˜")
                return True
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì•ˆì „ì„± ì²´í¬
            try:
                import psutil
                memory_usage = psutil.virtual_memory().percent
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 90:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŒ - ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜")
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜")
                    return self._fallback_initialization()
            except ImportError:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] psutil ì—†ìŒ - ë©”ëª¨ë¦¬ ì²´í¬ ê±´ë„ˆëœ€")
                pass
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘")
            logger.info(f"ğŸ”„ {self.step_name} Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ğŸ”¥ 1. Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ë¡œë”© ì‹œì‘")
                logger.info("ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                self._load_segmentation_models_via_central_hub()
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                logger.info("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                return self._fallback_initialization()
            
            # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€ ì‹œì‘")
            self.available_methods = self._detect_available_methods()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€ ì™„ë£Œ: {len(self.available_methods)}ê°œ")
            
            # 3. BaseStepMixin ì´ˆê¸°í™”
            print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin ì´ˆê¸°í™” ì‹œì‘")
            super_initialized = super().initialize() if hasattr(super(), 'initialize') else True
            print(f"ğŸ”¥ [ë””ë²„ê¹…] BaseStepMixin ì´ˆê¸°í™” ì™„ë£Œ: {super_initialized}")
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìµœì¢… ìƒíƒœ ì„¤ì • ì‹œì‘")
            self.is_initialized = True
            self.is_ready = True
            self.segmentation_ready = len(self.ai_models) > 0
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìµœì¢… ìƒíƒœ ì„¤ì • ì™„ë£Œ - segmentation_ready: {self.segmentation_ready}")
            
            # ì„±ê³µë¥  ê³„ì‚°
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì„±ê³µë¥  ê³„ì‚° ì‹œì‘")
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool))
            success_rate = (loaded_count / total_models * 100) if total_models > 0 else 0
            
            loaded_models = [k for k, v in self.models_loading_status.items() 
                           if isinstance(v, bool) and v]
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì„±ê³µë¥  ê³„ì‚° ì™„ë£Œ - ë¡œë“œëœ ëª¨ë¸: {loaded_count}/{total_models}")
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸ ì¶œë ¥")
            logger.info(f"âœ… {self.step_name} Central Hub AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            logger.info(f"   - ë¡œë”© ì„±ê³µë¥ : {loaded_count}/{total_models} ({success_rate:.1f}%)")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬")
            gc.collect()
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] initialize() ë©”ì„œë“œ ì™„ë£Œ - True ë°˜í™˜")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return self._fallback_initialization()
    
    def _load_segmentation_models_via_central_hub(self):
        """Central Hubë¥¼ í†µí•œ Segmentation ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            if self.model_loader:  # Central Hubì—ì„œ ìë™ ì£¼ì…ë¨
                logger.info("ğŸ”„ Central Hub ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )...")
                
                # ğŸ”¥ 1. U2Net ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 1 - ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
                u2net_model = self._load_u2net_via_central_hub_improved()
                if u2net_model:
                    self.ai_models['u2net'] = u2net_model
                    self.segmentation_models['u2net'] = u2net_model
                    self.models_loading_status['u2net'] = True
                    logger.info("âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                else:
                    logger.error("âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                
                # ğŸ”¥ 2. SAM ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 2 - ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
                sam_model = self._load_sam_via_central_hub_improved()
                if sam_model:
                    self.ai_models['sam'] = sam_model
                    self.segmentation_models['sam'] = sam_model
                    self.models_loading_status['sam'] = True
                    logger.info("âœ… SAM ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                else:
                    logger.error("âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                
                # ğŸ”¥ 3. DeepLabV3+ ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 3 - ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
                deeplabv3_model = self._load_deeplabv3_via_central_hub_improved()
                if deeplabv3_model:
                    self.ai_models['deeplabv3plus'] = deeplabv3_model
                    self.segmentation_models['deeplabv3plus'] = deeplabv3_model
                    self.models_loading_status['deeplabv3plus'] = True
                    logger.info("âœ… DeepLabV3+ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                else:
                    logger.error("âŒ DeepLabV3+ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                
                # ğŸ”¥ 4. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ íƒì§€
                self._detect_model_paths()
                
            else:
                logger.error("âŒ Central Hub ModelLoader ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Central Hub ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def _load_deeplabv3plus_model(self):
        """DeepLabV3+ ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 1) - Central Hub ModelLoader ì‚¬ìš©"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            
            # Central Hub ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                model_path = self.model_loader.get_model_path('deeplabv3_resnet101_ultra', step_name='step_03_cloth_segmentation')
                if model_path and os.path.exists(model_path):
                    deeplabv3_model = RealDeepLabV3PlusModel(model_path, self.device)
                    if deeplabv3_model.load():
                        self.ai_models['deeplabv3plus'] = deeplabv3_model
                        self.segmentation_models['deeplabv3plus'] = deeplabv3_model
                        self.models_loading_status['deeplabv3plus'] = True
                        self.model_paths['deeplabv3plus'] = model_path
                        self.logger.info(f"âœ… DeepLabV3+ ë¡œë”© ì™„ë£Œ: {model_path}")
                        return
                    else:
                        self.logger.error("âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            # í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€
            checkpoint_paths = [
                "step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth",
                "ai_models/step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth",
                "ultra_models/deeplabv3_resnet101_ultra.pth"
            ]
            
            for model_path in checkpoint_paths:
                if os.path.exists(model_path):
                    # model_architectures.pyì˜ DeepLabV3PlusModel ì‚¬ìš©
                    if MODEL_ARCHITECTURES_AVAILABLE and DeepLabV3PlusModel is not None:
                        deeplabv3_model = DeepLabV3PlusModel()
                        deeplabv3_model = deeplabv3_model.to(self.device)
                        self.logger.info("ğŸ”„ DeepLabV3PlusModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (model_architectures)")
                    else:
                        deeplabv3_model = RealDeepLabV3PlusModel(model_path, self.device)
                        self.logger.info("ğŸ”„ RealDeepLabV3PlusModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    
                    if hasattr(deeplabv3_model, 'load') and callable(deeplabv3_model.load):
                        if deeplabv3_model.load():
                            self.ai_models['deeplabv3plus'] = deeplabv3_model
                            self.segmentation_models['deeplabv3plus'] = deeplabv3_model
                            self.models_loading_status['deeplabv3plus'] = True
                            self.model_paths['deeplabv3plus'] = model_path
                            self.logger.info(f"âœ… DeepLabV3+ ë¡œë”© ì™„ë£Œ: {model_path}")
                            return
                        else:
                            self.logger.error("âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (í´ë°±)")
            
            self.logger.warning("âš ï¸ DeepLabV3+ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"DeepLabV3+: {e}")
    
    def _load_sam_model(self):
        """SAM ëª¨ë¸ ë¡œë”© (í´ë°±) - Central Hub ModelLoader ì‚¬ìš©"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            
            # M3 Max í™˜ê²½ì—ì„œë„ SAM ë¡œë”© ì‹œë„ (ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ)
            if IS_M3_MAX:
                self.logger.info("ğŸ M3 Max í™˜ê²½ - SAM ë¡œë”© ì‹œë„ (ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ)")
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                try:
                    import psutil
                    memory_usage = psutil.virtual_memory().percent
                    if memory_usage > 80:
                        self.logger.warning(f"ğŸ M3 Max í™˜ê²½ - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŒ ({memory_usage}%) - SAM ë¡œë”© ê±´ë„ˆëœ€")
                        self.models_loading_status['sam_huge'] = False
                        self.models_loading_status['loading_errors'].append(f"SAM: M3 Max í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡± ({memory_usage}%)")
                        return
                except ImportError:
                    pass
            
            # Central Hub ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ModelLoaderë¥¼ í†µí•œ SAM ê²½ë¡œ ì¡°íšŒ")
                model_path = self.model_loader.get_model_path('sam_vit_h_4b8939', step_name='step_03_cloth_segmentation')
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì¡°íšŒëœ SAM ê²½ë¡œ: {model_path}")
                if model_path and os.path.exists(model_path):
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                    # model_architectures.pyì˜ SAMModel ì‚¬ìš©
                    if MODEL_ARCHITECTURES_AVAILABLE and SAMModel is not None:
                        sam_model = SAMModel()
                        sam_model = sam_model.to(self.device)
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] SAMModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (model_architectures)")
                    else:
                        sam_model = RealSAMModel(model_path, self.device)
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] RealSAMModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    
                    if hasattr(sam_model, 'load') and callable(sam_model.load):
                        if sam_model.load():
                            self.ai_models['sam_huge'] = sam_model
                            self.segmentation_models['sam_huge'] = sam_model
                            self.models_loading_status['sam_huge'] = True
                            self.model_paths['sam_huge'] = model_path
                            self.logger.info(f"âœ… SAM ë¡œë”© ì™„ë£Œ: {model_path}")
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë¡œë”© ì™„ë£Œ")
                            return
                        else:
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                    else:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ì— load ë©”ì„œë“œê°€ ì—†ìŒ")
                else:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}")
            else:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ModelLoader ì—†ìŒ")
                
            # í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€ ì‹œì‘")
            checkpoint_paths = [
                "step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                "ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                "ultra_models/sam_vit_h_4b8939.pth"  # GeometricMatchingStepê³¼ ê³µìœ 
            ]
            
            for model_path in checkpoint_paths:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê²½ë¡œ í™•ì¸ ì¤‘: {model_path}")
                if os.path.exists(model_path):
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ ë°œê²¬ (í´ë°±): {model_path}")
                    sam_model = RealSAMModel(model_path, self.device)
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    if sam_model.load():
                        self.ai_models['sam_huge'] = sam_model
                        self.segmentation_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        self.model_paths['sam_huge'] = model_path
                        self.logger.info(f"âœ… SAM ë¡œë”© ì™„ë£Œ: {model_path}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë¡œë”© ì™„ë£Œ (í´ë°±)")
                        return
                    else:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (í´ë°±)")
                else:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ê²½ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}")
            
            self.logger.warning("âš ï¸ SAM ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"SAM: {e}")
    
    def _load_u2net_model(self):
        """U2Net ëª¨ë¸ ë¡œë”© (í´ë°±) - Central Hub ModelLoader ì‚¬ìš©"""
        try:
            self.logger.info("ğŸ”„ U2Net ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            
            # Central Hub ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                self.logger.info("ğŸ”„ Central Hub ModelLoaderë¥¼ í†µí•œ U2Net ê²½ë¡œ ì¡°íšŒ...")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ModelLoaderë¥¼ í†µí•œ U2Net ê²½ë¡œ ì¡°íšŒ")
                model_path = self.model_loader.get_model_path('u2net', step_name='step_03_cloth_segmentation')
                self.logger.info(f"ğŸ”„ ì¡°íšŒëœ U2Net ê²½ë¡œ: {model_path}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì¡°íšŒëœ U2Net ê²½ë¡œ: {model_path}")
                
                if model_path and os.path.exists(model_path):
                    self.logger.info(f"âœ… U2Net ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                    
                    # model_architectures.pyì˜ U2NetModel ì‚¬ìš©
                    if MODEL_ARCHITECTURES_AVAILABLE and U2NetModel is not None:
                        u2net_model = U2NetModel()
                        u2net_model = u2net_model.to(self.device)
                        self.logger.info("ğŸ”„ U2NetModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (model_architectures)")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2NetModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (model_architectures)")
                    else:
                        u2net_model = RealU2NetClothModel(model_path, self.device)
                        self.logger.info("ğŸ”„ RealU2NetClothModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] RealU2NetClothModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    
                    if hasattr(u2net_model, 'load') and callable(u2net_model.load):
                        if u2net_model.load():
                            self.ai_models['u2net_cloth'] = u2net_model
                            self.segmentation_models['u2net_cloth'] = u2net_model
                            self.models_loading_status['u2net_cloth'] = True
                            self.model_paths['u2net_cloth'] = model_path
                            self.logger.info(f"âœ… U2Net ë¡œë”© ì™„ë£Œ: {model_path}")
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ë¡œë”© ì™„ë£Œ")
                            return
                        else:
                            self.logger.error("âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                    else:
                        self.logger.error("âŒ U2Net ëª¨ë¸ì— load ë©”ì„œë“œê°€ ì—†ìŒ")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ì— load ë©”ì„œë“œê°€ ì—†ìŒ")
                else:
                    self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}")
            else:
                self.logger.warning("âš ï¸ Central Hub ModelLoader ì—†ìŒ")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ModelLoader ì—†ìŒ")
                
            # í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€
            self.logger.info("ğŸ”„ í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€ ì‹œì‘...")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í´ë°±: ì§ì ‘ ê²½ë¡œ íƒì§€ ì‹œì‘")
            checkpoint_paths = [
                "step_03_cloth_segmentation/u2net.pth",
                "ai_models/step_03_cloth_segmentation/u2net.pth",
                "ultra_models/u2net.pth"
            ]
            
            for model_path in checkpoint_paths:
                self.logger.info(f"ğŸ”„ ê²½ë¡œ í™•ì¸ ì¤‘: {model_path}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê²½ë¡œ í™•ì¸ ì¤‘: {model_path}")
                if os.path.exists(model_path):
                    self.logger.info(f"âœ… U2Net ëª¨ë¸ íŒŒì¼ ë°œê²¬ (í´ë°±): {model_path}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ íŒŒì¼ ë°œê²¬ (í´ë°±): {model_path}")
                    u2net_model = RealU2NetClothModel(model_path, self.device)
                    self.logger.info("ğŸ”„ U2Net ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (í´ë°±)")
                    
                    if u2net_model.load():
                        self.ai_models['u2net_cloth'] = u2net_model
                        self.segmentation_models['u2net_cloth'] = u2net_model
                        self.models_loading_status['u2net_cloth'] = True
                        self.model_paths['u2net_cloth'] = model_path
                        self.logger.info(f"âœ… U2Net ë¡œë”© ì™„ë£Œ (í´ë°±): {model_path}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ë¡œë”© ì™„ë£Œ (í´ë°±)")
                        return
                    else:
                        self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (í´ë°±): {model_path}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (í´ë°±): {model_path}")
                else:
                    self.logger.info(f"âŒ íŒŒì¼ ì—†ìŒ: {model_path}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] íŒŒì¼ ì—†ìŒ: {model_path}")
            
            self.logger.warning("âš ï¸ U2Net ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"U2Net: {e}")
    
    def _load_u2net_via_central_hub_improved(self) -> Optional[Any]:
        """U2Net ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            # 1. ë¨¼ì € model_loaderê°€ ìœ íš¨í•œì§€ í™•ì¸
            if self.model_loader is None:
                self.logger.warning("âš ï¸ model_loaderê°€ Noneì…ë‹ˆë‹¤")
                return None
            
            # 2. ModelLoaderë¥¼ í†µí•´ U2Net ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            u2net_models = [
                'u2net',
                'u2net_cloth',
                'u2net_cloth_segmentation'
            ]
            
            for model_name in u2net_models:
                try:
                    model_path = self.model_loader.get_model_path(model_name, step_name='step_03_cloth_segmentation')
                    if model_path and os.path.exists(model_path):
                        if MODEL_ARCHITECTURES_AVAILABLE and U2NetModel is not None:
                            u2net_model = U2NetModel()
                            u2net_model = u2net_model.to(self.device)
                            self.logger.info(f"âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                            return u2net_model
                        else:
                            u2net_model = RealU2NetClothModel(model_path, self.device)
                            if u2net_model.load():
                                self.logger.info(f"âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                                return u2net_model
                except Exception as e:
                    self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_name}): {e}")
                    continue
            
            self.logger.error("âŒ ëª¨ë“  U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_sam_via_central_hub_improved(self) -> Optional[Any]:
        """SAM ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            # 1. ë¨¼ì € model_loaderê°€ ìœ íš¨í•œì§€ í™•ì¸
            if self.model_loader is None:
                self.logger.warning("âš ï¸ model_loaderê°€ Noneì…ë‹ˆë‹¤")
                return None
            
            # 2. ModelLoaderë¥¼ í†µí•´ SAM ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            sam_models = [
                'sam_vit_h_4b8939',
                'sam_huge',
                'sam_vit_h'
            ]
            
            for model_name in sam_models:
                try:
                    model_path = self.model_loader.get_model_path(model_name, step_name='step_03_cloth_segmentation')
                    if model_path and os.path.exists(model_path):
                        if MODEL_ARCHITECTURES_AVAILABLE and SAMModel is not None:
                            sam_model = SAMModel()
                            sam_model = sam_model.to(self.device)
                            self.logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                            return sam_model
                        else:
                            sam_model = RealSAMModel(model_path, self.device)
                            if sam_model.load():
                                self.logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                                return sam_model
                except Exception as e:
                    self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_name}): {e}")
                    continue
            
            self.logger.error("âŒ ëª¨ë“  SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_deeplabv3_via_central_hub_improved(self) -> Optional[Any]:
        """DeepLabV3+ ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            # 1. ë¨¼ì € model_loaderê°€ ìœ íš¨í•œì§€ í™•ì¸
            if self.model_loader is None:
                self.logger.warning("âš ï¸ model_loaderê°€ Noneì…ë‹ˆë‹¤")
                return None
            
            # 2. ModelLoaderë¥¼ í†µí•´ DeepLabV3+ ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )
            deeplabv3_models = [
                'deeplabv3_resnet101_ultra',
                'deeplabv3plus',
                'deeplabv3_plus'
            ]
            
            for model_name in deeplabv3_models:
                try:
                    model_path = self.model_loader.get_model_path(model_name, step_name='step_03_cloth_segmentation')
                    if model_path and os.path.exists(model_path):
                        if MODEL_ARCHITECTURES_AVAILABLE and DeepLabV3PlusModel is not None:
                            deeplabv3_model = DeepLabV3PlusModel()
                            deeplabv3_model = deeplabv3_model.to(self.device)
                            self.logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                            return deeplabv3_model
                        else:
                            deeplabv3_model = RealDeepLabV3PlusModel(model_path, self.device)
                            if deeplabv3_model.load():
                                self.logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
                                return deeplabv3_model
                except Exception as e:
                    self.logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_name}): {e}")
                    continue
            
            self.logger.error("âŒ ëª¨ë“  DeepLabV3+ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _detect_model_paths(self):
        """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ íƒì§€"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
                
            # ê¸°ë³¸ ê²½ë¡œë“¤
            base_paths = [
                "step_03_cloth_segmentation/",
                "step_03_cloth_segmentation/ultra_models/",
                "step_04_geometric_matching/",  # SAM ê³µìœ 
                "step_04_geometric_matching/ultra_models/",
                "ai_models/step_03_cloth_segmentation/",
                "ultra_models/",
                "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation/"
            ]
            
            model_files = {
                'deeplabv3plus': ['deeplabv3plus_resnet101.pth', 'deeplabv3_resnet101_ultra.pth'],
                'sam_huge': ['sam_vit_h_4b8939.pth'],
                'u2net_cloth': ['u2net.pth', 'u2net_cloth.pth'],
                'maskrcnn': ['maskrcnn_resnet50_fpn.pth', 'maskrcnn_cloth_custom.pth']
            }
            
            # ëª¨ë¸ íŒŒì¼ íƒì§€
            for model_key, filenames in model_files.items():
                if model_key not in self.model_paths:  # ì´ë¯¸ ë¡œë“œëœ ê²ƒì€ ìŠ¤í‚µ
                    for filename in filenames:
                        for base_path in base_paths:
                            full_path = os.path.join(base_path, filename)
                            if os.path.exists(full_path):
                                self.model_paths[model_key] = full_path
                                self.logger.info(f"âœ… {model_key} ê²½ë¡œ ë°œê²¬: {full_path}")
                                break
                        if model_key in self.model_paths:
                            break
                            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            # ğŸ”§ ì•ˆì „ì„± ë³´ì¥
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
    
    def _create_fallback_models(self):
        """í´ë°± ëª¨ë¸ ìƒì„± (Central Hub ì—°ê²° ì‹¤íŒ¨ì‹œ)"""
        try:
            self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ìƒì„± ì¤‘...")
            
            # ê¸°ë³¸ DeepLabV3+ ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ì´)
            deeplabv3_model = RealDeepLabV3PlusModel("", self.device)
            deeplabv3_model.model = DeepLabV3PlusModel(num_classes=20)
            deeplabv3_model.model.to(self.device)
            deeplabv3_model.model.eval()
            deeplabv3_model.is_loaded = True
            
            self.ai_models['deeplabv3plus_fallback'] = deeplabv3_model
            self.segmentation_models['deeplabv3plus_fallback'] = deeplabv3_model
            self.models_loading_status['deeplabv3plus'] = True
            
            self.logger.info("âœ… í´ë°± DeepLabV3+ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        if 'deeplabv3plus' in self.ai_models or 'deeplabv3plus_fallback' in self.ai_models:
            methods.append(SegmentationMethod.DEEPLABV3_PLUS)
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
        if 'maskrcnn' in self.ai_models:
            methods.append(SegmentationMethod.MASK_RCNN)
        
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
        
        return methods
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (BaseStepMixin í‘œì¤€)
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ Cloth Segmentation AI ì¶”ë¡  (BaseStepMixin v20.0 í˜¸í™˜) - ì™„ì „ êµ¬í˜„"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference() ì§„ì…!")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] processed_input í‚¤ë“¤: {list(processed_input.keys())}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] processed_input ê°’ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            self.logger.info(f"ğŸ”¥ [Step 3] ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            self.logger.info(f"ğŸ”¥ [Step 3] ì…ë ¥ ë°ì´í„° íƒ€ì…ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            start_time = time.time()
            
            # ğŸ”¥ ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
            import gc
            gc.collect()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ ì•ˆì „ì„± ì²´í¬
            print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ë””ë°”ì´ìŠ¤ ì²´í¬ ì‹œì‘")
            if hasattr(torch, 'mps') and torch.mps.is_available():
                print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ì‚¬ìš© ê°€ëŠ¥ - ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„")
                try:
                    # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                    if hasattr(torch.mps, 'empty_cache'):
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] torch.mps.empty_cache() í˜¸ì¶œ")
                        torch.mps.empty_cache()
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] torch.mps.empty_cache() ì™„ë£Œ")
                except Exception as mps_error:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            else:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ì‚¬ìš© ë¶ˆê°€ëŠ¥ ë˜ëŠ” ì—†ìŒ")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ë””ë°”ì´ìŠ¤ ì²´í¬ ì™„ë£Œ")
            
            # ğŸ”¥ Sessionì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
            image = None
            if 'session_id' in processed_input:
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        # ì„¸ì…˜ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ ì§ì ‘ ë¡œë“œ (ë™ê¸°ì ìœ¼ë¡œ)
                        import asyncio
                        try:
                            # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ ìƒíƒœ í™•ì¸
                            try:
                                loop = asyncio.get_running_loop()
                                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° - ë™ê¸°ì  ì ‘ê·¼ ì‹œë„
                                if hasattr(session_manager, 'get_session_images_sync'):
                                    person_image, clothing_image = session_manager.get_session_images_sync(processed_input['session_id'])
                                else:
                                    # ë™ê¸°ì  ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° - ì„¸ì…˜ ë°ì´í„°ì—ì„œ ì§ì ‘ ì¶”ì¶œ
                                    session_data = session_manager.get_session_data(processed_input['session_id'])
                                    self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° íƒ€ì…: {type(session_data)}")
                                    if session_data and isinstance(session_data, dict):
                                        person_image = session_data.get('person_image')
                                        clothing_image = session_data.get('clothing_image')
                                        self.logger.info(f"ğŸ” person_image íƒ€ì…: {type(person_image)}")
                                        self.logger.info(f"ğŸ” clothing_image íƒ€ì…: {type(clothing_image)}")
                                    else:
                                        raise Exception("ì„¸ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            except RuntimeError:
                                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°
                                person_image, clothing_image = asyncio.run(session_manager.get_session_images(processed_input['session_id']))
                        except Exception as e:
                            # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° - ì„¸ì…˜ ë°ì´í„°ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                            try:
                                session_data = session_manager.get_session_data(processed_input['session_id'])
                                if session_data and isinstance(session_data, dict):
                                    person_image = session_data.get('person_image')
                                    clothing_image = session_data.get('clothing_image')
                                else:
                                    raise Exception("ì„¸ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            except Exception as inner_e:
                                raise Exception(f"ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(inner_e)}")
                        
                        # ì´ë¯¸ì§€ ì„ íƒ (ì˜ë¥˜ ë¶„í• ì€ ì˜ë¥˜ ì´ë¯¸ì§€ ìš°ì„ , ì—†ìœ¼ë©´ ì‚¬ëŒ ì´ë¯¸ì§€)
                        if clothing_image is not None:
                            image = clothing_image
                            self.logger.info(f"âœ… Sessionì—ì„œ ì˜ë¥˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {type(image)}")
                        elif person_image is not None:
                            image = person_image
                            self.logger.info(f"âœ… Sessionì—ì„œ ì‚¬ëŒ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {type(image)}")
                        else:
                            self.logger.warning("âš ï¸ Sessionì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ sessionì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ê²€ì¦
            self.logger.info(f"ğŸ” ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            
            # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ì—ì„œ ì‹œë„) - Sessionì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš°
            if image is None:
                for key in ['image', 'input_image', 'original_image', 'processed_image', 'clothing_image', 'person_image']:
                    if key in processed_input:
                        potential_image = processed_input[key]
                        if potential_image is not None:
                            image = potential_image
                            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë°ì´í„° ë°œê²¬: {key} (íƒ€ì…: {type(image)})")
                            break
            
            if image is None:
                self.logger.error("âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ (Step 3)")
                # í´ë°±: ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
                try:
                    self.logger.warning("âš ï¸ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì‹œë„")
                    dummy_image = np.ones((512, 512, 3), dtype=np.uint8) * 128
                    image = dummy_image
                    self.logger.info("âœ… ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                except Exception as dummy_error:
                    self.logger.error(f"âŒ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {dummy_error}")
                    return {'success': False, 'error': 'ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ'}
            
            self.logger.info("ğŸ§  Cloth Segmentation ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
                image_array = image
            elif PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
                image_array = np.array(image)
            else:
                return self._create_emergency_result("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ì´ì „ Step ë°ì´í„°
            person_parsing = processed_input.get('from_step_01', {})
            pose_info = processed_input.get('from_step_02', {})
            
            # ==============================================
            # ğŸ”¥ Phase 1: ê³ ê¸‰ ì „ì²˜ë¦¬
            # ==============================================
            
            preprocessing_start = time.time()
            
            # 1.1 ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
            quality_scores = self._assess_image_quality(image_array)
            
            # 1.2 ì¡°ëª… ì •ê·œí™”
            processed_image = self._normalize_lighting(image_array)
            
            # 1.3 ìƒ‰ìƒ ë³´ì •
            if self.config.enable_color_correction:
                processed_image = self._correct_colors(processed_image)
            
            preprocessing_time = time.time() - preprocessing_start
            self.ai_stats['preprocessing_time'] = self.ai_stats.get('preprocessing_time', 0) + preprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 2: ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì•ˆì „í•œ ë˜í¼ ì‚¬ìš©)
            # ==============================================
            
            segmentation_start = time.time()
            
            # í’ˆì§ˆ ë ˆë²¨ ê²°ì •
            quality_level = self._determine_quality_level(processed_input, quality_scores)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í’ˆì§ˆ ë ˆë²¨ ê²°ì •: {quality_level}")
            
            # ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ + ì„¸ê·¸ë©˜í…Œì´ì…˜ í´íŠ¸ ë°©ì§€)
            try:
                import gc
                gc.collect()  # ë©”ëª¨ë¦¬ ì •ë¦¬
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                import psutil
                memory_usage = psutil.virtual_memory().percent
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 90:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜")
                    segmentation_result = self._create_fallback_segmentation_result(processed_image.shape)
                else:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘")
                    segmentation_result = self._run_ai_segmentation_sync_safe(
                        processed_image, quality_level, person_parsing, pose_info
                    )
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë¥˜: {str(e)}")
                segmentation_result = self._create_fallback_segmentation_result(processed_image.shape)
            
            if not segmentation_result or not segmentation_result.get('masks'):
                # í´ë°±: ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„±
                segmentation_result = self._create_fallback_segmentation_result(processed_image.shape)
            
            segmentation_time = time.time() - segmentation_start
            self.ai_stats['segmentation_time'] = self.ai_stats.get('segmentation_time', 0) + segmentation_time
            
            # ==============================================
            # ğŸ”¥ Phase 3: í›„ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦
            # ==============================================
            
            postprocessing_start = time.time()
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            processed_masks = self._postprocess_masks(segmentation_result['masks'])
            
            # í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_segmentation_quality(processed_masks, processed_image)
            
            # ì‹œê°í™” ìƒì„±
            visualizations = self._create_segmentation_visualizations(processed_image, processed_masks)
            
            postprocessing_time = time.time() - postprocessing_start
            self.ai_stats['postprocessing_time'] = self.ai_stats.get('postprocessing_time', 0) + postprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 4: ê²°ê³¼ ìƒì„±
            # ==============================================
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_time = time.time() - start_time
            self._update_ai_stats(segmentation_result.get('method_used', 'unknown'), 
                                segmentation_result.get('confidence', 0.0), total_time, quality_metrics)
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€
            cloth_categories = self._detect_cloth_categories(processed_masks)
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (BaseStepMixin í‘œì¤€)
            ai_result = {
                # í•µì‹¬ ê²°ê³¼
                'success': True,
                'step': self.step_name,
                'segmentation_masks': processed_masks,
                'cloth_categories': cloth_categories,
                'segmentation_confidence': segmentation_result.get('confidence', 0.0),
                'processing_time': total_time,
                'model_used': segmentation_result.get('method_used', 'unknown'),
                'items_detected': len([cat for cat in cloth_categories if cat != 'background']),
                
                # ğŸ”¥ ë‹¤ë¥¸ Stepë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ í‚¤ë“¤
                'clothing_mask': processed_masks.get('all_clothes', None),  # Step 4/5/6 í˜¸í™˜ì„±
                'segmentation_result': processed_masks,  # Step 4 í˜¸í™˜ì„±
                'clothing_masks': processed_masks,  # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                'quality_score': quality_metrics.get('overall', 0.5),
                'quality_metrics': quality_metrics,
                'image_quality_scores': quality_scores,
                
                # ì „ì²˜ë¦¬ ê²°ê³¼
                'preprocessing_results': {
                    'lighting_normalized': self.config.enable_lighting_normalization,
                    'color_corrected': self.config.enable_color_correction,
                    'quality_assessed': self.config.enable_quality_assessment
                },
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                'performance_breakdown': {
                    'preprocessing_time': preprocessing_time,
                    'segmentation_time': segmentation_time,
                    'postprocessing_time': postprocessing_time
                },
                
                # ì‹œê°í™”
                **visualizations,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'ai_models_loaded': list(self.ai_models.keys()),
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'ai_enhanced': True,
                    'quality_level': quality_level.value,
                    'version': '33.0',
                    'central_hub_connected': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'num_classes': 21,  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
                    'segmentation_method': segmentation_result.get('method_used', 'unknown')
                },
                
                # Step ê°„ ì—°ë™ ë°ì´í„°
                'cloth_features': self._extract_cloth_features(processed_masks, processed_image),
                'cloth_contours': self._extract_cloth_contours(processed_masks.get('all_clothes', np.array([]))),
                'parsing_map': segmentation_result.get('parsing_map', np.array([]))
            }
            
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - {total_time:.2f}ì´ˆ")
            self.logger.info(f"   - ë°©ë²•: {segmentation_result.get('method_used', 'unknown')}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {segmentation_result.get('confidence', 0.0):.3f}")
            self.logger.info(f"   - í’ˆì§ˆ: {quality_metrics.get('overall', 0.5):.3f}")
            self.logger.info(f"   - íƒì§€ëœ ì•„ì´í…œ: {len([cat for cat in cloth_categories if cat != 'background'])}ê°œ")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_emergency_result(str(e))


    def _safe_model_predict(self, model_key: str, image: np.ndarray) -> Dict[str, Any]:
        """ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì˜ˆì¸¡ ë˜í¼ - segmentation fault ì™„ì „ ë°©ì§€"""
        try:
            import psutil
            import os
            
            # 1. ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024**3)
            
            self.logger.info(f"ğŸ”¥ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì „ ë©”ëª¨ë¦¬: {memory_gb:.2f}GB")
            
            # 2. ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì²´í¬ (M3 Max 128GB ê¸°ì¤€)
            if memory_gb > 100.0:  # 100GB ì´ˆê³¼ì‹œ ê°•ì œ ì •ë¦¬
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ ({memory_gb:.2f}GB), ê°•ì œ ì •ë¦¬ ì‹¤í–‰")
                for _ in range(5):
                    gc.collect()
                if hasattr(torch, 'mps') and torch.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as mps_error:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 3. ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 4. ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if model_key not in self.ai_models:
                self.logger.error(f"âŒ {model_key} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return self._create_fallback_segmentation_result(image.shape)
            
            model = self.ai_models[model_key]
            
            # 5. ëª¨ë¸ predict ë©”ì„œë“œ í™•ì¸
            if not hasattr(model, 'predict'):
                self.logger.error(f"âŒ {model_key} ëª¨ë¸ì— predict ë©”ì„œë“œ ì—†ìŒ")
                return self._create_fallback_segmentation_result(image.shape)
            
            # 6. ì•ˆì „í•œ ì˜ˆì¸¡ ì‹¤í–‰
            self.logger.info(f"ğŸš€ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘")
            
            # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦
            if image is None or image.size == 0:
                self.logger.error(f"âŒ {model_key} ì…ë ¥ ì´ë¯¸ì§€ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                return self._create_fallback_segmentation_result((512, 512, 3))
            
            # ì‹¤ì œ ì˜ˆì¸¡ ì‹¤í–‰
            result = model.predict(image)
            
            # 7. ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 8. ê²°ê³¼ ê²€ì¦
            if not isinstance(result, dict):
                self.logger.warning(f"âš ï¸ {model_key} ê²°ê³¼ê°€ dictê°€ ì•„ë‹˜, ë³€í™˜ ì‹œë„")
                result = {"masks": {}, "confidence": 0.0}
            
            if 'masks' not in result:
                result['masks'] = {}
            
            if 'confidence' not in result:
                result['confidence'] = 0.0
            
            self.logger.info(f"âœ… {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ - ì‹ ë¢°ë„: {result.get('confidence', 0.0):.3f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            
            # 9. ì˜ˆì™¸ ë°œìƒ ì‹œ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            for _ in range(3):
                gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            return self._create_fallback_segmentation_result(image.shape if image is not None else (512, 512, 3))

    # ==============================================
    # ğŸ”¥ AI í—¬í¼ ë©”ì„œë“œë“¤ (í•µì‹¬ ë¡œì§)
    # ==============================================
    
    def _postprocess_masks(self, masks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            processed_masks = masks.copy()
            
            # 1. í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°
            if self.config.enable_hole_filling:
                processed_masks = self._fill_holes_and_remove_noise_advanced(processed_masks)
            
            # 2. CRF í›„ì²˜ë¦¬
            if self.config.enable_crf_postprocessing and 'all_clothes' in processed_masks:
                # ì›ë³¸ ì´ë¯¸ì§€ê°€ í•„ìš”í•˜ì§€ë§Œ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
                pass
            
            # 3. ì—ì§€ ì •ì œ
            if self.config.enable_edge_refinement:
                # ê¸°ë³¸ ì´ë¯¸ì§€ê°€ í•„ìš”í•˜ë¯€ë¡œ ì„ì‹œë¡œ ìŠ¤í‚µ
                pass
            
            # 4. ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
            if self.config.enable_multiscale_processing and 'all_clothes' in processed_masks:
                # ê¸°ë³¸ ì´ë¯¸ì§€ê°€ í•„ìš”í•˜ë¯€ë¡œ ì„ì‹œë¡œ ìŠ¤í‚µ
                pass
            
            return processed_masks
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return masks
        
    def _assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_scores = {}
            
            # ë¸”ëŸ¬ ì •ë„ ì¸¡ì •
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            if NUMPY_AVAILABLE:
                grad_x = np.abs(np.diff(gray, axis=1))
                grad_y = np.abs(np.diff(gray, axis=0))
                sharpness = np.mean(grad_x) + np.mean(grad_y)
                quality_scores['sharpness'] = min(sharpness / 100.0, 1.0)
            else:
                quality_scores['sharpness'] = 0.5
            
            # ëŒ€ë¹„ ì¸¡ì •
            contrast = np.std(gray) if NUMPY_AVAILABLE else 50.0
            quality_scores['contrast'] = min(contrast / 128.0, 1.0)
            
            # í•´ìƒë„ í’ˆì§ˆ
            height, width = image.shape[:2]
            resolution_score = min((height * width) / (512 * 512), 1.0)
            quality_scores['resolution'] = resolution_score
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_scores['overall'] = np.mean(list(quality_scores.values())) if NUMPY_AVAILABLE else 0.5
            
            return quality_scores
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5, 'sharpness': 0.5, 'contrast': 0.5, 'resolution': 0.5}
    
    def _normalize_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            if not self.config.enable_lighting_normalization:
                return image
            
            if len(image.shape) == 3:
                # ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                normalized = np.zeros_like(image)
                for i in range(3):
                    channel = image[:, :, i]
                    channel_min, channel_max = channel.min(), channel.max()
                    if channel_max > channel_min:
                        normalized[:, :, i] = ((channel - channel_min) / (channel_max - channel_min) * 255).astype(np.uint8)
                    else:
                        normalized[:, :, i] = channel
                return normalized
            else:
                img_min, img_max = image.min(), image.max()
                if img_max > img_min:
                    return ((image - img_min) / (img_max - img_min) * 255).astype(np.uint8)
                else:
                    return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def _correct_colors(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if PIL_AVAILABLE and len(image.shape) == 3:
                pil_image = Image.fromarray(image)
                
                # ìë™ ëŒ€ë¹„ ì¡°ì •
                enhancer = ImageEnhance.Contrast(pil_image)
                enhanced = enhancer.enhance(1.2)
                
                # ìƒ‰ìƒ ì±„ë„ ì¡°ì •
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.1)
                
                return np.array(enhanced)
            else:
                return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _determine_quality_level(self, processed_input: Dict[str, Any], quality_scores: Dict[str, float]) -> QualityLevel:
        """í’ˆì§ˆ ë ˆë²¨ ê²°ì •"""
        try:
            # ì‚¬ìš©ì ì„¤ì • ìš°ì„ 
            if 'quality_level' in processed_input:
                user_level = processed_input['quality_level']
                if isinstance(user_level, str):
                    try:
                        return QualityLevel(user_level)
                    except ValueError:
                        pass
                elif isinstance(user_level, QualityLevel):
                    return user_level
            
            # ìë™ ê²°ì •
            overall_quality = quality_scores.get('overall', 0.5)
            
            if self.is_m3_max and overall_quality > 0.7:
                return QualityLevel.ULTRA
            elif overall_quality > 0.6:
                return QualityLevel.HIGH
            elif overall_quality > 0.4:
                return QualityLevel.BALANCED
            else:
                return QualityLevel.FAST
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë ˆë²¨ ê²°ì • ì‹¤íŒ¨: {e}")
            return QualityLevel.BALANCED
    
    def _run_ai_segmentation_with_memory_protection(
        self, 
        image: np.ndarray, 
        quality_level: QualityLevel, 
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ğŸ”¥ 128GB M3 Max ë©”ëª¨ë¦¬ ì•ˆì „ ë˜í¼ - bus error ì™„ì „ ë°©ì§€"""
        try:
            import psutil
            import os
            
            # 1. ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024**3)
            
            self.logger.info(f"ğŸ”¥ AI ì¶”ë¡  ì „ ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_gb:.2f}GB")
            
            # 2. ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ê°•ì œ ì •ë¦¬
            if memory_gb > 100:  # 100GB ì´ìƒ ì‚¬ìš© ì‹œ
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€: {memory_gb:.2f}GB - ê°•ì œ ì •ë¦¬")
                for _ in range(5):
                    gc.collect()
                if hasattr(torch, 'mps') and torch.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as mps_error:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 3. ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 4. ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            result = self._run_ai_segmentation_sync(image, quality_level, person_parsing, pose_info)
            
            # 5. ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì•ˆì „ ë˜í¼ ì‹¤íŒ¨: {e}")
            # 6. ì˜ˆì™¸ ë°œìƒ ì‹œ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            for _ in range(5):
                gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            return self._create_fallback_segmentation_result(image.shape)

    def _safe_model_predict(self, model_key: str, image: np.ndarray) -> Dict[str, Any]:
        """ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì˜ˆì¸¡ ë˜í¼ - segmentation fault ì™„ì „ ë°©ì§€"""
        try:
            import psutil
            import os
            
            # 1. ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024**3)
            
            self.logger.info(f"ğŸ”¥ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì „ ë©”ëª¨ë¦¬: {memory_gb:.2f}GB")
            
            # 2. ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ê°•ì œ ì •ë¦¬
            if memory_gb > 50:  # 50GB ì´ìƒ ì‚¬ìš© ì‹œ
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€: {memory_gb:.2f}GB - ê°•ì œ ì •ë¦¬")
                for _ in range(3):
                    gc.collect()
                if hasattr(torch, 'mps') and torch.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as mps_error:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 3. ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 4. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
            if model_key in self.ai_models:
                model = self.ai_models[model_key]
                if hasattr(model, 'predict'):
                    result = model.predict(image)
                    
                    # 5. ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    if hasattr(torch, 'mps') and torch.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except Exception as mps_error:
                            self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                    
                    return result
                else:
                    self.logger.error(f"âŒ {model_key} ëª¨ë¸ì— predict ë©”ì„œë“œ ì—†ìŒ")
                    return self._create_fallback_segmentation_result(image.shape)
            else:
                self.logger.error(f"âŒ {model_key} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return self._create_fallback_segmentation_result(image.shape)
                
        except Exception as e:
            self.logger.error(f"âŒ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # 6. ì˜ˆì™¸ ë°œìƒ ì‹œ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            for _ in range(3):
                gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            return self._create_fallback_segmentation_result(image.shape)

    def _safe_model_predict_with_prompts(self, model_key: str, image: np.ndarray, prompts: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì˜ˆì¸¡ ë˜í¼ (í”„ë¡¬í”„íŠ¸ í¬í•¨) - segmentation fault ì™„ì „ ë°©ì§€"""
        try:
            import psutil
            import os
            
            # 1. ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024**3)
            
            self.logger.info(f"ğŸ”¥ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì „ ë©”ëª¨ë¦¬: {memory_gb:.2f}GB")
            
            # 2. ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ê°•ì œ ì •ë¦¬
            if memory_gb > 50:  # 50GB ì´ìƒ ì‚¬ìš© ì‹œ
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€: {memory_gb:.2f}GB - ê°•ì œ ì •ë¦¬")
                for _ in range(3):
                    gc.collect()
                if hasattr(torch, 'mps') and torch.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as mps_error:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 3. ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            # 4. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
            if model_key in self.ai_models:
                model = self.ai_models[model_key]
                if hasattr(model, 'predict'):
                    result = model.predict(image, prompts)
                    
                    # 5. ì˜ˆì¸¡ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    if hasattr(torch, 'mps') and torch.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except Exception as mps_error:
                            self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                    
                    return result
                else:
                    self.logger.error(f"âŒ {model_key} ëª¨ë¸ì— predict ë©”ì„œë“œ ì—†ìŒ")
                    return self._create_fallback_segmentation_result(image.shape)
            else:
                self.logger.error(f"âŒ {model_key} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return self._create_fallback_segmentation_result(image.shape)
                
        except Exception as e:
            self.logger.error(f"âŒ {model_key} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # 6. ì˜ˆì™¸ ë°œìƒ ì‹œ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            for _ in range(3):
                gc.collect()
            if hasattr(torch, 'mps') and torch.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception as mps_error:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            return self._create_fallback_segmentation_result(image.shape)
    

    def _create_fallback_segmentation_result(self, image_shape: Tuple[int, ...]) -> Dict[str, Any]:
        """í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìƒì„±"""
        self.logger.warning("âš ï¸ [Step 3] í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìƒì„± - ì‹¤ì œ AI ëª¨ë¸ì´ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ!")
        try:
            height, width = image_shape[:2]
            
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ë“¤ ìƒì„±
            upper_mask = np.zeros((height, width), dtype=np.uint8)
            lower_mask = np.zeros((height, width), dtype=np.uint8)
            
            # ìƒì˜ ì˜ì—­ (ìƒë‹¨ 1/3)
            upper_mask[height//4:height//2, width//4:3*width//4] = 255
            
            # í•˜ì˜ ì˜ì—­ (í•˜ë‹¨ 1/3)  
            lower_mask[height//2:3*height//4, width//3:2*width//3] = 255
            
            masks = {
                "upper_body": upper_mask,
                "lower_body": lower_mask,
                "full_body": upper_mask + lower_mask,
                "accessories": np.zeros((height, width), dtype=np.uint8),
                "all_clothes": upper_mask + lower_mask
            }
            
            return {
                "masks": masks,
                "confidence": 0.5,
                "method_used": "fallback",
                "parsing_map": upper_mask + lower_mask
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            height, width = 512, 512
            return {
                "masks": {
                    "all_clothes": np.zeros((height, width), dtype=np.uint8)
                },
                "confidence": 0.0,
                "method_used": "emergency"
            }
    
    def _fill_holes_and_remove_noise_advanced(self, masks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° (ì›ë³¸)"""
        try:
            processed_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    processed_masks[mask_key] = mask
                    continue
                
                processed_mask = mask.copy()
                
                # 1. í™€ ì±„ìš°ê¸° (SciPy ì‚¬ìš©)
                if SCIPY_AVAILABLE:
                    filled = ndimage.binary_fill_holes(processed_mask > 128)
                    processed_mask = (filled * 255).astype(np.uint8)
                
                # 2. ëª¨í´ë¡œì§€ ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
                if SCIPY_AVAILABLE:
                    # Opening (ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°)
                    structure = ndimage.generate_binary_structure(2, 2)
                    opened = ndimage.binary_opening(processed_mask > 128, structure=structure, iterations=1)
                    
                    # Closing (ì‘ì€ í™€ ì±„ìš°ê¸°)
                    closed = ndimage.binary_closing(opened, structure=structure, iterations=2)
                    
                    processed_mask = (closed * 255).astype(np.uint8)
                
                # 3. ì‘ì€ ì—°ê²° êµ¬ì„±ìš”ì†Œ ì œê±° (Scikit-image ì‚¬ìš©)
                if SKIMAGE_AVAILABLE:
                    labeled = measure.label(processed_mask > 128)
                    regions = measure.regionprops(labeled)
                    
                    # ë©´ì ì´ ì‘ì€ ì˜ì—­ ì œê±° (ì „ì²´ ì´ë¯¸ì§€ì˜ 1% ì´í•˜)
                    min_area = processed_mask.size * 0.01
                    
                    for region in regions:
                        if region.area < min_area:
                            processed_mask[labeled == region.label] = 0
                
                processed_masks[mask_key] = processed_mask
            
            return processed_masks
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return masks
    
    def _evaluate_segmentation_quality(self, masks: Dict[str, np.ndarray], image: np.ndarray) -> Dict[str, float]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€ - ì•ˆì „í•œ í¬ê¸° ì¡°ì •"""
        try:
            quality_metrics = {}
            
            if 'all_clothes' in masks:
                mask = masks['all_clothes']
                
                # ğŸ”¥ ì•ˆì „í•œ í¬ê¸° ì¡°ì • ë¡œì§
                target_shape = image.shape[:2]
                if mask.shape != target_shape:
                    try:
                        # PILì„ ì‚¬ìš©í•œ ì•ˆì „í•œ ë¦¬ì‚¬ì´ì¦ˆ
                        if PIL_AVAILABLE:
                            mask_pil = Image.fromarray(mask.astype(np.uint8))
                            mask_resized = mask_pil.resize((target_shape[1], target_shape[0]), Image.Resampling.NEAREST)
                            mask = np.array(mask_resized)
                        else:
                            # OpenCVë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ ë¦¬ì‚¬ì´ì¦ˆ
                            mask = cv2.resize(mask, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST)
                        
                        self.logger.debug(f"âœ… ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • ì™„ë£Œ: {mask.shape} -> {target_shape}")
                    except Exception as resize_error:
                        self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {resize_error}")
                        # í¬ê¸° ì¡°ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                        return {'overall': 0.5, 'size_appropriateness': 0.5, 'continuity': 0.5, 'boundary_quality': 0.5}

                # ğŸ”¥ í¬ê¸° ê²€ì¦
                if mask.shape != target_shape:
                    self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ë¶ˆì¼ì¹˜: {mask.shape} vs {target_shape}")
                    return {'overall': 0.5, 'size_appropriateness': 0.5, 'continuity': 0.5, 'boundary_quality': 0.5}

                # 1. ì˜ì—­ í¬ê¸° ì ì ˆì„±
                size_ratio = np.sum(mask > 128) / mask.size if NUMPY_AVAILABLE and mask.size > 0 else 0
                if 0.1 <= size_ratio <= 0.7:  # ì ì ˆí•œ í¬ê¸° ë²”ìœ„
                    quality_metrics['size_appropriateness'] = 1.0
                else:
                    quality_metrics['size_appropriateness'] = max(0.0, 1.0 - abs(size_ratio - 0.3) / 0.3)
                
                # 2. ì—°ì†ì„± (ì—°ê²°ëœ êµ¬ì„±ìš”ì†Œ)
                if SKIMAGE_AVAILABLE and mask.size > 0:
                    try:
                        labeled = measure.label(mask > 128)
                        num_components = labeled.max() if labeled.max() > 0 else 0
                        if num_components > 0:
                            total_area = np.sum(mask > 128)
                            component_sizes = [np.sum(labeled == i) for i in range(1, num_components + 1)]
                            largest_component = max(component_sizes) if component_sizes else 0
                            quality_metrics['continuity'] = largest_component / total_area if total_area > 0 else 0.0
                        else:
                            quality_metrics['continuity'] = 0.0
                    except Exception as continuity_error:
                        self.logger.warning(f"âš ï¸ ì—°ì†ì„± ê³„ì‚° ì‹¤íŒ¨: {continuity_error}")
                        quality_metrics['continuity'] = 0.5
                else:
                    quality_metrics['continuity'] = 0.5
                
                # 3. ê²½ê³„ì„  í’ˆì§ˆ
                if NUMPY_AVAILABLE and mask.size > 0:
                    try:
                        # ğŸ”¥ ì•ˆì „í•œ ê²½ê³„ì„  ê³„ì‚° - shape ë¶ˆì¼ì¹˜ ë°©ì§€
                        mask_float = mask.astype(np.float32)
                        
                        # ìˆ˜í‰ ê²½ê³„ì„  (axis=1) - shape: (H, W-1)
                        diff_h = np.abs(np.diff(mask_float, axis=1))
                        
                        # ìˆ˜ì§ ê²½ê³„ì„  (axis=0) - shape: (H-1, W)
                        diff_v = np.abs(np.diff(mask_float, axis=0))
                        
                        # ê°ê°ì˜ ê²½ê³„ì„  ê¸¸ì´ ê³„ì‚°
                        edge_length_h = np.sum(diff_h > 10)
                        edge_length_v = np.sum(diff_v > 10)
                        
                        # ì „ì²´ ê²½ê³„ì„  ê¸¸ì´
                        edge_length = edge_length_h + edge_length_v
                        
                        # ë©´ì  ê³„ì‚°
                        area = np.sum(mask > 128)
                        
                        if area > 0:
                            boundary_ratio = edge_length / np.sqrt(area)
                            quality_metrics['boundary_quality'] = min(1.0, max(0.0, 1.0 - boundary_ratio / 10.0))
                        else:
                            quality_metrics['boundary_quality'] = 0.0
                            
                        self.logger.debug(f"âœ… ê²½ê³„ì„  í’ˆì§ˆ ê³„ì‚° ì™„ë£Œ: edge_length={edge_length}, area={area}, ratio={boundary_ratio if area > 0 else 0}")
                        
                    except Exception as boundary_error:
                        self.logger.warning(f"âš ï¸ ê²½ê³„ì„  í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {boundary_error}")
                        quality_metrics['boundary_quality'] = 0.5
                else:
                    quality_metrics['boundary_quality'] = 0.5
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            if quality_metrics:
                quality_metrics['overall'] = np.mean(list(quality_metrics.values())) if NUMPY_AVAILABLE else 0.5
            else:
                quality_metrics['overall'] = 0.5
            
            return quality_metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5, 'size_appropriateness': 0.5, 'continuity': 0.5, 'boundary_quality': 0.5}
    
    def _create_segmentation_visualizations(self, image: np.ndarray, masks: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œê°í™” ìƒì„± - Base64 ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            import base64
            from PIL import Image
            from io import BytesIO
            
            visualizations = {}
            
            if not masks:
                return visualizations
            
            # ğŸ”¥ ì•ˆì „í•œ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • í•¨ìˆ˜
            def safe_resize_mask(mask: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:
                """ì•ˆì „í•œ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •"""
                try:
                    if mask.shape != target_shape:
                        if PIL_AVAILABLE:
                            mask_pil = Image.fromarray(mask.astype(np.uint8))
                            mask_resized = mask_pil.resize((target_shape[1], target_shape[0]), Image.Resampling.NEAREST)
                            return np.array(mask_resized)
                        else:
                            return cv2.resize(mask, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST)
                    return mask
                except Exception as resize_error:
                    self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {resize_error}")
                    return mask
            
            target_shape = image.shape[:2]
            
            # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
            if 'all_clothes' in masks and PIL_AVAILABLE:
                try:
                    overlay_img = image.copy()
                    mask = safe_resize_mask(masks['all_clothes'], target_shape)
                    
                    # í¬ê¸° ê²€ì¦
                    if mask.shape == target_shape:
                        # ë¹¨ê°„ìƒ‰ ì˜¤ë²„ë ˆì´
                        overlay_img[mask > 128] = [255, 0, 0]
                        
                        # ë¸”ë Œë”©
                        alpha = 0.6
                        blended = (alpha * overlay_img + (1 - alpha) * image).astype(np.uint8)
                        
                        # Base64 ë³€í™˜
                        pil_image = Image.fromarray(blended)
                        buffer = BytesIO()
                        pil_image.save(buffer, format='JPEG', quality=95)
                        overlay_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                        visualizations['mask_overlay'] = f"data:image/jpeg;base64,{overlay_base64}"
                    else:
                        self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ì˜¤ë²„ë ˆì´ ìŠ¤í‚µ: {mask.shape} vs {target_shape}")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°í™”
            try:
                category_colors = {
                    'upper_body': [255, 0, 0],    # ë¹¨ê°•
                    'lower_body': [0, 255, 0],    # ì´ˆë¡
                    'full_body': [0, 0, 255],     # íŒŒë‘
                    'accessories': [255, 255, 0]  # ë…¸ë‘
                }
                
                category_overlay = image.copy()
                for category, color in category_colors.items():
                    if category in masks:
                        mask = safe_resize_mask(masks[category], target_shape)
                        if mask.shape == target_shape:
                            category_overlay[mask > 128] = color
                        else:
                            self.logger.warning(f"âš ï¸ {category} ë§ˆìŠ¤í¬ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ìŠ¤í‚µ: {mask.shape} vs {target_shape}")
                
                # ë¸”ë Œë”©
                alpha = 0.5
                category_blended = (alpha * category_overlay + (1 - alpha) * image).astype(np.uint8)
                
                # Base64 ë³€í™˜
                pil_image = Image.fromarray(category_blended)
                buffer = BytesIO()
                pil_image.save(buffer, format='JPEG', quality=95)
                category_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                visualizations['category_overlay'] = f"data:image/jpeg;base64,{category_base64}"
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ë¶„í• ëœ ì˜ë¥˜ ì´ë¯¸ì§€
            if 'all_clothes' in masks:
                try:
                    mask = safe_resize_mask(masks['all_clothes'], target_shape)
                    if mask.shape == target_shape:
                        segmented = image.copy()
                        segmented[mask <= 128] = [0, 0, 0]  # ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ
                        
                        # Base64 ë³€í™˜
                        pil_image = Image.fromarray(segmented)
                        buffer = BytesIO()
                        pil_image.save(buffer, format='JPEG', quality=95)
                        segmented_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                        visualizations['segmented_clothing'] = f"data:image/jpeg;base64,{segmented_base64}"
                    else:
                        self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ë¶„í•  ì´ë¯¸ì§€ ìŠ¤í‚µ: {mask.shape} vs {target_shape}")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë¶„í• ëœ ì˜ë¥˜ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì‹œê°í™” ìƒì„± ì—¬ë¶€ í‘œì‹œ
            visualizations['visualization_created'] = len(visualizations) > 0
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {'visualization_created': False}
    
    def _detect_cloth_categories(self, masks: Dict[str, np.ndarray]) -> List[str]:
        """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€"""
        try:
            detected_categories = []
            
            for mask_key, mask in masks.items():
                if mask is not None and np.sum(mask > 128) > 100:  # ìµœì†Œ í”½ì…€ ìˆ˜ ì²´í¬
                    if mask_key == 'upper_body':
                        detected_categories.extend(['shirt', 't_shirt'])
                    elif mask_key == 'lower_body':
                        detected_categories.extend(['pants', 'jeans'])
                    elif mask_key == 'full_body':
                        detected_categories.append('dress')
                    elif mask_key == 'accessories':
                        detected_categories.extend(['shoes', 'bag'])
            
            # ì¤‘ë³µ ì œê±°
            detected_categories = list(set(detected_categories))
            
            # ë°°ê²½ì€ í•­ìƒ í¬í•¨
            if 'background' not in detected_categories:
                detected_categories.insert(0, 'background')
            
            return detected_categories
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return ['background']
    
    def _extract_cloth_features(self, masks: Dict[str, np.ndarray], image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ - ì•ˆì „í•œ í¬ê¸° ì¡°ì •"""
        try:
            features = {}
            
            if 'all_clothes' in masks:
                mask = masks['all_clothes']
                
                # ğŸ”¥ ì•ˆì „í•œ í¬ê¸° ì¡°ì •
                target_shape = image.shape[:2]
                if mask.shape != target_shape:
                    try:
                        if PIL_AVAILABLE:
                            mask_pil = Image.fromarray(mask.astype(np.uint8))
                            mask_resized = mask_pil.resize((target_shape[1], target_shape[0]), Image.Resampling.NEAREST)
                            mask = np.array(mask_resized)
                        else:
                            mask = cv2.resize(mask, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST)
                        
                        self.logger.debug(f"âœ… íŠ¹ì§• ì¶”ì¶œìš© ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •: {mask.shape} -> {target_shape}")
                    except Exception as resize_error:
                        self.logger.warning(f"âš ï¸ íŠ¹ì§• ì¶”ì¶œìš© ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {resize_error}")
                        return {}
                
                # í¬ê¸° ê²€ì¦
                if mask.shape != target_shape:
                    self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ íŠ¹ì§• ì¶”ì¶œ ìŠ¤í‚µ: {mask.shape} vs {target_shape}")
                    return {}
                
                if NUMPY_AVAILABLE and mask.size > 0:
                    # ê¸°ë³¸ í†µê³„
                    features['area'] = int(np.sum(mask > 128))
                    features['centroid'] = self._calculate_centroid(mask)
                    features['bounding_box'] = self._calculate_bounding_box(mask)
                    
                    # ìƒ‰ìƒ íŠ¹ì§•
                    if len(image.shape) == 3:
                        try:
                            masked_pixels = image[mask > 128]
                            if len(masked_pixels) > 0:
                                features['dominant_color'] = [
                                    float(np.mean(masked_pixels[:, 0])),
                                    float(np.mean(masked_pixels[:, 1])),
                                    float(np.mean(masked_pixels[:, 2]))
                                ]
                            else:
                                features['dominant_color'] = [0.0, 0.0, 0.0]
                        except Exception as color_error:
                            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {color_error}")
                            features['dominant_color'] = [0.0, 0.0, 0.0]
            
            return features
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_centroid(self, mask: np.ndarray) -> Tuple[float, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE and mask.size > 0:
                y_coords, x_coords = np.where(mask > 128)
                if len(x_coords) > 0:
                    centroid_x = float(np.mean(x_coords))
                    centroid_y = float(np.mean(y_coords))
                    return (centroid_x, centroid_y)
            
            # í´ë°±
            h, w = mask.shape if mask.size > 0 else (512, 512)
            return (w / 2.0, h / 2.0)
            
        except Exception:
            return (256.0, 256.0)
    
    def _calculate_bounding_box(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """ê²½ê³„ ë°•ìŠ¤ ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE and mask.size > 0:
                rows = np.any(mask > 128, axis=1)
                cols = np.any(mask > 128, axis=0)
                
                if np.any(rows) and np.any(cols):
                    rmin, rmax = np.where(rows)[0][[0, -1]]
                    cmin, cmax = np.where(cols)[0][[0, -1]]
                    return (int(cmin), int(rmin), int(cmax), int(rmax))
            
            # í´ë°±
            h, w = mask.shape if mask.size > 0 else (512, 512)
            return (0, 0, w, h)
            
        except Exception:
            return (0, 0, 512, 512)
    
    def _extract_cloth_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """ì˜ë¥˜ ìœ¤ê³½ì„  ì¶”ì¶œ"""
        try:
            contours = []
            
            if SKIMAGE_AVAILABLE and mask.size > 0:
                # ìœ¤ê³½ì„  ì°¾ê¸°
                contour_coords = measure.find_contours(mask > 128, 0.5)
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                for contour in contour_coords:
                    if len(contour) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                        contours.append(contour.astype(np.int32))
            
            return contours
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_cloth_bounding_boxes(self, masks: Dict[str, np.ndarray]) -> Dict[str, Dict[str, int]]:
        """ì˜ë¥˜ë³„ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        try:
            bboxes = {}
            for category, mask in masks.items():
                if category != 'all_clothes' and np.any(mask):
                    bbox = self._calculate_bounding_box(mask)
                    bboxes[category] = {
                        'x1': bbox[0], 'y1': bbox[1], 'x2': bbox[2], 'y2': bbox[3],
                        'width': bbox[2] - bbox[0], 'height': bbox[3] - bbox[1],
                        'center_x': (bbox[0] + bbox[2]) // 2,
                        'center_y': (bbox[1] + bbox[3]) // 2
                    }
            return bboxes
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_cloth_centroids(self, masks: Dict[str, np.ndarray]) -> Dict[str, Tuple[float, float]]:
        """ì˜ë¥˜ë³„ ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            centroids = {}
            for category, mask in masks.items():
                if category != 'all_clothes' and np.any(mask):
                    centroid = self._calculate_centroid(mask)
                    centroids[category] = centroid
            return centroids
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ì¤‘ì‹¬ì  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_cloth_areas(self, masks: Dict[str, np.ndarray]) -> Dict[str, int]:
        """ì˜ë¥˜ë³„ ë©´ì  ê³„ì‚°"""
        try:
            areas = {}
            for category, mask in masks.items():
                if category != 'all_clothes':
                    area = int(np.sum(mask))
                    areas[category] = area
            return areas
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë©´ì  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_cloth_contours_dict(self, masks: Dict[str, np.ndarray]) -> Dict[str, List[np.ndarray]]:
        """ì˜ë¥˜ë³„ ìœ¤ê³½ì„  ê³„ì‚°"""
        try:
            contours_dict = {}
            for category, mask in masks.items():
                if category != 'all_clothes' and np.any(mask):
                    contours = self._extract_cloth_contours(mask)
                    contours_dict[category] = contours
            return contours_dict
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ìœ¤ê³½ì„  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _update_ai_stats(self, method: str, confidence: float, total_time: float, quality_metrics: Dict[str, float]):
        """AI í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.ai_stats['total_processed'] += 1
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            prev_avg = self.ai_stats.get('average_confidence', 0.0)
            count = self.ai_stats['total_processed']
            self.ai_stats['average_confidence'] = (prev_avg * (count - 1) + confidence) / count
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_emergency_result(self, reason: str) -> Dict[str, Any]:
        """ë¹„ìƒ ê²°ê³¼ ìƒì„±"""
        emergency_masks = {
            'all_clothes': np.zeros((512, 512), dtype=np.uint8),
            'upper_body': np.zeros((512, 512), dtype=np.uint8),
            'lower_body': np.zeros((512, 512), dtype=np.uint8),
            'full_body': np.zeros((512, 512), dtype=np.uint8),
            'accessories': np.zeros((512, 512), dtype=np.uint8)
        }
        
        return {
            'success': False,
            'step': self.step_name,
            'segmentation_masks': emergency_masks,
            'cloth_categories': ['background'],
            'segmentation_confidence': 0.0,
            'processing_time': 0.1,
            'model_used': 'emergency',
            'items_detected': 0,
            'emergency_reason': reason[:100],
            'metadata': {
                'emergency_mode': True,
                'version': '33.0',
                'central_hub_connected': False
            }
        }
    
    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return list(self.ai_models.keys())

    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(self, 'di_container') and self.di_container:
                return self.di_container.get_service(service_key)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (kwargs ë°©ì‹) - ê°•í™”ëœ ì´ë¯¸ì§€ ì „ë‹¬"""
        try:
            # ğŸ”¥ PIL Image ëª¨ë“ˆì„ ë¨¼ì € import
            import base64
            from io import BytesIO
            from PIL import Image
            
            step_input = api_input.copy()
            
            # ï¿½ï¿½ ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° ìƒì„¸ ë¶„ì„
            self.logger.info(f"ğŸ” convert_api_input_to_step_input ì‹œì‘")
            self.logger.info(f"ğŸ” api_input í‚¤ë“¤: {list(api_input.keys())}")
            self.logger.info(f"ğŸ” step_input í‚¤ë“¤: {list(step_input.keys())}")
            
            # ğŸ”¥ ê°•í™”ëœ ì´ë¯¸ì§€ ì ‘ê·¼ ë°©ì‹
            image = None
            
            # 1ìˆœìœ„: ì§ì ‘ ì „ë‹¬ëœ PIL Image ê°ì²´ (prepare_step_input_dataì—ì„œ ë³€í™˜ëœ ì´ë¯¸ì§€)
            clothing_image_keys = ['clothing_image', 'cloth_image', 'target_image', 'garment_image']
            for key in clothing_image_keys:
                if key in step_input and step_input[key] is not None:
                    image = step_input[key]
                    self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ {key} ì‚¬ìš© (PIL Image)")
                    break
            
            if image is None:
                general_image_keys = ['image', 'input_image', 'original_image', 'person_image']
                for key in general_image_keys:
                    if key in step_input and step_input[key] is not None:
                        image = step_input[key]
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ {key} ì‚¬ìš© (PIL Image)")
                        break
            
            # 2ìˆœìœ„: ì„¸ì…˜ ë°ì´í„°ì—ì„œ base64 ë¡œë“œ (fallback)
            if image is None and 'session_data' in step_input:
                session_data = step_input['session_data']
                self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° í‚¤ë“¤: {list(session_data.keys())}")
                self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° íƒ€ì…: {type(session_data)}")
                
                if isinstance(session_data, dict):
                    self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° ê¸¸ì´: {len(session_data)}")
                    
                    # original_clothing_image ì°¾ê¸° (ìš°ì„ ìˆœìœ„ 1)
                    if 'original_clothing_image' in session_data:
                        try:
                            clothing_b64 = session_data['original_clothing_image']
                            if clothing_b64 and len(clothing_b64) > 100:  # ìœ íš¨í•œ base64ì¸ì§€ í™•ì¸
                                clothing_bytes = base64.b64decode(clothing_b64)
                                image = Image.open(BytesIO(clothing_bytes)).convert('RGB')
                                self.logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_clothing_image ë¡œë“œ: {image.size}")
                            else:
                                self.logger.warning("âš ï¸ original_clothing_imageê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ")
                        except Exception as session_error:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ clothing ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                    
                    # original_person_image ì°¾ê¸° (clothing_imageê°€ ì—†ëŠ” ê²½ìš°)
                    if image is None and 'original_person_image' in session_data:
                        try:
                            person_b64 = session_data['original_person_image']
                            if person_b64 and len(person_b64) > 100:  # ìœ íš¨í•œ base64ì¸ì§€ í™•ì¸
                                person_bytes = base64.b64decode(person_b64)
                                image = Image.open(BytesIO(person_bytes)).convert('RGB')
                                self.logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_person_image ë¡œë“œ: {image.size}")
                            else:
                                self.logger.warning("âš ï¸ original_person_imageê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ")
                        except Exception as session_error:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ person ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                    
                    # ğŸ”¥ ì¶”ê°€: clothing_image í‚¤ë„ í™•ì¸
                    if image is None and 'clothing_image' in session_data:
                        try:
                            clothing_img = session_data['clothing_image']
                            if isinstance(clothing_img, str) and len(clothing_img) > 100:
                                clothing_bytes = base64.b64decode(clothing_img)
                                image = Image.open(BytesIO(clothing_bytes)).convert('RGB')
                                self.logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ clothing_image ë¡œë“œ: {image.size}")
                        except Exception as session_error:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ clothing_image ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                    else:
                        self.logger.warning(f"ğŸ” ì„¸ì…˜ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(session_data)}")
                else:
                    if 'session_data' not in step_input:
                        self.logger.warning("âš ï¸ session_dataê°€ api_inputì— ì—†ìŒ")
                        self.logger.warning(f"âš ï¸ api_inputì— ìˆëŠ” í‚¤ë“¤: {list(api_input.keys())}")
                
                # 3ìˆœìœ„: ê¸°ë³¸ê°’
                if image is None:
                    self.logger.warning("âš ï¸ ì´ë¯¸ì§€ê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                    image = None
            
            # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
            converted_input = {
                'image': image,
                'clothing_image': image,
                'cloth_image': image,
                'session_id': step_input.get('session_id'),
                'analysis_detail': step_input.get('analysis_detail', 'medium'),
                'clothing_type': step_input.get('clothing_type', 'shirt'),
                'session_data': step_input.get('session_data', {})  # ğŸ”¥ session_data ëª…ì‹œì  í¬í•¨
            }
            
            #  ìƒì„¸ ë¡œê¹…
            self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ìƒíƒœ: {'ìˆìŒ' if image is not None else 'ì—†ìŒ'}")
            if image is not None:
                self.logger.info(f"âœ… ì´ë¯¸ì§€ ì •ë³´: íƒ€ì…={type(image)}, í¬ê¸°={getattr(image, 'size', 'unknown')}")
            else:
                self.logger.error("âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - AI ì²˜ë¦¬ ë¶ˆê°€ëŠ¥")
            
            return converted_input
            
        except Exception as e:
            self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return api_input    

    def get_model_info(self, model_key: str = None) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if model_key:
            if model_key in self.ai_models:
                return {
                    'model_key': model_key,
                    'model_path': self.model_paths.get(model_key, 'unknown'),
                    'is_loaded': self.models_loading_status.get(model_key, False),
                    'model_type': self._get_model_type(model_key)
                }
            else:
                return {}
        else:
            return {
                key: {
                    'model_path': self.model_paths.get(key, 'unknown'),
                    'is_loaded': self.models_loading_status.get(key, False),
                    'model_type': self._get_model_type(key)
                }
                for key in self.ai_models.keys()
            }
    
    def _get_model_type(self, model_key: str) -> str:
        """ëª¨ë¸ í‚¤ì—ì„œ ëª¨ë¸ íƒ€ì… ì¶”ë¡ """
        type_mapping = {
            'deeplabv3plus': 'DeepLabV3PlusModel',
            'deeplabv3plus_fallback': 'DeepLabV3PlusModel',
            'sam_huge': 'SAMModel',
            'u2net_cloth': 'U2NetModel',
            'maskrcnn': 'MaskRCNNModel'
        }
        return type_mapping.get(model_key, 'BaseModel')
    
    def get_segmentation_stats(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„ ë°˜í™˜"""
        return dict(self.ai_stats)
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self.cache_lock:
                self.segmentation_cache.clear()
                self.cloth_cache.clear()
                self.logger.info("âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def reload_models(self):
        """AI ëª¨ë¸ ì¬ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ì¬ë¡œë”© ì‹œì‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬
            self.ai_models.clear()
            self.segmentation_models.clear()
            for key in self.models_loading_status:
                if isinstance(self.models_loading_status[key], bool):
                    self.models_loading_status[key] = False
            
            # Central Hubë¥¼ í†µí•œ ì¬ë¡œë”©
            self._load_segmentation_models_via_central_hub()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì¬ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool))
            self.logger.info(f"âœ… AI ëª¨ë¸ ì¬ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì¬ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def validate_configuration(self) -> Dict[str, Any]:
        """ì„¤ì • ê²€ì¦"""
        try:
            validation_result = {
                'valid': True,
                'errors': [],
                'warnings': [],
                'info': {}
            }
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ ê²€ì¦
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            if loaded_count == 0:
                validation_result['errors'].append("AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                validation_result['valid'] = False
            elif loaded_count < 2:
                validation_result['warnings'].append(f"ì¼ë¶€ AI ëª¨ë¸ë§Œ ë¡œë“œë¨: {loaded_count}ê°œ")
            
            # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
            if not TORCH_AVAILABLE:
                validation_result['errors'].append("PyTorchê°€ í•„ìš”í•¨")
                validation_result['valid'] = False
            
            if not PIL_AVAILABLE:
                validation_result['errors'].append("PILì´ í•„ìš”í•¨")
                validation_result['valid'] = False
            
            # ê²½ê³ ì‚¬í•­
            if not SAM_AVAILABLE:
                validation_result['warnings'].append("SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
            
            # ì •ë³´
            validation_result['info'] = {
                'models_loaded': loaded_count,
                'available_methods': len(self.available_methods),
                'device': self.device,
                'quality_level': self.config.quality_level.value,
                'central_hub_connected': hasattr(self, 'model_loader') and self.model_loader is not None
            }
            
            return validation_result
            
        except Exception as e:
            return {
                'valid': False,
                'errors': [f"ê²€ì¦ ì‹¤íŒ¨: {e}"],
                'warnings': [],
                'info': {}
            }

    def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
        """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not isinstance(step_output, dict):
                self.logger.warning(f"âš ï¸ step_outputì´ dictê°€ ì•„ë‹˜: {type(step_output)}")
                return {
                    'success': False,
                    'error': f'Invalid output type: {type(step_output)}',
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
            
            # ê¸°ë³¸ API ì‘ë‹µ êµ¬ì¡°
            api_response = {
                'success': step_output.get('success', True),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0),
                'timestamp': time.time()
            }
            
            # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
            if not api_response['success']:
                api_response['error'] = step_output.get('error', 'Unknown error')
                return api_response
            
            # ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ë³€í™˜
            if 'segmentation_result' in step_output:
                seg_result = step_output['segmentation_result']
                api_response['segmentation_data'] = {
                    'masks': seg_result.get('masks', {}),
                    'confidence_scores': seg_result.get('confidence_scores', {}),
                    'overall_confidence': seg_result.get('overall_confidence', 0.0),
                    'segmentation_quality': seg_result.get('segmentation_quality', 'unknown'),
                    'model_used': seg_result.get('model_used', 'unknown'),
                    'cloth_categories': seg_result.get('cloth_categories', []),
                    'cloth_features': seg_result.get('cloth_features', {}),
                    'visualization': seg_result.get('visualization', {})
                }
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            api_response['metadata'] = {
                'models_available': list(self.ai_models.keys()) if hasattr(self, 'ai_models') else [],
                'device_used': getattr(self, 'device', 'unknown'),
                'input_size': step_output.get('input_size', [0, 0]),
                'output_size': step_output.get('output_size', [0, 0]),
                'available_methods': [method.value for method in self.available_methods] if hasattr(self, 'available_methods') else []
            }
            
            # ì‹œê°í™” ë°ì´í„° (ìˆëŠ” ê²½ìš°)
            if 'visualization' in step_output:
                api_response['visualization'] = step_output['visualization']
            
            # ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
            if 'analysis' in step_output:
                api_response['analysis'] = step_output['analysis']
            
            self.logger.info(f"âœ… ClothSegmentationStep ì¶œë ¥ ë³€í™˜ ì™„ë£Œ: {len(api_response)}ê°œ í‚¤")
            return api_response
            
        except Exception as e:
            self.logger.error(f"âŒ ClothSegmentationStep ì¶œë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': f'Output conversion failed: {str(e)}',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0) if isinstance(step_output, dict) else 0.0
            }

    # ==============================================
    # ğŸ”¥ ëˆ„ë½ëœ ì‹ ê²½ë§ ê¸°ë°˜ AI ë©”ì„œë“œë“¤ ì¶”ê°€
    # ==============================================

    def _run_ai_segmentation_sync_safe(
        self, 
        image: np.ndarray, 
        quality_level: QualityLevel, 
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì•ˆì „í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ë³´í˜¸)"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_segmentation_sync_safe ì‹œì‘")
            result = self._run_ai_segmentation_sync(image, quality_level, person_parsing, pose_info)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_segmentation_sync_safe ì™„ë£Œ")
            return result
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_segmentation_sync_safe ì˜¤ë¥˜: {str(e)}")
            return self._create_fallback_segmentation_result(image.shape)

    def _run_ai_segmentation_sync(
        self, 
        image: np.ndarray, 
        quality_level: QualityLevel, 
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë™ê¸°) - ì™„ì „ êµ¬í˜„"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_segmentation_sync ì‹œì‘")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {list(self.ai_models.keys())}")
            
            # M3 Max í™˜ê²½ì—ì„œëŠ” SAM ìš°ì„  ì‚¬ìš© (U2Net ì˜¤ë¥˜ ë°©ì§€)
            if IS_M3_MAX and 'sam_huge' in self.ai_models:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] M3 Max í™˜ê²½ - SAM ìš°ì„  ì‚¬ìš© (U2Net ì˜¤ë¥˜ ë°©ì§€)")
                result = self.ai_models['sam_huge'].predict(image)
                self.ai_stats['sam_calls'] += 1
                result['method_used'] = 'sam_huge'
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì˜ˆì¸¡ ì™„ë£Œ")
                return result
            
            # SAM ìš°ì„ ìˆœìœ„ë¡œ ëª¨ë¸ ì„ íƒ ë° ì‹¤í–‰
            if 'sam_huge' in self.ai_models:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ì‚¬ìš© ì‹œì‘")
                # SAM ì‚¬ìš© (ìš°ì„ ìˆœìœ„ 1 - ê°€ì¥ ì•ˆì „)
                prompts = self._generate_sam_prompts(image, person_parsing, pose_info)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
                result = self.ai_models['sam_huge'].predict(image, prompts)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì˜ˆì¸¡ ì™„ë£Œ")
                self.ai_stats['sam_calls'] += 1
                result['method_used'] = 'sam_huge'
                
                # SAM ê²°ê³¼ í–¥ìƒ
                if result.get('masks'):
                    result['masks'] = self._enhance_sam_results(
                        result['masks'], image, person_parsing
                    )
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ê²°ê³¼ í–¥ìƒ ì™„ë£Œ")
                return result
                
            elif 'u2net_cloth' in self.ai_models:
                # U2Net ì‚¬ìš© (ìš°ì„ ìˆœìœ„ 2 - M3 Max ì•ˆì „)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ì‚¬ìš©")
                result = self.ai_models['u2net_cloth'].predict(image)
                self.ai_stats['u2net_calls'] += 1
                result['method_used'] = 'u2net_cloth'
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì˜ˆì¸¡ ì™„ë£Œ")
                return result
                
            elif quality_level == QualityLevel.ULTRA and 'deeplabv3plus' in self.ai_models:
                # DeepLabV3+ ì‚¬ìš© (ë‚˜ì¤‘ì— - í˜„ì¬ ë¹„í™œì„±í™”)
                result = self.ai_models['deeplabv3plus'].predict(image)
                self.ai_stats['deeplabv3_calls'] += 1
                result['method_used'] = 'deeplabv3plus'
                
                # ì¶”ê°€ í›„ì²˜ë¦¬ (ULTRA í’ˆì§ˆ)
                if result.get('masks'):
                    result['masks'] = self._apply_ultra_quality_postprocessing(
                        result['masks'], image, person_parsing, pose_info
                    )
                
                return result
                
            else:
                # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” (ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©)
                return self._run_hybrid_ensemble_sync(image, person_parsing, pose_info)
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0, "method_used": "error"}

    def _apply_ultra_quality_postprocessing(
        self, 
        masks: Dict[str, np.ndarray], 
        image: np.ndarray,
        person_parsing: Dict[str, Any], 
        pose_info: Dict[str, Any]
    ) -> Dict[str, np.ndarray]:
        """Ultra í’ˆì§ˆ í›„ì²˜ë¦¬ ì ìš©"""
        try:
            enhanced_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    enhanced_masks[mask_key] = mask
                    continue
                
                # 1. CRF í›„ì²˜ë¦¬
                if DENSECRF_AVAILABLE:
                    crf_mask = AdvancedPostProcessor.apply_crf_postprocessing(
                        mask, image, num_iterations=15
                    )
                else:
                    crf_mask = mask
                
                # 2. ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
                multiscale_mask = AdvancedPostProcessor.apply_multiscale_processing(
                    image, crf_mask
                )
                
                # 3. ì—£ì§€ ì •ì œ
                edge_refined_masks = AdvancedPostProcessor.apply_edge_refinement(
                    {mask_key: multiscale_mask}, image
                )
                
                # 4. Person parsing ì •ë³´ í™œìš©í•œ ì •ì œ
                if person_parsing and 'clothing_regions' in person_parsing:
                    refined_mask = self._refine_with_person_parsing(
                        edge_refined_masks[mask_key], person_parsing['clothing_regions'], mask_key
                    )
                else:
                    refined_mask = edge_refined_masks[mask_key]
                
                # 5. Pose ì •ë³´ í™œìš©í•œ ì •ì œ
                if pose_info and 'keypoints' in pose_info:
                    final_mask = self._refine_with_pose_info(
                        refined_mask, pose_info['keypoints'], mask_key
                    )
                else:
                    final_mask = refined_mask
                
                enhanced_masks[mask_key] = final_mask
            
            return enhanced_masks
            
        except Exception as e:
            self.logger.warning(f"Ultra í’ˆì§ˆ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return masks

    def _enhance_sam_results(
        self, 
        masks: Dict[str, np.ndarray], 
        image: np.ndarray,
        person_parsing: Dict[str, Any]
    ) -> Dict[str, np.ndarray]:
        """SAM ê²°ê³¼ í–¥ìƒ"""
        try:
            enhanced_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    enhanced_masks[mask_key] = mask
                    continue
                
                # SAM íŠ¹í™” í›„ì²˜ë¦¬
                # 1. ì—°ê²°ì„± í–¥ìƒ
                if SKIMAGE_AVAILABLE:
                    labeled = measure.label(mask > 128)
                    regions = measure.regionprops(labeled)
                    
                    if regions:
                        # ê°€ì¥ í° ì—°ê²° êµ¬ì„±ìš”ì†Œ ìœ ì§€
                        largest_region = max(regions, key=lambda r: r.area)
                        enhanced_mask = np.zeros_like(mask)
                        enhanced_mask[labeled == largest_region.label] = 255
                    else:
                        enhanced_mask = mask
                else:
                    enhanced_mask = mask
                
                # 2. ê²½ê³„ì„  ìŠ¤ë¬´ë”©
                if SCIPY_AVAILABLE:
                    enhanced_mask = ndimage.gaussian_filter(
                        enhanced_mask.astype(np.float32), sigma=0.5
                    )
                    enhanced_mask = (enhanced_mask > 127).astype(np.uint8) * 255
                
                enhanced_masks[mask_key] = enhanced_mask
            
            return enhanced_masks
            
        except Exception as e:
            self.logger.warning(f"SAM ê²°ê³¼ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return masks

    def _enhance_u2net_results(
        self, 
        masks: Dict[str, np.ndarray], 
        image: np.ndarray,
        person_parsing: Dict[str, Any]
    ) -> Dict[str, np.ndarray]:
        """U2Net ê²°ê³¼ í–¥ìƒ"""
        try:
            enhanced_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    enhanced_masks[mask_key] = mask
                    continue
                
                # U2Net íŠ¹í™” í›„ì²˜ë¦¬
                # 1. ì ì‘ì  ì„ê³„ê°’ ì ìš©
                if SKIMAGE_AVAILABLE:
                    try:
                        threshold = filters.threshold_otsu(mask)
                        enhanced_mask = (mask > threshold).astype(np.uint8) * 255
                    except:
                        enhanced_mask = mask
                else:
                    enhanced_mask = mask
                
                # 2. í™€ ì±„ìš°ê¸°
                if SCIPY_AVAILABLE:
                    filled = ndimage.binary_fill_holes(enhanced_mask > 128)
                    enhanced_mask = (filled * 255).astype(np.uint8)
                
                # 3. í˜•íƒœí•™ì  ì •ì œ
                if SCIPY_AVAILABLE:
                    structure = np.ones((3, 3))
                    opened = ndimage.binary_opening(enhanced_mask > 128, structure=structure)
                    closed = ndimage.binary_closing(opened, structure=np.ones((5, 5)))
                    enhanced_mask = (closed * 255).astype(np.uint8)
                
                enhanced_masks[mask_key] = enhanced_mask
            
            return enhanced_masks
            
        except Exception as e:
            self.logger.warning(f"U2Net ê²°ê³¼ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return masks

    def _generate_sam_prompts(
        self, 
        image: np.ndarray, 
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ìƒì„± - ì™„ì „ êµ¬í˜„"""
        try:
            prompts = {}
            h, w = image.shape[:2]
            
            # ê¸°ë³¸ í¬ì¸íŠ¸ë“¤
            points = []
            labels = []
            
            # ì¤‘ì•™ í¬ì¸íŠ¸ë“¤
            center_points = [
                (w // 2, h // 2),           # ì¤‘ì•™
                (w // 3, h // 2),           # ì¢Œì¸¡
                (2 * w // 3, h // 2),       # ìš°ì¸¡
                (w // 2, h // 3),           # ìƒë‹¨
                (w // 2, 2 * h // 3),       # í•˜ë‹¨
            ]
            
            points.extend(center_points)
            labels.extend([1] * len(center_points))
            
            # Person parsing ì •ë³´ í™œìš©
            if person_parsing and 'clothing_regions' in person_parsing:
                clothing_regions = person_parsing['clothing_regions']
                for region in clothing_regions[:5]:  # ìµœëŒ€ 5ê°œ ì˜ì—­
                    if 'center' in region and len(region['center']) >= 2:
                        center_x, center_y = region['center'][:2]
                        # ìœ íš¨í•œ ì¢Œí‘œì¸ì§€ í™•ì¸
                        if 0 <= center_x < w and 0 <= center_y < h:
                            points.append((int(center_x), int(center_y)))
                            labels.append(1)
                    
                    # ê²½ê³„ ë°•ìŠ¤ ì •ë³´ í™œìš©
                    if 'bbox' in region and len(region['bbox']) >= 4:
                        x1, y1, x2, y2 = region['bbox'][:4]
                        # ê²½ê³„ ë°•ìŠ¤ ì¤‘ì‹¬ì 
                        bbox_center_x = (x1 + x2) // 2
                        bbox_center_y = (y1 + y2) // 2
                        if 0 <= bbox_center_x < w and 0 <= bbox_center_y < h:
                            points.append((int(bbox_center_x), int(bbox_center_y)))
                            labels.append(1)
            
            # Pose ì •ë³´ í™œìš©
            if pose_info and 'keypoints' in pose_info:
                keypoints = pose_info['keypoints']
                
                # ì˜ë¥˜ ê´€ë ¨ í‚¤í¬ì¸íŠ¸ë“¤
                clothing_keypoints = [
                    'left_shoulder', 'right_shoulder',
                    'left_hip', 'right_hip',
                    'left_elbow', 'right_elbow',
                    'left_wrist', 'right_wrist'
                ]
                
                for kp_name in clothing_keypoints:
                    if kp_name in keypoints:
                        kp = keypoints[kp_name]
                        if len(kp) >= 2 and kp[2] > 0.5:  # ì‹ ë¢°ë„ ì²´í¬
                            kp_x, kp_y = int(kp[0]), int(kp[1])
                            if 0 <= kp_x < w and 0 <= kp_y < h:
                                points.append((kp_x, kp_y))
                                labels.append(1)
            
            prompts['points'] = points
            prompts['labels'] = labels
            
            # ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸ ìƒì„± (person parsingì—ì„œ)
            boxes = []
            if person_parsing and 'clothing_regions' in person_parsing:
                for region in person_parsing['clothing_regions'][:3]:  # ìµœëŒ€ 3ê°œ
                    if 'bbox' in region and len(region['bbox']) >= 4:
                        x1, y1, x2, y2 = region['bbox'][:4]
                        # ìœ íš¨ì„± ê²€ì¦
                        if 0 <= x1 < x2 < w and 0 <= y1 < y2 < h:
                            boxes.append([x1, y1, x2, y2])
            
            if boxes:
                prompts['boxes'] = boxes
            
            return prompts
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ SAM í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2]
            return {
                'points': [(w // 2, h // 2)],
                'labels': [1]
            }

    def _refine_with_person_parsing(
        self, 
        mask: np.ndarray, 
        clothing_regions: List[Dict[str, Any]], 
        mask_type: str
    ) -> np.ndarray:
        """Person parsing ì •ë³´ë¥¼ í™œìš©í•œ ë§ˆìŠ¤í¬ ì •ì œ"""
        try:
            refined_mask = mask.copy()
            
            # ë§ˆìŠ¤í¬ íƒ€ì…ì— ë”°ë¥¸ í•„í„°ë§
            relevant_categories = {
                'upper_body': ['shirt', 't_shirt', 'sweater', 'hoodie', 'jacket', 'coat'],
                'lower_body': ['pants', 'jeans', 'shorts', 'skirt'],
                'full_body': ['dress'],
                'accessories': ['shoes', 'boots', 'sneakers', 'bag', 'hat', 'glasses', 'scarf', 'belt']
            }
            
            target_categories = relevant_categories.get(mask_type, [])
            
            for region in clothing_regions:
                category = region.get('category', '').lower()
                confidence = region.get('confidence', 0.0)
                
                # ê´€ë ¨ ì¹´í…Œê³ ë¦¬ì´ê³  ì‹ ë¢°ë„ê°€ ë†’ì€ ê²½ìš°
                if category in target_categories and confidence > 0.7:
                    if 'mask' in region:
                        # Person parsing ë§ˆìŠ¤í¬ì™€ ê²°í•©
                        person_mask = region['mask']
                        if person_mask.shape == refined_mask.shape:
                            # êµì§‘í•©ì„ ê°•í™”
                            intersection = np.logical_and(refined_mask > 128, person_mask > 128)
                            refined_mask[intersection] = 255
                            
                            # Person parsingì—ì„œ í™•ì‹¤í•œ ì˜ì—­ ì¶”ê°€
                            high_conf_area = person_mask > 200
                            refined_mask[high_conf_area] = np.maximum(
                                refined_mask[high_conf_area], person_mask[high_conf_area]
                            )
            
            return refined_mask
            
        except Exception as e:
            self.logger.warning(f"Person parsing ê¸°ë°˜ ì •ì œ ì‹¤íŒ¨: {e}")
            return mask

    def _refine_with_pose_info(
        self, 
        mask: np.ndarray, 
        keypoints: Dict[str, Any], 
        mask_type: str
    ) -> np.ndarray:
        """Pose ì •ë³´ë¥¼ í™œìš©í•œ ë§ˆìŠ¤í¬ ì •ì œ"""
        try:
            refined_mask = mask.copy()
            h, w = mask.shape
            
            # ë§ˆìŠ¤í¬ íƒ€ì…ì— ë”°ë¥¸ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
            keypoint_mapping = {
                'upper_body': ['left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow'],
                'lower_body': ['left_hip', 'right_hip', 'left_knee', 'right_knee'],
                'accessories': ['left_ankle', 'right_ankle']  # ì‹ ë°œ ë“±
            }
            
            relevant_keypoints = keypoint_mapping.get(mask_type, [])
            
            # í‚¤í¬ì¸íŠ¸ ì£¼ë³€ ì˜ì—­ ê°•í™”
            for kp_name in relevant_keypoints:
                if kp_name in keypoints:
                    kp = keypoints[kp_name]
                    if len(kp) >= 3 and kp[2] > 0.5:  # ì‹ ë¢°ë„ ì²´í¬
                        kp_x, kp_y = int(kp[0]), int(kp[1])
                        
                        # í‚¤í¬ì¸íŠ¸ ì£¼ë³€ ë°˜ê²½ ë‚´ ë§ˆìŠ¤í¬ ê°•í™”
                        radius = min(h, w) // 20  # ì ì‘ì  ë°˜ê²½
                        
                        y_min = max(0, kp_y - radius)
                        y_max = min(h, kp_y + radius)
                        x_min = max(0, kp_x - radius)
                        x_max = min(w, kp_x + radius)
                        
                        # ì›í˜• ì˜ì—­ ìƒì„±
                        y_coords, x_coords = np.ogrid[y_min:y_max, x_min:x_max]
                        circle_mask = (x_coords - kp_x) ** 2 + (y_coords - kp_y) ** 2 <= radius ** 2
                        
                        # í•´ë‹¹ ì˜ì—­ì˜ ë§ˆìŠ¤í¬ ê°’ ì¦ê°•
                        region_slice = refined_mask[y_min:y_max, x_min:x_max]
                        region_slice[circle_mask] = np.maximum(
                            region_slice[circle_mask], 
                            (region_slice[circle_mask] * 1.2).clip(0, 255).astype(np.uint8)
                        )
            
            return refined_mask
            
        except Exception as e:
            self.logger.warning(f"Pose ì •ë³´ ê¸°ë°˜ ì •ì œ ì‹¤íŒ¨: {e}")
            return mask

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 8: íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„±"""
    m3_config = ClothSegmentationConfig(
        method=SegmentationMethod.DEEPLABV3_PLUS,
        quality_level=QualityLevel.ULTRA,
        enable_visualization=True,
        input_size=(512, 512),
        confidence_threshold=0.5
    )
    
    kwargs['segmentation_config'] = m3_config
    return ClothSegmentationStep(**kwargs)

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 9: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

def test_cloth_segmentation_ai():
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ (Central Hub DI Container v7.0)")
        print("=" * 80)
        
        # Step ìƒì„±
        step = create_cloth_segmentation_step(
            device="auto",
            segmentation_config=ClothSegmentationConfig(
                quality_level=QualityLevel.HIGH,
                enable_visualization=True,
                confidence_threshold=0.5
            )
        )
        
        # ì´ˆê¸°í™”
        if step.initialize():
            print(f"âœ… Step ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - ë¡œë“œëœ AI ëª¨ë¸: {len(step.ai_models)}ê°œ")
            print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {len(step.available_methods)}ê°œ")
            
            # ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ê³„ì‚°
            loaded_count = sum(1 for status in step.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in step.models_loading_status.values() 
                             if isinstance(status, bool))
            success_rate = (loaded_count / total_models * 100) if total_models > 0 else 0
            print(f"   - ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ : {loaded_count}/{total_models} ({success_rate:.1f}%)")
        else:
            print(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        test_image_array = np.array(test_image)
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        processed_input = {
            'image': test_image_array,
            'from_step_01': {},
            'from_step_02': {}
        }
        
        result = step._run_ai_inference(processed_input)
        
        if result and result.get('success', False):
            print(f"âœ… AI ì¶”ë¡  ì„±ê³µ")
            print(f"   - ë°©ë²•: {result.get('model_used', 'unknown')}")
            print(f"   - ì‹ ë¢°ë„: {result.get('segmentation_confidence', 0):.3f}")
            print(f"   - í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
            print(f"   - íƒì§€ëœ ì•„ì´í…œ: {result.get('items_detected', 0)}ê°œ")
            print(f"   - ì¹´í…Œê³ ë¦¬: {result.get('cloth_categories', [])}")
            print(f"   - Central Hub ì—°ê²°: {result.get('metadata', {}).get('central_hub_connected', False)}")
        else:
            print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_central_hub_compatibility():
    """Central Hub DI Container í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ Central Hub DI Container v7.0 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # Step ìƒì„±
        step = ClothSegmentationStep()
        
        # BaseStepMixin ìƒì† í™•ì¸
        print(f"âœ… BaseStepMixin ìƒì†: {isinstance(step, BaseStepMixin)}")
        print(f"âœ… Step ì´ë¦„: {step.step_name}")
        print(f"âœ… Step ID: {step.step_id}")
        
        # _run_ai_inference ë©”ì„œë“œ í™•ì¸
        import inspect
        is_async = inspect.iscoroutinefunction(step._run_ai_inference)
        print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async}")
        
        # í•„ìˆ˜ ì†ì„±ë“¤ í™•ì¸
        required_attrs = ['ai_models', 'models_loading_status', 'model_interface', 'loaded_models']
        for attr in required_attrs:
            has_attr = hasattr(step, attr)
            print(f"âœ… {attr} ì†ì„± ì¡´ì¬: {has_attr}")
        
        # Central Hub ì—°ê²° í™•ì¸
        central_hub_connected = hasattr(step, 'model_loader')
        print(f"âœ… Central Hub ì—°ê²°: {central_hub_connected}")
        
        print("âœ… Central Hub DI Container í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 10: ëª¨ë“ˆ ì •ë³´ ë° __all__
# ==============================================

__version__ = "33.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™"
__compatibility_version__ = "BaseStepMixin_v20.0"

__all__ = [
    'ClothSegmentationStep',
    'RealDeepLabV3PlusModel',
    'RealSAMModel',
    'RealU2NetClothModel',
    'DeepLabV3PlusModel',
    'DeepLabV3PlusBackbone',
    'ASPPModule',
    'SelfCorrectionModule',
    'SelfAttentionBlock',
    'SegmentationMethod',
    'ClothCategory',
    'QualityLevel',
    'ClothSegmentationConfig',
    'create_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    'test_cloth_segmentation_ai',
    'test_central_hub_compatibility'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger.info("=" * 120)
logger.info("ğŸ”¥ Step 03 Cloth Segmentation v33.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("=" * 120)
logger.info("ğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - 50% ì½”ë“œ ë‹¨ì¶•")
logger.info("   âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net ì§€ì›")
logger.info("   âœ… ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ 100% ìœ ì§€ - ASPP, Self-Correction, Progressive Parsing")
logger.info("   âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›")
logger.info("   âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬")
logger.info("   âœ… ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ - Mock ì œê±°í•˜ê³  ì§„ì§œ ëª¨ë¸ ì‚¬ìš©")

logger.info("ğŸ§  êµ¬í˜„ëœ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ (ì™„ì „ ë³µì›):")
logger.info("   ğŸ”¥ DeepLabV3+ ì•„í‚¤í…ì²˜ (Google ìµœì‹  ì„¸ê·¸ë©˜í…Œì´ì…˜)")
logger.info("   ğŸŒŠ ASPP (Atrous Spatial Pyramid Pooling) ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ” Self-Correction Learning ë©”ì»¤ë‹ˆì¦˜")
logger.info("   ğŸ“ˆ Progressive Parsing ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ¯ SAM + U2Net + DeepLabV3+ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”")
logger.info("   âš¡ CRF í›„ì²˜ë¦¬ + ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬")
logger.info("   ğŸ”€ Edge Detection ë¸Œëœì¹˜")
logger.info("   ğŸ’« Multi-scale Feature Fusion")
logger.info("   ğŸ¨ ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°")
logger.info("   ğŸ” ROI ê²€ì¶œ ë° ë°°ê²½ ë¶„ì„")
logger.info("   ğŸŒˆ ì¡°ëª… ì •ê·œí™” ë° ìƒ‰ìƒ ë³´ì •")
logger.info("   ğŸ“Š í’ˆì§ˆ í‰ê°€ ë° ìë™ ì¬ì‹œë„")

logger.info("ğŸ¨ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (20ê°œ í´ë˜ìŠ¤):")
logger.info("   - ìƒì˜: ì…”ì¸ , í‹°ì…”ì¸ , ìŠ¤ì›¨í„°, í›„ë“œí‹°, ì¬í‚·, ì½”íŠ¸")
logger.info("   - í•˜ì˜: ë°”ì§€, ì²­ë°”ì§€, ë°˜ë°”ì§€, ìŠ¤ì»¤íŠ¸")
logger.info("   - ì „ì‹ : ì›í”¼ìŠ¤")
logger.info("   - ì•¡ì„¸ì„œë¦¬: ì‹ ë°œ, ë¶€ì¸ , ìš´ë™í™”, ê°€ë°©, ëª¨ì, ì•ˆê²½, ìŠ¤ì¹´í”„, ë²¨íŠ¸")

logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - SAM: {SAM_AVAILABLE}")
logger.info(f"   - SciPy: {SCIPY_AVAILABLE}")
logger.info(f"   - Scikit-image: {SKIMAGE_AVAILABLE}")

logger.info("ğŸš€ Central Hub DI Container v7.0 ì—°ë™:")
logger.info("   â€¢ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
logger.info("   â€¢ ì˜ì¡´ì„± ì£¼ì… ìë™í™”")
logger.info("   â€¢ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   â€¢ 50% ì½”ë“œ ë‹¨ì¶• ë‹¬ì„±")
logger.info("   â€¢ ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ë³µì›")

logger.info("ğŸ“Š ëª©í‘œ ì„±ê³¼:")
logger.info("   ğŸ¯ ì½”ë“œ ë¼ì¸ ìˆ˜: 2000ì¤„ â†’ 1000ì¤„ (50% ë‹¨ì¶•)")
logger.info("   ğŸ”§ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("   âš¡ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
logger.info("   ğŸ§  ì‹¤ì œ AI ëª¨ë¸ (DeepLabV3+, SAM, U2Net) ì™„ì „ ë™ì‘")
logger.info("   ğŸ¨ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ (20ê°œ ì¹´í…Œê³ ë¦¬)")
logger.info("   ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ (Mock ì œê±°)")

logger.info("=" * 120)
logger.info("ğŸ‰ ClothSegmentationStep Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì™„ë£Œ!")

# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    try:
        gc.collect()
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif MPS_AVAILABLE:
                    torch.mps.empty_cache()
            except Exception as mem_error:
                logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mem_error}")
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def safe_torch_operation(operation_func, *args, **kwargs):
    """ì•ˆì „í•œ PyTorch ì‘ì—… ì‹¤í–‰"""
    try:
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        return operation_func(*args, **kwargs)
    except Exception as e:
        logger.error(f"âŒ PyTorch ì‘ì—… ì‹¤íŒ¨: {e}")
        if EXCEPTIONS_AVAILABLE:
            track_exception(e, {'operation': operation_func.__name__}, 2)
        raise

# ğŸ”¥ íŒŒì¼ ë í‘œì‹œ
if __name__ == "__main__":
    logger.info("ğŸ”¥ Step 03 Cloth Segmentation ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
    logger.info(f"ğŸ“Š PyTorch ì‚¬ìš© ê°€ëŠ¥: {TORCH_AVAILABLE}")
    logger.info(f"ğŸ MPS ì‚¬ìš© ê°€ëŠ¥: {MPS_AVAILABLE}")
    logger.info(f"ğŸ–¼ï¸ PIL ì‚¬ìš© ê°€ëŠ¥: {PIL_AVAILABLE}")
    logger.info(f"ğŸ“Š NumPy ì‚¬ìš© ê°€ëŠ¥: {NUMPY_AVAILABLE}")
    logger.info(f"ğŸ”¬ SciPy ì‚¬ìš© ê°€ëŠ¥: {SCIPY_AVAILABLE}")
    logger.info(f"ğŸ”¬ Scikit-image ì‚¬ìš© ê°€ëŠ¥: {SKIMAGE_AVAILABLE}")
    logger.info(f"ğŸ¯ SAM ì‚¬ìš© ê°€ëŠ¥: {SAM_AVAILABLE}")
    logger.info(f"ğŸ¤– Torchvision ì‚¬ìš© ê°€ëŠ¥: {TORCHVISION_AVAILABLE}")
    logger.info(f"ğŸ”¥ DenseCRF ì‚¬ìš© ê°€ëŠ¥: {DENSECRF_AVAILABLE}")

# ==============================================
# ğŸ”¥ í•µì‹¬ AI ì•Œê³ ë¦¬ì¦˜ - ì‹¤ì œ ì‹ ê²½ë§ êµ¬ì¡° ì™„ì „ êµ¬í˜„
# ==============================================

class ASPPModule(nn.Module):
    """ASPP ëª¨ë“ˆ - Multi-scale context aggregation (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()
        
        # 1x1 convolution branch
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Atrous convolutions with different dilation rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])
        
        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature fusion with dropout
        total_channels = out_channels * (1 + len(atrous_rates) + 1)
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        h, w = x.shape[2:]
        
        # 1x1 convolution
        feat1 = self.conv1x1(x)
        
        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # Global average pooling with upsampling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)
        
        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # Project to final features
        return self.project(concat_feat)

class SelfCorrectionModule(nn.Module):
    """Self-Correction Learning - SCHP í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, num_classes=21, hidden_dim=256):  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
        super().__init__()
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        
        # Context aggregation network
        self.context_conv = nn.Sequential(
            nn.Conv2d(num_classes, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Multi-head self-attention mechanism
        self.self_attention = MultiHeadSelfAttention(hidden_dim, num_heads=8)
        
        # Correction prediction network
        self.correction_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Conv2d(hidden_dim // 2, hidden_dim // 4, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim // 4, num_classes, 1)
        )
        
        # Confidence estimation network
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
        # Edge refinement branch
        self.edge_branch = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, initial_parsing, features=None):
        batch_size, num_classes, h, w = initial_parsing.shape
        
        # Convert initial parsing to feature space
        parsing_feat = self.context_conv(initial_parsing)
        
        # Apply multi-head self-attention for long-range dependencies
        attended_feat = self.self_attention(parsing_feat)
        
        # Predict corrections
        correction = self.correction_conv(attended_feat)
        
        # Estimate per-pixel confidence
        confidence = self.confidence_branch(attended_feat)
        
        # Edge refinement
        edge_weights = self.edge_branch(attended_feat)
        
        # Apply corrections with confidence and edge weighting
        corrected_parsing = initial_parsing + correction * confidence * edge_weights
        
        # Residual connection for stability
        alpha = 0.3  # Learnable parameter could be added
        final_parsing = alpha * corrected_parsing + (1 - alpha) * initial_parsing
        
        return final_parsing, confidence, edge_weights

class MultiHeadSelfAttention(nn.Module):
    """Multi-Head Self-Attention for context modeling (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, in_channels, num_heads=8):
        super().__init__()
        self.in_channels = in_channels
        self.num_heads = num_heads
        self.head_dim = in_channels // num_heads
        
        assert in_channels % num_heads == 0, "in_channels must be divisible by num_heads"
        
        # Query, Key, Value projections
        self.query_conv = nn.Conv2d(in_channels, in_channels, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        # Output projection
        self.out_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(in_channels)
        
        # Learnable position encoding
        self.pos_encoding = PositionalEncoding2D(in_channels)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
        # Temperature parameter for attention scaling
        self.temperature = nn.Parameter(torch.tensor(self.head_dim ** -0.5))
        
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in [self.query_conv, self.key_conv, self.value_conv, self.out_conv]:
            nn.init.xavier_uniform_(m.weight)
            nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # Add positional encoding
        x_pos = self.pos_encoding(x)
        
        # Generate Q, K, V
        Q = self.query_conv(x_pos).view(batch_size, self.num_heads, self.head_dim, H * W)
        K = self.key_conv(x_pos).view(batch_size, self.num_heads, self.head_dim, H * W)
        V = self.value_conv(x_pos).view(batch_size, self.num_heads, self.head_dim, H * W)
        
        # Transpose for attention computation
        Q = Q.permute(0, 1, 3, 2)  # (B, heads, HW, head_dim)
        K = K.permute(0, 1, 3, 2)  # (B, heads, HW, head_dim)
        V = V.permute(0, 1, 3, 2)  # (B, heads, HW, head_dim)
        
        # Scaled dot-product attention
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.temperature
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)  # (B, heads, HW, head_dim)
        
        # Concatenate heads
        attended = attended.permute(0, 1, 3, 2).contiguous()  # (B, heads, head_dim, HW)
        attended = attended.view(batch_size, C, H, W)
        
        # Output projection
        output = self.out_conv(attended)
        
        # Residual connection with layer normalization
        output = output + x  # Residual connection
        
        # Apply layer normalization (reshape for LayerNorm)
        output_flat = output.permute(0, 2, 3, 1).contiguous().view(-1, C)
        output_norm = self.layer_norm(output_flat)
        output = output_norm.view(batch_size, H, W, C).permute(0, 3, 1, 2)
        
        return output

class PositionalEncoding2D(nn.Module):
    """2D Positional Encoding for spatial features (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, channels):
        super().__init__()
        self.channels = channels
        
        # Learnable positional embeddings
        self.pos_embed_h = nn.Parameter(torch.randn(1, channels // 2, 1, 1))
        self.pos_embed_w = nn.Parameter(torch.randn(1, channels // 2, 1, 1))
        
    def forward(self, x):
        batch_size, channels, h, w = x.shape
        
        # Generate positional encodings
        pos_h = self.pos_embed_h.expand(batch_size, -1, h, 1).expand(-1, -1, -1, w)
        pos_w = self.pos_embed_w.expand(batch_size, -1, 1, w).expand(-1, -1, h, -1)
        
        # Concatenate position encodings
        pos_encoding = torch.cat([pos_h, pos_w], dim=1)
        
        return x + pos_encoding

class DeepLabV3PlusBackbone(nn.Module):
    """DeepLabV3+ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ - ResNet-101 ê¸°ë°˜ (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, backbone='resnet101', output_stride=16, pretrained=True):
        super().__init__()
        self.output_stride = output_stride
        
        # Backbone feature extractor
        if backbone == 'resnet101':
            self.backbone = self._make_resnet101_backbone(pretrained, output_stride)
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")
        
        # Low-level feature processing
        self.low_level_conv = nn.Sequential(
            nn.Conv2d(256, 48, 1, bias=False),
            nn.BatchNorm2d(48),
            nn.ReLU(inplace=True)
        )
        
        # Feature dimensions
        self.high_level_channels = 2048
        self.low_level_channels = 48
    
    def _make_resnet101_backbone(self, pretrained, output_stride):
        """ResNet-101 ë°±ë³¸ ìƒì„±"""
        
        # Custom ResNet-101 implementation for semantic segmentation
        class ResNetBlock(nn.Module):
            def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=dilation,
                                     dilation=dilation, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                
            def forward(self, x):
                identity = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    identity = self.downsample(x)
                
                out += identity
                out = self.relu(out)
                
                return out
        
        # Build ResNet-101 layers
        layers = nn.ModuleDict({
            # Stem
            'conv1': nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            'bn1': nn.BatchNorm2d(64),
            'relu': nn.ReLU(inplace=True),
            'maxpool': nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # Layer 1 (stride=1)
            'layer1': self._make_resnet_layer(ResNetBlock, 64, 64, 3, stride=1),
            
            # Layer 2 (stride=2)
            'layer2': self._make_resnet_layer(ResNetBlock, 256, 128, 4, stride=2),
            
            # Layer 3 (stride=2 for OS=16, stride=1+dilation=2 for OS=8)
            'layer3': self._make_resnet_layer(ResNetBlock, 512, 256, 23, 
                                            stride=2 if output_stride == 16 else 1,
                                            dilation=1 if output_stride == 16 else 2),
            
            # Layer 4 (stride=1, dilation=2 for OS=16, dilation=4 for OS=8)
            'layer4': self._make_resnet_layer(ResNetBlock, 1024, 512, 3, stride=1,
                                            dilation=2 if output_stride == 16 else 4)
        })
        
        return layers
    
    def _make_resnet_layer(self, block, inplanes, planes, num_blocks, stride=1, dilation=1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, 1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )
        
        layers = []
        layers.append(block(inplanes, planes, stride, dilation, downsample))
        inplanes = planes * 4
        
        for _ in range(1, num_blocks):
            layers.append(block(inplanes, planes, dilation=dilation))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Stem
        x = self.backbone['conv1'](x)
        x = self.backbone['bn1'](x)
        x = self.backbone['relu'](x)
        x = self.backbone['maxpool'](x)
        
        # ResNet layers
        x = self.backbone['layer1'](x)
        low_level_feat = x  # Save for decoder (1/4 resolution)
        
        x = self.backbone['layer2'](x)
        x = self.backbone['layer3'](x)
        x = self.backbone['layer4'](x)  # High-level features
        
        # Process low-level features
        low_level_feat = self.low_level_conv(low_level_feat)
        
        return x, low_level_feat

class DeepLabV3PlusDecoder(nn.Module):
    """DeepLabV3+ Decoder with Progressive Refinement (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, num_classes=21, aspp_channels=256, low_level_channels=48):  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
        super().__init__()
        
        # Decoder layers
        self.decoder = nn.Sequential(
            nn.Conv2d(aspp_channels + low_level_channels, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        # Progressive refinement branches
        self.coarse_classifier = nn.Conv2d(256, num_classes, 1)
        self.fine_classifier = nn.Conv2d(256, num_classes, 1)
        
        # Feature refinement
        self.refine_conv = nn.Sequential(
            nn.Conv2d(256 + num_classes, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # Final classifier
        self.final_classifier = nn.Conv2d(64, num_classes, 1)
        
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, aspp_features, low_level_features, input_shape):
        h_input, w_input = input_shape
        
        # Upsample ASPP features to match low-level features
        aspp_upsampled = F.interpolate(aspp_features, size=low_level_features.shape[2:],
                                     mode='bilinear', align_corners=False)
        
        # Concatenate features
        concat_features = torch.cat([aspp_upsampled, low_level_features], dim=1)
        
        # Decode features
        decoded_features = self.decoder(concat_features)
        
        # Progressive refinement
        # Stage 1: Coarse prediction
        coarse_pred = self.coarse_classifier(decoded_features)
        coarse_upsampled = F.interpolate(coarse_pred, size=(h_input, w_input),
                                       mode='bilinear', align_corners=False)
        
        # Stage 2: Feature refinement with coarse prediction
        refined_input = torch.cat([decoded_features, coarse_pred], dim=1)
        refined_features = self.refine_conv(refined_input)
        
        # Stage 3: Fine prediction
        fine_pred = self.final_classifier(refined_features)
        fine_upsampled = F.interpolate(fine_pred, size=(h_input, w_input),
                                     mode='bilinear', align_corners=False)
        
        return {
            'coarse_prediction': coarse_upsampled,
            'fine_prediction': fine_upsampled,
            'final_prediction': fine_upsampled,
            'decoder_features': decoded_features
        }

class DeepLabV3PlusModel(nn.Module):
    """Complete DeepLabV3+ Model - ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™” (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, num_classes=21, backbone='resnet101', output_stride=16):  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
        super().__init__()
        self.num_classes = num_classes
        self.output_stride = output_stride
        
        # 1. DeepLabV3+ Backbone
        self.backbone = DeepLabV3PlusBackbone(backbone, output_stride)
        
        # 2. ASPP Module for multi-scale context
        self.aspp = ASPPModule(in_channels=2048, out_channels=256)
        
        # 3. Decoder with progressive refinement
        self.decoder = DeepLabV3PlusDecoder(num_classes, aspp_channels=256, low_level_channels=48)
        
        # 4. Self-Correction Module
        self.self_correction = SelfCorrectionModule(num_classes)
        
        # 5. Auxiliary classifier for training
        self.aux_classifier = nn.Sequential(
            nn.Conv2d(1024, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Conv2d(256, num_classes, 1)  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
        )
        
        # 6. Cloth-specific feature extractor
        self.cloth_feature_extractor = ClothFeatureExtractor(256)
        
        self._init_weights()
    
    def _init_weights(self):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x, return_features=False):
        input_shape = x.shape[2:]
        
        # 1. Backbone feature extraction
        high_level_feat, low_level_feat = self.backbone(x)
        
        # 2. ASPP for multi-scale context
        aspp_feat = self.aspp(high_level_feat)
        
        # 3. Decoder with progressive refinement
        decoder_outputs = self.decoder(aspp_feat, low_level_feat, input_shape)
        initial_parsing = decoder_outputs['fine_prediction']
        
        # 4. Self-correction for refinement
        corrected_parsing, confidence, edge_weights = self.self_correction(
            torch.softmax(initial_parsing, dim=1)
        )
        
        # 5. Cloth-specific features
        cloth_features = self.cloth_feature_extractor(decoder_outputs['decoder_features'])
        
        # 6. Auxiliary prediction (for training)
        aux_pred = None
        if self.training:
            aux_feat = F.interpolate(high_level_feat, scale_factor=0.5, mode='bilinear', align_corners=False)
            aux_pred = self.aux_classifier(aux_feat)
            aux_pred = F.interpolate(aux_pred, size=input_shape, mode='bilinear', align_corners=False)
        
        outputs = {
            'parsing': corrected_parsing,
            'confidence': confidence,
            'edge_weights': edge_weights,
            'initial_parsing': initial_parsing,
            'coarse_parsing': decoder_outputs['coarse_prediction'],
            'cloth_features': cloth_features,
            'aux_prediction': aux_pred
        }
        
        if return_features:
            outputs.update({
                'high_level_features': high_level_feat,
                'low_level_features': low_level_feat,
                'aspp_features': aspp_feat,
                'decoder_features': decoder_outputs['decoder_features']
            })
        
        return outputs

class ClothFeatureExtractor(nn.Module):
    """ì˜ë¥˜ íŠ¹í™” íŠ¹ì§• ì¶”ì¶œê¸° (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self, in_channels=256):
        super().__init__()
        
        # Texture feature extractor
        self.texture_branch = nn.Sequential(
            nn.Conv2d(in_channels, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Shape feature extractor
        self.shape_branch = nn.Sequential(
            nn.Conv2d(in_channels, 128, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Color feature extractor
        self.color_branch = nn.Sequential(
            nn.Conv2d(in_channels, 64, 5, padding=2, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Feature fusion
        self.fusion = nn.Sequential(
            nn.Linear(64 * 3, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, 64)
        )
    
    def forward(self, x):
        # Extract different types of features
        texture_feat = self.texture_branch(x).flatten(1)
        shape_feat = self.shape_branch(x).flatten(1)
        color_feat = self.color_branch(x).flatten(1)
        
        # Fuse features
        combined_feat = torch.cat([texture_feat, shape_feat, color_feat], dim=1)
        fused_feat = self.fusion(combined_feat)
        
        return {
            'texture_features': texture_feat,
            'shape_features': shape_feat,
            'color_features': color_feat,
            'fused_features': fused_feat
        }



# ==============================================
# ğŸ”¥ ì„¹ì…˜ 6: ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì›ë³¸ ì™„ì „ ë³µì›)
# ==============================================

class RealDeepLabV3PlusModel(nn.Module):
    """ì‹¤ì œ DeepLabV3+ ëª¨ë¸ (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™”) - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        super().__init__()  # nn.Module ì´ˆê¸°í™”
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
        self.num_classes = 21  # ğŸ”¥ 21ê°œ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •
        
        # Preprocessing transforms
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # Post-processing parameters
        self.cloth_category_mapping = {
            0: 'background', 1: 'shirt', 2: 't_shirt', 3: 'sweater', 4: 'hoodie',
            5: 'jacket', 6: 'coat', 7: 'dress', 8: 'skirt', 9: 'pants',
            10: 'jeans', 11: 'shorts', 12: 'shoes', 13: 'boots', 14: 'sneakers',
            15: 'bag', 16: 'hat', 17: 'glasses', 18: 'scarf', 19: 'belt', 20: 'accessory'
        }
    
    def load(self) -> bool:
        """DeepLabV3+ ëª¨ë¸ ë¡œë“œ - ì™„ì „ êµ¬í˜„"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # DeepLabV3+ ëª¨ë¸ ìƒì„±
            self.model = DeepLabV3PlusModel(num_classes=self.num_classes)
            
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
            if os.path.exists(self.model_path):
                try:
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    import gc
                    gc.collect()
                    
                    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
                    try:
                        checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                    except:
                        checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                    
                    # ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
                    if isinstance(checkpoint, dict):
                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                        elif 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                        else:
                            state_dict = checkpoint
                    else:
                        state_dict = checkpoint
                    
                    # í‚¤ ì´ë¦„ ë§¤í•‘ (DeepLabV3+ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                    new_state_dict = {}
                    for k, v in state_dict.items():
                        # 1. module. ì ‘ë‘ì‚¬ ì œê±°
                        new_key = k.replace('module.', '') if k.startswith('module.') else k
                        
                        # 2. DeepLabV3+ íŠ¹í™” í‚¤ ë§¤í•‘
                        if new_key.startswith('backbone.backbone.'):
                            # backbone.backbone.conv1.weight -> backbone.conv1.weight
                            new_key = new_key.replace('backbone.backbone.', 'backbone.')
                        elif new_key.startswith('classifier.'):
                            # classifier.0.convs.0.0.weight -> aux_classifier.0.convs.0.0.weight
                            new_key = new_key.replace('classifier.', 'aux_classifier.')
                        elif new_key.startswith('decoder.'):
                            # decoder í‚¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                            pass
                        elif new_key.startswith('aspp.'):
                            # aspp í‚¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                            pass
                        
                        new_state_dict[new_key] = v
                    
                    # MPS í˜¸í™˜ì„± ë° ë©”ëª¨ë¦¬ ì•ˆì „ì„±
                    if self.device == "mps":
                        for key, value in new_state_dict.items():
                            if isinstance(value, torch.Tensor):
                                if value.dtype == torch.float64:
                                    new_state_dict[key] = value.float()
                                # ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•œ ë³µì‚¬
                                new_state_dict[key] = value.clone().detach()
                    
                    # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                    missing_keys, unexpected_keys = self.model.load_state_dict(new_state_dict, strict=False)
                    if missing_keys:
                        logger.warning(f"Missing keys in checkpoint: {missing_keys[:5]}...")
                    if unexpected_keys:
                        logger.warning(f"Unexpected keys in checkpoint: {unexpected_keys[:5]}...")
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del checkpoint, state_dict, new_state_dict
                    gc.collect()
                    
                except Exception as e:
                    logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    # í´ë°±: ëœë¤ ì´ˆê¸°í™”
                    def init_weights(m):
                        if isinstance(m, torch.nn.Conv2d):
                            torch.nn.init.kaiming_normal_(m.weight)
                        elif isinstance(m, torch.nn.BatchNorm2d):
                            torch.nn.init.constant_(m.weight, 1)
                            torch.nn.init.constant_(m.bias, 0)
                    self.model.apply(init_weights)
                    logger.warning("âš ï¸ ëœë¤ ì´ˆê¸°í™”ë¡œ í´ë°±")
            else:
                logger.warning(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ, ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©: {self.model_path}")
        
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ë° í‰ê°€ ëª¨ë“œ
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            
            logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def forward(self, x, y=None, z=None):
        """PyTorch nn.Module í‘œì¤€ forward ë©”ì„œë“œ"""
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()
        if y is not None and isinstance(y, torch.Tensor):
            y = y.detach().cpu().numpy()
        if z is not None and isinstance(z, torch.Tensor):
            z = z.detach().cpu().numpy()
        
        # predict ë©”ì„œë“œ í˜¸ì¶œ
        return self.predict(x)

    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """DeepLabV3+ ì˜ˆì¸¡ ì‹¤í–‰ - ì™„ì „ êµ¬í˜„"""
        try:
            if not self.is_loaded:
                return {"masks": {}, "confidence": 0.0}
            
            # ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # RGB ì´ë¯¸ì§€
                    input_tensor = self.transform(image).unsqueeze(0).to(self.device)
                else:
                    logger.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
                    return {"masks": {}, "confidence": 0.0}
            else:
                logger.error("ì´ë¯¸ì§€ëŠ” numpy arrayì—¬ì•¼ í•¨")
                return {"masks": {}, "confidence": 0.0}
            
            # ì‹¤ì œ DeepLabV3+ AI ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(input_tensor)
                
            # ê²°ê³¼ ì¶”ì¶œ ë° í›„ì²˜ë¦¬
            parsing = outputs['parsing']
            confidence_map = outputs['confidence']
            edge_weights = outputs['edge_weights']
            cloth_features = outputs['cloth_features']
            
            # Softmax ì ìš© ë° argmaxë¡œ í´ë˜ìŠ¤ ì˜ˆì¸¡
            parsing_probs = torch.softmax(parsing, dim=1)
            parsing_argmax = torch.argmax(parsing_probs, dim=1)
            
            # NumPy ë³€í™˜
            parsing_np = parsing_argmax.squeeze().cpu().numpy()
            confidence_np = confidence_map.squeeze().cpu().numpy()
            edge_np = edge_weights.squeeze().cpu().numpy()
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_size = image.shape[:2]
            parsing_pil = Image.fromarray(parsing_np.astype(np.uint8))
            parsing_resized = np.array(parsing_pil.resize((original_size[1], original_size[0]), 
                                                        Image.Resampling.NEAREST))
            
            confidence_pil = Image.fromarray((confidence_np * 255).astype(np.uint8))
            confidence_resized = np.array(confidence_pil.resize((original_size[1], original_size[0]), 
                                                              Image.Resampling.BILINEAR))
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            masks = self._create_category_masks(parsing_resized, confidence_resized, edge_np)
            
            # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            overall_confidence = float(np.mean(confidence_np))
            
            # ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ
            cloth_feat_dict = self._extract_cloth_features_from_tensor(cloth_features)
            
            return {
                "masks": masks,
                "confidence": overall_confidence,
                "parsing_map": parsing_resized,
                "confidence_map": confidence_resized,
                "edge_map": edge_np,
                "categories_detected": list(np.unique(parsing_resized)),
                "cloth_features": cloth_feat_dict,
                "model_outputs": {
                    "initial_parsing": outputs['initial_parsing'].cpu(),
                    "coarse_parsing": outputs['coarse_parsing'].cpu() if outputs['coarse_parsing'] is not None else None
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}
    
    def _create_category_masks(self, parsing_map: np.ndarray, confidence_map: np.ndarray, 
                             edge_map: np.ndarray) -> Dict[str, np.ndarray]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„± - ì™„ì „ êµ¬í˜„"""
        masks = {}
        
        # ìƒì˜ ì¹´í…Œê³ ë¦¬ (1-6)
        upper_categories = [1, 2, 3, 4, 5, 6]  # shirt, t_shirt, sweater, hoodie, jacket, coat
        upper_mask = np.isin(parsing_map, upper_categories).astype(np.uint8) * 255
        masks['upper_body'] = self._refine_mask_with_confidence(upper_mask, confidence_map, edge_map)
        
        # í•˜ì˜ ì¹´í…Œê³ ë¦¬ (9-11)
        lower_categories = [9, 10, 11, 8]  # pants, jeans, shorts, skirt
        lower_mask = np.isin(parsing_map, lower_categories).astype(np.uint8) * 255
        masks['lower_body'] = self._refine_mask_with_confidence(lower_mask, confidence_map, edge_map)
        
        # ì „ì‹  ì¹´í…Œê³ ë¦¬ (7)
        dress_categories = [7]  # dress
        full_body_mask = np.isin(parsing_map, dress_categories).astype(np.uint8) * 255
        masks['full_body'] = self._refine_mask_with_confidence(full_body_mask, confidence_map, edge_map)
        
        # ì•¡ì„¸ì„œë¦¬ ì¹´í…Œê³ ë¦¬ (12-19)
        accessory_categories = [12, 13, 14, 15, 16, 17, 18, 19]  # shoes, boots, sneakers, bag, hat, glasses, scarf, belt
        accessory_mask = np.isin(parsing_map, accessory_categories).astype(np.uint8) * 255
        masks['accessories'] = self._refine_mask_with_confidence(accessory_mask, confidence_map, edge_map)
        
        # ì „ì²´ ì˜ë¥˜ ë§ˆìŠ¤í¬
        all_categories = upper_categories + lower_categories + dress_categories + accessory_categories
        all_cloth_mask = np.isin(parsing_map, all_categories).astype(np.uint8) * 255
        masks['all_clothes'] = self._refine_mask_with_confidence(all_cloth_mask, confidence_map, edge_map)
        
        return masks

    def _refine_mask_with_confidence(self, mask: np.ndarray, confidence_map: np.ndarray, 
                                   edge_map: np.ndarray) -> np.ndarray:
        """ì‹ ë¢°ë„ì™€ ì—£ì§€ ì •ë³´ë¥¼ ì´ìš©í•œ ë§ˆìŠ¤í¬ ì •ì œ"""
        try:
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
            confidence_threshold = 0.5
            confidence_normalized = confidence_map.astype(np.float32) / 255.0
            
            # ì—£ì§€ ê°€ì¤‘ì¹˜ ì ìš©
            if len(edge_map.shape) == 2:
                edge_normalized = edge_map.astype(np.float32)
            else:
                edge_normalized = np.ones_like(mask, dtype=np.float32)
            
            # ë§ˆìŠ¤í¬ ì •ì œ
            refined_mask = mask.astype(np.float32) / 255.0
            refined_mask = refined_mask * confidence_normalized * edge_normalized
            
            # ì„ê³„ê°’ ì ìš© í›„ ì´ì§„í™”
            refined_mask = (refined_mask > confidence_threshold).astype(np.uint8) * 255
            
            # í˜•íƒœí•™ì  ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            if SCIPY_AVAILABLE:
                structure = np.ones((3, 3))
                refined_mask = ndimage.binary_opening(refined_mask > 128, structure=structure).astype(np.uint8) * 255
                refined_mask = ndimage.binary_closing(refined_mask > 128, structure=structure).astype(np.uint8) * 255
            
            return refined_mask
            
        except Exception as e:
            logger.warning(f"ë§ˆìŠ¤í¬ ì •ì œ ì‹¤íŒ¨: {e}")
            return mask
    
    def _extract_cloth_features_from_tensor(self, cloth_features: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """í…ì„œì—ì„œ ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            features = {}
            
            for feat_name, feat_tensor in cloth_features.items():
                if isinstance(feat_tensor, torch.Tensor):
                    feat_np = feat_tensor.squeeze().cpu().numpy()
                    if feat_np.ndim == 1:  # Feature vector
                        features[feat_name] = feat_np.tolist()
                    else:  # Feature map
                        features[feat_name] = {
                            'shape': feat_np.shape,
                            'mean': float(np.mean(feat_np)),
                            'std': float(np.std(feat_np)),
                            'max': float(np.max(feat_np)),
                            'min': float(np.min(feat_np))
                        }
            
            return features
            
        except Exception as e:
            logger.warning(f"ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

class RealU2NetClothModel(nn.Module):
    """ì‹¤ì œ U2Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        super().__init__()  # nn.Module ì´ˆê¸°í™”
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
        
        # U2Net ì „ìš© ì „ì²˜ë¦¬ (ì•ˆì •ì„± ìµœì í™”)
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((512, 512)),  # U2Net ì•ˆì • í¬ê¸°
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # U2Net ì…ë ¥ í¬ê¸° ì„¤ì •
        self.input_size = (512, 512)  # U2Net ì•ˆì • í¬ê¸°
        
    def load(self) -> bool:
        """U2Net ëª¨ë¸ ë¡œë“œ - ì™„ì „ êµ¬í˜„ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)"""
        try:
            logger.info("ğŸ”„ U2Net ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            
            if not TORCH_AVAILABLE:
                logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                import psutil
                memory_usage = psutil.virtual_memory().percent
                logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 90:
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - U2Net ë¡œë”© ê±´ë„ˆëœ€")
                    return False
            except ImportError:
                logger.info("âš ï¸ psutil ì—†ìŒ - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ê±´ë„ˆëœ€")
            
            # U2Net ì•„í‚¤í…ì²˜ ìƒì„±
            logger.info("ğŸ”„ U2Net ì•„í‚¤í…ì²˜ ìƒì„± ì¤‘...")
            self.model = self._create_u2net_architecture()
            logger.info("âœ… U2Net ì•„í‚¤í…ì²˜ ìƒì„± ì™„ë£Œ")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ì•ˆì „ ëª¨ë“œ)
            logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘: {self.model_path}")
            if os.path.exists(self.model_path):
                logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸")
                try:
                    logger.info("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (weights_only=True)...")
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                    logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ (weights_only=True)")
                except Exception as e:
                    logger.warning(f"âš ï¸ weights_only=True ì‹¤íŒ¨: {e}")
                    logger.info("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (weights_only=False)...")
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                    logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ (weights_only=False)")
                
                # ğŸ”¥ ê²€ì¦ëœ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ì²˜ë¦¬
                logger.info("ğŸ”„ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ ì¤‘...")
                if isinstance(checkpoint, dict):
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                        logger.info("âœ… state_dictì—ì„œ ì¶”ì¶œ")
                    elif 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                        logger.info("âœ… model_state_dictì—ì„œ ì¶”ì¶œ")
                    elif 'params_ema' in checkpoint:
                        # RealESRGAN ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” EMA íŒŒë¼ë¯¸í„°
                        state_dict = checkpoint['params_ema']
                        logger.info("âœ… params_emaì—ì„œ ì¶”ì¶œ")
                    else:
                        state_dict = checkpoint
                        logger.info("âœ… ì „ì²´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ì¶œ")
                else:
                    state_dict = checkpoint
                    logger.info("âœ… ì „ì²´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ì¶œ")
                
                # ğŸ”¥ ê²€ì¦ëœ U2Net ì•„í‚¤í…ì²˜ ì •ë³´ ì ìš©
                # U2Net: U-Net ê¸°ë°˜ ì•„í‚¤í…ì²˜ (RSU ë¸”ë¡ë“¤)
                # - RSU7, RSU6, RSU5, RSU4, RSU4F ë¸”ë¡ë“¤
                # - ê° ìŠ¤í…Œì´ì§€ë³„ side output
                # - ìµœì¢… fusion layer
                
                # MPS í˜¸í™˜ì„± ë° ë©”ëª¨ë¦¬ ì•ˆì „ì„±
                if self.device == "mps":
                    logger.info("ğŸ M3 Max í™˜ê²½ - MPS í˜¸í™˜ì„± ì²˜ë¦¬ ì¤‘...")
                    for key, value in state_dict.items():
                        if isinstance(value, torch.Tensor):
                            if value.dtype == torch.float64:
                                state_dict[key] = value.float()
                            state_dict[key] = value.clone().detach()  # ë©”ëª¨ë¦¬ ì•ˆì „ì„±
                    logger.info("âœ… MPS í˜¸í™˜ì„± ì²˜ë¦¬ ì™„ë£Œ")
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                logger.info("ğŸ”„ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘...")
                self.model.load_state_dict(state_dict, strict=False)
                logger.info("âœ… ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del checkpoint, state_dict
                gc.collect()
                logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            else:
                # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ëœë¤ ì´ˆê¸°í™”
                logger.warning(f"âš ï¸ U2Net ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.model_path} - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
                def init_weights(m):
                    if isinstance(m, torch.nn.Conv2d):
                        torch.nn.init.kaiming_normal_(m.weight)
                    elif isinstance(m, torch.nn.BatchNorm2d):
                        torch.nn.init.constant_(m.weight, 1)
                        torch.nn.init.constant_(m.bias, 0)
                self.model.apply(init_weights)
                logger.info("âœ… ëœë¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            logger.info(f"ğŸ”„ ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ì¤‘: {self.device}")
            self.model.to(self.device)
            logger.info("âœ… ë””ë°”ì´ìŠ¤ ì´ë™ ì™„ë£Œ")
            
            logger.info("ğŸ”„ ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • ì¤‘...")
            self.model.eval()
            logger.info("âœ… í‰ê°€ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            
            self.is_loaded = True
            
            logger.info(f"âœ… U2Net ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            return False

    def _create_u2net_architecture(self):
        """U2Net ì•„í‚¤í…ì²˜ ìƒì„± - ì™„ì „ êµ¬í˜„"""
        
        class ConvBNReLU(nn.Module):
            def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, dilation=1):
                super().__init__()
                self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, dilation, bias=False)
                self.bn = nn.BatchNorm2d(out_ch)
                self.relu = nn.ReLU(inplace=True)
                
            def forward(self, x):
                return self.relu(self.bn(self.conv(x)))
        
        class RSU7(nn.Module):
            """Residual U-block with 7 layers"""
            def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
                super().__init__()
                
                self.rebnconvin = ConvBNReLU(in_ch, out_ch, 1, 1, 0)
                
                self.rebnconv1 = ConvBNReLU(out_ch, mid_ch, 1, 1, 0)
                self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv2 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv3 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv4 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv5 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv6 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                
                self.rebnconv7 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                
                self.rebnconv6d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv5d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv4d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv3d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv2d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv1d = ConvBNReLU(mid_ch*2, out_ch, 1, 1, 0)
            
            def forward(self, x):
                hx = x
                hxin = self.rebnconvin(hx)
                
                hx1 = self.rebnconv1(hxin)
                hx = self.pool1(hx1)
                
                hx2 = self.rebnconv2(hx)
                hx = self.pool2(hx2)
                
                hx3 = self.rebnconv3(hx)
                hx = self.pool3(hx3)
                
                hx4 = self.rebnconv4(hx)
                hx = self.pool4(hx4)
                
                hx5 = self.rebnconv5(hx)
                hx = self.pool5(hx5)
                
                hx6 = self.rebnconv6(hx)
                hx7 = self.rebnconv7(hx6)
                
                hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
                hx6dup = F.interpolate(hx6d, size=(hx5.size(2), hx5.size(3)), mode='bilinear', align_corners=False)
                
                hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
                hx5dup = F.interpolate(hx5d, size=(hx4.size(2), hx4.size(3)), mode='bilinear', align_corners=False)
                
                hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
                hx4dup = F.interpolate(hx4d, size=(hx3.size(2), hx3.size(3)), mode='bilinear', align_corners=False)
                
                hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
                hx3dup = F.interpolate(hx3d, size=(hx2.size(2), hx2.size(3)), mode='bilinear', align_corners=False)
                
                hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
                hx2dup = F.interpolate(hx2d, size=(hx1.size(2), hx1.size(3)), mode='bilinear', align_corners=False)
                
                hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
                
                return hx1d + hxin
        
        class RSU6(nn.Module):
            """Residual U-block with 6 layers"""
            def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
                super().__init__()
                
                self.rebnconvin = ConvBNReLU(in_ch, out_ch, 1, 1, 0)
                
                self.rebnconv1 = ConvBNReLU(out_ch, mid_ch, 1, 1, 0)
                self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv2 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv3 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv4 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.rebnconv5 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                
                self.rebnconv6 = ConvBNReLU(mid_ch, mid_ch, 3, 1, 1)
                
                self.rebnconv5d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv4d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv3d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv2d = ConvBNReLU(mid_ch*2, mid_ch, 3, 1, 1)
                self.rebnconv1d = ConvBNReLU(mid_ch*2, out_ch, 1, 1, 0)
                
            def forward(self, x):
                hx = x
                hxin = self.rebnconvin(hx)
                
                hx1 = self.rebnconv1(hxin)
                hx = self.pool1(hx1)
                
                hx2 = self.rebnconv2(hx)
                hx = self.pool2(hx2)
                
                hx3 = self.rebnconv3(hx)
                hx = self.pool3(hx3)
                
                hx4 = self.rebnconv4(hx)
                hx = self.pool4(hx4)
                
                hx5 = self.rebnconv5(hx)
                hx6 = self.rebnconv6(hx5)
                
                hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
                hx5dup = F.interpolate(hx5d, size=(hx4.size(2), hx4.size(3)), mode='bilinear', align_corners=False)
                
                hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
                hx4dup = F.interpolate(hx4d, size=(hx3.size(2), hx3.size(3)), mode='bilinear', align_corners=False)
                
                hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
                hx3dup = F.interpolate(hx3d, size=(hx2.size(2), hx2.size(3)), mode='bilinear', align_corners=False)
                
                hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
                hx2dup = F.interpolate(hx2d, size=(hx1.size(2), hx1.size(3)), mode='bilinear', align_corners=False)
                
                hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
                
                return hx1d + hxin
        
        class U2NET(nn.Module):
            """Complete U2NET model"""
            def __init__(self, in_ch=3, out_ch=1):
                super().__init__()
                
                self.stage1 = RSU7(in_ch, 32, 64)
                self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.stage2 = RSU6(64, 32, 128)
                self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.stage3 = RSU6(128, 64, 256)
                self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.stage4 = RSU6(256, 128, 512)
                self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.stage5 = RSU6(512, 256, 512)
                self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
                
                self.stage6 = RSU6(512, 256, 512)
                
                # Decoder
                self.stage5d = RSU6(1024, 256, 512)
                self.stage4d = RSU6(1024, 128, 256)
                self.stage3d = RSU6(512, 64, 128)
                self.stage2d = RSU6(256, 32, 64)
                self.stage1d = RSU7(128, 16, 64)
                
                # Side outputs
                self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
                self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
                self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
                self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
                self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
                self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
                
                # Output fusion
                self.outconv = nn.Conv2d(6, out_ch, 1)
                
            def forward(self, x):
                try:
                    hx = x
                    
                    # Encoder
                    hx1 = self.stage1(hx)
                    hx = self.pool12(hx1)
                    
                    hx2 = self.stage2(hx)
                    hx = self.pool23(hx2)
                    
                    hx3 = self.stage3(hx)
                    hx = self.pool34(hx3)
                    
                    hx4 = self.stage4(hx)
                    hx = self.pool45(hx4)
                    
                    hx5 = self.stage5(hx)
                    hx = self.pool56(hx5)
                    
                    hx6 = self.stage6(hx)
                    hx6up = F.interpolate(hx6, size=(hx5.size(2), hx5.size(3)), mode='bilinear', align_corners=False)
                    
                    # Decoder
                    hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
                    hx5dup = F.interpolate(hx5d, size=(hx4.size(2), hx4.size(3)), mode='bilinear', align_corners=False)
                    
                    hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
                    hx4dup = F.interpolate(hx4d, size=(hx3.size(2), hx3.size(3)), mode='bilinear', align_corners=False)
                    
                    hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
                    hx3dup = F.interpolate(hx3d, size=(hx2.size(2), hx2.size(3)), mode='bilinear', align_corners=False)
                    
                    hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
                    hx2dup = F.interpolate(hx2d, size=(hx1.size(2), hx1.size(3)), mode='bilinear', align_corners=False)
                    
                    hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
                    
                    # Side outputs
                    side1 = self.side1(hx1d)
                    side2 = self.side2(hx2d)
                    side2 = F.interpolate(side2, size=(hx.size(2), hx.size(3)), mode='bilinear', align_corners=False)
                    
                    side3 = self.side3(hx3d)
                    side3 = F.interpolate(side3, size=(hx.size(2), hx.size(3)), mode='bilinear', align_corners=False)
                    
                    side4 = self.side4(hx4d)
                    side4 = F.interpolate(side4, size=(hx.size(2), hx.size(3)), mode='bilinear', align_corners=False)
                    
                    side5 = self.side5(hx5d)
                    side5 = F.interpolate(side5, size=(hx.size(2), hx.size(3)), mode='bilinear', align_corners=False)
                    
                    side6 = self.side6(hx6)
                    side6 = F.interpolate(side6, size=(hx.size(2), hx.size(3)), mode='bilinear', align_corners=False)
                    
                    # Output fusion
                    d0 = self.outconv(torch.cat((side1, side2, side3, side4, side5, side6), 1))
                    
                    return torch.sigmoid(d0), torch.sigmoid(side1), torch.sigmoid(side2), torch.sigmoid(side3), torch.sigmoid(side4), torch.sigmoid(side5), torch.sigmoid(side6)
                    
                except RuntimeError as e:
                    if "Sizes of tensors must match" in str(e):
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net í…ì„œ í¬ê¸° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜: {e}")
                        # í´ë°±: ê°„ë‹¨í•œ ë§ˆìŠ¤í¬ ìƒì„±
                        batch_size = x.size(0)
                        h, w = x.size(2), x.size(3)
                        dummy_mask = torch.zeros(batch_size, 1, h, w, device=x.device)
                        return (dummy_mask, dummy_mask, dummy_mask, dummy_mask, dummy_mask, dummy_mask, dummy_mask)
                    else:
                        raise e
        
        return U2NET(in_ch=3, out_ch=1)
    
    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """U2Net ì˜ˆì¸¡ ì‹¤í–‰ - ì™„ì „ êµ¬í˜„ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net predict ì‹œì‘")
            if not self.is_loaded:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {"masks": {}, "confidence": 0.0}
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ë¡œë“œë¨")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                import psutil
                memory_usage = psutil.virtual_memory().percent
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 85:  # 128GB í™˜ê²½ì—ì„œëŠ” ë” ê´€ëŒ€í•œ ì„ê³„ê°’
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - U2Net ì˜ˆì¸¡ ê±´ë„ˆëœ€")
                    return {"masks": {}, "confidence": 0.0}
            except ImportError:
                pass
            
            # ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘")
            if isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # RGB ì´ë¯¸ì§€
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
                    input_tensor = self.transform(image).unsqueeze(0).to(self.device)
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì…ë ¥ í…ì„œ í¬ê¸°: {input_tensor.shape}")
                else:
                    logger.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image.shape}")
                    return {"masks": {}, "confidence": 0.0}
            else:
                logger.error("ì´ë¯¸ì§€ëŠ” numpy arrayì—¬ì•¼ í•¨")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì´ë¯¸ì§€ íƒ€ì… ì˜¤ë¥˜: {type(image)}")
                return {"masks": {}, "confidence": 0.0}
            
            # ì‹¤ì œ U2Net AI ì¶”ë¡  (ì•ˆì „ ëª¨ë“œ)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                with torch.no_grad():
                    outputs = self.model(input_tensor)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
                    
                # U2Net ì¶œë ¥ ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì¶œë ¥ ì²˜ë¦¬ ì‹œì‘")
                    if isinstance(outputs, tuple) and len(outputs) == 7:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] U2Net ì¶œë ¥ íŠœí”Œ í™•ì¸: {len(outputs)}ê°œ")
                        d0, side1, side2, side3, side4, side5, side6 = outputs
                        
                        # ë©”ì¸ ë§ˆìŠ¤í¬ (d0) ì‚¬ìš©
                        main_mask = d0.squeeze().cpu().numpy()
                        
                        # ì‹ ë¢°ë„ ê³„ì‚°
                        confidence = self._calculate_u2net_confidence(outputs, main_mask)
                        
                        # ë§ˆìŠ¤í¬ ì •ì œ
                        refined_mask = self._refine_u2net_mask((main_mask * 255).astype(np.uint8))
                        
                        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                        original_size = image.shape[:2]
                        mask_pil = Image.fromarray(refined_mask)
                        mask_resized = np.array(mask_pil.resize((original_size[1], original_size[0]), 
                                                              Image.Resampling.BILINEAR))
                        
                        # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
                        masks = self._create_u2net_category_masks(mask_resized, image)
                        
                        return {
                            "masks": masks,
                            "confidence": confidence,
                            "main_mask": mask_resized,
                            "model_outputs": {
                                "d0": d0.cpu(),
                                "side1": side1.cpu(),
                                "side2": side2.cpu(),
                                "side3": side3.cpu(),
                                "side4": side4.cpu(),
                                "side5": side5.cpu(),
                                "side6": side6.cpu()
                            }
                        }
                    else:
                        logger.error("U2Net ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        return {"masks": {}, "confidence": 0.0}
                        
                except Exception as output_error:
                    logger.error(f"âŒ U2Net ì¶œë ¥ ì²˜ë¦¬ ì‹¤íŒ¨: {output_error}")
                    # í´ë°±: ê°„ë‹¨í•œ ë§ˆìŠ¤í¬ ìƒì„±
                    try:
                        # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„±
                        h, w = image.shape[:2]
                        fallback_mask = np.ones((h, w), dtype=np.uint8) * 128
                        
                        masks = {
                            "main": fallback_mask,
                            "clothing": fallback_mask,
                            "background": np.zeros((h, w), dtype=np.uint8)
                        }
                        
                        return {
                            "masks": masks,
                            "confidence": 0.5,
                            "main_mask": fallback_mask,
                            "model_outputs": {}
                        }
                    except Exception as fallback_error:
                        logger.error(f"âŒ U2Net í´ë°± ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {fallback_error}")
                        return {"masks": {}, "confidence": 0.0}
                    
            except Exception as predict_error:
                logger.error(f"âŒ U2Net ì˜ˆì¸¡ ì‹¤íŒ¨: {predict_error}")
                return {"masks": {}, "confidence": 0.0}
            
        except Exception as e:
            logger.error(f"âŒ U2Net ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}
    
    def _refine_u2net_mask(self, mask: np.ndarray) -> np.ndarray:
        """U2Net ë§ˆìŠ¤í¬ ì •ì œ"""
        try:
            # Gaussian blur for smoothing
            if SCIPY_AVAILABLE:
                mask_float = mask.astype(np.float32) / 255.0
                mask_smoothed = ndimage.gaussian_filter(mask_float, sigma=1.0)
                mask_smoothed = (mask_smoothed * 255).astype(np.uint8)
            else:
                mask_smoothed = mask
            
            # Morphological operations
            if SCIPY_AVAILABLE:
                # Opening to remove noise
                structure = np.ones((3, 3))
                mask_opened = ndimage.binary_opening(mask_smoothed > 128, structure=structure)
                
                # Closing to fill holes
                mask_closed = ndimage.binary_closing(mask_opened, structure=np.ones((5, 5)))
                
                mask_refined = (mask_closed * 255).astype(np.uint8)
            else:
                mask_refined = mask_smoothed
            
            return mask_refined
            
        except Exception as e:
            logger.warning(f"U2Net ë§ˆìŠ¤í¬ ì •ì œ ì‹¤íŒ¨: {e}")
            return mask
    
    def _create_u2net_category_masks(self, mask: np.ndarray, image: np.ndarray) -> Dict[str, np.ndarray]:
        """U2Net ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„± (íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)"""
        masks = {}
        
        # ì „ì²´ ì˜ë¥˜ ë§ˆìŠ¤í¬
        masks['all_clothes'] = mask
        
        # ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•œ ì˜ì—­ ë¶„í• 
        height, width = mask.shape
        
        # ìƒì˜ ì˜ì—­ (ìƒë‹¨ 60%)
        upper_region = np.zeros_like(mask)
        upper_region[:int(height * 0.6), :] = mask[:int(height * 0.6), :]
        masks['upper_body'] = upper_region
        
        # í•˜ì˜ ì˜ì—­ (í•˜ë‹¨ 60%)
        lower_region = np.zeros_like(mask)
        lower_region[int(height * 0.4):, :] = mask[int(height * 0.4):, :]
        masks['lower_body'] = lower_region
        
        # ì „ì‹  (ì „ì²´ ë§ˆìŠ¤í¬ì™€ ë™ì¼, ì›í”¼ìŠ¤ ë“±)
        masks['full_body'] = mask
        
        # ì•¡ì„¸ì„œë¦¬ (ê²½ê³„ ì˜ì—­)
        if SCIPY_AVAILABLE:
            # ê²½ê³„ì„  ê²€ì¶œ
            edges = ndimage.sobel(mask.astype(np.float32))
            edge_mask = (edges > 10).astype(np.uint8) * 255
            
            # ì‘ì€ ì—°ê²° ìš”ì†Œë“¤ì„ ì•¡ì„¸ì„œë¦¬ë¡œ ë¶„ë¥˜
            if SKIMAGE_AVAILABLE:
                labeled = measure.label(mask > 128)
                regions = measure.regionprops(labeled)
                
                accessory_mask = np.zeros_like(mask)
                main_area_threshold = mask.size * 0.1  # ì „ì²´ ë©´ì ì˜ 10% ì´í•˜
                
                for region in regions:
                    if region.area < main_area_threshold:
                        accessory_mask[labeled == region.label] = 255
                
                masks['accessories'] = accessory_mask
            else:
                masks['accessories'] = np.zeros_like(mask)
        else:
            masks['accessories'] = np.zeros_like(mask)
        
        return masks
    
    def _calculate_u2net_confidence(self, outputs: Tuple[torch.Tensor, ...], main_mask: np.ndarray) -> float:
        """U2Net ì‹ ë¢°ë„ ê³„ì‚° (side outputs ì¼ê´€ì„± ê¸°ë°˜)"""
        try:
            if len(outputs) < 2:
                return 0.5
            
            main_output = outputs[0].squeeze().cpu().numpy()
            side_outputs = [side.squeeze().cpu().numpy() for side in outputs[1:]]
            
            # Resize all outputs to same size for comparison
            target_size = main_output.shape
            resized_sides = []
            
            for side_output in side_outputs:
                if side_output.shape != target_size:
                    side_pil = Image.fromarray((side_output * 255).astype(np.uint8))
                    side_resized = np.array(side_pil.resize((target_size[1], target_size[0]), 
                                                          Image.Resampling.BILINEAR)) / 255.0
                else:
                    side_resized = side_output
                resized_sides.append(side_resized)
            
            # Calculate consistency between main and side outputs
            consistencies = []
            for side_output in resized_sides:
                # IoU-like metric
                intersection = np.logical_and(main_output > 0.5, side_output > 0.5).sum()
                union = np.logical_or(main_output > 0.5, side_output > 0.5).sum()
                
                if union > 0:
                    consistency = intersection / union
                else:
                    consistency = 1.0  # Perfect consistency if both are empty
                
                consistencies.append(consistency)
            
            # Average consistency as confidence
            confidence = np.mean(consistencies) if consistencies else 0.5
            
            return float(confidence)
            
        except Exception as e:
            logger.warning(f"U2Net ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

class RealSAMModel(nn.Module):
    """ì‹¤ì œ SAM AI ëª¨ë¸ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        super().__init__()  # nn.Module ì´ˆê¸°í™”
        self.model_path = model_path
        # M3 Max í™˜ê²½ì—ì„œëŠ” ê°•ì œë¡œ CPU ì‚¬ìš©
        if IS_M3_MAX:
            self.device = "cpu"
            print(f"ğŸ”¥ [ë””ë²„ê¹…] M3 Max í™˜ê²½ - SAMì„ CPUì—ì„œ ì‹¤í–‰")
        else:
            self.device = device
        self.model = None
        self.predictor = None
        self.is_loaded = False
        
        # SAM ì „ìš© ì„¤ì •
        self.image_encoder = None
        self.prompt_encoder = None
        self.mask_decoder = None
        
    def load(self) -> bool:
        """SAM ëª¨ë¸ ë¡œë“œ - ì™„ì „ êµ¬í˜„ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM load() ì‹œì‘")
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                return False
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                import psutil
                memory_usage = psutil.virtual_memory().percent
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 90:
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - SAM ë¡œë”© ê±´ë„ˆëœ€")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ì„œ SAM ë¡œë”© ê±´ë„ˆëœ€")
                    return False
            except ImportError:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] psutil ì—†ìŒ - ë©”ëª¨ë¦¬ í™•ì¸ ê±´ë„ˆëœ€")
                pass
            
            # M3 Max í™˜ê²½ì—ì„œ ì¶”ê°€ ì•ˆì „ì¥ì¹˜
            if IS_M3_MAX:
                logger.info("ğŸ M3 Max í™˜ê²½ì—ì„œ SAM ë¡œë”© - ì•ˆì „ ëª¨ë“œ í™œì„±í™”")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] M3 Max í™˜ê²½ - ì•ˆì „ ëª¨ë“œ í™œì„±í™”")
                
                # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
                try:
                    if TORCH_AVAILABLE and MPS_AVAILABLE:
                        if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                            torch.mps.set_per_process_memory_fraction(0.5)  # 50%ë¡œ ì œí•œ
                            print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •: 50%")
                except:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] MPS ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • ì‹¤íŒ¨")
                    pass
                
                # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                if TORCH_AVAILABLE:
                    try:
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        elif MPS_AVAILABLE:
                            torch.mps.empty_cache()
                    except:
                        pass
                import time
                time.sleep(1)  # ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ëŒ€ê¸°
                print(f"ğŸ”¥ [ë””ë²„ê¹…] M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # SAM ëª¨ë¸ ë¹Œë“œ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¹Œë“œ ì‹œì‘")
                from segment_anything import build_sam_vit_h, SamPredictor
                print(f"ğŸ”¥ [ë””ë²„ê¹…] segment_anything import ì™„ë£Œ")
                
                # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not os.path.exists(self.model_path):
                    logger.error(f"âŒ SAM ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {self.model_path}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {self.model_path}")
                    return False
                
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸: {self.model_path}")
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(self.model_path) / (1024 * 1024)  # MB
                logger.info(f"ğŸ“ SAM ëª¨ë¸ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                
                if file_size < 100:  # 100MB ë¯¸ë§Œì´ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼
                    logger.warning(f"âš ï¸ SAM ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {file_size:.1f}MB")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ: {file_size:.1f}MB")
                    return False
                
                # ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©
                logger.info("ğŸ”„ SAM ëª¨ë¸ ë¹Œë“œ ì‹œì‘...")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] build_sam_vit_h í˜¸ì¶œ ì‹œì‘")
                self.model = build_sam_vit_h(checkpoint=self.model_path)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] build_sam_vit_h ì™„ë£Œ")
                
                # ë””ë°”ì´ìŠ¤ ì´ë™ ì „ ì¶”ê°€ ê²€ì¦
                if self.model is None:
                    logger.error("âŒ SAM ëª¨ë¸ ë¹Œë“œ ì‹¤íŒ¨ - ëª¨ë¸ì´ None")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¹Œë“œ ì‹¤íŒ¨ - ëª¨ë¸ì´ None")
                    return False
                
                logger.info("ğŸ”„ SAM ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ì¤‘...")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™: {self.device}")
                self.model.to(self.device)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì´ë™ ì™„ë£Œ")
                
                # Predictor ìƒì„±
                logger.info("ğŸ”„ SAM Predictor ìƒì„± ì¤‘...")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SamPredictor ìƒì„± ì‹œì‘")
                self.predictor = SamPredictor(self.model)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SamPredictor ìƒì„± ì™„ë£Œ")
                
                # ì„œë¸Œ ëª¨ë“ˆ ì°¸ì¡°
                self.image_encoder = self.model.image_encoder
                self.prompt_encoder = self.model.prompt_encoder  
                self.mask_decoder = self.model.mask_decoder
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì„œë¸Œ ëª¨ë“ˆ ì°¸ì¡° ì™„ë£Œ")
                
                self.is_loaded = True
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
                logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
                return True
                
            except Exception as model_error:
                logger.error(f"âŒ SAM ëª¨ë¸ ë¹Œë“œ ì‹¤íŒ¨: {model_error}")
                logger.error(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(model_error).__name__}")
                if EXCEPTIONS_AVAILABLE:
                    track_exception(model_error, {'model': 'sam', 'operation': 'build'}, 2)
                import traceback
                logger.error(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¹Œë“œ ì‹¤íŒ¨: {model_error}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(model_error).__name__}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            if EXCEPTIONS_AVAILABLE:
                track_exception(e, {'model': 'sam', 'operation': 'load'}, 2)
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            return False
    
    def forward(self, x, y=None, z=None):
        """PyTorch nn.Module í‘œì¤€ forward ë©”ì„œë“œ"""
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()
        if y is not None and isinstance(y, torch.Tensor):
            y = y.detach().cpu().numpy()
        if z is not None and isinstance(z, torch.Tensor):
            z = z.detach().cpu().numpy()
        
        # predict ë©”ì„œë“œ í˜¸ì¶œ
        return self.predict(x)
    
    def predict(self, image: np.ndarray, prompts: Dict[str, Any] = None) -> Dict[str, Any]:
        """SAM ì˜ˆì¸¡ ì‹¤í–‰ - ì™„ì „ êµ¬í˜„ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)"""
        try:
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM predict ì‹œì‘")
            if not self.is_loaded:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {"masks": {}, "confidence": 0.0}
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ëª¨ë¸ ë¡œë“œë¨")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                import psutil
                memory_usage = psutil.virtual_memory().percent
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}%")
                if memory_usage > 95:
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤: {memory_usage}% - SAM ì˜ˆì¸¡ ê±´ë„ˆëœ€")
                    return {"masks": {}, "confidence": 0.0}
            except ImportError:
                pass
            
            # ì´ë¯¸ì§€ ì¸ì½”ë”© (ì•ˆì „ ëª¨ë“œ)
            try:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹œì‘")
                
                # M3 Max í™˜ê²½ì—ì„œ ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
                if IS_M3_MAX:
                    h, w = image.shape[:2]
                    if h > 1024 or w > 1024:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] M3 Max í™˜ê²½ - ì´ë¯¸ì§€ í¬ê¸° ì œí•œ: {w}x{h} -> 1024x1024")
                        import cv2
                        image = cv2.resize(image, (1024, 1024))
                
                # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬
                import gc
                gc.collect()
                
                self.predictor.set_image(image)
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ SAM ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                return {"masks": {}, "confidence": 0.0}
            
            # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
            if prompts is None:
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±")
                prompts = self._generate_default_prompts(image)
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {list(prompts.keys())}")
            
            # ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ì˜ˆì¸¡ (ì•ˆì „ ëª¨ë“œ)
            all_masks = []
            all_scores = []
            all_logits = []
            
            try:
                # Point prompts
                if 'points' in prompts and 'labels' in prompts:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Point í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œì‘")
                    point_coords = np.array(prompts['points'])
                    point_labels = np.array(prompts['labels'])
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Point í”„ë¡¬í”„íŠ¸: {len(point_coords)}ê°œ í¬ì¸íŠ¸")
                    
                    masks, scores, logits = self.predictor.predict(
                        point_coords=point_coords,
                        point_labels=point_labels,
                        multimask_output=True
                    )
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Point í”„ë¡¬í”„íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(masks)}ê°œ ë§ˆìŠ¤í¬")
                    
                    all_masks.extend(masks)
                    all_scores.extend(scores)
                    all_logits.extend(logits)
                
                # Box prompts
                if 'boxes' in prompts:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Box í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œì‘")
                    for i, box in enumerate(prompts['boxes']):
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Box {i+1} ì²˜ë¦¬")
                        box_array = np.array(box)
                        masks, scores, logits = self.predictor.predict(
                            box=box_array,
                            multimask_output=True
                        )
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Box {i+1} ì˜ˆì¸¡ ì™„ë£Œ: {len(masks)}ê°œ ë§ˆìŠ¤í¬")
                        
                        all_masks.extend(masks)
                        all_scores.extend(scores)
                        all_logits.extend(logits)
                
                # ìë™ ë§ˆìŠ¤í¬ ìƒì„± (í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš°)
                if not all_masks:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ìë™ ë§ˆìŠ¤í¬ ìƒì„± ì‹œì‘")
                    all_masks, all_scores = self._generate_automatic_masks(image)
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ìë™ ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ: {len(all_masks)}ê°œ ë§ˆìŠ¤í¬")
                
            except Exception as predict_error:
                logger.error(f"âŒ SAM ì˜ˆì¸¡ ì‹¤íŒ¨: {predict_error}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM ì˜ˆì¸¡ ì‹¤íŒ¨: {predict_error}")
                return {"masks": {}, "confidence": 0.0}
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì´ {len(all_masks)}ê°œ ë§ˆìŠ¤í¬ ìƒì„±ë¨")
            
            # ìµœê³  í’ˆì§ˆ ë§ˆìŠ¤í¬ë“¤ ì„ íƒ
            selected_masks, selected_scores = self._select_best_masks(all_masks, all_scores)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] {len(selected_masks)}ê°œ ë§ˆìŠ¤í¬ ì„ íƒë¨")
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            category_masks = self._create_sam_category_masks(selected_masks, selected_scores, image)
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ: {len(category_masks)}ê°œ ì¹´í…Œê³ ë¦¬")
            
            # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            overall_confidence = float(np.mean(selected_scores)) if selected_scores else 0.0
            print(f"ğŸ”¥ [ë””ë²„ê¹…] ì „ì²´ ì‹ ë¢°ë„: {overall_confidence}")
            
            print(f"ğŸ”¥ [ë””ë²„ê¹…] SAM predict ì™„ë£Œ")
            return {
                "masks": category_masks,
                "confidence": overall_confidence,
                "all_masks": all_masks,
                "all_scores": all_scores,
                "selected_masks": selected_masks,
                "selected_scores": selected_scores
            }
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}
    
    def _generate_default_prompts(self, image: np.ndarray) -> Dict[str, Any]:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        h, w = image.shape[:2]
        
        # Grid-based points
        grid_points = []
        grid_labels = []
        
        # 3x3 ê·¸ë¦¬ë“œ
        for i in range(3):
            for j in range(3):
                x = int(w * (j + 1) / 4)
                y = int(h * (i + 1) / 4)
                grid_points.append([x, y])
                grid_labels.append(1)  # Positive prompt
        
        # ì¤‘ì•™ ì¤‘ì‹¬ì˜ ì¶”ê°€ í¬ì¸íŠ¸ë“¤
        center_points = [
            [w // 2, h // 2],           # Center
            [w // 3, h // 2],           # Left
            [2 * w // 3, h // 2],       # Right
            [w // 2, h // 3],           # Top
            [w // 2, 2 * h // 3],       # Bottom
        ]
        
        center_labels = [1] * len(center_points)
        
        return {
            'points': grid_points + center_points,
            'labels': grid_labels + center_labels
        }
    
    def _generate_automatic_masks(self, image: np.ndarray) -> Tuple[List[np.ndarray], List[float]]:
        """ìë™ ë§ˆìŠ¤í¬ ìƒì„± (SAMì˜ everything mode ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            masks = []
            scores = []
            
            h, w = image.shape[:2]
            
            # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê·¸ë¦¬ë“œë¡œ ìë™ í¬ì¸íŠ¸ ìƒì„±
            for grid_size in [2, 3, 4]:
                for i in range(grid_size):
                    for j in range(grid_size):
                        x = int(w * (j + 0.5) / grid_size)
                        y = int(h * (i + 0.5) / grid_size)
                        
                        try:
                            pred_masks, pred_scores, _ = self.predictor.predict(
                                point_coords=np.array([[x, y]]),
                                point_labels=np.array([1]),
                                multimask_output=False
                            )
                            
                            if len(pred_masks) > 0:
                                masks.append(pred_masks[0])
                                scores.append(pred_scores[0])
                                
                        except Exception as e:
                            logger.debug(f"ìë™ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨ ({x}, {y}): {e}")
                            continue
            
            return masks, scores
            
        except Exception as e:
            logger.warning(f"ìë™ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return [], []
    
    def _select_best_masks(self, masks: List[np.ndarray], scores: List[float], 
                          max_masks: int = 5) -> Tuple[List[np.ndarray], List[float]]:
        """ìµœê³  í’ˆì§ˆ ë§ˆìŠ¤í¬ ì„ íƒ"""
        try:
            if not masks:
                return [], []
            
            # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
            indexed_scores = [(i, score) for i, score in enumerate(scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ ë§ˆìŠ¤í¬ë“¤ ì„ íƒ
            selected_masks = []
            selected_scores = []
            
            for i, score in indexed_scores[:max_masks]:
                mask = masks[i]
                
                # ë§ˆìŠ¤í¬ í’ˆì§ˆ ê²€ì¦
                if self._validate_mask_quality(mask):
                    selected_masks.append(mask)
                    selected_scores.append(score)
            
            return selected_masks, selected_scores
            
        except Exception as e:
            logger.warning(f"ë§ˆìŠ¤í¬ ì„ íƒ ì‹¤íŒ¨: {e}")
            return masks[:max_masks], scores[:max_masks]
    
    def _validate_mask_quality(self, mask: np.ndarray) -> bool:
        """ë§ˆìŠ¤í¬ í’ˆì§ˆ ê²€ì¦"""
        try:
            # ìµœì†Œ ë©´ì  ì²´í¬
            mask_area = np.sum(mask > 0)
            total_area = mask.size
            area_ratio = mask_area / total_area
            
            if area_ratio < 0.01 or area_ratio > 0.8:  # ë„ˆë¬´ ì‘ê±°ë‚˜ í° ë§ˆìŠ¤í¬ ì œì™¸
                return False
            
            # ì—°ê²°ì„± ì²´í¬ (SKIMAGE_AVAILABLEì¸ ê²½ìš°)
            if SKIMAGE_AVAILABLE:
                labeled = measure.label(mask)
                num_components = labeled.max()
                
                if num_components > 5:  # ë„ˆë¬´ ë§ì€ ë¶„ë¦¬ëœ ì˜ì—­
                    return False
            
            return True
            
        except Exception:
            return True  # ê²€ì¦ ì‹¤íŒ¨ì‹œ í—ˆìš©
    
    def _create_sam_category_masks(self, masks: List[np.ndarray], scores: List[float], 
                                  image: np.ndarray) -> Dict[str, np.ndarray]:
        """SAM ë§ˆìŠ¤í¬ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        try:
            if not masks:
                h, w = image.shape[:2]
                empty_mask = np.zeros((h, w), dtype=np.uint8)
                return {
                    'all_clothes': empty_mask,
                    'upper_body': empty_mask,
                    'lower_body': empty_mask,
                    'full_body': empty_mask,
                    'accessories': empty_mask
                }
            
            # ë§ˆìŠ¤í¬ í†µí•©
            combined_mask = np.zeros_like(masks[0], dtype=np.float32)
            
            for mask, score in zip(masks, scores):
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë§ˆìŠ¤í¬ í†µí•©
                combined_mask += mask.astype(np.float32) * score
            
            # ì •ê·œí™” ë° ì´ì§„í™”
            if len(masks) > 0:
                combined_mask /= np.sum(scores)
            combined_mask = (combined_mask > 0.5).astype(np.uint8) * 255
            
            # ìœ„ì¹˜ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„í• 
            h, w = combined_mask.shape
            
            # ìƒì˜ (ìƒë‹¨ 60%)
            upper_mask = np.zeros_like(combined_mask)
            upper_mask[:int(h * 0.6), :] = combined_mask[:int(h * 0.6), :]
            
            # í•˜ì˜ (í•˜ë‹¨ 60%, ì¢Œìš° ì—¬ë°± ì œì™¸)
            lower_mask = np.zeros_like(combined_mask)
            margin = int(w * 0.1)
            lower_mask[int(h * 0.4):, margin:w-margin] = combined_mask[int(h * 0.4):, margin:w-margin]
            
            # ì•¡ì„¸ì„œë¦¬ (ê°€ì¥ìë¦¬ ì˜ì—­ì˜ ì‘ì€ ë§ˆìŠ¤í¬ë“¤)
            accessory_mask = np.zeros_like(combined_mask)
            
            if SKIMAGE_AVAILABLE:
                labeled = measure.label(combined_mask > 128)
                regions = measure.regionprops(labeled)
                
                main_area_threshold = combined_mask.size * 0.05
                edge_threshold = min(h, w) * 0.1
                
                for region in regions:
                    # ì‘ì€ ì˜ì—­ì´ê±°ë‚˜ ê°€ì¥ìë¦¬ì— ìˆëŠ” ì˜ì—­
                    if (region.area < main_area_threshold or 
                        region.centroid[0] < edge_threshold or 
                        region.centroid[0] > h - edge_threshold or
                        region.centroid[1] < edge_threshold or 
                        region.centroid[1] > w - edge_threshold):
                        
                        accessory_mask[labeled == region.label] = 255
                        # ì•¡ì„¸ì„œë¦¬ë¡œ ë¶„ë¥˜ëœ ì˜ì—­ì€ ë‹¤ë¥¸ ë§ˆìŠ¤í¬ì—ì„œ ì œê±°
                        upper_mask[labeled == region.label] = 0
                        lower_mask[labeled == region.label] = 0
            
            return {
                'all_clothes': combined_mask,
                'upper_body': upper_mask,
                'lower_body': lower_mask,
                'full_body': combined_mask,  # SAMì€ ì „ì²´ ê°ì²´ë¥¼ ë¶„í• í•˜ë¯€ë¡œ ë™ì¼
                'accessories': accessory_mask
            }
            
        except Exception as e:
            logger.warning(f"SAM ì¹´í…Œê³ ë¦¬ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2] if len(image.shape) >= 2 else (512, 512)
            empty_mask = np.zeros((h, w), dtype=np.uint8)
            return {
                'all_clothes': empty_mask,
                'upper_body': empty_mask,
                'lower_body': empty_mask,
                'full_body': empty_mask,
                'accessories': empty_mask
            }
