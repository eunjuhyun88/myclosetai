# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Clothing Segmentation) - ì™„ì „í•œ ì‹¤ì œ AI ì—°ì‚° ë²„ì „
===============================================================================================

âœ… í´ë°± ì™„ì „ ì œê±° - ModelLoader ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë°˜í™˜, ì‹œë®¬ë ˆì´ì…˜ ì—†ìŒ
âœ… ì‹¤ì œ AIë§Œ ì‚¬ìš© - 100% ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ ëª¨ë¸ë§Œ
âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ - logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°
âœ… í•œë°©í–¥ ë°ì´í„° íë¦„ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€ - ì‹œê°í™”, í›„ì²˜ë¦¬, í†µê³„ ë“± ê¸°ì¡´ ê¸°ëŠ¥ ëˆ„ë½ ì—†ìŒ
âœ… MRO ì˜¤ë¥˜ ì—†ìŒ - ë‹¨ìˆœ ìƒì† êµ¬ì¡° (ClothSegmentationStep â†’ BaseStepMixin)
âœ… strict_mode=True - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨, ê°€ì§œ ë°ì´í„° ìƒì„± ì—†ìŒ
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›

ì²˜ë¦¬ íë¦„ (100% ì‹¤ì œ AI):
ğŸŒ API ìš”ì²­ â†’ ğŸ“‹ PipelineManager â†’ ğŸ¯ ClothSegmentationStep ìƒì„±
â†“
ğŸ”— ModelLoader.create_step_interface() â† ModelLoaderë§Œ ë‹´ë‹¹
â”œâ”€ StepModelInterface ìƒì„±
â”œâ”€ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡
â””â”€ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íƒì§€ ë° ë¡œë“œ
â†“  
ğŸš€ ClothSegmentationStep.initialize() â† Step + ModelLoader í˜‘ì—…
â”œâ”€ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ONLY â† ModelLoaderê°€ ì‹¤ì œ ë¡œë“œ (í´ë°± ì—†ìŒ)
â”œâ”€ ëª¨ë¸ ê²€ì¦ ë° ë””ë°”ì´ìŠ¤ ì´ë™ â† Step ì²˜ë¦¬
â””â”€ M3 Max ìµœì í™” ì ìš© â† Step ì ìš©
â†“
ğŸ§  ì‹¤ì œ AI ì¶”ë¡  process() â† Stepì´ AI ì¶”ë¡  ì£¼ë„
â”œâ”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â† Step ì²˜ë¦¬
â”œâ”€ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (U2Net, RemBG ë“±) â† ModelLoaderê°€ ì œê³µí•œ ëª¨ë¸ë¡œ Stepì´ ì¶”ë¡ 
â”œâ”€ í›„ì²˜ë¦¬ ë° ì‹œê°í™” â† Step ì²˜ë¦¬
â””â”€ í’ˆì§ˆ í‰ê°€ â† Step ì²˜ë¦¬
â†“
ğŸ“¤ ê²°ê³¼ ë°˜í™˜ â† Stepì´ ìµœì¢… ê²°ê³¼ ìƒì„± (ì‹¤ì œ AI ê²°ê³¼ë§Œ)

Author: MyCloset AI Team
Date: 2025-07-21
Version: v7.0 (Strict AI Only)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import gc
import hashlib
import json
import base64
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import weakref

# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms


# ê° íŒŒì¼ì— ì¶”ê°€í•  ê°œì„ ëœ ì½”ë“œ
try:
    from app.core.gpu_config import safe_mps_empty_cache
except ImportError:
    def safe_mps_empty_cache():
        import gc
        gc.collect()
        return {"success": True, "method": "fallback_gc"}
# ì•ˆì „í•œ OpenCV import (ëª¨ë“  Step íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€)
import os
import logging

# OpenCV ì•ˆì „ import (M3 Max + conda í™˜ê²½ ê³ ë ¤)
OPENCV_AVAILABLE = False
try:
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (iconv ì˜¤ë¥˜ í•´ê²°)
    os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'  # OpenEXR ë¹„í™œì„±í™”
    os.environ['OPENCV_IO_ENABLE_JASPER'] = '0'   # Jasper ë¹„í™œì„±í™”
    
    import cv2
    OPENCV_AVAILABLE = True
    logging.getLogger(__name__).info(f"âœ… OpenCV {cv2.__version__} ë¡œë“œ ì„±ê³µ")
    
except ImportError as e:
    logging.getLogger(__name__).warning(f"âš ï¸ OpenCV import ì‹¤íŒ¨: {e}")
    logging.getLogger(__name__).warning("ğŸ’¡ í•´ê²° ë°©ë²•: conda install opencv -c conda-forge")
    
    # OpenCV í´ë°± í´ë˜ìŠ¤
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            try:
                from PIL import Image
                if hasattr(img, 'shape'):  # numpy array
                    pil_img = Image.fromarray(img)
                    resized = pil_img.resize(size)
                    return np.array(resized)
                return img
            except:
                return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:  # BGR<->RGB
                    return img[:, :, ::-1]
            return img
        
        def imread(self, path):
            try:
                from PIL import Image
                import numpy as np
                img = Image.open(path)
                return np.array(img)
            except:
                return None
        
        def imwrite(self, path, img):
            try:
                from PIL import Image
                if hasattr(img, 'shape'):
                    Image.fromarray(img).save(path)
                    return True
            except:
                pass
            return False
    
    cv2 = OpenCVFallback()

except Exception as e:
    logging.getLogger(__name__).error(f"âŒ OpenCV ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ìµœí›„ í´ë°±
    class MinimalOpenCV:
        def __getattr__(self, name):
            def dummy_func(*args, **kwargs):
                logging.getLogger(__name__).warning(f"OpenCV {name} í˜¸ì¶œë¨ - í´ë°± ëª¨ë“œ")
                return None
            return dummy_func
    
    cv2 = MinimalOpenCV()
    OPENCV_AVAILABLE = False

# ì„ íƒì  AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
except ImportError:
    REMBG_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import segment_anything as sam
    SAM_AVAILABLE = True
except ImportError:
    SAM_AVAILABLE = False

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# ==============================================
# ğŸ”¥ í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡° - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
# ==============================================

# 1. BaseStepMixin ì—°ë™ (í•„ìˆ˜)
try:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
except ImportError:
    # âŒ í´ë°± ì œê±° - BaseStepMixin ì—†ìœ¼ë©´ ì—ëŸ¬
    raise ImportError(
        "âŒ BaseStepMixinì´ í•„ìš”í•©ë‹ˆë‹¤. "
        "app.ai_pipeline.steps.base_step_mixin ëª¨ë“ˆì„ í™•ì¸í•˜ì„¸ìš”."
    )

# 2. ModelLoader ì—°ë™ (í•„ìˆ˜)
try:
    from app.ai_pipeline.utils.model_loader import ModelLoader, get_global_model_loader
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    # âŒ í´ë°± ì œê±° - ModelLoader ì—†ìœ¼ë©´ ì—ëŸ¬
    raise ImportError(
        "âŒ ModelLoaderê°€ í•„ìš”í•©ë‹ˆë‹¤. "
        "app.ai_pipeline.utils.model_loader ëª¨ë“ˆì„ í™•ì¸í•˜ì„¸ìš”."
    )

# 3. Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì—°ë™ (í•„ìˆ˜)
try:
    from app.ai_pipeline.utils.step_model_requests import get_step_request, StepModelRequestAnalyzer
    STEP_REQUESTS_AVAILABLE = True
except ImportError:
    # âŒ í´ë°± ì œê±° - Step ìš”ì²­ì‚¬í•­ ì—†ìœ¼ë©´ ì—ëŸ¬
    raise ImportError(
        "âŒ StepModelRequestAnalyzerê°€ í•„ìš”í•©ë‹ˆë‹¤. "
        "app.ai_pipeline.utils.step_model_requests ëª¨ë“ˆì„ í™•ì¸í•˜ì„¸ìš”."
    )

# 4. ì„ íƒì  ìœ í‹¸ë¦¬í‹° ì—°ë™ (ì—†ì–´ë„ ì‘ë™, ê²½ê³ ë§Œ)
try:
    from app.ai_pipeline.utils.memory_manager import MemoryManager, get_global_memory_manager
    MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    MEMORY_MANAGER_AVAILABLE = False
    logging.warning("âš ï¸ MemoryManager ì—†ìŒ - ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‚¬ìš©")

try:
    from app.ai_pipeline.utils.data_converter import DataConverter, get_global_data_converter
    DATA_CONVERTER_AVAILABLE = True
except ImportError:
    DATA_CONVERTER_AVAILABLE = False
    logging.warning("âš ï¸ DataConverter ì—†ìŒ - ê¸°ë³¸ ë°ì´í„° ë³€í™˜ ì‚¬ìš©")

# ğŸ”¥ ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    U2NET = "u2net"
    REMBG = "rembg" 
    SAM = "sam"
    DEEP_LAB = "deeplab"
    MASK_RCNN = "mask_rcnn"
    TRADITIONAL = "traditional"
    HYBRID = "hybrid"
    AUTO = "auto"

class ClothingType(Enum):
    """ì˜ë¥˜ íƒ€ì…"""
    SHIRT = "shirt"
    DRESS = "dress"
    PANTS = "pants"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class SegmentationConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.AUTO
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    enable_visualization: bool = True
    enable_post_processing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    iou_threshold: float = 0.5
    edge_smoothing: bool = True
    remove_noise: bool = True
    visualization_quality: str = "high"
    enable_caching: bool = True
    cache_size: int = 100
    show_masks: bool = True
    show_boundaries: bool = True
    overlay_opacity: float = 0.6
    strict_mode: bool = True  # ğŸ”¥ ìƒˆë¡œìš´ strict ëª¨ë“œ

@dataclass
class SegmentationResult:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼"""
    success: bool
    mask: Optional[np.ndarray] = None
    segmented_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    quality_score: float = 0.0
    method_used: str = "unknown"
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    # ì‹œê°í™” ì´ë¯¸ì§€ë“¤
    visualization_image: Optional[Image.Image] = None
    overlay_image: Optional[Image.Image] = None
    mask_image: Optional[Image.Image] = None
    boundary_image: Optional[Image.Image] = None

# ==============================================
# ğŸ”¥ ì˜ë¥˜ë³„ ìƒ‰ìƒ ë§¤í•‘ (ì‹œê°í™”ìš©)
# ==============================================

CLOTHING_COLORS = {
    'shirt': (255, 100, 100),      # ë¹¨ê°•
    'pants': (100, 100, 255),      # íŒŒë‘
    'dress': (255, 100, 255),      # ë¶„í™
    'jacket': (100, 255, 100),     # ì´ˆë¡
    'skirt': (255, 255, 100),      # ë…¸ë‘
    'sweater': (138, 43, 226),     # ë¸”ë£¨ë°”ì´ì˜¬ë ›
    'coat': (165, 42, 42),         # ê°ˆìƒ‰
    'top': (0, 255, 255),          # ì‹œì•ˆ
    'bottom': (255, 165, 0),       # ì˜¤ë Œì§€
    'shoes': (255, 150, 0),        # ì£¼í™©
    'bag': (150, 75, 0),           # ê°ˆìƒ‰
    'hat': (128, 0, 128),          # ë³´ë¼
    'accessory': (0, 255, 255),    # ì‹œì•ˆ
    'unknown': (128, 128, 128),    # íšŒìƒ‰
}

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì›ë³¸ ìœ ì§€ - í´ë°±ìš© ì™„ì „ êµ¬í˜„)
# ==============================================

class REBNCONV(nn.Module):
    """U2-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ (í´ë°±ìš©)"""
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)
    
    def forward(self, x):
        hx = self.relu_s1(self.bn_s1(self.conv_s1(x)))
        return hx

class RSU7(nn.Module):
    """U2-Net RSU-7 ë¸”ë¡ (í´ë°±ìš©)"""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample6 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample5 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
    
    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = self.upsample6(hx6d)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = self.upsample5(hx5d)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = self.upsample4(hx4d)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = self.upsample3(hx3d)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = self.upsample2(hx2d)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class U2NET(nn.Module):
    """U2-Net ë©”ì¸ ëª¨ë¸ (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìµœì í™”, í´ë°±ìš©)"""
    def __init__(self, in_ch=3, out_ch=1):
        super(U2NET, self).__init__()
        
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU7(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU7(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU7(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU7(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU7(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU7(1024, 256, 512)
        self.stage4d = RSU7(1024, 128, 256)
        self.stage3d = RSU7(512, 64, 128)
        self.stage2d = RSU7(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx5d = self.stage5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # Side outputs
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=d1.shape[2:], mode='bilinear', align_corners=False)
        
        # ìµœì¢… ì¶œë ¥
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)

# ==============================================
# ğŸ”¥ ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤ (ì‹¤ì œ AIë§Œ)
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - ì‹¤ì œ AI ì—°ì‚°ë§Œ
    
    âœ… í´ë°± ì™„ì „ ì œê±° - ModelLoader ì‹¤íŒ¨ ì‹œ ì—ëŸ¬
    âœ… ì‹¤ì œ AIë§Œ ì‚¬ìš© - 100% ModelLoader ì˜ì¡´
    âœ… strict_mode=True - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
    âœ… ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€ - ì‹œê°í™”, í›„ì²˜ë¦¬ ë“±
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], SegmentationConfig]] = None,
        **kwargs
    ):
        """
        ğŸ”¥ ìƒì„±ì - strict ëª¨ë“œ
        """
        
        # ===== 1. ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” =====
        super().__init__(device=device, config=config, **kwargs)
        
        # ===== 2. Step íŠ¹í™” ì†ì„± ì„¤ì • =====
        self.step_name = "ClothSegmentationStep"
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.device = device or self._auto_detect_device()
        
        # ===== 3. ì„¤ì • ì²˜ë¦¬ =====
        if isinstance(config, dict):
            self.segmentation_config = SegmentationConfig(**config)
        elif isinstance(config, SegmentationConfig):
            self.segmentation_config = config
        else:
            self.segmentation_config = SegmentationConfig()
        
        # strict_mode ê°•ì œ í™œì„±í™”
        self.segmentation_config.strict_mode = True
        
        # ===== 4. ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” =====
        self.is_initialized = False
        self.models_loaded = {}
        self.available_methods = []
        self.model_interface = None
        self.rembg_sessions = {}
        
        # ===== 5. M3 Max ê°ì§€ ë° ìµœì í™” =====
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = kwargs.get('memory_gb', 128.0 if self.is_m3_max else 16.0)
        
        # ===== 6. í†µê³„ ë° ìºì‹œ ì´ˆê¸°í™” =====
        self.processing_stats = {
            'total_processed': 0,
            'successful_segmentations': 0,
            'failed_segmentations': 0,
            'average_time': 0.0,
            'average_quality': 0.0,
            'method_usage': {},
            'cache_hits': 0,
            'ai_model_calls': 0  # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ ìˆ˜
        }
        
        self.segmentation_cache = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(
            max_workers=4 if self.is_m3_max else 2, 
            thread_name_prefix="cloth_seg_strict"
        )
        
        self.logger.info("âœ… ClothSegmentationStep ìƒì„± ì™„ë£Œ (Strict AI Mode)")
        self.logger.info(f"   - Device: {self.device}")
        self.logger.info(f"   - Strict Mode: {self.segmentation_config.strict_mode}")

    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ - M3 Max ìµœì í™”"""
        try:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except Exception:
            return "cpu"

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True
                )
                cpu_info = result.stdout.strip()
                return 'M3 Max' in cpu_info or 'M3' in cpu_info
        except Exception:
            pass
        return False

    # ==============================================
    # ğŸ”¥ í•µì‹¬: ì´ˆê¸°í™” ë©”ì„œë“œ (ì‹¤ì œ AIë§Œ)
    # ==============================================
    
    async def initialize(self) -> bool:
        """
        ğŸ”¥ ì´ˆê¸°í™” - ì‹¤ì œ AI ëª¨ë¸ë§Œ ë¡œë“œ (í´ë°± ì—†ìŒ)
        """
        try:
            self.logger.info("ğŸ”„ ClothSegmentationStep ì´ˆê¸°í™” ì‹œì‘ (Strict AI Mode)")
            
            # ===== 1. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (í•„ìˆ˜) =====
            await self._setup_model_interface()
            
            # ===== 2. ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ (í•„ìˆ˜) =====
            success = await self._load_real_ai_models()
            if not success:
                raise RuntimeError("âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - Strict Modeì—ì„œëŠ” í´ë°± ë¶ˆê°€")
            
            # ===== 3. RemBG ì„¸ì…˜ ì´ˆê¸°í™” =====
            if REMBG_AVAILABLE:
                await self._initialize_rembg_sessions()
            
            # ===== 4. ëª¨ë¸ ê²€ì¦ =====
            self._validate_loaded_models()
            
            # ===== 5. M3 Max ìµœì í™” ì›Œë°ì—… =====
            if self.is_m3_max:
                await self._warmup_m3_max()
            
            # ===== 6. ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” =====
            self._initialize_visualization_system()
            
            # ===== 7. ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€ =====
            self.available_methods = self._detect_available_methods()
            if not self.available_methods:
                raise RuntimeError("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ===== 8. ì´ˆê¸°í™” ì™„ë£Œ =====
            self.is_initialized = True
            self.logger.info("âœ… ClothSegmentationStep ì´ˆê¸°í™” ì™„ë£Œ (Strict AI Mode)")
            self.logger.info(f"   - ë¡œë“œëœ ëª¨ë¸: {list(self.models_loaded.keys())}")
            self.logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            raise RuntimeError(f"ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _setup_model_interface(self):
        """ğŸ”¥ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (í•„ìˆ˜)"""
        try:
            self.logger.info("ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì¤‘...")
            
            # ì „ì—­ ModelLoader ê°€ì ¸ì˜¤ê¸°
            model_loader = get_global_model_loader()
            if not model_loader:
                raise RuntimeError("âŒ ì „ì—­ ModelLoaderê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            if not hasattr(model_loader, 'create_step_interface'):
                raise RuntimeError("âŒ ModelLoaderì— create_step_interface ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            
            self.model_interface = model_loader.create_step_interface(self.step_name)
            if not self.model_interface:
                raise RuntimeError("âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            
            self.logger.info("âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def _load_real_ai_models(self) -> bool:
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ë§Œ ë¡œë“œ (í´ë°± ì—†ìŒ)"""
        try:
            if not self.model_interface:
                raise RuntimeError("âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            self.logger.info("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            
            # Step ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            step_request = StepModelRequestAnalyzer.get_step_request_info(self.step_name)
            if not step_request:
                raise RuntimeError("âŒ Step ëª¨ë¸ ìš”ì²­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            self.logger.info(f"ğŸ“‹ Step ìš”ì²­ ì •ë³´: {step_request['model_name']}")
            
            # ===== U2-Net ëª¨ë¸ ë¡œë“œ (í•„ìˆ˜) =====
            try:
                self.logger.info("ğŸ”„ U2-Net ëª¨ë¸ ë¡œë“œ ì¤‘...")
                u2net_model = await self.model_interface.get_model("cloth_segmentation_u2net")
                if not u2net_model:
                    raise RuntimeError("âŒ U2-Net ëª¨ë¸ì´ ModelLoaderì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ")
                
                # ëª¨ë¸ ê²€ì¦
                if not hasattr(u2net_model, 'forward') and not callable(u2net_model):
                    raise RuntimeError("âŒ U2-Net ëª¨ë¸ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ (forward ë©”ì„œë“œ ì—†ìŒ)")
                
                # ë””ë°”ì´ìŠ¤ ì´ë™
                if hasattr(u2net_model, 'to'):
                    u2net_model = u2net_model.to(self.device)
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(u2net_model, 'eval'):
                    u2net_model.eval()
                
                self.models_loaded['u2net'] = u2net_model
                self.logger.info("âœ… U2-Net ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.error(f"âŒ U2-Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                if self.segmentation_config.strict_mode:
                    raise RuntimeError(f"Strict Mode: U2-Net ëª¨ë¸ í•„ìˆ˜ - {e}")
            
            # ===== DeepLab ëª¨ë¸ ë¡œë“œ (ì„ íƒì ) =====
            try:
                self.logger.info("ğŸ”„ DeepLab ëª¨ë¸ ë¡œë“œ ì¤‘...")
                deeplab_model = await self.model_interface.get_model("cloth_segmentation_deeplab")
                if deeplab_model:
                    if hasattr(deeplab_model, 'to'):
                        deeplab_model = deeplab_model.to(self.device)
                    if hasattr(deeplab_model, 'eval'):
                        deeplab_model.eval()
                    
                    self.models_loaded['deeplab'] = deeplab_model
                    self.logger.info("âœ… DeepLab ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ DeepLab ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì„ íƒì ): {e}")
            
            # ===== SAM ëª¨ë¸ ë¡œë“œ (ì„ íƒì ) =====
            try:
                self.logger.info("ğŸ”„ SAM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                sam_model = await self.model_interface.get_model("cloth_segmentation_sam")
                if sam_model:
                    if hasattr(sam_model, 'to'):
                        sam_model = sam_model.to(self.device)
                    if hasattr(sam_model, 'eval'):
                        sam_model.eval()
                    
                    self.models_loaded['sam'] = sam_model
                    self.logger.info("âœ… SAM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì„ íƒì ): {e}")
            
            # ===== ë¡œë“œ ê²°ê³¼ ê²€ì¦ =====
            if not self.models_loaded:
                raise RuntimeError("âŒ ì–´ë–¤ AI ëª¨ë¸ë„ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            self.logger.info(f"ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {list(self.models_loaded.keys())}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _validate_loaded_models(self):
        """ğŸ”¥ ë¡œë“œëœ ëª¨ë¸ ê²€ì¦"""
        try:
            for model_name, model in self.models_loaded.items():
                if model is None:
                    raise RuntimeError(f"âŒ {model_name} ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                
                # ëª¨ë¸ íƒ€ì… ê²€ì¦
                if not (hasattr(model, 'forward') or callable(model)):
                    raise RuntimeError(f"âŒ {model_name} ëª¨ë¸ì´ ì¶”ë¡  ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
                
                # ë””ë°”ì´ìŠ¤ ê²€ì¦
                if hasattr(model, 'device'):
                    model_device = str(model.device)
                    if self.device not in model_device:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜: {model_device} vs {self.device}")
                
                self.logger.info(f"âœ… {model_name} ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            
            self.logger.info("âœ… ëª¨ë“  ë¡œë“œëœ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise

    async def _initialize_rembg_sessions(self):
        """RemBG ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            if not REMBG_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘...")
            
            session_configs = {
                'u2net': 'u2net',
                'u2netp': 'u2netp', 
                'silueta': 'silueta',
            }
            
            for name, model_name in session_configs.items():
                try:
                    session = new_session(model_name)
                    self.rembg_sessions[name] = session
                    self.logger.info(f"âœ… RemBG ì„¸ì…˜ ìƒì„±: {name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ {name} ìƒì„± ì‹¤íŒ¨: {e}")
            
            if self.rembg_sessions:
                self.default_rembg_session = (
                    self.rembg_sessions.get('u2net') or 
                    list(self.rembg_sessions.values())[0]
                )
                self.logger.info("âœ… RemBG ê¸°ë³¸ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _warmup_m3_max(self):
        """M3 Max ì›Œë°ì—…"""
        try:
            if not self.is_m3_max:
                return
            
            self.logger.info("ğŸ”¥ M3 Max ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ í…ì„œë¡œ ì›Œë°ì—…
            dummy_input = torch.randn(1, 3, 512, 512, device=self.device)
            
            for model_name, model in self.models_loaded.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                        with torch.no_grad():
                            if hasattr(model, 'forward'):
                                _ = model(dummy_input)
                            elif callable(model):
                                _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} M3 Max ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # MPS ìºì‹œ ì •ë¦¬
            if torch.backends.mps.is_available():
                safe_mps_empty_cache()
            
            self.logger.info("âœ… M3 Max ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    def _initialize_visualization_system(self):
        """ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.visualization_config = {
                'mask_alpha': 0.7,
                'overlay_alpha': 0.5,
                'boundary_thickness': 2,
                'color_intensity': 200
            }
            
            # í°íŠ¸ ì„¤ì •
            try:
                self.font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
            except Exception:
                try:
                    self.font = ImageFont.load_default()
                except Exception:
                    self.font = None
            
            self.logger.info("âœ… ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€ (ì‹¤ì œ AI ê¸°ë°˜)"""
        methods = []
        
        # ë¡œë“œëœ AI ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ë°©ë²• ê²°ì •
        if 'u2net' in self.models_loaded:
            methods.append(SegmentationMethod.U2NET)
            self.logger.info("âœ… U2NET ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (ì‹¤ì œ AI ëª¨ë¸)")
        
        if 'deeplab' in self.models_loaded:
            methods.append(SegmentationMethod.DEEP_LAB)
            self.logger.info("âœ… DeepLab ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (ì‹¤ì œ AI ëª¨ë¸)")
        
        if 'sam' in self.models_loaded:
            methods.append(SegmentationMethod.SAM)
            self.logger.info("âœ… SAM ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (ì‹¤ì œ AI ëª¨ë¸)")
        
        # RemBG í™•ì¸
        if REMBG_AVAILABLE and self.rembg_sessions:
            methods.append(SegmentationMethod.REMBG)
            self.logger.info("âœ… RemBG ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # Traditional ë°©ë²• (í´ë°±ì´ ì•„ë‹Œ ë³´ì¡° ë°©ë²•)
        methods.append(SegmentationMethod.TRADITIONAL)
        
        # AUTO ë°©ë²• (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
        if len([m for m in methods if m != SegmentationMethod.TRADITIONAL]) > 0:
            methods.append(SegmentationMethod.AUTO)
        
        # HYBRID ë°©ë²• (2ê°œ ì´ìƒ AI ë°©ë²•ì´ ìˆì„ ë•Œ)
        ai_methods = [m for m in methods if m not in [SegmentationMethod.TRADITIONAL, SegmentationMethod.AUTO]]
        if len(ai_methods) >= 2:
            methods.append(SegmentationMethod.HYBRID)
        
        return methods

    # ==============================================
    # ğŸ”¥ í•µì‹¬: process ë©”ì„œë“œ (ì‹¤ì œ AI ì¶”ë¡ )
    # ==============================================
    
    async def process(
        self,
        image,
        clothing_type: Optional[str] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì‹¤ì œ AI ì¶”ë¡ ë§Œ
        """
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ”„ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹œì‘ (ì‹¤ì œ AI ì¶”ë¡ )")
            
            # ===== 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ =====
            processed_image = self._preprocess_image(image)
            if processed_image is None:
                return self._create_error_result("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ===== 2. ì˜ë¥˜ íƒ€ì… ê°ì§€ =====
            detected_clothing_type = self._detect_clothing_type(processed_image, clothing_type)
            
            # ===== 3. í’ˆì§ˆ ë ˆë²¨ ì„¤ì • =====
            quality = QualityLevel(quality_level or self.segmentation_config.quality_level.value)
            
            # ===== 4. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ =====
            mask, confidence = await self._run_real_ai_segmentation(
                processed_image, detected_clothing_type, quality
            )
            
            if mask is None:
                if self.segmentation_config.strict_mode:
                    return self._create_error_result("ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨ - Strict Mode")
                else:
                    return self._create_error_result("ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨")
            
            # ===== 5. í›„ì²˜ë¦¬ =====
            final_mask = self._post_process_mask(mask, quality)
            
            # ===== 6. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± =====
            visualizations = self._create_visualizations(
                processed_image, final_mask, detected_clothing_type
            )
            
            # ===== 7. ê²°ê³¼ ìƒì„± =====
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'mask': final_mask,
                'confidence': confidence,
                'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                'processing_time': processing_time,
                'method_used': self._get_current_method(),
                'ai_models_used': list(self.models_loaded.keys()),
                'metadata': {
                    'device': self.device,
                    'quality_level': quality.value,
                    'models_used': list(self.models_loaded.keys()),
                    'image_size': processed_image.size if hasattr(processed_image, 'size') else (512, 512),
                    'strict_mode': self.segmentation_config.strict_mode,
                    'ai_inference': True  # ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  í‘œì‹œ
                }
            }
            
            # ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            if visualizations:
                if 'visualization' in visualizations:
                    result['visualization_base64'] = self._image_to_base64(visualizations['visualization'])
                if 'overlay' in visualizations:
                    result['overlay_base64'] = self._image_to_base64(visualizations['overlay'])
                if 'mask' in visualizations:
                    result['mask_base64'] = self._image_to_base64(visualizations['mask'])
                if 'boundary' in visualizations:
                    result['boundary_base64'] = self._image_to_base64(visualizations['boundary'])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(processing_time, True)
            
            self.logger.info(f"âœ… ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_processing_stats(processing_time, False)
            
            self.logger.error(f"âŒ ì‹¤ì œ AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"ì‹¤ì œ AI ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    async def _run_real_ai_segmentation(
        self,
        image: Image.Image,
        clothing_type: ClothingType,
        quality: QualityLevel
    ) -> Tuple[Optional[np.ndarray], float]:
        """
        ğŸ”¥ ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  (í´ë°± ì—†ìŒ)
        """
        try:
            # ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ì‹¤ì œ AI ë°©ë²• ì‹œë„
            methods_to_try = self._get_ai_methods_by_priority(quality)
            
            for method in methods_to_try:
                try:
                    self.logger.info(f"ğŸ§  ì‹¤ì œ AI ë°©ë²• ì‹œë„: {method.value}")
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    
                    if mask is not None:
                        # ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸
                        self.processing_stats['ai_model_calls'] += 1
                        self.processing_stats['method_usage'][method.value] = (
                            self.processing_stats['method_usage'].get(method.value, 0) + 1
                        )
                        
                        self.logger.info(f"âœ… ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {method.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return mask, confidence
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ë°©ë²• {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨
            if self.segmentation_config.strict_mode:
                raise RuntimeError("âŒ ëª¨ë“  ì‹¤ì œ AI ë°©ë²• ì‹¤íŒ¨ - Strict Modeì—ì„œëŠ” í´ë°± ë¶ˆê°€")
            
            self.logger.error("âŒ ëª¨ë“  ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤íŒ¨")
            return None, 0.0
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None, 0.0

    def _get_ai_methods_by_priority(self, quality: QualityLevel) -> List[SegmentationMethod]:
        """í’ˆì§ˆ ë ˆë²¨ë³„ ì‹¤ì œ AI ë°©ë²• ìš°ì„ ìˆœìœ„ (í´ë°± ì œì™¸)"""
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•ë§Œ í•„í„°ë§
        available_ai_methods = [
            method for method in self.available_methods 
            if method not in [SegmentationMethod.TRADITIONAL, SegmentationMethod.AUTO, SegmentationMethod.HYBRID]
        ]
        
        if quality == QualityLevel.ULTRA:
            priority = [
                SegmentationMethod.U2NET,
                SegmentationMethod.SAM,
                SegmentationMethod.DEEP_LAB,
                SegmentationMethod.REMBG
            ]
        elif quality == QualityLevel.HIGH:
            priority = [
                SegmentationMethod.U2NET,
                SegmentationMethod.REMBG,
                SegmentationMethod.DEEP_LAB,
                SegmentationMethod.SAM
            ]
        elif quality == QualityLevel.BALANCED:
            priority = [
                SegmentationMethod.REMBG,
                SegmentationMethod.U2NET,
                SegmentationMethod.DEEP_LAB
            ]
        else:  # FAST
            priority = [
                SegmentationMethod.REMBG,
                SegmentationMethod.U2NET
            ]
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ë§Œ ë°˜í™˜
        return [method for method in priority if method in available_ai_methods]

    async def _run_ai_method(
        self,
        method: SegmentationMethod,
        image: Image.Image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ê°œë³„ ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤í–‰"""
        
        if method == SegmentationMethod.U2NET:
            return await self._run_u2net_inference(image)
        elif method == SegmentationMethod.REMBG:
            return await self._run_rembg_inference(image)
        elif method == SegmentationMethod.SAM:
            return await self._run_sam_inference(image)
        elif method == SegmentationMethod.DEEP_LAB:
            return await self._run_deeplab_inference(image)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ë°©ë²•: {method}")

    async def _run_u2net_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """
        ğŸ”¥ U2-Net ì‹¤ì œ AI ì¶”ë¡ 
        """
        try:
            if 'u2net' not in self.models_loaded:
                raise RuntimeError("âŒ U2-Net ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.models_loaded['u2net']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if self.is_m3_max and self.segmentation_config.use_fp16:
                    with torch.autocast(device_type='cpu'):  # M3 MaxëŠ” CPU autocast ì‚¬ìš©
                        output = model(input_tensor)
                else:
                    output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, tuple):
                    output = output[0]  # ì²« ë²ˆì§¸ ì¶œë ¥ ì‚¬ìš©
                elif isinstance(output, list):
                    output = output[0]
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if output.max() > 1.0:
                    prob_map = torch.sigmoid(output)
                else:
                    prob_map = output
                
                mask = (prob_map > self.segmentation_config.confidence_threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy()
                confidence = float(prob_map.max().item())
            
            self.logger.info(f"âœ… U2-Net ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ U2-Net ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_rembg_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """RemBG ì‹¤ì œ AI ì¶”ë¡ """
        try:
            if not self.rembg_sessions:
                raise RuntimeError("âŒ RemBG ì„¸ì…˜ì´ ì—†ìŒ")
            
            # ìµœì  ì„¸ì…˜ ì„ íƒ
            session = (
                self.rembg_sessions.get('u2net') or
                list(self.rembg_sessions.values())[0]
            )
            
            # ğŸ”¥ ì‹¤ì œ RemBG AI ì¶”ë¡ 
            result = remove(image, session=session)
            
            # ì•ŒíŒŒ ì±„ë„ì—ì„œ ë§ˆìŠ¤í¬ ì¶”ì¶œ
            if result.mode == 'RGBA':
                mask = np.array(result)[:, :, 3]  # ì•ŒíŒŒ ì±„ë„
                mask = (mask > 128).astype(np.uint8)  # ì´ì§„í™”
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = np.sum(mask) / mask.size
                confidence = min(confidence * 2, 1.0)  # ì •ê·œí™”
                
                self.logger.info(f"âœ… RemBG ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
                return mask, confidence
            else:
                raise RuntimeError("âŒ RemBG ê²°ê³¼ì— ì•ŒíŒŒ ì±„ë„ì´ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ RemBG ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_sam_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """SAM ì‹¤ì œ AI ì¶”ë¡ """
        try:
            if 'sam' not in self.models_loaded:
                raise RuntimeError("âŒ SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.models_loaded['sam']
            
            # ğŸ”¥ ì‹¤ì œ SAM AI ì¶”ë¡  (ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
            # ì—¬ê¸°ì„œëŠ” ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
            image_array = np.array(image)
            
            # SAM ëª¨ë¸ ì¶”ë¡  (ì‹¤ì œ êµ¬í˜„ì€ ë” ë³µì¡í•¨)
            if hasattr(model, 'forward'):
                # í…ì„œ ë³€í™˜
                input_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0).float().to(self.device)
                input_tensor = input_tensor / 255.0
                
                model.eval()
                with torch.no_grad():
                    output = model(input_tensor)
                
                if isinstance(output, dict) and 'masks' in output:
                    mask = output['masks'][0].cpu().numpy()
                elif torch.is_tensor(output):
                    mask = output.squeeze().cpu().numpy()
                else:
                    raise RuntimeError("âŒ SAM ì¶œë ¥ í˜•ì‹ì„ ì•Œ ìˆ˜ ì—†ìŒ")
                
                mask = (mask > 0.5).astype(np.uint8)
                confidence = 0.8  # SAMì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
                
                self.logger.info(f"âœ… SAM ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
                return mask, confidence
            else:
                raise RuntimeError("âŒ SAM ëª¨ë¸ì— forward ë©”ì„œë“œê°€ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ SAM ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_traditional_segmentation(
        self, 
        image: Image.Image, 
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ì „í†µì  ì„¸ê·¸ë©˜í…Œì´ì…˜ (ìƒ‰ìƒ ê¸°ë°˜) - ì™„ì „í•œ êµ¬í˜„"""
        try:
            # PIL to OpenCV ë³€í™˜
            image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            hsv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2HSV)
            
            # ìƒ‰ìƒ ë²”ìœ„ ì •ì˜ (ì›ë³¸ ì½”ë“œ ìœ ì§€)
            color_ranges = {
                'skin': {
                    'lower': np.array([0, 48, 80], dtype=np.uint8),
                    'upper': np.array([20, 255, 255], dtype=np.uint8)
                },
                'clothing': {
                    'lower': np.array([0, 0, 0], dtype=np.uint8),
                    'upper': np.array([180, 255, 200], dtype=np.uint8)
                }
            }
            
            # í”¼ë¶€ìƒ‰ ì˜ì—­ ì œê±°
            skin_mask = cv2.inRange(hsv, color_ranges['skin']['lower'], 
                                  color_ranges['skin']['upper'])
            
            # ì˜ë¥˜ ìƒ‰ìƒ ë²”ìœ„ ê°ì§€
            clothing_mask = cv2.inRange(hsv, color_ranges['clothing']['lower'],
                                      color_ranges['clothing']['upper'])
            
            # í”¼ë¶€ ì˜ì—­ ì œì™¸
            clothing_mask = cv2.bitwise_and(clothing_mask, cv2.bitwise_not(skin_mask))
            
            # í˜•íƒœí•™ì  ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
            
            clothing_mask = cv2.morphologyEx(clothing_mask, cv2.MORPH_CLOSE, kernel_medium)
            clothing_mask = cv2.morphologyEx(clothing_mask, cv2.MORPH_OPEN, kernel_small)
            
            # ê°€ì¥ í° ì—°ê²° ì˜ì—­ ì°¾ê¸°
            contours, _ = cv2.findContours(clothing_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                mask = np.zeros_like(clothing_mask)
                cv2.fillPoly(mask, [largest_contour], 255)
                mask = (mask > 0).astype(np.uint8)
            else:
                mask = (clothing_mask > 0).astype(np.uint8)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = np.sum(mask) / mask.size
            confidence = min(confidence * 2, 1.0)  # ì •ê·œí™”
            
            self.logger.info(f"âœ… ì „í†µì  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            if self.segmentation_config.strict_mode:
                raise RuntimeError(f"Strict Mode: ì „í†µì  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨ - {e}")
            return None, 0.0

    async def _run_deeplab_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """DeepLab ì‹¤ì œ AI ì¶”ë¡ """
        try:
            if 'deeplab' not in self.models_loaded:
                raise RuntimeError("âŒ DeepLab ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.models_loaded['deeplab']
            
            # ğŸ”¥ ì‹¤ì œ DeepLab AI ì¶”ë¡ 
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            model.eval()
            with torch.no_grad():
                output = model(input_tensor)
                
                # DeepLab ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, dict):
                    if 'out' in output:
                        logits = output['out']
                    else:
                        logits = list(output.values())[0]
                else:
                    logits = output
                
                # ì‚¬ëŒ/ì˜ë¥˜ í´ë˜ìŠ¤ ì¶”ì¶œ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
                # ì—¬ê¸°ì„œëŠ” ì¼ë°˜ì ì¸ COCO í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ëŒ(1)ì„ ì¶”ì¶œ
                person_mask = torch.argmax(logits, dim=1) == 1  # ì‚¬ëŒ í´ë˜ìŠ¤
                mask = person_mask.squeeze().cpu().numpy().astype(np.uint8)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence_map = torch.softmax(logits, dim=1)[:, 1, :, :]  # ì‚¬ëŒ í´ë˜ìŠ¤ í™•ë¥ 
                confidence = float(confidence_map.max().item())
                
                self.logger.info(f"âœ… DeepLab ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
                return mask, confidence
                
        except Exception as e:
            self.logger.error(f"âŒ DeepLab ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ê³ ê¸‰ ë©”ì„œë“œë“¤ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥ë“¤)
    # ==============================================

    def _setup_paths_and_cache(self):
        """ê²½ë¡œ ë° ìºì‹œ ì„¤ì • (ì›ë³¸ ê¸°ëŠ¥)"""
        try:
            # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
            self.model_base_path = Path(__file__).parent.parent.parent.parent / "ai_models"
            self.checkpoint_path = self.model_base_path / "checkpoints" / "step_03"
            self.checkpoint_path.mkdir(parents=True, exist_ok=True)
            
            self.logger.info("ğŸ“ ê²½ë¡œ ë° ìºì‹œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½ë¡œ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _create_visualizations(self, image, mask, clothing_type):
        """ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ ê¸°ëŠ¥ ì™„ì „ ìœ ì§€)"""
        try:
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            color = CLOTHING_COLORS.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type), 
                CLOTHING_COLORS['unknown']
            )
            
            # 1. ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ìƒ‰ìƒ êµ¬ë¶„)
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            visualizations['mask'] = Image.fromarray(mask_colored)
            
            # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) + 
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # ê²½ê³„ì„  ì¶”ê°€
            boundary = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
            overlay[boundary > 0] = (255, 255, 255)
            
            visualizations['overlay'] = Image.fromarray(overlay)
            
            # 3. ê²½ê³„ì„  ì´ë¯¸ì§€
            boundary_colored = np.zeros((*boundary.shape, 3), dtype=np.uint8)
            boundary_colored[boundary > 0] = (255, 255, 255)  # í°ìƒ‰ ê²½ê³„ì„ 
            
            # ì›ë³¸ ì´ë¯¸ì§€ì™€ í•©ì„±
            boundary_overlay = image_array.copy()
            boundary_overlay[boundary > 0] = (255, 255, 255)
            visualizations['boundary'] = Image.fromarray(boundary_overlay)
            
            # 4. ì¢…í•© ì‹œê°í™” ì´ë¯¸ì§€ (ì •ë³´ í¬í•¨)
            visualization = self._create_comprehensive_visualization(
                image, mask, clothing_type, color
            )
            visualizations['visualization'] = visualization
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    async def _run_hybrid_segmentation(
        self,
        image: Image.Image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì—¬ëŸ¬ ë°©ë²• ì¡°í•©) - ì›ë³¸ ê¸°ëŠ¥"""
        try:
            self.logger.info("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
            
            results = []
            weights = []
            
            # U2-Net ì‹œë„
            try:
                mask1, conf1 = await self._run_u2net_inference(image)
                if mask1 is not None:
                    results.append(mask1)
                    weights.append(conf1 * 0.4)  # ë†’ì€ ê°€ì¤‘ì¹˜
            except Exception:
                pass
            
            # RemBG ì‹œë„
            try:
                mask2, conf2 = await self._run_rembg_inference(image)
                if mask2 is not None:
                    results.append(mask2)
                    weights.append(conf2 * 0.3)
            except Exception:
                pass
            
            # ì „í†µì  ë°©ë²• ì‹œë„
            try:
                mask3, conf3 = self._run_traditional_segmentation(image, clothing_type)
                if mask3 is not None:
                    results.append(mask3)
                    weights.append(conf3 * 0.3)
            except Exception:
                pass
            
            if not results:
                if self.segmentation_config.strict_mode:
                    raise RuntimeError("âŒ í•˜ì´ë¸Œë¦¬ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ - Strict Mode")
                return None, 0.0
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë§ˆìŠ¤í¬ ì¡°í•©
            combined_mask = np.zeros_like(results[0], dtype=np.float32)
            total_weight = sum(weights)
            
            for mask, weight in zip(results, weights):
                combined_mask += mask.astype(np.float32) * (weight / total_weight)
            
            # ì´ì§„í™”
            final_mask = (combined_mask > 0.5).astype(np.uint8)
            final_confidence = total_weight / len(results)
            
            self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - {len(results)}ê°œ ë°©ë²• ì¡°í•©")
            return final_mask, final_confidence
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            if self.segmentation_config.strict_mode:
                raise
            return None, 0.0

    def _select_best_method_for_auto(self, image: Image.Image, clothing_type: ClothingType) -> SegmentationMethod:
        """AUTO ëª¨ë“œì—ì„œ ìµœì  ë°©ë²• ì„ íƒ (ì›ë³¸ ê¸°ëŠ¥)"""
        # ì´ë¯¸ì§€ íŠ¹ì„± ë¶„ì„
        width, height = image.size
        complexity_score = self._calculate_image_complexity(image)
        
        # ë³µì¡ë„ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ì— ë”°ë¼ ì„ íƒ
        if complexity_score > 0.7 and SegmentationMethod.U2NET in self.available_methods:
            return SegmentationMethod.U2NET
        elif SegmentationMethod.REMBG in self.available_methods:
            return SegmentationMethod.REMBG
        elif SegmentationMethod.U2NET in self.available_methods:
            return SegmentationMethod.U2NET
        else:
            return SegmentationMethod.TRADITIONAL

    def _calculate_image_complexity(self, image: Image.Image) -> float:
        """ì´ë¯¸ì§€ ë³µì¡ë„ ê³„ì‚° (ì›ë³¸ ê¸°ëŠ¥)"""
        try:
            # ê°„ë‹¨í•œ ë³µì¡ë„ ì¸¡ì • (ì—£ì§€ ë°€ë„ ê¸°ë°˜)
            gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            return min(edge_density * 10, 1.0)  # ì •ê·œí™”
        except Exception:
            return 0.5  # ê¸°ë³¸ê°’

    def _convert_result_to_dict(self, result: SegmentationResult) -> Dict[str, Any]:
        """SegmentationResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì›ë³¸ ê¸°ëŠ¥)"""
        try:
            result_dict = {
                'success': result.success,
                'confidence': result.confidence_score,
                'clothing_type': result.clothing_type.value if hasattr(result, 'clothing_type') else 'unknown',
                'method_used': result.method_used,
                'processing_time': result.processing_time,
                'metadata': result.metadata
            }
            
            # ì´ë¯¸ì§€ë“¤ì„ Base64ë¡œ ì¸ì½”ë”©
            if result.mask is not None:
                mask_image = Image.fromarray((result.mask * 255).astype(np.uint8))
                result_dict['mask_base64'] = self._image_to_base64(mask_image)
            
            if result.visualization_image:
                result_dict['visualization_base64'] = self._image_to_base64(result.visualization_image)
            
            if result.overlay_image:
                result_dict['overlay_base64'] = self._image_to_base64(result.overlay_image)
            
            if result.mask_image:
                result_dict['mask_image_base64'] = self._image_to_base64(result.mask_image)
            
            if result.boundary_image:
                result_dict['boundary_base64'] = self._image_to_base64(result.boundary_image)
            
            return result_dict
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²°ê³¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _preprocess_image(self, image):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì…ë ¥ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, str):
                if image.startswith('data:image'):
                    # Base64
                    header, data = image.split(',', 1)
                    image_data = base64.b64decode(data)
                    image = Image.open(BytesIO(image_data))
                else:
                    # íŒŒì¼ ê²½ë¡œ
                    image = Image.open(image)
            elif isinstance(image, np.ndarray):
                if image.shape[2] == 3:  # RGB
                    image = Image.fromarray(image)
                elif image.shape[2] == 4:  # RGBA
                    image = Image.fromarray(image).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image.shape}")
            elif not isinstance(image, Image.Image):
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ì¡°ì •
            target_size = self.segmentation_config.input_size
            if image.size != target_size:
                image = image.resize(target_size, Image.Resampling.LANCZOS)
            
            return image
                
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None

    def _detect_clothing_type(self, image, hint=None):
        """ì˜ë¥˜ íƒ€ì… ê°ì§€"""
        if hint:
            try:
                return ClothingType(hint.lower())
            except ValueError:
                pass
        
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê°ì§€
        if hasattr(image, 'size'):
            width, height = image.size
            aspect_ratio = height / width
            
            if aspect_ratio > 1.5:
                return ClothingType.DRESS
            elif aspect_ratio > 1.2:
                return ClothingType.SHIRT
            else:
                return ClothingType.PANTS
        
        return ClothingType.UNKNOWN

    def _post_process_mask(self, mask, quality):
        """ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            processed_mask = mask.copy()
            
            if self.segmentation_config.remove_noise:
                # ë…¸ì´ì¦ˆ ì œê±°
                kernel_size = 3 if quality == QualityLevel.FAST else 5
                kernel = np.ones((kernel_size, kernel_size), np.uint8)
                processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_OPEN, kernel)
                processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_CLOSE, kernel)
            
            if self.segmentation_config.edge_smoothing:
                # ì—£ì§€ ìŠ¤ë¬´ë”©
                processed_mask = cv2.GaussianBlur(processed_mask.astype(np.float32), (3, 3), 0.5)
                processed_mask = (processed_mask > 0.5).astype(np.uint8)
            
            # í™€ ì±„ìš°ê¸°
            if self.segmentation_config.enable_hole_filling:
                processed_mask = self._fill_holes(processed_mask)
            
            # ê²½ê³„ ê°œì„ 
            if self.segmentation_config.enable_edge_refinement:
                processed_mask = self._refine_edges(processed_mask)
            
            return processed_mask
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask

    def _fill_holes(self, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ ë‚´ë¶€ í™€ ì±„ìš°ê¸°"""
        try:
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            filled_mask = np.zeros_like(mask)
            for contour in contours:
                cv2.fillPoly(filled_mask, [contour], 1)
            return filled_mask
        except Exception as e:
            self.logger.warning(f"âš ï¸ í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask

    def _refine_edges(self, mask: np.ndarray) -> np.ndarray:
        """ê²½ê³„ ê°œì„ """
        try:
            if self.segmentation_config.enable_edge_refinement:
                # ê²½ê³„ ê²€ì¶œ
                edges = cv2.Canny(mask, 50, 150)
                
                # ê²½ê³„ ì£¼ë³€ ì˜ì—­ í™•ì¥
                kernel = np.ones((5, 5), np.uint8)
                edge_region = cv2.dilate(edges, kernel, iterations=1)
                
                # í•´ë‹¹ ì˜ì—­ì— ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
                blurred_mask = cv2.GaussianBlur(mask.astype(np.float32), (5, 5), 1.0)
                
                # ê²½ê³„ ì˜ì—­ë§Œ ë¸”ëŸ¬ëœ ê°’ìœ¼ë¡œ êµì²´
                refined_mask = mask.copy().astype(np.float32)
                refined_mask[edge_region > 0] = blurred_mask[edge_region > 0]
                
                return (refined_mask > 0.5).astype(np.uint8)
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²½ê³„ ê°œì„  ì‹¤íŒ¨: {e}")
            return mask

    def _create_visualizations(self, image, mask, clothing_type):
        """ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            color = CLOTHING_COLORS.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type), 
                CLOTHING_COLORS['unknown']
            )
            
            # 1. ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ìƒ‰ìƒ êµ¬ë¶„)
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            visualizations['mask'] = Image.fromarray(mask_colored)
            
            # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) + 
                np.array(color) * alpha
            ).astype(np.uint8)
            visualizations['overlay'] = Image.fromarray(overlay)
            
            # 3. ê²½ê³„ì„  ì´ë¯¸ì§€
            boundary = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
            boundary_colored = np.zeros((*boundary.shape, 3), dtype=np.uint8)
            boundary_colored[boundary > 0] = (255, 255, 255)
            
            boundary_overlay = image_array.copy()
            boundary_overlay[boundary > 0] = (255, 255, 255)
            visualizations['boundary'] = Image.fromarray(boundary_overlay)
            
            # 4. ì¢…í•© ì‹œê°í™” ì´ë¯¸ì§€
            visualization = self._create_comprehensive_visualization(
                image, mask, clothing_type, color
            )
            visualizations['visualization'] = visualization
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_comprehensive_visualization(self, image, mask, clothing_type, color):
        """ì¢…í•© ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ìº”ë²„ìŠ¤ ìƒì„±
            width, height = image.size
            canvas_width = width * 2 + 20
            canvas_height = height + 60
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), (240, 240, 240))
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(image, (10, 30))
            
            # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) + 
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # ê²½ê³„ì„  ì¶”ê°€
            boundary = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
            overlay[boundary > 0] = (255, 255, 255)
            
            overlay_image = Image.fromarray(overlay)
            canvas.paste(overlay_image, (width + 20, 30))
            
            # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            if self.font:
                draw = ImageDraw.Draw(canvas)
                
                # ì œëª©
                draw.text((10, 5), "Original", fill=(0, 0, 0), font=self.font)
                clothing_type_str = clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type)
                draw.text((width + 20, 5), f"AI Segmented ({clothing_type_str})", 
                         fill=(0, 0, 0), font=self.font)
                
                # í†µê³„ ì •ë³´
                mask_area = np.sum(mask)
                total_area = mask.size
                coverage = (mask_area / total_area) * 100
                
                info_text = f"Coverage: {coverage:.1f}% | AI Models: {len(self.models_loaded)} | Strict Mode: ON"
                draw.text((10, height + 35), info_text, fill=(0, 0, 0), font=self.font)
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¢…í•© ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return image

    def _get_current_method(self):
        """í˜„ì¬ ì‚¬ìš©ëœ ë°©ë²• ë°˜í™˜"""
        if self.models_loaded.get('u2net'):
            return 'u2net_real_ai'
        elif self.models_loaded.get('deeplab'):
            return 'deeplab_real_ai'
        elif self.models_loaded.get('sam'):
            return 'sam_real_ai'
        elif self.rembg_sessions:
            return 'rembg_ai'
        else:
            return 'traditional_fallback'

    def _image_to_base64(self, image):
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        try:
            buffer = BytesIO()
            if isinstance(image, Image.Image):
                image.save(buffer, format='PNG')
            else:
                img = Image.fromarray(image)
                img.save(buffer, format='PNG')
            image_data = buffer.getvalue()
            return base64.b64encode(image_data).decode()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Base64 ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'mask': None,
            'confidence': 0.0,
            'processing_time': 0.0,
            'method_used': 'error',
            'ai_models_used': [],
            'strict_mode': self.segmentation_config.strict_mode,
            'metadata': {
                'error_details': error_message,
                'available_models': list(self.models_loaded.keys()),
                'strict_mode': True
            }
        }

    def _update_processing_stats(self, processing_time: float, success: bool):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_processed'] += 1
            if success:
                self.processing_stats['successful_segmentations'] += 1
            else:
                self.processing_stats['failed_segmentations'] += 1
            
            # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.processing_stats['total_processed']
            current_avg = self.processing_stats['average_time']
            self.processing_stats['average_time'] = (
                (current_avg * (total - 1) + processing_time) / total
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ ê³ ê¸‰ ê¸°ëŠ¥ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================

    async def segment_clothing(self, image, **kwargs):
        """ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œ"""
        return await self.process(image, **kwargs)

    def get_segmentation_info(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ë³´ ë°˜í™˜ (ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜)"""
        return {
            'step_name': self.step_name,
            'device': self.device,
            'is_initialized': self.is_initialized,
            'strict_mode': self.segmentation_config.strict_mode,
            'available_methods': [m.value for m in self.available_methods],
            'loaded_ai_models': list(self.models_loaded.keys()),
            'rembg_sessions': list(self.rembg_sessions.keys()) if hasattr(self, 'rembg_sessions') else [],
            'processing_stats': self.processing_stats.copy(),
            'ai_model_stats': {
                'total_ai_calls': self.processing_stats['ai_model_calls'],
                'models_loaded': len(self.models_loaded),
                'fallback_used': False  # Strict Modeì—ì„œëŠ” í•­ìƒ False
            },
            'config': {
                'method': self.segmentation_config.method.value,
                'quality_level': self.segmentation_config.quality_level.value,
                'enable_visualization': self.segmentation_config.enable_visualization,
                'confidence_threshold': self.segmentation_config.confidence_threshold,
                'enable_edge_refinement': self.segmentation_config.enable_edge_refinement,
                'enable_hole_filling': self.segmentation_config.enable_hole_filling,
                'overlay_opacity': self.segmentation_config.overlay_opacity,
                'strict_mode': self.segmentation_config.strict_mode
            }
        }

    def get_segmentation_method_info(self, method_name: str) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ë³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        method_info = {
            'u2net': {
                'name': 'U2-Net',
                'description': 'Deep learning salient object detection for clothing (Real AI)',
                'quality': 'high',
                'speed': 'medium',
                'accuracy': 'high',
                'requirements': ['pytorch', 'torchvision'],
                'ai_model': True,
                'model_loaded': 'u2net' in self.models_loaded
            },
            'rembg': {
                'name': 'Remove Background',
                'description': 'AI-powered background removal tool (Real AI)',
                'quality': 'medium',
                'speed': 'fast',
                'accuracy': 'medium',
                'requirements': ['rembg'],
                'ai_model': True,
                'model_loaded': bool(self.rembg_sessions)
            },
            'sam': {
                'name': 'Segment Anything Model',
                'description': 'Meta\'s universal segmentation model (Real AI)',
                'quality': 'ultra',
                'speed': 'slow',
                'accuracy': 'ultra-high',
                'requirements': ['segment_anything'],
                'ai_model': True,
                'model_loaded': 'sam' in self.models_loaded
            },
            'deeplab': {
                'name': 'DeepLab v3',
                'description': 'Semantic segmentation with transformers (Real AI)',
                'quality': 'high',
                'speed': 'medium',
                'accuracy': 'high',
                'requirements': ['transformers'],
                'ai_model': True,
                'model_loaded': 'deeplab' in self.models_loaded
            },
            'traditional': {
                'name': 'Traditional CV',
                'description': 'Classical computer vision methods (Non-AI fallback)',
                'quality': 'medium',
                'speed': 'fast',
                'accuracy': 'medium',
                'requirements': ['opencv', 'scikit-learn'],
                'ai_model': False,
                'model_loaded': True
            },
            'hybrid': {
                'name': 'Hybrid AI Method',
                'description': 'Combination of multiple AI segmentation techniques',
                'quality': 'high',
                'speed': 'medium',
                'accuracy': 'high',
                'requirements': ['multiple AI models'],
                'ai_model': True,
                'model_loaded': len(self.models_loaded) >= 2
            },
            'auto': {
                'name': 'Auto AI Selection',
                'description': 'Automatically selects the best AI method',
                'quality': 'adaptive',
                'speed': 'adaptive',
                'accuracy': 'adaptive',
                'requirements': ['adaptive AI models'],
                'ai_model': True,
                'model_loaded': len(self.models_loaded) > 0
            }
        }
        
        return method_info.get(method_name, {
            'name': 'Unknown',
            'description': 'Unknown segmentation method',
            'quality': 'unknown',
            'speed': 'unknown',
            'accuracy': 'unknown',
            'requirements': [],
            'ai_model': False,
            'model_loaded': False
        })

    def get_clothing_mask(self, mask: np.ndarray, category: str) -> np.ndarray:
        """íŠ¹ì • ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì˜ í†µí•© ë§ˆìŠ¤í¬ ë°˜í™˜"""
        try:
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            if category in ['shirt', 'top', 'sweater']:
                return (mask > 128).astype(np.uint8)
            elif category in ['pants', 'skirt', 'bottom']:
                return (mask > 128).astype(np.uint8)
            elif category in ['dress']:
                return (mask > 128).astype(np.uint8)
            elif category in ['jacket', 'coat']:
                return (mask > 128).astype(np.uint8)
            else:
                return (mask > 128).astype(np.uint8)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros_like(mask, dtype=np.uint8)

    def visualize_segmentation(self, mask: np.ndarray, clothing_type: str = "shirt") -> np.ndarray:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì‹œê°í™”"""
        try:
            color = CLOTHING_COLORS.get(clothing_type, CLOTHING_COLORS['unknown'])
            height, width = mask.shape
            colored_image = np.zeros((height, width, 3), dtype=np.uint8)
            colored_image[mask > 0] = color
            return colored_image
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)

    def get_mask_statistics(self, mask: np.ndarray) -> Dict[str, float]:
        """ë§ˆìŠ¤í¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            total_pixels = mask.size
            mask_pixels = np.sum(mask > 0)
            coverage_ratio = mask_pixels / total_pixels
            
            # ì—°ê²° ì˜ì—­ ë¶„ì„
            contours, _ = cv2.findContours(
                (mask > 0).astype(np.uint8), 
                cv2.RETR_EXTERNAL, 
                cv2.CHAIN_APPROX_SIMPLE
            )
            
            num_regions = len(contours)
            largest_area = max([cv2.contourArea(c) for c in contours]) if contours else 0
            
            return {
                'coverage_ratio': coverage_ratio,
                'mask_pixels': int(mask_pixels),
                'total_pixels': int(total_pixels),
                'num_regions': num_regions,
                'largest_region_area': largest_area,
                'fragmentation_score': num_regions / max(1, coverage_ratio * 100),
                'ai_generated': True  # ì‹¤ì œ AI ìƒì„± ë§ˆìŠ¤í¬ í‘œì‹œ
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'coverage_ratio': 0.0,
                'mask_pixels': 0,
                'total_pixels': mask.size,
                'num_regions': 0,
                'largest_region_area': 0,
                'fragmentation_score': 0.0,
                'ai_generated': False
            }

    # ==============================================
    # ğŸ”¥ ì •ë¦¬ ë©”ì„œë“œ
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ClothSegmentationStep ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.models_loaded.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.models_loaded.clear()
            
            # RemBG ì„¸ì…˜ ì •ë¦¬
            if hasattr(self, 'rembg_sessions'):
                self.rembg_sessions.clear()
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'segmentation_cache'):
                self.segmentation_cache.clear()
            
            # ì‹¤í–‰ì ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and torch.backends.mps.is_available():
                safe_mps_empty_cache()
            elif self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.is_initialized = False
            self.logger.info("âœ… ClothSegmentationStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
# ==============================================

def create_cloth_segmentation_step(
    device: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜ (Strict AI Mode)"""
    if config is None:
        config = {}
    
    # Strict Mode ê°•ì œ í™œì„±í™”
    config['strict_mode'] = True
    
    return ClothSegmentationStep(device=device, config=config, **kwargs)

async def create_and_initialize_cloth_segmentation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """ClothSegmentationStep ìƒì„± ë° ì´ˆê¸°í™” (Strict AI Mode)"""
    step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
    await step.initialize()
    return step

def create_m3_max_segmentation_step(
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„± (Strict AI Mode)"""
    m3_config = {
        'method': SegmentationMethod.AUTO,
        'quality_level': QualityLevel.HIGH,
        'use_fp16': True,
        'batch_size': 8,  # M3 Max 128GB í™œìš©
        'cache_size': 200,
        'enable_visualization': True,
        'visualization_quality': 'high',
        'enable_edge_refinement': True,
        'enable_hole_filling': True,
        'strict_mode': True  # ğŸ”¥ ê°•ì œ í™œì„±í™”
    }
    
    if config:
        m3_config.update(config)
    
    # Strict Mode ì¬í™•ì¸
    m3_config['strict_mode'] = True
    
    return ClothSegmentationStep(device="mps", config=m3_config, **kwargs)

def create_production_segmentation_step(
    device: Optional[str] = None,
    **kwargs
) -> ClothSegmentationStep:
    """í”„ë¡œë•ì…˜ í™˜ê²½ìš© ClothSegmentationStep ìƒì„± (Strict AI Mode)"""
    production_config = {
        'method': SegmentationMethod.AUTO,
        'quality_level': QualityLevel.BALANCED,
        'enable_visualization': True,
        'enable_post_processing': True,
        'confidence_threshold': 0.7,
        'visualization_quality': 'medium',
        'enable_edge_refinement': True,
        'enable_hole_filling': True,
        'strict_mode': True  # ğŸ”¥ í”„ë¡œë•ì…˜ì—ì„œë„ Strict Mode
    }
    
    return ClothSegmentationStep(device=device, config=production_config, **kwargs)

    # ==============================================
    # ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ì›ë³¸ ì™„ì „ ìœ ì§€)
    # ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'ClothSegmentationStep',
    
    # ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
    'SegmentationMethod',
    'ClothingType', 
    'QualityLevel',
    'SegmentationConfig',
    'SegmentationResult',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì›ë³¸ ìœ ì§€)
    'U2NET',
    'REBNCONV',
    'RSU7',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_cloth_segmentation_step',
    'create_and_initialize_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    'create_production_segmentation_step',
    
    # ì‹œê°í™” ê´€ë ¨
    'CLOTHING_COLORS'
]

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ í•¨ìˆ˜ë“¤
# ==============================================

async def test_strict_ai_segmentation():
    """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ (Strict Mode)"""
    print("ğŸ§ª ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ (Strict Mode)")
    
    try:
        # Step ìƒì„± (Strict Mode)
        step = create_cloth_segmentation_step(
            device="auto",
            config={
                "method": "auto",
                "enable_visualization": True,
                "visualization_quality": "high",
                "quality_level": "balanced",
                "strict_mode": True
            }
        )
        
        # ì´ˆê¸°í™” (ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œë§Œ)
        await step.initialize()
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_image = Image.new('RGB', (512, 512), (200, 150, 100))
        
        # ì‹¤ì œ AI ì²˜ë¦¬ ì‹¤í–‰
        result = await step.process(dummy_image, clothing_type="shirt", quality_level="high")
        
        # ê²°ê³¼ í™•ì¸
        if result['success']:
            print("âœ… ì‹¤ì œ AI ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   - ì˜ë¥˜ íƒ€ì…: {result['clothing_type']}")
            print(f"   - ì‹ ë¢°ë„: {result['confidence']:.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            print(f"   - ì‚¬ìš© AI ëª¨ë¸: {result['ai_models_used']}")
            print(f"   - Strict Mode: {result['metadata']['strict_mode']}")
            
            if 'visualization_base64' in result:
                print("   - AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±ë¨")
            if 'overlay_base64' in result:
                print("   - AI ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±ë¨")
        else:
            print(f"âŒ ì‹¤ì œ AI ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # AI ëª¨ë¸ ì •ë³´ ì¶œë ¥
        info = step.get_segmentation_info()
        print(f"\nğŸ§  ì‹¤ì œ AI ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   - ë””ë°”ì´ìŠ¤: {info['device']}")
        print(f"   - Strict Mode: {info['strict_mode']}")
        print(f"   - ë¡œë“œëœ AI ëª¨ë¸: {info['loaded_ai_models']}")
        print(f"   - AI ëª¨ë¸ í˜¸ì¶œ ìˆ˜: {info['ai_model_stats']['total_ai_calls']}")
        print(f"   - í´ë°± ì‚¬ìš©: {info['ai_model_stats']['fallback_used']}")
        
        # ì •ë¦¬
        await step.cleanup()
        print("âœ… ì‹¤ì œ AI í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ì •ë¦¬")
        
    except Exception as e:
        print(f"âŒ ì‹¤ì œ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ModelLoaderì™€ ì‹¤ì œ AI ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

def example_strict_ai_usage():
    """ì‹¤ì œ AI ì‚¬ìš© ì˜ˆì‹œ (Strict Mode)"""
    print("ğŸ”¥ MyCloset AI Step 03 - ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‚¬ìš© ì˜ˆì‹œ (Strict Mode)")
    print("=" * 80)
    
    print("""
# ğŸ”¥ ì‹¤ì œ AIë§Œ ì‚¬ìš©í•˜ëŠ” Strict Mode ë²„ì „

# 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ì‹¤ì œ AI ëª¨ë¸ë§Œ)
from app.ai_pipeline.steps.step_03_cloth_segmentation import create_cloth_segmentation_step

# ì‹¤ì œ AI ë°©ë²• (ModelLoader ì˜ì¡´)
step = create_cloth_segmentation_step(device="mps", config={
    "method": "auto",
    "strict_mode": True  # í´ë°± ì—†ìŒ, ì‹¤ì œ AIë§Œ
})

# ì´ˆê¸°í™” (ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œë§Œ, í´ë°± ì—†ìŒ)
await step.initialize()  # ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ

# ì‹¤ì œ AI ì´ë¯¸ì§€ ì²˜ë¦¬
result = await step.process(image, clothing_type="shirt", quality_level="high")

# 2. M3 Max ìµœì í™” ë²„ì „ (128GB í™œìš©, ì‹¤ì œ AIë§Œ)
step = create_m3_max_segmentation_step({
    "quality_level": "ultra",
    "enable_visualization": True,
    "enable_edge_refinement": True,
    "strict_mode": True  # ê°•ì œ í™œì„±í™”
})

# 3. í”„ë¡œë•ì…˜ ë²„ì „ (ì•ˆì •ì„± + ì‹¤ì œ AI)
step = create_production_segmentation_step(device="cpu")

# 4. ì‹¤ì œ AI ê²°ê³¼ í™œìš©
if result['success']:
    # ì‹¤ì œ AI ìƒì„± ê²°ê³¼
    ai_mask = result['mask']                    # ì‹¤ì œ AI ë§ˆìŠ¤í¬
    ai_confidence = result['confidence']        # ì‹¤ì œ AI ì‹ ë¢°ë„
    ai_models_used = result['ai_models_used']   # ì‚¬ìš©ëœ ì‹¤ì œ AI ëª¨ë¸ë“¤
    strict_mode = result['metadata']['strict_mode']  # True
    ai_inference = result['metadata']['ai_inference']  # True
    
    print(f"ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©: {ai_models_used}")
    print(f"AI ì‹ ë¢°ë„: {ai_confidence}")
    print(f"Strict Mode: {strict_mode}")

# 5. ì‹¤ì œ AI ëª¨ë¸ ì •ë³´ í™•ì¸
info = step.get_segmentation_info()
print(f"ë¡œë“œëœ ì‹¤ì œ AI ëª¨ë¸: {info['loaded_ai_models']}")
print(f"AI ëª¨ë¸ í˜¸ì¶œ ìˆ˜: {info['ai_model_stats']['total_ai_calls']}")
print(f"í´ë°± ì‚¬ìš© ì—¬ë¶€: {info['ai_model_stats']['fallback_used']}")  # í•­ìƒ False

# 6. ì—ëŸ¬ ì²˜ë¦¬ (Strict Mode)
try:
    await step.initialize()  # ModelLoader ì—†ìœ¼ë©´ ì¦‰ì‹œ ì—ëŸ¬
    result = await step.process(image)  # AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬
except RuntimeError as e:
    print(f"ì‹¤ì œ AI ëª¨ë¸ í•„ìš”: {e}")
    # í´ë°± ì—†ìŒ, ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

# 7. conda í™˜ê²½ ì„¤ì • (ì‹¤ì œ AI ëª¨ë¸ìš©)
'''
conda create -n mycloset-ai-strict python=3.9 -y
conda activate mycloset-ai-strict

# ì‹¤ì œ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
conda install -c pytorch pytorch torchvision torchaudio -y
pip install rembg segment-anything transformers
pip install opencv-python pillow numpy

# M3 Max ìµœì í™”
conda install -c conda-forge accelerate -y

# ì‹¤í–‰
python -m app.ai_pipeline.steps.step_03_cloth_segmentation
'''

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬
await step.cleanup()
""")

def print_conda_setup_guide():
    """conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (ì‹¤ì œ AIìš©)"""
    print("""
ğŸ MyCloset AI - conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (ì‹¤ì œ AI ëª¨ë¸ìš©)

# 1. conda í™˜ê²½ ìƒì„± (Strict AI Mode)
conda create -n mycloset-ai-strict python=3.9 -y
conda activate mycloset-ai-strict

# 2. í•µì‹¬ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìˆ˜)
conda install -c pytorch pytorch torchvision torchaudio -y
conda install -c conda-forge opencv numpy pillow -y

# 3. ì‹¤ì œ AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìˆ˜)
pip install rembg segment-anything transformers
pip install scikit-learn psutil ultralytics

# 4. M3 Max ìµœì í™” (macOS, í•„ìˆ˜)
conda install -c conda-forge accelerate -y

# 5. ê²€ì¦ (ì‹¤ì œ AI ëª¨ë¸ í™•ì¸)
python -c "import torch; print(f'PyTorch: {torch.__version__}, MPS: {torch.backends.mps.is_available()}')"
python -c "import rembg; print('RemBG: âœ…')"
python -c "import transformers; print('Transformers: âœ…')"

# 6. ì‹¤í–‰ (Strict AI Mode)
cd backend
export STRICT_AI_MODE=true
python -m app.ai_pipeline.steps.step_03_cloth_segmentation

# 7. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MYCLOSET_AI_STRICT_MODE=true
export MYCLOSET_AI_DEVICE=mps
export MYCLOSET_AI_MODELS_PATH=/path/to/ai_models
""")

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
logger.info("âœ… Step 03 ì‹¤ì œ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë“ˆ ì™„ì „ êµ¬í˜„ ì™„ë£Œ")
logger.info(f"   - BaseStepMixin ì—°ë™: âœ…")
logger.info(f"   - ModelLoader ì—°ë™: âœ… (í•„ìˆ˜)")
logger.info(f"   - StepModelRequestAnalyzer: âœ… (í•„ìˆ˜)")
logger.info(f"   - í´ë°± ëª¨ë“œ: âŒ (ì™„ì „ ì œê±°)")
logger.info(f"   - Strict Mode: âœ… (ê°•ì œ í™œì„±í™”)")
logger.info(f"   - ì‹¤ì œ AIë§Œ: âœ… (ì‹œë®¬ë ˆì´ì…˜ ì—†ìŒ)")
logger.info("ğŸ”¥ í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡° - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("ğŸ§  100% ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© - ModelLoader ì˜ì¡´")
logger.info("ğŸš« í´ë°± ì™„ì „ ì œê±° - ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬")
logger.info("ğŸ¨ ì™„ì „í•œ ì‹œê°í™”: ìƒ‰ìƒí™”, ì˜¤ë²„ë ˆì´, ë§ˆìŠ¤í¬, ê²½ê³„ì„ ")
logger.info("ğŸ”§ ê³ ê¸‰ í›„ì²˜ë¦¬: ê²½ê³„ ê°œì„ , í™€ ì±„ìš°ê¸°, í˜•íƒœí•™ì  ì²˜ë¦¬")
logger.info("ğŸ M3 Max ìµœì í™”: ì›Œë°ì—…, ë©”ëª¨ë¦¬ ê´€ë¦¬, Neural Engine")
logger.info("ğŸ—ï¸ í”„ë¡œë•ì…˜ ì•ˆì •ì„±: ì—ëŸ¬ ì²˜ë¦¬, í†µê³„, ê²€ì¦")

if __name__ == "__main__":
    """ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ AI)"""
    print("ğŸ”¥ Step 03 ì‹¤ì œ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
    
    # ì˜ˆì‹œ ì¶œë ¥
    example_strict_ai_usage()
    
    # conda ê°€ì´ë“œ
    print_conda_setup_guide()
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸°)
    import asyncio
    try:
        asyncio.run(test_strict_ai_segmentation())
    except Exception as e:
        print(f"âŒ ì‹¤ì œ AI í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ModelLoaderì™€ ì‹¤ì œ AI ëª¨ë¸ì´ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        print("   1. ModelLoader ëª¨ë“ˆ ì„¤ì¹˜ ë° ì„¤ì •")
        print("   2. ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°°ì¹˜")
        print("   3. step_model_requests.py ì„¤ì • í™•ì¸")
        print("   4. base_step_mixin.py ì„¤ì • í™•ì¸")