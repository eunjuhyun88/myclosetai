# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™” v22.0
====================================================================================

ğŸ¯ BaseStepMixin v19.1 ì™„ì „ ì¤€ìˆ˜:
âœ… _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ (ë™ê¸° ì²˜ë¦¬)
âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì€ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬
âœ… step_model_requests.py DetailedDataSpec ì™„ì „ í™œìš©
âœ… GitHub í”„ë¡œì íŠ¸ 100% í˜¸í™˜ì„± ë³´ì¥
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€

AI ê°•í™” ì‚¬í•­ (100% ë³´ì¡´):
ğŸ§  ì‹¤ì œ SAM, U2Net, ISNet, Mobile SAM AI ì¶”ë¡  ë¡œì§
ğŸ”¥ OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
ğŸ¨ AI ê°•í™” ì‹œê°í™” (Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§)
âš¡ M3 Max MPS ê°€ì† ë° 128GB ë©”ëª¨ë¦¬ ìµœì í™”
ğŸ¯ ì‹¤ì œ ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
ğŸ”§ ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (2445.7MB SAM)
ğŸ“Š í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë° í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”

Author: MyCloset AI Team
Date: 2025-07-27  
Version: v22.0 (BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™”)
"""

import os
import sys
import logging
import time
import threading
import gc
import hashlib
import json
import base64
import math
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import platform
import subprocess

# ==============================================
# ğŸ”¥ 1. BaseStepMixin ìƒì† ë° TYPE_CHECKING ìˆœí™˜ì°¸ì¡° ë°©ì§€
# ==============================================

# BaseStepMixin ë™ì  import
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin í´ë˜ìŠ¤ ë¡œë”©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ (ê¸°ë³¸ ê¸°ëŠ¥ë§Œ)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
        
        def initialize(self): 
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader): 
            self.model_loader = model_loader
        
        def get_model(self, model_name=None): 
            return None

if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader, StepModelInterface
    from app.ai_pipeline.utils.step_model_requests import (
        EnhancedRealModelRequest, DetailedDataSpec, get_enhanced_step_request
    )

# ==============================================
# ğŸ”¥ 2. í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (conda í™˜ê²½ ìš°ì„ )
# ==============================================

# Logger ì„¤ì •
logger = logging.getLogger(__name__)

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½ ìš°ì„ )")
except ImportError:
    logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")

# PIL import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
except ImportError:
    logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# PyTorch import (conda í™˜ê²½ ìš°ì„ )
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥ (M3 Max ìµœì í™”)")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
REMBG_AVAILABLE = False
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
    logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ RemBG ì—†ìŒ - pip install rembg")

SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - pip install segment-anything")

ONNX_AVAILABLE = False
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
    logger.info("âš¡ ONNX Runtime ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ ONNX Runtime ì—†ìŒ - pip install onnxruntime")

# ==============================================
# ğŸ”¥ 3. step_model_requests.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ
# ==============================================

def get_step_requirements():
    """step_model_requests.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        requirements_module = importlib.import_module('app.ai_pipeline.utils.step_model_requests')
        
        get_enhanced_step_request = getattr(requirements_module, 'get_enhanced_step_request', None)
        if get_enhanced_step_request:
            return get_enhanced_step_request("ClothSegmentationStep")
        
        REAL_STEP_MODEL_REQUESTS = getattr(requirements_module, 'REAL_STEP_MODEL_REQUESTS', {})
        return REAL_STEP_MODEL_REQUESTS.get("ClothSegmentationStep")
        
    except ImportError as e:
        logger.warning(f"âš ï¸ step_model_requests ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ
STEP_REQUIREMENTS = get_step_requirements()

# ==============================================
# ğŸ”¥ 4. ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€
# ==============================================

IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ 5. ë°ì´í„° êµ¬ì¡° ì •ì˜ (step_model_requests.py í˜¸í™˜)
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• (step_model_requests.py í˜¸í™˜)"""
    SAM_HUGE = "sam_huge"           # SAM ViT-Huge (2445.7MB)
    U2NET_CLOTH = "u2net_cloth"     # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    MOBILE_SAM = "mobile_sam"       # Mobile SAM (38.8MB)
    ISNET = "isnet"                 # ISNet ONNX (168.1MB)
    HYBRID_AI = "hybrid_ai"         # ì—¬ëŸ¬ AI ëª¨ë¸ ì¡°í•©
    AUTO_AI = "auto_ai"             # ìë™ AI ëª¨ë¸ ì„ íƒ

class ClothingType(Enum):
    """ì˜ë¥˜ íƒ€ì… (step_model_requests.py í˜¸í™˜)"""
    SHIRT = "shirt"
    DRESS = "dress"
    PANTS = "pants"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # Mobile SAM
    BALANCED = "balanced"   # U2Net + ISNet
    HIGH = "high"          # SAM + U2Net
    ULTRA = "ultra"        # Hybrid AI (ëª¨ë“  ëª¨ë¸)

@dataclass
class SegmentationConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì • (step_model_requests.py í˜¸í™˜)"""
    method: SegmentationMethod = SegmentationMethod.AUTO_AI
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (1024, 1024)
    enable_visualization: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    use_fp16: bool = True
    confidence_threshold: float = 0.5
    remove_noise: bool = True
    overlay_opacity: float = 0.6

# ==============================================
# ğŸ”¥ 6. AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class AIImageProcessor:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    
    @staticmethod
    def ai_resize(image: Union[np.ndarray, Image.Image], target_size: Tuple[int, int]) -> Image.Image:
        """AI ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§"""
        try:
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            return image.resize(target_size, Image.Resampling.LANCZOS)
        except Exception as e:
            logger.warning(f"âš ï¸ AI ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            return image.resize(target_size, Image.Resampling.LANCZOS)
    
    @staticmethod
    def ai_detect_edges(image: np.ndarray, threshold1: int = 50, threshold2: int = 150) -> np.ndarray:
        """AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_edge_detection(image)
            
            tensor = torch.from_numpy(image).float()
            if len(tensor.shape) == 3:
                tensor = tensor.mean(dim=2)
            
            tensor = tensor.unsqueeze(0).unsqueeze(0)
            
            # Sobel í•„í„°
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            sobel_x = sobel_x.view(1, 1, 3, 3)
            sobel_y = sobel_y.view(1, 1, 3, 3)
            
            grad_x = F.conv2d(tensor, sobel_x, padding=1)
            grad_y = F.conv2d(tensor, sobel_y, padding=1)
            
            magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)
            edges = (magnitude > threshold1).float()
            
            return (edges.squeeze().numpy() * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_edge_detection(image)
    
    @staticmethod
    def _simple_edge_detection(image: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ í´ë°±"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            
            grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='edge')
            grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='edge')
            
            magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)
            edges = (magnitude > 50).astype(np.uint8) * 255
            
            return edges
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros_like(image[:, :, 0] if len(image.shape) == 3 else image, dtype=np.uint8)
    
    @staticmethod
    def ai_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """AI ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚°"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
            
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            padding = kernel_size // 2
            
            if operation.lower() == "closing":
                dilated = F.max_pool2d(tensor, kernel_size, stride=1, padding=padding)
                result = -F.max_pool2d(-dilated, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "opening":
                eroded = -F.max_pool2d(-tensor, kernel_size, stride=1, padding=padding)
                result = F.max_pool2d(eroded, kernel_size, stride=1, padding=padding)
            else:
                result = tensor
            
            return (result.squeeze().numpy() * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
    
    @staticmethod
    def _simple_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° í´ë°±"""
        try:
            if operation.lower() == "closing":
                try:
                    from scipy import ndimage
                    filled = ndimage.binary_fill_holes(mask > 128)
                    return (filled * 255).astype(np.uint8)
                except ImportError:
                    return mask
            else:
                return mask
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return mask

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì™„ì „í•œ êµ¬í˜„)
# ==============================================

class REBNCONV(nn.Module):
    """U2-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡"""
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class RSU7(nn.Module):
    """U2-Net RSU-7 ë¸”ë¡"""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample6 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample5 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
    
    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = self.upsample6(hx6d)
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = self.upsample5(hx5d)
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = self.upsample4(hx4d)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = self.upsample3(hx3d)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = self.upsample2(hx2d)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RealU2NetClothModel(nn.Module):
    """ì‹¤ì œ U2-Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸ (u2net.pth 168.1MB)"""
    def __init__(self, in_ch=3, out_ch=1):
        super(RealU2NetClothModel, self).__init__()
        
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage2 = RSU7(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage3 = RSU7(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage4 = RSU7(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage5 = RSU7(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage6 = RSU7(512, 256, 512)
        
        self.stage5d = RSU7(1024, 256, 512)
        self.stage4d = RSU7(1024, 128, 256)
        self.stage3d = RSU7(512, 64, 128)
        self.stage2d = RSU7(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
        
        self.model_name = "RealU2NetClothModel"
        self.version = "2.0"
        self.cloth_specialized = True
        
    def forward(self, x):
        hx = x
        
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        hx6 = self.stage6(hx)
        
        hx5d = self.stage5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=d1.shape[2:], mode='bilinear', align_corners=False)
        
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        model = cls()
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        model.load_state_dict(checkpoint['model'])
                    elif 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                else:
                    model.load_state_dict(checkpoint)
            except Exception as e:
                logger.warning(f"âš ï¸ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        model.to(device)
        model.eval()
        return model

class RealSAMModel(nn.Module):
    """ì‹¤ì œ SAM ëª¨ë¸ ë˜í¼ (sam_vit_h_4b8939.pth 2445.7MB)"""
    def __init__(self, model_type: str = "vit_h"):
        super(RealSAMModel, self).__init__()
        self.model_type = model_type
        self.model_name = f"RealSAMModel_{model_type}"
        self.version = "2.0"
        self.sam_model = None
        self.predictor = None
        self.is_loaded = False
        
    def load_sam_model(self, checkpoint_path: str):
        """SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ SAM ëª¨ë¸ ë¡œë”©: {checkpoint_path}")
            
            if self.model_type == "vit_h":
                self.sam_model = sam.build_sam_vit_h(checkpoint=checkpoint_path)
            elif self.model_type == "vit_b":
                self.sam_model = sam.build_sam_vit_b(checkpoint=checkpoint_path)
            else:
                self.sam_model = sam.build_sam(checkpoint=checkpoint_path)
            
            self.predictor = sam.SamPredictor(self.sam_model)
            self.is_loaded = True
            
            logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_type}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def forward(self, x):
        """ë”ë¯¸ forward (SAMì€ íŠ¹ë³„í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)"""
        if not self.is_loaded:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        return x
    
    def segment_clothing(self, image_array: np.ndarray, clothing_type: str = "shirt") -> Dict[str, np.ndarray]:
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            if not self.is_loaded or self.predictor is None:
                logger.warning("âš ï¸ SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {}
            
            self.predictor.set_image(image_array)
            
            height, width = image_array.shape[:2]
            clothing_prompts = self._generate_clothing_prompts(clothing_type, width, height)
            
            results = {}
            
            for cloth_area, points in clothing_prompts.items():
                try:
                    masks, scores, logits = self.predictor.predict(
                        point_coords=np.array(points),
                        point_labels=np.ones(len(points)),
                        multimask_output=True
                    )
                    
                    best_mask_idx = np.argmax(scores)
                    best_mask = masks[best_mask_idx].astype(np.uint8)
                    
                    results[cloth_area] = best_mask
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ SAM {cloth_area} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_clothing_prompts(self, clothing_type: str, width: int, height: int) -> Dict[str, List[Tuple[int, int]]]:
        """ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ìƒì„±"""
        prompts = {}
        
        if clothing_type in ["shirt", "top", "sweater"]:
            prompts["upper_body"] = [
                (width // 2, height // 3),
                (width // 3, height // 2),
                (2 * width // 3, height // 2),
            ]
        elif clothing_type in ["pants", "bottom"]:
            prompts["lower_body"] = [
                (width // 2, 2 * height // 3),
                (width // 3, 3 * height // 4),
                (2 * width // 3, 3 * height // 4),
            ]
        elif clothing_type == "dress":
            prompts["full_dress"] = [
                (width // 2, height // 3),
                (width // 2, 2 * height // 3),
                (width // 3, height // 2),
                (2 * width // 3, height // 2),
            ]
        else:
            prompts["clothing"] = [
                (width // 2, height // 2),
                (width // 3, height // 3),
                (2 * width // 3, 2 * height // 3),
            ]
        
        return prompts
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu", model_type: str = "vit_h"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ SAM ëª¨ë¸ ë¡œë“œ"""
        model = cls(model_type=model_type)
        model.load_sam_model(checkpoint_path)
        return model

class RealMobileSAMModel(nn.Module):
    """ì‹¤ì œ Mobile SAM ëª¨ë¸ (mobile_sam.pt 38.8MB)"""
    def __init__(self):
        super(RealMobileSAMModel, self).__init__()
        self.model_name = "RealMobileSAMModel"
        self.version = "2.0"
        self.sam_model = None
        self.is_loaded = False
        
    def load_mobile_sam(self, checkpoint_path: str):
        """Mobile SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ”„ Mobile SAM ë¡œë”©: {checkpoint_path}")
            
            if TORCH_AVAILABLE and os.path.exists(checkpoint_path):
                self.sam_model = torch.jit.load(checkpoint_path, map_location='cpu')
                self.sam_model.eval()
                self.is_loaded = True
                logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ")
                return True
            else:
                logger.warning("âš ï¸ Mobile SAM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def forward(self, x):
        """Mobile SAM ì¶”ë¡ """
        if not self.is_loaded or self.sam_model is None:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        
        try:
            with torch.no_grad():
                result = self.sam_model(x)
                return result
        except Exception as e:
            logger.warning(f"âš ï¸ Mobile SAM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ Mobile SAM ë¡œë“œ"""
        model = cls()
        model.load_mobile_sam(checkpoint_path)
        model.to(device)
        return model

class RealISNetModel:
    """ì‹¤ì œ ISNet ONNX ëª¨ë¸ (isnetis.onnx 168.1MB)"""
    def __init__(self):
        self.model_name = "RealISNetModel"
        self.version = "2.0"
        self.ort_session = None
        self.is_loaded = False
        
    def load_isnet_model(self, onnx_path: str):
        """ISNet ONNX ëª¨ë¸ ë¡œë“œ"""
        try:
            if not ONNX_AVAILABLE:
                logger.warning("âš ï¸ ONNX Runtimeì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ ISNet ONNX ë¡œë”©: {onnx_path}")
            
            providers = ['CPUExecutionProvider']
            if MPS_AVAILABLE:
                providers.insert(0, 'CoreMLExecutionProvider')
            
            self.ort_session = ort.InferenceSession(onnx_path, providers=providers)
            self.is_loaded = True
            
            logger.info("âœ… ISNet ONNX ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ISNet ONNX ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image_array: np.ndarray) -> np.ndarray:
        """ISNet ì˜ˆì¸¡"""
        try:
            if not self.is_loaded or self.ort_session is None:
                logger.warning("âš ï¸ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
            
            if len(image_array.shape) == 3:
                input_image = image_array[:, :, ::-1].astype(np.float32) / 255.0
                input_image = np.transpose(input_image, (2, 0, 1))
                input_image = np.expand_dims(input_image, axis=0)
            else:
                input_image = image_array.astype(np.float32) / 255.0
                input_image = np.expand_dims(input_image, axis=(0, 1))
            
            input_name = self.ort_session.get_inputs()[0].name
            result = self.ort_session.run(None, {input_name: input_image})
            
            mask = result[0][0, 0, :, :]
            mask = (mask > 0.5).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            logger.error(f"âŒ ISNet ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
    
    @classmethod
    def from_checkpoint(cls, onnx_path: str):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ISNet ë¡œë“œ"""
        model = cls()
        model.load_isnet_model(onnx_path)
        return model

# ==============================================
# ğŸ”¥ 8. ClothSegmentationStep ë©”ì¸ í´ë˜ìŠ¤ (BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜)
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™” v22.0
    
    BaseStepMixin ìƒì†ìœ¼ë¡œ ìë™ ì œê³µë˜ëŠ” ê¸°ëŠ¥:
    âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
    âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš© (DetailedDataSpec)
    âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ (ModelLoader, MemoryManager ë“±)
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
    âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
    
    ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤!
    """
    
    def __init__(self, **kwargs):
        """BaseStepMixin v19.1 ìƒì† ì´ˆê¸°í™”"""
        super().__init__(
            step_name="ClothSegmentationStep",
            step_id=3,
            **kwargs
        )
        
        # Step 03 íŠ¹í™” ì†ì„±ë“¤
        self.ai_models = {}
        self.model_paths = {}
        self.available_methods = []
        self.segmentation_config = SegmentationConfig()
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ
        self.models_loading_status = {
            'sam_huge': False,          # sam_vit_h_4b8939.pth (2445.7MB) 
            'u2net_cloth': False,       # u2net.pth (168.1MB)
            'mobile_sam': False,        # mobile_sam.pt (38.8MB)
            'isnet': False,             # isnetis.onnx (168.1MB)
        }
        
        # ì‹œìŠ¤í…œ ìµœì í™”
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = MEMORY_GB
        
        # ì‹¤í–‰ì ë° ìºì‹œ
        self.executor = ThreadPoolExecutor(
            max_workers=4 if self.is_m3_max else 2,
            thread_name_prefix="cloth_seg_ai"
        )
        self.segmentation_cache = {}
        self.cache_lock = threading.RLock()
        
        # AI ê°•í™” í†µê³„
        self.ai_stats = {
            'total_processed': 0,
            'sam_huge_calls': 0,
            'u2net_calls': 0,
            'mobile_sam_calls': 0,
            'isnet_calls': 0,
            'hybrid_calls': 0,
            'ai_model_calls': 0,
            'average_confidence': 0.0
        }
        
        logger.info(f"âœ… {self.step_name} BaseStepMixin v19.1 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - Device: {self.device}")
        logger.info(f"   - M3 Max: {self.is_m3_max}")
        logger.info(f"   - Memory: {self.memory_gb}GB")
    
    # ==============================================
    # ğŸ”¥ 9. ëª¨ë¸ ì´ˆê¸°í™” ë©”ì„œë“œë“¤
    # ==============================================
    
    def initialize(self) -> bool:
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            logger.info(f"ğŸ”„ {self.step_name} AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ëª¨ë¸ ê²½ë¡œ íƒì§€
            self._detect_model_paths()
            
            # AI ëª¨ë¸ ë¡œë”©
            self._load_all_ai_models()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            # BaseStepMixin ì´ˆê¸°í™”
            super_initialized = super().initialize()
            
            self.is_initialized = True
            self.is_ready = True
            
            loaded_models = list(self.ai_models.keys())
            logger.info(f"âœ… {self.step_name} AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    def _detect_model_paths(self):
        """ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            # step_model_requests.py ê¸°ë°˜ ê²½ë¡œ íƒì§€
            if STEP_REQUIREMENTS:
                search_paths = STEP_REQUIREMENTS.search_paths + STEP_REQUIREMENTS.fallback_paths
                
                # Primary íŒŒì¼
                primary_file = STEP_REQUIREMENTS.primary_file
                for search_path in search_paths:
                    full_path = os.path.join(search_path, primary_file)
                    if os.path.exists(full_path):
                        self.model_paths['sam_huge'] = full_path
                        logger.info(f"âœ… Primary SAM ë°œê²¬: {full_path}")
                        break
                
                # Alternative íŒŒì¼ë“¤
                for alt_file, alt_size in STEP_REQUIREMENTS.alternative_files:
                    for search_path in search_paths:
                        full_path = os.path.join(search_path, alt_file)
                        if os.path.exists(full_path):
                            if 'u2net' in alt_file.lower():
                                self.model_paths['u2net_cloth'] = full_path
                            elif 'mobile_sam' in alt_file.lower():
                                self.model_paths['mobile_sam'] = full_path
                            elif 'isnet' in alt_file.lower() or alt_file.endswith('.onnx'):
                                self.model_paths['isnet'] = full_path
                            logger.info(f"âœ… Alternative ëª¨ë¸ ë°œê²¬: {full_path}")
                            break
            
            # ê¸°ë³¸ ê²½ë¡œ í´ë°±
            if not self.model_paths:
                base_paths = [
                    "ai_models/step_03_cloth_segmentation/",
                    "models/step_03_cloth_segmentation/",
                ]
                
                model_files = {
                    'sam_huge': 'sam_vit_h_4b8939.pth',
                    'u2net_cloth': 'u2net.pth',
                    'mobile_sam': 'mobile_sam.pt',
                    'isnet': 'isnetis.onnx'
                }
                
                for model_key, filename in model_files.items():
                    for base_path in base_paths:
                        full_path = os.path.join(base_path, filename)
                        if os.path.exists(full_path):
                            self.model_paths[model_key] = full_path
                            logger.info(f"âœ… {model_key} ë°œê²¬: {full_path}")
                            break
                    else:
                        logger.warning(f"âš ï¸ {model_key} íŒŒì¼ ì—†ìŒ: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def _load_all_ai_models(self):
        """ëª¨ë“  AI ëª¨ë¸ ë¡œë”©"""
        try:
            if not TORCH_AVAILABLE:
                logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
                return
            
            logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # SAM Huge ë¡œë”© (Primary Model)
            if 'sam_huge' in self.model_paths:
                try:
                    sam_model = RealSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['sam_huge'],
                        device=self.device,
                        model_type="vit_h"
                    )
                    if sam_model.is_loaded:
                        self.ai_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        logger.info("âœ… SAM Huge ë¡œë”© ì™„ë£Œ (Primary Model)")
                except Exception as e:
                    logger.error(f"âŒ SAM Huge ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # U2Net Cloth ë¡œë”© (Alternative Model)
            if 'u2net_cloth' in self.model_paths:
                try:
                    u2net_model = RealU2NetClothModel.from_checkpoint(
                        checkpoint_path=self.model_paths['u2net_cloth'],
                        device=self.device
                    )
                    self.ai_models['u2net_cloth'] = u2net_model
                    self.models_loading_status['u2net_cloth'] = True
                    logger.info("âœ… U2Net Cloth ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ U2Net Cloth ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Mobile SAM ë¡œë”© (Alternative Model)
            if 'mobile_sam' in self.model_paths:
                try:
                    mobile_sam_model = RealMobileSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['mobile_sam'],
                        device=self.device
                    )
                    if mobile_sam_model.is_loaded:
                        self.ai_models['mobile_sam'] = mobile_sam_model
                        self.models_loading_status['mobile_sam'] = True
                        logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ISNet ë¡œë”© (Alternative Model)
            if 'isnet' in self.model_paths:
                try:
                    isnet_model = RealISNetModel.from_checkpoint(
                        onnx_path=self.model_paths['isnet']
                    )
                    if isnet_model.is_loaded:
                        self.ai_models['isnet'] = isnet_model
                        self.models_loading_status['isnet'] = True
                        logger.info("âœ… ISNet ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ ISNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # í´ë°± ëª¨ë¸ ìƒì„± (ì‹¤ì œ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
            if not self.ai_models:
                logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ë”ë¯¸ ëª¨ë¸ ìƒì„±")
                try:
                    dummy_u2net = RealU2NetClothModel(in_ch=3, out_ch=1).to(self.device)
                    dummy_u2net.eval()
                    self.ai_models['u2net_cloth'] = dummy_u2net
                    self.models_loading_status['u2net_cloth'] = True
                    logger.info("âœ… ë”ë¯¸ U2Net ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ ë”ë¯¸ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            
            loaded_count = sum(self.models_loading_status.values())
            total_models = len(self.models_loading_status)
            logger.info(f"ğŸ§  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
        if 'mobile_sam' in self.ai_models:
            methods.append(SegmentationMethod.MOBILE_SAM)
        if 'isnet' in self.ai_models:
            methods.append(SegmentationMethod.ISNET)
        
        if methods:
            methods.append(SegmentationMethod.AUTO_AI)
        
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
        
        return methods
    
    # ==============================================
    # ğŸ”¥ 10. í•µì‹¬: _run_ai_inference() ë©”ì„œë“œ (BaseStepMixin v19.1 í˜¸í™˜)
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ìˆœìˆ˜ AI ì¶”ë¡  ë¡œì§ (ë™ê¸° ì²˜ë¦¬) - BaseStepMixin v19.1ì—ì„œ í˜¸ì¶œë¨
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ ì…ë ¥
                - 'image': ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (PIL.Image ë˜ëŠ” np.ndarray)
                - 'from_step_01': Step 01 ê²°ê³¼ (íŒŒì‹± ì •ë³´)
                - 'from_step_02': Step 02 ê²°ê³¼ (í¬ì¦ˆ ì •ë³´)
                - ê¸°íƒ€ DetailedDataSpecì— ì •ì˜ëœ ì…ë ¥
        
        Returns:
            AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ (BaseStepMixinì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} AI ì¶”ë¡  ì‹œì‘")
            start_time = time.time()
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„° 'image'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            image = processed_input['image']
            
            # 2. ì´ì „ Step ë°ì´í„° í™œìš©
            person_parsing = processed_input.get('from_step_01', {})
            pose_info = processed_input.get('from_step_02', {})
            
            # 3. ì˜ë¥˜ íƒ€ì… ê°ì§€ (AI ê¸°ë°˜)
            clothing_type = self._detect_clothing_type_ai(image, processed_input.get('clothing_type'))
            
            # 4. í’ˆì§ˆ ë ˆë²¨ ê²°ì •
            quality_level = self._determine_quality_level(processed_input)
            
            # 5. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            self.logger.info("ğŸ§  ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
            mask, confidence, method_used = self._run_ai_segmentation(
                image, clothing_type, quality_level, person_parsing, pose_info
            )
            
            if mask is None:
                raise RuntimeError("AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨")
            
            # 6. AI ê¸°ë°˜ í›„ì²˜ë¦¬
            final_mask = self._postprocess_mask_ai(mask, quality_level)
            
            # 7. ì‹œê°í™” ìƒì„± (ì„¤ì •ëœ ê²½ìš°)
            visualizations = {}
            if self.segmentation_config.enable_visualization:
                visualizations = self._create_ai_visualizations(image, final_mask, clothing_type)
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_ai_stats(method_used, confidence, processing_time)
            
            # 9. ì›ì‹œ AI ê²°ê³¼ ë°˜í™˜ (BaseStepMixinì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
            ai_result = {
                'cloth_mask': final_mask,
                'segmented_clothing': self._apply_mask_to_image(image, final_mask),
                'confidence': confidence,
                'clothing_type': clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type),
                'method_used': method_used,
                'processing_time': processing_time,
                'quality_score': confidence * 0.9,
                'mask_area_ratio': np.sum(final_mask > 0) / final_mask.size if NUMPY_AVAILABLE else 0.0,
                'boundary_smoothness': self._calculate_boundary_smoothness(final_mask),
                
                # ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                **visualizations,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'ai_models_used': list(self.ai_models.keys()),
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'opencv_replaced': True,
                    'ai_inference': True,
                    'step_model_requests_compatible': True
                },
                
                # Step ê°„ ì—°ë™ì„ ìœ„í•œ ì¶”ê°€ ë°ì´í„°
                'cloth_features': self._extract_cloth_features(final_mask),
                'cloth_contours': self._extract_cloth_contours(final_mask),
                'clothing_category': self._classify_cloth_category(final_mask, clothing_type)
            }
            
            self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ - {processing_time:.2f}ì´ˆ")
            self.logger.info(f"   - ë°©ë²•: {method_used}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {confidence:.3f}")
            self.logger.info(f"   - ì˜ë¥˜ íƒ€ì…: {clothing_type}")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                'cloth_mask': None,
                'segmented_clothing': None,
                'confidence': 0.0,
                'clothing_type': 'error',
                'method_used': 'error',
                'error': str(e)
            }
    
    # ==============================================
    # ğŸ”¥ 11. AI ì¶”ë¡  ë©”ì„œë“œë“¤ (ì™„ì „í•œ êµ¬í˜„)
    # ==============================================
    
    def _detect_clothing_type_ai(self, image, hint: Optional[str] = None) -> ClothingType:
        """AI ê¸°ë°˜ ì˜ë¥˜ íƒ€ì… ê°ì§€"""
        try:
            if hint:
                try:
                    return ClothingType(hint.lower())
                except ValueError:
                    pass
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                width, height = image.size
                aspect_ratio = height / width
                
                if aspect_ratio > 1.5:
                    return ClothingType.DRESS
                elif aspect_ratio > 1.2:
                    return ClothingType.SHIRT
                else:
                    return ClothingType.PANTS
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                height, width = image.shape[:2]
                aspect_ratio = height / width
                
                if aspect_ratio > 1.5:
                    return ClothingType.DRESS
                elif aspect_ratio > 1.2:
                    return ClothingType.SHIRT
                else:
                    return ClothingType.PANTS
            
            return ClothingType.UNKNOWN
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì˜ë¥˜ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return ClothingType.UNKNOWN
    
    def _determine_quality_level(self, processed_input: Dict[str, Any]) -> QualityLevel:
        """í’ˆì§ˆ ë ˆë²¨ ê²°ì •"""
        try:
            # ëª…ì‹œì  ì§€ì •ì´ ìˆëŠ” ê²½ìš°
            if 'quality_level' in processed_input:
                quality_str = processed_input['quality_level']
                if isinstance(quality_str, str):
                    try:
                        return QualityLevel(quality_str.lower())
                    except ValueError:
                        pass
            
            # ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ ìë™ ê²°ì •
            image = processed_input.get('image')
            if image:
                if PIL_AVAILABLE and isinstance(image, Image.Image):
                    width, height = image.size
                    total_pixels = width * height
                elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                    height, width = image.shape[:2]
                    total_pixels = width * height
                else:
                    total_pixels = 512 * 512
                
                if total_pixels > 1024 * 1024:  # > 1MP
                    return QualityLevel.HIGH
                elif total_pixels > 512 * 512:  # > 0.25MP
                    return QualityLevel.BALANCED
                else:
                    return QualityLevel.FAST
            
            return self.segmentation_config.quality_level
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë ˆë²¨ ê²°ì • ì‹¤íŒ¨: {e}")
            return QualityLevel.BALANCED
    
    def _run_ai_segmentation(
        self, 
        image, 
        clothing_type: ClothingType, 
        quality_level: QualityLevel,
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Tuple[Optional[np.ndarray], float, str]:
        """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
        try:
            # í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ì„ íƒ
            ai_methods = self._get_ai_methods_by_quality(quality_level)
            
            for method in ai_methods:
                try:
                    self.logger.info(f"ğŸ§  AI ë°©ë²• ì‹œë„: {method.value}")
                    mask, confidence = self._run_ai_method(method, image, clothing_type)
                    
                    if mask is not None:
                        self.ai_stats['ai_model_calls'] += 1
                        
                        # ë°©ë²•ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                        if 'sam_huge' in method.value:
                            self.ai_stats['sam_huge_calls'] += 1
                        elif 'u2net' in method.value:
                            self.ai_stats['u2net_calls'] += 1
                        elif 'mobile_sam' in method.value:
                            self.ai_stats['mobile_sam_calls'] += 1
                        elif 'isnet' in method.value:
                            self.ai_stats['isnet_calls'] += 1
                        elif 'hybrid' in method.value:
                            self.ai_stats['hybrid_calls'] += 1
                        
                        self.logger.info(f"âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {method.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return mask, confidence, method.value
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ë°©ë²• {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±
            self.logger.warning("âš ï¸ ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨, ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±")
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                width, height = image.size
                dummy_mask = np.ones((height, width), dtype=np.uint8) * 128
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                height, width = image.shape[:2]
                dummy_mask = np.ones((height, width), dtype=np.uint8) * 128
            else:
                dummy_mask = np.ones((1024, 1024), dtype=np.uint8) * 128
            
            return dummy_mask, 0.5, "fallback_dummy"
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, 0.0, "error"
    
    def _get_ai_methods_by_quality(self, quality: QualityLevel) -> List[SegmentationMethod]:
        """í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ìš°ì„ ìˆœìœ„"""
        available_methods = [
            method for method in self.available_methods
            if method not in [SegmentationMethod.AUTO_AI]
        ]
        
        if quality == QualityLevel.ULTRA:
            priority = [
                SegmentationMethod.HYBRID_AI,
                SegmentationMethod.SAM_HUGE,
                SegmentationMethod.U2NET_CLOTH,
                SegmentationMethod.ISNET,
                SegmentationMethod.MOBILE_SAM,
            ]
        elif quality == QualityLevel.HIGH:
            priority = [
                SegmentationMethod.SAM_HUGE,
                SegmentationMethod.U2NET_CLOTH,
                SegmentationMethod.HYBRID_AI,
                SegmentationMethod.ISNET,
            ]
        elif quality == QualityLevel.BALANCED:
            priority = [
                SegmentationMethod.U2NET_CLOTH,
                SegmentationMethod.SAM_HUGE,
                SegmentationMethod.ISNET,
            ]
        else:  # FAST
            priority = [
                SegmentationMethod.MOBILE_SAM,
                SegmentationMethod.U2NET_CLOTH,
            ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ë§Œ ë°˜í™˜
        result = []
        for method in priority:
            if method in available_methods:
                result.append(method)
        
        return result
    
    def _run_ai_method(
        self,
        method: SegmentationMethod,
        image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ê°œë³„ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤í–‰"""
        
        if method == SegmentationMethod.SAM_HUGE:
            return self._run_sam_huge_inference(image, clothing_type)
        elif method == SegmentationMethod.U2NET_CLOTH:
            return self._run_u2net_cloth_inference(image)
        elif method == SegmentationMethod.MOBILE_SAM:
            return self._run_mobile_sam_inference(image)
        elif method == SegmentationMethod.ISNET:
            return self._run_isnet_inference(image)
        elif method == SegmentationMethod.HYBRID_AI:
            return self._run_hybrid_ai_inference(image, clothing_type)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ë°©ë²•: {method}")
    
    def _run_sam_huge_inference(self, image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """SAM Huge ì‹¤ì œ AI ì¶”ë¡  (sam_vit_h_4b8939.pth 2445.7MB)"""
        try:
            if 'sam_huge' not in self.ai_models:
                raise RuntimeError("âŒ SAM Huge ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            sam_model = self.ai_models['sam_huge']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                image_array = np.array(image)
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                image_array = image
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ğŸ”¥ ì‹¤ì œ SAM Huge AI ì¶”ë¡  (Primary Model)
            clothing_results = sam_model.segment_clothing(image_array, clothing_type.value)
            
            if not clothing_results:
                # ê¸°ë³¸ ì¤‘ì•™ í¬ì¸íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„
                height, width = image_array.shape[:2]
                center_points = [[width//2, height//2]]
                
                sam_model.predictor.set_image(image_array)
                masks, scores, logits = sam_model.predictor.predict(
                    point_coords=np.array(center_points),
                    point_labels=np.ones(len(center_points)),
                    multimask_output=True
                )
                
                best_mask_idx = np.argmax(scores)
                mask = masks[best_mask_idx].astype(np.uint8)
                confidence = float(scores[best_mask_idx])
            else:
                # ì˜ë¥˜ë³„ ë§ˆìŠ¤í¬ ì¡°í•©
                combined_mask = np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
                total_confidence = 0.0
                
                for cloth_area, area_mask in clothing_results.items():
                    combined_mask = np.logical_or(combined_mask, area_mask).astype(np.uint8)
                    total_confidence += np.sum(area_mask) / area_mask.size
                
                mask = combined_mask
                confidence = min(total_confidence / len(clothing_results), 1.0)
            
            self.logger.info(f"âœ… SAM Huge AI ì¶”ë¡  ì™„ë£Œ (Primary Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ SAM Huge AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    def _run_u2net_cloth_inference(self, image) -> Tuple[Optional[np.ndarray], float]:
        """U2Net Cloth ì‹¤ì œ AI ì¶”ë¡  (u2net.pth 168.1MB Alternative)"""
        try:
            if 'u2net_cloth' not in self.ai_models:
                raise RuntimeError("âŒ U2Net Cloth ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['u2net_cloth']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # step_model_requests.py í˜¸í™˜ ì „ì²˜ë¦¬
            if STEP_REQUIREMENTS and STEP_REQUIREMENTS.data_spec.normalization_mean:
                mean = STEP_REQUIREMENTS.data_spec.normalization_mean
                std = STEP_REQUIREMENTS.data_spec.normalization_std
            else:
                mean = (0.485, 0.456, 0.406)
                std = (0.229, 0.224, 0.225)
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std)
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ U2Net Cloth AI ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if self.is_m3_max and self.segmentation_config.use_fp16:
                    with torch.autocast(device_type='cpu'):
                        output = model(input_tensor)
                else:
                    output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬ (d0, d1, d2, d3, d4, d5, d6)
                if isinstance(output, tuple):
                    main_output = output[0]  # d0 (ìµœì¢… ì¶œë ¥)
                else:
                    main_output = output
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if main_output.max() > 1.0:
                    prob_map = torch.sigmoid(main_output)
                else:
                    prob_map = main_output
                
                threshold = self.segmentation_config.confidence_threshold
                mask = (prob_map > threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.max().item())
            
            self.logger.info(f"âœ… U2Net Cloth AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net Cloth AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    def _run_mobile_sam_inference(self, image) -> Tuple[Optional[np.ndarray], float]:
        """Mobile SAM ì‹¤ì œ AI ì¶”ë¡  (mobile_sam.pt 38.8MB Alternative)"""
        try:
            if 'mobile_sam' not in self.ai_models:
                raise RuntimeError("âŒ Mobile SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['mobile_sam']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ Mobile SAM AI ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, tuple):
                    output = output[0]
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if output.max() > 1.0:
                    prob_map = torch.sigmoid(output)
                else:
                    prob_map = output
                
                threshold = self.segmentation_config.confidence_threshold
                mask = (prob_map > threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.mean().item())  # Mobile SAMì€ í‰ê·  ì‹ ë¢°ë„ ì‚¬ìš©
            
            self.logger.info(f"âœ… Mobile SAM AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ Mobile SAM AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    def _run_isnet_inference(self, image) -> Tuple[Optional[np.ndarray], float]:
        """ISNet ì‹¤ì œ AI ì¶”ë¡  (isnetis.onnx 168.1MB Alternative)"""
        try:
            if 'isnet' not in self.ai_models:
                raise RuntimeError("âŒ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            isnet_model = self.ai_models['isnet']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                image_array = np.array(image)
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                image_array = image
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ğŸ”¥ ì‹¤ì œ ISNet ONNX AI ì¶”ë¡ 
            mask = isnet_model.predict(image_array)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë§ˆìŠ¤í¬ í’ˆì§ˆ ê¸°ë°˜)
            if mask is not None:
                confidence = np.sum(mask > 0) / mask.size
                confidence = min(confidence * 1.2, 1.0)  # ISNetì€ ê³ ì •ë°€ì´ë¯€ë¡œ ì‹ ë¢°ë„ í–¥ìƒ
                
                # ì´ì§„í™”
                threshold = self.segmentation_config.confidence_threshold
                mask = (mask > (threshold * 255)).astype(np.uint8)
            else:
                confidence = 0.0
            
            self.logger.info(f"âœ… ISNet AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ ISNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    def _run_hybrid_ai_inference(self, image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """HYBRID AI ì¶”ë¡  (ëª¨ë“  AI ëª¨ë¸ ì¡°í•©)"""
        try:
            self.logger.info("ğŸ”„ HYBRID AI ì¶”ë¡  ì‹œì‘ (ëª¨ë“  ëª¨ë¸ í™œìš©)...")
            
            masks = []
            confidences = []
            methods_used = []
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•ë“¤ë¡œ ì¶”ë¡  ì‹¤í–‰
            available_ai_methods = [
                method for method in self.available_methods 
                if method not in [SegmentationMethod.AUTO_AI, SegmentationMethod.HYBRID_AI]
            ]
            
            # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°©ë²•ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if len(available_ai_methods) < 2:
                raise RuntimeError("âŒ HYBRID ë°©ë²•ì€ ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ë°©ë²•ì´ í•„ìš”")
            
            # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
            for method in available_ai_methods:
                try:
                    mask, confidence = self._run_ai_method(method, image, clothing_type)
                    if mask is not None:
                        masks.append(mask.astype(np.float32))
                        confidences.append(confidence)
                        methods_used.append(method.value)
                        self.logger.info(f"âœ… HYBRID - {method.value} ì¶”ë¡  ì™„ë£Œ: {confidence:.3f}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HYBRID - {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            if not masks:
                raise RuntimeError("âŒ HYBRID - ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
            
            # ğŸ”¥ ê³ ê¸‰ ë§ˆìŠ¤í¬ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê·  + AI ê¸°ë°˜ í›„ì²˜ë¦¬)
            if len(masks) == 1:
                combined_mask = masks[0]
                combined_confidence = confidences[0]
            else:
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
                weights = np.array(confidences)
                weights = weights / np.sum(weights)  # ì •ê·œí™”
                
                # ë§ˆìŠ¤í¬ë“¤ì„ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤
                target_shape = masks[0].shape
                normalized_masks = []
                for mask in masks:
                    if mask.shape != target_shape:
                        # AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì¦ˆ ì‚¬ìš©
                        mask_image = Image.fromarray(mask.astype(np.uint8))
                        resized_mask = AIImageProcessor.ai_resize(mask_image, target_shape[::-1])
                        mask_resized = np.array(resized_mask).astype(np.float32)
                        normalized_masks.append(mask_resized)
                    else:
                        normalized_masks.append(mask.astype(np.float32))
                
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                combined_mask_float = np.zeros_like(normalized_masks[0])
                for mask, weight in zip(normalized_masks, weights):
                    combined_mask_float += mask * weight
                
                # AI ê¸°ë°˜ ì„ê³„ê°’ ì ìš©
                threshold = np.mean(combined_mask_float) + np.std(combined_mask_float) * 0.5
                combined_mask = (combined_mask_float > threshold).astype(np.float32)
                combined_confidence = float(np.mean(confidences))
            
            # AI ê¸°ë°˜ í›„ì²˜ë¦¬ (OpenCV ëŒ€ì²´)
            final_mask = AIImageProcessor.ai_morphology(
                (combined_mask * 255).astype(np.uint8), 
                "closing", 
                5
            )
            
            # ìµœì¢… ì´ì§„í™”
            final_mask = (final_mask > 128).astype(np.uint8)
            
            self.logger.info(f"âœ… HYBRID AI ì¶”ë¡  ì™„ë£Œ - ë°©ë²•: {methods_used} - ì‹ ë¢°ë„: {combined_confidence:.3f}")
            return final_mask, combined_confidence
            
        except Exception as e:
            self.logger.error(f"âŒ HYBRID AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    # ==============================================
    # ğŸ”¥ 12. AI ê¸°ë°˜ í›„ì²˜ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _postprocess_mask_ai(self, mask: np.ndarray, quality: QualityLevel) -> np.ndarray:
        """AI ê¸°ë°˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            processed_mask = mask.copy()
            
            # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            if self.segmentation_config.remove_noise:
                kernel_size = 3 if quality == QualityLevel.FAST else 5
                processed_mask = AIImageProcessor.ai_morphology(processed_mask, "opening", kernel_size)
                processed_mask = AIImageProcessor.ai_morphology(processed_mask, "closing", kernel_size)
            
            # í™€ ì±„ìš°ê¸° (AI ê¸°ë°˜)
            if self.segmentation_config.enable_hole_filling:
                processed_mask = self._fill_holes_ai(processed_mask)
            
            # ê²½ê³„ ê°œì„  (AI ê¸°ë°˜)
            if self.segmentation_config.enable_edge_refinement:
                processed_mask = self._refine_edges_ai(processed_mask)
            
            return processed_mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask
    
    def _fill_holes_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ í™€ ì±„ìš°ê¸°"""
        try:
            if not TORCH_AVAILABLE:
                return mask
            
            # PyTorch ê¸°ë°˜ í™€ ì±„ìš°ê¸°
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            
            # í˜•íƒœí•™ì  ë‹«í˜ ì—°ì‚°ìœ¼ë¡œ í™€ ì±„ìš°ê¸°
            kernel_size = 7
            filled = AIImageProcessor.ai_morphology((tensor.squeeze().numpy() * 255).astype(np.uint8), "closing", kernel_size)
            
            return filled
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask
    
    def _refine_edges_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ê²½ê³„ ê°œì„ """
        try:
            if self.segmentation_config.enable_edge_refinement:
                # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
                edges = AIImageProcessor.ai_detect_edges(mask, 50, 150)
                
                # ê²½ê³„ ì£¼ë³€ ì˜ì—­ í™•ì¥
                edge_region = AIImageProcessor.ai_morphology(edges, "dilation", 3)
                
                # í•´ë‹¹ ì˜ì—­ì— AI ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš© (ê°„ë‹¨ êµ¬í˜„)
                refined_mask = mask.copy().astype(np.float32)
                
                # ê°„ë‹¨í•œ ë¸”ëŸ¬ë§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ AI ê¸°ë°˜ êµ¬í˜„ í•„ìš”)
                if edge_region.sum() > 0:
                    refined_mask[edge_region > 0] = (refined_mask[edge_region > 0] * 0.8 + 128 * 0.2)
                
                return (refined_mask > 128).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ê²½ê³„ ê°œì„  ì‹¤íŒ¨: {e}")
            return mask
    
    # ==============================================
    # ğŸ”¥ 13. ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _create_ai_visualizations(self, image, mask: np.ndarray, clothing_type: ClothingType) -> Dict[str, Any]:
        """AI ê°•í™” ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE or not NUMPY_AVAILABLE:
                return {}
            
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            clothing_colors = {
                'shirt': (255, 100, 100),
                'pants': (100, 100, 255),
                'dress': (255, 100, 255),
                'jacket': (100, 255, 100),
                'skirt': (255, 255, 100),
                'unknown': (128, 128, 128),
            }
            
            color = clothing_colors.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type),
                clothing_colors['unknown']
            )
            
            # 1. ë§ˆìŠ¤í¬ ì´ë¯¸ì§€
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            visualizations['mask_image'] = Image.fromarray(mask_colored)
            
            # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                image_array = np.array(image)
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                image_array = image
            else:
                return visualizations
            
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê¸°ë°˜ ê²½ê³„ì„  ì¶”ê°€
            boundary = AIImageProcessor.ai_detect_edges(mask.astype(np.uint8))
            overlay[boundary > 0] = (255, 255, 255)
            
            visualizations['overlay_image'] = Image.fromarray(overlay)
            
            # 3. ê²½ê³„ì„  ì´ë¯¸ì§€
            boundary_overlay = image_array.copy()
            boundary_overlay[boundary > 0] = (255, 255, 255)
            visualizations['boundary_image'] = Image.fromarray(boundary_overlay)
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _apply_mask_to_image(self, image, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©"""
        try:
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                image_array = np.array(image)
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                image_array = image
            else:
                return np.zeros((512, 512, 3), dtype=np.uint8)
            
            if len(mask.shape) == 2:
                mask_3d = np.stack([mask, mask, mask], axis=2)
            else:
                mask_3d = mask
            
            # ë§ˆìŠ¤í¬ ì •ê·œí™”
            mask_normalized = mask_3d.astype(np.float32) / 255.0
            
            # ë°°ê²½ì„ íˆ¬ëª…í•˜ê²Œ ë§Œë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€
            segmented = image_array.astype(np.float32) * mask_normalized
            
            return segmented.astype(np.uint8)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                return np.array(image)
            else:
                return image
    
    def _calculate_boundary_smoothness(self, mask: np.ndarray) -> float:
        """ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚°"""
        try:
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œë¡œ ê²½ê³„ í’ˆì§ˆ ì¸¡ì •
            edges = AIImageProcessor.ai_detect_edges(mask)
            edge_pixels = np.sum(edges > 0)
            total_boundary = np.sum(mask > 0)
            
            if total_boundary > 0:
                smoothness = 1.0 - (edge_pixels / total_boundary)
                return max(0.0, min(1.0, smoothness))
            else:
                return 0.0
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_cloth_features(self, mask: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            features = {}
            
            if NUMPY_AVAILABLE and isinstance(mask, np.ndarray):
                # ê¸°ë³¸ í†µê³„
                features['area'] = int(np.sum(mask > 0))
                features['bounding_box'] = self._get_bounding_box(mask)
                features['centroid'] = self._get_centroid(mask)
                features['aspect_ratio'] = self._get_aspect_ratio(mask)
                
            return features
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _extract_cloth_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """ì˜ë¥˜ ìœ¤ê³½ì„  ì¶”ì¶œ"""
        try:
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œì„ ì‚¬ìš©í•œ ìœ¤ê³½ì„  ì¶”ì¶œ
            edges = AIImageProcessor.ai_detect_edges(mask)
            
            contours = []
            if np.any(edges > 0):
                y_coords, x_coords = np.where(edges > 0)
                if len(y_coords) > 0:
                    contour = np.column_stack((x_coords, y_coords))
                    contours.append(contour)
            
            return contours
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _classify_cloth_category(self, mask: np.ndarray, clothing_type: ClothingType) -> str:
        """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        try:
            # ê¸°ë³¸ì ìœ¼ë¡œ ê°ì§€ëœ íƒ€ì… ë°˜í™˜
            if hasattr(clothing_type, 'value'):
                return clothing_type.value
            else:
                return str(clothing_type)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _get_bounding_box(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        try:
            rows = np.any(mask > 0, axis=1)
            cols = np.any(mask > 0, axis=0)
            
            if not np.any(rows) or not np.any(cols):
                return (0, 0, 0, 0)
            
            rmin, rmax = np.where(rows)[0][[0, -1]]
            cmin, cmax = np.where(cols)[0][[0, -1]]
            
            return (int(cmin), int(rmin), int(cmax), int(rmax))
            
        except Exception:
            return (0, 0, 0, 0)
    
    def _get_centroid(self, mask: np.ndarray) -> Tuple[float, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            y_coords, x_coords = np.where(mask > 0)
            if len(y_coords) > 0:
                centroid_x = float(np.mean(x_coords))
                centroid_y = float(np.mean(y_coords))
                return (centroid_x, centroid_y)
            else:
                return (0.0, 0.0)
                
        except Exception:
            return (0.0, 0.0)
    
    def _get_aspect_ratio(self, mask: np.ndarray) -> float:
        """ì¢…íš¡ë¹„ ê³„ì‚°"""
        try:
            x1, y1, x2, y2 = self._get_bounding_box(mask)
            width = x2 - x1
            height = y2 - y1
            
            if width > 0:
                return height / width
            else:
                return 1.0
                
        except Exception:
            return 1.0
    
    # ==============================================
    # ğŸ”¥ 14. í†µê³„ ë° ìƒíƒœ ê´€ë¦¬
    # ==============================================
    
    def _update_ai_stats(self, method_used: str, confidence: float, processing_time: float):
        """AI í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.ai_stats['total_processed'] += 1
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            total = self.ai_stats['total_processed']
            current_avg = self.ai_stats['average_confidence']
            self.ai_stats['average_confidence'] = (
                (current_avg * (total - 1) + confidence) / total
            )
            
            self.logger.debug(f"ğŸ“Š AI í†µê³„ ì—…ë°ì´íŠ¸: {method_used}, ì‹ ë¢°ë„: {confidence:.3f}, ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ (BaseStepMixin í˜¸í™˜)"""
        try:
            base_status = super().get_status() if hasattr(super(), 'get_status') else {}
            
            ai_status = {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'ai_models_loaded': list(self.ai_models.keys()),
                'ai_models_status': self.models_loading_status.copy(),
                'available_methods': [m.value for m in self.available_methods],
                'ai_stats': self.ai_stats.copy(),
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'device': self.device,
                'opencv_replaced': True,
                'ai_inference': True,
                'step_model_requests_compatible': True,
                'basestepmixin_v19_compatible': True
            }
            
            return {**base_status, **ai_status}
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ 15. ì •ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
    # ==============================================
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    elif hasattr(model, 'sam_model') and model.sam_model and hasattr(model.sam_model, 'cpu'):
                        model.sam_model.cpu()
                    del model
                    self.logger.debug(f"ğŸ§¹ {model_name} AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.ai_models.clear()
            self.model_paths.clear()
            self.models_loading_status = {k: False for k in self.models_loading_status.keys()}
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.segmentation_cache.clear()
            
            # ì‹¤í–‰ì ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # BaseStepMixin ì •ë¦¬ (ìˆëŠ” ê²½ìš°)
            if hasattr(super(), 'cleanup'):
                super().cleanup()
            
            self.is_initialized = False
            self.is_ready = False
            
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

# ==============================================
# ğŸ”¥ 16. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„±"""
    m3_config = {
        'method': SegmentationMethod.HYBRID_AI,
        'quality_level': QualityLevel.ULTRA,
        'use_fp16': True,
        'enable_visualization': True,
        'enable_edge_refinement': True,
        'enable_hole_filling': True,
        'input_size': (1024, 1024),
        'confidence_threshold': 0.5
    }
    
    if 'config' in kwargs:
        kwargs['config'].update(m3_config)
    else:
        kwargs['config'] = m3_config
    
    return ClothSegmentationStep(**kwargs)

# ==============================================
# ğŸ”¥ 17. ëª¨ë“ˆ ì •ë³´
# ==============================================

__version__ = "22.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™”"
__compatibility_version__ = "BaseStepMixin_v19.1"

__all__ = [
    'ClothSegmentationStep',
    'RealSAMModel',
    'RealU2NetClothModel', 
    'RealMobileSAMModel',
    'RealISNetModel',
    'AIImageProcessor',
    'SegmentationMethod',
    'ClothingType',
    'QualityLevel',
    'SegmentationConfig',
    'create_cloth_segmentation_step',
    'create_m3_max_segmentation_step'
]

# ==============================================
# ğŸ”¥ 18. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger.info("=" * 120)
logger.info("ğŸ”¥ Step 03 Cloth Segmentation v22.0 - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™”")
logger.info("=" * 120)
logger.info("ğŸ¯ BaseStepMixin v19.1 ì™„ì „ ì¤€ìˆ˜:")
logger.info("   âœ… _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ (ë™ê¸° ì²˜ë¦¬)")
logger.info("   âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì€ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬")
logger.info("   âœ… step_model_requests.py DetailedDataSpec ì™„ì „ í™œìš©")
logger.info("   âœ… GitHub í”„ë¡œì íŠ¸ 100% í˜¸í™˜ì„± ë³´ì¥")
logger.info("ğŸ§  AI ê°•í™” ì‚¬í•­ (100% ë³´ì¡´):")
logger.info("   âœ… ì‹¤ì œ SAM, U2Net, ISNet, Mobile SAM AI ì¶”ë¡  ë¡œì§")
logger.info("   âœ… OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬")
logger.info("   âœ… AI ê°•í™” ì‹œê°í™” (Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§)")
logger.info("   âœ… M3 Max MPS ê°€ì† ë° 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("   âœ… ì‹¤ì œ ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (2445.7MB SAM)")
logger.info("   âœ… í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë° í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”")
logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - SAM: {SAM_AVAILABLE}")
logger.info(f"   - ONNX: {ONNX_AVAILABLE}")

if STEP_REQUIREMENTS:
    logger.info("âœ… step_model_requests.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì„±ê³µ")
    logger.info(f"   - ëª¨ë¸ëª…: {STEP_REQUIREMENTS.model_name}")
    logger.info(f"   - Primary íŒŒì¼: {STEP_REQUIREMENTS.primary_file} ({STEP_REQUIREMENTS.primary_size_mb}MB)")

logger.info("=" * 120)
logger.info("ğŸ‰ ClothSegmentationStep BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + AI ê°•í™” ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ’¡ ì´ì œ _run_ai_inference() ë©”ì„œë“œë§Œìœ¼ë¡œ ëª¨ë“  AI ì¶”ë¡ ì´ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ëª¨ë“  ë°ì´í„° ë³€í™˜ì€ BaseStepMixinì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("=" * 120)