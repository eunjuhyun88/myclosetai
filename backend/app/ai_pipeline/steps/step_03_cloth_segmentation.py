"""
3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation) - ë°°ê²½ ì œê±°
UÂ²-Net, rembg, ë˜ëŠ” ì»¤ìŠ¤í…€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì‚¬ìš©
"""
import os
import logging
import time
from typing import Dict, Any, Optional, Tuple
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from PIL import Image

# rembg ì„í¬íŠ¸ (ì‹¤ì œ êµ¬í˜„)
try:
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
except ImportError:
    REMBG_AVAILABLE = False
    logging.warning("rembgë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ëª¨ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)

class ClothSegmentationStep:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… - ë°°ê²½ ì œê±° ë° ì˜ë¥˜ ì˜ì—­ ë¶„í• """
    
    # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬
    CLOTHING_CATEGORIES = {
        'upper': ['shirt', 't-shirt', 'blouse', 'sweater', 'jacket', 'coat', 'dress'],
        'lower': ['pants', 'jeans', 'skirt', 'shorts', 'trousers'],
        'full': ['dress', 'jumpsuit', 'overall'],
        'accessories': ['hat', 'scarf', 'gloves', 'shoes', 'bag']
    }
    
    def __init__(self, model_loader, device: str, config: Dict[str, Any] = None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.model_loader = model_loader
        self.device = device
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.model_type = self.config.get('model_type', 'rembg')  # 'rembg', 'u2net', 'custom'
        self.input_size = self.config.get('input_size', (320, 320))
        self.confidence_threshold = self.config.get('confidence_threshold', 0.5)
        
        # ëª¨ë¸ ê´€ë ¨
        self.segmentation_model = None
        self.rembg_session = None
        self.is_initialized = False
        
        logger.info(f"ğŸ¯ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ì´ˆê¸°í™” - ëª¨ë¸: {self.model_type}, ë””ë°”ì´ìŠ¤: {device}")
    
    async def initialize(self) -> bool:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            if self.model_type == 'rembg' and REMBG_AVAILABLE:
                # rembg ì„¸ì…˜ ì´ˆê¸°í™”
                self.rembg_session = self._initialize_rembg()
            elif self.model_type == 'u2net':
                # UÂ²-Net ëª¨ë¸ ë¡œë“œ
                self.segmentation_model = await self._initialize_u2net()
            else:
                # ì»¤ìŠ¤í…€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸
                self.segmentation_model = await self._initialize_custom_model()
            
            self.is_initialized = True
            logger.info("âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    def _initialize_rembg(self):
        """rembg ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            # ì˜ë¥˜ì— íŠ¹í™”ëœ ëª¨ë¸ ì„ íƒ
            model_name = self.config.get('rembg_model', 'u2net')  # u2net, silueta, etc.
            session = new_session(model_name)
            logger.info(f"âœ… rembg ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {model_name}")
            return session
        except Exception as e:
            logger.warning(f"rembg ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    async def _initialize_u2net(self):
        """UÂ²-Net ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” UÂ²-Net ëª¨ë¸ ë¡œë“œ
            model = self._create_u2net_model()
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜)
            model_path = self._get_u2net_model_path()
            if os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location=self.device)
                model.load_state_dict(checkpoint['model_state_dict'])
                logger.info(f"âœ… UÂ²-Net ê°€ì¤‘ì¹˜ ë¡œë“œ: {model_path}")
            else:
                logger.warning(f"âš ï¸ UÂ²-Net ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ: {model_path}")
            
            # ëª¨ë¸ ìµœì í™”
            model = self.model_loader.optimize_model(model, 'cloth_segmentation')
            model.eval()
            
            return model
            
        except Exception as e:
            logger.warning(f"UÂ²-Net ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return self._create_demo_segmentation_model()
    
    async def _initialize_custom_model(self):
        """ì»¤ìŠ¤í…€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™”"""
        return self._create_demo_segmentation_model()
    
    def _create_u2net_model(self):
        """UÂ²-Net ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class U2NetSegmentation(torch.nn.Module):
            def __init__(self, in_ch=3, out_ch=1):
                super(U2NetSegmentation, self).__init__()
                
                # ì¸ì½”ë”
                self.encoder1 = self._conv_block(in_ch, 64)
                self.encoder2 = self._conv_block(64, 128)
                self.encoder3 = self._conv_block(128, 256)
                self.encoder4 = self._conv_block(256, 512)
                
                # ì¤‘ê°„ ë ˆì´ì–´
                self.middle = self._conv_block(512, 1024)
                
                # ë””ì½”ë”
                self.decoder4 = self._conv_block(1024 + 512, 512)
                self.decoder3 = self._conv_block(512 + 256, 256)
                self.decoder2 = self._conv_block(256 + 128, 128)
                self.decoder1 = self._conv_block(128 + 64, 64)
                
                # ì¶œë ¥ ë ˆì´ì–´
                self.final = torch.nn.Conv2d(64, out_ch, 1)
                
                # í’€ë§ ë° ì—…ìƒ˜í”Œë§
                self.pool = torch.nn.MaxPool2d(2)
                self.upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            
            def _conv_block(self, in_ch, out_ch):
                return torch.nn.Sequential(
                    torch.nn.Conv2d(in_ch, out_ch, 3, padding=1),
                    torch.nn.BatchNorm2d(out_ch),
                    torch.nn.ReLU(inplace=True),
                    torch.nn.Conv2d(out_ch, out_ch, 3, padding=1),
                    torch.nn.BatchNorm2d(out_ch),
                    torch.nn.ReLU(inplace=True)
                )
            
            def forward(self, x):
                # ì¸ì½”ë”
                e1 = self.encoder1(x)
                e2 = self.encoder2(self.pool(e1))
                e3 = self.encoder3(self.pool(e2))
                e4 = self.encoder4(self.pool(e3))
                
                # ì¤‘ê°„
                m = self.middle(self.pool(e4))
                
                # ë””ì½”ë”
                d4 = self.decoder4(torch.cat([self.upsample(m), e4], dim=1))
                d3 = self.decoder3(torch.cat([self.upsample(d4), e3], dim=1))
                d2 = self.decoder2(torch.cat([self.upsample(d3), e2], dim=1))
                d1 = self.decoder1(torch.cat([self.upsample(d2), e1], dim=1))
                
                # ì¶œë ¥
                output = torch.sigmoid(self.final(d1))
                
                return output
        
        return U2NetSegmentation().to(self.device)
    
    def _create_demo_segmentation_model(self):
        """ë°ëª¨ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
        class DemoSegmentationModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1)
                self.conv2 = torch.nn.Conv2d(64, 32, 3, padding=1)
                self.conv3 = torch.nn.Conv2d(32, 1, 1)
                
            def forward(self, x):
                x = torch.relu(self.conv1(x))
                x = torch.relu(self.conv2(x))
                x = torch.sigmoid(self.conv3(x))
                return x
        
        return DemoSegmentationModel().to(self.device)
    
    def _get_u2net_model_path(self) -> str:
        """UÂ²-Net ëª¨ë¸ íŒŒì¼ ê²½ë¡œ"""
        model_dir = self.config.get('model_dir', 'app/models/ai_models/u2net')
        model_file = self.config.get('model_file', 'u2net_cloth.pth')
        return os.path.join(model_dir, model_file)
    
    def process(self, clothing_image_tensor: torch.Tensor) -> Dict[str, Any]:
        """
        ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬
        
        Args:
            clothing_image_tensor: ì˜ë¥˜ ì´ë¯¸ì§€ í…ì„œ
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pil_image = self._tensor_to_pil(clothing_image_tensor)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
            if self.model_type == 'rembg' and self.rembg_session:
                segmented_image, mask = self._segment_with_rembg(pil_image)
            else:
                segmented_image, mask = self._segment_with_model(clothing_image_tensor)
            
            # ì˜ë¥˜ íƒ€ì… ë¶„ë¥˜
            clothing_type = self._classify_clothing_type(pil_image, mask)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_segmentation_quality(mask)
            
            # í›„ì²˜ë¦¬
            processed_mask = self._postprocess_mask(mask)
            refined_image = self._refine_segmentation(pil_image, processed_mask)
            
            processing_time = time.time() - start_time
            
            result = {
                "segmented_image": self._pil_to_base64(refined_image),
                "mask": self._array_to_base64(processed_mask),
                "raw_mask": self._array_to_base64(mask),
                "clothing_type": clothing_type,
                "quality_metrics": quality_metrics,
                "confidence": float(quality_metrics.get('confidence', 0.8)),
                "processing_time": processing_time,
                "background_removed": True,
                "mask_area_ratio": float(np.sum(processed_mask > 0) / processed_mask.size)
            }
            
            logger.info(f"âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}ì´ˆ, íƒ€ì…: {clothing_type}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """PyTorch í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # [1, 3, H, W] -> [3, H, W]
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        # [3, H, W] -> [H, W, 3]
        if tensor.shape[0] == 3:
            tensor = tensor.permute(1, 2, 0)
        
        # ì •ê·œí™” í•´ì œ
        if tensor.max() <= 1.0:
            tensor = tensor * 255
        
        # NumPyë¡œ ë³€í™˜
        array = tensor.cpu().numpy().astype(np.uint8)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        return Image.fromarray(array)
    
    def _segment_with_rembg(self, image: Image.Image) -> Tuple[Image.Image, np.ndarray]:
        """rembgë¥¼ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            # rembgë¡œ ë°°ê²½ ì œê±°
            if self.rembg_session:
                segmented = remove(image, session=self.rembg_session)
            else:
                segmented = remove(image)
            
            # ë§ˆìŠ¤í¬ ì¶”ì¶œ (ì•ŒíŒŒ ì±„ë„)
            if segmented.mode == 'RGBA':
                mask = np.array(segmented)[:, :, 3]  # ì•ŒíŒŒ ì±„ë„
                # RGBë¡œ ë³€í™˜
                segmented = segmented.convert('RGB')
            else:
                # ì•ŒíŒŒ ì±„ë„ì´ ì—†ëŠ” ê²½ìš° ì„ì‹œ ë§ˆìŠ¤í¬ ìƒì„±
                mask = np.ones((segmented.height, segmented.width), dtype=np.uint8) * 255
            
            return segmented, mask
            
        except Exception as e:
            logger.error(f"rembg ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ì „ì²´ ë§ˆìŠ¤í¬ ë°˜í™˜
            mask = np.ones((image.height, image.width), dtype=np.uint8) * 255
            return image, mask
    
    def _segment_with_model(self, image_tensor: torch.Tensor) -> Tuple[Image.Image, np.ndarray]:
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            # ì…ë ¥ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_for_model(image_tensor)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                mask_pred = self.segmentation_model(input_tensor)
                
                # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
                mask = mask_pred.squeeze().cpu().numpy()
                mask = (mask * 255).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            original_size = image_tensor.shape[2:]
            if mask.shape != original_size:
                mask = cv2.resize(mask, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ìƒì„±
            pil_image = self._tensor_to_pil(image_tensor)
            segmented_image = self._apply_mask_to_image(pil_image, mask)
            
            return segmented_image, mask
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë°ëª¨ ë§ˆìŠ¤í¬ ìƒì„±
            pil_image = self._tensor_to_pil(image_tensor)
            mask = self._create_demo_mask(pil_image)
            segmented_image = self._apply_mask_to_image(pil_image, mask)
            return segmented_image, mask
    
    def _preprocess_for_model(self, image_tensor: torch.Tensor) -> torch.Tensor:
        """ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì „ì²˜ë¦¬"""
        # í¬ê¸° ì¡°ì •
        if image_tensor.shape[2:] != self.input_size:
            image_tensor = F.interpolate(
                image_tensor, 
                size=self.input_size, 
                mode='bilinear', 
                align_corners=False
            )
        
        # ì •ê·œí™”
        if image_tensor.max() > 1.0:
            image_tensor = image_tensor / 255.0
        
        # ImageNet ì •ê·œí™”
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(self.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(self.device)
        
        normalized = (image_tensor - mean) / std
        
        return normalized
    
    def _create_demo_mask(self, image: Image.Image) -> np.ndarray:
        """ë°ëª¨ìš© ë§ˆìŠ¤í¬ ìƒì„± (ì¤‘ì•™ ì˜ì—­)"""
        w, h = image.size
        mask = np.zeros((h, w), dtype=np.uint8)
        
        # ì¤‘ì•™ ì˜ì—­ì„ ì˜ë¥˜ë¡œ ê°€ì •
        center_x, center_y = w // 2, h // 2
        radius_x, radius_y = w // 3, h // 3
        
        y, x = np.ogrid[:h, :w]
        center_mask = ((x - center_x) ** 2 / radius_x ** 2 + 
                      (y - center_y) ** 2 / radius_y ** 2) <= 1
        
        mask[center_mask] = 255
        
        return mask
    
    def _apply_mask_to_image(self, image: Image.Image, mask: np.ndarray) -> Image.Image:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©í•˜ì—¬ ë°°ê²½ ì œê±°"""
        # PIL ì´ë¯¸ì§€ë¥¼ RGBAë¡œ ë³€í™˜
        if image.mode != 'RGBA':
            image = image.convert('RGBA')
        
        # ë§ˆìŠ¤í¬ë¥¼ ì•ŒíŒŒ ì±„ë„ë¡œ ì‚¬ìš©
        image_array = np.array(image)
        image_array[:, :, 3] = mask  # ì•ŒíŒŒ ì±„ë„ ì„¤ì •
        
        # ë°°ê²½ì„ íˆ¬ëª…í•˜ê²Œ ë§Œë“  ì´ë¯¸ì§€ ìƒì„±
        segmented = Image.fromarray(image_array, 'RGBA')
        
        # RGB ë°°ê²½ (í°ìƒ‰)ìœ¼ë¡œ ë³€í™˜
        rgb_image = Image.new('RGB', image.size, (255, 255, 255))
        rgb_image.paste(segmented, mask=segmented.split()[3])
        
        return rgb_image
    
    def _classify_clothing_type(self, image: Image.Image, mask: np.ndarray) -> str:
        """ì˜ë¥˜ íƒ€ì… ë¶„ë¥˜"""
        try:
            # ë§ˆìŠ¤í¬ ì˜ì—­ ë¶„ì„
            h, w = mask.shape
            mask_coords = np.where(mask > 0)
            
            if len(mask_coords[0]) == 0:
                return "unknown"
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            y_min, y_max = mask_coords[0].min(), mask_coords[0].max()
            x_min, x_max = mask_coords[1].min(), mask_coords[1].max()
            
            # ì¢…íš¡ë¹„ ê³„ì‚°
            height_ratio = (y_max - y_min) / h
            width_ratio = (x_max - x_min) / w
            aspect_ratio = (y_max - y_min) / (x_max - x_min) if (x_max - x_min) > 0 else 1
            
            # ìœ„ì¹˜ ë¶„ì„
            vertical_center = (y_min + y_max) / 2 / h
            
            # ì˜ë¥˜ íƒ€ì… ë¶„ë¥˜ ê·œì¹™
            if height_ratio > 0.7 and aspect_ratio > 1.5:
                return "dress"  # ì „ì²´ ê¸¸ì´, ì„¸ë¡œë¡œ ê¸´ í˜•íƒœ
            elif vertical_center < 0.4 and height_ratio < 0.6:
                return "shirt"  # ìƒë‹¨ ìœ„ì¹˜, ë†’ì´ ì œí•œì 
            elif vertical_center > 0.6 and height_ratio < 0.6:
                return "pants"  # í•˜ë‹¨ ìœ„ì¹˜
            elif height_ratio > 0.8:
                return "full"  # ì „ì²´ ê¸¸ì´
            elif aspect_ratio > 2.0:
                return "skirt"  # ë§¤ìš° ì„¸ë¡œë¡œ ê¸´ í˜•íƒœ
            else:
                return "shirt"  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.warning(f"ì˜ë¥˜ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _evaluate_segmentation_quality(self, mask: np.ndarray) -> Dict[str, float]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€"""
        metrics = {}
        
        try:
            # ë§ˆìŠ¤í¬ ì˜ì—­ ë¹„ìœ¨
            mask_area = np.sum(mask > 0) / mask.size
            metrics["mask_area_ratio"] = float(mask_area)
            
            # ë§ˆìŠ¤í¬ ì—°ê²°ì„± ë¶„ì„
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            metrics["num_components"] = len(contours)
            
            # ê°€ì¥ í° ì—°ê²° ìš”ì†Œì˜ ë¹„ìœ¨
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                largest_area = cv2.contourArea(largest_contour)
                metrics["largest_component_ratio"] = float(largest_area / np.sum(mask > 0)) if np.sum(mask > 0) > 0 else 0
            else:
                metrics["largest_component_ratio"] = 0
            
            # ê²½ê³„ ë¶€ë“œëŸ¬ì›€ (ê²½ê³„ì„  ê¸¸ì´ ëŒ€ë¹„ ë©´ì )
            if contours:
                total_perimeter = sum(cv2.arcLength(c, True) for c in contours)
                total_area = np.sum(mask > 0)
                if total_perimeter > 0:
                    compactness = 4 * np.pi * total_area / (total_perimeter ** 2)
                    metrics["compactness"] = float(min(1.0, compactness))
                else:
                    metrics["compactness"] = 0
            else:
                metrics["compactness"] = 0
            
            # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            confidence = (
                metrics["mask_area_ratio"] * 0.3 +
                (1 / max(1, metrics["num_components"])) * 0.3 +
                metrics["largest_component_ratio"] * 0.2 +
                metrics["compactness"] * 0.2
            )
            metrics["confidence"] = float(min(1.0, max(0.0, confidence)))
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            metrics = {"confidence": 0.5, "mask_area_ratio": 0.5}
        
        return metrics
    
    def _postprocess_mask(self, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            # ë…¸ì´ì¦ˆ ì œê±°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ êµ¬ë© ë©”ìš°ê¸°
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            
            # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            # ê°€ì¥ìë¦¬ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê¸°
            mask = cv2.GaussianBlur(mask, (3, 3), 0)
            
            # ì´ì§„í™”
            _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
            
            return mask
            
        except Exception as e:
            logger.warning(f"ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask
    
    def _refine_segmentation(self, image: Image.Image, mask: np.ndarray) -> Image.Image:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ê°œì„ """
        try:
            # ë§ˆìŠ¤í¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
            soft_mask = cv2.GaussianBlur(mask, (5, 5), 0)
            soft_mask = soft_mask / 255.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            
            # ì›ë³¸ ì´ë¯¸ì§€ì— ë¶€ë“œëŸ¬ìš´ ë§ˆìŠ¤í¬ ì ìš©
            image_array = np.array(image)
            
            # ë°°ê²½ìƒ‰ ì„¤ì • (í°ìƒ‰)
            background = np.ones_like(image_array) * 255
            
            # ë¶€ë“œëŸ¬ìš´ ë¸”ë Œë”©
            refined_array = (image_array * soft_mask[:, :, np.newaxis] + 
                           background * (1 - soft_mask[:, :, np.newaxis]))
            
            refined_image = Image.fromarray(refined_array.astype(np.uint8))
            
            return refined_image
            
        except Exception as e:
            logger.warning(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ê°œì„  ì‹¤íŒ¨: {e}")
            return self._apply_mask_to_image(image, mask)
    
    def _pil_to_base64(self, image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
        import io
        import base64
        
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=90)
        img_str = base64.b64encode(buffer.getvalue()).decode()
        
        return img_str
    
    def _array_to_base64(self, array: np.ndarray) -> str:
        """NumPy ë°°ì—´ì„ base64ë¡œ ë³€í™˜"""
        import io
        import base64
        
        # ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        mask_image = Image.fromarray(array)
        
        buffer = io.BytesIO()
        mask_image.save(buffer, format='PNG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        
        return img_str
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "input_size": self.input_size,
            "confidence_threshold": self.confidence_threshold,
            "device": self.device,
            "initialized": self.is_initialized,
            "rembg_available": REMBG_AVAILABLE,
            "clothing_categories": self.CLOTHING_CATEGORIES,
            "supported_formats": ["RGB", "RGBA"]
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.segmentation_model:
            del self.segmentation_model
            self.segmentation_model = None
        
        if self.rembg_session:
            # rembg ì„¸ì…˜ ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)
            self.rembg_session = None
        
        self.is_initialized = False
        logger.info("ğŸ§¹ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")