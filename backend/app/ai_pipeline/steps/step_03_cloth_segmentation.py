# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - ì™„ì „ ê°•í™”ëœ í”„ë¡œë•ì…˜ ë²„ì „ v30.0
====================================================================================

ğŸ¯ BaseStepMixin v19.1 ì™„ì „ ì¤€ìˆ˜ + í”„ë¡œë•ì…˜ í•„ìˆ˜ ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„:
âœ… _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ (ë™ê¸° ì²˜ë¦¬)
âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì€ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬
âœ… step_model_requests.py DetailedDataSpec ì™„ì „ í™œìš©

ğŸš€ í”„ë¡œë•ì…˜ í•„ìˆ˜ ê¸°ëŠ¥ ì™„ì „ ì¶”ê°€:
ğŸ” ê³ ê¸‰ ì „ì²˜ë¦¬: í’ˆì§ˆ í‰ê°€, ì¡°ëª… ì •ê·œí™”, ROI ê²€ì¶œ
ğŸ§  ì‹¤ì œ ì˜ë¥˜ ë¶„ë¥˜ AI: Fashion-MNIST, ResNet, EfficientNet
ğŸ¯ ê³ ê¸‰ SAM í”„ë¡¬í”„íŠ¸: ë°•ìŠ¤+í¬ì¸íŠ¸+ë§ˆìŠ¤í¬ ì¡°í•©, ë°˜ë³µ ê°œì„ 
ğŸ¨ ê³ ê¸‰ í›„ì²˜ë¦¬: Graph Cut, Active Contour, Watershed
ğŸ“Š í’ˆì§ˆ ê²€ì¦: ìë™ í‰ê°€, ì¬ì‹œë„ ì‹œìŠ¤í…œ
ğŸ”„ ì‹¤ì‹œê°„ í”¼ë“œë°±: ì§„í–‰ë¥ , í’ˆì§ˆ ìŠ¤ì½”ì–´, ê°œì„  ì œì•ˆ
âš¡ ì„±ëŠ¥ ìµœì í™”: ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, ë©”ëª¨ë¦¬ ê´€ë¦¬
ğŸ”§ ì—ëŸ¬ ë³µêµ¬: ìë™ í´ë°±, ëª¨ë¸ ì „í™˜, ì¬ì‹œë„ ë¡œì§

Author: MyCloset AI Team
Date: 2025-07-27  
Version: v30.0 (ì™„ì „ ê°•í™”ëœ í”„ë¡œë•ì…˜ ë²„ì „)
"""

import time
import os
import sys
import logging
import threading
import gc
import hashlib
import json
import base64
import math
import weakref
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import platform
import subprocess
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1. ëª¨ë“ˆ ë ˆë²¨ Logger ë° Import
# ==============================================

def create_module_logger():
    """ëª¨ë“ˆ ë ˆë²¨ logger ì•ˆì „ ìƒì„±"""
    try:
        module_logger = logging.getLogger(__name__)
        if not module_logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            module_logger.addHandler(handler)
            module_logger.setLevel(logging.INFO)
        return module_logger
    except Exception as e:
        import sys
        print(f"âš ï¸ Logger ìƒì„± ì‹¤íŒ¨, stdout ì‚¬ìš©: {e}", file=sys.stderr)
        class FallbackLogger:
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def debug(self, msg): print(f"DEBUG: {msg}")
        return FallbackLogger()

logger = create_module_logger()

# BaseStepMixin ë™ì  Import
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logger
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
        
        def initialize(self): 
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader): 
            self.model_loader = model_loader
        
        async def _run_ai_inference(self, processed_input): 
            return {}

if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader, StepModelInterface
    from app.ai_pipeline.utils.step_model_requests import (
        EnhancedRealModelRequest, DetailedDataSpec, get_enhanced_step_request
    )

# ==============================================
# ğŸ”¥ 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ê°•í™”ëœ ë²„ì „)
# ==============================================

# NumPy
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ NumPy ì—†ìŒ")

# PIL
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ PIL ì—†ìŒ")

# PyTorch
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ")

# SAM
SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ")

# ONNX Runtime
ONNX_AVAILABLE = False
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
    logger.info("âš¡ ONNX Runtime ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ ONNX Runtime ì—†ìŒ")

# RemBG
REMBG_AVAILABLE = False
try:
    import rembg
    try:
        from rembg import remove, new_session
        REMBG_AVAILABLE = True
        logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        try:
            from rembg import remove
            REMBG_AVAILABLE = True
            logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ (ê¸°ë³¸)")
        except ImportError:
            logger.warning("âš ï¸ RemBG ê¸°ëŠ¥ ì œí•œ")
except ImportError:
    logger.warning("âš ï¸ RemBG ì—†ìŒ")

# SciPy (ê³ ê¸‰ í›„ì²˜ë¦¬ìš©)
SCIPY_AVAILABLE = False
try:
    from scipy import ndimage, optimize
    from scipy.spatial.distance import cdist
    SCIPY_AVAILABLE = True
    logger.info("ğŸ”¬ SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SciPy ì—†ìŒ")

# Scikit-image (ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬)
SKIMAGE_AVAILABLE = False
try:
    from skimage import measure, morphology, segmentation, filters, feature
    from skimage.color import rgb2lab, lab2rgb
    SKIMAGE_AVAILABLE = True
    logger.info("ğŸ”¬ Scikit-image ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Scikit-image ì—†ìŒ")

# DenseCRF
DENSECRF_AVAILABLE = False
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
    logger.info("ğŸ¨ DenseCRF ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ DenseCRF ì—†ìŒ")

# OpenCV (í´ë°±ìš©)
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
    logger.info("ğŸ“· OpenCV ë¡œë“œ ì™„ë£Œ (í´ë°±ìš©)")
except ImportError:
    logger.warning("âš ï¸ OpenCV ì—†ìŒ")

# Torchvision (ì˜ë¥˜ ë¶„ë¥˜ìš©)
TORCHVISION_AVAILABLE = False
try:
    import torchvision
    from torchvision import models, transforms
    TORCHVISION_AVAILABLE = True
    logger.info("ğŸ¤– Torchvision ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Torchvision ì—†ìŒ")

# ==============================================
# ğŸ”¥ 3. ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€
# ==============================================

IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ 4. Step Model Requests ë¡œë“œ
# ==============================================

def get_step_requirements():
    """step_model_requests.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        requirements_module = importlib.import_module('app.ai_pipeline.utils.step_model_requests')
        
        get_enhanced_step_request = getattr(requirements_module, 'get_enhanced_step_request', None)
        if get_enhanced_step_request:
            return get_enhanced_step_request("ClothSegmentationStep")
        
        REAL_STEP_MODEL_REQUESTS = getattr(requirements_module, 'REAL_STEP_MODEL_REQUESTS', {})
        return REAL_STEP_MODEL_REQUESTS.get("ClothSegmentationStep")
        
    except ImportError as e:
        logger.warning(f"âš ï¸ step_model_requests ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

STEP_REQUIREMENTS = get_step_requirements()

# ==============================================
# ğŸ”¥ 5. ê°•í™”ëœ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class SegmentationMethod(Enum):
    """ê°•í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    SAM_HUGE = "sam_huge"               # SAM ViT-Huge (2445.7MB)
    SAM_LARGE = "sam_large"             # SAM ViT-Large (1249.1MB)
    SAM_BASE = "sam_base"               # SAM ViT-Base (375.0MB)
    U2NET_CLOTH = "u2net_cloth"         # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    MOBILE_SAM = "mobile_sam"           # Mobile SAM (38.8MB)
    ISNET = "isnet"                     # ISNet ONNX (168.1MB)
    HYBRID_AI = "hybrid_ai"             # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”
    REMBG_U2NET = "rembg_u2net"         # RemBG U2Net
    REMBG_SILUETA = "rembg_silueta"     # RemBG Silueta
    DEEPLAB_V3 = "deeplab_v3"           # DeepLab v3+
    MASK_RCNN = "mask_rcnn"             # Mask R-CNN

class ClothingType(Enum):
    """ê°•í™”ëœ ì˜ë¥˜ íƒ€ì…"""
    SHIRT = "shirt"
    T_SHIRT = "t_shirt"
    DRESS = "dress"
    PANTS = "pants"
    JEANS = "jeans"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    HOODIE = "hoodie"
    BLOUSE = "blouse"
    SHORTS = "shorts"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # Mobile SAM, RemBG
    BALANCED = "balanced"   # U2Net + ISNet
    HIGH = "high"          # SAM + U2Net + Graph Cut
    ULTRA = "ultra"        # Hybrid AI + ëª¨ë“  í›„ì²˜ë¦¬
    PRODUCTION = "production"  # í”„ë¡œë•ì…˜ ìµœì í™”

@dataclass
class EnhancedSegmentationConfig:
    """ê°•í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.U2NET_CLOTH
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (1024, 1024)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    enable_roi_detection: bool = True
    enable_background_analysis: bool = True
    
    # ì˜ë¥˜ ë¶„ë¥˜ ì„¤ì •
    enable_clothing_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # SAM í”„ë¡¬í”„íŠ¸ ì„¤ì •
    enable_advanced_prompts: bool = True
    use_box_prompts: bool = True
    use_mask_prompts: bool = True
    enable_iterative_refinement: bool = True
    max_refinement_iterations: int = 3
    
    # í›„ì²˜ë¦¬ ì„¤ì •
    enable_graph_cut: bool = True
    enable_active_contour: bool = True
    enable_watershed: bool = True
    enable_multiscale_processing: bool = True
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    enable_visualization: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_crf_postprocessing: bool = True
    use_fp16: bool = True
    confidence_threshold: float = 0.5
    remove_noise: bool = True
    overlay_opacity: float = 0.6

# ==============================================
# ğŸ”¥ 6. ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
# ==============================================

class AdvancedPreprocessor:
    """ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def assess_image_quality(image: np.ndarray) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_scores = {}
            
            # ë¸”ëŸ¬ ì •ë„ ì¸¡ì • (Laplacian variance)
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            laplacian_var = np.var(filters.laplacian(gray)) if SKIMAGE_AVAILABLE else np.var(gray)
            quality_scores['sharpness'] = min(laplacian_var / 1000.0, 1.0)
            
            # ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •
            noise_level = np.std(gray - filters.gaussian(gray, sigma=1)) if SKIMAGE_AVAILABLE else 0.1
            quality_scores['noise_level'] = max(0.0, 1.0 - noise_level / 50.0)
            
            # ëŒ€ë¹„ ì¸¡ì •
            contrast = np.std(gray)
            quality_scores['contrast'] = min(contrast / 128.0, 1.0)
            
            # í•´ìƒë„ í’ˆì§ˆ
            height, width = image.shape[:2]
            resolution_score = min((height * width) / (1024 * 1024), 1.0)
            quality_scores['resolution'] = resolution_score
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_scores['overall'] = np.mean(list(quality_scores.values()))
            
            return quality_scores
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5, 'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5, 'resolution': 0.5}
    
    @staticmethod
    def normalize_lighting(image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            if not SKIMAGE_AVAILABLE:
                return image
            
            # LAB ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ë³€í™˜
            if len(image.shape) == 3:
                lab = rgb2lab(image.astype(np.float32) / 255.0)
                
                # L ì±„ë„ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                l_channel = lab[:, :, 0]
                l_normalized = (l_channel - l_channel.min()) / (l_channel.max() - l_channel.min()) * 100
                lab[:, :, 0] = l_normalized
                
                # RGBë¡œ ë‹¤ì‹œ ë³€í™˜
                rgb_normalized = lab2rgb(lab)
                return (rgb_normalized * 255).astype(np.uint8)
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                normalized = (image - image.min()) / (image.max() - image.min()) * 255
                return normalized.astype(np.uint8)
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    @staticmethod
    def correct_colors(image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if PIL_AVAILABLE and len(image.shape) == 3:
                pil_image = Image.fromarray(image)
                
                # ìë™ ëŒ€ë¹„ ì¡°ì •
                enhancer = ImageEnhance.Contrast(pil_image)
                enhanced = enhancer.enhance(1.2)
                
                # ìƒ‰ìƒ ì±„ë„ ì¡°ì •
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.1)
                
                # ì„ ëª…ë„ ì¡°ì •
                enhancer = ImageEnhance.Sharpness(enhanced)
                enhanced = enhancer.enhance(1.1)
                
                return np.array(enhanced)
            else:
                return image
                
        except Exception as e:
            logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    @staticmethod
    def detect_roi(image: np.ndarray) -> Tuple[int, int, int, int]:
        """ROI (ê´€ì‹¬ ì˜ì—­) ê²€ì¶œ"""
        try:
            if not SKIMAGE_AVAILABLE:
                h, w = image.shape[:2]
                return (w//4, h//4, 3*w//4, 3*h//4)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì—£ì§€ ê²€ì¶œ
            edges = feature.canny(gray, sigma=2, low_threshold=0.1, high_threshold=0.2)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ì—°ê²°ëœ ì˜ì—­ ì°¾ê¸°
            dilated = morphology.dilation(edges, morphology.disk(5))
            filled = ndimage.binary_fill_holes(dilated)
            
            # ê°€ì¥ í° ì—°ê²°ëœ ì˜ì—­ ì°¾ê¸°
            labeled = measure.label(filled)
            regions = measure.regionprops(labeled)
            
            if regions:
                largest_region = max(regions, key=lambda r: r.area)
                minr, minc, maxr, maxc = largest_region.bbox
                
                # íŒ¨ë”© ì¶”ê°€
                h, w = image.shape[:2]
                padding = 50
                minc = max(0, minc - padding)
                minr = max(0, minr - padding)
                maxc = min(w, maxc + padding)
                maxr = min(h, maxr + padding)
                
                return (minc, minr, maxc, maxr)
            else:
                # í´ë°±: ì¤‘ì•™ ì˜ì—­
                h, w = image.shape[:2]
                return (w//4, h//4, 3*w//4, 3*h//4)
                
        except Exception as e:
            logger.warning(f"âš ï¸ ROI ê²€ì¶œ ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2]
            return (w//4, h//4, 3*w//4, 3*h//4)
    
    @staticmethod
    def analyze_background_complexity(image: np.ndarray) -> float:
        """ë°°ê²½ ë³µì¡ë„ ë¶„ì„"""
        try:
            if not SKIMAGE_AVAILABLE:
                return 0.5
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì—£ì§€ ë°€ë„ ê³„ì‚°
            edges = feature.canny(gray, sigma=1)
            edge_density = np.sum(edges) / edges.size
            
            # í…ìŠ¤ì²˜ ë³µì¡ë„ (LBP ê¸°ë°˜)
            try:
                lbp = feature.local_binary_pattern(gray, P=8, R=1, method='uniform')
                texture_complexity = np.std(lbp) / 255.0
            except:
                texture_complexity = edge_density
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„± (íˆìŠ¤í† ê·¸ë¨ ì—”íŠ¸ë¡œí”¼)
            if len(image.shape) == 3:
                hist_r = np.histogram(image[:, :, 0], bins=32)[0]
                hist_g = np.histogram(image[:, :, 1], bins=32)[0]
                hist_b = np.histogram(image[:, :, 2], bins=32)[0]
                
                entropy_r = -np.sum(hist_r * np.log(hist_r + 1e-10))
                entropy_g = -np.sum(hist_g * np.log(hist_g + 1e-10))
                entropy_b = -np.sum(hist_b * np.log(hist_b + 1e-10))
                
                color_complexity = (entropy_r + entropy_g + entropy_b) / (3 * np.log(32))
            else:
                hist = np.histogram(gray, bins=32)[0]
                color_complexity = -np.sum(hist * np.log(hist + 1e-10)) / np.log(32)
            
            # ì „ì²´ ë³µì¡ë„ ì ìˆ˜
            complexity = (edge_density * 0.4 + texture_complexity * 0.3 + color_complexity * 0.3)
            return min(complexity, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë°°ê²½ ë³µì¡ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ ì˜ë¥˜ ë¶„ë¥˜ AI ëª¨ë¸
# ==============================================

class ClothingClassifier:
    """ì‹¤ì œ ì˜ë¥˜ ë¶„ë¥˜ AI ëª¨ë¸"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.model = None
        self.transform = None
        self.is_loaded = False
        self.class_names = [
            'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
        ]
        
    def load_model(self, model_path: str = None):
        """ì˜ë¥˜ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE or not TORCHVISION_AVAILABLE:
                logger.warning("âš ï¸ PyTorch/Torchvision ì—†ìŒ")
                return False
            
            # ì‚¬ì „ í›ˆë ¨ëœ ResNet ëª¨ë¸ ë¡œë“œ
            self.model = models.resnet18(pretrained=True)
            self.model.fc = nn.Linear(self.model.fc.in_features, len(self.class_names))
            
            # ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ë¡œë“œ
            if model_path and os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
                    self.model.load_state_dict(checkpoint)
                    logger.info(f"âœ… ì»¤ìŠ¤í…€ ì˜ë¥˜ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ: {model_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì»¤ìŠ¤í…€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            # ì „ì²˜ë¦¬ ë³€í™˜ ì •ì˜
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            self.is_loaded = True
            logger.info("âœ… ì˜ë¥˜ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def classify(self, image: Union[np.ndarray, Image.Image]) -> Tuple[str, float]:
        """ì˜ë¥˜ ë¶„ë¥˜ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return "unknown", 0.0
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                pil_image = image
            
            # RGB ë³€í™˜
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
            input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = F.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                class_name = self.class_names[predicted.item()]
                confidence_score = confidence.item()
                
                return class_name, confidence_score
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown", 0.0
    
    def extract_features(self, image: Union[np.ndarray, Image.Image]) -> np.ndarray:
        """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            if not self.is_loaded:
                return np.zeros(512)
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                pil_image = image
            
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                # FC ë ˆì´ì–´ ì´ì „ì˜ íŠ¹ì§• ì¶”ì¶œ
                features = self.model.avgpool(self.model.layer4(
                    self.model.layer3(self.model.layer2(self.model.layer1(
                        self.model.maxpool(self.model.relu(self.model.bn1(self.model.conv1(input_tensor))))
                    )))
                ))
                features = features.view(features.size(0), -1)
                return features.cpu().numpy().flatten()
                
        except Exception as e:
            logger.warning(f"âš ï¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(512)

# ==============================================
# ğŸ”¥ 8. ê³ ê¸‰ SAM í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
# ==============================================

class AdvancedSAMPrompter:
    """ê³ ê¸‰ SAM í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""
    
    @staticmethod
    def generate_clothing_prompts(
        clothing_type: str, 
        width: int, 
        height: int,
        roi_box: Tuple[int, int, int, int] = None,
        previous_mask: np.ndarray = None
    ) -> Dict[str, Dict[str, Any]]:
        """ê³ ê¸‰ ì˜ë¥˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = {}
        
        # ROI ì •ë³´ í™œìš©
        if roi_box:
            x1, y1, x2, y2 = roi_box
            roi_center_x = (x1 + x2) // 2
            roi_center_y = (y1 + y2) // 2
            roi_width = x2 - x1
            roi_height = y2 - y1
        else:
            roi_center_x, roi_center_y = width // 2, height // 2
            roi_width, roi_height = width, height
        
        if clothing_type.lower() in ["shirt", "t_shirt", "top", "blouse"]:
            # ìƒì˜ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            prompts["upper_body"] = {
                "points": [
                    (roi_center_x, roi_center_y - roi_height // 6),  # ê°€ìŠ´ ì¤‘ì•™
                    (roi_center_x - roi_width // 4, roi_center_y),   # ì¢Œì¸¡ ì†Œë§¤
                    (roi_center_x + roi_width // 4, roi_center_y),   # ìš°ì¸¡ ì†Œë§¤
                    (roi_center_x, roi_center_y + roi_height // 6),  # í•˜ë‹¨ ì¤‘ì•™
                ],
                "labels": [1, 1, 1, 1],
                "box": [x1, y1, x2, int(y1 + roi_height * 0.7)] if roi_box else None
            }
            
        elif clothing_type.lower() in ["pants", "jeans", "trouser"]:
            # í•˜ì˜ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            prompts["lower_body"] = {
                "points": [
                    (roi_center_x, roi_center_y + roi_height // 6),      # í—ˆë¦¬
                    (roi_center_x - roi_width // 6, roi_center_y + roi_height // 3),  # ì¢Œì¸¡ ë‹¤ë¦¬
                    (roi_center_x + roi_width // 6, roi_center_y + roi_height // 3),  # ìš°ì¸¡ ë‹¤ë¦¬
                    (roi_center_x, roi_center_y + roi_height // 2),      # í•˜ë‹¨
                ],
                "labels": [1, 1, 1, 1],
                "box": [x1, int(y1 + roi_height * 0.3), x2, y2] if roi_box else None
            }
            
        elif clothing_type.lower() == "dress":
            # ì›í”¼ìŠ¤ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            prompts["full_dress"] = {
                "points": [
                    (roi_center_x, roi_center_y - roi_height // 4),      # ìƒë‹¨
                    (roi_center_x, roi_center_y),                       # ì¤‘ì•™
                    (roi_center_x, roi_center_y + roi_height // 4),      # í•˜ë‹¨
                    (roi_center_x - roi_width // 4, roi_center_y - roi_height // 6),  # ì¢Œì¸¡ ìƒë‹¨
                    (roi_center_x + roi_width // 4, roi_center_y - roi_height // 6),  # ìš°ì¸¡ ìƒë‹¨
                ],
                "labels": [1, 1, 1, 1, 1],
                "box": [x1, y1, x2, y2] if roi_box else None
            }
            
        else:
            # ì¼ë°˜ ì˜ë¥˜ í”„ë¡¬í”„íŠ¸
            prompts["clothing"] = {
                "points": [
                    (roi_center_x, roi_center_y),                       # ì¤‘ì•™
                    (roi_center_x - roi_width // 4, roi_center_y - roi_height // 4),  # ì¢Œìƒ
                    (roi_center_x + roi_width // 4, roi_center_y - roi_height // 4),  # ìš°ìƒ
                    (roi_center_x - roi_width // 4, roi_center_y + roi_height // 4),  # ì¢Œí•˜
                    (roi_center_x + roi_width // 4, roi_center_y + roi_height // 4),  # ìš°í•˜
                ],
                "labels": [1, 1, 1, 1, 1],
                "box": [x1, y1, x2, y2] if roi_box else None
            }
        
        # ì´ì „ ë§ˆìŠ¤í¬ í™œìš©í•œ ê°œì„  í”„ë¡¬í”„íŠ¸
        if previous_mask is not None:
            try:
                # ë§ˆìŠ¤í¬ì—ì„œ ì¶”ê°€ í¬ì¸íŠ¸ ì¶”ì¶œ
                mask_points = AdvancedSAMPrompter._extract_points_from_mask(previous_mask)
                if mask_points:
                    for area_name in prompts:
                        prompts[area_name]["additional_points"] = mask_points
                        prompts[area_name]["additional_labels"] = [1] * len(mask_points)
            except Exception as e:
                logger.debug(f"ì´ì „ ë§ˆìŠ¤í¬ í™œìš© ì‹¤íŒ¨: {e}")
        
        return prompts
    
    @staticmethod
    def _extract_points_from_mask(mask: np.ndarray, num_points: int = 5) -> List[Tuple[int, int]]:
        """ë§ˆìŠ¤í¬ì—ì„œ ì¶”ê°€ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        try:
            if not SKIMAGE_AVAILABLE:
                return []
            
            # ë§ˆìŠ¤í¬ì˜ ìœ¤ê³½ì„  ì°¾ê¸°
            contours = measure.find_contours(mask > 128, 0.5)
            
            if not contours:
                return []
            
            # ê°€ì¥ ê¸´ ìœ¤ê³½ì„  ì„ íƒ
            longest_contour = max(contours, key=len)
            
            # ê· ë“±í•˜ê²Œ ë¶„ì‚°ëœ í¬ì¸íŠ¸ ì„ íƒ
            if len(longest_contour) > num_points:
                indices = np.linspace(0, len(longest_contour) - 1, num_points, dtype=int)
                selected_points = longest_contour[indices]
                
                # (row, col) -> (x, y) ë³€í™˜
                points = [(int(point[1]), int(point[0])) for point in selected_points]
                return points
            else:
                return [(int(point[1]), int(point[0])) for point in longest_contour[::2]]
                
        except Exception as e:
            logger.debug(f"ë§ˆìŠ¤í¬ í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

# ==============================================
# ğŸ”¥ 9. ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤
# ==============================================

class AdvancedPostProcessor:
    """ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤"""
    
    @staticmethod
    def apply_graph_cut(image: np.ndarray, initial_mask: np.ndarray, iterations: int = 5) -> np.ndarray:
        """Graph Cut ê¸°ë°˜ ê²½ê³„ ê°œì„ """
        try:
            if not SCIPY_AVAILABLE:
                return initial_mask
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì´ˆê¸° ë§ˆìŠ¤í¬ë¥¼ íŠ¸ë¼ì´ë§µìœ¼ë¡œ ë³€í™˜
            trimap = np.zeros_like(initial_mask)
            trimap[initial_mask > 200] = 255  # í™•ì‹¤í•œ ì „ê²½
            trimap[initial_mask < 50] = 0     # í™•ì‹¤í•œ ë°°ê²½
            trimap[(initial_mask >= 50) & (initial_mask <= 200)] = 128  # ë¶ˆí™•ì‹¤í•œ ì˜ì—­
            
            # ê°„ë‹¨í•œ ì—ë„ˆì§€ ìµœì†Œí™” (ì‹¤ì œ Graph Cut ëŒ€ì‹ )
            refined_mask = initial_mask.copy().astype(np.float32)
            
            for _ in range(iterations):
                # ìŠ¤ë¬´ë”© ì—ë„ˆì§€
                smoothed = ndimage.gaussian_filter(refined_mask, sigma=1.0)
                
                # ë°ì´í„° ì—ë„ˆì§€ (ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´ í™œìš©)
                gradient = np.gradient(gray)
                edge_weight = np.exp(-np.sqrt(gradient[0]**2 + gradient[1]**2) / 50.0)
                
                # ì—ë„ˆì§€ ê²°í•©
                refined_mask = 0.7 * refined_mask + 0.3 * smoothed * edge_weight
                
                # íŠ¸ë¼ì´ë§µ ì œì•½ ì ìš©
                refined_mask[trimap == 255] = 255
                refined_mask[trimap == 0] = 0
            
            return (refined_mask > 128).astype(np.uint8) * 255
            
        except Exception as e:
            logger.warning(f"âš ï¸ Graph Cut ì‹¤íŒ¨: {e}")
            return initial_mask
    
    @staticmethod
    def apply_active_contour(image: np.ndarray, initial_mask: np.ndarray, iterations: int = 100) -> np.ndarray:
        """Active Contour (Snake) ì•Œê³ ë¦¬ì¦˜"""
        try:
            if not SKIMAGE_AVAILABLE:
                return initial_mask
            
            # ì´ˆê¸° ìœ¤ê³½ì„  ì¶”ì¶œ
            contours = measure.find_contours(initial_mask > 128, 0.5)
            
            if not contours:
                return initial_mask
            
            # ê°€ì¥ ê¸´ ìœ¤ê³½ì„  ì„ íƒ
            longest_contour = max(contours, key=len)
            
            # Active Contour ì ìš©
            from skimage.segmentation import active_contour
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ìœ¤ê³½ì„  ê°œì„ 
            refined_contour = active_contour(
                gray, 
                longest_contour,
                alpha=0.015,
                beta=10,
                gamma=0.001,
                max_iterations=iterations
            )
            
            # ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
            refined_mask = np.zeros_like(initial_mask)
            rr, cc = np.round(refined_contour).astype(int).T
            
            # ê²½ê³„ ì²´í¬
            valid_indices = (rr >= 0) & (rr < initial_mask.shape[0]) & (cc >= 0) & (cc < initial_mask.shape[1])
            rr, cc = rr[valid_indices], cc[valid_indices]
            
            if len(rr) > 0:
                refined_mask[rr, cc] = 255
                
                # ë‚´ë¶€ ì±„ìš°ê¸°
                filled_mask = ndimage.binary_fill_holes(refined_mask > 0)
                refined_mask = (filled_mask * 255).astype(np.uint8)
            else:
                refined_mask = initial_mask
            
            return refined_mask
            
        except Exception as e:
            logger.warning(f"âš ï¸ Active Contour ì‹¤íŒ¨: {e}")
            return initial_mask
    
    @staticmethod
    def apply_watershed(image: np.ndarray, initial_mask: np.ndarray) -> np.ndarray:
        """Watershed ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            if not SKIMAGE_AVAILABLE:
                return initial_mask
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê±°ë¦¬ ë³€í™˜
            distance = ndimage.distance_transform_edt(initial_mask > 128)
            
            # ë¡œì»¬ ìµœëŒ“ê°’ ì°¾ê¸° (ì‹œë“œ í¬ì¸íŠ¸)
            from skimage.feature import peak_local_maxima
            local_maxima = peak_local_maxima(distance, min_distance=20, threshold_abs=0.3*distance.max())
            
            # ë§ˆì»¤ ìƒì„±
            markers = np.zeros_like(initial_mask, dtype=int)
            for i, (y, x) in enumerate(local_maxima[0]):
                markers[y, x] = i + 1
            
            # Watershed ì ìš©
            from skimage.segmentation import watershed
            labels = watershed(-distance, markers, mask=initial_mask > 128)
            
            # ê²°ê³¼ ë§ˆìŠ¤í¬ ìƒì„±
            refined_mask = (labels > 0).astype(np.uint8) * 255
            
            return refined_mask
            
        except Exception as e:
            logger.warning(f"âš ï¸ Watershed ì‹¤íŒ¨: {e}")
            return initial_mask
    
    @staticmethod
    def apply_multiscale_processing(image: np.ndarray, initial_mask: np.ndarray) -> np.ndarray:
        """ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬"""
        try:
            scales = [0.5, 1.0, 1.5]
            processed_masks = []
            
            for scale in scales:
                # ìŠ¤ì¼€ì¼ ì¡°ì •
                if scale != 1.0:
                    h, w = initial_mask.shape
                    new_h, new_w = int(h * scale), int(w * scale)
                    
                    if scale < 1.0:
                        # ë‹¤ìš´ìŠ¤ì¼€ì¼
                        scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.Resampling.LANCZOS))
                        scaled_mask = np.array(Image.fromarray(initial_mask).resize((new_w, new_h), Image.Resampling.NEAREST))
                    else:
                        # ì—…ìŠ¤ì¼€ì¼
                        scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.Resampling.LANCZOS))
                        scaled_mask = np.array(Image.fromarray(initial_mask).resize((new_w, new_h), Image.Resampling.NEAREST))
                    
                    # ì²˜ë¦¬
                    processed = AdvancedPostProcessor.apply_graph_cut(scaled_image, scaled_mask, iterations=3)
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    processed = np.array(Image.fromarray(processed).resize((w, h), Image.Resampling.NEAREST))
                else:
                    processed = AdvancedPostProcessor.apply_graph_cut(image, initial_mask, iterations=3)
                
                processed_masks.append(processed.astype(np.float32) / 255.0)
            
            # ìŠ¤ì¼€ì¼ë³„ ê²°ê³¼ í†µí•©
            if len(processed_masks) > 1:
                # ê°€ì¤‘ í‰ê· 
                weights = [0.3, 0.4, 0.3]  # ì¤‘ê°„ ìŠ¤ì¼€ì¼ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
                combined = np.zeros_like(processed_masks[0])
                
                for mask, weight in zip(processed_masks, weights):
                    combined += mask * weight
                
                final_mask = (combined > 0.5).astype(np.uint8) * 255
            else:
                final_mask = (processed_masks[0] > 0.5).astype(np.uint8) * 255
            
            return final_mask
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return initial_mask

# ==============================================
# ğŸ”¥ 10. í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
# ==============================================

class QualityValidator:
    """í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def evaluate_mask_quality(mask: np.ndarray, image: np.ndarray = None) -> Dict[str, float]:
        """ë§ˆìŠ¤í¬ í’ˆì§ˆ ìë™ í‰ê°€"""
        try:
            quality_metrics = {}
            
            # 1. ì˜ì—­ ì—°ì†ì„± (ê°€ì¥ í° ì—°ê²° ì„±ë¶„ ë¹„ìœ¨)
            if SKIMAGE_AVAILABLE:
                labeled = measure.label(mask > 128)
                regions = measure.regionprops(labeled)
                
                if regions:
                    total_area = np.sum(mask > 128)
                    largest_area = max(region.area for region in regions)
                    quality_metrics['continuity'] = largest_area / total_area if total_area > 0 else 0.0
                else:
                    quality_metrics['continuity'] = 0.0
            else:
                quality_metrics['continuity'] = 0.5
            
            # 2. ê²½ê³„ì„  ë¶€ë“œëŸ¬ì›€
            boundary_smoothness = QualityValidator._calculate_boundary_smoothness(mask)
            quality_metrics['boundary_smoothness'] = boundary_smoothness
            
            # 3. í˜•íƒœ ì™„ì„±ë„ (ì†”ë¦¬ë””í‹°)
            solidity = QualityValidator._calculate_solidity(mask)
            quality_metrics['solidity'] = solidity
            
            # 4. í¬ê¸° ì ì ˆì„±
            size_ratio = np.sum(mask > 128) / mask.size
            if 0.1 <= size_ratio <= 0.7:  # ì ì ˆí•œ í¬ê¸° ë²”ìœ„
                quality_metrics['size_appropriateness'] = 1.0
            else:
                quality_metrics['size_appropriateness'] = max(0.0, 1.0 - abs(size_ratio - 0.3) / 0.3)
            
            # 5. ì¢…íš¡ë¹„ í•©ë¦¬ì„±
            aspect_ratio = QualityValidator._calculate_aspect_ratio(mask)
            if 0.5 <= aspect_ratio <= 3.0:  # í•©ë¦¬ì ì¸ ì¢…íš¡ë¹„ ë²”ìœ„
                quality_metrics['aspect_ratio'] = 1.0
            else:
                quality_metrics['aspect_ratio'] = max(0.0, 1.0 - abs(aspect_ratio - 1.5) / 1.5)
            
            # 6. ì´ë¯¸ì§€ì™€ì˜ ì¼ì¹˜ë„ (ì œê³µëœ ê²½ìš°)
            if image is not None:
                alignment_score = QualityValidator._calculate_image_alignment(mask, image)
                quality_metrics['image_alignment'] = alignment_score
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_metrics['overall'] = np.mean(list(quality_metrics.values()))
            
            return quality_metrics
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5}
    
    @staticmethod
    def _calculate_boundary_smoothness(mask: np.ndarray) -> float:
        """ê²½ê³„ì„  ë¶€ë“œëŸ¬ì›€ ê³„ì‚°"""
        try:
            if not SKIMAGE_AVAILABLE:
                return 0.5
            
            # ìœ¤ê³½ì„  ì¶”ì¶œ
            contours = measure.find_contours(mask > 128, 0.5)
            
            if not contours:
                return 0.0
            
            # ê°€ì¥ ê¸´ ìœ¤ê³½ì„  ë¶„ì„
            longest_contour = max(contours, key=len)
            
            if len(longest_contour) < 10:
                return 0.0
            
            # ê³¡ë¥  ë³€í™” ê³„ì‚°
            curvatures = []
            for i in range(2, len(longest_contour) - 2):
                p1 = longest_contour[i-2]
                p2 = longest_contour[i]
                p3 = longest_contour[i+2]
                
                # ë²¡í„° ê³„ì‚°
                v1 = p2 - p1
                v2 = p3 - p2
                
                # ê°ë„ ë³€í™” ê³„ì‚°
                if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:
                    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                    cos_angle = np.clip(cos_angle, -1, 1)
                    angle_change = np.arccos(cos_angle)
                    curvatures.append(angle_change)
            
            if curvatures:
                # ê³¡ë¥  ë³€í™”ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ìš´ ê²½ê³„
                curvature_std = np.std(curvatures)
                smoothness = np.exp(-curvature_std)
                return min(smoothness, 1.0)
            else:
                return 0.5
                
        except Exception as e:
            logger.debug(f"ê²½ê³„ì„  ë¶€ë“œëŸ¬ì›€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    @staticmethod
    def _calculate_solidity(mask: np.ndarray) -> float:
        """ì†”ë¦¬ë””í‹° ê³„ì‚°"""
        try:
            if not SKIMAGE_AVAILABLE:
                return 0.5
            
            labeled = measure.label(mask > 128)
            regions = measure.regionprops(labeled)
            
            if regions:
                largest_region = max(regions, key=lambda r: r.area)
                return largest_region.solidity
            else:
                return 0.0
                
        except Exception as e:
            logger.debug(f"ì†”ë¦¬ë””í‹° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    @staticmethod
    def _calculate_aspect_ratio(mask: np.ndarray) -> float:
        """ì¢…íš¡ë¹„ ê³„ì‚°"""
        try:
            rows = np.any(mask > 128, axis=1)
            cols = np.any(mask > 128, axis=0)
            
            if not np.any(rows) or not np.any(cols):
                return 1.0
            
            rmin, rmax = np.where(rows)[0][[0, -1]]
            cmin, cmax = np.where(cols)[0][[0, -1]]
            
            width = cmax - cmin + 1
            height = rmax - rmin + 1
            
            return height / width if width > 0 else 1.0
            
        except Exception:
            return 1.0
    
    @staticmethod
    def _calculate_image_alignment(mask: np.ndarray, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ì™€ì˜ ì¼ì¹˜ë„ ê³„ì‚°"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì—£ì§€ ì¶”ì¶œ
            if SKIMAGE_AVAILABLE:
                image_edges = feature.canny(gray, sigma=1)
                mask_edges = feature.canny(mask.astype(np.float32), sigma=1)
                
                # ì—£ì§€ ì¼ì¹˜ë„ ê³„ì‚°
                intersection = np.logical_and(image_edges, mask_edges)
                union = np.logical_or(image_edges, mask_edges)
                
                if np.sum(union) > 0:
                    alignment_score = np.sum(intersection) / np.sum(union)
                    return alignment_score
                else:
                    return 0.5
            else:
                return 0.5
                
        except Exception as e:
            logger.debug(f"ì´ë¯¸ì§€ ì¼ì¹˜ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

# ==============================================
# ğŸ”¥ 11. ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤ (ì™„ì „ ê°•í™”ëœ ë²„ì „)
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - ì™„ì „ ê°•í™”ëœ í”„ë¡œë•ì…˜ ë²„ì „ v30.0
    
    BaseStepMixin v19.1ì—ì„œ ìë™ ì œê³µ:
    âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
    âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš© (DetailedDataSpec)
    âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ (ModelLoader, MemoryManager ë“±)
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
    âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
    
    ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„!
    """
    
    def __init__(self, **kwargs):
        """ì™„ì „ ê°•í™”ëœ ì´ˆê¸°í™”"""
        try:
            # BaseStepMixin ì´ˆê¸°í™”
            super().__init__(
                step_name="ClothSegmentationStep",
                step_id=3,
                **kwargs
            )
            
            # ê°•í™”ëœ ì„¤ì •
            self.config = EnhancedSegmentationConfig()
            if 'segmentation_config' in kwargs:
                config_dict = kwargs['segmentation_config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
                elif isinstance(config_dict, EnhancedSegmentationConfig):
                    self.config = config_dict
            
            # AI ëª¨ë¸ ë° ì‹œìŠ¤í…œ
            self.ai_models = {}
            self.model_paths = {}
            self.available_methods = []
            self.preprocessor = AdvancedPreprocessor()
            self.clothing_classifier = ClothingClassifier(self.device)
            self.sam_prompter = AdvancedSAMPrompter()
            self.postprocessor = AdvancedPostProcessor()
            self.quality_validator = QualityValidator()
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ
            self.models_loading_status = {
                'sam_huge': False,
                'sam_large': False,
                'sam_base': False,
                'u2net_cloth': False,
                'mobile_sam': False,
                'isnet': False,
                'rembg_u2net': False,
                'rembg_silueta': False,
                'clothing_classifier': False,
            }
            
            # ì‹œìŠ¤í…œ ìµœì í™”
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            
            # ì„±ëŠ¥ ë° ìºì‹±
            self.executor = ThreadPoolExecutor(
                max_workers=6 if self.is_m3_max else 3,
                thread_name_prefix="enhanced_cloth_seg"
            )
            self.segmentation_cache = {}
            self.quality_cache = {}
            self.cache_lock = threading.RLock()
            
            # ê°•í™”ëœ í†µê³„
            self.ai_stats = {
                'total_processed': 0,
                'preprocessing_time': 0.0,
                'classification_time': 0.0,
                'segmentation_time': 0.0,
                'postprocessing_time': 0.0,
                'quality_validation_time': 0.0,
                'sam_huge_calls': 0,
                'u2net_calls': 0,
                'mobile_sam_calls': 0,
                'isnet_calls': 0,
                'rembg_calls': 0,
                'hybrid_calls': 0,
                'retry_attempts': 0,
                'quality_failures': 0,
                'average_quality_score': 0.0,
                'average_confidence': 0.0
            }
            
            self.logger.info(f"âœ… {self.step_name} ì™„ì „ ê°•í™”ëœ ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"   - Device: {self.device}")
            self.logger.info(f"   - M3 Max: {self.is_m3_max}")
            self.logger.info(f"   - Memory: {self.memory_gb}GB")
            self.logger.info(f"   - ê°•í™”ëœ ê¸°ëŠ¥: ì „ì²˜ë¦¬, ë¶„ë¥˜, ê³ ê¸‰ í”„ë¡¬í”„íŠ¸, í’ˆì§ˆ ê²€ì¦")
            
        except Exception as e:
            self.logger.error(f"âŒ ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì •"""
        try:
            self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ")
            self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
            self.step_id = kwargs.get('step_id', 3)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.ai_models = {}
            self.model_paths = {}
            self.ai_stats = {'total_processed': 0}
            self.config = EnhancedSegmentationConfig()
            self.cache_lock = threading.RLock()
        except Exception as e:
            print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 12. ëª¨ë¸ ì´ˆê¸°í™” (ê°•í™”ëœ ë²„ì „)
    # ==============================================
    
    def initialize(self) -> bool:
        """ê°•í™”ëœ AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            logger.info(f"ğŸ”„ {self.step_name} ê°•í™”ëœ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ëª¨ë¸ ê²½ë¡œ íƒì§€
            self._detect_model_paths()
            
            # 2. ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©
            self._load_all_enhanced_models()
            
            # 3. ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            # 4. BaseStepMixin ì´ˆê¸°í™”
            super_initialized = super().initialize()
            
            self.is_initialized = True
            self.is_ready = True
            
            loaded_models = list(self.ai_models.keys())
            logger.info(f"âœ… {self.step_name} ê°•í™”ëœ AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            logger.info(f"   - ê°•í™”ëœ ê¸°ëŠ¥: í’ˆì§ˆ í‰ê°€, ì˜ë¥˜ ë¶„ë¥˜, ê³ ê¸‰ í›„ì²˜ë¦¬")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    def _detect_model_paths(self):
        """ê°•í™”ëœ ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            # step_model_requests.py ê¸°ë°˜ ê²½ë¡œ íƒì§€
            if STEP_REQUIREMENTS:
                search_paths = STEP_REQUIREMENTS.search_paths + STEP_REQUIREMENTS.fallback_paths
                
                # Primary íŒŒì¼ë“¤
                primary_file = STEP_REQUIREMENTS.primary_file
                for search_path in search_paths:
                    full_path = os.path.join(search_path, primary_file)
                    if os.path.exists(full_path):
                        self.model_paths['sam_huge'] = full_path
                        logger.info(f"âœ… Primary SAM ViT-Huge ë°œê²¬: {full_path}")
                        break
                
                # Alternative íŒŒì¼ë“¤
                for alt_file, alt_size in STEP_REQUIREMENTS.alternative_files:
                    for search_path in search_paths:
                        full_path = os.path.join(search_path, alt_file)
                        if os.path.exists(full_path):
                            if 'sam_vit_l' in alt_file.lower():
                                self.model_paths['sam_large'] = full_path
                            elif 'sam_vit_b' in alt_file.lower():
                                self.model_paths['sam_base'] = full_path
                            elif 'u2net' in alt_file.lower() and 'cloth' in alt_file.lower():
                                self.model_paths['u2net_cloth'] = full_path
                            elif 'mobile_sam' in alt_file.lower():
                                self.model_paths['mobile_sam'] = full_path
                            elif 'isnet' in alt_file.lower() or alt_file.endswith('.onnx'):
                                self.model_paths['isnet'] = full_path
                            elif 'clothing_classifier' in alt_file.lower():
                                self.model_paths['clothing_classifier'] = full_path
                            logger.info(f"âœ… Alternative ëª¨ë¸ ë°œê²¬: {full_path}")
                            break
            
            # ê¸°ë³¸ ê²½ë¡œ í´ë°±
            if not self.model_paths:
                base_paths = [
                    "ai_models/step_03_cloth_segmentation/",
                    "models/step_03_cloth_segmentation/",
                    "checkpoints/segmentation/",
                    "models/classification/",
                ]
                
                model_files = {
                    'sam_huge': 'sam_vit_h_4b8939.pth',
                    'sam_large': 'sam_vit_l_0b3195.pth',
                    'sam_base': 'sam_vit_b_01ec64.pth',
                    'u2net_cloth': 'u2net_cloth.pth',
                    'mobile_sam': 'mobile_sam.pt',
                    'isnet': 'isnetis.onnx',
                    'clothing_classifier': 'clothing_classifier.pth'
                }
                
                for model_key, filename in model_files.items():
                    for base_path in base_paths:
                        full_path = os.path.join(base_path, filename)
                        if os.path.exists(full_path):
                            self.model_paths[model_key] = full_path
                            logger.info(f"âœ… {model_key} ë°œê²¬: {full_path}")
                            break
            
        except Exception as e:
            logger.error(f"âŒ ê°•í™”ëœ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def _load_all_enhanced_models(self):
        """ëª¨ë“  ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”©"""
        try:
            if not TORCH_AVAILABLE:
                logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
                return
            
            logger.info("ğŸ”„ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # 1. SAM ëª¨ë¸ë“¤ ë¡œë”©
            self._load_sam_models()
            
            # 2. U2Net Cloth ë¡œë”©
            self._load_u2net_model()
            
            # 3. ê¸°íƒ€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë“¤
            self._load_other_segmentation_models()
            
            # 4. ì˜ë¥˜ ë¶„ë¥˜ê¸° ë¡œë”©
            self._load_clothing_classifier()
            
            # 5. RemBG ëª¨ë¸ë“¤ ë¡œë”©
            self._load_rembg_models()
            
            loaded_count = sum(self.models_loading_status.values())
            total_models = len(self.models_loading_status)
            logger.info(f"ğŸ§  ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            logger.error(f"âŒ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _load_sam_models(self):
        """SAM ëª¨ë¸ë“¤ ë¡œë”©"""
        if 'sam_huge' in self.model_paths:
            try:
                from segment_anything import build_sam_vit_h, SamPredictor
                sam_model = build_sam_vit_h(checkpoint=self.model_paths['sam_huge'])
                sam_model.to(self.device)
                predictor = SamPredictor(sam_model)
                
                self.ai_models['sam_huge'] = {
                    'model': sam_model,
                    'predictor': predictor,
                    'type': 'vit_h'
                }
                self.models_loading_status['sam_huge'] = True
                logger.info("âœ… SAM ViT-Huge ë¡œë”© ì™„ë£Œ (2445.7MB)")
            except Exception as e:
                logger.error(f"âŒ SAM ViT-Huge ë¡œë”© ì‹¤íŒ¨: {e}")
        
        if 'sam_large' in self.model_paths:
            try:
                from segment_anything import build_sam_vit_l, SamPredictor
                sam_model = build_sam_vit_l(checkpoint=self.model_paths['sam_large'])
                sam_model.to(self.device)
                predictor = SamPredictor(sam_model)
                
                self.ai_models['sam_large'] = {
                    'model': sam_model,
                    'predictor': predictor,
                    'type': 'vit_l'
                }
                self.models_loading_status['sam_large'] = True
                logger.info("âœ… SAM ViT-Large ë¡œë”© ì™„ë£Œ (1249.1MB)")
            except Exception as e:
                logger.error(f"âŒ SAM ViT-Large ë¡œë”© ì‹¤íŒ¨: {e}")
        
        if 'sam_base' in self.model_paths:
            try:
                from segment_anything import build_sam_vit_b, SamPredictor
                sam_model = build_sam_vit_b(checkpoint=self.model_paths['sam_base'])
                sam_model.to(self.device)
                predictor = SamPredictor(sam_model)
                
                self.ai_models['sam_base'] = {
                    'model': sam_model,
                    'predictor': predictor,
                    'type': 'vit_b'
                }
                self.models_loading_status['sam_base'] = True
                logger.info("âœ… SAM ViT-Base ë¡œë”© ì™„ë£Œ (375.0MB)")
            except Exception as e:
                logger.error(f"âŒ SAM ViT-Base ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _load_u2net_model(self):
        """U2Net ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ êµ¬í˜„ ìœ ì§€)"""
        if 'u2net_cloth' in self.model_paths:
            try:
                # ê¸°ì¡´ RealU2NetClothModel í´ë˜ìŠ¤ ì‚¬ìš©
                from .step_03_cloth_segmentation import RealU2NetClothModel
                
                model = RealU2NetClothModel.from_checkpoint(
                    checkpoint_path=self.model_paths['u2net_cloth'],
                    device=self.device
                )
                self.ai_models['u2net_cloth'] = model
                self.models_loading_status['u2net_cloth'] = True
                logger.info("âœ… U2Net Cloth ë¡œë”© ì™„ë£Œ (168.1MB)")
            except Exception as e:
                logger.error(f"âŒ U2Net Cloth ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _load_other_segmentation_models(self):
        """ê¸°íƒ€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë“¤ ë¡œë”©"""
        # Mobile SAM
        if 'mobile_sam' in self.model_paths:
            try:
                # ê¸°ì¡´ êµ¬í˜„ ì‚¬ìš©
                mobile_sam = torch.jit.load(self.model_paths['mobile_sam'], map_location=self.device)
                mobile_sam.eval()
                self.ai_models['mobile_sam'] = mobile_sam
                self.models_loading_status['mobile_sam'] = True
                logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ (38.8MB)")
            except Exception as e:
                logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # ISNet ONNX
        if 'isnet' in self.model_paths and ONNX_AVAILABLE:
            try:
                providers = ['CPUExecutionProvider']
                if MPS_AVAILABLE:
                    providers.insert(0, 'CoreMLExecutionProvider')
                
                ort_session = ort.InferenceSession(self.model_paths['isnet'], providers=providers)
                self.ai_models['isnet'] = ort_session
                self.models_loading_status['isnet'] = True
                logger.info("âœ… ISNet ONNX ë¡œë”© ì™„ë£Œ (168.1MB)")
            except Exception as e:
                logger.error(f"âŒ ISNet ONNX ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _load_clothing_classifier(self):
        """ì˜ë¥˜ ë¶„ë¥˜ê¸° ë¡œë”©"""
        try:
            model_path = self.model_paths.get('clothing_classifier')
            success = self.clothing_classifier.load_model(model_path)
            
            if success:
                self.models_loading_status['clothing_classifier'] = True
                logger.info("âœ… ì˜ë¥˜ ë¶„ë¥˜ê¸° ë¡œë”© ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ ì˜ë¥˜ ë¶„ë¥˜ê¸° ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ë¶„ë¥˜ê¸° ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _load_rembg_models(self):
        """RemBG ëª¨ë¸ë“¤ ë¡œë”©"""
        if REMBG_AVAILABLE:
            try:
                # U2Net ì„¸ì…˜
                try:
                    u2net_session = new_session("u2net")
                    self.ai_models['rembg_u2net'] = u2net_session
                    self.models_loading_status['rembg_u2net'] = True
                    logger.info("âœ… RemBG U2Net ë¡œë”© ì™„ë£Œ")
                except:
                    pass
                
                # Silueta ì„¸ì…˜
                try:
                    silueta_session = new_session("silueta")
                    self.ai_models['rembg_silueta'] = silueta_session
                    self.models_loading_status['rembg_silueta'] = True
                    logger.info("âœ… RemBG Silueta ë¡œë”© ì™„ë£Œ")
                except:
                    pass
                    
            except Exception as e:
                logger.error(f"âŒ RemBG ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ê°•í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
        if 'sam_large' in self.ai_models:
            methods.append(SegmentationMethod.SAM_LARGE)
        if 'sam_base' in self.ai_models:
            methods.append(SegmentationMethod.SAM_BASE)
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
        if 'mobile_sam' in self.ai_models:
            methods.append(SegmentationMethod.MOBILE_SAM)
        if 'isnet' in self.ai_models:
            methods.append(SegmentationMethod.ISNET)
        if 'rembg_u2net' in self.ai_models:
            methods.append(SegmentationMethod.REMBG_U2NET)
        if 'rembg_silueta' in self.ai_models:
            methods.append(SegmentationMethod.REMBG_SILUETA)
        
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
        
        return methods
    
    # ==============================================
    # ğŸ”¥ 13. í•µì‹¬: ê°•í™”ëœ _run_ai_inference() ë©”ì„œë“œ
    # ==============================================
    
    async def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  ë¡œì§ - BaseStepMixin v19.1ì—ì„œ í˜¸ì¶œë¨
        
        í”„ë¡œë•ì…˜ ë ˆë²¨ íŒŒì´í”„ë¼ì¸:
        1. ê³ ê¸‰ ì „ì²˜ë¦¬ (í’ˆì§ˆ í‰ê°€, ì¡°ëª… ì •ê·œí™”, ROI ê²€ì¶œ)
        2. ì˜ë¥˜ ë¶„ë¥˜ AI
        3. ê³ ê¸‰ SAM í”„ë¡¬í”„íŠ¸ ìƒì„±
        4. ë‹¤ì¤‘ ëª¨ë¸ ì„¸ê·¸ë©˜í…Œì´ì…˜
        5. ê³ ê¸‰ í›„ì²˜ë¦¬ (Graph Cut, Active Contour, Watershed)
        6. í’ˆì§ˆ ê²€ì¦ ë° ìë™ ì¬ì‹œë„
        7. ì‹¤ì‹œê°„ í”¼ë“œë°±
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  ì‹œì‘")
            start_time = time.time()
            
            # 0. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„° 'image'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            image = processed_input['image']
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
                image_array = image
            elif PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
                image_array = np.array(image)
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ì´ì „ Step ë°ì´í„°
            person_parsing = processed_input.get('from_step_01', {})
            pose_info = processed_input.get('from_step_02', {})
            
            progress_callback = processed_input.get('progress_callback')
            
            # ==============================================
            # ğŸ”¥ Phase 1: ê³ ê¸‰ ì „ì²˜ë¦¬
            # ==============================================
            
            if progress_callback:
                progress_callback(10, "ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹œì‘...")
            
            preprocessing_start = time.time()
            
            # 1.1 ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
            quality_scores = {}
            if self.config.enable_quality_assessment:
                quality_scores = self.preprocessor.assess_image_quality(image_array)
                self.logger.info(f"ğŸ“Š ì´ë¯¸ì§€ í’ˆì§ˆ: {quality_scores['overall']:.3f}")
            
            # 1.2 ì¡°ëª… ì •ê·œí™”
            processed_image = image_array
            if self.config.enable_lighting_normalization:
                processed_image = self.preprocessor.normalize_lighting(processed_image)
            
            # 1.3 ìƒ‰ìƒ ë³´ì •
            if self.config.enable_color_correction:
                processed_image = self.preprocessor.correct_colors(processed_image)
            
            # 1.4 ROI ê²€ì¶œ
            roi_box = None
            if self.config.enable_roi_detection:
                roi_box = self.preprocessor.detect_roi(processed_image)
                self.logger.info(f"ğŸ¯ ROI ê²€ì¶œ: {roi_box}")
            
            # 1.5 ë°°ê²½ ë³µì¡ë„ ë¶„ì„
            background_complexity = 0.5
            if self.config.enable_background_analysis:
                background_complexity = self.preprocessor.analyze_background_complexity(processed_image)
                self.logger.info(f"ğŸ” ë°°ê²½ ë³µì¡ë„: {background_complexity:.3f}")
            
            preprocessing_time = time.time() - preprocessing_start
            self.ai_stats['preprocessing_time'] += preprocessing_time
            
            if progress_callback:
                progress_callback(25, "ì „ì²˜ë¦¬ ì™„ë£Œ, ì˜ë¥˜ ë¶„ë¥˜ ì‹œì‘...")
            
            # ==============================================
            # ğŸ”¥ Phase 2: ì˜ë¥˜ ë¶„ë¥˜ AI
            # ==============================================
            
            classification_start = time.time()
            
            clothing_type_str = "unknown"
            classification_confidence = 0.0
            
            if self.config.enable_clothing_classification and self.clothing_classifier.is_loaded:
                clothing_type_str, classification_confidence = self.clothing_classifier.classify(processed_image)
                self.logger.info(f"ğŸ‘• ì˜ë¥˜ ë¶„ë¥˜: {clothing_type_str} (ì‹ ë¢°ë„: {classification_confidence:.3f})")
            
            # íŒíŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if 'clothing_type' in processed_input and processed_input['clothing_type']:
                hint_type = processed_input['clothing_type']
                if classification_confidence < self.config.classification_confidence_threshold:
                    clothing_type_str = hint_type
                    self.logger.info(f"ğŸ’¡ íŒíŠ¸ ì‚¬ìš©: {hint_type}")
            
            # ClothingType enumìœ¼ë¡œ ë³€í™˜
            try:
                clothing_type = ClothingType(clothing_type_str.lower())
            except ValueError:
                clothing_type = ClothingType.UNKNOWN
            
            classification_time = time.time() - classification_start
            self.ai_stats['classification_time'] += classification_time
            
            if progress_callback:
                progress_callback(40, f"ì˜ë¥˜ ë¶„ë¥˜ ì™„ë£Œ: {clothing_type_str}")
            
            # ==============================================
            # ğŸ”¥ Phase 3: í’ˆì§ˆ ë ˆë²¨ ê²°ì • ë° ì„¸ê·¸ë©˜í…Œì´ì…˜
            # ==============================================
            
            quality_level = self._determine_enhanced_quality_level(processed_input, quality_scores, background_complexity)
            
            segmentation_start = time.time()
            
            # ì¬ì‹œë„ ë¡œì§
            best_mask = None
            best_confidence = 0.0
            best_method = "none"
            retry_count = 0
            max_retries = self.config.max_retry_attempts if self.config.enable_auto_retry else 1
            
            while retry_count < max_retries:
                if progress_callback:
                    progress_callback(50 + retry_count * 15, f"AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„ {retry_count + 1}")
                
                try:
                    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
                    mask, confidence, method_used = await self._run_enhanced_segmentation(
                        processed_image, clothing_type, quality_level, roi_box, person_parsing, pose_info
                    )
                    
                    if mask is not None:
                        # í’ˆì§ˆ ê²€ì¦
                        if self.config.enable_quality_validation:
                            quality_metrics = self.quality_validator.evaluate_mask_quality(mask, processed_image)
                            overall_quality = quality_metrics['overall']
                            
                            self.logger.info(f"ğŸ“Š ë§ˆìŠ¤í¬ í’ˆì§ˆ: {overall_quality:.3f}")
                            
                            if overall_quality >= self.config.quality_threshold:
                                best_mask = mask
                                best_confidence = confidence
                                best_method = method_used
                                break
                            else:
                                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë¯¸ë‹¬ ({overall_quality:.3f} < {self.config.quality_threshold})")
                                self.ai_stats['quality_failures'] += 1
                        else:
                            best_mask = mask
                            best_confidence = confidence
                            best_method = method_used
                            break
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„ {retry_count + 1} ì‹¤íŒ¨: {e}")
                
                retry_count += 1
                self.ai_stats['retry_attempts'] += 1
                
                if retry_count < max_retries:
                    # ë‹¤ìŒ ì‹œë„ë¥¼ ìœ„í•´ ë°©ë²• ë³€ê²½
                    quality_level = QualityLevel.HIGH if quality_level == QualityLevel.BALANCED else QualityLevel.ULTRA
            
            if best_mask is None:
                raise RuntimeError("ëª¨ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„ ì‹¤íŒ¨")
            
            segmentation_time = time.time() - segmentation_start
            self.ai_stats['segmentation_time'] += segmentation_time
            
            if progress_callback:
                progress_callback(75, f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ: {best_method}")
            
            # ==============================================
            # ğŸ”¥ Phase 4: ê³ ê¸‰ í›„ì²˜ë¦¬
            # ==============================================
            
            postprocessing_start = time.time()
            
            final_mask = best_mask
            
            # Graph Cut ì ìš©
            if self.config.enable_graph_cut:
                final_mask = self.postprocessor.apply_graph_cut(processed_image, final_mask)
                self.logger.info("âœ… Graph Cut í›„ì²˜ë¦¬ ì™„ë£Œ")
            
            # Active Contour ì ìš©
            if self.config.enable_active_contour:
                final_mask = self.postprocessor.apply_active_contour(processed_image, final_mask)
                self.logger.info("âœ… Active Contour í›„ì²˜ë¦¬ ì™„ë£Œ")
            
            # Watershed ì ìš©
            if self.config.enable_watershed:
                final_mask = self.postprocessor.apply_watershed(processed_image, final_mask)
                self.logger.info("âœ… Watershed í›„ì²˜ë¦¬ ì™„ë£Œ")
            
            # ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
            if self.config.enable_multiscale_processing:
                final_mask = self.postprocessor.apply_multiscale_processing(processed_image, final_mask)
                self.logger.info("âœ… ë©€í‹°ìŠ¤ì¼€ì¼ í›„ì²˜ë¦¬ ì™„ë£Œ")
            
            postprocessing_time = time.time() - postprocessing_start
            self.ai_stats['postprocessing_time'] += postprocessing_time
            
            if progress_callback:
                progress_callback(90, "í›„ì²˜ë¦¬ ì™„ë£Œ, ê²°ê³¼ ìƒì„± ì¤‘...")
            
            # ==============================================
            # ğŸ”¥ Phase 5: ìµœì¢… í’ˆì§ˆ ê²€ì¦ ë° ê²°ê³¼ ìƒì„±
            # ==============================================
            
            quality_validation_start = time.time()
            
            # ìµœì¢… í’ˆì§ˆ í‰ê°€
            final_quality_metrics = {}
            if self.config.enable_quality_validation:
                final_quality_metrics = self.quality_validator.evaluate_mask_quality(final_mask, processed_image)
            
            quality_validation_time = time.time() - quality_validation_start
            self.ai_stats['quality_validation_time'] += quality_validation_time
            
            # ì‹œê°í™” ìƒì„±
            visualizations = {}
            if self.config.enable_visualization:
                visualizations = self._create_enhanced_visualizations(
                    processed_image, final_mask, clothing_type, roi_box
                )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_time = time.time() - start_time
            self._update_enhanced_stats(best_method, best_confidence, total_time, final_quality_metrics)
            
            if progress_callback:
                progress_callback(100, "ì™„ë£Œ!")
            
            # ==============================================
            # ğŸ”¥ ìµœì¢… ê²°ê³¼ ë°˜í™˜
            # ==============================================
            
            ai_result = {
                # í•µì‹¬ ê²°ê³¼
                'cloth_mask': final_mask,
                'segmented_clothing': self._apply_mask_to_image(processed_image, final_mask),
                'confidence': best_confidence,
                'clothing_type': clothing_type.value,
                'method_used': best_method,
                'processing_time': total_time,
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                'quality_score': final_quality_metrics.get('overall', 0.5),
                'quality_metrics': final_quality_metrics,
                'image_quality_scores': quality_scores,
                'mask_area_ratio': np.sum(final_mask > 0) / final_mask.size if NUMPY_AVAILABLE else 0.0,
                'boundary_smoothness': final_quality_metrics.get('boundary_smoothness', 0.5),
                
                # ë¶„ë¥˜ ê²°ê³¼
                'clothing_classification': {
                    'predicted_type': clothing_type_str,
                    'confidence': classification_confidence,
                    'features': self.clothing_classifier.extract_features(processed_image).tolist() if self.clothing_classifier.is_loaded else []
                },
                
                # ì „ì²˜ë¦¬ ê²°ê³¼
                'preprocessing_results': {
                    'roi_box': roi_box,
                    'background_complexity': background_complexity,
                    'lighting_normalized': self.config.enable_lighting_normalization,
                    'color_corrected': self.config.enable_color_correction
                },
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                'performance_breakdown': {
                    'preprocessing_time': preprocessing_time,
                    'classification_time': classification_time,
                    'segmentation_time': segmentation_time,
                    'postprocessing_time': postprocessing_time,
                    'quality_validation_time': quality_validation_time,
                    'retry_count': retry_count
                },
                
                # ì‹œê°í™”
                **visualizations,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'ai_models_used': list(self.ai_models.keys()),
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'enhanced_features': True,
                    'production_ready': True,
                    'quality_level': quality_level.value,
                    'step_model_requests_compatible': True,
                    'version': '30.0'
                },
                
                # Step ê°„ ì—°ë™ ë°ì´í„°
                'cloth_features': self._extract_enhanced_cloth_features(final_mask, processed_image),
                'cloth_contours': self._extract_cloth_contours(final_mask),
                'clothing_category': clothing_type.value,
                'roi_information': roi_box
            }
            
            self.logger.info(f"âœ… {self.step_name} ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  ì™„ë£Œ - {total_time:.2f}ì´ˆ")
            self.logger.info(f"   - ë°©ë²•: {best_method}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {best_confidence:.3f}")
            self.logger.info(f"   - í’ˆì§ˆ: {final_quality_metrics.get('overall', 0.5):.3f}")
            self.logger.info(f"   - ì˜ë¥˜ íƒ€ì…: {clothing_type.value}")
            self.logger.info(f"   - ì¬ì‹œë„: {retry_count}íšŒ")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                'cloth_mask': None,
                'segmented_clothing': None,
                'confidence': 0.0,
                'clothing_type': 'error',
                'method_used': 'error',
                'quality_score': 0.0,
                'error': str(e)
            }