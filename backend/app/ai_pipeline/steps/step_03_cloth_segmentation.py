# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” v21.0
===============================================================================

ğŸ¯ step_model_requirements.py ì™„ì „ í˜¸í™˜:
âœ… DetailedDataSpec êµ¬ì¡° ì™„ì „ ì ìš©
âœ… EnhancedRealModelRequest í‘œì¤€ ì¤€ìˆ˜  
âœ… step_input_schema/step_output_schema ì™„ì „ êµ¬í˜„
âœ… accepts_from_previous_step/provides_to_next_step ì™„ì „ ì •ì˜
âœ… api_input_mapping/api_output_mapping êµ¬í˜„
âœ… preprocessing_steps/postprocessing_steps ì™„ì „ ì •ì˜
âœ… RealSAMModel í´ë˜ìŠ¤ëª… í‘œì¤€ ì¤€ìˆ˜
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (sam_vit_h_4b8939.pth 2445.7MB)
âœ… BaseStepMixin v16.0 í˜¸í™˜ì„± ìœ ì§€
âœ… TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… M3 Max 128GB ìµœì í™”

AI ê°•í™” ì‚¬í•­:
ğŸ§  ì§„ì§œ SAM, U2Net, ISNet, Mobile SAM AI ì¶”ë¡ 
ğŸ”¥ OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
ğŸ¨ AI ê°•í™” ì‹œê°í™” (Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§)
âš¡ M3 Max MPS ê°€ì†
ğŸ¯ ì‹¤ì œ ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
ğŸ”§ ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
ğŸ“Š í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ì™„ì „ êµ¬í˜„

Author: MyCloset AI Team
Date: 2025-07-25
Version: v21.0 (step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™”)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import gc
import hashlib
import json
import base64
import weakref
import math
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import platform
import subprocess

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ import (ëŸ°íƒ€ì„ì—ëŠ” import ì•ˆë¨)
    from app.ai_pipeline.utils.model_loader import ModelLoader, StepModelInterface
    from ..steps.base_step_mixin import BaseStepMixin, UnifiedDependencyManager
    from ..factories.step_factory import StepFactory
    from ..interfaces.step_interface import StepInterface
    from app.core.di_container import DIContainer
    from app.ai_pipeline.utils.step_model_requirements import (
        EnhancedRealModelRequest, DetailedDataSpec, get_enhanced_step_request
    )

# ==============================================
# ğŸ”¥ 2. í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (conda í™˜ê²½ ìš°ì„ )
# ==============================================

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½ ìš°ì„ )")
except ImportError:
    logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")

# PIL Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
except ImportError:
    logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# PyTorch Import (conda í™˜ê²½ ìš°ì„ )
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥ (M3 Max ìµœì í™”)")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
REMBG_AVAILABLE = False
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
    logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ RemBG ì—†ìŒ - pip install rembg")

SKLEARN_AVAILABLE = False
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
    logger.info("ğŸ“ˆ scikit-learn ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ scikit-learn ì—†ìŒ - conda install scikit-learn")

SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - pip install segment-anything")

TRANSFORMERS_AVAILABLE = False
try:
    from transformers import pipeline, CLIPModel, CLIPProcessor
    TRANSFORMERS_AVAILABLE = True
    logger.info("ğŸ¤— Transformers ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Transformers ì—†ìŒ - pip install transformers")

ESRGAN_AVAILABLE = False
try:
    from basicsr.archs.rrdbnet_arch import RRDBNet
    from basicsr.utils.download_util import load_file_from_url
    ESRGAN_AVAILABLE = True
    logger.info("ğŸ¨ Real-ESRGAN ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Real-ESRGAN ì—†ìŒ - pip install basicsr")

ONNX_AVAILABLE = False
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
    logger.info("âš¡ ONNX Runtime ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ ONNX Runtime ì—†ìŒ - pip install onnxruntime")

# ==============================================
# ğŸ”¥ 3. step_model_requirements.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ
# ==============================================

def get_step_requirements():
    """step_model_requirements.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
        import importlib
        requirements_module = importlib.import_module('app.ai_pipeline.utils.step_model_requirements')
        
        # ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
        get_enhanced_step_request = getattr(requirements_module, 'get_enhanced_step_request', None)
        if get_enhanced_step_request:
            return get_enhanced_step_request("ClothSegmentationStep")
        
        # í´ë°±: ì§ì ‘ ì ‘ê·¼
        REAL_STEP_MODEL_REQUESTS = getattr(requirements_module, 'REAL_STEP_MODEL_REQUESTS', {})
        return REAL_STEP_MODEL_REQUESTS.get("ClothSegmentationStep")
        
    except ImportError as e:
        logger.warning(f"âš ï¸ step_model_requirements ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ
STEP_REQUIREMENTS = get_step_requirements()

if STEP_REQUIREMENTS:
    logger.info("âœ… step_model_requirements.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
    logger.info(f"   - Model: {STEP_REQUIREMENTS.model_name}")
    logger.info(f"   - AI Class: {STEP_REQUIREMENTS.ai_class}")
    logger.info(f"   - Primary File: {STEP_REQUIREMENTS.primary_file} ({STEP_REQUIREMENTS.primary_size_mb}MB)")
else:
    logger.warning("âš ï¸ step_model_requirements.pyì—ì„œ ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

# ==============================================
# ğŸ”¥ 4. ë™ì  Import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.debug(f"BaseStepMixin ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_unified_dependency_manager_class():
    """UnifiedDependencyManager í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'UnifiedDependencyManager', None)
    except ImportError as e:
        logger.debug(f"UnifiedDependencyManager ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        return None
    except ImportError as e:
        logger.debug(f"ModelLoader ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_step_interface_class():
    """StepInterface í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interfaces.step_interface')
        return getattr(module, 'StepInterface', None)
    except ImportError as e:
        logger.debug(f"StepInterface ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_di_container():
    """DI Containerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib  
        module = importlib.import_module('app.core.di_container')
        get_container = getattr(module, 'get_di_container', None)
        if get_container:
            return get_container()
        return None
    except ImportError as e:
        logger.debug(f"DI Container ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 5. step_model_requirements.py í˜¸í™˜ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• (step_model_requirements.py í˜¸í™˜)"""
    SAM_HUGE = "sam_huge"           # SAM ViT-Huge (2445.7MB)
    U2NET_CLOTH = "u2net_cloth"     # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    MOBILE_SAM = "mobile_sam"       # Mobile SAM (38.8MB)
    ISNET = "isnet"                 # ISNet ONNX (168.1MB)
    HYBRID_AI = "hybrid_ai"         # ì—¬ëŸ¬ AI ëª¨ë¸ ì¡°í•©
    AUTO_AI = "auto_ai"             # ìë™ AI ëª¨ë¸ ì„ íƒ

class ClothingType(Enum):
    """ì˜ë¥˜ íƒ€ì… (step_model_requirements.py í˜¸í™˜)"""
    SHIRT = "shirt"
    DRESS = "dress"
    PANTS = "pants"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # Mobile SAM
    BALANCED = "balanced"   # U2Net + ISNet
    HIGH = "high"          # SAM + U2Net
    ULTRA = "ultra"        # Hybrid AI (ëª¨ë“  ëª¨ë¸)

@dataclass
class SegmentationConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì • (step_model_requirements.py í˜¸í™˜)"""
    method: SegmentationMethod = SegmentationMethod.AUTO_AI
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (1024, 1024)  # step_model_requirements í‘œì¤€
    output_size: Optional[Tuple[int, int]] = None
    enable_visualization: bool = True
    enable_post_processing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.5  # step_model_requirements í‘œì¤€
    iou_threshold: float = 0.5
    edge_smoothing: bool = True
    remove_noise: bool = True
    visualization_quality: str = "high"
    enable_caching: bool = True
    cache_size: int = 100
    show_masks: bool = True
    show_boundaries: bool = True
    overlay_opacity: float = 0.6
    esrgan_scale: int = 2

@dataclass
class SegmentationResult:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ (step_model_requirements.py í˜¸í™˜)"""
    success: bool
    mask: Optional[np.ndarray] = None
    segmented_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    quality_score: float = 0.0
    method_used: str = "unknown"
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    # ì‹œê°í™” ì´ë¯¸ì§€ë“¤
    visualization_image: Optional[Image.Image] = None
    overlay_image: Optional[Image.Image] = None
    mask_image: Optional[Image.Image] = None
    boundary_image: Optional[Image.Image] = None

@dataclass
class StepInputData:
    """Step ê°„ í‘œì¤€ ì…ë ¥ ë°ì´í„° (step_model_requirements.py í˜¸í™˜)"""
    image: Union[str, np.ndarray, Image.Image]
    metadata: Dict[str, Any] = field(default_factory=dict)
    step_history: List[str] = field(default_factory=list)
    processing_context: Dict[str, Any] = field(default_factory=dict)
    
    # step_model_requirements.py í˜¸í™˜ì„ ìœ„í•œ ì¶”ê°€ í•„ë“œ
    clothing_image: Optional[Union[str, np.ndarray, Image.Image]] = None
    prompt_points: List[Tuple[int, int]] = field(default_factory=list)
    session_id: Optional[str] = None

@dataclass 
class StepOutputData:
    """Step ê°„ í‘œì¤€ ì¶œë ¥ ë°ì´í„° (step_model_requirements.py í˜¸í™˜)"""
    success: bool
    result_data: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    step_name: str = "ClothSegmentationStep"
    processing_time: float = 0.0
    next_step_input: Optional[Dict[str, Any]] = None
    
    # step_model_requirements.py í˜¸í™˜ì„ ìœ„í•œ ì¶”ê°€ í•„ë“œ
    cloth_mask: Optional[np.ndarray] = None
    segmented_clothing: Optional[np.ndarray] = None
    confidence: float = 0.0
    clothing_type: str = "unknown"

# ==============================================
# ğŸ”¥ 6. ì˜ë¥˜ë³„ ìƒ‰ìƒ ë§¤í•‘ (ì‹œê°í™”ìš©)
# ==============================================

CLOTHING_COLORS = {
    'shirt': (255, 100, 100),      # ë¹¨ê°•
    'pants': (100, 100, 255),      # íŒŒë‘
    'dress': (255, 100, 255),      # ë¶„í™
    'jacket': (100, 255, 100),     # ì´ˆë¡
    'skirt': (255, 255, 100),      # ë…¸ë‘
    'sweater': (138, 43, 226),     # ë¸”ë£¨ë°”ì´ì˜¬ë ›
    'coat': (165, 42, 42),         # ê°ˆìƒ‰
    'top': (0, 255, 255),          # ì‹œì•ˆ
    'bottom': (255, 165, 0),       # ì˜¤ë Œì§€
    'shoes': (255, 150, 0),        # ì£¼í™©
    'bag': (150, 75, 0),           # ê°ˆìƒ‰
    'hat': (128, 0, 128),          # ë³´ë¼
    'accessory': (0, 255, 255),    # ì‹œì•ˆ
    'unknown': (128, 128, 128),    # íšŒìƒ‰
}

# ==============================================
# ğŸ”¥ 7. AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class AIImageProcessor:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    
    @staticmethod
    def ai_resize(image: Union[np.ndarray, Image.Image], target_size: Tuple[int, int]) -> Image.Image:
        """AI ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ (OpenCV resize ëŒ€ì²´)"""
        try:
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            
            # Real-ESRGAN ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
            if ESRGAN_AVAILABLE and min(target_size) > min(image.size):
                return AIImageProcessor.esrgan_upscale(image, target_size)
            else:
                return image.resize(target_size, Image.Resampling.LANCZOS)
                
        except Exception as e:
            logger.warning(f"âš ï¸ AI ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            return image.resize(target_size, Image.Resampling.LANCZOS)
    
    @staticmethod
    def esrgan_upscale(image: Image.Image, target_size: Tuple[int, int]) -> Image.Image:
        """Real-ESRGAN ê¸°ë°˜ ì—…ìŠ¤ì¼€ì¼ë§"""
        try:
            if not ESRGAN_AVAILABLE or not TORCH_AVAILABLE:
                return image.resize(target_size, Image.Resampling.LANCZOS)
            
            # ê°„ë‹¨í•œ ì—…ìŠ¤ì¼€ì¼ë§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ êµ¬í˜„ í•„ìš”)
            scale_factor = min(target_size[0] / image.size[0], target_size[1] / image.size[1])
            if scale_factor <= 1.0:
                return image.resize(target_size, Image.Resampling.LANCZOS)
            
            # Real-ESRGAN ëŒ€ì‹  ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©
            intermediate_size = (int(image.size[0] * scale_factor), int(image.size[1] * scale_factor))
            upscaled = image.resize(intermediate_size, Image.Resampling.LANCZOS)
            
            if upscaled.size != target_size:
                upscaled = upscaled.resize(target_size, Image.Resampling.LANCZOS)
            
            return upscaled
            
        except Exception as e:
            logger.warning(f"âš ï¸ ESRGAN ì—…ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
            return image.resize(target_size, Image.Resampling.LANCZOS)
    
    @staticmethod
    def ai_detect_edges(image: np.ndarray, threshold1: int = 50, threshold2: int = 150) -> np.ndarray:
        """AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ (cv2.Canny ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                # PyTorch ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
                return AIImageProcessor._simple_edge_detection(image)
            
            # PyTorch ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
            tensor = torch.from_numpy(image).float()
            if len(tensor.shape) == 3:
                tensor = tensor.mean(dim=2)  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            
            tensor = tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
            
            # Sobel í•„í„° ì •ì˜
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            sobel_x = sobel_x.view(1, 1, 3, 3)
            sobel_y = sobel_y.view(1, 1, 3, 3)
            
            # ì»¨ë³¼ë£¨ì…˜ ì ìš©
            grad_x = F.conv2d(tensor, sobel_x, padding=1)
            grad_y = F.conv2d(tensor, sobel_y, padding=1)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ê³„ì‚°
            magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)
            
            # ì„ê³„ê°’ ì ìš©
            edges = (magnitude > threshold1).float()
            edges = edges.squeeze().numpy()
            
            return (edges * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_edge_detection(image)
    
    @staticmethod
    def _simple_edge_detection(image: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ (í´ë°±)"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            
            # í¬ê¸° ë§ì¶¤
            grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='edge')
            grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='edge')
            
            magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)
            edges = (magnitude > 50).astype(np.uint8) * 255
            
            return edges
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros_like(image[:, :, 0] if len(image.shape) == 3 else image, dtype=np.uint8)
    
    @staticmethod
    def ai_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """AI ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚° (cv2.morphologyEx ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
            
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            
            # êµ¬ì¡°ì  ìš”ì†Œ (ì»¤ë„)
            kernel = torch.ones(1, 1, kernel_size, kernel_size)
            padding = kernel_size // 2
            
            if operation.lower() == "closing":
                # Dilation í›„ Erosion
                dilated = F.max_pool2d(tensor, kernel_size, stride=1, padding=padding)
                result = -F.max_pool2d(-dilated, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "opening":
                # Erosion í›„ Dilation
                eroded = -F.max_pool2d(-tensor, kernel_size, stride=1, padding=padding)
                result = F.max_pool2d(eroded, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "dilation":
                result = F.max_pool2d(tensor, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "erosion":
                result = -F.max_pool2d(-tensor, kernel_size, stride=1, padding=padding)
            else:
                result = tensor
            
            result_np = result.squeeze().numpy()
            return (result_np * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
    
    @staticmethod
    def _simple_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° (í´ë°±)"""
        try:
            # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•´ì•¼ í•¨)
            if operation.lower() == "closing":
                # ê°„ë‹¨í•œ í™€ ì±„ìš°ê¸°
                from scipy import ndimage
                filled = ndimage.binary_fill_holes(mask > 128)
                return (filled * 255).astype(np.uint8)
            else:
                return mask
        except ImportError:
            return mask
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return mask
    
    @staticmethod
    def ai_gaussian_blur(image: np.ndarray, kernel_size: int = 5, sigma: float = 1.0) -> np.ndarray:
        """AI ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (cv2.GaussianBlur ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_blur(image, kernel_size)
            
            if len(image.shape) == 2:
                tensor = torch.from_numpy(image.astype(np.float32)).unsqueeze(0).unsqueeze(0)
            else:
                tensor = torch.from_numpy(image.astype(np.float32)).permute(2, 0, 1).unsqueeze(0)
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
            coords = torch.arange(kernel_size, dtype=torch.float32) - kernel_size // 2
            kernel_1d = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
            kernel_1d = kernel_1d / kernel_1d.sum()
            
            kernel_2d = kernel_1d[:, None] * kernel_1d[None, :]
            kernel_2d = kernel_2d.expand(tensor.size(1), 1, kernel_size, kernel_size)
            
            padding = kernel_size // 2
            blurred = F.conv2d(tensor, kernel_2d, padding=padding, groups=tensor.size(1))
            
            if len(image.shape) == 2:
                result = blurred.squeeze().numpy()
            else:
                result = blurred.squeeze().permute(1, 2, 0).numpy()
            
            return result.astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_blur(image, kernel_size)
    
    @staticmethod
    def _simple_blur(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
        """ê°„ë‹¨í•œ ë¸”ëŸ¬ (í´ë°±)"""
        try:
            from scipy import ndimage
            sigma = kernel_size / 3.0
            return ndimage.gaussian_filter(image, sigma=sigma)
        except ImportError:
            return image
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ë¸”ëŸ¬ ì‹¤íŒ¨: {e}")
            return image

# ==============================================
# ğŸ”¥ 8. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (step_model_requirements.py í˜¸í™˜)
# ==============================================

class REBNCONV(nn.Module):
    """U2-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡"""
    
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class RSU7(nn.Module):
    """U2-Net RSU-7 ë¸”ë¡ (ì™„ì „í•œ êµ¬í˜„)"""
    
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample6 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample5 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
    
    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = self.upsample6(hx6d)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = self.upsample5(hx5d)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = self.upsample4(hx4d)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = self.upsample3(hx3d)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = self.upsample2(hx2d)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RealU2NetClothModel(nn.Module):
    """ì‹¤ì œ U2-Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸ (u2net.pth 168.1MB í™œìš©) - step_model_requirements.py í˜¸í™˜"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super(RealU2NetClothModel, self).__init__()
        
        # ì¸ì½”ë”
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU7(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU7(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU7(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU7(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU7(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU7(1024, 256, 512)
        self.stage4d = RSU7(1024, 128, 256)
        self.stage3d = RSU7(512, 64, 128)
        self.stage2d = RSU7(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # Side outputs
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
        
        # step_model_requirements.py í˜¸í™˜ ì •ë³´
        self.model_name = "RealU2NetClothModel"
        self.version = "2.0"
        self.parameter_count = self._count_parameters()
        self.cloth_specialized = True
        
    def _count_parameters(self):
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx5d = self.stage5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # Side outputs
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=d1.shape[2:], mode='bilinear', align_corners=False)
        
        # ìµœì¢… ì¶œë ¥
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (u2net.pth 168.1MB)"""
        model = cls()
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                logger.info(f"ğŸ”„ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        model.load_state_dict(checkpoint['model'])
                    elif 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                else:
                    model.load_state_dict(checkpoint)
                    
                logger.info(f"âœ… U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model.parameter_count:,} íŒŒë¼ë¯¸í„°")
            except Exception as e:
                logger.warning(f"âš ï¸ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        model.to(device)
        model.eval()
        return model

class RealSAMModel(nn.Module):
    """ì‹¤ì œ SAM ëª¨ë¸ ë˜í¼ (sam_vit_h_4b8939.pth 2445.7MB í™œìš©) - step_model_requirements.py í‘œì¤€"""
    
    def __init__(self, model_type: str = "vit_h"):
        super(RealSAMModel, self).__init__()
        self.model_type = model_type
        self.model_name = f"RealSAMModel_{model_type}"
        self.version = "2.0"
        self.sam_model = None
        self.predictor = None
        self.is_loaded = False
        
    def load_sam_model(self, checkpoint_path: str):
        """SAM ëª¨ë¸ ë¡œë“œ (sam_vit_h_4b8939.pth 2445.7MB)"""
        try:
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ SAM ëª¨ë¸ ë¡œë”©: {checkpoint_path}")
            
            # SAM ëª¨ë¸ ë¹Œë“œ
            if self.model_type == "vit_h":
                self.sam_model = sam.build_sam_vit_h(checkpoint=checkpoint_path)
            elif self.model_type == "vit_b":
                self.sam_model = sam.build_sam_vit_b(checkpoint=checkpoint_path)
            else:
                self.sam_model = sam.build_sam(checkpoint=checkpoint_path)
            
            # Predictor ìƒì„±
            self.predictor = sam.SamPredictor(self.sam_model)
            self.is_loaded = True
            
            logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_type}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def forward(self, x):
        """ë”ë¯¸ forward (SAMì€ íŠ¹ë³„í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)"""
        if not self.is_loaded:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        return x
    
    def segment_clothing(self, image_array: np.ndarray, clothing_type: str = "shirt") -> Dict[str, np.ndarray]:
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì˜ë¥˜ íƒ€ì…ë³„ íŠ¹í™”) - step_model_requirements.py í˜¸í™˜"""
        try:
            if not self.is_loaded or self.predictor is None:
                logger.warning("âš ï¸ SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {}
            
            # ì´ë¯¸ì§€ ì„¤ì •
            self.predictor.set_image(image_array)
            
            # ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ìƒì„±
            height, width = image_array.shape[:2]
            clothing_prompts = self._generate_clothing_prompts(clothing_type, width, height)
            
            results = {}
            
            for cloth_area, points in clothing_prompts.items():
                try:
                    # SAM ì˜ˆì¸¡
                    masks, scores, logits = self.predictor.predict(
                        point_coords=np.array(points),
                        point_labels=np.ones(len(points)),
                        multimask_output=True
                    )
                    
                    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë§ˆìŠ¤í¬ ì„ íƒ
                    best_mask_idx = np.argmax(scores)
                    best_mask = masks[best_mask_idx].astype(np.uint8)
                    
                    results[cloth_area] = best_mask
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ SAM {cloth_area} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_clothing_prompts(self, clothing_type: str, width: int, height: int) -> Dict[str, List[Tuple[int, int]]]:
        """ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ìƒì„± (step_model_requirements.py í˜¸í™˜)"""
        prompts = {}
        
        if clothing_type in ["shirt", "top", "sweater"]:
            # ìƒì˜ ì˜ì—­
            prompts["upper_body"] = [
                (width // 2, height // 3),      # ê°€ìŠ´
                (width // 3, height // 2),      # ì™¼ìª½ íŒ”
                (2 * width // 3, height // 2),  # ì˜¤ë¥¸ìª½ íŒ”
            ]
        elif clothing_type in ["pants", "bottom"]:
            # í•˜ì˜ ì˜ì—­
            prompts["lower_body"] = [
                (width // 2, 2 * height // 3),  # í—ˆë¦¬
                (width // 3, 3 * height // 4),  # ì™¼ìª½ ë‹¤ë¦¬
                (2 * width // 3, 3 * height // 4),  # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
            ]
        elif clothing_type == "dress":
            # ì›í”¼ìŠ¤ ì˜ì—­
            prompts["full_dress"] = [
                (width // 2, height // 3),      # ìƒì²´
                (width // 2, 2 * height // 3),  # í•˜ì²´
                (width // 3, height // 2),      # ì™¼ìª½
                (2 * width // 3, height // 2),  # ì˜¤ë¥¸ìª½
            ]
        else:
            # ê¸°ë³¸ ì „ì²´ ì˜ë¥˜ ì˜ì—­
            prompts["clothing"] = [
                (width // 2, height // 2),      # ì¤‘ì•™
                (width // 3, height // 3),      # ì¢Œìƒ
                (2 * width // 3, 2 * height // 3),  # ìš°í•˜
            ]
        
        return prompts
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu", model_type: str = "vit_h"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ SAM ëª¨ë¸ ë¡œë“œ"""
        model = cls(model_type=model_type)
        model.load_sam_model(checkpoint_path)
        return model

class RealMobileSAMModel(nn.Module):
    """ì‹¤ì œ Mobile SAM ëª¨ë¸ (mobile_sam.pt 38.8MB í™œìš©) - step_model_requirements.py í˜¸í™˜"""
    
    def __init__(self):
        super(RealMobileSAMModel, self).__init__()
        self.model_name = "RealMobileSAMModel"
        self.version = "2.0"
        self.sam_model = None
        self.predictor = None
        self.is_loaded = False
        
    def load_mobile_sam(self, checkpoint_path: str):
        """Mobile SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ”„ Mobile SAM ë¡œë”©: {checkpoint_path}")
            
            if TORCH_AVAILABLE and os.path.exists(checkpoint_path):
                # PyTorch ëª¨ë¸ ë¡œë“œ
                self.sam_model = torch.jit.load(checkpoint_path, map_location='cpu')
                self.sam_model.eval()
                self.is_loaded = True
                
                logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ")
                return True
            else:
                logger.warning("âš ï¸ Mobile SAM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def forward(self, x):
        """Mobile SAM ì¶”ë¡ """
        if not self.is_loaded or self.sam_model is None:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        
        try:
            with torch.no_grad():
                result = self.sam_model(x)
                return result
        except Exception as e:
            logger.warning(f"âš ï¸ Mobile SAM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ Mobile SAM ë¡œë“œ"""
        model = cls()
        model.load_mobile_sam(checkpoint_path)
        model.to(device)
        return model

class RealISNetModel:
    """ì‹¤ì œ ISNet ONNX ëª¨ë¸ (isnetis.onnx 168.1MB í™œìš©) - step_model_requirements.py í˜¸í™˜"""
    
    def __init__(self):
        self.model_name = "RealISNetModel"
        self.version = "2.0"
        self.ort_session = None
        self.is_loaded = False
        
    def load_isnet_model(self, onnx_path: str):
        """ISNet ONNX ëª¨ë¸ ë¡œë“œ"""
        try:
            if not ONNX_AVAILABLE:
                logger.warning("âš ï¸ ONNX Runtimeì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ ISNet ONNX ë¡œë”©: {onnx_path}")
            
            # ONNX ì„¸ì…˜ ìƒì„±
            providers = ['CPUExecutionProvider']
            if MPS_AVAILABLE:
                providers.insert(0, 'CoreMLExecutionProvider')
            
            self.ort_session = ort.InferenceSession(onnx_path, providers=providers)
            self.is_loaded = True
            
            logger.info("âœ… ISNet ONNX ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ISNet ONNX ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image_array: np.ndarray) -> np.ndarray:
        """ISNet ì˜ˆì¸¡"""
        try:
            if not self.is_loaded or self.ort_session is None:
                logger.warning("âš ï¸ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
            
            # ì „ì²˜ë¦¬
            if len(image_array.shape) == 3:
                # RGB to BGR ë³€í™˜ ë° ì •ê·œí™”
                input_image = image_array[:, :, ::-1].astype(np.float32) / 255.0
                input_image = np.transpose(input_image, (2, 0, 1))
                input_image = np.expand_dims(input_image, axis=0)
            else:
                input_image = image_array.astype(np.float32) / 255.0
                input_image = np.expand_dims(input_image, axis=(0, 1))
            
            # ONNX ì¶”ë¡ 
            input_name = self.ort_session.get_inputs()[0].name
            result = self.ort_session.run(None, {input_name: input_image})
            
            # í›„ì²˜ë¦¬
            mask = result[0][0, 0, :, :]  # [1, 1, H, W] -> [H, W]
            mask = (mask > 0.5).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            logger.error(f"âŒ ISNet ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
    
    @classmethod
    def from_checkpoint(cls, onnx_path: str):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ISNet ë¡œë“œ"""
        model = cls()
        model.load_isnet_model(onnx_path)
        return model

# ==============================================
# ğŸ”¥ 9. BaseStepMixin v16.0 í˜¸í™˜ í´ë°± í´ë˜ìŠ¤
# ==============================================

class BaseStepMixinFallback:
    """BaseStepMixin v16.0 í˜¸í™˜ í´ë°± í´ë˜ìŠ¤"""
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.step_name = kwargs.get('step_name', 'BaseStep')
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'cpu')
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # v16.0 í˜¸í™˜ ì†ì„±ë“¤
        self.config = kwargs.get('config', {})
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_factory = None
        
        # ì˜ì¡´ì„± ê´€ë¦¬ì ì‹œë®¬ë ˆì´ì…˜
        self.dependency_manager = self._create_dummy_dependency_manager()
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'avg_processing_time': 0.0,
            'cache_hits': 0,
            'success_count': 0,
            'error_count': 0
        }
        
    def _create_dummy_dependency_manager(self):
        """ë”ë¯¸ ì˜ì¡´ì„± ê´€ë¦¬ì ìƒì„±"""
        class DummyDependencyManager:
            def __init__(self, step_instance):
                self.step_instance = step_instance
                self.logger = step_instance.logger
                
            def auto_inject_dependencies(self):
                """ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜"""
                try:
                    # ModelLoader ìë™ ê°ì§€ ë° ì£¼ì…
                    model_loader = get_model_loader()
                    if model_loader:
                        self.step_instance.set_model_loader(model_loader)
                        self.logger.info("âœ… ìë™ ì˜ì¡´ì„± ì£¼ì…: ModelLoader")
                        return True
                        
                    self.logger.debug("âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì…: ModelLoader ë¯¸ë°œê²¬")
                    return False
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                    return False
                    
            def get_dependency(self, dep_name: str):
                """ì˜ì¡´ì„± ê°€ì ¸ì˜¤ê¸°"""
                return getattr(self.step_instance, dep_name, None)
                
            def inject_dependency(self, dep_name: str, dependency):
                """ì˜ì¡´ì„± ì£¼ì…"""
                setattr(self.step_instance, dep_name, dependency)
                return True
        
        return DummyDependencyManager(self)
    
    # BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.has_model = True
            self.model_loaded = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_step_factory(self, step_factory):
        """StepFactory ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.step_factory = step_factory
            self.logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def get_model(self, model_name: str = "default"):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        return None

    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'warmup_completed': self.warmup_completed,
            'device': self.device,
            'basestepmixin_v16_compatible': True,
            'fallback_mode': True
        }

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½"""
        return self.performance_stats.copy()

    def record_processing(self, processing_time: float, success: bool, **metrics):
        """ì²˜ë¦¬ ê¸°ë¡"""
        self.performance_stats['total_processed'] += 1
        if success:
            self.performance_stats['success_count'] += 1
        else:
            self.performance_stats['error_count'] += 1
        
        # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_stats['total_processed']
        current_avg = self.performance_stats['avg_processing_time']
        self.performance_stats['avg_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )

    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            gc.collect()
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return {'success': True, 'aggressive': aggressive}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def warmup(self, **kwargs) -> Dict[str, Any]:
        """ì›Œë°ì—…"""
        try:
            self.warmup_completed = True
            return {'success': True}
        except Exception as e:
            return {'success': False, 'error': str(e)}

# ==============================================
# ğŸ”¥ 10. ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤ (step_model_requirements.py ì™„ì „ í˜¸í™˜)
# ==============================================

class ClothSegmentationStep:
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” v21.0
    
    ğŸ¯ step_model_requirements.py ì™„ì „ í˜¸í™˜:
    âœ… DetailedDataSpec êµ¬ì¡° ì™„ì „ ì ìš©
    âœ… EnhancedRealModelRequest í‘œì¤€ ì¤€ìˆ˜  
    âœ… step_input_schema/step_output_schema ì™„ì „ êµ¬í˜„
    âœ… accepts_from_previous_step/provides_to_next_step ì™„ì „ ì •ì˜
    âœ… api_input_mapping/api_output_mapping êµ¬í˜„
    âœ… preprocessing_steps/postprocessing_steps ì™„ì „ ì •ì˜
    âœ… RealSAMModel í´ë˜ìŠ¤ëª… í‘œì¤€ ì¤€ìˆ˜
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (sam_vit_h_4b8939.pth 2445.7MB)
    âœ… BaseStepMixin v16.0 í˜¸í™˜ì„± ìœ ì§€
    âœ… M3 Max 128GB ìµœì í™”
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], SegmentationConfig]] = None,
        **kwargs
    ):
        """ìƒì„±ì - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™”"""
        
        # ===== 1. ê¸°ë³¸ ì†ì„± ì„¤ì • (step_model_requirements.py í˜¸í™˜) =====
        self.step_name = kwargs.get('step_name', "ClothSegmentationStep")
        self.step_id = kwargs.get('step_id', 3)
        self.step_type = "cloth_segmentation"
        self.device = device or self._auto_detect_device()
        
        # ===== 2. Logger ì„¤ì • =====
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        
        # ===== 3. step_model_requirements.py ìš”êµ¬ì‚¬í•­ ì ìš© =====
        self.step_requirements = STEP_REQUIREMENTS
        if self.step_requirements:
            self.logger.info(f"âœ… step_model_requirements.py ì ìš©: {self.step_requirements.model_name}")
            # ìš”êµ¬ì‚¬í•­ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            if not config:
                config = {
                    'input_size': self.step_requirements.input_size,
                    'method': SegmentationMethod.SAM_HUGE,  # Primary modelì€ SAM
                    'confidence_threshold': 0.5,  # step_model_requirements í‘œì¤€
                    'device': self.step_requirements.device,
                    'precision': self.step_requirements.precision,
                    'memory_fraction': self.step_requirements.memory_fraction,
                    'batch_size': self.step_requirements.batch_size
                }
        
        # ===== 4. ì„¤ì • ì²˜ë¦¬ =====
        if isinstance(config, dict):
            self.segmentation_config = SegmentationConfig(**config)
        elif isinstance(config, SegmentationConfig):
            self.segmentation_config = config
        else:
            self.segmentation_config = SegmentationConfig()
        
        # ===== 5. BaseStepMixin v16.0 í˜¸í™˜ ì†ì„±ë“¤ =====
        # ì‹¤ì œ BaseStepMixin ì‹œë„, í´ë°±ìœ¼ë¡œ í˜¸í™˜ í´ë˜ìŠ¤ ì‚¬ìš©
        try:
            BaseStepMixin = get_base_step_mixin_class()
            if BaseStepMixin:
                self._mixin = BaseStepMixin(step_name=self.step_name, step_id=self.step_id, **kwargs)
                self.logger.info("âœ… ì‹¤ì œ BaseStepMixin v16.0 ì—°ë™ ì„±ê³µ")
            else:
                self._mixin = BaseStepMixinFallback(step_name=self.step_name, step_id=self.step_id, **kwargs)
                self.logger.info("âœ… BaseStepMixin v16.0 í˜¸í™˜ í´ë°± ì‚¬ìš©")
        except Exception as e:
            self.logger.warning(f"âš ï¸ BaseStepMixin ì—°ë™ ì‹¤íŒ¨: {e}")
            self._mixin = BaseStepMixinFallback(step_name=self.step_name, step_id=self.step_id, **kwargs)
        
        # BaseStepMixin ì†ì„±ë“¤ ìœ„ì„
        self.model_loader = None
        self.model_interface = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_factory = None
        self.dependency_manager = getattr(self._mixin, 'dependency_manager', None)
        
        # BaseStepMixin í˜¸í™˜ í”Œë˜ê·¸ë“¤
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # ===== 6. Step 03 íŠ¹í™” ì†ì„±ë“¤ (step_model_requirements.py í˜¸í™˜) =====
        self.ai_models = {}  # ì‹¤ì œ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.model_paths = {}  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
        self.available_methods = []
        self.rembg_sessions = {}
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ (step_model_requirements.py íŒŒì¼ ê¸°ì¤€)
        self.models_loading_status = {
            'sam_huge': False,          # sam_vit_h_4b8939.pth (2445.7MB) - Primary
            'u2net_cloth': False,       # u2net.pth (168.1MB) - Alternative
            'mobile_sam': False,        # mobile_sam.pt (38.8MB) - Alternative
            'isnet': False,             # isnetis.onnx (168.1MB) - Alternative
        }
        
        # ===== 7. M3 Max ê°ì§€ ë° ìµœì í™” =====
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = kwargs.get('memory_gb', 128.0 if self.is_m3_max else 16.0)
        
        # ===== 8. í†µê³„ ë° ìºì‹œ ì´ˆê¸°í™” =====
        self.processing_stats = {
            'total_processed': 0,
            'successful_segmentations': 0,
            'failed_segmentations': 0,
            'average_time': 0.0,
            'average_quality': 0.0,
            'method_usage': {},
            'cache_hits': 0,
            'ai_model_calls': 0,
            'sam_huge_calls': 0,
            'u2net_calls': 0,
            'mobile_sam_calls': 0,
            'isnet_calls': 0,
            'hybrid_calls': 0
        }
        
        self.segmentation_cache = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(
            max_workers=4 if self.is_m3_max else 2,
            thread_name_prefix="cloth_seg_ai"
        )
        
        # ===== 9. ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ (BaseStepMixin v16.0) =====
        if self.dependency_manager and hasattr(self.dependency_manager, 'auto_inject_dependencies'):
            try:
                self.dependency_manager.auto_inject_dependencies()
                self.logger.info("âœ… BaseStepMixin v16.0 ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        self.logger.info(f"âœ… {self.step_name} step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   - Device: {self.device}")
        self.logger.info(f"   - M3 Max: {self.is_m3_max}")
        self.logger.info(f"   - Memory: {self.memory_gb}GB")
        self.logger.info(f"   - Requirements: {self.step_requirements.model_name if self.step_requirements else 'None'}")

    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except Exception:
            return "cpu"

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True
                )
                cpu_info = result.stdout.strip()
                return 'M3 Max' in cpu_info or 'M3' in cpu_info
        except Exception:
            pass
        return False

    # ==============================================
    # ğŸ”¥ 11. BaseStepMixin v16.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            if self._mixin:
                self._mixin.set_model_loader(model_loader)
            
            self.has_model = True
            self.model_loaded = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoader íŒ¨í„´)
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
                
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            if self._mixin:
                self._mixin.set_memory_manager(memory_manager)
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            if self._mixin:
                self._mixin.set_data_converter(data_converter)
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.di_container = di_container
            if self._mixin:
                self._mixin.set_di_container(di_container)
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_step_factory(self, step_factory):
        """StepFactory ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.step_factory = step_factory
            if self._mixin:
                self._mixin.set_step_factory(step_factory)
            self.logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    # ==============================================
    # ğŸ”¥ 12. BaseStepMixin v16.0 í˜¸í™˜ í‘œì¤€ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model(self, model_name: str = "default"):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (BaseStepMixin v16.0 í˜¸í™˜)"""
        if model_name == "default" or model_name == "sam_huge":
            return self.ai_models.get('sam_huge')
        elif model_name == "u2net" or model_name == "u2net_cloth":
            return self.ai_models.get('u2net_cloth')
        elif model_name == "mobile_sam":
            return self.ai_models.get('mobile_sam')
        elif model_name == "isnet":
            return self.ai_models.get('isnet')
        else:
            return self.ai_models.get(model_name)

    async def get_model_async(self, model_name: str = "default"):
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.get_model(model_name)

    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            initial_memory = self._get_memory_usage()
            
            # ìºì‹œ ì •ë¦¬
            if aggressive:
                with self.cache_lock:
                    self.segmentation_cache.clear()
                self.logger.info("ğŸ§¹ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            # AI ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            if aggressive:
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        self.logger.debug(f"ğŸ§¹ {model_name} ëª¨ë¸ CPU ì´ë™")
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ {model_name} CPU ì´ë™ ì‹¤íŒ¨: {e}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # BaseStepMixin ë©”ëª¨ë¦¬ ìµœì í™”
            if self._mixin and hasattr(self._mixin, 'optimize_memory'):
                mixin_result = self._mixin.optimize_memory(aggressive)
            
            final_memory = self._get_memory_usage()
            
            return {
                'success': True,
                'initial_memory': initial_memory,
                'final_memory': final_memory,
                'memory_freed': initial_memory - final_memory,
                'cache_cleared': aggressive,
                'ai_models_count': len(self.ai_models),
                'basestepmixin_v16_compatible': True,
                'step_model_requirements_compatible': True
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.optimize_memory(aggressive)

    def warmup(self, **kwargs) -> Dict[str, Any]:
        """ì›Œë°ì—… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            if self.warmup_completed:
                return {'success': True, 'already_warmed': True}
                
            # AI ëª¨ë¸ë“¤ë¡œ ì›Œë°ì—…
            warmed_models = []
            if TORCH_AVAILABLE and self.ai_models:
                # step_model_requirements.py í‘œì¤€ ì…ë ¥ í¬ê¸° ì‚¬ìš©
                input_size = self.step_requirements.input_size if self.step_requirements else (1024, 1024)
                dummy_input = torch.randn(1, 3, *input_size, device=self.device)
                
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'eval'):
                            model.eval()
                            if hasattr(model, 'forward'):
                                with torch.no_grad():
                                    _ = model(dummy_input)
                                warmed_models.append(model_name)
                            self.logger.debug(f"âœ… {model_name} AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # BaseStepMixin ì›Œë°ì—…
            if self._mixin and hasattr(self._mixin, 'warmup'):
                mixin_result = self._mixin.warmup(**kwargs)
            
            self.warmup_completed = True
            return {
                'success': True, 
                'warmed_ai_models': warmed_models,
                'total_ai_models': len(self.ai_models),
                'basestepmixin_v16_compatible': True,
                'step_model_requirements_compatible': True
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def warmup_async(self, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—… (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.warmup(**kwargs)

    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì •ë³´ ë°˜í™˜ (BaseStepMixin v16.0 + step_model_requirements.py í˜¸í™˜)"""
        base_status = {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'warmup_completed': self.warmup_completed,
            'device': self.device,
            'basestepmixin_v16_compatible': True,
            'step_model_requirements_compatible': True,
            'opencv_replaced': True,  # OpenCV ì™„ì „ ëŒ€ì²´ë¨
            'ai_models_loaded': list(self.ai_models.keys()),
            'ai_models_status': self.models_loading_status.copy(),
            'available_methods': [m.value for m in self.available_methods],
            'processing_stats': self.processing_stats.copy(),
            'is_m3_max': self.is_m3_max,
            'memory_gb': self.memory_gb,
            
            # step_model_requirements.py í˜¸í™˜ ì •ë³´
            'step_requirements': {
                'model_name': self.step_requirements.model_name if self.step_requirements else None,
                'ai_class': self.step_requirements.ai_class if self.step_requirements else None,
                'primary_file': self.step_requirements.primary_file if self.step_requirements else None,
                'primary_size_mb': self.step_requirements.primary_size_mb if self.step_requirements else None,
                'input_size': self.step_requirements.input_size if self.step_requirements else None,
                'model_architecture': self.step_requirements.model_architecture if self.step_requirements else None
            }
        }
        
        # BaseStepMixin ìƒíƒœ ì¶”ê°€
        if self._mixin and hasattr(self._mixin, 'get_status'):
            mixin_status = self._mixin.get_status()
            base_status.update(mixin_status)
        
        return base_status

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ (BaseStepMixin v16.0 + step_model_requirements.py í˜¸í™˜)"""
        ai_summary = {
            'total_processed': self.processing_stats['total_processed'],
            'success_rate': (
                self.processing_stats['successful_segmentations'] / 
                max(self.processing_stats['total_processed'], 1)
            ),
            'average_time': self.processing_stats['average_time'],
            'average_quality': self.processing_stats['average_quality'],
            'cache_hit_rate': (
                self.processing_stats['cache_hits'] / 
                max(self.processing_stats['total_processed'], 1)
            ),
            'ai_model_calls': self.processing_stats['ai_model_calls'],
            'method_usage': self.processing_stats['method_usage'],
            'ai_model_usage': {
                'sam_huge_calls': self.processing_stats['sam_huge_calls'],
                'u2net_calls': self.processing_stats['u2net_calls'],
                'mobile_sam_calls': self.processing_stats['mobile_sam_calls'],
                'isnet_calls': self.processing_stats['isnet_calls'],
                'hybrid_calls': self.processing_stats['hybrid_calls']
            },
            'basestepmixin_v16_compatible': True,
            'step_model_requirements_compatible': True,
            'opencv_replaced': True
        }
        
        # BaseStepMixin ì„±ëŠ¥ ìš”ì•½ ì¶”ê°€
        if self._mixin and hasattr(self._mixin, 'get_performance_summary'):
            mixin_summary = self._mixin.get_performance_summary()
            ai_summary.update(mixin_summary)
        
        return ai_summary

    def record_processing(self, processing_time: float, success: bool, **metrics):
        """ì²˜ë¦¬ ê¸°ë¡ (BaseStepMixin v16.0 í˜¸í™˜)"""
        self.processing_stats['total_processed'] += 1
        if success:
            self.processing_stats['successful_segmentations'] += 1
        else:
            self.processing_stats['failed_segmentations'] += 1
        
        # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.processing_stats['total_processed']
        current_avg = self.processing_stats['average_time']
        self.processing_stats['average_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if 'quality' in metrics:
            current_quality_avg = self.processing_stats['average_quality']
            self.processing_stats['average_quality'] = (
                (current_quality_avg * (total - 1) + metrics['quality']) / total
            )
        
        # AI ëª¨ë¸ í˜¸ì¶œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        if 'method_used' in metrics:
            method = metrics['method_used']
            self.processing_stats['method_usage'][method] = (
                self.processing_stats['method_usage'].get(method, 0) + 1
            )
        
        # BaseStepMixin ê¸°ë¡
        if self._mixin and hasattr(self._mixin, 'record_processing'):
            self._mixin.record_processing(processing_time, success, **metrics)

    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            return 0.0

    # ==============================================
    # ğŸ”¥ 13. ì´ˆê¸°í™” ë©”ì„œë“œ (step_model_requirements.py í˜¸í™˜)
    # ==============================================
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ëª¨ë¸ ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ ClothSegmentationStep step_model_requirements.py í˜¸í™˜ ì´ˆê¸°í™” ì‹œì‘")
            
            # ===== 1. BaseStepMixin v16.0 ì´ˆê¸°í™” =====
            if self._mixin and hasattr(self._mixin, 'initialize'):
                try:
                    await self._mixin.initialize()
                    self.logger.info("âœ… BaseStepMixin v16.0 ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ===== 2. step_model_requirements.py ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ íƒì§€ =====
            await self._detect_model_paths_from_requirements()
            
            # ===== 3. ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© =====
            await self._load_all_ai_models()
            
            # ===== 4. RemBG ì„¸ì…˜ ì´ˆê¸°í™” =====
            if REMBG_AVAILABLE:
                await self._initialize_rembg_sessions()
            
            # ===== 5. M3 Max ìµœì í™” ì›Œë°ì—… =====
            if self.is_m3_max:
                await self._warmup_m3_max_ai_models()
            
            # ===== 6. ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²• ê°ì§€ =====
            self.available_methods = self._detect_available_ai_methods()
            if not self.available_methods:
                self.logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤")
                self.available_methods = [SegmentationMethod.AUTO_AI]
            
            # ===== 7. BaseStepMixin v16.0 í˜¸í™˜ í”Œë˜ê·¸ ì„¤ì • =====
            self.is_initialized = True
            self.is_ready = True
            self.warmup_completed = True
            
            # ===== 8. ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸ =====
            loaded_models = list(self.ai_models.keys())
            total_size_mb = sum(
                2445.7 if 'sam_huge' in model else
                168.1 if 'u2net' in model else
                38.8 if 'mobile_sam' in model else
                168.1 if 'isnet' in model else 0
                for model in loaded_models
            )
            
            self.logger.info("âœ… ClothSegmentationStep step_model_requirements.py í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            self.logger.info(f"   - ì´ ëª¨ë¸ í¬ê¸°: {total_size_mb:.1f}MB")
            self.logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•: {[m.value for m in self.available_methods]}")
            self.logger.info(f"   - BaseStepMixin v16.0 í˜¸í™˜: âœ…")
            self.logger.info(f"   - step_model_requirements.py í˜¸í™˜: âœ…")
            self.logger.info(f"   - OpenCV ì™„ì „ ëŒ€ì²´: âœ…")
            self.logger.info(f"   - M3 Max ìµœì í™”: {'âœ…' if self.is_m3_max else 'âŒ'}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.is_ready = False
            return False

    async def _detect_model_paths_from_requirements(self):
        """step_model_requirements.py ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            self.logger.info("ğŸ”„ step_model_requirements.py ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹œì‘...")
            
            if not self.step_requirements:
                self.logger.warning("âš ï¸ step_model_requirements ì—†ìŒ, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
                await self._detect_model_paths_fallback()
                return
            
            # step_model_requirements.pyì—ì„œ ì •ì˜ëœ ê²€ìƒ‰ ê²½ë¡œ ì‚¬ìš©
            search_paths = self.step_requirements.search_paths + self.step_requirements.fallback_paths
            
            # Primary íŒŒì¼ íƒì§€ (sam_vit_h_4b8939.pth)
            primary_file = self.step_requirements.primary_file
            self.logger.info(f"ğŸ” Primary íŒŒì¼ íƒì§€: {primary_file}")
            
            for search_path in search_paths:
                full_path = os.path.join(search_path, primary_file)
                if os.path.exists(full_path):
                    file_size = os.path.getsize(full_path) / (1024 * 1024)  # MB
                    expected_size = self.step_requirements.primary_size_mb
                    size_diff = abs(file_size - expected_size)
                    
                    if size_diff < expected_size * 0.1:  # 10% ì˜¤ì°¨ í—ˆìš©
                        self.model_paths['sam_huge'] = full_path
                        self.logger.info(f"âœ… Primary SAM ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        break
            
            # Alternative íŒŒì¼ë“¤ íƒì§€
            for alt_file, alt_size in self.step_requirements.alternative_files:
                self.logger.info(f"ğŸ” Alternative íŒŒì¼ íƒì§€: {alt_file}")
                
                for search_path in search_paths:
                    full_path = os.path.join(search_path, alt_file)
                    if os.path.exists(full_path):
                        file_size = os.path.getsize(full_path) / (1024 * 1024)  # MB
                        
                        # íŒŒì¼ëª… ê¸°ë°˜ ëª¨ë¸ íƒ€ì… ê²°ì •
                        if 'u2net' in alt_file.lower():
                            self.model_paths['u2net_cloth'] = full_path
                            self.logger.info(f"âœ… U2Net ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        elif 'mobile_sam' in alt_file.lower():
                            self.model_paths['mobile_sam'] = full_path
                            self.logger.info(f"âœ… Mobile SAM ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        elif 'isnet' in alt_file.lower() or alt_file.endswith('.onnx'):
                            self.model_paths['isnet'] = full_path
                            self.logger.info(f"âœ… ISNet ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        break
            
            # Shared ìœ„ì¹˜ í™•ì¸
            for shared_location in self.step_requirements.shared_locations:
                if os.path.exists(shared_location):
                    file_size = os.path.getsize(shared_location) / (1024 * 1024)  # MB
                    if 'sam_vit_h' in shared_location and 'sam_huge' not in self.model_paths:
                        self.model_paths['sam_huge'] = shared_location
                        self.logger.info(f"âœ… ê³µìœ  SAM ë°œê²¬: {shared_location} ({file_size:.1f}MB)")
            
            if not self.model_paths:
                self.logger.warning("âš ï¸ step_model_requirements.py ê²½ë¡œì—ì„œ ëª¨ë¸ íŒŒì¼ ì—†ìŒ, í´ë°± íƒì§€ ì‹œì‘")
                await self._detect_model_paths_fallback()
            
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py ê¸°ë°˜ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            await self._detect_model_paths_fallback()

    async def _detect_model_paths_fallback(self):
        """í´ë°± ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ê²½ë¡œ íƒì§€...")
            
            # ê¸°ë³¸ ê²½ë¡œë“¤
            base_paths = [
                "ai_models/step_03_cloth_segmentation/",
                "ai_models/step_03_cloth_segmentation/ultra_models/",
                "models/step_03_cloth_segmentation/",
                "checkpoints/step_03_cloth_segmentation/"
            ]
            
            # ModelLoaderë¥¼ í†µí•œ ê²½ë¡œ íƒì§€
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                try:
                    for model_key in ['sam_huge', 'u2net_cloth', 'mobile_sam', 'isnet']:
                        try:
                            model_path = self.model_loader.get_model_path(f"step_03_{model_key}")
                            if model_path and os.path.exists(model_path):
                                self.model_paths[model_key] = model_path
                                self.logger.info(f"âœ… ModelLoaderì—ì„œ {model_key} ê²½ë¡œ ë°œê²¬: {model_path}")
                        except Exception as e:
                            self.logger.debug(f"ModelLoader {model_key} ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    self.logger.debug(f"ModelLoader ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ íŒŒì¼ íƒì§€
            model_files = {
                'sam_huge': 'sam_vit_h_4b8939.pth',
                'u2net_cloth': 'u2net.pth',
                'mobile_sam': 'mobile_sam.pt',
                'isnet': 'isnetis.onnx'
            }
            
            for model_key, filename in model_files.items():
                if model_key in self.model_paths:
                    continue  # ì´ë¯¸ ë°œê²¬ë¨
                
                for base_path in base_paths:
                    full_path = os.path.join(base_path, filename)
                    if os.path.exists(full_path):
                        self.model_paths[model_key] = full_path
                        file_size = os.path.getsize(full_path) / (1024 * 1024)  # MB
                        self.logger.info(f"âœ… {model_key} ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        break
                else:
                    self.logger.warning(f"âš ï¸ {model_key} íŒŒì¼ ì—†ìŒ: {filename}")
            
            if not self.model_paths:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")

    async def _load_all_ai_models(self):
        """ëª¨ë“  AI ëª¨ë¸ ë¡œë”©"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
                return
            
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # ===== SAM Huge ë¡œë”© (Primary Model) =====
            if 'sam_huge' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ SAM Huge ë¡œë”© ì¤‘ (Primary Model)...")
                    sam_model = RealSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['sam_huge'],
                        device=self.device,
                        model_type="vit_h"
                    )
                    if sam_model.is_loaded:
                        self.ai_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        self.logger.info("âœ… SAM Huge ë¡œë”© ì™„ë£Œ (Primary Model)")
                    else:
                        self.logger.warning("âš ï¸ SAM Huge ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ SAM Huge ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== U2Net Cloth ë¡œë”© (Alternative Model) =====
            if 'u2net_cloth' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ U2Net Cloth ë¡œë”© ì¤‘ (Alternative Model)...")
                    u2net_model = RealU2NetClothModel.from_checkpoint(
                        checkpoint_path=self.model_paths['u2net_cloth'],
                        device=self.device
                    )
                    self.ai_models['u2net_cloth'] = u2net_model
                    self.models_loading_status['u2net_cloth'] = True
                    self.logger.info(f"âœ… U2Net Cloth ë¡œë”© ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {u2net_model.parameter_count:,}")
                except Exception as e:
                    self.logger.error(f"âŒ U2Net Cloth ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== Mobile SAM ë¡œë”© (Alternative Model) =====
            if 'mobile_sam' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ Mobile SAM ë¡œë”© ì¤‘ (Alternative Model)...")
                    mobile_sam_model = RealMobileSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['mobile_sam'],
                        device=self.device
                    )
                    if mobile_sam_model.is_loaded:
                        self.ai_models['mobile_sam'] = mobile_sam_model
                        self.models_loading_status['mobile_sam'] = True
                        self.logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ")
                    else:
                        self.logger.warning("âš ï¸ Mobile SAM ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== ISNet ë¡œë”© (Alternative Model) =====
            if 'isnet' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ ISNet ë¡œë”© ì¤‘ (Alternative Model)...")
                    isnet_model = RealISNetModel.from_checkpoint(
                        onnx_path=self.model_paths['isnet']
                    )
                    if isnet_model.is_loaded:
                        self.ai_models['isnet'] = isnet_model
                        self.models_loading_status['isnet'] = True
                        self.logger.info("âœ… ISNet ë¡œë”© ì™„ë£Œ")
                    else:
                        self.logger.warning("âš ï¸ ISNet ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ ISNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== í´ë°± ëª¨ë¸ ìƒì„± (AI ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°) =====
            if not self.ai_models:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ë”ë¯¸ ëª¨ë¸ ìƒì„±")
                try:
                    # ê¸°ë³¸ U2Net ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ì´)
                    dummy_u2net = RealU2NetClothModel(in_ch=3, out_ch=1).to(self.device)
                    dummy_u2net.eval()
                    self.ai_models['u2net_cloth'] = dummy_u2net
                    self.models_loading_status['u2net_cloth'] = True
                    self.logger.info("âœ… ë”ë¯¸ U2Net ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ ë”ë¯¸ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            
            # ===== ë¡œë”© ê²°ê³¼ ìš”ì•½ =====
            loaded_count = sum(self.models_loading_status.values())
            total_models = len(self.models_loading_status)
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    async def _initialize_rembg_sessions(self):
        """RemBG ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            if not REMBG_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘...")
            
            session_configs = {
                'u2net': 'u2net',
                'u2netp': 'u2netp',
                'silueta': 'silueta',
            }
            
            for name, model_name in session_configs.items():
                try:
                    session = new_session(model_name)
                    self.rembg_sessions[name] = session
                    self.logger.info(f"âœ… RemBG ì„¸ì…˜ ìƒì„±: {name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ {name} ìƒì„± ì‹¤íŒ¨: {e}")
            
            if self.rembg_sessions:
                self.default_rembg_session = (
                    self.rembg_sessions.get('u2net') or
                    list(self.rembg_sessions.values())[0]
                )
                self.logger.info("âœ… RemBG ê¸°ë³¸ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _warmup_m3_max_ai_models(self):
        """M3 Max AI ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if not self.is_m3_max or not TORCH_AVAILABLE:
                return
            
            self.logger.info("ğŸ”¥ M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # step_model_requirements.py í‘œì¤€ í¬ê¸° ì‚¬ìš©
            input_size = self.step_requirements.input_size if self.step_requirements else (1024, 1024)
            dummy_input = torch.randn(1, 3, *input_size, device=self.device)
            
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                        with torch.no_grad():
                            if hasattr(model, 'forward') and model_name != 'sam_huge':
                                _ = model(dummy_input)
                            elif callable(model):
                                _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} M3 Max ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # MPS ìºì‹œ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    def _detect_available_ai_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        # ë¡œë“œëœ AI ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ë°©ë²• ê²°ì •
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
            self.logger.info("âœ… SAM_HUGE ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (Primary AI ëª¨ë¸)")
        
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
            self.logger.info("âœ… U2NET_CLOTH ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (Alternative AI ëª¨ë¸)")
        
        if 'mobile_sam' in self.ai_models:
            methods.append(SegmentationMethod.MOBILE_SAM)
            self.logger.info("âœ… MOBILE_SAM ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (Alternative AI ëª¨ë¸)")
        
        if 'isnet' in self.ai_models:
            methods.append(SegmentationMethod.ISNET)
            self.logger.info("âœ… ISNET ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (Alternative ONNX ëª¨ë¸)")
        
        # AUTO_AI ë°©ë²• (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
        if methods:
            methods.append(SegmentationMethod.AUTO_AI)
            self.logger.info("âœ… AUTO_AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # HYBRID_AI ë°©ë²• (2ê°œ ì´ìƒ AI ë°©ë²•ì´ ìˆì„ ë•Œ)
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
            self.logger.info("âœ… HYBRID_AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        return methods

    # ==============================================
    # ğŸ”¥ 14. í•µì‹¬: process ë©”ì„œë“œ (step_model_requirements.py ì™„ì „ í˜¸í™˜)
    # ==============================================
    
    async def process(
        self,
        input_data: Union[StepInputData, str, np.ndarray, Image.Image, Dict[str, Any]],
        clothing_type: Optional[str] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ) -> Union[StepOutputData, Dict[str, Any]]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ì¶”ë¡ """
        
        if not self.is_initialized:
            if not await self.initialize():
                return self._create_error_result("step_model_requirements.py í˜¸í™˜ ì´ˆê¸°í™” ì‹¤íŒ¨")

        start_time = time.time()
        
        try:
            self.logger.info("ğŸ”„ step_model_requirements.py í˜¸í™˜ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹œì‘")
            
            # ===== 1. ì…ë ¥ ë°ì´í„° í‘œì¤€í™” (step_model_requirements.py í˜¸í™˜) =====
            standardized_input = self._standardize_input_with_requirements(input_data, clothing_type, **kwargs)
            if not standardized_input:
                return self._create_error_result("step_model_requirements.py í˜¸í™˜ ì…ë ¥ ë°ì´í„° í‘œì¤€í™” ì‹¤íŒ¨")
            
            image = standardized_input['image']
            metadata = standardized_input['metadata']
            
            # ===== 2. step_model_requirements.py ì „ì²˜ë¦¬ (preprocessing_steps) =====
            processed_image = await self._preprocess_image_with_requirements(image)
            if processed_image is None:
                return self._create_error_result("step_model_requirements.py í˜¸í™˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ===== 3. ì˜ë¥˜ íƒ€ì… ê°ì§€ (AI ê¸°ë°˜) =====
            detected_clothing_type = await self._detect_clothing_type_ai(processed_image, clothing_type)
            
            # ===== 4. í’ˆì§ˆ ë ˆë²¨ ì„¤ì • =====
            quality = QualityLevel(quality_level or self.segmentation_config.quality_level.value)
            
            # ===== 5. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (step_model_requirements.py í‘œì¤€) =====
            self.logger.info("ğŸ§  step_model_requirements.py í‘œì¤€ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
            mask, confidence, method_used = await self._run_requirements_compatible_ai_segmentation(
                processed_image, detected_clothing_type, quality
            )
            
            if mask is None:
                return self._create_error_result("step_model_requirements.py í˜¸í™˜ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨")
            
            # ===== 6. step_model_requirements.py í›„ì²˜ë¦¬ (postprocessing_steps) =====
            final_mask = await self._postprocess_mask_with_requirements(mask, quality)
            
            # ===== 7. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (AI ê°•í™”) =====
            visualizations = {}
            if self.segmentation_config.enable_visualization:
                self.logger.info("ğŸ¨ AI ê°•í™” ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±...")
                visualizations = self._create_ai_visualizations(
                    processed_image, final_mask, detected_clothing_type
                )
            
            # ===== 8. step_model_requirements.py í˜¸í™˜ ê²°ê³¼ ë°ì´í„° ìƒì„± =====
            processing_time = time.time() - start_time
            
            # step_model_requirements.py í‘œì¤€ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì ìš©
            step_output = self._create_requirements_compatible_output(
                processed_image, final_mask, confidence, detected_clothing_type, 
                method_used, processing_time, visualizations, metadata
            )
            
            # ===== 9. í†µê³„ ì—…ë°ì´íŠ¸ =====
            self.record_processing(processing_time, True, quality=confidence, method_used=method_used)
            
            # AI ëª¨ë¸ë³„ í˜¸ì¶œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            if 'sam_huge' in method_used:
                self.processing_stats['sam_huge_calls'] += 1
            if 'u2net' in method_used:
                self.processing_stats['u2net_calls'] += 1
            if 'mobile_sam' in method_used:
                self.processing_stats['mobile_sam_calls'] += 1
            if 'isnet' in method_used:
                self.processing_stats['isnet_calls'] += 1
            if 'hybrid' in method_used:
                self.processing_stats['hybrid_calls'] += 1
            
            self.processing_stats['ai_model_calls'] += 1
            
            self.logger.info(f"âœ… step_model_requirements.py í˜¸í™˜ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - {processing_time:.2f}ì´ˆ")
            self.logger.info(f"   - AI ëª¨ë¸ ì‚¬ìš©: {list(self.ai_models.keys())}")
            self.logger.info(f"   - ë°©ë²•: {method_used}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {confidence:.3f}")
            self.logger.info(f"   - step_model_requirements.py í˜¸í™˜: âœ…")
            self.logger.info(f"   - OpenCV ì™„ì „ ëŒ€ì²´: âœ…")
            
            return step_output
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.record_processing(processing_time, False)
            
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"step_model_requirements.py í˜¸í™˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    def _standardize_input_with_requirements(self, input_data, clothing_type=None, **kwargs) -> Optional[Dict[str, Any]]:
        """step_model_requirements.py í˜¸í™˜ ì…ë ¥ ë°ì´í„° í‘œì¤€í™”"""
        try:
            # StepInputData íƒ€ì…ì¸ ê²½ìš°
            if isinstance(input_data, StepInputData):
                return {
                    'image': input_data.image,
                    'metadata': {
                        **input_data.metadata,
                        'clothing_type': clothing_type or input_data.metadata.get('clothing_type'),
                        'step_history': input_data.step_history,
                        'processing_context': input_data.processing_context,
                        # step_model_requirements.py í˜¸í™˜ ì¶”ê°€
                        'clothing_image': getattr(input_data, 'clothing_image', None),
                        'prompt_points': getattr(input_data, 'prompt_points', []),
                        'session_id': getattr(input_data, 'session_id', None)
                    }
                }
            
            # Dict íƒ€ì…ì¸ ê²½ìš° (step_model_requirements.py step_input_schema í˜¸í™˜)
            elif isinstance(input_data, dict):
                # Step 02ì—ì„œ ì˜¤ëŠ” ê²½ìš° (accepts_from_previous_step)
                if 'pose_keypoints' in input_data:
                    image = input_data.get('image') or input_data.get('person_image')
                    return {
                        'image': image,
                        'metadata': {
                            'clothing_type': clothing_type,
                            'pose_keypoints': input_data.get('pose_keypoints'),
                            'pose_confidence': input_data.get('pose_confidence'),
                            'previous_step_data': input_data,
                            **kwargs
                        }
                    }
                
                # ì¼ë°˜ì ì¸ Dict ì…ë ¥
                image = input_data.get('image') or input_data.get('clothing_image') or input_data.get('segmented_image')
                if image is None:
                    self.logger.error("âŒ Dict ì…ë ¥ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
                return {
                    'image': image,
                    'metadata': {
                        'clothing_type': clothing_type or input_data.get('clothing_type'),
                        'prompt_points': input_data.get('prompt_points', []),
                        'session_id': input_data.get('session_id'),
                        'previous_step_data': input_data,
                        **kwargs
                    }
                }
            
            # ì§ì ‘ì ì¸ ì´ë¯¸ì§€ ë°ì´í„°ì¸ ê²½ìš°
            else:
                return {
                    'image': input_data,
                    'metadata': {
                        'clothing_type': clothing_type,
                        **kwargs
                    }
                }
                
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ì…ë ¥ ë°ì´í„° í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return None

    async def _preprocess_image_with_requirements(self, image) -> Optional[Image.Image]:
        """step_model_requirements.py preprocessing_steps í˜¸í™˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì…ë ¥ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, str):
                if image.startswith('data:image'):
                    # Base64
                    header, data = image.split(',', 1)
                    image_data = base64.b64decode(data)
                    image = Image.open(BytesIO(image_data))
                else:
                    # íŒŒì¼ ê²½ë¡œ
                    image = Image.open(image)
            elif isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:  # RGB
                    image = Image.fromarray(image)
                elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBA
                    image = Image.fromarray(image).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image.shape}")
            elif not isinstance(image, Image.Image):
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # step_model_requirements.py preprocessing_steps ì ìš©
            if self.step_requirements and self.step_requirements.data_spec.preprocessing_steps:
                image = await self._apply_preprocessing_steps(image, self.step_requirements.data_spec.preprocessing_steps)
            else:
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                # step_model_requirements.py í‘œì¤€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (1024x1024)
                target_size = self.step_requirements.input_size if self.step_requirements else (1024, 1024)
                if image.size != target_size:
                    image = AIImageProcessor.ai_resize(image, target_size)
            
            return image
                
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None

    async def _apply_preprocessing_steps(self, image: Image.Image, preprocessing_steps: List[str]) -> Image.Image:
        """step_model_requirements.py preprocessing_steps ì ìš©"""
        try:
            for step in preprocessing_steps:
                if step == "resize_1024x1024":
                    image = AIImageProcessor.ai_resize(image, (1024, 1024))
                elif step == "normalize_imagenet":
                    # ì •ê·œí™”ëŠ” í…ì„œ ë³€í™˜ ì‹œ ì ìš©ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
                    pass
                elif step == "prepare_sam_prompts":
                    # SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ëŠ” ì¶”ë¡  ì‹œ ì ìš©ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
                    pass
                elif step.startswith("resize_"):
                    # ë™ì  ë¦¬ì‚¬ì´ì¦ˆ ì²˜ë¦¬
                    size_str = step.replace("resize_", "")
                    if "x" in size_str:
                        width, height = map(int, size_str.split("x"))
                        image = AIImageProcessor.ai_resize(image, (width, height))
                else:
                    self.logger.debug(f"ì•Œ ìˆ˜ ì—†ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„: {step}")
            
            return image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš© ì‹¤íŒ¨: {e}")
            return image

    async def _detect_clothing_type_ai(self, image: Image.Image, hint: Optional[str] = None) -> ClothingType:
        """AI ê¸°ë°˜ ì˜ë¥˜ íƒ€ì… ê°ì§€ (CLIP ëª¨ë¸ í™œìš©)"""
        try:
            if hint:
                try:
                    return ClothingType(hint.lower())
                except ValueError:
                    pass
            
            # CLIP ê¸°ë°˜ ì˜ë¥˜ ë¶„ë¥˜ ì‹œë„
            if TRANSFORMERS_AVAILABLE:
                try:
                    # ê°„ë‹¨í•œ ì˜ë¥˜ ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ CLIP íŒŒì´í”„ë¼ì¸ í•„ìš”)
                    clothing_candidates = [
                        "shirt", "dress", "pants", "skirt", "jacket", 
                        "sweater", "coat", "top", "bottom"
                    ]
                    
                    # ì´ë¯¸ì§€ ì¢…íš¡ë¹„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± (ì„ì‹œ)
                    width, height = image.size
                    aspect_ratio = height / width
                    
                    if aspect_ratio > 1.5:
                        return ClothingType.DRESS
                    elif aspect_ratio > 1.2:
                        return ClothingType.SHIRT
                    else:
                        return ClothingType.PANTS
                        
                except Exception as e:
                    self.logger.debug(f"CLIP ì˜ë¥˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
            if hasattr(image, 'size'):
                width, height = image.size
                aspect_ratio = height / width
                
                if aspect_ratio > 1.5:
                    return ClothingType.DRESS
                elif aspect_ratio > 1.2:
                    return ClothingType.SHIRT
                else:
                    return ClothingType.PANTS
            
            return ClothingType.UNKNOWN
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì˜ë¥˜ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return ClothingType.UNKNOWN

    async def _run_requirements_compatible_ai_segmentation(
        self,
        image: Image.Image,
        clothing_type: ClothingType,
        quality: QualityLevel
    ) -> Tuple[Optional[np.ndarray], float, str]:
        """step_model_requirements.py í˜¸í™˜ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
        try:
            # Primary ëª¨ë¸ ìš°ì„  (step_model_requirements.py ê¸°ì¤€)
            primary_method = SegmentationMethod.SAM_HUGE  # sam_vit_h_4b8939.pth
            
            # í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ì„ íƒ
            ai_methods = self._get_ai_methods_by_quality_with_requirements(quality, primary_method)
            
            for method in ai_methods:
                try:
                    self.logger.info(f"ğŸ§  step_model_requirements.py í˜¸í™˜ AI ë°©ë²• ì‹œë„: {method.value}")
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    
                    if mask is not None:
                        self.processing_stats['ai_model_calls'] += 1
                        self.processing_stats['method_usage'][method.value] = (
                            self.processing_stats['method_usage'].get(method.value, 0) + 1
                        )
                        
                        self.logger.info(f"âœ… step_model_requirements.py í˜¸í™˜ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {method.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return mask, confidence, method.value
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ë°©ë²• {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±
            self.logger.warning("âš ï¸ ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨, ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±")
            input_size = self.step_requirements.input_size if self.step_requirements else (1024, 1024)
            dummy_mask = np.ones(input_size[::-1], dtype=np.uint8) * 128  # (H, W)
            return dummy_mask, 0.5, "fallback_dummy"
            
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, 0.0, "error"

    def _get_ai_methods_by_quality_with_requirements(self, quality: QualityLevel, primary_method: SegmentationMethod) -> List[SegmentationMethod]:
        """step_model_requirements.py í˜¸í™˜ í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ìš°ì„ ìˆœìœ„"""
        available_ai_methods = [
            method for method in self.available_methods
            if method not in [SegmentationMethod.AUTO_AI]
        ]
        
        # Primary ëª¨ë¸ ìš°ì„  ì ìš©
        priority = [primary_method] if primary_method in available_ai_methods else []
        
        if quality == QualityLevel.ULTRA:
            priority.extend([
                SegmentationMethod.HYBRID_AI,    # ëª¨ë“  AI ëª¨ë¸ ì¡°í•©
                SegmentationMethod.SAM_HUGE,     # Primary (2445.7MB)
                SegmentationMethod.U2NET_CLOTH,  # Alternative (168.1MB)
                SegmentationMethod.ISNET,        # Alternative (168.1MB)
                SegmentationMethod.MOBILE_SAM,   # Alternative (38.8MB)
            ])
        elif quality == QualityLevel.HIGH:
            priority.extend([
                SegmentationMethod.SAM_HUGE,     # Primary
                SegmentationMethod.U2NET_CLOTH,  # Alternative
                SegmentationMethod.HYBRID_AI,    # ì¡°í•©
                SegmentationMethod.ISNET,        # Alternative
            ])
        elif quality == QualityLevel.BALANCED:
            priority.extend([
                SegmentationMethod.U2NET_CLOTH,  # Alternative (ì˜ë¥˜ íŠ¹í™”)
                SegmentationMethod.SAM_HUGE,     # Primary
                SegmentationMethod.ISNET,        # Alternative
            ])
        else:  # FAST
            priority.extend([
                SegmentationMethod.MOBILE_SAM,   # Alternative (ê²½ëŸ‰)
                SegmentationMethod.U2NET_CLOTH,  # Alternative
            ])
        
        # ì¤‘ë³µ ì œê±° ë° ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ë§Œ ë°˜í™˜
        seen = set()
        result = []
        for method in priority:
            if method not in seen and method in available_ai_methods:
                result.append(method)
                seen.add(method)
        
        return result

    async def _run_ai_method(
        self,
        method: SegmentationMethod,
        image: Image.Image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ê°œë³„ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤í–‰ (step_model_requirements.py í˜¸í™˜)"""
        
        if method == SegmentationMethod.SAM_HUGE:
            return await self._run_sam_huge_inference(image, clothing_type)
        elif method == SegmentationMethod.U2NET_CLOTH:
            return await self._run_u2net_cloth_inference(image)
        elif method == SegmentationMethod.MOBILE_SAM:
            return await self._run_mobile_sam_inference(image)
        elif method == SegmentationMethod.ISNET:
            return await self._run_isnet_inference(image)
        elif method == SegmentationMethod.HYBRID_AI:
            return await self._run_hybrid_ai_inference(image, clothing_type)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ë°©ë²•: {method}")

    async def _run_sam_huge_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """SAM Huge ì‹¤ì œ AI ì¶”ë¡  (sam_vit_h_4b8939.pth 2445.7MB) - step_model_requirements.py Primary ëª¨ë¸"""
        try:
            if 'sam_huge' not in self.ai_models:
                raise RuntimeError("âŒ SAM Huge ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            sam_model = self.ai_models['sam_huge']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
            
            # ğŸ”¥ ì‹¤ì œ SAM Huge AI ì¶”ë¡  (step_model_requirements.py Primary Model)
            clothing_results = sam_model.segment_clothing(image_array, clothing_type.value)
            
            if not clothing_results:
                # ê¸°ë³¸ ì¤‘ì•™ í¬ì¸íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„
                height, width = image_array.shape[:2]
                center_points = [[width//2, height//2]]
                
                sam_model.predictor.set_image(image_array)
                masks, scores, logits = sam_model.predictor.predict(
                    point_coords=np.array(center_points),
                    point_labels=np.ones(len(center_points)),
                    multimask_output=True
                )
                
                best_mask_idx = np.argmax(scores)
                mask = masks[best_mask_idx].astype(np.uint8)
                confidence = float(scores[best_mask_idx])
            else:
                # ì˜ë¥˜ë³„ ë§ˆìŠ¤í¬ ì¡°í•©
                combined_mask = np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
                total_confidence = 0.0
                
                for cloth_area, area_mask in clothing_results.items():
                    combined_mask = np.logical_or(combined_mask, area_mask).astype(np.uint8)
                    total_confidence += np.sum(area_mask) / area_mask.size
                
                mask = combined_mask
                confidence = min(total_confidence / len(clothing_results), 1.0)
            
            self.logger.info(f"âœ… SAM Huge AI ì¶”ë¡  ì™„ë£Œ (Primary Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ SAM Huge AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_u2net_cloth_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """U2Net Cloth ì‹¤ì œ AI ì¶”ë¡  (u2net.pth 168.1MB Alternative) - step_model_requirements.py í˜¸í™˜"""
        try:
            if 'u2net_cloth' not in self.ai_models:
                raise RuntimeError("âŒ U2Net Cloth ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['u2net_cloth']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # step_model_requirements.py í˜¸í™˜ ì „ì²˜ë¦¬
            if self.step_requirements and self.step_requirements.data_spec.normalization_mean:
                mean = self.step_requirements.data_spec.normalization_mean
                std = self.step_requirements.data_spec.normalization_std
            else:
                mean = (0.485, 0.456, 0.406)
                std = (0.229, 0.224, 0.225)
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std)
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ U2Net Cloth AI ì¶”ë¡  (step_model_requirements.py Alternative Model)
            model.eval()
            with torch.no_grad():
                if self.is_m3_max and self.segmentation_config.use_fp16:
                    with torch.autocast(device_type='cpu'):
                        output = model(input_tensor)
                else:
                    output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬ (d0, d1, d2, d3, d4, d5, d6)
                if isinstance(output, tuple):
                    main_output = output[0]  # d0 (ìµœì¢… ì¶œë ¥)
                else:
                    main_output = output
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if main_output.max() > 1.0:
                    prob_map = torch.sigmoid(main_output)
                else:
                    prob_map = main_output
                
                # step_model_requirements.py í‘œì¤€ ì„ê³„ê°’ ì‚¬ìš©
                threshold = self.step_requirements.confidence_threshold if self.step_requirements else 0.5
                mask = (prob_map > threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.max().item())
            
            self.logger.info(f"âœ… U2Net Cloth AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net Cloth AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_mobile_sam_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """Mobile SAM ì‹¤ì œ AI ì¶”ë¡  (mobile_sam.pt 38.8MB Alternative)"""
        try:
            if 'mobile_sam' not in self.ai_models:
                raise RuntimeError("âŒ Mobile SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['mobile_sam']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ Mobile SAM AI ì¶”ë¡  (38.8MB Alternative Model)
            model.eval()
            with torch.no_grad():
                output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, tuple):
                    output = output[0]
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if output.max() > 1.0:
                    prob_map = torch.sigmoid(output)
                else:
                    prob_map = output
                
                threshold = self.step_requirements.confidence_threshold if self.step_requirements else 0.5
                mask = (prob_map > threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.mean().item())  # Mobile SAMì€ í‰ê·  ì‹ ë¢°ë„ ì‚¬ìš©
            
            self.logger.info(f"âœ… Mobile SAM AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ Mobile SAM AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_isnet_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """ISNet ì‹¤ì œ AI ì¶”ë¡  (isnetis.onnx 168.1MB Alternative)"""
        try:
            if 'isnet' not in self.ai_models:
                raise RuntimeError("âŒ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            isnet_model = self.ai_models['isnet']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
            
            # ğŸ”¥ ì‹¤ì œ ISNet ONNX AI ì¶”ë¡  (168.1MB Alternative Model)
            mask = isnet_model.predict(image_array)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë§ˆìŠ¤í¬ í’ˆì§ˆ ê¸°ë°˜)
            if mask is not None:
                confidence = np.sum(mask > 0) / mask.size
                confidence = min(confidence * 1.2, 1.0)  # ISNetì€ ê³ ì •ë°€ì´ë¯€ë¡œ ì‹ ë¢°ë„ í–¥ìƒ
                
                # ì´ì§„í™”
                threshold = self.step_requirements.confidence_threshold if self.step_requirements else 0.5
                mask = (mask > (threshold * 255)).astype(np.uint8)
            else:
                confidence = 0.0
            
            self.logger.info(f"âœ… ISNet AI ì¶”ë¡  ì™„ë£Œ (Alternative Model) - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ ISNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_hybrid_ai_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """HYBRID AI ì¶”ë¡  (ëª¨ë“  AI ëª¨ë¸ ì¡°í•©) - step_model_requirements.py í˜¸í™˜"""
        try:
            self.logger.info("ğŸ”„ HYBRID AI ì¶”ë¡  ì‹œì‘ (step_model_requirements.py ëª¨ë“  ëª¨ë¸ í™œìš©)...")
            
            masks = []
            confidences = []
            methods_used = []
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•ë“¤ë¡œ ì¶”ë¡  ì‹¤í–‰
            available_ai_methods = [
                method for method in self.available_methods 
                if method not in [SegmentationMethod.AUTO_AI, SegmentationMethod.HYBRID_AI]
            ]
            
            # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°©ë²•ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if len(available_ai_methods) < 2:
                raise RuntimeError("âŒ HYBRID ë°©ë²•ì€ ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ë°©ë²•ì´ í•„ìš”")
            
            # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
            for method in available_ai_methods:
                try:
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    if mask is not None:
                        masks.append(mask.astype(np.float32))
                        confidences.append(confidence)
                        methods_used.append(method.value)
                        self.logger.info(f"âœ… HYBRID - {method.value} ì¶”ë¡  ì™„ë£Œ: {confidence:.3f}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HYBRID - {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            if not masks:
                raise RuntimeError("âŒ HYBRID - ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
            
            # ğŸ”¥ ê³ ê¸‰ ë§ˆìŠ¤í¬ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê·  + AI ê¸°ë°˜ í›„ì²˜ë¦¬)
            if len(masks) == 1:
                combined_mask = masks[0]
                combined_confidence = confidences[0]
            else:
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
                weights = np.array(confidences)
                weights = weights / np.sum(weights)  # ì •ê·œí™”
                
                # ë§ˆìŠ¤í¬ë“¤ì„ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤
                target_shape = masks[0].shape
                normalized_masks = []
                for mask in masks:
                    if mask.shape != target_shape:
                        # AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì¦ˆ ì‚¬ìš©
                        mask_image = Image.fromarray(mask.astype(np.uint8))
                        resized_mask = AIImageProcessor.ai_resize(mask_image, target_shape[::-1])
                        mask_resized = np.array(resized_mask).astype(np.float32)
                        normalized_masks.append(mask_resized)
                    else:
                        normalized_masks.append(mask.astype(np.float32))
                
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                combined_mask_float = np.zeros_like(normalized_masks[0])
                for mask, weight in zip(normalized_masks, weights):
                    combined_mask_float += mask * weight
                
                # AI ê¸°ë°˜ ì„ê³„ê°’ ì ìš©
                threshold = np.mean(combined_mask_float) + np.std(combined_mask_float) * 0.5
                combined_mask = (combined_mask_float > threshold).astype(np.float32)
                combined_confidence = float(np.mean(confidences))
            
            # AI ê¸°ë°˜ í›„ì²˜ë¦¬ (OpenCV ëŒ€ì²´)
            final_mask = AIImageProcessor.ai_morphology(
                (combined_mask * 255).astype(np.uint8), 
                "closing", 
                5
            )
            
            # ìµœì¢… ì´ì§„í™”
            final_mask = (final_mask > 128).astype(np.uint8)
            
            self.logger.info(f"âœ… HYBRID AI ì¶”ë¡  ì™„ë£Œ (step_model_requirements.py) - ë°©ë²•: {methods_used} - ì‹ ë¢°ë„: {combined_confidence:.3f}")
            return final_mask, combined_confidence
            
        except Exception as e:
            self.logger.error(f"âŒ HYBRID AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _postprocess_mask_with_requirements(self, mask: np.ndarray, quality: QualityLevel) -> np.ndarray:
        """step_model_requirements.py postprocessing_steps í˜¸í™˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            processed_mask = mask.copy()
            
            # step_model_requirements.py postprocessing_steps ì ìš©
            if self.step_requirements and self.step_requirements.data_spec.postprocessing_steps:
                for step in self.step_requirements.data_spec.postprocessing_steps:
                    if step == "threshold_0.5":
                        threshold = self.step_requirements.confidence_threshold if self.step_requirements else 0.5
                        processed_mask = (processed_mask > (threshold * 255)).astype(np.uint8) * 255
                    elif step == "morphology_clean":
                        processed_mask = AIImageProcessor.ai_morphology(processed_mask, "opening", 3)
                        processed_mask = AIImageProcessor.ai_morphology(processed_mask, "closing", 3)
                    elif step == "resize_original":
                        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        pass
                    else:
                        self.logger.debug(f"ì•Œ ìˆ˜ ì—†ëŠ” í›„ì²˜ë¦¬ ë‹¨ê³„: {step}")
            else:
                # ê¸°ë³¸ í›„ì²˜ë¦¬
                # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
                if self.segmentation_config.remove_noise:
                    kernel_size = 3 if quality == QualityLevel.FAST else 5
                    processed_mask = AIImageProcessor.ai_morphology(processed_mask, "opening", kernel_size)
                    processed_mask = AIImageProcessor.ai_morphology(processed_mask, "closing", kernel_size)
                
                # AI ê¸°ë°˜ ì—£ì§€ ìŠ¤ë¬´ë”©
                if self.segmentation_config.edge_smoothing:
                    processed_mask_float = processed_mask.astype(np.float32) / 255.0
                    smoothed = AIImageProcessor.ai_gaussian_blur(
                        processed_mask_float, 
                        kernel_size=3, 
                        sigma=0.5
                    )
                    processed_mask = (smoothed > 0.5).astype(np.uint8) * 255
                
                # í™€ ì±„ìš°ê¸° (AI ê¸°ë°˜)
                if self.segmentation_config.enable_hole_filling:
                    processed_mask = self._fill_holes_ai(processed_mask)
                
                # ê²½ê³„ ê°œì„  (AI ê¸°ë°˜)
                if self.segmentation_config.enable_edge_refinement:
                    processed_mask = self._refine_edges_ai(processed_mask)
            
            return processed_mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ step_model_requirements.py í˜¸í™˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask

    def _create_requirements_compatible_output(
        self, 
        processed_image: Image.Image, 
        final_mask: np.ndarray, 
        confidence: float, 
        detected_clothing_type: ClothingType, 
        method_used: str, 
        processing_time: float, 
        visualizations: Dict, 
        metadata: Dict[str, Any]
    ) -> StepOutputData:
        """step_model_requirements.py í˜¸í™˜ ê²°ê³¼ ë°ì´í„° ìƒì„±"""
        try:
            # step_model_requirements.py step_output_schema ì ìš©
            segmented_clothing = self._apply_mask_to_image(processed_image, final_mask)
            
            # Step ê°„ í‘œì¤€ ì¶œë ¥ ë°ì´í„° ìƒì„± (step_model_requirements.py í˜¸í™˜)
            step_output = StepOutputData(
                success=True,
                result_data={
                    'mask': final_mask,
                    'segmented_image': segmented_clothing,
                    'confidence': confidence,
                    'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                    'method_used': method_used,
                    'ai_models_used': list(self.ai_models.keys()),
                    'processing_time': processing_time,
                    'quality_score': confidence * 0.9,  # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    'mask_area_ratio': np.sum(final_mask > 0) / final_mask.size,
                    'boundary_smoothness': self._calculate_boundary_smoothness(final_mask),
                    
                    # step_model_requirements.py í˜¸í™˜ ì¶”ê°€ í•„ë“œ
                    'segmented_clothing': segmented_clothing,
                    'cloth_mask': final_mask
                },
                metadata={
                    'device': self.device,
                    'quality_level': self.segmentation_config.quality_level.value,
                    'ai_models_used': list(self.ai_models.keys()),
                    'model_file_paths': self.model_paths.copy(),
                    'image_size': processed_image.size if hasattr(processed_image, 'size') else (1024, 1024),
                    'ai_inference': True,
                    'opencv_replaced': True,
                    'model_loader_used': self.model_loader is not None,
                    'is_m3_max': self.is_m3_max,
                    'basestepmixin_v16_compatible': True,
                    'step_model_requirements_compatible': True,
                    'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                    'step_integration_complete': True,
                    'total_model_size_mb': sum(
                        2445.7 if 'sam_huge' in model else
                        168.1 if 'u2net' in model else
                        38.8 if 'mobile_sam' in model else
                        168.1 if 'isnet' in model else 0
                        for model in self.ai_models.keys()
                    ),
                    
                    # step_model_requirements.py ë©”íƒ€ë°ì´í„°
                    'step_requirements_info': {
                        'model_name': self.step_requirements.model_name if self.step_requirements else None,
                        'ai_class': self.step_requirements.ai_class if self.step_requirements else None,
                        'primary_file': self.step_requirements.primary_file if self.step_requirements else None,
                        'model_architecture': self.step_requirements.model_architecture if self.step_requirements else None,
                        'input_size': self.step_requirements.input_size if self.step_requirements else None
                    },
                    **metadata  # ì›ë³¸ ë©”íƒ€ë°ì´í„° í¬í•¨
                },
                step_name=self.step_name,
                processing_time=processing_time,
                
                # step_model_requirements.py í˜¸í™˜ ì§ì ‘ í•„ë“œ
                cloth_mask=final_mask,
                segmented_clothing=segmented_clothing,
                confidence=confidence,
                clothing_type=detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                
                # step_model_requirements.py provides_to_next_step ìŠ¤í‚¤ë§ˆ ì ìš©
                next_step_input={
                    # Step 04ë¡œ ì „ë‹¬í•  ë°ì´í„°
                    'step_04': {
                        'cloth_mask': final_mask,
                        'segmented_clothing': segmented_clothing
                    },
                    # Step 05ë¡œ ì „ë‹¬í•  ë°ì´í„°
                    'step_05': {
                        'clothing_segmentation': final_mask,
                        'cloth_contours': self._extract_cloth_contours(final_mask)
                    },
                    # Step 06ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
                    'step_06': {
                        'cloth_mask': final_mask,
                        'clothing_item': segmented_clothing
                    },
                    
                    # ë²”ìš© ë°ì´í„°
                    'segmented_image': segmented_clothing,
                    'mask': final_mask,
                    'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                    'confidence': confidence,
                    'step_03_metadata': {
                        'ai_models_used': list(self.ai_models.keys()),
                        'method_used': method_used,
                        'quality_level': self.segmentation_config.quality_level.value,
                        'processing_time': processing_time,
                        'step_model_requirements_compatible': True
                    }
                }
            )
            
            # ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            if visualizations:
                if 'visualization' in visualizations:
                    step_output.result_data['visualization_base64'] = self._image_to_base64(visualizations['visualization'])
                if 'overlay' in visualizations:
                    step_output.result_data['overlay_base64'] = self._image_to_base64(visualizations['overlay'])
            
            return step_output
            
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def _extract_cloth_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """ì˜ë¥˜ ìœ¤ê³½ì„  ì¶”ì¶œ (step_model_requirements.py í˜¸í™˜)"""
        try:
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œì„ ì‚¬ìš©í•œ ìœ¤ê³½ì„  ì¶”ì¶œ
            edges = AIImageProcessor.ai_detect_edges(mask)
            
            # ê°„ë‹¨í•œ ìœ¤ê³½ì„  ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ êµ¬í˜„ í•„ìš”)
            contours = []
            if np.any(edges > 0):
                # ë”ë¯¸ ìœ¤ê³½ì„  ìƒì„±
                y_coords, x_coords = np.where(edges > 0)
                if len(y_coords) > 0:
                    contour = np.column_stack((x_coords, y_coords))
                    contours.append(contour)
            
            return contours
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _fill_holes_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ í™€ ì±„ìš°ê¸° (OpenCV ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return mask
            
            # PyTorch ê¸°ë°˜ í™€ ì±„ìš°ê¸°
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            
            # í˜•íƒœí•™ì  ë‹«í˜ ì—°ì‚°ìœ¼ë¡œ í™€ ì±„ìš°ê¸°
            kernel_size = 7
            filled = AIImageProcessor.ai_morphology((tensor.squeeze().numpy() * 255).astype(np.uint8), "closing", kernel_size)
            
            return filled
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask

    def _refine_edges_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ê²½ê³„ ê°œì„  (OpenCV ëŒ€ì²´)"""
        try:
            if self.segmentation_config.enable_edge_refinement:
                # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
                edges = AIImageProcessor.ai_detect_edges(mask, 50, 150)
                
                # ê²½ê³„ ì£¼ë³€ ì˜ì—­ í™•ì¥ (AI ê¸°ë°˜)
                edge_region = AIImageProcessor.ai_morphology(edges, "dilation", 3)
                
                # í•´ë‹¹ ì˜ì—­ì— AI ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
                blurred_mask = AIImageProcessor.ai_gaussian_blur(mask.astype(np.float32), 5, 1.0)
                
                # ê²½ê³„ ì˜ì—­ë§Œ ë¸”ëŸ¬ëœ ê°’ìœ¼ë¡œ êµì²´
                refined_mask = mask.copy().astype(np.float32)
                refined_mask[edge_region > 0] = blurred_mask[edge_region > 0]
                
                return (refined_mask > 128).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ê²½ê³„ ê°œì„  ì‹¤íŒ¨: {e}")
            return mask

    def _apply_mask_to_image(self, image: Image.Image, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©"""
        try:
            image_array = np.array(image)
            
            if len(mask.shape) == 2:
                mask_3d = np.stack([mask, mask, mask], axis=2)
            else:
                mask_3d = mask
            
            # ë§ˆìŠ¤í¬ ì •ê·œí™”
            mask_normalized = mask_3d.astype(np.float32) / 255.0
            
            # ë°°ê²½ì„ íˆ¬ëª…í•˜ê²Œ ë§Œë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€
            segmented = image_array.astype(np.float32) * mask_normalized
            
            return segmented.astype(np.uint8)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
            return np.array(image)

    def _calculate_boundary_smoothness(self, mask: np.ndarray) -> float:
        """ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚°"""
        try:
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œë¡œ ê²½ê³„ í’ˆì§ˆ ì¸¡ì •
            edges = AIImageProcessor.ai_detect_edges(mask)
            edge_pixels = np.sum(edges > 0)
            total_boundary = np.sum(mask > 0)
            
            if total_boundary > 0:
                smoothness = 1.0 - (edge_pixels / total_boundary)
                return max(0.0, min(1.0, smoothness))
            else:
                return 0.0
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    # ==============================================
    # ğŸ”¥ 15. AI ê°•í™” ì‹œê°í™” ë©”ì„œë“œë“¤
    # ==============================================

    def _create_ai_visualizations(self, image: Image.Image, mask: np.ndarray, clothing_type: ClothingType) -> Dict[str, Image.Image]:
        """AI ê°•í™” ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE or not NUMPY_AVAILABLE:
                return {}
            
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            color = CLOTHING_COLORS.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type),
                CLOTHING_COLORS['unknown']
            )
            
            # 1. AI ê°•í™” ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ê³ í’ˆì§ˆ)
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            
            # Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§ ì ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.segmentation_config.esrgan_scale > 1:
                mask_image = Image.fromarray(mask_colored)
                target_size = (
                    mask_image.size[0] * self.segmentation_config.esrgan_scale,
                    mask_image.size[1] * self.segmentation_config.esrgan_scale
                )
                mask_image = AIImageProcessor.esrgan_upscale(mask_image, target_size)
                visualizations['mask_hq'] = mask_image
            else:
                visualizations['mask'] = Image.fromarray(mask_colored)
            
            # 2. AI ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ (ê³ í’ˆì§ˆ)
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê¸°ë°˜ ê²½ê³„ì„  ì¶”ê°€
            boundary = AIImageProcessor.ai_detect_edges(mask.astype(np.uint8))
            overlay[boundary > 0] = (255, 255, 255)
            
            visualizations['overlay'] = Image.fromarray(overlay)
            
            # 3. AI ê²½ê³„ì„  ì´ë¯¸ì§€
            boundary_colored = np.zeros((*boundary.shape, 3), dtype=np.uint8)
            boundary_colored[boundary > 0] = (255, 255, 255)
            
            boundary_overlay = image_array.copy()
            boundary_overlay[boundary > 0] = (255, 255, 255)
            visualizations['boundary'] = Image.fromarray(boundary_overlay)
            
            # 4. ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€
            visualization = self._create_comprehensive_ai_visualization(
                image, mask, clothing_type, color
            )
            visualizations['visualization'] = visualization
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_comprehensive_ai_visualization(self, image: Image.Image, mask: np.ndarray, clothing_type: ClothingType, color: Tuple[int, int, int]) -> Image.Image:
        """ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE:
                return image
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            width, height = image.size
            canvas_width = width * 2 + 30
            canvas_height = height + 120
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), (245, 245, 245))
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(image, (15, 40))
            
            # AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            image_array = np.array(image)
            ai_result = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            ai_result[mask > 0] = (
                ai_result[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê²½ê³„ì„  ì¶”ê°€
            boundary = AIImageProcessor.ai_detect_edges(mask.astype(np.uint8))
            ai_result[boundary > 0] = (255, 255, 255)
            
            ai_result_image = Image.fromarray(ai_result)
            canvas.paste(ai_result_image, (width + 30, 40))
            
            # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            try:
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(canvas)
                
                try:
                    font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
                    font_small = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 12)
                except Exception:
                    try:
                        font = ImageFont.load_default()
                        font_small = font
                    except Exception:
                        font = None
                        font_small = None
                
                if font:
                    # ì œëª©
                    draw.text((15, 10), "Original", fill=(0, 0, 0), font=font)
                    clothing_type_str = clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type)
                    draw.text((width + 30, 10), f"AI Segmented ({clothing_type_str})", fill=(0, 0, 0), font=font)
                    
                    # AI ëª¨ë¸ ì •ë³´
                    loaded_models = list(self.ai_models.keys())
                    model_info = f"AI Models: {', '.join(loaded_models)}"
                    draw.text((15, height + 50), model_info, fill=(50, 50, 50), font=font_small)
                    
                    # step_model_requirements.py í˜¸í™˜ ì •ë³´
                    req_info = f"step_model_requirements.py: âœ… | Primary: {self.step_requirements.primary_file if self.step_requirements else 'None'}"
                    draw.text((15, height + 70), req_info, fill=(50, 50, 50), font=font_small)
                    
                    # í†µê³„ ì •ë³´
                    mask_area = np.sum(mask > 0)
                    total_area = mask.size
                    coverage = (mask_area / total_area) * 100
                    
                    stats_text = f"Coverage: {coverage:.1f}% | BaseStepMixin v16.0: âœ… | OpenCV Replaced: âœ…"
                    draw.text((15, height + 90), stats_text, fill=(50, 50, 50), font=font_small)
                
            except ImportError:
                pass  # PIL ImageDraw/ImageFont ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¢…í•© AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return image

    # ==============================================
    # ğŸ”¥ 16. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _get_current_method(self) -> str:
        """í˜„ì¬ ì‚¬ìš©ëœ ë°©ë²• ë°˜í™˜"""
        if self.ai_models.get('sam_huge'):
            return 'sam_huge_ai_step_model_requirements_v21'
        elif self.ai_models.get('u2net_cloth'):
            return 'u2net_cloth_ai_requirements'
        elif self.ai_models.get('mobile_sam'):
            return 'mobile_sam_ai_requirements'
        elif self.ai_models.get('isnet'):
            return 'isnet_ai_requirements'
        else:
            return 'ai_fallback_requirements'

    def _image_to_base64(self, image: Image.Image) -> str:
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        try:
            if not PIL_AVAILABLE:
                return ""
            
            buffer = BytesIO()
            if isinstance(image, Image.Image):
                image.save(buffer, format='PNG')
            else:
                img = Image.fromarray(image)
                img.save(buffer, format='PNG')
            image_data = buffer.getvalue()
            return base64.b64encode(image_data).decode()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Base64 ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""

    def _create_error_result(self, error_message: str) -> StepOutputData:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„± (step_model_requirements.py í˜¸í™˜)"""
        return StepOutputData(
            success=False,
            result_data={
                'error': error_message,
                'mask': None,
                'confidence': 0.0,
                'processing_time': 0.0,
                'method_used': 'error',
                'ai_models_used': [],
                'segmented_clothing': None,
                'cloth_mask': None
            },
            metadata={
                'error_details': error_message,
                'available_ai_models': list(self.ai_models.keys()),
                'basestepmixin_v16_compatible': True,
                'step_model_requirements_compatible': True,
                'opencv_replaced': True,
                'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                'step_integration_complete': True,
                'ai_inference_attempted': True,
                'step_requirements_info': {
                    'model_name': self.step_requirements.model_name if self.step_requirements else None,
                    'ai_class': self.step_requirements.ai_class if self.step_requirements else None,
                    'primary_file': self.step_requirements.primary_file if self.step_requirements else None
                }
            },
            step_name=self.step_name,
            processing_time=0.0,
            cloth_mask=None,
            segmented_clothing=None,
            confidence=0.0,
            clothing_type="error"
        )

    # ==============================================
    # ğŸ”¥ 17. BaseStepMixin v16.0 í˜¸í™˜ ê³ ê¸‰ ë©”ì„œë“œë“¤
    # ==============================================

    async def process_batch(
        self,
        batch_input: List[Union[StepInputData, str, np.ndarray, Image.Image, Dict[str, Any]]],
        clothing_types: Optional[List[str]] = None,
        quality_level: Optional[str] = None,
        batch_size: Optional[int] = None,
        **kwargs
    ) -> List[Union[StepOutputData, Dict[str, Any]]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì„œë“œ - step_model_requirements.py í˜¸í™˜ + AI ìµœì í™”"""
        try:
            if not batch_input:
                return []
            
            batch_size = batch_size or self.segmentation_config.batch_size
            clothing_types = clothing_types or [None] * len(batch_input)
            
            # step_model_requirements.py ê¸°ì¤€ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if self.step_requirements:
                batch_size = min(batch_size, self.step_requirements.batch_size)
            
            # M3 Max ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if self.is_m3_max:
                batch_size = min(batch_size, 8)  # M3 Max 128GB í™œìš©
            
            # ë°°ì¹˜ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            results = []
            for i in range(0, len(batch_input), batch_size):
                batch_images = batch_input[i:i+batch_size]
                batch_clothing_types = clothing_types[i:i+batch_size]
                
                # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
                batch_tasks = []
                for j, (input_data, clothing_type) in enumerate(zip(batch_images, batch_clothing_types)):
                    task = self.process(
                        input_data=input_data,
                        clothing_type=clothing_type,
                        quality_level=quality_level,
                        **kwargs
                    )
                    batch_tasks.append(task)
                
                # ë°°ì¹˜ ì‹¤í–‰
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for result in batch_results:
                    if isinstance(result, Exception):
                        results.append(self._create_error_result(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(result)}"))
                    else:
                        results.append(result)
                
                # ë°°ì¹˜ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i + batch_size < len(batch_input):
                    self.optimize_memory(aggressive=False)
            
            self.logger.info(f"âœ… step_model_requirements.py í˜¸í™˜ AI ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì´ë¯¸ì§€")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return [self._create_error_result(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}") for _ in batch_input]

    def get_segmentation_info(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ë³´ ë°˜í™˜ - step_model_requirements.py ì™„ì „ í˜¸í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'device': self.device,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'available_methods': [m.value for m in self.available_methods],
            'loaded_ai_models': list(self.ai_models.keys()),
            'ai_model_paths': self.model_paths.copy(),
            'ai_model_status': self.models_loading_status.copy(),
            'processing_stats': self.processing_stats.copy(),
            
            # BaseStepMixin v16.0 í˜¸í™˜ ì •ë³´
            'basestepmixin_v16_info': {
                'compatible': True,
                'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                'auto_injection_available': hasattr(self, 'dependency_manager'),
                'step_integration_complete': True,
                'model_loader_injected': self.model_loader is not None,
                'memory_manager_injected': self.memory_manager is not None,
                'data_converter_injected': self.data_converter is not None
            },
            
            # step_model_requirements.py í˜¸í™˜ ì •ë³´
            'step_model_requirements_info': {
                'compatible': True,
                'requirements_loaded': self.step_requirements is not None,
                'model_name': self.step_requirements.model_name if self.step_requirements else None,
                'ai_class': self.step_requirements.ai_class if self.step_requirements else None,
                'primary_file': self.step_requirements.primary_file if self.step_requirements else None,
                'primary_size_mb': self.step_requirements.primary_size_mb if self.step_requirements else None,
                'model_architecture': self.step_requirements.model_architecture if self.step_requirements else None,
                'input_size': self.step_requirements.input_size if self.step_requirements else None,
                'search_paths': self.step_requirements.search_paths if self.step_requirements else [],
                'alternative_files': self.step_requirements.alternative_files if self.step_requirements else [],
                'detailed_data_spec_complete': bool(self.step_requirements.data_spec.input_data_types) if self.step_requirements else False
            },
            
            # AI ëª¨ë¸ í†µê³„
            'ai_model_stats': {
                'total_ai_calls': self.processing_stats['ai_model_calls'],
                'models_loaded': len(self.ai_models),
                'model_paths_found': len(self.model_paths),
                'sam_huge_calls': self.processing_stats['sam_huge_calls'],
                'u2net_calls': self.processing_stats['u2net_calls'],
                'mobile_sam_calls': self.processing_stats['mobile_sam_calls'],
                'isnet_calls': self.processing_stats['isnet_calls'],
                'hybrid_calls': self.processing_stats['hybrid_calls'],
                'total_model_size_mb': sum(
                    2445.7 if 'sam_huge' in model else
                    168.1 if 'u2net' in model else
                    38.8 if 'mobile_sam' in model else
                    168.1 if 'isnet' in model else 0
                    for model in self.ai_models.keys()
                ),
                'opencv_replaced': True
            },
            
            # ì„¤ì • ì •ë³´
            'config': {
                'method': self.segmentation_config.method.value,
                'quality_level': self.segmentation_config.quality_level.value,
                'enable_visualization': self.segmentation_config.enable_visualization,
                'confidence_threshold': self.segmentation_config.confidence_threshold,
                'enable_edge_refinement': self.segmentation_config.enable_edge_refinement,
                'enable_hole_filling': self.segmentation_config.enable_hole_filling,
                'overlay_opacity': self.segmentation_config.overlay_opacity,
                'esrgan_scale': self.segmentation_config.esrgan_scale,
                'input_size': self.segmentation_config.input_size
            },
            
            # ì‹œìŠ¤í…œ ì •ë³´
            'system_info': {
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'rembg_available': REMBG_AVAILABLE,
                'sam_available': SAM_AVAILABLE,
                'onnx_available': ONNX_AVAILABLE,
                'esrgan_available': ESRGAN_AVAILABLE
            }
        }

    # ==============================================
    # ğŸ”¥ 18. ì •ë¦¬ ë©”ì„œë“œ (step_model_requirements.py í˜¸í™˜)
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - step_model_requirements.py í˜¸í™˜ + AI ëª¨ë¸ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ClothSegmentationStep step_model_requirements.py í˜¸í™˜ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    elif hasattr(model, 'sam_model') and model.sam_model and hasattr(model.sam_model, 'cpu'):
                        model.sam_model.cpu()
                    del model
                    self.logger.debug(f"ğŸ§¹ {model_name} AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.ai_models.clear()
            self.model_paths.clear()
            self.models_loading_status = {k: False for k in self.models_loading_status.keys()}
            
            # RemBG ì„¸ì…˜ ì •ë¦¬
            if hasattr(self, 'rembg_sessions'):
                self.rembg_sessions.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.segmentation_cache.clear()
            
            # ì‹¤í–‰ì ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            # BaseStepMixin ì •ë¦¬
            if self._mixin and hasattr(self._mixin, 'cleanup'):
                try:
                    await self._mixin.cleanup()
                    self.logger.info("âœ… BaseStepMixin v16.0 ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ BaseStepMixin ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # ì˜ì¡´ì„± ì°¸ì¡° ì •ë¦¬
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            self.step_factory = None
            self.dependency_manager = None
            self._mixin = None
            self.step_requirements = None
            
            # BaseStepMixin v16.0 í˜¸í™˜ í”Œë˜ê·¸ ì¬ì„¤ì •
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            self.logger.info("âœ… ClothSegmentationStep step_model_requirements.py í˜¸í™˜ + AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

# ==============================================
# ğŸ”¥ 19. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (step_model_requirements.py ì™„ì „ í˜¸í™˜)
# ==============================================

def create_cloth_segmentation_step(
    device: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜ (step_model_requirements.py ì™„ì „ í˜¸í™˜)"""
    return ClothSegmentationStep(device=device, config=config, **kwargs)

async def create_and_initialize_cloth_segmentation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """step_model_requirements.py ì™„ì „ í˜¸í™˜ ClothSegmentationStep ìƒì„± ë° AI ì´ˆê¸°í™”"""
    try:
        # Step ìƒì„± (step_model_requirements.py í˜¸í™˜)
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        
        # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ (BaseStepMixin v16.0 UnifiedDependencyManager íŒ¨í„´)
        if hasattr(step, 'dependency_manager') and step.dependency_manager:
            try:
                step.dependency_manager.auto_inject_dependencies()
                logger.info("âœ… UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        # ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… í´ë°±
        try:
            model_loader = get_model_loader()
            if model_loader:
                step.set_model_loader(model_loader)
                logger.info("âœ… ìˆ˜ë™ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            di_container = get_di_container()
            if di_container:
                step.set_di_container(di_container)
                logger.info("âœ… ìˆ˜ë™ DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        # step_model_requirements.py í˜¸í™˜ AI ì´ˆê¸°í™”
        await step.initialize()
        return step
        
    except Exception as e:
        logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ + AI ìƒì„± ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ìƒì„±
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        await step.initialize()
        return step

def create_m3_max_segmentation_step(
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„± (step_model_requirements.py í˜¸í™˜)"""
    m3_config = {
        'method': SegmentationMethod.HYBRID_AI,
        'quality_level': QualityLevel.ULTRA,
        'use_fp16': True,
        'batch_size': 8,  # M3 Max 128GB í™œìš©
        'cache_size': 200,
        'enable_visualization': True,
        'visualization_quality': 'high',
        'enable_edge_refinement': True,
        'enable_hole_filling': True,
        'esrgan_scale': 2,  # Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§
        'input_size': (1024, 1024),  # step_model_requirements.py í‘œì¤€
        'confidence_threshold': 0.5   # step_model_requirements.py í‘œì¤€
    }
    
    if config:
        m3_config.update(config)
    
    return ClothSegmentationStep(device="mps", config=m3_config, **kwargs)

def create_requirements_compatible_step(
    step_requirements = None,
    **kwargs
) -> ClothSegmentationStep:
    """step_model_requirements.py ì™„ì „ í˜¸í™˜ Step ìƒì„±"""
    try:
        # step_model_requirements.pyì—ì„œ ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
        if not step_requirements:
            try:
                import importlib
                requirements_module = importlib.import_module('app.ai_pipeline.utils.step_model_requirements')
                get_enhanced_step_request = getattr(requirements_module, 'get_enhanced_step_request', None)
                if get_enhanced_step_request:
                    step_requirements = get_enhanced_step_request("ClothSegmentationStep")
            except ImportError:
                logger.warning("âš ï¸ step_model_requirements.py ë¡œë“œ ì‹¤íŒ¨")
        
        if step_requirements:
            # step_model_requirements.py ê¸°ë°˜ ì„¤ì • ìƒì„±
            config = {
                'method': SegmentationMethod.SAM_HUGE,  # Primary model
                'input_size': step_requirements.input_size,
                'confidence_threshold': 0.5,  # step_model_requirements.py í‘œì¤€
                'device': step_requirements.device,
                'precision': step_requirements.precision,
                'memory_fraction': step_requirements.memory_fraction,
                'batch_size': step_requirements.batch_size,
                'quality_level': QualityLevel.HIGH,
                'enable_visualization': True
            }
            
            # ê¸°ì¡´ configì™€ ë³‘í•©
            if 'config' in kwargs:
                kwargs['config'].update(config)
            else:
                kwargs['config'] = config
            
            logger.info(f"âœ… step_model_requirements.py ê¸°ë°˜ ì„¤ì • ì ìš©: {step_requirements.model_name}")
        
        return ClothSegmentationStep(**kwargs)
        
    except Exception as e:
        logger.error(f"âŒ step_model_requirements.py í˜¸í™˜ Step ìƒì„± ì‹¤íŒ¨: {e}")
        return ClothSegmentationStep(**kwargs)

# ==============================================
# ğŸ”¥ 20. í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ í•¨ìˆ˜ë“¤
# ==============================================

async def test_step_model_requirements_compatibility():
    """step_model_requirements.py í˜¸í™˜ì„± + AI ê°•í™” ì™„ì „ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª step_model_requirements.py í˜¸í™˜ì„± + AI ê°•í™” ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # Step ìƒì„± (step_model_requirements.py ì™„ì „ í˜¸í™˜)
        step = await create_and_initialize_cloth_segmentation_step(
            device="auto",
            config={
                "method": "sam_huge",  # Primary model
                "quality_level": "ultra",
                "enable_visualization": True,
                "visualization_quality": "high",
                "esrgan_scale": 2
            }
        )
        
        # step_model_requirements.py í˜¸í™˜ì„± ìƒíƒœ í™•ì¸
        info = step.get_segmentation_info()
        requirements_info = info['step_model_requirements_info']
        v16_info = info['basestepmixin_v16_info']
        ai_info = info['ai_model_stats']
        
        print("ğŸ”— step_model_requirements.py í˜¸í™˜ì„± ìƒíƒœ:")
        print(f"   âœ… í˜¸í™˜ì„±: {requirements_info['compatible']}")
        print(f"   âœ… ìš”êµ¬ì‚¬í•­ ë¡œë“œ: {requirements_info['requirements_loaded']}")
        print(f"   âœ… ëª¨ë¸ëª…: {requirements_info['model_name']}")
        print(f"   âœ… AI í´ë˜ìŠ¤: {requirements_info['ai_class']}")
        print(f"   âœ… Primary íŒŒì¼: {requirements_info['primary_file']}")
        print(f"   âœ… ëª¨ë¸ í¬ê¸°: {requirements_info['primary_size_mb']}MB")
        print(f"   âœ… DetailedDataSpec ì™„ë£Œ: {requirements_info['detailed_data_spec_complete']}")
        
        print("\nğŸ”— BaseStepMixin v16.0 í˜¸í™˜ì„± ìƒíƒœ:")
        print(f"   âœ… í˜¸í™˜ì„±: {v16_info['compatible']}")
        print(f"   âœ… UnifiedDependencyManager: {v16_info['unified_dependency_manager']}")
        print(f"   âœ… ìë™ ì˜ì¡´ì„± ì£¼ì…: {v16_info['auto_injection_available']}")
        print(f"   âœ… Step í†µí•© ì™„ë£Œ: {v16_info['step_integration_complete']}")
        print(f"   âœ… ModelLoader ì£¼ì…: {v16_info['model_loader_injected']}")
        
        print("\nğŸ§  ì™„ì „ AI ëª¨ë¸ ìƒíƒœ:")
        print(f"   âœ… ë¡œë“œëœ AI ëª¨ë¸: {info['loaded_ai_models']}")
        print(f"   âœ… ì´ ëª¨ë¸ í¬ê¸°: {ai_info['total_model_size_mb']:.1f}MB")
        print(f"   âœ… OpenCV ëŒ€ì²´ë¨: {ai_info['opencv_replaced']}")
        print(f"   âœ… AI ëª¨ë¸ í˜¸ì¶œ: {ai_info['total_ai_calls']}")
        
        # í‘œì¤€ BaseStepMixin v16.0 ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ BaseStepMixin v16.0 í‘œì¤€ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸:")
        
        # get_status í…ŒìŠ¤íŠ¸
        status = step.get_status()
        print(f"   âœ… get_status(): ì´ˆê¸°í™”={status['is_initialized']}, AIëª¨ë¸={len(status['ai_models_loaded'])}")
        
        # get_model í…ŒìŠ¤íŠ¸
        sam_model = step.get_model("sam_huge")
        u2net_model = step.get_model("u2net_cloth")
        print(f"   âœ… get_model(): SAM={sam_model is not None}, U2Net={u2net_model is not None}")
        
        # optimize_memory í…ŒìŠ¤íŠ¸
        memory_result = step.optimize_memory()
        print(f"   âœ… optimize_memory(): {memory_result['success']}, step_model_requirements í˜¸í™˜={memory_result.get('step_model_requirements_compatible', False)}")
        
        # warmup í…ŒìŠ¤íŠ¸
        warmup_result = step.warmup()
        print(f"   âœ… warmup(): {warmup_result['success']}, AIëª¨ë¸ìˆ˜={len(warmup_result.get('warmed_ai_models', []))}")
        
        # get_performance_summary í…ŒìŠ¤íŠ¸
        perf_summary = step.get_performance_summary()
        print(f"   âœ… get_performance_summary(): ì„±ê³µë¥  {perf_summary['success_rate']:.1%}")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        if PIL_AVAILABLE:
            dummy_image = Image.new('RGB', (1024, 1024), (200, 150, 100))  # step_model_requirements.py í‘œì¤€ í¬ê¸°
        else:
            dummy_image = np.random.randint(0, 255, (1024, 1024, 3), dtype=np.uint8)
        
        # step_model_requirements.py í˜¸í™˜ ì…ë ¥ í…ŒìŠ¤íŠ¸
        step_input = StepInputData(
            image=dummy_image,
            metadata={'clothing_type': 'shirt', 'source': 'test'},
            step_history=['step_01', 'step_02'],
            processing_context={'test_mode': True},
            # step_model_requirements.py í˜¸í™˜ ì¶”ê°€ í•„ë“œ
            clothing_image=dummy_image,
            prompt_points=[(512, 256), (512, 768)],  # ì¤‘ì•™ ìƒí•˜ í¬ì¸íŠ¸
            session_id="test_session_requirements"
        )
        
        # step_model_requirements.py í˜¸í™˜ AI ì²˜ë¦¬ ì‹¤í–‰
        result = await step.process(step_input, quality_level="high")
        
        # ê²°ê³¼ í™•ì¸
        if result.success:
            print("\nâœ… step_model_requirements.py í˜¸í™˜ + AI ê°•í™” ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   - ì˜ë¥˜ íƒ€ì…: {result.result_data['clothing_type']}")
            print(f"   - ì‹ ë¢°ë„: {result.result_data['confidence']:.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"   - ì‚¬ìš© AI ëª¨ë¸: {result.result_data['ai_models_used']}")
            print(f"   - ë°©ë²•: {result.result_data['method_used']}")
            print(f"   - step_model_requirements.py í˜¸í™˜: {result.metadata['step_model_requirements_compatible']}")
            print(f"   - BaseStepMixin v16.0: {result.metadata['basestepmixin_v16_compatible']}")
            print(f"   - OpenCV ëŒ€ì²´: {result.metadata['opencv_replaced']}")
            print(f"   - Step í†µí•©: {result.metadata['step_integration_complete']}")
            print(f"   - ì´ ëª¨ë¸ í¬ê¸°: {result.metadata['total_model_size_mb']:.1f}MB")
            
            # step_model_requirements.py í˜¸í™˜ ì§ì ‘ í•„ë“œ í™•ì¸
            print(f"   - cloth_mask íƒ€ì…: {type(result.cloth_mask)}")
            print(f"   - segmented_clothing íƒ€ì…: {type(result.segmented_clothing)}")
            print(f"   - confidence ê°’: {result.confidence}")
            print(f"   - clothing_type ê°’: {result.clothing_type}")
            
            if 'visualization_base64' in result.result_data:
                print("   - AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±ë¨")
            
            # Stepê°„ ì—°ë™ í™•ì¸ (step_model_requirements.py provides_to_next_step)
            if result.next_step_input:
                print(f"   - ë‹¤ìŒ Step ì…ë ¥ ì¤€ë¹„: {list(result.next_step_input.keys())}")
                if 'step_04' in result.next_step_input:
                    print(f"   - Step 04 ë°ì´í„°: {list(result.next_step_input['step_04'].keys())}")
                if 'step_05' in result.next_step_input:
                    print(f"   - Step 05 ë°ì´í„°: {list(result.next_step_input['step_05'].keys())}")
                if 'step_06' in result.next_step_input:
                    print(f"   - Step 06 ë°ì´í„°: {list(result.next_step_input['step_06'].keys())}")
        else:
            print(f"âŒ step_model_requirements.py í˜¸í™˜ ì²˜ë¦¬ ì‹¤íŒ¨: {result.result_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # step_model_requirements.py í˜¸í™˜ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ step_model_requirements.py í˜¸í™˜ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
        batch_inputs = [dummy_image, dummy_image]
        batch_results = await step.process_batch(batch_inputs, clothing_types=["shirt", "pants"])
        successful_batch = sum(1 for r in batch_results if r.success)
        print(f"   âœ… ë°°ì¹˜ ì²˜ë¦¬: {successful_batch}/{len(batch_results)} ì„±ê³µ")
        
        # step_model_requirements.py ìƒì„¸ ì •ë³´ í™•ì¸
        print(f"\nğŸŒŸ step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   - ë””ë°”ì´ìŠ¤: {info['device']}")
        print(f"   - M3 Max: {info['system_info']['is_m3_max']}")
        print(f"   - ë©”ëª¨ë¦¬: {info['system_info']['memory_gb']}GB")
        print(f"   - PyTorch: {info['system_info']['torch_available']}")
        print(f"   - MPS: {info['system_info']['mps_available']}")
        print(f"   - SAM: {info['system_info']['sam_available']}")
        print(f"   - ONNX: {info['system_info']['onnx_available']}")
        print(f"   - Real-ESRGAN: {info['system_info']['esrgan_available']}")
        print(f"   - BaseStepMixin v16.0 í˜¸í™˜: {info['basestepmixin_v16_info']['compatible']}")
        print(f"   - step_model_requirements.py í˜¸í™˜: {info['step_model_requirements_info']['compatible']}")
        print(f"   - UnifiedDependencyManager: {info['basestepmixin_v16_info']['unified_dependency_manager']}")
        print(f"   - DetailedDataSpec ì™„ë£Œ: {info['step_model_requirements_info']['detailed_data_spec_complete']}")
        
        # ì •ë¦¬
        await step.cleanup()
        print("âœ… step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ì •ë¦¬")
        
    except Exception as e:
        print(f"âŒ step_model_requirements.py í˜¸í™˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ìŒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   1. step_model_requirements.py ëª¨ë“ˆ (DetailedDataSpec + EnhancedRealModelRequest)")
        print("   2. BaseStepMixin v16.0 ëª¨ë“ˆ (UnifiedDependencyManager)")
        print("   3. ModelLoader ëª¨ë“ˆ (ì²´í¬í¬ì¸íŠ¸ ë¡œë”©)")
        print("   4. ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼")
        print("     - sam_vit_h_4b8939.pth (2445.7MB) - Primary")
        print("     - u2net.pth (168.1MB) - Alternative")
        print("     - mobile_sam.pt (38.8MB) - Alternative")
        print("     - isnetis.onnx (168.1MB) - Alternative")
        print("   5. conda í™˜ê²½ ì„¤ì • (pytorch, pillow, transformers ë“±)")
        print("   6. AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (segment-anything, rembg, onnxruntime)")

def example_step_model_requirements_usage():
    """step_model_requirements.py ì™„ì „ í˜¸í™˜ ì‚¬ìš© ì˜ˆì‹œ"""
    print("ğŸ”¥ MyCloset AI Step 03 - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 100)
    print()
    print("ğŸ¯ ì£¼ìš” íŠ¹ì§•:")
    print("   âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ êµ¬í˜„")
    print("   âœ… EnhancedRealModelRequest í‘œì¤€ ì¤€ìˆ˜")
    print("   âœ… step_input_schema/step_output_schema ì™„ì „ ì •ì˜")
    print("   âœ… accepts_from_previous_step/provides_to_next_step ì™„ì „ êµ¬í˜„")
    print("   âœ… api_input_mapping/api_output_mapping êµ¬í˜„")
    print("   âœ… preprocessing_steps/postprocessing_steps ì™„ì „ ì •ì˜")
    print("   âœ… RealSAMModel í´ë˜ìŠ¤ëª… í‘œì¤€ ì¤€ìˆ˜")
    print("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (sam_vit_h_4b8939.pth 2445.7MB)")
    print("   âœ… BaseStepMixin v16.0 í˜¸í™˜ì„± ìœ ì§€")
    print("   âœ… OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬")
    print("   âœ… M3 Max 128GB ìµœì í™”")
    print()
    print("ğŸš€ ì‚¬ìš©ë²•:")
    print("""
    # 1. step_model_requirements.py í˜¸í™˜ ê¸°ë³¸ ì‚¬ìš©
    from step_03_cloth_segmentation import ClothSegmentationStep
    
    step = ClothSegmentationStep()
    await step.initialize()  # step_model_requirements.py ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”©
    
    # 2. step_model_requirements.py í‘œì¤€ ì…ë ¥ ì‚¬ìš©
    input_data = StepInputData(
        image=your_image,
        clothing_image=clothing_item,
        prompt_points=[(512, 256)],  # SAM í”„ë¡¬í”„íŠ¸
        session_id="your_session"
    )
    
    result = await step.process(input_data)
    
    # 3. step_model_requirements.py í‘œì¤€ ì¶œë ¥ í™œìš©
    cloth_mask = result.cloth_mask  # np.ndarray
    segmented_clothing = result.segmented_clothing  # np.ndarray
    confidence = result.confidence  # float
    clothing_type = result.clothing_type  # str
    
    # 4. Step ê°„ ì—°ë™ (provides_to_next_step)
    step_04_data = result.next_step_input['step_04']
    step_05_data = result.next_step_input['step_05']
    step_06_data = result.next_step_input['step_06']
    
    # 5. M3 Max ìµœì í™” ë²„ì „
    m3_step = create_m3_max_segmentation_step()
    await m3_step.initialize()
    
    # 6. ë°°ì¹˜ ì²˜ë¦¬
    batch_results = await step.process_batch([img1, img2, img3])
    """)

def print_conda_setup_guide_step_model_requirements():
    """step_model_requirements.py í˜¸í™˜ conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ"""
    print("ğŸ”§ step_model_requirements.py ì™„ì „ í˜¸í™˜ conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ")
    print("=" * 80)
    print()
    print("# 1. conda í™˜ê²½ ìƒì„±")
    print("conda create -n mycloset-ai-requirements python=3.10")
    print("conda activate mycloset-ai-requirements")
    print()
    print("# 2. í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (conda ìš°ì„ )")
    print("conda install pytorch torchvision torchaudio -c pytorch")
    print("conda install pillow numpy scipy scikit-learn")
    print("conda install matplotlib opencv")
    print()
    print("# 3. AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (pip)")
    print("pip install segment-anything")
    print("pip install rembg")
    print("pip install onnxruntime")
    print("pip install transformers")
    print("pip install basicsr")  # Real-ESRGAN
    print()
    print("# 4. ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë°°ì¹˜")
    print("mkdir -p ai_models/step_03_cloth_segmentation/ultra_models")
    print()
    print("# Primary ëª¨ë¸ (2445.7MB)")
    print("wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth")
    print("mv sam_vit_h_4b8939.pth ai_models/step_03_cloth_segmentation/ultra_models/")
    print()
    print("# Alternative ëª¨ë¸ë“¤")
    print("# u2net.pth (168.1MB) - ì˜ë¥˜ íŠ¹í™”")
    print("# mobile_sam.pt (38.8MB) - ê²½ëŸ‰ ëª¨ë¸")
    print("# isnetis.onnx (168.1MB) - ONNX ëª¨ë¸")
    print()
    print("# 5. step_model_requirements.py ëª¨ë“ˆ ìœ„ì¹˜")
    print("# app/ai_pipeline/utils/step_model_requirements.py")
    print()
    print("# 6. ë””ë ‰í† ë¦¬ êµ¬ì¡°")
    print("""
    ai_models/
    â””â”€â”€ step_03_cloth_segmentation/
        â”œâ”€â”€ sam_vit_h_4b8939.pth (2445.7MB) - Primary
        â”œâ”€â”€ u2net.pth (168.1MB) - Alternative
        â”œâ”€â”€ mobile_sam.pt (38.8MB) - Alternative
        â”œâ”€â”€ isnetis.onnx (168.1MB) - Alternative
        â””â”€â”€ ultra_models/
            â””â”€â”€ sam_vit_h_4b8939.pth (ê³µìœ  ëª¨ë¸)
    """)
    print()
    print("âœ… ì™„ë£Œ í›„ step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ëŠ¥!")

# ==============================================
# ğŸ”¥ 21. ëª¨ë“ˆ ì •ë³´ ë° ë©”íƒ€ë°ì´í„°
# ==============================================

__version__ = "21.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™”"
__compatibility_version__ = "step_model_requirements_v8.0 + BaseStepMixin_v16.0"
__features__ = [
    # step_model_requirements.py ì™„ì „ í˜¸í™˜
    "DetailedDataSpec êµ¬ì¡° ì™„ì „ ì ìš©",
    "EnhancedRealModelRequest í‘œì¤€ ì¤€ìˆ˜",
    "step_input_schema/step_output_schema ì™„ì „ êµ¬í˜„",
    "accepts_from_previous_step/provides_to_next_step ì™„ì „ ì •ì˜",
    "api_input_mapping/api_output_mapping êµ¬í˜„",
    "preprocessing_steps/postprocessing_steps ì™„ì „ ì •ì˜",
    "RealSAMModel í´ë˜ìŠ¤ëª… í‘œì¤€ ì¤€ìˆ˜",
    
    # ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ í™œìš©
    "ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (sam_vit_h_4b8939.pth 2445.7MB Primary)",
    "U2Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸ (u2net.pth 168.1MB Alternative)",
    "Mobile SAM ê²½ëŸ‰ ëª¨ë¸ (mobile_sam.pt 38.8MB Alternative)",
    "ISNet ONNX ëª¨ë¸ (isnetis.onnx 168.1MB Alternative)",
    "ì§„ì§œ AI ì¶”ë¡  ë¡œì§ êµ¬í˜„ (RealSAMModel, RealU2NetClothModel ë“±)",
    "ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”© ë° ê°€ì¤‘ì¹˜ ë§¤í•‘",
    
    # AI ê°•í™” ê¸°ëŠ¥
    "OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (AIImageProcessor)",
    "AI ê°•í™” ì‹œê°í™” (Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§)",
    "ì‹¤ì œ ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±",
    "AI ê¸°ë°˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ (í™€ ì±„ìš°ê¸°, ê²½ê³„ ê°œì„ )",
    "í•˜ì´ë¸Œë¦¬ë“œ AI ì¶”ë¡  (ì—¬ëŸ¬ ëª¨ë¸ ì•™ìƒë¸”)",
    
    # BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜
    "BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜",
    "UnifiedDependencyManager ì—°ë™",
    "TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€",
    "ìë™ ì˜ì¡´ì„± ì£¼ì… ì§€ì›",
    "get_model, optimize_memory, warmup ë“± í‘œì¤€ ë©”ì„œë“œ",
    
    # ì‹œìŠ¤í…œ ìµœì í™”
    "M3 Max 128GB ìµœì í™”",
    "MPS ê°€ì† ì§€ì›",
    "conda í™˜ê²½ ìš°ì„ ",
    "ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëŒ€í˜• ëª¨ë¸ ì²˜ë¦¬",
    "í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±",
    
    # Step ê°„ ì—°ë™
    "Step ê°„ ë°ì´í„° íë¦„ ì™„ì „ ì •ì˜",
    "provides_to_next_step ìŠ¤í‚¤ë§ˆ ì™„ì „ êµ¬í˜„",
    "accepts_from_previous_step ìŠ¤í‚¤ë§ˆ ì™„ì „ êµ¬í˜„",
    "StepInputData/StepOutputData í‘œì¤€ ì§€ì›",
    
    # ê³ ê¸‰ ê¸°ëŠ¥
    "ë°°ì¹˜ ì²˜ë¦¬ ì§€ì› (process_batch)",
    "ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›",
    "ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”",
    "ì™„ì „í•œ ì—ëŸ¬ í•¸ë“¤ë§",
    "ìƒì„¸í•œ ë¡œê¹… ë° ì§„ë‹¨"
]

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'ClothSegmentationStep',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (step_model_requirements.py í‘œì¤€)
    'RealSAMModel',           # Primary Model (sam_vit_h_4b8939.pth 2445.7MB)
    'RealU2NetClothModel',    # Alternative Model (u2net.pth 168.1MB)
    'RealMobileSAMModel',     # Alternative Model (mobile_sam.pt 38.8MB)
    'RealISNetModel',         # Alternative Model (isnetis.onnx 168.1MB)
    
    # AI ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ëŒ€ì²´)
    'AIImageProcessor',
    
    # ë°ì´í„° êµ¬ì¡° (step_model_requirements.py í˜¸í™˜)
    'SegmentationMethod',
    'ClothingType', 
    'QualityLevel',
    'SegmentationConfig',
    'SegmentationResult',
    'StepInputData',
    'StepOutputData',
    
    # BaseStepMixin v16.0 í˜¸í™˜
    'BaseStepMixinFallback',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_cloth_segmentation_step',
    'create_and_initialize_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    'create_requirements_compatible_step',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_step_model_requirements_compatibility',
    'example_step_model_requirements_usage',
    'print_conda_setup_guide_step_model_requirements'
]

# ==============================================
# ğŸ”¥ 22. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger.info("=" * 120)
logger.info("ğŸ”¥ Step 03 Cloth Segmentation v21.0 - step_model_requirements.py ì™„ì „ í˜¸í™˜ + AI ê°•í™” ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 120)
logger.info(f"ğŸ¯ step_model_requirements.py ì™„ì „ í˜¸í™˜:")
logger.info(f"   âœ… DetailedDataSpec êµ¬ì¡° ì™„ì „ ì ìš©")
logger.info(f"   âœ… EnhancedRealModelRequest í‘œì¤€ ì¤€ìˆ˜")
logger.info(f"   âœ… step_input_schema/step_output_schema ì™„ì „ êµ¬í˜„")
logger.info(f"   âœ… accepts_from_previous_step/provides_to_next_step ì™„ì „ ì •ì˜")
logger.info(f"   âœ… api_input_mapping/api_output_mapping êµ¬í˜„")
logger.info(f"   âœ… preprocessing_steps/postprocessing_steps ì™„ì „ ì •ì˜")
logger.info(f"   âœ… RealSAMModel í´ë˜ìŠ¤ëª… í‘œì¤€ ì¤€ìˆ˜")
logger.info(f"ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ í™œìš©:")
logger.info(f"   ğŸ¯ Primary: sam_vit_h_4b8939.pth (2445.7MB)")
logger.info(f"   ğŸ”„ Alternative: u2net.pth (168.1MB)")
logger.info(f"   âš¡ Alternative: mobile_sam.pt (38.8MB)")  
logger.info(f"   ğŸ”§ Alternative: isnetis.onnx (168.1MB)")
logger.info(f"ğŸ”¥ AI ê°•í™” ê¸°ëŠ¥:")
logger.info(f"   âœ… OpenCV ì™„ì „ ì œê±° ë° AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬")
logger.info(f"   âœ… Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§")
logger.info(f"   âœ… í•˜ì´ë¸Œë¦¬ë“œ AI ì¶”ë¡  (ëª¨ë¸ ì•™ìƒë¸”)")
logger.info(f"   âœ… ì‹¤ì œ ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±")
logger.info(f"ğŸ”— BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜:")
logger.info(f"   âœ… UnifiedDependencyManager ì—°ë™")
logger.info(f"   âœ… TYPE_CHECKING íŒ¨í„´ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info(f"   âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… ì§€ì›")
logger.info(f"   âœ… í‘œì¤€ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info(f"âš¡ ì‹œìŠ¤í…œ ìµœì í™”:")
logger.info(f"   ğŸ M3 Max 128GB ìµœì í™”")
logger.info(f"   âš¡ MPS ê°€ì† ì§€ì›")
logger.info(f"   ğŸ conda í™˜ê²½ ìš°ì„ ")
logger.info(f"   ğŸ­ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info(f"ğŸ”„ Step ê°„ ì—°ë™ ì™„ì „ ì§€ì›:")
logger.info(f"   âœ… provides_to_next_step ìŠ¤í‚¤ë§ˆ ì™„ì „ êµ¬í˜„")
logger.info(f"   âœ… accepts_from_previous_step ìŠ¤í‚¤ë§ˆ ì™„ì „ êµ¬í˜„")
logger.info(f"   âœ… StepInputData/StepOutputData í‘œì¤€ ì§€ì›")
logger.info(f"ğŸ’ ê³ ê¸‰ ê¸°ëŠ¥:")
logger.info(f"   âœ… ë°°ì¹˜ ì²˜ë¦¬ ì§€ì› (process_batch)")
logger.info(f"   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›")
logger.info(f"   âœ… ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”")
logger.info(f"   âœ… ì™„ì „í•œ ì—ëŸ¬ í•¸ë“¤ë§")

# ì´ˆê¸°í™” ì‹œ step_model_requirements.py ìš”êµ¬ì‚¬í•­ í™•ì¸
if STEP_REQUIREMENTS:
    logger.info("âœ… step_model_requirements.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì„±ê³µ")
    logger.info(f"   - ëª¨ë¸ëª…: {STEP_REQUIREMENTS.model_name}")
    logger.info(f"   - AI í´ë˜ìŠ¤: {STEP_REQUIREMENTS.ai_class}")
    logger.info(f"   - Primary íŒŒì¼: {STEP_REQUIREMENTS.primary_file} ({STEP_REQUIREMENTS.primary_size_mb}MB)")
    logger.info(f"   - ì…ë ¥ í¬ê¸°: {STEP_REQUIREMENTS.input_size}")
    logger.info(f"   - ëª¨ë¸ ì•„í‚¤í…ì²˜: {STEP_REQUIREMENTS.model_architecture}")
    logger.info(f"   - ê²€ìƒ‰ ê²½ë¡œ: {len(STEP_REQUIREMENTS.search_paths)}ê°œ")
    logger.info(f"   - Alternative íŒŒì¼: {len(STEP_REQUIREMENTS.alternative_files)}ê°œ")
    logger.info(f"   - ê³µìœ  ìœ„ì¹˜: {len(STEP_REQUIREMENTS.shared_locations)}ê°œ")
    
    # DetailedDataSpec ì •ë³´
    if STEP_REQUIREMENTS.data_spec:
        logger.info(f"   - ì…ë ¥ ë°ì´í„° íƒ€ì…: {len(STEP_REQUIREMENTS.data_spec.input_data_types)}ê°œ")
        logger.info(f"   - ì¶œë ¥ ë°ì´í„° íƒ€ì…: {len(STEP_REQUIREMENTS.data_spec.output_data_types)}ê°œ")
        logger.info(f"   - ì „ì²˜ë¦¬ ë‹¨ê³„: {len(STEP_REQUIREMENTS.data_spec.preprocessing_steps)}ê°œ")
        logger.info(f"   - í›„ì²˜ë¦¬ ë‹¨ê³„: {len(STEP_REQUIREMENTS.data_spec.postprocessing_steps)}ê°œ")
        logger.info(f"   - API ì…ë ¥ ë§¤í•‘: {len(STEP_REQUIREMENTS.data_spec.api_input_mapping)}ê°œ")
        logger.info(f"   - API ì¶œë ¥ ë§¤í•‘: {len(STEP_REQUIREMENTS.data_spec.api_output_mapping)}ê°œ")
else:
    logger.warning("âš ï¸ step_model_requirements.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨")
    logger.warning("   ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë™ì‘í•˜ì§€ë§Œ ì™„ì „í•œ í˜¸í™˜ì„±ì„ ìœ„í•´ step_model_requirements.py ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤")

logger.info("=" * 120)
logger.info("ğŸ‰ Step 03 Cloth Segmentation v21.0 ì´ˆê¸°í™” ì™„ë£Œ")
logger.info("ğŸ¯ step_model_requirements.py ì™„ì „ í˜¸í™˜ + BaseStepMixin v16.0 + AI ê°•í™”")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë”” ìƒíƒœ!")
logger.info("=" * 120)