# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Clothing Segmentation) - ë°°ê²½ ì œê±°
ê¸°ì¡´ ì½”ë“œë¥¼ í†µí•©í•˜ê³  ì‹¤ì œ ì‘ë™í•˜ë„ë¡ ê°œì„ í•œ ë²„ì „
UÂ²-Net + RemBG + ë°±ì—… ë°©ë²•ë“¤ì„ í†µí•©í•˜ì—¬ M3 Maxì—ì„œ ìµœì  ì„±ëŠ¥ ì œê³µ
"""
import os
import logging
import time
import asyncio
from typing import Dict, Any, Optional, Tuple, List, Union
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from PIL import Image
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# ë°°ê²½ ì œê±° ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
except ImportError:
    REMBG_AVAILABLE = False
    logging.warning("rembg ì„¤ì¹˜ í•„ìš”: pip install rembg")

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)

class ClothingSegmentationStep:
    """
    ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… - ê¸°ì¡´ ë‘ íŒŒì¼ì˜ ì¥ì ì„ í†µí•©
    - M3 Max MPS ìµœì í™”
    - RemBG + UÂ²-Net + ë°±ì—… ë°©ë²•ë“¤ í†µí•©
    - í’ˆì§ˆ ê¸°ë°˜ ìë™ ì„ íƒ ë° í´ë°±
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    CLOTHING_CATEGORIES = {
        'upper': ['shirt', 't-shirt', 'blouse', 'sweater', 'jacket', 'coat', 'dress'],
        'lower': ['pants', 'jeans', 'skirt', 'shorts', 'trousers'],
        'full': ['dress', 'jumpsuit', 'overall'],
        'accessories': ['hat', 'scarf', 'gloves', 'shoes', 'bag']
    }
    
    def __init__(self, model_loader, device: str, config: Dict[str, Any] = None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.model_loader = model_loader
        self.device = self._setup_optimal_device(device)
        self.config = config or {}
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì • (ë‘ íŒŒì¼ì˜ ì„¤ì • í†µí•©)
        self.segmentation_config = self.config.get('segmentation', {
            'method': 'auto',  # auto, rembg, u2net, grabcut, kmeans, threshold
            'model_name': 'u2net',  # rembg ëª¨ë¸ëª…
            'post_processing': True,
            'edge_refinement': True,
            'multi_scale': False,
            'quality_threshold': 0.7,
            'fallback_methods': ['rembg', 'u2net', 'grabcut', 'threshold'],
            'use_ensemble': False  # ê³ í’ˆì§ˆ ëª¨ë“œì—ì„œë§Œ í™œì„±í™”
        })
        
        # í›„ì²˜ë¦¬ ì„¤ì • (ê¸°ì¡´ ì½”ë“œ í™•ì¥)
        self.post_process_config = self.config.get('post_processing', {
            'morphology_enabled': True,
            'gaussian_blur': True,
            'edge_smoothing': True,
            'noise_removal': True,
            'bilateral_filter': True,
            'alpha_matting': False  # ê³ ê¸‰ ê¸°ëŠ¥
        })
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (M3 Max íŠ¹í™”)
        self.performance_config = self.config.get('performance', {
            'use_mps': self.device == 'mps',
            'batch_processing': True,
            'memory_efficient': True,
            'async_processing': True,
            'cache_models': True,
            'max_resolution': 1024
        })
        
        # ëª¨ë¸ë“¤ (ê¸°ì¡´ ì½”ë“œ êµ¬ì¡° ìœ ì§€)
        self.rembg_session = None
        self.rembg_sessions = {}  # ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
        self.segmentation_model = None  # UÂ²-Net ëª¨ë¸
        self.backup_segmenter = None
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'average_time': 0.0,
            'method_success_rate': {},
            'quality_distribution': []
        }
        
        # ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=2)
        
        self.is_initialized = False
        
        logger.info(f"ğŸ¯ í†µí•© ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _setup_optimal_device(self, preferred_device: str) -> str:
        """M3 Max ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if preferred_device == 'mps' and torch.backends.mps.is_available():
            logger.info("âœ… M3 Max MPS ë°±ì—”ë“œ ì‚¬ìš©")
            return 'mps'
        elif preferred_device == 'cuda' and torch.cuda.is_available():
            logger.info("âœ… CUDA ë°±ì—”ë“œ ì‚¬ìš©") 
            return 'cuda'
        else:
            logger.info("âš ï¸ CPU ë°±ì—”ë“œ ì‚¬ìš©")
            return 'cpu'
    
    async def initialize(self) -> bool:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œ êµ¬ì¡° ìœ ì§€)"""
        try:
            logger.info("ğŸ”„ í†µí•© ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # 1. RemBG ëª¨ë¸ë“¤ ì´ˆê¸°í™”
            await self._initialize_rembg_models()
            
            # 2. UÂ²-Net ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œ ë°©ì‹)
            await self._initialize_segmentation_models()
            
            # 3. ë°±ì—… ë°©ë²•ë“¤ ì´ˆê¸°í™”
            self.backup_segmenter = BackupSegmentationMethods(self.device)
            
            # 4. ëª¨ë¸ ì›Œë°ì—…
            await self._warmup_models()
            
            self.is_initialized = True
            logger.info("âœ… í†µí•© ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _initialize_rembg_models(self):
        """RemBG ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        if not REMBG_AVAILABLE:
            logger.warning("RemBG ì‚¬ìš© ë¶ˆê°€ - ë°±ì—… ë°©ë²• ì‚¬ìš©")
            return
        
        try:
            # ê¸°ë³¸ ëª¨ë¸ ë¨¼ì € ë¡œë“œ
            model_name = self.segmentation_config['model_name']
            logger.info(f"ğŸ“¦ RemBG ê¸°ë³¸ ëª¨ë¸ ë¡œë”©: {model_name}")
            self.rembg_session = new_session(model_name)
            self.rembg_sessions[model_name] = self.rembg_session
            
            # ì¶”ê°€ ëª¨ë¸ë“¤ (ì˜ë¥˜ë³„ íŠ¹í™”)
            additional_models = ['u2net_human_seg', 'silueta']
            for model in additional_models:
                if model != model_name:
                    try:
                        session = new_session(model)
                        self.rembg_sessions[model] = session
                        logger.info(f"âœ… ì¶”ê°€ ëª¨ë¸ ë¡œë“œ: {model}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì¶”ê°€ ëª¨ë¸ {model} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        
            logger.info("âœ… RemBG ëª¨ë¸ë“¤ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ RemBG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.rembg_session = None
    
    async def _initialize_segmentation_models(self):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œ ë°©ì‹ ìœ ì§€)"""
        try:
            model_type = self.config.get('model_type', 'rembg')
            
            if model_type == 'u2net' or not REMBG_AVAILABLE:
                # UÂ²-Net ëª¨ë¸ ì´ˆê¸°í™”
                self.segmentation_model = await self._initialize_u2net()
            elif model_type == 'custom':
                # ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™”
                self.segmentation_model = await self._initialize_custom_model()
            
            logger.info("âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.segmentation_model = None
    
    async def _initialize_u2net(self):
        """UÂ²-Net ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        try:
            # UÂ²-Net ì•„í‚¤í…ì²˜ ìƒì„±
            model = self._create_u2net_model()
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œë„
            model_path = self._get_u2net_model_path()
            if os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"âœ… UÂ²-Net ê°€ì¤‘ì¹˜ ë¡œë“œ: {model_path}")
            else:
                logger.warning(f"âš ï¸ UÂ²-Net ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ: {model_path}")
            
            # ëª¨ë¸ ìµœì í™” (model_loader ì‚¬ìš©)
            if self.model_loader:
                model = self.model_loader.optimize_model(model, 'cloth_segmentation')
            
            model = model.to(self.device)
            model.eval()
            
            # M3 Max ìµœì í™”
            if self.device == 'mps':
                model = torch.jit.optimize_for_inference(model)
            
            return model
            
        except Exception as e:
            logger.warning(f"UÂ²-Net ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return self._create_demo_segmentation_model()
    
    async def _initialize_custom_model(self):
        """ì»¤ìŠ¤í…€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œ)"""
        return self._create_demo_segmentation_model()
    
    def _create_u2net_model(self):
        """UÂ²-Net ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„± (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        class U2NetSegmentation(torch.nn.Module):
            def __init__(self, in_ch=3, out_ch=1):
                super(U2NetSegmentation, self).__init__()
                
                # ì¸ì½”ë” (ë” íš¨ìœ¨ì ì¸ êµ¬ì¡°)
                self.encoder1 = self._conv_block(in_ch, 64)
                self.encoder2 = self._conv_block(64, 128)
                self.encoder3 = self._conv_block(128, 256)
                self.encoder4 = self._conv_block(256, 512)
                
                # ì¤‘ê°„ ë ˆì´ì–´
                self.middle = self._conv_block(512, 1024)
                
                # ë””ì½”ë”
                self.decoder4 = self._conv_block(1024 + 512, 512)
                self.decoder3 = self._conv_block(512 + 256, 256)
                self.decoder2 = self._conv_block(256 + 128, 128)
                self.decoder1 = self._conv_block(128 + 64, 64)
                
                # ì¶œë ¥ ë ˆì´ì–´
                self.final = torch.nn.Conv2d(64, out_ch, 1)
                
                # í’€ë§ ë° ì—…ìƒ˜í”Œë§
                self.pool = torch.nn.MaxPool2d(2)
                self.upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
                
                # ë“œë¡­ì•„ì›ƒ (ì •ê·œí™”)
                self.dropout = torch.nn.Dropout2d(0.2)
            
            def _conv_block(self, in_ch, out_ch):
                return torch.nn.Sequential(
                    torch.nn.Conv2d(in_ch, out_ch, 3, padding=1),
                    torch.nn.BatchNorm2d(out_ch),
                    torch.nn.ReLU(inplace=True),
                    torch.nn.Conv2d(out_ch, out_ch, 3, padding=1),
                    torch.nn.BatchNorm2d(out_ch),
                    torch.nn.ReLU(inplace=True)
                )
            
            def forward(self, x):
                # ì¸ì½”ë”
                e1 = self.encoder1(x)
                e2 = self.encoder2(self.pool(e1))
                e3 = self.encoder3(self.pool(e2))
                e4 = self.encoder4(self.pool(e3))
                
                # ì¤‘ê°„
                m = self.middle(self.pool(e4))
                m = self.dropout(m)
                
                # ë””ì½”ë”
                d4 = self.decoder4(torch.cat([self.upsample(m), e4], dim=1))
                d3 = self.decoder3(torch.cat([self.upsample(d4), e3], dim=1))
                d2 = self.decoder2(torch.cat([self.upsample(d3), e2], dim=1))
                d1 = self.decoder1(torch.cat([self.upsample(d2), e1], dim=1))
                
                # ì¶œë ¥
                output = torch.sigmoid(self.final(d1))
                
                return output
        
        return U2NetSegmentation().to(self.device)
    
    def _create_demo_segmentation_model(self):
        """ë°ëª¨ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ (ê¸°ì¡´ ì½”ë“œ)"""
        class DemoSegmentationModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1)
                self.conv2 = torch.nn.Conv2d(64, 32, 3, padding=1)
                self.conv3 = torch.nn.Conv2d(32, 1, 1)
                
            def forward(self, x):
                x = torch.relu(self.conv1(x))
                x = torch.relu(self.conv2(x))
                x = torch.sigmoid(self.conv3(x))
                return x
        
        return DemoSegmentationModel().to(self.device)
    
    def _get_u2net_model_path(self) -> str:
        """UÂ²-Net ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ ì½”ë“œ)"""
        model_dir = self.config.get('model_dir', 'app/ai_pipeline/models')
        model_file = self.config.get('model_file', 'u2net_cloth.pth')
        return os.path.join(model_dir, model_file)
    
    async def _warmup_models(self):
        """ëª¨ë¸ ì›Œë°ì—…"""
        logger.info("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
        
        try:
            # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì›Œë°ì—…
            dummy_input = torch.randn(1, 3, 320, 320).to(self.device)
            
            if self.segmentation_model:
                with torch.no_grad():
                    _ = self.segmentation_model(dummy_input)
            
            logger.info("âœ… ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def process(
        self, 
        clothing_image_tensor: torch.Tensor,
        clothing_type: str = "shirt",
        quality_level: str = "high"
    ) -> Dict[str, Any]:
        """
        ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
        
        Args:
            clothing_image_tensor: ì˜ë¥˜ ì´ë¯¸ì§€ í…ì„œ [1, 3, H, W]
            clothing_type: ì˜ë¥˜ íƒ€ì… (shirt, pants, dress, etc.)
            quality_level: í’ˆì§ˆ ë ˆë²¨ (low, medium, high)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # 1. í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            clothing_pil = self._tensor_to_pil(clothing_image_tensor)
            
            # 2. ìµœì  ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì„ íƒ
            method = self._select_best_method(clothing_pil, clothing_type, quality_level)
            
            # 3. ë©”ì¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
            logger.info(f"ğŸ” ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰: {method}")
            segmentation_result = self._perform_main_segmentation(clothing_pil, method)
            
            # 4. ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€
            logger.info("ğŸ“Š í’ˆì§ˆ í‰ê°€ ì¤‘...")
            quality_score = self._evaluate_segmentation_quality(
                clothing_pil, segmentation_result['mask']
            )
            
            # 5. í’ˆì§ˆì´ ë‚®ìœ¼ë©´ í´ë°± ë°©ë²• ì‹œë„
            if quality_score < self.segmentation_config['quality_threshold']:
                logger.info(f"ğŸ”„ í’ˆì§ˆ ê°œì„  ì‹œë„ (í˜„ì¬: {quality_score:.3f})")
                improved_result = self._try_fallback_methods(clothing_pil, clothing_type)
                if improved_result and improved_result.get('quality', 0) > quality_score:
                    segmentation_result = improved_result
                    quality_score = improved_result['quality']
            
            # 6. í›„ì²˜ë¦¬ ì ìš©
            logger.info("âœ¨ í›„ì²˜ë¦¬ ì ìš© ì¤‘...")
            processed_result = self._apply_post_processing(
                segmentation_result, quality_level
            )
            
            # 7. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            
            result = self._build_result(
                processed_result, quality_score, processing_time, 
                method, clothing_type
            )
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(method, processing_time, quality_score)
            
            logger.info(f"âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - {processing_time:.3f}ì´ˆ, í’ˆì§ˆ: {quality_score:.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _select_best_method(self, image: Image.Image, clothing_type: str, quality_level: str) -> str:
        """ìµœì  ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì„ íƒ"""
        method = self.segmentation_config['method']
        
        if method == 'auto':
            # ì´ë¯¸ì§€ ë³µì¡ë„ ë¶„ì„
            complexity = self._analyze_image_complexity(image)
            
            # í’ˆì§ˆ ë ˆë²¨ê³¼ ë³µì¡ë„ì— ë”°ë¥¸ ë°©ë²• ì„ íƒ
            if quality_level == 'high' and complexity > 0.7:
                return 'ensemble' if self.segmentation_config['use_ensemble'] else 'rembg'
            elif REMBG_AVAILABLE and complexity < 0.5:
                return 'rembg'
            elif self.segmentation_model and complexity > 0.3:
                return 'u2net'
            else:
                return 'grabcut'
        
        return method
    
    def _analyze_image_complexity(self, image: Image.Image) -> float:
        """ì´ë¯¸ì§€ ë³µì¡ë„ ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
            
            # ì—£ì§€ ë°€ë„
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            
            # í…ìŠ¤ì²˜ ë³µì¡ë„ (LBP ê°„ì†Œí™” ë²„ì „)
            texture_var = np.var(gray)
            normalized_texture = min(texture_var / 10000, 1.0)
            
            # ë³µì¡ë„ ì ìˆ˜
            complexity = edge_density * 0.6 + normalized_texture * 0.4
            
            return min(complexity, 1.0)
            
        except Exception as e:
            logger.warning(f"ë³µì¡ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _perform_main_segmentation(self, image: Image.Image, method: str) -> Dict[str, Any]:
        """ë©”ì¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰"""
        
        if method == 'ensemble':
            return self._ensemble_segmentation(image)
        elif method == 'rembg' and self.rembg_session:
            return self._segment_with_rembg(image)
        elif method == 'u2net' and self.segmentation_model:
            return self._segment_with_u2net_model(image)
        elif method == 'grabcut':
            return self._segment_with_grabcut(image)
        elif method == 'kmeans':
            return self._segment_with_kmeans(image)
        else:
            return self._segment_with_threshold(image)
    
    def _ensemble_segmentation(self, image: Image.Image) -> Dict[str, Any]:
        """ì•™ìƒë¸” ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì—¬ëŸ¬ ë°©ë²• ì¡°í•©)"""
        results = []
        methods = ['rembg', 'u2net', 'grabcut']
        
        for method in methods:
            try:
                if method == 'rembg' and self.rembg_session:
                    result = self._segment_with_rembg(image)
                elif method == 'u2net' and self.segmentation_model:
                    result = self._segment_with_u2net_model(image)
                else:
                    result = self.backup_segmenter.grabcut_segmentation(image)
                
                if result:
                    results.append(result)
            except Exception as e:
                logger.warning(f"ì•™ìƒë¸” ë°©ë²• {method} ì‹¤íŒ¨: {e}")
        
        if not results:
            return self._segment_with_threshold(image)
        
        # ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ
        best_result = max(results, key=lambda x: x.get('confidence', 0))
        best_result['method'] = 'ensemble'
        
        return best_result
    
    def _segment_with_rembg(self, image: Image.Image) -> Dict[str, Any]:
        """RemBGë¥¼ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        try:
            # ì˜ë¥˜ë³„ ëª¨ë¸ ì„ íƒ
            model_selection = {
                'dress': 'u2net_human_seg',
                'upper': 'u2net',
                'lower': 'silueta'
            }
            
            # ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
            preferred_model = model_selection.get('upper', 'u2net')  # ê¸°ë³¸ê°’
            session = self.rembg_sessions.get(preferred_model, self.rembg_session)
            
            # RemBGë¡œ ë°°ê²½ ì œê±°
            result_image = remove(image, session=session)
            
            # ì•ŒíŒŒ ì±„ë„ì—ì„œ ë§ˆìŠ¤í¬ ì¶”ì¶œ
            if result_image.mode == 'RGBA':
                mask = np.array(result_image)[:, :, 3]
                segmented_rgb = result_image.convert('RGB')
            else:
                # ì•ŒíŒŒ ì±„ë„ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì„ê³„ê°’ ì‚¬ìš©
                gray = np.array(result_image.convert('L'))
                mask = (gray > 10).astype(np.uint8) * 255
                segmented_rgb = result_image.convert('RGB')
            
            return {
                'segmented_image': segmented_rgb,
                'mask': mask,
                'method': f'rembg_{preferred_model}',
                'confidence': 0.9
            }
            
        except Exception as e:
            logger.warning(f"RemBG ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return self._segment_with_threshold(image)
    
    def _segment_with_u2net_model(self, image: Image.Image) -> Dict[str, Any]:
        """UÂ²-Net ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        try:
            # ì…ë ¥ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_for_model(image)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                if self.device == 'mps':
                    # M3 Max ìµœì í™”
                    mask_pred = self.segmentation_model(input_tensor)
                else:
                    mask_pred = self.segmentation_model(input_tensor)
                
                # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
                mask = mask_pred.squeeze().cpu().numpy()
                mask = (mask * 255).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            if mask.shape != (image.height, image.width):
                mask = cv2.resize(mask, (image.width, image.height), interpolation=cv2.INTER_NEAREST)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ìƒì„±
            segmented_image = self._apply_mask_to_image(image, mask)
            
            return {
                'segmented_image': segmented_image,
                'mask': mask,
                'method': 'u2net',
                'confidence': 0.8
            }
            
        except Exception as e:
            logger.warning(f"UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return self._segment_with_threshold(image)
    
    def _segment_with_grabcut(self, image: Image.Image) -> Dict[str, Any]:
        """GrabCutì„ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê¸°ì¡´ ì½”ë“œ)"""
        return self.backup_segmenter.grabcut_segmentation(image)
    
    def _segment_with_kmeans(self, image: Image.Image) -> Dict[str, Any]:
        """K-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê¸°ì¡´ ì½”ë“œ)"""
        return self.backup_segmenter.kmeans_segmentation(image)
    
    def _segment_with_threshold(self, image: Image.Image) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê¸°ì¡´ ì½”ë“œ)"""
        return self.backup_segmenter.threshold_segmentation(image)
    
    def _try_fallback_methods(self, image: Image.Image, clothing_type: str) -> Optional[Dict[str, Any]]:
        """í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ë“¤ ì‹œë„ (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        fallback_methods = self.segmentation_config['fallback_methods']
        best_result = None
        best_quality = 0.0
        
        for method in fallback_methods:
            try:
                if method == 'rembg' and self.rembg_session:
                    result = self._segment_with_rembg(image)
                elif method == 'u2net' and self.segmentation_model:
                    result = self._segment_with_u2net_model(image)
                elif method == 'grabcut':
                    result = self.backup_segmenter.grabcut_segmentation(image)
                elif method == 'kmeans':
                    result = self.backup_segmenter.kmeans_segmentation(image)
                else:
                    result = self.backup_segmenter.threshold_segmentation(image)
                
                if result:
                    quality = self._evaluate_segmentation_quality(image, result['mask'])
                    result['quality'] = quality
                    
                    if quality > best_quality:
                        best_quality = quality
                        best_result = result
                        
            except Exception as e:
                logger.warning(f"í´ë°± ë°©ë²• {method} ì‹¤íŒ¨: {e}")
                continue
        
        return best_result
    
    def _apply_post_processing(
        self, 
        segmentation_result: Dict[str, Any], 
        quality_level: str
    ) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ì ìš© (ê¸°ì¡´ ì½”ë“œ í™•ì¥)"""
        
        mask = segmentation_result['mask']
        
        # í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ì²˜ë¦¬ ê°•ë„
        intensity_map = {'low': 1, 'medium': 2, 'high': 3}
        intensity = intensity_map.get(quality_level, 2)
        
        processed_mask = mask.copy()
        
        # 1. í˜•íƒœí•™ì  ì—°ì‚°
        if self.post_process_config['morphology_enabled']:
            kernel_size = 3 + intensity
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_CLOSE, kernel)
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_OPEN, kernel)
        
        # 2. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
        if self.post_process_config['gaussian_blur']:
            blur_kernel = 3 + intensity * 2
            if blur_kernel % 2 == 0:
                blur_kernel += 1
            processed_mask = cv2.GaussianBlur(processed_mask, (blur_kernel, blur_kernel), 0)
            processed_mask = (processed_mask > 127).astype(np.uint8) * 255
        
        # 3. ì—£ì§€ ìŠ¤ë¬´ë”©
        if self.post_process_config['edge_smoothing']:
            processed_mask = self._smooth_edges(processed_mask, intensity)
        
        # 4. ë…¸ì´ì¦ˆ ì œê±°
        if self.post_process_config['noise_removal']:
            processed_mask = self._remove_noise(processed_mask, intensity)
        
        # ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜
        mask_tensor = torch.from_numpy(processed_mask).float() / 255.0
        mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0).to(self.device)
        
        # ì²˜ë¦¬ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€ ìƒì„±
        processed_segmented = self._apply_mask_to_image(
            segmentation_result['segmented_image'], processed_mask
        )
        
        return {
            'segmented_image': processed_segmented,
            'mask_tensor': mask_tensor,
            'binary_mask': processed_mask,
            'confidence_map': self._generate_confidence_map(processed_mask)
        }
    
    def _smooth_edges(self, mask: np.ndarray, intensity: int) -> np.ndarray:
        """ì—£ì§€ ìŠ¤ë¬´ë”©"""
        # ê±°ë¦¬ ë³€í™˜
        dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)
        
        # ì„ê³„ê°’ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ê²½ê³„ ìƒì„±
        threshold = intensity * 2
        smooth_mask = (dist_transform > threshold).astype(np.uint8) * 255
        
        return smooth_mask
    
    def _remove_noise(self, mask: np.ndarray, intensity: int) -> np.ndarray:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        # ì—°ê²° ì„±ë¶„ ë¶„ì„
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
        
        if num_labels <= 1:
            return mask
        
        # ê°€ì¥ í° ì—°ê²° ì„±ë¶„ ì°¾ê¸°
        largest_component = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
        
        # ì‘ì€ ì„±ë¶„ë“¤ ì œê±°
        min_area = stats[largest_component, cv2.CC_STAT_AREA] * 0.1 / intensity
        
        cleaned_mask = np.zeros_like(mask)
        for i in range(1, num_labels):
            if stats[i, cv2.CC_STAT_AREA] >= min_area:
                cleaned_mask[labels == i] = 255
        
        return cleaned_mask
    
    def _generate_confidence_map(self, mask: np.ndarray) -> np.ndarray:
        """ì‹ ë¢°ë„ ë§µ ìƒì„±"""
        # ê±°ë¦¬ ë³€í™˜ìœ¼ë¡œ ì¤‘ì‹¬ë¶€ì¼ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
        dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)
        
        # ì •ê·œí™”
        if dist_transform.max() > 0:
            confidence_map = dist_transform / dist_transform.max()
        else:
            confidence_map = np.zeros_like(mask, dtype=np.float32)
        
        return confidence_map
    
    def _build_result(
        self,
        processed_result: Dict[str, Any],
        quality_score: float,
        processing_time: float,
        method: str,
        clothing_type: str
    ) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„± (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        
        mask = processed_result['binary_mask']
        segmented_image = processed_result['segmented_image']
        
        return {
            "success": True,
            "segmented_image": segmented_image,
            "clothing_mask": processed_result['mask_tensor'],
            "binary_mask": mask,
            "confidence_map": processed_result.get('confidence_map', None),
            "segmentation_quality": quality_score,
            "clothing_analysis": {
                "dominant_colors": self._extract_dominant_colors(segmented_image),
                "clothing_area": self._calculate_clothing_area(mask),
                "edge_complexity": self._calculate_edge_complexity(mask),
                "background_removed": True
            },
            "processing_info": {
                "method_used": method,
                "processing_time": processing_time,
                "post_processing_applied": True,
                "device": self.device
            }
        }
    
    def _extract_dominant_colors(self, image: Image.Image) -> List[List[int]]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        if not SKLEARN_AVAILABLE:
            return [[128, 128, 128]]  # ê¸°ë³¸ íšŒìƒ‰
        
        img_array = np.array(image)
        pixels = img_array.reshape(-1, 3)
        
        # ë°°ê²½ìƒ‰ ì œê±° (í°ìƒ‰ ê·¼ì²˜)
        non_white_pixels = pixels[np.sum(pixels, axis=1) < 700]
        
        if len(non_white_pixels) < 10:
            return [[128, 128, 128]]
        
        # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
        n_colors = min(5, len(non_white_pixels) // 100)
        if n_colors < 1:
            n_colors = 1
            
        kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
        kmeans.fit(non_white_pixels)
        
        dominant_colors = kmeans.cluster_centers_.astype(int).tolist()
        
        return dominant_colors
    
    def _calculate_clothing_area(self, mask: np.ndarray) -> Dict[str, float]:
        """ì˜ë¥˜ ì˜ì—­ ê³„ì‚°"""
        total_pixels = mask.size
        clothing_pixels = np.sum(mask > 0)
        
        return {
            'total_pixels': float(total_pixels),
            'clothing_pixels': float(clothing_pixels),
            'coverage_ratio': float(clothing_pixels / total_pixels),
            'area_score': min(1.0, clothing_pixels / 50000)  # ì •ê·œí™”
        }
    
    def _calculate_edge_complexity(self, mask: np.ndarray) -> Dict[str, float]:
        """ì—£ì§€ ë³µì¡ë„ ê³„ì‚°"""
        # ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(mask, 50, 150)
        edge_pixels = np.sum(edges > 0)
        
        # ìœ¤ê³½ì„  ë¶„ì„
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            main_contour = max(contours, key=cv2.contourArea)
            perimeter = cv2.arcLength(main_contour, True)
            area = cv2.contourArea(main_contour)
            
            # ë³µì¡ë„ ê³„ì‚° (ë‘˜ë ˆÂ²/ë©´ì  ë¹„ìœ¨)
            if area > 0:
                complexity = (perimeter * perimeter) / (4 * np.pi * area)
            else:
                complexity = 0.0
        else:
            perimeter = 0.0
            complexity = 0.0
        
        return {
            'edge_pixels': float(edge_pixels),
            'perimeter': float(perimeter),
            'complexity_ratio': float(complexity),
            'edge_density': float(edge_pixels / mask.size)
        }
    
    def _evaluate_segmentation_quality(self, original: Image.Image, mask: np.ndarray) -> float:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ ì½”ë“œ)"""
        # ë§ˆìŠ¤í¬ ì—°ê²°ì„± í‰ê°€
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return 0.0
        
        # ê°€ì¥ í° ì—°ê²° ìš”ì†Œ
        main_contour = max(contours, key=cv2.contourArea)
        main_area = cv2.contourArea(main_contour)
        total_mask_area = np.sum(mask > 0)
        
        # ì—°ê²°ì„± ì ìˆ˜ (ì£¼ìš” ì˜ì—­ì´ ì „ì²´ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨)
        connectivity_score = main_area / (total_mask_area + 1e-6)
        
        # ì—£ì§€ í’ˆì§ˆ í‰ê°€
        edge_quality = self._evaluate_edge_quality(mask)
        
        # í¬ê¸° ì ì ˆì„± (ì „ì²´ ì´ë¯¸ì§€ì—ì„œ ì˜ë¥˜ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨)
        image_area = original.width * original.height
        size_ratio = total_mask_area / image_area
        size_score = 1.0 if 0.1 <= size_ratio <= 0.8 else max(0.0, 1.0 - abs(size_ratio - 0.4) * 2)
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality = (connectivity_score * 0.4 + edge_quality * 0.4 + size_score * 0.2)
        
        return quality
    
    def _evaluate_edge_quality(self, mask: np.ndarray) -> float:
        """ì—£ì§€ í’ˆì§ˆ í‰ê°€"""
        # ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(mask, 50, 150)
        
        # ì—£ì§€ì˜ ë¶€ë“œëŸ¬ì›€ ì¸¡ì •
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return 0.5
        
        main_contour = max(contours, key=cv2.contourArea)
        
        # ìœ¤ê³½ì„  ê·¼ì‚¬í™”
        epsilon = 0.02 * cv2.arcLength(main_contour, True)
        approx = cv2.approxPolyDP(main_contour, epsilon, True)
        
        # ë¶€ë“œëŸ¬ì›€ ì ìˆ˜ (ê·¼ì‚¬í™” í›„ ì ì˜ ê°œìˆ˜ê°€ ì ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ì›€)
        if len(main_contour) > 0:
            smoothness = 1.0 - (len(approx) / len(main_contour))
        else:
            smoothness = 0.5
        
        return max(0.0, min(1.0, smoothness))
    
    def _update_stats(self, method: str, processing_time: float, quality_score: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_processed'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_stats['total_processed']
        current_avg = self.performance_stats['average_time']
        self.performance_stats['average_time'] = (current_avg * (total - 1) + processing_time) / total
        
        # ë°©ë²•ë³„ ì„±ê³µë¥ 
        if method not in self.performance_stats['method_success_rate']:
            self.performance_stats['method_success_rate'][method] = {'success': 0, 'total': 0}
        
        self.performance_stats['method_success_rate'][method]['total'] += 1
        if quality_score > 0.5:  # ì„±ê³µ ê¸°ì¤€
            self.performance_stats['method_success_rate'][method]['success'] += 1
        
        # í’ˆì§ˆ ë¶„í¬
        self.performance_stats['quality_distribution'].append(quality_score)
        if len(self.performance_stats['quality_distribution']) > 100:
            self.performance_stats['quality_distribution'] = self.performance_stats['quality_distribution'][-100:]
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì½”ë“œ)
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """PyTorch í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # [1, 3, H, W] -> [3, H, W]
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        # [3, H, W] -> [H, W, 3]
        if tensor.shape[0] == 3:
            tensor = tensor.permute(1, 2, 0)
        
        # ì •ê·œí™” í•´ì œ
        if tensor.max() <= 1.0:
            tensor = tensor * 255
        
        # NumPyë¡œ ë³€í™˜
        array = tensor.cpu().numpy().astype(np.uint8)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        return Image.fromarray(array)
    
    def _preprocess_for_model(self, image: Image.Image) -> torch.Tensor:
        """ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì „ì²˜ë¦¬"""
        # í¬ê¸° ì¡°ì •
        input_size = (320, 320)
        resized = image.resize(input_size, Image.Resampling.LANCZOS)
        
        # í…ì„œë¡œ ë³€í™˜
        tensor = torch.from_numpy(np.array(resized)).float() / 255.0
        tensor = tensor.permute(2, 0, 1).unsqueeze(0)
        
        # ì •ê·œí™” (ImageNet)
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        tensor = (tensor - mean) / std
        
        return tensor.to(self.device)
    
    def _apply_mask_to_image(self, image: Image.Image, mask: np.ndarray) -> Image.Image:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©í•˜ì—¬ ë°°ê²½ ì œê±°"""
        img_array = np.array(image)
        
        # 3ì±„ë„ ë§ˆìŠ¤í¬ ìƒì„±
        if len(mask.shape) == 2:
            mask_3ch = np.stack([mask] * 3, axis=2) / 255.0
        else:
            mask_3ch = mask / 255.0
        
        # ë°°ê²½ì„ í°ìƒ‰ìœ¼ë¡œ ì„¤ì •
        background = np.ones_like(img_array) * 255
        
        # ë§ˆìŠ¤í¬ ì ìš©
        result = img_array * mask_3ch + background * (1 - mask_3ch)
        
        return Image.fromarray(result.astype(np.uint8))
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ê¸°ì¡´ ì½”ë“œ)"""
        return {
            "step_name": "ClothingSegmentation",
            "version": "2.0",
            "device": self.device,
            "use_mps": self.device == 'mps',
            "initialized": self.is_initialized,
            "segmentation_config": self.segmentation_config,
            "post_process_config": self.post_process_config,
            "available_methods": ["rembg", "u2net", "grabcut", "kmeans", "threshold"],
            "rembg_available": REMBG_AVAILABLE,
            "sklearn_available": SKLEARN_AVAILABLE,
            "supported_models": list(self.rembg_sessions.keys()) if REMBG_AVAILABLE else [],
            "performance_stats": self.performance_stats
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ê¸°ì¡´ ì½”ë“œ)"""
        # ëª¨ë¸ë“¤ ì •ë¦¬
        if self.segmentation_model:
            del self.segmentation_model
            self.segmentation_model = None
        
        # RemBG ì„¸ì…˜ë“¤ ì •ë¦¬
        for session in self.rembg_sessions.values():
            del session
        self.rembg_sessions.clear()
        self.rembg_session = None
        
        # ë°±ì—… ì„¸ê·¸ë©˜í„° ì •ë¦¬
        if self.backup_segmenter:
            del self.backup_segmenter
            self.backup_segmenter = None
        
        # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
        self.thread_pool.shutdown(wait=True)
        
        # MPS ìºì‹œ ì •ë¦¬ (M3 Max)
        if self.device == 'mps':
            torch.mps.empty_cache()
        elif self.device == 'cuda':
            torch.cuda.empty_cache()
        
        self.is_initialized = False
        logger.info("ğŸ§¹ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


class BackupSegmentationMethods:
    """ë°±ì—… ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ë“¤"""
    
    def __init__(self, device: str):
        self.device = device
    
    def grabcut_segmentation(self, image: Image.Image) -> Dict[str, Any]:
        """GrabCut ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        h, w = img_cv.shape[:2]
        
        # ì´ˆê¸° ì‚¬ê°í˜• (ì¤‘ì•™ 80% ì˜ì—­)
        margin_h, margin_w = int(h * 0.1), int(w * 0.1)
        rect = (margin_w, margin_h, w - 2*margin_w, h - 2*margin_h)
        
        # GrabCut ì´ˆê¸°í™”
        mask = np.zeros((h, w), np.uint8)
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)
        
        try:
            # GrabCut ì‹¤í–‰
            cv2.grabCut(img_cv, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8') * 255
            
            # ê²°ê³¼ ì´ë¯¸ì§€
            result_cv = img_cv.copy()
            result_cv[mask2 == 0] = [255, 255, 255]
            result_rgb = cv2.cvtColor(result_cv, cv2.COLOR_BGR2RGB)
            result_pil = Image.fromarray(result_rgb)
            
            return {
                'segmented_image': result_pil,
                'mask': mask2,
                'method': 'grabcut',
                'confidence': 0.7
            }
        except Exception as e:
            logger.warning(f"GrabCut ì‹¤íŒ¨: {e}")
            return self.threshold_segmentation(image)
    
    def kmeans_segmentation(self, image: Image.Image) -> Dict[str, Any]:
        """K-means í´ëŸ¬ìŠ¤í„°ë§ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        if not SKLEARN_AVAILABLE:
            return self.threshold_segmentation(image)
        
        img_array = np.array(image)
        h, w, c = img_array.shape
        
        # í”½ì…€ ë°ì´í„° ì¤€ë¹„
        pixel_data = img_array.reshape((-1, 3))
        
        try:
            # K-means (3ê°œ í´ëŸ¬ìŠ¤í„°)
            kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
            labels = kmeans.fit_predict(pixel_data)
            
            # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ë¥¼ ë°°ê²½ìœ¼ë¡œ ê°„ì£¼
            unique_labels, counts = np.unique(labels, return_counts=True)
            background_label = unique_labels[np.argmax(counts)]
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = (labels != background_label).reshape((h, w)).astype(np.uint8) * 255
            
            # ê²°ê³¼ ì´ë¯¸ì§€
            result = img_array.copy()
            result[mask == 0] = [255, 255, 255]
            result_pil = Image.fromarray(result)
            
            return {
                'segmented_image': result_pil,
                'mask': mask,
                'method': 'kmeans',
                'confidence': 0.6
            }
        except Exception as e:
            logger.warning(f"K-means ì‹¤íŒ¨: {e}")
            return self.threshold_segmentation(image)
    
    def threshold_segmentation(self, image: Image.Image) -> Dict[str, Any]:
        """ì„ê³„ê°’ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = np.array(image.convert('L'))
            
            # Otsu ì„ê³„ê°’
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # ë°°ê²½ì´ ë°ì€ ê²½ìš° ë°˜ì „
            if np.mean(binary) > 127:
                binary = 255 - binary
            
            # í˜•íƒœí•™ì  ì—°ì‚°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
            
            # ê²°ê³¼ ì´ë¯¸ì§€
            img_array = np.array(image)
            result = img_array.copy()
            result[binary == 0] = [255, 255, 255]
            result_pil = Image.fromarray(result)
            
            return {
                'segmented_image': result_pil,
                'mask': binary,
                'method': 'threshold',
                'confidence': 0.5
            }
        except Exception as e:
            logger.error(f"ì„ê³„ê°’ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì „ê²½ìœ¼ë¡œ
            h, w = image.size
            mask = np.ones((w, h), dtype=np.uint8) * 255
            return {
                'segmented_image': image,
                'mask': mask,
                'method': 'fallback',
                'confidence': 0.3
            }