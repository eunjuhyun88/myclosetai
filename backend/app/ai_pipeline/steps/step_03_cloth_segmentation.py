# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (AI ëª¨ë¸ ì™„ì „ ì—°ë™ + BaseStepMixin v16.0 í˜¸í™˜)
================================================================================

âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ì„± ë³´ì¥
âœ… OpenCV ì™„ì „ ì œê±° ë° AI ëª¨ë¸ë¡œ ëŒ€ì²´
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€  
âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ (U2Net, SAM, RemBG, CLIP, Real-ESRGAN)
âœ… UnifiedDependencyManager ì™„ì „ í™œìš©
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥
âœ… Python êµ¬ì¡° ì™„ì „ ì •ë¦¬ (ë“¤ì—¬ì“°ê¸°, ë¬¸ë²• ì˜¤ë¥˜ ì—†ìŒ)

Author: MyCloset AI Team
Date: 2025-07-25  
Version: v10.0 (AI Complete + BaseStepMixin v16.0 Compatible)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import gc
import hashlib
import json
import base64
import weakref
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import platform
import subprocess

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ import (ëŸ°íƒ€ì„ì—ëŠ” import ì•ˆë¨) 
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.interface.step_interface import StepModelInterface
    from ..steps.base_step_mixin import BaseStepMixin
    from ..factories.step_factory import StepFactory
    from app.core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 2. í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (conda í™˜ê²½ ìš°ì„ ) - OpenCV ì™„ì „ ì œê±°
# ==============================================

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½ ìš°ì„ )")
except ImportError:
    logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")

# PIL Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
except ImportError:
    logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# PyTorch Import (conda í™˜ê²½ ìš°ì„ )
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥ (M3 Max ìµœì í™”)")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (OpenCV ëŒ€ì²´)
REMBG_AVAILABLE = False
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
    logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ (OpenCV ë°°ê²½ ì œê±° ëŒ€ì²´)")
except ImportError:
    logger.warning("âš ï¸ RemBG ì—†ìŒ - pip install rembg")

SKLEARN_AVAILABLE = False
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
    logger.info("ğŸ“ˆ scikit-learn ë¡œë“œ ì™„ë£Œ (OpenCV í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ì²´)")
except ImportError:
    logger.warning("âš ï¸ scikit-learn ì—†ìŒ - conda install scikit-learn")

SAM_AVAILABLE = False
try:
    import segment_anything as sam
    from segment_anything import sam_model_registry, SamPredictor
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ (OpenCV ì„¸ê·¸ë©˜í…Œì´ì…˜ ëŒ€ì²´)")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - pip install segment-anything")

TRANSFORMERS_AVAILABLE = False
try:
    from transformers import pipeline, CLIPProcessor, CLIPModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("ğŸ¤— Transformers ë¡œë“œ ì™„ë£Œ (OpenCV íŠ¹ì§• ì¶”ì¶œ ëŒ€ì²´)")
except ImportError:
    logger.warning("âš ï¸ Transformers ì—†ìŒ - pip install transformers")

ESRGAN_AVAILABLE = False
try:
    try:
        import basicsr
        from basicsr.archs.rrdbnet_arch import RRDBNet
        from basicsr.utils.download_util import load_file_from_url
        ESRGAN_AVAILABLE = True
        logger.info("âœ¨ Real-ESRGAN ë¡œë“œ ì™„ë£Œ (OpenCV ì´ë¯¸ì§€ ì²˜ë¦¬ ëŒ€ì²´)")
    except ImportError:
        # í´ë°±: ê¸°ë³¸ PyTorch ì—…ìƒ˜í”Œë§ ì‚¬ìš©
        ESRGAN_AVAILABLE = False
        logger.info("ğŸ”„ Real-ESRGAN ì—†ìŒ - ê¸°ë³¸ PyTorch ì—…ìƒ˜í”Œë§ ì‚¬ìš©")
except ImportError:
    logger.warning("âš ï¸ Real-ESRGAN ì—†ìŒ - pip install basicsr")

# ==============================================
# ğŸ”¥ 3. ë™ì  Import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

def get_base_step_mixin():
    """BaseStepMixinì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING íŒ¨í„´)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.error(f"âŒ BaseStepMixin ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING íŒ¨í„´)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        return None
    except ImportError as e:
        logger.error(f"âŒ ModelLoader ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_step_interface():
    """StepModelInterfaceë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING íŒ¨í„´)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interface.step_interface')
        return getattr(module, 'StepModelInterface', None)
    except ImportError as e:
        logger.error(f"âŒ StepModelInterface ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_di_container():
    """DI Containerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING íŒ¨í„´)"""
    try:
        import importlib  
        module = importlib.import_module('app.core.di_container')
        get_container = getattr(module, 'get_di_container', None)
        if get_container:
            return get_container()
        return None
    except ImportError as e:
        logger.warning(f"âš ï¸ DI Container ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 4. ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class SegmentationMethod(Enum):
    """AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• (OpenCV ì™„ì „ ëŒ€ì²´)"""
    U2NET = "u2net"                    # U2Net AI ëª¨ë¸
    REMBG = "rembg"                    # RemBG AI ëª¨ë¸
    SAM = "sam"                        # Segment Anything AI ëª¨ë¸
    CLIP_GUIDED = "clip_guided"        # CLIP ê¸°ë°˜ ì§€ëŠ¥ì  ì„¸ê·¸ë©˜í…Œì´ì…˜
    HYBRID_AI = "hybrid_ai"            # ì—¬ëŸ¬ AI ëª¨ë¸ ê²°í•©
    AUTO_AI = "auto_ai"                # ìë™ AI ë°©ë²• ì„ íƒ
    ESRGAN_ENHANCED = "esrgan_enhanced" # Real-ESRGAN í–¥ìƒëœ ì„¸ê·¸ë©˜í…Œì´ì…˜

class ClothingType(Enum):
    """ì˜ë¥˜ íƒ€ì…"""
    SHIRT = "shirt"
    DRESS = "dress"
    PANTS = "pants"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class SegmentationConfig:
    """AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.AUTO_AI
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    enable_visualization: bool = True
    enable_post_processing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    iou_threshold: float = 0.5
    ai_edge_smoothing: bool = True          # AI ê¸°ë°˜ ì—£ì§€ ìŠ¤ë¬´ë”©
    ai_noise_removal: bool = True           # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
    visualization_quality: str = "high"
    enable_caching: bool = True
    cache_size: int = 100
    show_masks: bool = True
    show_boundaries: bool = True
    overlay_opacity: float = 0.6
    clip_threshold: float = 0.5             # CLIP ê¸°ë°˜ ì„ê³„ê°’
    esrgan_scale: int = 2                   # Real-ESRGAN ìŠ¤ì¼€ì¼

@dataclass
class SegmentationResult:
    """AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼"""
    success: bool
    mask: Optional[np.ndarray] = None
    segmented_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    quality_score: float = 0.0
    method_used: str = "unknown"
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    # AI ê¸°ë°˜ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
    visualization_image: Optional[Image.Image] = None
    overlay_image: Optional[Image.Image] = None
    mask_image: Optional[Image.Image] = None
    boundary_image: Optional[Image.Image] = None
    ai_enhanced_image: Optional[Image.Image] = None  # Real-ESRGAN í–¥ìƒ

# ==============================================
# ğŸ”¥ 5. ì˜ë¥˜ë³„ ìƒ‰ìƒ ë§¤í•‘ (AI ê¸°ë°˜ ì‹œê°í™”ìš©)
# ==============================================

CLOTHING_COLORS = {
    'shirt': (255, 100, 100),      # ë¹¨ê°•
    'pants': (100, 100, 255),      # íŒŒë‘
    'dress': (255, 100, 255),      # ë¶„í™
    'jacket': (100, 255, 100),     # ì´ˆë¡
    'skirt': (255, 255, 100),      # ë…¸ë‘
    'sweater': (138, 43, 226),     # ë¸”ë£¨ë°”ì´ì˜¬ë ›
    'coat': (165, 42, 42),         # ê°ˆìƒ‰
    'top': (0, 255, 255),          # ì‹œì•ˆ
    'bottom': (255, 165, 0),       # ì˜¤ë Œì§€
    'shoes': (255, 150, 0),        # ì£¼í™©
    'bag': (150, 75, 0),           # ê°ˆìƒ‰
    'hat': (128, 0, 128),          # ë³´ë¼
    'accessory': (0, 255, 255),    # ì‹œì•ˆ
    'unknown': (128, 128, 128),    # íšŒìƒ‰
}

# ==============================================
# ğŸ”¥ 6. AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class REBNCONV(nn.Module):
    """U2-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡"""
    
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class RSU7(nn.Module):
    """U2-Net RSU-7 ë¸”ë¡ (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìµœì í™”)"""
    
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample6 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample5 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')
        
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
    
    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = self.upsample6(hx6d)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = self.upsample5(hx5d)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = self.upsample4(hx4d)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = self.upsample3(hx3d)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = self.upsample2(hx2d)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class U2NET(nn.Module):
    """U2-Net ë©”ì¸ ëª¨ë¸ (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìµœì í™”) - OpenCV ëŒ€ì²´"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super(U2NET, self).__init__()
        
        # ì¸ì½”ë”
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU7(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU7(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU7(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU7(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU7(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU7(1024, 256, 512)
        self.stage4d = RSU7(1024, 128, 256)
        self.stage3d = RSU7(512, 64, 128)
        self.stage2d = RSU7(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # Side outputs
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx5d = self.stage5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # Side outputs
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=d1.shape[2:], mode='bilinear', align_corners=False)
        
        # ìµœì¢… ì¶œë ¥
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)

class AIImageProcessor(nn.Module):
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ëŒ€ì²´)"""
    
    def __init__(self, device="cpu"):
        super(AIImageProcessor, self).__init__()
        self.device = device
        
        # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
        self.edge_detector = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 3, padding=1),
            nn.Sigmoid()
        )
        
        # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
        self.denoiser = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 3, padding=1),
            nn.Sigmoid()
        )
    
    def detect_edges_ai(self, mask: torch.Tensor) -> torch.Tensor:
        """AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ (OpenCV Canny ëŒ€ì²´)"""
        try:
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)
            elif mask.dim() == 3:
                mask = mask.unsqueeze(0)
            
            edges = self.edge_detector(mask.to(self.device).float())
            return edges.squeeze()
        except Exception as e:
            logger.warning(f"AI ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê²€ì¶œ
            return self._gradient_edge_detection(mask.squeeze())
    
    def remove_noise_ai(self, mask: torch.Tensor) -> torch.Tensor:
        """AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±° (OpenCV morphology ëŒ€ì²´)"""
        try:
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)
            elif mask.dim() == 3:
                mask = mask.unsqueeze(0)
            
            denoised = self.denoiser(mask.to(self.device).float())
            return denoised.squeeze()
        except Exception as e:
            logger.warning(f"AI ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            return self._gaussian_denoise(mask.squeeze())
    
    def _gradient_edge_detection(self, mask: torch.Tensor) -> torch.Tensor:
        """ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ (í´ë°±)"""
        try:
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)
            
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)
            
            grad_x = F.conv2d(mask.float(), sobel_x, padding=1)
            grad_y = F.conv2d(mask.float(), sobel_y, padding=1)
            
            edges = torch.sqrt(grad_x**2 + grad_y**2)
            return (edges > 0.1).float().squeeze()
        except Exception:
            return mask.squeeze()
    
    def _gaussian_denoise(self, mask: torch.Tensor) -> torch.Tensor:
        """ê°€ìš°ì‹œì•ˆ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±° (í´ë°±)"""
        try:
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)
            
            gaussian_kernel = torch.tensor([
                [1, 2, 1],
                [2, 4, 2],
                [1, 2, 1]
            ], dtype=torch.float32).view(1, 1, 3, 3) / 16.0
            
            blurred = F.conv2d(mask.float(), gaussian_kernel, padding=1)
            return (blurred > 0.5).float().squeeze()
        except Exception:
            return mask.squeeze()

# ==============================================
# ğŸ”¥ 7. ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤ (BaseStepMixin v16.0 í˜¸í™˜)
# ==============================================

class ClothSegmentationStep:
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ + AI ëª¨ë¸ ì™„ì „ ì—°ë™
    
    âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ì„± ë³´ì¥
    âœ… UnifiedDependencyManager ì™„ì „ í™œìš©
    âœ… TYPE_CHECKING íŒ¨í„´ ì™„ì „ ì ìš©
    âœ… OpenCV ì™„ì „ ì œê±° ë° AI ëª¨ë¸ë¡œ ëŒ€ì²´
    âœ… ì‹¤ì œ AI ì¶”ë¡  (U2Net, SAM, RemBG, CLIP, Real-ESRGAN)
    âœ… M3 Max ìµœì í™”
    âœ… conda í™˜ê²½ ì§€ì›
    âœ… Python êµ¬ì¡° ì™„ì „ ì •ë¦¬
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], SegmentationConfig]] = None,
        **kwargs
    ):
        """ìƒì„±ì - BaseStepMixin v16.0 í˜¸í™˜ íŒ¨í„´"""
        
        # ===== 1. BaseStepMixin v16.0 í˜¸í™˜ ê¸°ë³¸ ì†ì„± =====
        self.step_name = kwargs.get('step_name', "ClothSegmentationStep")
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.step_id = kwargs.get('step_id', 3)
        self.device = device or self._auto_detect_device()
        
        # ===== 2. BaseStepMixin v16.0 í˜¸í™˜ ìƒíƒœ ë³€ìˆ˜ =====
        self.is_initialized = False
        self.has_model = False
        self.model_loaded = False
        self.dependencies_injected = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False,
            'step_interface': False
        }
        
        # ===== 3. Logger ì„¤ì • =====
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        
        # ===== 4. ì˜ì¡´ì„± ì£¼ì… ë³€ìˆ˜ (BaseStepMixin v16.0 í˜¸í™˜) =====
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_interface = None
        self.model_interface = None
        
        # ===== 5. ì„¤ì • ì²˜ë¦¬ =====
        if isinstance(config, dict):
            self.segmentation_config = SegmentationConfig(**config)
        elif isinstance(config, SegmentationConfig):
            self.segmentation_config = config
        else:
            self.segmentation_config = SegmentationConfig()
        
        # ===== 6. AI ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜ =====
        self.models_loaded = {}
        self.checkpoints_loaded = {}
        self.available_methods = []
        self.rembg_sessions = {}
        self.sam_predictors = {}
        self.clip_processor = None
        self.clip_model = None
        self.ai_image_processor = None
        self.esrgan_model = None
        
        # ===== 7. M3 Max ê°ì§€ ë° ìµœì í™” =====
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = kwargs.get('memory_gb', 128.0 if self.is_m3_max else 16.0)
        
        # ===== 8. í†µê³„ ë° ìºì‹œ ì´ˆê¸°í™” =====
        self.processing_stats = {
            'total_processed': 0,
            'successful_segmentations': 0,
            'failed_segmentations': 0,
            'average_time': 0.0,
            'average_quality': 0.0,
            'method_usage': {},
            'cache_hits': 0,
            'ai_model_calls': 0
        }
        
        self.segmentation_cache = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(
            max_workers=4 if self.is_m3_max else 2,
            thread_name_prefix="cloth_seg_ai"
        )
        
        self.logger.info("âœ… ClothSegmentationStep ìƒì„± ì™„ë£Œ (BaseStepMixin v16.0 í˜¸í™˜)")
        self.logger.info(f"   - Device: {self.device}")
        self.logger.info(f"   - AI ëª¨ë¸ ìš°ì„  ì‚¬ìš© (OpenCV ì™„ì „ ëŒ€ì²´)")

    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except Exception:
            return "cpu"

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True
                )
                cpu_info = result.stdout.strip()
                return 'M3 Max' in cpu_info or 'M3' in cpu_info
        except Exception:
            pass
        return False

    # ==============================================
    # ğŸ”¥ 8. BaseStepMixin v16.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            self.dependencies_injected['model_loader'] = True
            
            # Step Interface ìƒì„± ì‹œë„
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.step_interface = model_loader.create_step_interface(self.step_name)
                    self.model_interface = self.step_interface
                    self.dependencies_injected['step_interface'] = True
                    self.logger.info("âœ… Step Interface ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
                    self.step_interface = model_loader
                    self.model_interface = model_loader
            else:
                self.step_interface = model_loader
                self.model_interface = model_loader
            
            self.has_model = True
            self.model_loaded = True
            
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ (BaseStepMixin v16.0 í˜¸í™˜)")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.dependencies_injected['model_loader'] = False
            return False

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            self.dependencies_injected['memory_manager'] = False
            return False

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            self.dependencies_injected['data_converter'] = False
            return False

    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.di_container = di_container
            self.dependencies_injected['di_container'] = True
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            self.dependencies_injected['di_container'] = False
            return False

    # ==============================================
    # ğŸ”¥ 9. BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ë©”ì„œë“œ
    # ==============================================
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” - BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©"""
        try:
            if self.is_initialized:
                return True
                
            self.logger.info("ğŸ”„ ClothSegmentationStep ì´ˆê¸°í™” ì‹œì‘ (BaseStepMixin v16.0 í˜¸í™˜)")
            
            # ===== 1. ë™ì  ì˜ì¡´ì„± í•´ê²° (TYPE_CHECKING íŒ¨í„´) =====
            if not self._resolve_dependencies():
                self.logger.warning("âš ï¸ ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨, í´ë°± ëª¨ë“œë¡œ ì§„í–‰")
            
            # ===== 2. AI ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” =====
            self.ai_image_processor = AIImageProcessor(self.device)
            
            # ===== 3. ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© =====
            if not await self._load_checkpoints_via_model_loader():
                self.logger.warning("âš ï¸ ModelLoader ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ===== 4. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ ìƒì„± =====
            if not await self._create_ai_models_from_checkpoints():
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            
            # ===== 5. RemBG ì„¸ì…˜ ì´ˆê¸°í™” =====
            if REMBG_AVAILABLE:
                await self._initialize_rembg_sessions()
            
            # ===== 6. SAM ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” =====
            if SAM_AVAILABLE:
                await self._initialize_sam_predictors()
            
            # ===== 7. CLIP ëª¨ë¸ ì´ˆê¸°í™” =====
            if TRANSFORMERS_AVAILABLE:
                await self._initialize_clip_models()
            
            # ===== 8. Real-ESRGAN ëª¨ë¸ ì´ˆê¸°í™” =====
            if ESRGAN_AVAILABLE:
                await self._initialize_esrgan_model()
            
            # ===== 9. M3 Max ìµœì í™” ì›Œë°ì—… =====
            if self.is_m3_max:
                await self._warmup_m3_max()
            
            # ===== 10. ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²• ê°ì§€ =====
            self.available_methods = self._detect_available_ai_methods()
            if not self.available_methods:
                self.logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤")
                self.available_methods = [SegmentationMethod.AUTO_AI]
            
            # ===== 11. ì´ˆê¸°í™” ì™„ë£Œ =====
            self.is_initialized = True
            self.logger.info("âœ… ClothSegmentationStep ì´ˆê¸°í™” ì™„ë£Œ (BaseStepMixin v16.0 í˜¸í™˜)")
            self.logger.info(f"   - ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸: {list(self.checkpoints_loaded.keys())}")
            self.logger.info(f"   - ìƒì„±ëœ AI ëª¨ë¸: {list(self.models_loaded.keys())}")
            self.logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•: {[m.value for m in self.available_methods]}")
            self.logger.info(f"   - OpenCV ì™„ì „ ëŒ€ì²´: AI ëª¨ë¸ ìš°ì„  ì‚¬ìš©")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False

    def _resolve_dependencies(self):
        """ë™ì  importë¡œ ì˜ì¡´ì„± í•´ê²° (TYPE_CHECKING íŒ¨í„´)"""
        try:
            # ModelLoader ë™ì  ë¡œë”©
            if not self.model_loader:
                model_loader = get_model_loader()
                if model_loader:
                    self.set_model_loader(model_loader)
                    self.logger.info("âœ… ModelLoader ë™ì  ë¡œë”© ì„±ê³µ")
            
            # DI Container ë™ì  ë¡œë”©
            if not self.di_container:
                di_container = get_di_container()
                if di_container:
                    self.set_di_container(di_container)
                    self.logger.info("âœ… DI Container ë™ì  ë¡œë”© ì„±ê³µ")
                    
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
            return False

    async def _load_checkpoints_via_model_loader(self) -> bool:
        """ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if not self.model_loader:
                return False
            
            self.logger.info("ğŸ”„ ModelLoaderë¥¼ í†µí•œ AI ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘...")
            
            # ===== U2-Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© =====
            try:
                self.logger.info("ğŸ”„ U2-Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
                
                if hasattr(self.model_loader, 'load_model_async'):
                    u2net_checkpoint = await self.model_loader.load_model_async("cloth_segmentation_u2net")
                elif hasattr(self.model_loader, 'load_model'):
                    u2net_checkpoint = self.model_loader.load_model("cloth_segmentation_u2net")
                else:
                    u2net_checkpoint = None
                
                if u2net_checkpoint:
                    self.checkpoints_loaded['u2net'] = u2net_checkpoint
                    self.logger.info("âœ… U2-Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ U2-Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© =====
            try:
                self.logger.info("ğŸ”„ SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
                
                if hasattr(self.model_loader, 'load_model_async'):
                    sam_checkpoint = await self.model_loader.load_model_async("cloth_segmentation_sam")
                elif hasattr(self.model_loader, 'load_model'):
                    sam_checkpoint = self.model_loader.load_model("cloth_segmentation_sam")
                else:
                    sam_checkpoint = None
                
                if sam_checkpoint:
                    self.checkpoints_loaded['sam'] = sam_checkpoint
                    self.logger.info("âœ… SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            return len(self.checkpoints_loaded) > 0
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    async def _create_ai_models_from_checkpoints(self) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ìƒì„± ë¶ˆê°€")
                return False
            
            self.logger.info("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ ìƒì„± ì‹œì‘...")
            
            # ===== U2-Net ëª¨ë¸ ìƒì„± =====
            if 'u2net' in self.checkpoints_loaded:
                try:
                    self.logger.info("ğŸ”„ U2-Net AI ëª¨ë¸ ìƒì„± ì¤‘...")
                    
                    # U2-Net ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    u2net_model = U2NET(in_ch=3, out_ch=1)
                    
                    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                    checkpoint = self.checkpoints_loaded['u2net']
                    if isinstance(checkpoint, dict):
                        if 'model' in checkpoint:
                            u2net_model.load_state_dict(checkpoint['model'])
                        elif 'state_dict' in checkpoint:
                            u2net_model.load_state_dict(checkpoint['state_dict'])
                        else:
                            u2net_model.load_state_dict(checkpoint)
                    elif hasattr(checkpoint, 'state_dict'):
                        u2net_model.load_state_dict(checkpoint.state_dict())
                    else:
                        u2net_model.load_state_dict(checkpoint)
                    
                    # ë””ë°”ì´ìŠ¤ ì´ë™ ë° í‰ê°€ ëª¨ë“œ
                    u2net_model = u2net_model.to(self.device)
                    u2net_model.eval()
                    
                    self.models_loaded['u2net'] = u2net_model
                    self.logger.info("âœ… U2-Net AI ëª¨ë¸ ìƒì„± ë° ë¡œë”© ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2-Net AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
            return len(self.models_loaded) > 0
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    async def _initialize_rembg_sessions(self):
        """RemBG ì„¸ì…˜ ì´ˆê¸°í™” (OpenCV ë°°ê²½ ì œê±° ëŒ€ì²´)"""
        try:
            if not REMBG_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ RemBG AI ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘...")
            
            session_configs = {
                'u2net': 'u2net',
                'u2netp': 'u2netp', 
                'silueta': 'silueta',
                'cloth': 'isnet-general-use'
            }
            
            for name, model_name in session_configs.items():
                try:
                    session = new_session(model_name)
                    self.rembg_sessions[name] = session
                    self.logger.info(f"âœ… RemBG AI ì„¸ì…˜ ìƒì„±: {name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ {name} ìƒì„± ì‹¤íŒ¨: {e}")
            
            if self.rembg_sessions:
                self.default_rembg_session = (
                    self.rembg_sessions.get('cloth') or
                    self.rembg_sessions.get('u2net') or
                    list(self.rembg_sessions.values())[0]
                )
                self.logger.info("âœ… RemBG AI ê¸°ë³¸ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ RemBG AI ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _initialize_sam_predictors(self):
        """SAM ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” (OpenCV ì„¸ê·¸ë©˜í…Œì´ì…˜ ëŒ€ì²´)"""
        try:
            if not SAM_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ SAM AI ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            # SAM ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°
            if 'sam' in self.checkpoints_loaded:
                try:
                    checkpoint_path = self.checkpoints_loaded['sam']
                    
                    # SAM ëª¨ë¸ ìƒì„±
                    sam_model = sam_model_registry["vit_h"](checkpoint=checkpoint_path)
                    sam_model.to(device=self.device)
                    
                    # SAM ì˜ˆì¸¡ê¸° ìƒì„±
                    sam_predictor = SamPredictor(sam_model)
                    self.sam_predictors['default'] = sam_predictor
                    
                    self.logger.info("âœ… SAM AI ì˜ˆì¸¡ê¸° ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ SAM AI ì˜ˆì¸¡ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ SAM AI ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _initialize_clip_models(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™” (OpenCV íŠ¹ì§• ì¶”ì¶œ ëŒ€ì²´)"""
        try:
            if not TRANSFORMERS_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ CLIP AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            try:
                model_name = "openai/clip-vit-base-patch32"
                self.clip_processor = CLIPProcessor.from_pretrained(model_name)
                self.clip_model = CLIPModel.from_pretrained(model_name)
                self.clip_model.to(self.device)
                self.clip_model.eval()
                
                self.logger.info("âœ… CLIP AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ CLIP AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ CLIP AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _initialize_esrgan_model(self):
        """Real-ESRGAN ëª¨ë¸ ì´ˆê¸°í™” (OpenCV ì´ë¯¸ì§€ í–¥ìƒ ëŒ€ì²´)"""
        try:
            if not ESRGAN_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ Real-ESRGAN AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            try:
                # Real-ESRGAN ëª¨ë¸ ìƒì„±
                model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ModelLoaderë¥¼ í†µí•´)
                if hasattr(self.model_loader, 'load_model'):
                    try:
                        esrgan_checkpoint = self.model_loader.load_model("esrgan_model")
                        if esrgan_checkpoint:
                            model.load_state_dict(esrgan_checkpoint)
                            model.to(self.device)
                            model.eval()
                            self.esrgan_model = model
                            self.logger.info("âœ… Real-ESRGAN AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Real-ESRGAN ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Real-ESRGAN AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Real-ESRGAN AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _warmup_m3_max(self):
        """M3 Max ì›Œë°ì—…"""
        try:
            if not self.is_m3_max or not TORCH_AVAILABLE:
                return
            
            self.logger.info("ğŸ”¥ M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ í…ì„œë¡œ ì›Œë°ì—…
            dummy_input = torch.randn(1, 3, 512, 512, device=self.device)
            
            for model_name, model in self.models_loaded.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                        with torch.no_grad():
                            if hasattr(model, 'forward'):
                                _ = model(dummy_input)
                            elif callable(model):
                                _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} M3 Max ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # AI ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì›Œë°ì—…
            if self.ai_image_processor:
                try:
                    dummy_mask = torch.randn(1, 1, 512, 512, device=self.device)
                    with torch.no_grad():
                        self.ai_image_processor.detect_edges_ai(dummy_mask)
                        self.ai_image_processor.remove_noise_ai(dummy_mask)
                    self.logger.info("âœ… AI Image Processor M3 Max ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI Image Processor ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # MPS ìºì‹œ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    def _detect_available_ai_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€ (OpenCV ì œì™¸)"""
        methods = []
        
        # ë¡œë“œëœ AI ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ë°©ë²• ê²°ì •
        if 'u2net' in self.models_loaded:
            methods.append(SegmentationMethod.U2NET)
            self.logger.info("âœ… U2NET AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        if 'sam' in self.sam_predictors:
            methods.append(SegmentationMethod.SAM)
            self.logger.info("âœ… SAM AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # RemBG í™•ì¸
        if REMBG_AVAILABLE and self.rembg_sessions:
            methods.append(SegmentationMethod.REMBG)
            self.logger.info("âœ… RemBG AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # CLIP ê¸°ë°˜ ë°©ë²•
        if self.clip_model and self.clip_processor:
            methods.append(SegmentationMethod.CLIP_GUIDED)
            self.logger.info("âœ… CLIP Guided AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # Real-ESRGAN í–¥ìƒëœ ë°©ë²•
        if self.esrgan_model:
            methods.append(SegmentationMethod.ESRGAN_ENHANCED)
            self.logger.info("âœ… Real-ESRGAN Enhanced AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # AUTO AI ë°©ë²• (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
        ai_methods = [m for m in methods]
        if ai_methods:
            methods.append(SegmentationMethod.AUTO_AI)
            self.logger.info("âœ… AUTO AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # HYBRID AI ë°©ë²• (2ê°œ ì´ìƒ AI ë°©ë²•ì´ ìˆì„ ë•Œ)
        if len(ai_methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
            self.logger.info("âœ… HYBRID AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        return methods

    # ==============================================
    # ğŸ”¥ 10. í•µì‹¬: process ë©”ì„œë“œ (ì‹¤ì œ AI ì¶”ë¡ ) - BaseStepMixin v16.0 í˜¸í™˜
    # ==============================================
    
    async def process(
        self,
        image,
        clothing_type: Optional[str] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ì¶”ë¡ """
        
        if not self.is_initialized:
            if not await self.initialize():
                return self._create_error_result("ì´ˆê¸°í™” ì‹¤íŒ¨")

        start_time = time.time()
        
        try:
            self.logger.info("ğŸ”„ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹œì‘ (BaseStepMixin v16.0 í˜¸í™˜)")
            
            # ===== 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (AI ê¸°ë°˜) =====
            processed_image = self._preprocess_image_ai(image)
            if processed_image is None:
                return self._create_error_result("AI ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ===== 2. ì˜ë¥˜ íƒ€ì… ê°ì§€ (CLIP ê¸°ë°˜) =====
            detected_clothing_type = await self._detect_clothing_type_ai(processed_image, clothing_type)
            
            # ===== 3. í’ˆì§ˆ ë ˆë²¨ ì„¤ì • =====
            quality = QualityLevel(quality_level or self.segmentation_config.quality_level.value)
            
            # ===== 4. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ =====
            self.logger.info("ğŸ”„ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
            mask, confidence = await self._run_ai_segmentation(
                processed_image, detected_clothing_type, quality
            )
            
            if mask is None:
                return self._create_error_result("AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨")
            
            # ===== 5. AI ê¸°ë°˜ í›„ì²˜ë¦¬ =====
            final_mask = await self._post_process_mask_ai(mask, quality)
            
            # ===== 6. AI ê¸°ë°˜ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± =====
            visualizations = {}
            if self.segmentation_config.enable_visualization:
                self.logger.info("ğŸ”„ AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±...")
                visualizations = await self._create_ai_visualizations(
                    processed_image, final_mask, detected_clothing_type
                )
            
            # ===== 7. ê²°ê³¼ ìƒì„± =====
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'mask': final_mask,
                'confidence': confidence,
                'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                'processing_time': processing_time,
                'method_used': self._get_current_ai_method(),
                'ai_models_used': list(self.models_loaded.keys()) if hasattr(self, 'models_loaded') else [],
                'metadata': {
                    'device': self.device,
                    'quality_level': quality.value,
                    'models_used': list(self.models_loaded.keys()) if hasattr(self, 'models_loaded') else [],
                    'checkpoints_loaded': list(self.checkpoints_loaded.keys()) if hasattr(self, 'checkpoints_loaded') else [],
                    'image_size': processed_image.size if hasattr(processed_image, 'size') else (512, 512),
                    'ai_inference': True,
                    'opencv_replaced': True,
                    'model_loader_used': self.model_loader is not None,
                    'is_m3_max': self.is_m3_max,
                    'basestepmixin_v16_compatible': True,
                    'dependency_injection_status': self.dependencies_injected.copy()
                }
            }
            
            # AI ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            if visualizations:
                if 'visualization' in visualizations:
                    result['visualization_base64'] = self._image_to_base64(visualizations['visualization'])
                if 'overlay' in visualizations:
                    result['overlay_base64'] = self._image_to_base64(visualizations['overlay'])
                if 'ai_enhanced' in visualizations:
                    result['ai_enhanced_base64'] = self._image_to_base64(visualizations['ai_enhanced'])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(processing_time, True)
            
            self.logger.info(f"âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ (BaseStepMixin v16.0) - {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_processing_stats(processing_time, False)
            
            self.logger.error(f"âŒ AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"AI ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    # ==============================================
    # ğŸ”¥ 11. AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ë©”ì„œë“œë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
    # ==============================================

    def _preprocess_image_ai(self, image):
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenCV ëŒ€ì²´)"""
        try:
            # ì…ë ¥ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, str):
                if image.startswith('data:image'):
                    # Base64
                    header, data = image.split(',', 1)
                    image_data = base64.b64decode(data)
                    image = Image.open(BytesIO(image_data))
                else:
                    # íŒŒì¼ ê²½ë¡œ
                    image = Image.open(image)
            elif isinstance(image, np.ndarray):
                if image.shape[2] == 3:  # RGB
                    image = Image.fromarray(image)
                elif image.shape[2] == 4:  # RGBA
                    image = Image.fromarray(image).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image.shape}")
            elif not isinstance(image, Image.Image):
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # AI ê¸°ë°˜ í¬ê¸° ì¡°ì • (PILì˜ ê³ ê¸‰ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©)
            target_size = self.segmentation_config.input_size
            if image.size != target_size:
                # Lanczos ë¦¬ìƒ˜í”Œë§ (AI í’ˆì§ˆì— ê°€ê¹Œìš´ ê³ í’ˆì§ˆ)
                image = image.resize(target_size, Image.Resampling.LANCZOS)
            
            # Real-ESRGAN ê¸°ë°˜ í–¥ìƒ (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.esrgan_model and self.segmentation_config.quality_level in [QualityLevel.HIGH, QualityLevel.ULTRA]:
                try:
                    image = self._enhance_image_esrgan(image)
                except Exception as e:
                    self.logger.debug(f"Real-ESRGAN í–¥ìƒ ì‹¤íŒ¨: {e}")
            
            return image
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None

    def _enhance_image_esrgan(self, image: Image.Image) -> Image.Image:
        """Real-ESRGAN ê¸°ë°˜ ì´ë¯¸ì§€ í–¥ìƒ (OpenCV ëŒ€ì²´)"""
        try:
            if not self.esrgan_model or not TORCH_AVAILABLE:
                return image
            
            # PIL to Tensor
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # Real-ESRGAN ì¶”ë¡ 
            with torch.no_grad():
                enhanced_tensor = self.esrgan_model(input_tensor)
                enhanced_tensor = torch.clamp(enhanced_tensor, 0, 1)
            
            # Tensor to PIL
            to_pil = transforms.ToPILImage()
            enhanced_image = to_pil(enhanced_tensor.squeeze().cpu())
            
            return enhanced_image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Real-ESRGAN í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image

    async def _detect_clothing_type_ai(self, image, hint=None):
        """CLIP ê¸°ë°˜ ì˜ë¥˜ íƒ€ì… ê°ì§€ (OpenCV íŠ¹ì§• ì¶”ì¶œ ëŒ€ì²´)"""
        try:
            if hint:
                try:
                    return ClothingType(hint.lower())
                except ValueError:
                    pass
            
            # CLIP ê¸°ë°˜ ì˜ë¥˜ íƒ€ì… ë¶„ë¥˜
            if self.clip_model and self.clip_processor:
                try:
                    # ì˜ë¥˜ íƒ€ì… í›„ë³´ë“¤
                    clothing_candidates = [
                        "a shirt", "a dress", "pants", "a skirt", "a jacket", 
                        "a sweater", "a coat", "a top", "bottom clothing"
                    ]
                    
                    # CLIP ê¸°ë°˜ ë¶„ë¥˜
                    inputs = self.clip_processor(
                        text=clothing_candidates,
                        images=image,
                        return_tensors="pt",
                        padding=True
                    )
                    
                    # GPUë¡œ ì´ë™
                    for key in inputs:
                        if hasattr(inputs[key], 'to'):
                            inputs[key] = inputs[key].to(self.device)
                    
                    with torch.no_grad():
                        outputs = self.clip_model(**inputs)
                        logits_per_image = outputs.logits_per_image
                        probs = logits_per_image.softmax(dim=-1)
                    
                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì˜ë¥˜ íƒ€ì… ì„ íƒ
                    predicted_idx = probs.argmax().item()
                    predicted_text = clothing_candidates[predicted_idx]
                    confidence = probs.max().item()
                    
                    if confidence > self.segmentation_config.clip_threshold:
                        # í…ìŠ¤íŠ¸ë¥¼ ClothingTypeìœ¼ë¡œ ë§¤í•‘
                        type_mapping = {
                            "a shirt": ClothingType.SHIRT,
                            "a dress": ClothingType.DRESS,
                            "pants": ClothingType.PANTS,
                            "a skirt": ClothingType.SKIRT,
                            "a jacket": ClothingType.JACKET,
                            "a sweater": ClothingType.SWEATER,
                            "a coat": ClothingType.COAT,
                            "a top": ClothingType.TOP,
                            "bottom clothing": ClothingType.BOTTOM
                        }
                        
                        detected_type = type_mapping.get(predicted_text, ClothingType.UNKNOWN)
                        self.logger.info(f"âœ… CLIP AI ì˜ë¥˜ íƒ€ì… ê°ì§€: {detected_type.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return detected_type
                
                except Exception as e:
                    self.logger.warning(f"âš ï¸ CLIP ì˜ë¥˜ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì´ë¯¸ì§€ ë¹„ìœ¨ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
            if hasattr(image, 'size'):
                width, height = image.size
                aspect_ratio = height / width
                
                if aspect_ratio > 1.5:
                    return ClothingType.DRESS
                elif aspect_ratio > 1.2:
                    return ClothingType.SHIRT
                else:
                    return ClothingType.PANTS
            
            return ClothingType.UNKNOWN
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì˜ë¥˜ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return ClothingType.UNKNOWN

    async def _run_ai_segmentation(
        self,
        image: Image.Image,
        clothing_type: ClothingType,
        quality: QualityLevel
    ) -> Tuple[Optional[np.ndarray], float]:
        """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  (OpenCV ì™„ì „ ëŒ€ì²´)"""
        try:
            # ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ AI ë°©ë²• ì‹œë„
            methods_to_try = self._get_ai_methods_by_priority(quality)
            
            for method in methods_to_try:
                try:
                    self.logger.info(f"ğŸ§  AI ë°©ë²• ì‹œë„: {method.value}")
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    
                    if mask is not None:
                        self.processing_stats['ai_model_calls'] += 1
                        self.processing_stats['method_usage'][method.value] = (
                            self.processing_stats['method_usage'].get(method.value, 0) + 1
                        )
                        
                        self.logger.info(f"âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {method.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return mask, confidence
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ë°©ë²• {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨ ì‹œ í´ë°±
            self.logger.warning("âš ï¸ ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨, ê¸°ë³¸ AI ë°©ë²• ì‹œë„")
            return await self._run_fallback_ai_segmentation(image)
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None, 0.0

    def _get_ai_methods_by_priority(self, quality: QualityLevel) -> List[SegmentationMethod]:
        """í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ìš°ì„ ìˆœìœ„ (OpenCV ì œì™¸)"""
        available_ai_methods = [
            method for method in self.available_methods
        ]
        
        if quality == QualityLevel.ULTRA:
            priority = [
                SegmentationMethod.HYBRID_AI,
                SegmentationMethod.ESRGAN_ENHANCED,
                SegmentationMethod.SAM,
                SegmentationMethod.U2NET,
                SegmentationMethod.CLIP_GUIDED,
                SegmentationMethod.REMBG
            ]
        elif quality == QualityLevel.HIGH:
            priority = [
                SegmentationMethod.U2NET,
                SegmentationMethod.SAM,
                SegmentationMethod.HYBRID_AI,
                SegmentationMethod.CLIP_GUIDED,
                SegmentationMethod.REMBG
            ]
        elif quality == QualityLevel.BALANCED:
            priority = [
                SegmentationMethod.REMBG,
                SegmentationMethod.U2NET,
                SegmentationMethod.CLIP_GUIDED
            ]
        else:  # FAST
            priority = [
                SegmentationMethod.REMBG,
                SegmentationMethod.U2NET
            ]
        
        return [method for method in priority if method in available_ai_methods]

    async def _run_ai_method(
        self,
        method: SegmentationMethod,
        image: Image.Image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ê°œë³„ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤í–‰"""
        
        if method == SegmentationMethod.U2NET:
            return await self._run_u2net_inference(image)
        elif method == SegmentationMethod.REMBG:
            return await self._run_rembg_inference(image)
        elif method == SegmentationMethod.SAM:
            return await self._run_sam_inference(image)
        elif method == SegmentationMethod.CLIP_GUIDED:
            return await self._run_clip_guided_inference(image, clothing_type)
        elif method == SegmentationMethod.HYBRID_AI:
            return await self._run_hybrid_ai_inference(image, clothing_type)
        elif method == SegmentationMethod.ESRGAN_ENHANCED:
            return await self._run_esrgan_enhanced_inference(image)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ë°©ë²•: {method}")

    async def _run_u2net_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """U2-Net ì‹¤ì œ AI ì¶”ë¡ """
        try:
            if 'u2net' not in self.models_loaded:
                raise RuntimeError("âŒ U2-Net ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.models_loaded['u2net']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if self.is_m3_max and self.segmentation_config.use_fp16:
                    with torch.autocast(device_type='cpu'):
                        output = model(input_tensor)
                else:
                    output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, tuple):
                    output = output[0]
                elif isinstance(output, list):
                    output = output[0]
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if output.max() > 1.0:
                    prob_map = torch.sigmoid(output)
                else:
                    prob_map = output
                
                mask = (prob_map > self.segmentation_config.confidence_threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy()
                confidence = float(prob_map.max().item())
            
            self.logger.info(f"âœ… U2-Net AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ U2-Net AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_rembg_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """RemBG AI ì¶”ë¡ """
        try:
            if not self.rembg_sessions:
                raise RuntimeError("âŒ RemBG ì„¸ì…˜ì´ ì—†ìŒ")
            
            # ìµœì  ì„¸ì…˜ ì„ íƒ
            session = (
                self.rembg_sessions.get('cloth') or
                self.rembg_sessions.get('u2net') or
                list(self.rembg_sessions.values())[0]
            )
            
            # ğŸ”¥ ì‹¤ì œ RemBG AI ì¶”ë¡ 
            result = remove(image, session=session)
            
            # ì•ŒíŒŒ ì±„ë„ì—ì„œ ë§ˆìŠ¤í¬ ì¶”ì¶œ
            if result.mode == 'RGBA':
                mask = np.array(result)[:, :, 3]  # ì•ŒíŒŒ ì±„ë„
                mask = (mask > 128).astype(np.uint8)  # ì´ì§„í™”
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = np.sum(mask) / mask.size
                confidence = min(confidence * 2, 1.0)  # ì •ê·œí™”
                
                self.logger.info(f"âœ… RemBG AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
                return mask, confidence
            else:
                raise RuntimeError("âŒ RemBG ê²°ê³¼ì— ì•ŒíŒŒ ì±„ë„ì´ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ RemBG AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_sam_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """SAM AI ì¶”ë¡  (OpenCV ì„¸ê·¸ë©˜í…Œì´ì…˜ ëŒ€ì²´)"""
        try:
            if not self.sam_predictors:
                raise RuntimeError("âŒ SAM ì˜ˆì¸¡ê¸°ê°€ ì—†ìŒ")
            
            predictor = self.sam_predictors['default']
            
            # ì´ë¯¸ì§€ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
            
            # SAM ì´ë¯¸ì§€ ì„¤ì •
            predictor.set_image(image_array)
            
            # ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ìë™ ë§ˆìŠ¤í¬ ìƒì„±
            # ì¤‘ì•™ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡
            h, w = image_array.shape[:2]
            input_point = np.array([[w//2, h//2]])
            input_label = np.array([1])
            
            # ğŸ”¥ ì‹¤ì œ SAM AI ì¶”ë¡ 
            masks, scores, logits = predictor.predict(
                point_coords=input_point,
                point_labels=input_label,
                multimask_output=True
            )
            
            # ê°€ì¥ ì¢‹ì€ ë§ˆìŠ¤í¬ ì„ íƒ
            best_mask_idx = np.argmax(scores)
            best_mask = masks[best_mask_idx]
            confidence = float(scores[best_mask_idx])
            
            self.logger.info(f"âœ… SAM AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return best_mask.astype(np.uint8), confidence
            
        except Exception as e:
            self.logger.error(f"âŒ SAM AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_clip_guided_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """CLIP ê¸°ë°˜ ì§€ëŠ¥ì  ì„¸ê·¸ë©˜í…Œì´ì…˜ (OpenCV íŠ¹ì§• ê¸°ë°˜ ëŒ€ì²´)"""
        try:
            if not self.clip_model or not self.clip_processor:
                raise RuntimeError("âŒ CLIP ëª¨ë¸ì´ ì—†ìŒ")
            
            # CLIP ê¸°ë°˜ìœ¼ë¡œ ì˜ë¥˜ ì˜ì—­ ì˜ˆì¸¡
            clothing_descriptions = [
                f"a {clothing_type.value}",
                f"{clothing_type.value} clothing",
                f"person wearing {clothing_type.value}",
                "background", "person", "face", "skin"
            ]
            
            # CLIP ì…ë ¥ ì¤€ë¹„
            inputs = self.clip_processor(
                text=clothing_descriptions,
                images=image,
                return_tensors="pt",
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            for key in inputs:
                if hasattr(inputs[key], 'to'):
                    inputs[key] = inputs[key].to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ CLIP AI ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.clip_model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=-1)
            
            # ì˜ë¥˜ ê´€ë ¨ í™•ë¥  í•©ê³„
            clothing_prob = probs[0][:3].sum().item()  # ì²˜ìŒ 3ê°œëŠ” ì˜ë¥˜ ê´€ë ¨
            
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë°©ë²• ì‚¬ìš©)
            if clothing_prob > self.segmentation_config.clip_threshold:
                # ì¤‘ì•™ ì˜ì—­ì„ ì˜ë¥˜ë¡œ ê°€ì •í•˜ëŠ” ê°„ë‹¨í•œ ë§ˆìŠ¤í¬
                h, w = self.segmentation_config.input_size
                mask = np.zeros((h, w), dtype=np.uint8)
                
                # ì´ë¯¸ì§€ ì¤‘ì•™ 60% ì˜ì—­ì„ ì˜ë¥˜ë¡œ ì„¤ì •
                start_h, end_h = int(h * 0.2), int(h * 0.8)
                start_w, end_w = int(w * 0.2), int(w * 0.8)
                mask[start_h:end_h, start_w:end_w] = 1
                
                confidence = float(clothing_prob)
                
                self.logger.info(f"âœ… CLIP Guided AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
                return mask, confidence
            else:
                raise RuntimeError("âŒ CLIPì—ì„œ ì˜ë¥˜ë¥¼ ê°ì§€í•˜ì§€ ëª»í•¨")
                
        except Exception as e:
            self.logger.error(f"âŒ CLIP Guided AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_hybrid_ai_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """HYBRID AI ì¶”ë¡  (ì—¬ëŸ¬ AI ëª¨ë¸ ê²°í•©) - OpenCV ì—†ìŒ"""
        try:
            self.logger.info("ğŸ”„ HYBRID AI ì¶”ë¡  ì‹œì‘...")
            
            masks = []
            confidences = []
            methods_used = []
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•ë“¤ë¡œ ì¶”ë¡  ì‹¤í–‰
            available_ai_methods = [
                method for method in self.available_methods 
                if method not in [SegmentationMethod.AUTO_AI, SegmentationMethod.HYBRID_AI]
            ]
            
            # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°©ë²•ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if len(available_ai_methods) < 2:
                raise RuntimeError("âŒ HYBRID ë°©ë²•ì€ ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ë°©ë²•ì´ í•„ìš”")
            
            for method in available_ai_methods[:3]:  # ìµœëŒ€ 3ê°œ ë°©ë²• ì‚¬ìš©
                try:
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    if mask is not None:
                        masks.append(mask)
                        confidences.append(confidence)
                        methods_used.append(method.value)
                        self.logger.info(f"âœ… HYBRID - {method.value} ì¶”ë¡  ì™„ë£Œ: {confidence:.3f}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HYBRID - {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            if not masks:
                raise RuntimeError("âŒ HYBRID - ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨")
            
            # ë§ˆìŠ¤í¬ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· ) - PyTorch ê¸°ë°˜ (OpenCV ëŒ€ì²´)
            if len(masks) == 1:
                combined_mask = masks[0]
                combined_confidence = confidences[0]
            else:
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
                weights = np.array(confidences)
                weights = weights / np.sum(weights)  # ì •ê·œí™”
                
                # ë§ˆìŠ¤í¬ë“¤ì„ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤ (PyTorch ê¸°ë°˜)
                target_shape = masks[0].shape
                normalized_masks = []
                for mask in masks:
                    if mask.shape != target_shape:
                        # PyTorch ê¸°ë°˜ ë¦¬ì‚¬ì´ì§• (OpenCV ëŒ€ì²´)
                        mask_tensor = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0).unsqueeze(0)
                        resized_tensor = F.interpolate(
                            mask_tensor, 
                            size=target_shape, 
                            mode='bilinear', 
                            align_corners=False
                        )
                        mask_resized = resized_tensor.squeeze().numpy()
                        normalized_masks.append(mask_resized)
                    else:
                        normalized_masks.append(mask.astype(np.float32))
                
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                combined_mask_float = np.zeros_like(normalized_masks[0])
                for mask, weight in zip(normalized_masks, weights):
                    combined_mask_float += mask * weight
                
                # ì„ê³„ê°’ ì ìš©
                combined_mask = (combined_mask_float > 0.5).astype(np.uint8)
                combined_confidence = float(np.mean(confidences))
            
            self.logger.info(f"âœ… HYBRID AI ì¶”ë¡  ì™„ë£Œ - ë°©ë²•: {methods_used} - ì‹ ë¢°ë„: {combined_confidence:.3f}")
            return combined_mask, combined_confidence
            
        except Exception as e:
            self.logger.error(f"âŒ HYBRID AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_esrgan_enhanced_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """Real-ESRGAN í–¥ìƒëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ (OpenCV ì´ë¯¸ì§€ í–¥ìƒ ëŒ€ì²´)"""
        try:
            if not self.esrgan_model:
                raise RuntimeError("âŒ Real-ESRGAN ëª¨ë¸ì´ ì—†ìŒ")
            
            # 1. Real-ESRGANìœ¼ë¡œ ì´ë¯¸ì§€ í–¥ìƒ
            enhanced_image = self._enhance_image_esrgan(image)
            
            # 2. í–¥ìƒëœ ì´ë¯¸ì§€ë¡œ U2Net ì¶”ë¡ 
            if 'u2net' in self.models_loaded:
                mask, confidence = await self._run_u2net_inference(enhanced_image)
                # í–¥ìƒëœ ì¶”ë¡ ì´ë¯€ë¡œ ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                enhanced_confidence = min(confidence * 1.1, 1.0)
                
                self.logger.info(f"âœ… Real-ESRGAN Enhanced AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {enhanced_confidence:.3f}")
                return mask, enhanced_confidence
            else:
                # U2Netì´ ì—†ìœ¼ë©´ RemBG ì‚¬ìš©
                return await self._run_rembg_inference(enhanced_image)
                
        except Exception as e:
            self.logger.error(f"âŒ Real-ESRGAN Enhanced AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_fallback_ai_segmentation(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """í´ë°± AI ì„¸ê·¸ë©˜í…Œì´ì…˜ (ìˆœìˆ˜ PyTorch ê¸°ë°˜, OpenCV ì—†ìŒ)"""
        try:
            if not TORCH_AVAILABLE or not PIL_AVAILABLE:
                return None, 0.0
            
            self.logger.info("ğŸ”„ í´ë°± AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
            
            image_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ê°„ë‹¨í•œ AI ê¸°ë°˜ ì„ê³„ê°’ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ + ì„ê³„ê°’)
            # RGBë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ê°€ì¤‘ í‰ê· )
            gray_tensor = 0.299 * image_tensor[:, 0:1, :, :] + \
                         0.587 * image_tensor[:, 1:2, :, :] + \
                         0.114 * image_tensor[:, 2:3, :, :]
            
            # Otsu ë°©ë²• ìœ ì‚¬í•œ ìë™ ì„ê³„ê°’ ê³„ì‚° (PyTorch ê¸°ë°˜)
            hist = torch.histc(gray_tensor, bins=256, min=0, max=1)
            
            # ê°„ë‹¨í•œ ì„ê³„ê°’ (ì¤‘ê°„ê°’ ì‚¬ìš©)
            threshold = 0.5
            
            # ì„ê³„ê°’ ì ìš©
            mask_tensor = (gray_tensor > threshold).float()
            
            # í˜•íƒœí•™ì  ì—°ì‚° (PyTorch ê¸°ë°˜) - OpenCV ëŒ€ì²´
            # ê°„ë‹¨í•œ ì¹¨ì‹ê³¼ í™•ì¥
            kernel_size = 5
            kernel = torch.ones(1, 1, kernel_size, kernel_size, device=self.device) / (kernel_size * kernel_size)
            
            # ì¹¨ì‹ (ìµœì†Ÿê°’ í’€ë§ ê·¼ì‚¬)
            eroded = F.conv2d(mask_tensor, kernel, padding=kernel_size//2)
            eroded = (eroded > 0.7).float()
            
            # í™•ì¥ (ìµœëŒ“ê°’ í’€ë§ ê·¼ì‚¬)
            dilated = F.conv2d(eroded, kernel, padding=kernel_size//2)
            dilated = (dilated > 0.3).float()
            
            # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
            mask_np = dilated.squeeze().cpu().numpy().astype(np.uint8)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë§ˆìŠ¤í¬ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜)
            confidence = float(np.sum(mask_np) / mask_np.size)
            confidence = min(confidence * 2, 0.6)  # í´ë°± ë°©ë²•ì´ë¯€ë¡œ ë‚®ì€ ì‹ ë¢°ë„
            
            self.logger.info(f"âœ… í´ë°± AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return None, 0.0

    # ==============================================
    # ğŸ”¥ 12. AI ê¸°ë°˜ í›„ì²˜ë¦¬ ë©”ì„œë“œë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
    # ==============================================
    
    async def _post_process_mask_ai(self, mask, quality):
        """AI ê¸°ë°˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE or not NUMPY_AVAILABLE:
                return mask
            
            processed_mask = mask.copy()
            
            # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            if self.segmentation_config.ai_noise_removal and self.ai_image_processor:
                try:
                    mask_tensor = torch.from_numpy(processed_mask.astype(np.float32))
                    denoised_tensor = self.ai_image_processor.remove_noise_ai(mask_tensor)
                    processed_mask = (denoised_tensor.cpu().numpy() > 0.5).astype(np.uint8)
                except Exception as e:
                    self.logger.debug(f"AI ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            
            # AI ê¸°ë°˜ ì—£ì§€ ìŠ¤ë¬´ë”©
            if self.segmentation_config.ai_edge_smoothing:
                try:
                    # PyTorch ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (OpenCV ëŒ€ì²´)
                    processed_mask = self._gaussian_smooth_ai(processed_mask)
                except Exception as e:
                    self.logger.debug(f"AI ì—£ì§€ ìŠ¤ë¬´ë”© ì‹¤íŒ¨: {e}")
            
            # í™€ ì±„ìš°ê¸° (AI ê¸°ë°˜)
            if self.segmentation_config.enable_hole_filling:
                processed_mask = self._fill_holes_ai(processed_mask)
            
            # ê²½ê³„ ê°œì„  (AI ê¸°ë°˜)
            if self.segmentation_config.enable_edge_refinement:
                processed_mask = self._refine_edges_ai(processed_mask)
            
            return processed_mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask

    def _gaussian_smooth_ai(self, mask: np.ndarray) -> np.ndarray:
        """PyTorch ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ ìŠ¤ë¬´ë”© (OpenCV GaussianBlur ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return mask
            
            # NumPy to Tensor
            mask_tensor = torch.from_numpy(mask.astype(np.float32))
            if mask_tensor.dim() == 2:
                mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„± (OpenCV ëŒ€ì²´)
            kernel_size = 5
            sigma = 1.0
            
            # 1D ê°€ìš°ì‹œì•ˆ ì»¤ë„
            x = torch.arange(kernel_size) - kernel_size // 2
            gaussian_1d = torch.exp(-0.5 * (x / sigma) ** 2)
            gaussian_1d = gaussian_1d / gaussian_1d.sum()
            
            # 2D ê°€ìš°ì‹œì•ˆ ì»¤ë„
            gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)
            gaussian_2d = gaussian_2d.unsqueeze(0).unsqueeze(0)
            
            # ì»¨ë³¼ë£¨ì…˜ ì ìš©
            blurred = F.conv2d(mask_tensor, gaussian_2d, padding=kernel_size//2)
            
            # ì„ê³„ê°’ ì ìš© ë° ë³€í™˜
            smoothed = (blurred > 0.5).float()
            smoothed_np = smoothed.squeeze().numpy().astype(np.uint8)
            
            return smoothed_np
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ê°€ìš°ì‹œì•ˆ ìŠ¤ë¬´ë”© ì‹¤íŒ¨: {e}")
            return mask

    def _fill_holes_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ í™€ ì±„ìš°ê¸° (OpenCV findContours ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return mask
            
            # PyTorch ê¸°ë°˜ í˜•íƒœí•™ì  ë‹«ê¸° ì—°ì‚° (OpenCV ëŒ€ì²´)
            mask_tensor = torch.from_numpy(mask.astype(np.float32))
            if mask_tensor.dim() == 2:
                mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)
            
            # êµ¬ì¡° ìš”ì†Œ (ì›í˜• ì»¤ë„)
            kernel_size = 7
            kernel = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)
            
            # í™•ì¥ (Dilation)
            dilated = F.conv2d(mask_tensor, kernel, padding=kernel_size//2)
            dilated = (dilated > 0.3).float()
            
            # ì¹¨ì‹ (Erosion)
            eroded = F.conv2d(dilated, kernel, padding=kernel_size//2)
            eroded = (eroded > 0.7).float()
            
            # Tensor to NumPy
            filled_np = eroded.squeeze().numpy().astype(np.uint8)
            
            return filled_np
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask

    def _refine_edges_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ê²½ê³„ ê°œì„  (OpenCV Canny ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE or not self.ai_image_processor:
                return mask
            
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
            mask_tensor = torch.from_numpy(mask.astype(np.float32))
            edges_tensor = self.ai_image_processor.detect_edges_ai(mask_tensor)
            
            # ì—£ì§€ ì£¼ë³€ ì˜ì—­ì— ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
            if edges_tensor is not None:
                # ì—£ì§€ ì˜ì—­ í™•ì¥
                edges_expanded = F.max_pool2d(
                    edges_tensor.unsqueeze(0).unsqueeze(0),
                    kernel_size=5,
                    stride=1,
                    padding=2
                ).squeeze()
                
                # ì›ë³¸ ë§ˆìŠ¤í¬ì— ë¸”ëŸ¬ ì ìš©
                blurred_mask = self._gaussian_smooth_ai(mask)
                
                # ì—£ì§€ ì˜ì—­ë§Œ ë¸”ëŸ¬ëœ ê°’ìœ¼ë¡œ êµì²´
                edges_np = edges_expanded.cpu().numpy() > 0.1
                refined_mask = mask.copy().astype(np.float32)
                refined_mask[edges_np] = blurred_mask[edges_np]
                
                return (refined_mask > 0.5).astype(np.uint8)
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ê²½ê³„ ê°œì„  ì‹¤íŒ¨: {e}")
            return mask

    # ==============================================
    # ğŸ”¥ 13. AI ê¸°ë°˜ ì‹œê°í™” ë©”ì„œë“œë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
    # ==============================================

    async def _create_ai_visualizations(self, image, mask, clothing_type):
        """AI ê¸°ë°˜ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (OpenCV ì™„ì „ ëŒ€ì²´)"""
        try:
            if not PIL_AVAILABLE or not NUMPY_AVAILABLE:
                return {}
            
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            color = CLOTHING_COLORS.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type),
                CLOTHING_COLORS['unknown']
            )
            
            # 1. AI ê¸°ë°˜ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ìƒ‰ìƒ êµ¬ë¶„)
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            visualizations['mask'] = Image.fromarray(mask_colored)
            
            # 2. AI ê¸°ë°˜ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # 3. AI ê¸°ë°˜ ê²½ê³„ì„  ì¶”ê°€ (OpenCV Canny ëŒ€ì²´)
            if self.ai_image_processor:
                try:
                    mask_tensor = torch.from_numpy(mask.astype(np.float32))
                    boundary_tensor = self.ai_image_processor.detect_edges_ai(mask_tensor)
                    boundary_np = (boundary_tensor.cpu().numpy() > 0.1).astype(np.uint8)
                    overlay[boundary_np > 0] = (255, 255, 255)
                except Exception as e:
                    self.logger.debug(f"AI ê²½ê³„ì„  ìƒì„± ì‹¤íŒ¨: {e}")
            
            visualizations['overlay'] = Image.fromarray(overlay)
            
            # 4. AI ê¸°ë°˜ ê²½ê³„ì„  ì´ë¯¸ì§€ (OpenCV Canny ëŒ€ì²´)
            if self.ai_image_processor:
                try:
                    mask_tensor = torch.from_numpy(mask.astype(np.float32))
                    boundary_tensor = self.ai_image_processor.detect_edges_ai(mask_tensor)
                    boundary_np = (boundary_tensor.cpu().numpy() > 0.1).astype(np.uint8)
                    
                    boundary_colored = np.zeros((*boundary_np.shape, 3), dtype=np.uint8)
                    boundary_colored[boundary_np > 0] = (255, 255, 255)
                    
                    boundary_overlay = image_array.copy()
                    boundary_overlay[boundary_np > 0] = (255, 255, 255)
                    visualizations['boundary'] = Image.fromarray(boundary_overlay)
                except Exception as e:
                    self.logger.debug(f"AI ê²½ê³„ì„  ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 5. Real-ESRGAN í–¥ìƒëœ ì´ë¯¸ì§€ (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.esrgan_model:
                try:
                    enhanced_image = self._enhance_image_esrgan(image)
                    visualizations['ai_enhanced'] = enhanced_image
                except Exception as e:
                    self.logger.debug(f"Real-ESRGAN í–¥ìƒ ì‹¤íŒ¨: {e}")
            
            # 6. ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€
            visualization = await self._create_comprehensive_ai_visualization(
                image, mask, clothing_type, color
            )
            visualizations['visualization'] = visualization
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    async def _create_comprehensive_ai_visualization(self, image, mask, clothing_type, color):
        """ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (OpenCV ì—†ìŒ)"""
        try:
            if not PIL_AVAILABLE:
                return image
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            width, height = image.size
            canvas_width = width * 2 + 20
            canvas_height = height + 80
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), (240, 240, 240))
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(image, (10, 30))
            
            # AI ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê¸°ë°˜ ê²½ê³„ì„  ì¶”ê°€
            if self.ai_image_processor:
                try:
                    mask_tensor = torch.from_numpy(mask.astype(np.float32))
                    boundary_tensor = self.ai_image_processor.detect_edges_ai(mask_tensor)
                    boundary_np = (boundary_tensor.cpu().numpy() > 0.1).astype(np.uint8)
                    overlay[boundary_np > 0] = (255, 255, 255)
                except Exception as e:
                    self.logger.debug(f"AI ê²½ê³„ì„  ì¶”ê°€ ì‹¤íŒ¨: {e}")
            
            overlay_image = Image.fromarray(overlay)
            canvas.paste(overlay_image, (width + 20, 30))
            
            # AI ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
            try:
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(canvas)
                
                try:
                    font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 14)
                except Exception:
                    try:
                        font = ImageFont.load_default()
                    except Exception:
                        font = None
                
                if font:
                    # ì œëª©
                    draw.text((10, 5), "Original", fill=(0, 0, 0), font=font)
                    clothing_type_str = clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type)
                    draw.text((width + 20, 5), f"AI Segmented ({clothing_type_str})",
                             fill=(0, 0, 0), font=font)
                    
                    # AI í†µê³„ ì •ë³´
                    mask_area = np.sum(mask)
                    total_area = mask.size
                    coverage = (mask_area / total_area) * 100
                    
                    # AI ëª¨ë¸ ì •ë³´
                    ai_models_used = ', '.join(self.models_loaded.keys()) if self.models_loaded else 'None'
                    ai_methods_available = len(self.available_methods)
                    
                    info_lines = [
                        f"Coverage: {coverage:.1f}% | AI Models: {len(self.models_loaded)}",
                        f"Methods: {ai_methods_available} | Device: {self.device}",
                        f"Models: {ai_models_used[:30]}{'...' if len(ai_models_used) > 30 else ''}",
                        f"BaseStepMixin v16.0 | OpenCV Replaced: AI"
                    ]
                    
                    for i, info_text in enumerate(info_lines):
                        draw.text((10, height + 35 + i * 15), info_text, fill=(0, 0, 0), font=font)
                
            except ImportError:
                pass  # PIL ImageDraw/ImageFont ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¢…í•© AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return image

    # ==============================================
    # ğŸ”¥ 14. BaseStepMixin v16.0 í˜¸í™˜ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================

    def get_status(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'step_type': self.step_type,
            'is_initialized': self.is_initialized,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'device': self.device,
            'dependencies_injected': self.dependencies_injected.copy(),
            'basestepmixin_v16_compatible': True,
            'opencv_replaced': True,
            'ai_models_loaded': list(self.models_loaded.keys()),
            'ai_methods_available': [m.value for m in self.available_methods]
        }

    def get_performance_summary(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ì„±ëŠ¥ ìš”ì•½"""
        return {
            'processing_stats': self.processing_stats.copy(),
            'cache_status': {
                'enabled': self.segmentation_config.enable_caching,
                'size': len(self.segmentation_cache),
                'hits': self.processing_stats['cache_hits']
            },
            'ai_model_performance': {
                'models_loaded': len(self.models_loaded),
                'total_ai_calls': self.processing_stats['ai_model_calls'],
                'methods_usage': self.processing_stats['method_usage'].copy()
            },
            'memory_optimization': {
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'device': self.device
            }
        }

    def _get_current_ai_method(self):
        """í˜„ì¬ ì‚¬ìš©ëœ AI ë°©ë²• ë°˜í™˜"""
        if self.models_loaded.get('u2net'):
            return 'u2net_ai_basestepmixin_v16'
        elif self.sam_predictors:
            return 'sam_ai'
        elif self.rembg_sessions:
            return 'rembg_ai'
        elif self.clip_model:
            return 'clip_guided_ai'
        else:
            return 'fallback_ai'

    def _image_to_base64(self, image):
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        try:
            if not PIL_AVAILABLE:
                return ""
            
            buffer = BytesIO()
            if isinstance(image, Image.Image):
                image.save(buffer, format='PNG')
            else:
                img = Image.fromarray(image)
                img.save(buffer, format='PNG')
            image_data = buffer.getvalue()
            return base64.b64encode(image_data).decode()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Base64 ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)"""
        return {
            'success': False,
            'error': error_message,
            'mask': None,
            'confidence': 0.0,
            'processing_time': 0.0,
            'method_used': 'error',
            'ai_models_used': [],
            'metadata': {
                'error_details': error_message,
                'available_ai_models': list(self.models_loaded.keys()),
                'basestepmixin_v16_compatible': True,
                'opencv_replaced': True,
                'dependencies_status': self.dependencies_injected.copy()
            }
        }

    def _update_processing_stats(self, processing_time: float, success: bool):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_processed'] += 1
            if success:
                self.processing_stats['successful_segmentations'] += 1
            else:
                self.processing_stats['failed_segmentations'] += 1
            
            # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.processing_stats['total_processed']
            current_avg = self.processing_stats['average_time']
            self.processing_stats['average_time'] = (
                (current_avg * (total - 1) + processing_time) / total
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ 15. BaseStepMixin v16.0 í˜¸í™˜ ê³ ê¸‰ ê¸°ëŠ¥ ë©”ì„œë“œë“¤
    # ==============================================

    async def process_batch(
        self,
        images: List[Union[str, np.ndarray, Image.Image]],
        clothing_types: Optional[List[str]] = None,
        quality_level: Optional[str] = None,
        batch_size: Optional[int] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        try:
            if not images:
                return []
            
            batch_size = batch_size or self.segmentation_config.batch_size
            clothing_types = clothing_types or [None] * len(images)
            
            # ë°°ì¹˜ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ AI ì²˜ë¦¬
            results = []
            for i in range(0, len(images), batch_size):
                batch_images = images[i:i+batch_size]
                batch_clothing_types = clothing_types[i:i+batch_size]
                
                # ë°°ì¹˜ ë‚´ ë³‘ë ¬ AI ì²˜ë¦¬
                batch_tasks = []
                for j, (image, clothing_type) in enumerate(zip(batch_images, batch_clothing_types)):
                    task = self.process(
                        image=image,
                        clothing_type=clothing_type,
                        quality_level=quality_level,
                        **kwargs
                    )
                    batch_tasks.append(task)
                
                # ë°°ì¹˜ ì‹¤í–‰
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for result in batch_results:
                    if isinstance(result, Exception):
                        results.append(self._create_error_result(f"ë°°ì¹˜ AI ì²˜ë¦¬ ì˜¤ë¥˜: {str(result)}"))
                    else:
                        results.append(result)
            
            self.logger.info(f"âœ… AI ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì´ë¯¸ì§€")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ AI ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return [self._create_error_result(f"AI ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}") for _ in images]

    async def process_with_cache(self, image, **kwargs) -> Dict[str, Any]:
        """ìºì‹±ì„ ì‚¬ìš©í•œ AI ì²˜ë¦¬ - BaseStepMixin v16.0 í˜¸í™˜"""
        try:
            if not self.segmentation_config.enable_caching:
                return await self.process(image, **kwargs)
            
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._generate_cache_key(image, **kwargs)
            
            # ìºì‹œ í™•ì¸
            with self.cache_lock:
                if cache_key in self.segmentation_cache:
                    cached_result = self.segmentation_cache[cache_key]
                    self.processing_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ AI ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜: {cache_key[:10]}...")
                    return cached_result
            
            # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ AI ì²˜ë¦¬
            result = await self.process(image, **kwargs)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìºì‹œ
            if result.get('success', False):
                with self.cache_lock:
                    # ìºì‹œ í¬ê¸° ì œí•œ
                    if len(self.segmentation_cache) >= self.segmentation_config.cache_size:
                        # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ë‹¨ìˆœ FIFO)
                        oldest_key = next(iter(self.segmentation_cache))
                        del self.segmentation_cache[oldest_key]
                    
                    self.segmentation_cache[cache_key] = result
                    self.logger.debug(f"ğŸ’¾ AI ê²°ê³¼ ìºì‹œ ì €ì¥: {cache_key[:10]}...")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ìºì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return await self.process(image, **kwargs)

    def _generate_cache_key(self, image, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ í•´ì‹œ
            if isinstance(image, str):
                image_hash = hashlib.md5(image.encode()).hexdigest()[:8]
            elif isinstance(image, np.ndarray):
                image_hash = hashlib.md5(image.tobytes()).hexdigest()[:8]
            elif isinstance(image, Image.Image):
                import io
                buffer = io.BytesIO()
                image.save(buffer, format='PNG')
                image_hash = hashlib.md5(buffer.getvalue()).hexdigest()[:8]
            else:
                image_hash = "unknown"
            
            # íŒŒë¼ë¯¸í„° í•´ì‹œ
            params = {
                'clothing_type': kwargs.get('clothing_type'),
                'quality_level': kwargs.get('quality_level'),
                'method': self.segmentation_config.method.value,
                'confidence_threshold': self.segmentation_config.confidence_threshold,
                'ai_models': list(self.models_loaded.keys())
            }
            params_str = json.dumps(params, sort_keys=True)
            params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
            
            return f"ai_{image_hash}_{params_hash}"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ai_fallback_{time.time()}"

    def calculate_quality_score(self, mask: np.ndarray, original_image: Image.Image) -> float:
        """AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (OpenCV ëŒ€ì²´)"""
        try:
            if mask is None or not NUMPY_AVAILABLE:
                return 0.0
            
            # 1. ë§ˆìŠ¤í¬ ì™„ì „ì„± (ì „ì²´ ëŒ€ë¹„ ë§ˆìŠ¤í¬ ë¹„ìœ¨)
            mask_coverage = np.sum(mask) / mask.size
            coverage_score = min(mask_coverage * 2, 1.0)  # 0~1 ì •ê·œí™”
            
            # 2. AI ê¸°ë°˜ ê²½ê³„ í’ˆì§ˆ (OpenCV Canny ëŒ€ì²´)
            if self.ai_image_processor:
                try:
                    mask_tensor = torch.from_numpy(mask.astype(np.float32))
                    edges_tensor = self.ai_image_processor.detect_edges_ai(mask_tensor)
                    edge_density = torch.sum(edges_tensor > 0.1).item() / edges_tensor.numel()
                    edge_score = 1.0 - min(edge_density * 10, 1.0)  # ê²½ê³„ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
                except Exception:
                    edge_score = 0.8
            else:
                edge_score = 0.8
            
            # 3. PyTorch ê¸°ë°˜ ì—°ê²°ì„± ì ìˆ˜ (OpenCV connectedComponents ëŒ€ì²´)
            try:
                # ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì—°ê²°ì„± ì¸¡ì •
                mask_tensor = torch.from_numpy(mask.astype(np.float32))
                if mask_tensor.dim() == 2:
                    mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)
                
                # í’€ë§ì„ í†µí•œ ì—°ê²° ì˜ì—­ ì¶”ì •
                pooled = F.avg_pool2d(mask_tensor, kernel_size=8, stride=8)
                active_regions = torch.sum(pooled > 0.1).item()
                connectivity_score = 1.0 / max(active_regions, 1)  # ì˜ì—­ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            except Exception:
                connectivity_score = 0.8
            
            # 4. AI ê¸°ë°˜ í˜•íƒœ ì ìˆ˜ (CLIP í™œìš© ê°€ëŠ¥ì‹œ)
            if np.sum(mask) > 0:
                # ë§ˆìŠ¤í¬ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                y_indices, x_indices = np.where(mask > 0)
                if len(y_indices) > 0 and len(x_indices) > 0:
                    height = np.max(y_indices) - np.min(y_indices)
                    width = np.max(x_indices) - np.min(x_indices)
                    aspect_ratio = height / max(width, 1)
                    # ì˜ë¥˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì„¸ë¡œê°€ ë” ê¸´ í˜•íƒœ
                    shape_score = 1.0 - abs(aspect_ratio - 1.5) / 1.5
                    shape_score = max(0.2, shape_score)
                else:
                    shape_score = 0.2
            else:
                shape_score = 0.0
            
            # 5. AI ëª¨ë¸ ê¸°ë°˜ ë³´ë„ˆìŠ¤ ì ìˆ˜
            ai_bonus = 0.0
            if self.models_loaded:
                ai_bonus = min(len(self.models_loaded) * 0.05, 0.2)  # AI ëª¨ë¸ ìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            quality_score = (
                coverage_score * 0.35 +
                edge_score * 0.25 +
                connectivity_score * 0.2 +
                shape_score * 0.1 +
                ai_bonus * 0.1
            )
            
            return max(0.0, min(1.0, quality_score))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def get_segmentation_info(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ë³´ ë°˜í™˜ - BaseStepMixin v16.0 í˜¸í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'device': self.device,
            'is_initialized': self.is_initialized,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'available_ai_methods': [m.value for m in self.available_methods],
            'loaded_ai_models': list(self.models_loaded.keys()),
            'loaded_checkpoints': list(self.checkpoints_loaded.keys()),
            'rembg_sessions': list(self.rembg_sessions.keys()) if hasattr(self, 'rembg_sessions') else [],
            'sam_predictors': list(self.sam_predictors.keys()) if hasattr(self, 'sam_predictors') else [],
            'processing_stats': self.processing_stats.copy(),
            'basestepmixin_v16_info': {
                'compatible': True,
                'dependency_injection_status': self.dependencies_injected.copy(),
                'opencv_replaced': True,
                'ai_models_priority': True,
                'model_loader_connected': self.model_loader is not None,
                'step_interface_connected': self.step_interface is not None
            },
            'ai_model_stats': {
                'total_ai_calls': self.processing_stats['ai_model_calls'],
                'models_loaded': len(self.models_loaded),
                'checkpoints_loaded': len(self.checkpoints_loaded),
                'ai_processor_available': self.ai_image_processor is not None,
                'clip_available': self.clip_model is not None,
                'esrgan_available': self.esrgan_model is not None
            },
            'config': {
                'method': self.segmentation_config.method.value,
                'quality_level': self.segmentation_config.quality_level.value,
                'enable_visualization': self.segmentation_config.enable_visualization,
                'confidence_threshold': self.segmentation_config.confidence_threshold,
                'ai_edge_smoothing': self.segmentation_config.ai_edge_smoothing,
                'ai_noise_removal': self.segmentation_config.ai_noise_removal,
                'overlay_opacity': self.segmentation_config.overlay_opacity,
                'clip_threshold': self.segmentation_config.clip_threshold,
                'esrgan_scale': self.segmentation_config.esrgan_scale
            }
        }

    # ==============================================
    # ğŸ”¥ 16. BaseStepMixin v16.0 í˜¸í™˜ ì •ë¦¬ ë©”ì„œë“œ
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - BaseStepMixin v16.0 í˜¸í™˜"""
        try:
            self.logger.info("ğŸ§¹ ClothSegmentationStep ì •ë¦¬ ì‹œì‘ (BaseStepMixin v16.0)...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.models_loaded.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.models_loaded.clear()
            self.checkpoints_loaded.clear()
            
            # AI í”„ë¡œì„¸ì„œ ì •ë¦¬
            if hasattr(self, 'ai_image_processor'):
                self.ai_image_processor = None
            
            # CLIP ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'clip_model') and self.clip_model:
                try:
                    if hasattr(self.clip_model, 'cpu'):
                        self.clip_model.cpu()
                    del self.clip_model
                    del self.clip_processor
                except Exception as e:
                    self.logger.warning(f"âš ï¸ CLIP ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # Real-ESRGAN ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'esrgan_model') and self.esrgan_model:
                try:
                    if hasattr(self.esrgan_model, 'cpu'):
                        self.esrgan_model.cpu()
                    del self.esrgan_model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Real-ESRGAN ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # RemBG ì„¸ì…˜ ì •ë¦¬
            if hasattr(self, 'rembg_sessions'):
                self.rembg_sessions.clear()
            
            # SAM ì˜ˆì¸¡ê¸° ì •ë¦¬
            if hasattr(self, 'sam_predictors'):
                for name, predictor in self.sam_predictors.items():
                    try:
                        if hasattr(predictor, 'model') and hasattr(predictor.model, 'cpu'):
                            predictor.model.cpu()
                        del predictor
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ SAM ì˜ˆì¸¡ê¸° {name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
                self.sam_predictors.clear()
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'segmentation_cache'):
                self.segmentation_cache.clear()
            
            # ì‹¤í–‰ì ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # BaseStepMixin v16.0 í˜¸í™˜ ìƒíƒœ ë¦¬ì…‹
            self.is_initialized = False
            self.has_model = False
            self.model_loaded = False
            self.dependencies_injected = {key: False for key in self.dependencies_injected}
            
            # ì˜ì¡´ì„± ì°¸ì¡° ì •ë¦¬
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            self.step_interface = None
            self.model_interface = None
            
            self.logger.info("âœ… ClothSegmentationStep ì •ë¦¬ ì™„ë£Œ (BaseStepMixin v16.0)")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

    # ==============================================
    # ğŸ”¥ 17. BaseStepMixin v16.0 í˜¸í™˜ ë³„ì¹­ ë©”ì„œë“œë“¤
    # ==============================================

    async def segment_clothing(self, image, **kwargs):
        """ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        return await self.process(image, **kwargs)
    
    async def segment_clothing_batch(self, images, **kwargs):
        """ë°°ì¹˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í˜¸í™˜ì„± ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        return await self.process_batch(images, **kwargs)
    
    async def segment_clothing_with_cache(self, image, **kwargs):
        """ìºì‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ í˜¸í™˜ì„± ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        return await self.process_with_cache(image, **kwargs)

    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        try:
            if not self.is_initialized:
                return {"success": False, "error": "Step not initialized"}
            
            # AI ëª¨ë¸ ì›Œë°ì—…
            dummy_result = {"success": True, "models_warmed": []}
            
            if TORCH_AVAILABLE and self.models_loaded:
                dummy_input = torch.randn(1, 3, 256, 256, device=self.device)
                
                for model_name, model in self.models_loaded.items():
                    try:
                        with torch.no_grad():
                            if hasattr(model, 'forward'):
                                _ = model(dummy_input)
                                dummy_result["models_warmed"].append(model_name)
                    except Exception as e:
                        self.logger.debug(f"ì›Œë°ì—… ì‹¤íŒ¨ {model_name}: {e}")
            
            return dummy_result
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def warmup_async(self) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—… ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜"""
        return self.warmup()

# ==============================================
# ğŸ”¥ 18. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜)
# ==============================================

def create_cloth_segmentation_step(
    device: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜ (BaseStepMixin v16.0 í˜¸í™˜)"""
    return ClothSegmentationStep(device=device, config=config, **kwargs)

async def create_and_initialize_cloth_segmentation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """BaseStepMixin v16.0 í˜¸í™˜ ClothSegmentationStep ìƒì„± ë° ì´ˆê¸°í™”"""
    try:
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        
        # ë™ì  ì˜ì¡´ì„± ì£¼ì… ì‹œë„
        try:
            model_loader = get_model_loader()
            if model_loader:
                step.set_model_loader(model_loader)
            
            di_container = get_di_container()
            if di_container:
                step.set_di_container(di_container)
        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì  ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        await step.initialize()
        return step
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ìƒì„±
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        await step.initialize()
        return step

def create_m3_max_segmentation_step(
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)"""
    m3_config = {
        'method': SegmentationMethod.AUTO_AI,
        'quality_level': QualityLevel.HIGH,
        'use_fp16': True,
        'batch_size': 8,  # M3 Max 128GB í™œìš©
        'cache_size': 200,
        'enable_visualization': True,
        'visualization_quality': 'high',
        'ai_edge_smoothing': True,
        'ai_noise_removal': True,
        'clip_threshold': 0.6,
        'esrgan_scale': 2
    }
    
    if config:
        m3_config.update(config)
    
    return ClothSegmentationStep(device="mps", config=m3_config, **kwargs)

# ==============================================
# ğŸ”¥ 19. í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ í•¨ìˆ˜ë“¤
# ==============================================

async def test_basestepmixin_v16_ai_segmentation():
    """BaseStepMixin v16.0 í˜¸í™˜ + AI ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª BaseStepMixin v16.0 í˜¸í™˜ + AI ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)
        step = await create_and_initialize_cloth_segmentation_step(
            device="auto",
            config={
                "method": "auto_ai",
                "enable_visualization": True,
                "visualization_quality": "high",
                "quality_level": "balanced",
                "ai_edge_smoothing": True,
                "ai_noise_removal": True
            }
        )
        
        # BaseStepMixin v16.0 í˜¸í™˜ì„± í™•ì¸
        status = step.get_status()
        print("ğŸ”— BaseStepMixin v16.0 í˜¸í™˜ì„± ìƒíƒœ:")
        print(f"   âœ… v16.0 í˜¸í™˜: {status['basestepmixin_v16_compatible']}")
        print(f"   âœ… OpenCV ëŒ€ì²´: {status['opencv_replaced']}")
        print(f"   âœ… ì˜ì¡´ì„± ì£¼ì…: {status['dependencies_injected']}")
        print(f"   âœ… AI ëª¨ë¸ ë¡œë“œ: {status['ai_models_loaded']}")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        if PIL_AVAILABLE:
            dummy_image = Image.new('RGB', (512, 512), (200, 150, 100))
        else:
            dummy_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # AI ì²˜ë¦¬ ì‹¤í–‰
        result = await step.process(dummy_image, clothing_type="shirt", quality_level="high")
        
        # ê²°ê³¼ í™•ì¸
        if result['success']:
            print("âœ… BaseStepMixin v16.0 + AI ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   - ì˜ë¥˜ íƒ€ì…: {result['clothing_type']}")
            print(f"   - ì‹ ë¢°ë„: {result['confidence']:.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            print(f"   - ì‚¬ìš© AI ëª¨ë¸: {result['ai_models_used']}")
            print(f"   - OpenCV ëŒ€ì²´ë¨: {result['metadata']['opencv_replaced']}")
            print(f"   - BaseStepMixin v16.0: {result['metadata']['basestepmixin_v16_compatible']}")
            
            if 'visualization_base64' in result:
                print("   - AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±ë¨")
            if 'ai_enhanced_base64' in result:
                print("   - Real-ESRGAN í–¥ìƒ ì´ë¯¸ì§€ ìƒì„±ë¨")
        else:
            print(f"âŒ BaseStepMixin v16.0 + AI ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        info = step.get_segmentation_info()
        print(f"\nğŸ§  BaseStepMixin v16.0 + AI ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   - ë””ë°”ì´ìŠ¤: {info['device']}")
        print(f"   - ë¡œë“œëœ AI ëª¨ë¸: {info['loaded_ai_models']}")
        print(f"   - ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸: {info['loaded_checkpoints']}")
        print(f"   - AI ëª¨ë¸ í˜¸ì¶œ ìˆ˜: {info['ai_model_stats']['total_ai_calls']}")
        print(f"   - BaseStepMixin v16.0 í˜¸í™˜: {info['basestepmixin_v16_info']['compatible']}")
        print(f"   - OpenCV ëŒ€ì²´ë¨: {info['basestepmixin_v16_info']['opencv_replaced']}")
        
        # ì •ë¦¬
        await step.cleanup()
        print("âœ… BaseStepMixin v16.0 + AI í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ì •ë¦¬")
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 + AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ìŒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   1. BaseStepMixin v16.0 ëª¨ë“ˆ")
        print("   2. ModelLoader ëª¨ë“ˆ")
        print("   3. ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼")
        print("   4. conda í™˜ê²½ ì„¤ì •")
        print("   5. BaseStepMixin v16.0 í˜¸í™˜ í™˜ê²½")

def example_basestepmixin_v16_usage():
    """BaseStepMixin v16.0 í˜¸í™˜ ì‚¬ìš© ì˜ˆì‹œ"""
    print("ğŸ”¥ MyCloset AI Step 03 - BaseStepMixin v16.0 í˜¸í™˜ + AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 80)
    
    print("""
# ğŸ”¥ BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ + AI ëª¨ë¸ ì—°ë™ ë²„ì „ (OpenCV ì™„ì „ ëŒ€ì²´)

# 1. ê¸°ë³¸ ì‚¬ìš©ë²• (BaseStepMixin v16.0 í˜¸í™˜)
from app.ai_pipeline.steps.step_03_cloth_segmentation import create_cloth_segmentation_step

step = create_cloth_segmentation_step(device="mps")

# 2. ì™„ì „ ìë™í™” ìƒì„± ë° ì´ˆê¸°í™” (BaseStepMixin v16.0 í˜¸í™˜)
step = await create_and_initialize_cloth_segmentation_step(
    device="mps",
    config={
        "quality_level": "ultra",
        "enable_visualization": True,
        "method": "auto_ai",
        "ai_edge_smoothing": True,
        "ai_noise_removal": True
    }
)

# 3. M3 Max ìµœì í™” ë²„ì „ (BaseStepMixin v16.0 í˜¸í™˜)
step = create_m3_max_segmentation_step({
    "quality_level": "ultra",
    "enable_visualization": True,
    "batch_size": 8,  # M3 Max 128GB í™œìš©
    "esrgan_scale": 4  # Real-ESRGAN ê³ í’ˆì§ˆ
})

# 4. BaseStepMixin v16.0 ì˜ì¡´ì„± ì£¼ì… í™•ì¸
print("ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ:", step.dependencies_injected)
print("BaseStepMixin v16.0 í˜¸í™˜:", step.get_status()['basestepmixin_v16_compatible'])

# 5. ì‹¤ì œ AI + BaseStepMixin v16.0 ê²°ê³¼ í™œìš©
result = await step.process(image, clothing_type="shirt", quality_level="high")

if result['success']:
    # ì‹¤ì œ AI ìƒì„± ê²°ê³¼
    ai_mask = result['mask']
    ai_confidence = result['confidence']
    ai_models_used = result['ai_models_used']
    
    # BaseStepMixin v16.0 ì •ë³´
    v16_compatible = result['metadata']['basestepmixin_v16_compatible']
    opencv_replaced = result['metadata']['opencv_replaced']
    dependency_status = result['metadata']['dependency_injection_status']
    
    print(f"AI ëª¨ë¸: {ai_models_used}")
    print(f"BaseStepMixin v16.0: {v16_compatible}")
    print(f"OpenCV ëŒ€ì²´ë¨: {opencv_replaced}")
    print(f"ì˜ì¡´ì„± ìƒíƒœ: {dependency_status}")

# 6. BaseStepMixin v16.0 ìƒíƒœ í™•ì¸
status = step.get_status()
basestepmixin_info = status['basestepmixin_v16_compatible']
print("BaseStepMixin v16.0 í˜¸í™˜ì„±:", basestepmixin_info)

# 7. AI ëª¨ë¸ ì •ë³´ ì¡°íšŒ
info = step.get_segmentation_info()
ai_info = info['ai_model_stats']
print("AI ëª¨ë¸ í†µê³„:")
for key, value in ai_info.items():
    print(f"  {key}: {value}")

# 8. conda í™˜ê²½ ì„¤ì • (BaseStepMixin v16.0 + AI ëª¨ë¸ìš©)
'''
conda create -n mycloset-ai-v16 python=3.9 -y
conda activate mycloset-ai-v16

# BaseStepMixin v16.0 í˜¸í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
conda install -c pytorch pytorch torchvision torchaudio -y
conda install -c conda-forge pillow numpy scikit-learn -y

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (OpenCV ëŒ€ì²´)
pip install rembg segment-anything transformers
pip install basicsr  # Real-ESRGAN

# M3 Max ìµœì í™”
conda install -c conda-forge accelerate -y

# BaseStepMixin v16.0 íŒŒì¼ ì—…ë°ì´íŠ¸
cp improved_base_step_mixin.py backend/app/ai_pipeline/steps/base_step_mixin.py
cp improved_model_loader.py backend/app/ai_pipeline/utils/model_loader.py
cp improved_step_factory.py backend/app/ai_pipeline/factories/step_factory.py
cp improved_step_interface.py backend/app/ai_pipeline/interface/step_interface.py

# ì‹¤í–‰
cd backend
python -m app.ai_pipeline.steps.step_03_cloth_segmentation
'''

# 9. ì—ëŸ¬ ì²˜ë¦¬ (BaseStepMixin v16.0 í˜¸í™˜)
try:
    await step.initialize()
except ImportError as e:
    print(f"ì˜ì¡´ì„± ë¡œë“œ ì‹¤íŒ¨: {e}")
    # BaseStepMixin v16.0 ìë™ í´ë°± ì²˜ë¦¬

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (BaseStepMixin v16.0 í˜¸í™˜)
await step.cleanup()
""")

def print_conda_setup_guide_v16():
    """conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (BaseStepMixin v16.0 + AIìš©)"""
    print("""
ğŸ MyCloset AI - conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (BaseStepMixin v16.0 + AI ëª¨ë¸ìš©)

# 1. conda í™˜ê²½ ìƒì„± (BaseStepMixin v16.0 + AI)
conda create -n mycloset-ai-v16 python=3.9 -y
conda activate mycloset-ai-v16

# 2. BaseStepMixin v16.0 í˜¸í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìˆ˜)
conda install -c pytorch pytorch torchvision torchaudio -y
conda install -c conda-forge pillow numpy scikit-learn -y

# 3. AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (OpenCV ì™„ì „ ëŒ€ì²´)
pip install rembg segment-anything transformers
pip install basicsr  # Real-ESRGAN
pip install ultralytics  # YOLO ê´€ë ¨

# 4. M3 Max ìµœì í™” (macOS)
conda install -c conda-forge accelerate -y

# 5. BaseStepMixin v16.0 íŒŒì¼ ì—…ë°ì´íŠ¸ (ì¤‘ìš”!)
cp improved_base_step_mixin.py backend/app/ai_pipeline/steps/base_step_mixin.py
cp improved_model_loader.py backend/app/ai_pipeline/utils/model_loader.py
cp improved_step_factory.py backend/app/ai_pipeline/factories/step_factory.py
cp improved_step_interface.py backend/app/ai_pipeline/interface/step_interface.py

# 6. BaseStepMixin v16.0 í˜¸í™˜ì„± ê²€ì¦
python -c "
import torch
from typing import TYPE_CHECKING

print(f'PyTorch: {torch.__version__}')
print(f'MPS: {torch.backends.mps.is_available()}')
print(f'TYPE_CHECKING: {TYPE_CHECKING}')

# BaseStepMixin v16.0 import í…ŒìŠ¤íŠ¸
try:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    print('âœ… BaseStepMixin v16.0 import ì„±ê³µ')
    
    # UnifiedDependencyManager í…ŒìŠ¤íŠ¸
    step = BaseStepMixin(step_name='TestStep')
    print('âœ… UnifiedDependencyManager ì‘ë™')
    print('dependency_manager:', hasattr(step, 'dependency_manager'))
except Exception as e:
    print(f'âŒ BaseStepMixin v16.0 import ì‹¤íŒ¨: {e}')
"

# 7. ì‹¤í–‰ (BaseStepMixin v16.0 + AI)
cd backend
export MYCLOSET_AI_BASESTEPMIXIN_V16=true
python -m app.ai_pipeline.steps.step_03_cloth_segmentation

# 8. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MYCLOSET_AI_BASESTEPMIXIN_V16=true
export MYCLOSET_AI_OPENCV_REPLACED=true
export MYCLOSET_AI_DEVICE=mps
export MYCLOSET_AI_MODELS_PATH=/path/to/ai_models

# 9. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -c "
import asyncio
from app.ai_pipeline.steps.step_03_cloth_segmentation import test_basestepmixin_v16_ai_segmentation
asyncio.run(test_basestepmixin_v16_ai_segmentation())
"
""")

# ==============================================
# ğŸ”¥ 20. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'ClothSegmentationStep',
    
    # ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
    'SegmentationMethod',
    'ClothingType',
    'QualityLevel',
    'SegmentationConfig',
    'SegmentationResult',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (OpenCV ëŒ€ì²´)
    'U2NET',
    'REBNCONV',
    'RSU7',
    'AIImageProcessor',
    
    # ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
    'get_base_step_mixin',
    'get_model_loader',
    'get_step_interface',
    'get_di_container',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜)
    'create_cloth_segmentation_step',
    'create_and_initialize_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    
    # ì‹œê°í™” ê´€ë ¨
    'CLOTHING_COLORS',
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'REMBG_AVAILABLE',
    'SKLEARN_AVAILABLE',
    'SAM_AVAILABLE',
    'TRANSFORMERS_AVAILABLE',
    'ESRGAN_AVAILABLE'
]

# ==============================================
# ğŸ”¥ 21. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger.info("=" * 80)
logger.info("âœ… Step 03 BaseStepMixin v16.0 í˜¸í™˜ + AI ì—°ë™ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ”¥ í•µì‹¬ í•´ê²°ì‚¬í•­:")
logger.info("   âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ì„± ë³´ì¥")
logger.info("   âœ… UnifiedDependencyManager ì™„ì „ í™œìš©")
logger.info("   âœ… OpenCV ì™„ì „ ì œê±° ë° AI ëª¨ë¸ë¡œ ëŒ€ì²´")
logger.info("   âœ… TYPE_CHECKING íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ë° ì¶”ë¡  (U2Net, SAM, RemBG, CLIP, Real-ESRGAN)")
logger.info("   âœ… M3 Max 128GB ìµœì í™”")
logger.info("   âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›")
logger.info("   âœ… Python êµ¬ì¡° ì™„ì „ ì •ë¦¬ (ë¬¸ë²• ì˜¤ë¥˜ ì—†ìŒ)")
logger.info("")
logger.info("ğŸ”— BaseStepMixin v16.0 í˜¸í™˜ì„±:")
logger.info("   âœ… set_model_loader() - ModelLoader ì˜ì¡´ì„± ì£¼ì…")
logger.info("   âœ… set_memory_manager() - MemoryManager ì˜ì¡´ì„± ì£¼ì…")
logger.info("   âœ… set_data_converter() - DataConverter ì˜ì¡´ì„± ì£¼ì…")
logger.info("   âœ… set_di_container() - DI Container ì˜ì¡´ì„± ì£¼ì…")
logger.info("   âœ… get_status() - ìƒíƒœ ì¡°íšŒ")
logger.info("   âœ… get_performance_summary() - ì„±ëŠ¥ ìš”ì•½")
logger.info("   âœ… warmup() / warmup_async() - ì›Œë°ì—…")
logger.info("   âœ… cleanup() - ë¦¬ì†ŒìŠ¤ ì •ë¦¬")
logger.info("")
logger.info("ğŸ§  AI ëª¨ë¸ ì™„ì „ ì—°ë™:")
logger.info("   âœ… U2-Net: ì •ë°€í•œ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜")
logger.info("   âœ… SAM: ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜")
logger.info("   âœ… RemBG: ë°°ê²½ ì œê±° ì „ë¬¸")
logger.info("   âœ… CLIP: ì§€ëŠ¥ì  ì˜ë¥˜ íƒ€ì… ê°ì§€")
logger.info("   âœ… Real-ESRGAN: ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ")
logger.info("   âœ… AIImageProcessor: ì—£ì§€ ê²€ì¶œ ë° ë…¸ì´ì¦ˆ ì œê±°")
logger.info("")
logger.info("ğŸš« OpenCV ì™„ì „ ëŒ€ì²´:")
logger.info("   âŒ cv2.resize â†’ AI ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ + Real-ESRGAN")
logger.info("   âŒ cv2.Canny â†’ AIImageProcessor.detect_edges_ai()")
logger.info("   âŒ cv2.morphologyEx â†’ PyTorch ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚°")
logger.info("   âŒ cv2.GaussianBlur â†’ PyTorch ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬")
logger.info("   âŒ cv2.findContours â†’ PyTorch ê¸°ë°˜ ì—°ê²°ì„± ë¶„ì„")
logger.info("   âŒ cv2.threshold â†’ AI ê¸°ë°˜ ìë™ ì„ê³„ê°’")
logger.info("")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
logger.info(f"   - PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
logger.info(f"   - RemBG: {'âœ…' if REMBG_AVAILABLE else 'âŒ'}")
logger.info(f"   - SAM: {'âœ…' if SAM_AVAILABLE else 'âŒ'}")
logger.info(f"   - Transformers: {'âœ…' if TRANSFORMERS_AVAILABLE else 'âŒ'}")
logger.info(f"   - Real-ESRGAN: {'âœ…' if ESRGAN_AVAILABLE else 'âŒ'}")
logger.info(f"   - OpenCV: âŒ (ì™„ì „ ëŒ€ì²´ë¨)")
logger.info("")
logger.info("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
logger.info("   # BaseStepMixin v16.0 í˜¸í™˜ + AI ì—°ë™")
logger.info("   step = await create_and_initialize_cloth_segmentation_step()")
logger.info("   result = await step.process(image)")
logger.info("   print('BaseStepMixin v16.0:', result['metadata']['basestepmixin_v16_compatible'])")
logger.info("   print('OpenCV ëŒ€ì²´ë¨:', result['metadata']['opencv_replaced'])")
logger.info("")
logger.info("=" * 80)
logger.info("ğŸš€ BaseStepMixin v16.0 í˜¸í™˜ + AI ì—°ë™ Step 03 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜")
logger.info("   âœ… OpenCV ì™„ì „ ëŒ€ì²´")
logger.info("   âœ… AI ëª¨ë¸ ì™„ì „ ì—°ë™")
logger.info("   âœ… TYPE_CHECKING íŒ¨í„´")
logger.info("   âœ… M3 Max ìµœì í™”")
logger.info("   âœ… conda í™˜ê²½ ì§€ì›")
logger.info("   âœ… Python ë¬¸ë²• ì™„ì „ ì •ë¦¬")
logger.info("=" * 80)

if __name__ == "__main__":
    """ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ (BaseStepMixin v16.0 + AI)"""
    print("ğŸ”¥ Step 03 BaseStepMixin v16.0 í˜¸í™˜ + AI ì„¸ê·¸ë©˜í…Œì´ì…˜ - ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
    
    # ì˜ˆì‹œ ì¶œë ¥
    example_basestepmixin_v16_usage()
    
    # conda ê°€ì´ë“œ
    print_conda_setup_guide_v16()
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸°)
    import asyncio
    try:
        asyncio.run(test_basestepmixin_v16_ai_segmentation())
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 + AI í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤:")
        print("   1. BaseStepMixin v16.0 ëª¨ë“ˆ (improved_base_step_mixin.py)")
        print("   2. ModelLoader ëª¨ë“ˆ (improved_model_loader.py)")
        print("   3. StepFactory ëª¨ë“ˆ (improved_step_factory.py)")
        print("   4. StepInterface ëª¨ë“ˆ (improved_step_interface.py)")
        print("   5. ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼")
        print("   6. conda í™˜ê²½ ì„¤ì •")
        print("   7. BaseStepMixin v16.0 í˜¸í™˜ í™˜ê²½")