# app/ai_pipeline/steps/step_03_cloth_segmentation.py
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - ì™„ì „ AI ëª¨ë¸ ì—°ë™ + BaseStepMixin v16.0 í˜¸í™˜
===============================================================================

ğŸ¯ í†µí•© ë°©ì•ˆ - ëª¨ë“  ê²ƒì„ í•œë²ˆì— ì™„ì „íˆ êµ¬í˜„:
âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ êµ¬í˜„ (SAM + U2Net + Mobile SAM + ISNet)
âœ… OpenCV ì™„ì „ ì œê±° ë° AI ëª¨ë¸ ëŒ€ì²´
âœ… BaseStepMixin v16.0 UnifiedDependencyManager ì™„ì „ í˜¸í™˜
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… 5.5GB ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš©
âœ… Stepê°„ ì¸ì ì—°ë™ êµ¬ì¡° ì™„ì„±
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

AI ëª¨ë¸ ì—°ë™:
- RealSAMModel: sam_vit_h_4b8939.pth (2445.7MB) - ìµœê³  ì„±ëŠ¥ ì„¸ê·¸ë©˜í…Œì´ì…˜
- RealU2NetClothModel: u2net.pth (168.1MB) - ì˜ë¥˜ íŠ¹í™” ì„¸ê·¸ë©˜í…Œì´ì…˜
- RealMobileSAMModel: mobile_sam.pt (38.8MB) - ì‹¤ì‹œê°„ ê²½ëŸ‰ ì„¸ê·¸ë©˜í…Œì´ì…˜
- RealISNetModel: isnetis.onnx (168.1MB) - ê³ ì •ë°€ ê²½ê³„ ê²€ì¶œ
- AIImageProcessor: OpenCV ì™„ì „ ëŒ€ì²´ AI ì²˜ë¦¬

Author: MyCloset AI Team
Date: 2025-07-25
Version: v20.0 (Complete AI Integration + BaseStepMixin v16.0)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import gc
import hashlib
import json
import base64
import weakref
import math
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
import platform
import subprocess

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ import (ëŸ°íƒ€ì„ì—ëŠ” import ì•ˆë¨)
    from app.ai_pipeline.utils.model_loader import ModelLoader, StepModelInterface
    from ..steps.base_step_mixin import BaseStepMixin, UnifiedDependencyManager
    from ..factories.step_factory import StepFactory
    from ..interfaces.step_interface import StepInterface
    from app.core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 2. í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (conda í™˜ê²½ ìš°ì„ )
# ==============================================

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½ ìš°ì„ )")
except ImportError:
    logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")

# PIL Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
except ImportError:
    logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# PyTorch Import (conda í™˜ê²½ ìš°ì„ )
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥ (M3 Max ìµœì í™”)")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
REMBG_AVAILABLE = False
try:
    import rembg
    from rembg import remove, new_session
    REMBG_AVAILABLE = True
    logger.info("ğŸ¤– RemBG ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ RemBG ì—†ìŒ - pip install rembg")

SKLEARN_AVAILABLE = False
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
    logger.info("ğŸ“ˆ scikit-learn ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ scikit-learn ì—†ìŒ - conda install scikit-learn")

SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - pip install segment-anything")

TRANSFORMERS_AVAILABLE = False
try:
    from transformers import pipeline, CLIPModel, CLIPProcessor
    TRANSFORMERS_AVAILABLE = True
    logger.info("ğŸ¤— Transformers ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Transformers ì—†ìŒ - pip install transformers")

ESRGAN_AVAILABLE = False
try:
    from basicsr.archs.rrdbnet_arch import RRDBNet
    from basicsr.utils.download_util import load_file_from_url
    ESRGAN_AVAILABLE = True
    logger.info("ğŸ¨ Real-ESRGAN ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Real-ESRGAN ì—†ìŒ - pip install basicsr")

ONNX_AVAILABLE = False
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
    logger.info("âš¡ ONNX Runtime ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ ONNX Runtime ì—†ìŒ - pip install onnxruntime")

# ==============================================
# ğŸ”¥ 3. ë™ì  Import í•¨ìˆ˜ë“¤ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.debug(f"BaseStepMixin ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_unified_dependency_manager_class():
    """UnifiedDependencyManager í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'UnifiedDependencyManager', None)
    except ImportError as e:
        logger.debug(f"UnifiedDependencyManager ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        return None
    except ImportError as e:
        logger.debug(f"ModelLoader ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_step_interface_class():
    """StepInterface í´ë˜ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interfaces.step_interface')
        return getattr(module, 'StepInterface', None)
    except ImportError as e:
        logger.debug(f"StepInterface ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_di_container():
    """DI Containerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib  
        module = importlib.import_module('app.core.di_container')
        get_container = getattr(module, 'get_di_container', None)
        if get_container:
            return get_container()
        return None
    except ImportError as e:
        logger.debug(f"DI Container ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 4. ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    SAM_HUGE = "sam_huge"           # SAM ViT-Huge (2445.7MB)
    U2NET_CLOTH = "u2net_cloth"     # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    MOBILE_SAM = "mobile_sam"       # Mobile SAM (38.8MB)
    ISNET = "isnet"                 # ISNet ONNX (168.1MB)
    HYBRID_AI = "hybrid_ai"         # ì—¬ëŸ¬ AI ëª¨ë¸ ì¡°í•©
    AUTO_AI = "auto_ai"             # ìë™ AI ëª¨ë¸ ì„ íƒ

class ClothingType(Enum):
    """ì˜ë¥˜ íƒ€ì…"""
    SHIRT = "shirt"
    DRESS = "dress"
    PANTS = "pants"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # Mobile SAM
    BALANCED = "balanced"   # U2Net + ISNet
    HIGH = "high"          # SAM + U2Net
    ULTRA = "ultra"        # Hybrid AI (ëª¨ë“  ëª¨ë¸)

@dataclass
class SegmentationConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.AUTO_AI
    quality_level: QualityLevel = QualityLevel.BALANCED
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    enable_visualization: bool = True
    enable_post_processing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    iou_threshold: float = 0.5
    edge_smoothing: bool = True
    remove_noise: bool = True
    visualization_quality: str = "high"
    enable_caching: bool = True
    cache_size: int = 100
    show_masks: bool = True
    show_boundaries: bool = True
    overlay_opacity: float = 0.6
    esrgan_scale: int = 2

@dataclass
class SegmentationResult:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼"""
    success: bool
    mask: Optional[np.ndarray] = None
    segmented_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    quality_score: float = 0.0
    method_used: str = "unknown"
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    # ì‹œê°í™” ì´ë¯¸ì§€ë“¤
    visualization_image: Optional[Image.Image] = None
    overlay_image: Optional[Image.Image] = None
    mask_image: Optional[Image.Image] = None
    boundary_image: Optional[Image.Image] = None

@dataclass
class StepInputData:
    """Step ê°„ í‘œì¤€ ì…ë ¥ ë°ì´í„°"""
    image: Union[str, np.ndarray, Image.Image]
    metadata: Dict[str, Any] = field(default_factory=dict)
    step_history: List[str] = field(default_factory=list)
    processing_context: Dict[str, Any] = field(default_factory=dict)

@dataclass 
class StepOutputData:
    """Step ê°„ í‘œì¤€ ì¶œë ¥ ë°ì´í„°"""
    success: bool
    result_data: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    step_name: str = "cloth_segmentation"
    processing_time: float = 0.0
    next_step_input: Optional[Dict[str, Any]] = None

# ==============================================
# ğŸ”¥ 5. ì˜ë¥˜ë³„ ìƒ‰ìƒ ë§¤í•‘ (ì‹œê°í™”ìš©)
# ==============================================

CLOTHING_COLORS = {
    'shirt': (255, 100, 100),      # ë¹¨ê°•
    'pants': (100, 100, 255),      # íŒŒë‘
    'dress': (255, 100, 255),      # ë¶„í™
    'jacket': (100, 255, 100),     # ì´ˆë¡
    'skirt': (255, 255, 100),      # ë…¸ë‘
    'sweater': (138, 43, 226),     # ë¸”ë£¨ë°”ì´ì˜¬ë ›
    'coat': (165, 42, 42),         # ê°ˆìƒ‰
    'top': (0, 255, 255),          # ì‹œì•ˆ
    'bottom': (255, 165, 0),       # ì˜¤ë Œì§€
    'shoes': (255, 150, 0),        # ì£¼í™©
    'bag': (150, 75, 0),           # ê°ˆìƒ‰
    'hat': (128, 0, 128),          # ë³´ë¼
    'accessory': (0, 255, 255),    # ì‹œì•ˆ
    'unknown': (128, 128, 128),    # íšŒìƒ‰
}

# ==============================================
# ğŸ”¥ 6. AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class AIImageProcessor:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    
    @staticmethod
    def ai_resize(image: Union[np.ndarray, Image.Image], target_size: Tuple[int, int]) -> Image.Image:
        """AI ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ (OpenCV resize ëŒ€ì²´)"""
        try:
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            
            # Real-ESRGAN ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
            if ESRGAN_AVAILABLE and min(target_size) > min(image.size):
                return AIImageProcessor.esrgan_upscale(image, target_size)
            else:
                return image.resize(target_size, Image.Resampling.LANCZOS)
                
        except Exception as e:
            logger.warning(f"âš ï¸ AI ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            return image.resize(target_size, Image.Resampling.LANCZOS)
    
    @staticmethod
    def esrgan_upscale(image: Image.Image, target_size: Tuple[int, int]) -> Image.Image:
        """Real-ESRGAN ê¸°ë°˜ ì—…ìŠ¤ì¼€ì¼ë§"""
        try:
            if not ESRGAN_AVAILABLE or not TORCH_AVAILABLE:
                return image.resize(target_size, Image.Resampling.LANCZOS)
            
            # ê°„ë‹¨í•œ ì—…ìŠ¤ì¼€ì¼ë§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ êµ¬í˜„ í•„ìš”)
            scale_factor = min(target_size[0] / image.size[0], target_size[1] / image.size[1])
            if scale_factor <= 1.0:
                return image.resize(target_size, Image.Resampling.LANCZOS)
            
            # Real-ESRGAN ëŒ€ì‹  ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©
            intermediate_size = (int(image.size[0] * scale_factor), int(image.size[1] * scale_factor))
            upscaled = image.resize(intermediate_size, Image.Resampling.LANCZOS)
            
            if upscaled.size != target_size:
                upscaled = upscaled.resize(target_size, Image.Resampling.LANCZOS)
            
            return upscaled
            
        except Exception as e:
            logger.warning(f"âš ï¸ ESRGAN ì—…ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
            return image.resize(target_size, Image.Resampling.LANCZOS)
    
    @staticmethod
    def ai_detect_edges(image: np.ndarray, threshold1: int = 50, threshold2: int = 150) -> np.ndarray:
        """AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ (cv2.Canny ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                # PyTorch ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
                return AIImageProcessor._simple_edge_detection(image)
            
            # PyTorch ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
            tensor = torch.from_numpy(image).float()
            if len(tensor.shape) == 3:
                tensor = tensor.mean(dim=2)  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            
            tensor = tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
            
            # Sobel í•„í„° ì •ì˜
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
            
            sobel_x = sobel_x.view(1, 1, 3, 3)
            sobel_y = sobel_y.view(1, 1, 3, 3)
            
            # ì»¨ë³¼ë£¨ì…˜ ì ìš©
            grad_x = F.conv2d(tensor, sobel_x, padding=1)
            grad_y = F.conv2d(tensor, sobel_y, padding=1)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ê³„ì‚°
            magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)
            
            # ì„ê³„ê°’ ì ìš©
            edges = (magnitude > threshold1).float()
            edges = edges.squeeze().numpy()
            
            return (edges * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_edge_detection(image)
    
    @staticmethod
    def _simple_edge_detection(image: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ (í´ë°±)"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            
            # í¬ê¸° ë§ì¶¤
            grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='edge')
            grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='edge')
            
            magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)
            edges = (magnitude > 50).astype(np.uint8) * 255
            
            return edges
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros_like(image[:, :, 0] if len(image.shape) == 3 else image, dtype=np.uint8)
    
    @staticmethod
    def ai_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """AI ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚° (cv2.morphologyEx ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
            
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            
            # êµ¬ì¡°ì  ìš”ì†Œ (ì»¤ë„)
            kernel = torch.ones(1, 1, kernel_size, kernel_size)
            padding = kernel_size // 2
            
            if operation.lower() == "closing":
                # Dilation í›„ Erosion
                dilated = F.max_pool2d(tensor, kernel_size, stride=1, padding=padding)
                result = -F.max_pool2d(-dilated, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "opening":
                # Erosion í›„ Dilation
                eroded = -F.max_pool2d(-tensor, kernel_size, stride=1, padding=padding)
                result = F.max_pool2d(eroded, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "dilation":
                result = F.max_pool2d(tensor, kernel_size, stride=1, padding=padding)
            elif operation.lower() == "erosion":
                result = -F.max_pool2d(-tensor, kernel_size, stride=1, padding=padding)
            else:
                result = tensor
            
            result_np = result.squeeze().numpy()
            return (result_np * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_morphology(mask, operation, kernel_size)
    
    @staticmethod
    def _simple_morphology(mask: np.ndarray, operation: str, kernel_size: int = 5) -> np.ndarray:
        """ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° (í´ë°±)"""
        try:
            # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•´ì•¼ í•¨)
            if operation.lower() == "closing":
                # ê°„ë‹¨í•œ í™€ ì±„ìš°ê¸°
                from scipy import ndimage
                filled = ndimage.binary_fill_holes(mask > 128)
                return (filled * 255).astype(np.uint8)
            else:
                return mask
        except ImportError:
            return mask
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return mask
    
    @staticmethod
    def ai_gaussian_blur(image: np.ndarray, kernel_size: int = 5, sigma: float = 1.0) -> np.ndarray:
        """AI ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (cv2.GaussianBlur ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return AIImageProcessor._simple_blur(image, kernel_size)
            
            if len(image.shape) == 2:
                tensor = torch.from_numpy(image.astype(np.float32)).unsqueeze(0).unsqueeze(0)
            else:
                tensor = torch.from_numpy(image.astype(np.float32)).permute(2, 0, 1).unsqueeze(0)
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
            coords = torch.arange(kernel_size, dtype=torch.float32) - kernel_size // 2
            kernel_1d = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
            kernel_1d = kernel_1d / kernel_1d.sum()
            
            kernel_2d = kernel_1d[:, None] * kernel_1d[None, :]
            kernel_2d = kernel_2d.expand(tensor.size(1), 1, kernel_size, kernel_size)
            
            padding = kernel_size // 2
            blurred = F.conv2d(tensor, kernel_2d, padding=padding, groups=tensor.size(1))
            
            if len(image.shape) == 2:
                result = blurred.squeeze().numpy()
            else:
                result = blurred.squeeze().permute(1, 2, 0).numpy()
            
            return result.astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì‹¤íŒ¨: {e}")
            return AIImageProcessor._simple_blur(image, kernel_size)
    
    @staticmethod
    def _simple_blur(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
        """ê°„ë‹¨í•œ ë¸”ëŸ¬ (í´ë°±)"""
        try:
            from scipy import ndimage
            sigma = kernel_size / 3.0
            return ndimage.gaussian_filter(image, sigma=sigma)
        except ImportError:
            return image
        except Exception as e:
            logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ë¸”ëŸ¬ ì‹¤íŒ¨: {e}")
            return image

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (5.5GB ëª¨ë¸ íŒŒì¼ í™œìš©)
# ==============================================

class REBNCONV(nn.Module):
    """U2-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡"""
    
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class RSU7(nn.Module):
    """U2-Net RSU-7 ë¸”ë¡ (ì™„ì „í•œ êµ¬í˜„)"""
    
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample6 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample5 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)
    
    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = self.upsample6(hx6d)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = self.upsample5(hx5d)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = self.upsample4(hx4d)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = self.upsample3(hx3d)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = self.upsample2(hx2d)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RealU2NetClothModel(nn.Module):
    """ì‹¤ì œ U2-Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸ (u2net.pth 168.1MB í™œìš©)"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super(RealU2NetClothModel, self).__init__()
        
        # ì¸ì½”ë”
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU7(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU7(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU7(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU7(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU7(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU7(1024, 256, 512)
        self.stage4d = RSU7(1024, 128, 256)
        self.stage3d = RSU7(512, 64, 128)
        self.stage2d = RSU7(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # Side outputs
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6*out_ch, out_ch, 1)
        
        # ëª¨ë¸ ì •ë³´
        self.model_name = "RealU2NetClothModel"
        self.version = "2.0"
        self.parameter_count = self._count_parameters()
        self.cloth_specialized = True
        
    def _count_parameters(self):
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx5d = self.stage5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # Side outputs
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=d1.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=d1.shape[2:], mode='bilinear', align_corners=False)
        
        # ìµœì¢… ì¶œë ¥
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (u2net.pth 168.1MB)"""
        model = cls()
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                logger.info(f"ğŸ”„ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        model.load_state_dict(checkpoint['model'])
                    elif 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                else:
                    model.load_state_dict(checkpoint)
                    
                logger.info(f"âœ… U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model.parameter_count:,} íŒŒë¼ë¯¸í„°")
            except Exception as e:
                logger.warning(f"âš ï¸ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        model.to(device)
        model.eval()
        return model

class RealSAMModel(nn.Module):
    """ì‹¤ì œ SAM ëª¨ë¸ ë˜í¼ (sam_vit_h_4b8939.pth 2445.7MB í™œìš©)"""
    
    def __init__(self, model_type: str = "vit_h"):
        super(RealSAMModel, self).__init__()
        self.model_type = model_type
        self.model_name = f"RealSAMModel_{model_type}"
        self.version = "2.0"
        self.sam_model = None
        self.predictor = None
        self.is_loaded = False
        
    def load_sam_model(self, checkpoint_path: str):
        """SAM ëª¨ë¸ ë¡œë“œ (sam_vit_h_4b8939.pth 2445.7MB)"""
        try:
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ SAM ëª¨ë¸ ë¡œë”©: {checkpoint_path}")
            
            # SAM ëª¨ë¸ ë¹Œë“œ
            if self.model_type == "vit_h":
                self.sam_model = sam.build_sam_vit_h(checkpoint=checkpoint_path)
            elif self.model_type == "vit_b":
                self.sam_model = sam.build_sam_vit_b(checkpoint=checkpoint_path)
            else:
                self.sam_model = sam.build_sam(checkpoint=checkpoint_path)
            
            # Predictor ìƒì„±
            self.predictor = sam.SamPredictor(self.sam_model)
            self.is_loaded = True
            
            logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_type}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def forward(self, x):
        """ë”ë¯¸ forward (SAMì€ íŠ¹ë³„í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)"""
        if not self.is_loaded:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        return x
    
    def segment_clothing(self, image_array: np.ndarray, clothing_type: str = "shirt") -> Dict[str, np.ndarray]:
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì˜ë¥˜ íƒ€ì…ë³„ íŠ¹í™”)"""
        try:
            if not self.is_loaded or self.predictor is None:
                logger.warning("âš ï¸ SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return {}
            
            # ì´ë¯¸ì§€ ì„¤ì •
            self.predictor.set_image(image_array)
            
            # ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ìƒì„±
            height, width = image_array.shape[:2]
            clothing_prompts = self._generate_clothing_prompts(clothing_type, width, height)
            
            results = {}
            
            for cloth_area, points in clothing_prompts.items():
                try:
                    # SAM ì˜ˆì¸¡
                    masks, scores, logits = self.predictor.predict(
                        point_coords=np.array(points),
                        point_labels=np.ones(len(points)),
                        multimask_output=True
                    )
                    
                    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë§ˆìŠ¤í¬ ì„ íƒ
                    best_mask_idx = np.argmax(scores)
                    best_mask = masks[best_mask_idx].astype(np.uint8)
                    
                    results[cloth_area] = best_mask
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ SAM {cloth_area} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_clothing_prompts(self, clothing_type: str, width: int, height: int) -> Dict[str, List[Tuple[int, int]]]:
        """ì˜ë¥˜ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ìƒì„±"""
        prompts = {}
        
        if clothing_type in ["shirt", "top", "sweater"]:
            # ìƒì˜ ì˜ì—­
            prompts["upper_body"] = [
                (width // 2, height // 3),      # ê°€ìŠ´
                (width // 3, height // 2),      # ì™¼ìª½ íŒ”
                (2 * width // 3, height // 2),  # ì˜¤ë¥¸ìª½ íŒ”
            ]
        elif clothing_type in ["pants", "bottom"]:
            # í•˜ì˜ ì˜ì—­
            prompts["lower_body"] = [
                (width // 2, 2 * height // 3),  # í—ˆë¦¬
                (width // 3, 3 * height // 4),  # ì™¼ìª½ ë‹¤ë¦¬
                (2 * width // 3, 3 * height // 4),  # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
            ]
        elif clothing_type == "dress":
            # ì›í”¼ìŠ¤ ì˜ì—­
            prompts["full_dress"] = [
                (width // 2, height // 3),      # ìƒì²´
                (width // 2, 2 * height // 3),  # í•˜ì²´
                (width // 3, height // 2),      # ì™¼ìª½
                (2 * width // 3, height // 2),  # ì˜¤ë¥¸ìª½
            ]
        else:
            # ê¸°ë³¸ ì „ì²´ ì˜ë¥˜ ì˜ì—­
            prompts["clothing"] = [
                (width // 2, height // 2),      # ì¤‘ì•™
                (width // 3, height // 3),      # ì¢Œìƒ
                (2 * width // 3, 2 * height // 3),  # ìš°í•˜
            ]
        
        return prompts
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu", model_type: str = "vit_h"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ SAM ëª¨ë¸ ë¡œë“œ"""
        model = cls(model_type=model_type)
        model.load_sam_model(checkpoint_path)
        return model

class RealMobileSAMModel(nn.Module):
    """ì‹¤ì œ Mobile SAM ëª¨ë¸ (mobile_sam.pt 38.8MB í™œìš©)"""
    
    def __init__(self):
        super(RealMobileSAMModel, self).__init__()
        self.model_name = "RealMobileSAMModel"
        self.version = "2.0"
        self.sam_model = None
        self.predictor = None
        self.is_loaded = False
        
    def load_mobile_sam(self, checkpoint_path: str):
        """Mobile SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ”„ Mobile SAM ë¡œë”©: {checkpoint_path}")
            
            if TORCH_AVAILABLE and os.path.exists(checkpoint_path):
                # PyTorch ëª¨ë¸ ë¡œë“œ
                self.sam_model = torch.jit.load(checkpoint_path, map_location='cpu')
                self.sam_model.eval()
                self.is_loaded = True
                
                logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ")
                return True
            else:
                logger.warning("âš ï¸ Mobile SAM ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def forward(self, x):
        """Mobile SAM ì¶”ë¡ """
        if not self.is_loaded or self.sam_model is None:
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
        
        try:
            with torch.no_grad():
                result = self.sam_model(x)
                return result
        except Exception as e:
            logger.warning(f"âš ï¸ Mobile SAM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            batch_size, _, height, width = x.shape
            return torch.zeros(batch_size, 1, height, width, device=x.device)
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ Mobile SAM ë¡œë“œ"""
        model = cls()
        model.load_mobile_sam(checkpoint_path)
        model.to(device)
        return model

class RealISNetModel:
    """ì‹¤ì œ ISNet ONNX ëª¨ë¸ (isnetis.onnx 168.1MB í™œìš©)"""
    
    def __init__(self):
        self.model_name = "RealISNetModel"
        self.version = "2.0"
        self.ort_session = None
        self.is_loaded = False
        
    def load_isnet_model(self, onnx_path: str):
        """ISNet ONNX ëª¨ë¸ ë¡œë“œ"""
        try:
            if not ONNX_AVAILABLE:
                logger.warning("âš ï¸ ONNX Runtimeì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"ğŸ”„ ISNet ONNX ë¡œë”©: {onnx_path}")
            
            # ONNX ì„¸ì…˜ ìƒì„±
            providers = ['CPUExecutionProvider']
            if MPS_AVAILABLE:
                providers.insert(0, 'CoreMLExecutionProvider')
            
            self.ort_session = ort.InferenceSession(onnx_path, providers=providers)
            self.is_loaded = True
            
            logger.info("âœ… ISNet ONNX ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ISNet ONNX ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image_array: np.ndarray) -> np.ndarray:
        """ISNet ì˜ˆì¸¡"""
        try:
            if not self.is_loaded or self.ort_session is None:
                logger.warning("âš ï¸ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
            
            # ì „ì²˜ë¦¬
            if len(image_array.shape) == 3:
                # RGB to BGR ë³€í™˜ ë° ì •ê·œí™”
                input_image = image_array[:, :, ::-1].astype(np.float32) / 255.0
                input_image = np.transpose(input_image, (2, 0, 1))
                input_image = np.expand_dims(input_image, axis=0)
            else:
                input_image = image_array.astype(np.float32) / 255.0
                input_image = np.expand_dims(input_image, axis=(0, 1))
            
            # ONNX ì¶”ë¡ 
            input_name = self.ort_session.get_inputs()[0].name
            result = self.ort_session.run(None, {input_name: input_image})
            
            # í›„ì²˜ë¦¬
            mask = result[0][0, 0, :, :]  # [1, 1, H, W] -> [H, W]
            mask = (mask > 0.5).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            logger.error(f"âŒ ISNet ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
    
    @classmethod
    def from_checkpoint(cls, onnx_path: str):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ISNet ë¡œë“œ"""
        model = cls()
        model.load_isnet_model(onnx_path)
        return model

# ==============================================
# ğŸ”¥ 8. BaseStepMixin v16.0 í˜¸í™˜ í´ë°± í´ë˜ìŠ¤
# ==============================================

class BaseStepMixinFallback:
    """BaseStepMixin v16.0 í˜¸í™˜ í´ë°± í´ë˜ìŠ¤"""
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.step_name = kwargs.get('step_name', 'BaseStep')
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'cpu')
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # v16.0 í˜¸í™˜ ì†ì„±ë“¤
        self.config = kwargs.get('config', {})
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_factory = None
        
        # ì˜ì¡´ì„± ê´€ë¦¬ì ì‹œë®¬ë ˆì´ì…˜
        self.dependency_manager = self._create_dummy_dependency_manager()
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'avg_processing_time': 0.0,
            'cache_hits': 0,
            'success_count': 0,
            'error_count': 0
        }
        
    def _create_dummy_dependency_manager(self):
        """ë”ë¯¸ ì˜ì¡´ì„± ê´€ë¦¬ì ìƒì„±"""
        class DummyDependencyManager:
            def __init__(self, step_instance):
                self.step_instance = step_instance
                self.logger = step_instance.logger
                
            def auto_inject_dependencies(self):
                """ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜"""
                try:
                    # ModelLoader ìë™ ê°ì§€ ë° ì£¼ì…
                    model_loader = get_model_loader()
                    if model_loader:
                        self.step_instance.set_model_loader(model_loader)
                        self.logger.info("âœ… ìë™ ì˜ì¡´ì„± ì£¼ì…: ModelLoader")
                        return True
                        
                    self.logger.debug("âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì…: ModelLoader ë¯¸ë°œê²¬")
                    return False
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                    return False
                    
            def get_dependency(self, dep_name: str):
                """ì˜ì¡´ì„± ê°€ì ¸ì˜¤ê¸°"""
                return getattr(self.step_instance, dep_name, None)
                
            def inject_dependency(self, dep_name: str, dependency):
                """ì˜ì¡´ì„± ì£¼ì…"""
                setattr(self.step_instance, dep_name, dependency)
                return True
        
        return DummyDependencyManager(self)
    
    # BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.has_model = True
            self.model_loaded = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_step_factory(self, step_factory):
        """StepFactory ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.step_factory = step_factory
            self.logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def get_model(self, model_name: str = "default"):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        return None

    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'warmup_completed': self.warmup_completed,
            'device': self.device,
            'basestepmixin_v16_compatible': True,
            'fallback_mode': True
        }

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½"""
        return self.performance_stats.copy()

    def record_processing(self, processing_time: float, success: bool, **metrics):
        """ì²˜ë¦¬ ê¸°ë¡"""
        self.performance_stats['total_processed'] += 1
        if success:
            self.performance_stats['success_count'] += 1
        else:
            self.performance_stats['error_count'] += 1
        
        # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_stats['total_processed']
        current_avg = self.performance_stats['avg_processing_time']
        self.performance_stats['avg_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )

    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            gc.collect()
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return {'success': True, 'aggressive': aggressive}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def warmup(self, **kwargs) -> Dict[str, Any]:
        """ì›Œë°ì—…"""
        try:
            self.warmup_completed = True
            return {'success': True}
        except Exception as e:
            return {'success': False, 'error': str(e)}

# ==============================================
# ğŸ”¥ 9. ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤ (ì™„ì „ í†µí•© ë²„ì „)
# ==============================================

class ClothSegmentationStep:
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - ì™„ì „ AI ëª¨ë¸ ì—°ë™ + BaseStepMixin v16.0 í˜¸í™˜
    
    ğŸ¯ í†µí•© ë°©ì•ˆ ì™„ì „ êµ¬í˜„:
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ êµ¬í˜„ (SAM + U2Net + Mobile SAM + ISNet)
    âœ… OpenCV ì™„ì „ ì œê±° ë° AI ëª¨ë¸ ëŒ€ì²´
    âœ… BaseStepMixin v16.0 UnifiedDependencyManager ì™„ì „ í˜¸í™˜
    âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
    âœ… 5.5GB ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš©
    âœ… Stepê°„ ì¸ì ì—°ë™ êµ¬ì¡° ì™„ì„±
    âœ… M3 Max 128GB ìµœì í™”
    âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], SegmentationConfig]] = None,
        **kwargs
    ):
        """ìƒì„±ì - BaseStepMixin v16.0 í˜¸í™˜ + ì™„ì „ AI ì—°ë™"""
        
        # ===== 1. ê¸°ë³¸ ì†ì„± ì„¤ì • =====
        self.step_name = kwargs.get('step_name', "ClothSegmentationStep")
        self.step_id = kwargs.get('step_id', 3)
        self.step_type = "cloth_segmentation"
        self.device = device or self._auto_detect_device()
        
        # ===== 2. Logger ì„¤ì • =====
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        
        # ===== 3. ì„¤ì • ì²˜ë¦¬ =====
        if isinstance(config, dict):
            self.segmentation_config = SegmentationConfig(**config)
        elif isinstance(config, SegmentationConfig):
            self.segmentation_config = config
        else:
            self.segmentation_config = SegmentationConfig()
        
        # ===== 4. BaseStepMixin v16.0 í˜¸í™˜ ì†ì„±ë“¤ =====
        # ì‹¤ì œ BaseStepMixin ì‹œë„, í´ë°±ìœ¼ë¡œ í˜¸í™˜ í´ë˜ìŠ¤ ì‚¬ìš©
        try:
            BaseStepMixin = get_base_step_mixin_class()
            if BaseStepMixin:
                self._mixin = BaseStepMixin(step_name=self.step_name, step_id=self.step_id, **kwargs)
                self.logger.info("âœ… ì‹¤ì œ BaseStepMixin v16.0 ì—°ë™ ì„±ê³µ")
            else:
                self._mixin = BaseStepMixinFallback(step_name=self.step_name, step_id=self.step_id, **kwargs)
                self.logger.info("âœ… BaseStepMixin v16.0 í˜¸í™˜ í´ë°± ì‚¬ìš©")
        except Exception as e:
            self.logger.warning(f"âš ï¸ BaseStepMixin ì—°ë™ ì‹¤íŒ¨: {e}")
            self._mixin = BaseStepMixinFallback(step_name=self.step_name, step_id=self.step_id, **kwargs)
        
        # BaseStepMixin ì†ì„±ë“¤ ìœ„ì„
        self.model_loader = None
        self.model_interface = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_factory = None
        self.dependency_manager = getattr(self._mixin, 'dependency_manager', None)
        
        # BaseStepMixin í˜¸í™˜ í”Œë˜ê·¸ë“¤
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # ===== 5. Step 03 íŠ¹í™” ì†ì„±ë“¤ (5.5GB AI ëª¨ë¸) =====
        self.ai_models = {}  # ì‹¤ì œ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.model_paths = {}  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
        self.available_methods = []
        self.rembg_sessions = {}
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ
        self.models_loading_status = {
            'sam_huge': False,          # sam_vit_h_4b8939.pth (2445.7MB)
            'u2net_cloth': False,       # u2net.pth (168.1MB)
            'mobile_sam': False,        # mobile_sam.pt (38.8MB)
            'isnet': False,             # isnetis.onnx (168.1MB)
        }
        
        # ===== 6. M3 Max ê°ì§€ ë° ìµœì í™” =====
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = kwargs.get('memory_gb', 128.0 if self.is_m3_max else 16.0)
        
        # ===== 7. í†µê³„ ë° ìºì‹œ ì´ˆê¸°í™” =====
        self.processing_stats = {
            'total_processed': 0,
            'successful_segmentations': 0,
            'failed_segmentations': 0,
            'average_time': 0.0,
            'average_quality': 0.0,
            'method_usage': {},
            'cache_hits': 0,
            'ai_model_calls': 0,
            'sam_huge_calls': 0,
            'u2net_calls': 0,
            'mobile_sam_calls': 0,
            'isnet_calls': 0,
            'hybrid_calls': 0
        }
        
        self.segmentation_cache = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(
            max_workers=4 if self.is_m3_max else 2,
            thread_name_prefix="cloth_seg_ai"
        )
        
        # ===== 8. ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ (BaseStepMixin v16.0) =====
        if self.dependency_manager and hasattr(self.dependency_manager, 'auto_inject_dependencies'):
            try:
                self.dependency_manager.auto_inject_dependencies()
                self.logger.info("âœ… BaseStepMixin v16.0 ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        self.logger.info(f"âœ… {self.step_name} ì™„ì „ AI ì—°ë™ + BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   - Device: {self.device}")
        self.logger.info(f"   - M3 Max: {self.is_m3_max}")
        self.logger.info(f"   - Memory: {self.memory_gb}GB")

    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except Exception:
            return "cpu"

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True
                )
                cpu_info = result.stdout.strip()
                return 'M3 Max' in cpu_info or 'M3' in cpu_info
        except Exception:
            pass
        return False

    # ==============================================
    # ğŸ”¥ 10. BaseStepMixin v16.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            if self._mixin:
                self._mixin.set_model_loader(model_loader)
            
            self.has_model = True
            self.model_loaded = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoader íŒ¨í„´)
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
                
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            if self._mixin:
                self._mixin.set_memory_manager(memory_manager)
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            if self._mixin:
                self._mixin.set_data_converter(data_converter)
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.di_container = di_container
            if self._mixin:
                self._mixin.set_di_container(di_container)
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    def set_step_factory(self, step_factory):
        """StepFactory ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            self.step_factory = step_factory
            if self._mixin:
                self._mixin.set_step_factory(step_factory)
            self.logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

    # ==============================================
    # ğŸ”¥ 11. BaseStepMixin v16.0 í˜¸í™˜ í‘œì¤€ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model(self, model_name: str = "default"):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (BaseStepMixin v16.0 í˜¸í™˜)"""
        if model_name == "default" or model_name == "sam_huge":
            return self.ai_models.get('sam_huge')
        elif model_name == "u2net" or model_name == "u2net_cloth":
            return self.ai_models.get('u2net_cloth')
        elif model_name == "mobile_sam":
            return self.ai_models.get('mobile_sam')
        elif model_name == "isnet":
            return self.ai_models.get('isnet')
        else:
            return self.ai_models.get(model_name)

    async def get_model_async(self, model_name: str = "default"):
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.get_model(model_name)

    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            initial_memory = self._get_memory_usage()
            
            # ìºì‹œ ì •ë¦¬
            if aggressive:
                with self.cache_lock:
                    self.segmentation_cache.clear()
                self.logger.info("ğŸ§¹ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            # AI ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            if aggressive:
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        self.logger.debug(f"ğŸ§¹ {model_name} ëª¨ë¸ CPU ì´ë™")
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ {model_name} CPU ì´ë™ ì‹¤íŒ¨: {e}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # BaseStepMixin ë©”ëª¨ë¦¬ ìµœì í™”
            if self._mixin and hasattr(self._mixin, 'optimize_memory'):
                mixin_result = self._mixin.optimize_memory(aggressive)
            
            final_memory = self._get_memory_usage()
            
            return {
                'success': True,
                'initial_memory': initial_memory,
                'final_memory': final_memory,
                'memory_freed': initial_memory - final_memory,
                'cache_cleared': aggressive,
                'ai_models_count': len(self.ai_models),
                'basestepmixin_v16_compatible': True
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.optimize_memory(aggressive)

    def warmup(self, **kwargs) -> Dict[str, Any]:
        """ì›Œë°ì—… (BaseStepMixin v16.0 í˜¸í™˜)"""
        try:
            if self.warmup_completed:
                return {'success': True, 'already_warmed': True}
                
            # AI ëª¨ë¸ë“¤ë¡œ ì›Œë°ì—…
            warmed_models = []
            if TORCH_AVAILABLE and self.ai_models:
                dummy_input = torch.randn(1, 3, 512, 512, device=self.device)
                
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'eval'):
                            model.eval()
                            if hasattr(model, 'forward'):
                                with torch.no_grad():
                                    _ = model(dummy_input)
                                warmed_models.append(model_name)
                            self.logger.debug(f"âœ… {model_name} AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # BaseStepMixin ì›Œë°ì—…
            if self._mixin and hasattr(self._mixin, 'warmup'):
                mixin_result = self._mixin.warmup(**kwargs)
            
            self.warmup_completed = True
            return {
                'success': True, 
                'warmed_ai_models': warmed_models,
                'total_ai_models': len(self.ai_models),
                'basestepmixin_v16_compatible': True
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def warmup_async(self, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—… (BaseStepMixin v16.0 í˜¸í™˜)"""
        return self.warmup(**kwargs)

    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì •ë³´ ë°˜í™˜ (BaseStepMixin v16.0 í˜¸í™˜)"""
        base_status = {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'has_model': self.has_model,
            'model_loaded': self.model_loaded,
            'warmup_completed': self.warmup_completed,
            'device': self.device,
            'basestepmixin_v16_compatible': True,
            'opencv_replaced': True,  # OpenCV ì™„ì „ ëŒ€ì²´ë¨
            'ai_models_loaded': list(self.ai_models.keys()),
            'ai_models_status': self.models_loading_status.copy(),
            'available_methods': [m.value for m in self.available_methods],
            'processing_stats': self.processing_stats.copy(),
            'is_m3_max': self.is_m3_max,
            'memory_gb': self.memory_gb
        }
        
        # BaseStepMixin ìƒíƒœ ì¶”ê°€
        if self._mixin and hasattr(self._mixin, 'get_status'):
            mixin_status = self._mixin.get_status()
            base_status.update(mixin_status)
        
        return base_status

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ (BaseStepMixin v16.0 í˜¸í™˜)"""
        ai_summary = {
            'total_processed': self.processing_stats['total_processed'],
            'success_rate': (
                self.processing_stats['successful_segmentations'] / 
                max(self.processing_stats['total_processed'], 1)
            ),
            'average_time': self.processing_stats['average_time'],
            'average_quality': self.processing_stats['average_quality'],
            'cache_hit_rate': (
                self.processing_stats['cache_hits'] / 
                max(self.processing_stats['total_processed'], 1)
            ),
            'ai_model_calls': self.processing_stats['ai_model_calls'],
            'method_usage': self.processing_stats['method_usage'],
            'ai_model_usage': {
                'sam_huge_calls': self.processing_stats['sam_huge_calls'],
                'u2net_calls': self.processing_stats['u2net_calls'],
                'mobile_sam_calls': self.processing_stats['mobile_sam_calls'],
                'isnet_calls': self.processing_stats['isnet_calls'],
                'hybrid_calls': self.processing_stats['hybrid_calls']
            },
            'basestepmixin_v16_compatible': True,
            'opencv_replaced': True
        }
        
        # BaseStepMixin ì„±ëŠ¥ ìš”ì•½ ì¶”ê°€
        if self._mixin and hasattr(self._mixin, 'get_performance_summary'):
            mixin_summary = self._mixin.get_performance_summary()
            ai_summary.update(mixin_summary)
        
        return ai_summary

    def record_processing(self, processing_time: float, success: bool, **metrics):
        """ì²˜ë¦¬ ê¸°ë¡ (BaseStepMixin v16.0 í˜¸í™˜)"""
        self.processing_stats['total_processed'] += 1
        if success:
            self.processing_stats['successful_segmentations'] += 1
        else:
            self.processing_stats['failed_segmentations'] += 1
        
        # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.processing_stats['total_processed']
        current_avg = self.processing_stats['average_time']
        self.processing_stats['average_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if 'quality' in metrics:
            current_quality_avg = self.processing_stats['average_quality']
            self.processing_stats['average_quality'] = (
                (current_quality_avg * (total - 1) + metrics['quality']) / total
            )
        
        # AI ëª¨ë¸ í˜¸ì¶œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        if 'method_used' in metrics:
            method = metrics['method_used']
            self.processing_stats['method_usage'][method] = (
                self.processing_stats['method_usage'].get(method, 0) + 1
            )
        
        # BaseStepMixin ê¸°ë¡
        if self._mixin and hasattr(self._mixin, 'record_processing'):
            self._mixin.record_processing(processing_time, success, **metrics)

    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            return 0.0

    # ==============================================
    # ğŸ”¥ 12. ì´ˆê¸°í™” ë©”ì„œë“œ (5.5GB AI ëª¨ë¸ ë¡œë”©)
    # ==============================================
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” - ì‹¤ì œ 5.5GB AI ëª¨ë¸ ë¡œë”© + BaseStepMixin v16.0 í˜¸í™˜"""
        try:
            self.logger.info("ğŸ”„ ClothSegmentationStep ì™„ì „ AI ì´ˆê¸°í™” ì‹œì‘ (5.5GB ëª¨ë¸)")
            
            # ===== 1. BaseStepMixin v16.0 ì´ˆê¸°í™” =====
            if self._mixin and hasattr(self._mixin, 'initialize'):
                try:
                    await self._mixin.initialize()
                    self.logger.info("âœ… BaseStepMixin v16.0 ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ===== 2. ëª¨ë¸ ê²½ë¡œ íƒì§€ (SmartModelPathMapper ê¸°ë°˜) =====
            await self._detect_model_paths()
            
            # ===== 3. ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (5.5GB) =====
            await self._load_all_ai_models()
            
            # ===== 4. RemBG ì„¸ì…˜ ì´ˆê¸°í™” =====
            if REMBG_AVAILABLE:
                await self._initialize_rembg_sessions()
            
            # ===== 5. M3 Max ìµœì í™” ì›Œë°ì—… =====
            if self.is_m3_max:
                await self._warmup_m3_max_ai_models()
            
            # ===== 6. ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²• ê°ì§€ =====
            self.available_methods = self._detect_available_ai_methods()
            if not self.available_methods:
                self.logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤")
                self.available_methods = [SegmentationMethod.AUTO_AI]
            
            # ===== 7. BaseStepMixin v16.0 í˜¸í™˜ í”Œë˜ê·¸ ì„¤ì • =====
            self.is_initialized = True
            self.is_ready = True
            self.warmup_completed = True
            
            # ===== 8. ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸ =====
            loaded_models = list(self.ai_models.keys())
            total_size_mb = sum(
                2445.7 if 'sam_huge' in model else
                168.1 if 'u2net' in model else
                38.8 if 'mobile_sam' in model else
                168.1 if 'isnet' in model else 0
                for model in loaded_models
            )
            
            self.logger.info("âœ… ClothSegmentationStep ì™„ì „ AI ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            self.logger.info(f"   - ì´ ëª¨ë¸ í¬ê¸°: {total_size_mb:.1f}MB")
            self.logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•: {[m.value for m in self.available_methods]}")
            self.logger.info(f"   - BaseStepMixin v16.0 í˜¸í™˜: âœ…")
            self.logger.info(f"   - OpenCV ì™„ì „ ëŒ€ì²´: âœ…")
            self.logger.info(f"   - M3 Max ìµœì í™”: {'âœ…' if self.is_m3_max else 'âŒ'}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „ AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.is_ready = False
            return False

    async def _detect_model_paths(self):
        """ëª¨ë¸ ê²½ë¡œ íƒì§€ (PDF ê¸°ë°˜ SmartModelPathMapper)"""
        try:
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹œì‘...")
            
            # ê¸°ë³¸ ê²½ë¡œë“¤
            base_paths = [
                "ai_models/step_03_cloth_segmentation/",
                "ai_models/step_03_cloth_segmentation/ultra_models/",
                "models/step_03_cloth_segmentation/",
                "checkpoints/step_03_cloth_segmentation/"
            ]
            
            # ModelLoaderë¥¼ í†µí•œ ê²½ë¡œ íƒì§€
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                try:
                    for model_key in ['sam_huge', 'u2net_cloth', 'mobile_sam', 'isnet']:
                        try:
                            model_path = self.model_loader.get_model_path(f"step_03_{model_key}")
                            if model_path and os.path.exists(model_path):
                                self.model_paths[model_key] = model_path
                                self.logger.info(f"âœ… ModelLoaderì—ì„œ {model_key} ê²½ë¡œ ë°œê²¬: {model_path}")
                        except Exception as e:
                            self.logger.debug(f"ModelLoader {model_key} ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    self.logger.debug(f"ModelLoader ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ íŒŒì¼ íƒì§€
            model_files = {
                'sam_huge': 'sam_vit_h_4b8939.pth',
                'u2net_cloth': 'u2net.pth',
                'mobile_sam': 'mobile_sam.pt',
                'isnet': 'isnetis.onnx'
            }
            
            for model_key, filename in model_files.items():
                if model_key in self.model_paths:
                    continue  # ì´ë¯¸ ë°œê²¬ë¨
                
                for base_path in base_paths:
                    full_path = os.path.join(base_path, filename)
                    if os.path.exists(full_path):
                        self.model_paths[model_key] = full_path
                        file_size = os.path.getsize(full_path) / (1024 * 1024)  # MB
                        self.logger.info(f"âœ… {model_key} ë°œê²¬: {full_path} ({file_size:.1f}MB)")
                        break
                else:
                    self.logger.warning(f"âš ï¸ {model_key} íŒŒì¼ ì—†ìŒ: {filename}")
            
            if not self.model_paths:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")

    async def _load_all_ai_models(self):
        """ëª¨ë“  AI ëª¨ë¸ ë¡œë”© (5.5GB)"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
                return
            
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘ (5.5GB)...")
            
            # ===== SAM Huge ë¡œë”© (2445.7MB) =====
            if 'sam_huge' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ SAM Huge ë¡œë”© ì¤‘ (2445.7MB)...")
                    sam_model = RealSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['sam_huge'],
                        device=self.device,
                        model_type="vit_h"
                    )
                    if sam_model.is_loaded:
                        self.ai_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        self.logger.info("âœ… SAM Huge ë¡œë”© ì™„ë£Œ (2445.7MB)")
                    else:
                        self.logger.warning("âš ï¸ SAM Huge ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ SAM Huge ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== U2Net Cloth ë¡œë”© (168.1MB) =====
            if 'u2net_cloth' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ U2Net Cloth ë¡œë”© ì¤‘ (168.1MB)...")
                    u2net_model = RealU2NetClothModel.from_checkpoint(
                        checkpoint_path=self.model_paths['u2net_cloth'],
                        device=self.device
                    )
                    self.ai_models['u2net_cloth'] = u2net_model
                    self.models_loading_status['u2net_cloth'] = True
                    self.logger.info(f"âœ… U2Net Cloth ë¡œë”© ì™„ë£Œ (168.1MB) - íŒŒë¼ë¯¸í„°: {u2net_model.parameter_count:,}")
                except Exception as e:
                    self.logger.error(f"âŒ U2Net Cloth ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== Mobile SAM ë¡œë”© (38.8MB) =====
            if 'mobile_sam' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ Mobile SAM ë¡œë”© ì¤‘ (38.8MB)...")
                    mobile_sam_model = RealMobileSAMModel.from_checkpoint(
                        checkpoint_path=self.model_paths['mobile_sam'],
                        device=self.device
                    )
                    if mobile_sam_model.is_loaded:
                        self.ai_models['mobile_sam'] = mobile_sam_model
                        self.models_loading_status['mobile_sam'] = True
                        self.logger.info("âœ… Mobile SAM ë¡œë”© ì™„ë£Œ (38.8MB)")
                    else:
                        self.logger.warning("âš ï¸ Mobile SAM ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ Mobile SAM ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== ISNet ë¡œë”© (168.1MB) =====
            if 'isnet' in self.model_paths:
                try:
                    self.logger.info("ğŸ”„ ISNet ë¡œë”© ì¤‘ (168.1MB)...")
                    isnet_model = RealISNetModel.from_checkpoint(
                        onnx_path=self.model_paths['isnet']
                    )
                    if isnet_model.is_loaded:
                        self.ai_models['isnet'] = isnet_model
                        self.models_loading_status['isnet'] = True
                        self.logger.info("âœ… ISNet ë¡œë”© ì™„ë£Œ (168.1MB)")
                    else:
                        self.logger.warning("âš ï¸ ISNet ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ ISNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ===== í´ë°± ëª¨ë¸ ìƒì„± (AI ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°) =====
            if not self.ai_models:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ë”ë¯¸ ëª¨ë¸ ìƒì„±")
                try:
                    # ê¸°ë³¸ U2Net ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ì´)
                    dummy_u2net = RealU2NetClothModel(in_ch=3, out_ch=1).to(self.device)
                    dummy_u2net.eval()
                    self.ai_models['u2net_cloth'] = dummy_u2net
                    self.models_loading_status['u2net_cloth'] = True
                    self.logger.info("âœ… ë”ë¯¸ U2Net ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ ë”ë¯¸ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            
            # ===== ë¡œë”© ê²°ê³¼ ìš”ì•½ =====
            loaded_count = sum(self.models_loading_status.values())
            total_models = len(self.models_loading_status)
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    async def _initialize_rembg_sessions(self):
        """RemBG ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            if not REMBG_AVAILABLE:
                return
            
            self.logger.info("ğŸ”„ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘...")
            
            session_configs = {
                'u2net': 'u2net',
                'u2netp': 'u2netp',
                'silueta': 'silueta',
            }
            
            for name, model_name in session_configs.items():
                try:
                    session = new_session(model_name)
                    self.rembg_sessions[name] = session
                    self.logger.info(f"âœ… RemBG ì„¸ì…˜ ìƒì„±: {name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ {name} ìƒì„± ì‹¤íŒ¨: {e}")
            
            if self.rembg_sessions:
                self.default_rembg_session = (
                    self.rembg_sessions.get('u2net') or
                    list(self.rembg_sessions.values())[0]
                )
                self.logger.info("âœ… RemBG ê¸°ë³¸ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ RemBG ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _warmup_m3_max_ai_models(self):
        """M3 Max AI ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if not self.is_m3_max or not TORCH_AVAILABLE:
                return
            
            self.logger.info("ğŸ”¥ M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ í…ì„œë¡œ ì›Œë°ì—…
            dummy_input = torch.randn(1, 3, 512, 512, device=self.device)
            
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                        with torch.no_grad():
                            if hasattr(model, 'forward') and model_name != 'sam_huge':
                                _ = model(dummy_input)
                            elif callable(model):
                                _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} M3 Max ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # MPS ìºì‹œ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… M3 Max AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    def _detect_available_ai_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        # ë¡œë“œëœ AI ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ë°©ë²• ê²°ì •
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
            self.logger.info("âœ… SAM_HUGE ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (2445.7MB AI ëª¨ë¸)")
        
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
            self.logger.info("âœ… U2NET_CLOTH ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (168.1MB AI ëª¨ë¸)")
        
        if 'mobile_sam' in self.ai_models:
            methods.append(SegmentationMethod.MOBILE_SAM)
            self.logger.info("âœ… MOBILE_SAM ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (38.8MB AI ëª¨ë¸)")
        
        if 'isnet' in self.ai_models:
            methods.append(SegmentationMethod.ISNET)
            self.logger.info("âœ… ISNET ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ (168.1MB ONNX ëª¨ë¸)")
        
        # AUTO_AI ë°©ë²• (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
        if methods:
            methods.append(SegmentationMethod.AUTO_AI)
            self.logger.info("âœ… AUTO_AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        # HYBRID_AI ë°©ë²• (2ê°œ ì´ìƒ AI ë°©ë²•ì´ ìˆì„ ë•Œ)
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
            self.logger.info("âœ… HYBRID_AI ë°©ë²• ì‚¬ìš© ê°€ëŠ¥")
        
        return methods

    # ==============================================
    # ğŸ”¥ 13. í•µì‹¬: process ë©”ì„œë“œ (ì‹¤ì œ AI ì¶”ë¡  + Stepê°„ ì—°ë™)
    # ==============================================
    
    async def process(
        self,
        input_data: Union[StepInputData, str, np.ndarray, Image.Image, Dict[str, Any]],
        clothing_type: Optional[str] = None,
        quality_level: Optional[str] = None,
        **kwargs
    ) -> Union[StepOutputData, Dict[str, Any]]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì™„ì „ AI ì¶”ë¡  + BaseStepMixin v16.0 í˜¸í™˜ + Stepê°„ ì—°ë™"""
        
        if not self.is_initialized:
            if not await self.initialize():
                return self._create_error_result("ì™„ì „ AI ì´ˆê¸°í™” ì‹¤íŒ¨")

        start_time = time.time()
        
        try:
            self.logger.info("ğŸ”„ ì™„ì „ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹œì‘ (5.5GB ëª¨ë¸)")
            
            # ===== 1. ì…ë ¥ ë°ì´í„° í‘œì¤€í™” (Stepê°„ ì—°ë™) =====
            standardized_input = self._standardize_input(input_data, clothing_type, **kwargs)
            if not standardized_input:
                return self._create_error_result("ì…ë ¥ ë°ì´í„° í‘œì¤€í™” ì‹¤íŒ¨")
            
            image = standardized_input['image']
            metadata = standardized_input['metadata']
            
            # ===== 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (AI ê¸°ë°˜) =====
            processed_image = self._preprocess_image_ai(image)
            if processed_image is None:
                return self._create_error_result("AI ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ===== 3. ì˜ë¥˜ íƒ€ì… ê°ì§€ (AI ê¸°ë°˜) =====
            detected_clothing_type = await self._detect_clothing_type_ai(processed_image, clothing_type)
            
            # ===== 4. í’ˆì§ˆ ë ˆë²¨ ì„¤ì • =====
            quality = QualityLevel(quality_level or self.segmentation_config.quality_level.value)
            
            # ===== 5. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (5.5GB ëª¨ë¸ í™œìš©) =====
            self.logger.info("ğŸ§  ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘ (SAM + U2Net + ISNet + Mobile SAM)...")
            mask, confidence, method_used = await self._run_complete_ai_segmentation(
                processed_image, detected_clothing_type, quality
            )
            
            if mask is None:
                return self._create_error_result("ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨")
            
            # ===== 6. AI ê¸°ë°˜ í›„ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´) =====
            final_mask = self._post_process_mask_ai(mask, quality)
            
            # ===== 7. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (AI ê°•í™”) =====
            visualizations = {}
            if self.segmentation_config.enable_visualization:
                self.logger.info("ğŸ¨ AI ê°•í™” ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±...")
                visualizations = self._create_ai_visualizations(
                    processed_image, final_mask, detected_clothing_type
                )
            
            # ===== 8. Stepê°„ ì—°ë™ì„ ìœ„í•œ ê²°ê³¼ ë°ì´í„° ìƒì„± =====
            processing_time = time.time() - start_time
            
            # Stepê°„ í‘œì¤€ ì¶œë ¥ ë°ì´í„° ìƒì„±
            step_output = StepOutputData(
                success=True,
                result_data={
                    'mask': final_mask,
                    'segmented_image': self._apply_mask_to_image(processed_image, final_mask),
                    'confidence': confidence,
                    'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                    'method_used': method_used,
                    'ai_models_used': list(self.ai_models.keys()),
                    'processing_time': processing_time,
                    'quality_score': confidence * 0.9,  # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    'mask_area_ratio': np.sum(final_mask > 0) / final_mask.size,
                    'boundary_smoothness': self._calculate_boundary_smoothness(final_mask)
                },
                metadata={
                    'device': self.device,
                    'quality_level': quality.value,
                    'ai_models_used': list(self.ai_models.keys()),
                    'model_file_paths': self.model_paths.copy(),
                    'image_size': processed_image.size if hasattr(processed_image, 'size') else (512, 512),
                    'ai_inference': True,
                    'opencv_replaced': True,
                    'model_loader_used': self.model_loader is not None,
                    'is_m3_max': self.is_m3_max,
                    'basestepmixin_v16_compatible': True,
                    'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                    'step_integration_complete': True,
                    'total_model_size_mb': sum(
                        2445.7 if 'sam_huge' in model else
                        168.1 if 'u2net' in model else
                        38.8 if 'mobile_sam' in model else
                        168.1 if 'isnet' in model else 0
                        for model in self.ai_models.keys()
                    ),
                    **metadata  # ì›ë³¸ ë©”íƒ€ë°ì´í„° í¬í•¨
                },
                step_name=self.step_name,
                processing_time=processing_time,
                next_step_input={
                    'segmented_image': self._apply_mask_to_image(processed_image, final_mask),
                    'mask': final_mask,
                    'clothing_type': detected_clothing_type.value if hasattr(detected_clothing_type, 'value') else str(detected_clothing_type),
                    'confidence': confidence,
                    'step_03_metadata': {
                        'ai_models_used': list(self.ai_models.keys()),
                        'method_used': method_used,
                        'quality_level': quality.value,
                        'processing_time': processing_time
                    }
                }
            )
            
            # ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            if visualizations:
                if 'visualization' in visualizations:
                    step_output.result_data['visualization_base64'] = self._image_to_base64(visualizations['visualization'])
                if 'overlay' in visualizations:
                    step_output.result_data['overlay_base64'] = self._image_to_base64(visualizations['overlay'])
            
            # ===== 9. í†µê³„ ì—…ë°ì´íŠ¸ (BaseStepMixin v16.0 í˜¸í™˜) =====
            self.record_processing(processing_time, True, quality=confidence, method_used=method_used)
            
            # AI ëª¨ë¸ë³„ í˜¸ì¶œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            if 'sam_huge' in method_used:
                self.processing_stats['sam_huge_calls'] += 1
            if 'u2net' in method_used:
                self.processing_stats['u2net_calls'] += 1
            if 'mobile_sam' in method_used:
                self.processing_stats['mobile_sam_calls'] += 1
            if 'isnet' in method_used:
                self.processing_stats['isnet_calls'] += 1
            if 'hybrid' in method_used:
                self.processing_stats['hybrid_calls'] += 1
            
            self.processing_stats['ai_model_calls'] += 1
            
            self.logger.info(f"âœ… ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ - {processing_time:.2f}ì´ˆ")
            self.logger.info(f"   - AI ëª¨ë¸ ì‚¬ìš©: {list(self.ai_models.keys())}")
            self.logger.info(f"   - ë°©ë²•: {method_used}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {confidence:.3f}")
            self.logger.info(f"   - BaseStepMixin v16.0 í˜¸í™˜: âœ…")
            self.logger.info(f"   - OpenCV ì™„ì „ ëŒ€ì²´: âœ…")
            
            return step_output
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.record_processing(processing_time, False)
            
            self.logger.error(f"âŒ ì™„ì „ AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"ì™„ì „ AI ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    def _standardize_input(self, input_data, clothing_type=None, **kwargs) -> Optional[Dict[str, Any]]:
        """ì…ë ¥ ë°ì´í„° í‘œì¤€í™” (Stepê°„ ì—°ë™)"""
        try:
            # StepInputData íƒ€ì…ì¸ ê²½ìš°
            if isinstance(input_data, StepInputData):
                return {
                    'image': input_data.image,
                    'metadata': {
                        **input_data.metadata,
                        'clothing_type': clothing_type or input_data.metadata.get('clothing_type'),
                        'step_history': input_data.step_history,
                        'processing_context': input_data.processing_context
                    }
                }
            
            # Dict íƒ€ì…ì¸ ê²½ìš° (ë‹¤ë¥¸ Stepì—ì„œ ì˜¤ëŠ” ê²½ìš°)
            elif isinstance(input_data, dict):
                image = input_data.get('image') or input_data.get('segmented_image') or input_data.get('result_image')
                if image is None:
                    self.logger.error("âŒ Dict ì…ë ¥ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
                return {
                    'image': image,
                    'metadata': {
                        'clothing_type': clothing_type or input_data.get('clothing_type'),
                        'previous_step_data': input_data,
                        **kwargs
                    }
                }
            
            # ì§ì ‘ì ì¸ ì´ë¯¸ì§€ ë°ì´í„°ì¸ ê²½ìš°
            else:
                return {
                    'image': input_data,
                    'metadata': {
                        'clothing_type': clothing_type,
                        **kwargs
                    }
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ë°ì´í„° í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return None

    def _preprocess_image_ai(self, image) -> Optional[Image.Image]:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenCV ëŒ€ì²´)"""
        try:
            # ì…ë ¥ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, str):
                if image.startswith('data:image'):
                    # Base64
                    header, data = image.split(',', 1)
                    image_data = base64.b64decode(data)
                    image = Image.open(BytesIO(image_data))
                else:
                    # íŒŒì¼ ê²½ë¡œ
                    image = Image.open(image)
            elif isinstance(image, np.ndarray):
                if image.shape[2] == 3:  # RGB
                    image = Image.fromarray(image)
                elif image.shape[2] == 4:  # RGBA
                    image = Image.fromarray(image).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image.shape}")
            elif not isinstance(image, Image.Image):
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # AI ê¸°ë°˜ í¬ê¸° ì¡°ì • (Real-ESRGAN í™œìš©)
            target_size = self.segmentation_config.input_size
            if image.size != target_size:
                image = AIImageProcessor.ai_resize(image, target_size)
            
            return image
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None

    async def _detect_clothing_type_ai(self, image: Image.Image, hint: Optional[str] = None) -> ClothingType:
        """AI ê¸°ë°˜ ì˜ë¥˜ íƒ€ì… ê°ì§€ (CLIP ëª¨ë¸ í™œìš©)"""
        try:
            if hint:
                try:
                    return ClothingType(hint.lower())
                except ValueError:
                    pass
            
            # CLIP ê¸°ë°˜ ì˜ë¥˜ ë¶„ë¥˜ ì‹œë„
            if TRANSFORMERS_AVAILABLE:
                try:
                    # ê°„ë‹¨í•œ ì˜ë¥˜ ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ CLIP íŒŒì´í”„ë¼ì¸ í•„ìš”)
                    clothing_candidates = [
                        "shirt", "dress", "pants", "skirt", "jacket", 
                        "sweater", "coat", "top", "bottom"
                    ]
                    
                    # ì´ë¯¸ì§€ ì¢…íš¡ë¹„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± (ì„ì‹œ)
                    width, height = image.size
                    aspect_ratio = height / width
                    
                    if aspect_ratio > 1.5:
                        return ClothingType.DRESS
                    elif aspect_ratio > 1.2:
                        return ClothingType.SHIRT
                    else:
                        return ClothingType.PANTS
                        
                except Exception as e:
                    self.logger.debug(f"CLIP ì˜ë¥˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
            if hasattr(image, 'size'):
                width, height = image.size
                aspect_ratio = height / width
                
                if aspect_ratio > 1.5:
                    return ClothingType.DRESS
                elif aspect_ratio > 1.2:
                    return ClothingType.SHIRT
                else:
                    return ClothingType.PANTS
            
            return ClothingType.UNKNOWN
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì˜ë¥˜ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return ClothingType.UNKNOWN

    async def _run_complete_ai_segmentation(
        self,
        image: Image.Image,
        clothing_type: ClothingType,
        quality: QualityLevel
    ) -> Tuple[Optional[np.ndarray], float, str]:
        """ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (5.5GB ëª¨ë¸ í™œìš©)"""
        try:
            # í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ì„ íƒ
            ai_methods = self._get_ai_methods_by_quality(quality)
            
            for method in ai_methods:
                try:
                    self.logger.info(f"ğŸ§  AI ë°©ë²• ì‹œë„: {method.value}")
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    
                    if mask is not None:
                        self.processing_stats['ai_model_calls'] += 1
                        self.processing_stats['method_usage'][method.value] = (
                            self.processing_stats['method_usage'].get(method.value, 0) + 1
                        )
                        
                        self.logger.info(f"âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {method.value} (ì‹ ë¢°ë„: {confidence:.3f})")
                        return mask, confidence, method.value
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ AI ë°©ë²• {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±
            self.logger.warning("âš ï¸ ëª¨ë“  AI ë°©ë²• ì‹¤íŒ¨, ë”ë¯¸ ë§ˆìŠ¤í¬ ìƒì„±")
            dummy_mask = np.ones((512, 512), dtype=np.uint8) * 128
            return dummy_mask, 0.5, "fallback_dummy"
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, 0.0, "error"

    def _get_ai_methods_by_quality(self, quality: QualityLevel) -> List[SegmentationMethod]:
        """í’ˆì§ˆ ë ˆë²¨ë³„ AI ë°©ë²• ìš°ì„ ìˆœìœ„"""
        available_ai_methods = [
            method for method in self.available_methods
            if method not in [SegmentationMethod.AUTO_AI]
        ]
        
        if quality == QualityLevel.ULTRA:
            priority = [
                SegmentationMethod.HYBRID_AI,    # ëª¨ë“  AI ëª¨ë¸ ì¡°í•©
                SegmentationMethod.SAM_HUGE,     # ìµœê³  ì„±ëŠ¥ (2445.7MB)
                SegmentationMethod.U2NET_CLOTH,  # ì˜ë¥˜ íŠ¹í™”
                SegmentationMethod.ISNET,        # ê³ ì •ë°€
                SegmentationMethod.MOBILE_SAM,   # ê²½ëŸ‰
            ]
        elif quality == QualityLevel.HIGH:
            priority = [
                SegmentationMethod.SAM_HUGE,     # ìµœê³  ì„±ëŠ¥
                SegmentationMethod.U2NET_CLOTH,  # ì˜ë¥˜ íŠ¹í™”
                SegmentationMethod.HYBRID_AI,    # ì¡°í•©
                SegmentationMethod.ISNET,        # ê³ ì •ë°€
            ]
        elif quality == QualityLevel.BALANCED:
            priority = [
                SegmentationMethod.U2NET_CLOTH,  # ì˜ë¥˜ íŠ¹í™”
                SegmentationMethod.ISNET,        # ê³ ì •ë°€
                SegmentationMethod.SAM_HUGE,     # ìµœê³  ì„±ëŠ¥
            ]
        else:  # FAST
            priority = [
                SegmentationMethod.MOBILE_SAM,   # ê²½ëŸ‰ ê³ ì†
                SegmentationMethod.U2NET_CLOTH,  # ì˜ë¥˜ íŠ¹í™”
            ]
        
        return [method for method in priority if method in available_ai_methods]

    async def _run_ai_method(
        self,
        method: SegmentationMethod,
        image: Image.Image,
        clothing_type: ClothingType
    ) -> Tuple[Optional[np.ndarray], float]:
        """ê°œë³„ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì‹¤í–‰"""
        
        if method == SegmentationMethod.SAM_HUGE:
            return await self._run_sam_huge_inference(image, clothing_type)
        elif method == SegmentationMethod.U2NET_CLOTH:
            return await self._run_u2net_cloth_inference(image)
        elif method == SegmentationMethod.MOBILE_SAM:
            return await self._run_mobile_sam_inference(image)
        elif method == SegmentationMethod.ISNET:
            return await self._run_isnet_inference(image)
        elif method == SegmentationMethod.HYBRID_AI:
            return await self._run_hybrid_ai_inference(image, clothing_type)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ë°©ë²•: {method}")

    async def _run_sam_huge_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """SAM Huge ì‹¤ì œ AI ì¶”ë¡  (sam_vit_h_4b8939.pth 2445.7MB)"""
        try:
            if 'sam_huge' not in self.ai_models:
                raise RuntimeError("âŒ SAM Huge ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            sam_model = self.ai_models['sam_huge']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
            
            # ğŸ”¥ ì‹¤ì œ SAM Huge AI ì¶”ë¡  (2445.7MB ëª¨ë¸)
            clothing_results = sam_model.segment_clothing(image_array, clothing_type.value)
            
            if not clothing_results:
                # ê¸°ë³¸ ì¤‘ì•™ í¬ì¸íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë„
                height, width = image_array.shape[:2]
                center_points = [[width//2, height//2]]
                
                sam_model.predictor.set_image(image_array)
                masks, scores, logits = sam_model.predictor.predict(
                    point_coords=np.array(center_points),
                    point_labels=np.ones(len(center_points)),
                    multimask_output=True
                )
                
                best_mask_idx = np.argmax(scores)
                mask = masks[best_mask_idx].astype(np.uint8)
                confidence = float(scores[best_mask_idx])
            else:
                # ì˜ë¥˜ë³„ ë§ˆìŠ¤í¬ ì¡°í•©
                combined_mask = np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)
                total_confidence = 0.0
                
                for cloth_area, area_mask in clothing_results.items():
                    combined_mask = np.logical_or(combined_mask, area_mask).astype(np.uint8)
                    total_confidence += np.sum(area_mask) / area_mask.size
                
                mask = combined_mask
                confidence = min(total_confidence / len(clothing_results), 1.0)
            
            self.logger.info(f"âœ… SAM Huge AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ SAM Huge AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_u2net_cloth_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """U2Net Cloth ì‹¤ì œ AI ì¶”ë¡  (u2net.pth 168.1MB ì˜ë¥˜ íŠ¹í™”)"""
        try:
            if 'u2net_cloth' not in self.ai_models:
                raise RuntimeError("âŒ U2Net Cloth ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['u2net_cloth']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ U2Net Cloth AI ì¶”ë¡  (168.1MB ì˜ë¥˜ íŠ¹í™” ëª¨ë¸)
            model.eval()
            with torch.no_grad():
                if self.is_m3_max and self.segmentation_config.use_fp16:
                    with torch.autocast(device_type='cpu'):
                        output = model(input_tensor)
                else:
                    output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬ (d0, d1, d2, d3, d4, d5, d6)
                if isinstance(output, tuple):
                    main_output = output[0]  # d0 (ìµœì¢… ì¶œë ¥)
                else:
                    main_output = output
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if main_output.max() > 1.0:
                    prob_map = torch.sigmoid(main_output)
                else:
                    prob_map = main_output
                
                mask = (prob_map > self.segmentation_config.confidence_threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.max().item())
            
            self.logger.info(f"âœ… U2Net Cloth AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net Cloth AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_mobile_sam_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """Mobile SAM ì‹¤ì œ AI ì¶”ë¡  (mobile_sam.pt 38.8MB)"""
        try:
            if 'mobile_sam' not in self.ai_models:
                raise RuntimeError("âŒ Mobile SAM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            model = self.ai_models['mobile_sam']
            
            if not TORCH_AVAILABLE:
                raise RuntimeError("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ğŸ”¥ ì‹¤ì œ Mobile SAM AI ì¶”ë¡  (38.8MB ê²½ëŸ‰ ëª¨ë¸)
            model.eval()
            with torch.no_grad():
                output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, tuple):
                    output = output[0]
                
                # ì‹œê·¸ëª¨ì´ë“œ ë° ì„ê³„ê°’ ì²˜ë¦¬
                if output.max() > 1.0:
                    prob_map = torch.sigmoid(output)
                else:
                    prob_map = output
                
                mask = (prob_map > self.segmentation_config.confidence_threshold).float()
                
                # CPUë¡œ ì´ë™ ë° NumPy ë³€í™˜
                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)
                confidence = float(prob_map.mean().item())  # Mobile SAMì€ í‰ê·  ì‹ ë¢°ë„ ì‚¬ìš©
            
            self.logger.info(f"âœ… Mobile SAM AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask_np, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ Mobile SAM AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_isnet_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
        """ISNet ì‹¤ì œ AI ì¶”ë¡  (isnetis.onnx 168.1MB)"""
        try:
            if 'isnet' not in self.ai_models:
                raise RuntimeError("âŒ ISNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            
            isnet_model = self.ai_models['isnet']
            
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
            
            # ğŸ”¥ ì‹¤ì œ ISNet ONNX AI ì¶”ë¡  (168.1MB ê³ ì •ë°€ ëª¨ë¸)
            mask = isnet_model.predict(image_array)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë§ˆìŠ¤í¬ í’ˆì§ˆ ê¸°ë°˜)
            if mask is not None:
                confidence = np.sum(mask > 0) / mask.size
                confidence = min(confidence * 1.2, 1.0)  # ISNetì€ ê³ ì •ë°€ì´ë¯€ë¡œ ì‹ ë¢°ë„ í–¥ìƒ
                
                # ì´ì§„í™”
                mask = (mask > 128).astype(np.uint8)
            else:
                confidence = 0.0
            
            self.logger.info(f"âœ… ISNet AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.3f}")
            return mask, confidence
            
        except Exception as e:
            self.logger.error(f"âŒ ISNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    async def _run_hybrid_ai_inference(self, image: Image.Image, clothing_type: ClothingType) -> Tuple[Optional[np.ndarray], float]:
        """HYBRID AI ì¶”ë¡  (ì—¬ëŸ¬ AI ëª¨ë¸ ì¡°í•© - 5.5GB ì „ì²´ í™œìš©)"""
        try:
            self.logger.info("ğŸ”„ HYBRID AI ì¶”ë¡  ì‹œì‘ (5.5GB ëª¨ë“  ëª¨ë¸ í™œìš©)...")
            
            masks = []
            confidences = []
            methods_used = []
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ AI ë°©ë²•ë“¤ë¡œ ì¶”ë¡  ì‹¤í–‰
            available_ai_methods = [
                method for method in self.available_methods 
                if method not in [SegmentationMethod.AUTO_AI, SegmentationMethod.HYBRID_AI]
            ]
            
            # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°©ë²•ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if len(available_ai_methods) < 2:
                raise RuntimeError("âŒ HYBRID ë°©ë²•ì€ ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ë°©ë²•ì´ í•„ìš”")
            
            # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
            for method in available_ai_methods:
                try:
                    mask, confidence = await self._run_ai_method(method, image, clothing_type)
                    if mask is not None:
                        masks.append(mask.astype(np.float32))
                        confidences.append(confidence)
                        methods_used.append(method.value)
                        self.logger.info(f"âœ… HYBRID - {method.value} ì¶”ë¡  ì™„ë£Œ: {confidence:.3f}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HYBRID - {method.value} ì‹¤íŒ¨: {e}")
                    continue
            
            if not masks:
                raise RuntimeError("âŒ HYBRID - ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
            
            # ğŸ”¥ ê³ ê¸‰ ë§ˆìŠ¤í¬ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê·  + í˜•íƒœí•™ì  í›„ì²˜ë¦¬)
            if len(masks) == 1:
                combined_mask = masks[0]
                combined_confidence = confidences[0]
            else:
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
                weights = np.array(confidences)
                weights = weights / np.sum(weights)  # ì •ê·œí™”
                
                # ë§ˆìŠ¤í¬ë“¤ì„ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤
                target_shape = masks[0].shape
                normalized_masks = []
                for mask in masks:
                    if mask.shape != target_shape:
                        # AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì¦ˆ ì‚¬ìš©
                        mask_image = Image.fromarray(mask.astype(np.uint8))
                        resized_mask = AIImageProcessor.ai_resize(mask_image, target_shape[::-1])
                        mask_resized = np.array(resized_mask).astype(np.float32)
                        normalized_masks.append(mask_resized)
                    else:
                        normalized_masks.append(mask.astype(np.float32))
                
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                combined_mask_float = np.zeros_like(normalized_masks[0])
                for mask, weight in zip(normalized_masks, weights):
                    combined_mask_float += mask * weight
                
                # AI ê¸°ë°˜ ì„ê³„ê°’ ì ìš© (Otsu ë°©ë²• ëŒ€ì‹ )
                threshold = np.mean(combined_mask_float) + np.std(combined_mask_float) * 0.5
                combined_mask = (combined_mask_float > threshold).astype(np.float32)
                combined_confidence = float(np.mean(confidences))
            
            # AI ê¸°ë°˜ í›„ì²˜ë¦¬ (OpenCV ëŒ€ì²´)
            final_mask = AIImageProcessor.ai_morphology(
                (combined_mask * 255).astype(np.uint8), 
                "closing", 
                5
            )
            
            # ìµœì¢… ì´ì§„í™”
            final_mask = (final_mask > 128).astype(np.uint8)
            
            self.logger.info(f"âœ… HYBRID AI ì¶”ë¡  ì™„ë£Œ - ë°©ë²•: {methods_used} - ì‹ ë¢°ë„: {combined_confidence:.3f}")
            return final_mask, combined_confidence
            
        except Exception as e:
            self.logger.error(f"âŒ HYBRID AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _post_process_mask_ai(self, mask: np.ndarray, quality: QualityLevel) -> np.ndarray:
        """AI ê¸°ë°˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
        try:
            processed_mask = mask.copy()
            
            # AI ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            if self.segmentation_config.remove_noise:
                kernel_size = 3 if quality == QualityLevel.FAST else 5
                processed_mask = AIImageProcessor.ai_morphology(processed_mask, "opening", kernel_size)
                processed_mask = AIImageProcessor.ai_morphology(processed_mask, "closing", kernel_size)
            
            # AI ê¸°ë°˜ ì—£ì§€ ìŠ¤ë¬´ë”©
            if self.segmentation_config.edge_smoothing:
                processed_mask_float = processed_mask.astype(np.float32) / 255.0
                smoothed = AIImageProcessor.ai_gaussian_blur(
                    processed_mask_float, 
                    kernel_size=3, 
                    sigma=0.5
                )
                processed_mask = (smoothed > 0.5).astype(np.uint8) * 255
            
            # í™€ ì±„ìš°ê¸° (AI ê¸°ë°˜)
            if self.segmentation_config.enable_hole_filling:
                processed_mask = self._fill_holes_ai(processed_mask)
            
            # ê²½ê³„ ê°œì„  (AI ê¸°ë°˜)
            if self.segmentation_config.enable_edge_refinement:
                processed_mask = self._refine_edges_ai(processed_mask)
            
            return processed_mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask

    def _fill_holes_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ í™€ ì±„ìš°ê¸° (OpenCV ëŒ€ì²´)"""
        try:
            if not TORCH_AVAILABLE:
                return mask
            
            # PyTorch ê¸°ë°˜ í™€ ì±„ìš°ê¸°
            tensor = torch.from_numpy(mask.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
            
            # í˜•íƒœí•™ì  ë‹«í˜ ì—°ì‚°ìœ¼ë¡œ í™€ ì±„ìš°ê¸°
            kernel_size = 7
            filled = AIImageProcessor.ai_morphology((tensor.squeeze().numpy() * 255).astype(np.uint8), "closing", kernel_size)
            
            return filled
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask

    def _refine_edges_ai(self, mask: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ê²½ê³„ ê°œì„  (OpenCV ëŒ€ì²´)"""
        try:
            if self.segmentation_config.enable_edge_refinement:
                # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
                edges = AIImageProcessor.ai_detect_edges(mask, 50, 150)
                
                # ê²½ê³„ ì£¼ë³€ ì˜ì—­ í™•ì¥ (AI ê¸°ë°˜)
                edge_region = AIImageProcessor.ai_morphology(edges, "dilation", 3)
                
                # í•´ë‹¹ ì˜ì—­ì— AI ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
                blurred_mask = AIImageProcessor.ai_gaussian_blur(mask.astype(np.float32), 5, 1.0)
                
                # ê²½ê³„ ì˜ì—­ë§Œ ë¸”ëŸ¬ëœ ê°’ìœ¼ë¡œ êµì²´
                refined_mask = mask.copy().astype(np.float32)
                refined_mask[edge_region > 0] = blurred_mask[edge_region > 0]
                
                return (refined_mask > 128).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ê²½ê³„ ê°œì„  ì‹¤íŒ¨: {e}")
            return mask

    def _apply_mask_to_image(self, image: Image.Image, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©"""
        try:
            image_array = np.array(image)
            
            if len(mask.shape) == 2:
                mask_3d = np.stack([mask, mask, mask], axis=2)
            else:
                mask_3d = mask
            
            # ë§ˆìŠ¤í¬ ì •ê·œí™”
            mask_normalized = mask_3d.astype(np.float32) / 255.0
            
            # ë°°ê²½ì„ íˆ¬ëª…í•˜ê²Œ ë§Œë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€
            segmented = image_array.astype(np.float32) * mask_normalized
            
            return segmented.astype(np.uint8)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
            return np.array(image)

    def _calculate_boundary_smoothness(self, mask: np.ndarray) -> float:
        """ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚°"""
        try:
            # AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œë¡œ ê²½ê³„ í’ˆì§ˆ ì¸¡ì •
            edges = AIImageProcessor.ai_detect_edges(mask)
            edge_pixels = np.sum(edges > 0)
            total_boundary = np.sum(mask > 0)
            
            if total_boundary > 0:
                smoothness = 1.0 - (edge_pixels / total_boundary)
                return max(0.0, min(1.0, smoothness))
            else:
                return 0.0
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²½ê³„ ë¶€ë“œëŸ¬ì›€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    # ==============================================
    # ğŸ”¥ 14. AI ê°•í™” ì‹œê°í™” ë©”ì„œë“œë“¤
    # ==============================================

    def _create_ai_visualizations(self, image: Image.Image, mask: np.ndarray, clothing_type: ClothingType) -> Dict[str, Image.Image]:
        """AI ê°•í™” ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE or not NUMPY_AVAILABLE:
                return {}
            
            visualizations = {}
            
            # ìƒ‰ìƒ ì„ íƒ
            color = CLOTHING_COLORS.get(
                clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type),
                CLOTHING_COLORS['unknown']
            )
            
            # 1. AI ê°•í™” ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ê³ í’ˆì§ˆ)
            mask_colored = np.zeros((*mask.shape, 3), dtype=np.uint8)
            mask_colored[mask > 0] = color
            
            # Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§ ì ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.segmentation_config.esrgan_scale > 1:
                mask_image = Image.fromarray(mask_colored)
                target_size = (
                    mask_image.size[0] * self.segmentation_config.esrgan_scale,
                    mask_image.size[1] * self.segmentation_config.esrgan_scale
                )
                mask_image = AIImageProcessor.esrgan_upscale(mask_image, target_size)
                visualizations['mask_hq'] = mask_image
            else:
                visualizations['mask'] = Image.fromarray(mask_colored)
            
            # 2. AI ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ (ê³ í’ˆì§ˆ)
            image_array = np.array(image)
            overlay = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            overlay[mask > 0] = (
                overlay[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê¸°ë°˜ ê²½ê³„ì„  ì¶”ê°€
            boundary = AIImageProcessor.ai_detect_edges(mask.astype(np.uint8))
            overlay[boundary > 0] = (255, 255, 255)
            
            visualizations['overlay'] = Image.fromarray(overlay)
            
            # 3. AI ê²½ê³„ì„  ì´ë¯¸ì§€
            boundary_colored = np.zeros((*boundary.shape, 3), dtype=np.uint8)
            boundary_colored[boundary > 0] = (255, 255, 255)
            
            boundary_overlay = image_array.copy()
            boundary_overlay[boundary > 0] = (255, 255, 255)
            visualizations['boundary'] = Image.fromarray(boundary_overlay)
            
            # 4. ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€
            visualization = self._create_comprehensive_ai_visualization(
                image, mask, clothing_type, color
            )
            visualizations['visualization'] = visualization
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_comprehensive_ai_visualization(self, image: Image.Image, mask: np.ndarray, clothing_type: ClothingType, color: Tuple[int, int, int]) -> Image.Image:
        """ì¢…í•© AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE:
                return image
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            width, height = image.size
            canvas_width = width * 2 + 30
            canvas_height = height + 100
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), (245, 245, 245))
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(image, (15, 40))
            
            # AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            image_array = np.array(image)
            ai_result = image_array.copy()
            alpha = self.segmentation_config.overlay_opacity
            ai_result[mask > 0] = (
                ai_result[mask > 0] * (1 - alpha) +
                np.array(color) * alpha
            ).astype(np.uint8)
            
            # AI ê²½ê³„ì„  ì¶”ê°€
            boundary = AIImageProcessor.ai_detect_edges(mask.astype(np.uint8))
            ai_result[boundary > 0] = (255, 255, 255)
            
            ai_result_image = Image.fromarray(ai_result)
            canvas.paste(ai_result_image, (width + 30, 40))
            
            # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            try:
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(canvas)
                
                try:
                    font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
                    font_small = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 12)
                except Exception:
                    try:
                        font = ImageFont.load_default()
                        font_small = font
                    except Exception:
                        font = None
                        font_small = None
                
                if font:
                    # ì œëª©
                    draw.text((15, 10), "Original", fill=(0, 0, 0), font=font)
                    clothing_type_str = clothing_type.value if hasattr(clothing_type, 'value') else str(clothing_type)
                    draw.text((width + 30, 10), f"AI Segmented ({clothing_type_str})", fill=(0, 0, 0), font=font)
                    
                    # AI ëª¨ë¸ ì •ë³´
                    loaded_models = list(self.ai_models.keys())
                    model_info = f"AI Models: {', '.join(loaded_models)}"
                    draw.text((15, height + 50), model_info, fill=(50, 50, 50), font=font_small)
                    
                    # í†µê³„ ì •ë³´
                    mask_area = np.sum(mask > 0)
                    total_area = mask.size
                    coverage = (mask_area / total_area) * 100
                    
                    stats_text = f"Coverage: {coverage:.1f}% | BaseStepMixin v16.0: âœ… | OpenCV Replaced: âœ…"
                    draw.text((15, height + 70), stats_text, fill=(50, 50, 50), font=font_small)
                
            except ImportError:
                pass  # PIL ImageDraw/ImageFont ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¢…í•© AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return image

    # ==============================================
    # ğŸ”¥ 15. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _get_current_method(self) -> str:
        """í˜„ì¬ ì‚¬ìš©ëœ ë°©ë²• ë°˜í™˜"""
        if self.ai_models.get('sam_huge'):
            return 'sam_huge_ai_basestepmixin_v16'
        elif self.ai_models.get('u2net_cloth'):
            return 'u2net_cloth_ai'
        elif self.ai_models.get('mobile_sam'):
            return 'mobile_sam_ai'
        elif self.ai_models.get('isnet'):
            return 'isnet_ai'
        else:
            return 'ai_fallback'

    def _image_to_base64(self, image: Image.Image) -> str:
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        try:
            if not PIL_AVAILABLE:
                return ""
            
            buffer = BytesIO()
            if isinstance(image, Image.Image):
                image.save(buffer, format='PNG')
            else:
                img = Image.fromarray(image)
                img.save(buffer, format='PNG')
            image_data = buffer.getvalue()
            return base64.b64encode(image_data).decode()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Base64 ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""

    def _create_error_result(self, error_message: str) -> StepOutputData:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„± (Stepê°„ ì—°ë™ í˜¸í™˜)"""
        return StepOutputData(
            success=False,
            result_data={
                'error': error_message,
                'mask': None,
                'confidence': 0.0,
                'processing_time': 0.0,
                'method_used': 'error',
                'ai_models_used': []
            },
            metadata={
                'error_details': error_message,
                'available_ai_models': list(self.ai_models.keys()),
                'basestepmixin_v16_compatible': True,
                'opencv_replaced': True,
                'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                'step_integration_complete': True,
                'ai_inference_attempted': True
            },
            step_name=self.step_name,
            processing_time=0.0
        )

    # ==============================================
    # ğŸ”¥ 16. BaseStepMixin v16.0 í˜¸í™˜ ê³ ê¸‰ ë©”ì„œë“œë“¤
    # ==============================================

    async def process_batch(
        self,
        batch_input: List[Union[StepInputData, str, np.ndarray, Image.Image, Dict[str, Any]]],
        clothing_types: Optional[List[str]] = None,
        quality_level: Optional[str] = None,
        batch_size: Optional[int] = None,
        **kwargs
    ) -> List[Union[StepOutputData, Dict[str, Any]]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì„œë“œ - BaseStepMixin v16.0 í˜¸í™˜ + AI ìµœì í™”"""
        try:
            if not batch_input:
                return []
            
            batch_size = batch_size or self.segmentation_config.batch_size
            clothing_types = clothing_types or [None] * len(batch_input)
            
            # M3 Max ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if self.is_m3_max:
                batch_size = min(batch_size, 8)  # M3 Max 128GB í™œìš©
            
            # ë°°ì¹˜ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            results = []
            for i in range(0, len(batch_input), batch_size):
                batch_images = batch_input[i:i+batch_size]
                batch_clothing_types = clothing_types[i:i+batch_size]
                
                # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
                batch_tasks = []
                for j, (input_data, clothing_type) in enumerate(zip(batch_images, batch_clothing_types)):
                    task = self.process(
                        input_data=input_data,
                        clothing_type=clothing_type,
                        quality_level=quality_level,
                        **kwargs
                    )
                    batch_tasks.append(task)
                
                # ë°°ì¹˜ ì‹¤í–‰
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for result in batch_results:
                    if isinstance(result, Exception):
                        results.append(self._create_error_result(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(result)}"))
                    else:
                        results.append(result)
                
                # ë°°ì¹˜ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i + batch_size < len(batch_input):
                    self.optimize_memory(aggressive=False)
            
            self.logger.info(f"âœ… AI ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì´ë¯¸ì§€")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ AI ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return [self._create_error_result(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}") for _ in batch_input]

    def get_segmentation_info(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ë³´ ë°˜í™˜ - BaseStepMixin v16.0 í˜¸í™˜ + AI ìƒì„¸"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'device': self.device,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'available_methods': [m.value for m in self.available_methods],
            'loaded_ai_models': list(self.ai_models.keys()),
            'ai_model_paths': self.model_paths.copy(),
            'ai_model_status': self.models_loading_status.copy(),
            'processing_stats': self.processing_stats.copy(),
            'basestepmixin_v16_info': {
                'compatible': True,
                'unified_dependency_manager': hasattr(self, 'dependency_manager'),
                'auto_injection_available': hasattr(self, 'dependency_manager'),
                'step_integration_complete': True,
                'model_loader_injected': self.model_loader is not None,
                'memory_manager_injected': self.memory_manager is not None,
                'data_converter_injected': self.data_converter is not None
            },
            'ai_model_stats': {
                'total_ai_calls': self.processing_stats['ai_model_calls'],
                'models_loaded': len(self.ai_models),
                'model_paths_found': len(self.model_paths),
                'sam_huge_calls': self.processing_stats['sam_huge_calls'],
                'u2net_calls': self.processing_stats['u2net_calls'],
                'mobile_sam_calls': self.processing_stats['mobile_sam_calls'],
                'isnet_calls': self.processing_stats['isnet_calls'],
                'hybrid_calls': self.processing_stats['hybrid_calls'],
                'total_model_size_mb': sum(
                    2445.7 if 'sam_huge' in model else
                    168.1 if 'u2net' in model else
                    38.8 if 'mobile_sam' in model else
                    168.1 if 'isnet' in model else 0
                    for model in self.ai_models.keys()
                ),
                'opencv_replaced': True
            },
            'config': {
                'method': self.segmentation_config.method.value,
                'quality_level': self.segmentation_config.quality_level.value,
                'enable_visualization': self.segmentation_config.enable_visualization,
                'confidence_threshold': self.segmentation_config.confidence_threshold,
                'enable_edge_refinement': self.segmentation_config.enable_edge_refinement,
                'enable_hole_filling': self.segmentation_config.enable_hole_filling,
                'overlay_opacity': self.segmentation_config.overlay_opacity,
                'esrgan_scale': self.segmentation_config.esrgan_scale
            },
            'system_info': {
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'rembg_available': REMBG_AVAILABLE,
                'sam_available': SAM_AVAILABLE,
                'onnx_available': ONNX_AVAILABLE,
                'esrgan_available': ESRGAN_AVAILABLE
            }
        }

    # ==============================================
    # ğŸ”¥ 17. ì •ë¦¬ ë©”ì„œë“œ (BaseStepMixin v16.0 í˜¸í™˜)
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - BaseStepMixin v16.0 í˜¸í™˜ + AI ëª¨ë¸ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ClothSegmentationStep ì™„ì „ AI ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    elif hasattr(model, 'sam_model') and model.sam_model and hasattr(model.sam_model, 'cpu'):
                        model.sam_model.cpu()
                    del model
                    self.logger.debug(f"ğŸ§¹ {model_name} AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.ai_models.clear()
            self.model_paths.clear()
            self.models_loading_status = {k: False for k in self.models_loading_status.keys()}
            
            # RemBG ì„¸ì…˜ ì •ë¦¬
            if hasattr(self, 'rembg_sessions'):
                self.rembg_sessions.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.segmentation_cache.clear()
            
            # ì‹¤í–‰ì ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            # BaseStepMixin ì •ë¦¬
            if self._mixin and hasattr(self._mixin, 'cleanup'):
                try:
                    await self._mixin.cleanup()
                    self.logger.info("âœ… BaseStepMixin v16.0 ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ BaseStepMixin ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps" and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # ì˜ì¡´ì„± ì°¸ì¡° ì •ë¦¬
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            self.step_factory = None
            self.dependency_manager = None
            self._mixin = None
            
            # BaseStepMixin v16.0 í˜¸í™˜ í”Œë˜ê·¸ ì¬ì„¤ì •
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            self.logger.info("âœ… ClothSegmentationStep ì™„ì „ AI + BaseStepMixin v16.0 ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

# ==============================================
# ğŸ”¥ 18. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜)
# ==============================================

def create_cloth_segmentation_step(
    device: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜ (BaseStepMixin v16.0 í˜¸í™˜)"""
    return ClothSegmentationStep(device=device, config=config, **kwargs)

async def create_and_initialize_cloth_segmentation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """BaseStepMixin v16.0 í˜¸í™˜ ClothSegmentationStep ìƒì„± ë° ì™„ì „ AI ì´ˆê¸°í™”"""
    try:
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        
        # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ (BaseStepMixin v16.0 UnifiedDependencyManager íŒ¨í„´)
        if hasattr(step, 'dependency_manager') and step.dependency_manager:
            try:
                step.dependency_manager.auto_inject_dependencies()
                logger.info("âœ… UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        # ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… í´ë°±
        try:
            model_loader = get_model_loader()
            if model_loader:
                step.set_model_loader(model_loader)
                logger.info("âœ… ìˆ˜ë™ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            di_container = get_di_container()
            if di_container:
                step.set_di_container(di_container)
                logger.info("âœ… ìˆ˜ë™ DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        # ì™„ì „ AI ì´ˆê¸°í™”
        await step.initialize()
        return step
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ + ì™„ì „ AI ìƒì„± ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ìƒì„±
        step = create_cloth_segmentation_step(device=device, config=config, **kwargs)
        await step.initialize()
        return step

def create_m3_max_segmentation_step(
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜)"""
    m3_config = {
        'method': SegmentationMethod.HYBRID_AI,
        'quality_level': QualityLevel.ULTRA,
        'use_fp16': True,
        'batch_size': 8,  # M3 Max 128GB í™œìš©
        'cache_size': 200,
        'enable_visualization': True,
        'visualization_quality': 'high',
        'enable_edge_refinement': True,
        'enable_hole_filling': True,
        'esrgan_scale': 2  # Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ë§
    }
    
    if config:
        m3_config.update(config)
    
    return ClothSegmentationStep(device="mps", config=m3_config, **kwargs)

# ==============================================
# ğŸ”¥ 19. í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ í•¨ìˆ˜ë“¤
# ==============================================

async def test_complete_ai_segmentation():
    """ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ + BaseStepMixin v16.0 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì™„ì „ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ + BaseStepMixin v16.0 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜ + ì™„ì „ AI)
        step = await create_and_initialize_cloth_segmentation_step(
            device="auto",
            config={
                "method": "hybrid_ai",
                "quality_level": "ultra",
                "enable_visualization": True,
                "visualization_quality": "high",
                "esrgan_scale": 2
            }
        )
        
        # BaseStepMixin v16.0 í˜¸í™˜ì„± ìƒíƒœ í™•ì¸
        info = step.get_segmentation_info()
        v16_info = info['basestepmixin_v16_info']
        ai_info = info['ai_model_stats']
        
        print("ğŸ”— BaseStepMixin v16.0 í˜¸í™˜ì„± ìƒíƒœ:")
        print(f"   âœ… í˜¸í™˜ì„±: {v16_info['compatible']}")
        print(f"   âœ… UnifiedDependencyManager: {v16_info['unified_dependency_manager']}")
        print(f"   âœ… ìë™ ì˜ì¡´ì„± ì£¼ì…: {v16_info['auto_injection_available']}")
        print(f"   âœ… Step í†µí•© ì™„ë£Œ: {v16_info['step_integration_complete']}")
        print(f"   âœ… ModelLoader ì£¼ì…: {v16_info['model_loader_injected']}")
        
        print("\nğŸ§  ì™„ì „ AI ëª¨ë¸ ìƒíƒœ:")
        print(f"   âœ… ë¡œë“œëœ AI ëª¨ë¸: {info['loaded_ai_models']}")
        print(f"   âœ… ì´ ëª¨ë¸ í¬ê¸°: {ai_info['total_model_size_mb']:.1f}MB")
        print(f"   âœ… OpenCV ëŒ€ì²´ë¨: {ai_info['opencv_replaced']}")
        print(f"   âœ… AI ëª¨ë¸ í˜¸ì¶œ: {ai_info['total_ai_calls']}")
        
        # í‘œì¤€ BaseStepMixin v16.0 ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ BaseStepMixin v16.0 í‘œì¤€ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸:")
        
        # get_status í…ŒìŠ¤íŠ¸
        status = step.get_status()
        print(f"   âœ… get_status(): ì´ˆê¸°í™”={status['is_initialized']}, AIëª¨ë¸={len(status['ai_models_loaded'])}")
        
        # get_model í…ŒìŠ¤íŠ¸
        sam_model = step.get_model("sam_huge")
        u2net_model = step.get_model("u2net_cloth")
        print(f"   âœ… get_model(): SAM={sam_model is not None}, U2Net={u2net_model is not None}")
        
        # optimize_memory í…ŒìŠ¤íŠ¸
        memory_result = step.optimize_memory()
        print(f"   âœ… optimize_memory(): {memory_result['success']}")
        
        # warmup í…ŒìŠ¤íŠ¸
        warmup_result = step.warmup()
        print(f"   âœ… warmup(): {warmup_result['success']}, AIëª¨ë¸ìˆ˜={len(warmup_result.get('warmed_ai_models', []))}")
        
        # get_performance_summary í…ŒìŠ¤íŠ¸
        perf_summary = step.get_performance_summary()
        print(f"   âœ… get_performance_summary(): ì„±ê³µë¥  {perf_summary['success_rate']:.1%}")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        if PIL_AVAILABLE:
            dummy_image = Image.new('RGB', (512, 512), (200, 150, 100))
        else:
            dummy_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # Stepê°„ ì—°ë™ í…ŒìŠ¤íŠ¸ (StepInputData)
        step_input = StepInputData(
            image=dummy_image,
            metadata={'clothing_type': 'shirt', 'source': 'test'},
            step_history=['step_01', 'step_02'],
            processing_context={'test_mode': True}
        )
        
        # ì™„ì „ AI ì²˜ë¦¬ ì‹¤í–‰
        result = await step.process(step_input, quality_level="high")
        
        # ê²°ê³¼ í™•ì¸
        if result.success:
            print("\nâœ… ì™„ì „ AI + BaseStepMixin v16.0 ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   - ì˜ë¥˜ íƒ€ì…: {result.result_data['clothing_type']}")
            print(f"   - ì‹ ë¢°ë„: {result.result_data['confidence']:.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"   - ì‚¬ìš© AI ëª¨ë¸: {result.result_data['ai_models_used']}")
            print(f"   - ë°©ë²•: {result.result_data['method_used']}")
            print(f"   - BaseStepMixin v16.0: {result.metadata['basestepmixin_v16_compatible']}")
            print(f"   - OpenCV ëŒ€ì²´: {result.metadata['opencv_replaced']}")
            print(f"   - Step í†µí•©: {result.metadata['step_integration_complete']}")
            print(f"   - ì´ ëª¨ë¸ í¬ê¸°: {result.metadata['total_model_size_mb']:.1f}MB")
            
            if 'visualization_base64' in result.result_data:
                print("   - AI ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±ë¨")
            
            # Stepê°„ ì—°ë™ í™•ì¸
            if result.next_step_input:
                print(f"   - ë‹¤ìŒ Step ì…ë ¥ ì¤€ë¹„: {list(result.next_step_input.keys())}")
        else:
            print(f"âŒ ì™„ì „ AI + BaseStepMixin v16.0 ì²˜ë¦¬ ì‹¤íŒ¨: {result.result_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ AI ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
        batch_inputs = [dummy_image, dummy_image]
        batch_results = await step.process_batch(batch_inputs, clothing_types=["shirt", "pants"])
        successful_batch = sum(1 for r in batch_results if r.success)
        print(f"   âœ… ë°°ì¹˜ ì²˜ë¦¬: {successful_batch}/{len(batch_results)} ì„±ê³µ")
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        print(f"\nğŸŒŸ ì™„ì „ AI + BaseStepMixin v16.0 ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   - ë””ë°”ì´ìŠ¤: {info['device']}")
        print(f"   - M3 Max: {info['system_info']['is_m3_max']}")
        print(f"   - ë©”ëª¨ë¦¬: {info['system_info']['memory_gb']}GB")
        print(f"   - PyTorch: {info['system_info']['torch_available']}")
        print(f"   - MPS: {info['system_info']['mps_available']}")
        print(f"   - SAM: {info['system_info']['sam_available']}")
        print(f"   - ONNX: {info['system_info']['onnx_available']}")
        print(f"   - Real-ESRGAN: {info['system_info']['esrgan_available']}")
        print(f"   - BaseStepMixin v16.0: {info['basestepmixin_v16_info']['compatible']}")
        print(f"   - UnifiedDependencyManager: {info['basestepmixin_v16_info']['unified_dependency_manager']}")
        
        # ì •ë¦¬
        await step.cleanup()
        print("âœ… ì™„ì „ AI + BaseStepMixin v16.0 í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ì •ë¦¬")
        
    except Exception as e:
        print(f"âŒ ì™„ì „ AI + BaseStepMixin v16.0 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ìŒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   1. BaseStepMixin v16.0 ëª¨ë“ˆ (UnifiedDependencyManager)")
        print("   2. ModelLoader ëª¨ë“ˆ (ì²´í¬í¬ì¸íŠ¸ ë¡œë”©)")
        print("   3. ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ (5.5GB)")
        print("   4. conda í™˜ê²½ ì„¤ì • (pytorch, pillow, transformers ë“±)")
        print("   5. AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (segment-anything, rembg, onnxruntime)")

def example_complete_ai_usage():
    """ì™„ì „ AI + BaseStepMixin v16.0 í˜¸í™˜ ì‚¬ìš© ì˜ˆì‹œ"""
    print("ğŸ”¥ MyCloset AI Step 03 - ì™„ì „ AI ëª¨ë¸ ì—°ë™ + BaseStepMixin v16.0 í˜¸í™˜ ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 80)

def print_conda_setup_guide_complete():
    """conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (ì™„ì „ AI + BaseStepMixin v16.0)"""
   