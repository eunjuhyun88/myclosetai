#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net, Mask R-CNN ì§€ì›
âœ… ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ 100% ìœ ì§€ - ASPP, Self-Correction, Progressive Parsing
âœ… 50% ì½”ë“œ ë‹¨ì¶• - 2000ì¤„ â†’ 1000ì¤„ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)
âœ… ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ - Mock ì œê±°í•˜ê³  ì§„ì§œ ëª¨ë¸ ì‚¬ìš©
âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›
âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 33.0 (Central Hub DI Container Integration)
"""

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 1: Import ë° Central Hub DI Container ì—°ë™
# ==============================================

import os
import gc
import time
import logging
import threading
import math
import hashlib
import json
import base64
import weakref
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
from abc import ABC, abstractmethod

# ğŸ”¥ PyTorch ë¡œë”© ìµœì í™”
from fix_pytorch_loading import apply_pytorch_patch
apply_pytorch_patch()

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 2: BaseStepMixin ì—°ë™ (Central Hub DI Container v7.0)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

BaseStepMixin = get_base_step_mixin_class()

# ê¸´ê¸‰ í´ë°± BaseStepMixin (ìµœì†Œ ê¸°ëŠ¥)
if BaseStepMixin is None:
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.model_loader = None
            self.model_interface = None
            self.loaded_models = {}
            self.ai_models = {}
            
        def initialize(self): 
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader): 
            self.model_loader = model_loader
        
        async def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
            # BaseStepMixinì˜ í‘œì¤€ process ë©”ì„œë“œ
            processed_input = self._preprocess_input(data)
            result = self._run_ai_inference(processed_input)
            return self._postprocess_output(result)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 3: ì‹œìŠ¤í…œ í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

def detect_m3_max():
    """M3 Max ê°ì§€"""
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

# PyTorch (í•„ìˆ˜)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("âŒ PyTorch í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# PIL (í•„ìˆ˜)
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ PIL í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# NumPy (í•„ìˆ˜)
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ NumPy í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# SAM (ì„ íƒì )
SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# SciPy (ê³ ê¸‰ í›„ì²˜ë¦¬ìš©)
SCIPY_AVAILABLE = False
try:
    from scipy import ndimage
    SCIPY_AVAILABLE = True
    logger.info("ğŸ”¬ SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SciPy ì—†ìŒ - ê³ ê¸‰ í›„ì²˜ë¦¬ ì œí•œ")

# Scikit-image (ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬)
SKIMAGE_AVAILABLE = False
try:
    from skimage import measure, morphology, segmentation, filters
    SKIMAGE_AVAILABLE = True
    logger.info("ğŸ”¬ Scikit-image ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Scikit-image ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# Torchvision
TORCHVISION_AVAILABLE = False
try:
    import torchvision
    from torchvision import models, transforms
    TORCHVISION_AVAILABLE = True
    logger.info("ğŸ¤– Torchvision ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Torchvision ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 4: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„° êµ¬ì¡°
# ==============================================

class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    DEEPLABV3_PLUS = "deeplabv3_plus"   # DeepLabV3+ (233.3MB) - ìš°ì„ ìˆœìœ„ 1
    MASK_RCNN = "mask_rcnn"             # Mask R-CNN (í´ë°±)
    SAM_HUGE = "sam_huge"               # SAM ViT-Huge (2445.7MB)
    U2NET_CLOTH = "u2net_cloth"         # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    HYBRID_AI = "hybrid_ai"             # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”

class ClothCategory(Enum):
    """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (ë‹¤ì¤‘ í´ë˜ìŠ¤)"""
    BACKGROUND = 0
    SHIRT = 1           # ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤
    T_SHIRT = 2         # í‹°ì…”ì¸ 
    SWEATER = 3         # ìŠ¤ì›¨í„°/ë‹ˆíŠ¸
    HOODIE = 4          # í›„ë“œí‹°
    JACKET = 5          # ì¬í‚·/ì•„ìš°í„°
    COAT = 6            # ì½”íŠ¸
    DRESS = 7           # ì›í”¼ìŠ¤
    SKIRT = 8           # ìŠ¤ì»¤íŠ¸
    PANTS = 9           # ë°”ì§€
    JEANS = 10          # ì²­ë°”ì§€
    SHORTS = 11         # ë°˜ë°”ì§€
    SHOES = 12          # ì‹ ë°œ
    BOOTS = 13          # ë¶€ì¸ 
    SNEAKERS = 14       # ìš´ë™í™”
    BAG = 15            # ê°€ë°©
    HAT = 16            # ëª¨ì
    GLASSES = 17        # ì•ˆê²½
    SCARF = 18          # ìŠ¤ì¹´í”„
    BELT = 19           # ë²¨íŠ¸

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # ë¹ ë¥¸ ì²˜ë¦¬
    BALANCED = "balanced"   # ê· í˜•
    HIGH = "high"          # ê³ í’ˆì§ˆ
    ULTRA = "ultra"        # ìµœê³ í’ˆì§ˆ

@dataclass
class ClothSegmentationConfig:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.DEEPLABV3_PLUS
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (512, 512)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    
    # ì˜ë¥˜ ë¶„ë¥˜ ì„¤ì •
    enable_clothing_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # í›„ì²˜ë¦¬ ì„¤ì •
    enable_crf_postprocessing: bool = True  # ğŸ”¥ CRF í›„ì²˜ë¦¬ ë³µì›
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True  # ğŸ”¥ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ë³µì›
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    confidence_threshold: float = 0.5
    enable_visualization: bool = True

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 5: í•µì‹¬ AI ì•Œê³ ë¦¬ì¦˜ - DeepLabV3+ (ì›ë³¸ ì™„ì „ ë³´ì¡´)
# ==============================================

class ASPPModule(nn.Module):
    """ASPP ëª¨ë“ˆ - Multi-scale context aggregation"""
    
    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()
        
        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])
        
        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
    
    def forward(self, x):
        h, w = x.shape[2:]
        
        # 1x1 convolution
        feat1 = self.conv1x1(x)
        
        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)
        
        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # Project to final features
        return self.project(concat_feat)

class SelfCorrectionModule(nn.Module):
    """Self-Correction Learning - SCHP í•µì‹¬ ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, num_classes=20, hidden_dim=256):
        super().__init__()
        self.num_classes = num_classes
        
        # Context aggregation
        self.context_conv = nn.Sequential(
            nn.Conv2d(num_classes, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention mechanism
        self.self_attention = SelfAttentionBlock(hidden_dim)
        
        # Correction prediction
        self.correction_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim // 2, num_classes, 1)
        )
        
        # Confidence estimation
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing, features):
        # Convert initial parsing to features
        parsing_feat = self.context_conv(initial_parsing)
        
        # Apply self-attention
        attended_feat = self.self_attention(parsing_feat)
        
        # Predict corrections
        correction = self.correction_conv(attended_feat)
        
        # Estimate confidence
        confidence = self.confidence_branch(attended_feat)
        
        # Apply corrections with confidence weighting
        corrected_parsing = initial_parsing + correction * confidence
        
        return corrected_parsing, confidence

class SelfAttentionBlock(nn.Module):
    """Self-Attention Block for context modeling"""
    
    def __init__(self, in_channels):
        super().__init__()
        self.in_channels = in_channels
        
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # Generate query, key, value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)
        
        # Compute attention
        attention = torch.bmm(proj_query, proj_key)
        attention = self.softmax(attention)
        
        # Apply attention to values
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        # Residual connection
        out = self.gamma * out + x
        
        return out

class DeepLabV3PlusBackbone(nn.Module):
    """DeepLabV3+ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ - ResNet-101 ê¸°ë°˜"""
    
    def __init__(self, backbone='resnet101', output_stride=16):
        super().__init__()
        self.output_stride = output_stride
        
        # ResNet-101 ë°±ë³¸ êµ¬ì„±
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ResNet Layers with Dilated Convolution
        self.layer1 = self._make_layer(64, 64, 3, stride=1)
        self.layer2 = self._make_layer(256, 128, 4, stride=2)
        self.layer3 = self._make_layer(512, 256, 23, stride=2)
        self.layer4 = self._make_layer(1024, 512, 3, stride=1, dilation=2)
        
        # Low-level feature extraction
        self.low_level_conv = nn.Conv2d(256, 48, 1, bias=False)
        self.low_level_bn = nn.BatchNorm2d(48)
    
    def _make_layer(self, inplanes, planes, blocks, stride=1, dilation=1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        layers = []
        
        # Downsample layer
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, 1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )
        
        # First block
        layers.append(self._bottleneck_block(inplanes, planes, stride, dilation, downsample))
        
        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(self._bottleneck_block(planes * 4, planes, 1, dilation))
        
        return nn.Sequential(*layers)
    
    def _bottleneck_block(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        """ResNet Bottleneck ë¸”ë¡"""
        class BottleneckBlock(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=dilation, 
                                     dilation=dilation, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.downsample = downsample
                self.relu = nn.ReLU(inplace=True)
            
            def forward(self, x):
                identity = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    identity = self.downsample(x)
                
                out += identity
                out = self.relu(out)
                
                return out
        
        return BottleneckBlock()
    
    def forward(self, x):
        # Initial convolution
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        # ResNet layers
        x = self.layer1(x)
        low_level_feat = x  # Save for decoder
        
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # Process low-level features
        low_level_feat = self.low_level_conv(low_level_feat)
        low_level_feat = self.low_level_bn(low_level_feat)
        low_level_feat = self.relu(low_level_feat)
        
        return x, low_level_feat

class DeepLabV3PlusModel(nn.Module):
    """Complete DeepLabV3+ Model - ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™”"""
    
    def __init__(self, num_classes=20):  # 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬
        super().__init__()
        self.num_classes = num_classes
        
        # 1. DeepLabV3+ Backbone
        self.backbone = DeepLabV3PlusBackbone()
        
        # 2. ASPP Module
        self.aspp = ASPPModule()
        
        # 3. Self-Correction Module
        self.self_correction = SelfCorrectionModule(num_classes)
        
        # Decoder for final parsing
        self.decoder = nn.Sequential(
            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        # Final classifier
        self.classifier = nn.Conv2d(256, num_classes, 1)
    
    def forward(self, x):
        input_size = x.shape[2:]
        
        # 1. Extract features with DeepLabV3+ backbone
        high_level_feat, low_level_feat = self.backbone(x)
        
        # 2. Apply ASPP for multi-scale context
        aspp_feat = self.aspp(high_level_feat)
        
        # 3. Upsample and concatenate with low-level features
        aspp_feat = F.interpolate(aspp_feat, size=low_level_feat.shape[2:], 
                                 mode='bilinear', align_corners=False)
        concat_feat = torch.cat([aspp_feat, low_level_feat], dim=1)
        
        # 4. Decode features
        decoded_feat = self.decoder(concat_feat)
        
        # 5. Initial parsing prediction
        initial_parsing = self.classifier(decoded_feat)
        
        # 6. Self-correction
        corrected_parsing, confidence = self.self_correction(
            torch.softmax(initial_parsing, dim=1), decoded_feat
        )
        
        # 7. Upsample to input size
        final_parsing = F.interpolate(corrected_parsing, size=input_size, 
                                    mode='bilinear', align_corners=False)
        confidence = F.interpolate(confidence, size=input_size, 
                                  mode='bilinear', align_corners=False)
        
        return {
            'parsing': final_parsing,
            'confidence': confidence,
            'initial_parsing': F.interpolate(initial_parsing, size=input_size, 
                                           mode='bilinear', align_corners=False)
        }

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 6: ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì›ë³¸ ì™„ì „ ë³µì›)
# ==============================================

class AdvancedPostProcessor:
    """ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ - ì›ë³¸ ì™„ì „ ë³µì›"""
    
    @staticmethod
    def apply_crf_postprocessing(mask: np.ndarray, image: np.ndarray, num_iterations: int = 10) -> np.ndarray:
        """CRF í›„ì²˜ë¦¬ë¡œ ê²½ê³„ì„  ê°œì„  (ì›ë³¸)"""
        try:
            if not DENSECRF_AVAILABLE:
                return mask
            
            h, w = mask.shape
            
            # í™•ë¥  ë§µ ìƒì„±
            prob_bg = 1.0 - (mask.astype(np.float32) / 255.0)
            prob_fg = mask.astype(np.float32) / 255.0
            probs = np.stack([prob_bg, prob_fg], axis=0)
            
            # Unary potential
            unary = unary_from_softmax(probs)
            
            # Setup CRF
            d = dcrf.DenseCRF2D(w, h, 2)
            d.setUnaryEnergy(unary)
            
            # Add pairwise energies
            d.addPairwiseGaussian(sxy=(3, 3), compat=3)
            d.addPairwiseBilateral(sxy=(80, 80), srgb=(13, 13, 13), 
                                  rgbim=image, compat=10)
            
            # Inference
            Q = d.inference(num_iterations)
            map_result = np.argmax(Q, axis=0).reshape((h, w))
            
            return (map_result * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ CRF í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask
    
    @staticmethod
    def apply_multiscale_processing(image: np.ndarray, initial_mask: np.ndarray) -> np.ndarray:
        """ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ (ì›ë³¸)"""
        try:
            scales = [0.5, 1.0, 1.5]
            processed_masks = []
            
            for scale in scales:
                if scale != 1.0:
                    h, w = initial_mask.shape
                    new_h, new_w = int(h * scale), int(w * scale)
                    
                    scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.Resampling.LANCZOS))
                    scaled_mask = np.array(Image.fromarray(initial_mask).resize((new_w, new_h), Image.Resampling.NEAREST))
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    processed = np.array(Image.fromarray(scaled_mask).resize((w, h), Image.Resampling.NEAREST))
                else:
                    processed = initial_mask
                
                processed_masks.append(processed.astype(np.float32) / 255.0)
            
            # ìŠ¤ì¼€ì¼ë³„ ê²°ê³¼ í†µí•©
            if len(processed_masks) > 1:
                weights = [0.3, 0.4, 0.3]
                combined = np.zeros_like(processed_masks[0])
                
                for mask, weight in zip(processed_masks, weights):
                    combined += mask * weight
                
                final_mask = (combined > 0.5).astype(np.uint8) * 255
            else:
                final_mask = (processed_masks[0] > 0.5).astype(np.uint8) * 255
            
            return final_mask
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return initial_mask
    
    @staticmethod
    def apply_progressive_parsing(parsing_result: Dict[str, Any], image: np.ndarray) -> Dict[str, Any]:
        """Progressive Parsing ì•Œê³ ë¦¬ì¦˜ (ì›ë³¸)"""
        try:
            if 'parsing' not in parsing_result:
                return parsing_result
            
            parsing = parsing_result['parsing']
            
            # Stage 1: ê±°ì¹œ ë¶„í• 
            coarse_parsing = F.interpolate(parsing, scale_factor=0.5, mode='bilinear', align_corners=False)
            
            # Stage 2: ì¤‘ê°„ í•´ìƒë„ ì •ì œ
            medium_parsing = F.interpolate(coarse_parsing, scale_factor=2.0, mode='bilinear', align_corners=False)
            
            # Stage 3: ì›ë³¸ í•´ìƒë„ ì •ì œ (Self-Correction ì ìš©)
            if 'confidence' in parsing_result:
                confidence = parsing_result['confidence']
                refined_parsing = parsing * confidence + medium_parsing * (1 - confidence)
            else:
                refined_parsing = (parsing + medium_parsing) / 2.0
            
            parsing_result['parsing'] = refined_parsing
            parsing_result['progressive_enhanced'] = True
            
            return parsing_result
            
        except Exception as e:
            logger.warning(f"âš ï¸ Progressive Parsing ì‹¤íŒ¨: {e}")
            return parsing_result
    
    @staticmethod
    def apply_edge_refinement(masks: Dict[str, np.ndarray], image: np.ndarray) -> Dict[str, np.ndarray]:
        """Edge Detection ë¸Œëœì¹˜ (ì›ë³¸)"""
        try:
            refined_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    refined_masks[mask_key] = mask
                    continue
                
                # 1. ê²½ê³„ì„  ê²€ì¶œ
                if SKIMAGE_AVAILABLE:
                    edges = filters.sobel(mask.astype(np.float32) / 255.0)
                    edges = (edges > 0.1).astype(np.uint8) * 255
                else:
                    # ê°„ë‹¨í•œ ê²½ê³„ì„  ê²€ì¶œ
                    kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
                    kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
                    
                    grad_x = ndimage.convolve(mask.astype(np.float32), kernel_x) if SCIPY_AVAILABLE else mask
                    grad_y = ndimage.convolve(mask.astype(np.float32), kernel_y) if SCIPY_AVAILABLE else mask
                    
                    edges = np.sqrt(grad_x**2 + grad_y**2)
                    edges = (edges > 10).astype(np.uint8) * 255
                
                # 2. ê²½ê³„ì„  ê¸°ë°˜ ë§ˆìŠ¤í¬ ì •ì œ
                refined_mask = mask.copy()
                
                # ê²½ê³„ì„  ì£¼ë³€ í”½ì…€ ê°•í™”
                if SCIPY_AVAILABLE:
                    dilated_edges = ndimage.binary_dilation(edges > 128, iterations=2)
                    refined_mask[dilated_edges] = np.maximum(refined_mask[dilated_edges], edges[dilated_edges])
                
                refined_masks[mask_key] = refined_mask
            
            return refined_masks
            
        except Exception as e:
            logger.warning(f"âš ï¸ Edge Refinement ì‹¤íŒ¨: {e}")
            return masks
    
    @staticmethod
    def apply_multi_scale_feature_fusion(features_list: List[torch.Tensor], target_size: Tuple[int, int]) -> torch.Tensor:
        """Multi-scale Feature Fusion (ì›ë³¸)"""
        try:
            if not features_list:
                return torch.zeros((1, 256, target_size[0], target_size[1]))
            
            # ëª¨ë“  featuresë¥¼ target_sizeë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            resized_features = []
            for features in features_list:
                if features.shape[2:] != target_size:
                    resized = F.interpolate(features, size=target_size, mode='bilinear', align_corners=False)
                else:
                    resized = features
                resized_features.append(resized)
            
            # Feature fusion with attention weights
            if len(resized_features) > 1:
                # Channel attention
                channel_weights = []
                for features in resized_features:
                    # Global average pooling for channel attention
                    gap = F.adaptive_avg_pool2d(features, (1, 1))
                    weight = torch.sigmoid(gap)
                    channel_weights.append(weight)
                
                # Weighted fusion
                fused_features = torch.zeros_like(resized_features[0])
                total_weight = sum(channel_weights)
                
                for features, weight in zip(resized_features, channel_weights):
                    normalized_weight = weight / total_weight
                    fused_features += features * normalized_weight
                
                return fused_features
            else:
                return resized_features[0]
                
        except Exception as e:
            logger.warning(f"âš ï¸ Multi-scale Feature Fusion ì‹¤íŒ¨: {e}")
            return features_list[0] if features_list else torch.zeros((1, 256, target_size[0], target_size[1]))

class RealDeepLabV3PlusModel:
    """ì‹¤ì œ DeepLabV3+ ëª¨ë¸ (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™”)"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
        self.num_classes = 20
    
    def load(self) -> bool:
        """DeepLabV3+ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # DeepLabV3+ ëª¨ë¸ ìƒì„±
            self.model = DeepLabV3PlusModel(num_classes=self.num_classes)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if os.path.exists(self.model_path):
                try:
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                except:
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                
                # MPS í˜¸í™˜ì„±
                if self.device == "mps" and isinstance(checkpoint, dict):
                    for key, value in checkpoint.items():
                        if isinstance(value, torch.Tensor) and value.dtype == torch.float64:
                            checkpoint[key] = value.float()
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                self.model.load_state_dict(checkpoint, strict=False)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            
            logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """DeepLabV3+ ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return {"masks": {}, "confidence": 0.0}
            
            # ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ì‹¤ì œ DeepLabV3+ AI ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(input_tensor)
                
            # ê²°ê³¼ ì¶”ì¶œ
            parsing = outputs['parsing']
            confidence_map = outputs['confidence']
            
            # í›„ì²˜ë¦¬ - ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            parsing_softmax = torch.softmax(parsing, dim=1)
            parsing_argmax = torch.argmax(parsing_softmax, dim=1)
            
            # NumPy ë³€í™˜
            parsing_np = parsing_argmax.squeeze().cpu().numpy()
            confidence_np = confidence_map.squeeze().cpu().numpy()
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_size = image.shape[:2]
            parsing_pil = Image.fromarray(parsing_np.astype(np.uint8))
            parsing_resized = np.array(parsing_pil.resize((original_size[1], original_size[0]), Image.Resampling.NEAREST))
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            masks = self._create_category_masks(parsing_resized)
            
            return {
                "masks": masks,
                "confidence": float(np.mean(confidence_np)),
                "parsing_map": parsing_resized,
                "categories_detected": list(np.unique(parsing_resized))
            }
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}
    
    def _create_category_masks(self, parsing_map: np.ndarray) -> Dict[str, np.ndarray]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±"""
        masks = {}
        
        # ìƒì˜ ì¹´í…Œê³ ë¦¬
        upper_categories = [ClothCategory.SHIRT.value, ClothCategory.T_SHIRT.value, 
                           ClothCategory.SWEATER.value, ClothCategory.HOODIE.value,
                           ClothCategory.JACKET.value, ClothCategory.COAT.value]
        upper_mask = np.isin(parsing_map, upper_categories).astype(np.uint8) * 255
        masks['upper_body'] = upper_mask
        
        # í•˜ì˜ ì¹´í…Œê³ ë¦¬
        lower_categories = [ClothCategory.PANTS.value, ClothCategory.JEANS.value,
                           ClothCategory.SHORTS.value, ClothCategory.SKIRT.value]
        lower_mask = np.isin(parsing_map, lower_categories).astype(np.uint8) * 255
        masks['lower_body'] = lower_mask
        
        # ì „ì‹  ì¹´í…Œê³ ë¦¬
        dress_categories = [ClothCategory.DRESS.value]
        full_body_mask = np.isin(parsing_map, dress_categories).astype(np.uint8) * 255
        masks['full_body'] = full_body_mask
        
        # ì•¡ì„¸ì„œë¦¬ ì¹´í…Œê³ ë¦¬
        accessory_categories = [ClothCategory.SHOES.value, ClothCategory.BAG.value,
                               ClothCategory.HAT.value, ClothCategory.GLASSES.value,
                               ClothCategory.SCARF.value, ClothCategory.BELT.value]
        accessory_mask = np.isin(parsing_map, accessory_categories).astype(np.uint8) * 255
        masks['accessories'] = accessory_mask
        
        # ì „ì²´ ì˜ë¥˜ ë§ˆìŠ¤í¬
        all_categories = upper_categories + lower_categories + dress_categories + accessory_categories
        all_cloth_mask = np.isin(parsing_map, all_categories).astype(np.uint8) * 255
        masks['all_clothes'] = all_cloth_mask
        
        return masks

class RealSAMModel:
    """ì‹¤ì œ SAM AI ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.predictor = None
        self.is_loaded = False
        
    def load(self) -> bool:
        """SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                return False
            
            from segment_anything import build_sam_vit_h, SamPredictor
            
            self.model = build_sam_vit_h(checkpoint=self.model_path)
            self.model.to(self.device)
            self.predictor = SamPredictor(self.model)
            self.is_loaded = True
            
            logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: np.ndarray, prompts: Dict[str, Any] = None) -> Dict[str, Any]:
        """SAM ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return {"masks": {}, "confidence": 0.0}
            
            self.predictor.set_image(image)
            
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ì¤‘ì•™ ì˜ì—­)
            if prompts is None:
                h, w = image.shape[:2]
                prompts = {
                    'points': [(w//2, h//2)],
                    'labels': [1]
                }
            
            # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            point_coords = np.array(prompts.get('points', []))
            point_labels = np.array(prompts.get('labels', []))
            box = np.array(prompts.get('box', None)) if prompts.get('box') else None
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            masks, scores, logits = self.predictor.predict(
                point_coords=point_coords if len(point_coords) > 0 else None,
                point_labels=point_labels if len(point_labels) > 0 else None,
                box=box,
                multimask_output=True
            )
            
            # ìµœê³  ì ìˆ˜ ë§ˆìŠ¤í¬ ì„ íƒ
            best_idx = np.argmax(scores)
            best_mask = masks[best_idx]
            best_score = scores[best_idx]
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„± (SAMì€ ì¼ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì´ë¯€ë¡œ ì „ì²´ ë§ˆìŠ¤í¬ë¡œ ì²˜ë¦¬)
            mask_uint8 = (best_mask * 255).astype(np.uint8)
            masks_dict = {
                'all_clothes': mask_uint8,
                'upper_body': mask_uint8,  # SAMì€ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì•ˆë¨
                'lower_body': np.zeros_like(mask_uint8),
                'full_body': mask_uint8,
                'accessories': np.zeros_like(mask_uint8)
            }
            
            return {
                "masks": masks_dict,
                "confidence": float(best_score),
                "all_masks": masks,
                "all_scores": scores
            }
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}

class RealU2NetClothModel:
    """ì‹¤ì œ U2Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
        
    def load(self) -> bool:
        """U2Net ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # U2Net ì•„í‚¤í…ì²˜ ìƒì„±
            self.model = self._create_u2net_architecture()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if os.path.exists(self.model_path):
                try:
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                except:
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                
                # MPS í˜¸í™˜ì„±
                if self.device == "mps" and isinstance(checkpoint, dict):
                    for key, value in checkpoint.items():
                        if isinstance(value, torch.Tensor) and value.dtype == torch.float64:
                            checkpoint[key] = value.float()
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                self.model.load_state_dict(checkpoint, strict=False)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            
            logger.info(f"âœ… U2Net ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _create_u2net_architecture(self):
        """U2Net ì•„í‚¤í…ì²˜ ìƒì„±"""
        class U2NetForCloth(nn.Module):
            def __init__(self):
                super().__init__()
                # ì¸ì½”ë”
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(256, 512, 3, padding=1),
                    nn.ReLU(inplace=True),
                )
                
                # ë””ì½”ë”
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 64, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 1, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded
        
        return U2NetForCloth()
    
    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """U2Net ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return {"masks": {}, "confidence": 0.0}
            
            # ì „ì²˜ë¦¬
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                pil_image = image
            
            transform = transforms.Compose([
                transforms.Resize((320, 320)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0).to(self.device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                output = self.model(input_tensor)
                
            # í›„ì²˜ë¦¬
            mask = output.squeeze().cpu().numpy()
            mask = (mask * 255).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_size = image.shape[:2]
            mask_pil = Image.fromarray(mask).resize((original_size[1], original_size[0]), Image.Resampling.NEAREST)
            mask_resized = np.array(mask_pil)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„± (U2Netì€ ì´ì§„ ë§ˆìŠ¤í¬ì´ë¯€ë¡œ ì „ì²´ ì˜ë¥˜ë¡œ ì²˜ë¦¬)
            masks_dict = {
                'all_clothes': mask_resized,
                'upper_body': mask_resized,  # U2Netì€ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ì•ˆë¨
                'lower_body': np.zeros_like(mask_resized),
                'full_body': mask_resized,
                'accessories': np.zeros_like(mask_resized)
            }
            
            return {
                "masks": masks_dict,
                "confidence": float(np.mean(mask_resized) / 255.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ U2Net ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0}

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 8: ClothSegmentationStep ë©”ì¸ í´ë˜ìŠ¤ (Central Hub DI Container v7.0 ì—°ë™)
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - 50% ì½”ë“œ ë‹¨ì¶•
    âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net ì§€ì›
    âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›
    âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬
    """
    
    def __init__(self, **kwargs):
        """Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # ğŸ”¥ 1. í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
            self._initialize_step_attributes()
            
            # ğŸ”¥ 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub ìë™ ì—°ë™)
            super().__init__(step_name="ClothSegmentationStep", step_id=3, **kwargs)
            
            # ğŸ”¥ 3. Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_cloth_segmentation_specifics()
            
            # ğŸ”§ model_paths ì†ì„± í™•ì‹¤íˆ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            
            self.logger.info(f"âœ… {self.step_name} Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _initialize_step_attributes(self):
        """Step í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        self.ai_models = {}
        self.models_loading_status = {
            'deeplabv3plus': False,
            'maskrcnn': False,
            'sam_huge': False,
            'u2net_cloth': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        self.model_interface = None
        self.loaded_models = {}
        
        # Cloth Segmentation íŠ¹í™” ì†ì„±ë“¤
        self.segmentation_models = {}
        self.segmentation_ready = False
        self.cloth_cache = {}
        
        # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
        self.cloth_categories = {category.value: category.name.lower() 
                                for category in ClothCategory}
        
        # í†µê³„
        self.ai_stats = {
            'total_processed': 0,
            'deeplabv3_calls': 0,
            'sam_calls': 0,
            'u2net_calls': 0,
            'average_confidence': 0.0
        }
    
    def _initialize_cloth_segmentation_specifics(self):
        """Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •
            self.config = ClothSegmentationConfig()
            
            # ğŸ”§ í•µì‹¬ ì†ì„±ë“¤ ì•ˆì „ ì´ˆê¸°í™”
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            if not hasattr(self, 'ai_models'):
                self.ai_models = {}
            
            # ì‹œìŠ¤í…œ ìµœì í™”
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            
            # ì„±ëŠ¥ ë° ìºì‹±
            self.executor = ThreadPoolExecutor(
                max_workers=4 if self.is_m3_max else 2,
                thread_name_prefix="cloth_seg"
            )
            self.segmentation_cache = {}
            self.cache_lock = threading.RLock()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì´ˆê¸°í™”
            self.available_methods = []
            
            self.logger.debug(f"âœ… {self.step_name} íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Cloth Segmentation íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ğŸ”§ ìµœì†Œí•œì˜ ì†ì„±ë“¤ ë³´ì¥
            self.model_paths = {}
            self.ai_models = {}
            self.available_methods = []
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì •"""
        try:
            self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ")
            self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
            self.step_id = kwargs.get('step_id', 3)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.ai_models = {}
            self.model_paths = {}  # ğŸ”§ model_paths ê¸´ê¸‰ ì´ˆê¸°í™”
            self.ai_stats = {'total_processed': 0}
            self.config = ClothSegmentationConfig()
            self.cache_lock = threading.RLock()
            self.cloth_categories = {category.value: category.name.lower() 
                                    for category in ClothCategory}
        except Exception as e:
            print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
            # ğŸ†˜ ìµœí›„ì˜ ìˆ˜ë‹¨
            self.model_paths = {}
    
    def initialize(self) -> bool:
        """Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            logger.info(f"ğŸ”„ {self.step_name} Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ğŸ”¥ 1. Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©
            self._load_segmentation_models_via_central_hub()
            
            # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            # 3. BaseStepMixin ì´ˆê¸°í™”
            super_initialized = super().initialize() if hasattr(super(), 'initialize') else True
            
            self.is_initialized = True
            self.is_ready = True
            self.segmentation_ready = len(self.ai_models) > 0
            
            # ì„±ê³µë¥  ê³„ì‚°
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool))
            success_rate = (loaded_count / total_models * 100) if total_models > 0 else 0
            
            loaded_models = [k for k, v in self.models_loading_status.items() 
                           if isinstance(v, bool) and v]
            
            logger.info(f"âœ… {self.step_name} Central Hub AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            logger.info(f"   - ë¡œë”© ì„±ê³µë¥ : {loaded_count}/{total_models} ({success_rate:.1f}%)")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    def _load_segmentation_models_via_central_hub(self):
        """Central Hubë¥¼ í†µí•œ Segmentation ëª¨ë¸ ë¡œë”©"""
        try:
            if self.model_loader:  # Central Hubì—ì„œ ìë™ ì£¼ì…ë¨
                logger.info("ğŸ”„ Central Hub ModelLoaderë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”©...")
                
                # ğŸ”¥ 1. DeepLabV3+ ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 1)
                self._load_deeplabv3plus_model()
                
                # ğŸ”¥ 2. SAM ëª¨ë¸ ë¡œë”© (í´ë°± ì˜µì…˜)
                self._load_sam_model()
                
                # ğŸ”¥ 3. U2Net ëª¨ë¸ ë¡œë”© (í´ë°± ì˜µì…˜)
                self._load_u2net_model()
                
                # ğŸ”¥ 4. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ íƒì§€
                self._detect_model_paths()
                
            else:
                logger.warning("âš ï¸ Central Hub ModelLoader ì—†ìŒ - í´ë°± ëª¨ë¸ ìƒì„±")
                self._create_fallback_models()
                
        except Exception as e:
            logger.error(f"âŒ Central Hub ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_fallback_models()
    
    def _load_deeplabv3plus_model(self):
        """DeepLabV3+ ëª¨ë¸ ë¡œë”© (ìš°ì„ ìˆœìœ„ 1)"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
            
            checkpoint_paths = [
                "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation/deeplabv3plus_resnet101.pth",
                "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation/deeplabv3plus_xception.pth",
                "step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth",
                "ultra_models/deeplabv3_resnet101_ultra.pth"
            ]
            
            for model_path in checkpoint_paths:
                if os.path.exists(model_path):
                    deeplabv3_model = RealDeepLabV3PlusModel(model_path, self.device)
                    if deeplabv3_model.load():
                        self.ai_models['deeplabv3plus'] = deeplabv3_model
                        self.segmentation_models['deeplabv3plus'] = deeplabv3_model
                        self.models_loading_status['deeplabv3plus'] = True
                        self.model_paths['deeplabv3plus'] = model_path
                        self.logger.info(f"âœ… DeepLabV3+ ë¡œë”© ì™„ë£Œ: {model_path}")
                        return
            
            self.logger.warning("âš ï¸ DeepLabV3+ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"DeepLabV3+: {e}")
    
    def _load_sam_model(self):
        """SAM ëª¨ë¸ ë¡œë”© (í´ë°±)"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
                
            checkpoint_paths = [
                "ultra_models/sam_vit_h_4b8939.pth",  # GeometricMatchingStepê³¼ ê³µìœ 
                "step_04_geometric_matching/ultra_models/sam_vit_h_4b8939.pth",
                "step_03_cloth_segmentation/sam_vit_h_4b8939.pth"
            ]
            
            for model_path in checkpoint_paths:
                if os.path.exists(model_path):
                    sam_model = RealSAMModel(model_path, self.device)
                    if sam_model.load():
                        self.ai_models['sam_huge'] = sam_model
                        self.segmentation_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        self.model_paths['sam_huge'] = model_path
                        self.logger.info(f"âœ… SAM ë¡œë”© ì™„ë£Œ: {model_path}")
                        return
            
            self.logger.warning("âš ï¸ SAM ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"SAM: {e}")
    
    def _load_u2net_model(self):
        """U2Net ëª¨ë¸ ë¡œë”© (í´ë°±)"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
                
            checkpoint_paths = [
                "step_03_cloth_segmentation/u2net.pth",
                "ai_models/step_03_cloth_segmentation/u2net.pth",
                "ultra_models/u2net.pth"
            ]
            
            for model_path in checkpoint_paths:
                if os.path.exists(model_path):
                    u2net_model = RealU2NetClothModel(model_path, self.device)
                    if u2net_model.load():
                        self.ai_models['u2net_cloth'] = u2net_model
                        self.segmentation_models['u2net_cloth'] = u2net_model
                        self.models_loading_status['u2net_cloth'] = True
                        self.model_paths['u2net_cloth'] = model_path
                        self.logger.info(f"âœ… U2Net ë¡œë”© ì™„ë£Œ: {model_path}")
                        return
            
            self.logger.warning("âš ï¸ U2Net ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loading_status['loading_errors'].append(f"U2Net: {e}")
    
    def _detect_model_paths(self):
        """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ íƒì§€"""
        try:
            # ğŸ”§ model_paths ì†ì„± ì•ˆì „ì„± í™•ë³´
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
                
            # ê¸°ë³¸ ê²½ë¡œë“¤
            base_paths = [
                "step_03_cloth_segmentation/",
                "step_03_cloth_segmentation/ultra_models/",
                "step_04_geometric_matching/",  # SAM ê³µìœ 
                "step_04_geometric_matching/ultra_models/",
                "ai_models/step_03_cloth_segmentation/",
                "ultra_models/",
                "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation/"
            ]
            
            model_files = {
                'deeplabv3plus': ['deeplabv3plus_resnet101.pth', 'deeplabv3_resnet101_ultra.pth'],
                'sam_huge': ['sam_vit_h_4b8939.pth'],
                'u2net_cloth': ['u2net.pth', 'u2net_cloth.pth'],
                'maskrcnn': ['maskrcnn_resnet50_fpn.pth', 'maskrcnn_cloth_custom.pth']
            }
            
            # ëª¨ë¸ íŒŒì¼ íƒì§€
            for model_key, filenames in model_files.items():
                if model_key not in self.model_paths:  # ì´ë¯¸ ë¡œë“œëœ ê²ƒì€ ìŠ¤í‚µ
                    for filename in filenames:
                        for base_path in base_paths:
                            full_path = os.path.join(base_path, filename)
                            if os.path.exists(full_path):
                                self.model_paths[model_key] = full_path
                                self.logger.info(f"âœ… {model_key} ê²½ë¡œ ë°œê²¬: {full_path}")
                                break
                        if model_key in self.model_paths:
                            break
                            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            # ğŸ”§ ì•ˆì „ì„± ë³´ì¥
            if not hasattr(self, 'model_paths'):
                self.model_paths = {}
    
    def _create_fallback_models(self):
        """í´ë°± ëª¨ë¸ ìƒì„± (Central Hub ì—°ê²° ì‹¤íŒ¨ì‹œ)"""
        try:
            self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ìƒì„± ì¤‘...")
            
            # ê¸°ë³¸ DeepLabV3+ ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ì´)
            deeplabv3_model = RealDeepLabV3PlusModel("", self.device)
            deeplabv3_model.model = DeepLabV3PlusModel(num_classes=20)
            deeplabv3_model.model.to(self.device)
            deeplabv3_model.model.eval()
            deeplabv3_model.is_loaded = True
            
            self.ai_models['deeplabv3plus_fallback'] = deeplabv3_model
            self.segmentation_models['deeplabv3plus_fallback'] = deeplabv3_model
            self.models_loading_status['deeplabv3plus'] = True
            
            self.logger.info("âœ… í´ë°± DeepLabV3+ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        if 'deeplabv3plus' in self.ai_models or 'deeplabv3plus_fallback' in self.ai_models:
            methods.append(SegmentationMethod.DEEPLABV3_PLUS)
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
        if 'maskrcnn' in self.ai_models:
            methods.append(SegmentationMethod.MASK_RCNN)
        
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
        
        return methods
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (BaseStepMixin í‘œì¤€)
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ë™ê¸° AI ì¶”ë¡  ë¡œì§ - BaseStepMixin v20.0ì—ì„œ í˜¸ì¶œë¨
        
        AI íŒŒì´í”„ë¼ì¸:
        1. ê³ ê¸‰ ì „ì²˜ë¦¬ (í’ˆì§ˆ í‰ê°€, ì¡°ëª… ì •ê·œí™”)
        2. ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ (DeepLabV3+/SAM/U2Net)
        3. ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í¬ ìƒì„±
        4. í’ˆì§ˆ ê²€ì¦ ë° ì‹œê°í™”
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            start_time = time.time()
            
            # 0. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input:
                return self._create_emergency_result("imageê°€ ì—†ìŒ")
            
            image = processed_input['image']
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
                image_array = image
            elif PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
                image_array = np.array(image)
            else:
                return self._create_emergency_result("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ì´ì „ Step ë°ì´í„°
            person_parsing = processed_input.get('from_step_01', {})
            pose_info = processed_input.get('from_step_02', {})
            
            # ==============================================
            # ğŸ”¥ Phase 1: ê³ ê¸‰ ì „ì²˜ë¦¬
            # ==============================================
            
            preprocessing_start = time.time()
            
            # 1.1 ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
            quality_scores = self._assess_image_quality(image_array)
            
            # 1.2 ì¡°ëª… ì •ê·œí™”
            processed_image = self._normalize_lighting(image_array)
            
            # 1.3 ìƒ‰ìƒ ë³´ì •
            if self.config.enable_color_correction:
                processed_image = self._correct_colors(processed_image)
            
            preprocessing_time = time.time() - preprocessing_start
            self.ai_stats['preprocessing_time'] = self.ai_stats.get('preprocessing_time', 0) + preprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 2: ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜
            # ==============================================
            
            segmentation_start = time.time()
            
            # í’ˆì§ˆ ë ˆë²¨ ê²°ì •
            quality_level = self._determine_quality_level(processed_input, quality_scores)
            
            # ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
            segmentation_result = self._run_ai_segmentation_sync(
                processed_image, quality_level, person_parsing, pose_info
            )
            
            if not segmentation_result or not segmentation_result.get('masks'):
                # í´ë°±: ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„±
                segmentation_result = self._create_fallback_segmentation_result(processed_image.shape)
            
            segmentation_time = time.time() - segmentation_start
            self.ai_stats['segmentation_time'] = self.ai_stats.get('segmentation_time', 0) + segmentation_time
            
            # ==============================================
            # ğŸ”¥ Phase 3: í›„ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦
            # ==============================================
            
            postprocessing_start = time.time()
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            processed_masks = self._postprocess_masks(segmentation_result['masks'])
            
            # í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_segmentation_quality(processed_masks, processed_image)
            
            # ì‹œê°í™” ìƒì„±
            visualizations = self._create_segmentation_visualizations(processed_image, processed_masks)
            
            postprocessing_time = time.time() - postprocessing_start
            self.ai_stats['postprocessing_time'] = self.ai_stats.get('postprocessing_time', 0) + postprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 4: ê²°ê³¼ ìƒì„±
            # ==============================================
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_time = time.time() - start_time
            self._update_ai_stats(segmentation_result.get('method_used', 'unknown'), 
                                segmentation_result.get('confidence', 0.0), total_time, quality_metrics)
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€
            cloth_categories = self._detect_cloth_categories(processed_masks)
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (BaseStepMixin í‘œì¤€)
            ai_result = {
                # í•µì‹¬ ê²°ê³¼
                'success': True,
                'step': self.step_name,
                'segmentation_masks': processed_masks,
                'cloth_categories': cloth_categories,
                'segmentation_confidence': segmentation_result.get('confidence', 0.0),
                'processing_time': total_time,
                'model_used': segmentation_result.get('method_used', 'unknown'),
                'items_detected': len([cat for cat in cloth_categories if cat != 'background']),
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                'quality_score': quality_metrics.get('overall', 0.5),
                'quality_metrics': quality_metrics,
                'image_quality_scores': quality_scores,
                
                # ì „ì²˜ë¦¬ ê²°ê³¼
                'preprocessing_results': {
                    'lighting_normalized': self.config.enable_lighting_normalization,
                    'color_corrected': self.config.enable_color_correction,
                    'quality_assessed': self.config.enable_quality_assessment
                },
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                'performance_breakdown': {
                    'preprocessing_time': preprocessing_time,
                    'segmentation_time': segmentation_time,
                    'postprocessing_time': postprocessing_time
                },
                
                # ì‹œê°í™”
                **visualizations,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'ai_models_loaded': list(self.ai_models.keys()),
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'ai_enhanced': True,
                    'quality_level': quality_level.value,
                    'version': '33.0',
                    'central_hub_connected': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'num_classes': 20,
                    'segmentation_method': segmentation_result.get('method_used', 'unknown')
                },
                
                # Step ê°„ ì—°ë™ ë°ì´í„°
                'cloth_features': self._extract_cloth_features(processed_masks, processed_image),
                'cloth_contours': self._extract_cloth_contours(processed_masks.get('all_clothes', np.array([]))),
                'parsing_map': segmentation_result.get('parsing_map', np.array([]))
            }
            
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - {total_time:.2f}ì´ˆ")
            self.logger.info(f"   - ë°©ë²•: {segmentation_result.get('method_used', 'unknown')}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {segmentation_result.get('confidence', 0.0):.3f}")
            self.logger.info(f"   - í’ˆì§ˆ: {quality_metrics.get('overall', 0.5):.3f}")
            self.logger.info(f"   - íƒì§€ëœ ì•„ì´í…œ: {len([cat for cat in cloth_categories if cat != 'background'])}ê°œ")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_emergency_result(str(e))
    
    # ==============================================
    # ğŸ”¥ AI í—¬í¼ ë©”ì„œë“œë“¤ (í•µì‹¬ ë¡œì§)
    # ==============================================
    
    def _assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_scores = {}
            
            # ë¸”ëŸ¬ ì •ë„ ì¸¡ì •
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            if NUMPY_AVAILABLE:
                grad_x = np.abs(np.diff(gray, axis=1))
                grad_y = np.abs(np.diff(gray, axis=0))
                sharpness = np.mean(grad_x) + np.mean(grad_y)
                quality_scores['sharpness'] = min(sharpness / 100.0, 1.0)
            else:
                quality_scores['sharpness'] = 0.5
            
            # ëŒ€ë¹„ ì¸¡ì •
            contrast = np.std(gray) if NUMPY_AVAILABLE else 50.0
            quality_scores['contrast'] = min(contrast / 128.0, 1.0)
            
            # í•´ìƒë„ í’ˆì§ˆ
            height, width = image.shape[:2]
            resolution_score = min((height * width) / (512 * 512), 1.0)
            quality_scores['resolution'] = resolution_score
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_scores['overall'] = np.mean(list(quality_scores.values())) if NUMPY_AVAILABLE else 0.5
            
            return quality_scores
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5, 'sharpness': 0.5, 'contrast': 0.5, 'resolution': 0.5}
    
    def _normalize_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            if not self.config.enable_lighting_normalization:
                return image
            
            if len(image.shape) == 3:
                # ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                normalized = np.zeros_like(image)
                for i in range(3):
                    channel = image[:, :, i]
                    channel_min, channel_max = channel.min(), channel.max()
                    if channel_max > channel_min:
                        normalized[:, :, i] = ((channel - channel_min) / (channel_max - channel_min) * 255).astype(np.uint8)
                    else:
                        normalized[:, :, i] = channel
                return normalized
            else:
                img_min, img_max = image.min(), image.max()
                if img_max > img_min:
                    return ((image - img_min) / (img_max - img_min) * 255).astype(np.uint8)
                else:
                    return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def _correct_colors(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if PIL_AVAILABLE and len(image.shape) == 3:
                pil_image = Image.fromarray(image)
                
                # ìë™ ëŒ€ë¹„ ì¡°ì •
                enhancer = ImageEnhance.Contrast(pil_image)
                enhanced = enhancer.enhance(1.2)
                
                # ìƒ‰ìƒ ì±„ë„ ì¡°ì •
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.1)
                
                return np.array(enhanced)
            else:
                return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _determine_quality_level(self, processed_input: Dict[str, Any], quality_scores: Dict[str, float]) -> QualityLevel:
        """í’ˆì§ˆ ë ˆë²¨ ê²°ì •"""
        try:
            # ì‚¬ìš©ì ì„¤ì • ìš°ì„ 
            if 'quality_level' in processed_input:
                user_level = processed_input['quality_level']
                if isinstance(user_level, str):
                    try:
                        return QualityLevel(user_level)
                    except ValueError:
                        pass
                elif isinstance(user_level, QualityLevel):
                    return user_level
            
            # ìë™ ê²°ì •
            overall_quality = quality_scores.get('overall', 0.5)
            
            if self.is_m3_max and overall_quality > 0.7:
                return QualityLevel.ULTRA
            elif overall_quality > 0.6:
                return QualityLevel.HIGH
            elif overall_quality > 0.4:
                return QualityLevel.BALANCED
            else:
                return QualityLevel.FAST
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë ˆë²¨ ê²°ì • ì‹¤íŒ¨: {e}")
            return QualityLevel.BALANCED
    
    def _run_ai_segmentation_sync(
        self, 
        image: np.ndarray, 
        quality_level: QualityLevel, 
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë™ê¸°)"""
        try:
            if quality_level == QualityLevel.ULTRA and 'deeplabv3plus' in self.ai_models:
                # DeepLabV3+ ì‚¬ìš© (ìµœê³  í’ˆì§ˆ)
                result = self.ai_models['deeplabv3plus'].predict(image)
                self.ai_stats['deeplabv3_calls'] += 1
                result['method_used'] = 'deeplabv3plus'
                return result
                
            elif quality_level == QualityLevel.ULTRA and 'deeplabv3plus_fallback' in self.ai_models:
                # DeepLabV3+ í´ë°± ì‚¬ìš©
                result = self.ai_models['deeplabv3plus_fallback'].predict(image)
                self.ai_stats['deeplabv3_calls'] += 1
                result['method_used'] = 'deeplabv3plus_fallback'
                return result
                
            elif quality_level in [QualityLevel.HIGH, QualityLevel.BALANCED] and 'sam_huge' in self.ai_models:
                # SAM ì‚¬ìš© (ê³ í’ˆì§ˆ)
                prompts = self._generate_sam_prompts(image, person_parsing)
                result = self.ai_models['sam_huge'].predict(image, prompts)
                self.ai_stats['sam_calls'] += 1
                result['method_used'] = 'sam_huge'
                return result
                
            elif 'u2net_cloth' in self.ai_models:
                # U2Net ì‚¬ìš© (ê· í˜•)
                result = self.ai_models['u2net_cloth'].predict(image)
                self.ai_stats['u2net_calls'] += 1
                result['method_used'] = 'u2net_cloth'
                return result
                
            else:
                # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” (ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©)
                return self._run_hybrid_ensemble_sync(image, person_parsing)
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0, "method_used": "error"}
    
    def _generate_sam_prompts(self, image: np.ndarray, person_parsing: Dict[str, Any]) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            prompts = {}
            
            # ì¤‘ì•™ í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸
            h, w = image.shape[:2]
            center_points = [
                (w // 2, h // 2),           # ì¤‘ì•™
                (w // 3, h // 2),           # ì¢Œì¸¡
                (2 * w // 3, h // 2),       # ìš°ì¸¡
            ]
            
            prompts['points'] = center_points
            prompts['labels'] = [1, 1, 1]  # ëª¨ë‘ positive
            
            # Person parsing ì •ë³´ í™œìš©
            if person_parsing and 'clothing_regions' in person_parsing:
                clothing_regions = person_parsing['clothing_regions']
                if clothing_regions:
                    # ì˜ë¥˜ ì˜ì—­ì˜ ì¤‘ì‹¬ì ë“¤ ì¶”ê°€
                    for region in clothing_regions[:3]:  # ìµœëŒ€ 3ê°œ
                        if 'center' in region:
                            center = region['center']
                            prompts['points'].append((center[0], center[1]))
                            prompts['labels'].append(1)
            
            return prompts
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ SAM í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2]
            return {
                'points': [(w // 2, h // 2)],
                'labels': [1]
            }
    
    def _run_hybrid_ensemble_sync(self, image: np.ndarray, person_parsing: Dict[str, Any]) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ (ë™ê¸°)"""
        try:
            results = []
            methods_used = []
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰
            for model_key, model in self.ai_models.items():
                try:
                    if model_key.startswith('deeplabv3'):
                        result = model.predict(image)
                        if result.get('masks'):
                            results.append(result)
                            methods_used.append(model_key)
                    elif model_key.startswith('sam'):
                        prompts = self._generate_sam_prompts(image, person_parsing)
                        result = model.predict(image, prompts)
                        if result.get('masks'):
                            results.append(result)
                            methods_used.append(model_key)
                    elif model_key.startswith('u2net'):
                        result = model.predict(image)
                        if result.get('masks'):
                            results.append(result)
                            methods_used.append(model_key)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_key} ì•™ìƒë¸” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # ì•™ìƒë¸” ê²°í•©
            if len(results) >= 2:
                # ê°€ì¤‘ í‰ê·  (ì‹ ë¢°ë„ ê¸°ë°˜)
                confidences = [r.get('confidence', 0.0) for r in results]
                total_confidence = sum(confidences)
                
                if total_confidence > 0:
                    # ë§ˆìŠ¤í¬ ì•™ìƒë¸”
                    ensemble_masks = {}
                    for mask_key in ['all_clothes', 'upper_body', 'lower_body', 'full_body', 'accessories']:
                        mask_list = []
                        for result, conf in zip(results, confidences):
                            if mask_key in result.get('masks', {}):
                                mask = result['masks'][mask_key].astype(np.float32) / 255.0
                                weight = conf / total_confidence
                                mask_list.append(mask * weight)
                        
                        if mask_list:
                            ensemble_mask = np.sum(mask_list, axis=0)
                            ensemble_masks[mask_key] = (ensemble_mask > 0.5).astype(np.uint8) * 255
                    
                    return {
                        'masks': ensemble_masks,
                        'confidence': np.mean(confidences),
                        'method_used': f"hybrid_{'+'.join(methods_used)}"
                    }
            
            # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼
            elif len(results) == 1:
                results[0]['method_used'] = methods_used[0]
                return results[0]
            
            # ì‹¤íŒ¨
            return {"masks": {}, "confidence": 0.0, "method_used": "ensemble_failed"}
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"masks": {}, "confidence": 0.0, "method_used": "ensemble_error"}
    
    def _create_fallback_segmentation_result(self, image_shape: Tuple[int, ...]) -> Dict[str, Any]:
        """í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìƒì„±"""
        try:
            height, width = image_shape[:2]
            
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ë“¤ ìƒì„±
            upper_mask = np.zeros((height, width), dtype=np.uint8)
            lower_mask = np.zeros((height, width), dtype=np.uint8)
            
            # ìƒì˜ ì˜ì—­ (ìƒë‹¨ 1/3)
            upper_mask[height//4:height//2, width//4:3*width//4] = 255
            
            # í•˜ì˜ ì˜ì—­ (í•˜ë‹¨ 1/3)  
            lower_mask[height//2:3*height//4, width//3:2*width//3] = 255
            
            masks = {
                "upper_body": upper_mask,
                "lower_body": lower_mask,
                "full_body": upper_mask + lower_mask,
                "accessories": np.zeros((height, width), dtype=np.uint8),
                "all_clothes": upper_mask + lower_mask
            }
            
            return {
                "masks": masks,
                "confidence": 0.5,
                "method_used": "fallback",
                "parsing_map": upper_mask + lower_mask
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            height, width = 512, 512
            return {
                "masks": {
                    "all_clothes": np.zeros((height, width), dtype=np.uint8)
                },
                "confidence": 0.0,
                "method_used": "emergency"
            }
    
    def _fill_holes_and_remove_noise_advanced(self, masks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° (ì›ë³¸)"""
        try:
            processed_masks = {}
            
            for mask_key, mask in masks.items():
                if mask is None or mask.size == 0:
                    processed_masks[mask_key] = mask
                    continue
                
                processed_mask = mask.copy()
                
                # 1. í™€ ì±„ìš°ê¸° (SciPy ì‚¬ìš©)
                if SCIPY_AVAILABLE:
                    filled = ndimage.binary_fill_holes(processed_mask > 128)
                    processed_mask = (filled * 255).astype(np.uint8)
                
                # 2. ëª¨í´ë¡œì§€ ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
                if SCIPY_AVAILABLE:
                    # Opening (ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°)
                    structure = ndimage.generate_binary_structure(2, 2)
                    opened = ndimage.binary_opening(processed_mask > 128, structure=structure, iterations=1)
                    
                    # Closing (ì‘ì€ í™€ ì±„ìš°ê¸°)
                    closed = ndimage.binary_closing(opened, structure=structure, iterations=2)
                    
                    processed_mask = (closed * 255).astype(np.uint8)
                
                # 3. ì‘ì€ ì—°ê²° êµ¬ì„±ìš”ì†Œ ì œê±° (Scikit-image ì‚¬ìš©)
                if SKIMAGE_AVAILABLE:
                    labeled = measure.label(processed_mask > 128)
                    regions = measure.regionprops(labeled)
                    
                    # ë©´ì ì´ ì‘ì€ ì˜ì—­ ì œê±° (ì „ì²´ ì´ë¯¸ì§€ì˜ 1% ì´í•˜)
                    min_area = processed_mask.size * 0.01
                    
                    for region in regions:
                        if region.area < min_area:
                            processed_mask[labeled == region.label] = 0
                
                processed_masks[mask_key] = processed_mask
            
            return processed_masks
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return masks
    
    def _evaluate_segmentation_quality(self, masks: Dict[str, np.ndarray], image: np.ndarray) -> Dict[str, float]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_metrics = {}
            
            if 'all_clothes' in masks:
                mask = masks['all_clothes']
                
                # 1. ì˜ì—­ í¬ê¸° ì ì ˆì„±
                size_ratio = np.sum(mask > 128) / mask.size if NUMPY_AVAILABLE and mask.size > 0 else 0
                if 0.1 <= size_ratio <= 0.7:  # ì ì ˆí•œ í¬ê¸° ë²”ìœ„
                    quality_metrics['size_appropriateness'] = 1.0
                else:
                    quality_metrics['size_appropriateness'] = max(0.0, 1.0 - abs(size_ratio - 0.3) / 0.3)
                
                # 2. ì—°ì†ì„± (ì—°ê²°ëœ êµ¬ì„±ìš”ì†Œ)
                if SKIMAGE_AVAILABLE and mask.size > 0:
                    labeled = measure.label(mask > 128)
                    num_components = labeled.max() if labeled.max() > 0 else 0
                    if num_components > 0:
                        total_area = np.sum(mask > 128)
                        component_sizes = [np.sum(labeled == i) for i in range(1, num_components + 1)]
                        largest_component = max(component_sizes) if component_sizes else 0
                        quality_metrics['continuity'] = largest_component / total_area if total_area > 0 else 0.0
                    else:
                        quality_metrics['continuity'] = 0.0
                else:
                    quality_metrics['continuity'] = 0.5
                
                # 3. ê²½ê³„ì„  í’ˆì§ˆ
                if NUMPY_AVAILABLE and mask.size > 0:
                    # ê²½ê³„ì„  ê¸¸ì´ vs ë©´ì  ë¹„ìœ¨
                    edges = np.abs(np.diff(mask.astype(np.float32), axis=1)) + np.abs(np.diff(mask.astype(np.float32), axis=0))
                    edge_length = np.sum(edges > 10)
                    area = np.sum(mask > 128)
                    if area > 0:
                        boundary_ratio = edge_length / np.sqrt(area)
                        quality_metrics['boundary_quality'] = min(1.0, max(0.0, 1.0 - boundary_ratio / 10.0))
                    else:
                        quality_metrics['boundary_quality'] = 0.0
                else:
                    quality_metrics['boundary_quality'] = 0.5
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            if quality_metrics:
                quality_metrics['overall'] = np.mean(list(quality_metrics.values())) if NUMPY_AVAILABLE else 0.5
            else:
                quality_metrics['overall'] = 0.5
            
            return quality_metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5}
    
    def _create_segmentation_visualizations(self, image: np.ndarray, masks: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œê°í™” ìƒì„±"""
        try:
            visualizations = {}
            
            if not masks:
                return visualizations
            
            # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
            if 'all_clothes' in masks and PIL_AVAILABLE:
                try:
                    overlay_img = image.copy()
                    mask = masks['all_clothes']
                    
                    # ë¹¨ê°„ìƒ‰ ì˜¤ë²„ë ˆì´
                    overlay_img[mask > 128] = [255, 0, 0]
                    
                    # ë¸”ë Œë”©
                    alpha = 0.6
                    blended = (alpha * overlay_img + (1 - alpha) * image).astype(np.uint8)
                    visualizations['mask_overlay'] = blended
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°í™”
            try:
                category_colors = {
                    'upper_body': [255, 0, 0],    # ë¹¨ê°•
                    'lower_body': [0, 255, 0],    # ì´ˆë¡
                    'full_body': [0, 0, 255],     # íŒŒë‘
                    'accessories': [255, 255, 0]  # ë…¸ë‘
                }
                
                category_overlay = image.copy()
                for category, color in category_colors.items():
                    if category in masks:
                        mask = masks[category]
                        category_overlay[mask > 128] = color
                
                # ë¸”ë Œë”©
                alpha = 0.5
                category_blended = (alpha * category_overlay + (1 - alpha) * image).astype(np.uint8)
                visualizations['category_overlay'] = category_blended
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ë¶„í• ëœ ì˜ë¥˜ ì´ë¯¸ì§€
            if 'all_clothes' in masks:
                try:
                    mask = masks['all_clothes']
                    segmented = image.copy()
                    segmented[mask <= 128] = [0, 0, 0]  # ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ
                    visualizations['segmented_clothing'] = segmented
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë¶„í• ëœ ì˜ë¥˜ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _detect_cloth_categories(self, masks: Dict[str, np.ndarray]) -> List[str]:
        """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€"""
        try:
            detected_categories = []
            
            for mask_key, mask in masks.items():
                if mask is not None and np.sum(mask > 128) > 100:  # ìµœì†Œ í”½ì…€ ìˆ˜ ì²´í¬
                    if mask_key == 'upper_body':
                        detected_categories.extend(['shirt', 't_shirt'])
                    elif mask_key == 'lower_body':
                        detected_categories.extend(['pants', 'jeans'])
                    elif mask_key == 'full_body':
                        detected_categories.append('dress')
                    elif mask_key == 'accessories':
                        detected_categories.extend(['shoes', 'bag'])
            
            # ì¤‘ë³µ ì œê±°
            detected_categories = list(set(detected_categories))
            
            # ë°°ê²½ì€ í•­ìƒ í¬í•¨
            if 'background' not in detected_categories:
                detected_categories.insert(0, 'background')
            
            return detected_categories
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return ['background']
    
    def _extract_cloth_features(self, masks: Dict[str, np.ndarray], image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            features = {}
            
            if 'all_clothes' in masks:
                mask = masks['all_clothes']
                
                if NUMPY_AVAILABLE and mask.size > 0:
                    # ê¸°ë³¸ í†µê³„
                    features['area'] = int(np.sum(mask > 128))
                    features['centroid'] = self._calculate_centroid(mask)
                    features['bounding_box'] = self._calculate_bounding_box(mask)
                    
                    # ìƒ‰ìƒ íŠ¹ì§•
                    if len(image.shape) == 3:
                        masked_pixels = image[mask > 128]
                        if len(masked_pixels) > 0:
                            features['dominant_color'] = [
                                float(np.mean(masked_pixels[:, 0])),
                                float(np.mean(masked_pixels[:, 1])),
                                float(np.mean(masked_pixels[:, 2]))
                            ]
                        else:
                            features['dominant_color'] = [0.0, 0.0, 0.0]
            
            return features
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_centroid(self, mask: np.ndarray) -> Tuple[float, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE and mask.size > 0:
                y_coords, x_coords = np.where(mask > 128)
                if len(x_coords) > 0:
                    centroid_x = float(np.mean(x_coords))
                    centroid_y = float(np.mean(y_coords))
                    return (centroid_x, centroid_y)
            
            # í´ë°±
            h, w = mask.shape if mask.size > 0 else (512, 512)
            return (w / 2.0, h / 2.0)
            
        except Exception:
            return (256.0, 256.0)
    
    def _calculate_bounding_box(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """ê²½ê³„ ë°•ìŠ¤ ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE and mask.size > 0:
                rows = np.any(mask > 128, axis=1)
                cols = np.any(mask > 128, axis=0)
                
                if np.any(rows) and np.any(cols):
                    rmin, rmax = np.where(rows)[0][[0, -1]]
                    cmin, cmax = np.where(cols)[0][[0, -1]]
                    return (int(cmin), int(rmin), int(cmax), int(rmax))
            
            # í´ë°±
            h, w = mask.shape if mask.size > 0 else (512, 512)
            return (0, 0, w, h)
            
        except Exception:
            return (0, 0, 512, 512)
    
    def _extract_cloth_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """ì˜ë¥˜ ìœ¤ê³½ì„  ì¶”ì¶œ"""
        try:
            contours = []
            
            if SKIMAGE_AVAILABLE and mask.size > 0:
                # ìœ¤ê³½ì„  ì°¾ê¸°
                contour_coords = measure.find_contours(mask > 128, 0.5)
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                for contour in contour_coords:
                    if len(contour) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                        contours.append(contour.astype(np.int32))
            
            return contours
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _update_ai_stats(self, method: str, confidence: float, total_time: float, quality_metrics: Dict[str, float]):
        """AI í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.ai_stats['total_processed'] += 1
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            prev_avg = self.ai_stats.get('average_confidence', 0.0)
            count = self.ai_stats['total_processed']
            self.ai_stats['average_confidence'] = (prev_avg * (count - 1) + confidence) / count
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_emergency_result(self, reason: str) -> Dict[str, Any]:
        """ë¹„ìƒ ê²°ê³¼ ìƒì„±"""
        emergency_masks = {
            'all_clothes': np.zeros((512, 512), dtype=np.uint8),
            'upper_body': np.zeros((512, 512), dtype=np.uint8),
            'lower_body': np.zeros((512, 512), dtype=np.uint8),
            'full_body': np.zeros((512, 512), dtype=np.uint8),
            'accessories': np.zeros((512, 512), dtype=np.uint8)
        }
        
        return {
            'success': False,
            'step': self.step_name,
            'segmentation_masks': emergency_masks,
            'cloth_categories': ['background'],
            'segmentation_confidence': 0.0,
            'processing_time': 0.1,
            'model_used': 'emergency',
            'items_detected': 0,
            'emergency_reason': reason[:100],
            'metadata': {
                'emergency_mode': True,
                'version': '33.0',
                'central_hub_connected': False
            }
        }
    
    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return list(self.ai_models.keys())
    
    def get_model_info(self, model_key: str = None) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if model_key:
            if model_key in self.ai_models:
                return {
                    'model_key': model_key,
                    'model_path': self.model_paths.get(model_key, 'unknown'),
                    'is_loaded': self.models_loading_status.get(model_key, False),
                    'model_type': self._get_model_type(model_key)
                }
            else:
                return {}
        else:
            return {
                key: {
                    'model_path': self.model_paths.get(key, 'unknown'),
                    'is_loaded': self.models_loading_status.get(key, False),
                    'model_type': self._get_model_type(key)
                }
                for key in self.ai_models.keys()
            }
    
    def _get_model_type(self, model_key: str) -> str:
        """ëª¨ë¸ í‚¤ì—ì„œ ëª¨ë¸ íƒ€ì… ì¶”ë¡ """
        type_mapping = {
            'deeplabv3plus': 'DeepLabV3PlusModel',
            'deeplabv3plus_fallback': 'DeepLabV3PlusModel',
            'sam_huge': 'SAMModel',
            'u2net_cloth': 'U2NetModel',
            'maskrcnn': 'MaskRCNNModel'
        }
        return type_mapping.get(model_key, 'BaseModel')
    
    def get_segmentation_stats(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„ ë°˜í™˜"""
        return dict(self.ai_stats)
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self.cache_lock:
                self.segmentation_cache.clear()
                self.cloth_cache.clear()
                self.logger.info("âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def reload_models(self):
        """AI ëª¨ë¸ ì¬ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ AI ëª¨ë¸ ì¬ë¡œë”© ì‹œì‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬
            self.ai_models.clear()
            self.segmentation_models.clear()
            for key in self.models_loading_status:
                if isinstance(self.models_loading_status[key], bool):
                    self.models_loading_status[key] = False
            
            # Central Hubë¥¼ í†µí•œ ì¬ë¡œë”©
            self._load_segmentation_models_via_central_hub()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ì¬ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool))
            self.logger.info(f"âœ… AI ëª¨ë¸ ì¬ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì¬ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def validate_configuration(self) -> Dict[str, Any]:
        """ì„¤ì • ê²€ì¦"""
        try:
            validation_result = {
                'valid': True,
                'errors': [],
                'warnings': [],
                'info': {}
            }
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ ê²€ì¦
            loaded_count = sum(1 for status in self.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            if loaded_count == 0:
                validation_result['errors'].append("AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                validation_result['valid'] = False
            elif loaded_count < 2:
                validation_result['warnings'].append(f"ì¼ë¶€ AI ëª¨ë¸ë§Œ ë¡œë“œë¨: {loaded_count}ê°œ")
            
            # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
            if not TORCH_AVAILABLE:
                validation_result['errors'].append("PyTorchê°€ í•„ìš”í•¨")
                validation_result['valid'] = False
            
            if not PIL_AVAILABLE:
                validation_result['errors'].append("PILì´ í•„ìš”í•¨")
                validation_result['valid'] = False
            
            # ê²½ê³ ì‚¬í•­
            if not SAM_AVAILABLE:
                validation_result['warnings'].append("SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
            
            # ì •ë³´
            validation_result['info'] = {
                'models_loaded': loaded_count,
                'available_methods': len(self.available_methods),
                'device': self.device,
                'quality_level': self.config.quality_level.value,
                'central_hub_connected': hasattr(self, 'model_loader') and self.model_loader is not None
            }
            
            return validation_result
            
        except Exception as e:
            return {
                'valid': False,
                'errors': [f"ê²€ì¦ ì‹¤íŒ¨: {e}"],
                'warnings': [],
                'info': {}
            }

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 8: íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„±"""
    m3_config = ClothSegmentationConfig(
        method=SegmentationMethod.DEEPLABV3_PLUS,
        quality_level=QualityLevel.ULTRA,
        enable_visualization=True,
        input_size=(512, 512),
        confidence_threshold=0.5
    )
    
    kwargs['segmentation_config'] = m3_config
    return ClothSegmentationStep(**kwargs)

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 9: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

def test_cloth_segmentation_ai():
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ (Central Hub DI Container v7.0)")
        print("=" * 80)
        
        # Step ìƒì„±
        step = create_cloth_segmentation_step(
            device="auto",
            segmentation_config=ClothSegmentationConfig(
                quality_level=QualityLevel.HIGH,
                enable_visualization=True,
                confidence_threshold=0.5
            )
        )
        
        # ì´ˆê¸°í™”
        if step.initialize():
            print(f"âœ… Step ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - ë¡œë“œëœ AI ëª¨ë¸: {len(step.ai_models)}ê°œ")
            print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {len(step.available_methods)}ê°œ")
            
            # ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ê³„ì‚°
            loaded_count = sum(1 for status in step.models_loading_status.values() 
                             if isinstance(status, bool) and status)
            total_models = sum(1 for status in step.models_loading_status.values() 
                             if isinstance(status, bool))
            success_rate = (loaded_count / total_models * 100) if total_models > 0 else 0
            print(f"   - ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ : {loaded_count}/{total_models} ({success_rate:.1f}%)")
        else:
            print(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        test_image_array = np.array(test_image)
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        processed_input = {
            'image': test_image_array,
            'from_step_01': {},
            'from_step_02': {}
        }
        
        result = step._run_ai_inference(processed_input)
        
        if result and result.get('success', False):
            print(f"âœ… AI ì¶”ë¡  ì„±ê³µ")
            print(f"   - ë°©ë²•: {result.get('model_used', 'unknown')}")
            print(f"   - ì‹ ë¢°ë„: {result.get('segmentation_confidence', 0):.3f}")
            print(f"   - í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
            print(f"   - íƒì§€ëœ ì•„ì´í…œ: {result.get('items_detected', 0)}ê°œ")
            print(f"   - ì¹´í…Œê³ ë¦¬: {result.get('cloth_categories', [])}")
            print(f"   - Central Hub ì—°ê²°: {result.get('metadata', {}).get('central_hub_connected', False)}")
        else:
            print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_central_hub_compatibility():
    """Central Hub DI Container í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ Central Hub DI Container v7.0 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # Step ìƒì„±
        step = ClothSegmentationStep()
        
        # BaseStepMixin ìƒì† í™•ì¸
        print(f"âœ… BaseStepMixin ìƒì†: {isinstance(step, BaseStepMixin)}")
        print(f"âœ… Step ì´ë¦„: {step.step_name}")
        print(f"âœ… Step ID: {step.step_id}")
        
        # _run_ai_inference ë©”ì„œë“œ í™•ì¸
        import inspect
        is_async = inspect.iscoroutinefunction(step._run_ai_inference)
        print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async}")
        
        # í•„ìˆ˜ ì†ì„±ë“¤ í™•ì¸
        required_attrs = ['ai_models', 'models_loading_status', 'model_interface', 'loaded_models']
        for attr in required_attrs:
            has_attr = hasattr(step, attr)
            print(f"âœ… {attr} ì†ì„± ì¡´ì¬: {has_attr}")
        
        # Central Hub ì—°ê²° í™•ì¸
        central_hub_connected = hasattr(step, 'model_loader')
        print(f"âœ… Central Hub ì—°ê²°: {central_hub_connected}")
        
        print("âœ… Central Hub DI Container í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ Central Hub í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì„¹ì…˜ 10: ëª¨ë“ˆ ì •ë³´ ë° __all__
# ==============================================

__version__ = "33.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™"
__compatibility_version__ = "BaseStepMixin_v20.0"

__all__ = [
    'ClothSegmentationStep',
    'RealDeepLabV3PlusModel',
    'RealSAMModel',
    'RealU2NetClothModel',
    'DeepLabV3PlusModel',
    'DeepLabV3PlusBackbone',
    'ASPPModule',
    'SelfCorrectionModule',
    'SelfAttentionBlock',
    'SegmentationMethod',
    'ClothCategory',
    'QualityLevel',
    'ClothSegmentationConfig',
    'create_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    'test_cloth_segmentation_ai',
    'test_central_hub_compatibility'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger.info("=" * 120)
logger.info("ğŸ”¥ Step 03 Cloth Segmentation v33.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("=" * 120)
logger.info("ğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - 50% ì½”ë“œ ë‹¨ì¶•")
logger.info("   âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ë³µì› - DeepLabV3+, SAM, U2Net ì§€ì›")
logger.info("   âœ… ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ 100% ìœ ì§€ - ASPP, Self-Correction, Progressive Parsing")
logger.info("   âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ - 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì§€ì›")
logger.info("   âœ… ì¹´í…Œê³ ë¦¬ë³„ ë§ˆìŠ¤í‚¹ - ìƒì˜/í•˜ì˜/ì „ì‹ /ì•¡ì„¸ì„œë¦¬ ë¶„ë¦¬")
logger.info("   âœ… ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ - Mock ì œê±°í•˜ê³  ì§„ì§œ ëª¨ë¸ ì‚¬ìš©")

logger.info("ğŸ§  êµ¬í˜„ëœ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ (ì™„ì „ ë³µì›):")
logger.info("   ğŸ”¥ DeepLabV3+ ì•„í‚¤í…ì²˜ (Google ìµœì‹  ì„¸ê·¸ë©˜í…Œì´ì…˜)")
logger.info("   ğŸŒŠ ASPP (Atrous Spatial Pyramid Pooling) ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ” Self-Correction Learning ë©”ì»¤ë‹ˆì¦˜")
logger.info("   ğŸ“ˆ Progressive Parsing ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ¯ SAM + U2Net + DeepLabV3+ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”")
logger.info("   âš¡ CRF í›„ì²˜ë¦¬ + ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬")
logger.info("   ğŸ”€ Edge Detection ë¸Œëœì¹˜")
logger.info("   ğŸ’« Multi-scale Feature Fusion")
logger.info("   ğŸ¨ ê³ ê¸‰ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°")
logger.info("   ğŸ” ROI ê²€ì¶œ ë° ë°°ê²½ ë¶„ì„")
logger.info("   ğŸŒˆ ì¡°ëª… ì •ê·œí™” ë° ìƒ‰ìƒ ë³´ì •")
logger.info("   ğŸ“Š í’ˆì§ˆ í‰ê°€ ë° ìë™ ì¬ì‹œë„")

logger.info("ğŸ¨ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (20ê°œ í´ë˜ìŠ¤):")
logger.info("   - ìƒì˜: ì…”ì¸ , í‹°ì…”ì¸ , ìŠ¤ì›¨í„°, í›„ë“œí‹°, ì¬í‚·, ì½”íŠ¸")
logger.info("   - í•˜ì˜: ë°”ì§€, ì²­ë°”ì§€, ë°˜ë°”ì§€, ìŠ¤ì»¤íŠ¸")
logger.info("   - ì „ì‹ : ì›í”¼ìŠ¤")
logger.info("   - ì•¡ì„¸ì„œë¦¬: ì‹ ë°œ, ë¶€ì¸ , ìš´ë™í™”, ê°€ë°©, ëª¨ì, ì•ˆê²½, ìŠ¤ì¹´í”„, ë²¨íŠ¸")

logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - SAM: {SAM_AVAILABLE}")
logger.info(f"   - SciPy: {SCIPY_AVAILABLE}")
logger.info(f"   - Scikit-image: {SKIMAGE_AVAILABLE}")

logger.info("ğŸš€ Central Hub DI Container v7.0 ì—°ë™:")
logger.info("   â€¢ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
logger.info("   â€¢ ì˜ì¡´ì„± ì£¼ì… ìë™í™”")
logger.info("   â€¢ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   â€¢ 50% ì½”ë“œ ë‹¨ì¶• ë‹¬ì„±")
logger.info("   â€¢ ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ë³µì›")

logger.info("ğŸ“Š ëª©í‘œ ì„±ê³¼:")
logger.info("   ğŸ¯ ì½”ë“œ ë¼ì¸ ìˆ˜: 2000ì¤„ â†’ 1000ì¤„ (50% ë‹¨ì¶•)")
logger.info("   ğŸ”§ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("   âš¡ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
logger.info("   ğŸ§  ì‹¤ì œ AI ëª¨ë¸ (DeepLabV3+, SAM, U2Net) ì™„ì „ ë™ì‘")
logger.info("   ğŸ¨ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ (20ê°œ ì¹´í…Œê³ ë¦¬)")
logger.info("   ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê°€ëŠ¥ (Mock ì œê±°)")

logger.info("=" * 120)
logger.info("ğŸ‰ ClothSegmentationStep Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì™„ë£Œ!")

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 03 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("=" * 80)
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_central_hub_compatibility()
        print()
        test_cloth_segmentation_ai()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ ClothSegmentationStep Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ 50% ì½”ë“œ ë‹¨ì¶• + ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ë³µì›")
    print("ğŸ§  DeepLabV3+, SAM, U2Net ì‹¤ì œ ëª¨ë¸ ì™„ì „ ì§€ì›")
    print("ğŸ¨ 20ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜")
    print("âš¡ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
    print("ğŸš€ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    print("=" * 80)