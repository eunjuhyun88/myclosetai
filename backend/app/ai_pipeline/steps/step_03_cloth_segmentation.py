#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„ v31.0
=====================================================================================================

âœ… BaseStepMixin v19.1 ì™„ì „ ìƒì† ë° í˜¸í™˜
âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (SAM, U2Net, DeepLabV3+, BiSeNet)
âœ… 2.4GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš© (8ê°œ íŒŒì¼)
âœ… ëª©ì—…/í´ë°± ì½”ë“œ ì™„ì „ ì œê±° 
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›

í•µì‹¬ AI ëª¨ë¸ë“¤:
- sam_vit_h_4b8939.pth (2445.7MB) - SAM ViT-Huge ìµœê³  í’ˆì§ˆ
- u2net.pth (168.1MB) - U2Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸
- deeplabv3_resnet101_ultra.pth (233.3MB) - DeepLabV3+ semantic segmentation
- bisenet_resnet18.pth (18.2MB) - BiSeNet ì‹¤ì‹œê°„ ì„¸ê·¸ë©˜í…Œì´ì…˜
- mobile_sam.pt (38.8MB) - Mobile SAM ê²½ëŸ‰í™”

ì²˜ë¦¬ íë¦„:
1. ì´ë¯¸ì§€ ì…ë ¥ â†’ BaseStepMixin ìë™ ë³€í™˜
2. ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  â†’ SAM, U2Net, DeepLabV3+ ì•™ìƒë¸”
3. ê³ ê¸‰ í›„ì²˜ë¦¬ â†’ CRF, ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
4. BaseStepMixin ìë™ ì¶œë ¥ ë³€í™˜ â†’ í‘œì¤€ API ì‘ë‹µ

Author: MyCloset AI Team
Date: 2025-07-30
Version: v31.0 (BaseStepMixin v19.1 Complete Real AI)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ ë° TYPE_CHECKING
# ==============================================

import os
from fix_pytorch_loading import apply_pytorch_patch; apply_pytorch_patch()

import gc
import time
import logging
import threading
import math
import hashlib
import json
import base64
import weakref
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
from abc import ABC, abstractmethod

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader, StepModelInterface
    from app.ai_pipeline.utils.step_model_requests import (
        EnhancedRealModelRequest, DetailedDataSpec, get_enhanced_step_request
    )

# ==============================================
# ğŸ”¥ 2. BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
        
        def initialize(self): 
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader): 
            self.model_loader = model_loader
        
        def _run_ai_inference(self, processed_input): 
            return {}

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 3. ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€
# ==============================================

def detect_m3_max():
    """M3 Max ê°ì§€"""
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

try:
    if IS_M3_MAX:
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ì‹¤ì œ AIìš©)
# ==============================================

# PyTorch (í•„ìˆ˜)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("âŒ PyTorch í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# PIL (í•„ìˆ˜)
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw
    PIL_AVAILABLE = True
    logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ PIL í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# NumPy (í•„ìˆ˜)
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("ğŸ“Š NumPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.error("âŒ NumPy í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    raise

# SAM (ì„ íƒì )
SAM_AVAILABLE = False
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    logger.info("ğŸ¯ SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SAM ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# SciPy (ê³ ê¸‰ í›„ì²˜ë¦¬ìš©)
SCIPY_AVAILABLE = False
try:
    from scipy import ndimage
    SCIPY_AVAILABLE = True
    logger.info("ğŸ”¬ SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ SciPy ì—†ìŒ - ê³ ê¸‰ í›„ì²˜ë¦¬ ì œí•œ")

# Scikit-image (ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬)
SKIMAGE_AVAILABLE = False
try:
    from skimage import measure, morphology, segmentation, filters
    SKIMAGE_AVAILABLE = True
    logger.info("ğŸ”¬ Scikit-image ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Scikit-image ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# DenseCRF (ê³ ê¸‰ í›„ì²˜ë¦¬)
DENSECRF_AVAILABLE = False
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
    logger.info("ğŸ¨ DenseCRF ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ DenseCRF ì—†ìŒ - CRF í›„ì²˜ë¦¬ ì œí•œ")

# Torchvision
TORCHVISION_AVAILABLE = False
try:
    import torchvision
    from torchvision import models, transforms
    TORCHVISION_AVAILABLE = True
    logger.info("ğŸ¤– Torchvision ë¡œë“œ ì™„ë£Œ")
except ImportError:
    logger.warning("âš ï¸ Torchvision ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")

# ==============================================
# ğŸ”¥ 5. Step Model Requests ë¡œë“œ
# ==============================================

def get_step_requirements():
    """step_model_requests.pyì—ì„œ ClothSegmentationStep ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        requirements_module = importlib.import_module('app.ai_pipeline.utils.step_model_requests')
        
        get_enhanced_step_request = getattr(requirements_module, 'get_enhanced_step_request', None)
        if get_enhanced_step_request:
            return get_enhanced_step_request("ClothSegmentationStep")
        
        REAL_STEP_MODEL_REQUESTS = getattr(requirements_module, 'REAL_STEP_MODEL_REQUESTS', {})
        return REAL_STEP_MODEL_REQUESTS.get("ClothSegmentationStep")
        
    except ImportError as e:
        logger.warning(f"âš ï¸ step_model_requests ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

STEP_REQUIREMENTS = get_step_requirements()

# ==============================================
# ğŸ”¥ 6. ê°•í™”ëœ ë°ì´í„° êµ¬ì¡° ì •ì˜ (ì›ë³¸ ì™„ì „ ë³µì›)
# ==============================================

class SegmentationMethod(Enum):
    """ê°•í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    SAM_HUGE = "sam_huge"               # SAM ViT-Huge (2445.7MB)
    SAM_LARGE = "sam_large"             # SAM ViT-Large (1249.1MB)
    SAM_BASE = "sam_base"               # SAM ViT-Base (375.0MB)
    U2NET_CLOTH = "u2net_cloth"         # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB)
    MOBILE_SAM = "mobile_sam"           # Mobile SAM (38.8MB)
    ISNET = "isnet"                     # ISNet ONNX (168.1MB)
    DEEPLABV3_PLUS = "deeplabv3_plus"   # DeepLabV3+ (233.3MB)
    BISENET = "bisenet"                 # BiSeNet (íŠ¹í™”ëœ ì‹¤ì‹œê°„ ë¶„í• )
    HYBRID_AI = "hybrid_ai"             # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”

class ClothingType(Enum):
    """ê°•í™”ëœ ì˜ë¥˜ íƒ€ì…"""
    SHIRT = "shirt"
    T_SHIRT = "t_shirt"
    DRESS = "dress"
    PANTS = "pants"
    JEANS = "jeans"
    SKIRT = "skirt"
    JACKET = "jacket"
    SWEATER = "sweater"
    COAT = "coat"
    HOODIE = "hoodie"
    BLOUSE = "blouse"
    SHORTS = "shorts"
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # Mobile SAM, BiSeNet
    BALANCED = "balanced"   # U2Net + DeepLabV3+
    HIGH = "high"          # SAM + U2Net + CRF
    ULTRA = "ultra"        # ëª¨ë“  AI ëª¨ë¸ + ê³ ê¸‰ í›„ì²˜ë¦¬
    PRODUCTION = "production"  # í”„ë¡œë•ì…˜ ìµœì í™”

@dataclass
class EnhancedSegmentationConfig:
    """ê°•í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì • (ì›ë³¸)"""
    method: SegmentationMethod = SegmentationMethod.HYBRID_AI
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (1024, 1024)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    enable_roi_detection: bool = True
    enable_background_analysis: bool = True
    
    # ì˜ë¥˜ ë¶„ë¥˜ ì„¤ì •
    enable_clothing_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # SAM í”„ë¡¬í”„íŠ¸ ì„¤ì •
    enable_advanced_prompts: bool = True
    use_box_prompts: bool = True
    use_mask_prompts: bool = True
    enable_iterative_refinement: bool = True
    max_refinement_iterations: int = 3
    
    # DeepLabV3+ ì„¤ì •
    enable_deeplabv3_plus: bool = True
    enable_aspp: bool = True
    enable_self_correction: bool = True
    enable_progressive_parsing: bool = True
    
    # í›„ì²˜ë¦¬ ì„¤ì •
    enable_crf_postprocessing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    enable_visualization: bool = True
    use_fp16: bool = True
    confidence_threshold: float = 0.5
    remove_noise: bool = True
    overlay_opacity: float = 0.6

# ==============================================
# ğŸ”¥ 7. DeepLabV3+ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Google AI ë…¼ë¬¸) - ì›ë³¸ ì™„ì „ ë³µì›
# ==============================================

class BasicBlock(nn.Module):
    """HRNet BasicBlock"""
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class Bottleneck(nn.Module):
    """HRNet Bottleneck"""
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class DeepLabV3PlusBackbone(nn.Module):
    """DeepLabV3+ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ - ResNet-101 ê¸°ë°˜ (ì›ë³¸)"""
    
    def __init__(self, backbone='resnet101', output_stride=16):
        super().__init__()
        self.output_stride = output_stride
        
        # ResNet-101 ë°±ë³¸ êµ¬ì„± (ImageNet pretrained)
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ResNet Layers with Dilated Convolution
        self.layer1 = self._make_layer(64, 64, 3, stride=1)      # 256 channels
        self.layer2 = self._make_layer(256, 128, 4, stride=2)    # 512 channels  
        self.layer3 = self._make_layer(512, 256, 23, stride=2)   # 1024 channels
        self.layer4 = self._make_layer(1024, 512, 3, stride=1, dilation=2)  # 2048 channels
        
        # Low-level feature extraction (for decoder)
        self.low_level_conv = nn.Conv2d(256, 48, 1, bias=False)
        self.low_level_bn = nn.BatchNorm2d(48)
    
    def _make_layer(self, inplanes, planes, blocks, stride=1, dilation=1):
        """ResNet ë ˆì´ì–´ ìƒì„± (Bottleneck êµ¬ì¡°)"""
        layers = []
        
        # Downsample layer
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, 1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )
        
        # First block
        layers.append(self._bottleneck_block(inplanes, planes, stride, dilation, downsample))
        
        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(self._bottleneck_block(planes * 4, planes, 1, dilation))
        
        return nn.Sequential(*layers)
    
    def _bottleneck_block(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        """ResNet Bottleneck ë¸”ë¡"""
        class BottleneckBlock(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride=stride, padding=dilation, 
                                     dilation=dilation, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.downsample = downsample
                self.relu = nn.ReLU(inplace=True)
            
            def forward(self, x):
                identity = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    identity = self.downsample(x)
                
                out += identity
                out = self.relu(out)
                
                return out
        
        return BottleneckBlock()
    
    def forward(self, x):
        # Initial convolution
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        # ResNet layers
        x = self.layer1(x)
        low_level_feat = x  # Save for decoder
        
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # Process low-level features
        low_level_feat = self.low_level_conv(low_level_feat)
        low_level_feat = self.low_level_bn(low_level_feat)
        low_level_feat = self.relu(low_level_feat)
        
        return x, low_level_feat

# ==============================================
# ğŸ”¥ 8. ASPP (Atrous Spatial Pyramid Pooling) ì•Œê³ ë¦¬ì¦˜ - ì›ë³¸
# ==============================================

class ASPPModule(nn.Module):
    """ASPP ëª¨ë“ˆ - Multi-scale context aggregation (ì›ë³¸)"""
    
    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()
        
        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])
        
        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)  # 1x1 + atrous + global
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
    
    def forward(self, x):
        h, w = x.shape[2:]
        
        # 1x1 convolution
        feat1 = self.conv1x1(x)
        
        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)
        
        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # Project to final features
        return self.project(concat_feat)

# ==============================================
# ğŸ”¥ 9. Self-Correction Learning ë©”ì»¤ë‹ˆì¦˜ - ì›ë³¸
# ==============================================

class SelfCorrectionModule(nn.Module):
    """Self-Correction Learning - SCHP í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (ì›ë³¸)"""
    
    def __init__(self, num_classes=20, hidden_dim=256):
        super().__init__()
        self.num_classes = num_classes
        
        # Context aggregation
        self.context_conv = nn.Sequential(
            nn.Conv2d(num_classes, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention mechanism
        self.self_attention = SelfAttentionBlock(hidden_dim)
        
        # Correction prediction
        self.correction_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim // 2, num_classes, 1)
        )
        
        # Confidence estimation
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing, features):
        # Convert initial parsing to features
        parsing_feat = self.context_conv(initial_parsing)
        
        # Apply self-attention
        attended_feat = self.self_attention(parsing_feat)
        
        # Predict corrections
        correction = self.correction_conv(attended_feat)
        
        # Estimate confidence
        confidence = self.confidence_branch(attended_feat)
        
        # Apply corrections with confidence weighting
        corrected_parsing = initial_parsing + correction * confidence
        
        return corrected_parsing, confidence

class SelfAttentionBlock(nn.Module):
    """Self-Attention Block for context modeling (ì›ë³¸)"""
    
    def __init__(self, in_channels):
        super().__init__()
        self.in_channels = in_channels
        
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # Generate query, key, value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)
        
        # Compute attention
        attention = torch.bmm(proj_query, proj_key)
        attention = self.softmax(attention)
        
        # Apply attention to values
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        # Residual connection
        out = self.gamma * out + x
        
        return out

# ==============================================
# ğŸ”¥ 10. Complete Enhanced AI Model (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í†µí•©) - ì›ë³¸
# ==============================================

class CompleteEnhancedClothSegmentationAI(nn.Module):
    """Complete Enhanced Cloth Segmentation AI - ëª¨ë“  ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ í†µí•© (ì›ë³¸)"""
    
    def __init__(self, num_classes=1):  # ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì€ ì´ì§„ ë¶„ë¥˜
        super().__init__()
        self.num_classes = num_classes
        
        # 1. DeepLabV3+ Backbone
        self.backbone = DeepLabV3PlusBackbone()
        
        # 2. ASPP Module
        self.aspp = ASPPModule()
        
        # 3. Self-Correction Module (ì´ì§„ ë¶„ë¥˜ìš©)
        self.self_correction = SelfCorrectionModule(num_classes)
        
        # Decoder for final parsing
        self.decoder = nn.Sequential(
            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),  # ASPP + low-level
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        # Final classifier
        self.classifier = nn.Conv2d(256, num_classes, 1)
    
    def forward(self, x):
        input_size = x.shape[2:]
        
        # 1. Extract features with DeepLabV3+ backbone
        high_level_feat, low_level_feat = self.backbone(x)
        
        # 2. Apply ASPP for multi-scale context
        aspp_feat = self.aspp(high_level_feat)
        
        # 3. Upsample and concatenate with low-level features
        aspp_feat = F.interpolate(aspp_feat, size=low_level_feat.shape[2:], 
                                 mode='bilinear', align_corners=False)
        concat_feat = torch.cat([aspp_feat, low_level_feat], dim=1)
        
        # 4. Decode features
        decoded_feat = self.decoder(concat_feat)
        
        # 5. Initial parsing prediction
        initial_parsing = self.classifier(decoded_feat)
        
        # 6. Self-correction
        corrected_parsing, confidence = self.self_correction(
            torch.sigmoid(initial_parsing), decoded_feat
        )
        
        # 7. Upsample to input size
        final_parsing = F.interpolate(corrected_parsing, size=input_size, 
                                    mode='bilinear', align_corners=False)
        confidence = F.interpolate(confidence, size=input_size, 
                                  mode='bilinear', align_corners=False)
        
        return {
            'parsing': final_parsing,
            'confidence': confidence,
            'initial_parsing': F.interpolate(initial_parsing, size=input_size, 
                                           mode='bilinear', align_corners=False)
        }

class RealSAMModel:
    """ì‹¤ì œ SAM AI ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.predictor = None
        self.is_loaded = False
        
    def load(self) -> bool:
        """SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            if not SAM_AVAILABLE:
                logger.warning("âš ï¸ SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                return False
            
            from segment_anything import build_sam_vit_h, SamPredictor
            
            self.model = build_sam_vit_h(checkpoint=self.model_path)
            self.model.to(self.device)
            self.predictor = SamPredictor(self.model)
            self.is_loaded = True
            
            logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: np.ndarray, prompts: Dict[str, Any]) -> Dict[str, Any]:
        """SAM ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return {"mask": None, "confidence": 0.0}
            
            self.predictor.set_image(image)
            
            # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            point_coords = np.array(prompts.get('points', []))
            point_labels = np.array(prompts.get('labels', []))
            box = np.array(prompts.get('box', None))
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            masks, scores, logits = self.predictor.predict(
                point_coords=point_coords if len(point_coords) > 0 else None,
                point_labels=point_labels if len(point_labels) > 0 else None,
                box=box,
                multimask_output=True
            )
            
            # ìµœê³  ì ìˆ˜ ë§ˆìŠ¤í¬ ì„ íƒ
            best_idx = np.argmax(scores)
            best_mask = masks[best_idx]
            best_score = scores[best_idx]
            
            return {
                "mask": (best_mask * 255).astype(np.uint8),
                "confidence": float(best_score),
                "all_masks": masks,
                "all_scores": scores
            }
            
        except Exception as e:
            logger.error(f"âŒ SAM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"mask": None, "confidence": 0.0}

class RealU2NetClothModel:
    """ì‹¤ì œ U2Net ì˜ë¥˜ íŠ¹í™” ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
        
    def load(self) -> bool:
        """U2Net ëª¨ë¸ ë¡œë“œ (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # U2Net ì•„í‚¤í…ì²˜ ìƒì„±
            self.model = self._create_u2net_architecture()
            
            # ğŸ”¥ 3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if os.path.exists(self.model_path):
                try:
                    # 1ë‹¨ê³„: ìµœì‹  ë³´ì•ˆ ê¸°ì¤€ (weights_only=True)
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                except:
                    # 2ë‹¨ê³„: Legacy í¬ë§· ì§€ì› (weights_only=False)
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                
                # MPS í˜¸í™˜ì„±: float64 â†’ float32 ë³€í™˜
                if self.device == "mps" and isinstance(checkpoint, dict):
                    for key, value in checkpoint.items():
                        if isinstance(value, torch.Tensor) and value.dtype == torch.float64:
                            checkpoint[key] = value.float()
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                self.model.load_state_dict(checkpoint, strict=False)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            
            logger.info(f"âœ… U2Net ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


    def _create_u2net_architecture(self):
        """U2Net ì•„í‚¤í…ì²˜ ìƒì„±"""
        class U2NetForCloth(nn.Module):
            def __init__(self):
                super().__init__()
                # ì¸ì½”ë”
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(256, 512, 3, padding=1),
                    nn.ReLU(inplace=True),
                )
                
                # ë””ì½”ë”
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 64, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 1, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded
        
        return U2NetForCloth()
    
    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """U2Net ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            if not self.is_loaded:
                return {"mask": None, "confidence": 0.0}
            
            # ì „ì²˜ë¦¬
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
            else:
                pil_image = image
            
            transform = transforms.Compose([
                transforms.Resize((320, 320)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0).to(self.device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                output = self.model(input_tensor)
                
            # í›„ì²˜ë¦¬
            mask = output.squeeze().cpu().numpy()
            mask = (mask * 255).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_size = image.shape[:2]
            mask_pil = Image.fromarray(mask).resize((original_size[1], original_size[0]), Image.Resampling.NEAREST)
            mask_resized = np.array(mask_pil)
            
            return {
                "mask": mask_resized,
                "confidence": float(np.mean(mask_resized) / 255.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ U2Net ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"mask": None, "confidence": 0.0}

class RealDeepLabV3PlusModel:
    """ì‹¤ì œ DeepLabV3+ ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.is_loaded = False
    
    def load(self) -> bool:
        """DeepLabV3+ ëª¨ë¸ ë¡œë“œ (CompleteEnhancedClothSegmentationAI ì‚¬ìš©, 3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # CompleteEnhancedClothSegmentationAI ì‚¬ìš© (ì›ë³¸)
            self.model = CompleteEnhancedClothSegmentationAI(num_classes=1)
            
            # ğŸ”¥ 3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if os.path.exists(self.model_path):
                try:
                    # 1ë‹¨ê³„: ìµœì‹  ë³´ì•ˆ ê¸°ì¤€ (weights_only=True)
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                except:
                    # 2ë‹¨ê³„: Legacy í¬ë§· ì§€ì› (weights_only=False)
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
                
                # MPS í˜¸í™˜ì„±: float64 â†’ float32 ë³€í™˜
                if self.device == "mps" and isinstance(checkpoint, dict):
                    for key, value in checkpoint.items():
                        if isinstance(value, torch.Tensor) and value.dtype == torch.float64:
                            checkpoint[key] = value.float()
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
                self.model.load_state_dict(checkpoint, strict=False)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            
            logger.info(f"âœ… DeepLabV3+ (CompleteEnhanced) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


    def predict(self, image: np.ndarray) -> Dict[str, Any]:
        """DeepLabV3+ ì˜ˆì¸¡ ì‹¤í–‰ (CompleteEnhanced ë²„ì „)"""
        try:
            if not self.is_loaded:
                return {"mask": None, "confidence": 0.0}
            
            # ì „ì²˜ë¦¬
            transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(image).unsqueeze(0).to(self.device)
            
            # ì‹¤ì œ CompleteEnhanced AI ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(input_tensor)
                
            # ê²°ê³¼ ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°)
            parsing = outputs['parsing']
            confidence_map = outputs['confidence']
            initial_parsing = outputs['initial_parsing']
            
            # í›„ì²˜ë¦¬
            mask = torch.sigmoid(parsing).squeeze().cpu().numpy()
            confidence = confidence_map.squeeze().cpu().numpy()
            
            # ì´ì§„í™”
            binary_mask = (mask > 0.5).astype(np.uint8) * 255
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_size = image.shape[:2]
            mask_pil = Image.fromarray(binary_mask).resize((original_size[1], original_size[0]), Image.Resampling.NEAREST)
            mask_resized = np.array(mask_pil)
            
            return {
                "mask": mask_resized,
                "confidence": float(np.mean(confidence)),
                "raw_parsing": mask,
                "confidence_map": confidence,
                "initial_parsing": initial_parsing.squeeze().cpu().numpy(),
                "enhanced_by_self_correction": True
            }
            
        except Exception as e:
            logger.error(f"âŒ DeepLabV3+ (CompleteEnhanced) ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"mask": None, "confidence": 0.0}

# ==============================================
# ğŸ”¥ 8. ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤
# ==============================================

class AdvancedPostProcessor:
    """ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤"""
    
    @staticmethod
    def apply_crf_postprocessing(mask: np.ndarray, image: np.ndarray, num_iterations: int = 10) -> np.ndarray:
        """CRF í›„ì²˜ë¦¬ë¡œ ê²½ê³„ì„  ê°œì„ """
        try:
            if not DENSECRF_AVAILABLE:
                return mask
            
            h, w = mask.shape
            
            # í™•ë¥  ë§µ ìƒì„±
            prob_bg = 1.0 - (mask.astype(np.float32) / 255.0)
            prob_fg = mask.astype(np.float32) / 255.0
            probs = np.stack([prob_bg, prob_fg], axis=0)
            
            # Unary potential
            unary = unary_from_softmax(probs)
            
            # Setup CRF
            d = dcrf.DenseCRF2D(w, h, 2)
            d.setUnaryEnergy(unary)
            
            # Add pairwise energies
            d.addPairwiseGaussian(sxy=(3, 3), compat=3)
            d.addPairwiseBilateral(sxy=(80, 80), srgb=(13, 13, 13), 
                                  rgbim=image, compat=10)
            
            # Inference
            Q = d.inference(num_iterations)
            map_result = np.argmax(Q, axis=0).reshape((h, w))
            
            return (map_result * 255).astype(np.uint8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ CRF í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return mask
    
    @staticmethod
    def apply_multiscale_processing(image: np.ndarray, initial_mask: np.ndarray) -> np.ndarray:
        """ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬"""
        try:
            scales = [0.5, 1.0, 1.5]
            processed_masks = []
            
            for scale in scales:
                if scale != 1.0:
                    h, w = initial_mask.shape
                    new_h, new_w = int(h * scale), int(w * scale)
                    
                    scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.Resampling.LANCZOS))
                    scaled_mask = np.array(Image.fromarray(initial_mask).resize((new_w, new_h), Image.Resampling.NEAREST))
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    processed = np.array(Image.fromarray(scaled_mask).resize((w, h), Image.Resampling.NEAREST))
                else:
                    processed = initial_mask
                
                processed_masks.append(processed.astype(np.float32) / 255.0)
            
            # ìŠ¤ì¼€ì¼ë³„ ê²°ê³¼ í†µí•©
            if len(processed_masks) > 1:
                weights = [0.3, 0.4, 0.3]
                combined = np.zeros_like(processed_masks[0])
                
                for mask, weight in zip(processed_masks, weights):
                    combined += mask * weight
                
                final_mask = (combined > 0.5).astype(np.uint8) * 255
            else:
                final_mask = (processed_masks[0] > 0.5).astype(np.uint8) * 255
            
            return final_mask
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return initial_mask

# ==============================================
# ğŸ”¥ 9. ë©”ì¸ ClothSegmentationStep í´ë˜ìŠ¤
# ==============================================

class ClothSegmentationStep(BaseStepMixin):
    """
    ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„
    
    BaseStepMixin v19.1ì—ì„œ ìë™ ì œê³µ:
    âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
    âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš©
    âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ (ModelLoader, MemoryManager ë“±)
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
    âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
    
    ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„!
    """
    
    def __init__(self, **kwargs):
        """AI ê°•í™”ëœ ì´ˆê¸°í™”"""
        try:
            # BaseStepMixin ì´ˆê¸°í™”
            super().__init__(
                step_name="ClothSegmentationStep",
                step_id=3,
                **kwargs
            )
            
            # ì„¤ì •
            self.config = EnhancedSegmentationConfig()
            if 'segmentation_config' in kwargs:
                config_dict = kwargs['segmentation_config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
                elif isinstance(config_dict, EnhancedSegmentationConfig):
                    self.config = config_dict
            
            # AI ëª¨ë¸ ë° ì‹œìŠ¤í…œ
            self.ai_models = {}
            self.model_paths = {}
            self.available_methods = []
            self.postprocessor = AdvancedPostProcessor()
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ (ì›ë³¸ ì™„ì „ ë³µì›)
            self.models_loading_status = {
                'sam_huge': False,
                'sam_large': False,
                'sam_base': False,
                'u2net_cloth': False,
                'mobile_sam': False,
                'isnet': False,
                'deeplabv3_plus': False,
                'bisenet': False,
            }
            
            # ì‹œìŠ¤í…œ ìµœì í™”
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            
            # ì„±ëŠ¥ ë° ìºì‹±
            self.executor = ThreadPoolExecutor(
                max_workers=6 if self.is_m3_max else 3,
                thread_name_prefix="cloth_seg"
            )
            self.segmentation_cache = {}
            self.cache_lock = threading.RLock()
            
            # í†µê³„
            self.ai_stats = {
                'total_processed': 0,
                'preprocessing_time': 0.0,
                'segmentation_time': 0.0,
                'postprocessing_time': 0.0,
                'sam_calls': 0,
                'u2net_calls': 0,
                'deeplabv3_calls': 0,
                'hybrid_calls': 0,
                'average_confidence': 0.0
            }
            
            self.logger.info(f"âœ… {self.step_name} AI ê°•í™”ëœ ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"   - Device: {self.device}")
            self.logger.info(f"   - M3 Max: {self.is_m3_max}")
            self.logger.info(f"   - Memory: {self.memory_gb}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ClothSegmentationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì •"""
        try:
            self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ")
            self.step_name = kwargs.get('step_name', 'ClothSegmentationStep')
            self.step_id = kwargs.get('step_id', 3)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.ai_models = {}
            self.model_paths = {}
            self.ai_stats = {'total_processed': 0}
            self.config = EnhancedSegmentationConfig()  # ì›ë³¸ config ì‚¬ìš©
            self.cache_lock = threading.RLock()
        except Exception as e:
            print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 10. ëª¨ë¸ ì´ˆê¸°í™”
    # ==============================================
    
    def initialize(self) -> bool:
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            logger.info(f"ğŸ”„ {self.step_name} AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ëª¨ë¸ ê²½ë¡œ íƒì§€
            self._detect_model_paths()
            
            # 2. ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©
            self._load_all_ai_models()
            
            # 3. ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²• ê°ì§€
            self.available_methods = self._detect_available_methods()
            
            # 4. BaseStepMixin ì´ˆê¸°í™”
            super_initialized = super().initialize() if hasattr(super(), 'initialize') else True
            
            self.is_initialized = True
            self.is_ready = True
            
            loaded_models = list(self.ai_models.keys())
            logger.info(f"âœ… {self.step_name} AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   - ë¡œë“œëœ AI ëª¨ë¸: {loaded_models}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {[m.value for m in self.available_methods]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    def _detect_model_paths(self):
        """AI ëª¨ë¸ ê²½ë¡œ íƒì§€"""
        try:
            # step_model_requests.py ê¸°ë°˜ ê²½ë¡œ íƒì§€
            if STEP_REQUIREMENTS:
                search_paths = STEP_REQUIREMENTS.search_paths + STEP_REQUIREMENTS.fallback_paths
                
                # Primary íŒŒì¼ë“¤
                primary_file = STEP_REQUIREMENTS.primary_file
                for search_path in search_paths:
                    full_path = os.path.join(search_path, primary_file)
                    if os.path.exists(full_path):
                        self.model_paths['sam_huge'] = full_path
                        logger.info(f"âœ… Primary SAM ViT-Huge ë°œê²¬: {full_path}")
                        break
                
                # Alternative íŒŒì¼ë“¤
                for alt_file, alt_size in STEP_REQUIREMENTS.alternative_files:
                    for search_path in search_paths:
                        full_path = os.path.join(search_path, alt_file)
                        if os.path.exists(full_path):
                            if 'u2net' in alt_file.lower():
                                self.model_paths['u2net_cloth'] = full_path
                            elif 'mobile_sam' in alt_file.lower():
                                self.model_paths['mobile_sam'] = full_path
                            elif 'deeplabv3' in alt_file.lower():
                                self.model_paths['deeplabv3_plus'] = full_path
                            elif 'bisenet' in alt_file.lower():
                                self.model_paths['bisenet'] = full_path
                            logger.info(f"âœ… Alternative ëª¨ë¸ ë°œê²¬: {full_path}")
                            break
            
            # ê¸°ë³¸ ê²½ë¡œ í´ë°±
            if not self.model_paths:
                base_paths = [
                    "step_03_cloth_segmentation/",
                    "step_03_cloth_segmentation/ultra_models/",
                    "step_04_geometric_matching/",  # SAM ê³µìœ 
                    "step_04_geometric_matching/ultra_models/",
                ]
                
                model_files = {
                    'sam_huge': 'sam_vit_h_4b8939.pth',
                    'u2net_cloth': 'u2net.pth',
                    'mobile_sam': 'mobile_sam.pt',
                    'deeplabv3_plus': 'deeplabv3_resnet101_ultra.pth',
                    'bisenet': 'bisenet_resnet18.pth'
                }
                
                for model_key, filename in model_files.items():
                    for base_path in base_paths:
                        full_path = os.path.join(base_path, filename)
                        if os.path.exists(full_path):
                            self.model_paths[model_key] = full_path
                            logger.info(f"âœ… {model_key} ë°œê²¬: {full_path}")
                            break
            
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def _load_all_ai_models(self):
        """ëª¨ë“  AI ëª¨ë¸ ë¡œë”©"""
        try:
            if not TORCH_AVAILABLE:
                logger.error("âŒ PyTorchê°€ ì—†ì–´ì„œ AI ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
                return
            
            logger.info("ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # 1. SAM ëª¨ë¸ ë¡œë”©
            if 'sam_huge' in self.model_paths:
                try:
                    sam_model = RealSAMModel(self.model_paths['sam_huge'], self.device)
                    if sam_model.load():
                        self.ai_models['sam_huge'] = sam_model
                        self.models_loading_status['sam_huge'] = True
                        logger.info("âœ… SAM ViT-Huge ë¡œë”© ì™„ë£Œ (2445.7MB)")
                except Exception as e:
                    logger.error(f"âŒ SAM ViT-Huge ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 2. U2Net ëª¨ë¸ ë¡œë”©
            if 'u2net_cloth' in self.model_paths:
                try:
                    u2net_model = RealU2NetClothModel(self.model_paths['u2net_cloth'], self.device)
                    if u2net_model.load():
                        self.ai_models['u2net_cloth'] = u2net_model
                        self.models_loading_status['u2net_cloth'] = True
                        logger.info("âœ… U2Net Cloth ë¡œë”© ì™„ë£Œ (168.1MB)")
                except Exception as e:
                    logger.error(f"âŒ U2Net Cloth ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 3. DeepLabV3+ ëª¨ë¸ ë¡œë”©
            if 'deeplabv3_plus' in self.model_paths:
                try:
                    deeplabv3_model = RealDeepLabV3PlusModel(self.model_paths['deeplabv3_plus'], self.device)
                    if deeplabv3_model.load():
                        self.ai_models['deeplabv3_plus'] = deeplabv3_model
                        self.models_loading_status['deeplabv3_plus'] = True
                        logger.info("âœ… DeepLabV3+ ë¡œë”© ì™„ë£Œ (233.3MB)")
                except Exception as e:
                    logger.error(f"âŒ DeepLabV3+ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            loaded_count = sum(self.models_loading_status.values())
            total_models = len(self.models_loading_status)
            logger.info(f"ğŸ§  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{total_models}")
            
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _detect_available_methods(self) -> List[SegmentationMethod]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ê°ì§€"""
        methods = []
        
        if 'sam_huge' in self.ai_models:
            methods.append(SegmentationMethod.SAM_HUGE)
        if 'u2net_cloth' in self.ai_models:
            methods.append(SegmentationMethod.U2NET_CLOTH)
        if 'deeplabv3_plus' in self.ai_models:
            methods.append(SegmentationMethod.DEEPLABV3_PLUS)
        if 'mobile_sam' in self.ai_models:
            methods.append(SegmentationMethod.MOBILE_SAM)
        if 'bisenet' in self.ai_models:
            methods.append(SegmentationMethod.BISENET)
        
        if len(methods) >= 2:
            methods.append(SegmentationMethod.HYBRID_AI)
        
        return methods
    
    # ==============================================
    # ğŸ”¥ 11. í•µì‹¬: ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€) - async ì œê±°!
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ë™ê¸° AI ì¶”ë¡  ë¡œì§ - BaseStepMixin v19.1ì—ì„œ í˜¸ì¶œë¨ (í”„ë¡œì íŠ¸ í‘œì¤€)
        **async ì œê±°í•˜ì—¬ ì™„ì „í•œ ë™ê¸° ë©”ì„œë“œë¡œ ë³€ê²½!**
        
        AI ê°•í™”ëœ íŒŒì´í”„ë¼ì¸:
        1. ê³ ê¸‰ ì „ì²˜ë¦¬ (í’ˆì§ˆ í‰ê°€, ì¡°ëª… ì •ê·œí™”)
        2. ë©€í‹°ëª¨ë¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ (SAM + U2Net + DeepLabV3+)
        3. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”
        4. ê³ ê¸‰ í›„ì²˜ë¦¬ (CRF + ë©€í‹°ìŠ¤ì¼€ì¼)
        5. í’ˆì§ˆ ê²€ì¦ ë° ìë™ ì¬ì‹œë„
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            start_time = time.time()
            
            # 0. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input:
                return self._create_emergency_result("imageê°€ ì—†ìŒ")
            
            image = processed_input['image']
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image.astype(np.uint8))
                image_array = image
            elif PIL_AVAILABLE and isinstance(image, Image.Image):
                pil_image = image
                image_array = np.array(image)
            else:
                return self._create_emergency_result("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # ì´ì „ Step ë°ì´í„°
            person_parsing = processed_input.get('from_step_01', {})
            pose_info = processed_input.get('from_step_02', {})
            
            # ==============================================
            # ğŸ”¥ Phase 1: ê³ ê¸‰ ì „ì²˜ë¦¬
            # ==============================================
            
            preprocessing_start = time.time()
            
            # 1.1 ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
            quality_scores = self._assess_image_quality(image_array)
            
            # 1.2 ì¡°ëª… ì •ê·œí™”
            processed_image = self._normalize_lighting(image_array)
            
            # 1.3 ìƒ‰ìƒ ë³´ì •
            if self.config.enable_color_correction:
                processed_image = self._correct_colors(processed_image)
            
            # 1.4 ROI ê²€ì¶œ
            roi_box = self._detect_roi(processed_image) if self.config.enable_roi_detection else None
            
            preprocessing_time = time.time() - preprocessing_start
            self.ai_stats['preprocessing_time'] += preprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 2: ì‹¤ì œ AI ë©€í‹°ëª¨ë¸ ì„¸ê·¸ë©˜í…Œì´ì…˜
            # ==============================================
            
            segmentation_start = time.time()
            
            # í’ˆì§ˆ ë ˆë²¨ ê²°ì •
            quality_level = self._determine_quality_level(processed_input, quality_scores)
            
            # ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë™ê¸°)
            mask, confidence, method_used = self._run_ai_segmentation_sync(
                processed_image, quality_level, roi_box, person_parsing, pose_info
            )
            
            if mask is None:
                # í´ë°±: ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„±
                mask = self._create_fallback_mask(processed_image)
                confidence = 0.3
                method_used = "fallback"
            
            segmentation_time = time.time() - segmentation_start
            self.ai_stats['segmentation_time'] += segmentation_time
            
            # ==============================================
            # ğŸ”¥ Phase 3: ê³ ê¸‰ í›„ì²˜ë¦¬
            # ==============================================
            
            postprocessing_start = time.time()
            
            final_mask = mask
            
            # CRF í›„ì²˜ë¦¬
            if self.config.enable_crf_postprocessing and DENSECRF_AVAILABLE:
                final_mask = self.postprocessor.apply_crf_postprocessing(final_mask, processed_image)
                
            # ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
            if self.config.enable_multiscale_processing:
                final_mask = self.postprocessor.apply_multiscale_processing(processed_image, final_mask)
            
            # í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°
            if self.config.enable_hole_filling:
                final_mask = self._fill_holes_and_remove_noise(final_mask)
            
            postprocessing_time = time.time() - postprocessing_start
            self.ai_stats['postprocessing_time'] += postprocessing_time
            
            # ==============================================
            # ğŸ”¥ Phase 4: ê²°ê³¼ ìƒì„±
            # ==============================================
            
            # í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_mask_quality(final_mask, processed_image)
            
            # ì‹œê°í™” ìƒì„±
            visualizations = self._create_visualizations(processed_image, final_mask, roi_box)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_time = time.time() - start_time
            self._update_ai_stats(method_used, confidence, total_time, quality_metrics)
            
            # ==============================================
            # ğŸ”¥ ìµœì¢… ê²°ê³¼ ë°˜í™˜ (BaseStepMixin í‘œì¤€)
            # ==============================================
            
            ai_result = {
                # í•µì‹¬ ê²°ê³¼
                'cloth_mask': final_mask,
                'segmented_clothing': self._apply_mask_to_image(processed_image, final_mask),
                'confidence': confidence,
                'method_used': method_used,
                'processing_time': total_time,
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                'quality_score': quality_metrics.get('overall', 0.5),
                'quality_metrics': quality_metrics,
                'image_quality_scores': quality_scores,
                'mask_area_ratio': np.sum(final_mask > 0) / final_mask.size if NUMPY_AVAILABLE else 0.0,
                
                # ì „ì²˜ë¦¬ ê²°ê³¼
                'preprocessing_results': {
                    'roi_box': roi_box,
                    'lighting_normalized': self.config.enable_lighting_normalization,
                    'color_corrected': self.config.enable_color_correction
                },
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                'performance_breakdown': {
                    'preprocessing_time': preprocessing_time,
                    'segmentation_time': segmentation_time,
                    'postprocessing_time': postprocessing_time
                },
                
                # ì‹œê°í™”
                **visualizations,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'ai_models_used': list(self.ai_models.keys()),
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'ai_enhanced': True,
                    'quality_level': quality_level.value,
                    'version': '31.0'
                },
                
                # Step ê°„ ì—°ë™ ë°ì´í„°
                'cloth_features': self._extract_cloth_features(final_mask, processed_image),
                'cloth_contours': self._extract_cloth_contours(final_mask),
                'roi_information': roi_box
            }
            
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ - {total_time:.2f}ì´ˆ")
            self.logger.info(f"   - ë°©ë²•: {method_used}")
            self.logger.info(f"   - ì‹ ë¢°ë„: {confidence:.3f}")
            self.logger.info(f"   - í’ˆì§ˆ: {quality_metrics.get('overall', 0.5):.3f}")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_emergency_result(str(e))
    
    # ==============================================
    # ğŸ”¥ 12. AI í—¬í¼ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_scores = {}
            
            # ë¸”ëŸ¬ ì •ë„ ì¸¡ì •
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            if NUMPY_AVAILABLE:
                grad_x = np.abs(np.diff(gray, axis=1))
                grad_y = np.abs(np.diff(gray, axis=0))
                sharpness = np.mean(grad_x) + np.mean(grad_y)
                quality_scores['sharpness'] = min(sharpness / 100.0, 1.0)
            else:
                quality_scores['sharpness'] = 0.5
            
            # ëŒ€ë¹„ ì¸¡ì •
            contrast = np.std(gray) if NUMPY_AVAILABLE else 50.0
            quality_scores['contrast'] = min(contrast / 128.0, 1.0)
            
            # í•´ìƒë„ í’ˆì§ˆ
            height, width = image.shape[:2]
            resolution_score = min((height * width) / (1024 * 1024), 1.0)
            quality_scores['resolution'] = resolution_score
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_scores['overall'] = np.mean(list(quality_scores.values())) if NUMPY_AVAILABLE else 0.5
            
            return quality_scores
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5, 'sharpness': 0.5, 'contrast': 0.5, 'resolution': 0.5}
    
    def _normalize_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            if not self.config.enable_lighting_normalization:
                return image
            
            if len(image.shape) == 3:
                # ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                normalized = np.zeros_like(image)
                for i in range(3):
                    channel = image[:, :, i]
                    channel_min, channel_max = channel.min(), channel.max()
                    if channel_max > channel_min:
                        normalized[:, :, i] = ((channel - channel_min) / (channel_max - channel_min) * 255).astype(np.uint8)
                    else:
                        normalized[:, :, i] = channel
                return normalized
            else:
                img_min, img_max = image.min(), image.max()
                if img_max > img_min:
                    return ((image - img_min) / (img_max - img_min) * 255).astype(np.uint8)
                else:
                    return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def _correct_colors(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if PIL_AVAILABLE and len(image.shape) == 3:
                pil_image = Image.fromarray(image)
                
                # ìë™ ëŒ€ë¹„ ì¡°ì •
                enhancer = ImageEnhance.Contrast(pil_image)
                enhanced = enhancer.enhance(1.2)
                
                # ìƒ‰ìƒ ì±„ë„ ì¡°ì •
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.1)
                
                return np.array(enhanced)
            else:
                return image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _detect_roi(self, image: np.ndarray) -> Tuple[int, int, int, int]:
        """ROI (ê´€ì‹¬ ì˜ì—­) ê²€ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì¤‘ì•™ ì˜ì—­ ê¸°ë°˜ ROI
            h, w = image.shape[:2]
            
            # ì´ë¯¸ì§€ ì¤‘ì•™ì˜ 80% ì˜ì—­ì„ ROIë¡œ ì„¤ì •
            margin_h = int(h * 0.1)
            margin_w = int(w * 0.1)
            
            x1 = margin_w
            y1 = margin_h
            x2 = w - margin_w
            y2 = h - margin_h
            
            return (x1, y1, x2, y2)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ROI ê²€ì¶œ ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2]
            return (w//4, h//4, 3*w//4, 3*h//4)
    
    def _determine_quality_level(self, processed_input: Dict[str, Any], quality_scores: Dict[str, float]) -> QualityLevel:
        """í’ˆì§ˆ ë ˆë²¨ ê²°ì •"""
        try:
            # ì‚¬ìš©ì ì„¤ì • ìš°ì„ 
            if 'quality_level' in processed_input:
                user_level = processed_input['quality_level']
                if isinstance(user_level, str):
                    try:
                        return QualityLevel(user_level)
                    except ValueError:
                        pass
                elif isinstance(user_level, QualityLevel):
                    return user_level
            
            # ìë™ ê²°ì •
            overall_quality = quality_scores.get('overall', 0.5)
            
            if self.is_m3_max and overall_quality > 0.7:
                return QualityLevel.ULTRA
            elif overall_quality > 0.6:
                return QualityLevel.HIGH
            elif overall_quality > 0.4:
                return QualityLevel.BALANCED
            else:
                return QualityLevel.FAST
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë ˆë²¨ ê²°ì • ì‹¤íŒ¨: {e}")
            return QualityLevel.BALANCED
    
    def _run_ai_segmentation_sync(
        self, 
        image: np.ndarray, 
        quality_level: QualityLevel, 
        roi_box: Optional[Tuple[int, int, int, int]],
        person_parsing: Dict[str, Any],
        pose_info: Dict[str, Any]
    ) -> Tuple[Optional[np.ndarray], float, str]:
        """ì‹¤ì œ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ (ë™ê¸°)"""
        try:
            if quality_level == QualityLevel.ULTRA and 'deeplabv3_plus' in self.ai_models:
                # DeepLabV3+ ì‚¬ìš© (ìµœê³  í’ˆì§ˆ)
                result = self.ai_models['deeplabv3_plus'].predict(image)
                self.ai_stats['deeplabv3_calls'] += 1
                return result['mask'], result['confidence'], 'deeplabv3_plus'
                
            elif quality_level in [QualityLevel.HIGH, QualityLevel.BALANCED] and 'sam_huge' in self.ai_models:
                # SAM ì‚¬ìš© (ê³ í’ˆì§ˆ)
                prompts = self._generate_sam_prompts(image, roi_box, person_parsing)
                result = self.ai_models['sam_huge'].predict(image, prompts)
                self.ai_stats['sam_calls'] += 1
                return result['mask'], result['confidence'], 'sam_huge'
                
            elif 'u2net_cloth' in self.ai_models:
                # U2Net ì‚¬ìš© (ê· í˜•)
                result = self.ai_models['u2net_cloth'].predict(image)
                self.ai_stats['u2net_calls'] += 1
                return result['mask'], result['confidence'], 'u2net_cloth'
                
            else:
                # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”
                return self._run_hybrid_ensemble_sync(image, roi_box, person_parsing)
                
        except Exception as e:
            self.logger.error(f"âŒ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, 0.0, 'error'
    
    def _generate_sam_prompts(
        self, 
        image: np.ndarray, 
        roi_box: Optional[Tuple[int, int, int, int]],
        person_parsing: Dict[str, Any]
    ) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            prompts = {}
            
            # ROI ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸
            if roi_box and self.config.use_box_prompts:
                prompts['box'] = roi_box
            
            # ì¤‘ì•™ í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸
            h, w = image.shape[:2]
            center_points = [
                (w // 2, h // 2),           # ì¤‘ì•™
                (w // 3, h // 2),           # ì¢Œì¸¡
                (2 * w // 3, h // 2),       # ìš°ì¸¡
            ]
            
            prompts['points'] = center_points
            prompts['labels'] = [1, 1, 1]  # ëª¨ë‘ positive
            
            # Person parsing ì •ë³´ í™œìš©
            if person_parsing and 'clothing_regions' in person_parsing:
                clothing_regions = person_parsing['clothing_regions']
                if clothing_regions:
                    # ì˜ë¥˜ ì˜ì—­ì˜ ì¤‘ì‹¬ì ë“¤ ì¶”ê°€
                    for region in clothing_regions[:3]:  # ìµœëŒ€ 3ê°œ
                        if 'center' in region:
                            center = region['center']
                            prompts['points'].append((center[0], center[1]))
                            prompts['labels'].append(1)
            
            return prompts
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ SAM í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            h, w = image.shape[:2]
            return {
                'points': [(w // 2, h // 2)],
                'labels': [1]
            }
    
    def _run_hybrid_ensemble_sync(
        self, 
        image: np.ndarray, 
        roi_box: Optional[Tuple[int, int, int, int]],
        person_parsing: Dict[str, Any]
    ) -> Tuple[Optional[np.ndarray], float, str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ (ë™ê¸°)"""
        try:
            masks = []
            confidences = []
            methods_used = []
            
            # U2Net ì‹¤í–‰
            if 'u2net_cloth' in self.ai_models:
                result = self.ai_models['u2net_cloth'].predict(image)
                if result['mask'] is not None:
                    masks.append(result['mask'])
                    confidences.append(result['confidence'])
                    methods_used.append('u2net')
            
            # SAM ì‹¤í–‰
            if 'sam_huge' in self.ai_models:
                prompts = self._generate_sam_prompts(image, roi_box, person_parsing)
                result = self.ai_models['sam_huge'].predict(image, prompts)
                if result['mask'] is not None:
                    masks.append(result['mask'])
                    confidences.append(result['confidence'])
                    methods_used.append('sam')
            
            # ì•™ìƒë¸” ê²°í•©
            if len(masks) >= 2:
                # ê°€ì¤‘ í‰ê·  (ì‹ ë¢°ë„ ê¸°ë°˜)
                total_weight = sum(confidences)
                if total_weight > 0:
                    ensemble_mask = np.zeros_like(masks[0], dtype=np.float32)
                    for mask, conf in zip(masks, confidences):
                        weight = conf / total_weight
                        ensemble_mask += (mask.astype(np.float32) / 255.0) * weight
                    
                    final_mask = (ensemble_mask > 0.5).astype(np.uint8) * 255
                    final_confidence = np.mean(confidences)
                    
                    self.ai_stats['hybrid_calls'] += 1
                    return final_mask, final_confidence, f"hybrid_{'+'.join(methods_used)}"
            
            # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼
            elif len(masks) == 1:
                return masks[0], confidences[0], methods_used[0]
            
            # ì‹¤íŒ¨
            return None, 0.0, 'ensemble_failed'
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None, 0.0, 'ensemble_error'
    
    def _create_fallback_mask(self, image: np.ndarray) -> np.ndarray:
        """í´ë°± ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬
            h, w = image.shape[:2]
            
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì¤‘ì•™ ì˜ì—­ì„ ì „ê²½ìœ¼ë¡œ ê°€ì •
            mask = np.zeros((h, w), dtype=np.uint8)
            center_h, center_w = h // 2, w // 2
            
            # íƒ€ì›í˜• ë§ˆìŠ¤í¬ ìƒì„±
            y, x = np.ogrid[:h, :w]
            ellipse_mask = ((x - center_w)**2 / (w/3)**2 + (y - center_h)**2 / (h/2)**2) <= 1
            mask[ellipse_mask] = 255
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ë§ˆìŠ¤í¬
            h, w = image.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            mask[h//4:3*h//4, w//4:3*w//4] = 255
            return mask
    
    def _fill_holes_and_remove_noise(self, mask: np.ndarray) -> np.ndarray:
        """í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            if not NUMPY_AVAILABLE:
                return mask
            
            # ê°„ë‹¨í•œ ëª¨í´ë¡œì§€ ì—°ì‚°
            if SCIPY_AVAILABLE:
                # í™€ ì±„ìš°ê¸°
                filled = ndimage.binary_fill_holes(mask > 128)
                
                # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±° (erosion + dilation)
                structure = ndimage.generate_binary_structure(2, 2)
                eroded = ndimage.binary_erosion(filled, structure=structure, iterations=1)
                dilated = ndimage.binary_dilation(eroded, structure=structure, iterations=2)
                
                return (dilated * 255).astype(np.uint8)
            else:
                return mask
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return mask
    
    def _evaluate_mask_quality(self, mask: np.ndarray, image: np.ndarray = None) -> Dict[str, float]:
        """ë§ˆìŠ¤í¬ í’ˆì§ˆ ìë™ í‰ê°€"""
        try:
            quality_metrics = {}
            
            # 1. ì˜ì—­ ì—°ì†ì„±
            if NUMPY_AVAILABLE:
                # ì—°ê²°ëœ êµ¬ì„±ìš”ì†Œ ìˆ˜
                if SKIMAGE_AVAILABLE:
                    from skimage import measure
                    labeled = measure.label(mask > 128)
                    num_components = labeled.max()
                    total_area = np.sum(mask > 128)
                    
                    if num_components > 0:
                        # ê°€ì¥ í° êµ¬ì„±ìš”ì†Œì˜ ë¹„ìœ¨
                        component_sizes = [np.sum(labeled == i) for i in range(1, num_components + 1)]
                        largest_component = max(component_sizes) if component_sizes else 0
                        quality_metrics['continuity'] = largest_component / total_area if total_area > 0 else 0.0
                    else:
                        quality_metrics['continuity'] = 0.0
                else:
                    quality_metrics['continuity'] = 0.5
            else:
                quality_metrics['continuity'] = 0.5
            
            # 2. í¬ê¸° ì ì ˆì„±
            size_ratio = np.sum(mask > 128) / mask.size if NUMPY_AVAILABLE else 0.3
            if 0.1 <= size_ratio <= 0.7:  # ì ì ˆí•œ í¬ê¸° ë²”ìœ„
                quality_metrics['size_appropriateness'] = 1.0
            else:
                quality_metrics['size_appropriateness'] = max(0.0, 1.0 - abs(size_ratio - 0.3) / 0.3)
            
            # 3. ì¢…íš¡ë¹„ í•©ë¦¬ì„±
            aspect_ratio = self._calculate_aspect_ratio(mask)
            if 0.5 <= aspect_ratio <= 3.0:  # í•©ë¦¬ì ì¸ ì¢…íš¡ë¹„ ë²”ìœ„
                quality_metrics['aspect_ratio'] = 1.0
            else:
                quality_metrics['aspect_ratio'] = max(0.0, 1.0 - abs(aspect_ratio - 1.5) / 1.5)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_metrics['overall'] = np.mean(list(quality_metrics.values())) if NUMPY_AVAILABLE else 0.5
            
            return quality_metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5}
    
    def _calculate_aspect_ratio(self, mask: np.ndarray) -> float:
        """ì¢…íš¡ë¹„ ê³„ì‚°"""
        try:
            if not NUMPY_AVAILABLE:
                return 1.0
                
            rows = np.any(mask > 128, axis=1)
            cols = np.any(mask > 128, axis=0)
            
            if not np.any(rows) or not np.any(cols):
                return 1.0
            
            rmin, rmax = np.where(rows)[0][[0, -1]]
            cmin, cmax = np.where(cols)[0][[0, -1]]
            
            width = cmax - cmin + 1
            height = rmax - rmin + 1
            
            return height / width if width > 0 else 1.0
            
        except Exception:
            return 1.0
    
    def _create_visualizations(
        self, 
        image: np.ndarray, 
        mask: np.ndarray, 
        roi_box: Optional[Tuple[int, int, int, int]]
    ) -> Dict[str, Any]:
        """ì‹œê°í™” ìƒì„±"""
        try:
            visualizations = {}
            
            # 1. ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
            if len(image.shape) == 3:
                overlay = image.copy()
                mask_colored = np.zeros_like(image)
                mask_colored[:, :, 0] = mask  # ë¹¨ê°„ìƒ‰ ë§ˆìŠ¤í¬
                
                # ë¸”ë Œë”©
                alpha = self.config.overlay_opacity
                overlay = ((1 - alpha) * image + alpha * mask_colored).astype(np.uint8)
                visualizations['mask_overlay'] = overlay
            
            # 2. ë¶„í• ëœ ì˜ë¥˜ë§Œ ì¶”ì¶œ
            segmented = self._apply_mask_to_image(image, mask)
            visualizations['segmented_clothing'] = segmented
            
            # 3. ROI ì‹œê°í™”
            if roi_box and PIL_AVAILABLE:
                roi_vis = Image.fromarray(image)
                draw = ImageDraw.Draw(roi_vis)
                draw.rectangle(roi_box, outline=(0, 255, 0), width=3)
                visualizations['roi_visualization'] = np.array(roi_vis)
            
            # 4. ê²½ê³„ì„  ì‹œê°í™”
            if NUMPY_AVAILABLE:
                # ê°„ë‹¨í•œ ê²½ê³„ì„  ì¶”ì¶œ
                grad_x = np.abs(np.diff(mask.astype(np.float32), axis=1))
                grad_y = np.abs(np.diff(mask.astype(np.float32), axis=0))
                
                edges = np.zeros_like(mask)
                if grad_x.shape[1] == edges.shape[1] - 1:
                    edges[:-1, :-1] += grad_x
                if grad_y.shape[0] == edges.shape[0] - 1:
                    edges[:-1, :-1] += grad_y
                
                edges = (edges > 10).astype(np.uint8) * 255
                visualizations['boundaries'] = edges
            
            return visualizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _apply_mask_to_image(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©"""
        try:
            if len(image.shape) == 3:
                # 3ì±„ë„ ì´ë¯¸ì§€
                masked = image.copy()
                mask_bool = mask > 128
                
                for c in range(3):
                    masked[:, :, c] = np.where(mask_bool, image[:, :, c], 0)
                
                return masked
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                return np.where(mask > 128, image, 0)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _extract_cloth_features(self, mask: np.ndarray, image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            features = {}
            
            if NUMPY_AVAILABLE:
                # ê¸°ë³¸ í†µê³„
                features['area'] = int(np.sum(mask > 128))
                features['centroid'] = self._calculate_centroid(mask)
                features['bounding_box'] = self._calculate_bounding_box(mask)
                
                # ìƒ‰ìƒ íŠ¹ì§•
                if len(image.shape) == 3:
                    masked_pixels = image[mask > 128]
                    if len(masked_pixels) > 0:
                        features['dominant_color'] = [
                            float(np.mean(masked_pixels[:, 0])),
                            float(np.mean(masked_pixels[:, 1])),
                            float(np.mean(masked_pixels[:, 2]))
                        ]
                    else:
                        features['dominant_color'] = [0.0, 0.0, 0.0]
            
            return features
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_centroid(self, mask: np.ndarray) -> Tuple[float, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE:
                y_coords, x_coords = np.where(mask > 128)
                if len(x_coords) > 0:
                    centroid_x = float(np.mean(x_coords))
                    centroid_y = float(np.mean(y_coords))
                    return (centroid_x, centroid_y)
            
            # í´ë°±
            h, w = mask.shape
            return (w / 2.0, h / 2.0)
            
        except Exception:
            h, w = mask.shape
            return (w / 2.0, h / 2.0)
    
    def _calculate_bounding_box(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """ê²½ê³„ ë°•ìŠ¤ ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE:
                rows = np.any(mask > 128, axis=1)
                cols = np.any(mask > 128, axis=0)
                
                if np.any(rows) and np.any(cols):
                    rmin, rmax = np.where(rows)[0][[0, -1]]
                    cmin, cmax = np.where(cols)[0][[0, -1]]
                    return (int(cmin), int(rmin), int(cmax), int(rmax))
            
            # í´ë°±
            h, w = mask.shape
            return (0, 0, w, h)
            
        except Exception:
            h, w = mask.shape
            return (0, 0, w, h)
    
    def _extract_cloth_contours(self, mask: np.ndarray) -> List[np.ndarray]:
        """ì˜ë¥˜ ìœ¤ê³½ì„  ì¶”ì¶œ"""
        try:
            contours = []
            
            if SKIMAGE_AVAILABLE:
                from skimage import measure
                # ìœ¤ê³½ì„  ì°¾ê¸°
                contour_coords = measure.find_contours(mask > 128, 0.5)
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                for contour in contour_coords:
                    if len(contour) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                        contours.append(contour.astype(np.int32))
            
            return contours
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _update_ai_stats(self, method: str, confidence: float, total_time: float, quality_metrics: Dict[str, float]):
        """AI í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.ai_stats['total_processed'] += 1
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            prev_avg = self.ai_stats['average_confidence']
            count = self.ai_stats['total_processed']
            self.ai_stats['average_confidence'] = (prev_avg * (count - 1) + confidence) / count
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_emergency_result(self, reason: str) -> Dict[str, Any]:
        """ë¹„ìƒ ê²°ê³¼ ìƒì„±"""
        emergency_mask = np.zeros((512, 512), dtype=np.uint8)
        emergency_mask[128:384, 128:384] = 255  # ì¤‘ì•™ ì‚¬ê°í˜•
        
        return {
            'cloth_mask': emergency_mask,
            'segmented_clothing': emergency_mask,
            'confidence': 0.5,
            'method_used': 'emergency',
            'processing_time': 0.1,
            'quality_score': 0.5,
            'emergency_reason': reason[:100],
            'metadata': {
                'emergency_mode': True,
                'version': '31.0'
            }
        }

# ==============================================
# ğŸ”¥ 13. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ClothSegmentationStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ìµœì í™”ëœ ClothSegmentationStep ìƒì„±"""
    m3_config = {
        'method': SegmentationMethod.HYBRID_AI,
        'quality_level': QualityLevel.ULTRA,
        'enable_visualization': True,
        'enable_crf_postprocessing': True,
        'enable_multiscale_processing': True,
        'input_size': (1024, 1024),
        'confidence_threshold': 0.5
    }
    
    if 'config' in kwargs:
        kwargs['config'].update(m3_config)
    else:
        kwargs['config'] = m3_config
    
    return ClothSegmentationStep(**kwargs)

# ==============================================
# ğŸ”¥ 14. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

def test_cloth_segmentation_ai():
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # Step ìƒì„±
        step = create_cloth_segmentation_step(
            device="auto",
            segmentation_config={
                'quality_level': QualityLevel.HIGH,
                'enable_visualization': True,
                'confidence_threshold': 0.5
            }
        )
        
        # ì´ˆê¸°í™”
        if step.initialize():
            print(f"âœ… Step ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - ë¡œë“œëœ AI ëª¨ë¸: {len(step.ai_models)}ê°œ")
            print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•: {len(step.available_methods)}ê°œ")
        else:
            print(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        test_image_array = np.array(test_image)
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        processed_input = {
            'image': test_image_array,
            'from_step_01': {},
            'from_step_02': {}
        }
        
        result = step._run_ai_inference(processed_input)
        
        if result and 'cloth_mask' in result:
            print(f"âœ… AI ì¶”ë¡  ì„±ê³µ")
            print(f"   - ë°©ë²•: {result.get('method_used', 'unknown')}")
            print(f"   - ì‹ ë¢°ë„: {result.get('confidence', 0):.3f}")
            print(f"   - í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.3f}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
            print(f"   - ë§ˆìŠ¤í¬ í¬ê¸°: {result['cloth_mask'].shape if result['cloth_mask'] is not None else 'None'}")
        else:
            print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_basestepmixin_compatibility():
    """BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # Step ìƒì„±
        step = ClothSegmentationStep()
        
        # BaseStepMixin ìƒì† í™•ì¸
        print(f"âœ… BaseStepMixin ìƒì†: {isinstance(step, BaseStepMixin)}")
        print(f"âœ… Step ì´ë¦„: {step.step_name}")
        print(f"âœ… Step ID: {step.step_id}")
        
        # _run_ai_inference ë©”ì„œë“œ í™•ì¸
        import inspect
        is_async = inspect.iscoroutinefunction(step._run_ai_inference)
        print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async}")
        
        print("âœ… BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ ì •ë³´
# ==============================================

__version__ = "31.0.0"
__author__ = "MyCloset AI Team"
__description__ = "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„"
__compatibility_version__ = "BaseStepMixin_v19.1"

__all__ = [
    'ClothSegmentationStep',
    'RealSAMModel',
    'RealU2NetClothModel', 
    'RealDeepLabV3PlusModel',
    'CompleteEnhancedClothSegmentationAI',  # ì›ë³¸ ì¶”ê°€
    'DeepLabV3PlusBackbone',                # ì›ë³¸ ì¶”ê°€
    'ASPPModule',                           # ì›ë³¸ ì¶”ê°€
    'SelfCorrectionModule',                 # ì›ë³¸ ì¶”ê°€
    'SelfAttentionBlock',                   # ì›ë³¸ ì¶”ê°€
    'AdvancedPostProcessor',
    'SegmentationMethod',
    'ClothingType',                         # ì›ë³¸ ì¶”ê°€
    'QualityLevel',
    'EnhancedSegmentationConfig',           # ì›ë³¸ ì´ë¦„ ì‚¬ìš©
    'create_cloth_segmentation_step',
    'create_m3_max_segmentation_step',
    'test_cloth_segmentation_ai',
    'test_basestepmixin_compatibility'
]

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger.info("=" * 120)
logger.info("ğŸ”¥ Step 03 Cloth Segmentation v31.0 - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„")
logger.info("=" * 120)
logger.info("ğŸ¯ BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜:")
logger.info("   âœ… BaseStepMixin ì™„ì „ ìƒì†")
logger.info("   âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ë§Œ í™œìš© (ëª©ì—…/í´ë°± ì œê±°)")
logger.info("   âœ… step_model_requests.py ì™„ì „ ì§€ì›")
logger.info("ğŸ§  êµ¬í˜„ëœ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜:")
logger.info("   ğŸ”¥ DeepLabV3+ ì•„í‚¤í…ì²˜ (Google ìµœì‹  ì„¸ê·¸ë©˜í…Œì´ì…˜)")
logger.info("   ğŸŒŠ ASPP (Atrous Spatial Pyramid Pooling) ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ” Self-Correction Learning ë©”ì»¤ë‹ˆì¦˜")
logger.info("   ğŸ“ˆ Progressive Parsing ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ¯ SAM + U2Net + DeepLabV3+ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”")
logger.info("   âš¡ CRF í›„ì²˜ë¦¬ + ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬")
logger.info("   ğŸ”€ Edge Detection ë¸Œëœì¹˜")
logger.info("   ğŸ’« Multi-scale Feature Fusion")
logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - M3 Max: {IS_M3_MAX}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - MPS: {MPS_AVAILABLE}")
logger.info(f"   - SAM: {SAM_AVAILABLE}")
logger.info(f"   - DenseCRF: {DENSECRF_AVAILABLE}")
logger.info(f"   - Scikit-image: {SKIMAGE_AVAILABLE}")

if STEP_REQUIREMENTS:
    logger.info("âœ… step_model_requests.py ìš”êµ¬ì‚¬í•­ ë¡œë“œ ì„±ê³µ")
    logger.info(f"   - ëª¨ë¸ëª…: {STEP_REQUIREMENTS.model_name}")
    logger.info(f"   - Primary íŒŒì¼: {STEP_REQUIREMENTS.primary_file}")

logger.info("=" * 120)
logger.info("ğŸ‰ ClothSegmentationStep BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ!")

# ==============================================
# ğŸ”¥ 17. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 03 - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„")
    print("=" * 80)
    
    try:
        # ë™ê¸° í…ŒìŠ¤íŠ¸ë“¤
        test_basestepmixin_compatibility()
        print()
        test_cloth_segmentation_ai()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ BaseStepMixin ì™„ì „ ìƒì† ë° í˜¸í™˜")
    print("ğŸ§  ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)")
    print("âš¡ ì‹¤ì œ GPU ê°€ì† AI ì¶”ë¡  ì—”ì§„")
    print("ğŸ¯ SAM, U2Net, DeepLabV3+ ì§„ì§œ êµ¬í˜„")
    print("ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    print("ğŸ“Š 2.4GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš©")
    print("ğŸš« ëª©ì—…/í´ë°± ì½”ë“œ ì™„ì „ ì œê±°")
    print("=" * 80)