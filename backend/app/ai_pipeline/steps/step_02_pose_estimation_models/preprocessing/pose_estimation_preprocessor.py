"""
ğŸ”¥ Pose Estimation ì „ìš© ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
=====================================

í¬ì¦ˆ ì¶”ì •ì— ìµœì í™”ëœ ì „ì²˜ë¦¬ ê¸°ëŠ¥ë“¤:
1. ì¸ì²´ ì¤‘ì‹¬ ì •ë ¬ ë° í¬ë¡­
2. í•´ìƒë„ í‘œì¤€í™” (368x368, 256x256)
3. ê´€ì ˆ ì˜ì—­ ê°•í™”
4. ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±°
5. ì¸ì²´ ìì„¸ ì •ê·œí™”

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0 (ì™„ì „ êµ¬í˜„)
"""

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple, List
import logging
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

logger = logging.getLogger(__name__)

class PoseEstimationPreprocessor:
    """í¬ì¦ˆ ì¶”ì •ì— ìµœì í™”ëœ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, target_size: Tuple[int, int] = (368, 368)):
        self.target_size = target_size
        self.logger = logging.getLogger(f"{__name__}.PoseEstimationPreprocessor")
        
        # ì¸ì²´ ê°ì§€ê¸° (OpenCV HOG)
        self.hog = cv2.HOGDescriptor()
        self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        
        # í¬ì¦ˆ ì¶”ì •ìš© ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
        self.pose_params = {
            'joint_enhancement': True,
            'background_removal': True,
            'pose_normalization': True,
            'lighting_correction': True
        }
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'images_processed': 0,
            'human_detected': 0,
            'pose_aligned': 0,
            'joint_enhanced': 0
        }
    
    def preprocess_image(self, image: np.ndarray, mode: str = 'advanced') -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì •ì„ ìœ„í•œ ì™„ì „í•œ ì „ì²˜ë¦¬"""
        try:
            self.processing_stats['images_processed'] += 1
            self.logger.info(f"ğŸ”¥ í¬ì¦ˆ ì¶”ì • ì „ì²˜ë¦¬ ì‹œì‘ (ëª¨ë“œ: {mode})")
            
            # 1. ì´ë¯¸ì§€ ê²€ì¦
            validated_image = self._validate_image(image)
            
            # 2. ì¸ì²´ ê°ì§€ ë° í¬ì¦ˆ ì •ë ¬
            aligned_image, alignment_info = self._detect_and_align_human(validated_image)
            if alignment_info['human_detected']:
                self.processing_stats['human_detected'] += 1
                self.processing_stats['pose_aligned'] += 1
            
            # 3. í•´ìƒë„ í‘œì¤€í™”
            resized_image = self._standardize_resolution(aligned_image)
            
            # 4. í¬ì¦ˆ ì¶”ì • ìµœì í™”
            if mode == 'advanced':
                optimized_image = self._optimize_for_pose_estimation(resized_image)
                self.processing_stats['joint_enhanced'] += 1
            else:
                optimized_image = resized_image
            
            # 5. ì •ê·œí™” ë° í…ì„œ ë³€í™˜
            normalized_tensor = self._normalize_and_convert(optimized_image)
            
            # 6. ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
            preprocessing_result = {
                'processed_image': optimized_image,
                'tensor': normalized_tensor,
                'alignment_info': alignment_info,
                'target_size': self.target_size,
                'mode': mode,
                'pose_params': self.pose_params,
                'success': True
            }
            
            self.logger.info("âœ… í¬ì¦ˆ ì¶”ì • ì „ì²˜ë¦¬ ì™„ë£Œ")
            return preprocessing_result
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'processed_image': image,
                'tensor': torch.randn(1, 3, *self.target_size)
            }
    
    def _validate_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦ ë° ë³€í™˜"""
        try:
            # PIL Imageë¥¼ NumPyë¡œ ë³€í™˜
            if isinstance(image, Image.Image):
                image = np.array(image)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ RGBë¡œ ë³€í™˜
            if len(image.shape) == 2:
                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
            elif image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            
            # ë°ì´í„° íƒ€ì… ì •ê·œí™”
            if image.dtype != np.uint8:
                image = image.astype(np.uint8)
            
            return image
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return image
    
    def _detect_and_align_human(self, image: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """ì¸ì²´ ê°ì§€ ë° í¬ì¦ˆ ì •ë ¬"""
        try:
            # ì¸ì²´ ê°ì§€
            boxes, weights = self.hog.detectMultiScale(
                image, 
                winStride=(8, 8), 
                padding=(4, 4), 
                scale=1.05
            )
            
            if len(boxes) > 0:
                # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ì¸ì²´ ì˜ì—­ ì„ íƒ
                best_box = boxes[np.argmax(weights)]
                x, y, w, h = best_box
                
                # í¬ì¦ˆ ì¶”ì •ì„ ìœ„í•œ ì—¬ë°± ì¶”ê°€ (ìƒì²´ ì¤‘ì‹¬)
                margin_x = int(w * 0.3)
                margin_y = int(h * 0.4)  # ìƒì²´ì— ë” ë§ì€ ì—¬ë°±
                
                x1 = max(0, x - margin_x)
                y1 = max(0, y - margin_y)
                x2 = min(image.shape[1], x + w + margin_x)
                y2 = min(image.shape[0], y + h + margin_y)
                
                # í¬ë¡­ëœ ì´ë¯¸ì§€
                cropped = image[y1:y2, x1:x2]
                
                # í¬ì¦ˆ ì •ë ¬ (ìƒì²´ ì¤‘ì‹¬)
                aligned = self._align_pose_center(cropped)
                
                alignment_info = {
                    'human_detected': True,
                    'bbox': [x1, y1, x2, y2],
                    'confidence': float(np.max(weights)),
                    'original_size': image.shape[:2],
                    'cropped_size': cropped.shape[:2],
                    'aligned_size': aligned.shape[:2],
                    'pose_centered': True
                }
                
                return aligned, alignment_info
            else:
                # ì¸ì²´ê°€ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš° ì¤‘ì•™ í¬ë¡­
                h, w = image.shape[:2]
                center_x, center_y = w // 2, h // 2
                crop_size = min(w, h)
                
                x1 = max(0, center_x - crop_size // 2)
                y1 = max(0, center_y - crop_size // 2)
                x2 = min(w, x1 + crop_size)
                y2 = min(h, y1 + crop_size)
                
                cropped = image[y1:y2, x1:x2]
                aligned = self._align_pose_center(cropped)
                
                alignment_info = {
                    'human_detected': False,
                    'bbox': [x1, y1, x2, y2],
                    'confidence': 0.0,
                    'original_size': image.shape[:2],
                    'cropped_size': cropped.shape[:2],
                    'aligned_size': aligned.shape[:2],
                    'pose_centered': False
                }
                
                return aligned, alignment_info
                
        except Exception as e:
            self.logger.warning(f"ì¸ì²´ ê°ì§€ ë° ì •ë ¬ ì‹¤íŒ¨: {e}")
            return image, {
                'human_detected': False,
                'bbox': [0, 0, image.shape[1], image.shape[0]],
                'confidence': 0.0,
                'original_size': image.shape[:2],
                'cropped_size': image.shape[:2],
                'aligned_size': image.shape[:2],
                'pose_centered': False
            }
    
    def _align_pose_center(self, image: np.ndarray) -> np.ndarray:
        """í¬ì¦ˆ ì¤‘ì‹¬ ì •ë ¬"""
        try:
            h, w = image.shape[:2]
            
            # ìƒì²´ ì¤‘ì‹¬ ê³„ì‚° (ìƒë‹¨ 60% ì˜ì—­)
            upper_h = int(h * 0.6)
            center_x = w // 2
            center_y = upper_h // 2
            
            # ì •ì‚¬ê°í˜• í¬ë¡­
            crop_size = min(w, upper_h)
            x1 = max(0, center_x - crop_size // 2)
            y1 = max(0, center_y - crop_size // 2)
            x2 = min(w, x1 + crop_size)
            y2 = min(upper_h, y1 + crop_size)
            
            # í¬ë¡­ ë° ì •ì‚¬ê°í˜•ìœ¼ë¡œ íŒ¨ë”©
            cropped = image[y1:y2, x1:x2]
            
            # ì •ì‚¬ê°í˜•ì´ ë˜ë„ë¡ íŒ¨ë”©
            target_size = max(cropped.shape[:2])
            padded = np.zeros((target_size, target_size, 3), dtype=np.uint8)
            
            # ì¤‘ì•™ì— ì´ë¯¸ì§€ ë°°ì¹˜
            y_offset = (target_size - cropped.shape[0]) // 2
            x_offset = (target_size - cropped.shape[1]) // 2
            
            padded[y_offset:y_offset+cropped.shape[0], 
                   x_offset:x_offset+cropped.shape[1]] = cropped
            
            return padded
            
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ ì¤‘ì‹¬ ì •ë ¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _standardize_resolution(self, image: np.ndarray) -> np.ndarray:
        """í•´ìƒë„ í‘œì¤€í™”"""
        try:
            # ëª©í‘œ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LANCZOS4)
            return resized
            
        except Exception as e:
            self.logger.warning(f"í•´ìƒë„ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _optimize_for_pose_estimation(self, image: np.ndarray) -> np.ndarray:
        """í¬ì¦ˆ ì¶”ì • ìµœì í™”"""
        try:
            optimized = image.copy()
            
            # 1. ê´€ì ˆ ì˜ì—­ ê°•í™”
            if self.pose_params['joint_enhancement']:
                optimized = self._enhance_joint_regions(optimized)
            
            # 2. ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±°
            if self.pose_params['background_removal']:
                optimized = self._remove_background_noise(optimized)
            
            # 3. ì¡°ëª… ë³´ì •
            if self.pose_params['lighting_correction']:
                optimized = self._correct_lighting(optimized)
            
            # 4. í¬ì¦ˆ ì •ê·œí™”
            if self.pose_params['pose_normalization']:
                optimized = self._normalize_pose(optimized)
            
            return optimized
            
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ ì¶”ì • ìµœì í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _enhance_joint_regions(self, image: np.ndarray) -> np.ndarray:
        """ê´€ì ˆ ì˜ì—­ ê°•í™”"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # ì—£ì§€ ê°ì§€ (ê´€ì ˆ ê²½ê³„ ê°•í™”)
            edges = cv2.Canny(gray, 50, 150)
            
            # ì—£ì§€ë¥¼ RGBë¡œ ë³€í™˜
            edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
            
            # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì—£ì§€ í•©ì„±
            enhanced = cv2.addWeighted(image, 0.8, edges_rgb, 0.2, 0)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"ê´€ì ˆ ì˜ì—­ ê°•í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _remove_background_noise(self, image: np.ndarray) -> np.ndarray:
        """ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.GaussianBlur(image, (3, 3), 0)
            
            # ì–‘ë°©í–¥ í•„í„°ë¡œ ì—£ì§€ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.bilateralFilter(denoised, 9, 75, 75)
            
            return denoised
            
        except Exception as e:
            self.logger.warning(f"ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    def _correct_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ë³´ì •"""
        try:
            # LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            
            # CLAHEë¡œ ì¡°ëª… ì •ê·œí™”
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            
            # ìƒ‰ìƒ ë³´ì •
            a = cv2.convertScaleAbs(a, alpha=1.1, beta=0)
            b = cv2.convertScaleAbs(b, alpha=1.1, beta=0)
            
            # RGBë¡œ ë³€í™˜
            corrected = cv2.merge([l, a, b])
            corrected = cv2.cvtColor(corrected, cv2.COLOR_LAB2RGB)
            
            return corrected
            
        except Exception as e:
            self.logger.warning(f"ì¡°ëª… ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _normalize_pose(self, image: np.ndarray) -> np.ndarray:
        """í¬ì¦ˆ ì •ê·œí™”"""
        try:
            # íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
            normalized = image.copy()
            
            # ê° ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
            for i in range(3):
                normalized[:, :, i] = cv2.equalizeHist(normalized[:, :, i])
            
            # ëŒ€ë¹„ í–¥ìƒ
            normalized = cv2.convertScaleAbs(normalized, alpha=1.1, beta=5)
            
            return normalized
            
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def _normalize_and_convert(self, image: np.ndarray) -> torch.Tensor:
        """ì •ê·œí™” ë° í…ì„œ ë³€í™˜"""
        try:
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized = image.astype(np.float32) / 255.0
            
            # ImageNet í‰ê· /í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            
            normalized = (normalized - mean) / std
            
            # í…ì„œë¡œ ë³€í™˜ [H, W, C] -> [C, H, W] -> [1, C, H, W]
            tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
            
            return tensor
            
        except Exception as e:
            self.logger.warning(f"ì •ê·œí™” ë° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.randn(1, 3, *self.target_size)
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.processing_stats.copy()
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.processing_stats = {
            'images_processed': 0,
            'human_detected': 0,
            'pose_aligned': 0,
            'joint_enhanced': 0
        }
    
    def update_pose_params(self, **kwargs):
        """í¬ì¦ˆ ì¶”ì • íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if key in self.pose_params:
                self.pose_params[key] = value
                self.logger.info(f"í¬ì¦ˆ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: {key} = {value}")
