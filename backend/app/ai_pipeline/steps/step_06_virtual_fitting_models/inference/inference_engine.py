#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Virtual Fitting Inference Engine
=================================================

ğŸ¯ ê°€ìƒ í”¼íŒ… ì¶”ë¡  ì—”ì§„
âœ… ë‹¤ì¤‘ ëª¨ë¸ ì¶”ë¡  ê´€ë¦¬
âœ… M3 Max ìµœì í™”
âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
âœ… ì‹¤ì‹œê°„ í”¼íŒ… ì§€ì›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import time
import cv2

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •"""
    batch_size: int = 1
    use_mps: bool = True
    memory_efficient: bool = True
    enable_ensemble: bool = True
    confidence_threshold: float = 0.5
    max_models: int = 8
    fitting_resolution: Tuple[int, int] = (512, 512)

class VirtualFittingInferenceEngine(nn.Module):
    """
    ğŸ”¥ Virtual Fitting ì¶”ë¡  ì—”ì§„
    
    ê°€ìƒ í”¼íŒ…ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    """
    
    def __init__(self, config: InferenceConfig = None):
        super().__init__()
        self.config = config or InferenceConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Virtual Fitting ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.models = {}
        self.model_weights = {}
        self.ensemble_system = None
        
        # ì¶”ë¡  í†µê³„
        self.inference_stats = {
            "total_inferences": 0,
            "total_time": 0.0,
            "avg_time": 0.0,
            "success_rate": 1.0
        }
        
        self.logger.info("âœ… Virtual Fitting ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model(self, model_name: str, model_path: str, weight: float = 1.0):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ë¡œì§ (ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ëª¨ë¸ ìƒì„±)
            model = self._create_dummy_model(model_name)
            model.to(self.device)
            model.eval()
            
            self.models[model_name] = model
            self.model_weights[model_name] = weight
            
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} (ê°€ì¤‘ì¹˜: {weight})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {str(e)}")
            return False
    
    def _create_dummy_model(self, model_name: str) -> nn.Module:
        """ë”ë¯¸ ëª¨ë¸ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ ë¡œë“œ)"""
        class DummyVirtualFittingModel(nn.Module):
            def __init__(self, name: str):
                super().__init__()
                self.name = name
                # ê°„ë‹¨í•œ CNN ê¸°ë°˜ í”¼íŒ… ëª¨ë¸
                self.feature_extractor = nn.Sequential(
                    nn.Conv2d(9, 64, 3, padding=1),  # 3ì±„ë„ ì‚¬ëŒ + 3ì±„ë„ ì˜ë¥˜ + 3ì±„ë„ í¬ì¦ˆ
                    nn.ReLU(),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.ReLU()
                )
                
                self.fitting_head = nn.Sequential(
                    nn.Conv2d(256, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 3, 1),  # 3ì±„ë„ (RGB)
                    nn.Sigmoid()  # 0-1 ë²”ìœ„
                )
            
            def forward(self, person_image, cloth_image, pose_info):
                # ì‚¬ëŒ, ì˜ë¥˜, í¬ì¦ˆ ì •ë³´ ê²°í•©
                combined_input = torch.cat([person_image, cloth_image, pose_info], dim=1)
                features = self.feature_extractor(combined_input)
                fitted_result = self.fitting_head(features)
                return fitted_result
        
        return DummyVirtualFittingModel(model_name)
    
    def set_ensemble_system(self, ensemble_system):
        """ì•™ìƒë¸” ì‹œìŠ¤í…œ ì„¤ì •"""
        self.ensemble_system = ensemble_system
        self.logger.info("âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
    
    def forward(self, person_image: torch.Tensor, cloth_image: torch.Tensor, 
                pose_keypoints: Optional[torch.Tensor] = None,
                body_shape: Optional[torch.Tensor] = None,
                fitting_style: Optional[str] = None) -> Dict[str, torch.Tensor]:
        """
        ì¶”ë¡  ìˆ˜í–‰
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€ (B, C, H, W)
            cloth_image: ì˜ë¥˜ ì´ë¯¸ì§€ (B, C, H, W)
            pose_keypoints: í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ (B, N, 2)
            body_shape: ì‹ ì²´ í˜•íƒœ ì •ë³´ (B, M)
            fitting_style: í”¼íŒ… ìŠ¤íƒ€ì¼ ("tight", "loose", "normal")
        
        Returns:
            í”¼íŒ… ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not self._validate_inputs(person_image, cloth_image):
                raise ValueError("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            person_image = person_image.to(self.device)
            cloth_image = cloth_image.to(self.device)
            if pose_keypoints is not None:
                pose_keypoints = pose_keypoints.to(self.device)
            if body_shape is not None:
                body_shape = body_shape.to(self.device)
            
            # í¬ì¦ˆ ì •ë³´ ìƒì„±
            pose_info = self._create_pose_info(person_image, pose_keypoints, body_shape)
            
            # ê°œë³„ ëª¨ë¸ ì¶”ë¡ 
            model_outputs = []
            model_confidences = []
            
            for model_name, model in self.models.items():
                try:
                    with torch.no_grad():
                        # ëª¨ë¸ë³„ ì¶”ë¡ 
                        output = self._inference_single_model(model, person_image, cloth_image, pose_info, fitting_style)
                        confidence = self._calculate_confidence(output, person_image, cloth_image)
                        
                        model_outputs.append(output)
                        model_confidences.append(confidence)
                        
                except Exception as e:
                    self.logger.warning(f"ëª¨ë¸ {model_name} ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                    continue
            
            if not model_outputs:
                raise RuntimeError("ëª¨ë“  ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨")
            
            # ì•™ìƒë¸” ì¶”ë¡ 
            if self.ensemble_system and len(model_outputs) > 1:
                ensemble_output = self.ensemble_system(model_outputs, model_confidences)
            else:
                ensemble_output = model_outputs[0] if model_outputs else torch.zeros_like(person_image)
            
            # í›„ì²˜ë¦¬
            final_output = self._postprocess_output(ensemble_output, person_image, cloth_image)
            
            # í”¼íŒ… í’ˆì§ˆ í‰ê°€
            fitting_quality = self._evaluate_fitting_quality(final_output, person_image, cloth_image)
            
            # ì¶”ë¡  í†µê³„ ì—…ë°ì´íŠ¸
            inference_time = time.time() - start_time
            self._update_inference_stats(inference_time, True)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "fitted_result": final_output,
                "fitting_quality": fitting_quality,
                "model_outputs": model_outputs,
                "model_confidences": model_confidences,
                "ensemble_output": ensemble_output,
                "inference_time": inference_time,
                "success": True
            }
            
            self.logger.debug(f"âœ… ê°€ìƒ í”¼íŒ… ì¶”ë¡  ì™„ë£Œ - ì‹œê°„: {inference_time:.3f}ì´ˆ")
            return result
            
        except Exception as e:
            # ì¶”ë¡  ì‹¤íŒ¨ ì²˜ë¦¬
            inference_time = time.time() - start_time
            self._update_inference_stats(inference_time, False)
            
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "fitted_result": person_image,
                "fitting_quality": 0.0,
                "model_outputs": [],
                "model_confidences": [],
                "ensemble_output": person_image,
                "inference_time": inference_time,
                "success": False,
                "error": str(e)
            }
    
    def _validate_inputs(self, person_image: torch.Tensor, cloth_image: torch.Tensor) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        if person_image.dim() != 4 or cloth_image.dim() != 4:
            return False
        
        if person_image.size(0) != cloth_image.size(0):
            return False
        
        if person_image.size(2) != cloth_image.size(2) or person_image.size(3) != cloth_image.size(3):
            return False
        
        return True
    
    def _create_pose_info(self, person_image: torch.Tensor, pose_keypoints: Optional[torch.Tensor],
                          body_shape: Optional[torch.Tensor]) -> torch.Tensor:
        """í¬ì¦ˆ ì •ë³´ ìƒì„±"""
        batch_size, channels, height, width = person_image.shape
        
        if pose_keypoints is not None:
            # í‚¤í¬ì¸íŠ¸ë¥¼ ì´ë¯¸ì§€ í˜•íƒœë¡œ ë³€í™˜
            pose_info = torch.zeros(batch_size, 3, height, width, device=self.device)
            
            for b in range(batch_size):
                keypoints = pose_keypoints[b]
                if keypoints.numel() > 0:
                    # í‚¤í¬ì¸íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°
                    for kp in keypoints:
                        if kp[0] >= 0 and kp[1] >= 0:
                            x, y = int(kp[0] * width), int(kp[1] * height)
                            if 0 <= x < width and 0 <= y < height:
                                pose_info[b, 0, y, x] = 1.0  # R
                                pose_info[b, 1, y, x] = 1.0  # G
                                pose_info[b, 2, y, x] = 1.0  # B
        else:
            # ê¸°ë³¸ í¬ì¦ˆ ì •ë³´ (ì „ì²´ ì´ë¯¸ì§€)
            pose_info = torch.ones(batch_size, 3, height, width, device=self.device) * 0.5
        
        return pose_info
    
    def _inference_single_model(self, model: nn.Module, person_image: torch.Tensor, 
                               cloth_image: torch.Tensor, pose_info: torch.Tensor,
                               fitting_style: Optional[str] = None) -> torch.Tensor:
        """ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡ """
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        processed_person = self._preprocess_image(person_image)
        processed_cloth = self._preprocess_image(cloth_image)
        processed_pose = self._preprocess_image(pose_info)
        
        # ëª¨ë¸ ì¶”ë¡ 
        output = model(processed_person, processed_cloth, processed_pose)
        
        # í”¼íŒ… ìŠ¤íƒ€ì¼ ì ìš©
        if fitting_style:
            output = self._apply_fitting_style(output, fitting_style)
        
        return output
    
    def _preprocess_image(self, image: torch.Tensor) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        if image.max() > 1.0:
            image = image / 255.0
        
        # í¬ê¸° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
        target_size = self.config.fitting_resolution
        if image.size(2) != target_size[0] or image.size(3) != target_size[1]:
            image = F.interpolate(image, size=target_size, mode='bilinear', align_corners=False)
        
        return image
    
    def _apply_fitting_style(self, output: torch.Tensor, fitting_style: str) -> torch.Tensor:
        """í”¼íŒ… ìŠ¤íƒ€ì¼ ì ìš©"""
        if fitting_style == "tight":
            # íƒ€ì´íŠ¸í•œ í”¼íŒ… (ì˜ë¥˜ë¥¼ ë” ë°€ì°©)
            output = output * 1.2
        elif fitting_style == "loose":
            # ë£¨ì¦ˆí•œ í”¼íŒ… (ì˜ë¥˜ë¥¼ ë” ëŠìŠ¨í•˜ê²Œ)
            output = output * 0.8
        # "normal"ì€ ê¸°ë³¸ê°’
        
        # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
        output = torch.clamp(output, 0.0, 1.0)
        return output
    
    def _calculate_confidence(self, output: torch.Tensor, person_image: torch.Tensor, 
                             cloth_image: torch.Tensor) -> float:
        """ì¶œë ¥ ì‹ ë¢°ë„ ê³„ì‚°"""
        if output.numel() == 0:
            return 0.0
        
        # í”¼íŒ… ê²°ê³¼ì˜ í’ˆì§ˆ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        
        # 1. ì˜ë¥˜ì™€ ì‚¬ëŒ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ë„
        if output.shape == person_image.shape:
            similarity = F.cosine_similarity(
                output.flatten(), person_image.flatten(), dim=0
            )
            similarity_score = float(similarity.item())
        else:
            similarity_score = 0.5
        
        # 2. ì¶œë ¥ì˜ í’ˆì§ˆ (ì—£ì§€, í…ìŠ¤ì²˜ ë“±)
        quality_score = self._calculate_output_quality(output)
        
        # 3. ì˜ë¥˜ ì •ë³´ ë³´ì¡´ ì •ë„
        cloth_preservation = self._calculate_cloth_preservation(output, cloth_image)
        
        # ì¢…í•© ì‹ ë¢°ë„
        confidence = (similarity_score * 0.4 + quality_score * 0.3 + cloth_preservation * 0.3)
        
        return max(0.0, min(1.0, confidence))
    
    def _calculate_output_quality(self, output: torch.Tensor) -> float:
        """ì¶œë ¥ í’ˆì§ˆ ê³„ì‚°"""
        if output.numel() == 0:
            return 0.0
        
        # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­
        # 1. ì—£ì§€ í’ˆì§ˆ
        if output.dim() == 4:
            edge_quality = 0.0
            for b in range(output.size(0)):
                for c in range(output.size(1)):
                    channel = output[b, c]
                    if channel.numel() > 0:
                        # Sobel ì—£ì§€ ê²€ì¶œ
                        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                             dtype=torch.float32, device=output.device)
                        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                             dtype=torch.float32, device=output.device)
                        
                        edge_x = F.conv2d(channel.unsqueeze(0).unsqueeze(0), sobel_x.unsqueeze(0).unsqueeze(0))
                        edge_y = F.conv2d(channel.unsqueeze(0).unsqueeze(0), sobel_y.unsqueeze(0).unsqueeze(0))
                        
                        edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
                        edge_quality += float(edge_magnitude.mean().item())
            
            edge_quality /= (output.size(0) * output.size(1))
        else:
            edge_quality = 0.5
        
        # 2. í…ìŠ¤ì²˜ í’ˆì§ˆ
        texture_quality = 1.0 / (1.0 + float(output.std().item()))
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality_score = (edge_quality * 0.6 + texture_quality * 0.4)
        
        return quality_score
    
    def _calculate_cloth_preservation(self, output: torch.Tensor, cloth_image: torch.Tensor) -> float:
        """ì˜ë¥˜ ì •ë³´ ë³´ì¡´ ì •ë„ ê³„ì‚°"""
        if output.numel() == 0 or cloth_image.numel() == 0:
            return 0.0
        
        # ì˜ë¥˜ ì´ë¯¸ì§€ì™€ ì¶œë ¥ ê°„ì˜ ìƒê´€ê´€ê³„
        try:
            if output.shape == cloth_image.shape:
                correlation = F.cosine_similarity(
                    output.flatten(), cloth_image.flatten(), dim=0
                )
                preservation_score = float(correlation.item())
            else:
                preservation_score = 0.5
        except:
            preservation_score = 0.5
        
        return max(0.0, min(1.0, preservation_score))
    
    def _postprocess_output(self, output: torch.Tensor, person_image: torch.Tensor, 
                           cloth_image: torch.Tensor) -> torch.Tensor:
        """ì¶œë ¥ í›„ì²˜ë¦¬"""
        # ì¶œë ¥ í¬ê¸°ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¡°ì •
        if output.size(2) != person_image.size(2) or output.size(3) != person_image.size(3):
            output = F.interpolate(output, size=(person_image.size(2), person_image.size(3)), 
                                 mode='bilinear', align_corners=False)
        
        # ìƒ‰ìƒ ë³´ì •
        output = self._color_correction(output, person_image, cloth_image)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        output = self._denoise_output(output)
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
        if self.config.confidence_threshold > 0:
            confidence_mask = self._calculate_fitting_confidence(output) > self.config.confidence_threshold
            output = output * confidence_mask.float()
        
        return output
    
    def _color_correction(self, output: torch.Tensor, person_image: torch.Tensor, 
                          cloth_image: torch.Tensor) -> torch.Tensor:
        """ìƒ‰ìƒ ë³´ì •"""
        corrected_output = output.clone()
        
        # ì‚¬ëŒ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ë¶„í¬ì— ë§ì¶° ë³´ì •
        for b in range(output.size(0)):
            for c in range(output.size(1)):
                person_channel = person_image[b, c]
                output_channel = output[b, c]
                
                if person_channel.numel() > 0 and output_channel.numel() > 0:
                    # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­
                    person_mean = person_channel.mean()
                    output_mean = output_channel.mean()
                    
                    if output_mean > 0:
                        corrected_output[b, c] = output_channel * (person_mean / output_mean)
        
        # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
        corrected_output = torch.clamp(corrected_output, 0.0, 1.0)
        
        return corrected_output
    
    def _denoise_output(self, output: torch.Tensor) -> torch.Tensor:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        denoised_output = output.clone()
        
        # ê°€ìš°ì‹œì•ˆ ìŠ¤ë¬´ë”©ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        for b in range(output.size(0)):
            for c in range(output.size(1)):
                channel = output[b, c]
                if channel.numel() > 0:
                    denoised_output[b, c] = self._gaussian_smooth_2d(channel)
        
        return denoised_output
    
    def _gaussian_smooth_2d(self, channel: torch.Tensor) -> torch.Tensor:
        """2D ê°€ìš°ì‹œì•ˆ ìŠ¤ë¬´ë”©"""
        if channel.dim() != 2:
            return channel
        
        # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
        kernel_size = 3
        sigma = 0.5
        
        # 1D ê°€ìš°ì‹œì•ˆ ì»¤ë„
        x = torch.arange(-kernel_size // 2, kernel_size // 2 + 1, device=channel.device)
        gaussian_1d = torch.exp(-(x ** 2) / (2 * sigma ** 2))
        gaussian_1d = gaussian_1d / gaussian_1d.sum()
        
        # 2D ê°€ìš°ì‹œì•ˆ ì»¤ë„
        gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)
        
        # íŒ¨ë”© ì¶”ê°€
        padded_channel = F.pad(channel.unsqueeze(0).unsqueeze(0), 
                              (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2), 
                              mode='reflect')
        
        # ì»¨ë³¼ë£¨ì…˜ ì ìš©
        smoothed_channel = F.conv2d(padded_channel, gaussian_2d.unsqueeze(0).unsqueeze(0))
        
        return smoothed_channel.squeeze()
    
    def _calculate_fitting_confidence(self, output: torch.Tensor) -> torch.Tensor:
        """í”¼íŒ… ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚°
        confidence = torch.ones_like(output[:, :1, :, :])
        
        # ì—£ì§€ ì˜ì—­ì—ì„œ ë‚®ì€ ì‹ ë¢°ë„
        if output.dim() == 4:
            for b in range(output.size(0)):
                for c in range(output.size(1)):
                    channel = output[b, c]
                    if channel.numel() > 0:
                        # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ
                        edge = torch.abs(channel[:, 1:] - channel[:, :-1]) + torch.abs(channel[1:, :] - channel[:-1, :])
                        edge_mask = edge > 0.1
                        confidence[b, 0][edge_mask] = 0.7
        
        return confidence
    
    def _evaluate_fitting_quality(self, fitted_result: torch.Tensor, person_image: torch.Tensor, 
                                 cloth_image: torch.Tensor) -> float:
        """í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        if fitted_result.numel() == 0:
            return 0.0
        
        # 1. ìì—°ìŠ¤ëŸ¬ì›€ (ì‚¬ëŒ ì´ë¯¸ì§€ì™€ì˜ ìœ ì‚¬ë„)
        naturalness = self._calculate_naturalness(fitted_result, person_image)
        
        # 2. ì˜ë¥˜ ë³´ì¡´ ì •ë„
        cloth_preservation = self._calculate_cloth_preservation(fitted_result, cloth_image)
        
        # 3. ì‹œê°ì  í’ˆì§ˆ
        visual_quality = self._calculate_output_quality(fitted_result)
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality_score = (naturalness * 0.4 + cloth_preservation * 0.3 + visual_quality * 0.3)
        
        return quality_score
    
    def _calculate_naturalness(self, fitted_result: torch.Tensor, person_image: torch.Tensor) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚°"""
        if fitted_result.numel() == 0 or person_image.numel() == 0:
            return 0.0
        
        try:
            if fitted_result.shape == person_image.shape:
                # êµ¬ì¡°ì  ìœ ì‚¬ë„
                structural_similarity = F.cosine_similarity(
                    fitted_result.flatten(), person_image.flatten(), dim=0
                )
                naturalness_score = float(structural_similarity.item())
            else:
                naturalness_score = 0.5
        except:
            naturalness_score = 0.5
        
        return max(0.0, min(1.0, naturalness_score))
    
    def _update_inference_stats(self, inference_time: float, success: bool):
        """ì¶”ë¡  í†µê³„ ì—…ë°ì´íŠ¸"""
        self.inference_stats["total_inferences"] += 1
        self.inference_stats["total_time"] += inference_time
        self.inference_stats["avg_time"] = self.inference_stats["total_time"] / self.inference_stats["total_inferences"]
        
        if not success:
            failed_count = int((1 - self.inference_stats["success_rate"]) * self.inference_stats["total_inferences"])
            self.inference_stats["success_rate"] = (self.inference_stats["total_inferences"] - failed_count - 1) / self.inference_stats["total_inferences"]
    
    def get_inference_stats(self) -> Dict[str, Any]:
        """ì¶”ë¡  í†µê³„ ë°˜í™˜"""
        return self.inference_stats.copy()
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        model_info = {}
        for model_name, model in self.models.items():
            model_info[model_name] = {
                "weight": self.model_weights.get(model_name, 1.0),
                "parameters": sum(p.numel() for p in model.parameters()),
                "device": str(next(model.parameters()).device)
            }
        return model_info
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.logger.info("âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

# ì¶”ë¡  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_virtual_fitting_inference_engine(config: InferenceConfig = None) -> VirtualFittingInferenceEngine:
    """Virtual Fitting ì¶”ë¡  ì—”ì§„ ìƒì„±"""
    return VirtualFittingInferenceEngine(config)

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¶”ë¡  ì—”ì§„ ìƒì„±
def create_default_inference_engine() -> VirtualFittingInferenceEngine:
    """ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¶”ë¡  ì—”ì§„ ìƒì„±"""
    config = InferenceConfig(
        batch_size=1,
        use_mps=True,
        memory_efficient=True,
        enable_ensemble=True,
        confidence_threshold=0.5,
        max_models=8,
        fitting_resolution=(512, 512)
    )
    return VirtualFittingInferenceEngine(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ì¶”ë¡  ì—”ì§„ ìƒì„±
    engine = create_default_inference_engine()
    
    # ë”ë¯¸ ëª¨ë¸ ë¡œë“œ
    engine.load_model("fitting_model_1", "dummy_path_1", 1.0)
    engine.load_model("fitting_model_2", "dummy_path_2", 0.8)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, channels, height, width = 2, 3, 256, 256
    test_person = torch.randn(batch_size, channels, height, width)
    test_cloth = torch.randn(batch_size, channels, height, width)
    
    # ì¶”ë¡  ìˆ˜í–‰
    result = engine(test_person, test_cloth)
    print(f"ê°€ìƒ í”¼íŒ… ì¶”ë¡  ê²°ê³¼: {result['success']}")
    print(f"ì¶”ë¡  ì‹œê°„: {result['inference_time']:.3f}ì´ˆ")
    print(f"í”¼íŒ… í’ˆì§ˆ: {result['fitting_quality']:.3f}")
    print(f"ëª¨ë¸ ì •ë³´: {engine.get_model_info()}")
    print(f"ì¶”ë¡  í†µê³„: {engine.get_inference_stats()}")
