#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Virtual Fitting Core
====================================

ğŸ¯ ê°€ìƒ í”¼íŒ… í•µì‹¬ ê¸°ëŠ¥
âœ… ì˜ë¥˜ í”¼íŒ… ì•Œê³ ë¦¬ì¦˜
âœ… ì‹ ì²´ í˜•íƒœ ë¶„ì„
âœ… í”¼íŒ… í’ˆì§ˆ í‰ê°€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class FittingConfig:
    """í”¼íŒ… ì„¤ì •"""
    input_size: Tuple[int, int] = (512, 512)
    output_size: Tuple[int, int] = (512, 512)
    use_mps: bool = True
    enable_quality_assessment: bool = True
    fitting_style: str = "normal"  # tight, loose, normal

class VirtualFittingCore(nn.Module):
    """ê°€ìƒ í”¼íŒ… í•µì‹¬ ê¸°ëŠ¥"""
    
    def __init__(self, config: FittingConfig = None):
        super().__init__()
        self.config = config or FittingConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Virtual Fitting ì½”ì–´ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # í”¼íŒ… í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ
        if self.config.enable_quality_assessment:
            self.quality_assessor = self._create_quality_assessor()
        
        # í”¼íŒ… ìŠ¤íƒ€ì¼ë³„ ê°€ì¤‘ì¹˜
        self.fitting_weights = {
            "tight": 1.2,
            "loose": 0.8,
            "normal": 1.0
        }
        
        self.logger.info("âœ… Virtual Fitting ì½”ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_quality_assessor(self) -> nn.Module:
        """í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ ìƒì„±"""
        return nn.Sequential(
            nn.Linear(512 * 512 * 3, 256),  # RGB ì´ë¯¸ì§€
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        ).to(self.device)
    
    def forward(self, person_image: torch.Tensor, cloth_image: torch.Tensor,
                pose_keypoints: Optional[torch.Tensor] = None,
                body_shape: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        ê°€ìƒ í”¼íŒ… ìˆ˜í–‰
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€ (B, C, H, W)
            cloth_image: ì˜ë¥˜ ì´ë¯¸ì§€ (B, C, H, W)
            pose_keypoints: í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ (B, N, 2)
            body_shape: ì‹ ì²´ í˜•íƒœ ì •ë³´ (B, M)
        
        Returns:
            í”¼íŒ… ê²°ê³¼
        """
        # ì…ë ¥ ê²€ì¦
        if not self._validate_inputs(person_image, cloth_image):
            raise ValueError("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        person_image = person_image.to(self.device)
        cloth_image = cloth_image.to(self.device)
        if pose_keypoints is not None:
            pose_keypoints = pose_keypoints.to(self.device)
        if body_shape is not None:
            body_shape = body_shape.to(self.device)
        
        # 1ë‹¨ê³„: ì‹ ì²´ í˜•íƒœ ë¶„ì„
        body_analysis = self._analyze_body_shape(person_image, pose_keypoints, body_shape)
        
        # 2ë‹¨ê³„: ì˜ë¥˜ ì „ì²˜ë¦¬
        processed_cloth = self._preprocess_cloth(cloth_image, body_analysis)
        
        # 3ë‹¨ê³„: í”¼íŒ… ì•Œê³ ë¦¬ì¦˜ ì ìš©
        fitted_result = self._apply_fitting_algorithm(person_image, processed_cloth, body_analysis)
        
        # 4ë‹¨ê³„: í”¼íŒ… ìŠ¤íƒ€ì¼ ì ìš©
        styled_result = self._apply_fitting_style(fitted_result)
        
        # 5ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
        quality_score = self._assess_fitting_quality(styled_result, person_image, cloth_image)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "fitted_result": styled_result,
            "body_analysis": body_analysis,
            "quality_score": quality_score,
            "fitting_style": self.config.fitting_style
        }
        
        return result
    
    def _validate_inputs(self, person_image: torch.Tensor, cloth_image: torch.Tensor) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        if person_image.dim() != 4 or cloth_image.dim() != 4:
            return False
        
        if person_image.size(0) != cloth_image.size(0):
            return False
        
        if person_image.size(2) != cloth_image.size(2) or person_image.size(3) != cloth_image.size(3):
            return False
        
        return True
    
    def _analyze_body_shape(self, person_image: torch.Tensor, 
                           pose_keypoints: Optional[torch.Tensor],
                           body_shape: Optional[torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ì‹ ì²´ í˜•íƒœ ë¶„ì„"""
        body_analysis = {}
        
        # 1. ì´ë¯¸ì§€ ê¸°ë°˜ ì‹ ì²´ ë¶„ì„
        body_analysis['image_features'] = self._extract_body_features(person_image)
        
        # 2. í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ë¶„ì„
        if pose_keypoints is not None:
            body_analysis['pose_features'] = self._analyze_pose_keypoints(pose_keypoints)
        else:
            body_analysis['pose_features'] = torch.zeros(person_image.size(0), 128, device=self.device)
        
        # 3. ì‹ ì²´ í˜•íƒœ ì •ë³´ ë¶„ì„
        if body_shape is not None:
            body_analysis['shape_features'] = self._analyze_body_shape_info(body_shape)
        else:
            body_analysis['shape_features'] = torch.zeros(person_image.size(0), 64, device=self.device)
        
        # 4. ì¢…í•© ì‹ ì²´ ë¶„ì„
        body_analysis['combined_features'] = self._combine_body_features(body_analysis)
        
        return body_analysis
    
    def _extract_body_features(self, person_image: torch.Tensor) -> torch.Tensor:
        """ì‹ ì²´ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©)
        features = F.adaptive_avg_pool2d(person_image, (8, 8))
        features = features.flatten(1)
        
        # íŠ¹ì§• ì°¨ì› ì¡°ì •
        if features.size(1) != 128:
            features = F.linear(features, torch.randn(128, features.size(1), device=self.device))
        
        return features
    
    def _analyze_pose_keypoints(self, pose_keypoints: torch.Tensor) -> torch.Tensor:
        """í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ë¶„ì„"""
        # í‚¤í¬ì¸íŠ¸ íŠ¹ì§• ë¶„ì„
        batch_size, num_keypoints, coords = pose_keypoints.shape
        
        # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ê³„ì‚°
        pose_features = []
        for b in range(batch_size):
            keypoints = pose_keypoints[b]
            distances = []
            
            for i in range(num_keypoints):
                for j in range(i + 1, num_keypoints):
                    dist = torch.norm(keypoints[i] - keypoints[j])
                    distances.append(dist)
            
            # ê±°ë¦¬ íŠ¹ì§•ì„ 128ì°¨ì›ìœ¼ë¡œ ì¡°ì •
            if len(distances) > 0:
                distances = torch.stack(distances)
                if distances.numel() < 128:
                    # íŒ¨ë”©
                    padding = torch.zeros(128 - distances.numel(), device=self.device)
                    distances = torch.cat([distances, padding])
                else:
                    # ìë¥´ê¸°
                    distances = distances[:128]
            else:
                distances = torch.zeros(128, device=self.device)
            
            pose_features.append(distances)
        
        return torch.stack(pose_features)
    
    def _analyze_body_shape_info(self, body_shape: torch.Tensor) -> torch.Tensor:
        """ì‹ ì²´ í˜•íƒœ ì •ë³´ ë¶„ì„"""
        # ì‹ ì²´ í˜•íƒœ ì •ë³´ë¥¼ 64ì°¨ì›ìœ¼ë¡œ ì¡°ì •
        if body_shape.size(1) != 64:
            if body_shape.size(1) < 64:
                # íŒ¨ë”©
                padding = torch.zeros(body_shape.size(0), 64 - body_shape.size(1), device=self.device)
                body_shape = torch.cat([body_shape, padding], dim=1)
            else:
                # ìë¥´ê¸°
                body_shape = body_shape[:, :64]
        
        return body_shape
    
    def _combine_body_features(self, body_analysis: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ì‹ ì²´ íŠ¹ì§• ê²°í•©"""
        image_features = body_analysis['image_features']
        pose_features = body_analysis['pose_features']
        shape_features = body_analysis['shape_features']
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([image_features, pose_features, shape_features], dim=1)
        
        # ì°¨ì› ì¡°ì • (256ì°¨ì›ìœ¼ë¡œ)
        if combined_features.size(1) != 256:
            combined_features = F.linear(
                combined_features, 
                torch.randn(256, combined_features.size(1), device=self.device)
            )
        
        return combined_features
    
    def _preprocess_cloth(self, cloth_image: torch.Tensor, 
                          body_analysis: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ì˜ë¥˜ ì „ì²˜ë¦¬"""
        # ì‹ ì²´ í˜•íƒœì— ë§ì¶° ì˜ë¥˜ ì¡°ì •
        processed_cloth = cloth_image.clone()
        
        # ì‹ ì²´ íŠ¹ì§•ì— ë”°ë¥¸ ì˜ë¥˜ ìŠ¤ì¼€ì¼ë§
        body_features = body_analysis['combined_features']
        
        for b in range(cloth_image.size(0)):
            # ì‹ ì²´ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¥˜ í¬ê¸° ì¡°ì •
            scale_factor = torch.sigmoid(body_features[b, 0]).item() * 0.5 + 0.75  # 0.75 ~ 1.25
            
            # ì˜ë¥˜ í¬ê¸° ì¡°ì •
            processed_cloth[b] = F.interpolate(
                cloth_image[b:b+1], 
                scale_factor=scale_factor, 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
        
        return processed_cloth
    
    def _apply_fitting_algorithm(self, person_image: torch.Tensor, 
                                processed_cloth: torch.Tensor,
                                body_analysis: Dict[str, torch.Tensor]) -> torch.Tensor:
        """í”¼íŒ… ì•Œê³ ë¦¬ì¦˜ ì ìš©"""
        # ê°„ë‹¨í•œ ì•ŒíŒŒ ë¸”ë Œë”© ê¸°ë°˜ í”¼íŒ…
        fitted_result = torch.zeros_like(person_image)
        
        for b in range(person_image.size(0)):
            person = person_image[b]
            cloth = processed_cloth[b]
            
            # ì‹ ì²´ ë§ˆìŠ¤í¬ ìƒì„± (ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜)
            person_gray = person.mean(dim=0)
            body_mask = (person_gray > 0.1).float()
            
            # ì˜ë¥˜ë¥¼ ì‹ ì²´ ì˜ì—­ì—ë§Œ ì ìš©
            fitted_result[b] = person * (1 - body_mask.unsqueeze(0)) + cloth * body_mask.unsqueeze(0)
        
        return fitted_result
    
    def _apply_fitting_style(self, fitted_result: torch.Tensor) -> torch.Tensor:
        """í”¼íŒ… ìŠ¤íƒ€ì¼ ì ìš©"""
        style_weight = self.fitting_weights.get(self.config.fitting_style, 1.0)
        
        if style_weight != 1.0:
            # í”¼íŒ… ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ì¡°ì •
            if self.config.fitting_style == "tight":
                # íƒ€ì´íŠ¸í•œ í”¼íŒ…: ì˜ë¥˜ë¥¼ ë” ë°€ì°©
                fitted_result = fitted_result * style_weight
            elif self.config.fitting_style == "loose":
                # ë£¨ì¦ˆí•œ í”¼íŒ…: ì˜ë¥˜ë¥¼ ë” ëŠìŠ¨í•˜ê²Œ
                fitted_result = fitted_result * style_weight
        
        # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
        fitted_result = torch.clamp(fitted_result, 0.0, 1.0)
        
        return fitted_result
    
    def _assess_fitting_quality(self, fitted_result: torch.Tensor, 
                               person_image: torch.Tensor, 
                               cloth_image: torch.Tensor) -> float:
        """í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        if not self.config.enable_quality_assessment:
            return 0.8  # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
        
        try:
            # í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ ì ìš©
            with torch.no_grad():
                # ì´ë¯¸ì§€ë¥¼ 1Dë¡œ í‰íƒ„í™”
                result_flat = fitted_result.view(fitted_result.size(0), -1)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self.quality_assessor(result_flat)
                
                return float(quality_score.mean().item())
                
        except Exception as e:
            self.logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
    
    def get_fitting_info(self) -> Dict[str, Any]:
        """í”¼íŒ… ì •ë³´ ë°˜í™˜"""
        return {
            "input_size": self.config.input_size,
            "output_size": self.config.output_size,
            "device": str(self.device),
            "fitting_style": self.config.fitting_style,
            "enable_quality_assessment": self.config.enable_quality_assessment
        }

# ê°€ìƒ í”¼íŒ… ì½”ì–´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def create_virtual_fitting_core(config: FittingConfig = None) -> VirtualFittingCore:
    """Virtual Fitting ì½”ì–´ ìƒì„±"""
    return VirtualFittingCore(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ê°€ìƒ í”¼íŒ… ì½”ì–´ ìƒì„±
    core = create_virtual_fitting_core()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, channels, height, width = 2, 3, 256, 256
    test_person = torch.randn(batch_size, channels, height, width)
    test_cloth = torch.randn(batch_size, channels, height, width)
    
    # ê°€ìƒ í”¼íŒ… ìˆ˜í–‰
    result = core(test_person, test_cloth)
    print(f"í”¼íŒ… ê²°ê³¼ í˜•íƒœ: {result['fitted_result'].shape}")
    print(f"í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.3f}")
    print(f"í”¼íŒ… ì •ë³´: {core.get_fitting_info()}")
