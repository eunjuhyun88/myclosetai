"""
ğŸ”¥ Virtual Fitting Processor
============================

ê°€ìƒ í”¼íŒ…ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œì…ë‹ˆë‹¤.
ë…¼ë¬¸ ê¸°ë°˜ì˜ AI ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
import cv2
from PIL import Image
import logging

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • import
try:
    from backend.app.core.logging_config import get_logger
    logger = get_logger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class VirtualFittingProcessor:
    """
    ê°€ìƒ í”¼íŒ…ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ
    """

    def __init__(self, device: str = 'auto'):
        """
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda')
        """
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # í”„ë¡œì„¸ì„œ ì„¤ì •
        self.processor_config = {
            'default_input_size': (512, 512),
            'default_output_size': (1024, 1024),
            'enable_body_alignment': True,
            'enable_clothing_enhancement': True,
            'enable_lighting_adjustment': True,
            'enable_shadow_generation': True
        }
        
        logger.info(f"VirtualFittingProcessor initialized on device: {self.device}")

    def preprocess_for_virtual_fitting(self, person_image: Union[np.ndarray, Image.Image, torch.Tensor],
                                     clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
                                     target_size: Tuple[int, int] = None,
                                     normalize: bool = True,
                                     **kwargs) -> Dict[str, torch.Tensor]:
        """
        ê°€ìƒ í”¼íŒ…ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€
            target_size: ëª©í‘œ í¬ê¸°
            normalize: ì •ê·œí™” ì—¬ë¶€
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë“¤ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ëª©í‘œ í¬ê¸° ì„¤ì •
            if target_size is None:
                target_size = self.processor_config['default_input_size']
            
            # ì‚¬ëŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_person = self._preprocess_single_image(
                person_image, target_size, normalize, 'person'
            )
            
            # ì˜ë¥˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_clothing = self._preprocess_single_image(
                clothing_image, target_size, normalize, 'clothing'
            )
            
            # ì‹ ì²´ ì •ë ¬ (í•„ìš”ì‹œ)
            if self.processor_config['enable_body_alignment']:
                processed_person = self._align_body_pose(processed_person, **kwargs)
            
            # ì˜ë¥˜ í–¥ìƒ (í•„ìš”ì‹œ)
            if self.processor_config['enable_clothing_enhancement']:
                processed_clothing = self._enhance_clothing(processed_clothing, **kwargs)
            
            result = {
                'person_image': processed_person,
                'clothing_image': processed_clothing,
                'target_size': target_size
            }
            
            logger.info(f"âœ… Virtual fitting preprocessing completed: {processed_person.shape}, {processed_clothing.shape}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Virtual fitting preprocessing failed: {e}")
            raise

    def _preprocess_single_image(self, image: Union[np.ndarray, Image.Image, torch.Tensor],
                                target_size: Tuple[int, int],
                                normalize: bool,
                                image_type: str) -> torch.Tensor:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            # ì´ë¯¸ì§€ íƒ€ì… í†µì¼
            if isinstance(image, Image.Image):
                image = np.array(image)
            
            if isinstance(image, np.ndarray):
                # RGBë¡œ ë³€í™˜
                if len(image.shape) == 2:
                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
                elif image.shape[2] == 4:
                    image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
                
                # í…ì„œë¡œ ë³€í™˜
                image = torch.from_numpy(image).permute(2, 0, 1).float()
            elif isinstance(image, torch.Tensor):
                if len(image.shape) == 2:
                    image = image.unsqueeze(0).repeat(3, 1, 1)
                elif len(image.shape) == 3 and image.shape[0] == 1:
                    image = image.repeat(3, 1, 1)
            
            # í¬ê¸° ì¡°ì •
            if image.shape[-2:] != target_size:
                image = F.interpolate(image.unsqueeze(0), size=target_size, 
                                    mode='bilinear', align_corners=False).squeeze(0)
            
            # ì •ê·œí™”
            if normalize:
                if image.max() > 1.0:
                    image = image / 255.0
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if len(image.shape) == 3:
                image = image.unsqueeze(0)
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            image = image.to(self.device)
            
            return image
            
        except Exception as e:
            logger.error(f"âŒ Single image preprocessing failed for {image_type}: {e}")
            raise

    def _align_body_pose(self, person_image: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        ì‹ ì²´ ìì„¸ë¥¼ ì •ë ¬í•©ë‹ˆë‹¤.
        """
        try:
            # ê°„ë‹¨í•œ ì‹ ì²´ ì •ë ¬ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
            aligned_image = person_image
            
            # ì´ë¯¸ì§€ ì¤‘ì‹¬ì  ê³„ì‚°
            h, w = person_image.shape[-2:]
            center_h, center_w = h // 2, w // 2
            
            # ì‹ ì²´ ì¤‘ì‹¬ì„ ì´ë¯¸ì§€ ì¤‘ì‹¬ì— ë§ì¶¤
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í¬ì¦ˆ ì¶”ì • ê²°ê³¼ë¥¼ ì‚¬ìš©
            logger.debug(f"Body alignment applied: center at ({center_h}, {center_w})")
            
            return aligned_image
            
        except Exception as e:
            logger.warning(f"âš ï¸ Body alignment failed: {e}")
            return person_image

    def _enhance_clothing(self, clothing_image: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        """
        try:
            enhanced_image = clothing_image
            
            # ì„ ëª…ë„ í–¥ìƒ
            enhanced_image = self._sharpen_image(enhanced_image, strength=0.3)
            
            # ìƒ‰ìƒ ë³´ì •
            enhanced_image = self._correct_colors(enhanced_image, 
                                               brightness=0.0, 
                                               contrast=1.1, 
                                               saturation=1.2)
            
            logger.debug("Clothing enhancement applied")
            return enhanced_image
            
        except Exception as e:
            logger.warning(f"âš ï¸ Clothing enhancement failed: {e}")
            return clothing_image

    def _sharpen_image(self, image: torch.Tensor, strength: float = 0.5) -> torch.Tensor:
        """
        ì´ë¯¸ì§€ ì„ ëª…ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
        """
        # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ ì ìš©
        kernel = torch.tensor([
            [0, -1, 0],
            [-1, 5, -1],
            [0, -1, 0]
        ], dtype=torch.float32, device=self.device)
        
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        
        sharpened = torch.zeros_like(image)
        for c in range(image.shape[1]):
            sharpened[:, c:c+1] = F.conv2d(
                image[:, c:c+1], 
                kernel,
                padding=1
            )
        
        # ì›ë³¸ê³¼ ë¸”ë Œë”©
        result = image + strength * (sharpened - image)
        return torch.clamp(result, 0, 1)

    def _correct_colors(self, image: torch.Tensor, 
                       brightness: float = 0.0,
                       contrast: float = 1.0,
                       saturation: float = 1.0) -> torch.Tensor:
        """
        ì´ë¯¸ì§€ ìƒ‰ìƒì„ ë³´ì •í•©ë‹ˆë‹¤.
        """
        corrected = image
        
        # ë°ê¸° ì¡°ì •
        if brightness != 0.0:
            corrected = corrected + brightness
            corrected = torch.clamp(corrected, 0, 1)
        
        # ëŒ€ë¹„ ì¡°ì •
        if contrast != 1.0:
            mean_val = corrected.mean()
            corrected = (corrected - mean_val) * contrast + mean_val
            corrected = torch.clamp(corrected, 0, 1)
        
        # ì±„ë„ ì¡°ì •
        if saturation != 1.0 and corrected.shape[1] == 3:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ê³„ì‚°
            gray = 0.299 * corrected[:, 0] + 0.587 * corrected[:, 1] + 0.114 * corrected[:, 2]
            gray = gray.unsqueeze(1)
            
            # ì±„ë„ ì¡°ì •
            corrected = gray + saturation * (corrected - gray)
            corrected = torch.clamp(corrected, 0, 1)
        
        return corrected

    def postprocess_virtual_fitting(self, fitted_image: torch.Tensor,
                                  target_size: Tuple[int, int] = None,
                                  denormalize: bool = True,
                                  **kwargs) -> torch.Tensor:
        """
        ê°€ìƒ í”¼íŒ… ê²°ê³¼ë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            processed_output = fitted_image
            
            # í¬ê¸° ì¡°ì •
            if target_size is not None and fitted_image.shape[-2:] != target_size:
                processed_output = F.interpolate(processed_output, size=target_size, 
                                              mode='bilinear', align_corners=False)
            
            # ì¡°ëª… ì¡°ì •
            if self.processor_config['enable_lighting_adjustment']:
                processed_output = self._adjust_lighting(processed_output, **kwargs)
            
            # ê·¸ë¦¼ì ìƒì„±
            if self.processor_config['enable_shadow_generation']:
                processed_output = self._generate_shadows(processed_output, **kwargs)
            
            # ì—­ì •ê·œí™”
            if denormalize:
                processed_output = torch.clamp(processed_output, 0, 1)
                if processed_output.max() <= 1.0:
                    processed_output = processed_output * 255.0
            
            logger.info(f"âœ… Virtual fitting postprocessing completed: {processed_output.shape}")
            return processed_output
            
        except Exception as e:
            logger.error(f"âŒ Virtual fitting postprocessing failed: {e}")
            return fitted_image

    def _adjust_lighting(self, image: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        ì¡°ëª…ì„ ì¡°ì •í•©ë‹ˆë‹¤.
        """
        try:
            # ê°„ë‹¨í•œ ì¡°ëª… ì¡°ì •
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì¡°ëª… ëª¨ë¸ ì‚¬ìš©
            adjusted_image = image
            
            # ì „ì²´ ë°ê¸° ì¡°ì •
            brightness_factor = kwargs.get('brightness_factor', 1.0)
            if brightness_factor != 1.0:
                adjusted_image = adjusted_image * brightness_factor
                adjusted_image = torch.clamp(adjusted_image, 0, 1)
            
            logger.debug(f"Lighting adjustment applied: brightness_factor={brightness_factor}")
            return adjusted_image
            
        except Exception as e:
            logger.warning(f"âš ï¸ Lighting adjustment failed: {e}")
            return image

    def _generate_shadows(self, image: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        ê·¸ë¦¼ìë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # ê°„ë‹¨í•œ ê·¸ë¦¼ì ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
            shadowed_image = image
            
            # ê·¸ë¦¼ì ê°•ë„
            shadow_strength = kwargs.get('shadow_strength', 0.2)
            
            # ì´ë¯¸ì§€ í•˜ë‹¨ì— ê·¸ë¦¼ì íš¨ê³¼ ì¶”ê°€
            h, w = image.shape[-2:]
            shadow_height = int(h * 0.3)  # í•˜ë‹¨ 30%ì— ê·¸ë¦¼ì
            
            # ê·¸ë¦¼ì ë§ˆìŠ¤í¬ ìƒì„±
            shadow_mask = torch.ones_like(image)
            shadow_mask[:, :, h-shadow_height:, :] = 1 - shadow_strength
            
            # ê·¸ë¦¼ì ì ìš©
            shadowed_image = image * shadow_mask
            
            logger.debug(f"Shadow generation applied: strength={shadow_strength}")
            return shadowed_image
            
        except Exception as e:
            logger.warning(f"âš ï¸ Shadow generation failed: {e}")
            return image

    def generate_fitting_batch(self, person_images: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                             clothing_images: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                             target_size: Tuple[int, int] = None,
                             **kwargs) -> List[Dict[str, torch.Tensor]]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•´ ê°€ìƒ í”¼íŒ… ì „ì²˜ë¦¬ë¥¼ ì¼ê´„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        try:
            if len(person_images) != len(clothing_images):
                raise ValueError("Person images and clothing images must have the same length")
            
            processed_batch = []
            
            for i, (person_img, clothing_img) in enumerate(zip(person_images, clothing_images)):
                try:
                    processed = self.preprocess_for_virtual_fitting(
                        person_img, clothing_img, target_size, **kwargs
                    )
                    processed['batch_index'] = i
                    processed_batch.append(processed)
                except Exception as e:
                    logger.error(f"âŒ Failed to process batch item {i}: {e}")
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ í…ì„œ ìƒì„±
                    default_tensor = torch.zeros(1, 3, 512, 512, device=self.device)
                    processed_batch.append({
                        'person_image': default_tensor,
                        'clothing_image': default_tensor,
                        'batch_index': i,
                        'error': str(e)
                    })
            
            logger.info(f"âœ… Batch processing completed: {len(processed_batch)} items")
            return processed_batch
            
        except Exception as e:
            logger.error(f"âŒ Batch processing failed: {e}")
            return []

    def get_processor_info(self) -> Dict[str, Any]:
        """
        í”„ë¡œì„¸ì„œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            'processor_name': 'VirtualFittingProcessor',
            'device': str(self.device),
            'config': self.processor_config,
            'supported_features': [
                'body_alignment', 'clothing_enhancement', 
                'lighting_adjustment', 'shadow_generation'
            ]
        }

    def update_processor_config(self, **kwargs):
        """
        í”„ë¡œì„¸ì„œ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        try:
            for key, value in kwargs.items():
                if key in self.processor_config:
                    self.processor_config[key] = value
                    logger.info(f"âœ… Processor config updated: {key} = {value}")
                else:
                    logger.warning(f"âš ï¸ Unknown config key: {key}")
        except Exception as e:
            logger.error(f"âŒ Processor config update failed: {e}")
