"""
ğŸ”¥ Virtual Fitting Service
==========================

ê°€ìƒ í”¼íŒ… ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
ë…¼ë¬¸ ê¸°ë°˜ì˜ AI ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
from PIL import Image
import logging
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • import
try:
    from backend.app.core.logging_config import get_logger
    logger = get_logger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class VirtualFittingService:
    """
    ê°€ìƒ í”¼íŒ… ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """

    def __init__(self, model_loader=None, processor=None, inference_engine=None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
            processor: ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
            inference_engine: ì¶”ë¡  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        self.model_loader = model_loader
        self.processor = processor
        self.inference_engine = inference_engine
        
        # ì„œë¹„ìŠ¤ ì„¤ì •
        self.service_config = {
            'default_model': 'virtual_fitting',
            'batch_size': 8,
            'enable_caching': True,
            'fitting_quality_threshold': 0.7,
            'enable_real_time_preview': True,
            'max_fitting_attempts': 3
        }
        
        # ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self._fitting_cache = {}
        self._max_cache_size = 300
        
        logger.info("âœ… VirtualFittingService initialized")

    def perform_virtual_fitting(self, person_image: Union[np.ndarray, Image.Image, torch.Tensor],
                              clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
                              model_type: str = None,
                              **kwargs) -> Dict[str, Any]:
        """
        ê°€ìƒ í”¼íŒ…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ê°€ìƒ í”¼íŒ… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            start_time = time.time()
            
            # ëª¨ë¸ íƒ€ì… ì„¤ì •
            if model_type is None:
                model_type = self.service_config['default_model']
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(person_image, clothing_image, model_type)
            if self.service_config['enable_caching'] and cache_key in self._fitting_cache:
                logger.info("âœ… Virtual fitting result found in cache")
                return self._fitting_cache[cache_key]
            
            # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            if self.processor:
                processed_data = self.processor.preprocess_for_virtual_fitting(
                    person_image, clothing_image, **kwargs
                )
            else:
                processed_data = {
                    'person_image': person_image,
                    'clothing_image': clothing_image
                }
            
            # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            if self.inference_engine:
                fitting_result = self.inference_engine.perform_virtual_fitting(
                    processed_data, model_type, **kwargs
                )
            else:
                # ê¸°ë³¸ ê°€ìƒ í”¼íŒ… (ê°„ë‹¨í•œ ì²˜ë¦¬)
                fitting_result = self._basic_virtual_fitting(processed_data, model_type)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            if self.processor:
                final_output = self.processor.postprocess_virtual_fitting(
                    fitting_result['fitted_image'], **kwargs
                )
            else:
                final_output = fitting_result['fitted_image']
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'fitted_image': final_output,
                'person_image': processed_data['person_image'],
                'clothing_image': processed_data['clothing_image'],
                'fitting_quality': fitting_result.get('fitting_quality', 0.0),
                'fitting_confidence': fitting_result.get('confidence', 0.0),
                'model_type': model_type,
                'processing_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat()
            }
            
            # ìºì‹œì— ì €ì¥
            if self.service_config['enable_caching']:
                self._add_to_cache(cache_key, result)
            
            logger.info(f"âœ… Virtual fitting completed: quality={result['fitting_quality']:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Virtual fitting failed: {e}")
            return {
                'error': str(e),
                'fitting_quality': 0.0,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'timestamp': datetime.now().isoformat()
            }

    def perform_batch_fitting(self, person_images: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                            clothing_images: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                            model_type: str = None,
                            **kwargs) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•´ ê°€ìƒ í”¼íŒ…ì„ ì¼ê´„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            person_images: ì‚¬ëŒ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            clothing_images: ì˜ë¥˜ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ê°€ìƒ í”¼íŒ… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if len(person_images) != len(clothing_images):
                raise ValueError("Person images and clothing images must have the same length")
            
            start_time = time.time()
            batch_size = self.service_config['batch_size']
            results = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(person_images), batch_size):
                batch_person = person_images[i:i + batch_size]
                batch_clothing = clothing_images[i:i + batch_size]
                batch_results = []
                
                for j, (person_img, clothing_img) in enumerate(zip(batch_person, batch_clothing)):
                    try:
                        result = self.perform_virtual_fitting(
                            person_img, clothing_img, model_type, **kwargs
                        )
                        result['batch_index'] = i + j
                        batch_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ Failed to perform fitting for batch {i + j}: {e}")
                        batch_results.append({
                            'batch_index': i + j,
                            'error': str(e),
                            'fitting_quality': 0.0,
                            'timestamp': datetime.now().isoformat()
                        })
                
                results.extend(batch_results)
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ìƒí™© ë¡œê¹…
                logger.info(f"âœ… Batch {i//batch_size + 1} completed: {len(batch_results)} fittings")
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_time = time.time() - start_time
            self._log_batch_statistics(results, total_time)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Batch virtual fitting failed: {e}")
            return []

    def _basic_virtual_fitting(self, processed_data: Dict[str, torch.Tensor], 
                              model_type: str) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ê°€ìƒ í”¼íŒ… (ì¶”ë¡  ì—”ì§„ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        """
        try:
            person_image = processed_data['person_image']
            clothing_image = processed_data['clothing_image']
            
            # ê°„ë‹¨í•œ ì´ë¯¸ì§€ í•©ì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
            if len(person_image.shape) == 4:
                person_image = person_image.squeeze(0)
            if len(clothing_image.shape) == 4:
                clothing_image = clothing_image.squeeze(0)
            
            # ì˜ë¥˜ë¥¼ ì‚¬ëŒ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹ ì²´ ë¶€ìœ„ë³„ ë§ˆìŠ¤í‚¹ê³¼ ë¸”ë Œë”© ì‚¬ìš©
            fitted_image = self._overlay_clothing_on_person(person_image, clothing_image)
            
            # í”¼íŒ… í’ˆì§ˆ ê³„ì‚°
            fitting_quality = self._calculate_fitting_quality(fitted_image)
            
            return {
                'fitted_image': fitted_image,
                'fitting_quality': fitting_quality,
                'confidence': 0.7,
                'model_type': 'basic'
            }
            
        except Exception as e:
            logger.error(f"âŒ Basic virtual fitting failed: {e}")
            return {
                'fitted_image': processed_data['person_image'],
                'fitting_quality': 0.0,
                'confidence': 0.0,
                'model_type': 'basic',
                'error': str(e)
            }

    def _overlay_clothing_on_person(self, person_image: torch.Tensor, 
                                   clothing_image: torch.Tensor) -> torch.Tensor:
        """
        ì˜ë¥˜ë¥¼ ì‚¬ëŒ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤.
        """
        try:
            # ê°„ë‹¨í•œ ì•ŒíŒŒ ë¸”ë Œë”©
            alpha = 0.8  # ì˜ë¥˜ íˆ¬ëª…ë„
            
            # ì˜ë¥˜ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            if clothing_image.shape[-2:] != person_image.shape[-2:]:
                clothing_image = F.interpolate(
                    clothing_image.unsqueeze(0), 
                    size=person_image.shape[-2:], 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
            
            # ì˜¤ë²„ë ˆì´
            fitted_image = person_image * (1 - alpha) + clothing_image * alpha
            
            return torch.clamp(fitted_image, 0, 1)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Clothing overlay failed: {e}")
            return person_image

    def _calculate_fitting_quality(self, fitted_image: torch.Tensor) -> float:
        """
        í”¼íŒ… í’ˆì§ˆì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        try:
            # ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
            if len(fitted_image.shape) == 4:
                fitted_image = fitted_image.squeeze(0)
            
            # ë°ê¸°
            brightness = fitted_image.mean().item()
            
            # ëŒ€ë¹„
            contrast = fitted_image.std().item()
            
            # ì„ ëª…ë„ (ê°„ë‹¨í•œ ì—ì§€ ê²€ì¶œ)
            if fitted_image.shape[0] == 3:  # RGB
                gray = 0.299 * fitted_image[0] + 0.587 * fitted_image[1] + 0.114 * fitted_image[2]
            else:
                gray = fitted_image[0]
            
            # Sobel í•„í„°ë¡œ ì—ì§€ ê²€ì¶œ
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                 dtype=torch.float32, device=fitted_image.device)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                 dtype=torch.float32, device=fitted_image.device)
            
            edge_x = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_x.unsqueeze(0).unsqueeze(0), padding=1)
            edge_y = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_y.unsqueeze(0).unsqueeze(0), padding=1)
            
            edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
            sharpness = edge_magnitude.mean().item()
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)
            quality_score = min(1.0, (brightness * 0.3 + contrast * 0.3 + sharpness * 0.4))
            
            return quality_score
            
        except Exception as e:
            logger.warning(f"âš ï¸ Fitting quality calculation failed: {e}")
            return 0.5

    def _generate_cache_key(self, person_image: Union[np.ndarray, Image.Image, torch.Tensor],
                           clothing_image: Union[np.ndarray, Image.Image, torch.Tensor],
                           model_type: str) -> str:
        """
        ìºì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•)
            person_hash = self._generate_image_hash(person_image)
            clothing_hash = self._generate_image_hash(clothing_image)
            
            return f"{model_type}_{person_hash}_{clothing_hash}"
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache key generation failed: {e}")
            return f"{model_type}_{hash(str(person_image))}_{hash(str(clothing_image))}"

    def _generate_image_hash(self, image: Union[np.ndarray, Image.Image, torch.Tensor]) -> str:
        """
        ì´ë¯¸ì§€ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            if isinstance(image, np.ndarray):
                # numpy arrayì˜ í‰ê· ê°’ê³¼ í‘œì¤€í¸ì°¨ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.mean():.6f}_{image.std():.6f}_{image.shape}"
            elif isinstance(image, Image.Image):
                # PIL Imageì˜ í¬ê¸°ì™€ ëª¨ë“œë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.size}_{image.mode}"
            elif isinstance(image, torch.Tensor):
                # torch tensorì˜ í†µê³„ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.mean().item():.6f}_{image.std().item():.6f}_{image.shape}"
            else:
                hash_value = str(hash(str(image)))
            
            return hash_value
            
        except Exception as e:
            logger.warning(f"âš ï¸ Image hash generation failed: {e}")
            return str(hash(str(image)))

    def _add_to_cache(self, key: str, result: Dict[str, Any]):
        """
        ê²°ê³¼ë¥¼ ìºì‹œì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        try:
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self._fitting_cache) >= self._max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._fitting_cache))
                del self._fitting_cache[oldest_key]
                logger.debug("ğŸ—‘ï¸ Oldest cache entry removed")
            
            self._fitting_cache[key] = result
            logger.debug(f"ğŸ’¾ Result cached: {key}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache addition failed: {e}")

    def _log_batch_statistics(self, results: List[Dict[str, Any]], total_time: float):
        """
        ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
        """
        try:
            if not results:
                return
            
            # ì„±ê³µí•œ í”¼íŒ… ìˆ˜
            successful_results = [r for r in results if 'error' not in r]
            error_count = len(results) - len(successful_results)
            
            # í’ˆì§ˆ ì ìˆ˜ í†µê³„
            quality_scores = [r.get('fitting_quality', 0.0) for r in successful_results if 'fitting_quality' in r]
            avg_quality = np.mean(quality_scores) if quality_scores else 0.0
            max_quality = max(quality_scores) if quality_scores else 0.0
            min_quality = min(quality_scores) if quality_scores else 0.0
            
            # í†µê³„ ë¡œê¹…
            logger.info(f"ğŸ“Š Batch Fitting Statistics:")
            logger.info(f"   Total fittings: {len(results)}")
            logger.info(f"   Successful: {len(successful_results)}")
            logger.info(f"   Errors: {error_count}")
            logger.info(f"   Average quality: {avg_quality:.3f}")
            logger.info(f"   Quality range: {min_quality:.3f} - {max_quality:.3f}")
            logger.info(f"   Total processing time: {total_time:.2f}s")
            logger.info(f"   Average time per fitting: {total_time/len(results):.3f}s")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Statistics logging failed: {e}")

    def get_service_info(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            'service_name': 'VirtualFittingService',
            'version': '1.0.0',
            'supported_models': self.inference_engine.supported_models if self.inference_engine else [],
            'cache_enabled': self.service_config['enable_caching'],
            'cache_size': len(self._fitting_cache),
            'max_cache_size': self._max_cache_size,
            'default_model': self.service_config['default_model'],
            'fitting_quality_threshold': self.service_config['fitting_quality_threshold'],
            'max_fitting_attempts': self.service_config['max_fitting_attempts']
        }

    def clear_cache(self):
        """
        ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            cache_size = len(self._fitting_cache)
            self._fitting_cache.clear()
            logger.info(f"ğŸ—‘ï¸ Cache cleared: {cache_size} entries removed")
        except Exception as e:
            logger.error(f"âŒ Cache clearing failed: {e}")

    def update_service_config(self, **kwargs):
        """
        ì„œë¹„ìŠ¤ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        try:
            for key, value in kwargs.items():
                if key in self.service_config:
                    self.service_config[key] = value
                    logger.info(f"âœ… Service config updated: {key} = {value}")
                else:
                    logger.warning(f"âš ï¸ Unknown config key: {key}")
        except Exception as e:
            logger.error(f"âŒ Service config update failed: {e}")

    def validate_fitting_result(self, fitted_image: torch.Tensor, 
                              threshold: float = None) -> Dict[str, Any]:
        """
        í”¼íŒ… ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            fitted_image: í”¼íŒ…ëœ ì´ë¯¸ì§€
            threshold: í’ˆì§ˆ ì„ê³„ê°’
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        try:
            if threshold is None:
                threshold = self.service_config['fitting_quality_threshold']
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_fitting_quality(fitted_image)
            
            # ê²€ì¦ ê²°ê³¼
            validation_result = {
                'quality_score': quality_score,
                'meets_threshold': quality_score >= threshold,
                'threshold': threshold,
                'validation_passed': quality_score >= threshold,
                'quality_grade': self._get_quality_grade(quality_score)
            }
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ Fitting result validation failed: {e}")
            return {
                'quality_score': 0.0,
                'validation_passed': False,
                'error': str(e)
            }

    def _get_quality_grade(self, quality_score: float) -> str:
        """
        í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if quality_score >= 0.9:
            return 'Excellent'
        elif quality_score >= 0.7:
            return 'Good'
        elif quality_score >= 0.5:
            return 'Fair'
        else:
            return 'Poor'
