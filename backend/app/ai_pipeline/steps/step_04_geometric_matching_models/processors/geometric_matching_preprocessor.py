#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Geometric Matching Preprocessor
================================================

ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ ì „ì²˜ë¦¬ê¸°
âœ… ì´ë¯¸ì§€ ì •ê·œí™” ë° ì „ì²˜ë¦¬
âœ… íŠ¹ì§• ì¶”ì¶œ ë° ê°•í™”
âœ… í’ˆì§ˆ í–¥ìƒ ë° ë…¸ì´ì¦ˆ ì œê±°
âœ… M3 Max ìµœì í™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import cv2
from PIL import Image, ImageEnhance, ImageFilter

logger = logging.getLogger(__name__)

@dataclass
class PreprocessingConfig:
    """ì „ì²˜ë¦¬ ì„¤ì •"""
    input_size: Tuple[int, int] = (256, 256)
    output_size: Tuple[int, int] = (256, 256)
    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)
    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)
    enable_augmentation: bool = True
    enable_noise_reduction: bool = True
    enable_contrast_enhancement: bool = True
    enable_edge_detection: bool = True
    use_mps: bool = True

class GeometricMatchingPreprocessor(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: PreprocessingConfig = None):
        super().__init__()
        self.config = config or PreprocessingConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Geometric Matching ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ë…¸ì´ì¦ˆ ì œê±° ë„¤íŠ¸ì›Œí¬
        if self.config.enable_noise_reduction:
            self.noise_reduction_net = self._create_noise_reduction_net()
        
        # íŠ¹ì§• ê°•í™” ë„¤íŠ¸ì›Œí¬
        self.feature_enhancement_net = self._create_feature_enhancement_net()
        
        # í’ˆì§ˆ í–¥ìƒ ë„¤íŠ¸ì›Œí¬
        self.quality_enhancement_net = self._create_quality_enhancement_net()
        
        # ì •ê·œí™” ë ˆì´ì–´
        self.normalize = nn.Parameter(
            torch.tensor([self.config.normalize_mean, self.config.normalize_std]), 
            requires_grad=False
        )
        
        self.logger.info("âœ… Geometric Matching ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_noise_reduction_net(self) -> nn.Module:
        """ë…¸ì´ì¦ˆ ì œê±° ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, kernel_size=3, padding=1),
            nn.Sigmoid()
        ).to(self.device)
    
    def _create_feature_enhancement_net(self) -> nn.Module:
        """íŠ¹ì§• ê°•í™” ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, kernel_size=3, padding=1),
            nn.Tanh()
        ).to(self.device)
    
    def _create_quality_enhancement_net(self) -> nn.Module:
        """í’ˆì§ˆ í–¥ìƒ ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(6, 128, kernel_size=3, padding=1),  # 6 channels: 3 for image1 + 3 for image2
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 6, kernel_size=3, padding=1),  # Output: enhanced image1 + enhanced image2
            nn.Sigmoid()
        ).to(self.device)
    
    def forward(self, image1: torch.Tensor, image2: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìˆ˜í–‰
        
        Args:
            image1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (B, C, H, W)
            image2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (B, C, H, W)
        
        Returns:
            ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë“¤
        """
        # ì…ë ¥ ê²€ì¦
        if not self._validate_inputs(image1, image2):
            raise ValueError("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        image1 = image1.to(self.device)
        image2 = image2.to(self.device)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì „ì²˜ë¦¬
        processed_image1 = self._basic_preprocessing(image1)
        processed_image2 = self._basic_preprocessing(image2)
        
        # 2ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°
        if self.config.enable_noise_reduction:
            processed_image1 = self._reduce_noise(processed_image1)
            processed_image2 = self._reduce_noise(processed_image2)
        
        # 3ë‹¨ê³„: íŠ¹ì§• ê°•í™”
        enhanced_image1 = self._enhance_features(processed_image1)
        enhanced_image2 = self._enhance_features(processed_image2)
        
        # 4ë‹¨ê³„: í’ˆì§ˆ í–¥ìƒ
        quality_enhanced_images = self._enhance_quality(enhanced_image1, enhanced_image2)
        
        # 5ë‹¨ê³„: ìµœì¢… ì •ê·œí™”
        final_image1 = self._final_normalization(quality_enhanced_images[:, :3, :, :])
        final_image2 = self._final_normalization(quality_enhanced_images[:, 3:, :, :])
        
        # 6ë‹¨ê³„: í¬ê¸° ì¡°ì •
        if self.config.input_size != self.config.output_size:
            final_image1 = F.interpolate(final_image1, size=self.config.output_size, 
                                       mode='bilinear', align_corners=False)
            final_image2 = F.interpolate(final_image2, size=self.config.output_size, 
                                       mode='bilinear', align_corners=False)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "processed_image1": final_image1,
            "processed_image2": final_image2,
            "intermediate_features": {
                "enhanced_image1": enhanced_image1,
                "enhanced_image2": enhanced_image2,
                "quality_enhanced": quality_enhanced_images
            }
        }
        
        return result
    
    def _validate_inputs(self, image1: torch.Tensor, image2: torch.Tensor) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        if image1.dim() != 4 or image2.dim() != 4:
            return False
        
        if image1.size(0) != image2.size(0):
            return False
        
        if image1.size(2) != image2.size(2) or image1.size(3) != image2.size(3):
            return False
        
        if image1.size(1) != 3 or image2.size(1) != 3:
            return False
        
        return True
    
    def _basic_preprocessing(self, image: torch.Tensor) -> torch.Tensor:
        """ê¸°ë³¸ ì „ì²˜ë¦¬"""
        # 1. í”½ì…€ ê°’ ì •ê·œí™” (0-1 ë²”ìœ„)
        if image.max() > 1.0:
            image = image / 255.0
        
        # 2. ëŒ€ë¹„ í–¥ìƒ
        if self.config.enable_contrast_enhancement:
            image = self._enhance_contrast(image)
        
        # 3. ì—£ì§€ ê²€ì¶œ
        if self.config.enable_edge_detection:
            image = self._detect_edges(image)
        
        return image
    
    def _enhance_contrast(self, image: torch.Tensor) -> torch.Tensor:
        """ëŒ€ë¹„ í–¥ìƒ"""
        # íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
        batch_size, channels, height, width = image.shape
        enhanced_image = torch.zeros_like(image)
        
        for b in range(batch_size):
            for c in range(channels):
                # ê° ì±„ë„ë³„ë¡œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                channel = image[b, c, :, :]
                
                # íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
                hist = torch.histc(channel, bins=256, min=0, max=1)
                cdf = torch.cumsum(hist, dim=0)
                cdf_normalized = cdf / cdf.max()
                
                # LUT ì ìš©
                enhanced_channel = cdf_normalized[(channel * 255).long()]
                enhanced_image[b, c, :, :] = enhanced_channel
        
        return enhanced_image
    
    def _detect_edges(self, image: torch.Tensor) -> torch.Tensor:
        """ì—£ì§€ ê²€ì¶œ"""
        # Sobel ì—£ì§€ ê²€ì¶œ
        batch_size, channels, height, width = image.shape
        edge_image = torch.zeros_like(image)
        
        # Sobel ì»¤ë„
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                              dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                              dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
        
        for b in range(batch_size):
            for c in range(channels):
                channel = image[b:b+1, c:c+1, :, :]
                
                # X ë°©í–¥ ì—£ì§€
                edge_x = F.conv2d(channel, sobel_x, padding=1)
                
                # Y ë°©í–¥ ì—£ì§€
                edge_y = F.conv2d(channel, sobel_y, padding=1)
                
                # ì—£ì§€ ê°•ë„
                edge_magnitude = torch.sqrt(edge_x ** 2 + edge_y ** 2)
                
                # ì—£ì§€ ê°•í™”
                enhanced_channel = image[b, c, :, :] + 0.1 * edge_magnitude.squeeze()
                edge_image[b, c, :, :] = torch.clamp(enhanced_channel, 0, 1)
        
        return edge_image
    
    def _reduce_noise(self, image: torch.Tensor) -> torch.Tensor:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        # 1. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        blurred = F.avg_pool2d(image, kernel_size=3, stride=1, padding=1)
        
        # 2. ë…¸ì´ì¦ˆ ì œê±° ë„¤íŠ¸ì›Œí¬ ì ìš©
        noise_reduced = self.noise_reduction_net(blurred)
        
        # 3. ì›ë³¸ê³¼ ë…¸ì´ì¦ˆ ì œê±°ëœ ì´ë¯¸ì§€ ê²°í•©
        denoised = 0.7 * image + 0.3 * noise_reduced
        
        return torch.clamp(denoised, 0, 1)
    
    def _enhance_features(self, image: torch.Tensor) -> torch.Tensor:
        """íŠ¹ì§• ê°•í™”"""
        # íŠ¹ì§• ê°•í™” ë„¤íŠ¸ì›Œí¬ ì ìš©
        enhanced = self.feature_enhancement_net(image)
        
        # ì›ë³¸ê³¼ ê°•í™”ëœ íŠ¹ì§• ê²°í•©
        enhanced_features = 0.8 * image + 0.2 * enhanced
        
        return torch.clamp(enhanced_features, 0, 1)
    
    def _enhance_quality(self, image1: torch.Tensor, image2: torch.Tensor) -> torch.Tensor:
        """í’ˆì§ˆ í–¥ìƒ"""
        # ë‘ ì´ë¯¸ì§€ ê²°í•©
        combined_input = torch.cat([image1, image2], dim=1)
        
        # í’ˆì§ˆ í–¥ìƒ ë„¤íŠ¸ì›Œí¬ ì ìš©
        quality_enhanced = self.quality_enhancement_net(combined_input)
        
        return quality_enhanced
    
    def _final_normalization(self, image: torch.Tensor) -> torch.Tensor:
        """ìµœì¢… ì •ê·œí™”"""
        # ImageNet ì •ê·œí™” ì ìš©
        mean = self.normalize[0].view(1, 3, 1, 1)
        std = self.normalize[1].view(1, 3, 1, 1)
        
        normalized = (image - mean) / std
        
        return normalized
    
    def preprocess_single_image(self, image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if image.dim() == 3:
            image = image.unsqueeze(0)  # Add batch dimension
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰
        dummy_image = torch.zeros_like(image)
        result = self.forward(image, dummy_image)
        
        return result["processed_image1"]
    
    def preprocess_batch(self, images: List[torch.Tensor]) -> List[torch.Tensor]:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if not images:
            return []
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¡œ ë°°ì¹˜ í¬ê¸° í™•ì¸
        batch_size = len(images)
        first_image = images[0]
        
        # ë°°ì¹˜ í…ì„œ ìƒì„±
        batch_tensor = torch.stack(images)
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰
        dummy_batch = torch.zeros_like(batch_tensor)
        result = self.forward(batch_tensor, dummy_batch)
        
        # ê²°ê³¼ë¥¼ ê°œë³„ ì´ë¯¸ì§€ë¡œ ë¶„ë¦¬
        processed_images = []
        for i in range(batch_size):
            processed_images.append(result["processed_image1"][i])
        
        return processed_images
    
    def get_preprocessing_info(self) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ì •ë³´ ë°˜í™˜"""
        return {
            "input_size": self.config.input_size,
            "output_size": self.config.output_size,
            "normalize_mean": self.config.normalize_mean,
            "normalize_std": self.config.normalize_std,
            "enable_augmentation": self.config.enable_augmentation,
            "enable_noise_reduction": self.config.enable_noise_reduction,
            "enable_contrast_enhancement": self.config.enable_contrast_enhancement,
            "enable_edge_detection": self.config.enable_edge_detection,
            "device": str(self.device),
            "total_parameters": sum(p.numel() for p in self.parameters()),
            "trainable_parameters": sum(p.numel() for p in self.parameters() if p.requires_grad)
        }

# ì „ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def create_geometric_matching_preprocessor(config: PreprocessingConfig = None) -> GeometricMatchingPreprocessor:
    """Geometric Matching ì „ì²˜ë¦¬ê¸° ìƒì„±"""
    return GeometricMatchingPreprocessor(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = create_geometric_matching_preprocessor()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, channels, height, width = 2, 3, 256, 256
    test_image1 = torch.randn(batch_size, channels, height, width)
    test_image2 = torch.randn(batch_size, channels, height, width)
    
    # ì „ì²˜ë¦¬ ìˆ˜í–‰
    result = preprocessor(test_image1, test_image2)
    
    print(f"ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€1 í˜•íƒœ: {result['processed_image1'].shape}")
    print(f"ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€2 í˜•íƒœ: {result['processed_image2'].shape}")
    
    # ì „ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
    preprocess_info = preprocessor.get_preprocessing_info()
    print(f"ì „ì²˜ë¦¬ ì •ë³´: {preprocess_info}")
    
    # ë‹¨ì¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    single_image = torch.randn(channels, height, width)
    processed_single = preprocessor.preprocess_single_image(single_image)
    print(f"ë‹¨ì¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê²°ê³¼ í˜•íƒœ: {processed_single.shape}")
    
    # ë°°ì¹˜ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    image_list = [torch.randn(channels, height, width) for _ in range(3)]
    processed_batch = preprocessor.preprocess_batch(image_list)
    print(f"ë°°ì¹˜ ì „ì²˜ë¦¬ ê²°ê³¼ ê°œìˆ˜: {len(processed_batch)}")
    for i, img in enumerate(processed_batch):
        print(f"  ì´ë¯¸ì§€ {i+1} í˜•íƒœ: {img.shape}")
