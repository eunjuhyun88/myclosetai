#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Geometric Matching Core
========================================

ğŸ¯ ê¸°í•˜í•™ì  ë§¤ì¹­ í•µì‹¬ ê¸°ëŠ¥
âœ… ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°
âœ… ì‹¤ì œ ì¶”ë¡  ë¡œì§ êµ¬í˜„
âœ… ë‹¤ì¤‘ ì•„í‚¤í…ì²˜ ì§€ì›
âœ… M3 Max ìµœì í™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import math

logger = logging.getLogger(__name__)

@dataclass
class GeometricMatchingConfig:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ì„¤ì •"""
    input_size: Tuple[int, int] = (256, 256)
    output_size: Tuple[int, int] = (256, 256)
    embedding_dim: int = 512
    num_heads: int = 8
    num_layers: int = 6
    use_mps: bool = True
    enable_attention: bool = True
    enable_geometric_constraints: bool = True
    matching_threshold: float = 0.5

class MultiHeadAttention(nn.Module):
    """ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ëª¨ë“ˆ"""
    
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                attn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # Linear projections
        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        
        if attn_mask is not None:
            attn_weights = attn_weights.masked_fill(attn_mask == 0, float('-inf'))
        
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.embed_dim
        )
        
        output = self.out_proj(attn_output)
        
        return output, attn_weights

class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡"""
    
    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout: float = 0.1):
        super().__init__()
        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim)
        )
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention with residual connection
        attn_output, _ = self.attention(x, x, x, attn_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class GeometricMatchingCore(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ í•µì‹¬ ê¸°ëŠ¥"""
    
    def __init__(self, config: GeometricMatchingConfig = None):
        super().__init__()
        self.config = config or GeometricMatchingConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Geometric Matching ì½”ì–´ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ì…ë ¥ ì„ë² ë”©
        self.input_embedding = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, padding=1),  # 6 channels: 3 for image1 + 3 for image2
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, self.config.embedding_dim, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = self._create_positional_encoding()
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´
        if self.config.enable_attention:
            self.transformer_layers = nn.ModuleList([
                TransformerBlock(
                    self.config.embedding_dim,
                    self.config.num_heads,
                    self.config.embedding_dim * 4
                )
                for _ in range(self.config.num_layers)
            ])
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.output_projection = nn.Sequential(
            nn.Conv2d(self.config.embedding_dim, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 2, kernel_size=3, padding=1),  # 2 channels for x, y displacement
            nn.Tanh()  # Output range -1 to 1
        )
        
        # ê¸°í•˜í•™ì  ì œì•½ ì¡°ê±´
        if self.config.enable_geometric_constraints:
            self.geometric_constraint_net = self._create_geometric_constraint_net()
        
        # í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ
        self.quality_assessor = self._create_quality_assessor()
        
        self.logger.info("âœ… Geometric Matching ì½”ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_positional_encoding(self) -> nn.Parameter:
        """ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±"""
        pos_encoding = torch.zeros(1, self.config.embedding_dim, 
                                 self.config.input_size[0], self.config.input_size[1])
        
        for h in range(self.config.input_size[0]):
            for w in range(self.config.input_size[1]):
                for d in range(self.config.embedding_dim):
                    if d % 2 == 0:
                        pos_encoding[0, d, h, w] = math.sin(h / (10000 ** (d / self.config.embedding_dim)))
                    else:
                        pos_encoding[0, d, h, w] = math.cos(h / (10000 ** ((d - 1) / self.config.embedding_dim)))
        
        return nn.Parameter(pos_encoding, requires_grad=False)
    
    def _create_geometric_constraint_net(self) -> nn.Module:
        """ê¸°í•˜í•™ì  ì œì•½ ì¡°ê±´ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(2, 64, kernel_size=3, padding=1),  # 2 channels for displacement
            nn.ReLU(),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 1, kernel_size=3, padding=1),
            nn.Sigmoid()  # Constraint satisfaction probability
        )
    
    def _create_quality_assessor(self) -> nn.Module:
        """í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Linear(self.config.input_size[0] * self.config.input_size[1] * 2, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, image1: torch.Tensor, image2: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰
        
        Args:
            image1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (B, C, H, W)
            image2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (B, C, H, W)
        
        Returns:
            ë§¤ì¹­ ê²°ê³¼
        """
        # ì…ë ¥ ê²€ì¦
        if not self._validate_inputs(image1, image2):
            raise ValueError("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        image1 = image1.to(self.device)
        image2 = image2.to(self.device)
        
        # 1ë‹¨ê³„: ì…ë ¥ ê²°í•© ë° ì„ë² ë”©
        combined_input = torch.cat([image1, image2], dim=1)  # (B, 6, H, W)
        features = self.input_embedding(combined_input)  # (B, embedding_dim, H, W)
        
        # 2ë‹¨ê³„: ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        features = features + self.pos_encoding.to(self.device)
        
        # 3ë‹¨ê³„: íŠ¸ëœìŠ¤í¬ë¨¸ ì²˜ë¦¬
        if self.config.enable_attention:
            # (B, embedding_dim, H, W) -> (B, H*W, embedding_dim)
            batch_size, embed_dim, height, width = features.shape
            features = features.flatten(2).transpose(1, 2)
            
            # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì ìš©
            for transformer_layer in self.transformer_layers:
                features = transformer_layer(features)
            
            # (B, H*W, embedding_dim) -> (B, embedding_dim, H, W)
            features = features.transpose(1, 2).view(batch_size, embed_dim, height, width)
        
        # 4ë‹¨ê³„: ì¶œë ¥ í”„ë¡œì ì…˜
        displacement_field = self.output_projection(features)
        
        # 5ë‹¨ê³„: ê¸°í•˜í•™ì  ì œì•½ ì¡°ê±´ ì ìš©
        if self.config.enable_geometric_constraints:
            constraint_satisfaction = self.geometric_constraint_net(displacement_field)
            displacement_field = displacement_field * constraint_satisfaction
        
        # 6ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
        quality_score = self._assess_matching_quality(displacement_field, image1, image2)
        
        # 7ë‹¨ê³„: ë§¤ì¹­ ê²°ê³¼ ìƒì„±
        matching_result = self._generate_matching_result(displacement_field, image1, image2)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "displacement_field": displacement_field,
            "matching_result": matching_result,
            "quality_score": quality_score,
            "constraint_satisfaction": constraint_satisfaction if self.config.enable_geometric_constraints else None,
            "features": features
        }
        
        return result
    
    def _validate_inputs(self, image1: torch.Tensor, image2: torch.Tensor) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        if image1.dim() != 4 or image2.dim() != 4:
            return False
        
        if image1.size(0) != image2.size(0):
            return False
        
        if image1.size(2) != image2.size(2) or image1.size(3) != image2.size(3):
            return False
        
        if image1.size(1) != 3 or image2.size(1) != 3:
            return False
        
        return True
    
    def _assess_matching_quality(self, displacement_field: torch.Tensor, 
                                image1: torch.Tensor, image2: torch.Tensor) -> float:
        """ë§¤ì¹­ í’ˆì§ˆ í‰ê°€"""
        try:
            with torch.no_grad():
                # ë³€ìœ„ í•„ë“œë¥¼ 1Dë¡œ í‰íƒ„í™”
                displacement_flat = displacement_field.view(displacement_field.size(0), -1)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self.quality_assessor(displacement_flat)
                
                return float(quality_score.mean().item())
                
        except Exception as e:
            self.logger.warning(f"ë§¤ì¹­ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
    
    def _generate_matching_result(self, displacement_field: torch.Tensor, 
                                 image1: torch.Tensor, image2: torch.Tensor) -> torch.Tensor:
        """ë§¤ì¹­ ê²°ê³¼ ìƒì„±"""
        batch_size, channels, height, width = image1.shape
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid_y, grid_x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=self.device),
            torch.linspace(-1, 1, width, device=self.device),
            indexing='ij'
        )
        
        # ë³€ìœ„ í•„ë“œ ì ìš©
        warped_grid_x = grid_x + displacement_field[:, 0, :, :]
        warped_grid_y = grid_y + displacement_field[:, 1, :, :]
        
        # ê·¸ë¦¬ë“œ ì •ê·œí™” ë° í´ë¨í•‘
        warped_grid_x = torch.clamp(warped_grid_x, -1, 1)
        warped_grid_y = torch.clamp(warped_grid_y, -1, 1)
        
        # ê·¸ë¦¬ë“œ ê²°í•©
        warped_grid = torch.stack([warped_grid_x, warped_grid_y], dim=-1)
        
        # ì´ë¯¸ì§€2ë¥¼ ë³€ìœ„ í•„ë“œì— ë”°ë¼ ì›Œí•‘
        warped_image2 = F.grid_sample(image2, warped_grid, mode='bilinear', 
                                     padding_mode='border', align_corners=False)
        
        # ë§¤ì¹­ ê²°ê³¼ (ì´ë¯¸ì§€1ê³¼ ì›Œí•‘ëœ ì´ë¯¸ì§€2ì˜ ê²°í•©)
        matching_result = (image1 + warped_image2) / 2
        
        return matching_result
    
    def compute_matching_metrics(self, displacement_field: torch.Tensor, 
                                image1: torch.Tensor, image2: torch.Tensor) -> Dict[str, float]:
        """ë§¤ì¹­ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        try:
            with torch.no_grad():
                # 1. ë³€ìœ„ í¬ê¸° í†µê³„
                displacement_magnitude = torch.sqrt(
                    displacement_field[:, 0, :, :] ** 2 + displacement_field[:, 1, :, :] ** 2
                )
                metrics['mean_displacement'] = float(displacement_magnitude.mean().item())
                metrics['max_displacement'] = float(displacement_magnitude.max().item())
                metrics['std_displacement'] = float(displacement_magnitude.std().item())
                
                # 2. ë§¤ì¹­ ì¼ê´€ì„±
                matching_result = self._generate_matching_result(displacement_field, image1, image2)
                consistency_score = 1.0 - F.mse_loss(image1, matching_result)
                metrics['matching_consistency'] = float(consistency_score.item())
                
                # 3. ê¸°í•˜í•™ì  ì¼ê´€ì„±
                if self.config.enable_geometric_constraints:
                    # ë³€ìœ„ í•„ë“œì˜ ê¸°ìš¸ê¸° ê³„ì‚°
                    grad_x = torch.gradient(displacement_field[:, 0, :, :], dim=2)[0]
                    grad_y = torch.gradient(displacement_field[:, 1, :, :], dim=1)[0]
                    geometric_consistency = 1.0 - torch.mean(torch.abs(grad_x - grad_y))
                    metrics['geometric_consistency'] = float(geometric_consistency.item())
                
                # 4. í’ˆì§ˆ ì ìˆ˜
                quality_score = self._assess_matching_quality(displacement_field, image1, image2)
                metrics['quality_score'] = quality_score
                
        except Exception as e:
            self.logger.warning(f"ë§¤ì¹­ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            metrics = {
                'mean_displacement': 0.0,
                'max_displacement': 0.0,
                'std_displacement': 0.0,
                'matching_consistency': 0.0,
                'quality_score': 0.0
            }
        
        return metrics
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "input_size": self.config.input_size,
            "output_size": self.config.output_size,
            "embedding_dim": self.config.embedding_dim,
            "num_heads": self.config.num_heads,
            "num_layers": self.config.num_layers,
            "device": str(self.device),
            "enable_attention": self.config.enable_attention,
            "enable_geometric_constraints": self.config.enable_geometric_constraints,
            "matching_threshold": self.config.matching_threshold,
            "total_parameters": sum(p.numel() for p in self.parameters()),
            "trainable_parameters": sum(p.numel() for p in self.parameters() if p.requires_grad)
        }

# ê¸°í•˜í•™ì  ë§¤ì¹­ ì½”ì–´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def create_geometric_matching_core(config: GeometricMatchingConfig = None) -> GeometricMatchingCore:
    """Geometric Matching ì½”ì–´ ìƒì„±"""
    return GeometricMatchingCore(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ê¸°í•˜í•™ì  ë§¤ì¹­ ì½”ì–´ ìƒì„±
    core = create_geometric_matching_core()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, channels, height, width = 2, 3, 256, 256
    test_image1 = torch.randn(batch_size, channels, height, width)
    test_image2 = torch.randn(batch_size, channels, height, width)
    
    # ê¸°í•˜í•™ì  ë§¤ì¹­ ìˆ˜í–‰
    result = core(test_image1, test_image2)
    
    print(f"ë³€ìœ„ í•„ë“œ í˜•íƒœ: {result['displacement_field'].shape}")
    print(f"ë§¤ì¹­ ê²°ê³¼ í˜•íƒœ: {result['matching_result'].shape}")
    print(f"í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.3f}")
    
    # ë§¤ì¹­ ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = core.compute_matching_metrics(result['displacement_field'], test_image1, test_image2)
    print(f"ë§¤ì¹­ ë©”íŠ¸ë¦­: {metrics}")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = core.get_model_info()
    print(f"ëª¨ë¸ ì •ë³´: {model_info}")
