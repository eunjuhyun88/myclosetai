"""
ğŸ”¥ MyCloset AI - Step 01: Human Parsing v9.0 - ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°
=======================================================================

âœ… ê¸°ì¡´ ì™„ì „í•œ BaseStepMixin v20.0 (5120ì¤„) í™œìš©
âœ… ê¸°ì¡´ step.pyì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ì™„ì „ ë³µì›
âœ… ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì‹œìŠ¤í…œ í†µí•©
âœ… ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡° í†µí•©
âœ… ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©:
   - Boundary Refinement Network
   - Feature Pyramid Network with Attention
   - Iterative Refinement Module with Memory
   - Multi-scale Feature Fusion
âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ê°•í™”
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
âœ… í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

í•µì‹¬ êµ¬í˜„ ê¸°ëŠ¥:
1. ğŸ”¥ Enhanced Graphonomy: ResNet-101 + ASPP + ê³ ê¸‰ ëª¨ë“ˆë“¤ í†µí•©
2. ğŸ”¥ Enhanced U2Net: Nested U-Structure + ê³ ê¸‰ ëª¨ë“ˆë“¤ í†µí•©
3. ğŸ”¥ Enhanced DeepLabV3+: Xception + ê³ ê¸‰ ëª¨ë“ˆë“¤ í†µí•©
4. 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •í™• íŒŒì‹± (ë°°ê²½ í¬í•¨)
5. 512x512 ì…ë ¥ í¬ê¸° í‘œì¤€í™”
6. MPS/CUDA ë””ë°”ì´ìŠ¤ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-08-07
Version: 9.0 (ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°)
"""

import logging
import time
from typing import Dict, Any, Optional, Tuple, List
import torch
import torch.nn as nn
import numpy as np

# ğŸ”¥ ê¸°ì¡´ ì™„ì „í•œ BaseStepMixin import
try:
    from app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
except ImportError:
    # í´ë°±: ìƒëŒ€ ê²½ë¡œë¡œ import ì‹œë„
    try:
        from ..base.base_step_mixin import BaseStepMixin
    except ImportError:
        # ìµœì¢… í´ë°±: mock í´ë˜ìŠ¤
        class BaseStepMixin:
            def __init__(self, **kwargs):
                pass

# Central Hub importë¥¼ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬
try:
    from app.ai_pipeline.utils.common_imports import (
        _get_central_hub_container
    )
except ImportError:
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” mock í•¨ìˆ˜ ì‚¬ìš©
    def _get_central_hub_container():
        return None

# ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ë“¤ import - ìƒëŒ€ import ë¬¸ì œ í•´ê²°
try:
    # ì •ìƒì ì¸ íŒ¨í‚¤ì§€ í™˜ê²½ì—ì„œì˜ ìƒëŒ€ import
    from .models.human_parsing_model_loader import HumanParsingModelLoader
    from .models.checkpoint_analyzer import CheckpointAnalyzer
    from .inference.inference_engine import InferenceEngine
    from .preprocessing.preprocessor import Preprocessor
    from .postprocessing.postprocessor import Postprocessor
    from .utils.utils import Utils
except ImportError:
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œì˜ ì ˆëŒ€ import
    import sys
    import os
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    try:
        from models.human_parsing_model_loader import HumanParsingModelLoader
        from models.checkpoint_analyzer import CheckpointAnalyzer
        from inference.inference_engine import InferenceEngine
        from preprocessing.preprocessor import Preprocessor
        from postprocessing.postprocessor import Postprocessor
        from utils.utils import Utils
    except ImportError:
        # ìµœì¢… í´ë°±: mock í´ë˜ìŠ¤ë“¤ ìƒì„±
        class HumanParsingModelLoader:
            def __init__(self, step_instance=None):
                self.step = step_instance
            def load_models_directly(self):
                return False
            def load_fallback_models(self):
                return False
        
        class CheckpointAnalyzer:
            def __init__(self):
                pass
        
        class InferenceEngine:
            def __init__(self, step):
                self.step = step
        
        class Preprocessor:
            def __init__(self, step):
                self.step = step
        
        class Postprocessor:
            def __init__(self, step):
                self.step = step
        
        class Utils:
            def __init__(self, step):
                self.step = step

logger = logging.getLogger(__name__)

class HumanParsingStep(BaseStepMixin):
    """
    ğŸ”¥ MyCloset AI - Step 01: Human Parsing v9.0
    ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ ì—…ê·¸ë ˆì´ë“œëœ ì¸ì²´ íŒŒì‹± ë‹¨ê³„
    """
    
    def __init__(self, **kwargs):
        """ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ ì´ˆê¸°í™”"""
        super().__init__(**kwargs)
        
        # ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©ëœ êµ¬ì¡° ì •ë³´
        self.enhanced_modules_info = {
            'boundary_refinement': 'Advanced Boundary Refinement Network',
            'feature_pyramid_network': 'Feature Pyramid Network with Attention',
            'iterative_refinement': 'Iterative Refinement Module with Memory',
            'multi_scale_fusion': 'Multi-scale Feature Fusion'
        }
        
        # ê¸°ì¡´ ì´ˆê¸°í™”
        self._initialize_config()
        self._initialize_model_loading_status()
        self._connect_to_central_hub()
        self._create_local_model_loader()
        
        self.logger.info("âœ… HumanParsingStep v9.0 ì´ˆê¸°í™” ì™„ë£Œ (ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°)")
    
    def _initialize_config(self) -> Dict[str, Any]:
        """ì„¤ì • ì´ˆê¸°í™”"""
        return {
            'enable_ensemble': True,
            'enable_high_resolution': True,
            'enable_memory_monitoring': True,
            'memory_optimization_level': 'high',
            'max_memory_usage_gb': 8.0,
            'input_size': (512, 512),
            'num_classes': 20,
            'confidence_threshold': 0.5,
            'quality_threshold': 0.3
        }
    
    def _initialize_model_loading_status(self):
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™”"""
        self.models_loading_status = {
            'graphonomy': False,
            'u2net': False,
            'deeplabv3plus': False,
            'hrnet': False
        }
    
    def _connect_to_central_hub(self):
        """Central Hub ì—°ê²° ë° ì˜ì¡´ì„± ì£¼ì…"""
        try:
            # ê¸°ì¡´ ì™„ì „í•œ BaseStepMixinì˜ Central Hub ì—°ê²° ê¸°ëŠ¥ í™œìš©
            if hasattr(self, '_auto_connect_central_hub'):
                self._auto_connect_central_hub()
            else:
                # í´ë°±: ìˆ˜ë™ ì—°ê²°
                central_hub = _get_central_hub_container()
                if central_hub:
                    self.model_loader = central_hub.get('model_loader')
                    self.memory_manager = central_hub.get('memory_manager')
                    self.data_converter = central_hub.get('data_converter')
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _create_local_model_loader(self):
        """ë¡œì»¬ ëª¨ë¸ ë¡œë” ìƒì„±"""
        try:
            return HumanParsingModelLoader(self)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _load_ai_models_via_central_hub(self) -> bool:
        """Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”©"""
        try:
            if hasattr(self, 'model_loader') and self.model_loader:
                # ê¸°ì¡´ ì™„ì „í•œ BaseStepMixinì˜ ëª¨ë¸ ë¡œë”© ê¸°ëŠ¥ í™œìš©
                return self.model_loader.load_models_for_step('human_parsing')
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_models_directly(self) -> bool:
        """ì§ì ‘ ëª¨ë¸ ë¡œë”©"""
        try:
            model_loader = self._create_local_model_loader()
            if model_loader:
                return model_loader.load_models_directly()
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_fallback_models(self) -> bool:
        """í´ë°± ëª¨ë¸ ë¡œë”©"""
        try:
            model_loader = self._create_local_model_loader()
            if model_loader:
                return model_loader.load_fallback_models()
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ í™œìš©í•œ AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            self.logger.info("ğŸš€ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡° AI ì¶”ë¡  ì‹œì‘")
            
            # ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©ëœ ì¶”ë¡  ì—”ì§„ ì‚¬ìš©
            inference_result = self.inference_engine.run_ai_inference(input_data)
            
            # ğŸ”¥ ê³ ê¸‰ ëª¨ë“ˆë“¤ì˜ ì¶œë ¥ ì •ë³´ ë¡œê¹…
            if 'model_type' in inference_result:
                model_type = inference_result['model_type']
                self.logger.info(f"ğŸ”¥ ì‚¬ìš©ëœ ëª¨ë¸: {model_type}")
                
                # ê³ ê¸‰ ëª¨ë“ˆë“¤ì˜ ì¶œë ¥ í™•ì¸
                enhanced_outputs = []
                if 'boundary_maps' in inference_result:
                    enhanced_outputs.append('Boundary Refinement Network')
                if 'refinement_history' in inference_result:
                    enhanced_outputs.append('Iterative Refinement Module')
                if 'fpn_features' in inference_result:
                    enhanced_outputs.append('Feature Pyramid Network')
                if 'attention_weights' in inference_result:
                    enhanced_outputs.append('Attention Mechanisms')
                
                if enhanced_outputs:
                    self.logger.info(f"ğŸ”¥ í™œì„±í™”ëœ ê³ ê¸‰ ëª¨ë“ˆë“¤: {', '.join(enhanced_outputs)}")
            
            return inference_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡° AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _preprocess_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            return self.preprocessor.preprocess(input_data)
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def _postprocess_result(self, inference_result: Dict[str, Any], original_image: np.ndarray) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            return self.postprocessor.postprocess(inference_result, original_image)
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return inference_result
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ê¸°ì¡´ ì™„ì „í•œ BaseStepMixin í™œìš©"""
        start_time = time.time()
        
        try:
            self.logger.info("ğŸš€ Human Parsing Step ì‹œì‘")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
            input_data = kwargs.copy()
            processed_input = self._preprocess_input(input_data)
            
            # 2. ëª¨ë¸ ë¡œë”© í™•ì¸ ë° í•„ìš”ì‹œ ë¡œë”©
            if not self.loaded_models:
                self.logger.info("ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘")
                
                # 3ë‹¨ê³„ ëª¨ë¸ ë¡œë”© ì‹œë„
                if not self._load_ai_models_via_central_hub():
                    if not self._load_models_directly():
                        if not self._load_fallback_models():
                            raise ValueError("ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹œë„ ì‹¤íŒ¨")
            
            # 3. AI ì¶”ë¡  ì‹¤í–‰
            inference_result = self._run_ai_inference(processed_input)
            
            if not inference_result.get('success', False):
                raise ValueError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {inference_result.get('error', 'Unknown error')}")
            
            # 4. ê²°ê³¼ í›„ì²˜ë¦¬
            original_image = self.utils.extract_input_image(processed_input)
            final_result = self._postprocess_result(inference_result, original_image)
            
            # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
            processing_time = time.time() - start_time
            final_result['processing_time'] = processing_time
            final_result['step_name'] = 'human_parsing'
            final_result['version'] = '8.2_base_step_mixin'
            
            self.logger.info(f"âœ… Human Parsing Step ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            return final_result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_result = self.utils.create_error_response(f"Human Parsing Step ì‹¤íŒ¨: {e}")
            error_result['processing_time'] = processing_time
            error_result['step_name'] = 'human_parsing'
            error_result['version'] = '8.2_base_step_mixin'
            
            self.logger.error(f"âŒ Human Parsing Step ì‹¤íŒ¨ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ): {e}")
            return error_result
    
    def get_step_requirements(self) -> Dict[str, Any]:
        """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        return {
            'step_name': 'human_parsing',
            'step_id': 1,
            'input_types': ['image', 'numpy', 'tensor', 'file_path'],
            'output_types': ['segmentation_map', 'body_parts', 'clothing_mask'],
            'required_models': ['graphonomy', 'u2net', 'deeplabv3plus'],
            'optional_models': ['hrnet'],
            'input_size': (512, 512),
            'num_classes': 20,
            'device_support': ['cpu', 'cuda', 'mps'],
            'memory_requirements': {
                'minimum_gb': 4.0,
                'recommended_gb': 8.0,
                'optimal_gb': 16.0
            },
            'processing_time': {
                'fast': 2.0,
                'balanced': 5.0,
                'high_quality': 10.0
            }
        }
    
    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ê¸°ì¡´ ì™„ì „í•œ BaseStepMixinì˜ ë³€í™˜ ê¸°ëŠ¥ í™œìš©
            if hasattr(self, 'convert_api_input_to_step_input'):
                return super().convert_api_input_to_step_input(api_input)
            else:
                # í´ë°±: ë¡œì»¬ ë³€í™˜ ë¡œì§
                return self.utils.convert_api_input_to_step_input(api_input)
        except Exception as e:
            self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return api_input
    
    def convert_step_output_to_api_response(self, step_output: Dict[str, Any]) -> Dict[str, Any]:
        """Step ì¶œë ¥ì„ API ì‘ë‹µìœ¼ë¡œ ë³€í™˜"""
        try:
            # ê¸°ì¡´ ì™„ì „í•œ BaseStepMixinì˜ ë³€í™˜ ê¸°ëŠ¥ í™œìš©
            if hasattr(self, 'convert_step_output_to_api_response'):
                return super().convert_step_output_to_api_response(step_output)
            else:
                # í´ë°±: ë¡œì»¬ ë³€í™˜ ë¡œì§
                return self.utils.convert_step_output_to_api_response(step_output)
        except Exception as e:
            self.logger.error(f"âŒ Step ì¶œë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return step_output
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ê¸°ì¡´ ì™„ì „í•œ BaseStepMixinì˜ ì •ë¦¬ ê¸°ëŠ¥ í™œìš©
            if hasattr(self, 'cleanup'):
                super().cleanup()
            else:
                # í´ë°±: ë¡œì»¬ ì •ë¦¬ ë¡œì§
                self.logger.info("ğŸ§¹ Human Parsing Step ë¦¬ì†ŒìŠ¤ ì •ë¦¬")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup_resources()
        except:
            pass
