"""
ìœ í‹¸ë¦¬í‹° ê´€ë ¨ ë©”ì„œë“œë“¤ - ê¸°ì¡´ step.pyì˜ ëª¨ë“  ê¸°ëŠ¥ ë³µì›
"""
import logging
from typing import Dict, Any, Optional, Tuple, List
import torch
import torch.nn as nn
import numpy as np
import cv2
from PIL import Image
import time

logger = logging.getLogger(__name__)

class Utils:
    """ìœ í‹¸ë¦¬í‹° ê´€ë ¨ ë©”ì„œë“œë“¤ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ - ê¸°ì¡´ step.pyì˜ ëª¨ë“  ê¸°ëŠ¥ ë³µì›"""
    
    def __init__(self, step_instance):
        self.step = step_instance
        self.logger = logging.getLogger(f"{__name__}.Utils")
    
    def safe_tensor_to_scalar(self, tensor_value) -> Any:
        """í…ì„œë¥¼ ì•ˆì „í•˜ê²Œ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            if isinstance(tensor_value, torch.Tensor):
                if tensor_value.numel() == 1:
                    return tensor_value.item()
                else:
                    # í…ì„œì˜ í‰ê· ê°’ ì‚¬ìš©
                    return tensor_value.mean().item()
            else:
                return float(tensor_value)
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ê°’
    
    def safe_extract_tensor_from_list(self, data_list) -> Any:
        """ë¦¬ìŠ¤íŠ¸ì—ì„œ ì•ˆì „í•˜ê²Œ í…ì„œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        try:
            if not isinstance(data_list, list) or len(data_list) == 0:
                return None
            
            first_element = data_list[0]
            
            # ì§ì ‘ í…ì„œì¸ ê²½ìš°
            if isinstance(first_element, torch.Tensor):
                return first_element
            
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° í…ì„œ ì°¾ê¸°
            elif isinstance(first_element, dict):
                # ğŸ”¥ ìš°ì„ ìˆœìœ„ í‚¤ ìˆœì„œë¡œ í…ì„œ ì°¾ê¸°
                priority_keys = ['parsing_pred', 'parsing_output', 'output', 'parsing']
                for key in priority_keys:
                    if key in first_element and isinstance(first_element[key], torch.Tensor):
                        return first_element[key]
                
                # ğŸ”¥ ëª¨ë“  ê°’ì—ì„œ í…ì„œ ì°¾ê¸°
                for key, value in first_element.items():
                    if isinstance(value, torch.Tensor):
                        return value
            
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def safe_convert_to_numpy(self, data) -> np.ndarray:
        """ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            if isinstance(data, np.ndarray):
                return data
            elif isinstance(data, torch.Tensor):
                # ğŸ”¥ ê·¸ë˜ë””ì–¸íŠ¸ ë¬¸ì œ í•´ê²°: detach() ì‚¬ìš©
                return data.detach().cpu().numpy()
            elif isinstance(data, list):
                tensor = self.safe_extract_tensor_from_list(data)
                if tensor is not None:
                    return tensor.detach().cpu().numpy()
            elif isinstance(data, dict):
                for key in ['parsing', 'parsing_pred', 'output', 'parsing_output']:
                    if key in data and isinstance(data[key], torch.Tensor):
                        return data[key].detach().cpu().numpy()
            
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return np.zeros((512, 512), dtype=np.uint8)
        except Exception as e:
            self.logger.warning(f"âš ï¸ NumPy ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.zeros((512, 512), dtype=np.uint8)
    
    def standardize_tensor_sizes(self, tensors: List[torch.Tensor], target_size: Optional[Tuple[int, int]] = None) -> List[torch.Tensor]:
        """í…ì„œ í¬ê¸° í‘œì¤€í™”"""
        try:
            if not tensors:
                return tensors
            
            if target_size is None:
                # ì²« ë²ˆì§¸ í…ì„œì˜ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
                target_size = (tensors[0].shape[-2], tensors[0].shape[-1])
            
            standardized_tensors = []
            for tensor in tensors:
                if tensor.shape[-2:] != target_size:
                    # F.interpolateë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ê¸° ì¡°ì •
                    tensor = torch.nn.functional.interpolate(
                        tensor.unsqueeze(0) if tensor.dim() == 3 else tensor,
                        size=target_size,
                        mode='bilinear',
                        align_corners=False
                    )
                    if tensor.dim() == 4:
                        tensor = tensor.squeeze(0)
                standardized_tensors.append(tensor)
            
            return standardized_tensors
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ í¬ê¸° í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return tensors
    
    def get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            from app.ai_pipeline.utils.common_imports import _get_central_hub_container
            container = _get_central_hub_container()
            if container:
                return container.get(service_key)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def assess_image_quality(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            if image is None or image.size == 0:
                return 0.0
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            # 1. ì„ ëª…ë„ í‰ê°€ (Laplacian variance)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(laplacian_var / 1000.0, 1.0)
            
            # 2. ëŒ€ë¹„ í‰ê°€
            contrast_score = gray.std() / 255.0
            
            # 3. ë°ê¸° í‰ê°€
            brightness_score = 1.0 - abs(gray.mean() - 128) / 128.0
            
            # 4. ë…¸ì´ì¦ˆ í‰ê°€ (ê°„ë‹¨í•œ ë°©ë²•)
            noise_score = 1.0 - min(gray.std() / 50.0, 1.0)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            quality_score = (sharpness_score + contrast_score + brightness_score + noise_score) / 4.0
            
            return max(0.0, min(1.0, quality_score))
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def memory_efficient_resize(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¦¬ì‚¬ì´ì¦ˆ"""
        try:
            if image is None:
                return np.zeros(target_size + (3,), dtype=np.uint8)
            
            # PILì„ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¦¬ì‚¬ì´ì¦ˆ
            if len(image.shape) == 3:
                pil_image = Image.fromarray(image)
                resized_pil = pil_image.resize(target_size[::-1], Image.LANCZOS)
                return np.array(resized_pil)
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
                pil_image = Image.fromarray(image, mode='L')
                resized_pil = pil_image.resize(target_size[::-1], Image.LANCZOS)
                return np.array(resized_pil)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            return cv2.resize(image, target_size[::-1]) if image is not None else np.zeros(target_size + (3,), dtype=np.uint8)
    
    def normalize_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            if image is None:
                return image
            
            # LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            if len(image.shape) == 3:
                lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                l, a, b = cv2.split(lab)
                
                # L ì±„ë„ì— CLAHE ì ìš©
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                l = clahe.apply(l)
                
                # ë‹¤ì‹œ RGBë¡œ ë³€í™˜
                lab = cv2.merge([l, a, b])
                normalized = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
                return normalized
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                return clahe.apply(image)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def correct_colors(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if image is None or len(image.shape) != 3:
                return image
            
            # ìë™ ìƒ‰ìƒ ë³´ì •
            # 1. í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì ìš©
            def white_balance(img):
                result = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
                avg_a = np.average(result[:, :, 1])
                avg_b = np.average(result[:, :, 2])
                result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
                result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
                result = cv2.cvtColor(result, cv2.COLOR_LAB2RGB)
                return result
            
            corrected = white_balance(image)
            
            # 2. ê°ë§ˆ ë³´ì •
            gamma = 1.1
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
            corrected = cv2.LUT(corrected, table)
            
            return corrected
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def detect_roi(self, image: np.ndarray) -> Dict[str, Any]:
        """ê´€ì‹¬ ì˜ì—­ ê°ì§€"""
        try:
            if image is None:
                return {'roi_detected': False, 'roi_bbox': None, 'confidence': 0.0}
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            # 1. ì—£ì§€ ê°ì§€
            edges = cv2.Canny(gray, 50, 150)
            
            # 2. ìœ¤ê³½ì„  ì°¾ê¸°
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if not contours:
                return {'roi_detected': False, 'roi_bbox': None, 'confidence': 0.0}
            
            # 3. ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)
            
            # 4. ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            image_area = image.shape[0] * image.shape[1]
            confidence = min(area / image_area * 10, 1.0)
            
            return {
                'roi_detected': True,
                'roi_bbox': (x, y, w, h),
                'confidence': confidence,
                'area': area
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ROI ê°ì§€ ì‹¤íŒ¨: {e}")
            return {'roi_detected': False, 'roi_bbox': None, 'confidence': 0.0}
    
    def create_safe_input_tensor(self, image: np.ndarray, device_str: str) -> torch.Tensor:
        """ì•ˆì „í•œ ì…ë ¥ í…ì„œ ìƒì„±"""
        try:
            if image is None:
                # ê¸°ë³¸ í…ì„œ ìƒì„±
                return torch.zeros((1, 3, 512, 512), device=device_str)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if len(image.shape) == 2:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ RGBë¡œ ë³€í™˜
                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
            
            # í¬ê¸° ì¡°ì •
            if image.shape[:2] != (512, 512):
                image = self.memory_efficient_resize(image, (512, 512))
            
            # ì •ê·œí™”
            image = image.astype(np.float32) / 255.0
            
            # í…ì„œ ë³€í™˜
            tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
            
            return tensor.to(device_str)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì•ˆì „í•œ ì…ë ¥ í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 3, 512, 512), device=device_str)
    
    def create_fallback_parsing(self, image: np.ndarray) -> np.ndarray:
        """í´ë°± íŒŒì‹± ìƒì„±"""
        try:
            if image is None:
                return np.zeros((512, 512), dtype=np.uint8)
            
            # ê°„ë‹¨í•œ í´ë°± íŒŒì‹± ìƒì„±
            height, width = image.shape[:2]
            parsing = np.zeros((height, width), dtype=np.uint8)
            
            # ì¤‘ì•™ ì˜ì—­ì„ ë°°ê²½ìœ¼ë¡œ ì„¤ì •
            center_y, center_x = height // 2, width // 2
            radius = min(height, width) // 4
            
            # ì›í˜• ë§ˆìŠ¤í¬ ìƒì„±
            y, x = np.ogrid[:height, :width]
            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
            
            # ë°°ê²½ (0)ê³¼ ì „ê²½ (1) ì„¤ì •
            parsing[mask] = 1
            
            return parsing
        except Exception as e:
            self.logger.warning(f"âš ï¸ í´ë°± íŒŒì‹± ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros((512, 512), dtype=np.uint8)
    
    def extract_input_image(self, input_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            if not input_data:
                return None
            
            # ë‹¤ì–‘í•œ í‚¤ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
            image_keys = ['image', 'input_image', 'file_path', 'file', 'data']
            
            for key in image_keys:
                if key in input_data:
                    data = input_data[key]
                    
                    # NumPy ë°°ì—´ì¸ ê²½ìš°
                    if isinstance(data, np.ndarray):
                        return data
                    
                    # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                    elif isinstance(data, str):
                        if data.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                            try:
                                image = cv2.imread(data)
                                if image is not None:
                                    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
                    
                    # PIL ì´ë¯¸ì§€ì¸ ê²½ìš°
                    elif hasattr(data, 'convert'):
                        try:
                            return np.array(data.convert('RGB'))
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ PIL ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì…ë ¥ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def preprocess_image_for_model(self, image: np.ndarray, model_name: str) -> torch.Tensor:
        """ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if image is None:
                return torch.zeros((1, 3, 512, 512))
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self.memory_efficient_resize(image, (512, 512))
            processed_image = self.normalize_lighting(processed_image)
            processed_image = self.correct_colors(processed_image)
            
            # ì •ê·œí™”
            processed_image = processed_image.astype(np.float32) / 255.0
            
            # í…ì„œ ë³€í™˜
            tensor = torch.from_numpy(processed_image).permute(2, 0, 1).unsqueeze(0)
            
            return tensor
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 3, 512, 512))
    
    def calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced') -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if parsing_probs is None:
                return 0.5
            
            if isinstance(parsing_probs, torch.Tensor):
                # í…ì„œì¸ ê²½ìš°
                if parsing_probs.dim() == 4:
                    # ë°°ì¹˜ ì°¨ì›ì´ ìˆëŠ” ê²½ìš°
                    probs = parsing_probs.squeeze(0)
                else:
                    probs = parsing_probs
                
                # ìµœëŒ€ í™•ë¥ ê°’ ì‚¬ìš©
                max_probs = torch.max(probs, dim=0)[0]
                confidence = torch.mean(max_probs).item()
            else:
                # NumPy ë°°ì—´ì¸ ê²½ìš°
                if len(parsing_probs.shape) == 4:
                    probs = parsing_probs.squeeze(0)
                else:
                    probs = parsing_probs
                
                max_probs = np.max(probs, axis=0)
                confidence = np.mean(max_probs)
            
            return max(0.0, min(1.0, confidence))
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'parsing_map': np.zeros((512, 512), dtype=np.uint8),
            'confidence': 0.0,
            'processing_time': 0.0,
            'model_used': 'none',
            'quality_metrics': {
                'overall_quality': 0.0,
                'sharpness': 0.0,
                'contrast': 0.0,
                'brightness': 0.0
            }
        }
