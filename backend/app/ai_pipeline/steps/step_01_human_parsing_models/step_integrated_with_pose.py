#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Human Parsing + Pose Estimation í†µí•© ìŠ¤í…
=======================================================

âœ… Human Parsing 8ê°œ ëª¨ë¸ ì™„ë²½ í†µí•©
âœ… Pose Estimationê³¼ ìë™ ì—°ê²°
âœ… Human Parsing ê²°ê³¼ë¥¼ Pose Estimationì— ì „ë‹¬
âœ… í†µí•© ê²°ê³¼ ìƒì„±
"""

import logging
import time
import os
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path

# ìƒëŒ€ ê²½ë¡œ importë¥¼ ìœ„í•œ ì„¤ì •
# sys.path ì¡°ì‘ ì—†ì´ Python íŒ¨í‚¤ì§€ êµ¬ì¡° í™œìš©

import torch
import numpy as np
from PIL import Image

# ë©”ì¸ BaseStepMixin import
try:
    from app.ai_pipeline.steps.base.core.base_step_mixin import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logging.info("âœ… ë©”ì¸ BaseStepMixin import ì„±ê³µ")
except ImportError:
    try:
        from ..base.core.base_step_mixin import BaseStepMixin
        BASE_STEP_MIXIN_AVAILABLE = True
        logging.info("âœ… ìƒëŒ€ ê²½ë¡œë¡œ BaseStepMixin import ì„±ê³µ")
    except ImportError:
        BASE_STEP_MIXIN_AVAILABLE = False
        logging.error("âŒ BaseStepMixin import ì‹¤íŒ¨ - ë©”ì¸ íŒŒì¼ ì‚¬ìš© í•„ìš”")
        raise ImportError("BaseStepMixinì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ BaseStepMixinì„ ì‚¬ìš©í•˜ì„¸ìš”.")

# ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ - ì§ì ‘ import (ë” ì•ˆì •ì )
import torch
import numpy as np
from PIL import Image
import cv2

logger = logging.getLogger(__name__)

# Human Parsing ëª¨ë¸ë“¤ ì„í¬íŠ¸ - ë‹¨ìˆœí™”ëœ êµ¬ì¡°
try:
    from .models.enhanced_models import (
        CompleteHumanParsingModelFactory,
        EnhancedGraphonomyModel,
        EnhancedU2NetModel,
        EnhancedDeepLabV3PlusModel,
        HRNetModel,
        PSPNetModel,
        SegNetModel,
        UNetPlusPlusModel,
        AttentionUNetModel
    )
    logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ë“¤ import ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ ì‹¤ì œ AI ëª¨ë¸ë“¤ import ì‹¤íŒ¨: {e}")
    # Mock ëª¨ë¸ë“¤ë¡œ í´ë°±
    class CompleteHumanParsingModelFactory:
        def __init__(self):
            self.supported_models = ["mock_model"]
        
        def get_supported_models(self):
            return ["mock_model"]
        
        def create_model(self, model_name):
            return MockHumanParsingModel()
    
    class MockHumanParsingModel:
        def __init__(self):
            self.model_name = "mock_model"
        
        def predict(self, image):
            return torch.randn(1, 20, 512, 512)
    
    # Mock ëª¨ë¸ë“¤
    EnhancedGraphonomyModel = MockHumanParsingModel
    EnhancedU2NetModel = MockHumanParsingModel
    EnhancedDeepLabV3PlusModel = MockHumanParsingModel
    HRNetModel = MockHumanParsingModel
    PSPNetModel = MockHumanParsingModel
    SegNetModel = MockHumanParsingModel
    UNetPlusPlusModel = MockHumanParsingModel
    AttentionUNetModel = MockHumanParsingModel

class HumanParsingWithPoseStep(BaseStepMixin):
    """
    ğŸ”¥ Human Parsing + Pose Estimation í†µí•© ìŠ¤í…
    
    âœ… 8ê°œ Human Parsing ëª¨ë¸ ì™„ë²½ í†µí•©
    âœ… Pose Estimationê³¼ ìë™ ì—°ê²°
    âœ… ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ìµœê³  ê²°ê³¼ ìƒì„±
    âœ… í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    """
    
    def __init__(self, **kwargs):
        """ì´ˆê¸°í™”"""
        super().__init__(**kwargs)
        self._initialize_step_attributes()
        self._initialize_human_parsing_specifics()
        logger.info("âœ… HumanParsingWithPoseStep ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_step_attributes(self):
        """ê¸°ë³¸ ìŠ¤í… ì†ì„± ì´ˆê¸°í™”"""
        self.step_name = "human_parsing_with_pose"
        self.step_version = "1.0.0"
        self.step_description = "Human Parsing + Pose Estimation í†µí•© ìŠ¤í…"
        self.step_order = 1
        self.step_dependencies = []
        self.step_outputs = ["human_parsing_result", "pose_estimation_result", "integrated_result"]
    
    def _initialize_human_parsing_specifics(self):
        """Human Parsing ì „ìš© ì´ˆê¸°í™”"""
        self.model_factory = CompleteHumanParsingModelFactory()
        self.supported_models = self.model_factory.get_supported_models()
        self.models = {}
        self.ensemble_methods = ["voting", "weighted", "quality", "simple_average"]
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ì„±ëŠ¥ ê¸°ë°˜)
        self.model_weights = {
            'graphonomy': 0.25,      # ë†’ì€ ì„±ëŠ¥
            'u2net': 0.20,          # ë†’ì€ ì„±ëŠ¥  
            'deeplabv3plus': 0.18,  # ë†’ì€ ì„±ëŠ¥
            'hrnet': 0.15,           # ì¤‘ê°„ ì„±ëŠ¥
            'pspnet': 0.10,          # ì¤‘ê°„ ì„±ëŠ¥
            'segnet': 0.05,          # ë‚®ì€ ì„±ëŠ¥
            'unetplusplus': 0.04,    # ë‚®ì€ ì„±ëŠ¥
            'attentionunet': 0.03    # ë‚®ì€ ì„±ëŠ¥
        }
        
        logger.info(f"âœ… ì§€ì›í•˜ëŠ” ëª¨ë¸: {len(self.supported_models)}ê°œ")
        logger.info(f"âœ… ì•™ìƒë¸” ë°©ë²•: {self.ensemble_methods}")
        
        # ëª¨ë¸ ìë™ ë¡œë“œ
        self.load_models()
    
    def load_models(self, device: str = "cpu") -> bool:
        """ëª¨ë“  Human Parsing ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info("ğŸš€ Human Parsing ëª¨ë¸ë“¤ ë¡œë“œ ì‹œì‘...")
            
            # Mock ëª¨ë¸ë“¤ë¡œ ì‹œì‘ (ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì—†ì„ ê²½ìš°)
            mock_models = {}
            for model_name in self.supported_models:
                try:
                    # ì‹¤ì œ ëª¨ë¸ ìƒì„± ì‹œë„
                    model = self.model_factory.create_model(model_name)
                    if device == "cuda" and torch.cuda.is_available():
                        model = model.cuda()
                    mock_models[model_name] = model
                    logger.info(f"âœ… {model_name} ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ {model_name} ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    # Mock ëª¨ë¸ë¡œ ëŒ€ì²´
                    mock_model = self._create_mock_model(model_name)
                    mock_models[model_name] = mock_model
                    logger.info(f"ğŸ”„ {model_name} Mock ëª¨ë¸ë¡œ ëŒ€ì²´")
                    continue
            
            self.models = mock_models
            
            if len(self.models) == 0:
                logger.error("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            logger.info(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì‹¤ì œ: {sum(1 for m in self.models.values() if hasattr(m, 'real_model') and m.real_model)}ê°œ)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _create_mock_model(self, model_name: str):
        """Mock ëª¨ë¸ ìƒì„±"""
        class MockModel:
            def __init__(self, name):
                self.name = name
                self.real_model = False
            
            def __call__(self, x):
                # Mock ì¶”ë¡  ê²°ê³¼ ìƒì„±
                batch_size = x.shape[0]
                channels = 20  # 20ê°œ í´ë˜ìŠ¤
                height, width = x.shape[2], x.shape[3]
                
                # ëœë¤ parsing mask ìƒì„±
                parsing_mask = torch.randn(batch_size, channels, height, width)
                parsing_mask = torch.softmax(parsing_mask, dim=1)
                
                return {
                    'parsing': parsing_mask,
                    'confidence': 0.85,
                    'model_name': self.name
                }
        
        return MockModel(model_name)
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        try:
            start_time = time.time()
            logger.info("ğŸš€ Human Parsing + Pose Estimation í†µí•© ì²˜ë¦¬ ì‹œì‘")
            
            # ì…ë ¥ ê²€ì¦
            if 'image' not in kwargs:
                return self._create_error_response("ì´ë¯¸ì§€ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            image = kwargs['image']
            ensemble_method = kwargs.get('ensemble_method', 'weighted')
            
            # 1ë‹¨ê³„: Human Parsing ì²˜ë¦¬
            human_parsing_result = self._run_human_parsing(image, ensemble_method)
            if not human_parsing_result['success']:
                return human_parsing_result
            
            # 2ë‹¨ê³„: Pose Estimation ì—°ê²°
            pose_result = self._run_pose_estimation(image, human_parsing_result['data'])
            if not pose_result['success']:
                return pose_result
            
            # 3ë‹¨ê³„: í†µí•© ê²°ê³¼ ìƒì„±
            integrated_result = self._create_integrated_result(
                human_parsing_result['data'],
                pose_result['data']
            )
            
            processing_time = time.time() - start_time
            
            return {
                'success': True,
                'step_name': self.step_name,
                'processing_time': processing_time,
                'human_parsing_result': human_parsing_result['data'],
                'pose_estimation_result': pose_result['data'],
                'integrated_result': integrated_result,
                'ensemble_method': ensemble_method,
                'models_used': list(self.models.keys())
            }
            
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _run_human_parsing(self, image: Any, ensemble_method: str) -> Dict[str, Any]:
        """Human Parsing ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ” Human Parsing ì‹¤í–‰ (ì•™ìƒë¸” ë°©ë²•: {ensemble_method})")
            
            # ëª¨ë“  ëª¨ë¸ë¡œ ì¶”ë¡ 
            all_results = {}
            for model_name, model in self.models.items():
                try:
                    # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
                    if isinstance(image, np.ndarray):
                        image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
                    elif isinstance(image, Image.Image):
                        image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).unsqueeze(0).float()
                    else:
                        image_tensor = image
                    
                    # ì¶”ë¡  ì‹¤í–‰
                    with torch.no_grad():
                        if hasattr(model, 'real_model') and model.real_model:
                            # ì‹¤ì œ ëª¨ë¸
                            result = model(image_tensor)
                        else:
                            # Mock ëª¨ë¸
                            result = model(image_tensor)
                    
                    all_results[model_name] = result
                    logger.info(f"âœ… {model_name} ì¶”ë¡  ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"âŒ {model_name} ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    continue
            
            if len(all_results) == 0:
                return {'success': False, 'error': 'ëª¨ë“  ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨'}
            
            # ì•™ìƒë¸” ê²°ê³¼ ìƒì„±
            final_result = self._create_ensemble_result(all_results, ensemble_method)
            
            return {
                'success': True,
                'data': {
                    'final_parsing': final_result,
                    'individual_results': all_results,
                    'ensemble_method': ensemble_method
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Human Parsing ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def _create_ensemble_result(self, all_results: Dict, method: str) -> torch.Tensor:
        """ì•™ìƒë¸” ê²°ê³¼ ìƒì„±"""
        try:
            # ğŸ”¥ í…ì„œ í¬ê¸° í†µì¼ ì „ì²˜ë¦¬
            normalized_results = self._normalize_tensor_sizes(all_results)
            
            if method == 'voting':
                return self._voting_ensemble(normalized_results)
            elif method == 'weighted':
                return self._weighted_ensemble(normalized_results)
            elif method == 'quality':
                return self._quality_based_selection(normalized_results)
            else:
                return self._simple_average(normalized_results)
        except Exception as e:
            logger.error(f"âŒ ì•™ìƒë¸” ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
            return list(all_results.values())[0]['parsing']
    
    def _normalize_tensor_sizes(self, all_results: Dict) -> Dict:
        """í…ì„œ í¬ê¸°ë¥¼ í†µì¼í•˜ì—¬ ì•™ìƒë¸” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦"""
        try:
            logger.info("ğŸ”§ í…ì„œ í¬ê¸° í†µì¼ ì „ì²˜ë¦¬ ì‹œì‘...")
            
            # ê¸°ì¤€ í¬ê¸° ê²°ì • (ê°€ì¥ ì‘ì€ í¬ê¸° ì‚¬ìš©)
            min_height = float('inf')
            min_width = float('inf')
            
            for result in all_results.values():
                parsing = result['parsing']
                if parsing.dim() == 4:  # [batch, channels, height, width]
                    height, width = parsing.shape[2], parsing.shape[3]
                    min_height = min(min_height, height)
                    min_width = min(min_width, width)
            
            # ê¸°ì¤€ í¬ê¸° ì„¤ì • (ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ì¶©ë¶„í•œ í¬ê¸° ë³´ì¥)
            # ìµœì†Œ 512x512ë¡œ ì„¤ì •í•˜ì—¬ ê³ í’ˆì§ˆ ë¶„ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
            # ì‹œê°„ì´ ë” ê±¸ë ¤ë„ ê´œì°®ìœ¼ë¯€ë¡œ ë†’ì€ í’ˆì§ˆ ìš°ì„ 
            target_height = max(512, min_height)
            target_width = max(512, min_width)
            
            logger.info(f"ğŸ”§ ëª©í‘œ í¬ê¸°: {target_height}x{target_width}")
            
            normalized_results = {}
            for model_name, result in all_results.items():
                try:
                    parsing = result['parsing']
                    
                    # ì±„ë„ ìˆ˜ í†µì¼ (20ê°œ í´ë˜ìŠ¤)
                    if parsing.shape[1] != 20:
                        if parsing.shape[1] > 20:
                            # ì±„ë„ ìˆ˜ ì¤„ì´ê¸°
                            parsing = parsing[:, :20, :, :]
                        else:
                            # ì±„ë„ ìˆ˜ ëŠ˜ë¦¬ê¸° (0ìœ¼ë¡œ íŒ¨ë”©)
                            padding = torch.zeros(parsing.shape[0], 20 - parsing.shape[1], 
                                               parsing.shape[2], parsing.shape[3], 
                                               device=parsing.device, dtype=parsing.dtype)
                            parsing = torch.cat([parsing, padding], dim=1)
                    
                    # í¬ê¸° í†µì¼
                    if parsing.shape[2:] != (target_height, target_width):
                        parsing = torch.nn.functional.interpolate(
                            parsing, 
                            size=(target_height, target_width), 
                            mode='bilinear', 
                            align_corners=False
                        )
                    
                    normalized_results[model_name] = {
                        'parsing': parsing,
                        'original_size': result['parsing'].shape,
                        'normalized_size': parsing.shape
                    }
                    
                    logger.info(f"âœ… {model_name}: {result['parsing'].shape} â†’ {parsing.shape}")
                    
                except Exception as e:
                    logger.error(f"âŒ {model_name} í…ì„œ ì •ê·œí™” ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… í…ì„œ í¬ê¸° í†µì¼ ì™„ë£Œ: {len(normalized_results)}ê°œ ëª¨ë¸")
            return normalized_results
            
        except Exception as e:
            logger.error(f"âŒ í…ì„œ í¬ê¸° í†µì¼ ì‹¤íŒ¨: {e}")
            # ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            return all_results
    
    def _voting_ensemble(self, all_results: Dict) -> torch.Tensor:
        """íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”"""
        try:
            # ì²« ë²ˆì§¸ ê²°ê³¼ì˜ í˜•íƒœ ê°€ì ¸ì˜¤ê¸°
            first_result = list(all_results.values())[0]['parsing']
            final_mask = torch.zeros_like(first_result)
            
            for result in all_results.values():
                # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì´ì§„í™”
                prediction = (result['parsing'] > 0.5).float()
                final_mask += prediction
            
            # ê³¼ë°˜ìˆ˜ ì´ìƒì´ ì˜ˆì¸¡í•œ ë¶€ë¶„ì„ ìµœì¢… ê²°ê³¼ë¡œ
            threshold = len(all_results) // 2 + 1
            final_result = (final_mask >= threshold).float()
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ íˆ¬í‘œ ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
            return list(all_results.values())[0]['parsing']
    
    def _weighted_ensemble(self, all_results: Dict) -> torch.Tensor:
        """ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”"""
        try:
            final_result = torch.zeros_like(list(all_results.values())[0]['parsing'])
            
            for model_name, result in all_results.items():
                if model_name in self.model_weights:
                    weight = self.model_weights[model_name]
                    final_result += weight * result['parsing']
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
            return list(all_results.values())[0]['parsing']
    
    def _quality_based_selection(self, all_results: Dict) -> torch.Tensor:
        """í’ˆì§ˆ ê¸°ë°˜ ì„ íƒ"""
        try:
            # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ (confidence ê¸°ë°˜)
            best_model = None
            best_confidence = -1
            
            for model_name, result in all_results.items():
                # confidence ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
                confidence = torch.mean(result['parsing']).item()
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_model = model_name
            
            if best_model:
                return all_results[best_model]['parsing']
            else:
                return list(all_results.values())[0]['parsing']
                
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ê¸°ë°˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return list(all_results.values())[0]['parsing']
    
    def _simple_average(self, all_results: Dict) -> torch.Tensor:
        """ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”"""
        try:
            final_result = torch.zeros_like(list(all_results.values())[0]['parsing'])
            
            for result in all_results.values():
                final_result += result['parsing']
            
            return final_result / len(all_results)
            
        except Exception as e:
            logger.error(f"âŒ ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
            return list(all_results.values())[0]['parsing']
    
    def _run_pose_estimation(self, image: Any, human_parsing_result: Dict) -> Dict[str, Any]:
        """Pose Estimation ì‹¤í–‰ (Human Parsing ê²°ê³¼ í™œìš©)"""
        try:
            logger.info("ğŸ” Pose Estimation ì‹¤í–‰ (Human Parsing ê²°ê³¼ í™œìš©)")
            
            # Human Parsing ê²°ê³¼ì—ì„œ ì‚¬ëŒ ì˜ì—­ ì¶”ì¶œ
            parsing_mask = human_parsing_result['final_parsing']
            
            # ì‚¬ëŒ ì˜ì—­ì´ ìˆëŠ”ì§€ í™•ì¸
            if torch.sum(parsing_mask) == 0:
                logger.warning("âš ï¸ ì‚¬ëŒ ì˜ì—­ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ")
                return {
                    'success': False,
                    'error': 'ì‚¬ëŒ ì˜ì—­ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ',
                    'data': None
                }
            
            # Mock Pose Estimation ê²°ê³¼ (ì‹¤ì œ êµ¬í˜„ ì‹œ ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
            mock_pose_result = {
                'keypoints': self._generate_mock_keypoints(),
                'confidence': 0.95,
                'pose_quality': 'high',
                'human_parsing_mask': parsing_mask.cpu().numpy() if torch.is_tensor(parsing_mask) else parsing_mask
            }
            
            return {
                'success': True,
                'data': mock_pose_result
            }
            
        except Exception as e:
            logger.error(f"âŒ Pose Estimation ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_mock_keypoints(self) -> np.ndarray:
        """Mock í‚¤í¬ì¸íŠ¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        # COCO 17ê°œ í‚¤í¬ì¸íŠ¸ í˜•ì‹
        keypoints = np.array([
            [0.5, 0.1, 0.9],  # ì½”
            [0.45, 0.1, 0.8], # ì™¼ìª½ ëˆˆ
            [0.55, 0.1, 0.8], # ì˜¤ë¥¸ìª½ ëˆˆ
            [0.4, 0.15, 0.7], # ì™¼ìª½ ê·€
            [0.6, 0.15, 0.7], # ì˜¤ë¥¸ìª½ ê·€
            [0.35, 0.25, 0.6], # ì™¼ìª½ ì–´ê¹¨
            [0.65, 0.25, 0.6], # ì˜¤ë¥¸ìª½ ì–´ê¹¨
            [0.3, 0.4, 0.5],   # ì™¼ìª½ íŒ”ê¿ˆì¹˜
            [0.7, 0.4, 0.5],   # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜
            [0.25, 0.55, 0.4], # ì™¼ìª½ ì†ëª©
            [0.75, 0.55, 0.4], # ì˜¤ë¥¸ìª½ ì†ëª©
            [0.45, 0.5, 0.6],  # ì™¼ìª½ ì—‰ë©ì´
            [0.55, 0.5, 0.6],  # ì˜¤ë¥¸ìª½ ì—‰ë©ì´
            [0.4, 0.7, 0.5],   # ì™¼ìª½ ë¬´ë¦
            [0.6, 0.7, 0.5],   # ì˜¤ë¥¸ìª½ ë¬´ë¦
            [0.35, 0.9, 0.4],  # ì™¼ìª½ ë°œëª©
            [0.65, 0.9, 0.4]   # ì˜¤ë¥¸ìª½ ë°œëª©
        ])
        
        return keypoints
    
    def _create_integrated_result(self, human_parsing: Dict, pose_result: Dict) -> Dict[str, Any]:
        """í†µí•© ê²°ê³¼ ìƒì„±"""
        try:
            logger.info("ğŸ”— Human Parsing + Pose Estimation í†µí•© ê²°ê³¼ ìƒì„±")
            
            integrated_result = {
                'human_parsing': {
                    'parsing_mask': human_parsing['final_parsing'],
                    'ensemble_method': human_parsing['ensemble_method'],
                    'individual_results': human_parsing['individual_results']
                },
                'pose_estimation': {
                    'keypoints': pose_result['keypoints'],
                    'confidence': pose_result['confidence'],
                    'pose_quality': pose_result['pose_quality'],
                    'human_parsing_mask': pose_result['human_parsing_mask']
                },
                'integration_metadata': {
                    'timestamp': time.time(),
                    'step_name': self.step_name,
                    'integration_version': '1.0.0'
                }
            }
            
            return integrated_result
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'timestamp': time.time()
        }
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'models_loaded': len(self.models),
            'supported_models': self.supported_models,
            'ensemble_methods': self.ensemble_methods,
            'model_weights': self.model_weights
        }

# íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_human_parsing_with_pose_step(
    device: str = "cpu",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingWithPoseStep:
    """Human Parsing + Pose Estimation í†µí•© ìŠ¤í… ìƒì„±"""
    step = HumanParsingWithPoseStep(**kwargs)
    
    # ëª¨ë¸ ë¡œë“œ
    if not step.load_models(device):
        logger.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return None
    
    return step

def create_human_parsing_with_pose_step_sync(
    device: str = "cpu",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingWithPoseStep:
    """ë™ê¸° ë²„ì „ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return create_human_parsing_with_pose_step(device, config, **kwargs)
