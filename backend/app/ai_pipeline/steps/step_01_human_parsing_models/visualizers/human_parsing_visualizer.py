"""
ğŸ”¥ Human Parsing ì‹œê°í™” ì‹œìŠ¤í…œ
================================

ì¸ì²´ íŒŒì‹± ê²°ê³¼ë¥¼ ìœ„í•œ ì™„ì „í•œ ì‹œê°í™” ê¸°ëŠ¥:
1. ì›ë³¸ ì´ë¯¸ì§€ì™€ ê²°ê³¼ ë¹„êµ
2. íŒŒì‹± ë§ˆìŠ¤í¬ ìƒ‰ìƒë³„ ì‹œê°í™”
3. ì „ì²˜ë¦¬ ê³¼ì • ì‹œê°í™”
4. ê²°ê³¼ í’ˆì§ˆ ë¶„ì„ ì‹œê°í™”
5. ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0 (ì™„ì „ êµ¬í˜„)
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.colors import ListedColormap
import seaborn as sns
from typing import Dict, Any, Optional, Tuple, List, Union
import logging
from PIL import Image
import torch
import os

logger = logging.getLogger(__name__)

class HumanParsingVisualizer:
    """ì¸ì²´ íŒŒì‹± ê²°ê³¼ ì‹œê°í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, save_dir: str = "./visualization_results"):
        self.save_dir = save_dir
        # ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
        os.makedirs(self.save_dir, exist_ok=True)
        self.logger = logging.getLogger(f"{__name__}.HumanParsingVisualizer")
        
        # ì¸ì²´ íŒŒì‹± í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ì •ì˜ (COCO-20 ê¸°ì¤€)
        self.parsing_colors = {
            0: [0, 0, 0],        # Background
            1: [255, 0, 0],      # Hat
            2: [255, 85, 0],     # Hair
            3: [255, 170, 0],    # Glove
            4: [255, 255, 0],    # Sunglasses
            5: [170, 255, 0],    # Upper-clothes
            6: [85, 255, 0],     # Dress
            7: [0, 255, 0],      # Coat
            8: [0, 255, 85],     # Socks
            9: [0, 255, 170],    # Pants
            10: [0, 255, 255],   # Jumpsuits
            11: [0, 170, 255],   # Scarf
            12: [0, 85, 255],    # Skirt
            13: [0, 0, 255],     # Face
            14: [85, 0, 255],    # Left-arm
            15: [170, 0, 255],   # Right-arm
            16: [255, 0, 255],   # Left-leg
            17: [255, 0, 170],   # Right-leg
            18: [255, 0, 85],    # Left-shoe
            19: [255, 0, 0]      # Right-shoe
        }
        
        # í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜
        self.class_names = {
            0: "Background", 1: "Hat", 2: "Hair", 3: "Glove", 4: "Sunglasses",
            5: "Upper-clothes", 6: "Dress", 7: "Coat", 8: "Socks", 9: "Pants",
            10: "Jumpsuits", 11: "Scarf", 12: "Skirt", 13: "Face", 14: "Left-arm",
            15: "Right-arm", 16: "Left-leg", 17: "Right-leg", 18: "Left-shoe", 19: "Right-shoe"
        }
        
        # ì‹œê°í™” í†µê³„
        self.visualization_stats = {
            'images_visualized': 0,
            'comparisons_created': 0,
            'masks_visualized': 0,
            'quality_analyses': 0
        }
    
    def visualize_preprocessing_pipeline(self, 
                                       original_image: np.ndarray,
                                       preprocessing_result: Dict[str, Any],
                                       save_path: Optional[str] = None) -> str:
        """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™”"""
        try:
            self.visualization_stats['images_visualized'] += 1
            self.logger.info("ğŸ”¥ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì‹œì‘")
            
            # ì´ë¯¸ì§€ ì¤€ë¹„
            processed_image = preprocessing_result['processed_image']
            crop_info = preprocessing_result['crop_info']
            
            # ì‹œê°í™” ìƒì„±
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Human Parsing Preprocessing Pipeline', fontsize=16, fontweight='bold')
            
            # 1. ì›ë³¸ ì´ë¯¸ì§€
            axes[0, 0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
            axes[0, 0].set_title('Original Image', fontweight='bold')
            axes[0, 0].axis('off')
            
            # ì¸ì²´ ê°ì§€ ë°•ìŠ¤ í‘œì‹œ
            if crop_info['human_detected']:
                bbox = crop_info['bbox']
                rect = patches.Rectangle(
                    (bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],
                    linewidth=3, edgecolor='red', facecolor='none'
                )
                axes[0, 0].add_patch(rect)
                axes[0, 0].text(bbox[0], bbox[1]-10, f"Human: {crop_info['confidence']:.2f}", 
                               color='red', fontweight='bold', fontsize=10)
            
            # 2. í¬ë¡­ëœ ì´ë¯¸ì§€
            axes[0, 1].imshow(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))
            axes[0, 1].set_title(f'Cropped & Resized ({crop_info["cropped_size"][1]}x{crop_info["cropped_size"][0]})', 
                                fontweight='bold')
            axes[0, 1].axis('off')
            
            # 3. ì „ì²˜ë¦¬ ì •ë³´
            info_text = f"""
Preprocessing Info:
â€¢ Target Size: {preprocessing_result['target_size']}
â€¢ Mode: {preprocessing_result['mode']}
â€¢ Human Detected: {crop_info['human_detected']}
â€¢ Confidence: {crop_info['confidence']:.3f}
â€¢ Original Size: {crop_info['original_size'][1]}x{crop_info['original_size'][0]}
â€¢ Cropped Size: {crop_info['cropped_size'][1]}x{crop_info['cropped_size'][0]}
            """
            axes[1, 0].text(0.1, 0.9, info_text, transform=axes[1, 0].transAxes, 
                           fontsize=10, verticalalignment='top', fontfamily='monospace',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
            axes[1, 0].set_title('Processing Information', fontweight='bold')
            axes[1, 0].axis('off')
            
            # 4. í’ˆì§ˆ í–¥ìƒ ë¹„êµ
            if preprocessing_result['mode'] == 'advanced':
                # ì›ë³¸ê³¼ í–¥ìƒëœ ì´ë¯¸ì§€ ë¹„êµ
                axes[1, 1].imshow(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))
                axes[1, 1].set_title('Enhanced Image (Advanced Mode)', fontweight='bold')
                axes[1, 1].axis('off')
            else:
                axes[1, 1].text(0.5, 0.5, 'Basic Mode\n(No Enhancement)', 
                               transform=axes[1, 1].transAxes, ha='center', va='center',
                               fontsize=12, fontweight='bold')
                axes[1, 1].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            if save_path is None:
                save_path = f"{self.save_dir}/preprocessing_pipeline_{self.visualization_stats['images_visualized']:03d}.png"
            
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì™„ë£Œ: {save_path}")
            return save_path
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return ""
    
    def visualize_parsing_result(self, 
                                original_image: np.ndarray,
                                parsing_mask: Union[np.ndarray, torch.Tensor],
                                confidence: float = 1.0,
                                save_path: Optional[str] = None) -> str:
        """íŒŒì‹± ê²°ê³¼ ì‹œê°í™”"""
        try:
            self.visualization_stats['masks_visualized'] += 1
            self.logger.info("ğŸ”¥ íŒŒì‹± ê²°ê³¼ ì‹œê°í™” ì‹œì‘")
            
            # í…ì„œë¥¼ NumPyë¡œ ë³€í™˜
            if isinstance(parsing_mask, torch.Tensor):
                parsing_mask = parsing_mask.detach().cpu().numpy()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if len(parsing_mask.shape) == 4:
                parsing_mask = parsing_mask[0]  # [B, C, H, W] -> [C, H, W]
            
            # í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            num_classes = parsing_mask.shape[0]
            colored_mask = self._create_colored_parsing_mask(parsing_mask)
            
            # ì‹œê°í™” ìƒì„±
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('Human Parsing Results', fontsize=16, fontweight='bold')
            
            # 1. ì›ë³¸ ì´ë¯¸ì§€
            axes[0, 0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
            axes[0, 0].set_title('Original Image', fontweight='bold')
            axes[0, 0].axis('off')
            
            # 2. íŒŒì‹± ë§ˆìŠ¤í¬ (ìƒ‰ìƒ)
            axes[0, 1].imshow(colored_mask)
            axes[0, 1].set_title(f'Parsing Mask (Confidence: {confidence:.3f})', fontweight='bold')
            axes[0, 1].axis('off')
            
            # 3. íŒŒì‹± ë§ˆìŠ¤í¬ (ì˜¤ë²„ë ˆì´)
            overlay = self._create_overlay(original_image, colored_mask, alpha=0.7)
            axes[0, 2].imshow(overlay)
            axes[0, 2].set_title('Overlay Result', fontweight='bold')
            axes[0, 2].axis('off')
            
            # 4. í´ë˜ìŠ¤ë³„ ë¶„í¬
            class_distribution = self._calculate_class_distribution(parsing_mask)
            axes[1, 0].bar(range(len(class_distribution)), list(class_distribution.values()))
            axes[1, 0].set_title('Class Distribution', fontweight='bold')
            axes[1, 0].set_xlabel('Class ID')
            axes[1, 0].set_ylabel('Pixel Count')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 5. í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ íˆíŠ¸ë§µ
            confidence_map = np.max(parsing_mask, axis=0)
            im = axes[1, 1].imshow(confidence_map, cmap='hot', interpolation='nearest')
            axes[1, 1].set_title('Confidence Heatmap', fontweight='bold')
            axes[1, 1].axis('off')
            plt.colorbar(im, ax=axes[1, 1])
            
            # 6. ì£¼ìš” í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬
            main_classes = [5, 9, 13, 14, 15]  # Upper-clothes, Pants, Face, Left-arm, Right-arm
            main_mask = np.zeros_like(parsing_mask[0])
            for class_id in main_classes:
                if class_id < num_classes:
                    main_mask = np.logical_or(main_mask, parsing_mask[class_id] > 0.5)
            
            axes[1, 2].imshow(main_mask, cmap='gray')
            axes[1, 2].set_title('Main Body Parts', fontweight='bold')
            axes[1, 2].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            if save_path is None:
                save_path = f"{self.save_dir}/parsing_result_{self.visualization_stats['masks_visualized']:03d}.png"
            
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… íŒŒì‹± ê²°ê³¼ ì‹œê°í™” ì™„ë£Œ: {save_path}")
            return save_path
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì‹± ê²°ê³¼ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return ""
    
    def _create_colored_parsing_mask(self, parsing_mask: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒì´ ìˆëŠ” íŒŒì‹± ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            H, W = parsing_mask.shape[1], parsing_mask.shape[2]
            colored_mask = np.zeros((H, W, 3), dtype=np.uint8)
            
            for class_id in range(parsing_mask.shape[0]):
                if class_id in self.parsing_colors:
                    mask = parsing_mask[class_id] > 0.5
                    color = np.array(self.parsing_colors[class_id])
                    colored_mask[mask] = color
            
            return colored_mask
            
        except Exception as e:
            self.logger.warning(f"ìƒ‰ìƒ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros((parsing_mask.shape[1], parsing_mask.shape[2], 3), dtype=np.uint8)
    
    def _create_overlay(self, image: np.ndarray, mask: np.ndarray, alpha: float = 0.7) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            if image.shape[:2] != mask.shape[:2]:
                mask = cv2.resize(mask, (image.shape[1], image.shape[0]))
            
            # ì˜¤ë²„ë ˆì´ ìƒì„±
            overlay = image.copy()
            mask_rgb = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)
            overlay = cv2.addWeighted(overlay, 1-alpha, mask_rgb, alpha, 0)
            
            return cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)
            
        except Exception as e:
            self.logger.warning(f"ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_class_distribution(self, parsing_mask: np.ndarray) -> Dict[int, int]:
        """í´ë˜ìŠ¤ë³„ í”½ì…€ ë¶„í¬ ê³„ì‚°"""
        try:
            distribution = {}
            for class_id in range(parsing_mask.shape[0]):
                if class_id in self.class_names:
                    pixel_count = np.sum(parsing_mask[class_id] > 0.5)
                    distribution[class_id] = pixel_count
            
            return distribution
            
        except Exception as e:
            self.logger.warning(f"í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def create_comparison_visualization(self, 
                                      original_image: np.ndarray,
                                      preprocessing_result: Dict[str, Any],
                                      parsing_result: Dict[str, Any],
                                      save_path: Optional[str] = None) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹œê°í™”"""
        try:
            self.visualization_stats['comparisons_created'] += 1
            self.logger.info("ğŸ”¥ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹œê°í™” ì‹œì‘")
            
            # ì‹œê°í™” ìƒì„±
            fig, axes = plt.subplots(2, 4, figsize=(20, 10))
            fig.suptitle('Complete Human Parsing Pipeline', fontsize=18, fontweight='bold')
            
            # 1. ì›ë³¸ ì´ë¯¸ì§€
            axes[0, 0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
            axes[0, 0].set_title('Original', fontweight='bold')
            axes[0, 0].axis('off')
            
            # 2. ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€
            processed_img = preprocessing_result['processed_image']
            axes[0, 1].imshow(cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB))
            axes[0, 1].set_title('Preprocessed', fontweight='bold')
            axes[0, 1].axis('off')
            
            # 3. íŒŒì‹± ë§ˆìŠ¤í¬
            if 'parsing_mask' in parsing_result:
                parsing_mask = parsing_result['parsing_mask']
                colored_mask = self._create_colored_parsing_mask(parsing_mask)
                axes[0, 2].imshow(colored_mask)
                axes[0, 2].set_title('Parsing Mask', fontweight='bold')
                axes[0, 2].axis('off')
            
            # 4. ìµœì¢… ê²°ê³¼ (ì˜¤ë²„ë ˆì´)
            if 'parsing_mask' in parsing_result:
                overlay = self._create_overlay(processed_img, colored_mask, alpha=0.6)
                axes[0, 3].imshow(overlay)
                axes[0, 3].set_title('Final Result', fontweight='bold')
                axes[0, 3].axis('off')
            
            # 5. ì²˜ë¦¬ ì •ë³´
            info_text = f"""
Processing Info:
â€¢ Target Size: {preprocessing_result['target_size']}
â€¢ Mode: {preprocessing_result['mode']}
â€¢ Human Detected: {preprocessing_result['crop_info']['human_detected']}
â€¢ Confidence: {preprocessing_result['crop_info']['confidence']:.3f}
            """
            axes[1, 0].text(0.1, 0.9, info_text, transform=axes[1, 0].transAxes, 
                           fontsize=9, verticalalignment='top', fontfamily='monospace',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            axes[1, 0].set_title('Processing Info', fontweight='bold')
            axes[1, 0].axis('off')
            
            # 6. í’ˆì§ˆ ë©”íŠ¸ë¦­
            if 'quality_metrics' in parsing_result:
                quality_text = f"""
Quality Metrics:
â€¢ Overall Score: {parsing_result['quality_metrics'].get('overall_score', 'N/A'):.3f}
â€¢ Boundary Quality: {parsing_result['quality_metrics'].get('boundary_quality', 'N/A'):.3f}
â€¢ Segmentation Quality: {parsing_result['quality_metrics'].get('segmentation_quality', 'N/A'):.3f}
                """
            else:
                quality_text = "Quality metrics not available"
            
            axes[1, 1].text(0.1, 0.9, quality_text, transform=axes[1, 1].transAxes, 
                           fontsize=9, verticalalignment='top', fontfamily='monospace',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
            axes[1, 1].set_title('Quality Metrics', fontweight='bold')
            axes[1, 1].axis('off')
            
            # 7. í´ë˜ìŠ¤ ë¶„í¬
            if 'parsing_mask' in parsing_result:
                class_dist = self._calculate_class_distribution(parsing_result['parsing_mask'])
                if class_dist:
                    axes[1, 2].bar(range(len(class_dist)), list(class_dist.values()))
                    axes[1, 2].set_title('Class Distribution', fontweight='bold')
                    axes[1, 2].set_xlabel('Class ID')
                    axes[1, 2].set_ylabel('Pixel Count')
                    axes[1, 2].tick_params(axis='x', rotation=45)
            
            # 8. ì²˜ë¦¬ í†µê³„
            stats_text = f"""
Processing Stats:
â€¢ Images Processed: {self.visualization_stats['images_visualized']}
â€¢ Masks Visualized: {self.visualization_stats['masks_visualized']}
â€¢ Comparisons Created: {self.visualization_stats['comparisons_created']}
â€¢ Quality Analyses: {self.visualization_stats['quality_analyses']}
            """
            axes[1, 3].text(0.1, 0.9, stats_text, transform=axes[1, 3].transAxes, 
                           fontsize=9, verticalalignment='top', fontfamily='monospace',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
            axes[1, 3].set_title('Processing Stats', fontweight='bold')
            axes[1, 3].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            if save_path is None:
                save_path = f"{self.save_dir}/complete_pipeline_{self.visualization_stats['comparisons_created']:03d}.png"
            
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹œê°í™” ì™„ë£Œ: {save_path}")
            return save_path
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return ""
    
    def get_visualization_stats(self) -> Dict[str, Any]:
        """ì‹œê°í™” í†µê³„ ë°˜í™˜"""
        return self.visualization_stats.copy()
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.visualization_stats = {
            'images_visualized': 0,
            'comparisons_created': 0,
            'masks_visualized': 0,
            'quality_analyses': 0
        }
