"""
ğŸ”¥ ì¶”ë¡  ê´€ë ¨ ë©”ì„œë“œë“¤ - ê¸°ì¡´ step.pyì˜ ëª¨ë“  ê¸°ëŠ¥ ë³µì› + inference_engines.py í†µí•©
"""
import logging
import time
from typing import Dict, Any, Optional, Tuple, List
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

logger = logging.getLogger(__name__)

class InferenceEngine:
    """ì¶”ë¡  ê´€ë ¨ ë©”ì„œë“œë“¤ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ - ê¸°ì¡´ step.pyì˜ ëª¨ë“  ê¸°ëŠ¥ ë³µì›"""
    
    def __init__(self, step_instance):
        self.step = step_instance
        self.logger = logging.getLogger(f"{__name__}.InferenceEngine")
    
    def run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ M3 Max ìµœì í™” ê³ ë„í™”ëœ AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì¶”ë¡  ì‹œìŠ¤í…œ"""
        self.logger.info("ğŸš€ M3 Max ìµœì í™” AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì‹œì‘")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = 'mps:0' if torch.backends.mps.is_available() else 'cpu'
        device_str = str(device)
        self.step.device = device
        self.step.device_str = device_str
        
        try:
            start_time = time.time()
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
            image = self.extract_input_image(input_data)
            if image is None:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            ensemble_results = {}
            model_confidences = {}
            
            # 3. ê° ëª¨ë¸ë³„ ì¶”ë¡  ì‹¤í–‰
            for model_name, model in self.step.loaded_models.items():
                try:
                    self.logger.info(f"ğŸ”¥ {model_name} ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    processed_input = self.preprocess_image_for_model(image, model_name)
                    
                    # ëª¨ë¸ë³„ ì•ˆì „ ì¶”ë¡  ì‹¤í–‰
                    if model_name == 'graphonomy':
                        result = self.run_graphonomy_safe_inference(processed_input, model, device_str)
                    elif model_name == 'hrnet':
                        result = self.run_hrnet_safe_inference(processed_input, model, device_str)
                    elif model_name == 'deeplabv3plus':
                        result = self.run_deeplabv3plus_safe_inference(processed_input, model, device_str)
                    elif model_name == 'u2net':
                        result = self.run_u2net_safe_inference(processed_input, model, device_str)
                    else:
                        result = self.run_generic_safe_inference(processed_input, model, device_str)
                    
                    # ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
                    if result and 'parsing_output' in result and result['parsing_output'] is not None:
                        ensemble_results[model_name] = result['parsing_output']
                        
                        # ì‹ ë¢°ë„ ê³„ì‚°
                        confidence = result.get('confidence', 0.8)
                        if isinstance(confidence, torch.Tensor):
                            confidence = self.step.utils.safe_tensor_to_scalar(confidence)
                        elif isinstance(confidence, (list, tuple)):
                            confidence = float(confidence[0]) if confidence else 0.8
                        else:
                            confidence = float(confidence)
                        
                        # NaN ê°’ ë°©ì§€
                        if not (confidence > 0 and confidence <= 1):
                            confidence = 0.8
                        
                        model_confidences[model_name] = confidence
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ (ì‹ ë¢°ë„: {confidence:.3f})")
                    else:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        continue
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    continue
            
            # 4. ì•™ìƒë¸” ìœµí•© ì‹¤í–‰
            if len(ensemble_results) >= 2:
                self.logger.info("ğŸ”¥ ê³ ê¸‰ ì•™ìƒë¸” ìœµí•© ì‹œìŠ¤í…œ ì‹¤í–‰")
                
                try:
                    # ëª¨ë¸ ì¶œë ¥ë“¤ì„ í…ì„œë¡œ ë³€í™˜
                    model_outputs_list = []
                    for model_name, output in ensemble_results.items():
                        if isinstance(output, dict):
                            if 'parsing_output' in output:
                                model_outputs_list.append(output['parsing_output'])
                            else:
                                # ì²« ë²ˆì§¸ í…ì„œ ê°’ ì°¾ê¸°
                                for key, value in output.items():
                                    if isinstance(value, torch.Tensor):
                                        model_outputs_list.append(value)
                                        break
                        else:
                            model_outputs_list.append(output)
                    
                    # ê° ëª¨ë¸ ì¶œë ¥ì˜ ì±„ë„ ìˆ˜ë¥¼ 20ê°œë¡œ í†µì¼
                    standardized_outputs = []
                    for output in model_outputs_list:
                        if hasattr(output, 'device') and str(output.device).startswith('mps'):
                            if output.dtype != torch.float32:
                                output = output.to(torch.float32)
                        else:
                            output = output.to(torch.float32)
                        
                        if output.shape[1] != 20:
                            if output.shape[1] > 20:
                                output = output[:, :20, :, :]
                            else:
                                # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš° íŒ¨ë”©
                                padding = torch.zeros(output.shape[0], 20 - output.shape[1], output.shape[2], output.shape[3], device=output.device)
                                output = torch.cat([output, padding], dim=1)
                        
                        standardized_outputs.append(output)
                    
                    # ì•™ìƒë¸” ìœµí•©
                    if standardized_outputs:
                        ensemble_output = torch.stack(standardized_outputs, dim=0)
                        ensemble_output = torch.mean(ensemble_output, dim=0, keepdim=True)
                        
                        # ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê· 
                        if model_confidences:
                            weights = torch.tensor([model_confidences.get(name, 0.5) for name in ensemble_results.keys()], device=ensemble_output.device)
                            weights = weights / weights.sum()
                            
                            weighted_output = torch.zeros_like(ensemble_output)
                            for i, (name, output) in enumerate(ensemble_results.items()):
                                if i < len(weights):
                                    weighted_output += weights[i] * standardized_outputs[i]
                            
                            final_output = weighted_output
                        else:
                            final_output = ensemble_output
                        
                        # ê²°ê³¼ ìƒì„±
                        result = {
                            'parsing_output': final_output,
                            'confidence': sum(model_confidences.values()) / len(model_confidences) if model_confidences else 0.8,
                            'model_used': 'ensemble',
                            'ensemble_results': ensemble_results,
                            'model_confidences': model_confidences
                        }
                    else:
                        raise ValueError("ì•™ìƒë¸” ì¶œë ¥ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì•™ìƒë¸” ìœµí•© ì‹¤íŒ¨: {e}")
                    # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ ì‚¬ìš©
                    if ensemble_results:
                        model_name = list(ensemble_results.keys())[0]
                        result = {
                            'parsing_output': ensemble_results[model_name],
                            'confidence': model_confidences.get(model_name, 0.8),
                            'model_used': model_name,
                            'ensemble_results': ensemble_results,
                            'model_confidences': model_confidences
                        }
                    else:
                        raise ValueError("ìœ íš¨í•œ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            elif len(ensemble_results) == 1:
                # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼
                model_name = list(ensemble_results.keys())[0]
                result = {
                    'parsing_output': ensemble_results[model_name],
                    'confidence': model_confidences.get(model_name, 0.8),
                    'model_used': model_name,
                    'ensemble_results': ensemble_results,
                    'model_confidences': model_confidences
                }
            else:
                raise ValueError("ìœ íš¨í•œ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 5. í›„ì²˜ë¦¬
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            result['success'] = True
            
            self.logger.info(f"âœ… AI ì¶”ë¡  ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(str(e))
    
    def extract_input_image(self, input_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        return self.step.utils.extract_input_image(input_data)
    
    def preprocess_image_for_model(self, image: np.ndarray, model_name: str) -> torch.Tensor:
        """ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        return self.step.utils.preprocess_image_for_model(image, model_name)
    
    def run_graphonomy_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
        """ğŸ”¥ Enhanced Graphonomy ëª¨ë¸ ì•ˆì „ ì¶”ë¡  - ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©"""
        try:
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model = self.prepare_model_for_inference(model, device)
            model.eval()
            
            with torch.no_grad():
                # ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©ëœ ì¶œë ¥ ì²˜ë¦¬
                output = model(input_tensor)
                
                # ìƒˆë¡œìš´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬
                if isinstance(output, dict):
                    # ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ì˜ ì¶œë ¥ ì²˜ë¦¬
                    if 'parsing' in output:
                        parsing_output = output['parsing']
                    elif 'parsing_pred' in output:
                        parsing_output = output['parsing_pred']
                    else:
                        parsing_output = output.get('final_predictions', None)
                    
                    # ê²½ê³„ ë§µ ì²˜ë¦¬
                    boundary_maps = output.get('boundary_maps', None)
                    
                    # ì •ì œ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
                    refinement_history = output.get('refinement_history', None)
                    
                    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì²˜ë¦¬
                    attention_weights = output.get('attention_weights', None)
                    
                    # FPN íŠ¹ì§• ì²˜ë¦¬
                    fpn_features = output.get('fpn_features', None)
                    
                    # ìœµí•© íŠ¹ì§• ì²˜ë¦¬
                    fused_features = output.get('fused_features', None)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚° (ìƒˆë¡œìš´ êµ¬ì¡° ê¸°ë°˜)
                    confidence = self._calculate_enhanced_confidence(
                        parsing_output, boundary_maps, refinement_history
                    )
                    
                    return {
                        'parsing_output': parsing_output,
                        'confidence': confidence,
                        'boundary_maps': boundary_maps,
                        'refinement_history': refinement_history,
                        'attention_weights': attention_weights,
                        'fpn_features': fpn_features,
                        'fused_features': fused_features,
                        'model_type': 'enhanced_graphonomy'
                    }
                else:
                    # ê¸°ì¡´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
                    return self._extract_parsing_from_output(output, device)
                    
        except Exception as e:
            self.logger.error(f"Enhanced Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(f"Enhanced Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def run_hrnet_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
        """HRNet ì•ˆì „ ì¶”ë¡ """
        try:
            model = self.prepare_model_for_inference(model, device)
            model.eval()
            
            with torch.no_grad():
                output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, dict):
                    parsing_output = output.get('parsing_pred', output.get('parsing_output', output.get('output')))
                    confidence = output.get('confidence', 0.8)
                else:
                    parsing_output = output
                    confidence = 0.8
                
                # í…ì„œ ì •ê·œí™”
                if isinstance(parsing_output, torch.Tensor):
                    if parsing_output.dim() == 4:
                        parsing_output = parsing_output.squeeze(0)
                    
                    # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
                    if parsing_output.shape[0] > 1:
                        parsing_output = F.softmax(parsing_output, dim=0)
                
                return {
                    'parsing_output': parsing_output,
                    'confidence': confidence,
                    'model_used': 'hrnet'
                }
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(f"HRNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def run_deeplabv3plus_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
        """ğŸ”¥ Enhanced DeepLabV3+ ëª¨ë¸ ì•ˆì „ ì¶”ë¡  - ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©"""
        try:
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model = self.prepare_model_for_inference(model, device)
            model.eval()
            
            with torch.no_grad():
                # ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©ëœ ì¶œë ¥ ì²˜ë¦¬
                output = model(input_tensor)
                
                # ìƒˆë¡œìš´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬
                if isinstance(output, dict):
                    # ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ì˜ ì¶œë ¥ ì²˜ë¦¬
                    if 'parsing' in output:
                        parsing_output = output['parsing']
                    elif 'parsing_pred' in output:
                        parsing_output = output['parsing_pred']
                    else:
                        parsing_output = output.get('final_predictions', None)
                    
                    # ê²½ê³„ ë§µ ì²˜ë¦¬
                    boundary_maps = output.get('boundary_maps', None)
                    
                    # ì •ì œ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
                    refinement_history = output.get('refinement_history', None)
                    
                    # FPN íŠ¹ì§• ì²˜ë¦¬
                    fpn_features = output.get('fpn_features', None)
                    
                    # ë°±ë³¸ íŠ¹ì§• ì²˜ë¦¬
                    backbone_features = output.get('backbone_features', None)
                    
                    # ASPP íŠ¹ì§• ì²˜ë¦¬
                    aspp_features = output.get('aspp_features', None)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚° (ìƒˆë¡œìš´ êµ¬ì¡° ê¸°ë°˜)
                    confidence = self._calculate_enhanced_confidence(
                        parsing_output, boundary_maps, refinement_history
                    )
                    
                    return {
                        'parsing_output': parsing_output,
                        'confidence': confidence,
                        'boundary_maps': boundary_maps,
                        'refinement_history': refinement_history,
                        'fpn_features': fpn_features,
                        'backbone_features': backbone_features,
                        'aspp_features': aspp_features,
                        'model_type': 'enhanced_deeplabv3plus'
                    }
                else:
                    # ê¸°ì¡´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
                    return self._extract_parsing_from_output(output, device)
                    
        except Exception as e:
            self.logger.error(f"Enhanced DeepLabV3+ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(f"Enhanced DeepLabV3+ ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def run_u2net_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
        """ğŸ”¥ Enhanced U2Net ëª¨ë¸ ì•ˆì „ ì¶”ë¡  - ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©"""
        try:
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model = self.prepare_model_for_inference(model, device)
            model.eval()
            
            with torch.no_grad():
                # ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ê³¼ í†µí•©ëœ ì¶œë ¥ ì²˜ë¦¬
                output = model(input_tensor)
                
                # ìƒˆë¡œìš´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬
                if isinstance(output, dict):
                    # ğŸ”¥ ì™„ì „í•œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°ì˜ ì¶œë ¥ ì²˜ë¦¬
                    if 'parsing' in output:
                        parsing_output = output['parsing']
                    elif 'parsing_pred' in output:
                        parsing_output = output['parsing_pred']
                    else:
                        parsing_output = output.get('final_predictions', None)
                    
                    # ê²½ê³„ ë§µ ì²˜ë¦¬
                    boundary_maps = output.get('boundary_maps', None)
                    
                    # ì •ì œ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
                    refinement_history = output.get('refinement_history', None)
                    
                    # FPN íŠ¹ì§• ì²˜ë¦¬
                    fpn_features = output.get('fpn_features', None)
                    
                    # ì¸ì½”ë”/ë””ì½”ë” íŠ¹ì§• ì²˜ë¦¬
                    encoder_features = output.get('encoder_features', None)
                    decoder_features = output.get('decoder_features', None)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚° (ìƒˆë¡œìš´ êµ¬ì¡° ê¸°ë°˜)
                    confidence = self._calculate_enhanced_confidence(
                        parsing_output, boundary_maps, refinement_history
                    )
                    
                    return {
                        'parsing_output': parsing_output,
                        'confidence': confidence,
                        'boundary_maps': boundary_maps,
                        'refinement_history': refinement_history,
                        'fpn_features': fpn_features,
                        'encoder_features': encoder_features,
                        'decoder_features': decoder_features,
                        'model_type': 'enhanced_u2net'
                    }
                else:
                    # ê¸°ì¡´ ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
                    return self._extract_parsing_from_output(output, device)
                    
        except Exception as e:
            self.logger.error(f"Enhanced U2Net ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(f"Enhanced U2Net ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def run_generic_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
        """ì¼ë°˜ ëª¨ë¸ ì•ˆì „ ì¶”ë¡ """
        try:
            model = self.prepare_model_for_inference(model, device)
            model.eval()
            
            with torch.no_grad():
                output = model(input_tensor)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, dict):
                    parsing_output = output.get('parsing_pred', output.get('parsing_output', output.get('output')))
                    confidence = output.get('confidence', 0.8)
                else:
                    parsing_output = output
                    confidence = 0.8
                
                # í…ì„œ ì •ê·œí™”
                if isinstance(parsing_output, torch.Tensor):
                    if parsing_output.dim() == 4:
                        parsing_output = parsing_output.squeeze(0)
                    
                    # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
                    if parsing_output.shape[0] > 1:
                        parsing_output = F.softmax(parsing_output, dim=0)
                
                return {
                    'parsing_output': parsing_output,
                    'confidence': confidence,
                    'model_used': 'generic'
                }
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¼ë°˜ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self.create_error_response(f"ì¼ë°˜ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def prepare_model_for_inference(self, model: nn.Module, device_str: str) -> nn.Module:
        """ì¶”ë¡ ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„"""
        try:
            if not isinstance(model, nn.Module):
                self.logger.warning("âš ï¸ ëª¨ë¸ì´ nn.Moduleì´ ì•„ë‹™ë‹ˆë‹¤")
                return model
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            if device_str.startswith('mps') and torch.backends.mps.is_available():
                model = model.to('mps')
            elif device_str.startswith('cuda') and torch.cuda.is_available():
                model = model.to('cuda')
            else:
                model = model.to('cpu')
            
            return model
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return model
    
    def create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'parsing_output': torch.zeros((20, 512, 512)),
            'confidence': 0.0,
            'model_used': 'none',
            'processing_time': 0.0
        }
    
    # ğŸ”¥ inference_engines.pyì—ì„œ ì¶”ê°€ëœ ë©”ì„œë“œë“¤
    
    def _safe_tensor_to_scalar(self, tensor_value):
        """í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        if isinstance(tensor_value, torch.Tensor):
            if tensor_value.numel() == 1:
                return tensor_value.item()
            else:
                return tensor_value.mean().item()
        elif isinstance(tensor_value, (int, float)):
            return float(tensor_value)
        else:
            return 0.8  # ê¸°ë³¸ê°’
    
    def _extract_actual_model(self, model) -> Optional[nn.Module]:
        """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)"""
        try:
            if hasattr(model, 'model_instance') and model.model_instance is not None:
                return model.model_instance
            elif hasattr(model, 'get_model_instance'):
                return model.get_model_instance()
            elif callable(model):
                return model
            else:
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_standard_output(self, device) -> Dict[str, Any]:
        """í‘œì¤€ ì¶œë ¥ ìƒì„±"""
        return {
            'parsing_pred': torch.zeros((1, 20, 512, 512), device=device),
            'parsing_output': torch.zeros((1, 20, 512, 512), device=device),
            'confidence': 0.5,
            'edge_output': None
        }
    
    def _extract_parsing_from_output(self, output, device) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ íŒŒì‹± ê²°ê³¼ ì¶”ì¶œ"""
        try:
            if output is None:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶œë ¥ì´ Noneì…ë‹ˆë‹¤.")
                return torch.zeros((1, 20, 512, 512), device=device), None
            
            if isinstance(output, dict):
                parsing_keys = ['parsing', 'parsing_pred', 'output', 'parsing_output', 'logits', 'pred', 'prediction']
                parsing_tensor = None
                confidence_tensor = None
                
                for key in parsing_keys:
                    if key in output and output[key] is not None:
                        if isinstance(output[key], torch.Tensor):
                            parsing_tensor = output[key]
                            break
                        elif isinstance(output[key], (list, tuple)) and len(output[key]) > 0:
                            if isinstance(output[key][0], torch.Tensor):
                                parsing_tensor = output[key][0]
                                break
                
                return parsing_tensor, confidence_tensor
            else:
                return output, None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ íŒŒì‹± ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 20, 512, 512), device=device), None
    
    def _standardize_channels(self, tensor: torch.Tensor, target_channels: int = 20) -> torch.Tensor:
        """í…ì„œ ì±„ë„ ìˆ˜ í‘œì¤€í™”"""
        try:
            if tensor.dim() == 3:
                current_channels = tensor.shape[0]
            elif tensor.dim() == 4:
                current_channels = tensor.shape[1]
            else:
                return tensor
            
            if current_channels == target_channels:
                return tensor
            elif current_channels > target_channels:
                if tensor.dim() == 3:
                    return tensor[:target_channels]
                else:
                    return tensor[:, :target_channels]
            else:
                # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš° íŒ¨ë”©
                if tensor.dim() == 3:
                    padding = torch.zeros(target_channels - current_channels, tensor.shape[1], tensor.shape[2], device=tensor.device)
                    return torch.cat([tensor, padding], dim=0)
                else:
                    padding = torch.zeros(tensor.shape[0], target_channels - current_channels, tensor.shape[2], tensor.shape[3], device=tensor.device)
                    return torch.cat([tensor, padding], dim=1)
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì±„ë„ ìˆ˜ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return tensor

    def _calculate_enhanced_confidence(self, parsing_output, boundary_maps, refinement_history):
        """ğŸ”¥ ìƒˆë¡œ êµ¬í˜„í•œ ê³ ê¸‰ ëª¨ë“ˆë“¤ì˜ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°"""
        try:
            if parsing_output is None:
                return 0.8
            
            # ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°
            if isinstance(parsing_output, torch.Tensor):
                # í…ì„œì˜ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                with torch.no_grad():
                    # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
                    if parsing_output.dim() > 1:
                        probs = F.softmax(parsing_output, dim=1)
                        # ìµœëŒ€ í™•ë¥ ê°’ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
                        confidence = torch.max(probs).item()
                    else:
                        confidence = torch.sigmoid(parsing_output).mean().item()
            else:
                confidence = 0.8
            
            # ê²½ê³„ ë§µì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ í–¥ìƒ
            if boundary_maps is not None:
                confidence = min(confidence * 1.1, 0.95)
            
            # ì •ì œ íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ í–¥ìƒ
            if refinement_history is not None:
                confidence = min(confidence * 1.05, 0.95)
            
            # NaN ê°’ ë°©ì§€
            if not (confidence > 0 and confidence <= 1):
                confidence = 0.8
            
            return confidence
            
        except Exception as e:
            self.logger.warning(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8
