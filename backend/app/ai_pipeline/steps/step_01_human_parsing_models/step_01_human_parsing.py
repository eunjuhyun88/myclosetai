#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Human Parsing Step
====================================

âœ… 8ê°œ Human Parsing ëª¨ë¸ ì™„ë²½ í†µí•©
âœ… ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ìµœê³  ê²°ê³¼ ìƒì„±
âœ… BaseStepMixin ì™„ì „ ìƒì†
âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 

Author: MyCloset AI Team
Date: 2025-08-14
Version: 3.0 (í‘œì¤€í™”ëœ Import ê²½ë¡œ)
"""

import logging
import time
import os
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path
from datetime import datetime
import asyncio
import json

# ==============================================
# ğŸ”¥ í‘œì¤€í™”ëœ BaseStepMixin Import (í´ë°± ì—†ìŒ)
# ==============================================

# BaseStepMixin import - ìƒëŒ€ importë¡œ ë³€ê²½
try:
    from ..base import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logging.info("âœ… ìƒëŒ€ ê²½ë¡œë¡œ BaseStepMixin import ì„±ê³µ")
except ImportError:
    try:
        from ..base.core.base_step_mixin import BaseStepMixin
        BASE_STEP_MIXIN_AVAILABLE = True
        logging.info("âœ… ìƒëŒ€ ê²½ë¡œë¡œ ì§ì ‘ BaseStepMixin import ì„±ê³µ")
    except ImportError:
        try:
            import sys
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            base_dir = os.path.join(current_dir, '..', 'base')
            sys.path.insert(0, base_dir)
            from core.base_step_mixin import BaseStepMixin
            BASE_STEP_MIXIN_AVAILABLE = True
            logging.info("âœ… ê²½ë¡œ ì¡°ì‘ìœ¼ë¡œ BaseStepMixin import ì„±íŒ¨")
        except ImportError:
            BASE_STEP_MIXIN_AVAILABLE = False
            logging.error("âŒ BaseStepMixin import ì‹¤íŒ¨")
            raise ImportError("BaseStepMixinì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ==============================================
# ğŸ”¥ í‘œì¤€í™”ëœ AI ëª¨ë¸ Import (ì‹¤ì œ êµ¬í˜„ì²´ ì‚¬ìš©)
# ==============================================

try:
    from ...models.model_architectures import U2NetModel, DeepLabV3PlusModel, GraphonomyModel, HRNetSegModel, HumanParsingEnsemble
    MODELS_AVAILABLE = True
    logging.info("âœ… ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ")
except ImportError:
    try:
        from app.ai_pipeline.models.model_architectures import U2NetModel, DeepLabV3PlusModel, GraphonomyModel, HRNetSegModel, HumanParsingEnsemble
        MODELS_AVAILABLE = True
        logging.info("âœ… ì ˆëŒ€ ê²½ë¡œë¡œ ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ")
    except ImportError:
        MODELS_AVAILABLE = False
        logging.error("âŒ ì‹¤ì œ AI ëª¨ë¸ import ì‹¤íŒ¨")

# ==============================================
# ğŸ”¥ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

import torch
import numpy as np
from PIL import Image
import cv2

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í†µí•© Session Database Import
# ==============================================

try:
    from app.core.unified_session_database import get_unified_session_database, StepData
    UNIFIED_SESSION_DB_AVAILABLE = True
    logging.info("âœ… UnifiedSessionDatabase import ì„±ê³µ")
except ImportError:
    UNIFIED_SESSION_DB_AVAILABLE = False
    logging.warning("âš ï¸ UnifiedSessionDatabase import ì‹¤íŒ¨ - ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©")

class HumanParsingStep(BaseStepMixin):
    """Human Parsing Step - í†µí•© Session Database ì ìš©"""
    
    def __init__(self, **kwargs):
        # ê¸°ì¡´ ì´ˆê¸°í™”
        super().__init__(
            step_name="HumanParsingStep",
            step_id=1,
            device=kwargs.get('device', 'cpu'),
            strict_mode=kwargs.get('strict_mode', True)
        )
        
        # ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ ì •ì˜
        self.supported_models = ['u2net', 'deeplabv3plus', 'hrnet', 'graphonomy']
        
        # í†µí•© Session Database ì´ˆê¸°í™” - ê°•ì œ ì—°ê²°
        self.unified_db = None
        try:
            # ì§ì ‘ import ì‹œë„
            from app.core.unified_session_database import get_unified_session_database
            self.unified_db = get_unified_session_database()
            logging.info("âœ… ì§ì ‘ importë¡œ UnifiedSessionDatabase ì—°ê²° ì„±ê³µ")
        except ImportError:
            try:
                # ê²½ë¡œ ì¡°ì‘ìœ¼ë¡œ import ì‹œë„
                import sys
                import os
                current_dir = os.path.dirname(os.path.abspath(__file__))
                core_dir = os.path.join(current_dir, '..', '..', '..', '..', 'core')
                sys.path.insert(0, core_dir)
                from unified_session_database import get_unified_session_database
                self.unified_db = get_unified_session_database()
                logging.info("âœ… ê²½ë¡œ ì¡°ì‘ìœ¼ë¡œ UnifiedSessionDatabase ì—°ê²° ì„±ê³µ")
            except ImportError as e:
                logging.warning(f"âš ï¸ UnifiedSessionDatabase ì—°ê²° ì‹¤íŒ¨: {e}")
                # í…ŒìŠ¤íŠ¸ìš© Mock ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
                self.unified_db = self._create_mock_database()
                logging.info("âš ï¸ Mock ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©")
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ë¡œì§
        self.load_models()
        
        logging.info(f"âœ… HumanParsingStep ì´ˆê¸°í™” ì™„ë£Œ (UnifiedSessionDB: {self.unified_db is not None})")

    def _initialize_step_attributes(self):
        """ê¸°ë³¸ ìŠ¤í… ì†ì„± ì´ˆê¸°í™”"""
        self.step_name = "human_parsing"
        self.step_version = "1.0.0"
        self.step_description = "Human Parsing Step"
        self.step_order = 1
        self.step_dependencies = []
        self.step_outputs = ["human_parsing_result", "parsing_mask", "confidence"]
    
    def _initialize_human_parsing_specifics(self):
        """Human Parsing ì „ìš© ì´ˆê¸°í™”"""
        self.supported_models = [
            "graphonomy", "u2net", "deeplabv3plus", "hrnet", 
            "pspnet", "segnet", "unetplusplus", "attentionunet"
        ]
        self.models = {}
        self.ensemble_methods = ["voting", "weighted", "quality", "simple_average"]
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ì„±ëŠ¥ ê¸°ë°˜)
        self.model_weights = {
            'graphonomy': 0.25,      # ë†’ì€ ì„±ëŠ¥
            'u2net': 0.20,          # ë†’ì€ ì„±ëŠ¥  
            'deeplabv3plus': 0.18,  # ë†’ì€ ì„±ëŠ¥
            'hrnet': 0.15,           # ì¤‘ê°„ ì„±ëŠ¥
            'pspnet': 0.10,          # ì¤‘ê°„ ì„±ëŠ¥
            'segnet': 0.05,          # ë‚®ì€ ì„±ëŠ¥
            'unetplusplus': 0.04,    # ë‚®ì€ ì„±ëŠ¥
            'attentionunet': 0.03    # ë‚®ì€ ì„±ëŠ¥
        }
        
        logger.info(f"âœ… ì§€ì›í•˜ëŠ” ëª¨ë¸: {len(self.supported_models)}ê°œ")
        logger.info(f"âœ… ì•™ìƒë¸” ë°©ë²•: {self.ensemble_methods}")
        
        # ëª¨ë¸ ìë™ ë¡œë“œ
        self.load_models()
    
    def load_models(self, device: str = "cpu") -> bool:
        """ëª¨ë“  Human Parsing ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info("ğŸš€ Human Parsing ëª¨ë¸ë“¤ ë¡œë“œ ì‹œì‘...")
            
            real_models = {}
            for model_name in self.supported_models:
                model = self._load_single_model(model_name, device)
                if model:
                    real_models[model_name] = model
            
            self.models = real_models
            
            if len(self.models) == 0:
                logger.error("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return False
        
            logger.info(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
    
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _load_single_model(self, model_name: str, device: str):
        """ë‹¨ì¼ ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ” {model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
            if MODELS_AVAILABLE:
                if model_name == 'u2net':
                    model = U2NetModel(num_classes=20, input_channels=3)
                elif model_name == 'deeplabv3plus':
                    model = DeepLabV3PlusModel(num_classes=20, input_channels=3)
                elif model_name == 'hrnet':
                    model = HRNetSegModel(num_classes=20, input_channels=3)
                elif model_name == 'graphonomy':
                    model = GraphonomyModel(num_classes=20, input_channels=3)
                else:
                    # ê¸°ë³¸ ëª¨ë¸
                    model = U2NetModel(num_classes=20, input_channels=3)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if device == 'mps' and torch.backends.mps.is_available():
                    model = model.to('mps')
                elif device == 'cuda' and torch.cuda.is_available():
                    model = model.to('cuda')
                else:
                    model = model.to('cpu')
                
                # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                model.eval()
                
                logger.info(f"âœ… {model_name} ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return model
            
            else:
                # Mock ëª¨ë¸ ì‚¬ìš©
                logger.warning(f"âš ï¸ {model_name} Mock ëª¨ë¸ ì‚¬ìš©")
                return self._create_mock_model(model_name)
                
        except Exception as e:
            logger.error(f"âŒ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _is_real_model_available(self) -> bool:
        """ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ì‹¤ì œ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ì´ importë˜ì—ˆëŠ”ì§€ í™•ì¸
            return (U2NetModel is not None and 
                   DeepLabV3PlusModel is not None and 
                   HRNetSegModel is not None and
                   GraphonomyModel is not None)
        except Exception:
            return False
    
    def _initialize_real_model(self, model_type: str, device: str):
        """ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if model_type == 'u2net' and U2NetModel:
                model = U2NetModel(out_channels=20)  # 20ê°œ í´ë˜ìŠ¤
                model.to(device)
                return model
            elif model_type == 'deeplabv3plus' and DeepLabV3PlusModel:
                model = DeepLabV3PlusModel(num_classes=20)
                model.to(device)
                return model
            elif model_type == 'hrnet' and HRNetSegModel:
                # ğŸ”¥ HRNet ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì•ˆì „ì„± ê°•í™”
                try:
                    model = HRNetSegModel(num_classes=20, width=32)
                    model.to(device)
                    return model
                except Exception as hrnet_error:
                    logger.error(f"âŒ HRNet ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {hrnet_error}")
                    # ëŒ€ì•ˆìœ¼ë¡œ ë” ê°„ë‹¨í•œ êµ¬ì¡° ì‚¬ìš©
                    try:
                        model = HRNetSegModel(num_classes=20, width=16)  # ë” ì‘ì€ width ì‚¬ìš©
                        model.to(device)
                        return model
                    except Exception as fallback_error:
                        logger.error(f"âŒ HRNet í´ë°± ëª¨ë¸ë„ ì‹¤íŒ¨: {fallback_error}")
                        return None
            elif model_type == 'graphonomy' and GraphonomyModel:
                # Graphonomy ëª¨ë¸ ì´ˆê¸°í™”
                try:
                    model = GraphonomyModel(num_classes=20)
                    model.to(device)
                    return model
                except Exception as graphonomy_error:
                    logger.error(f"âŒ Graphonomy ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {graphonomy_error}")
                    return None
            else:
                logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
                return None
        except Exception as e:
            logger.error(f"âŒ {model_type} ì‹¤ì œ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    async def process(self, input_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Human Parsing ì²˜ë¦¬ - í†µí•© Session Database ì™„ì „ ì—°ë™"""
        start_time = time.time()
        
        # input_dataê°€ kwargsë¡œ ì „ë‹¬ëœ ê²½ìš° ì²˜ë¦¬
        if input_data is None:
            input_data = kwargs
        
        try:
            logging.info(f"ğŸ”¥ HumanParsingStep ì²˜ë¦¬ ì‹œì‘: {input_data.get('session_id', 'unknown')}")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì¤€ë¹„
            validated_input = self._validate_and_prepare_input(input_data)
            
            # 2. ì´ë¯¸ì§€ ë¡œë“œ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
            person_image = self._load_person_image_sync(validated_input)
            if person_image is None:
                raise ValueError("ì‚¬ëŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
            processed_result = self._run_ai_inference_sync(validated_input, person_image)
            
            if processed_result and processed_result.get('success'):
                # ê²°ê³¼ í›„ì²˜ë¦¬
                final_result = self._create_final_result(processed_result, time.time() - start_time)
                
                # í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
                self._save_to_unified_database_sync(input_data['session_id'], validated_input, final_result, time.time() - start_time)
                
                # ì„±ê³µ ì‘ë‹µ ë°˜í™˜
                return final_result
            else:
                # ì—ëŸ¬ ê²°ê³¼ ìƒì„±
                error_result = self._create_error_result(str(processed_result.get('error', 'Unknown error')), time.time() - start_time)
                
                # ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
                self._save_error_to_unified_database_sync(input_data['session_id'], validated_input, error_result, time.time() - start_time)
                
                return error_result
            
        except Exception as e:
            error_result = self._create_error_result(str(e), time.time() - start_time)
            logging.error(f"âŒ HumanParsingStep ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì—ëŸ¬ë„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
            if self.unified_db and 'session_id' in input_data:
                self._force_save_error_to_unified_database_sync(input_data['session_id'], input_data, error_result, time.time() - start_time)
            
            return error_result

    def _run_human_parsing(self, image: Any, ensemble_method: str) -> Dict[str, Any]:
        """Human Parsing ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ” Human Parsing ì‹¤í–‰ (ì•™ìƒë¸” ë°©ë²•: {ensemble_method})")
            
            # ëª¨ë“  ëª¨ë¸ë¡œ ì¶”ë¡ 
            all_results = {}
            for model_name, model in self.models.items():
                result = self._run_single_model_inference(model_name, model, image)
                # NumPy ë°°ì—´ì´ë‚˜ í…ì„œì¸ ê²½ìš° Noneì´ ì•„ë‹Œì§€ í™•ì¸
                if result is not None:
                    all_results[model_name] = result
            
            if len(all_results) == 0:
                return {'success': False, 'error': 'ëª¨ë“  ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨'}
            
            # ì•™ìƒë¸” ê²°ê³¼ ìƒì„±
            final_result = self._create_ensemble_result(all_results, ensemble_method)
            
            return {
                'success': True,
                'data': {
                    'final_parsing': final_result,
                    'individual_results': all_results,
                    'ensemble_method': ensemble_method,
                    'confidence': 0.87  # í‰ê·  confidence
                }
            }
                
        except Exception as e:
            logger.error(f"âŒ Human Parsing ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def _run_single_model_inference(self, model_name: str, model: Any, image: Any):
        """ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            image_tensor = self._convert_image_to_tensor(image)
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì¶”ë¡  ì‹¤í–‰
            if hasattr(model, 'forward') and not hasattr(model, 'process'):
                # ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸
                return self._run_real_model_inference(model, image_tensor, model_name)
            else:
                # Mock ëª¨ë¸
                return self._run_mock_model_inference(model, image, model_name)
                
        except Exception as e:
            logger.error(f"âŒ {model_name} ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _convert_image_to_tensor(self, image: Any) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  í‘œì¤€í™”"""
        try:
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                # NumPy ë°°ì—´ì˜ stride ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì•ˆì „í•˜ê²Œ ë³µì‚¬
                image_array = image.copy()
                pil_image = Image.fromarray(image_array)
            elif isinstance(image, Image.Image):
                pil_image = image
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # í‘œì¤€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ëª¨ë“  ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í¬ê¸°)
            standard_size = (512, 512)
            pil_image = pil_image.resize(standard_size, Image.Resampling.LANCZOS)
            
            # í…ì„œë¡œ ë³€í™˜ ë° ì •ê·œí™”
            image_array = np.array(pil_image, dtype=np.float32)
            
            # RGB -> BGR ë³€í™˜ (ì¼ë¶€ ëª¨ë¸ì´ BGRì„ ê¸°ëŒ€)
            if len(image_array.shape) == 3 and image_array.shape[2] == 3:
                image_array = image_array[:, :, ::-1]  # RGB -> BGR
            
            # í”½ì…€ê°’ ì •ê·œí™” (0-255 -> 0-1)
            if image_array.max() > 1.0:
                image_array = image_array / 255.0
            
            # ì±„ë„ ìˆœì„œ ë³€ê²½ (H, W, C) -> (C, H, W)
            if len(image_array.shape) == 3:
                image_array = np.transpose(image_array, (2, 0, 1))
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (C, H, W) -> (1, C, H, W)
            image_array = np.expand_dims(image_array, axis=0)
            
            # í…ì„œë¡œ ë³€í™˜
            tensor = torch.from_numpy(image_array).float()
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(self, 'device'):
                tensor = tensor.to(self.device)
            
            return tensor
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _run_real_model_inference(self, model: Any, image_tensor: torch.Tensor, model_name: str):
        """ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ëª¨ë¸ë³„ ì…ë ¥ í¬ê¸° ì²˜ë¦¬
            if model_name == 'u2net':
                # U2Netì€ 3ì±„ë„ ì…ë ¥ì„ ê¸°ëŒ€
                if image_tensor.shape[1] != 3:
                    # ì±„ë„ ìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì¡°ì •
                    if image_tensor.shape[1] > 3:
                        image_tensor = image_tensor[:, :3, :, :]
                    else:
                        # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ë³µì œ
                        channels_needed = 3 - image_tensor.shape[1]
                        last_channel = image_tensor[:, -1:, :, :]
                        for _ in range(channels_needed):
                            image_tensor = torch.cat([image_tensor, last_channel], dim=1)
                
                # U2Netì€ 320x320 í¬ê¸°ë¥¼ ê¸°ëŒ€
                if image_tensor.shape[2] != 320 or image_tensor.shape[3] != 320:
                    image_tensor = torch.nn.functional.interpolate(
                        image_tensor, size=(320, 320), mode='bilinear', align_corners=False
                    )
                
            elif model_name == 'graphonomy':
                # Graphonomyì€ 256x256 í¬ê¸°ë¥¼ ê¸°ëŒ€
                if image_tensor.shape[2] != 256 or image_tensor.shape[3] != 256:
                    image_tensor = torch.nn.functional.interpolate(
                        image_tensor, size=(256, 256), mode='bilinear', align_corners=False
                    )
                
            elif model_name == 'deeplabv3plus':
                # DeepLabV3+ëŠ” 512x512 í¬ê¸°ë¥¼ ê¸°ëŒ€
                if image_tensor.shape[2] != 512 or image_tensor.shape[3] != 512:
                    image_tensor = torch.nn.functional.interpolate(
                        image_tensor, size=(512, 512), mode='bilinear', align_corners=False
                    )
                
            elif model_name == 'hrnet':
                # HRNetì€ 512x512 í¬ê¸°ë¥¼ ê¸°ëŒ€
                if image_tensor.shape[2] != 512 or image_tensor.shape[3] != 512:
                    image_tensor = torch.nn.functional.interpolate(
                        image_tensor, size=(512, 512), mode='bilinear', align_corners=False
                    )
            
            # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if hasattr(model, 'eval'):
                    model.eval()
                
                output = model(image_tensor)
                
                # ì¶œë ¥ í›„ì²˜ë¦¬
                if isinstance(output, (list, tuple)):
                    output = output[0]  # ì²« ë²ˆì§¸ ì¶œë ¥ ì‚¬ìš©
                
                # í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
                if output.dim() == 4:
                    output = torch.softmax(output, dim=1)
                
                return output
                
        except Exception as e:
            logger.error(f"âŒ {model_name} ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _run_mock_model_inference(self, model: Any, image: Any, model_name: str):
        """Mock ëª¨ë¸ ì¶”ë¡ """
        try:
            result = model.process(image=image)
            logger.info(f"âœ… {model_name} Mock ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {model_name} Mock ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _create_ensemble_result(self, all_results: Dict, ensemble_method: str) -> torch.Tensor:
        """ì•™ìƒë¸” ê²°ê³¼ ìƒì„±"""
        try:
            if len(all_results) == 1:
                # ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš°
                result = list(all_results.values())[0]
                if isinstance(result, np.ndarray):
                    result = torch.from_numpy(result)
                return result
            
            # ëª¨ë“  ê²°ê³¼ë¥¼ ë™ì¼í•œ í˜•íƒœë¡œ ë³€í™˜
            processed_results = []
            target_size = (256, 256)  # í‘œì¤€ ì¶œë ¥ í¬ê¸°
            
            for result in all_results.values():
                if result is None:
                    continue
                    
                # NumPy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(result, np.ndarray):
                    result = torch.from_numpy(result)
                
                # í…ì„œì¸ ê²½ìš° í¬ê¸° ì¡°ì •
                if isinstance(result, torch.Tensor):
                    # ë°°ì¹˜ ì°¨ì›ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
                    if result.dim() == 3:
                        result = result.unsqueeze(0)
                    
                    # í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° ë¦¬ì‚¬ì´ì¦ˆ
                    if result.shape[2] != target_size[0] or result.shape[3] != target_size[1]:
                        result = torch.nn.functional.interpolate(
                            result, 
                            size=target_size, 
                            mode='bilinear', 
                            align_corners=False
                        )
                    
                    processed_results.append(result)
            
            if not processed_results:
                logger.warning("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ì—†ìŒ, ê¸°ë³¸ ì¶œë ¥ ë°˜í™˜")
                return torch.zeros((1, 20, target_size[0], target_size[1]))
            
            # ëª¨ë“  ê²°ê³¼ê°€ ë™ì¼í•œ í¬ê¸°ì¸ì§€ í™•ì¸
            first_shape = processed_results[0].shape
            for i, result in enumerate(processed_results):
                if result.shape != first_shape:
                    logger.warning(f"âš ï¸ ê²°ê³¼ {i} í¬ê¸° ë¶ˆì¼ì¹˜: {result.shape} vs {first_shape}")
                    # í¬ê¸° ì¡°ì •
                    processed_results[i] = torch.nn.functional.interpolate(
                        result, 
                        size=(first_shape[2], first_shape[3]), 
                        mode='bilinear', 
                        align_corners=False
                    )
            
            # ì•™ìƒë¸” ë°©ë²•ì— ë”°ë¥¸ ê²°ê³¼ ìƒì„±
            if ensemble_method == 'weighted_average':
                # ê°€ì¤‘ í‰ê·  (ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜)
                weights = torch.ones(len(processed_results)) / len(processed_results)
                ensemble_result = sum(w * r for w, r in zip(weights, processed_results))
                
            elif ensemble_method == 'confidence_based':
                # ì‹ ë¢°ë„ ê¸°ë°˜ ì•™ìƒë¸”
                # ê°„ë‹¨í•œ êµ¬í˜„: ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ì‹ ë¢°ë„
                confidence_weights = torch.ones(len(processed_results)) / len(processed_results)
                ensemble_result = sum(w * r for w, r in zip(confidence_weights, processed_results))
                
            elif ensemble_method == 'spatial_consistency':
                # ê³µê°„ ì¼ê´€ì„± ê¸°ë°˜ ì•™ìƒë¸”
                # ê°„ë‹¨í•œ êµ¬í˜„: í‰ê·  ì‚¬ìš©
                ensemble_result = torch.stack(processed_results).mean(dim=0)
                
            else:
                # ê¸°ë³¸: í‰ê· 
                ensemble_result = torch.stack(processed_results).mean(dim=0)
            
            logger.info(f"âœ… ì•™ìƒë¸” ê²°ê³¼ ìƒì„± ì™„ë£Œ ({ensemble_method}): {ensemble_result.shape}")
            return ensemble_result
            
        except Exception as e:
            logger.error(f"âŒ ì•™ìƒë¸” ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì¶œë ¥ ë°˜í™˜
            return torch.zeros((1, 20, 256, 256))
    
    def _create_simple_average_result(self, all_results: Dict) -> torch.Tensor:
        """ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ê²°ê³¼ ìƒì„±"""
        parsing_masks = []
        for result in all_results.values():
            # NumPy ë°°ì—´ì´ë‚˜ í…ì„œì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
            if isinstance(result, (np.ndarray, torch.Tensor)):
                parsing_masks.append(result)
            elif isinstance(result, dict) and 'parsing' in result:
                parsing_masks.append(result['parsing'])
            elif result is not None:  # Noneì´ ì•„ë‹Œ ê²½ìš°
                parsing_masks.append(result)
        
        if parsing_masks:
            # ëª¨ë“  ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜
            tensor_masks = []
            for mask in parsing_masks:
                if isinstance(mask, np.ndarray):
                    tensor_masks.append(torch.from_numpy(mask))
                elif isinstance(mask, torch.Tensor):
                    tensor_masks.append(mask)
                else:
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹ˆ í…ì„œ ìƒì„±
                    tensor_masks.append(torch.zeros((512, 512), dtype=torch.float32))
            
            if tensor_masks:
                ensemble_result = torch.stack(tensor_masks).mean(dim=0)
                return ensemble_result
        
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return torch.zeros((512, 512), dtype=torch.float32)
    
    def _create_weighted_average_result(self, all_results: Dict) -> torch.Tensor:
        """ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” ê²°ê³¼ ìƒì„±"""
        weighted_masks = []
        total_weight = 0
        
        for model_name, result in all_results.items():
            # NumPy ë°°ì—´ì´ë‚˜ í…ì„œì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
            if isinstance(result, (np.ndarray, torch.Tensor)):
                weight = self.model_weights.get(model_name, 0.1)
                if isinstance(result, np.ndarray):
                    weighted_masks.append(torch.from_numpy(result) * weight)
                else:
                    weighted_masks.append(result * weight)
                total_weight += weight
            elif isinstance(result, dict) and 'parsing' in result:
                weight = self.model_weights.get(model_name, 0.1)
                if isinstance(result['parsing'], np.ndarray):
                    weighted_masks.append(torch.from_numpy(result['parsing']) * weight)
                else:
                    weighted_masks.append(result['parsing'] * weight)
                total_weight += weight
            elif result is not None:  # Noneì´ ì•„ë‹Œ ê²½ìš°
                weight = self.model_weights.get(model_name, 0.1)
                if isinstance(result, np.ndarray):
                    weighted_masks.append(torch.from_numpy(result) * weight)
                else:
                    weighted_masks.append(torch.tensor(result, dtype=torch.float32) * weight)
                total_weight += weight
        
        if weighted_masks and total_weight > 0:
            ensemble_result = torch.stack(weighted_masks).sum(dim=0) / total_weight
            return ensemble_result
        else:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros((512, 512), dtype=torch.float32)
    
    def _validate_output_data(self, result: Dict[str, Any]) -> bool:
        """ì¶œë ¥ ë°ì´í„° ê²€ì¦"""
        try:
            required_keys = ['parsing_mask', 'confidence', 'human_parsing_result']
            missing_keys = [key for key in required_keys if key not in result]
            
            if missing_keys:
                logger.warning(f"âš ï¸ ëˆ„ë½ëœ í‚¤: {missing_keys}")
                return False
            
            logger.info("âœ… ì¶œë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name
        }
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'models_loaded': len(self.models),
            'supported_models': self.supported_models,
            'ensemble_methods': self.ensemble_methods,
            'model_weights': self.model_weights
        }

    def _create_final_result(self, processed_result: Dict[str, Any], processing_time: float) -> Dict[str, Any]:
        """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í˜•ì‹ì— ë§ëŠ” ìµœì¢… ê²°ê³¼ ìƒì„± - ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° í¬í•¨"""
        return {
            'success': True,
            'step_name': 'HumanParsingStep',
            'step_id': 1,
            'processing_time': processing_time,
            
            # Step 2 (Pose Estimation)ë¥¼ ìœ„í•œ ë°ì´í„°
            'segmentation_mask': processed_result.get('segmentation_mask'),
            'segmentation_mask_path': processed_result.get('segmentation_mask_path'),
            'human_parsing_result': processed_result.get('human_parsing_result'),
            'confidence': processed_result.get('confidence'),
            
            # Step 3 (Cloth Segmentation)ë¥¼ ìœ„í•œ ë°ì´í„°
            'person_image_path': processed_result.get('person_image_path'),
            
            # ì¶”ê°€ í•„ë“œë“¤ (ë°ì´í„° íë¦„ ì •ì˜ì— ë§ì¶¤)
            'parsing_confidence': processed_result.get('confidence'),  # Step 2ì—ì„œ í•„ìš”
            'mask': processed_result.get('segmentation_mask'),         # Step 3ì—ì„œ í•„ìš”
            'mask_path': processed_result.get('segmentation_mask_path'), # Step 3ì—ì„œ í•„ìš”
            
            # í’ˆì§ˆ ë° ë©”íƒ€ë°ì´í„°
            'quality_score': processed_result.get('quality_score'),
            'processing_metadata': processed_result.get('processing_metadata'),
            'status': 'completed'
        }

    def _create_error_result(self, error_message: str, processing_time: float) -> Dict[str, Any]:
        """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í˜•ì‹ì— ë§ëŠ” ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'step_name': 'HumanParsingStep',
            'processing_time': processing_time,
            'error': error_message,
            'quality_score': 0.0,
            'status': 'failed'
        }

    def _validate_and_prepare_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì¤€ë¹„"""
        try:
            validated_input = {
                'session_id': input_data.get('session_id'),
                'timestamp': datetime.now().isoformat(),
                'step_id': 1
            }
            
            # ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸
            if 'person_image_path' in input_data:
                validated_input['person_image_path'] = input_data['person_image_path']
            elif 'person_image' in input_data:
                validated_input['person_image'] = input_data['person_image']
            else:
                raise ValueError("ì‚¬ëŒ ì´ë¯¸ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì¸¡ì •ê°’ ì¶”ê°€
            if 'measurements' in input_data:
                validated_input['measurements'] = input_data['measurements']
            
            # í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì´ì „ Step ê²°ê³¼ í™•ì¸
            if self.unified_db and validated_input.get('session_id'):
                asyncio.create_task(self._log_step_dependencies(validated_input['session_id']))
            
            return validated_input
            
        except Exception as e:
            logging.error(f"âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise

    async def _log_step_dependencies(self, session_id: str):
        """Step ì˜ì¡´ì„± ì •ë³´ ë¡œê¹…"""
        try:
            if not self.unified_db:
                return
            
            # Step 1ì€ ì˜ì¡´ì„±ì´ ì—†ì§€ë§Œ, í–¥í›„ í™•ì¥ì„±ì„ ìœ„í•´ ë¡œê¹…
            dependencies = await self.unified_db.validate_step_dependencies(session_id, 1)
            logging.info(f"ğŸ“‹ Step 1 ì˜ì¡´ì„± ê²€ì¦: {dependencies}")
            
        except Exception as e:
            logging.debug(f"âš ï¸ ì˜ì¡´ì„± ê²€ì¦ ë¡œê¹… ì‹¤íŒ¨: {e}")

    async def _load_person_image(self, input_data: Dict[str, Any]) -> Any:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ ì‚¬ëŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œ"""
        try:
            if 'person_image_path' in input_data:
                image_path = Path(input_data['person_image_path'])
                if image_path.exists():
                    from PIL import Image
                    return Image.open(image_path)
                else:
                    raise FileNotFoundError(f"ì‚¬ëŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            elif 'person_image' in input_data:
                # ì´ë¯¸ì§€ê°€ ì´ë¯¸ í…ì„œ ë˜ëŠ” numpy ë°°ì—´ í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                return input_data['person_image']
            else:
                raise ValueError("ì‚¬ëŒ ì´ë¯¸ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logging.error(f"âŒ ì‚¬ëŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _load_person_image_sync(self, input_data: Dict[str, Any]) -> Any:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ ì‚¬ëŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œ (ë™ê¸° ë²„ì „)"""
        try:
            if 'person_image_path' in input_data:
                image_path = Path(input_data['person_image_path'])
                if image_path.exists():
                    from PIL import Image
                    return Image.open(image_path)
                else:
                    raise FileNotFoundError(f"ì‚¬ëŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            elif 'person_image' in input_data:
                # ì´ë¯¸ì§€ê°€ ì´ë¯¸ í…ì„œ ë˜ëŠ” numpy ë°°ì—´ í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                return input_data['person_image']
            else:
                raise ValueError("ì‚¬ëŒ ì´ë¯¸ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logging.error(f"âŒ ì‚¬ëŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    async def _run_ai_inference(self, input_data: Dict[str, Any], person_image: Any) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            ensemble_method = input_data.get('ensemble_method', 'weighted_average')
            
            if person_image is None:
                return {'success': False, 'error': 'ì‚¬ëŒ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            logging.info(f"ğŸš€ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•™ìƒë¸” ë°©ë²•: {ensemble_method})")
            result = self._run_human_parsing(person_image, ensemble_method)
            
            if result.get('success'):
                confidence = result.get('data', {}).get('confidence', 0.0)
                logging.info(f"âœ… AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {confidence:.2f}")
                
                # ê²°ê³¼ ë°ì´í„° êµ¬ì¡°í™”
                processed_result = {
                    'success': True,
                    'segmentation_mask': result.get('data', {}).get('parsing_mask'),
                    'segmentation_mask_path': result.get('data', {}).get('mask_path'),
                    'human_parsing_result': result.get('data', {}).get('parsing_mask'),
                    'confidence': confidence,
                    'person_image_path': input_data.get('person_image_path'),
                    'quality_score': confidence,
                    'processing_metadata': {
                        'ensemble_method': ensemble_method,
                        'models_used': list(self.models.keys()),
                        'timestamp': datetime.now().isoformat()
                    }
                }
                
                return processed_result
            else:
                logging.error(f"âŒ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return {'success': False, 'error': result.get('error', 'Unknown error')}
                
        except Exception as e:
            logging.error(f"âŒ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    def _run_ai_inference_sync(self, input_data: Dict[str, Any], person_image: Any) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ë²„ì „)"""
        try:
            ensemble_method = input_data.get('ensemble_method', 'weighted_average')
            
            if person_image is None:
                return {'success': False, 'error': 'ì‚¬ëŒ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            logging.info(f"ğŸš€ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•™ìƒë¸” ë°©ë²•: {ensemble_method})")
            result = self._run_human_parsing(person_image, ensemble_method)
            
            if result.get('success'):
                confidence = result.get('data', {}).get('confidence', 0.0)
                logging.info(f"âœ… AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {confidence:.2f}")
                
                # ê²°ê³¼ ë°ì´í„° êµ¬ì¡°í™”
                processed_result = {
                    'success': True,
                    'segmentation_mask': result.get('data', {}).get('parsing_mask'),
                    'segmentation_mask_path': result.get('data', {}).get('mask_path'),
                    'human_parsing_result': result.get('data', {}).get('parsing_mask'),
                    'confidence': confidence,
                    'person_image_path': input_data.get('person_image_path'),
                    'quality_score': confidence,
                    'processing_metadata': {
                        'ensemble_method': ensemble_method,
                        'models_used': list(self.models.keys()),
                        'timestamp': datetime.now().isoformat()
                    }
                }
                
                return processed_result
            else:
                logging.error(f"âŒ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return {'success': False, 'error': result.get('error', 'Unknown error')}
                
        except Exception as e:
            logging.error(f"âŒ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def _save_to_unified_database(self, session_id: str, input_data: Dict[str, Any], 
                                           output_data: Dict[str, Any], processing_time: float):
        """í†µí•© Session Databaseì— ê²°ê³¼ ì €ì¥"""
        try:
            if not self.unified_db:
                logging.warning("âš ï¸ UnifiedSessionDatabaseê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return
            
            # Step ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            success = await self.unified_db.save_step_result(
                session_id=session_id,
                step_id=1,
                step_name="HumanParsingStep",
                input_data=input_data,
                output_data=output_data,
                processing_time=processing_time,
                quality_score=output_data.get('quality_score', 0.0),
                status='completed'
            )
            
            if success:
                logging.info(f"âœ… Step 1 ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ: {session_id}")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹… (Mock ë°ì´í„°ë² ì´ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
                if hasattr(self.unified_db, 'get_performance_metrics'):
                    metrics = self.unified_db.get_performance_metrics()
                    logging.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­: {metrics}")
                
                # ì„¸ì…˜ ì§„í–‰ë¥ ì€ í‘œì¤€ APIë¥¼ í†µí•´ ìë™ ì—…ë°ì´íŠ¸ë¨
                logging.info("âœ… ì„¸ì…˜ ì§„í–‰ë¥ ì€ í‘œì¤€ APIë¥¼ í†µí•´ ìë™ ì—…ë°ì´íŠ¸ë¨")
            else:
                logging.error(f"âŒ Step 1 ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì‹¤íŒ¨: {session_id}")
                
        except Exception as e:
            logging.error(f"âŒ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_to_unified_database_sync(self, session_id: str, input_data: Dict[str, Any], 
                                          output_data: Dict[str, Any], processing_time: float):
        """í†µí•© Session Databaseì— ê²°ê³¼ ì €ì¥ (ë™ê¸° ë²„ì „)"""
        try:
            if not self.unified_db:
                logging.warning("âš ï¸ UnifiedSessionDatabaseê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return
            
            # ë™ê¸°ì ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ê°„ë‹¨í•œ ë¡œê¹…ë§Œ)
            logging.info(f"âœ… Step 1 ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ: {session_id}")
            logging.info(f"ğŸ“Š ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            logging.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {output_data.get('quality_score', 0.0)}")
            
        except Exception as e:
            logging.error(f"âŒ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _save_error_to_unified_database(self, session_id: str, input_data: Dict[str, Any], 
                                                 error_result: Dict[str, Any], processing_time: float):
        """ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© Session Databaseì— ê°•ì œ ì €ì¥"""
        try:
            if not self.unified_db:
                return
            
            await self.unified_db.save_step_result(
                session_id=session_id,
                step_id=1,
                step_name="HumanParsingStep",
                input_data=input_data,
                output_data=error_result,
                processing_time=processing_time,
                quality_score=0.0,
                status='failed',
                error_message=error_result.get('error', 'Unknown error')
            )
            
            logging.info(f"âœ… Step 1 ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ: {session_id}")
            
        except Exception as e:
            logging.error(f"âŒ ì—ëŸ¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_error_to_unified_database_sync(self, session_id: str, input_data: Dict[str, Any], 
                                                error_result: Dict[str, Any], processing_time: float):
        """ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© Session Databaseì— ê°•ì œ ì €ì¥ (ë™ê¸° ë²„ì „)"""
        try:
            if not self.unified_db:
                return
            
            # ë™ê¸°ì ìœ¼ë¡œ ì—ëŸ¬ ê²°ê³¼ ë¡œê¹…
            logging.info(f"âœ… Step 1 ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ: {session_id}")
            logging.error(f"âŒ ì—ëŸ¬ ë‚´ìš©: {error_result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logging.error(f"âŒ ì—ëŸ¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _force_save_error_to_unified_database_sync(self, session_id: str, input_data: Dict[str, Any], 
                                                      error_result: Dict[str, Any], processing_time: float):
        """ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© Session Databaseì— ê°•ì œ ì €ì¥ (ë™ê¸° ë²„ì „)"""
        try:
            if not self.unified_db:
                return
            
            # ë™ê¸°ì ìœ¼ë¡œ ì—ëŸ¬ ê²°ê³¼ ë¡œê¹…
            logging.info(f"âœ… Step 1 ì—ëŸ¬ ê²°ê³¼ë¥¼ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ê°•ì œ ì €ì¥ ì™„ë£Œ: {session_id}")
            logging.error(f"âŒ ì—ëŸ¬ ë‚´ìš©: {error_result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logging.error(f"âŒ ì—ëŸ¬ ê²°ê³¼ ê°•ì œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _force_update_session_progress(self, session_id: str):
        """ì„¸ì…˜ ì§„í–‰ë¥  ê°•ì œ ì—…ë°ì´íŠ¸"""
        try:
            if not self.unified_db:
                return
            
            # Mock ë°ì´í„°ë² ì´ìŠ¤ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if not hasattr(self.unified_db, '_get_connection'):
                logging.info("âš ï¸ Mock ë°ì´í„°ë² ì´ìŠ¤: ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸°")
                return
            
            # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„ (ë°ì´í„°ë² ì´ìŠ¤ ë½ í•´ì œ ëŒ€ê¸°)
            await asyncio.sleep(0.5)
            
            # ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
            session_info = await self.unified_db.get_session_info(session_id)
            if session_info:
                # ì™„ë£Œëœ Stepì— 1 ì¶”ê°€
                completed_steps = session_info.completed_steps.copy()
                if 1 not in completed_steps:
                    completed_steps.append(1)
                    logging.info(f"ğŸ“‹ ì™„ë£Œëœ Stepì— 1 ì¶”ê°€: {completed_steps}")
                else:
                    logging.info(f"ğŸ“‹ Step 1ì´ ì´ë¯¸ ì™„ë£Œëœ Stepì— í¬í•¨ë¨: {completed_steps}")
                
                # ì§„í–‰ë¥  ê³„ì‚° (8ê°œ Step ê¸°ì¤€)
                progress_percent = (len(completed_steps) / 8) * 100
                logging.info(f"ğŸ“Š ì§„í–‰ë¥  ê³„ì‚°: {len(completed_steps)}/8 = {progress_percent:.1f}%")
                
                # ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        with self.unified_db._get_connection() as conn:
                            cursor = conn.cursor()
                            
                            # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ í™•ì¸
                            cursor.execute("""
                                SELECT completed_steps, progress_percent 
                                FROM sessions 
                                WHERE session_id = ?
                            """, (session_id,))
                            current_result = cursor.fetchone()
                            
                            if current_result:
                                current_completed = json.loads(current_result[0]) if current_result[0] else []
                                current_progress = current_result[1] or 0.0
                                logging.info(f"ğŸ“‹ í˜„ì¬ DB ìƒíƒœ - ì™„ë£Œëœ Step: {current_completed}, ì§„í–‰ë¥ : {current_progress:.1f}%")
                            
                            # ì—…ë°ì´íŠ¸ ì‹¤í–‰
                            cursor.execute("""
                                UPDATE sessions 
                                SET completed_steps = ?, progress_percent = ?, updated_at = ?
                                WHERE session_id = ?
                            """, (
                                json.dumps(completed_steps),
                                progress_percent,
                                datetime.now().isoformat(),
                                session_id
                            ))
                            
                            # ì—…ë°ì´íŠ¸ëœ í–‰ ìˆ˜ í™•ì¸
                            if cursor.rowcount > 0:
                                conn.commit()
                                logging.info(f"âœ… ì„¸ì…˜ ì§„í–‰ë¥  ê°•ì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {progress_percent:.1f}% (ì‹œë„ {attempt + 1})")
                                logging.info(f"   - ì—…ë°ì´íŠ¸ëœ í–‰ ìˆ˜: {cursor.rowcount}")
                                break
                            else:
                                logging.warning(f"âš ï¸ ì—…ë°ì´íŠ¸ëœ í–‰ì´ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(0.5 * (attempt + 1))
                        
                    except Exception as e:
                        if attempt < max_retries - 1:
                            logging.warning(f"âš ï¸ ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                            await asyncio.sleep(0.5 * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        else:
                            logging.error(f"âŒ ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ìµœì¢… ì‹¤íŒ¨: {e}")
                            raise
                
                # ì—…ë°ì´íŠ¸ í›„ í™•ì¸
                await asyncio.sleep(0.5)
                updated_session = await self.unified_db.get_session_info(session_id)
                if updated_session:
                    logging.info(f"ğŸ“Š ì—…ë°ì´íŠ¸ í›„ í™•ì¸ - ì™„ë£Œëœ Step: {updated_session.completed_steps}")
                    logging.info(f"ğŸ“Š ì—…ë°ì´íŠ¸ í›„ í™•ì¸ - ì§„í–‰ë¥ : {updated_session.progress_percent:.1f}%")
                
            else:
                logging.warning("âš ï¸ ì„¸ì…˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸°")
                
        except Exception as e:
            logging.error(f"âŒ ì„¸ì…˜ ì§„í–‰ë¥  ê°•ì œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _postprocess_result(self, raw_result: Dict[str, Any], original_image: Any) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬ - í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í˜•ì‹ì— ë§ê²Œ ì¡°ì •"""
        try:
            # raw_resultì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
            if raw_result.get('success') and 'data' in raw_result:
                data = raw_result['data']
                processed_result = {
                    'segmentation_mask': data.get('final_parsing') or data.get('segmentation_mask'),
                    'human_parsing_result': data.get('human_parsing_result') or data.get('final_parsing'),
                    'confidence': data.get('confidence', 0.0),
                    'quality_score': self._calculate_quality_score(data),
                    'processing_metadata': {
                        'model_used': raw_result.get('model_used', 'ensemble'),
                        'ensemble_method': raw_result.get('ensemble_method', 'weighted_average'),
                        'input_image_size': getattr(original_image, 'size', 'unknown'),
                        'models_used': raw_result.get('models_used', [])
                    }
                }
            else:
                # ì—ëŸ¬ ë˜ëŠ” ê¸°ë³¸ ê²°ê³¼
                processed_result = {
                    'segmentation_mask': None,
                    'human_parsing_result': None,
                    'confidence': 0.0,
                    'quality_score': 0.0,
                    'processing_metadata': {
                        'error': raw_result.get('error', 'Unknown error')
                    }
                }
            
            # ì´ë¯¸ì§€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜
            if 'segmentation_mask' in processed_result and processed_result['segmentation_mask'] is not None:
                mask_path = self._save_segmentation_mask(processed_result['segmentation_mask'])
                if mask_path:
                    processed_result['segmentation_mask_path'] = str(mask_path)
            
            return processed_result
            
        except Exception as e:
            logging.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'segmentation_mask': None,
                'human_parsing_result': None,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_metadata': {'error': str(e)}
            }

    def _calculate_quality_score(self, result: Dict[str, Any]) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            base_score = 0.5
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ ì ìˆ˜
            confidence = result.get('confidence', 0.0)
            base_score += confidence * 0.3
            
            # ê²°ê³¼ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€
            if result.get('final_parsing') or result.get('segmentation_mask'):
                base_score += 0.2
            
            return min(base_score, 1.0)
            
        except Exception as e:
            logging.debug(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def _save_segmentation_mask(self, mask: Any) -> Optional[Path]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            temp_dir = Path("temp_masks")
            temp_dir.mkdir(exist_ok=True)
            
            mask_filename = f"mask_{int(time.time())}.png"
            mask_path = temp_dir / mask_filename
            
            # PIL Imageë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            if hasattr(mask, 'numpy'):
                mask_array = mask.numpy()
            elif hasattr(mask, 'cpu'):
                mask_array = mask.cpu().numpy()
            else:
                mask_array = mask
            
            from PIL import Image
            mask_image = Image.fromarray(mask_array.astype('uint8'))
            mask_image.save(mask_path)
            
            return mask_path
            
        except Exception as e:
            logging.debug(f"âš ï¸ ë§ˆìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def _create_mock_database(self):
        """í…ŒìŠ¤íŠ¸ìš© Mock ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        class MockDatabase:
            async def save_step_result(self, *args, **kwargs):
                logging.info("âœ… Mock ë°ì´í„°ë² ì´ìŠ¤: Step ê²°ê³¼ ì €ì¥")
                return True
            
            async def get_step_result(self, *args, **kwargs):
                logging.info("âœ… Mock ë°ì´í„°ë² ì´ìŠ¤: Step ê²°ê³¼ ì¡°íšŒ")
                return None
            
            async def get_session_info(self, *args, **kwargs):
                logging.info("âœ… Mock ë°ì´í„°ë² ì´ìŠ¤: ì„¸ì…˜ ì •ë³´ ì¡°íšŒ")
                return None
            
            def _get_connection(self):
                class MockConnection:
                    def __enter__(self):
                        return self
                    def __exit__(self, *args):
                        pass
                    def cursor(self):
                        return self
                    def execute(self, *args):
                        pass
                    def commit(self):
                        pass
                return MockConnection()
        
        return MockDatabase()

    def _create_mock_model(self, model_name: str):
        """Mock ëª¨ë¸ ìƒì„±"""
        class MockModel:
            def __init__(self, name):
                self.name = name
            
            def process(self, image):
                # ê°„ë‹¨í•œ Mock ê²°ê³¼ ìƒì„±
                if hasattr(image, 'shape'):
                    height, width = image.shape[:2]
                else:
                    height, width = 256, 256
                
                # 20ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ Mock ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ
                mock_mask = np.random.randint(0, 20, (height, width), dtype=np.uint8)
                
                return {
                    'parsing_mask': mock_mask,
                    'confidence': 0.8,
                    'model_name': self.name
                }
        
        return MockModel(model_name)

# íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_human_parsing_step(
    device: str = "cpu",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingStep:
    """Human Parsing Step ìƒì„±"""
    step = HumanParsingStep(**kwargs)
    
    # ëª¨ë¸ ë¡œë“œ
    if not step.load_models(device):
        logger.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return None
    
    return step

def create_human_parsing_step_sync(
    device: str = "cpu",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingStep:
    """ë™ê¸° ë²„ì „ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return create_human_parsing_step(device, config, **kwargs)

# ë¹„ë™ê¸° ì´ˆê¸°í™”
async def initialize_human_parsing_step_async(**kwargs) -> HumanParsingStep:
    """HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        logger.info("ğŸ”„ HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘")
        step = HumanParsingStep(**kwargs)
        await step.initialize_async()
        logger.info("âœ… HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        return step
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ì •ë¦¬ í•¨ìˆ˜
async def cleanup_human_parsing_step_async(step: HumanParsingStep) -> None:
    """HumanParsingStep ë¹„ë™ê¸° ì •ë¦¬"""
    try:
        logger.info("ğŸ§¹ HumanParsingStep ì •ë¦¬ ì‹œì‘")
        await step.cleanup_async()
        logger.info("âœ… HumanParsingStep ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ë™ê¸° ë²„ì „
def create_human_parsing_step_sync_simple(**kwargs) -> HumanParsingStep:
    """HumanParsingStep ë™ê¸° ìƒì„± (ê°„ë‹¨ ë²„ì „)"""
    try:
        step = HumanParsingStep(**kwargs)
        return step
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def create_human_parsing_step_async_simple(**kwargs) -> HumanParsingStep:
    """HumanParsingStep ë¹„ë™ê¸° ìƒì„± (ê°„ë‹¨ ë²„ì „)"""
    try:
        step = HumanParsingStep(**kwargs)
        return step
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ìƒì„± ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    logger.info("âœ… HumanParsingStep ëª¨ë“ˆí™”ëœ ë²„ì „ ë¡œë“œ ì™„ë£Œ (ë²„ì „: v8.0 - Modularized)")
