# backend/app/ai_pipeline/steps/human_parsing_integrated_loader.py
"""
ğŸ”¥ HumanParsingStep í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ
================================================================================

âœ… Central Hub í†µí•©
âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ ì—°ë™
âœ… ëª¨ë¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ìƒì„±
âœ… ë‹¨ê³„ì  í´ë°± ì‹œìŠ¤í…œ
âœ… BaseStepMixin ì™„ì „ í˜¸í™˜

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0 (í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ)
"""

import os
import sys
import gc
import time
import json
import logging
import traceback
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass

# PyTorch ì•ˆì „ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None

# NumPy ì•ˆì „ import
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# MPS ì§€ì› í™•ì¸
MPS_AVAILABLE = TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()

# ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
DEFAULT_DEVICE = "mps" if MPS_AVAILABLE else ("cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu")

logger = logging.getLogger(__name__)

@dataclass
class ModelLoadingResult:
    """ëª¨ë¸ ë¡œë”© ê²°ê³¼"""
    success: bool
    model: Optional[Any] = None
    model_name: str = ""
    loading_method: str = ""
    error_message: str = ""
    processing_time: float = 0.0

class HumanParsingIntegratedLoader:
    """HumanParsingStep í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, logger=None):
        self.device = self._setup_device(device)
        self.logger = logger or logging.getLogger(self.__class__.__name__)
        self.models = {}
        self.loaded_models = {}
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if MPS_AVAILABLE:
                return "mps"
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def load_models_integrated(self) -> bool:
        """í†µí•©ëœ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ - ë©”ì¸ ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ HumanParsing í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ ì‹œì‘")
            start_time = time.time()
            
            # 1ë‹¨ê³„: Central Hub ì‹œë„
            if self._load_via_central_hub():
                self.logger.info("âœ… Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                return True
            
            # 2ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ë¡œë”©
            models_loaded = 0
            for model_name in ['graphonomy', 'u2net', 'hrnet']:
                result = self._load_with_checkpoint_analysis(model_name)
                if result.success:
                    self.models[model_name] = result.model
                    models_loaded += 1
            
            if models_loaded > 0:
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {models_loaded}ê°œ")
                return True
            
            # 3ë‹¨ê³„: ì•„í‚¤í…ì²˜ ê¸°ë°˜ ìƒì„±
            fallback_models = {
                'graphonomy': {'num_classes': 20, 'architecture_type': 'graphonomy'},
                'u2net': {'num_classes': 20, 'architecture_type': 'u2net'},
                'hrnet': {'num_classes': 20, 'architecture_type': 'hrnet'}
            }
            
            for model_name, config in fallback_models.items():
                result = self._create_with_architecture(model_name, config)
                if result.success:
                    self.models[model_name] = result.model
                    models_loaded += 1
            
            if models_loaded > 0:
                self.logger.info(f"âœ… ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„± ì„±ê³µ: {models_loaded}ê°œ")
                return True
            
            self.logger.error("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ë°©ë²• ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_via_central_hub(self) -> bool:
        """Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            # Central Hubì—ì„œ ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            model_loader_service = self._get_service_from_central_hub('model_loader')
            if not model_loader_service:
                self.logger.warning("âš ï¸ Central Hubì—ì„œ ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # Stepë³„ ìµœì  ëª¨ë¸ ë¡œë“œ
            models_to_load = {
                'graphonomy': 'human_parsing',
                'u2net': 'human_parsing',
                'hrnet': 'human_parsing'
            }
            
            models_loaded = 0
            for model_name, step_type in models_to_load.items():
                try:
                    model = model_loader_service.load_model_for_step(
                        step_type=step_type,
                        model_name=model_name,
                        device=self.device
                    )
                    if model:
                        self.models[model_name] = model
                        models_loaded += 1
                        self.logger.info(f"âœ… {model_name} Central Hub ë¡œë“œ ì™„ë£Œ")
                    else:
                        self.logger.warning(f"âš ï¸ {model_name} Central Hub ë¡œë“œ ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} Central Hub ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            return models_loaded > 0
            
        except Exception as e:
            self.logger.error(f"âŒ Central Hub ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_with_checkpoint_analysis(self, model_name: str) -> ModelLoadingResult:
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”©"""
        start_time = time.time()
        try:
            from app.ai_pipeline.models.model_loader import DynamicModelCreator
            from app.ai_pipeline.models.checkpoint_model_loader import get_checkpoint_model_loader
            
            # ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ê°€ì ¸ì˜¤ê¸°
            checkpoint_loader = get_checkpoint_model_loader(device=self.device)
            
            # ë™ì  ëª¨ë¸ ìƒì„±ê¸°
            model_creator = DynamicModelCreator()
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            checkpoint_path = checkpoint_loader.get_checkpoint_path(model_name)
            if not checkpoint_path:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_name,
                    loading_method="checkpoint_analysis",
                    error_message=f"ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_name}",
                    processing_time=time.time() - start_time
                )
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±
            model = model_creator.create_model_from_checkpoint(
                checkpoint_path=checkpoint_path,
                step_type='human_parsing',
                device=self.device
            )
            
            if model:
                self.logger.info(f"âœ… {model_name} ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name=model_name,
                    loading_method="checkpoint_analysis",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_name,
                    loading_method="checkpoint_analysis",
                    error_message="ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name=model_name,
                loading_method="checkpoint_analysis",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _create_with_architecture(self, model_type: str, config: Dict[str, Any]) -> ModelLoadingResult:
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„± - ê¸°ì¡´ ëª¨ë“ˆí™”ëœ êµ¬ì¡° í™œìš©"""
        start_time = time.time()
        try:
            # ê¸°ì¡´ ëª¨ë“ˆí™”ëœ êµ¬ì¡° í™œìš©
            if model_type == 'graphonomy':
                return self._load_graphonomy_from_modules(config)
            elif model_type == 'u2net':
                return self._load_u2net_from_modules(config)
            elif model_type == 'hrnet':
                return self._load_hrnet_from_modules(config)
            else:
                # í´ë°±: ê¸°ë³¸ ì•„í‚¤í…ì²˜ ì‚¬ìš©
                return self._create_basic_architecture(model_type, config)
                
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name=model_type,
                loading_method="architecture_based",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_graphonomy_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ Graphonomy ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .human_parsing.models.graphonomy_models import GraphonomyModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = GraphonomyModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… Graphonomy ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='graphonomy',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='graphonomy',
                    loading_method="modular_architecture",
                    error_message="Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('graphonomy', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='graphonomy',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_u2net_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ U2Net ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .human_parsing.models.u2net_model import U2NetModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = U2NetModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… U2Net ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='u2net',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='u2net',
                    loading_method="modular_architecture",
                    error_message="U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ U2Net ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('u2net', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='u2net',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_hrnet_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ HRNet ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .human_parsing.models.hrnet_model import HRNetModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = HRNetModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… HRNet ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='hrnet',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='hrnet',
                    loading_method="modular_architecture",
                    error_message="HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ HRNet ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('hrnet', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='hrnet',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _create_basic_architecture(self, model_type: str, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„± (í´ë°±)"""
        start_time = time.time()
        try:
            from app.ai_pipeline.models.model_loader import HumanParsingArchitecture
            
            # Human Parsing íŠ¹í™” ì•„í‚¤í…ì²˜ ìƒì„±
            architecture = HumanParsingArchitecture(
                step_type='human_parsing',
                device=self.device
            )
            
            # ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„±
            model = architecture.create_model(config)
            
            # ëª¨ë¸ ê²€ì¦
            if architecture.validate_model(model):
                self.logger.info(f"âœ… {model_type} ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„± ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name=model_type,
                    loading_method="basic_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_type,
                    loading_method="basic_architecture",
                    error_message="ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name=model_type,
                loading_method="basic_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Central Hub import ì‹œë„
            from app.api.central_hub import get_service
            return get_service(service_key)
        except ImportError:
            try:
                # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
                from app.core.di_container import get_service
                return get_service(service_key)
            except ImportError:
                self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {service_key}")
                return None
    
    def get_loaded_models(self) -> Dict[str, Any]:
        """ë¡œë“œëœ ëª¨ë¸ë“¤ ë°˜í™˜"""
        return self.models.copy()
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except:
                    pass
            
            self.models.clear()
            self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            for _ in range(3):
                gc.collect()
            if TORCH_AVAILABLE and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… HumanParsingIntegratedLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
_global_integrated_loader: Optional[HumanParsingIntegratedLoader] = None

def get_integrated_loader(device: str = DEFAULT_DEVICE, logger=None) -> HumanParsingIntegratedLoader:
    """ì „ì—­ í†µí•© ë¡œë” ë°˜í™˜"""
    global _global_integrated_loader
    if _global_integrated_loader is None:
        _global_integrated_loader = HumanParsingIntegratedLoader(device=device, logger=logger)
    return _global_integrated_loader

# ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
__all__ = [
    "HumanParsingIntegratedLoader",
    "ModelLoadingResult", 
    "get_integrated_loader"
]
