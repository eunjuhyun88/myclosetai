# app/ai_pipeline/steps/step_08_quality_assessment.py
"""
‚úÖ MyCloset AI - 8Îã®Í≥Ñ: ÌíàÏßà ÌèâÍ∞Ä (Quality Assessment) - ÏôÑÏ†ÑÌïú Í∏∞Îä• Íµ¨ÌòÑ
‚úÖ AI Î™®Îç∏ Î°úÎçîÏôÄ ÏôÑÎ≤Ω Ïó∞Îèô
‚úÖ Pipeline Manager 100% Ìò∏Ìôò
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ Ïã§Ï†ú ÏûëÎèôÌïòÎäî Î™®Îì† ÌíàÏßà ÌèâÍ∞Ä Í∏∞Îä•
‚úÖ ÌÜµÏùºÎêú ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥

ÌååÏùº ÏúÑÏπò: backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache
import numpy as np
import base64
import io

# ÌïÑÏàò Ìå®ÌÇ§ÏßÄÎì§ - ÏïàÏ†ÑÌïú import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ùå PyTorch ÌïÑÏàò: pip install torch torchvision")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("‚ùå OpenCV ÌïÑÏàò: pip install opencv-python")

try:
    from PIL import Image, ImageStat, ImageEnhance, ImageFilter, ImageDraw
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("‚ùå Pillow ÌïÑÏàò: pip install Pillow")

try:
    from scipy import stats, ndimage, spatial
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è SciPy Í∂åÏû•: pip install scipy")

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("‚ö†Ô∏è Scikit-learn Í∂åÏû•: pip install scikit-learn")

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False
    print("‚ö†Ô∏è Scikit-image Í∂åÏû•: pip install scikit-image")

# Î°úÍ±∞ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)

# ==============================================
# üî• Ïó¥Í±∞Ìòï Î∞è ÏÉÅÏàò Ï†ïÏùò
# ==============================================

class QualityGrade(Enum):
    """ÌíàÏßà Îì±Í∏â"""
    EXCELLENT = "excellent"      # 90-100Ï†ê
    GOOD = "good"               # 75-89Ï†ê
    ACCEPTABLE = "acceptable"   # 60-74Ï†ê
    POOR = "poor"              # 40-59Ï†ê
    VERY_POOR = "very_poor"    # 0-39Ï†ê

class AssessmentMode(Enum):
    """ÌèâÍ∞Ä Î™®Îìú"""
    FAST = "fast"              # Îπ†Î•∏ Í∏∞Î≥∏ ÌèâÍ∞Ä
    COMPREHENSIVE = "comprehensive"  # Ï¢ÖÌï© ÌèâÍ∞Ä
    DETAILED = "detailed"      # ÏÉÅÏÑ∏ Î∂ÑÏÑù
    NEURAL = "neural"          # AI Í∏∞Î∞ò ÌèâÍ∞Ä

class QualityAspect(Enum):
    """ÌíàÏßà Ï∏°Î©¥"""
    TECHNICAL = "technical"    # Í∏∞Ïà†Ï†Å ÌíàÏßà
    PERCEPTUAL = "perceptual"  # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà
    AESTHETIC = "aesthetic"    # ÎØ∏Ï†Å ÌíàÏßà
    FUNCTIONAL = "functional"  # Í∏∞Îä•Ï†Å ÌíàÏßà

# ==============================================
# üî• ÌíàÏßà Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§
# ==============================================

@dataclass
class QualityMetrics:
    """ÌíàÏßà Î©îÌä∏Î¶≠"""
    
    # Í∏∞Ïà†Ï†Å ÌíàÏßà
    sharpness: float = 0.0
    noise_level: float = 0.0
    contrast: float = 0.0
    saturation: float = 0.0
    brightness: float = 0.0
    color_accuracy: float = 0.0
    
    # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà
    structural_similarity: float = 0.0
    perceptual_similarity: float = 0.0
    visual_quality: float = 0.0
    artifact_level: float = 0.0
    
    # ÎØ∏Ï†Å ÌíàÏßà
    composition: float = 0.0
    color_harmony: float = 0.0
    symmetry: float = 0.0
    balance: float = 0.0
    
    # Í∏∞Îä•Ï†Å ÌíàÏßà
    fitting_quality: float = 0.0
    edge_preservation: float = 0.0
    texture_quality: float = 0.0
    detail_preservation: float = 0.0
    
    # Ï†ÑÏ≤¥ Ï†êÏàò
    overall_score: float = 0.0
    confidence: float = 0.0
    
    def calculate_overall_score(self, weights: Optional[Dict[str, float]] = None) -> float:
        """Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞"""
        if weights is None:
            weights = {
                'technical': 0.3,
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.2
            }
        
        technical_score = np.mean([
            self.sharpness, self.contrast, self.color_accuracy,
            1.0 - self.noise_level  # ÎÖ∏Ïù¥Ï¶àÎäî ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå
        ])
        
        perceptual_score = np.mean([
            self.structural_similarity, self.perceptual_similarity,
            self.visual_quality, 1.0 - self.artifact_level
        ])
        
        aesthetic_score = np.mean([
            self.composition, self.color_harmony,
            self.symmetry, self.balance
        ])
        
        functional_score = np.mean([
            self.fitting_quality, self.edge_preservation,
            self.texture_quality, self.detail_preservation
        ])
        
        self.overall_score = (
            technical_score * weights['technical'] +
            perceptual_score * weights['perceptual'] +
            aesthetic_score * weights['aesthetic'] +
            functional_score * weights['functional']
        )
        
        return self.overall_score
    
    def get_grade(self) -> QualityGrade:
        """Îì±Í∏â Î∞òÌôò"""
        score = self.overall_score * 100
        
        if score >= 90:
            return QualityGrade.EXCELLENT
        elif score >= 75:
            return QualityGrade.GOOD
        elif score >= 60:
            return QualityGrade.ACCEPTABLE
        elif score >= 40:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

# ==============================================
# üî• AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
# ==============================================

class PerceptualQualityModel(nn.Module):
    """ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏"""
    
    def __init__(self):
        super().__init__()
        
        # CNN Í∏∞Î∞ò ÌäπÏßï Ï∂îÏ∂úÍ∏∞
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8))
        )
        
        # ÌíàÏßà ÏòàÏ∏°Í∏∞
        self.quality_predictor = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        quality = self.quality_predictor(features)
        return quality

class AestheticQualityModel(nn.Module):
    """ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏"""
    
    def __init__(self):
        super().__init__()
        
        # ResNet Í∏∞Î∞ò Î∞±Î≥∏
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet Î∏îÎ°ùÎì§
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ÎØ∏Ï†Å Ï†êÏàò ÏòàÏ∏°Í∏∞
        self.aesthetic_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 4),  # composition, harmony, symmetry, balance
            nn.Sigmoid()
        )
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        layers.append(self._make_resnet_block(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(self._make_resnet_block(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _make_resnet_block(self, in_channels, out_channels, stride=1):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        aesthetic_scores = self.aesthetic_head(features)
        return aesthetic_scores

class TechnicalQualityAnalyzer:
    """Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
    
    def analyze_sharpness(self, image: np.ndarray) -> float:
        """ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                # Ï†ïÍ∑úÌôî (0-1 Î≤îÏúÑ)
                return min(laplacian_var / 1000.0, 1.0)
            else:
                # PIL Í∏∞Î∞ò Í∑ºÏÇ¨Ïπò
                pil_img = Image.fromarray(image).convert('L')
                edges = pil_img.filter(ImageFilter.FIND_EDGES)
                stat = ImageStat.Stat(edges)
                return min(stat.stddev[0] / 50.0, 1.0)
        except Exception:
            return 0.5
    
    def analyze_noise_level(self, image: np.ndarray) -> float:
        """ÎÖ∏Ïù¥Ï¶à Î†àÎ≤® Î∂ÑÏÑù"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                # Í≥†Ï£ºÌåå ÏÑ±Î∂Ñ Î∂ÑÏÑùÏúºÎ°ú ÎÖ∏Ïù¥Ï¶à Ï∂îÏ†ï
                kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                filtered = cv2.filter2D(gray, -1, kernel)
                noise_level = np.std(filtered) / 255.0
                return min(noise_level, 1.0)
            else:
                # Í∞ÑÎã®Ìïú ÌëúÏ§ÄÌé∏Ï∞® Í∏∞Î∞ò Ï∂îÏ†ï
                gray = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                return min(np.std(gray) / 128.0, 1.0)
        except Exception:
            return 0.3
    
    def analyze_contrast(self, image: np.ndarray) -> float:
        """ÎåÄÎπÑ Î∂ÑÏÑù"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                contrast = gray.std() / 128.0
            else:
                gray = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                contrast = gray.std() / 128.0
            
            return min(contrast, 1.0)
        except Exception:
            return 0.5
    
    def analyze_color_accuracy(self, original: np.ndarray, processed: np.ndarray) -> float:
        """ÏÉâÏÉÅ Ï†ïÌôïÎèÑ Î∂ÑÏÑù"""
        try:
            # RGB ÌûàÏä§ÌÜ†Í∑∏Îû® ÎπÑÍµê
            hist_orig = [cv2.calcHist([original], [i], None, [256], [0, 256]) for i in range(3)] if CV2_AVAILABLE else []
            hist_proc = [cv2.calcHist([processed], [i], None, [256], [0, 256]) for i in range(3)] if CV2_AVAILABLE else []
            
            if hist_orig and hist_proc:
                correlations = [cv2.compareHist(hist_orig[i], hist_proc[i], cv2.HISTCMP_CORREL) for i in range(3)]
                return np.mean(correlations)
            else:
                # Í∞ÑÎã®Ìïú ÌèâÍ∑† ÏÉâÏÉÅ ÎπÑÍµê
                mean_orig = np.mean(original, axis=(0, 1))
                mean_proc = np.mean(processed, axis=(0, 1))
                diff = np.linalg.norm(mean_orig - mean_proc) / (255 * np.sqrt(3))
                return max(0, 1.0 - diff)
        except Exception:
            return 0.7

# ==============================================
# üî• Î©îÏù∏ QualityAssessmentStep ÌÅ¥ÎûòÏä§
# ==============================================

class QualityAssessmentStep:
    """
    ‚úÖ 8Îã®Í≥Ñ: ÏôÑÏ†ÑÌïú ÌíàÏßà ÌèâÍ∞Ä ÏãúÏä§ÌÖú
    ‚úÖ AI Î™®Îç∏ Î°úÎçîÏôÄ ÏôÑÎ≤Ω Ïó∞Îèô
    ‚úÖ Pipeline Manager Ìò∏ÌôòÏÑ±
    ‚úÖ M3 Max ÏµúÏ†ÅÌôî
    ‚úÖ Ïã§Ï†ú ÏûëÎèôÌïòÎäî Î™®Îì† Í∏∞Îä•
    """
    
    # ÏùòÎ•ò ÌÉÄÏûÖÎ≥Ñ ÌíàÏßà Í∞ÄÏ§ëÏπò
    CLOTHING_QUALITY_WEIGHTS = {
        'shirt': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'dress': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'pants': {'fitting': 0.6, 'texture': 0.2, 'edge': 0.1, 'color': 0.1},
        'jacket': {'fitting': 0.3, 'texture': 0.4, 'edge': 0.2, 'color': 0.1},
        'skirt': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'top': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'default': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1}
    }
    
    # ÏõêÎã® ÌÉÄÏûÖÎ≥Ñ ÌíàÏßà Í∏∞Ï§Ä
    FABRIC_QUALITY_STANDARDS = {
        'cotton': {'texture_importance': 0.8, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.3},
        'silk': {'texture_importance': 0.9, 'drape_importance': 0.9, 'wrinkle_tolerance': 0.2},
        'wool': {'texture_importance': 0.7, 'drape_importance': 0.7, 'wrinkle_tolerance': 0.4},
        'polyester': {'texture_importance': 0.5, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.8},
        'denim': {'texture_importance': 0.9, 'drape_importance': 0.4, 'wrinkle_tolerance': 0.6},
        'leather': {'texture_importance': 0.95, 'drape_importance': 0.3, 'wrinkle_tolerance': 0.9},
        'default': {'texture_importance': 0.7, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.5}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """‚úÖ ÌÜµÏùºÎêú ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ - Pipeline Manager ÏôÑÎ≤Ω Ìò∏Ìôò"""
        
        # 1. Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # 2. ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï∂îÏ∂ú
        self.device_type = kwargs.get('device_type', self._get_device_type())
        self.memory_gb = float(kwargs.get('memory_gb', self._get_memory_gb()))
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # 3. ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏
        self._update_config_from_kwargs(kwargs)
        
        # 4. Ï¥àÍ∏∞Ìôî
        self.is_initialized = False
        self.initialization_error = None
        self.performance_stats = {
            'total_assessments': 0,
            'total_time': 0.0,
            'average_time': 0.0,
            'last_assessment_time': 0.0,
            'average_score': 0.0,
            'peak_memory_usage': 0.0,
            'error_count': 0
        }
        
        # 5. ÌíàÏßà ÌèâÍ∞Ä ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        try:
            self._initialize_step_specific()
            self._setup_model_loader()
            self._initialize_analyzers()
            self._setup_assessment_pipeline()
            self.is_initialized = True
            self.logger.info(f"‚úÖ {self.step_name} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - M3 Max: {self.is_m3_max}")
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"‚ùå {self.step_name} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ - M3 Max ÏµúÏ†ÅÌôî"""
        if device:
            return device
        
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        
        return "cpu"
    
    def _get_device_type(self) -> str:
        """ÎîîÎ∞îÏù¥Ïä§ ÌÉÄÏûÖ Î∞òÌôò"""
        if self.device == "mps":
            return "apple_silicon"
        elif self.device == "cuda":
            return "nvidia_gpu"
        else:
            return "cpu"
    
    def _get_memory_gb(self) -> float:
        """Î©îÎ™®Î¶¨ ÌÅ¨Í∏∞ Í∞êÏßÄ"""
        try:
            if self.is_m3_max:
                return 128.0  # M3 Max Í∏∞Î≥∏Í∞í
            else:
                import psutil
                return psutil.virtual_memory().total / (1024**3)
        except:
            return 16.0  # Í∏∞Î≥∏Í∞í
    
    def _detect_m3_max(self) -> bool:
        """M3 Max Í∞êÏßÄ"""
        try:
            import platform
            if platform.system() == "Darwin":
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return "M3" in result.stdout and "Max" in result.stdout
        except:
            pass
        return False
    
    def _update_config_from_kwargs(self, kwargs: Dict[str, Any]):
        """kwargsÏóêÏÑú config ÏóÖÎç∞Ïù¥Ìä∏"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        }
        
        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _setup_model_loader(self):
        """AI Î™®Îç∏ Î°úÎçî Ïó∞Îèô"""
        try:
            from app.ai_pipeline.utils.model_loader import BaseStepMixin, get_global_model_loader
            
            # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï
            model_loader = get_global_model_loader()
            self.model_interface = model_loader.create_step_interface(self.step_name)
            
            # Ï∂îÏ≤ú Î™®Îç∏ ÏûêÎèô Î°úÎìú
            self._load_recommended_models()
            
            self.logger.info(f"üîó {self.step_name} Î™®Îç∏ Î°úÎçî Ïó∞Îèô ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Î°úÎçî Ïó∞Îèô Ïã§Ìå®, ÎÇ¥Ïû• Î™®Îç∏ ÏÇ¨Ïö©: {e}")
            self.model_interface = None
    
    def _initialize_step_specific(self):
        """8Îã®Í≥Ñ Ï†ÑÏö© Ï¥àÍ∏∞Ìôî"""
        
        # ÌíàÏßà ÌèâÍ∞Ä ÏÑ§Ï†ï
        self.assessment_config = {
            'mode': self.config.get('assessment_mode', 'comprehensive'),
            'technical_analysis_enabled': self.config.get('technical_analysis_enabled', True),
            'perceptual_analysis_enabled': self.config.get('perceptual_analysis_enabled', True),
            'aesthetic_analysis_enabled': self.config.get('aesthetic_analysis_enabled', True),
            'functional_analysis_enabled': self.config.get('functional_analysis_enabled', True),
            'detailed_analysis_enabled': self.config.get('detailed_analysis_enabled', False),
            'neural_analysis_enabled': self.config.get('neural_analysis_enabled', True)
        }
        
        # ÌíàÏßà ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.75,
            'acceptable': 0.6,
            'poor': 0.4,
            'minimum_acceptable': self.config.get('minimum_quality', 0.6)
        }
        
        # ÏµúÏ†ÅÌôî Î†àÎ≤® ÏÑ§Ï†ï
        if self.is_m3_max:
            self.optimization_level = 'maximum'
            self.batch_processing = True
            self.parallel_analysis = True
        elif self.memory_gb >= 32:
            self.optimization_level = 'high'
            self.batch_processing = True
            self.parallel_analysis = False
        else:
            self.optimization_level = 'basic'
            self.batch_processing = False
            self.parallel_analysis = False
        
        # Ï∫êÏãú ÏãúÏä§ÌÖú
        cache_size = min(200 if self.is_m3_max else 100, int(self.memory_gb * 3))
        self.assessment_cache = {}
        self.cache_max_size = cache_size
        
        self.logger.info(f"üìä 8Îã®Í≥Ñ ÏÑ§Ï†ï ÏôÑÎ£å - Î™®Îìú: {self.assessment_config['mode']}, ÏµúÏ†ÅÌôî: {self.optimization_level}")
    
    def _initialize_analyzers(self):
        """Î∂ÑÏÑùÍ∏∞Îì§ Ï¥àÍ∏∞Ìôî"""
        try:
            # 1. Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑùÍ∏∞
            self.technical_analyzer = TechnicalQualityAnalyzer(self.device)
            
            # 2. AI Î™®Îç∏Îì§ Ï¥àÍ∏∞Ìôî
            self._initialize_ai_models()
            
            # 3. ÏßÄÍ∞ÅÏ†Å Î∂ÑÏÑùÍ∏∞ (AI Í∏∞Î∞ò)
            self.perceptual_analyzer = self._create_perceptual_analyzer()
            
            # 4. ÎØ∏Ï†Å Î∂ÑÏÑùÍ∏∞ (AI Í∏∞Î∞ò)
            self.aesthetic_analyzer = self._create_aesthetic_analyzer()
            
            # 5. Í∏∞Îä•Ï†Å Î∂ÑÏÑùÍ∏∞
            self.functional_analyzer = self._create_functional_analyzer()
            
            # 6. ÏñºÍµ¥ Í∞êÏßÄÍ∏∞ (ÏñºÍµ¥ ÌíàÏßà ÌèâÍ∞ÄÏö©)
            self.face_detector = self._create_face_detector()
            
            self.logger.info("üîß Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise
    
    def _initialize_ai_models(self):
        """AI Î™®Îç∏Îì§ Ï¥àÍ∏∞Ìôî"""
        self.ai_models = {}
        
        if not TORCH_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è PyTorch ÏóÜÏùå, AI Î™®Îç∏ Í∏∞Îä• ÎπÑÌôúÏÑ±Ìôî")
            return
        
        try:
            # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏
            if self.assessment_config['perceptual_analysis_enabled']:
                self.ai_models['perceptual_quality'] = PerceptualQualityModel()
                self.ai_models['perceptual_quality'].to(self.device)
                self.ai_models['perceptual_quality'].eval()
            
            # ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏
            if self.assessment_config['aesthetic_analysis_enabled']:
                self.ai_models['aesthetic_quality'] = AestheticQualityModel()
                self.ai_models['aesthetic_quality'].to(self.device)
                self.ai_models['aesthetic_quality'].eval()
            
            # M3 Max ÏµúÏ†ÅÌôî
            if self.is_m3_max and self.device == "mps":
                for model in self.ai_models.values():
                    if hasattr(model, 'half'):
                        model.half()
            
            self.logger.info(f"üß† AI Î™®Îç∏ {len(self.ai_models)}Í∞ú Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.ai_models = {}
    
    def _setup_assessment_pipeline(self):
        """ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï"""
        
        # ÌèâÍ∞Ä ÏàúÏÑú Ï†ïÏùò
        self.assessment_pipeline = []
        
        # 1. Í∏∞Î≥∏ Ï†ÑÏ≤òÎ¶¨
        self.assessment_pipeline.append(('preprocessing', self._preprocess_for_assessment))
        
        # 2. Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù
        if self.assessment_config['technical_analysis_enabled']:
            self.assessment_pipeline.append(('technical_analysis', self._analyze_technical_quality))
        
        # 3. ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Î∂ÑÏÑù
        if self.assessment_config['perceptual_analysis_enabled']:
            self.assessment_pipeline.append(('perceptual_analysis', self._analyze_perceptual_quality))
        
        # 4. ÎØ∏Ï†Å ÌíàÏßà Î∂ÑÏÑù
        if self.assessment_config['aesthetic_analysis_enabled']:
            self.assessment_pipeline.append(('aesthetic_analysis', self._analyze_aesthetic_quality))
        
        # 5. Í∏∞Îä•Ï†Å ÌíàÏßà Î∂ÑÏÑù
        if self.assessment_config['functional_analysis_enabled']:
            self.assessment_pipeline.append(('functional_analysis', self._analyze_functional_quality))
        
        # 6. Ï¢ÖÌï© Î∂ÑÏÑù
        self.assessment_pipeline.append(('comprehensive_analysis', self._perform_comprehensive_analysis))
        
        self.logger.info(f"üîÑ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ÏôÑÎ£å - {len(self.assessment_pipeline)}Îã®Í≥Ñ")
    
    # =================================================================
    # üöÄ Î©îÏù∏ Ï≤òÎ¶¨ Ìï®Ïàò (Pipeline Manager Ìò∏Ï∂ú)
    # =================================================================
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ‚úÖ Î©îÏù∏ ÌíàÏßà ÌèâÍ∞Ä Ìï®Ïàò - Pipeline Manager ÌëúÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
        
        Args:
            fitted_image: ÌõÑÏ≤òÎ¶¨Îêú Í∞ÄÏÉÅ ÌîºÌåÖ Í≤∞Í≥º Ïù¥ÎØ∏ÏßÄ
            person_image: ÏõêÎ≥∏ Ïù∏Î¨º Ïù¥ÎØ∏ÏßÄ (ÏÑ†ÌÉùÏ†Å)
            clothing_image: ÏùòÎ•ò Ïù¥ÎØ∏ÏßÄ (ÏÑ†ÌÉùÏ†Å)
            fabric_type: ÏõêÎã® ÌÉÄÏûÖ
            clothing_type: ÏùòÎ•ò ÌÉÄÏûÖ
            **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
        
        Returns:
            Dict[str, Any]: ÌíàÏßà ÌèâÍ∞Ä Í≤∞Í≥º
        """
        start_time = time.time()
        
        try:
            # 1. Ï¥àÍ∏∞Ìôî Í≤ÄÏ¶ù
            if not self.is_initialized:
                raise ValueError(f"QualityAssessmentStepÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: {self.initialization_error}")
            
            # 2. Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Í≤ÄÏ¶ù
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None:
                raise ValueError("Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ fitted_imageÏûÖÎãàÎã§")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image is not None else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image is not None else None
            
            # 3. Ï∫êÏãú ÌôïÏù∏
            cache_key = self._generate_cache_key(fitted_img, fabric_type, clothing_type, kwargs)
            if cache_key in self.assessment_cache:
                self.logger.info("üìã Ï∫êÏãúÏóêÏÑú ÌíàÏßà ÌèâÍ∞Ä Í≤∞Í≥º Î∞òÌôò")
                cached_result = self.assessment_cache[cache_key].copy()
                cached_result['from_cache'] = True
                return cached_result
            
            # 4. Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
            if self.is_m3_max:
                self._optimize_m3_max_memory()
            
            # 5. Î©îÏù∏ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
            quality_metrics = await self._execute_assessment_pipeline(
                fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs
            )
            
            # 6. Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ±
            recommendations = await self._generate_recommendations(quality_metrics, fabric_type, clothing_type)
            
            # 7. ÏÉÅÏÑ∏ Î∂ÑÏÑù (ÏÑ†ÌÉùÏ†Å)
            detailed_analysis = {}
            if self.assessment_config['detailed_analysis_enabled']:
                detailed_analysis = await self._generate_detailed_analysis(
                    quality_metrics, fitted_img, person_img, clothing_img, fabric_type
                )
            
            # 8. ÏµúÏ¢Ö Í≤∞Í≥º Íµ¨ÏÑ±
            processing_time = time.time() - start_time
            result = self._build_final_result(
                quality_metrics, recommendations, detailed_analysis,
                processing_time, fabric_type, clothing_type
            )
            
            # 9. Ï∫êÏãú Ï†ÄÏû•
            self._save_to_cache(cache_key, result)
            
            # 10. ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self._update_performance_stats(processing_time, quality_metrics.overall_score)
            
            self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä ÏôÑÎ£å - Ï†êÏàò: {quality_metrics.overall_score:.3f} ({quality_metrics.get_grade().value})")
            return result
            
        except Exception as e:
            error_msg = f"ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}"
            self.logger.error(f"‚ùå {error_msg}")
            
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, 0.0, success=False)
            
            return {
                "success": False,
                "step_name": self.step_name,
                "error": error_msg,
                "processing_time": processing_time,
                "quality_metrics": None,
                "overall_score": 0.0,
                "grade": QualityGrade.VERY_POOR.value
            }
    
    # =================================================================
    # üîß ÌíàÏßà ÌèâÍ∞Ä ÌïµÏã¨ Ìï®ÏàòÎì§
    # =================================================================
    
    async def _execute_assessment_pipeline(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> QualityMetrics:
        """ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ"""
        
        metrics = QualityMetrics()
        intermediate_results = {}
        
        self.logger.info(f"üîÑ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë - ÏùòÎ•ò: {clothing_type}, ÏõêÎã®: {fabric_type}")
        
        for step_name, analyzer_func in self.assessment_pipeline:
            try:
                step_start = time.time()
                
                # Î≥ëÎ†¨ Ï≤òÎ¶¨ Í∞ÄÎä•Ìïú Îã®Í≥ÑÎì§ (M3 Max ÏµúÏ†ÅÌôî)
                if self.parallel_analysis and step_name in ['technical_analysis', 'perceptual_analysis']:
                    step_result = await self._process_with_m3_max_optimization(
                        fitted_img, person_img, clothing_img, analyzer_func, step_name
                    )
                else:
                    step_result = await analyzer_func(
                        fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs
                    )
                
                step_time = time.time() - step_start
                intermediate_results[step_name] = {
                    'processing_time': step_time,
                    'success': True,
                    'result': step_result
                }
                
                # Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
                if isinstance(step_result, dict):
                    for key, value in step_result.items():
                        if hasattr(metrics, key) and isinstance(value, (int, float)):
                            setattr(metrics, key, float(value))
                
                self.logger.debug(f"  ‚úì {step_name} ÏôÑÎ£å - {step_time:.3f}Ï¥à")
                
            except Exception as e:
                self.logger.warning(f"  ‚ö†Ô∏è {step_name} Ïã§Ìå®: {e}")
                intermediate_results[step_name] = {
                    'processing_time': 0,
                    'success': False,
                    'error': str(e)
                }
                continue
        
        # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
        fabric_weights = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
        clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
        
        # Í∞ÄÏ§ëÏπò Ï°∞Ìï©
        combined_weights = {
            'technical': 0.3 * fabric_weights['texture_importance'],
            'perceptual': 0.3,
            'aesthetic': 0.2,
            'functional': 0.2 * clothing_weights['fitting']
        }
        
        metrics.calculate_overall_score(combined_weights)
        
        self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å - {len(intermediate_results)}Îã®Í≥Ñ Ï≤òÎ¶¨")
        return metrics
    
    async def _process_with_m3_max_optimization(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        analyzer_func: Callable,
        step_name: str
    ) -> Dict[str, Any]:
        """M3 Max ÏµúÏ†ÅÌôî Ï≤òÎ¶¨"""
        
        if not self.is_m3_max or self.device != "mps":
            return await analyzer_func(fitted_img, person_img, clothing_img)
        
        try:
            # M3 Max Neural Engine ÌôúÏö©
            if TORCH_AVAILABLE and step_name in ['perceptual_analysis', 'aesthetic_analysis']:
                return await self._process_with_neural_engine(fitted_img, step_name)
            else:
                return await analyzer_func(fitted_img, person_img, clothing_img)
                
        except Exception as e:
            self.logger.warning(f"M3 Max ÏµúÏ†ÅÌôî Ïã§Ìå®, ÏùºÎ∞ò Ï≤òÎ¶¨Î°ú Ï†ÑÌôò: {e}")
            return await analyzer_func(fitted_img, person_img, clothing_img)
    
    async def _process_with_neural_engine(self, image: np.ndarray, analysis_type: str) -> Dict[str, Any]:
        """Neural Engine ÌôúÏö© Î∂ÑÏÑù"""
        
        if analysis_type not in self.ai_models:
            raise ValueError(f"AI Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§: {analysis_type}")
        
        model = self.ai_models[analysis_type]
        
        # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
        tensor_img = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
        tensor_img = tensor_img.unsqueeze(0).to(self.device)
        
        # Î∞òÏ†ïÎ∞ÄÎèÑ Ïó∞ÏÇ∞ (M3 Max ÏµúÏ†ÅÌôî)
        if self.is_m3_max:
            tensor_img = tensor_img.half()
        
        # Î™®Îç∏ Ï∂îÎ°†
        with torch.no_grad():
            if self.device == "mps":
                # MPS Î∞±ÏóîÎìú ÏµúÏ†ÅÌôî
                with autocast(device_type='cpu', dtype=torch.float16):
                    result = model(tensor_img)
            else:
                result = model(tensor_img)
        
        # Í≤∞Í≥º Ï≤òÎ¶¨
        if analysis_type == 'perceptual_analysis':
            return {'perceptual_similarity': float(result.cpu().squeeze())}
        elif analysis_type == 'aesthetic_analysis':
            scores = result.cpu().squeeze().numpy()
            return {
                'composition': float(scores[0]),
                'color_harmony': float(scores[1]),
                'symmetry': float(scores[2]),
                'balance': float(scores[3])
            }
        
        return {}
    
    # =================================================================
    # üîß Í∞úÎ≥Ñ Î∂ÑÏÑù Ìï®ÏàòÎì§
    # =================================================================
    
    async def _preprocess_for_assessment(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, Any]:
        """ÌèâÍ∞ÄÎ•º ÏúÑÌïú Ï†ÑÏ≤òÎ¶¨"""
        
        # 1. Ïù¥ÎØ∏ÏßÄ Ï†ïÍ∑úÌôî
        if fitted_img.dtype != np.uint8:
            fitted_img = np.clip(fitted_img * 255, 0, 255).astype(np.uint8)
        
        # 2. Ìï¥ÏÉÅÎèÑ ÌôïÏù∏ Î∞è Ï°∞Ï†ï
        h, w = fitted_img.shape[:2]
        if max(h, w) > 1024:
            scale = 1024 / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            if CV2_AVAILABLE:
                fitted_img = cv2.resize(fitted_img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # 3. Í∏∞Î≥∏ ÌíàÏßà Ï≤¥ÌÅ¨
        basic_checks = {
            'valid_shape': fitted_img.ndim == 3 and fitted_img.shape[2] == 3,
            'valid_size': fitted_img.size > 0,
            'valid_range': np.all(fitted_img >= 0) and np.all(fitted_img <= 255),
            'not_corrupted': not np.any(np.isnan(fitted_img))
        }
        
        return {
            'preprocessing_success': all(basic_checks.values()),
            'basic_checks': basic_checks,
            'processed_shape': fitted_img.shape
        }
    
    async def _analyze_technical_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        
        try:
            results = {}
            
            # 1. ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù
            results['sharpness'] = self.technical_analyzer.analyze_sharpness(fitted_img)
            
            # 2. ÎÖ∏Ïù¥Ï¶à Î†àÎ≤® Î∂ÑÏÑù
            results['noise_level'] = self.technical_analyzer.analyze_noise_level(fitted_img)
            
            # 3. ÎåÄÎπÑ Î∂ÑÏÑù
            results['contrast'] = self.technical_analyzer.analyze_contrast(fitted_img)
            
            # 4. ÏÉâÏÉÅ Ï†ïÌôïÎèÑ (ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÎäî Í≤ΩÏö∞)
            if person_img is not None:
                results['color_accuracy'] = self.technical_analyzer.analyze_color_accuracy(person_img, fitted_img)
            else:
                results['color_accuracy'] = 0.8  # Í∏∞Î≥∏Í∞í
            
            # 5. Ï±ÑÎèÑ Î∂ÑÏÑù
            results['saturation'] = self._analyze_saturation(fitted_img)
            
            # 6. Î∞ùÍ∏∞ Î∂ÑÏÑù
            results['brightness'] = self._analyze_brightness(fitted_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {
                'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5,
                'color_accuracy': 0.5, 'saturation': 0.5, 'brightness': 0.5
            }
    
    async def _analyze_perceptual_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Î∂ÑÏÑù"""
        
        try:
            results = {}
            
            # 1. AI Î™®Îç∏ Í∏∞Î∞ò ÏßÄÍ∞ÅÏ†Å ÌíàÏßà
            if 'perceptual_quality' in self.ai_models:
                neural_result = await self._process_with_neural_engine(fitted_img, 'perceptual_analysis')
                results.update(neural_result)
            
            # 2. Íµ¨Ï°∞Ï†Å Ïú†ÏÇ¨ÏÑ± (SSIM)
            if person_img is not None and SKIMAGE_AVAILABLE:
                # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
                if person_img.shape != fitted_img.shape:
                    person_resized = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
                else:
                    person_resized = person_img
                
                try:
                    ssim_score = ssim(person_resized, fitted_img, multichannel=True, channel_axis=2)
                    results['structural_similarity'] = max(0, ssim_score)
                except:
                    results['structural_similarity'] = 0.7
            else:
                results['structural_similarity'] = 0.7
            
            # 3. ÏãúÍ∞ÅÏ†Å ÌíàÏßà (Ï†ÑÌÜµÏ†Å Î∞©Î≤ï)
            results['visual_quality'] = self._calculate_visual_quality(fitted_img)
            
            # 4. ÏïÑÌã∞Ìå©Ìä∏ Î†àÎ≤®
            results['artifact_level'] = self._detect_artifacts(fitted_img)
            
            # 5. ÏßÄÍ∞ÅÏ†Å Ïú†ÏÇ¨ÏÑ± (Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï)
            if 'perceptual_similarity' not in results:
                results['perceptual_similarity'] = results.get('structural_similarity', 0.7)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {
                'structural_similarity': 0.5, 'perceptual_similarity': 0.5,
                'visual_quality': 0.5, 'artifact_level': 0.5
            }
    
    async def _analyze_aesthetic_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ÎØ∏Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        
        try:
            results = {}
            
            # 1. AI Î™®Îç∏ Í∏∞Î∞ò ÎØ∏Ï†Å ÌíàÏßà
            if 'aesthetic_quality' in self.ai_models:
                neural_result = await self._process_with_neural_engine(fitted_img, 'aesthetic_analysis')
                results.update(neural_result)
            
            # 2. Ï†ÑÌÜµÏ†Å Î∞©Î≤ïÎì§Î°ú Î≥¥ÏôÑ
            if 'composition' not in results:
                results['composition'] = self._analyze_composition(fitted_img)
            
            if 'color_harmony' not in results:
                results['color_harmony'] = self._analyze_color_harmony(fitted_img)
            
            if 'symmetry' not in results:
                results['symmetry'] = self._analyze_symmetry(fitted_img)
            
            if 'balance' not in results:
                results['balance'] = self._analyze_balance(fitted_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ÎØ∏Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {
                'composition': 0.5, 'color_harmony': 0.5,
                'symmetry': 0.5, 'balance': 0.5
            }
    
    async def _analyze_functional_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """Í∏∞Îä•Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        
        try:
            results = {}
            
            # 1. ÌîºÌåÖ ÌíàÏßà
            results['fitting_quality'] = self._analyze_fitting_quality(fitted_img, person_img, clothing_type)
            
            # 2. Ïó£ÏßÄ Î≥¥Ï°¥
            results['edge_preservation'] = self._analyze_edge_preservation(fitted_img, person_img)
            
            # 3. ÌÖçÏä§Ï≤ò ÌíàÏßà
            results['texture_quality'] = self._analyze_texture_quality(fitted_img, clothing_img, fabric_type)
            
            # 4. ÎîîÌÖåÏùº Î≥¥Ï°¥
            results['detail_preservation'] = self._analyze_detail_preservation(fitted_img, person_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Í∏∞Îä•Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {
                'fitting_quality': 0.5, 'edge_preservation': 0.5,
                'texture_quality': 0.5, 'detail_preservation': 0.5
            }
    
    async def _perform_comprehensive_analysis(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """Ï¢ÖÌï© Î∂ÑÏÑù"""
        
        try:
            results = {}
            
            # 1. Ï†ÑÏ≤¥Ï†Å ÏùºÍ¥ÄÏÑ±
            results['overall_consistency'] = self._analyze_overall_consistency(fitted_img)
            
            # 2. ÌòÑÏã§ÏÑ±
            results['realism'] = self._analyze_realism(fitted_img, person_img)
            
            # 3. ÏôÑÏÑ±ÎèÑ
            results['completeness'] = self._analyze_completeness(fitted_img)
            
            # 4. Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            confidence_factors = [
                results.get('overall_consistency', 0.5),
                results.get('realism', 0.5),
                results.get('completeness', 0.5)
            ]
            results['confidence'] = np.mean(confidence_factors)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Ï¢ÖÌï© Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {
                'overall_consistency': 0.5, 'realism': 0.5,
                'completeness': 0.5, 'confidence': 0.5
            }
    
    # =================================================================
    # üîß Í∞úÎ≥Ñ Î∂ÑÏÑù Î©îÏÑúÎìúÎì§
    # =================================================================
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """Ï±ÑÎèÑ Î∂ÑÏÑù"""
        try:
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation = hsv[:, :, 1].mean() / 255.0
            else:
                # RGBÏóêÏÑú Í∑ºÏÇ¨ Ï±ÑÎèÑ Í≥ÑÏÇ∞
                r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]
                max_rgb = np.maximum(np.maximum(r, g), b)
                min_rgb = np.minimum(np.minimum(r, g), b)
                saturation = np.mean((max_rgb - min_rgb) / (max_rgb + 1e-8)) 
            
            return min(saturation, 1.0)
        except:
            return 0.5
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """Î∞ùÍ∏∞ Î∂ÑÏÑù"""
        try:
            brightness = np.mean(image) / 255.0
            # Ï†ÅÏ†àÌïú Î∞ùÍ∏∞ Î≤îÏúÑ (0.3-0.7)ÏóêÏÑú 1.0Ïóê Í∞ÄÍπåÏö¥ Ï†êÏàò
            if 0.3 <= brightness <= 0.7:
                return 1.0 - abs(brightness - 0.5) * 2
            else:
                return max(0, 1.0 - abs(brightness - 0.5) * 4)
        except:
            return 0.5
    
    def _calculate_visual_quality(self, image: np.ndarray) -> float:
        """ÏãúÍ∞ÅÏ†Å ÌíàÏßà Í≥ÑÏÇ∞"""
        try:
            # Ïó¨Îü¨ ÏöîÏÜå Ï¢ÖÌï©
            factors = []
            
            # 1. ÏÉâÏÉÅ Î∂ÑÌè¨
            color_std = np.std(image, axis=(0, 1)).mean() / 255.0
            factors.append(min(color_std * 2, 1.0))
            
            # 2. Í∑∏ÎùºÎîîÏñ∏Ìä∏ Í∞ïÎèÑ
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2).mean()
                factors.append(min(gradient_magnitude / 100.0, 1.0))
            
            # 3. ÏóîÌä∏Î°úÌîº (Ï†ïÎ≥¥Îüâ)
            if len(factors) == 0:
                factors.append(0.5)
            
            return np.mean(factors)
        except:
            return 0.5
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ÏïÑÌã∞Ìå©Ìä∏ Í∞êÏßÄ"""
        try:
            artifacts = 0.0
            
            # 1. Î∏îÎ°úÌÇπ ÏïÑÌã∞Ìå©Ìä∏ Í∞êÏßÄ
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                # DCT Í∏∞Î∞ò Î∏îÎ°ù Í≤ΩÍ≥Ñ Í∞êÏßÄ
                for i in range(0, gray.shape[0]-8, 8):
                    for j in range(0, gray.shape[1]-8, 8):
                        block = gray[i:i+8, j:j+8]
                        if block.shape == (8, 8):
                            # Î∏îÎ°ù Í≤ΩÍ≥ÑÏóêÏÑúÏùò Í∏âÍ≤©Ìïú Î≥ÄÌôî Í∞êÏßÄ
                            edge_diff = np.abs(np.diff(block, axis=0)).mean() + np.abs(np.diff(block, axis=1)).mean()
                            if edge_diff > 30:
                                artifacts += 0.1
            
            # 2. ÎßÅÍπÖ ÏïÑÌã∞Ìå©Ìä∏ (Í≥ºÎèÑÌïú ÏÉ§ÌîÑÎãù)
            if CV2_AVAILABLE:
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                if laplacian_var > 2000:  # Í≥ºÎèÑÌïú Ïó£ÏßÄ Í∞ïÌôî
                    artifacts += 0.2
            
            # 3. ÎÖ∏Ïù¥Ï¶à Ìå®ÌÑ¥
            noise_level = np.std(image) / 255.0
            if noise_level > 0.15:
                artifacts += 0.3
            
            return min(artifacts, 1.0)
        except:
            return 0.3
    
    def _analyze_composition(self, image: np.ndarray) -> float:
        """Íµ¨ÎèÑ Î∂ÑÏÑù"""
        try:
            # Ìô©Í∏àÎπÑ, 3Î∂ÑÌï† Î≤ïÏπô Îì±ÏùÑ Í≥†Î†§Ìïú Íµ¨ÎèÑ Î∂ÑÏÑù
            h, w = image.shape[:2]
            
            # 3Î∂ÑÌï† ÏßÄÏ†êÎì§
            thirds_h = [h//3, 2*h//3]
            thirds_w = [w//3, 2*w//3]
            
            # Í¥ÄÏã¨ ÏòÅÏó≠ Í∞êÏßÄ (Ïó£ÏßÄÍ∞Ä ÎßéÏùÄ ÏòÅÏó≠)
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # 3Î∂ÑÌï† ÏßÄÏ†ê Í∑ºÏ≤òÏùò Ïó£ÏßÄ Î∞ÄÎèÑ
                composition_score = 0
                for th in thirds_h:
                    for tw in thirds_w:
                        region = edges[max(0, th-20):min(h, th+20), max(0, tw-20):min(w, tw+20)]
                        if region.size > 0:
                            edge_density = np.sum(region) / (region.size * 255)
                            composition_score += edge_density
                
                return min(composition_score / 4, 1.0)
            else:
                return 0.6  # Í∏∞Î≥∏Í∞í
        except:
            return 0.5
    
    def _analyze_color_harmony(self, image: np.ndarray) -> float:
        """ÏÉâÏÉÅ Ï°∞Ìôî Î∂ÑÏÑù"""
        try:
            if SKLEARN_AVAILABLE:
                # ÏÉâÏÉÅ ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏúºÎ°ú Ï£ºÏöî ÏÉâÏÉÅ Ï∂îÏ∂ú
                pixels = image.reshape(-1, 3)
                
                # ÏÉòÌîåÎßÅÏúºÎ°ú ÏÑ±Îä• ÏµúÏ†ÅÌôî
                if len(pixels) > 10000:
                    indices = np.random.choice(len(pixels), 10000, replace=False)
                    pixels = pixels[indices]
                
                kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                kmeans.fit(pixels)
                
                # Ï£ºÏöî ÏÉâÏÉÅÎì§ Í∞ÑÏùò Í±∞Î¶¨ Î∂ÑÏÑù
                centers = kmeans.cluster_centers_
                distances = []
                for i in range(len(centers)):
                    for j in range(i+1, len(centers)):
                        dist = np.linalg.norm(centers[i] - centers[j])
                        distances.append(dist)
                
                # Ï†ÅÏ†àÌïú ÏÉâÏÉÅ Í∞ÑÍ≤© (ÎÑàÎ¨¥ Í∞ÄÍπùÏßÄÎèÑ Î©ÄÏßÄÎèÑ ÏïäÍ≤å)
                avg_distance = np.mean(distances)
                optimal_distance = 100  # RGB Í≥µÍ∞ÑÏóêÏÑú Ï†ÅÏ†àÌïú Í±∞Î¶¨
                harmony_score = 1.0 - abs(avg_distance - optimal_distance) / optimal_distance
                
                return max(0, min(harmony_score, 1.0))
            else:
                # Í∞ÑÎã®Ìïú ÏÉâÏÉÅ Î∂ÑÏÇ∞ Í∏∞Î∞ò Î∂ÑÏÑù
                color_std = np.std(image, axis=(0, 1))
                balance = 1.0 - np.std(color_std) / 128.0
                return max(0, min(balance, 1.0))
        except:
            return 0.6
    
    def _analyze_symmetry(self, image: np.ndarray) -> float:
        """ÎåÄÏπ≠ÏÑ± Î∂ÑÏÑù"""
        try:
            h, w = image.shape[:2]
            
            # ÏàòÏßÅ ÎåÄÏπ≠ÏÑ± (Ï¢åÏö∞)
            left_half = image[:, :w//2]
            right_half = np.fliplr(image[:, w//2:])
            
            # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
            min_width = min(left_half.shape[1], right_half.shape[1])
            left_half = left_half[:, :min_width]
            right_half = right_half[:, :min_width]
            
            # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            if SKIMAGE_AVAILABLE and left_half.shape == right_half.shape:
                try:
                    symmetry_score = ssim(left_half, right_half, multichannel=True, channel_axis=2)
                    return max(0, symmetry_score)
                except:
                    pass
            
            # ÎåÄÏïà: MSE Í∏∞Î∞ò
            mse = np.mean((left_half.astype(float) - right_half.astype(float))**2)
            symmetry_score = max(0, 1.0 - mse / (255**2))
            
            return symmetry_score
        except:
            return 0.4
    
    def _analyze_balance(self, image: np.ndarray) -> float:
        """Í∑†Ìòï Î∂ÑÏÑù"""
        try:
            h, w = image.shape[:2]
            
            # Ïù¥ÎØ∏ÏßÄÎ•º 4Î∂ÑÌï†Î°ú ÎÇòÎàÑÏñ¥ Í∞Å ÏòÅÏó≠Ïùò ÏãúÍ∞ÅÏ†Å Î¨¥Í≤å Í≥ÑÏÇ∞
            quarters = [
                image[:h//2, :w//2],      # Ï¢åÏÉÅ
                image[:h//2, w//2:],      # Ïö∞ÏÉÅ
                image[h//2:, :w//2],      # Ï¢åÌïò
                image[h//2:, w//2:]       # Ïö∞Ìïò
            ]
            
            # Í∞Å ÏòÅÏó≠Ïùò ÏãúÍ∞ÅÏ†Å Î¨¥Í≤å Í≥ÑÏÇ∞
            weights = []
            for quarter in quarters:
                if quarter.size > 0:
                    # Î∞ùÍ∏∞ + ÎåÄÎπÑ + Ï±ÑÎèÑÎ•º Ï¢ÖÌï©Ìïú ÏãúÍ∞ÅÏ†Å Î¨¥Í≤å
                    brightness = np.mean(quarter)
                    contrast = np.std(quarter)
                    weight = brightness * 0.5 + contrast * 0.5
                    weights.append(weight)
            
            if len(weights) == 4:
                # ÎåÄÍ∞ÅÏÑ† Í∑†Ìòï (Ï¢åÏÉÅ+Ïö∞Ìïò vs Ïö∞ÏÉÅ+Ï¢åÌïò)
                diagonal1 = weights[0] + weights[3]  # Ï¢åÏÉÅ + Ïö∞Ìïò
                diagonal2 = weights[1] + weights[2]  # Ïö∞ÏÉÅ + Ï¢åÌïò
                diagonal_balance = 1.0 - abs(diagonal1 - diagonal2) / max(diagonal1 + diagonal2, 1)
                
                # ÏàòÏßÅ Í∑†Ìòï (ÏÉÅÎã® vs ÌïòÎã®)
                top = weights[0] + weights[1]
                bottom = weights[2] + weights[3]
                vertical_balance = 1.0 - abs(top - bottom) / max(top + bottom, 1)
                
                # ÏàòÌèâ Í∑†Ìòï (Ï¢åÏ∏° vs Ïö∞Ï∏°)
                left = weights[0] + weights[2]
                right = weights[1] + weights[3]
                horizontal_balance = 1.0 - abs(left - right) / max(left + right, 1)
                
                # Ï¢ÖÌï© Í∑†Ìòï
                balance_score = (diagonal_balance + vertical_balance + horizontal_balance) / 3
                return max(0, min(balance_score, 1.0))
            
            return 0.5
        except:
            return 0.5
    
    def _analyze_fitting_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_type: str) -> float:
        """ÌîºÌåÖ ÌíàÏßà Î∂ÑÏÑù"""
        try:
            if person_img is None:
                return 0.6  # Í∏∞Î≥∏Í∞í
            
            # 1. Ïã†Ï≤¥ Ïú§Í≥ΩÏÑ†Í≥º ÏùòÎ•òÏùò ÏùºÏπòÎèÑ
            fitting_score = 0.0
            
            if CV2_AVAILABLE:
                # Ïó£ÏßÄ Í∏∞Î∞ò Î∂ÑÏÑù
                fitted_edges = cv2.Canny(cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY), 50, 150)
                person_edges = cv2.Canny(cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY), 50, 150)
                
                # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
                if fitted_edges.shape != person_edges.shape:
                    person_edges = cv2.resize(person_edges, (fitted_edges.shape[1], fitted_edges.shape[0]))
                
                # Ïó£ÏßÄ ÏùºÏπòÎèÑ
                edge_overlap = np.sum((fitted_edges > 0) & (person_edges > 0))
                total_edges = np.sum(fitted_edges > 0) + np.sum(person_edges > 0)
                if total_edges > 0:
                    fitting_score = (2 * edge_overlap) / total_edges
            
            # 2. ÏùòÎ•ò ÌÉÄÏûÖÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©
            clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
            fitting_weight = clothing_weights['fitting']
            
            return min(fitting_score * fitting_weight + 0.3, 1.0)
        except:
            return 0.5
    
    def _analyze_edge_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """Ïó£ÏßÄ Î≥¥Ï°¥ Î∂ÑÏÑù"""
        try:
            if person_img is None or not CV2_AVAILABLE:
                return 0.6
            
            # ÏõêÎ≥∏Í≥º ÌîºÌåÖ Í≤∞Í≥ºÏùò Ïó£ÏßÄ ÎπÑÍµê
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
            if fitted_gray.shape != person_gray.shape:
                person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
            
            # Ïó£ÏßÄ Í∞êÏßÄ
            fitted_edges = cv2.Canny(fitted_gray, 50, 150)
            person_edges = cv2.Canny(person_gray, 50, 150)
            
            # Ïó£ÏßÄ Î≥¥Ï°¥Î•† Í≥ÑÏÇ∞
            preserved_edges = np.sum((fitted_edges > 0) & (person_edges > 0))
            original_edges = np.sum(person_edges > 0)
            
            if original_edges > 0:
                preservation_rate = preserved_edges / original_edges
                return min(preservation_rate, 1.0)
            
            return 0.6
        except:
            return 0.5
    
    def _analyze_texture_quality(self, fitted_img: np.ndarray, clothing_img: Optional[np.ndarray], fabric_type: str) -> float:
        """ÌÖçÏä§Ï≤ò ÌíàÏßà Î∂ÑÏÑù"""
        try:
            # 1. ÌÖçÏä§Ï≤ò ÏùºÍ¥ÄÏÑ±
            texture_score = 0.0
            
            if SKIMAGE_AVAILABLE:
                # LBP (Local Binary Pattern) Í∏∞Î∞ò ÌÖçÏä§Ï≤ò Î∂ÑÏÑù
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                # LBP ÌäπÏßï Ï∂îÏ∂ú
                radius = 3
                n_points = 8 * radius
                lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')
                
                # ÌÖçÏä§Ï≤ò Í∑†ÏùºÏÑ± (LBP ÌûàÏä§ÌÜ†Í∑∏Îû®Ïùò ÏóîÌä∏Î°úÌîº)
                hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, range=(0, n_points + 2))
                hist = hist.astype(float)
                hist /= (hist.sum() + 1e-8)
                
                # ÏóîÌä∏Î°úÌîº Í≥ÑÏÇ∞ (ÎÜíÏùÑÏàòÎ°ù ÌÖçÏä§Ï≤òÍ∞Ä Î≥µÏû°)
                entropy_score = -np.sum(hist * np.log2(hist + 1e-8))
                texture_score = min(entropy_score / 8.0, 1.0)  # Ï†ïÍ∑úÌôî
            
            # 2. ÏõêÎã® ÌÉÄÏûÖÎ≥Ñ Í∏∞Ï§Ä Ï†ÅÏö©
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            texture_importance = fabric_standards['texture_importance']
            
            # 3. ÌÖçÏä§Ï≤ò ÏÑ†Î™ÖÎèÑ
            if CV2_AVAILABLE:
                # Sobel ÌïÑÌÑ∞Î°ú ÌÖçÏä§Ï≤ò ÏÑ∏Î∂ÄÏÇ¨Ìï≠ Î∂ÑÏÑù
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                texture_sharpness = np.sqrt(sobel_x**2 + sobel_y**2).mean() / 255.0
                texture_score = (texture_score + min(texture_sharpness * 2, 1.0)) / 2
            
            return texture_score * texture_importance + (1 - texture_importance) * 0.7
        except:
            return 0.6
    
    def _analyze_detail_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """ÎîîÌÖåÏùº Î≥¥Ï°¥ Î∂ÑÏÑù"""
        try:
            if person_img is None:
                return 0.6
            
            # 1. Í≥†Ï£ºÌåå ÏÑ±Î∂Ñ ÎπÑÍµê
            detail_score = 0.0
            
            if CV2_AVAILABLE:
                # ÎùºÌîåÎùºÏãúÏïà ÌïÑÌÑ∞Î°ú ÏÑ∏Î∂ÄÏÇ¨Ìï≠ Ï∂îÏ∂ú
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
                
                # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
                if fitted_gray.shape != person_gray.shape:
                    person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
                
                # Í≥†Ï£ºÌåå ÏÑ±Î∂Ñ Ï∂îÏ∂ú
                fitted_detail = cv2.Laplacian(fitted_gray, cv2.CV_64F)
                person_detail = cv2.Laplacian(person_gray, cv2.CV_64F)
                
                # ÏÑ∏Î∂ÄÏÇ¨Ìï≠ Î≥¥Ï°¥Î•†
                fitted_detail_energy = np.sum(np.abs(fitted_detail))
                person_detail_energy = np.sum(np.abs(person_detail))
                
                if person_detail_energy > 0:
                    detail_score = min(fitted_detail_energy / person_detail_energy, 1.0)
                else:
                    detail_score = 0.6
            
            # 2. ÏñºÍµ¥ ÏÑ∏Î∂ÄÏÇ¨Ìï≠ ÌäπÎ≥Ñ Î∂ÑÏÑù
            faces = self._detect_faces_for_quality(fitted_img)
            if len(faces) > 0 and person_img is not None:
                face_detail_score = self._analyze_face_detail_preservation(fitted_img, person_img, faces)
                detail_score = (detail_score + face_detail_score) / 2
            
            return detail_score
        except:
            return 0.5
    
    def _analyze_overall_consistency(self, image: np.ndarray) -> float:
        """Ï†ÑÏ≤¥Ï†Å ÏùºÍ¥ÄÏÑ± Î∂ÑÏÑù"""
        try:
            consistency_factors = []
            
            # 1. ÏÉâÏÉÅ ÏùºÍ¥ÄÏÑ±
            color_consistency = self._calculate_color_consistency(image)
            consistency_factors.append(color_consistency)
            
            # 2. Ï°∞Î™Ö ÏùºÍ¥ÄÏÑ±
            lighting_consistency = self._calculate_lighting_consistency(image)
            consistency_factors.append(lighting_consistency)
            
            # 3. Ïä§ÌÉÄÏùº ÏùºÍ¥ÄÏÑ±
            style_consistency = self._calculate_style_consistency(image)
            consistency_factors.append(style_consistency)
            
            return np.mean(consistency_factors)
        except:
            return 0.6
    
    def _analyze_realism(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """ÌòÑÏã§ÏÑ± Î∂ÑÏÑù"""
        try:
            realism_factors = []
            
            # 1. ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï°∞Î™Ö
            lighting_realism = self._assess_lighting_realism(fitted_img)
            realism_factors.append(lighting_realism)
            
            # 2. Î¨ºÎ¶¨Ï†Å ÌÉÄÎãπÏÑ± (ÎìúÎ†àÏù¥Ìïë, Ï£ºÎ¶Ñ Îì±)
            physics_realism = self._assess_physics_realism(fitted_img)
            realism_factors.append(physics_realism)
            
            # 3. Ïù∏Ï≤¥ ÎπÑÎ°Ä
            if person_img is not None:
                proportion_realism = self._assess_proportion_realism(fitted_img, person_img)
                realism_factors.append(proportion_realism)
            
            return np.mean(realism_factors)
        except:
            return 0.6
    
    def _analyze_completeness(self, image: np.ndarray) -> float:
        """ÏôÑÏÑ±ÎèÑ Î∂ÑÏÑù"""
        try:
            completeness_factors = []
            
            # 1. Ïù¥ÎØ∏ÏßÄ Í≤ΩÍ≥Ñ ÏôÑÏÑ±ÎèÑ
            boundary_completeness = self._check_boundary_completeness(image)
            completeness_factors.append(boundary_completeness)
            
            # 2. ÏùòÎ•ò ÏôÑÏÑ±ÎèÑ
            clothing_completeness = self._check_clothing_completeness(image)
            completeness_factors.append(clothing_completeness)
            
            # 3. Ï†ÑÏ≤¥Ï†Å ÏôÑÏÑ±ÎèÑ
            overall_completeness = self._check_overall_completeness(image)
            completeness_factors.append(overall_completeness)
            
            return np.mean(completeness_factors)
        except:
            return 0.7
    
    # =================================================================
    # üîß Ïú†Ìã∏Î¶¨Ìã∞ Î∂ÑÏÑù Î©îÏÑúÎìúÎì§
    # =================================================================
    
    def _detect_faces_for_quality(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """ÌíàÏßà ÌèâÍ∞ÄÏö© ÏñºÍµ¥ Í∞êÏßÄ"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                return faces.tolist()
            else:
                # Ìú¥Î¶¨Ïä§Ìã± Í∏∞Î∞ò ÏñºÍµ¥ ÏòÅÏó≠ Ï∂îÏ†ï
                h, w = image.shape[:2]
                return [(w//4, h//6, w//2, h//3)]
        except:
            return []
    
    def _analyze_face_detail_preservation(self, fitted_img: np.ndarray, person_img: np.ndarray, faces: List) -> float:
        """ÏñºÍµ¥ ÏÑ∏Î∂ÄÏÇ¨Ìï≠ Î≥¥Ï°¥ Î∂ÑÏÑù"""
        try:
            if len(faces) == 0:
                return 0.6
            
            face_scores = []
            for (x, y, w, h) in faces:
                # ÏñºÍµ¥ ÏòÅÏó≠ Ï∂îÏ∂ú
                fitted_face = fitted_img[y:y+h, x:x+w]
                person_face = person_img[y:y+h, x:x+w] if person_img.shape[:2] == fitted_img.shape[:2] else person_img
                
                if fitted_face.size > 0 and person_face.size > 0:
                    # ÌÅ¨Í∏∞ ÎßûÏ∂îÍ∏∞
                    if fitted_face.shape != person_face.shape:
                        person_face = cv2.resize(person_face, (fitted_face.shape[1], fitted_face.shape[0])) if CV2_AVAILABLE else person_face
                    
                    # SSIMÏúºÎ°ú ÏñºÍµ¥ Ïú†ÏÇ¨ÎèÑ Ï∏°Ï†ï
                    if SKIMAGE_AVAILABLE and fitted_face.shape == person_face.shape:
                        try:
                            face_similarity = ssim(fitted_face, person_face, multichannel=True, channel_axis=2)
                            face_scores.append(max(0, face_similarity))
                        except:
                            face_scores.append(0.6)
                    else:
                        face_scores.append(0.6)
            
            return np.mean(face_scores) if face_scores else 0.6
        except:
            return 0.5
    
    def _calculate_color_consistency(self, image: np.ndarray) -> float:
        """ÏÉâÏÉÅ ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞"""
        try:
            # Ïù¥ÎØ∏ÏßÄÎ•º Ïó¨Îü¨ ÏòÅÏó≠ÏúºÎ°ú ÎÇòÎàÑÏñ¥ ÏÉâÏÉÅ Î∂ÑÌè¨ Î∂ÑÏÑù
            h, w = image.shape[:2]
            regions = [
                image[:h//2, :w//2],      # Ï¢åÏÉÅ
                image[:h//2, w//2:],      # Ïö∞ÏÉÅ
                image[h//2:, :w//2],      # Ï¢åÌïò
                image[h//2:, w//2:]       # Ïö∞Ìïò
            ]
            
            # Í∞Å ÏòÅÏó≠Ïùò ÌèâÍ∑† ÏÉâÏÉÅ
            region_colors = []
            for region in regions:
                if region.size > 0:
                    mean_color = np.mean(region, axis=(0, 1))
                    region_colors.append(mean_color)
            
            if len(region_colors) >= 2:
                # ÏòÅÏó≠ Í∞Ñ ÏÉâÏÉÅ Ï∞®Ïù¥ Í≥ÑÏÇ∞
                color_diffs = []
                for i in range(len(region_colors)):
                    for j in range(i+1, len(region_colors)):
                        diff = np.linalg.norm(region_colors[i] - region_colors[j])
                        color_diffs.append(diff)
                
                # Ï†ÅÏ†àÌïú ÏÉâÏÉÅ ÏùºÍ¥ÄÏÑ± (ÎÑàÎ¨¥ uniformÌïòÏßÄÎèÑ diverseÌïòÏßÄÎèÑ ÏïäÍ≤å)
                avg_diff = np.mean(color_diffs)
                consistency = max(0, 1.0 - avg_diff / 128.0)
                return min(consistency, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _calculate_lighting_consistency(self, image: np.ndarray) -> float:
        """Ï°∞Î™Ö ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞"""
        try:
            # Î∞ùÍ∏∞ Î∂ÑÌè¨Ïùò ÏùºÍ¥ÄÏÑ± Î∂ÑÏÑù
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # Ïù¥ÎØ∏ÏßÄÎ•º Í∑∏Î¶¨ÎìúÎ°ú ÎÇòÎàÑÏñ¥ Í∞Å ÏòÅÏó≠Ïùò Î∞ùÍ∏∞ Î∂ÑÏÑù
            h, w = gray.shape
            grid_size = 4
            brightnesses = []
            
            for i in range(grid_size):
                for j in range(grid_size):
                    y1, y2 = i * h // grid_size, (i + 1) * h // grid_size
                    x1, x2 = j * w // grid_size, (j + 1) * w // grid_size
                    region = gray[y1:y2, x1:x2]
                    if region.size > 0:
                        brightnesses.append(np.mean(region))
            
            if len(brightnesses) > 1:
                # Î∞ùÍ∏∞ Î∂ÑÌè¨Ïùò ÌëúÏ§ÄÌé∏Ï∞®Í∞Ä ÎÇÆÏùÑÏàòÎ°ù ÏùºÍ¥ÄÏÑ±Ïù¥ ÎÜíÏùå
                brightness_std = np.std(brightnesses)
                consistency = max(0, 1.0 - brightness_std / 128.0)
                return min(consistency, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _calculate_style_consistency(self, image: np.ndarray) -> float:
        """Ïä§ÌÉÄÏùº ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞"""
        try:
            # ÌÖçÏä§Ï≤òÏôÄ Ìå®ÌÑ¥Ïùò ÏùºÍ¥ÄÏÑ±
            if SKIMAGE_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                
                # Ïó¨Îü¨ ÏòÅÏó≠ÏóêÏÑú LBP ÌäπÏßï Ï∂îÏ∂ú
                h, w = gray.shape
                regions = [
                    gray[:h//2, :w//2],
                    gray[:h//2, w//2:],
                    gray[h//2:, :w//2],
                    gray[h//2:, w//2:]
                ]
                
                lbp_histograms = []
                for region in regions:
                    if region.size > 64:  # ÏµúÏÜå ÌÅ¨Í∏∞ ÌôïÏù∏
                        lbp = feature.local_binary_pattern(region, 8, 1, method='uniform')
                        hist, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10))
                        hist = hist.astype(float) / (hist.sum() + 1e-8)
                        lbp_histograms.append(hist)
                
                if len(lbp_histograms) >= 2:
                    # ÌûàÏä§ÌÜ†Í∑∏Îû® Í∞Ñ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                    similarities = []
                    for i in range(len(lbp_histograms)):
                        for j in range(i+1, len(lbp_histograms)):
                            # Bhattacharyya distance
                            bc = np.sum(np.sqrt(lbp_histograms[i] * lbp_histograms[j]))
                            similarity = bc
                            similarities.append(similarity)
                    
                    return np.mean(similarities)
            
            return 0.7
        except:
            return 0.6
    
    def _assess_lighting_realism(self, image: np.ndarray) -> float:
        """Ï°∞Î™Ö ÌòÑÏã§ÏÑ± ÌèâÍ∞Ä"""
        try:
            # Í∑∏Î¶ºÏûêÏôÄ ÌïòÏù¥ÎùºÏù¥Ìä∏Ïùò ÏûêÏó∞Ïä§Îü¨ÏõÄ
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # ÌûàÏä§ÌÜ†Í∑∏Îû® Î∂ÑÏÑù
            hist, _ = np.histogram(gray, bins=256, range=(0, 256))
            hist = hist.astype(float) / hist.sum()
            
            # ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï°∞Î™ÖÏùÄ Î≥¥ÌÜµ Ï†ïÍ∑úÎ∂ÑÌè¨Ïóê Í∞ÄÍπåÏõÄ
            # Í∑πÎã®Ï†ÅÏù∏ Í∞íÎì§ (ÏàúÎ∞±, ÏàúÌùë)Ïù¥ ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ Î∂ÄÏûêÏó∞Ïä§Îü¨ÏõÄ
            extreme_ratio = (hist[0] + hist[-1])  # 0Í≥º 255 Í∞íÏùò ÎπÑÏú®
            if extreme_ratio < 0.1:  # 10% ÎØ∏ÎßåÏù¥Î©¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ
                return 0.9
            elif extreme_ratio < 0.2:
                return 0.7
            else:
                return max(0.3, 1.0 - extreme_ratio)
        except:
            return 0.6
    
    def _assess_physics_realism(self, image: np.ndarray) -> float:
        """Î¨ºÎ¶¨Ï†Å ÌòÑÏã§ÏÑ± ÌèâÍ∞Ä"""
        try:
            # ÏùòÎ•òÏùò ÎìúÎ†àÏù¥ÌïëÍ≥º Ï£ºÎ¶ÑÏùò ÏûêÏó∞Ïä§Îü¨ÏõÄ
            physics_score = 0.7  # Í∏∞Î≥∏ Ï†êÏàò
            
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # Ïó£ÏßÄ Î∞©Ìñ•ÏÑ± Î∂ÑÏÑù (ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï£ºÎ¶ÑÏùÄ ÌäπÏ†ï Î∞©Ìñ•ÏÑ±ÏùÑ Í∞ÄÏßê)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                
                # Í∑∏ÎùºÎîîÏñ∏Ìä∏ Î∞©Ìñ• Í≥ÑÏÇ∞
                angles = np.arctan2(grad_y, grad_x)
                
                # Î∞©Ìñ•ÏÑ±Ïùò ÏùºÍ¥ÄÏÑ± (ÏôÑÏ†ÑÌûà randomÌïòÏßÄ ÏïäÍ≥† Ïñ¥ÎäêÏ†ïÎèÑ Ìå®ÌÑ¥Ïù¥ ÏûàÏñ¥Ïïº Ìï®)
                angle_hist, _ = np.histogram(angles, bins=8, range=(-np.pi, np.pi))
                angle_hist = angle_hist.astype(float) / (angle_hist.sum() + 1e-8)
                
                # ÏóîÌä∏Î°úÌîºÍ∞Ä Ï†ÅÎãπÌï¥Ïïº Ìï® (ÎÑàÎ¨¥ uniformÌïòÏßÄÎèÑ ÎÑàÎ¨¥ concentratedÌïòÏßÄÎèÑ ÏïäÍ≤å)
                angle_entropy = -np.sum(angle_hist * np.log2(angle_hist + 1e-8))
                optimal_entropy = 2.5  # Í≤ΩÌóòÏ†Å Í∞í
                entropy_score = max(0, 1.0 - abs(angle_entropy - optimal_entropy) / optimal_entropy)
                
                physics_score = (physics_score + entropy_score) / 2
            
            return physics_score
        except:
            return 0.6
    
    def _assess_proportion_realism(self, fitted_img: np.ndarray, person_img: np.ndarray) -> float:
        """ÎπÑÎ°Ä ÌòÑÏã§ÏÑ± ÌèâÍ∞Ä"""
        try:
            # Ïã†Ï≤¥ ÎπÑÎ°ÄÏùò Ïú†ÏßÄ Ï†ïÎèÑ
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            # Ïú§Í≥ΩÏÑ† Í∏∞Î∞ò ÎπÑÎ°Ä Î∂ÑÏÑù
            if CV2_AVAILABLE:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
                
                # Ï£ºÏöî Ïã†Ï≤¥ Î∂ÄÏúÑÏùò Ïú§Í≥Ω Í∞êÏßÄ
                fitted_contours, _ = cv2.findContours(cv2.Canny(fitted_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                person_contours, _ = cv2.findContours(cv2.Canny(person_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                if fitted_contours and person_contours:
                    # Í∞ÄÏû• ÌÅ∞ Ïú§Í≥ΩÏÑ† (Ïã†Ï≤¥) ÎπÑÍµê
                    fitted_main = max(fitted_contours, key=cv2.contourArea)
                    person_main = max(person_contours, key=cv2.contourArea)
                    
                    # Ïú§Í≥ΩÏÑ†Ïùò Î™®Î©òÌä∏ ÎπÑÍµê (ÌòïÌÉú Ïú†ÏÇ¨ÏÑ±)
                    fitted_moments = cv2.moments(fitted_main)
                    person_moments = cv2.moments(person_main)
                    
                    # Hu moments (ÌòïÌÉú Î∂àÎ≥Ä ÌäπÏßï)
                    fitted_hu = cv2.HuMoments(fitted_moments).flatten()
                    person_hu = cv2.HuMoments(person_moments).flatten()
                    
                    # Î°úÍ∑∏ Î≥ÄÌôòÏúºÎ°ú Ï†ïÍ∑úÌôî
                    fitted_hu = -np.sign(fitted_hu) * np.log10(np.abs(fitted_hu) + 1e-10)
                    person_hu = -np.sign(person_hu) * np.log10(np.abs(person_hu) + 1e-10)
                    
                    # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                    similarity = np.exp(-np.sum(np.abs(fitted_hu - person_hu)))
                    return min(similarity, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _check_boundary_completeness(self, image: np.ndarray) -> float:
        """Í≤ΩÍ≥Ñ ÏôÑÏÑ±ÎèÑ ÌôïÏù∏"""
        try:
            h, w = image.shape[:2]
            
            # Ïù¥ÎØ∏ÏßÄ Í≤ΩÍ≥ÑÏùò Í∏âÍ≤©Ìïú Î≥ÄÌôî Í∞êÏßÄ
            boundary_issues = 0
            
            # ÏÉÅÌïòÏ¢åÏö∞ Í≤ΩÍ≥Ñ Ï≤¥ÌÅ¨
            boundaries = [
                image[0, :],      # ÏÉÅÎã®
                image[-1, :],     # ÌïòÎã®
                image[:, 0],      # Ï¢åÏ∏°
                image[:, -1]      # Ïö∞Ï∏°
            ]
            
            for boundary in boundaries:
                if boundary.size > 0:
                    # Í≤ΩÍ≥ÑÏóêÏÑúÏùò Í∏âÍ≤©Ìïú ÏÉâÏÉÅ Î≥ÄÌôî
                    if len(boundary.shape) == 2:  # RGB
                        diff = np.sum(np.abs(np.diff(boundary, axis=0)))
                    else:  # 1D
                        diff = np.sum(np.abs(np.diff(boundary)))
                    
                    # Ï†ïÍ∑úÌôîÎêú Î≥ÄÌôîÎüâ
                    normalized_diff = diff / (len(boundary) * 255 * 3)
                    if normalized_diff > 0.5:  # ÏûÑÍ≥ÑÍ∞í
                        boundary_issues += 1
            
            completeness = max(0, 1.0 - boundary_issues / 4.0)
            return completeness
        except:
            return 0.8
    
    def _check_clothing_completeness(self, image: np.ndarray) -> float:
        """ÏùòÎ•ò ÏôÑÏÑ±ÎèÑ ÌôïÏù∏"""
        try:
            # ÏùòÎ•ò ÏòÅÏó≠Ïùò Ïó∞ÏÜçÏÑ±Í≥º ÏôÑÏÑ±ÎèÑ
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ÏùòÎ•ò ÏòÅÏó≠ Ï∂îÏ†ï (Ï§ëÍ∞Ñ Î∞ùÍ∏∞ ÏòÅÏó≠)
                clothing_mask = ((gray > 50) & (gray < 200)).astype(np.uint8)
                
                # Ïó∞Í≤∞Îêú Íµ¨ÏÑ±ÏöîÏÜå Î∂ÑÏÑù
                num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(clothing_mask)
                
                if num_labels > 1:
                    # Í∞ÄÏû• ÌÅ∞ Ïó∞Í≤∞ Íµ¨ÏÑ±ÏöîÏÜå (Ï£º ÏùòÎ•ò)
                    main_component_size = np.max(stats[1:, cv2.CC_STAT_AREA])
                    total_clothing_area = np.sum(stats[1:, cv2.CC_STAT_AREA])
                    
                    # Ï£º ÏùòÎ•òÍ∞Ä Ï†ÑÏ≤¥ ÏùòÎ•ò ÏòÅÏó≠ÏóêÏÑú Ï∞®ÏßÄÌïòÎäî ÎπÑÏú®
                    main_ratio = main_component_size / (total_clothing_area + 1e-8)
                    
                    # ÎπÑÏú®Ïù¥ ÎÜíÏùÑÏàòÎ°ù ÏôÑÏÑ±ÎèÑÍ∞Ä Ï¢ãÏùå (ÌååÌé∏ÌôîÎêòÏßÄ ÏïäÏùå)
                    return min(main_ratio, 1.0)
            
            return 0.8
        except:
            return 0.7
    
    def _check_overall_completeness(self, image: np.ndarray) -> float:
        """Ï†ÑÏ≤¥Ï†Å ÏôÑÏÑ±ÎèÑ ÌôïÏù∏"""
        try:
            completeness_factors = []
            
            # 1. ÏÉâÏÉÅ ÏùºÍ¥ÄÏÑ±
            color_completeness = self._calculate_color_consistency(image)
            completeness_factors.append(color_completeness)
            
            # 2. Ïù¥ÎØ∏ÏßÄ ÌíàÏßà
            if not self._is_image_corrupted(image):
                completeness_factors.append(0.9)
            else:
                completeness_factors.append(0.3)
            
            # 3. Ìï¥ÏÉÅÎèÑ Ï†ÅÏ†àÏÑ±
            h, w = image.shape[:2]
            if min(h, w) >= 256:
                resolution_score = min((min(h, w) / 512.0), 1.0)
            else:
                resolution_score = min(h, w) / 256.0
            completeness_factors.append(resolution_score)
            
            return np.mean(completeness_factors)
        except:
            return 0.7
    
    # =================================================================
    # üîß Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    # =================================================================
    
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], input_name: str) -> Optional[np.ndarray]:
        """Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Í≤ÄÏ¶ù"""
        try:
            if isinstance(image_input, np.ndarray):
                image = image_input
            elif isinstance(image_input, (str, Path)):
                if PIL_AVAILABLE:
                    pil_img = Image.open(image_input)
                    image = np.array(pil_img.convert('RGB'))
                else:
                    raise ImportError("PILÏù¥ ÌïÑÏöîÌï©ÎãàÎã§")
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïù¥ÎØ∏ÏßÄ ÌÉÄÏûÖ: {type(image_input)}")
            
            # Í≤ÄÏ¶ù
            if image.ndim != 3 or image.shape[2] != 3:
                raise ValueError("RGB Ïù¥ÎØ∏ÏßÄÏó¨Ïïº Ìï©ÎãàÎã§")
            
            if image.size == 0:
                raise ValueError("Îπà Ïù¥ÎØ∏ÏßÄÏûÖÎãàÎã§")
            
            return image
            
        except Exception as e:
            self.logger.error(f"{input_name} Î°úÎìú Ïã§Ìå®: {e}")
            return None
    
    def _is_image_corrupted(self, image: np.ndarray) -> bool:
        """Ïù¥ÎØ∏ÏßÄ ÏÜêÏÉÅ Ïó¨Î∂Ä ÌôïÏù∏"""
        try:
            # 1. NaN/Inf Ï≤¥ÌÅ¨
            if np.any(np.isnan(image)) or np.any(np.isinf(image)):
                return True
            
            # 2. Í∞í Î≤îÏúÑ Ï≤¥ÌÅ¨
            if np.any(image < 0) or np.any(image > 255):
                return True
            
            # 3. ÌòïÌÉú Ï≤¥ÌÅ¨
            if image.ndim != 3 or image.shape[2] != 3:
                return True
            
            # 4. ÌÅ¨Í∏∞ Ï≤¥ÌÅ¨
            if image.size == 0:
                return True
            
            return False
            
        except Exception:
            return True
    
    def _generate_cache_key(self, image: np.ndarray, fabric_type: str, clothing_type: str, kwargs: Dict[str, Any]) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        try:
            import hashlib
            
            # Ïù¥ÎØ∏ÏßÄ Ìï¥Ïãú
            image_hash = hashlib.md5(image.tobytes()).hexdigest()[:16]
            
            # ÏÑ§Ï†ï Ìï¥Ïãú
            config_data = {
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config.get('mode', 'comprehensive'),
                **{k: v for k, v in kwargs.items() if isinstance(v, (str, int, float, bool))}
            }
            config_str = json.dumps(config_data, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"qa_{image_hash}_{config_hash}"
            
        except Exception:
            return f"qa_fallback_{time.time()}"
    
    def _optimize_m3_max_memory(self):
        """M3 Max Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        if self.is_m3_max and TORCH_AVAILABLE:
            try:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                # Python Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
                gc.collect()
                
            except Exception as e:
                self.logger.warning(f"Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
    
    async def _generate_recommendations(self, metrics: QualityMetrics, fabric_type: str, clothing_type: str) -> List[Dict[str, Any]]:
        """Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ±"""
        recommendations = []
        
        try:
            # 1. Í∏∞Ïà†Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.sharpness < 0.6:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'low_sharpness',
                    'description': 'Ïù¥ÎØ∏ÏßÄ ÏÑ†Î™ÖÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'ÏÉ§ÌîÑÎãù ÌïÑÌÑ∞Î•º Ï†ÅÏö©ÌïòÍ±∞ÎÇò Îçî ÎÜíÏùÄ Ìï¥ÏÉÅÎèÑÎ°ú Ï≤òÎ¶¨ÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            if metrics.noise_level > 0.4:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'high_noise',
                    'description': 'ÎÖ∏Ïù¥Ï¶à Î†àÎ≤®Ïù¥ ÎÜíÏäµÎãàÎã§',
                    'suggestion': 'ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞ ÌïÑÌÑ∞Î•º Í∞ïÌôîÌïòÍ±∞ÎÇò Ï†ÑÏ≤òÎ¶¨ Îã®Í≥ÑÎ•º Í∞úÏÑ†ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            if metrics.contrast < 0.5:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'low_contrast',
                    'description': 'ÎåÄÎπÑÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§',
                    'suggestion': 'ÌûàÏä§ÌÜ†Í∑∏Îû® ÌèâÌôúÌôîÎÇò Ï†ÅÏùëÌòï ÎåÄÎπÑ Ìñ•ÏÉÅÏùÑ Ï†ÅÏö©ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            # 2. ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.structural_similarity < 0.7:
                recommendations.append({
                    'category': 'perceptual',
                    'issue': 'low_similarity',
                    'description': 'ÏõêÎ≥∏Í≥ºÏùò Íµ¨Ï°∞Ï†Å Ïú†ÏÇ¨ÏÑ±Ïù¥ ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'ÏßÄÏò§Î©îÌä∏Î¶≠ Îß§Ïπ≠ Îã®Í≥ÑÎ•º Í∞úÏÑ†ÌïòÍ±∞ÎÇò ÏõåÌïë ÏïåÍ≥†Î¶¨Ï¶òÏùÑ Ï°∞Ï†ïÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            # 3. ÎØ∏Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.color_harmony < 0.6:
                recommendations.append({
                    'category': 'aesthetic',
                    'issue': 'poor_color_harmony',
                    'description': 'ÏÉâÏÉÅ Ï°∞ÌôîÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§',
                    'suggestion': 'ÏÉâÏÉÅ Î≥¥Ï†ïÏù¥ÎÇò ÏÉâÏò®ÎèÑ Ï°∞Ï†ïÏùÑ Í≥†Î†§ÌïòÏÑ∏Ïöî',
                    'priority': 'low'
                })
            
            # 4. Í∏∞Îä•Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.fitting_quality < 0.7:
                recommendations.append({
                    'category': 'functional',
                    'issue': 'poor_fitting',
                    'description': f'{clothing_type} ÌîºÌåÖ ÌíàÏßàÏù¥ ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'Ïù∏Ï≤¥ ÌååÏã± Ï†ïÌôïÎèÑÎ•º ÎÜíÏù¥Í±∞ÎÇò ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖòÏùÑ Í∞úÏÑ†ÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            # 5. ÏõêÎã®Î≥Ñ ÌäπÌôî Ï†úÏïà
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            if metrics.texture_quality < fabric_standards['texture_importance'] * 0.8:
                recommendations.append({
                    'category': 'fabric_specific',
                    'issue': 'texture_quality',
                    'description': f'{fabric_type} ÏõêÎã®Ïùò ÌÖçÏä§Ï≤ò ÌíàÏßàÏù¥ Í∏∞Ï§Ä ÎØ∏Îã¨ÏûÖÎãàÎã§',
                    'suggestion': f'{fabric_type}Ïóê ÌäπÌôîÎêú ÌÖçÏä§Ï≤ò Ìñ•ÏÉÅ Í∏∞Î≤ïÏùÑ Ï†ÅÏö©ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            # Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Ï†ïÎ†¨
            priority_order = {'high': 0, 'medium': 1, 'low': 2}
            recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))
            
        except Exception as e:
            self.logger.error(f"Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ± Ïã§Ìå®: {e}")
            recommendations.append({
                'category': 'general',
                'issue': 'analysis_error',
                'description': 'ÌíàÏßà Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§',
                'suggestion': 'ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎÇò ÏÑ§Ï†ïÏùÑ ÌôïÏù∏ÌïòÍ≥† Îã§Ïãú ÏãúÎèÑÌïòÏÑ∏Ïöî',
                'priority': 'high'
            })
        
        return recommendations
    
    async def _generate_detailed_analysis(
        self,
        metrics: QualityMetrics,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str
    ) -> Dict[str, Any]:
        """ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±"""
        detailed_analysis = {}
        
        try:
            # 1. Ïù¥ÎØ∏ÏßÄ ÌÜµÍ≥Ñ
            detailed_analysis['image_statistics'] = {
                'mean_brightness': float(np.mean(fitted_img)),
                'std_brightness': float(np.std(fitted_img)),
                'color_distribution': {
                    'red_mean': float(np.mean(fitted_img[:, :, 0])),
                    'green_mean': float(np.mean(fitted_img[:, :, 1])),
                    'blue_mean': float(np.mean(fitted_img[:, :, 2]))
                },
                'shape': fitted_img.shape,
                'total_pixels': int(fitted_img.size)
            }
            
            # 2. ÌíàÏßà Î©îÌä∏Î¶≠ ÏÉÅÏÑ∏ Î∂ÑÏÑù
            detailed_analysis['quality_breakdown'] = {
                'technical_quality': {
                    'sharpness': float(metrics.sharpness),
                    'noise_level': float(metrics.noise_level),
                    'contrast': float(metrics.contrast),
                    'color_accuracy': float(metrics.color_accuracy)
                },
                'perceptual_quality': {
                    'structural_similarity': float(metrics.structural_similarity),
                    'visual_quality': float(metrics.visual_quality),
                    'artifact_level': float(metrics.artifact_level)
                },
                'aesthetic_quality': {
                    'composition': float(metrics.composition),
                    'color_harmony': float(metrics.color_harmony),
                    'balance': float(metrics.balance)
                },
                'functional_quality': {
                    'fitting_quality': float(metrics.fitting_quality),
                    'texture_quality': float(metrics.texture_quality),
                    'detail_preservation': float(metrics.detail_preservation)
                }
            }
            
            # 3. ÏñºÍµ¥ Î∂ÑÏÑù (ÏûàÎäî Í≤ΩÏö∞)
            faces = self._detect_faces_for_quality(fitted_img)
            if faces:
                detailed_analysis['face_analysis'] = {
                    'faces_detected': len(faces),
                    'face_regions': [{'x': int(x), 'y': int(y), 'width': int(w), 'height': int(h)} for x, y, w, h in faces],
                    'face_quality_preserved': bool(metrics.detail_preservation > 0.7)
                }
            
            # 4. ÏõêÎã®Î≥Ñ ÌäπÏÑ± Î∂ÑÏÑù
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            detailed_analysis['fabric_analysis'] = {
                'fabric_type': fabric_type,
                'texture_importance': fabric_standards['texture_importance'],
                'texture_meets_standard': bool(metrics.texture_quality >= fabric_standards['texture_importance'] * 0.8),
                'draping_quality': float(metrics.fitting_quality * fabric_standards['drape_importance'])
            }
            
            # 5. Ï≤òÎ¶¨ ÏãúÍ∞Ñ Î∞è ÏÑ±Îä•
            detailed_analysis['performance_analysis'] = {
                'processing_device': self.device,
                'is_m3_max_optimized': self.is_m3_max,
                'memory_usage_gb': self.memory_gb,
                'optimization_level': self.optimization_level
            }
            
        except Exception as e:
            self.logger.error(f"ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ± Ïã§Ìå®: {e}")
            detailed_analysis['error'] = str(e)
        
        return detailed_analysis
    
    def _build_final_result(
        self,
        metrics: QualityMetrics,
        recommendations: List[Dict[str, Any]],
        detailed_analysis: Dict[str, Any],
        processing_time: float,
        fabric_type: str,
        clothing_type: str
    ) -> Dict[str, Any]:
        """ÏµúÏ¢Ö Í≤∞Í≥º Íµ¨ÏÑ±"""
        
        return {
            "success": True,
            "step_name": self.step_name,
            "processing_time": processing_time,
            
            # ÌïµÏã¨ ÌíàÏßà Î©îÌä∏Î¶≠
            "quality_metrics": asdict(metrics),
            "overall_score": float(metrics.overall_score),
            "grade": metrics.get_grade().value,
            "confidence": float(metrics.confidence),
            
            # Í∞úÏÑ† Ï†úÏïà
            "recommendations": recommendations,
            "recommendation_count": len(recommendations),
            "high_priority_issues": len([r for r in recommendations if r.get('priority') == 'high']),
            
            # ÏÉÅÏÑ∏ Î∂ÑÏÑù (ÏÑ†ÌÉùÏ†Å)
            "detailed_analysis": detailed_analysis if detailed_analysis else None,
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            "fabric_type": fabric_type,
            "clothing_type": clothing_type,
            "assessment_mode": self.assessment_config['mode'],
            
            # ÏãúÏä§ÌÖú Ï†ïÎ≥¥
            "device_info": {
                "device": self.device,
                "device_type": self.device_type,
                "is_m3_max": self.is_m3_max,
                "memory_gb": self.memory_gb,
                "optimization_level": self.optimization_level
            },
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ
            "performance_stats": self.performance_stats.copy(),
            
            # ÌíàÏßà ÌÜµÍ≥º Ïó¨Î∂Ä
            "quality_passed": metrics.overall_score >= self.quality_thresholds['minimum_acceptable'],
            "quality_thresholds": self.quality_thresholds.copy(),
            
            "from_cache": False
        }
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """Ï∫êÏãúÏóê Í≤∞Í≥º Ï†ÄÏû•"""
        try:
            if len(self.assessment_cache) >= self.cache_max_size:
                # LRU Î∞©ÏãùÏúºÎ°ú Ïò§ÎûòÎêú Ìï≠Î™© Ï†úÍ±∞
                oldest_key = min(self.assessment_cache.keys())
                del self.assessment_cache[oldest_key]
            
            # Î©îÎ™®Î¶¨ Ï†àÏïΩÏùÑ ÏúÑÌï¥ ÏÉÅÏÑ∏ Î∂ÑÏÑùÏùÄ Ï∫êÏãúÏóêÏÑú Ï†úÏô∏
            cached_result = result.copy()
            if 'detailed_analysis' in cached_result:
                cached_result['detailed_analysis'] = None
            
            self.assessment_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    def _update_performance_stats(self, processing_time: float, quality_score: float, success: bool = True):
        """ÏÑ±Îä• ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            if success:
                self.performance_stats['total_assessments'] += 1
                self.performance_stats['total_time'] += processing_time
                self.performance_stats['average_time'] = (
                    self.performance_stats['total_time'] / self.performance_stats['total_assessments']
                )
                
                # ÌèâÍ∑† ÌíàÏßà Ï†êÏàò ÏóÖÎç∞Ïù¥Ìä∏
                current_avg = self.performance_stats.get('average_score', 0.0)
                total_assessments = self.performance_stats['total_assessments']
                self.performance_stats['average_score'] = (
                    (current_avg * (total_assessments - 1) + quality_score) / total_assessments
                )
            else:
                self.performance_stats['error_count'] += 1
            
            self.performance_stats['last_assessment_time'] = processing_time
            
            # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï∂îÏ†Å (M3 Max)
            if self.is_m3_max:
                try:
                    import psutil
                    memory_usage = psutil.virtual_memory().percent
                    self.performance_stats['peak_memory_usage'] = max(
                        self.performance_stats.get('peak_memory_usage', 0),
                        memory_usage
                    )
                except:
                    pass
            
        except Exception as e:
            self.logger.warning(f"ÏÑ±Îä• ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
    
    # =================================================================
    # üîß Ìå©ÌÜ†Î¶¨ Î©îÏÑúÎìúÎì§ (Î∂ÑÏÑùÍ∏∞ ÏÉùÏÑ±)
    # =================================================================
    
    def _create_perceptual_analyzer(self) -> Optional[Callable]:
        """ÏßÄÍ∞ÅÏ†Å Î∂ÑÏÑùÍ∏∞ ÏÉùÏÑ±"""
        if 'perceptual_quality' in self.ai_models:
            return self.ai_models['perceptual_quality']
        return None
    
    def _create_aesthetic_analyzer(self) -> Optional[Callable]:
        """ÎØ∏Ï†Å Î∂ÑÏÑùÍ∏∞ ÏÉùÏÑ±"""
        if 'aesthetic_quality' in self.ai_models:
            return self.ai_models['aesthetic_quality']
        return None
    
    def _create_functional_analyzer(self) -> Callable:
        """Í∏∞Îä•Ï†Å Î∂ÑÏÑùÍ∏∞ ÏÉùÏÑ±"""
        def functional_analyzer(image: np.ndarray) -> Dict[str, float]:
            # Í∏∞Î≥∏ Í∏∞Îä•Ï†Å Î∂ÑÏÑù
            return {
                'fitting_quality': 0.7,
                'edge_preservation': 0.7,
                'texture_quality': 0.7,
                'detail_preservation': 0.7
            }
        return functional_analyzer
    
    def _create_face_detector(self) -> Optional[Callable]:
        """ÏñºÍµ¥ Í∞êÏßÄÍ∏∞ ÏÉùÏÑ±"""
        if CV2_AVAILABLE:
            return lambda img: self._detect_faces_for_quality(img)
        return None
    
    async def _load_recommended_models(self):
        """Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú"""
        if self.model_interface is None:
            return
        
        try:
            # ÌíàÏßà ÌèâÍ∞ÄÏö© Ï∂îÏ≤ú Î™®Îç∏Îì§
            recommended_models = [
                'quality_assessment_combined',
                'perceptual_quality_model',
                'aesthetic_quality_model'
            ]
            
            for model_name in recommended_models:
                try:
                    model = await self.model_interface.get_model(model_name)
                    if model:
                        self.logger.info(f"üì¶ Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {model_name}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú Í≥ºÏ†ïÏóêÏÑú Ïò§Î•ò: {e}")
    
    # =================================================================
    # üîç ÌëúÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Î©îÏÑúÎìúÎì§ (Pipeline Manager Ìò∏Ìôò)
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "step_name": "QualityAssessment",
            "class_name": self.__class__.__name__,
            "version": "4.0-m3max",
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "config_keys": list(self.config.keys()),
            "performance_stats": self.performance_stats.copy(),
            "capabilities": {
                "technical_analysis": self.assessment_config['technical_analysis_enabled'],
                "perceptual_analysis": self.assessment_config['perceptual_analysis_enabled'],
                "aesthetic_analysis": self.assessment_config['aesthetic_analysis_enabled'],
                "functional_analysis": self.assessment_config['functional_analysis_enabled'],
                "detailed_analysis": self.assessment_config['detailed_analysis_enabled'],
                "neural_analysis": bool(self.ai_models) if hasattr(self, 'ai_models') else False,
                "m3_max_acceleration": self.is_m3_max and self.device == 'mps'
            },
            "supported_fabrics": list(self.FABRIC_QUALITY_STANDARDS.keys()),
            "supported_clothing_types": list(self.CLOTHING_QUALITY_WEIGHTS.keys()),
            "quality_settings": {
                "optimization_level": self.optimization_level,
                "quality_thresholds": self.quality_thresholds,
                "assessment_mode": self.assessment_config['mode']
            },
            "assessment_pipeline": [name for name, _ in self.assessment_pipeline] if hasattr(self, 'assessment_pipeline') else [],
            "cache_status": {
                "enabled": True,
                "size": len(self.assessment_cache) if hasattr(self, 'assessment_cache') else 0,
                "max_size": self.cache_max_size if hasattr(self, 'cache_max_size') else 0
            }
        }
    
    def cleanup_resources(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            # AI Î™®Îç∏ Ï†ïÎ¶¨
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.ai_models.clear()
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            if hasattr(self, 'assessment_cache'):
                self.assessment_cache.clear()
            
            # Î∂ÑÏÑùÍ∏∞ Ï†ïÎ¶¨
            if hasattr(self, 'technical_analyzer'):
                del self.technical_analyzer
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if TORCH_AVAILABLE and self.device in ["mps", "cuda"]:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("‚úÖ QualityAssessmentStep Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def __del__(self):
        """ÏÜåÎ©∏Ïûê"""
        try:
            self.cleanup_resources()
        except:
            pass

# =================================================================
# üî• Ìò∏ÌôòÏÑ± ÏßÄÏõê Ìï®ÏàòÎì§ (Í∏∞Ï°¥ ÏΩîÎìú Ìò∏Ìôò)
# =================================================================

def create_quality_assessment_step(
    device: str = "mps",
    config: Optional[Dict[str, Any]] = None
) -> QualityAssessmentStep:
    """Í∏∞Ï°¥ Î∞©Ïãù Ìò∏Ìôò ÏÉùÏÑ±Ïûê"""
    return QualityAssessmentStep(device=device, config=config)

def create_m3_max_quality_assessment_step(
    memory_gb: float = 128.0,
    assessment_mode: str = "comprehensive",
    **kwargs
) -> QualityAssessmentStep:
    """M3 Max ÏµúÏ†ÅÌôî ÏÉùÏÑ±Ïûê"""
    return QualityAssessmentStep(
        device=None,  # ÏûêÎèô Í∞êÏßÄ
        memory_gb=memory_gb,
        quality_level=assessment_mode,
        is_m3_max=True,
        optimization_enabled=True,
        assessment_mode=assessment_mode,
        **kwargs
    )

# Î™®Îìà ÏùµÏä§Ìè¨Ìä∏
__all__ = [
    'QualityAssessmentStep',
    'QualityMetrics',
    'QualityGrade',
    'AssessmentMode',
    'QualityAspect',
    'PerceptualQualityModel',
    'AestheticQualityModel',
    'TechnicalQualityAnalyzer',
    'create_quality_assessment_step',
    'create_m3_max_quality_assessment_step'
]

# Î™®Îìà Ï¥àÍ∏∞Ìôî Î°úÍ∑∏
logger.info("‚úÖ QualityAssessmentStep Î™®Îìà Î°úÎìú ÏôÑÎ£å - ÏôÑÏ†ÑÌïú Í∏∞Îä• Íµ¨ÌòÑ")