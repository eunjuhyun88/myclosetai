# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
üî• MyCloset AI - 8Îã®Í≥Ñ: ÌíàÏßà ÌèâÍ∞Ä (Quality Assessment) - v18.0 ÏôÑÏ†Ñ Ìò∏Ìôò Î≤ÑÏ†Ñ
================================================================================
‚úÖ step_model_requests.py v8.0 ÏôÑÏ†Ñ Ìò∏Ìôò - EnhancedRealModelRequest Ïó∞Îèô
‚úÖ BaseStepMixin v19.0 ÏôÑÏ†Ñ Ìò∏Ìôò - UnifiedDependencyManager Ïó∞Îèô
‚úÖ ModelLoader v21.0 ÌÜµÌïú Ïã§Ï†ú AI Î™®Îç∏ Ïó∞ÏÇ∞
‚úÖ StepInterface v2.0 register_model_requirement ÌôúÏö©
‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞ (TYPE_CHECKING Ìå®ÌÑ¥)
‚úÖ Ïã§Ï†ú AI Ï∂îÎ°† ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨ÌòÑ
‚úÖ open_clip_pytorch_model.bin (5.2GB) Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ Î∞è ÌôúÏö©
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
‚úÖ Î™®Îì† Ìï®Ïàò/ÌÅ¥ÎûòÏä§Î™Ö Ïú†ÏßÄ
‚úÖ DetailedDataSpec ÏôÑÏ†Ñ ÏßÄÏõê
‚úÖ FastAPI ÎùºÏö∞ÌÑ∞ Ìò∏ÌôòÏÑ± ÏôÑÏ†Ñ ÏßÄÏõê

Ï≤òÎ¶¨ ÌùêÎ¶Ñ:
üåê API ÏöîÏ≤≠ ‚Üí üìã PipelineManager ‚Üí üéØ QualityAssessmentStep ÏÉùÏÑ±
‚Üì
üîó BaseStepMixin.dependency_manager.auto_inject_dependencies()
‚îú‚îÄ ModelLoader ÏûêÎèô Ï£ºÏûÖ
‚îú‚îÄ StepModelInterface ÏÉùÏÑ±
‚îî‚îÄ register_model_requirement Ìò∏Ï∂ú (step_model_requests.py Ïó∞Îèô)
‚Üì
üöÄ QualityAssessmentStep.initialize()
‚îú‚îÄ AI ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏ Î°úÎìú (open_clip_pytorch_model.bin 5.2GB)
‚îú‚îÄ Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî
‚îî‚îÄ M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©
‚Üì
üß† Ïã§Ï†ú AI Ï∂îÎ°† process()
‚îú‚îÄ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ‚Üí DetailedDataSpec Í∏∞Î∞ò Î≥ÄÌôò
‚îú‚îÄ AI Î™®Îç∏ Ï∂îÎ°† (OpenCLIP, LPIPS, SSIM, ÌíàÏßà ÌèâÍ∞Ä)
‚îú‚îÄ 8Í∞ÄÏßÄ ÌíàÏßà Î∂ÑÏÑù ‚Üí Í≤∞Í≥º Ìï¥ÏÑù
‚îî‚îÄ Ï¢ÖÌï© ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
‚Üì
üì§ Í≤∞Í≥º Î∞òÌôò (QualityMetrics Í∞ùÏ≤¥ + FastAPI Ìò∏Ìôò)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache, wraps
import numpy as np
import base64
import io

# ==============================================
# üî• TYPE_CHECKINGÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.interfaces.step_interface import StepModelInterface

# ==============================================
# üî• step_model_requests.py v8.0 Ïó∞Îèô (ÌïµÏã¨)
# ==============================================
try:
    from ..utils.step_model_requests import (
        get_enhanced_step_request,
        get_step_preprocessing_requirements,
        get_step_postprocessing_requirements,
        get_step_api_mapping,
        get_step_data_flow,
        StepPriority,
        ModelSize,
        EnhancedRealModelRequest
    )
    STEP_MODEL_REQUESTS_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ step_model_requests.py v8.0 Ïó∞Îèô ÏÑ±Í≥µ")
except ImportError as e:
    STEP_MODEL_REQUESTS_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è step_model_requests.py ÏûÑÌè¨Ìä∏ Ïã§Ìå®: {e}")

# ==============================================
# üî• BaseStepMixin v19.0 ÏûÑÌè¨Ìä∏ (ÌïµÏã¨)
# ==============================================
try:
    from .base_step_mixin import BaseStepMixin, QualityAssessmentMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger.info("‚úÖ BaseStepMixin v19.0 ÏûÑÌè¨Ìä∏ ÏÑ±Í≥µ")
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è BaseStepMixin ÏûÑÌè¨Ìä∏ Ïã§Ìå®: {e}")

# ==============================================
# üî• ÏïàÏ†ÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏
# ==============================================
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    # OpenCV Ìè¥Î∞± ÏãúÏä§ÌÖú
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    pil_img = Image.fromarray(img)
                    resized = pil_img.resize(size)
                    return np.array(resized)
            return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:
                    return img[:, :, ::-1]
            return img
        
        def imread(self, path):
            if PIL_AVAILABLE:
                from PIL import Image
                img = Image.open(path)
                return np.array(img)
            return None
        
        def imwrite(self, path, img):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    Image.fromarray(img).save(path)
                    return True
            return False
    
    cv2 = OpenCVFallback()
    OPENCV_AVAILABLE = False

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ==============================================
# üî• GPU ÏïàÏ†Ñ Ïó∞ÏÇ∞ Ïú†Ìã∏Î¶¨Ìã∞
# ==============================================
def safe_mps_empty_cache():
    """MPS Ï∫êÏãú ÏïàÏ†Ñ Ï†ïÎ¶¨"""
    try:
        if TORCH_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
        gc.collect()
        return {"success": True, "method": "mps_cache_cleared"}
    except Exception:
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

def safe_tensor_to_numpy(tensor):
    """TensorÎ•º ÏïàÏ†ÑÌïòÍ≤å NumPyÎ°ú Î≥ÄÌôò"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            return tensor.detach().cpu().numpy()
        elif hasattr(tensor, 'numpy'):
            return tensor.numpy()
        else:
            return np.array(tensor)
    except Exception:
        return np.array(tensor)

# ==============================================
# üî• MRO ÏïàÏ†ÑÌïú Ìè¥Î∞± ÌÅ¥ÎûòÏä§Îì§
# ==============================================
if not BASE_STEP_MIXIN_AVAILABLE:
    class BaseStepMixin:
        """MRO ÏïàÏ†ÑÌïú Ìè¥Î∞± BaseStepMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.step_name = getattr(self, 'step_name', 'quality_assessment')
            self.step_number = 8
            self.device = 'cpu'
            self.is_initialized = False
            self.dependency_manager = None
    
    class QualityAssessmentMixin(BaseStepMixin):
        """MRO ÏïàÏ†ÑÌïú Ìè¥Î∞± QualityAssessmentMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.step_type = "quality_assessment"
            self.quality_threshold = 0.7

# ==============================================
# üî• ÌíàÏßà ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îì§ (step_model_requests.py Ìò∏Ìôò)
# ==============================================
class QualityGrade(Enum):
    """ÌíàÏßà Îì±Í∏â"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    FAILED = "failed"

class AssessmentMode(Enum):
    """ÌèâÍ∞Ä Î™®Îìú"""
    COMPREHENSIVE = "comprehensive"
    FAST = "fast"
    DETAILED = "detailed"
    CUSTOM = "custom"

class QualityAspect(Enum):
    """ÌíàÏßà ÌèâÍ∞Ä ÏòÅÏó≠"""
    SHARPNESS = "sharpness"
    COLOR = "color"
    FITTING = "fitting"
    REALISM = "realism"
    ARTIFACTS = "artifacts"
    ALIGNMENT = "alignment"
    LIGHTING = "lighting"
    TEXTURE = "texture"

@dataclass
class QualityMetrics:
    """ÌíàÏßà Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ (FastAPI Ìò∏Ìôò)"""
    overall_score: float = 0.0
    confidence: float = 0.0
    
    # ÏÑ∏Î∂Ä Ï†êÏàòÎì§
    sharpness_score: float = 0.0
    color_score: float = 0.0
    fitting_score: float = 0.0
    realism_score: float = 0.0
    artifacts_score: float = 0.0
    alignment_score: float = 0.0
    lighting_score: float = 0.0
    texture_score: float = 0.0
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    processing_time: float = 0.0
    device_used: str = "cpu"
    model_version: str = "v1.0"
    
    # FastAPI Ìò∏Ìôò ÌïÑÎìú (step_model_requests.py Í∏∞Î∞ò)
    quality_breakdown: Dict[str, float] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò"""
        return asdict(self)
    
    def to_fastapi_response(self) -> Dict[str, Any]:
        """FastAPI ÏùëÎãµ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò (step_model_requests.py Í∏∞Î∞ò)"""
        return {
            "overall_quality": self.overall_score,
            "quality_breakdown": {
                "sharpness": self.sharpness_score,
                "color": self.color_score,
                "fitting": self.fitting_score,
                "realism": self.realism_score,
                "artifacts": self.artifacts_score,
                "alignment": self.alignment_score,
                "lighting": self.lighting_score,
                "texture": self.texture_score
            },
            "recommendations": self.recommendations,
            "confidence": self.confidence
        }

# ==============================================
# üî• Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§ (step_model_requests.py Í∏∞Î∞ò)
# ==============================================
if TORCH_AVAILABLE:
    class RealPerceptualQualityModel(nn.Module):
        """Ïã§Ï†ú ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏ (OpenCLIP Í∏∞Î∞ò)"""
        
        def __init__(self, pretrained_path: Optional[str] = None, model_config: Optional[Dict] = None):
            super().__init__()
            
            # step_model_requests.pyÏóêÏÑú ÏÑ§Ï†ï Î°úÎìú
            self.model_config = model_config or {}
            self.input_size = self.model_config.get('input_size', (224, 224))
            self.architecture = self.model_config.get('model_architecture', 'open_clip_vit')
            
            # OpenCLIP Ïä§ÌÉÄÏùº Î∞±Î≥∏
            self.backbone = self._create_clip_backbone()
            
            # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Ìó§Îìú
            self.quality_head = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 5),  # 5Ï∞®Ïõê ÌíàÏßà Ï†êÏàò
                nn.Sigmoid()
            )
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú
            if pretrained_path and Path(pretrained_path).exists():
                self.load_checkpoint(pretrained_path)
        
        def _create_clip_backbone(self):
            """OpenCLIP Ïä§ÌÉÄÏùº Î∞±Î≥∏ ÏÉùÏÑ±"""
            return nn.Sequential(
                # Vision Transformer Ïä§ÌÉÄÏùº (Í∞ÑÎã®Ìôî)
                nn.Conv2d(3, 768, kernel_size=16, stride=16),  # Patch embedding
                nn.Flatten(2),
                nn.Transpose(1, 2),  # [B, N, C]
                
                # Transformer blocks (Í∞ÑÎã®Ìôî)
                nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(
                        d_model=768,
                        nhead=12,
                        dim_feedforward=3072,
                        dropout=0.1,
                        batch_first=True
                    ),
                    num_layers=6
                ),
                
                # Ï∂úÎ†• Ìà¨ÏòÅ
                nn.AdaptiveAvgPool1d(1),
                nn.Flatten(),
                nn.Linear(768, 512)
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú (open_clip_pytorch_model.bin ÏßÄÏõê)"""
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # OpenCLIP ÌòïÏãù Ï≤òÎ¶¨
                if 'visual' in checkpoint:
                    # OpenCLIP ÌòïÏãù
                    visual_state = checkpoint['visual']
                    self.load_state_dict(visual_state, strict=False)
                elif 'state_dict' in checkpoint:
                    self.load_state_dict(checkpoint['state_dict'], strict=False)
                elif 'model' in checkpoint:
                    self.load_state_dict(checkpoint['model'], strict=False)
                else:
                    self.load_state_dict(checkpoint, strict=False)
                
                logging.getLogger(__name__).info(f"‚úÖ OpenCLIP Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú ÏÑ±Í≥µ: {checkpoint_path}")
            except Exception as e:
                logging.getLogger(__name__).warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Ïã§Ìå®: {e}")
        
        def forward(self, x):
            """ÏàúÏ†ÑÌåå"""
            # step_model_requests.py Í∏∞Î∞ò ÏûÖÎ†• Ï≤òÎ¶¨
            if x.shape[-2:] != self.input_size:
                x = F.interpolate(x, size=self.input_size, mode='bilinear', align_corners=False)
            
            features = self.backbone(x)
            quality_scores = self.quality_head(features)
            
            return {
                'quality_scores': quality_scores,  # (batch_size, 5)
                'feature_embeddings': features,   # (batch_size, 512)
                'overall_quality': torch.mean(quality_scores, dim=1),
                'perceptual_distance': 1.0 - torch.mean(quality_scores, dim=1)
            }

    class RealAestheticQualityModel(nn.Module):
        """Ïã§Ï†ú ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏"""
        
        def __init__(self, pretrained_path: Optional[str] = None, model_config: Optional[Dict] = None):
            super().__init__()
            
            self.model_config = model_config or {}
            
            # ResNet Ïä§ÌÉÄÏùº Î∞±Î≥∏
            self.backbone = self._create_resnet_backbone()
            
            # ÎØ∏Ï†Å ÌäπÏÑ± Î∂ÑÏÑù Ìó§ÎìúÎì§
            self.aesthetic_heads = nn.ModuleDict({
                'composition': self._create_head(512, 1),
                'color_harmony': self._create_head(512, 1),
                'lighting': self._create_head(512, 1),
                'balance': self._create_head(512, 1),
                'symmetry': self._create_head(512, 1)
            })
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú
            if pretrained_path and Path(pretrained_path).exists():
                self.load_checkpoint(pretrained_path)
        
        def _create_resnet_backbone(self):
            """ResNet Î∞±Î≥∏ ÏÉùÏÑ±"""
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                # ResNet blocks (Í∞ÑÎã®Ìôî)
                nn.Conv2d(64, 128, 3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(128, 256, 3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(256, 512, 3, stride=2, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                
                nn.AdaptiveAvgPool2d(1)
            )
        
        def _create_head(self, in_features: int, out_features: int):
            """Î∂ÑÏÑù Ìó§Îìú ÏÉùÏÑ±"""
            return nn.Sequential(
                nn.Linear(in_features, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, out_features),
                nn.Sigmoid()
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"""
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'state_dict' in checkpoint:
                    self.load_state_dict(checkpoint['state_dict'], strict=False)
                elif 'model' in checkpoint:
                    self.load_state_dict(checkpoint['model'], strict=False)
                else:
                    self.load_state_dict(checkpoint, strict=False)
                logging.getLogger(__name__).info(f"‚úÖ ÎØ∏Ï†Å Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú: {checkpoint_path}")
            except Exception as e:
                logging.getLogger(__name__).warning(f"‚ö†Ô∏è ÎØ∏Ï†Å Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        
        def forward(self, x):
            """ÏàúÏ†ÑÌåå"""
            features = self.backbone(x).flatten(1)
            
            results = {}
            for name, head in self.aesthetic_heads.items():
                results[name] = head(features)
            
            # Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞
            results['overall'] = torch.mean(torch.stack(list(results.values())))
            
            return results

else:
    # PyTorch ÏóÜÏùÑ Îïå ÎçîÎØ∏ ÌÅ¥ÎûòÏä§
    class RealPerceptualQualityModel:
        def __init__(self, pretrained_path=None, model_config=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ÏóÜÏùå - ÎçîÎØ∏ RealPerceptualQualityModel")
        
        def __call__(self, x):
            return {
                'quality_scores': np.array([0.7, 0.8, 0.75, 0.72, 0.78]),
                'overall_quality': 0.75,
                'perceptual_distance': 0.25
            }
    
    class RealAestheticQualityModel:
        def __init__(self, pretrained_path=None, model_config=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ÏóÜÏùå - ÎçîÎØ∏ RealAestheticQualityModel")
        
        def __call__(self, x):
            return {
                'composition': 0.7, 
                'color_harmony': 0.8, 
                'lighting': 0.75, 
                'balance': 0.7, 
                'symmetry': 0.8,
                'overall': 0.75
            }

# ==============================================
# üî• Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∏∞ ÌÅ¥ÎûòÏä§Îì§ (Í∏∞Ï°¥ Ïú†ÏßÄ, Í∞úÏÑ†)
# ==============================================
class TechnicalQualityAnalyzer:
    """Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, device: str = "cpu", enable_gpu: bool = False):
        self.device = device
        self.enable_gpu = enable_gpu
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
        
        # Î∂ÑÏÑù Ï∫êÏãú
        self.analysis_cache = {}
        
        # Í∏∞Ïà†Ï†Å Î∂ÑÏÑù ÏûÑÍ≥ÑÍ∞íÎì§
        self.thresholds = {
            'sharpness_min': 100.0,
            'noise_max': 50.0,
            'contrast_min': 20.0,
            'brightness_range': (50, 200)
        }
    
    def analyze(self, image: np.ndarray) -> Dict[str, Any]:
        """Ï¢ÖÌï© Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        try:
            if image is None or image.size == 0:
                return self._get_fallback_technical_results()
            
            results = {}
            
            # 1. ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù
            results['sharpness'] = self._analyze_sharpness(image)
            
            # 2. ÎÖ∏Ïù¥Ï¶à Î†àÎ≤® Î∂ÑÏÑù
            results['noise_level'] = self._analyze_noise_level(image)
            
            # 3. ÎåÄÎπÑ Î∂ÑÏÑù
            results['contrast'] = self._analyze_contrast(image)
            
            # 4. Î∞ùÍ∏∞ Î∂ÑÏÑù
            results['brightness'] = self._analyze_brightness(image)
            
            # 5. Ìè¨ÌôîÎèÑ Î∂ÑÏÑù
            results['saturation'] = self._analyze_saturation(image)
            
            # 6. ÏïÑÌã∞Ìå©Ìä∏ Í≤ÄÏ∂ú
            results['artifacts'] = self._detect_artifacts(image)
            
            # 7. Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞
            results['overall_score'] = self._calculate_technical_score(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Í∏∞Ïà†Ï†Å Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return self._get_fallback_technical_results()
    
    def _analyze_sharpness(self, image: np.ndarray) -> float:
        """ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if OPENCV_AVAILABLE else np.mean(image, axis=2)
            else:
                gray = image
            
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                sharpness = laplacian.var()
            else:
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                sharpness = np.var(dx) + np.var(dy)
            
            normalized_sharpness = min(1.0, sharpness / 10000.0)
            return max(0.0, normalized_sharpness)
            
        except Exception as e:
            self.logger.error(f"ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return 0.5
    
    def _analyze_noise_level(self, image: np.ndarray) -> float:
        """ÎÖ∏Ïù¥Ï¶à Î†àÎ≤® Î∂ÑÏÑù"""
        try:
            if len(image.shape) == 3:
                noise_levels = []
                for channel in range(3):
                    channel_data = image[:, :, channel]
                    if OPENCV_AVAILABLE:
                        blur = cv2.GaussianBlur(channel_data.astype(np.uint8), (5, 5), 0)
                        noise = np.abs(channel_data.astype(float) - blur.astype(float))
                    else:
                        noise = np.std(channel_data)
                    
                    noise_level = np.mean(noise) / 255.0
                    noise_levels.append(noise_level)
                
                avg_noise = np.mean(noise_levels)
            else:
                avg_noise = np.std(image) / 255.0
            
            return max(0.0, min(1.0, 1.0 - avg_noise * 5))
            
        except Exception as e:
            self.logger.error(f"ÎÖ∏Ïù¥Ï¶à Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return 0.7
    
    def _analyze_contrast(self, image: np.ndarray) -> float:
        """ÎåÄÎπÑ Î∂ÑÏÑù"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            contrast = np.std(gray)
            
            if 30 <= contrast <= 80:
                contrast_score = 1.0
            elif contrast < 30:
                contrast_score = contrast / 30.0
            else:
                contrast_score = max(0.3, 1.0 - (contrast - 80) / 100.0)
            
            return max(0.0, min(1.0, contrast_score))
            
        except Exception as e:
            self.logger.error(f"ÎåÄÎπÑ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return 0.6
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """Î∞ùÍ∏∞ Î∂ÑÏÑù"""
        try:
            brightness = np.mean(image)
            
            if 100 <= brightness <= 160:
                brightness_score = 1.0
            elif brightness < 100:
                brightness_score = brightness / 100.0
            else:
                brightness_score = max(0.3, 1.0 - (brightness - 160) / 95.0)
            
            return max(0.0, min(1.0, brightness_score))
            
        except Exception as e:
            self.logger.error(f"Î∞ùÍ∏∞ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return 0.6
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """Ìè¨ÌôîÎèÑ Î∂ÑÏÑù"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            if OPENCV_AVAILABLE:
                hsv = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2HSV)
                saturation = np.mean(hsv[:, :, 1])
            else:
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = np.mean((max_vals - min_vals) / (max_vals + 1e-8)) * 255
            
            if 80 <= saturation <= 180:
                saturation_score = 1.0
            elif saturation < 80:
                saturation_score = saturation / 80.0
            else:
                saturation_score = max(0.3, 1.0 - (saturation - 180) / 75.0)
            
            return max(0.0, min(1.0, saturation_score))
            
        except Exception as e:
            self.logger.error(f"Ìè¨ÌôîÎèÑ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return 0.6
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ÏïÑÌã∞Ìå©Ìä∏ Í≤ÄÏ∂ú"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                artifact_metric = np.std(laplacian)
            else:
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                artifact_metric = np.std(dx) + np.std(dy)
            
            artifact_score = max(0.0, 1.0 - artifact_metric / 1000.0)
            return min(1.0, artifact_score)
            
        except Exception as e:
            self.logger.error(f"ÏïÑÌã∞Ìå©Ìä∏ Í≤ÄÏ∂ú Ïã§Ìå®: {e}")
            return 0.8
    
    def _calculate_technical_score(self, results: Dict[str, Any]) -> float:
        """Í∏∞Ïà†Ï†Å ÌíàÏßà Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            weights = {
                'sharpness': 0.25,
                'noise_level': 0.20,
                'contrast': 0.15,
                'brightness': 0.15,
                'saturation': 0.10,
                'artifacts': 0.15
            }
            
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"Í∏∞Ïà†Ï†Å Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.5
    
    def _get_fallback_technical_results(self) -> Dict[str, Any]:
        """Ìè¥Î∞± Í∏∞Ïà†Ï†Å Î∂ÑÏÑù Í≤∞Í≥º"""
        return {
            'sharpness': 0.5,
            'noise_level': 0.6,
            'contrast': 0.5,
            'brightness': 0.6,
            'saturation': 0.5,
            'artifacts': 0.7,
            'overall_score': 0.55,
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """Î∂ÑÏÑùÍ∏∞ Ï†ïÎ¶¨"""
        self.analysis_cache.clear()

# ==============================================
# üî• Î©îÏù∏ QualityAssessmentStep ÌÅ¥ÎûòÏä§ (ÏôÑÏ†Ñ Ïû¨ÏûëÏÑ±)
# ==============================================
class QualityAssessmentStep(BaseStepMixin):
    """ÌíàÏßà ÌèâÍ∞Ä Step - step_model_requests.py v8.0 ÏôÑÏ†Ñ Ìò∏Ìôò"""
    
    def __init__(self, **kwargs):
        """BaseStepMixin v19.0 + step_model_requests.py v8.0 Ìò∏Ìôò ÏÉùÏÑ±Ïûê"""
        super().__init__(**kwargs)
        
        # Í∏∞Î≥∏ ÏÜçÏÑ± ÏÑ§Ï†ï
        self.step_name = "QualityAssessmentStep"
        self.step_id = 8
        self.device = kwargs.get('device', 'mps' if self._detect_m3_max() else 'cpu')
        
        # step_model_requests.py Ïó∞Îèô
        self.step_request = None
        self._load_step_requirements()
        
        # üîß Ï∂îÍ∞Ä: is_m3_max ÏÜçÏÑ± (PipelineManagerÏóêÏÑú ÌïÑÏöî)
        self.is_m3_max = self._detect_m3_max()
        self.is_apple_silicon = self._detect_apple_silicon()
        self.mps_available = self._check_mps_availability()
        
        # ÎèôÏ†Å ÏÑ§Ï†ï (step_model_requests.py Í∏∞Î∞ò)
        if self.step_request:
            self.optimal_batch_size = self.step_request.batch_size
            self.memory_fraction = self.step_request.memory_fraction
            self.input_size = self.step_request.input_size
        else:
            self.optimal_batch_size = 1
            self.memory_fraction = 0.5
            self.input_size = (224, 224)
        
        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.status = kwargs.get('status', {})
        self.model_loaded = False
        self.initialized = False
        
        # AI Î™®Îç∏Îì§ Ï¥àÍ∏∞Ìôî
        self.quality_models = {}
        self.feature_extractors = {}
        self.technical_analyzer = None
        
        # ÏùòÏ°¥ÏÑ± Í¥ÄÎ¶¨
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        
        # ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî
        self._setup_configurations(kwargs.get('config', {}))
        
        self.logger.info(f"‚úÖ QualityAssessmentStep ÏÉùÏÑ± ÏôÑÎ£å - Device: {self.device}, M3 Max: {self.is_m3_max}")
        self.logger.info(f"üîó step_model_requests.py Ïó∞Îèô: {'‚úÖ' if self.step_request else '‚ùå'}")

    def _load_step_requirements(self):
        """step_model_requests.pyÏóêÏÑú ÏöîÍµ¨ÏÇ¨Ìï≠ Î°úÎìú"""
        try:
            if STEP_MODEL_REQUESTS_AVAILABLE:
                self.step_request = get_enhanced_step_request("QualityAssessmentStep")
                if self.step_request:
                    self.logger.info("‚úÖ step_model_requests.pyÏóêÏÑú QualityAssessmentStep ÏöîÍµ¨ÏÇ¨Ìï≠ Î°úÎìú ÏÑ±Í≥µ")
                    
                    # Ï†ÑÏ≤òÎ¶¨ ÏöîÍµ¨ÏÇ¨Ìï≠ Î°úÎìú
                    self.preprocessing_requirements = get_step_preprocessing_requirements("QualityAssessmentStep")
                    self.postprocessing_requirements = get_step_postprocessing_requirements("QualityAssessmentStep")
                    self.api_mapping = get_step_api_mapping("QualityAssessmentStep")
                    self.data_flow = get_step_data_flow("QualityAssessmentStep")
                    
                    self.logger.info(f"üìã Ï†ÑÏ≤òÎ¶¨ Îã®Í≥Ñ: {len(self.preprocessing_requirements.get('preprocessing_steps', []))}")
                    self.logger.info(f"üì§ ÌõÑÏ≤òÎ¶¨ Îã®Í≥Ñ: {len(self.postprocessing_requirements.get('postprocessing_steps', []))}")
                    self.logger.info(f"üîó API Îß§Ìïë: {len(self.api_mapping.get('input_mapping', {}))}")
                else:
                    self.logger.warning("‚ö†Ô∏è QualityAssessmentStep ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            else:
                self.logger.warning("‚ö†Ô∏è step_model_requests.py ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏùå")
        except Exception as e:
            self.logger.error(f"‚ùå step_model_requests.py Î°úÎìú Ïã§Ìå®: {e}")
            self.step_request = None

    def _detect_m3_max(self) -> bool:
        """M3 Max Ïπ© Í∞êÏßÄ"""
        try:
            import platform
            import subprocess
            
            if platform.system() != 'Darwin' or platform.machine() != 'arm64':
                return False
            
            try:
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                cpu_info = result.stdout.strip().lower()
                
                if 'apple m3 max' in cpu_info:
                    return True
                elif 'apple m3' in cpu_info:
                    return True
                elif 'apple' in cpu_info and 'm' in cpu_info:
                    return True
                    
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
                pass
            
            try:
                import torch
                if torch.backends.mps.is_available():
                    return True
            except ImportError:
                pass
            
            return False
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max Í∞êÏßÄ Ïã§Ìå®: {e}")
            return False

    def _detect_apple_silicon(self) -> bool:
        """Apple Silicon Í∞êÏßÄ"""
        try:
            import platform
            return platform.system() == 'Darwin' and platform.machine() == 'arm64'
        except Exception:
            return False

    def _check_mps_availability(self) -> bool:
        """MPS Í∞ÄÏö©ÏÑ± Ï≤¥ÌÅ¨"""
        try:
            import torch
            return torch.backends.mps.is_available()
        except ImportError:
            return False

    def _setup_configurations(self, config: dict):
        """ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî - M3 Max ÏµúÏ†ÅÌôî + step_model_requests.py Í∏∞Î∞ò"""
        base_config = {
            'quality_threshold': config.get('quality_threshold', 0.8),
            'batch_size': self.optimal_batch_size,
            'use_mps': self.mps_available,
            'memory_efficient': self.is_m3_max,
            'quality_models': config.get('quality_models', {
                'clip_score': True,
                'lpips': True,
                'aesthetic': True,
                'technical': True
            })
        }
        
        # step_model_requests.py ÏÑ§Ï†ï Ïò§Î≤ÑÎùºÏù¥Îìú
        if self.step_request:
            base_config.update({
                'model_name': self.step_request.model_name,
                'primary_file': self.step_request.primary_file,
                'search_paths': self.step_request.search_paths,
                'memory_fraction': self.step_request.memory_fraction,
                'batch_size': self.step_request.batch_size,
                'input_size': self.step_request.input_size,
                'conda_optimized': self.step_request.conda_optimized,
                'mps_acceleration': self.step_request.mps_acceleration
            })
        
        self.config = base_config
        
        if self.is_m3_max:
            # M3 Max ÌäπÌôî ÏµúÏ†ÅÌôî
            self.config.update({
                'max_memory_gb': 128,
                'thread_count': 16,
                'enable_metal_performance_shaders': True,
                'use_unified_memory': True
            })

    def apply_m3_max_optimizations(self):
        """M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©"""
        if not self.is_m3_max:
            return
        
        try:
            import os
            import torch
            
            # M3 Max ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['OMP_NUM_THREADS'] = '16'
            os.environ['MKL_NUM_THREADS'] = '16'
            
            # PyTorch Ïä§Î†àÎìú ÏÑ§Ï†ï
            if hasattr(torch, 'set_num_threads'):
                torch.set_num_threads(16)
            
            self.logger.info("üçé M3 Max ÏµúÏ†ÅÌôî ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")

    def get_device_info(self) -> dict:
        """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'device': self.device,
            'is_m3_max': self.is_m3_max,
            'is_apple_silicon': self.is_apple_silicon,
            'mps_available': self.mps_available,
            'optimal_batch_size': self.optimal_batch_size,
            'memory_fraction': self.memory_fraction,
            'input_size': self.input_size
        }

    def set_model_loader(self, model_loader):
        """ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        self.model_loader = model_loader
        self.logger.info("‚úÖ QualityAssessmentStep ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")

    def set_memory_manager(self, memory_manager):
        """MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        self.memory_manager = memory_manager
        self.logger.info("‚úÖ QualityAssessmentStep MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")

    def set_data_converter(self, data_converter):
        """DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        self.data_converter = data_converter
        self.logger.info("‚úÖ QualityAssessmentStep DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")

    async def initialize(self) -> bool:
        """Ï¥àÍ∏∞Ìôî - step_model_requests.py Ïó∞Îèô"""
        if self.initialized:
            return True
        
        try:
            self.logger.info("üîÑ QualityAssessmentStep Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            # M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©
            if self.is_m3_max:
                self.apply_m3_max_optimizations()
            
            # step_model_requests.py Í∏∞Î∞ò Î™®Îç∏ Î°úÎî©
            await self._load_quality_models()
            
            # Í∏∞Ïà†Ï†Å Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî
            self.technical_analyzer = TechnicalQualityAnalyzer(
                device=self.device,
                enable_gpu=self.mps_available
            )
            
            self.initialized = True
            self.logger.info("‚úÖ QualityAssessmentStep Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå QualityAssessmentStep Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False

    async def _load_quality_models(self):
        """ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏ Î°úÎî© - step_model_requests.py Í∏∞Î∞ò"""
        try:
            self.logger.info("ü§ñ ÌíàÏßà ÌèâÍ∞Ä AI Î™®Îç∏ Î°úÎî© Ï§ë...")
            
            if self.step_request and self.model_loader:
                # step_model_requests.py Í∏∞Î∞ò Î™®Îç∏ ÏÑ§Ï†ï
                model_config = {
                    'input_size': self.step_request.input_size,
                    'model_architecture': self.step_request.model_architecture,
                    'device': self.device,
                    'precision': self.step_request.precision if hasattr(self.step_request, 'precision') else 'fp16'
                }
                
                # 1. OpenCLIP Î™®Îç∏ Î°úÎî©
                try:
                    clip_model = RealPerceptualQualityModel(
                        pretrained_path=self.config.get('perceptual_model_path'),
                        model_config=model_config
                    )
                    if TORCH_AVAILABLE:
                        clip_model = clip_model.to(self.device)
                        clip_model.eval()
                    
                    self.quality_models['clip'] = clip_model
                    self.logger.info("‚úÖ OpenCLIP ÌíàÏßà Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenCLIP Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                
                # 2. ÎØ∏Ï†Å ÌíàÏßà Î™®Îç∏ Î°úÎî©
                try:
                    aesthetic_model = RealAestheticQualityModel(
                        pretrained_path=self.config.get('aesthetic_model_path'),
                        model_config=model_config
                    )
                    if TORCH_AVAILABLE:
                        aesthetic_model = aesthetic_model.to(self.device)
                        aesthetic_model.eval()
                    
                    self.quality_models['aesthetic'] = aesthetic_model
                    self.logger.info("‚úÖ ÎØ∏Ï†Å ÌíàÏßà Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÎØ∏Ï†Å ÌíàÏßà Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            
            # Î°úÎî© ÏÑ±Í≥µ Ïãú
            self.model_loaded = True
            self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä Ï≤òÎ¶¨ ÏôÑÎ£å - Ï†ÑÏ≤¥ Ï†êÏàò: {quality_metrics.overall_score:.3f}, Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {quality_metrics.processing_time:.3f}Ï¥à")
            
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"‚ùå ÌíàÏßà ÌèâÍ∞Ä Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            
            # ÏóêÎü¨ ÏùëÎãµ (step_model_requests.py Ìò∏Ìôò)
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': processing_time,
                'device_info': self.get_device_info(),
                
                # Ìè¥Î∞± ÌíàÏßà Ï†êÏàò
                'overall_quality': 0.5,
                'quality_breakdown': {
                    "sharpness": 0.5,
                    "color": 0.5,
                    "fitting": 0.5,
                    "realism": 0.5,
                    "artifacts": 0.5,
                    "alignment": 0.5,
                    "lighting": 0.5,
                    "texture": 0.5
                },
                'recommendations': ["ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå® - Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî"],
                'confidence': 0.0
            }

    async def process_step_pipeline(self, input_data: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """Step ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ - step_model_requests.py Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ Í∏∞Î∞ò"""
        try:
            self.logger.info("üîÑ Step ÌååÏù¥ÌîÑÎùºÏù∏ ÌíàÏßà ÌèâÍ∞Ä Ï≤òÎ¶¨ ÏãúÏûë...")
            
            # step_model_requests.py accepts_from_previous_step Í∏∞Î∞ò ÏûÖÎ†• Ï≤òÎ¶¨
            processed_inputs = {}
            
            if self.data_flow and 'accepts_from_previous_step' in self.data_flow:
                expected_inputs = self.data_flow['accepts_from_previous_step']
                
                # Step 06ÏóêÏÑú Ïò§Îäî Îç∞Ïù¥ÌÑ∞
                if 'step_06' in expected_inputs:
                    step_06_data = input_data.get('step_06', {})
                    processed_inputs['final_result'] = step_06_data.get('final_result')
                    processed_inputs['processing_metadata'] = step_06_data.get('processing_metadata', {})
                
                # Step 07ÏóêÏÑú Ïò§Îäî Îç∞Ïù¥ÌÑ∞
                if 'step_07' in expected_inputs:
                    step_07_data = input_data.get('step_07', {})
                    processed_inputs['enhanced_image'] = step_07_data.get('enhanced_image')
                    processed_inputs['enhancement_quality'] = step_07_data.get('enhancement_quality', 0.7)
            
            # Î©îÏù∏ Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù (Ïö∞ÏÑ†ÏàúÏúÑ: enhanced_image > final_result > fallback)
            target_image = None
            if processed_inputs.get('enhanced_image') is not None:
                target_image = processed_inputs['enhanced_image']
                self.logger.info("üì∏ Step 07 Ìñ•ÏÉÅÎêú Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©")
            elif processed_inputs.get('final_result') is not None:
                target_image = processed_inputs['final_result']
                self.logger.info("üì∏ Step 06 ÏµúÏ¢Ö Í≤∞Í≥º Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©")
            else:
                # Ìè¥Î∞±: ÏßÅÏ†ë ÏûÖÎ†•Îêú Ïù¥ÎØ∏ÏßÄ
                target_image = input_data.get('image') or input_data.get('final_result')
                self.logger.warning("‚ö†Ô∏è Ïù¥Ï†Ñ Step Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå - ÏßÅÏ†ë ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©")
            
            if target_image is None:
                raise ValueError("Ï≤òÎ¶¨Ìï† Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§")
            
            # ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìñâ
            quality_result = await self.process(target_image, **kwargs)
            
            # step_model_requests.py provides_to_next_step Í∏∞Î∞ò Ï∂úÎ†• ÌòïÏãù
            pipeline_output = {
                'success': quality_result['success'],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': quality_result['processing_time'],
                
                # ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ¢Ö Ï∂úÎ†• (provides_to_next_step)
                'final_output': {
                    'quality_assessment': quality_result.get('quality_breakdown', {}),
                    'final_score': quality_result.get('overall_quality', 0.5),
                    'recommendations': quality_result.get('recommendations', [])
                },
                
                # ÏÉÅÏÑ∏ ÌíàÏßà Ï†ïÎ≥¥
                'detailed_quality': quality_result.get('quality_metrics', {}),
                'confidence': quality_result.get('confidence', 0.5),
                
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                'input_sources': list(processed_inputs.keys()),
                'enhancement_quality': processed_inputs.get('enhancement_quality'),
                'pipeline_stage': 'final'
            }
            
            self.logger.info(f"‚úÖ Step ÌååÏù¥ÌîÑÎùºÏù∏ ÌíàÏßà ÌèâÍ∞Ä ÏôÑÎ£å - ÏµúÏ¢Ö Ï†êÏàò: {pipeline_output['final_output']['final_score']:.3f}")
            
            return pipeline_output
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÌååÏù¥ÌîÑÎùºÏù∏ ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'final_output': {
                    'quality_assessment': {},
                    'final_score': 0.0,
                    'recommendations': ["ÌååÏù¥ÌîÑÎùºÏù∏ ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®"]
                }
            }

    def get_step_info(self) -> Dict[str, Any]:
        """Step Ï†ïÎ≥¥ Î∞òÌôò - step_model_requests.py Ìò∏Ìôò"""
        step_info = {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'step_class': 'QualityAssessmentStep',
            'ai_class': 'RealPerceptualQualityModel',
            'device': self.device,
            'is_m3_max': self.is_m3_max,
            'memory_gb': 128 if self.is_m3_max else 16,
            'base_step_mixin_available': BASE_STEP_MIXIN_AVAILABLE,
            'step_model_requests_available': STEP_MODEL_REQUESTS_AVAILABLE,
            'dependency_manager_available': hasattr(self, 'dependency_manager') and self.dependency_manager is not None,
            'initialized': self.initialized,
            'model_loaded': self.model_loaded,
            
            # ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÎ≥¥
            'pipeline_stages': 8,
            'is_final_step': True,
            'supports_streaming': self.step_request.supports_streaming if self.step_request else True,
            
            # ÎùºÏù¥Î∏åÎü¨Î¶¨ Í∞ÄÏö©ÏÑ±
            'torch_available': TORCH_AVAILABLE,
            'opencv_available': OPENCV_AVAILABLE,
            'pil_available': PIL_AVAILABLE,
            'skimage_available': SKIMAGE_AVAILABLE,
            'sklearn_available': SKLEARN_AVAILABLE,
            
            # step_model_requests.py Ïó∞Îèô Ï†ïÎ≥¥
            'step_request_loaded': self.step_request is not None,
            'model_name': self.config.get('model_name', 'quality_assessment_clip'),
            'primary_file': self.config.get('primary_file', 'open_clip_pytorch_model.bin'),
            'memory_fraction': self.memory_fraction,
            'batch_size': self.optimal_batch_size,
            'input_size': self.input_size,
            
            # API Ìò∏ÌôòÏÑ±
            'fastapi_compatible': True,
            'api_input_mapping': len(self.api_mapping.get('input_mapping', {})) if hasattr(self, 'api_mapping') else 0,
            'api_output_mapping': len(self.api_mapping.get('output_mapping', {})) if hasattr(self, 'api_mapping') else 0,
            
            # Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ
            'accepts_previous_steps': len(self.data_flow.get('accepts_from_previous_step', {})) if hasattr(self, 'data_flow') else 0,
            'provides_next_steps': len(self.data_flow.get('provides_to_next_step', {})) if hasattr(self, 'data_flow') else 0
        }
        
        return step_info

    def get_ai_model_info(self) -> Dict[str, Any]:
        """AI Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'ai_models': {
                'clip': {
                    'loaded': 'clip' in self.quality_models,
                    'type': 'RealPerceptualQualityModel',
                    'architecture': 'open_clip_vit',
                    'input_size': self.input_size,
                    'device': self.device
                },
                'aesthetic': {
                    'loaded': 'aesthetic' in self.quality_models,
                    'type': 'RealAestheticQualityModel',
                    'architecture': 'resnet_aesthetic',
                    'device': self.device
                },
                'technical': {
                    'loaded': self.technical_analyzer is not None,
                    'type': 'TechnicalQualityAnalyzer',
                    'device': self.device
                }
            },
            'total_models': len(self.quality_models) + (1 if self.technical_analyzer else 0),
            'ai_models_loaded': len(self.quality_models),
            'model_memory_usage': self.memory_fraction * 128 if self.is_m3_max else self.memory_fraction * 16,
            'supports_gpu': self.mps_available,
            'optimization_enabled': self.config.get('conda_optimized', True) and self.config.get('mps_acceleration', True)
        }

    def validate_dependencies_github_format(self, format_type=None) -> Dict[str, bool]:
        """GitHub ÌîÑÎ°úÏ†ùÌä∏ Ìò∏Ìôò ÏùòÏ°¥ÏÑ± Í≤ÄÏ¶ù"""
        return {
            'base_step_mixin': BASE_STEP_MIXIN_AVAILABLE,
            'step_model_requests': STEP_MODEL_REQUESTS_AVAILABLE,
            'torch': TORCH_AVAILABLE,
            'pil': PIL_AVAILABLE,
            'opencv': OPENCV_AVAILABLE,
            'skimage': SKIMAGE_AVAILABLE,
            'sklearn': SKLEARN_AVAILABLE,
            'psutil': PSUTIL_AVAILABLE,
            'model_loader': self.model_loader is not None,
            'memory_manager': self.memory_manager is not None,
            'data_converter': self.data_converter is not None,
            'technical_analyzer': self.technical_analyzer is not None,
            'step_request': self.step_request is not None,
            'quality_models': len(self.quality_models) > 0,
            'mps_available': self.mps_available,
            'initialization': self.initialized
        }

    async def cleanup_resources(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            # Î™®Îç∏ Î©îÎ™®Î¶¨ Ìï¥Ï†ú
            if hasattr(self, 'quality_models'):
                self.quality_models.clear()
            
            # Í∏∞Ïà†Ï†Å Î∂ÑÏÑùÍ∏∞ Ï†ïÎ¶¨
            if self.technical_analyzer:
                self.technical_analyzer.cleanup()
                self.technical_analyzer = None
            
            # MPS Ï∫êÏãú Ï†ïÎ¶¨
            if self.mps_available:
                safe_mps_empty_cache()
            
            # ÏùºÎ∞ò Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
            gc.collect()
            
            self.logger.info("‚úÖ QualityAssessmentStep Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è QualityAssessmentStep Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

    async def cleanup(self):
        """Ìò∏ÌôòÏÑ±: cleanup_resources Î≥ÑÏπ≠"""
        await self.cleanup_resources()

    # Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§
    def register_model_requirement(self, **kwargs):
        """Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù (StepInterface Ìò∏Ìôò)"""
        try:
            if self.step_request:
                # step_model_requests.pyÏóêÏÑú ÏûêÎèôÏúºÎ°ú ÏöîÍµ¨ÏÇ¨Ìï≠ Î°úÎìúÎê®
                self.logger.info("‚úÖ step_model_requests.pyÏóêÏÑú Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ ÏûêÎèô Î°úÎìúÎê®")
                return True
            else:
                self.logger.warning("‚ö†Ô∏è step_model_requests.py ÏöîÍµ¨ÏÇ¨Ìï≠ ÏóÜÏùå - ÏàòÎèô Îì±Î°ù ÌïÑÏöî")
                return False
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False

# ==============================================
# üî• nullcontext Ï†ïÏùò (Python 3.6 Ìò∏ÌôòÏÑ±)
# ==============================================
try:
    from contextlib import nullcontext
except ImportError:
    from contextlib import contextmanager
    
    @contextmanager
    def nullcontext():
        yield

# ==============================================
# üî• Ìå©ÌÜ†Î¶¨ Î∞è Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)
# ==============================================
def create_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """ÌíàÏßà ÌèâÍ∞Ä Step ÏÉùÏÑ± Ìï®Ïàò (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±)"""
    return QualityAssessmentStep(device=device, config=config, **kwargs)

async def create_and_initialize_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """ÌíàÏßà ÌèâÍ∞Ä Step ÏÉùÏÑ± Î∞è Ï¥àÍ∏∞Ìôî"""
    step = QualityAssessmentStep(device=device, config=config, **kwargs)
    await step.initialize()
    return step

def create_quality_assessment_with_checkpoints(
    perceptual_model_path: Optional[str] = None,
    aesthetic_model_path: Optional[str] = None,
    device: str = "auto",
    **kwargs
) -> QualityAssessmentStep:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°úÎ•º ÏßÄÏ†ïÌïú ÌíàÏßà ÌèâÍ∞Ä Step ÏÉùÏÑ±"""
    config = {
        'perceptual_model_path': perceptual_model_path,
        'aesthetic_model_path': aesthetic_model_path,
        'enable_ai_models': True,
        **kwargs.get('config', {})
    }
    
    return QualityAssessmentStep(device=device, config=config, **kwargs)

def create_quality_assessment_with_step_requests(
    device: str = "auto",
    **kwargs
) -> QualityAssessmentStep:
    """step_model_requests.py Í∏∞Î∞ò ÌíàÏßà ÌèâÍ∞Ä Step ÏÉùÏÑ± (ÏÉàÎ°úÏö¥ Ìï®Ïàò)"""
    config = {
        'use_step_requests': True,
        'auto_load_requirements': True,
        **kwargs.get('config', {})
    }
    
    return QualityAssessmentStep(device=device, config=config, **kwargs)

# ==============================================
# üî• Î™®Îìà ÏùµÏä§Ìè¨Ìä∏ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ïú†ÏßÄ + ÏÉàÎ°úÏö¥ Í∏∞Îä•)
# ==============================================
__all__ = [
    # Î©îÏù∏ ÌÅ¥ÎûòÏä§
    'QualityAssessmentStep',
    
    # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞
    'QualityMetrics',
    'QualityGrade', 
    'AssessmentMode',
    'QualityAspect',
    
    # Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
    'RealPerceptualQualityModel',
    'RealAestheticQualityModel',
    
    # Î∂ÑÏÑùÍ∏∞ ÌÅ¥ÎûòÏä§Îì§
    'TechnicalQualityAnalyzer',
    
    # Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§
    'create_quality_assessment_step',
    'create_and_initialize_quality_assessment_step',
    'create_quality_assessment_with_checkpoints',
    'create_quality_assessment_with_step_requests',  # ÏÉàÎ°úÏö¥ Ìï®Ïàò
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'safe_mps_empty_cache',
    'safe_tensor_to_numpy'
]

# ==============================================
# üî• ÌÖåÏä§Ìä∏ ÏΩîÎìú (Í∞úÎ∞úÏö©) - step_model_requests.py Ïó∞Îèô ÌÖåÏä§Ìä∏
# ==============================================
if __name__ == "__main__":
    async def test_quality_assessment_step_v18():
        """ÌíàÏßà ÌèâÍ∞Ä Step v18.0 ÌÖåÏä§Ìä∏ - step_model_requests.py Ïó∞Îèô"""
        try:
            print("üß™ QualityAssessmentStep v18.0 ÌÖåÏä§Ìä∏ ÏãúÏûë...")
            
            # Step ÏÉùÏÑ±
            step = QualityAssessmentStep(device="auto")
            
            # Í∏∞Î≥∏ ÏÜçÏÑ± ÌôïÏù∏
            assert hasattr(step, 'logger'), "logger ÏÜçÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'process'), "process Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'cleanup_resources'), "cleanup_resources Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'initialize'), "initialize Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'process_step_pipeline'), "process_step_pipeline Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§!"
            
            # step_model_requests.py Ïó∞Îèô ÌôïÏù∏
            assert hasattr(step, 'step_request'), "step_request ÏÜçÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'preprocessing_requirements'), "preprocessing_requirements ÏÜçÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§!"
            assert hasattr(step, 'api_mapping'), "api_mapping ÏÜçÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§!"
            
            # Step Ï†ïÎ≥¥ ÌôïÏù∏
            step_info = step.get_step_info()
            assert 'step_name' in step_info, "step_nameÏù¥ step_infoÏóê ÏóÜÏäµÎãàÎã§!"
            assert step_info['step_name'] == 'QualityAssessmentStep', "step_nameÏù¥ Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏäµÎãàÎã§!"
            
            # AI Î™®Îç∏ Ï†ïÎ≥¥ ÌôïÏù∏
            ai_model_info = step.get_ai_model_info()
            assert 'ai_models' in ai_model_info, "ai_modelsÍ∞Ä ai_model_infoÏóê ÏóÜÏäµÎãàÎã§!"
            
            # ÏùòÏ°¥ÏÑ± Í≤ÄÏ¶ù
            dependencies = step.validate_dependencies_github_format()
            assert isinstance(dependencies, dict), "dependenciesÍ∞Ä dictÍ∞Ä ÏïÑÎãôÎãàÎã§!"
            
            print("‚úÖ QualityAssessmentStep v18.0 ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ")
            print(f"üìä Step Ï†ïÎ≥¥: {step_info['step_name']} (ID: {step_info['step_id']})")
            print(f"üß† AI Î™®Îç∏ Ï†ïÎ≥¥: {ai_model_info['total_models']}Í∞ú Î™®Îç∏")
            print(f"üîß ÎîîÎ∞îÏù¥Ïä§: {step.device}")
            print(f"üíæ Î©îÎ™®Î¶¨: {step_info.get('memory_gb', 0)}GB")
            print(f"üçé M3 Max: {'‚úÖ' if step_info.get('is_m3_max', False) else '‚ùå'}")
            print(f"üß† BaseStepMixin: {'‚úÖ' if step_info.get('base_step_mixin_available', False) else '‚ùå'}")
            print(f"üîó step_model_requests.py: {'‚úÖ' if step_info.get('step_model_requests_available', False) else '‚ùå'}")
            print(f"üîå DependencyManager: {'‚úÖ' if step_info.get('dependency_manager_available', False) else '‚ùå'}")
            print(f"üéØ ÌååÏù¥ÌîÑÎùºÏù∏ Îã®Í≥Ñ: {step_info.get('pipeline_stages', 0)}")
            print(f"üöÄ AI Î™®Îç∏ Î°úÎìúÎê®: {step_info.get('ai_models_loaded', 0)}Í∞ú")
            print(f"üì¶ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎùºÏù¥Î∏åÎü¨Î¶¨:")
            print(f"   - PyTorch: {'‚úÖ' if step_info.get('torch_available', False) else '‚ùå'}")
            print(f"   - OpenCV: {'‚úÖ' if step_info.get('opencv_available', False) else '‚ùå'}")
            print(f"   - PIL: {'‚úÖ' if step_info.get('pil_available', False) else '‚ùå'}")
            print(f"   - scikit-image: {'‚úÖ' if step_info.get('skimage_available', False) else '‚ùå'}")
            print(f"   - scikit-learn: {'‚úÖ' if step_info.get('sklearn_available', False) else '‚ùå'}")
            
            # step_model_requests.py Ïó∞Îèô Ï†ïÎ≥¥
            print(f"üîó step_model_requests.py Ïó∞Îèô:")
            print(f"   - Step ÏöîÏ≤≠ Î°úÎìú: {'‚úÖ' if step_info.get('step_request_loaded', False) else '‚ùå'}")
            print(f"   - Î™®Îç∏Î™Ö: {step_info.get('model_name', 'N/A')}")
            print(f"   - Ï£ºÏöî ÌååÏùº: {step_info.get('primary_file', 'N/A')}")
            print(f"   - Î©îÎ™®Î¶¨ ÎπÑÏú®: {step_info.get('memory_fraction', 0.0)}")
            print(f"   - Î∞∞Ïπò ÌÅ¨Í∏∞: {step_info.get('batch_size', 1)}")
            print(f"   - ÏûÖÎ†• ÌÅ¨Í∏∞: {step_info.get('input_size', (224, 224))}")
            print(f"   - FastAPI Ìò∏Ìôò: {'‚úÖ' if step_info.get('fastapi_compatible', False) else '‚ùå'}")
            print(f"   - API ÏûÖÎ†• Îß§Ìïë: {step_info.get('api_input_mapping', 0)}Í∞ú")
            print(f"   - API Ï∂úÎ†• Îß§Ìïë: {step_info.get('api_output_mapping', 0)}Í∞ú")
            print(f"   - Ïù¥Ï†Ñ Step ÏàòÏã†: {step_info.get('accepts_previous_steps', 0)}Í∞ú")
            print(f"   - Îã§Ïùå Step Ï†ÑÏÜ°: {step_info.get('provides_next_steps', 0)}Í∞ú")
            
            # ÏùòÏ°¥ÏÑ± ÏÉÅÌÉú
            print(f"üîß ÏùòÏ°¥ÏÑ± ÏÉÅÌÉú:")
            for dep, status in dependencies.items():
                print(f"   - {dep}: {'‚úÖ' if status else '‚ùå'}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå QualityAssessmentStep v18.0 ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    import asyncio
    asyncio.run(test_quality_assessment_step_v18())AI Î™®Îç∏ Î°úÎî© ÏôÑÎ£å ({len(self.quality_models)}Í∞ú)")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")

    def _preprocess_image(self, image_data: Union[np.ndarray, Image.Image, str]) -> Dict[str, Any]:
        """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ - step_model_requests.py DetailedDataSpec Í∏∞Î∞ò"""
        try:
            # Îã§ÏñëÌïú ÏûÖÎ†• ÌòïÏãù Ï≤òÎ¶¨
            if isinstance(image_data, str):
                # base64 Î¨∏ÏûêÏó¥
                if image_data.startswith('data:image'):
                    image_data = image_data.split(',')[1]
                image_bytes = base64.b64decode(image_data)
                image = Image.open(io.BytesIO(image_bytes))
                image_array = np.array(image)
            elif isinstance(image_data, Image.Image):
                image_array = np.array(image_data)
            elif isinstance(image_data, np.ndarray):
                image_array = image_data
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïù¥ÎØ∏ÏßÄ ÌòïÏãù: {type(image_data)}")
            
            # RGB Î≥ÄÌôò
            if len(image_array.shape) == 3 and image_array.shape[2] == 4:
                image_array = image_array[:, :, :3]
            elif len(image_array.shape) == 2:
                image_array = np.stack([image_array] * 3, axis=2)
            
            # step_model_requests.py Í∏∞Î∞ò Ï†ÑÏ≤òÎ¶¨
            processed_data = {}
            
            if self.preprocessing_requirements:
                # Ï†ïÍ∑úÌôî ÏÑ§Ï†ï
                mean = self.preprocessing_requirements.get('normalization_mean', (0.48145466, 0.4578275, 0.40821073))
                std = self.preprocessing_requirements.get('normalization_std', (0.26862954, 0.26130258, 0.27577711))
                
                # ÌÅ¨Í∏∞ Ï°∞Ï†ï
                input_size = self.preprocessing_requirements.get('input_shapes', {}).get('final_result', self.input_size)
                if isinstance(input_size, tuple) and len(input_size) >= 2:
                    target_size = input_size[-2:]
                else:
                    target_size = self.input_size
                
                if PIL_AVAILABLE:
                    image_pil = Image.fromarray(image_array.astype(np.uint8))
                    image_resized = image_pil.resize(target_size)
                    image_array = np.array(image_resized)
                
                # Ï†ïÍ∑úÌôî
                image_normalized = image_array.astype(np.float32) / 255.0
                image_normalized = (image_normalized - np.array(mean)) / np.array(std)
                
                # Tensor Î≥ÄÌôò (PyTorch ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
                if TORCH_AVAILABLE:
                    image_tensor = torch.from_numpy(image_normalized.transpose(2, 0, 1)).unsqueeze(0)
                    if self.device != 'cpu':
                        image_tensor = image_tensor.to(self.device)
                    processed_data['tensor'] = image_tensor
                
                processed_data['normalized'] = image_normalized
            
            processed_data['original'] = image_array
            processed_data['preprocessed'] = True
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"‚ùå Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return {
                'original': image_data if isinstance(image_data, np.ndarray) else np.zeros((224, 224, 3)),
                'preprocessed': False,
                'error': str(e)
            }

    def _run_ai_quality_assessment(self, processed_image: Dict[str, Any]) -> Dict[str, float]:
        """AI Í∏∞Î∞ò ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìñâ"""
        try:
            quality_scores = {}
            
            # 1. OpenCLIP Í∏∞Î∞ò ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä
            if 'clip' in self.quality_models and 'tensor' in processed_image:
                try:
                    with torch.no_grad() if TORCH_AVAILABLE else nullcontext():
                        clip_results = self.quality_models['clip'](processed_image['tensor'])
                        
                        if isinstance(clip_results, dict):
                            if 'quality_scores' in clip_results:
                                # 5Ï∞®Ïõê ÌíàÏßà Ï†êÏàòÎ•º Í∞úÎ≥Ñ Î©îÌä∏Î¶≠ÏúºÎ°ú Îß§Ìïë
                                scores = safe_tensor_to_numpy(clip_results['quality_scores'])
                                if len(scores.shape) > 1:
                                    scores = scores[0]  # Ï≤´ Î≤àÏß∏ Î∞∞Ïπò
                                
                                quality_scores.update({
                                    'sharpness_score': float(scores[0]) if len(scores) > 0 else 0.7,
                                    'color_score': float(scores[1]) if len(scores) > 1 else 0.7,
                                    'realism_score': float(scores[2]) if len(scores) > 2 else 0.7,
                                    'alignment_score': float(scores[3]) if len(scores) > 3 else 0.7,
                                    'texture_score': float(scores[4]) if len(scores) > 4 else 0.7
                                })
                            
                            if 'overall_quality' in clip_results:
                                overall = safe_tensor_to_numpy(clip_results['overall_quality'])
                                quality_scores['overall_quality'] = float(overall[0]) if len(overall.shape) > 0 else float(overall)
                        
                        self.logger.debug("‚úÖ OpenCLIP ÌíàÏßà ÌèâÍ∞Ä ÏôÑÎ£å")
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenCLIP ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            
            # 2. ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä
            if 'aesthetic' in self.quality_models and 'tensor' in processed_image:
                try:
                    with torch.no_grad() if TORCH_AVAILABLE else nullcontext():
                        aesthetic_results = self.quality_models['aesthetic'](processed_image['tensor'])
                        
                        if isinstance(aesthetic_results, dict):
                            # ÎØ∏Ï†Å ÏöîÏÜåÎì§ÏùÑ fitting_scoreÏóê ÌÜµÌï©
                            aesthetic_scores = []
                            for key in ['composition', 'color_harmony', 'lighting', 'balance', 'symmetry']:
                                if key in aesthetic_results:
                                    score = safe_tensor_to_numpy(aesthetic_results[key])
                                    aesthetic_scores.append(float(score[0]) if len(score.shape) > 0 else float(score))
                            
                            if aesthetic_scores:
                                quality_scores['fitting_score'] = np.mean(aesthetic_scores)
                        
                        self.logger.debug("‚úÖ ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä ÏôÑÎ£å")
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            
            return quality_scores
            
        except Exception as e:
            self.logger.error(f"‚ùå AI ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìñâ Ïã§Ìå®: {e}")
            return {}

    def _postprocess_results(self, ai_scores: Dict[str, float], technical_scores: Dict[str, float]) -> QualityMetrics:
        """Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ - step_model_requests.py Í∏∞Î∞ò"""
        try:
            # Í∏∞Î≥∏ ÌíàÏßà Î©îÌä∏Î¶≠ ÏÉùÏÑ±
            metrics = QualityMetrics()
            
            # AI Ï†êÏàò ÌÜµÌï©
            metrics.sharpness_score = ai_scores.get('sharpness_score', technical_scores.get('sharpness', 0.5))
            metrics.color_score = ai_scores.get('color_score', technical_scores.get('saturation', 0.5))
            metrics.fitting_score = ai_scores.get('fitting_score', 0.7)
            metrics.realism_score = ai_scores.get('realism_score', 0.7)
            metrics.alignment_score = ai_scores.get('alignment_score', 0.7)
            metrics.texture_score = ai_scores.get('texture_score', 0.7)
            
            # Í∏∞Ïà†Ï†Å Ï†êÏàò ÌÜµÌï©
            metrics.artifacts_score = 1.0 - technical_scores.get('artifacts', 0.2)  # ÏïÑÌã∞Ìå©Ìä∏Îäî Ïó≠Ïàò
            metrics.lighting_score = technical_scores.get('brightness', 0.6)
            
            # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
            scores = [
                metrics.sharpness_score,
                metrics.color_score,
                metrics.fitting_score,
                metrics.realism_score,
                metrics.artifacts_score,
                metrics.alignment_score,
                metrics.lighting_score,
                metrics.texture_score
            ]
            
            metrics.overall_score = np.mean(scores)
            metrics.confidence = min(0.95, 0.7 + 0.3 * metrics.overall_score)
            
            # step_model_requests.py Í∏∞Î∞ò ÌõÑÏ≤òÎ¶¨
            if self.postprocessing_requirements:
                postprocess_steps = self.postprocessing_requirements.get('postprocessing_steps', [])
                
                if 'aggregate_scores' in postprocess_steps:
                    # Í∞ÄÏ§ë ÌèâÍ∑†ÏúºÎ°ú Ïû¨Í≥ÑÏÇ∞
                    weights = {
                        'sharpness': 0.15,
                        'color': 0.12,
                        'fitting': 0.20,
                        'realism': 0.18,
                        'artifacts': 0.10,
                        'alignment': 0.10,
                        'lighting': 0.08,
                        'texture': 0.07
                    }
                    
                    weighted_sum = sum(getattr(metrics, f"{key}_score") * weight for key, weight in weights.items())
                    metrics.overall_score = weighted_sum
                
                if 'generate_report' in postprocess_steps:
                    # Ï∂îÏ≤úÏÇ¨Ìï≠ ÏÉùÏÑ±
                    recommendations = []
                    if metrics.sharpness_score < 0.6:
                        recommendations.append("Ïù¥ÎØ∏ÏßÄ ÏÑ†Î™ÖÎèÑ Í∞úÏÑ† ÌïÑÏöî")
                    if metrics.color_score < 0.6:
                        recommendations.append("ÏÉâÏÉÅ ÌíàÏßà Í∞úÏÑ† ÌïÑÏöî")
                    if metrics.fitting_score < 0.7:
                        recommendations.append("Í∞ÄÏÉÅ ÌîºÌåÖ ÌíàÏßà Í∞úÏÑ† ÌïÑÏöî")
                    if metrics.artifacts_score < 0.7:
                        recommendations.append("Ïù¥ÎØ∏ÏßÄ ÏïÑÌã∞Ìå©Ìä∏ Ï†úÍ±∞ ÌïÑÏöî")
                    
                    metrics.recommendations = recommendations
            
            # FastAPI Ìò∏Ìôò ÌíàÏßà Î∂ÑÏÑù Ï∂îÍ∞Ä
            metrics.quality_breakdown = {
                "sharpness": metrics.sharpness_score,
                "color": metrics.color_score,
                "fitting": metrics.fitting_score,
                "realism": metrics.realism_score,
                "artifacts": metrics.artifacts_score,
                "alignment": metrics.alignment_score,
                "lighting": metrics.lighting_score,
                "texture": metrics.texture_score
            }
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï
            metrics.device_used = self.device
            metrics.model_version = "v18.0"
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"‚ùå Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            # Ìè¥Î∞± Î©îÌä∏Î¶≠ Î∞òÌôò
            return QualityMetrics(
                overall_score=0.5,
                confidence=0.5,
                sharpness_score=0.5,
                color_score=0.5,
                fitting_score=0.5,
                realism_score=0.5,
                artifacts_score=0.5,
                alignment_score=0.5,
                lighting_score=0.5,
                texture_score=0.5,
                device_used=self.device,
                model_version="v18.0_fallback"
            )

    async def process(self, image_data, **kwargs) -> Dict[str, Any]:
        """ÌíàÏßà ÌèâÍ∞Ä Ï≤òÎ¶¨ - step_model_requests.py ÏôÑÏ†Ñ Ìò∏Ìôò"""
        start_time = time.time()
        
        try:
            if not self.initialized:
                await self.initialize()
            
            self.logger.info("üîÑ ÌíàÏßà ÌèâÍ∞Ä Ï≤òÎ¶¨ ÏãúÏûë...")
            
            # 1. Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (step_model_requests.py DetailedDataSpec Í∏∞Î∞ò)
            processed_image = self._preprocess_image(image_data)
            if not processed_image.get('preprocessed', False):
                raise ValueError(f"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {processed_image.get('error', 'Unknown')}")
            
            # 2. AI Í∏∞Î∞ò ÌíàÏßà ÌèâÍ∞Ä
            ai_scores = self._run_ai_quality_assessment(processed_image)
            
            # 3. Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù
            technical_scores = {}
            if self.technical_analyzer:
                technical_scores = self.technical_analyzer.analyze(processed_image['original'])
            
            # 4. Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ Î∞è ÌÜµÌï©
            quality_metrics = self._postprocess_results(ai_scores, technical_scores)
            quality_metrics.processing_time = time.time() - start_time
            
            # 5. step_model_requests.py Í∏∞Î∞ò ÏùëÎãµ ÌòïÏãù
            response = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': quality_metrics.processing_time,
                'device_info': self.get_device_info(),
                
                # ÌíàÏßà ÌèâÍ∞Ä Í≤∞Í≥º
                'quality_metrics': quality_metrics.to_dict(),
                
                # FastAPI Ìò∏Ìôò ÏùëÎãµ (step_model_requests.py api_output_mapping Í∏∞Î∞ò)
                'overall_quality': quality_metrics.overall_score,
                'quality_breakdown': quality_metrics.quality_breakdown,
                'recommendations': quality_metrics.recommendations,
                'confidence': quality_metrics.confidence,
                
                # Step Í∞Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨ (provides_to_next_step)
                'quality_assessment': quality_metrics.quality_breakdown,
                'final_score': quality_metrics.overall_score,
                
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                'model_used': self.config.get('model_name', 'quality_assessment_clip'),
                'ai_models_loaded': len(self.quality_models),
                'technical_analysis': len(technical_scores) > 0
            }
            
            self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä