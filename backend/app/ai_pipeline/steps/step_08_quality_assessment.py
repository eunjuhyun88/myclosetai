# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - ìˆœí™˜ì°¸ì¡° í•´ê²° ì™„ì „ ë²„ì „
âœ… BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (í•œë°©í–¥ ì°¸ì¡°)
âœ… ModelLoader ì˜ì¡´ì„± ì—­ì „ íŒ¨í„´ ì ìš©
âœ… ê¸°ì¡´ íŒŒì¼ì˜ ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ ëª¨ë‘ í¬í•¨
âœ… Pipeline Manager 100% í˜¸í™˜
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ìœ ì§€
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache, wraps
import numpy as np
import base64
import io

# ğŸ”¥ BaseStepMixin ìƒì† - ìˆœí™˜ì°¸ì¡° í•´ê²°
try:
    from .base_step_mixin import (
        QualityAssessmentMixin, 
        ensure_step_initialization, 
        safe_step_method, 
        performance_monitor
    )
    BASE_STEP_MIXIN_AVAILABLE = True
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logging.warning(f"BaseStepMixin ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ í´ë°± í´ë˜ìŠ¤ - ìˆœí™˜ì°¸ì¡° ë°©ì§€
    class QualityAssessmentMixin:
        def __init__(self, *args, **kwargs):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.device = kwargs.get('device', 'cpu')
            self.step_name = 'quality_assessment'
            self.step_number = 8
            self.quality_threshold = 0.7
    
    def ensure_step_initialization(func):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            return await func(self, *args, **kwargs)
        return wrapper
    
    def safe_step_method(func):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            try:
                return await func(self, *args, **kwargs)
            except Exception as e:
                self.logger.error(f"Step ë©”ì„œë“œ ì˜¤ë¥˜: {e}")
                raise
        return wrapper
    
    def performance_monitor(name):
        def decorator(func):
            @wraps(func)
            async def wrapper(self, *args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(self, *args, **kwargs)
                    duration = time.time() - start_time
                    self.logger.info(f"âš¡ {name} ì™„ë£Œ: {duration:.2f}ì´ˆ")
                    return result
                except Exception as e:
                    duration = time.time() - start_time
                    self.logger.error(f"âŒ {name} ì‹¤íŒ¨: {e} ({duration:.2f}ì´ˆ)")
                    raise
            return wrapper
        return decorator

# ğŸ”¥ ModelLoader ì˜ì¡´ì„± ì—­ì „ íŒ¨í„´ - ìˆœí™˜ì°¸ì¡° í•´ê²°
class ModelLoaderInterface:
    """ModelLoader ì¸í„°í˜ì´ìŠ¤ (ìˆœí™˜ì°¸ì¡° í•´ê²°)"""
    
    def __init__(self):
        self._model_loader = None
        self._models = {}
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì„¤ì • (ë‚˜ì¤‘ì— ì£¼ì…)"""
        self._model_loader = model_loader
    
    async def load_model(self, model_name: str, **kwargs) -> Any:
        """ëª¨ë¸ ë¡œë“œ"""
        if self._model_loader:
            try:
                model = await self._model_loader.load_model(model_name, **kwargs)
                self._models[model_name] = model
                return model
            except Exception as e:
                logging.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
        return None
    
    def get_model(self, model_name: str) -> Any:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        return self._models.get(model_name)
    
    def cleanup(self):
        """ëª¨ë¸ ì •ë¦¬"""
        self._models.clear()
        if self._model_loader:
            try:
                self._model_loader.cleanup()
            except Exception:
                pass

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ - conda í™˜ê²½ ìµœì í™”
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âŒ OpenCV í•„ìˆ˜: conda install opencv")

try:
    from PIL import Image, ImageStat, ImageEnhance, ImageFilter, ImageDraw
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âŒ Pillow í•„ìˆ˜: conda install pillow")

try:
    from scipy import stats, ndimage, spatial
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ê¶Œì¥: conda install scipy")

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ Scikit-learn ê¶Œì¥: conda install scikit-learn")

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False
    print("âš ï¸ Scikit-image ê¶Œì¥: conda install scikit-image")

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜ (í†µí•©)
# ==============================================

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"      # 90-100ì 
    GOOD = "good"               # 75-89ì 
    ACCEPTABLE = "acceptable"    # 60-74ì 
    POOR = "poor"               # 40-59ì 
    VERY_POOR = "very_poor"     # 0-39ì 

class AssessmentMode(Enum):
    """í‰ê°€ ëª¨ë“œ"""
    FAST = "fast"              # ë¹ ë¥¸ ê¸°ë³¸ í‰ê°€
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© í‰ê°€
    DETAILED = "detailed"      # ìƒì„¸ ë¶„ì„
    NEURAL = "neural"          # AI ê¸°ë°˜ í‰ê°€

class QualityAspect(Enum):
    """í’ˆì§ˆ ì¸¡ë©´"""
    TECHNICAL = "technical"    # ê¸°ìˆ ì  í’ˆì§ˆ
    PERCEPTUAL = "perceptual"  # ì§€ê°ì  í’ˆì§ˆ
    AESTHETIC = "aesthetic"    # ë¯¸ì  í’ˆì§ˆ
    FUNCTIONAL = "functional"  # ê¸°ëŠ¥ì  í’ˆì§ˆ

# ==============================================
# ğŸ”¥ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤ (í†µí•©)
# ==============================================

@dataclass
class QualityMetrics:
    """ì™„ì „í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê¸°ì¡´ + ìƒˆë¡œìš´ ê¸°ëŠ¥ í†µí•©)"""
    
    # ê¸°ìˆ ì  í’ˆì§ˆ
    sharpness: float = 0.0
    noise_level: float = 0.0
    contrast: float = 0.0
    saturation: float = 0.0
    brightness: float = 0.0
    color_accuracy: float = 0.0
    
    # ì§€ê°ì  í’ˆì§ˆ
    structural_similarity: float = 0.0
    perceptual_similarity: float = 0.0
    visual_quality: float = 0.0
    artifact_level: float = 0.0
    ssim_score: float = 0.0
    psnr_score: float = 0.0
    lpips_score: float = 0.0
    
    # ë¯¸ì  í’ˆì§ˆ
    composition: float = 0.0
    color_harmony: float = 0.0
    symmetry: float = 0.0
    balance: float = 0.0
    
    # ê¸°ëŠ¥ì  í’ˆì§ˆ
    fitting_quality: float = 0.0
    edge_preservation: float = 0.0
    texture_quality: float = 0.0
    detail_preservation: float = 0.0
    
    # ì „ì²´ ì ìˆ˜
    overall_score: float = 0.0
    confidence: float = 0.0
    
    def calculate_overall_score(self, weights: Optional[Dict[str, float]] = None) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚° (í–¥ìƒëœ ë²„ì „)"""
        if weights is None:
            weights = {
                'technical': 0.3,
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.2
            }
        
        # ê¸°ìˆ ì  í’ˆì§ˆ (ë…¸ì´ì¦ˆëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        technical_score = np.mean([
            self.sharpness, 
            1.0 - self.noise_level,  # ë…¸ì´ì¦ˆ ì—­ì „
            self.contrast, 
            self.brightness, 
            self.saturation,
            self.color_accuracy
        ])
        
        # ì§€ê°ì  í’ˆì§ˆ (ì•„í‹°íŒ©íŠ¸ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        perceptual_score = np.mean([
            self.ssim_score or self.structural_similarity,
            self.perceptual_similarity,
            self.visual_quality,
            1.0 - self.artifact_level,  # ì•„í‹°íŒ©íŠ¸ ì—­ì „
            self.psnr_score
        ])
        
        # ë¯¸ì  í’ˆì§ˆ
        aesthetic_score = np.mean([
            self.composition, 
            self.color_harmony,
            self.symmetry, 
            self.balance
        ])
        
        # ê¸°ëŠ¥ì  í’ˆì§ˆ
        functional_score = np.mean([
            self.fitting_quality, 
            self.edge_preservation,
            self.texture_quality, 
            self.detail_preservation
        ])
        
        # ê°€ì¤‘ í‰ê· 
        self.overall_score = (
            technical_score * weights['technical'] +
            perceptual_score * weights['perceptual'] +
            aesthetic_score * weights['aesthetic'] +
            functional_score * weights['functional']
        )
        
        return self.overall_score
    
    def get_grade(self) -> QualityGrade:
        """ë“±ê¸‰ ë°˜í™˜"""
        score = self.overall_score * 100
        
        if score >= 90:
            return QualityGrade.EXCELLENT
        elif score >= 75:
            return QualityGrade.GOOD
        elif score >= 60:
            return QualityGrade.ACCEPTABLE
        elif score >= 40:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (í–¥ìƒëœ ë²„ì „)
# ==============================================

class EnhancedPerceptualQualityModel(nn.Module):
    """í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
    
    def __init__(self):
        super().__init__()
        
        # ë” ê¹Šì€ CNN ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_extractor = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8))
        )
        
        # ë‹¤ì¤‘ í’ˆì§ˆ ì˜ˆì¸¡ê¸°
        feature_dim = 256 * 8 * 8
        self.quality_predictors = nn.ModuleDict({
            'overall': nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1),
                nn.Sigmoid()
            ),
            'sharpness': nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'artifacts': nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
        })
    
    def forward(self, x):
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        
        results = {}
        for name, predictor in self.quality_predictors.items():
            results[name] = predictor(features)
        
        return results

class EnhancedAestheticQualityModel(nn.Module):
    """í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (ResNet ë°±ë³¸)"""
    
    def __init__(self):
        super().__init__()
        
        # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
        self.backbone = nn.Sequential(
            # Initial conv
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet blocks
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ë¯¸ì  ì ìˆ˜ ì˜ˆì¸¡ í—¤ë“œë“¤
        self.aesthetic_heads = nn.ModuleDict({
            'composition': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'color_harmony': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'symmetry': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'balance': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
        })
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        layers.append(self._make_resnet_block(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(self._make_resnet_block(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _make_resnet_block(self, in_channels, out_channels, stride=1):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )
    
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        
        results = {}
        for name, head in self.aesthetic_heads.items():
            results[name] = head(features)
        
        return results

# ==============================================
# ğŸ”¥ ì „ë¬¸ ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ í†µí•©)
# ==============================================

class TechnicalQualityAnalyzer:
    """í–¥ìƒëœ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
    
    def analyze_comprehensive(self, image: np.ndarray) -> Dict[str, float]:
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. ì„ ëª…ë„ ë¶„ì„ (í–¥ìƒëœ ë²„ì „)
            results['sharpness'] = self._analyze_sharpness_enhanced(image)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„ (ë‹¤ì¤‘ ë°©ë²•)
            results['noise_level'] = self._analyze_noise_multi_method(image)
            
            # 3. ëŒ€ë¹„ ë¶„ì„ (ì ì‘í˜•)
            results['contrast'] = self._analyze_contrast_adaptive(image)
            
            # 4. ë°ê¸° ë¶„ì„ (íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜)
            results['brightness'] = self._analyze_brightness_histogram(image)
            
            # 5. ì±„ë„ ë¶„ì„ (HSV ê¸°ë°˜)
            results['saturation'] = self._analyze_saturation_hsv(image)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5,
                'brightness': 0.5, 'saturation': 0.5
            }
    
    def _analyze_sharpness_enhanced(self, image: np.ndarray) -> float:
        """í–¥ìƒëœ ì„ ëª…ë„ ë¶„ì„"""
        try:
            if not CV2_AVAILABLE:
                return 0.5
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # ë‹¤ì¤‘ ì—£ì§€ ê°ì§€ ë°©ë²• ì¡°í•©
            methods = []
            
            # 1. Laplacian variance
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            methods.append(min(laplacian_var / 1000.0, 1.0))
            
            # 2. Sobel magnitude
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2).mean()
            methods.append(min(sobel_magnitude / 100.0, 1.0))
            
            # 3. Canny edge density
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            methods.append(min(edge_density * 10, 1.0))
            
            return np.mean(methods)
            
        except Exception as e:
            self.logger.error(f"ì„ ëª…ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_noise_multi_method(self, image: np.ndarray) -> float:
        """ë‹¤ì¤‘ ë°©ë²• ë…¸ì´ì¦ˆ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            noise_levels = []
            
            # 1. ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
            if CV2_AVAILABLE:
                kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                filtered = cv2.filter2D(gray, -1, kernel)
                noise_levels.append(np.std(filtered) / 255.0)
            
            # 2. ê°€ìš°ì‹œì•ˆ í•„í„° ì°¨ì´
            if CV2_AVAILABLE:
                blurred = cv2.GaussianBlur(gray, (5, 5), 0)
                noise_levels.append(np.std(gray - blurred) / 255.0)
            
            # 3. ì›¨ì´ë¸”ë¦¿ ê¸°ë°˜ (ê·¼ì‚¬)
            if len(noise_levels) == 0:
                noise_levels.append(np.std(gray) / 255.0 * 0.3)  # í´ë°±
            
            return min(np.mean(noise_levels), 1.0)
            
        except Exception as e:
            self.logger.error(f"ë…¸ì´ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.3
    
    def _analyze_contrast_adaptive(self, image: np.ndarray) -> float:
        """ì ì‘í˜• ëŒ€ë¹„ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # 1. RMS ëŒ€ë¹„
            rms_contrast = np.std(gray) / 255.0
            
            # 2. Michelson ëŒ€ë¹„
            max_val, min_val = np.max(gray), np.min(gray)
            if max_val + min_val > 0:
                michelson_contrast = (max_val - min_val) / (max_val + min_val)
            else:
                michelson_contrast = 0
            
            # 3. íˆìŠ¤í† ê·¸ë¨ ë¶„ì‚°
            hist, _ = np.histogram(gray, bins=256)
            hist_normalized = hist / np.sum(hist)
            hist_entropy = -np.sum(hist_normalized * np.log2(hist_normalized + 1e-8))
            entropy_contrast = hist_entropy / 8.0  # ì •ê·œí™”
            
            # ì¢…í•© ëŒ€ë¹„ ì ìˆ˜
            contrast_score = np.mean([rms_contrast, michelson_contrast, entropy_contrast])
            return min(contrast_score, 1.0)
            
        except Exception as e:
            self.logger.error(f"ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_brightness_histogram(self, image: np.ndarray) -> float:
        """íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ë°ê¸° ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            hist, bins = np.histogram(gray, bins=256, range=(0, 256))
            hist_normalized = hist / np.sum(hist)
            
            # ê°€ì¤‘ í‰ê·  ë°ê¸°
            bin_centers = (bins[:-1] + bins[1:]) / 2
            weighted_brightness = np.sum(hist_normalized * bin_centers) / 255.0
            
            # ì ì • ë°ê¸° ë²”ìœ„ í‰ê°€ (0.3-0.7)
            optimal_min, optimal_max = 0.3, 0.7
            
            if optimal_min <= weighted_brightness <= optimal_max:
                brightness_score = 1.0
            elif weighted_brightness < optimal_min:
                brightness_score = weighted_brightness / optimal_min
            else:
                brightness_score = 1.0 - (weighted_brightness - optimal_max) / (1.0 - optimal_max)
            
            return max(0.0, min(brightness_score, 1.0))
            
        except Exception as e:
            self.logger.error(f"ë°ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_saturation_hsv(self, image: np.ndarray) -> float:
        """HSV ê¸°ë°˜ ì±„ë„ ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation = hsv[:, :, 1]
                
                # í‰ê·  ë° ë¶„ì‚° ë¶„ì„
                mean_sat = np.mean(saturation) / 255.0
                std_sat = np.std(saturation) / 255.0
                
                # ì ì • ì±„ë„ ë²”ìœ„ (0.3-0.8) + ë¶„ì‚° ê³ ë ¤
                optimal_range = 0.3 <= mean_sat <= 0.8
                good_variance = 0.1 <= std_sat <= 0.3
                
                saturation_score = 0.0
                if optimal_range:
                    saturation_score += 0.7
                else:
                    saturation_score += max(0, 0.7 - abs(mean_sat - 0.55) * 2)
                
                if good_variance:
                    saturation_score += 0.3
                else:
                    saturation_score += max(0, 0.3 - abs(std_sat - 0.2) * 3)
                
                return min(saturation_score, 1.0)
            else:
                # RGB ê¸°ë°˜ ê·¼ì‚¬ ì±„ë„
                r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]
                max_rgb = np.maximum(np.maximum(r, g), b)
                min_rgb = np.minimum(np.minimum(r, g), b)
                saturation = np.mean((max_rgb - min_rgb) / (max_rgb + 1e-8))
                return min(saturation, 1.0)
                
        except Exception as e:
            self.logger.error(f"ì±„ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5

class FittingQualityAnalyzer:
    """ì˜ë¥˜ í”¼íŒ… í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.FittingQualityAnalyzer")
    
    def analyze_comprehensive(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray] = None, clothing_type: str = "default") -> Dict[str, float]:
        """ì¢…í•© í”¼íŒ… í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. í”¼íŒ… ì •í™•ë„
            results['fitting_accuracy'] = self._analyze_fitting_accuracy_advanced(fitted_img, person_img, clothing_type)
            
            # 2. ì—£ì§€ ë³´ì¡´
            results['edge_preservation'] = self._analyze_edge_preservation_advanced(fitted_img, person_img)
            
            # 3. í…ìŠ¤ì²˜ ë³´ì¡´
            results['texture_preservation'] = self._analyze_texture_preservation_advanced(fitted_img, person_img)
            
            # 4. í˜•íƒœ ì¼ê´€ì„±
            results['shape_consistency'] = self._analyze_shape_consistency(fitted_img, person_img)
            
            # 5. ì˜ë¥˜ë³„ íŠ¹í™” ë¶„ì„
            results['clothing_specific_quality'] = self._analyze_clothing_specific_quality(fitted_img, clothing_type)
            
            return results
            
        except Exception as e:
            self.logger.error(f"í”¼íŒ… í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'fitting_accuracy': 0.5, 'edge_preservation': 0.5,
                'texture_preservation': 0.5, 'shape_consistency': 0.5,
                'clothing_specific_quality': 0.5
            }
    
    def _analyze_fitting_accuracy_advanced(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray] = None, clothing_type: str = "default") -> float:
        """ê³ ê¸‰ í”¼íŒ… ì •í™•ë„ ë¶„ì„"""
        try:
            if person_img is None:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            accuracy_factors = []
            
            # 1. ìœ¤ê³½ì„  ì¼ì¹˜ë„
            if CV2_AVAILABLE:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
                
                fitted_contours, _ = cv2.findContours(cv2.Canny(fitted_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                person_contours, _ = cv2.findContours(cv2.Canny(person_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                if fitted_contours and person_contours:
                    # ê°€ì¥ í° ìœ¤ê³½ì„  ë¹„êµ
                    fitted_main = max(fitted_contours, key=cv2.contourArea)
                    person_main = max(person_contours, key=cv2.contourArea)
                    
                    # ìœ¤ê³½ì„  ë§¤ì¹­ ì ìˆ˜
                    match_score = cv2.matchShapes(fitted_main, person_main, cv2.CONTOURS_MATCH_I1, 0)
                    contour_similarity = max(0, 1.0 - match_score)
                    accuracy_factors.append(contour_similarity)
            
            # 2. SSIM ê¸°ë°˜ êµ¬ì¡° ìœ ì‚¬ì„±
            if SKIMAGE_AVAILABLE:
                ssim_score = ssim(person_img, fitted_img, multichannel=True, channel_axis=2)
                accuracy_factors.append(max(0, ssim_score))
            
            # 3. ì˜ë¥˜ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            clothing_weights = {
                'shirt': {'fitting': 0.4},
                'pants': {'fitting': 0.6},
                'dress': {'fitting': 0.5},
                'default': {'fitting': 0.4}
            }
            
            base_accuracy = np.mean(accuracy_factors) if accuracy_factors else 0.6
            weight = clothing_weights.get(clothing_type, clothing_weights['default'])['fitting']
            weighted_accuracy = base_accuracy * weight + 0.3 * (1 - weight)
            
            return min(weighted_accuracy, 1.0)
            
        except Exception as e:
            self.logger.error(f"í”¼íŒ… ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_edge_preservation_advanced(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray] = None) -> float:
        """ê³ ê¸‰ ì—£ì§€ ë³´ì¡´ ë¶„ì„"""
        try:
            if person_img is None or not CV2_AVAILABLE:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0]))
            
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            preservation_scores = []
            
            # 1. ë‹¤ì¤‘ ì„ê³„ê°’ Canny ì—£ì§€ ë¹„êµ
            thresholds = [(50, 150), (100, 200), (30, 100)]
            
            for low, high in thresholds:
                fitted_edges = cv2.Canny(fitted_gray, low, high)
                person_edges = cv2.Canny(person_gray, low, high)
                
                # IoU ê³„ì‚°
                intersection = np.sum((fitted_edges > 0) & (person_edges > 0))
                union = np.sum((fitted_edges > 0) | (person_edges > 0))
                
                if union > 0:
                    iou = intersection / union
                    preservation_scores.append(iou)
            
            # 2. ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ ì¼ì¹˜ë„
            grad_x_fitted = cv2.Sobel(fitted_gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y_fitted = cv2.Sobel(fitted_gray, cv2.CV_64F, 0, 1, ksize=3)
            grad_x_person = cv2.Sobel(person_gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y_person = cv2.Sobel(person_gray, cv2.CV_64F, 0, 1, ksize=3)
            
            # ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ ê°ë„
            angles_fitted = np.arctan2(grad_y_fitted, grad_x_fitted)
            angles_person = np.arctan2(grad_y_person, grad_x_person)
            
            # ê°ë„ ì°¨ì´ (circular distance)
            angle_diff = np.abs(angles_fitted - angles_person)
            angle_diff = np.minimum(angle_diff, 2*np.pi - angle_diff)
            
            # ê°•í•œ ì—£ì§€ì—ì„œë§Œ ê³„ì‚°
            edge_strength = np.sqrt(grad_x_fitted**2 + grad_y_fitted**2)
            strong_edges = edge_strength > np.percentile(edge_strength, 75)
            
            if np.sum(strong_edges) > 0:
                angle_similarity = np.mean(1.0 - angle_diff[strong_edges] / np.pi)
                preservation_scores.append(angle_similarity)
            
            return np.mean(preservation_scores) if preservation_scores else 0.6
            
        except Exception as e:
            self.logger.error(f"ì—£ì§€ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_texture_preservation_advanced(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray] = None) -> float:
        """ê³ ê¸‰ í…ìŠ¤ì²˜ ë³´ì¡´ ë¶„ì„"""
        try:
            if person_img is None:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            texture_scores = []
            
            # 1. LBP (Local Binary Pattern) ë¹„êµ
            if SKIMAGE_AVAILABLE:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(person_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ LBP
                for radius in [1, 2, 3]:
                    n_points = 8 * radius
                    fitted_lbp = feature.local_binary_pattern(fitted_gray, n_points, radius, method='uniform')
                    person_lbp = feature.local_binary_pattern(person_gray, n_points, radius, method='uniform')
                    
                    # íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
                    fitted_hist, _ = np.histogram(fitted_lbp.ravel(), bins=n_points + 2)
                    person_hist, _ = np.histogram(person_lbp.ravel(), bins=n_points + 2)
                    
                    # ì •ê·œí™”
                    fitted_hist = fitted_hist.astype(float) / (fitted_hist.sum() + 1e-8)
                    person_hist = person_hist.astype(float) / (person_hist.sum() + 1e-8)
                    
                    # Bhattacharyya coefficient
                    similarity = np.sum(np.sqrt(fitted_hist * person_hist))
                    texture_scores.append(similarity)
            
            # 2. ê°€ë³´ í•„í„° ì‘ë‹µ ë¹„êµ
            if SKIMAGE_AVAILABLE:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(person_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:
                    for frequency in [0.1, 0.3, 0.5]:
                        fitted_gabor, _ = filters.gabor(fitted_gray, frequency=frequency, theta=theta)
                        person_gabor, _ = filters.gabor(person_gray, frequency=frequency, theta=theta)
                        
                        # ì‘ë‹µ ìƒê´€ê´€ê³„
                        correlation = np.corrcoef(fitted_gabor.ravel(), person_gabor.ravel())[0, 1]
                        if not np.isnan(correlation):
                            texture_scores.append(max(0, correlation))
            
            # 3. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¶„ì„
            if len(texture_scores) == 0:
                # í´ë°±: ê°„ë‹¨í•œ í…ìŠ¤ì²˜ ë¶„ì„
                fitted_std = np.std(fitted_img)
                person_std = np.std(person_img)
                if person_std > 0:
                    texture_similarity = min(fitted_std / person_std, person_std / fitted_std)
                    texture_scores.append(texture_similarity)
            
            return np.mean(texture_scores) if texture_scores else 0.6
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤ì²˜ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_shape_consistency(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray] = None) -> float:
        """í˜•íƒœ ì¼ê´€ì„± ë¶„ì„"""
        try:
            if person_img is None or not CV2_AVAILABLE:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0]))
            
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            # ëª¨ë©˜íŠ¸ ê¸°ë°˜ í˜•íƒœ ë¶„ì„
            fitted_moments = cv2.moments(fitted_gray)
            person_moments = cv2.moments(person_gray)
            
            # Hu ëª¨ë©˜íŠ¸ (í˜•íƒœ ë¶ˆë³€ íŠ¹ì§•)
            fitted_hu = cv2.HuMoments(fitted_moments).flatten()
            person_hu = cv2.HuMoments(person_moments).flatten()
            
            # ë¡œê·¸ ë³€í™˜
            fitted_hu = -np.sign(fitted_hu) * np.log10(np.abs(fitted_hu) + 1e-10)
            person_hu = -np.sign(person_hu) * np.log10(np.abs(person_hu) + 1e-10)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            hu_similarity = np.exp(-np.sum(np.abs(fitted_hu - person_hu)))
            
            return min(hu_similarity, 1.0)
            
        except Exception as e:
            self.logger.error(f"í˜•íƒœ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_clothing_specific_quality(self, fitted_img: np.ndarray, clothing_type: str) -> float:
        """ì˜ë¥˜ë³„ íŠ¹í™” í’ˆì§ˆ ë¶„ì„"""
        try:
            # ì˜ë¥˜ íƒ€ì…ë³„ íŠ¹í™” ë¶„ì„
            if clothing_type in ['shirt', 'top']:
                return self._analyze_shirt_quality(fitted_img)
            elif clothing_type in ['pants', 'jeans']:
                return self._analyze_pants_quality(fitted_img)
            elif clothing_type == 'dress':
                return self._analyze_dress_quality(fitted_img)
            elif clothing_type == 'jacket':
                return self._analyze_jacket_quality(fitted_img)
            else:
                return self._analyze_general_clothing_quality(fitted_img)
                
        except Exception as e:
            self.logger.error(f"ì˜ë¥˜ë³„ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_shirt_quality(self, fitted_img: np.ndarray) -> float:
        """ì…”ì¸  í’ˆì§ˆ ë¶„ì„"""
        # ì…”ì¸  íŠ¹í™”: ì£¼ë¦„, ë‹¨ì¶”, ì¹¼ë¼ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_pants_quality(self, fitted_img: np.ndarray) -> float:
        """ë°”ì§€ í’ˆì§ˆ ë¶„ì„"""
        # ë°”ì§€ íŠ¹í™”: ë‹¤ë¦¬ ì„ , ì£¼ë¦„, í• ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_dress_quality(self, fitted_img: np.ndarray) -> float:
        """ë“œë ˆìŠ¤ í’ˆì§ˆ ë¶„ì„"""
        # ë“œë ˆìŠ¤ íŠ¹í™”: ë“œë ˆì´í•‘, ì‹¤ë£¨ì—£ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_jacket_quality(self, fitted_img: np.ndarray) -> float:
        """ì¬í‚· í’ˆì§ˆ ë¶„ì„"""
        # ì¬í‚· íŠ¹í™”: ì–´ê¹¨ì„ , ë¼í , í…ìŠ¤ì²˜ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_general_clothing_quality(self, fitted_img: np.ndarray) -> float:
        """ì¼ë°˜ ì˜ë¥˜ í’ˆì§ˆ ë¶„ì„"""
        return 0.6  # ê¸°ë³¸ í’ˆì§ˆ

class ColorQualityAnalyzer:
    """ìƒ‰ìƒ í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.ColorQualityAnalyzer")
    
    def analyze_comprehensive(self, fitted_img: np.ndarray, original_img: Optional[np.ndarray] = None) -> Dict[str, float]:
        """ì¢…í•© ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. ìƒ‰ìƒ ì •í™•ë„ (ì›ë³¸ ëŒ€ë¹„)
            if original_img is not None:
                results['color_accuracy'] = self._analyze_color_accuracy_advanced(fitted_img, original_img)
            else:
                results['color_accuracy'] = 0.7
            
            # 2. ìƒ‰ìƒ ì¡°í™”
            results['color_harmony'] = self._analyze_color_harmony_advanced(fitted_img)
            
            # 3. ìƒ‰ìƒ ì¼ê´€ì„±
            results['color_consistency'] = self._analyze_color_consistency_advanced(fitted_img)
            
            # 4. ìƒ‰ìƒ ìƒë™ê°
            results['color_vibrancy'] = self._analyze_color_vibrancy(fitted_img)
            
            # 5. í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤
            results['white_balance'] = self._analyze_white_balance(fitted_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_accuracy': 0.5, 'color_harmony': 0.5,
                'color_consistency': 0.5, 'color_vibrancy': 0.5,
                'white_balance': 0.5
            }
    
    def _analyze_color_accuracy_advanced(self, fitted_img: np.ndarray, original_img: np.ndarray) -> float:
        """ê³ ê¸‰ ìƒ‰ìƒ ì •í™•ë„ ë¶„ì„"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_img.shape != original_img.shape:
                original_img = cv2.resize(original_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else original_img
            
            accuracy_scores = []
            
            # 1. LAB ìƒ‰ê³µê°„ì—ì„œ Delta E ê³„ì‚°
            if CV2_AVAILABLE:
                fitted_lab = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2LAB)
                original_lab = cv2.cvtColor(original_img, cv2.COLOR_RGB2LAB)
                
                # í”½ì…€ë³„ Delta E
                delta_e = np.sqrt(np.sum((fitted_lab.astype(float) - original_lab.astype(float))**2, axis=2))
                mean_delta_e = np.mean(delta_e)
                
                # Delta Eë¥¼ 0-1 ìŠ¤ì½”ì–´ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                delta_e_score = max(0, 1.0 - mean_delta_e / 100.0)
                accuracy_scores.append(delta_e_score)
            
            # 2. HSV ìƒ‰ê³µê°„ ë¹„êµ
            if CV2_AVAILABLE:
                fitted_hsv = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2HSV)
                original_hsv = cv2.cvtColor(original_img, cv2.COLOR_RGB2HSV)
                
                # ìƒ‰ìƒ(H), ì±„ë„(S), ëª…ë„(V) ê°ê° ë¹„êµ
                h_diff = np.mean(np.abs(fitted_hsv[:,:,0].astype(float) - original_hsv[:,:,0].astype(float))) / 180.0
                s_diff = np.mean(np.abs(fitted_hsv[:,:,1].astype(float) - original_hsv[:,:,1].astype(float))) / 255.0
                v_diff = np.mean(np.abs(fitted_hsv[:,:,2].astype(float) - original_hsv[:,:,2].astype(float))) / 255.0
                
                hsv_score = 1.0 - np.mean([h_diff, s_diff, v_diff])
                accuracy_scores.append(max(0, hsv_score))
            
            # 3. íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
            if CV2_AVAILABLE:
                hist_scores = []
                for i in range(3):  # RGB ê° ì±„ë„
                    fitted_hist = cv2.calcHist([fitted_img], [i], None, [256], [0, 256])
                    original_hist = cv2.calcHist([original_img], [i], None, [256], [0, 256])
                    
                    # íˆìŠ¤í† ê·¸ë¨ ìƒê´€ê´€ê³„
                    correlation = cv2.compareHist(fitted_hist, original_hist, cv2.HISTCMP_CORREL)
                    hist_scores.append(max(0, correlation))
                
                accuracy_scores.append(np.mean(hist_scores))
            
            # í´ë°±: ê°„ë‹¨í•œ í‰ê·  ìƒ‰ìƒ ë¹„êµ
            if len(accuracy_scores) == 0:
                fitted_mean = np.mean(fitted_img, axis=(0, 1))
                original_mean = np.mean(original_img, axis=(0, 1))
                diff = np.linalg.norm(fitted_mean - original_mean) / (255 * np.sqrt(3))
                accuracy_scores.append(max(0, 1.0 - diff))
            
            return np.mean(accuracy_scores)
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_color_harmony_advanced(self, image: np.ndarray) -> float:
        """ê³ ê¸‰ ìƒ‰ìƒ ì¡°í™” ë¶„ì„"""
        try:
            harmony_scores = []
            
            # 1. ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ ë° ì¡°í™” ë¶„ì„
            if SKLEARN_AVAILABLE:
                pixels = image.reshape(-1, 3)
                
                # ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
                if len(pixels) > 20000:
                    indices = np.random.choice(len(pixels), 20000, replace=False)
                    pixels = pixels[indices]
                
                # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
                for n_colors in [3, 5, 7]:
                    try:
                        kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
                        kmeans.fit(pixels)
                        
                        centers = kmeans.cluster_centers_
                        
                        # ìƒ‰ìƒ ê°„ ê°ë„ ê´€ê³„ ë¶„ì„ (HSV ê³µê°„)
                        if CV2_AVAILABLE:
                            hsv_centers = []
                            for center in centers:
                                center_rgb = center.reshape(1, 1, 3).astype(np.uint8)
                                center_hsv = cv2.cvtColor(center_rgb, cv2.COLOR_RGB2HSV)[0, 0]
                                hsv_centers.append(center_hsv[0])  # ìƒ‰ìƒê°’ë§Œ
                            
                            # ì¡°í™”ë¡œìš´ ìƒ‰ìƒ ê´€ê³„ í™•ì¸ (ë³´ìƒ‰, 3ìƒ‰ ì¡°í™”, ìœ ì‚¬ìƒ‰ ë“±)
                            angles = np.array(hsv_centers) * 2  # 0-360ë„ ë³€í™˜
                            angle_diffs = []
                            
                            for i in range(len(angles)):
                                for j in range(i+1, len(angles)):
                                    diff = abs(angles[i] - angles[j])
                                    diff = min(diff, 360 - diff)  # ì›í˜• ê±°ë¦¬
                                    angle_diffs.append(diff)
                            
                            # ì¡°í™”ë¡œìš´ ê°ë„ë“¤ (60ë„ ë°°ìˆ˜)
                            harmonic_angles = [60, 120, 180]
                            harmony_score = 0
                            
                            for diff in angle_diffs:
                                for harmonic in harmonic_angles:
                                    if abs(diff - harmonic) <= 15:  # 15ë„ í—ˆìš© ì˜¤ì°¨
                                        harmony_score += 1
                            
                            harmony_scores.append(min(harmony_score / len(angle_diffs), 1.0))
                    
                    except Exception as e:
                        continue
            
            # 2. ìƒ‰ìƒ ë¶„ì‚° ë¶„ì„
            color_std = np.std(image, axis=(0, 1))
            color_balance = 1.0 - np.std(color_std) / (np.mean(color_std) + 1e-8)
            harmony_scores.append(max(0, min(color_balance, 1.0)))
            
            # 3. ì±„ë„ ì¼ê´€ì„±
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation_std = np.std(hsv[:, :, 1]) / 255.0
                saturation_consistency = max(0, 1.0 - saturation_std)
                harmony_scores.append(saturation_consistency)
            
            return np.mean(harmony_scores) if harmony_scores else 0.6
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì¡°í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_color_consistency_advanced(self, image: np.ndarray) -> float:
        """ê³ ê¸‰ ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„"""
        try:
            h, w = image.shape[:2]
            consistency_scores = []
            
            # 1. ì§€ì—­ë³„ ìƒ‰ìƒ ë¶„í¬ ë¹„êµ
            regions = [
                image[:h//2, :w//2],      # ì¢Œìƒ
                image[:h//2, w//2:],      # ìš°ìƒ
                image[h//2:, :w//2],      # ì¢Œí•˜
                image[h//2:, w//2:]       # ìš°í•˜
            ]
            
            region_stats = []
            for region in regions:
                if region.size > 0:
                    mean_color = np.mean(region, axis=(0, 1))
                    std_color = np.std(region, axis=(0, 1))
                    region_stats.append({'mean': mean_color, 'std': std_color})
            
            # ì§€ì—­ ê°„ ì¼ê´€ì„± ê³„ì‚°
            if len(region_stats) > 1:
                mean_diffs = []
                std_diffs = []
                
                for i in range(len(region_stats)):
                    for j in range(i+1, len(region_stats)):
                        mean_diff = np.linalg.norm(region_stats[i]['mean'] - region_stats[j]['mean'])
                        std_diff = np.linalg.norm(region_stats[i]['std'] - region_stats[j]['std'])
                        
                        mean_diffs.append(mean_diff)
                        std_diffs.append(std_diff)
                
                # ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ì¼ê´€ì„±ì´ ë†’ìŒ
                mean_consistency = max(0, 1.0 - np.mean(mean_diffs) / 128.0)
                std_consistency = max(0, 1.0 - np.mean(std_diffs) / 64.0)
                
                consistency_scores.extend([mean_consistency, std_consistency])
            
            # 2. ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
                
                # ì ë‹¹í•œ ê·¸ë¼ë””ì–¸íŠ¸ (ë„ˆë¬´ uniformí•˜ì§€ë„ chaoticí•˜ì§€ë„ ì•Šê²Œ)
                grad_mean = np.mean(gradient_magnitude)
                grad_std = np.std(gradient_magnitude)
                
                # ì ì • ë²”ìœ„ í‰ê°€
                optimal_grad_mean = 50  # ê²½í—˜ì  ê°’
                optimal_grad_std = 30
                
                grad_score = (
                    max(0, 1.0 - abs(grad_mean - optimal_grad_mean) / optimal_grad_mean) +
                    max(0, 1.0 - abs(grad_std - optimal_grad_std) / optimal_grad_std)
                ) / 2
                
                consistency_scores.append(grad_score)
            
            return np.mean(consistency_scores) if consistency_scores else 0.6
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_color_vibrancy(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ìƒë™ê° ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                
                # ì±„ë„ì™€ ëª…ë„ë¥¼ ì¢…í•©í•œ ìƒë™ê°
                saturation = hsv[:, :, 1] / 255.0
                value = hsv[:, :, 2] / 255.0
                
                # ë†’ì€ ì±„ë„ + ì ë‹¹í•œ ëª…ë„ = ìƒë™ê°
                vibrancy = saturation * value * (1.0 - np.abs(value - 0.7))  # 0.7 ì£¼ë³€ì´ ìµœì 
                
                mean_vibrancy = np.mean(vibrancy)
                return min(mean_vibrancy * 2, 1.0)  # ìŠ¤ì¼€ì¼ë§
            else:
                # RGB ê¸°ë°˜ ê·¼ì‚¬
                rgb_range = np.max(image, axis=2) - np.min(image, axis=2)
                vibrancy = np.mean(rgb_range) / 255.0
                return min(vibrancy * 1.5, 1.0)
                
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ìƒë™ê° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_white_balance(self, image: np.ndarray) -> float:
        """í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë¶„ì„"""
        try:
            # RGB ì±„ë„ ê°„ ê· í˜• ë¶„ì„
            r_mean = np.mean(image[:, :, 0])
            g_mean = np.mean(image[:, :, 1])
            b_mean = np.mean(image[:, :, 2])
            
            # ì´ìƒì ì¸ í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ì—ì„œëŠ” RGBê°€ ë¹„ìŠ·í•´ì•¼ í•¨
            rgb_means = np.array([r_mean, g_mean, b_mean])
            overall_mean = np.mean(rgb_means)
            
            if overall_mean > 0:
                deviations = np.abs(rgb_means - overall_mean) / overall_mean
                balance_score = max(0, 1.0 - np.mean(deviations))
            else:
                balance_score = 0.5
            
            return balance_score
            
        except Exception as e:
            self.logger.error(f"í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤ (í†µí•© ë²„ì „)
# ==============================================

class QualityAssessmentStep(QualityAssessmentMixin):
    """
    ğŸ”¥ 8ë‹¨ê³„: ì™„ì „ í†µí•© í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
    âœ… BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (í•œë°©í–¥ ì°¸ì¡°)
    âœ… ModelLoader ì˜ì¡´ì„± ì—­ì „ íŒ¨í„´ ì ìš©
    âœ… ê¸°ì¡´ íŒŒì¼ì˜ ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨
    âœ… Pipeline Manager 100% í˜¸í™˜
    âœ… M3 Max ìµœì í™” + conda í™˜ê²½ ìµœì í™”
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í’ˆì§ˆ ê°€ì¤‘ì¹˜ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
    CLOTHING_QUALITY_WEIGHTS = {
        'shirt': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'dress': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'pants': {'fitting': 0.6, 'texture': 0.2, 'edge': 0.1, 'color': 0.1},
        'jacket': {'fitting': 0.3, 'texture': 0.4, 'edge': 0.2, 'color': 0.1},
        'skirt': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'top': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'default': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1}
    }
    
    # ì›ë‹¨ íƒ€ì…ë³„ í’ˆì§ˆ ê¸°ì¤€ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
    FABRIC_QUALITY_STANDARDS = {
        'cotton': {'texture_importance': 0.8, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.3},
        'silk': {'texture_importance': 0.9, 'drape_importance': 0.9, 'wrinkle_tolerance': 0.2},
        'wool': {'texture_importance': 0.7, 'drape_importance': 0.7, 'wrinkle_tolerance': 0.4},
        'polyester': {'texture_importance': 0.5, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.8},
        'denim': {'texture_importance': 0.9, 'drape_importance': 0.4, 'wrinkle_tolerance': 0.6},
        'leather': {'texture_importance': 0.95, 'drape_importance': 0.3, 'wrinkle_tolerance': 0.9},
        'default': {'texture_importance': 0.7, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.5}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… QualityAssessmentMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°"""
        
        # ğŸ”¥ QualityAssessmentMixin ì´ˆê¸°í™” - logger ì†ì„± ìë™ ì„¤ì •
        super().__init__(device=device, config=config, **kwargs)
        
        # í’ˆì§ˆ í‰ê°€ ì„¤ì •
        self.assessment_config = {
            'mode': getattr(self.config, 'assessment_mode', 'comprehensive') if hasattr(self.config, 'assessment_mode') else 'comprehensive',
            'technical_analysis_enabled': getattr(self.config, 'technical_analysis_enabled', True) if hasattr(self.config, 'technical_analysis_enabled') else True,
            'perceptual_analysis_enabled': getattr(self.config, 'perceptual_analysis_enabled', True) if hasattr(self.config, 'perceptual_analysis_enabled') else True,
            'aesthetic_analysis_enabled': getattr(self.config, 'aesthetic_analysis_enabled', True) if hasattr(self.config, 'aesthetic_analysis_enabled') else True,
            'functional_analysis_enabled': getattr(self.config, 'functional_analysis_enabled', True) if hasattr(self.config, 'functional_analysis_enabled') else True,
            'detailed_analysis_enabled': getattr(self.config, 'detailed_analysis_enabled', True) if hasattr(self.config, 'detailed_analysis_enabled') else True,
            'neural_analysis_enabled': getattr(self.config, 'neural_analysis_enabled', True) if hasattr(self.config, 'neural_analysis_enabled') else True,
            'ai_models_enabled': TORCH_AVAILABLE,
            'quality_threshold': 0.7,
            'save_intermediate_results': False
        }
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        if config:
            self.assessment_config.update(config)
        
        # M3 Max ìµœì í™” ì„¤ì •
        self._setup_m3_max_optimization()
        
        # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° í•´ê²°)
        self.model_interface = ModelLoaderInterface()
        
        # ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
        self._initialize_professional_analyzers()
        
        # AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._initialize_enhanced_ai_models()
        
        # í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self._setup_comprehensive_assessment_pipeline()
        
        self.logger.info(f"âœ… {self.step_name} í†µí•© ì´ˆê¸°í™” ì™„ë£Œ - ìˆœí™˜ì°¸ì¡° ë¬¸ì œ í•´ê²°ë¨")
    
    def _setup_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì„¤ì • (í–¥ìƒëœ ë²„ì „)"""
        if TORCH_AVAILABLE:
            try:
                # M3 Max íŠ¹í™” ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    torch.backends.mps.empty_cache()
                
                # 128GB ë©”ëª¨ë¦¬ í™œìš©ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì„¤ì •
                self.batch_size = 16  # M3 Max 128GBì— ìµœì í™”
                self.use_mixed_precision = True
                self.parallel_analysis = True
                
                self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì™„ë£Œ (128GB + í–¥ìƒëœ ë¶„ì„)")
            except Exception as e:
                self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
        else:
            self.batch_size = 8
            self.use_mixed_precision = False
            self.parallel_analysis = False
    
    def _initialize_professional_analyzers(self):
        """ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”"""
        try:
            # 1. í–¥ìƒëœ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(self.device)
            
            # 2. í”¼íŒ… í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°
            self.fitting_analyzer = FittingQualityAnalyzer(self.device)
            
            # 3. ìƒ‰ìƒ í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°
            self.color_analyzer = ColorQualityAnalyzer(self.device)
            
            self.logger.info("ğŸ”§ ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_enhanced_ai_models(self):
        """í–¥ìƒëœ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™” - ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™"""
        self.ai_models = {}
        
        if not TORCH_AVAILABLE:
            self.logger.warning("âš ï¸ AI ëª¨ë¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
        
        try:
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹œë„
            if hasattr(self, 'model_interface'):
                self._load_models_via_interface()
            
            # í´ë°±: ì§ì ‘ ëª¨ë¸ ìƒì„±
            self._create_fallback_models()
            
            # M3 Max ìµœì í™”
            self._optimize_models_for_m3_max()
            
            self.logger.info(f"ğŸ§  í–¥ìƒëœ AI ëª¨ë¸ {len(self.ai_models)}ê°œ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒëœ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_models = {}
    
    def _load_models_via_interface(self):
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            # ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©ì€ ë‚˜ì¤‘ì— ì²˜ë¦¬
            self.pending_model_loads = [
                'enhanced_perceptual_quality',
                'enhanced_aesthetic_quality',
                'quality_assessment_combined'
            ]
            self.logger.info("ğŸ“‹ ModelLoader ì¸í„°í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì˜ˆì•½ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì¸í„°í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _create_fallback_models(self):
        """í´ë°± ëª¨ë¸ë“¤ ì§ì ‘ ìƒì„±"""
        try:
            # í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if TORCH_AVAILABLE:
                self.ai_models['enhanced_perceptual'] = EnhancedPerceptualQualityModel()
                self.ai_models['enhanced_perceptual'].to(self.device)
                self.ai_models['enhanced_perceptual'].eval()
            
            # í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if TORCH_AVAILABLE:
                self.ai_models['enhanced_aesthetic'] = EnhancedAestheticQualityModel()
                self.ai_models['enhanced_aesthetic'].to(self.device)
                self.ai_models['enhanced_aesthetic'].eval()
            
        except Exception as e:
            self.logger.error(f"í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _optimize_models_for_m3_max(self):
        """M3 Maxìš© ëª¨ë¸ ìµœì í™”"""
        if TORCH_AVAILABLE:
            try:
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'half'):
                        model.half() if self.device != "cpu" else model.float()
                    # M3 Max Neural Engine ìµœì í™” ì„¤ì •
                    if hasattr(model, 'eval'):
                        model.eval()
                
                self.logger.info("ğŸ AI ëª¨ë¸ë“¤ M3 Max ìµœì í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _setup_comprehensive_assessment_pipeline(self):
        """ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # í‰ê°€ ìˆœì„œ ì •ì˜ (ë” ì„¸ë°€í•œ ë‹¨ê³„ë“¤)
        self.assessment_pipeline = []
        
        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
        self.assessment_pipeline.append(('preprocessing', self._preprocess_for_assessment))
        self.assessment_pipeline.append(('validation', self._validate_inputs))
        
        # 2. ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ (í–¥ìƒëœ ë²„ì „)
        if self.assessment_config['technical_analysis_enabled']:
            self.assessment_pipeline.append(('technical_analysis', self._analyze_technical_quality_comprehensive))
        
        # 3. ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ (AI + ì „í†µì  ë°©ë²•)
        if self.assessment_config['perceptual_analysis_enabled']:
            self.assessment_pipeline.append(('perceptual_analysis', self._analyze_perceptual_quality_enhanced))
        
        # 4. ë¯¸ì  í’ˆì§ˆ ë¶„ì„ (AI + ì „í†µì  ë°©ë²•)
        if self.assessment_config['aesthetic_analysis_enabled']:
            self.assessment_pipeline.append(('aesthetic_analysis', self._analyze_aesthetic_quality_enhanced))
        
        # 5. ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ (ì˜ë¥˜ íŠ¹í™”)
        if self.assessment_config['functional_analysis_enabled']:
            self.assessment_pipeline.append(('functional_analysis', self._analyze_functional_quality_comprehensive))
        
        # 6. ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ (ì „ë¬¸ ë¶„ì„)
        self.assessment_pipeline.append(('color_analysis', self._analyze_color_quality_professional))
        
        # 7. ì¢…í•© í‰ê°€ ë° ì ìˆ˜ ê³„ì‚°
        self.assessment_pipeline.append(('final_assessment', self._calculate_comprehensive_final_score))
        
        self.logger.info(f"ğŸ“‹ ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ ({len(self.assessment_pipeline)}ë‹¨ê³„)")
    
    # =================================================================
    # ğŸ”¥ í–¥ìƒëœ ë¶„ì„ ë©”ì„œë“œë“¤
    # =================================================================
    
    def _preprocess_for_assessment(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            processed_img = data.get('processed_image')
            if processed_img is None:
                raise ValueError("processed_imageê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ í˜•íƒœ í™•ì¸
            if not isinstance(processed_img, np.ndarray):
                raise ValueError("processed_imageëŠ” numpy ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ ì°¨ì› í™•ì¸
            if len(processed_img.shape) != 3 or processed_img.shape[2] != 3:
                raise ValueError("ì´ë¯¸ì§€ëŠ” 3ì±„ë„ RGB í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
            
            # ì¶”ê°€ ì „ì²˜ë¦¬
            result = {
                'preprocessing_successful': True,
                'processed_image': processed_img,
                'image_shape': processed_img.shape,
                'image_dtype': processed_img.dtype
            }
            
            # ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ ì „ì²˜ë¦¬ (ì„ íƒì )
            if CV2_AVAILABLE:
                enhanced_img = self._enhance_image_for_assessment(processed_img)
                result['enhanced_image'] = enhanced_img
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒëœ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'preprocessing_successful': False, 'error': str(e)}
    
    def _validate_inputs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ê²€ì¦"""
        try:
            validation_results = {
                'validation_successful': True,
                'warnings': [],
                'recommendations': []
            }
            
            processed_img = data.get('processed_image')
            original_img = data.get('original_image')
            
            # 1. ì´ë¯¸ì§€ í•´ìƒë„ ê²€ì¦
            if processed_img is not None:
                h, w = processed_img.shape[:2]
                if h < 256 or w < 256:
                    validation_results['warnings'].append("ë‚®ì€ í•´ìƒë„ë¡œ ì¸í•´ ë¶„ì„ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                    validation_results['recommendations'].append("ìµœì†Œ 512x512 í•´ìƒë„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
                
                if h > 2048 or w > 2048:
                    validation_results['warnings'].append("ë§¤ìš° ë†’ì€ í•´ìƒë„ë¡œ ì¸í•´ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # 2. ì›ë³¸ ì´ë¯¸ì§€ ìœ ë¬´ í™•ì¸
            if original_img is None:
                validation_results['warnings'].append("ì›ë³¸ ì´ë¯¸ì§€ê°€ ì—†ì–´ ë¹„êµ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤")
                validation_results['recommendations'].append("ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì›ë³¸ ì´ë¯¸ì§€ ì œê³µì„ ê¶Œì¥í•©ë‹ˆë‹¤")
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'validation_successful': False, 'error': str(e)}
    
    def _analyze_technical_quality_comprehensive(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.technical_analyzer.analyze_comprehensive(processed_img)
            
            # ì¶”ê°€ ë©”íŠ¸ë¦­ë“¤
            results.update({
                'image_entropy': self._calculate_image_entropy(processed_img),
                'compression_artifacts': self._detect_compression_artifacts(processed_img),
                'blur_detection': self._detect_blur(processed_img)
            })
            
            self.logger.info(f"ğŸ“Š ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'technical_analysis_successful': True,
                'technical_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'technical_analysis_successful': False, 'error': str(e)}
    
    async def _analyze_perceptual_quality_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            
            results = {}
            
            # 1. í–¥ìƒëœ AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
            if 'enhanced_perceptual' in self.ai_models:
                try:
                    ai_results = await self._run_enhanced_perceptual_model(processed_img)
                    results.update(ai_results)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í–¥ìƒëœ ì§€ê°ì  AI ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # 2. ì „í†µì  ë°©ë²•ë“¤
            if original_img is not None:
                # SSIM, PSNR ë“±
                traditional_results = self._calculate_traditional_perceptual_metrics(processed_img, original_img)
                results.update(traditional_results)
            
            # 3. ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­
            no_ref_results = self._calculate_no_reference_quality_metrics(processed_img)
            results.update(no_ref_results)
            
            self.logger.info(f"ğŸ‘ í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'perceptual_analysis_successful': True,
                'perceptual_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'perceptual_analysis_successful': False, 'error': str(e)}
    
    async def _analyze_aesthetic_quality_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            
            results = {}
            
            # 1. í–¥ìƒëœ AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
            if 'enhanced_aesthetic' in self.ai_models:
                try:
                    ai_results = await self._run_enhanced_aesthetic_model(processed_img)
                    results.update(ai_results)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í–¥ìƒëœ ë¯¸ì  AI ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # 2. ì „í†µì  ë¯¸ì  ë¶„ì„
            traditional_results = self._calculate_traditional_aesthetic_metrics(processed_img)
            results.update(traditional_results)
            
            # 3. ê³ ê¸‰ êµ¬ë„ ë¶„ì„
            advanced_composition = self._analyze_advanced_composition(processed_img)
            results.update(advanced_composition)
            
            self.logger.info(f"ğŸ¨ í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'aesthetic_analysis_successful': True,
                'aesthetic_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'aesthetic_analysis_successful': False, 'error': str(e)}
    
    def _analyze_functional_quality_comprehensive(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            clothing_type = data.get('clothing_type', 'default')
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.fitting_analyzer.analyze_comprehensive(processed_img, original_img, clothing_type)
            
            self.logger.info(f"âš™ï¸ ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'functional_analysis_successful': True,
                'functional_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'functional_analysis_successful': False, 'error': str(e)}
    
    def _analyze_color_quality_professional(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.color_analyzer.analyze_comprehensive(processed_img, original_img)
            
            self.logger.info(f"ğŸŒˆ ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'color_analysis_successful': True,
                'color_results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'color_analysis_successful': False, 'error': str(e)}
    
    def _calculate_comprehensive_final_score(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
        try:
            # ëª¨ë“  ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
            technical_results = data.get('technical_results', {})
            perceptual_results = data.get('perceptual_results', {})
            aesthetic_results = data.get('aesthetic_results', {})
            functional_results = data.get('functional_results', {})
            color_results = data.get('color_results', {})
            
            # QualityMetrics ê°ì²´ ìƒì„±
            metrics = QualityMetrics()
            
            # ê¸°ìˆ ì  í’ˆì§ˆ ë§¤í•‘
            metrics.sharpness = technical_results.get('sharpness', 0.5)
            metrics.noise_level = technical_results.get('noise_level', 0.5)
            metrics.contrast = technical_results.get('contrast', 0.5)
            metrics.brightness = technical_results.get('brightness', 0.5)
            metrics.saturation = technical_results.get('saturation', 0.5)
            
            # ì§€ê°ì  í’ˆì§ˆ ë§¤í•‘
            metrics.structural_similarity = perceptual_results.get('ssim_score', perceptual_results.get('structural_similarity', 0.5))
            metrics.perceptual_similarity = perceptual_results.get('perceptual_similarity', 0.5)
            metrics.visual_quality = perceptual_results.get('visual_quality', 0.5)
            metrics.artifact_level = perceptual_results.get('artifact_level', 0.5)
            
            # ë¯¸ì  í’ˆì§ˆ ë§¤í•‘
            metrics.composition = aesthetic_results.get('composition', 0.5)
            metrics.color_harmony = color_results.get('color_harmony', aesthetic_results.get('color_harmony', 0.5))
            metrics.symmetry = aesthetic_results.get('symmetry', 0.5)
            metrics.balance = aesthetic_results.get('balance', 0.5)
            
            # ê¸°ëŠ¥ì  í’ˆì§ˆ ë§¤í•‘
            metrics.fitting_quality = functional_results.get('fitting_accuracy', functional_results.get('fitting_quality', 0.5))
            metrics.edge_preservation = functional_results.get('edge_preservation', 0.5)
            metrics.texture_quality = functional_results.get('texture_preservation', functional_results.get('texture_quality', 0.5))
            metrics.detail_preservation = functional_results.get('detail_preservation', 0.5)
            
            # ìƒ‰ìƒ í’ˆì§ˆ ë§¤í•‘
            metrics.color_accuracy = color_results.get('color_accuracy', 0.5)
            
            # ì˜ë¥˜/ì›ë‹¨ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            clothing_type = data.get('clothing_type', 'default')
            fabric_type = data.get('fabric_type', 'default')
            
            clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            
            # ê°€ì¤‘ì¹˜ ì¡°í•©
            combined_weights = {
                'technical': 0.25 * fabric_standards['texture_importance'],
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.25 * clothing_weights['fitting']
            }
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            metrics.calculate_overall_score(combined_weights)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            metrics.confidence = self._calculate_enhanced_confidence(data)
            
            # ë“±ê¸‰ ê²°ì •
            grade = metrics.get_grade()
            
            self.logger.info(f"ğŸ¯ ì¢…í•© ìµœì¢… í‰ê°€ ì™„ë£Œ (ì ìˆ˜: {metrics.overall_score:.3f}, ë“±ê¸‰: {grade.value})")
            
            return {
                'final_assessment_successful': True,
                'quality_metrics': metrics,
                'overall_score': metrics.overall_score,
                'confidence': metrics.confidence,
                'grade': grade.value,
                'grade_description': self._get_grade_description(grade)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© ìµœì¢… ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'final_assessment_successful': False, 'error': str(e)}
    
    # =================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (í–¥ìƒëœ ë²„ì „)
    # =================================================================
    
    def _enhance_image_for_assessment(self, image: np.ndarray) -> np.ndarray:
        """í‰ê°€ ì „ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            if not CV2_AVAILABLE:
                return image
            
            # 1. ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.bilateralFilter(image, 9, 75, 75)
            
            # 2. ì„ ëª…í™”
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(denoised, -1, kernel)
            
            # 3. ì›ë³¸ê³¼ ë¸”ë Œë”© (ê³¼ë„í•œ ì²˜ë¦¬ ë°©ì§€)
            enhanced = cv2.addWeighted(image, 0.7, sharpened, 0.3, 0)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_image_entropy(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            hist, _ = np.histogram(gray, bins=256, range=(0, 256))
            hist = hist / hist.sum()
            entropy = -np.sum(hist * np.log2(hist + 1e-8))
            return entropy / 8.0  # ì •ê·œí™”
        except Exception:
            return 0.5
    
    def _detect_compression_artifacts(self, image: np.ndarray) -> float:
        """ì••ì¶• ì•„í‹°íŒ©íŠ¸ ê°ì§€"""
        try:
            if not CV2_AVAILABLE:
                return 0.3
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # DCT ê¸°ë°˜ ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê°ì§€
            artifacts = 0.0
            for i in range(0, gray.shape[0]-8, 8):
                for j in range(0, gray.shape[1]-8, 8):
                    block = gray[i:i+8, j:j+8]
                    if block.shape == (8, 8):
                        # ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ë¶ˆì—°ì†ì„±
                        h_diff = np.mean(np.abs(np.diff(block, axis=1)))
                        v_diff = np.mean(np.abs(np.diff(block, axis=0)))
                        if h_diff > 20 or v_diff > 20:
                            artifacts += 0.01
            
            return min(artifacts, 1.0)
        except Exception:
            return 0.3
    
    def _detect_blur(self, image: np.ndarray) -> float:
        """ë¸”ëŸ¬ ê°ì§€"""
        try:
            if not CV2_AVAILABLE:
                return 0.3
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ë¸”ëŸ¬ ì •ë„ (ë‚®ì„ìˆ˜ë¡ ë¸”ëŸ¬ë¨)
            blur_score = min(laplacian_var / 1000.0, 1.0)
            return 1.0 - blur_score  # ë¸”ëŸ¬ ê°ì§€ëŠ” ë†’ì„ìˆ˜ë¡ ë¸”ëŸ¬ë¨
        except Exception:
            return 0.3
    
    async def _run_enhanced_perceptual_model(self, image: np.ndarray) -> Dict[str, float]:
        """í–¥ìƒëœ ì§€ê°ì  ëª¨ë¸ ì‹¤í–‰"""
        try:
            model = self.ai_models['enhanced_perceptual']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            tensor_img = self._image_to_tensor(image)
            
            with torch.no_grad():
                if self.use_mixed_precision:
                    with autocast('cpu'):
                        results = model(tensor_img)
                else:
                    results = model(tensor_img)
            
            # ê²°ê³¼ ì²˜ë¦¬
            return {
                'perceptual_overall': float(results['overall'].cpu().squeeze()),
                'ai_sharpness': float(results['sharpness'].cpu().squeeze()),
                'ai_artifacts': float(results['artifacts'].cpu().squeeze())
            }
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ ì§€ê°ì  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _run_enhanced_aesthetic_model(self, image: np.ndarray) -> Dict[str, float]:
        """í–¥ìƒëœ ë¯¸ì  ëª¨ë¸ ì‹¤í–‰"""
        try:
            model = self.ai_models['enhanced_aesthetic']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            tensor_img = self._image_to_tensor(image)
            
            with torch.no_grad():
                if self.use_mixed_precision:
                    with autocast('cpu'):
                        results = model(tensor_img)
                else:
                    results = model(tensor_img)
            
            # ê²°ê³¼ ì²˜ë¦¬
            return {
                'ai_composition': float(results['composition'].cpu().squeeze()),
                'ai_color_harmony': float(results['color_harmony'].cpu().squeeze()),
                'ai_symmetry': float(results['symmetry'].cpu().squeeze()),
                'ai_balance': float(results['balance'].cpu().squeeze())
            }
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ ë¯¸ì  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def _image_to_tensor(self, image: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        try:
            # ì •ê·œí™” (0-255 -> 0-1)
            if image.max() > 1.0:
                image = image.astype(np.float32) / 255.0
            
            # HWC -> CHW
            tensor = torch.from_numpy(image).permute(2, 0, 1)
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            tensor = tensor.unsqueeze(0)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            tensor = tensor.to(self.device)
            
            return tensor
            
        except Exception as e:
            self.logger.error(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _calculate_traditional_perceptual_metrics(self, processed_img: np.ndarray, original_img: np.ndarray) -> Dict[str, float]:
        """ì „í†µì  ì§€ê°ì  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # í¬ê¸° ë§ì¶”ê¸°
            if processed_img.shape != original_img.shape:
                original_img = cv2.resize(original_img, (processed_img.shape[1], processed_img.shape[0])) if CV2_AVAILABLE else original_img
            
            # SSIM
            if SKIMAGE_AVAILABLE:
                ssim_score = ssim(original_img, processed_img, multichannel=True, channel_axis=2)
                results['ssim_score'] = max(0, ssim_score)
            
            # PSNR
            mse = np.mean((original_img.astype(float) - processed_img.astype(float)) ** 2)
            if mse > 0:
                psnr = 20 * np.log10(255.0 / np.sqrt(mse))
                results['psnr_score'] = min(1.0, max(0.0, (psnr - 20) / 30))  # 20-50 ë²”ìœ„ ì •ê·œí™”
            else:
                results['psnr_score'] = 1.0
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì „í†µì  ì§€ê°ì  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_no_reference_quality_metrics(self, image: np.ndarray) -> Dict[str, float]:
        """ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # 1. BRISQUE ìŠ¤íƒ€ì¼ ë©”íŠ¸ë¦­ (ê°„ì†Œí™”)
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ í†µê³„
                mu = np.mean(gray)
                sigma = np.std(gray)
                
                # ì •ê·œí™”ëœ ì´ë¯¸ì§€
                normalized = (gray - mu) / (sigma + 1e-8)
                
                # ì™œê³¡ ì¸¡ì •
                alpha = np.mean(normalized**4) - 3  # ì²¨ë„
                beta = np.mean(normalized**3)       # ì™œë„
                
                # ì ìˆ˜ ê³„ì‚° (ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ì¼ìˆ˜ë¡ 0ì— ê°€ê¹Œì›€)
                naturalness = max(0, 1.0 - (abs(alpha) + abs(beta)) / 2.0)
                results['naturalness'] = naturalness
            
            # 2. ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€
            color_naturalness = self._assess_color_naturalness(image)
            results['color_naturalness'] = color_naturalness
            
            return results
            
        except Exception as e:
            self.logger.error(f"ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_traditional_aesthetic_metrics(self, image: np.ndarray) -> Dict[str, float]:
        """ì „í†µì  ë¯¸ì  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # 1. 3ë¶„í•  ë²•ì¹™
            rule_of_thirds = self._evaluate_rule_of_thirds(image)
            results['rule_of_thirds'] = rule_of_thirds
            
            # 2. ìƒ‰ìƒ ë¶„í¬
            color_distribution = self._evaluate_color_distribution(image)
            results['color_distribution'] = color_distribution
            
            # 3. ì‹œê°ì  ë³µì¡ì„±
            visual_complexity = self._calculate_visual_complexity(image)
            results['visual_complexity'] = visual_complexity
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì „í†µì  ë¯¸ì  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_advanced_composition(self, image: np.ndarray) -> Dict[str, float]:
        """ê³ ê¸‰ êµ¬ë„ ë¶„ì„"""
        try:
            results = {}
            
            # 1. í™©ê¸ˆë¹„ ë¶„ì„
            golden_ratio = self._evaluate_golden_ratio(image)
            results['golden_ratio'] = golden_ratio
            
            # 2. ëŒ€ê°ì„  êµ¬ë„
            diagonal_composition = self._evaluate_diagonal_composition(image)
            results['diagonal_composition'] = diagonal_composition
            
            # 3. í”„ë ˆì´ë°
            framing_quality = self._evaluate_framing(image)
            results['framing_quality'] = framing_quality
            
            return results
            
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ êµ¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_enhanced_confidence(self, data: Dict[str, Any]) -> float:
        """í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_factors = []
            
            # 1. ë¶„ì„ ì„±ê³µë¥ 
            success_count = sum([
                data.get('technical_analysis_successful', False),
                data.get('perceptual_analysis_successful', False),
                data.get('aesthetic_analysis_successful', False),
                data.get('functional_analysis_successful', False),
                data.get('color_analysis_successful', False)
            ])
            success_rate = success_count / 5.0
            confidence_factors.append(success_rate)
            
            # 2. AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            ai_usage = len(self.ai_models) / 2.0  # ìµœëŒ€ 2ê°œ ëª¨ë¸
            confidence_factors.append(min(ai_usage, 1.0))
            
            # 3. ë°ì´í„° í’ˆì§ˆ
            has_original = data.get('original_image') is not None
            confidence_factors.append(0.9 if has_original else 0.6)
            
            # 4. ì´ë¯¸ì§€ í’ˆì§ˆ
            validation_warnings = len(data.get('warnings', []))
            image_quality = max(0.3, 1.0 - validation_warnings * 0.2)
            confidence_factors.append(image_quality)
            
            # 5. ë¶„ì„ ì¼ê´€ì„±
            consistency = self._calculate_analysis_consistency(data)
            confidence_factors.append(consistency)
            
            return np.mean(confidence_factors)
            
        except Exception as e:
            self.logger.error(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _calculate_analysis_consistency(self, data: Dict[str, Any]) -> float:
        """ë¶„ì„ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ì„œë¡œ ë‹¤ë¥¸ ë¶„ì„ ë°©ë²•ë“¤ì˜ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            consistency_checks = []
            
            # ê¸°ìˆ ì  vs ì§€ê°ì  ë¶„ì„ ì¼ê´€ì„±
            tech_sharpness = data.get('technical_results', {}).get('sharpness', 0.5)
            perc_quality = data.get('perceptual_results', {}).get('visual_quality', 0.5)
            if abs(tech_sharpness - perc_quality) < 0.2:
                consistency_checks.append(1.0)
            else:
                consistency_checks.append(0.5)
            
            # ìƒ‰ìƒ ë¶„ì„ ì¼ê´€ì„±
            color_harmony_1 = data.get('aesthetic_results', {}).get('color_harmony', 0.5)
            color_harmony_2 = data.get('color_results', {}).get('color_harmony', 0.5)
            if abs(color_harmony_1 - color_harmony_2) < 0.3:
                consistency_checks.append(1.0)
            else:
                consistency_checks.append(0.7)
            
            return np.mean(consistency_checks) if consistency_checks else 0.7
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _get_grade_description(self, grade: QualityGrade) -> str:
        """ë“±ê¸‰ ì„¤ëª… ë°˜í™˜"""
        descriptions = {
            QualityGrade.EXCELLENT: "ë›°ì–´ë‚œ í’ˆì§ˆ - ìƒì—…ì  ì‚¬ìš©ì— ì í•©í•œ ìµœê³  í’ˆì§ˆ",
            QualityGrade.GOOD: "ì¢‹ì€ í’ˆì§ˆ - ì¼ë°˜ì ì¸ ì‚¬ìš©ì— ì¶©ë¶„í•œ í’ˆì§ˆ",
            QualityGrade.ACCEPTABLE: "ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ - ê¸°ë³¸ì ì¸ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±",
            QualityGrade.POOR: "ë‚®ì€ í’ˆì§ˆ - ê°œì„ ì´ í•„ìš”í•œ í’ˆì§ˆ",
            QualityGrade.VERY_POOR: "ë§¤ìš° ë‚®ì€ í’ˆì§ˆ - ëŒ€í­ì ì¸ ê°œì„  í•„ìš”"
        }
        return descriptions.get(grade, "ì•Œ ìˆ˜ ì—†ëŠ” í’ˆì§ˆ")
    
    # ì¶”ê°€ ë¶„ì„ ë©”ì„œë“œë“¤ (ê°„ì†Œí™”ëœ ë²„ì „)
    def _assess_color_naturalness(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
        return 0.7  # ê¸°ë³¸ê°’
    
    def _evaluate_rule_of_thirds(self, image: np.ndarray) -> float:
        """3ë¶„í•  ë²•ì¹™ í‰ê°€"""
        return 0.6  # ê¸°ë³¸ê°’
    
    def _evaluate_color_distribution(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ë¶„í¬ í‰ê°€"""
        return 0.6  # ê¸°ë³¸ê°’
    
    def _calculate_visual_complexity(self, image: np.ndarray) -> float:
        """ì‹œê°ì  ë³µì¡ì„± ê³„ì‚°"""
        return 0.6  # ê¸°ë³¸ê°’
    
    def _evaluate_golden_ratio(self, image: np.ndarray) -> float:
        """í™©ê¸ˆë¹„ í‰ê°€"""
        return 0.6  # ê¸°ë³¸ê°’
    
    def _evaluate_diagonal_composition(self, image: np.ndarray) -> float:
        """ëŒ€ê°ì„  êµ¬ë„ í‰ê°€"""
        return 0.5  # ê¸°ë³¸ê°’
    
    def _evaluate_framing(self, image: np.ndarray) -> float:
        """í”„ë ˆì´ë° í’ˆì§ˆ í‰ê°€"""
        return 0.6  # ê¸°ë³¸ê°’
    
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], image_name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if isinstance(image_input, np.ndarray):
                return image_input
            elif isinstance(image_input, (str, Path)):
                if PIL_AVAILABLE:
                    img = Image.open(image_input)
                    return np.array(img)
                else:
                    self.logger.error("PIL ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                    return None
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image_input)}")
                return None
        except Exception as e:
            self.logger.error(f"{image_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _optimize_m3_max_memory(self):
        """M3 Max ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps'):
                torch.backends.mps.empty_cache()
            gc.collect()
        except Exception:
            pass
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except Exception:
                    pass
            
            self.ai_models.clear()
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'model_interface'):
                self.model_interface.cleanup()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self._optimize_m3_max_memory()
            
            self.logger.info(f"ğŸ§¹ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': self.step_number,
            'device': self.device,
            'ai_models_loaded': len(self.ai_models),
            'assessment_modes': getattr(self, 'assessment_modes', []),
            'quality_threshold': self.quality_threshold,
            'pipeline_stages': len(getattr(self, 'assessment_pipeline', [])),
            'is_initialized': True
        }
    
    # =================================================================
    # ğŸ”§ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (BaseStepMixin ë°ì½”ë ˆì´í„° ì‚¬ìš©)
    # =================================================================
    
    @ensure_step_initialization
    @safe_step_method
    @performance_monitor("quality_assessment_comprehensive")
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜ - í†µí•© ë²„ì „
        âœ… BaseStepMixin ë°ì½”ë ˆì´í„°ë¡œ ì•ˆì „ì„± ë³´ì¥
        âœ… logger ì†ì„± ìë™ í™•ì¸
        âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìë™ ì ìš©
        âœ… ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨
        âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
        """
        
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ¯ {self.step_name} í†µí•© í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ fitted_imageì…ë‹ˆë‹¤")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image is not None else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image is not None else None
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            assessment_data = {
                'processed_image': fitted_img,
                'original_image': person_img,
                'clothing_image': clothing_img,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                **kwargs
            }
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if TORCH_AVAILABLE:
                self._optimize_m3_max_memory()
            
            # 4. ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            for stage_name, stage_func in self.assessment_pipeline:
                self.logger.info(f"ğŸ”„ {stage_name} ì‹¤í–‰ ì¤‘...")
                
                stage_start = time.time()
                
                if asyncio.iscoroutinefunction(stage_func):
                    stage_result = await stage_func(assessment_data)
                else:
                    stage_result = stage_func(assessment_data)
                
                stage_duration = time.time() - stage_start
                
                # ê²°ê³¼ ë³‘í•©
                assessment_data.update(stage_result)
                
                self.logger.info(f"âœ… {stage_name} ì™„ë£Œ ({stage_duration:.2f}ì´ˆ)")
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            quality_metrics = assessment_data.get('quality_metrics')
            
            if quality_metrics is None:
                raise ValueError("í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
            
            result = {
                'success': True,
                'step_name': self.step_name,
                'overall_score': quality_metrics.overall_score,
                'confidence': quality_metrics.confidence,
                'grade': assessment_data.get('grade', 'acceptable'),
                'grade_description': assessment_data.get('grade_description', 'ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ'),
                
                # ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
                'detailed_scores': {
                    'technical': assessment_data.get('technical_results', {}),
                    'perceptual': assessment_data.get('perceptual_results', {}),
                    'aesthetic': assessment_data.get('aesthetic_results', {}),
                    'functional': assessment_data.get('functional_results', {}),
                    'color': assessment_data.get('color_results', {})
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì „ì²´
                'quality_metrics': asdict(quality_metrics),
                
                # ë©”íƒ€ë°ì´í„°
                'processing_time': processing_time,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                
                # ì‹œìŠ¤í…œ ì •ë³´
                'device_info': {
                    'device': self.device,
                    'is_m3_max': getattr(self, 'is_m3_max', False),
                    'ai_models_used': len(self.ai_models),
                    'optimization_enabled': getattr(self, 'optimization_enabled', False)
                },
                
                # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
                'warnings': assessment_data.get('warnings', []),
                'recommendations': assessment_data.get('recommendations', [])
            }
            
            self.logger.info(f"âœ… {self.step_name} í†µí•© í‰ê°€ ì™„ë£Œ - í’ˆì§ˆ ì ìˆ˜: {result['overall_score']:.3f} ({processing_time:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ {self.step_name} í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'step_name': self.step_name,
                'error': str(e),
                'error_type': type(e).__name__,
                'processing_time': processing_time,
                'metadata': {
                    'device': self.device,
                    'pipeline_stages': len(self.assessment_pipeline) if hasattr(self, 'assessment_pipeline') else 0,
                    'error_location': 'comprehensive_quality_assessment'
                }
            }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (í†µí•© ë²„ì „)
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'QualityAssessmentStep',
    
    # ë°ì´í„° êµ¬ì¡°
    'QualityMetrics',
    'QualityGrade',
    'AssessmentMode',
    'QualityAspect',
    
    # AI ëª¨ë¸ë“¤
    'EnhancedPerceptualQualityModel',
    'EnhancedAestheticQualityModel',
    
    # ì „ë¬¸ ë¶„ì„ê¸°ë“¤
    'TechnicalQualityAnalyzer',
    'FittingQualityAnalyzer',
    'ColorQualityAnalyzer',
    
    # ì¸í„°í˜ì´ìŠ¤
    'ModelLoaderInterface'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
logger = logging.getLogger(__name__)
logger.info("âœ… QualityAssessmentStep ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°")
logger.info("ğŸ”„ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡°")
logger.info("ğŸ§  ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í†µí•©")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ“¦ conda í™˜ê²½ ìµœì í™” ì™„ë£Œ")

# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_comprehensive_quality_assessment():
    """í†µí•© í’ˆì§ˆ í‰ê°€ ìŠ¤í… í…ŒìŠ¤íŠ¸"""
    try:
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        step = QualityAssessmentStep()
        
        # logger ì†ì„± í™•ì¸
        assert hasattr(step, 'logger'), "logger ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
        assert step.logger is not None, "loggerê°€ Noneì…ë‹ˆë‹¤!"
        
        # ê¸°ë³¸ ë©”ì„œë“œ í™•ì¸
        assert hasattr(step, 'process'), "process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'cleanup_resources'), "cleanup_resources ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
        
        # ì „ë¬¸ ë¶„ì„ê¸° í™•ì¸
        assert hasattr(step, 'technical_analyzer'), "technical_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'fitting_analyzer'), "fitting_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'color_analyzer'), "color_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        
        # Step ì •ë³´ í™•ì¸
        step_info = step.get_step_info()
        assert 'step_name' in step_info, "step_nameì´ step_infoì— ì—†ìŠµë‹ˆë‹¤!"
        
        print("âœ… ìˆœí™˜ì°¸ì¡° í•´ê²° QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"ğŸ“Š Step ì •ë³´: {step_info}")
        print(f"ğŸ§  AI ëª¨ë¸ ìˆ˜: {len(step.ai_models)}")
        print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {len(step.assessment_pipeline)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ìˆœí™˜ì°¸ì¡° í•´ê²° QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ§ª ìˆœí™˜ì°¸ì¡° í•´ê²° Quality Assessment Step í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    test_comprehensive_quality_assessment()