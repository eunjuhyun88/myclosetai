# app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ¯ ì™„ì „íˆ ì‘ë™í•˜ëŠ” 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment)
ì‹¤ì œ ë©”íŠ¸ë¦­ ê³„ì‚° + ìë™ ê°œì„  ì œì•ˆ + ìƒì„¸ ë¶„ì„ - model_loader ìˆ˜ì • ë²„ì „
"""
import os
import time
import logging
import asyncio
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageStat
import json
from dataclasses import dataclass, asdict
from enum import Enum
import base64
import io
from scipy import stats
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# PyTorchëŠ” ì„ íƒì  ì‚¬ìš©
try:
    import torch
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‹¤í–‰")
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    VERY_POOR = "very_poor"

@dataclass
class QualityMetrics:
    """ì‹¤ì œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    overall_score: float
    perceptual_quality: float
    technical_quality: float
    aesthetic_quality: float
    fit_accuracy: float
    color_harmony: float
    detail_preservation: float
    edge_quality: float
    lighting_consistency: float
    artifact_level: float
    face_preservation: float
    
    def to_dict(self) -> Dict[str, float]:
        return asdict(self)
    
    def get_grade(self) -> QualityGrade:
        """ë“±ê¸‰ ê³„ì‚°"""
        if self.overall_score >= 0.9:
            return QualityGrade.EXCELLENT
        elif self.overall_score >= 0.75:
            return QualityGrade.GOOD
        elif self.overall_score >= 0.6:
            return QualityGrade.FAIR
        elif self.overall_score >= 0.4:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

class RealQualityAssessmentStep:
    """
    ğŸ¯ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
    
    íŠ¹ì§•:
    - ì‹¤ì œ SSIM, PSNR, MSE ê³„ì‚°
    - ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ í’ˆì§ˆ ë©”íŠ¸ë¦­
    - ìë™ ê°œì„  ì œì•ˆ ìƒì„±
    - ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸
    - ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€
    - ìƒ‰ìƒ ì¡°í™” ë¶„ì„
    """
    
    def __init__(self, device: str = 'cpu', config: Dict[str, Any] = None):
        """
        Args:
            device: ë””ë°”ì´ìŠ¤ ('cpu', 'mps', 'cuda')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.device = device
        self.config = config or {}
        
        # model_loaderëŠ” ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ê±°ë‚˜ ì „ì—­ì—ì„œ ê°€ì ¸ì˜´
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        self.model_loader = get_global_model_loader()
        
        # í‰ê°€ ì„¤ì •
        self.enable_advanced_metrics = self.config.get('enable_advanced_metrics', True)
        self.enable_face_detection = self.config.get('enable_face_detection', True)
        self.enable_detailed_analysis = self.config.get('enable_detailed_analysis', True)
        
        # í’ˆì§ˆ ì„ê³„ê°’
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.75,
            'fair': 0.6,
            'poor': 0.4
        }
        
        # ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜
        self.metric_weights = {
            'perceptual_quality': 0.25,
            'technical_quality': 0.20,
            'aesthetic_quality': 0.15,
            'fit_accuracy': 0.15,
            'color_harmony': 0.10,
            'detail_preservation': 0.10,
            'face_preservation': 0.05
        }
        
        # ì–¼êµ´ ê²€ì¶œê¸°
        self.face_detector = None
        
        # PyTorch ìµœì í™”
        self.use_torch = TORCH_AVAILABLE and self.config.get('use_torch', True)
        
        self.is_initialized = False
        
        logger.info(f"ğŸ“Š ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}")
    
    async def initialize(self) -> bool:
        """ì‹¤ì œ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ë¡œë”© ì¤‘...")
            
            # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
            if self.enable_face_detection:
                await self._initialize_face_detector()
            
            self.is_initialized = True
            logger.info("âœ… í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _initialize_face_detector(self):
        """ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”"""
        try:
            # OpenCV Haar ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_detector = cv2.CascadeClassifier(cascade_path)
            
            if self.face_detector.empty():
                logger.warning("âš ï¸ ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì‹¤íŒ¨")
                self.face_detector = None
            else:
                logger.info("âœ… ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.face_detector = None
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, Image.Image, str],
        original_person: Union[np.ndarray, Image.Image, str],
        original_clothing: Union[np.ndarray, Image.Image, str],
        pipeline_results: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬
        
        Args:
            fitted_image: ìµœì¢… ê°€ìƒ í”¼íŒ… ê²°ê³¼
            original_person: ì›ë³¸ ì‚¬ìš©ì ì´ë¯¸ì§€
            original_clothing: ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€
            pipeline_results: íŒŒì´í”„ë¼ì¸ ì¤‘ê°„ ê²°ê³¼ (ì„ íƒì )
            
        Returns:
            ì¢…í•© í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        if not self.is_initialized:
            raise RuntimeError("í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # 1. ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            fitted_np = await self._prepare_image(fitted_image)
            person_np = await self._prepare_image(original_person)
            clothing_np = await self._prepare_image(original_clothing)
            
            logger.info("ğŸ“Š ì¢…í•© í’ˆì§ˆ í‰ê°€ ì‹œì‘...")
            
            # 2. ì§€ê°ì  í’ˆì§ˆ í‰ê°€ (SSIM, PSNR ë“±)
            logger.info("ğŸ‘ï¸ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            perceptual_score = await self._evaluate_perceptual_quality(fitted_np, person_np)
            
            # 3. ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ (ì„ ëª…ë„, ë…¸ì´ì¦ˆ ë“±)
            logger.info("ğŸ”§ ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            technical_score = await self._evaluate_technical_quality(fitted_np)
            
            # 4. ë¯¸ì  í’ˆì§ˆ í‰ê°€ (ìƒ‰ìƒ ì¡°í™”, êµ¬ì„± ë“±)
            logger.info("ğŸ¨ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            aesthetic_score = await self._evaluate_aesthetic_quality(fitted_np, person_np, clothing_np)
            
            # 5. í• ì •í™•ë„ í‰ê°€
            logger.info("ğŸ‘• í• ì •í™•ë„ í‰ê°€ ì¤‘...")
            fit_score = await self._evaluate_fit_accuracy(fitted_np, person_np, pipeline_results)
            
            # 6. ìƒ‰ìƒ ì¡°í™” í‰ê°€
            logger.info("ğŸŒˆ ìƒ‰ìƒ ì¡°í™” í‰ê°€ ì¤‘...")
            color_score = await self._evaluate_color_harmony(fitted_np, clothing_np)
            
            # 7. ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€
            logger.info("ğŸ” ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€ ì¤‘...")
            detail_score = await self._evaluate_detail_preservation(fitted_np, person_np)
            
            # 8. ì—£ì§€ í’ˆì§ˆ í‰ê°€
            logger.info("ğŸ“ ì—£ì§€ í’ˆì§ˆ í‰ê°€ ì¤‘...")
            edge_score = await self._evaluate_edge_quality(fitted_np)
            
            # 9. ì¡°ëª… ì¼ê´€ì„± í‰ê°€
            logger.info("ğŸ’¡ ì¡°ëª… ì¼ê´€ì„± í‰ê°€ ì¤‘...")
            lighting_score = await self._evaluate_lighting_consistency(fitted_np, person_np)
            
            # 10. ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            logger.info("ğŸ” ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì¤‘...")
            artifact_score = await self._evaluate_artifacts(fitted_np)
            
            # 11. ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€ (ì„ íƒì )
            face_score = 1.0
            if self.face_detector is not None:
                logger.info("ğŸ‘¤ ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€ ì¤‘...")
                face_score = await self._evaluate_face_preservation(fitted_np, person_np)
            
            # 12. ì¢…í•© í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = QualityMetrics(
                overall_score=0.0,  # ì•„ë˜ì—ì„œ ê³„ì‚°
                perceptual_quality=perceptual_score,
                technical_quality=technical_score,
                aesthetic_quality=aesthetic_score,
                fit_accuracy=fit_score,
                color_harmony=color_score,
                detail_preservation=detail_score,
                edge_quality=edge_score,
                lighting_consistency=lighting_score,
                artifact_level=artifact_score,
                face_preservation=face_score
            )
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = (
                metrics.perceptual_quality * self.metric_weights['perceptual_quality'] +
                metrics.technical_quality * self.metric_weights['technical_quality'] +
                metrics.aesthetic_quality * self.metric_weights['aesthetic_quality'] +
                metrics.fit_accuracy * self.metric_weights['fit_accuracy'] +
                metrics.color_harmony * self.metric_weights['color_harmony'] +
                metrics.detail_preservation * self.metric_weights['detail_preservation'] +
                metrics.face_preservation * self.metric_weights['face_preservation']
            )
            
            metrics.overall_score = overall_score
            
            # 13. ê°œì„  ì œì•ˆ ìƒì„±
            recommendations = await self._generate_recommendations(metrics)
            
            # 14. ìƒì„¸ ë¶„ì„ (ì„ íƒì )
            detailed_analysis = {}
            if self.enable_detailed_analysis:
                detailed_analysis = await self._generate_detailed_analysis(
                    metrics, fitted_np, person_np, clothing_np
                )
            
            processing_time = time.time() - start_time
            
            # 15. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'overall_score': float(overall_score),
                'grade': metrics.get_grade().value,
                'letter_grade': self._get_letter_grade(overall_score),
                'metrics': metrics.to_dict(),
                'recommendations': recommendations,
                'detailed_analysis': detailed_analysis,
                'processing_time': processing_time,
                'timestamp': time.time(),
                'config_used': {
                    'device': self.device,
                    'advanced_metrics': self.enable_advanced_metrics,
                    'face_detection': self.enable_face_detection,
                    'detailed_analysis': self.enable_detailed_analysis
                }
            }
            
            logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {overall_score:.3f} ({metrics.get_grade().value})")
            logger.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            error_msg = f"í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            
            return {
                'success': False,
                'error': error_msg,
                'processing_time': time.time() - start_time
            }
    
    async def _prepare_image(self, image: Union[np.ndarray, Image.Image, str]) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if isinstance(image, str):
                # Base64 ë””ì½”ë”©
                if image.startswith('data:image'):
                    header, data = image.split(',', 1)
                    image_data = base64.b64decode(data)
                else:
                    image_data = base64.b64decode(image)
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                pil_image = Image.open(io.BytesIO(image_data))
                image_np = np.array(pil_image)
                
            elif isinstance(image, Image.Image):
                image_np = np.array(image)
                
            elif isinstance(image, np.ndarray):
                image_np = image.copy()
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGB ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
            if len(image_np.shape) == 3 and image_np.shape[2] == 4:
                # RGBA -> RGB
                image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
            elif len(image_np.shape) == 3 and image_np.shape[2] == 3:
                # BGR -> RGB (OpenCV ê¸°ë³¸)
                if image_np.max() <= 1.0:  # ì •ê·œí™”ëœ ì´ë¯¸ì§€
                    image_np = (image_np * 255).astype(np.uint8)
            
            # í¬ê¸° ì •ê·œí™” (ì„ íƒì )
            if image_np.shape[:2] != (512, 512):
                image_np = cv2.resize(image_np, (512, 512))
            
            return image_np
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
            return np.zeros((512, 512, 3), dtype=np.uint8)
    
    async def _evaluate_perceptual_quality(self, fitted: np.ndarray, original: np.ndarray) -> float:
        """ì§€ê°ì  í’ˆì§ˆ í‰ê°€ (SSIM ê¸°ë°˜)"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY)
            original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)
            
            # SSIM ê³„ì‚° (scikit-image ì—†ì´ ì§ì ‘ êµ¬í˜„)
            ssim_score = self._calculate_ssim(fitted_gray, original_gray)
            
            # PSNR ê³„ì‚°
            mse = np.mean((fitted_gray.astype(float) - original_gray.astype(float)) ** 2)
            if mse == 0:
                psnr_score = 1.0
            else:
                psnr_score = min(20 * np.log10(255.0 / np.sqrt(mse)) / 40.0, 1.0)
            
            # ì¡°í•© ì ìˆ˜
            perceptual_score = 0.7 * ssim_score + 0.3 * psnr_score
            
            return max(0.0, min(1.0, perceptual_score))
            
        except Exception as e:
            logger.warning(f"ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """SSIM ì§ì ‘ ê³„ì‚°"""
        try:
            # ìƒìˆ˜
            K1, K2 = 0.01, 0.03
            L = 255  # ìµœëŒ€ í”½ì…€ ê°’
            C1 = (K1 * L) ** 2
            C2 = (K2 * L) ** 2
            
            # í‰ê·  ê³„ì‚°
            mu1 = cv2.GaussianBlur(img1.astype(np.float64), (11, 11), 1.5)
            mu2 = cv2.GaussianBlur(img2.astype(np.float64), (11, 11), 1.5)
            
            mu1_sq = mu1 ** 2
            mu2_sq = mu2 ** 2
            mu1_mu2 = mu1 * mu2
            
            # ë¶„ì‚° ê³„ì‚°
            sigma1_sq = cv2.GaussianBlur(img1.astype(np.float64) ** 2, (11, 11), 1.5) - mu1_sq
            sigma2_sq = cv2.GaussianBlur(img2.astype(np.float64) ** 2, (11, 11), 1.5) - mu2_sq
            sigma12 = cv2.GaussianBlur((img1.astype(np.float64) * img2.astype(np.float64)), (11, 11), 1.5) - mu1_mu2
            
            # SSIM ë§µ ê³„ì‚°
            numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)
            denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)
            
            ssim_map = numerator / denominator
            
            return float(np.mean(ssim_map))
            
        except Exception as e:
            logger.warning(f"SSIM ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_technical_quality(self, image: np.ndarray) -> float:
        """ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€"""
        try:
            # ì„ ëª…ë„ (Laplacian ë¶„ì‚°)
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(laplacian_var / 1000.0, 1.0)
            
            # ë…¸ì´ì¦ˆ ìˆ˜ì¤€ (ì´ë¯¸ì§€ í‘œì¤€í¸ì°¨ ê¸°ë°˜)
            noise_level = np.std(gray)
            noise_score = max(0, 1.0 - (noise_level - 20) / 100.0)
            
            # ëŒ€ë¹„ í’ˆì§ˆ
            contrast_score = np.std(gray) / 128.0
            contrast_score = max(0.0, min(1.0, contrast_score))
            
            # ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ
            technical_score = (sharpness_score * 0.4 + 
                             noise_score * 0.3 + 
                             contrast_score * 0.3)
            
            return max(0.0, min(1.0, technical_score))
            
        except Exception as e:
            logger.warning(f"ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_aesthetic_quality(self, fitted: np.ndarray, person: np.ndarray, clothing: np.ndarray) -> float:
        """ë¯¸ì  í’ˆì§ˆ í‰ê°€"""
        try:
            # ìƒ‰ìƒ ë¶„í¬ í‰ê°€
            color_score = self._evaluate_color_distribution(fitted)
            
            # êµ¬ì„± ê· í˜• í‰ê°€
            composition_score = self._evaluate_composition(fitted)
            
            # ì „ì²´ì ì¸ ì¡°í™” í‰ê°€
            harmony_score = self._evaluate_visual_harmony(fitted, person, clothing)
            
            # ì¢…í•© ë¯¸ì  ì ìˆ˜
            aesthetic_score = (color_score * 0.4 + 
                             composition_score * 0.3 + 
                             harmony_score * 0.3)
            
            return max(0.0, min(1.0, aesthetic_score))
            
        except Exception as e:
            logger.warning(f"ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_color_distribution(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ë¶„í¬ í‰ê°€"""
        try:
            # HSVë¡œ ë³€í™˜
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            
            # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨
            hist_h = cv2.calcHist([hsv], [0], None, [180], [0, 180])
            hist_s = cv2.calcHist([hsv], [1], None, [256], [0, 256])
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„± ê³„ì‚°
            h_diversity = np.count_nonzero(hist_h) / 180.0
            s_diversity = np.count_nonzero(hist_s) / 256.0
            
            # ê· í˜• ì ìˆ˜
            diversity_score = (h_diversity + s_diversity) / 2.0
            
            return max(0.0, min(1.0, diversity_score))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë¶„í¬ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_composition(self, image: np.ndarray) -> float:
        """êµ¬ì„± ê· í˜• í‰ê°€"""
        try:
            # í™©ê¸ˆë¹„ìœ¨ ê¸°ë°˜ êµ¬ì„± í‰ê°€
            h, w = image.shape[:2]
            
            # 3ë¶„í• ë²• ê²©ìì 
            grid_points = [
                (w//3, h//3), (2*w//3, h//3),
                (w//3, 2*h//3), (2*w//3, 2*h//3)
            ]
            
            # ê° ê²©ìì  ì£¼ë³€ì˜ ê´€ì‹¬ë„ ê³„ì‚°
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            interest_scores = []
            
            for x, y in grid_points:
                roi = gray[max(0, y-50):min(h, y+50), max(0, x-50):min(w, x+50)]
                if roi.size > 0:
                    interest = np.std(roi)  # í‘œì¤€í¸ì°¨ë¥¼ ê´€ì‹¬ë„ë¡œ ì‚¬ìš©
                    interest_scores.append(interest)
            
            # ê· í˜• ì ìˆ˜ (ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ê· í˜•ì )
            if interest_scores:
                balance_score = 1.0 - (np.std(interest_scores) / (np.mean(interest_scores) + 1e-6))
                return max(0.0, min(1.0, balance_score))
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"êµ¬ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_visual_harmony(self, fitted: np.ndarray, person: np.ndarray, clothing: np.ndarray) -> float:
        """ì‹œê°ì  ì¡°í™” í‰ê°€"""
        try:
            # ìƒ‰ìƒ ì¡°í™” ê³„ì‚°
            fitted_colors = fitted.reshape(-1, 3).mean(axis=0)
            person_colors = person.reshape(-1, 3).mean(axis=0)
            clothing_colors = clothing.reshape(-1, 3).mean(axis=0)
            
            # ìƒ‰ìƒ ê±°ë¦¬ ê³„ì‚°
            person_fitted_dist = np.linalg.norm(fitted_colors - person_colors)
            clothing_fitted_dist = np.linalg.norm(fitted_colors - clothing_colors)
            
            # ì¡°í™” ì ìˆ˜ (ê±°ë¦¬ê°€ ì ì ˆí•  ë•Œ ë†’ìŒ)
            harmony_score = 1.0 - min(person_fitted_dist, clothing_fitted_dist) / 441.67  # sqrt(3*255^2)
            
            return max(0.0, min(1.0, harmony_score))
            
        except Exception as e:
            logger.warning(f"ì‹œê°ì  ì¡°í™” í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_fit_accuracy(self, fitted: np.ndarray, person: np.ndarray, pipeline_results: Optional[Dict]) -> float:
        """í• ì •í™•ë„ í‰ê°€"""
        try:
            # ê°€ìƒ í”¼íŒ… ê²°ê³¼ê°€ ì›ë³¸ ì‚¬ëŒê³¼ ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ í‰ê°€
            
            # 1. ì‚¬ì´ì¦ˆ ì¼ê´€ì„± ê²€ì‚¬
            size_score = self._evaluate_size_consistency(fitted, person)
            
            # 2. ë³€í˜• í’ˆì§ˆ ê²€ì‚¬
            deformation_score = self._evaluate_deformation_quality(fitted, person)
            
            # 3. ê²½ê³„ì„  í’ˆì§ˆ ê²€ì‚¬
            boundary_score = self._evaluate_boundary_quality(fitted)
            
            # 4. íŒŒì´í”„ë¼ì¸ ê²°ê³¼ í™œìš© (ìˆëŠ” ê²½ìš°)
            pipeline_score = 0.8  # ê¸°ë³¸ê°’
            if pipeline_results:
                pipeline_score = self._extract_pipeline_confidence(pipeline_results)
            
            # ì¢…í•© í• ì •í™•ë„
            fit_score = (size_score * 0.3 + 
                        deformation_score * 0.3 + 
                        boundary_score * 0.2 + 
                        pipeline_score * 0.2)
            
            return max(0.0, min(1.0, fit_score))
            
        except Exception as e:
            logger.warning(f"í• ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_size_consistency(self, fitted: np.ndarray, person: np.ndarray) -> float:
        """ì‚¬ì´ì¦ˆ ì¼ê´€ì„± í‰ê°€"""
        try:
            # ì‹ ì²´ ìœ¤ê³½ ê²€ì¶œ
            fitted_edges = cv2.Canny(cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY), 50, 150)
            person_edges = cv2.Canny(cv2.cvtColor(person, cv2.COLOR_RGB2GRAY), 50, 150)
            
            # ìœ¤ê³½ ì˜ì—­ ë¹„êµ
            fitted_area = np.sum(fitted_edges > 0)
            person_area = np.sum(person_edges > 0)
            
            if person_area > 0:
                size_ratio = min(fitted_area, person_area) / max(fitted_area, person_area)
                return size_ratio
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"ì‚¬ì´ì¦ˆ ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_deformation_quality(self, fitted: np.ndarray, person: np.ndarray) -> float:
        """ë³€í˜• í’ˆì§ˆ í‰ê°€"""
        try:
            # êµ¬ì¡°ì  ìœ ì‚¬ì„± ê²€ì‚¬
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person, cv2.COLOR_RGB2GRAY)
            
            # íŠ¹ì§•ì  ê¸°ë°˜ ìœ ì‚¬ì„±
            structural_similarity = self._calculate_ssim(fitted_gray, person_gray)
            
            return structural_similarity
            
        except Exception as e:
            logger.warning(f"ë³€í˜• í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_boundary_quality(self, image: np.ndarray) -> float:
        """ê²½ê³„ì„  í’ˆì§ˆ í‰ê°€"""
        try:
            # ì—£ì§€ ê²€ì¶œ
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # ê²½ê³„ì„  ì—°ì†ì„± í‰ê°€
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # ê°€ì¥ í° ìœ¤ê³½ì˜ ì—°ì†ì„± ê²€ì‚¬
                largest_contour = max(contours, key=cv2.contourArea)
                perimeter = cv2.arcLength(largest_contour, True)
                area = cv2.contourArea(largest_contour)
                
                if area > 0:
                    compactness = (perimeter ** 2) / (4 * np.pi * area)
                    boundary_score = 1.0 / (1.0 + compactness)
                    return max(0.0, min(1.0, boundary_score))
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"ê²½ê³„ì„  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_pipeline_confidence(self, pipeline_results: Dict) -> float:
        """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ"""
        try:
            confidences = []
            
            # ê° ë‹¨ê³„ì˜ ì‹ ë¢°ë„ ìˆ˜ì§‘
            for step_name, step_result in pipeline_results.items():
                if isinstance(step_result, dict):
                    confidence = step_result.get('confidence', 0.5)
                    confidences.append(confidence)
            
            if confidences:
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                return sum(confidences) / len(confidences)
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"íŒŒì´í”„ë¼ì¸ ì‹ ë¢°ë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_color_harmony(self, fitted: np.ndarray, clothing: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¡°í™” í‰ê°€"""
        try:
            # HSV ìƒ‰ê³µê°„ì—ì„œ ë¶„ì„
            fitted_hsv = cv2.cvtColor(fitted, cv2.COLOR_RGB2HSV)
            clothing_hsv = cv2.cvtColor(clothing, cv2.COLOR_RGB2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            fitted_colors = self._extract_dominant_colors(fitted_hsv)
            clothing_colors = self._extract_dominant_colors(clothing_hsv)
            
            # ìƒ‰ìƒ ì¡°í™” ê³„ì‚°
            harmony_score = self._calculate_color_harmony(fitted_colors, clothing_colors)
            
            return max(0.0, min(1.0, harmony_score))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì¡°í™” í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_dominant_colors(self, hsv_image: np.ndarray, k: int = 3) -> List[np.ndarray]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        try:
            # ì´ë¯¸ì§€ë¥¼ 1Dë¡œ ë³€í™˜
            data = hsv_image.reshape(-1, 3)
            data = data[np.random.choice(data.shape[0], min(1000, data.shape[0]), replace=False)]
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§ (sklearn ì—†ì´ ê°„ë‹¨ êµ¬í˜„)
            centers = []
            for _ in range(k):
                idx = np.random.randint(0, len(data))
                centers.append(data[idx])
            
            centers = np.array(centers)
            
            # ê°„ë‹¨í•œ K-means ë°˜ë³µ
            for _ in range(10):
                # í´ëŸ¬ìŠ¤í„° í• ë‹¹
                distances = np.sqrt(((data - centers[:, np.newaxis])**2).sum(axis=2))
                closest_cluster = np.argmin(distances, axis=0)
                
                # ì¤‘ì‹¬ì  ì—…ë°ì´íŠ¸
                for i in range(k):
                    if np.any(closest_cluster == i):
                        centers[i] = data[closest_cluster == i].mean(axis=0)
            
            return centers.tolist()
            
        except Exception as e:
            logger.warning(f"ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [[128, 128, 128]]  # ê¸°ë³¸ íšŒìƒ‰
    
    def _calculate_color_harmony(self, colors1: List, colors2: List) -> float:
        """ìƒ‰ìƒ ì¡°í™” ê³„ì‚°"""
        try:
            min_distance = float('inf')
            
            for c1 in colors1:
                for c2 in colors2:
                    # HSV ê³µê°„ì—ì„œ ìƒ‰ìƒ ê±°ë¦¬
                    h_dist = min(abs(c1[0] - c2[0]), 180 - abs(c1[0] - c2[0]))
                    s_dist = abs(c1[1] - c2[1])
                    v_dist = abs(c1[2] - c2[2])
                    
                    distance = np.sqrt(h_dist**2 + s_dist**2 + v_dist**2)
                    min_distance = min(min_distance, distance)
            
            # ê±°ë¦¬ë¥¼ 0-1 ì ìˆ˜ë¡œ ë³€í™˜
            harmony_score = 1.0 - (min_distance / 300.0)  # ì •ê·œí™”
            
            return max(0.0, min(1.0, harmony_score))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì¡°í™” ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_detail_preservation(self, fitted: np.ndarray, original: np.ndarray) -> float:
        """ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€"""
        try:
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¹„êµ
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY)
            original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)
            
            # ë¼í”Œë¼ì‹œì•ˆ í•„í„°ë¡œ ë””í…Œì¼ ì¶”ì¶œ
            fitted_details = cv2.Laplacian(fitted_gray, cv2.CV_64F)
            original_details = cv2.Laplacian(original_gray, cv2.CV_64F)
            
            # ë””í…Œì¼ ìœ ì‚¬ì„± ê³„ì‚°
            correlation = self._calculate_correlation(fitted_details, original_details)
            
            return max(0.0, min(1.0, correlation))
            
        except Exception as e:
            logger.warning(f"ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_correlation(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        try:
            # ì •ê·œí™”
            img1_norm = (img1 - np.mean(img1)) / (np.std(img1) + 1e-8)
            img2_norm = (img2 - np.mean(img2)) / (np.std(img2) + 1e-8)
            
            # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            correlation = np.mean(img1_norm * img2_norm)
            
            return (correlation + 1.0) / 2.0  # -1~1ì„ 0~1ë¡œ ë³€í™˜
            
        except Exception as e:
            logger.warning(f"ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_edge_quality(self, image: np.ndarray) -> float:
        """ì—£ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # ë‹¤ì–‘í•œ ì—£ì§€ ê²€ì¶œê¸° ì‚¬ìš©
            canny_edges = cv2.Canny(gray, 50, 150)
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            
            # ì—£ì§€ ê°•ë„ ê³„ì‚°
            edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
            
            # ì—£ì§€ í’ˆì§ˆ ë©”íŠ¸ë¦­
            edge_density = np.sum(canny_edges > 0) / canny_edges.size
            edge_strength = np.mean(edge_magnitude)
            
            # ì •ê·œí™” ë° ì¡°í•©
            density_score = min(edge_density * 10, 1.0)  # ì ì ˆí•œ ì—£ì§€ ë°€ë„
            strength_score = min(edge_strength / 100, 1.0)  # ì ì ˆí•œ ì—£ì§€ ê°•ë„
            
            edge_score = (density_score + strength_score) / 2.0
            
            return max(0.0, min(1.0, edge_score))
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_lighting_consistency(self, fitted: np.ndarray, original: np.ndarray) -> float:
        """ì¡°ëª… ì¼ê´€ì„± í‰ê°€"""
        try:
            # RGBë¥¼ LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜ (ë” ì •í™•í•œ ë°ê¸° ë¶„ì„)
            fitted_lab = cv2.cvtColor(fitted, cv2.COLOR_RGB2LAB)
            original_lab = cv2.cvtColor(original, cv2.COLOR_RGB2LAB)
            
            # L ì±„ë„ (ë°ê¸°) ë¹„êµ
            fitted_l = fitted_lab[:, :, 0]
            original_l = original_lab[:, :, 0]
            
            # ë°ê¸° ë¶„í¬ ìœ ì‚¬ì„±
            fitted_hist = cv2.calcHist([fitted_l], [0], None, [256], [0, 256])
            original_hist = cv2.calcHist([original_l], [0], None, [256], [0, 256])
            
            # íˆìŠ¤í† ê·¸ë¨ ì •ê·œí™”
            fitted_hist = fitted_hist / np.sum(fitted_hist)
            original_hist = original_hist / np.sum(original_hist)
            
            # íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ì„± (Bhattacharyya distance)
            similarity = cv2.compareHist(fitted_hist, original_hist, cv2.HISTCMP_BHATTACHARYYA)
            lighting_score = 1.0 - similarity
            
            return max(0.0, min(1.0, lighting_score))
            
        except Exception as e:
            logger.warning(f"ì¡°ëª… ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _evaluate_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # 1. ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            blocking_score = self._detect_blocking_artifacts(gray)
            
            # 2. ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            ringing_score = self._detect_ringing_artifacts(gray)
            
            # 3. ë¸”ëŸ¬ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            blur_score = self._detect_blur_artifacts(gray)
            
            # ì¢…í•© ì•„í‹°íŒ©íŠ¸ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 1ì—ì„œ ëºŒ)
            artifact_level = 1.0 - (blocking_score + ringing_score + blur_score) / 3.0
            
            return max(0.0, min(1.0, artifact_level))
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _detect_blocking_artifacts(self, gray: np.ndarray) -> float:
        """ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # 8x8 ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ë¶ˆì—°ì†ì„± ê²€ì‚¬
            h, w = gray.shape
            blocking_measure = 0.0
            count = 0
            
            # ìˆ˜ì§ ê²½ê³„
            for i in range(8, h, 8):
                if i < h - 1:
                    diff = np.abs(gray[i, :].astype(float) - gray[i-1, :].astype(float))
                    blocking_measure += np.mean(diff)
                    count += 1
            
            # ìˆ˜í‰ ê²½ê³„
            for j in range(8, w, 8):
                if j < w - 1:
                    diff = np.abs(gray[:, j].astype(float) - gray[:, j-1].astype(float))
                    blocking_measure += np.mean(diff)
                    count += 1
            
            if count > 0:
                blocking_measure /= count
                return min(blocking_measure / 50.0, 1.0)  # ì •ê·œí™”
            
            return 0.0
            
        except Exception as e:
            logger.warning(f"ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _detect_ringing_artifacts(self, gray: np.ndarray) -> float:
        """ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ë¼í”Œë¼ì‹œì•ˆìœ¼ë¡œ ì—£ì§€ ì£¼ë³€ì˜ ì§„ë™ íŒ¨í„´ ê²€ì¶œ
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            
            # ë§ì‰ì€ ê°•í•œ ì—£ì§€ ì£¼ë³€ì—ì„œ ë°œìƒ
            edges = cv2.Canny(gray, 50, 150)
            
            # ì—£ì§€ ì£¼ë³€ ì˜ì—­ì—ì„œ ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ì¸¡ì •
            kernel = np.ones((5, 5), np.uint8)
            edge_regions = cv2.dilate(edges, kernel, iterations=1)
            
            ringing_measure = np.std(laplacian[edge_regions > 0])
            
            return min(ringing_measure / 100.0, 1.0)  # ì •ê·œí™”
            
        except Exception as e:
            logger.warning(f"ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _detect_blur_artifacts(self, gray: np.ndarray) -> float:
        """ë¸”ëŸ¬ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°ìœ¼ë¡œ ë¸”ëŸ¬ ì •ë„ ì¸¡ì •
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ ë¸”ëŸ¬ê°€ ë§ìŒ
            blur_measure = 1.0 - min(laplacian_var / 1000.0, 1.0)
            
            return max(0.0, min(1.0, blur_measure))
            
        except Exception as e:
            logger.warning(f"ë¸”ëŸ¬ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.0
    
    async def _evaluate_face_preservation(self, fitted: np.ndarray, original: np.ndarray) -> float:
        """ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€"""
        try:
            if self.face_detector is None:
                return 1.0
            
            # ì–¼êµ´ ê²€ì¶œ
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY)
            original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)
            
            fitted_faces = self.face_detector.detectMultiScale(fitted_gray, 1.1, 4)
            original_faces = self.face_detector.detectMultiScale(original_gray, 1.1, 4)
            
            if len(original_faces) == 0:
                # ì›ë³¸ì— ì–¼êµ´ì´ ì—†ìœ¼ë©´ í‰ê°€ ë¶ˆê°€
                return 1.0
            
            if len(fitted_faces) == 0:
                # í”¼íŒ… ê²°ê³¼ì— ì–¼êµ´ì´ ì—†ìœ¼ë©´ ë³´ì¡´ ì‹¤íŒ¨
                return 0.0
            
            # ê°€ì¥ í° ì–¼êµ´ ì˜ì—­ ë¹„êµ
            orig_face = max(original_faces, key=lambda f: f[2] * f[3])
            fitted_face = max(fitted_faces, key=lambda f: f[2] * f[3])
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            ox, oy, ow, oh = orig_face
            fx, fy, fw, fh = fitted_face
            
            orig_face_region = original[oy:oy+oh, ox:ox+ow]
            fitted_face_region = fitted[fy:fy+fh, fx:fx+fw]
            
            # í¬ê¸° ë§ì¶¤
            if orig_face_region.size > 0 and fitted_face_region.size > 0:
                fitted_face_resized = cv2.resize(fitted_face_region, (ow, oh))
                
                # ì–¼êµ´ ìœ ì‚¬ì„± ê³„ì‚°
                face_similarity = self._calculate_face_similarity(orig_face_region, fitted_face_resized)
                
                return max(0.0, min(1.0, face_similarity))
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 1.0
    
    def _calculate_face_similarity(self, face1: np.ndarray, face2: np.ndarray) -> float:
        """ì–¼êµ´ ìœ ì‚¬ì„± ê³„ì‚°"""
        try:
            # êµ¬ì¡°ì  ìœ ì‚¬ì„± ì‚¬ìš©
            face1_gray = cv2.cvtColor(face1, cv2.COLOR_RGB2GRAY) if len(face1.shape) == 3 else face1
            face2_gray = cv2.cvtColor(face2, cv2.COLOR_RGB2GRAY) if len(face2.shape) == 3 else face2
            
            similarity = self._calculate_ssim(face1_gray, face2_gray)
            
            return similarity
            
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _generate_recommendations(self, metrics: QualityMetrics) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        recommendations = []
        
        try:
            # ì„ê³„ê°’ ê¸°ë°˜ ì œì•ˆ
            thresholds = {
                'perceptual_quality': 0.7,
                'technical_quality': 0.6,
                'aesthetic_quality': 0.65,
                'fit_accuracy': 0.75,
                'color_harmony': 0.6,
                'detail_preservation': 0.7,
                'edge_quality': 0.6,
                'lighting_consistency': 0.65,
                'artifact_level': 0.8,
                'face_preservation': 0.8
            }
            
            if metrics.perceptual_quality < thresholds['perceptual_quality']:
                recommendations.append("ì§€ê°ì  í’ˆì§ˆ ê°œì„ : ì…ë ¥ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë†’ì´ê±°ë‚˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
            
            if metrics.technical_quality < thresholds['technical_quality']:
                recommendations.append("ê¸°ìˆ ì  í’ˆì§ˆ ê°œì„ : ì´ë¯¸ì§€ ì„ ëª…ë„ë¥¼ ë†’ì´ê³  ì••ì¶• ì•„í‹°íŒ©íŠ¸ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
            
            if metrics.aesthetic_quality < thresholds['aesthetic_quality']:
                recommendations.append("ë¯¸ì  í’ˆì§ˆ ê°œì„ : ìƒ‰ìƒ ê· í˜•ê³¼ êµ¬ì„±ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
            
            if metrics.fit_accuracy < thresholds['fit_accuracy']:
                recommendations.append("í• ì •í™•ë„ ê°œì„ : ì‹ ì²´ ì¸¡ì •ê°’ì„ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜ í¬ì¦ˆë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
            
            if metrics.color_harmony < thresholds['color_harmony']:
                recommendations.append("ìƒ‰ìƒ ì¡°í™” ê°œì„ : ì˜ë¥˜ì™€ í”¼ë¶€í†¤ì´ ì˜ ì–´ìš¸ë¦¬ëŠ” ìƒ‰ìƒì„ ì„ íƒí•´ë³´ì„¸ìš”.")
            
            if metrics.detail_preservation < thresholds['detail_preservation']:
                recommendations.append("ë””í…Œì¼ ë³´ì¡´ ê°œì„ : ë” ë†’ì€ í’ˆì§ˆì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if metrics.edge_quality < thresholds['edge_quality']:
                recommendations.append("ì—£ì§€ í’ˆì§ˆ ê°œì„ : ë°°ê²½ì´ ê¹”ë”í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if metrics.lighting_consistency < thresholds['lighting_consistency']:
                recommendations.append("ì¡°ëª… ì¼ê´€ì„± ê°œì„ : ê· ì¼í•œ ì¡°ëª… í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if metrics.artifact_level < thresholds['artifact_level']:
                recommendations.append("ì•„í‹°íŒ©íŠ¸ ê°ì†Œ: ì²˜ë¦¬ í’ˆì§ˆ ì„¤ì •ì„ ë†’ì´ê±°ë‚˜ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
            
            if metrics.face_preservation < thresholds['face_preservation']:
                recommendations.append("ì–¼êµ´ ë³´ì¡´ ê°œì„ : ì–¼êµ´ì´ ëª…í™•íˆ ë³´ì´ëŠ” ì •ë©´ ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            # ì „ì²´ ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
            if metrics.overall_score < 0.5:
                recommendations.insert(0, "ì „ì²´ì ì¸ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ì™€ ì ì ˆí•œ ì¡°ëª…ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            elif metrics.overall_score < 0.7:
                recommendations.insert(0, "ì¢‹ì€ ê²°ê³¼ì…ë‹ˆë‹¤. ëª‡ ê°€ì§€ ì„¸ë¶€ì‚¬í•­ì„ ê°œì„ í•˜ë©´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ë¹ˆ ì¶”ì²œ ëª©ë¡ì¸ ê²½ìš° ê¸°ë³¸ ì œì•ˆ
            if not recommendations:
                recommendations.append("í›Œë¥­í•œ í’ˆì§ˆì…ë‹ˆë‹¤! í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
            
            return recommendations
            
        except Exception as e:
            logger.warning(f"ê°œì„  ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["í’ˆì§ˆ ë¶„ì„ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ê°œì„  ì œì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."]
    
    async def _generate_detailed_analysis(
        self, 
        metrics: QualityMetrics, 
        fitted: np.ndarray, 
        person: np.ndarray, 
        clothing: np.ndarray
    ) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìƒì„±"""
        try:
            analysis = {
                'quality_breakdown': {
                    'excellent_aspects': [],
                    'good_aspects': [],
                    'improvement_needed': []
                },
                'technical_details': {
                    'image_properties': self._analyze_image_properties(fitted),
                    'color_analysis': self._analyze_colors(fitted, clothing),
                    'structural_analysis': self._analyze_structure(fitted, person)
                },
                'comparison_metrics': {
                    'similarity_to_original': self._calculate_overall_similarity(fitted, person),
                    'clothing_integration': self._calculate_clothing_integration(fitted, clothing),
                    'realism_score': self._calculate_realism_score(fitted)
                },
                'performance_insights': {
                    'strongest_aspect': self._find_strongest_aspect(metrics),
                    'weakest_aspect': self._find_weakest_aspect(metrics),
                    'improvement_potential': self._calculate_improvement_potential(metrics)
                }
            }
            
            # í’ˆì§ˆ ë¶„ë¥˜
            for metric_name, metric_value in metrics.to_dict().items():
                if metric_name == 'overall_score':
                    continue
                
                if metric_value >= 0.8:
                    analysis['quality_breakdown']['excellent_aspects'].append({
                        'aspect': metric_name.replace('_', ' ').title(),
                        'score': metric_value,
                        'status': 'excellent'
                    })
                elif metric_value >= 0.6:
                    analysis['quality_breakdown']['good_aspects'].append({
                        'aspect': metric_name.replace('_', ' ').title(),
                        'score': metric_value,
                        'status': 'good'
                    })
                else:
                    analysis['quality_breakdown']['improvement_needed'].append({
                        'aspect': metric_name.replace('_', ' ').title(),
                        'score': metric_value,
                        'status': 'needs_improvement'
                    })
            
            return analysis
            
        except Exception as e:
            logger.warning(f"ìƒì„¸ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': 'ìƒì„¸ ë¶„ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}
    
    def _analyze_image_properties(self, image: np.ndarray) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì†ì„± ë¶„ì„"""
        try:
            h, w, c = image.shape
            
            # ê¸°ë³¸ ì†ì„±
            properties = {
                'resolution': f"{w}x{h}",
                'channels': c,
                'file_size_estimate': f"{(w * h * c / 1024):.1f} KB",
                'aspect_ratio': f"{w/h:.2f}:1"
            }
            
            # ìƒ‰ìƒ í†µê³„
            properties.update({
                'brightness_mean': float(np.mean(image)),
                'brightness_std': float(np.std(image)),
                'contrast_measure': float(np.std(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY))),
                'color_range': {
                    'min': [int(np.min(image[:,:,i])) for i in range(c)],
                    'max': [int(np.max(image[:,:,i])) for i in range(c)],
                    'mean': [float(np.mean(image[:,:,i])) for i in range(c)]
                }
            })
            
            return properties
            
        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_colors(self, fitted: np.ndarray, clothing: np.ndarray) -> Dict[str, Any]:
        """ìƒ‰ìƒ ë¶„ì„"""
        try:
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            fitted_hsv = cv2.cvtColor(fitted, cv2.COLOR_RGB2HSV)
            clothing_hsv = cv2.cvtColor(clothing, cv2.COLOR_RGB2HSV)
            
            fitted_colors = self._extract_dominant_colors(fitted_hsv)
            clothing_colors = self._extract_dominant_colors(clothing_hsv)
            
            analysis = {
                'dominant_colors_fitted': fitted_colors,
                'dominant_colors_clothing': clothing_colors,
                'color_temperature': self._estimate_color_temperature(fitted),
                'saturation_level': float(np.mean(fitted_hsv[:,:,1])),
                'brightness_level': float(np.mean(fitted_hsv[:,:,2]))
            }
            
            return analysis
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _estimate_color_temperature(self, image: np.ndarray) -> str:
        """ìƒ‰ì˜¨ë„ ì¶”ì •"""
        try:
            # RGB í‰ê· ê°’ìœ¼ë¡œ ìƒ‰ì˜¨ë„ ì¶”ì •
            r_mean = np.mean(image[:,:,0])
            g_mean = np.mean(image[:,:,1])
            b_mean = np.mean(image[:,:,2])
            
            # ê°„ë‹¨í•œ ìƒ‰ì˜¨ë„ ë¶„ë¥˜
            if b_mean > r_mean * 1.1:
                return "Cool (ì°¨ê°€ìš´ í†¤)"
            elif r_mean > b_mean * 1.1:
                return "Warm (ë”°ëœ»í•œ í†¤)"
            else:
                return "Neutral (ì¤‘ì„± í†¤)"
                
        except Exception as e:
            logger.warning(f"ìƒ‰ì˜¨ë„ ì¶”ì • ì‹¤íŒ¨: {e}")
            return "Unknown"
    
    def _analyze_structure(self, fitted: np.ndarray, person: np.ndarray) -> Dict[str, Any]:
        """êµ¬ì¡° ë¶„ì„"""
        try:
            # ì—£ì§€ ë¶„ì„
            fitted_edges = cv2.Canny(cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY), 50, 150)
            person_edges = cv2.Canny(cv2.cvtColor(person, cv2.COLOR_RGB2GRAY), 50, 150)
            
            analysis = {
                'edge_density_fitted': float(np.sum(fitted_edges > 0) / fitted_edges.size),
                'edge_density_person': float(np.sum(person_edges > 0) / person_edges.size),
                'structural_similarity': self._calculate_ssim(fitted_edges, person_edges),
                'geometric_distortion': self._calculate_geometric_distortion(fitted, person)
            }
            
            return analysis
            
        except Exception as e:
            logger.warning(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_geometric_distortion(self, fitted: np.ndarray, person: np.ndarray) -> float:
        """ê¸°í•˜í•™ì  ì™œê³¡ ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì™œê³¡ ì¸¡ì • (ê°„ë‹¨ ë²„ì „)
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person, cv2.COLOR_RGB2GRAY)
            
            # êµ¬ì¡°ì  ì°¨ì´ ì¸¡ì •
            diff = cv2.absdiff(fitted_gray, person_gray)
            distortion_level = np.mean(diff) / 255.0
            
            return float(1.0 - distortion_level)  # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™œê³¡ì´ ì ìŒ
            
        except Exception as e:
            logger.warning(f"ê¸°í•˜í•™ì  ì™œê³¡ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_overall_similarity(self, fitted: np.ndarray, person: np.ndarray) -> float:
        """ì „ì²´ ìœ ì‚¬ì„± ê³„ì‚°"""
        try:
            # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ì¡°í•©
            structural_sim = self._calculate_ssim(
                cv2.cvtColor(fitted, cv2.COLOR_RGB2GRAY),
                cv2.cvtColor(person, cv2.COLOR_RGB2GRAY)
            )
            
            # ìƒ‰ìƒ ìœ ì‚¬ì„±
            fitted_mean = np.mean(fitted.reshape(-1, 3), axis=0)
            person_mean = np.mean(person.reshape(-1, 3), axis=0)
            color_sim = 1.0 - np.linalg.norm(fitted_mean - person_mean) / 441.67
            
            # ì¡°í•©
            overall_sim = 0.6 * structural_sim + 0.4 * color_sim
            
            return max(0.0, min(1.0, overall_sim))
            
        except Exception as e:
            logger.warning(f"ì „ì²´ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_clothing_integration(self, fitted: np.ndarray, clothing: np.ndarray) -> float:
        """ì˜ë¥˜ í†µí•©ë„ ê³„ì‚°"""
        try:
            # ì˜ë¥˜ê°€ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©ë˜ì—ˆëŠ”ì§€ í‰ê°€
            fitted_colors = np.mean(fitted.reshape(-1, 3), axis=0)
            clothing_colors = np.mean(clothing.reshape(-1, 3), axis=0)
            
            # ìƒ‰ìƒ ì¼ê´€ì„±
            color_consistency = 1.0 - np.linalg.norm(fitted_colors - clothing_colors) / 441.67
            
            # ê²½ê³„ì„  í’ˆì§ˆ
            boundary_quality = self._evaluate_boundary_quality(fitted)
            
            # ì¡°í•©
            integration_score = 0.5 * color_consistency + 0.5 * boundary_quality
            
            return max(0.0, min(1.0, integration_score))
            
        except Exception as e:
            logger.warning(f"ì˜ë¥˜ í†µí•©ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_realism_score(self, image: np.ndarray) -> float:
        """í˜„ì‹¤ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # í˜„ì‹¤ì ì¸ ì´ë¯¸ì§€ì¸ì§€ í‰ê°€
            
            # 1. ì¡°ëª… ìì—°ìŠ¤ëŸ¬ì›€
            lighting_score = self._evaluate_lighting_naturalness(image)
            
            # 2. í…ìŠ¤ì²˜ í’ˆì§ˆ
            texture_score = self._evaluate_texture_quality(image)
            
            # 3. ê·¸ë¦¼ì ì¼ê´€ì„±
            shadow_score = self._evaluate_shadow_consistency(image)
            
            # ì¡°í•©
            realism_score = (lighting_score + texture_score + shadow_score) / 3.0
            
            return max(0.0, min(1.0, realism_score))
            
        except Exception as e:
            logger.warning(f"í˜„ì‹¤ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_lighting_naturalness(self, image: np.ndarray) -> float:
        """ì¡°ëª… ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
        try:
            # LAB ìƒ‰ê³µê°„ì—ì„œ L ì±„ë„ ë¶„ì„
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l_channel = lab[:, :, 0]
            
            # ì¡°ëª… ë¶„í¬ ë¶„ì„
            hist = cv2.calcHist([l_channel], [0], None, [256], [0, 256])
            hist_norm = hist / np.sum(hist)
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ì¡°ëª…ì€ ì ì ˆí•œ ë¶„í¬ë¥¼ ê°€ì§
            entropy = -np.sum(hist_norm * np.log(hist_norm + 1e-8))
            naturalness = min(entropy / 8.0, 1.0)  # ì •ê·œí™”
            
            return naturalness
            
        except Exception as e:
            logger.warning(f"ì¡°ëª… ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_texture_quality(self, image: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ í’ˆì§ˆ í‰ê°€"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # LBP (Local Binary Pattern) ìœ ì‚¬ ê³„ì‚°
            texture_variance = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ì ì ˆí•œ í…ìŠ¤ì²˜ ë³µì¡ë„
            texture_score = min(texture_variance / 1000.0, 1.0)
            
            return texture_score
            
        except Exception as e:
            logger.warning(f"í…ìŠ¤ì²˜ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_shadow_consistency(self, image: np.ndarray) -> float:
        """ê·¸ë¦¼ì ì¼ê´€ì„± í‰ê°€"""
        try:
            # ê·¸ë¦¼ì ì˜ì—­ ê²€ì¶œ (ê°„ë‹¨ ë²„ì „)
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            v_channel = hsv[:, :, 2]
            
            # ì–´ë‘ìš´ ì˜ì—­ (ê·¸ë¦¼ì í›„ë³´) ê²€ì¶œ
            shadow_mask = v_channel < np.percentile(v_channel, 20)
            
            # ê·¸ë¦¼ì ì˜ì—­ì˜ ì—°ì†ì„± í‰ê°€
            contours, _ = cv2.findContours(
                shadow_mask.astype(np.uint8), 
                cv2.RETR_EXTERNAL, 
                cv2.CHAIN_APPROX_SIMPLE
            )
            
            if contours:
                # ê°€ì¥ í° ê·¸ë¦¼ì ì˜ì—­ì˜ í˜•íƒœ ë¶„ì„
                largest_shadow = max(contours, key=cv2.contourArea)
                area = cv2.contourArea(largest_shadow)
                perimeter = cv2.arcLength(largest_shadow, True)
                
                if perimeter > 0:
                    compactness = (4 * np.pi * area) / (perimeter ** 2)
                    consistency_score = min(compactness * 2, 1.0)
                    return consistency_score
            
            return 0.7  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.warning(f"ê·¸ë¦¼ì ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _find_strongest_aspect(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """ê°€ì¥ ê°•í•œ ì¸¡ë©´ ì°¾ê¸°"""
        try:
            metric_dict = metrics.to_dict()
            metric_dict.pop('overall_score', None)  # ì „ì²´ ì ìˆ˜ ì œì™¸
            
            if metric_dict:
                best_metric = max(metric_dict.items(), key=lambda x: x[1])
                return {
                    'aspect': best_metric[0].replace('_', ' ').title(),
                    'score': best_metric[1],
                    'description': self._get_aspect_description(best_metric[0])
                }
            
            return {'aspect': 'Unknown', 'score': 0.0, 'description': ''}
            
        except Exception as e:
            logger.warning(f"ê°€ì¥ ê°•í•œ ì¸¡ë©´ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return {'aspect': 'Unknown', 'score': 0.0, 'description': ''}
    
    def _find_weakest_aspect(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """ê°€ì¥ ì•½í•œ ì¸¡ë©´ ì°¾ê¸°"""
        try:
            metric_dict = metrics.to_dict()
            metric_dict.pop('overall_score', None)  # ì „ì²´ ì ìˆ˜ ì œì™¸
            
            if metric_dict:
                worst_metric = min(metric_dict.items(), key=lambda x: x[1])
                return {
                    'aspect': worst_metric[0].replace('_', ' ').title(),
                    'score': worst_metric[1],
                    'description': self._get_aspect_description(worst_metric[0]),
                    'improvement_suggestion': self._get_improvement_suggestion(worst_metric[0])
                }
            
            return {'aspect': 'Unknown', 'score': 0.0, 'description': '', 'improvement_suggestion': ''}
            
        except Exception as e:
            logger.warning(f"ê°€ì¥ ì•½í•œ ì¸¡ë©´ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return {'aspect': 'Unknown', 'score': 0.0, 'description': '', 'improvement_suggestion': ''}
    
    def _get_aspect_description(self, aspect_name: str) -> str:
        """ì¸¡ë©´ ì„¤ëª… ë°˜í™˜"""
        descriptions = {
            'perceptual_quality': 'ì‚¬ëŒì´ ì¸ì§€í•˜ëŠ” ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆ',
            'technical_quality': 'ì„ ëª…ë„, ë…¸ì´ì¦ˆ, ëŒ€ë¹„ ë“± ê¸°ìˆ ì  ì¸¡ë©´ì˜ í’ˆì§ˆ',
            'aesthetic_quality': 'ìƒ‰ìƒ ì¡°í™”, êµ¬ì„± ë“± ë¯¸ì  ì¸¡ë©´ì˜ í’ˆì§ˆ',
            'fit_accuracy': 'ì˜ë¥˜ê°€ ì‹ ì²´ì— ì–¼ë§ˆë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ë§ëŠ”ì§€',
            'color_harmony': 'ì˜ë¥˜ì™€ ì‹ ì²´, ë°°ê²½ ê°„ì˜ ìƒ‰ìƒ ì¡°í™”',
            'detail_preservation': 'ì›ë³¸ ì´ë¯¸ì§€ì˜ ì„¸ë¶€ì‚¬í•­ì´ ì–¼ë§ˆë‚˜ ë³´ì¡´ë˜ì—ˆëŠ”ì§€',
            'edge_quality': 'ê°ì²´ ê²½ê³„ì„ ì˜ ì„ ëª…ë„ì™€ ìì—°ìŠ¤ëŸ¬ì›€',
            'lighting_consistency': 'ì „ì²´ ì´ë¯¸ì§€ì˜ ì¡°ëª… ì¼ê´€ì„±',
            'artifact_level': 'ì••ì¶•, ë¸”ëŸ¬ ë“± ì¸ê³µì  ê²°í•¨ì˜ ì •ë„',
            'face_preservation': 'ì–¼êµ´ íŠ¹ì§•ì´ ì–¼ë§ˆë‚˜ ì˜ ë³´ì¡´ë˜ì—ˆëŠ”ì§€'
        }
        
        return descriptions.get(aspect_name, 'í•´ë‹¹ ì¸¡ë©´ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.')
    
    def _get_improvement_suggestion(self, aspect_name: str) -> str:
        """ê°œì„  ì œì•ˆ ë°˜í™˜"""
        suggestions = {
            'perceptual_quality': 'ë” ë†’ì€ í•´ìƒë„ì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê³  ë…¸ì´ì¦ˆë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.',
            'technical_quality': 'ì´ë¯¸ì§€ ì••ì¶•ì„ ì¤„ì´ê³  ì„ ëª…í•œ ì›ë³¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.',
            'aesthetic_quality': 'ìƒ‰ìƒ ê· í˜•ì„ ì¡°ì •í•˜ê³  êµ¬ì„±ì„ ê°œì„ í•´ë³´ì„¸ìš”.',
            'fit_accuracy': 'ì •í™•í•œ ì‹ ì²´ ì¸¡ì •ê°’ì„ ì…ë ¥í•˜ê³  ì ì ˆí•œ í¬ì¦ˆë¥¼ ì·¨í•´ë³´ì„¸ìš”.',
            'color_harmony': 'í”¼ë¶€í†¤ê³¼ ì˜ ì–´ìš¸ë¦¬ëŠ” ì˜ë¥˜ ìƒ‰ìƒì„ ì„ íƒí•´ë³´ì„¸ìš”.',
            'detail_preservation': 'ë” ë†’ì€ í’ˆì§ˆì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.',
            'edge_quality': 'ë°°ê²½ì´ ê¹”ë”í•˜ê³  ëŒ€ë¹„ê°€ ì¢‹ì€ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.',
            'lighting_consistency': 'ê· ì¼í•œ ì¡°ëª… í™˜ê²½ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”.',
            'artifact_level': 'ì²˜ë¦¬ í’ˆì§ˆ ì„¤ì •ì„ ë†’ì´ê±°ë‚˜ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‹œë„í•´ë³´ì„¸ìš”.',
            'face_preservation': 'ì–¼êµ´ì´ ëª…í™•íˆ ë³´ì´ëŠ” ì •ë©´ ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.'
        }
        
        return suggestions.get(aspect_name, 'êµ¬ì²´ì ì¸ ê°œì„  ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    
    def _calculate_improvement_potential(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """ê°œì„  ì ì¬ë ¥ ê³„ì‚°"""
        try:
            metric_dict = metrics.to_dict()
            metric_dict.pop('overall_score', None)
            
            if not metric_dict:
                return {}
            
            # í‰ê·  ì ìˆ˜ì™€ ìµœì € ì ìˆ˜ ì°¨ì´
            scores = list(metric_dict.values())
            avg_score = sum(scores) / len(scores)
            min_score = min(scores)
            max_score = max(scores)
            
            improvement_potential = max_score - min_score
            
            # ê°œì„  ìš°ì„ ìˆœìœ„ (ë‚®ì€ ì ìˆ˜ ìˆœ)
            sorted_metrics = sorted(metric_dict.items(), key=lambda x: x[1])
            priority_areas = [
                {
                    'aspect': metric[0].replace('_', ' ').title(),
                    'current_score': metric[1],
                    'potential_gain': avg_score - metric[1] if metric[1] < avg_score else 0,
                    'priority': 'high' if metric[1] < 0.6 else 'medium' if metric[1] < 0.75 else 'low'
                }
                for metric in sorted_metrics[:3]  # ìƒìœ„ 3ê°œ
            ]
            
            return {
                'improvement_potential': improvement_potential,
                'current_average': avg_score,
                'target_score': min(avg_score + improvement_potential * 0.5, 1.0),
                'priority_areas': priority_areas,
                'overall_status': "overall" if improvement_potential < 0.2 else min_score[0].replace("_", " ")
            }
            
        except Exception as e:
            logger.warning(f"ê°œì„  ì ì¬ë ¥ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_letter_grade(self, score: float) -> str:
        """ë¬¸ì ë“±ê¸‰ ë°˜í™˜"""
        if score >= 0.97:
            return "A+"
        elif score >= 0.93:
            return "A"
        elif score >= 0.90:
            return "A-"
        elif score >= 0.87:
            return "B+"
        elif score >= 0.83:
            return "B"
        elif score >= 0.80:
            return "B-"
        elif score >= 0.77:
            return "C+"
        elif score >= 0.73:
            return "C"
        elif score >= 0.70:
            return "C-"
        elif score >= 0.65:
            return "D+"
        elif score >= 0.60:
            return "D"
        elif score >= 0.50:
            return "D-"
        else:
            return "F"
    
    def _get_metric_status(self, score: float) -> str:
        """ë©”íŠ¸ë¦­ ìƒíƒœ ë°˜í™˜"""
        if score >= 0.9:
            return "excellent"
        elif score >= 0.75:
            return "good"
        elif score >= 0.6:
            return "fair"
        elif score >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "RealQualityAssessment",
            "version": "1.0",
            "device": self.device,
            "initialized": self.is_initialized,
            "torch_available": TORCH_AVAILABLE,
            "face_detection_enabled": self.face_detector is not None,
            "advanced_metrics_enabled": self.enable_advanced_metrics,
            "detailed_analysis_enabled": self.enable_detailed_analysis,
            "supported_metrics": list(QualityMetrics.__annotations__.keys()),
            "quality_thresholds": self.quality_thresholds,
            "metric_weights": self.metric_weights
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.face_detector = None
        self.is_initialized = False
        logger.info("ğŸ§¹ ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# === ì‚¬ìš© ì˜ˆì‹œ ===
async def test_real_quality_assessment():
    """ì‹¤ì œ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    quality_assessor = RealQualityAssessmentStep(
        device='cpu',  # ë˜ëŠ” 'mps' (M3 Max)
        config={
            'enable_advanced_metrics': True,
            'enable_face_detection': True,
            'enable_detailed_analysis': True,
            'use_torch': TORCH_AVAILABLE
        }
    )
    
    success = await quality_assessor.initialize()
    if not success:
        print("âŒ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    fitted_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    person_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    clothing_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    
    # 3. í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
    print("ğŸš€ í’ˆì§ˆ í‰ê°€ ì‹œì‘...")
    
    result = await quality_assessor.process(
        fitted_image=fitted_image,
        original_person=person_image,
        original_clothing=clothing_image,
        pipeline_results={
            'step_01': {'confidence': 0.85},
            'step_02': {'confidence': 0.78},
            'step_03': {'confidence': 0.92}
        }
    )
    
    # 4. ê²°ê³¼ ì¶œë ¥
    if result['success']:
        print("\n" + "="*50)
        print("ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
        print("="*50)
        
        print(f"ğŸ¯ ì „ì²´ ì ìˆ˜: {result['overall_score']:.3f}")
        print(f"ğŸ“ˆ ë“±ê¸‰: {result['grade'].upper()} ({result['letter_grade']})")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        
        print(f"\nğŸ“‹ ì„¸ë¶€ ë©”íŠ¸ë¦­:")
        for metric, value in result['metrics'].items():
            if metric != 'overall_score':
                status = quality_assessor._get_metric_status(value)
                print(f"  â€¢ {metric.replace('_', ' ').title()}: {value:.3f} ({status})")
        
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        for i, rec in enumerate(result['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        if result.get('detailed_analysis'):
            analysis = result['detailed_analysis']
            
            print(f"\nğŸ” ìƒì„¸ ë¶„ì„:")
            
            if 'performance_insights' in analysis:
                insights = analysis['performance_insights']
                print(f"  â€¢ ìµœê³  ì¸¡ë©´: {insights.get('strongest_aspect', {}).get('aspect', 'N/A')}")
                print(f"  â€¢ ê°œì„  í•„ìš”: {insights.get('weakest_aspect', {}).get('aspect', 'N/A')}")
            
            if 'comparison_metrics' in analysis:
                comp = analysis['comparison_metrics']
                print(f"  â€¢ ì›ë³¸ ìœ ì‚¬ë„: {comp.get('similarity_to_original', 0):.3f}")
                print(f"  â€¢ ì˜ë¥˜ í†µí•©ë„: {comp.get('clothing_integration', 0):.3f}")
                print(f"  â€¢ í˜„ì‹¤ì„± ì ìˆ˜: {comp.get('realism_score', 0):.3f}")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        with open("quality_report.md", "w", encoding='utf-8') as f:
            f.write(f"# ê°€ìƒ í”¼íŒ… í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ì „ì²´ ì ìˆ˜**: {result['overall_score']:.3f} ({result['letter_grade']})\n\n")
            f.write(f"**ë“±ê¸‰**: {result['grade'].upper()}\n\n")
            f.write(f"**ì²˜ë¦¬ ì‹œê°„**: {result['processing_time']:.2f}ì´ˆ\n\n")
            
            f.write(f"## ì„¸ë¶€ ë©”íŠ¸ë¦­\n\n")
            for metric, value in result['metrics'].items():
                if metric != 'overall_score':
                    f.write(f"- **{metric.replace('_', ' ').title()}**: {value:.3f}\n")
            
            f.write(f"\n## ê°œì„  ì œì•ˆ\n\n")
            for i, suggestion in enumerate(result['recommendations'], 1):
                priority = "HIGH" if i <= 2 else "MEDIUM" if i <= 4 else "LOW"
                f.write(f"**[{priority}]** {suggestion['issue']}\n")
                f.write(f"   â†’ {suggestion['suggestion']}\n\n")
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            with open("quality_report.md", "w", encoding='utf-8') as f:
                f.write(report)
            
            print("ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: quality_report.md")
            
    else:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {result['error']}")

    await quality_assessor.cleanup()


# === FastAPI í†µí•© ì˜ˆì‹œ ===
class QualityAssessmentAPI:
    """í’ˆì§ˆ í‰ê°€ API ë˜í¼"""
    
    def __init__(self):
        self.assessor = None
    
    async def initialize(self):
        """API ì´ˆê¸°í™”"""
        self.assessor = RealQualityAssessmentStep(
            device='cpu',  # ë˜ëŠ” í™˜ê²½ì— ë”°ë¼ 'mps'
            config={
                'enable_advanced_metrics': True,
                'enable_face_detection': True,
                'enable_detailed_analysis': True
            }
        )
        return await self.assessor.initialize()
    
    async def assess_quality(
        self,
        fitted_image_base64: str,
        person_image_base64: str,
        clothing_image_base64: str,
        pipeline_results: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ API ì—”ë“œí¬ì¸íŠ¸"""
        
        if not self.assessor or not self.assessor.is_initialized:
            raise RuntimeError("í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return await self.assessor.process(
            fitted_image=fitted_image_base64,
            original_person=person_image_base64,
            original_clothing=clothing_image_base64,
            pipeline_results=pipeline_results
        )
    
    async def cleanup(self):
        """API ì •ë¦¬"""
        if self.assessor:
            await self.assessor.cleanup()


# === ë³„ì¹­ í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±) ===
QualityAssessmentStep = RealQualityAssessmentStep


if __name__ == "__main__":
    print("ğŸ¯ ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    asyncio.run(test_real_quality_assessment())
    
    print("\n" + "=" * 50)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")