# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - v17.0 ì™„ì „ í˜¸í™˜ ë²„ì „
================================================================================
âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ - UnifiedDependencyManager ì—°ë™
âœ… ModelLoader v21.0 í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ì—°ì‚°
âœ… StepInterface v2.0 register_model_requirement í™œìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING íŒ¨í„´)
âœ… ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬í˜„
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë° í™œìš©
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ìœ ì§€

ì²˜ë¦¬ íë¦„:
ğŸŒ API ìš”ì²­ â†’ ğŸ“‹ PipelineManager â†’ ğŸ¯ QualityAssessmentStep ìƒì„±
â†“
ğŸ”— BaseStepMixin.dependency_manager.auto_inject_dependencies()
â”œâ”€ ModelLoader ìë™ ì£¼ì…
â”œâ”€ StepModelInterface ìƒì„±
â””â”€ register_model_requirement í˜¸ì¶œ
â†“
ğŸš€ QualityAssessmentStep.initialize()
â”œâ”€ AI í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸)
â”œâ”€ ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
â””â”€ M3 Max ìµœì í™” ì ìš©
â†“
ğŸ§  ì‹¤ì œ AI ì¶”ë¡  process()
â”œâ”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â†’ Tensor ë³€í™˜
â”œâ”€ AI ëª¨ë¸ ì¶”ë¡  (LPIPS, SSIM, í’ˆì§ˆ í‰ê°€)
â”œâ”€ 8ê°€ì§€ í’ˆì§ˆ ë¶„ì„ â†’ ê²°ê³¼ í•´ì„
â””â”€ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
â†“
ğŸ“¤ ê²°ê³¼ ë°˜í™˜ (QualityMetrics ê°ì²´)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache, wraps
import numpy as np
import base64
import io

# ==============================================
# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.interfaces.step_interface import StepModelInterface

# ==============================================
# ğŸ”¥ BaseStepMixin v16.0 ì„í¬íŠ¸ (í•µì‹¬)
# ==============================================
try:
    from .base_step_mixin import BaseStepMixin, QualityAssessmentMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… BaseStepMixin v16.0 ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ BaseStepMixin ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================================
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    # OpenCV í´ë°± ì‹œìŠ¤í…œ
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    pil_img = Image.fromarray(img)
                    resized = pil_img.resize(size)
                    return np.array(resized)
            return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:
                    return img[:, :, ::-1]
            return img
        
        def imread(self, path):
            if PIL_AVAILABLE:
                from PIL import Image
                img = Image.open(path)
                return np.array(img)
            return None
        
        def imwrite(self, path, img):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    Image.fromarray(img).save(path)
                    return True
            return False
    
    cv2 = OpenCVFallback()
    OPENCV_AVAILABLE = False

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ==============================================
# ğŸ”¥ GPU ì•ˆì „ ì—°ì‚° ìœ í‹¸ë¦¬í‹°
# ==============================================
def safe_mps_empty_cache():
    """MPS ìºì‹œ ì•ˆì „ ì •ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
        gc.collect()
        return {"success": True, "method": "mps_cache_cleared"}
    except Exception:
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

def safe_tensor_to_numpy(tensor):
    """Tensorë¥¼ ì•ˆì „í•˜ê²Œ NumPyë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            return tensor.detach().cpu().numpy()
        elif hasattr(tensor, 'numpy'):
            return tensor.numpy()
        else:
            return np.array(tensor)
    except Exception:
        return np.array(tensor)

# ==============================================
# ğŸ”¥ MRO ì•ˆì „í•œ í´ë°± í´ë˜ìŠ¤ë“¤
# ==============================================
if not BASE_STEP_MIXIN_AVAILABLE:
    class BaseStepMixin:
        """MRO ì•ˆì „í•œ í´ë°± BaseStepMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.step_name = getattr(self, 'step_name', 'quality_assessment')
            self.step_number = 8
            self.device = 'cpu'
            self.is_initialized = False
            self.dependency_manager = None
    
    class QualityAssessmentMixin(BaseStepMixin):
        """MRO ì•ˆì „í•œ í´ë°± QualityAssessmentMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.step_type = "quality_assessment"
            self.quality_threshold = 0.7

# ==============================================
# ğŸ”¥ í’ˆì§ˆ í‰ê°€ ë°ì´í„° êµ¬ì¡°ë“¤
# ==============================================
class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    FAILED = "failed"

class AssessmentMode(Enum):
    """í‰ê°€ ëª¨ë“œ"""
    COMPREHENSIVE = "comprehensive"
    FAST = "fast"
    DETAILED = "detailed"
    CUSTOM = "custom"

class QualityAspect(Enum):
    """í’ˆì§ˆ í‰ê°€ ì˜ì—­"""
    SHARPNESS = "sharpness"
    COLOR = "color"
    FITTING = "fitting"
    REALISM = "realism"
    ARTIFACTS = "artifacts"
    ALIGNMENT = "alignment"
    LIGHTING = "lighting"
    TEXTURE = "texture"

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° êµ¬ì¡°"""
    overall_score: float = 0.0
    confidence: float = 0.0
    
    # ì„¸ë¶€ ì ìˆ˜ë“¤
    sharpness_score: float = 0.0
    color_score: float = 0.0
    fitting_score: float = 0.0
    realism_score: float = 0.0
    artifacts_score: float = 0.0
    alignment_score: float = 0.0
    lighting_score: float = 0.0
    texture_score: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    processing_time: float = 0.0
    device_used: str = "cpu"
    model_version: str = "v1.0"
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)
# ==============================================
if TORCH_AVAILABLE:
    class RealPerceptualQualityModel(nn.Module):
        """ì‹¤ì œ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (LPIPS ê¸°ë°˜)"""
        
        def __init__(self, pretrained_path: Optional[str] = None):
            super().__init__()
            
            # VGG ë°±ë³¸ (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ)
            self.backbone = self._create_vgg_backbone()
            
            # LPIPS ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œ
            self.feature_extractors = nn.ModuleList([
                nn.Conv2d(64, 64, 1),
                nn.Conv2d(128, 128, 1),
                nn.Conv2d(256, 256, 1),
                nn.Conv2d(512, 512, 1),
                nn.Conv2d(512, 512, 1)
            ])
            
            # í’ˆì§ˆ ì˜ˆì¸¡ í—¤ë“œ
            self.quality_head = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if pretrained_path and Path(pretrained_path).exists():
                self.load_checkpoint(pretrained_path)
        
        def _create_vgg_backbone(self):
            """VGG ë°±ë³¸ ìƒì„±"""
            return nn.Sequential(
                # Block 1
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, stride=2),
                
                # Block 2
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, stride=2),
                
                # Block 3
                nn.Conv2d(128, 256, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, stride=2),
                
                # Block 4
                nn.Conv2d(256, 512, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, stride=2),
                
                # Block 5
                nn.Conv2d(512, 512, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, padding=1),
                nn.ReLU(inplace=True)
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'state_dict' in checkpoint:
                    self.load_state_dict(checkpoint['state_dict'], strict=False)
                elif 'model' in checkpoint:
                    self.load_state_dict(checkpoint['model'], strict=False)
                else:
                    self.load_state_dict(checkpoint, strict=False)
                logging.getLogger(__name__).info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {checkpoint_path}")
            except Exception as e:
                logging.getLogger(__name__).warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        def forward(self, x):
            """ìˆœì „íŒŒ"""
            features = self.backbone(x)
            quality_score = self.quality_head(features)
            
            return {
                'overall_quality': quality_score,
                'features': features,
                'perceptual_distance': 1.0 - quality_score
            }

    class RealAestheticQualityModel(nn.Module):
        """ì‹¤ì œ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
        
        def __init__(self, pretrained_path: Optional[str] = None):
            super().__init__()
            
            # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
            self.backbone = self._create_resnet_backbone()
            
            # ë¯¸ì  íŠ¹ì„± ë¶„ì„ í—¤ë“œë“¤
            self.aesthetic_heads = nn.ModuleDict({
                'composition': self._create_head(512, 1),
                'color_harmony': self._create_head(512, 1),
                'lighting': self._create_head(512, 1),
                'balance': self._create_head(512, 1),
                'symmetry': self._create_head(512, 1)
            })
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if pretrained_path and Path(pretrained_path).exists():
                self.load_checkpoint(pretrained_path)
        
        def _create_resnet_backbone(self):
            """ResNet ë°±ë³¸ ìƒì„±"""
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                # ResNet blocks would go here
                # ê°„ë‹¨í™”ëœ ë²„ì „
                nn.Conv2d(64, 128, 3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(128, 256, 3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(256, 512, 3, stride=2, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                
                nn.AdaptiveAvgPool2d(1)
            )
        
        def _create_head(self, in_features: int, out_features: int):
            """ë¶„ì„ í—¤ë“œ ìƒì„±"""
            return nn.Sequential(
                nn.Linear(in_features, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, out_features),
                nn.Sigmoid()
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'state_dict' in checkpoint:
                    self.load_state_dict(checkpoint['state_dict'], strict=False)
                elif 'model' in checkpoint:
                    self.load_state_dict(checkpoint['model'], strict=False)
                else:
                    self.load_state_dict(checkpoint, strict=False)
                logging.getLogger(__name__).info(f"âœ… ë¯¸ì  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            except Exception as e:
                logging.getLogger(__name__).warning(f"âš ï¸ ë¯¸ì  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        def forward(self, x):
            """ìˆœì „íŒŒ"""
            features = self.backbone(x).flatten(1)
            
            results = {}
            for name, head in self.aesthetic_heads.items():
                results[name] = head(features)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall'] = torch.mean(torch.stack(list(results.values())))
            
            return results

else:
    # PyTorch ì—†ì„ ë•Œ ë”ë¯¸ í´ë˜ìŠ¤
    class RealPerceptualQualityModel:
        def __init__(self, pretrained_path=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ RealPerceptualQualityModel")
        
        def predict(self, x):
            return {'overall_quality': 0.7, 'perceptual_distance': 0.3}
    
    class RealAestheticQualityModel:
        def __init__(self, pretrained_path=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ RealAestheticQualityModel")
        
        def predict(self, x):
            return {'composition': 0.7, 'color_harmony': 0.8, 'lighting': 0.75, 'balance': 0.7, 'symmetry': 0.8}

# ==============================================
# ğŸ”¥ ì „ë¬¸ ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€, ê°œì„ )
# ==============================================
class TechnicalQualityAnalyzer:
    """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu", enable_gpu: bool = False):
        self.device = device
        self.enable_gpu = enable_gpu
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
        
        # ë¶„ì„ ìºì‹œ
        self.analysis_cache = {}
        
        # ê¸°ìˆ ì  ë¶„ì„ ì„ê³„ê°’ë“¤
        self.thresholds = {
            'sharpness_min': 100.0,
            'noise_max': 50.0,
            'contrast_min': 20.0,
            'brightness_range': (50, 200)
        }
    
    def analyze(self, image: np.ndarray) -> Dict[str, Any]:
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            if image is None or image.size == 0:  # NumPy ë°°ì—´ ì¡°ê±´ë¬¸ ìˆ˜ì •
                return self._get_fallback_technical_results()
            
            results = {}
            
            # 1. ì„ ëª…ë„ ë¶„ì„
            results['sharpness'] = self._analyze_sharpness(image)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„
            results['noise_level'] = self._analyze_noise_level(image)
            
            # 3. ëŒ€ë¹„ ë¶„ì„
            results['contrast'] = self._analyze_contrast(image)
            
            # 4. ë°ê¸° ë¶„ì„
            results['brightness'] = self._analyze_brightness(image)
            
            # 5. í¬í™”ë„ ë¶„ì„
            results['saturation'] = self._analyze_saturation(image)
            
            # 6. ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            results['artifacts'] = self._detect_artifacts(image)
            
            # 7. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall_score'] = self._calculate_technical_score(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_technical_results()
    
    def _analyze_sharpness(self, image: np.ndarray) -> float:
        """ì„ ëª…ë„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if OPENCV_AVAILABLE else np.mean(image, axis=2)
            else:
                gray = image
            
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                sharpness = laplacian.var()
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜ ì„ ëª…ë„
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                sharpness = np.var(dx) + np.var(dy)
            
            # ì •ê·œí™” (0-1)
            normalized_sharpness = min(1.0, sharpness / 10000.0)
            return max(0.0, normalized_sharpness)
            
        except Exception as e:
            self.logger.error(f"ì„ ëª…ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                # ê° ì±„ë„ë³„ ë…¸ì´ì¦ˆ ë¶„ì„
                noise_levels = []
                for channel in range(3):
                    channel_data = image[:, :, channel]
                    # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
                    if OPENCV_AVAILABLE:
                        blur = cv2.GaussianBlur(channel_data.astype(np.uint8), (5, 5), 0)
                        noise = np.abs(channel_data.astype(float) - blur.astype(float))
                    else:
                        # ê°„ë‹¨í•œ í‘œì¤€í¸ì°¨ ê¸°ë°˜
                        noise = np.std(channel_data)
                    
                    noise_level = np.mean(noise) / 255.0
                    noise_levels.append(noise_level)
                
                # í‰ê·  ë…¸ì´ì¦ˆ ë ˆë²¨
                avg_noise = np.mean(noise_levels)
            else:
                avg_noise = np.std(image) / 255.0
            
            # ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ìŒ (ì—­ìˆœ)
            return max(0.0, min(1.0, 1.0 - avg_noise * 5))
            
        except Exception as e:
            self.logger.error(f"ë…¸ì´ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_contrast(self, image: np.ndarray) -> float:
        """ëŒ€ë¹„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # RMS ëŒ€ë¹„ ê³„ì‚°
            contrast = np.std(gray)
            
            # ì •ê·œí™” (ì ì ˆí•œ ëŒ€ë¹„ ë²”ìœ„: 30-80)
            if 30 <= contrast <= 80:
                contrast_score = 1.0
            elif contrast < 30:
                contrast_score = contrast / 30.0
            else:
                contrast_score = max(0.3, 1.0 - (contrast - 80) / 100.0)
            
            return max(0.0, min(1.0, contrast_score))
            
        except Exception as e:
            self.logger.error(f"ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """ë°ê¸° ë¶„ì„"""
        try:
            brightness = np.mean(image)
            
            # ì ì ˆí•œ ë°ê¸° ë²”ìœ„ (100-160)
            if 100 <= brightness <= 160:
                brightness_score = 1.0
            elif brightness < 100:
                brightness_score = brightness / 100.0
            else:
                brightness_score = max(0.3, 1.0 - (brightness - 160) / 95.0)
            
            return max(0.0, min(1.0, brightness_score))
            
        except Exception as e:
            self.logger.error(f"ë°ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """í¬í™”ë„ ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # HSV ë³€í™˜ ë° í¬í™”ë„ ë¶„ì„
            if OPENCV_AVAILABLE:
                hsv = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2HSV)
                saturation = np.mean(hsv[:, :, 1])
            else:
                # RGB ê¸°ë°˜ í¬í™”ë„ ê·¼ì‚¬
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = np.mean((max_vals - min_vals) / (max_vals + 1e-8)) * 255
            
            # ì ì ˆí•œ í¬í™”ë„ ë²”ìœ„ (80-180)
            if 80 <= saturation <= 180:
                saturation_score = 1.0
            elif saturation < 80:
                saturation_score = saturation / 80.0
            else:
                saturation_score = max(0.3, 1.0 - (saturation - 180) / 75.0)
            
            return max(0.0, min(1.0, saturation_score))
            
        except Exception as e:
            self.logger.error(f"í¬í™”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„ìœ¼ë¡œ ì•„í‹°íŒ©íŠ¸ ì¶”ì •
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                artifact_metric = np.std(laplacian)
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                artifact_metric = np.std(dx) + np.std(dy)
            
            # ì•„í‹°íŒ©íŠ¸ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            artifact_score = max(0.0, 1.0 - artifact_metric / 1000.0)
            return min(1.0, artifact_score)
            
        except Exception as e:
            self.logger.error(f"ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _calculate_technical_score(self, results: Dict[str, Any]) -> float:
        """ê¸°ìˆ ì  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'sharpness': 0.25,
                'noise_level': 0.20,
                'contrast': 0.15,
                'brightness': 0.15,
                'saturation': 0.10,
                'artifacts': 0.15
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            # ì •ê·œí™”
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _get_fallback_technical_results(self) -> Dict[str, Any]:
        """í´ë°± ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼"""
        return {
            'sharpness': 0.5,
            'noise_level': 0.6,
            'contrast': 0.5,
            'brightness': 0.6,
            'saturation': 0.5,
            'artifacts': 0.7,
            'overall_score': 0.55,
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """ë¶„ì„ê¸° ì •ë¦¬"""
        self.analysis_cache.clear()

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤
# ==============================================
class QualityAssessmentStep(QualityAssessmentMixin):
    """
    ğŸ”¥ 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ Step - v17.0 ì™„ì „ í˜¸í™˜ ë²„ì „
    âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ (UnifiedDependencyManager)
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ì‚° (ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)
    âœ… ModelLoader v21.0 ì¸í„°í˜ì´ìŠ¤ í†µí•œ ëª¨ë¸ í˜¸ì¶œ
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ê¸°ì¡´ ëª¨ë“  ë¶„ì„ ê¸°ëŠ¥ ìœ ì§€
    âœ… M3 Max ìµœì í™”
    """
    
    def __init__(
        self,
        device: str = "auto",
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """í’ˆì§ˆ í‰ê°€ Step ì´ˆê¸°í™”"""
        
        # ğŸ”¥ BaseStepMixin v16.0 MRO ì•ˆì „í•œ ì´ˆê¸°í™”
        super().__init__(
            step_name='quality_assessment',
            step_number=8,
            device=device,
            **kwargs
        )
        
        # ğŸ”¥ ì„¤ì • ì´ˆê¸°í™”
        self.config = config or {}
        self.quality_threshold = self.config.get('quality_threshold', 0.7)
        self.assessment_mode = AssessmentMode(self.config.get('mode', 'comprehensive'))
        
        # ğŸ”¥ í’ˆì§ˆ í‰ê°€ ì„¤ì •
        self.assessment_config = {
            'mode': self.assessment_mode,
            'quality_threshold': self.quality_threshold,
            'enable_detailed_analysis': self.config.get('detailed_analysis', True),
            'enable_visualization': self.config.get('visualization', True),
            'enable_ai_models': self.config.get('enable_ai_models', True)
        }
        
        # ğŸ”¥ AI ëª¨ë¸ ê´€ë¦¬ (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)
        self.ai_models = {}
        self.model_paths = {
            'perceptual_quality': self.config.get('perceptual_model_path'),
            'aesthetic_quality': self.config.get('aesthetic_model_path'),
            'lpips_model': self.config.get('lpips_model_path')
        }
        
        # ğŸ”¥ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸
        self.assessment_pipeline = []
        
        # ğŸ”¥ ë¶„ì„ê¸°ë“¤
        self.technical_analyzer = None
        self.perceptual_analyzer = None
        self.aesthetic_analyzer = None
        
        # ğŸ”¥ ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_error = None
        self.error_count = 0
        self.last_error = None
        
        # ğŸ”¥ ì„±ëŠ¥ ìµœì í™”
        self.optimization_enabled = self.is_m3_max and self.memory_gb >= 64
        
        self.logger.info(f"âœ… {self.step_name} v17.0 ì´ˆê¸°í™” ì™„ë£Œ - Device: {self.device}")
    
    # ==============================================
    # ğŸ”¥ ì´ˆê¸°í™” ë° ModelLoader ì—°ë™ ë©”ì„œë“œë“¤
    # ==============================================
    async def initialize(self) -> bool:
        """í’ˆì§ˆ í‰ê°€ Step ì´ˆê¸°í™”"""
        try:
            self.logger.info(f"ğŸš€ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. BaseStepMixin v16.0 ì˜ì¡´ì„± ì£¼ì… í™œìš©
            await self._setup_dependency_injection()
            
            # 2. AI ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡
            self._register_model_requirements()
            
            # 3. ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
            await self._load_real_ai_models()
            
            # 4. í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            self._setup_assessment_pipeline()
            
            # 5. ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self._initialize_analyzers()
            
            # 6. M3 Max ìµœì í™” ì„¤ì •
            if self.optimization_enabled:
                self._optimize_for_m3_max()
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _setup_dependency_injection(self):
        """BaseStepMixin v16.0 ì˜ì¡´ì„± ì£¼ì… í™œìš©"""
        try:
            # UnifiedDependencyManager í™œìš©
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤í–‰
                injection_result = self.dependency_manager.auto_inject_dependencies()
                
                # ModelLoader ì ‘ê·¼
                if hasattr(self, 'model_loader') and self.model_loader:
                    # StepModelInterface ìƒì„±
                    self.model_interface = self.model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ")
                else:
                    self.logger.warning("âš ï¸ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨ - í´ë°± ëª¨ë“œ")
                    self.model_interface = None
            else:
                self.logger.warning("âš ï¸ UnifiedDependencyManager ì—†ìŒ - ìˆ˜ë™ ì„¤ì •")
                
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    def _register_model_requirements(self):
        """AI ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (StepInterface v2.0)"""
        try:
            if self.model_interface and hasattr(self.model_interface, 'register_model_requirement'):
                # ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="perceptual_quality_model",
                    model_type="lpips_quality",
                    device=self.device,
                    priority=8,
                    min_memory_mb=512.0,
                    max_memory_mb=2048.0,
                    input_size=(512, 512),
                    metadata={
                        "architecture": "vgg_lpips",
                        "purpose": "perceptual_quality_assessment",
                        "checkpoint_required": True
                    }
                )
                
                # ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="aesthetic_quality_model",
                    model_type="aesthetic_scorer",
                    device=self.device,
                    priority=7,
                    min_memory_mb=256.0,
                    max_memory_mb=1024.0,
                    input_size=(224, 224),
                    metadata={
                        "architecture": "resnet_aesthetic",
                        "purpose": "aesthetic_quality_assessment"
                    }
                )
                
                # ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="technical_quality_model",
                    model_type="image_quality_assessor",
                    device=self.device,
                    priority=6,
                    min_memory_mb=128.0,
                    max_memory_mb=512.0,
                    metadata={
                        "purpose": "technical_quality_analysis"
                    }
                )
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {self.step_name}")
            else:
                self.logger.warning("âš ï¸ StepModelInterface ì—†ìŒ - ì§ì ‘ ë¡œë“œ ëª¨ë“œ")
        
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    async def _load_real_ai_models(self):
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)"""
        try:
            # 1. StepModelInterfaceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì‹œë„
            if self.model_interface:
                await self._load_models_via_interface()
            
            # 2. ì§ì ‘ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (í´ë°±)
            await self._load_models_directly()
            
            self.logger.info(f"âœ… {len(self.ai_models)}ê°œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            await self._setup_fallback_models()
    
    async def _load_models_via_interface(self):
        """StepModelInterfaceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ
            perceptual_model = await self.model_interface.get_model("perceptual_quality_model")
            if perceptual_model:
                self.ai_models['perceptual'] = perceptual_model
                self.logger.info("âœ… ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ (Interface)")
            
            # ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ
            aesthetic_model = await self.model_interface.get_model("aesthetic_quality_model")
            if aesthetic_model:
                self.ai_models['aesthetic'] = aesthetic_model
                self.logger.info("âœ… ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ (Interface)")
                
        except Exception as e:
            self.logger.error(f"âŒ Interface ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _load_models_directly(self):
        """ì§ì ‘ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (í´ë°±)"""
        try:
            if TORCH_AVAILABLE:
                # ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸
                if self.model_paths.get('perceptual_quality'):
                    perceptual_model = RealPerceptualQualityModel(
                        pretrained_path=self.model_paths['perceptual_quality']
                    )
                    perceptual_model.to(self.device)
                    perceptual_model.eval()
                    self.ai_models['perceptual'] = perceptual_model
                    self.logger.info("âœ… ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ì§ì ‘ ë¡œë“œ")
                
                # ë¯¸ì  í’ˆì§ˆ ëª¨ë¸
                if self.model_paths.get('aesthetic_quality'):
                    aesthetic_model = RealAestheticQualityModel(
                        pretrained_path=self.model_paths['aesthetic_quality']
                    )
                    aesthetic_model.to(self.device)
                    aesthetic_model.eval()
                    self.ai_models['aesthetic'] = aesthetic_model
                    self.logger.info("âœ… ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ì§ì ‘ ë¡œë“œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _setup_fallback_models(self):
        """í´ë°± ëª¨ë¸ ì„¤ì •"""
        try:
            # ë”ë¯¸ ëª¨ë¸ë“¤ ìƒì„±
            self.ai_models['perceptual'] = RealPerceptualQualityModel()
            self.ai_models['aesthetic'] = RealAestheticQualityModel()
            
            self.logger.warning("âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš© - ì„±ëŠ¥ ì œí•œë¨")
        
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_assessment_pipeline(self):
        """í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„±"""
        try:
            self.assessment_pipeline = [
                ("ê¸°ìˆ ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_technical_quality),
                ("ì§€ê°ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_perceptual_quality_ai), 
                ("ë¯¸ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_aesthetic_quality_ai),
                ("ê¸°ëŠ¥ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_functional_quality),
                ("ìƒ‰ìƒ_í’ˆì§ˆ_ë¶„ì„", self._analyze_color_quality),
                ("ì¢…í•©_ì ìˆ˜_ê³„ì‚°", self._calculate_overall_quality),
                ("ë“±ê¸‰_ë¶€ì—¬", self._assign_quality_grade),
                ("ì‹œê°í™”_ìƒì„±", self._generate_quality_visualization)
            ]
            
            self.logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ: {len(self.assessment_pipeline)}ë‹¨ê³„")
        
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì‹¤íŒ¨: {e}")
            self.assessment_pipeline = []
    
    def _initialize_analyzers(self):
        """ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”"""
        try:
            # ê¸°ìˆ ì  ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(
                device=self.device,
                enable_gpu=TORCH_AVAILABLE and self.device != 'cpu'
            )
            
            self.logger.info("âœ… ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (process)
    # ==============================================
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ í•¨ìˆ˜
        âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  í¬í•¨
        âœ… 8ê°€ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
        âœ… ì¢…í•© ì ìˆ˜ ê³„ì‚°
        """
        
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ¯ {self.step_name} í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None or fitted_img.size == 0:  # NumPy ì¡°ê±´ë¬¸ ìˆ˜ì •
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ fitted_imageì…ë‹ˆë‹¤")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image else None
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            assessment_data = {
                'processed_image': fitted_img,
                'original_image': person_img,
                'clothing_image': clothing_img,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                **kwargs
            }
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if TORCH_AVAILABLE and self.optimization_enabled:
                self._optimize_memory()
            
            # 4. í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            for stage_name, stage_func in self.assessment_pipeline:
                self.logger.info(f"ğŸ”„ {stage_name} ì‹¤í–‰ ì¤‘...")
                
                stage_start = time.time()
                
                if asyncio.iscoroutinefunction(stage_func):
                    stage_result = await stage_func(assessment_data)
                else:
                    stage_result = stage_func(assessment_data)
                
                stage_duration = time.time() - stage_start
                
                # ê²°ê³¼ ë³‘í•©
                assessment_data.update(stage_result)
                
                self.logger.info(f"âœ… {stage_name} ì™„ë£Œ ({stage_duration:.2f}ì´ˆ)")
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            quality_metrics = assessment_data.get('quality_metrics')
            
            if quality_metrics is None:
                raise ValueError("í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
            
            result = {
                'success': True,
                'step_name': self.step_name,
                'overall_score': quality_metrics.overall_score,
                'confidence': quality_metrics.confidence,
                'grade': assessment_data.get('grade', 'acceptable'),
                'grade_description': assessment_data.get('grade_description', 'ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ'),
                
                # ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
                'detailed_scores': {
                    'technical': assessment_data.get('technical_results', {}),
                    'perceptual': assessment_data.get('perceptual_results', {}),
                    'aesthetic': assessment_data.get('aesthetic_results', {}),
                    'functional': assessment_data.get('functional_results', {}),
                    'color': assessment_data.get('color_results', {})
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì „ì²´
                'quality_metrics': asdict(quality_metrics),
                
                # ë©”íƒ€ë°ì´í„°
                'processing_time': processing_time,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'].value,
                
                # AI ëª¨ë¸ ì •ë³´
                'ai_models_used': list(self.ai_models.keys()),
                'device_info': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'optimization_enabled': self.optimization_enabled
                },
                
                # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
                'warnings': assessment_data.get('warnings', []),
                'recommendations': assessment_data.get('recommendations', [])
            }
            
            self.logger.info(f"âœ… {self.step_name} í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {result['overall_score']:.3f} ({processing_time:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'step_name': self.step_name,
                'error': str(e),
                'error_type': type(e).__name__,
                'processing_time': processing_time,
                'metadata': {
                    'device': self.device,
                    'pipeline_stages': len(self.assessment_pipeline),
                    'error_location': 'main_process'
                }
            }
    
    # ==============================================
    # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ë©”ì„œë“œë“¤
    # ==============================================
    async def _analyze_perceptual_quality_ai(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            original = data.get('original_image')
            
            # AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if 'perceptual' in self.ai_models and TORCH_AVAILABLE:
                model = self.ai_models['perceptual']
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed_tensor = self._preprocess_for_ai_model(image)
                
                # ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                with torch.no_grad():
                    if hasattr(model, 'forward'):
                        ai_result = model(processed_tensor)
                    else:
                        ai_result = model.predict(processed_tensor)
                
                # AI ê²°ê³¼ í•´ì„
                perceptual_scores = self._interpret_perceptual_ai_result(ai_result)
                
                self.logger.info("âœ… ì‹¤ì œ AI ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            else:
                # ì „í†µì  ë°©ë²•ìœ¼ë¡œ í´ë°±
                perceptual_scores = self._traditional_perceptual_analysis(image, original)
                self.logger.info("âœ… ì „í†µì  ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‚¬ìš©")
            
            return {
                'perceptual_results': perceptual_scores,
                'perceptual_score': perceptual_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'perceptual_results': {'error': str(e)},
                'perceptual_score': 0.3
            }
    
    async def _analyze_aesthetic_quality_ai(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            
            # AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if 'aesthetic' in self.ai_models and TORCH_AVAILABLE:
                model = self.ai_models['aesthetic']
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed_tensor = self._preprocess_for_ai_model(image)
                
                # ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                with torch.no_grad():
                    if hasattr(model, 'forward'):
                        ai_result = model(processed_tensor)
                    else:
                        ai_result = model.predict(processed_tensor)
                
                # AI ê²°ê³¼ í•´ì„
                aesthetic_scores = self._interpret_aesthetic_ai_result(ai_result)
                
                self.logger.info("âœ… ì‹¤ì œ AI ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            else:
                # ì „í†µì  ë°©ë²•ìœ¼ë¡œ í´ë°±
                aesthetic_scores = self._traditional_aesthetic_analysis(image)
                self.logger.info("âœ… ì „í†µì  ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‚¬ìš©")
            
            return {
                'aesthetic_results': aesthetic_scores,
                'aesthetic_score': aesthetic_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'aesthetic_results': {'error': str(e)},
                'aesthetic_score': 0.3
            }
    
    def _analyze_technical_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ (ì „í†µì  ë°©ë²•)"""
        try:
            image = data['processed_image']
            
            # ì „ë¬¸ ë¶„ì„ê¸° ì‚¬ìš©
            if self.technical_analyzer:
                technical_scores = self.technical_analyzer.analyze(image)
            else:
                # í´ë°± ë¶„ì„
                technical_scores = self._basic_technical_analysis(image)
            
            return {
                'technical_results': technical_scores,
                'technical_score': technical_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'technical_results': {'error': str(e)},
                'technical_score': 0.3
            }
    
    def _analyze_functional_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            clothing_type = data.get('clothing_type', 'default')
            
            # ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„
            functional_scores = self._basic_functional_analysis(image, clothing_type)
            
            return {
                'functional_results': functional_scores,
                'functional_score': functional_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'functional_results': {'error': str(e)},
                'functional_score': 0.3
            }
    
    def _analyze_color_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            
            # ê¸°ë³¸ ìƒ‰ìƒ ë¶„ì„
            color_scores = self._basic_color_analysis(image)
            
            return {
                'color_results': color_scores,
                'color_score': color_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_results': {'error': str(e)},
                'color_score': 0.3
            }
    
    def _calculate_overall_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê° ì˜ì—­ë³„ ì ìˆ˜ ìˆ˜ì§‘
            scores = {
                'technical': data.get('technical_score', 0.5),
                'perceptual': data.get('perceptual_score', 0.5),
                'aesthetic': data.get('aesthetic_score', 0.5),
                'functional': data.get('functional_score', 0.5),
                'color': data.get('color_score', 0.5)
            }
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weights = {
                'technical': 0.25,
                'perceptual': 0.25,
                'aesthetic': 0.20,
                'functional': 0.20,
                'color': 0.10
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            overall_score = sum(scores[key] * weights[key] for key in scores.keys())
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(data)
            
            # QualityMetrics ê°ì²´ ìƒì„±
            quality_metrics = QualityMetrics(
                overall_score=overall_score,
                confidence=confidence,
                sharpness_score=data.get('technical_results', {}).get('sharpness', 0.5),
                color_score=data.get('color_score', 0.5),
                fitting_score=data.get('functional_results', {}).get('fitting_accuracy', 0.5),
                realism_score=data.get('perceptual_score', 0.5),
                artifacts_score=data.get('technical_results', {}).get('artifacts', 0.5),
                alignment_score=data.get('functional_results', {}).get('clothing_alignment', 0.5),
                lighting_score=data.get('aesthetic_results', {}).get('lighting', 0.5),
                texture_score=data.get('aesthetic_results', {}).get('texture', 0.5),
                device_used=self.device,
                model_version="v17.0"
            )
            
            return {
                'quality_metrics': quality_metrics,
                'overall_score': overall_score,
                'confidence': confidence
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'quality_metrics': QualityMetrics(),
                'overall_score': 0.3,
                'confidence': 0.1
            }
    
    def _assign_quality_grade(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë“±ê¸‰ ë¶€ì—¬"""
        try:
            overall_score = data.get('overall_score', 0.5)
            
            if overall_score >= 0.9:
                grade = QualityGrade.EXCELLENT
                description = "íƒì›”í•œ í’ˆì§ˆ - ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥"
            elif overall_score >= 0.75:
                grade = QualityGrade.GOOD  
                description = "ì¢‹ì€ í’ˆì§ˆ - ì¼ë°˜ì  ì‚¬ìš© ì í•©"
            elif overall_score >= 0.6:
                grade = QualityGrade.ACCEPTABLE
                description = "ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ - ê°œì„  ê¶Œì¥"
            elif overall_score >= 0.4:
                grade = QualityGrade.POOR
                description = "ë¶ˆëŸ‰í•œ í’ˆì§ˆ - ìƒë‹¹í•œ ê°œì„  í•„ìš”"
            else:
                grade = QualityGrade.FAILED
                description = "ì‹¤íŒ¨í•œ í’ˆì§ˆ - ì¬ì²˜ë¦¬ í•„ìš”"
            
            return {
                'grade': grade.value,
                'grade_description': description
            }
        
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ë“±ê¸‰ ë¶€ì—¬ ì‹¤íŒ¨: {e}")
            return {
                'grade': QualityGrade.ACCEPTABLE.value,
                'grade_description': "ë“±ê¸‰ ê³„ì‚° ì‹¤íŒ¨"
            }
    
    def _generate_quality_visualization(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ ì‹œê°í™” ìƒì„±"""
        try:
            # ê¸°ë³¸ ì‹œê°í™” ì •ë³´ë§Œ ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±ì€ ì„ íƒì‚¬í•­)
            visualization_info = {
                'has_visualization': True,
                'quality_chart': f"í’ˆì§ˆ ì ìˆ˜: {data.get('overall_score', 0.5):.3f}",
                'grade_display': data.get('grade_description', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                'detailed_breakdown': data.get('detailed_scores', {}),
                'ai_models_used': list(self.ai_models.keys()) if hasattr(self, 'ai_models') else []
            }
            
            return {
                'visualization': visualization_info
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'visualization': {'error': str(e)}
            }
    
    # ==============================================
    # ğŸ”¥ AI ëª¨ë¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    def _preprocess_for_ai_model(self, image: np.ndarray) -> Any:
        """AI ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if TORCH_AVAILABLE:
                # NumPy to Tensor ë³€í™˜
                if len(image.shape) == 3:
                    # RGB ì´ë¯¸ì§€ ì²˜ë¦¬
                    tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                else:
                    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì²˜ë¦¬
                    tensor = torch.from_numpy(image).float() / 255.0
                    tensor = tensor.unsqueeze(0)  # ì±„ë„ ì°¨ì› ì¶”ê°€
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                tensor = tensor.unsqueeze(0)
                
                # í‘œì¤€í™” (ImageNet ê¸°ì¤€)
                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
                
                if tensor.shape[1] == 3:  # RGBì¸ ê²½ìš°ë§Œ ì •ê·œí™”
                    tensor = (tensor - mean) / std
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if self.device != 'cpu' and torch.cuda.is_available():
                    tensor = tensor.to(self.device)
                elif self.device == 'mps' and torch.backends.mps.is_available():
                    tensor = tensor.to('mps')
                
                return tensor
            else:
                # PyTorch ì—†ì„ ë•ŒëŠ” NumPy ë°°ì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                return image / 255.0 if image.max() > 1.0 else image
        
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ìš© ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _interpret_perceptual_ai_result(self, ai_result: Any) -> Dict[str, Any]:
        """ì§€ê°ì  í’ˆì§ˆ AI ê²°ê³¼ í•´ì„"""
        try:
            if isinstance(ai_result, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê²°ê³¼ ì²˜ë¦¬
                overall_quality = ai_result.get('overall_quality', 0.7)
                perceptual_distance = ai_result.get('perceptual_distance', 0.3)
                
                if TORCH_AVAILABLE and hasattr(overall_quality, 'cpu'):
                    overall_quality = float(overall_quality.cpu().item())
                    perceptual_distance = float(perceptual_distance.cpu().item())
                elif hasattr(overall_quality, 'item'):
                    overall_quality = float(overall_quality.item())
                    perceptual_distance = float(perceptual_distance.item())
                
                return {
                    'overall_score': max(0.0, min(1.0, overall_quality)),
                    'visual_quality': overall_quality,
                    'structural_similarity': 1.0 - perceptual_distance,
                    'perceptual_distance': perceptual_distance,
                    'analysis_method': 'ai_model_lpips'
                }
            else:
                # ë‹¨ì¼ ê°’ ê²°ê³¼ ì²˜ë¦¬
                result_data = safe_tensor_to_numpy(ai_result)
                if isinstance(result_data, np.ndarray):
                    overall_score = float(np.mean(result_data))
                else:
                    overall_score = float(result_data) if isinstance(result_data, (int, float)) else 0.7
                
                return {
                    'overall_score': max(0.0, min(1.0, overall_score)),
                    'visual_quality': overall_score,
                    'structural_similarity': overall_score * 0.95 + 0.05,
                    'perceptual_distance': 1.0 - overall_score,
                    'analysis_method': 'ai_model_simple'
                }
        
        except Exception as e:
            self.logger.error(f"âŒ ì§€ê°ì  AI ê²°ê³¼ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._traditional_perceptual_analysis(None, None)
    
    def _interpret_aesthetic_ai_result(self, ai_result: Any) -> Dict[str, Any]:
        """ë¯¸ì  í’ˆì§ˆ AI ê²°ê³¼ í•´ì„"""
        try:
            if isinstance(ai_result, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê²°ê³¼ ì²˜ë¦¬
                scores = {}
                for key, value in ai_result.items():
                    if TORCH_AVAILABLE and hasattr(value, 'cpu'):
                        scores[key] = float(value.cpu().item())
                    elif hasattr(value, 'item'):
                        scores[key] = float(value.item())
                    else:
                        scores[key] = float(value)
                
                overall_score = scores.get('overall', np.mean(list(scores.values())))
                
                return {
                    'overall_score': max(0.0, min(1.0, overall_score)),
                    'composition': scores.get('composition', overall_score * 0.9 + 0.1),
                    'lighting': scores.get('lighting', overall_score * 0.95 + 0.05),
                    'color_harmony': scores.get('color_harmony', overall_score * 0.8 + 0.2),
                    'balance': scores.get('balance', overall_score * 0.85 + 0.15),
                    'symmetry': scores.get('symmetry', overall_score * 0.8 + 0.2),
                    'analysis_method': 'ai_model_aesthetic'
                }
            else:
                # ë‹¨ì¼ ê°’ ê²°ê³¼ ì²˜ë¦¬
                result_data = safe_tensor_to_numpy(ai_result)
                if isinstance(result_data, np.ndarray):
                    overall_score = float(np.mean(result_data))
                else:
                    overall_score = float(result_data) if isinstance(result_data, (int, float)) else 0.7
                
                return {
                    'overall_score': max(0.0, min(1.0, overall_score)),
                    'composition': overall_score * 0.9 + 0.1,
                    'lighting': overall_score * 0.95 + 0.05,
                    'color_harmony': overall_score * 0.8 + 0.2,
                    'balance': overall_score * 0.85 + 0.15,
                    'symmetry': overall_score * 0.8 + 0.2,
                    'analysis_method': 'ai_model_simple'
                }
        
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ì  AI ê²°ê³¼ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._traditional_aesthetic_analysis(None)
    
    # ==============================================
    # ğŸ”¥ ì „í†µì  ë¶„ì„ ë©”ì„œë“œë“¤ (AI ëª¨ë¸ ì—†ì„ ë•Œ)
    # ==============================================
    def _traditional_perceptual_analysis(self, image1: Optional[np.ndarray], image2: Optional[np.ndarray]) -> Dict[str, Any]:
        """ì „í†µì  ì§€ê°ì  ë¶„ì„ ë°©ë²•"""
        try:
            if image1 is None or image1.size == 0:
                return {
                    'overall_score': 0.5,
                    'visual_quality': 0.5,
                    'structural_similarity': 0.5,
                    'perceptual_distance': 0.5,
                    'analysis_method': 'fallback'
                }
            
            # SSIM ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
            if image2 is not None and SKIMAGE_AVAILABLE:
                try:
                    # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
                    if image1.shape != image2.shape:
                        min_h = min(image1.shape[0], image2.shape[0])
                        min_w = min(image1.shape[1], image2.shape[1])
                        image1 = image1[:min_h, :min_w]
                        image2 = image2[:min_h, :min_w]
                    
                    # SSIM ê³„ì‚°
                    if len(image1.shape) == 3:
                        ssim_score = ssim(image1, image2, multichannel=True, channel_axis=2)
                    else:
                        ssim_score = ssim(image1, image2)
                    
                    overall_score = max(0.0, ssim_score)
                except Exception:
                    overall_score = 0.7
            else:
                # ê°„ë‹¨í•œ í†µê³„ì  ë¶„ì„
                mean_brightness = np.mean(image1) / 255.0 if image1.max() > 1.0 else np.mean(image1)
                brightness_score = 1.0 - abs(mean_brightness - 0.5) * 2  # 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
                
                # ëŒ€ë¹„ ë¶„ì„
                contrast = np.std(image1) / 255.0 if image1.max() > 1.0 else np.std(image1)
                contrast_score = min(1.0, contrast * 2)  # ì ì ˆí•œ ëŒ€ë¹„
                
                overall_score = (brightness_score + contrast_score) / 2
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'visual_quality': overall_score,
                'structural_similarity': overall_score * 0.9 + 0.1,
                'perceptual_distance': 1.0 - overall_score,
                'analysis_method': 'traditional_ssim'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ì§€ê° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'visual_quality': 0.5,
                'structural_similarity': 0.5,
                'perceptual_distance': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    def _traditional_aesthetic_analysis(self, image: Optional[np.ndarray]) -> Dict[str, Any]:
        """ì „í†µì  ë¯¸ì  ë¶„ì„ ë°©ë²•"""
        try:
            if image is None or image.size == 0:
                return {
                    'overall_score': 0.5,
                    'composition': 0.5,
                    'lighting': 0.5,
                    'color_harmony': 0.5,
                    'balance': 0.5,
                    'symmetry': 0.5,
                    'analysis_method': 'fallback'
                }
            
            # ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
            if len(image.shape) == 3:
                color_std = np.mean([np.std(image[:,:,i]) for i in range(3)]) / 255.0
            else:
                color_std = np.std(image) / 255.0
            
            color_harmony = min(1.0, color_std * 1.5)
            
            # ë°ê¸° ë¶„í¬ ë¶„ì„
            brightness = np.mean(image) / 255.0 if image.max() > 1.0 else np.mean(image)
            lighting_score = 1.0 - abs(brightness - 0.5) * 1.5
            lighting_score = max(0.0, min(1.0, lighting_score))
            
            # ëŒ€ì¹­ì„± ë¶„ì„
            height, width = image.shape[:2]
            left_half = image[:, :width//2]
            right_half = image[:, width//2:]
            right_half_flipped = np.flip(right_half, axis=1)
            
            # í¬ê¸° ë§ì¶”ê¸°
            min_width = min(left_half.shape[1], right_half_flipped.shape[1])
            left_half = left_half[:, :min_width]
            right_half_flipped = right_half_flipped[:, :min_width]
            
            # ëŒ€ì¹­ì„± ì ìˆ˜
            symmetry_diff = np.mean(np.abs(left_half.astype(float) - right_half_flipped.astype(float)))
            symmetry_score = max(0.0, 1.0 - symmetry_diff / 128.0)
            
            # êµ¬ì„± ì ìˆ˜ (ì—£ì§€ ë¶„í¬ ê¸°ë°˜)
            if OPENCV_AVAILABLE:
                if len(image.shape) == 3:
                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                else:
                    gray = image
                edges = cv2.Canny(gray.astype(np.uint8), 50, 150)
                edge_density = np.sum(edges > 0) / edges.size
                composition_score = min(1.0, edge_density * 10)  # ì ì ˆí•œ ì—£ì§€ ë°€ë„
            else:
                composition_score = 0.7  # ê¸°ë³¸ê°’
            
            overall_score = np.mean([color_harmony, lighting_score, symmetry_score, composition_score])
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'composition': composition_score,
                'lighting': lighting_score,
                'color_harmony': color_harmony,
                'balance': (composition_score + symmetry_score) / 2,
                'symmetry': symmetry_score,
                'analysis_method': 'traditional_aesthetic'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ë¯¸ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'composition': 0.5,
                'lighting': 0.5,
                'color_harmony': 0.5,
                'balance': 0.5,
                'symmetry': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    # ==============================================
    # ğŸ”¥ í´ë°± ë¶„ì„ ë©”ì„œë“œë“¤
    # ==============================================
    def _basic_technical_analysis(self, image: np.ndarray) -> Dict[str, Any]:
        """ê¸°ë³¸ ê¸°ìˆ ì  ë¶„ì„ (í´ë°±)"""
        try:
            if image is None or image.size == 0:
                return {
                    'overall_score': 0.5,
                    'sharpness': 0.5,
                    'artifacts': 0.7,
                    'noise_level': 0.6,
                    'contrast': 0.5,
                    'brightness': 0.5,
                    'analysis_method': 'error_fallback'
                }
            
            # ê°„ë‹¨í•œ ì„ ëª…ë„ ì¸¡ì •
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì„ ëª…ë„ ê³„ì‚° (Laplacian ë¶„ì‚°)
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                sharpness = laplacian.var() / 10000.0  # ì •ê·œí™”
            else:
                # OpenCV ì—†ì„ ë•Œ ê°„ë‹¨í•œ gradient ê³„ì‚°
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                sharpness = (np.var(dx) + np.var(dy)) / 20000.0
            
            sharpness = max(0.0, min(1.0, sharpness))
            
            # ëŒ€ë¹„ ê³„ì‚°
            contrast = np.std(gray) / 255.0 if gray.max() > 1.0 else np.std(gray)
            contrast_score = min(1.0, contrast * 2)
            
            # ë°ê¸° ê³„ì‚°
            brightness = np.mean(gray) / 255.0 if gray.max() > 1.0 else np.mean(gray)
            brightness_score = 1.0 - abs(brightness - 0.5) * 2
            brightness_score = max(0.0, min(1.0, brightness_score))
            
            # ë…¸ì´ì¦ˆ ë ˆë²¨ (ê°„ë‹¨í•œ ì¶”ì •)
            noise_level = min(1.0, np.std(gray) / 50.0)
            noise_score = 1.0 - noise_level
            
            overall_score = (sharpness + contrast_score + brightness_score + noise_score) / 4
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'sharpness': sharpness,
                'artifacts': 0.8,  # ê¸°ë³¸ê°’
                'noise_level': noise_score,
                'contrast': contrast_score,
                'brightness': brightness_score,
                'analysis_method': 'basic_technical'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ê¸°ìˆ  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'sharpness': 0.5,
                'artifacts': 0.7,
                'noise_level': 0.6,
                'contrast': 0.5,
                'brightness': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    def _basic_functional_analysis(self, image: np.ndarray, clothing_type: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„ (í´ë°±)"""
        try:
            if image is None or image.size == 0:
                return {
                    'fitting_accuracy': 0.5,
                    'clothing_alignment': 0.5,
                    'naturalness': 0.5,
                    'overall_score': 0.5,
                    'analysis_method': 'error_fallback'
                }
            
            # ê°„ë‹¨í•œ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì²´í¬
            height, width = image.shape[:2]
            aspect_ratio = width / height
            
            # ì˜ë¥˜ íƒ€ì…ë³„ ê¸°ëŒ€ ë¹„ìœ¨
            expected_ratios = {
                'shirt': (0.7, 1.3),
                'dress': (0.6, 1.0),
                'pants': (0.8, 1.2),
                'jacket': (0.6, 1.2),
                'top': (0.7, 1.4),
                'default': (0.5, 1.5)
            }
            
            min_ratio, max_ratio = expected_ratios.get(clothing_type, expected_ratios['default'])
            
            if min_ratio <= aspect_ratio <= max_ratio:
                fitting_score = 1.0
            else:
                center_ratio = (min_ratio + max_ratio) / 2
                fitting_score = max(0.0, 1.0 - abs(aspect_ratio - center_ratio) * 2)
            
            # ì •ë ¬ ë¶„ì„ (ì¢Œìš° ëŒ€ì¹­ì„±)
            left_half = image[:, :width//2]
            right_half = image[:, width//2:]
            right_half_flipped = np.flip(right_half, axis=1)
            
            min_width = min(left_half.shape[1], right_half_flipped.shape[1])
            left_half = left_half[:, :min_width]
            right_half_flipped = right_half_flipped[:, :min_width]
            
            diff = np.mean(np.abs(left_half.astype(float) - right_half_flipped.astype(float)))
            alignment_score = max(0.0, 1.0 - diff / 128.0)
            
            # ìì—°ìŠ¤ëŸ¬ì›€ (ìƒ‰ìƒ ë¶„í¬ ê¸°ë°˜)
            if len(image.shape) == 3:
                color_variance = np.mean([np.var(image[:,:,i]) for i in range(3)])
                naturalness = min(1.0, color_variance / (255.0 * 255.0) * 10)
            else:
                naturalness = 0.7
            
            overall_score = (fitting_score + alignment_score + naturalness) / 3
            
            return {
                'fitting_accuracy': fitting_score,
                'clothing_alignment': alignment_score,
                'naturalness': naturalness,
                'overall_score': max(0.0, min(1.0, overall_score)),
                'analysis_method': 'basic_functional'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'fitting_accuracy': 0.5,
                'clothing_alignment': 0.5,
                'naturalness': 0.5,
                'overall_score': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    def _basic_color_analysis(self, image: np.ndarray) -> Dict[str, Any]:
        """ê¸°ë³¸ ìƒ‰ìƒ ë¶„ì„ (í´ë°±)"""
        try:
            if image is None or image.size == 0:
                return {
                    'color_consistency': 0.5,
                    'color_naturalness': 0.5,
                    'color_contrast': 0.5,
                    'color_harmony': 0.5,
                    'overall_score': 0.5,
                    'analysis_method': 'error_fallback'
                }
            
            if len(image.shape) == 3:
                # RGB ì±„ë„ë³„ ë¶„ì„
                color_means = [np.mean(image[:,:,i]) for i in range(3)]
                color_stds = [np.std(image[:,:,i]) for i in range(3)]
                
                # ìƒ‰ìƒ ì¼ê´€ì„± (ì±„ë„ ê°„ ê· í˜•)
                consistency = 1.0 - np.std(color_means) / (np.mean(color_means) + 1e-8)
                consistency = max(0.0, min(1.0, consistency))
                
                # ìƒ‰ìƒ ëŒ€ë¹„
                contrast = np.mean(color_stds) / 255.0 if image.max() > 1.0 else np.mean(color_stds)
                contrast_score = min(1.0, contrast * 2)
                
                # ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ (í¬í™”ë„ ê¸°ë°˜)
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = np.mean((max_vals - min_vals) / (max_vals + 1e-8))
                naturalness = min(1.0, saturation * 1.5)
                
                # ìƒ‰ìƒ ì¡°í™” (ë¶„ì‚° ê¸°ë°˜)
                harmony = min(1.0, np.mean(color_stds) / 64.0)
                
                overall_score = (consistency + contrast_score + naturalness + harmony) / 4
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                consistency = 0.6
                contrast_score = min(1.0, np.std(image) / 64.0)
                naturalness = 0.6
                harmony = 0.6
                overall_score = (consistency + contrast_score + naturalness + harmony) / 4
            
            return {
                'color_consistency': consistency,
                'color_naturalness': naturalness,
                'color_contrast': contrast_score,
                'color_harmony': harmony,
                'overall_score': max(0.0, min(1.0, overall_score)),
                'analysis_method': 'basic_color'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_consistency': 0.5,
                'color_naturalness': 0.5,
                'color_contrast': 0.5,
                'color_harmony': 0.5,
                'overall_score': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if image_input is None:
                return None
            
            if isinstance(image_input, np.ndarray):
                # NumPy ë°°ì—´ ê²€ì¦
                if image_input.size == 0:
                    self.logger.warning(f"âŒ ë¹ˆ ì´ë¯¸ì§€ ë°°ì—´: {name}")
                    return None
                return image_input
            elif isinstance(image_input, (str, Path)):
                image_path = Path(image_input)
                if image_path.exists():
                    if PIL_AVAILABLE:
                        from PIL import Image
                        with Image.open(image_path) as img:
                            return np.array(img)
                    elif OPENCV_AVAILABLE:
                        img = cv2.imread(str(image_path))
                        if img is not None:
                            return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            self.logger.warning(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {name}")
            return None
        
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜ {name}: {e}")
            return None
    
    def _calculate_confidence(self, data: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_factors = []
            
            # 1. AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            ai_model_count = len(self.ai_models) if hasattr(self, 'ai_models') else 0
            ai_confidence = min(1.0, ai_model_count / 2.0)  # ìµœëŒ€ 2ê°œ ëª¨ë¸ (ì§€ê°ì , ë¯¸ì )
            confidence_factors.append(ai_confidence)
            
            # 2. ì…ë ¥ ë°ì´í„° í’ˆì§ˆ
            has_original = data.get('original_image') is not None
            has_clothing = data.get('clothing_image') is not None
            data_quality = (0.5 + 0.3 * has_original + 0.2 * has_clothing)
            confidence_factors.append(data_quality)
            
            # 3. ë¶„ì„ ê²°ê³¼ ì¼ê´€ì„±
            scores = [
                data.get('technical_score', 0.5),
                data.get('perceptual_score', 0.5),
                data.get('aesthetic_score', 0.5),
                data.get('functional_score', 0.5),
                data.get('color_score', 0.5)
            ]
            score_std = np.std(scores)
            consistency = max(0.0, 1.0 - score_std * 2)  # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
            confidence_factors.append(consistency)
            
            # 4. ì‹œìŠ¤í…œ ìµœì í™” ìƒíƒœ
            optimization_factor = 0.9 if self.optimization_enabled else 0.7
            confidence_factors.append(optimization_factor)
            
            # 5. ì—ëŸ¬ ë°œìƒ ì—¬ë¶€
            error_factor = max(0.3, 1.0 - self.error_count * 0.1)
            confidence_factors.append(error_factor)
            
            return max(0.1, min(1.0, np.mean(confidence_factors)))
        
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    # ==============================================
    # ğŸ”¥ ì‹œìŠ¤í…œ ìµœì í™” ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    def _optimize_for_m3_max(self):
        """M3 Max ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE and self.device == "mps":
                # MPS ìµœì í™” ì„¤ì •
                if hasattr(torch.mps, 'set_high_watermark_ratio'):
                    torch.mps.set_high_watermark_ratio(0.0)
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„¤ì •
                if hasattr(torch.backends.mps, 'set_max_memory_allocation'):
                    max_memory = int(self.memory_gb * 0.8 * 1024**3)  # 80% ì‚¬ìš©
                    torch.backends.mps.set_max_memory_allocation(max_memory)
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    safe_mps_empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ìµœì í™” ì˜¤ë¥˜: {e}")
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
                self.ai_models.clear()
            
            # StepModelInterface ì •ë¦¬
            if hasattr(self, 'model_interface') and self.model_interface:
                try:
                    if hasattr(self.model_interface, 'cleanup'):
                        self.model_interface.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ StepModelInterface ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë¶„ì„ê¸° ì •ë¦¬
            for analyzer_name in ['technical_analyzer', 'perceptual_analyzer', 'aesthetic_analyzer']:
                analyzer = getattr(self, analyzer_name, None)
                if analyzer and hasattr(analyzer, 'cleanup'):
                    try:
                        analyzer.cleanup()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {analyzer_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
                setattr(self, analyzer_name, None)
            
            # íŒŒì´í”„ë¼ì¸ ì •ë¦¬
            self.assessment_pipeline.clear()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory()
            
            self.is_initialized = False
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': self.step_number,
            'device': self.device,
            'device_type': getattr(self, 'device_type', 'Unknown'),
            'memory_gb': getattr(self, 'memory_gb', 0),
            'is_m3_max': getattr(self, 'is_m3_max', False),
            'ai_models_loaded': len(self.ai_models) if hasattr(self, 'ai_models') else 0,
            'ai_models_available': list(self.ai_models.keys()) if hasattr(self, 'ai_models') else [],
            'assessment_modes': [mode.value for mode in AssessmentMode],
            'quality_threshold': self.quality_threshold,
            'pipeline_stages': len(self.assessment_pipeline),
            'optimization_enabled': self.optimization_enabled,
            'is_initialized': self.is_initialized,
            'model_interface_available': hasattr(self, 'model_interface') and self.model_interface is not None,
            'base_step_mixin_available': BASE_STEP_MIXIN_AVAILABLE,
            'dependency_manager_available': hasattr(self, 'dependency_manager') and self.dependency_manager is not None,
            'torch_available': TORCH_AVAILABLE,
            'opencv_available': OPENCV_AVAILABLE,
            'pil_available': PIL_AVAILABLE,
            'skimage_available': SKIMAGE_AVAILABLE,
            'sklearn_available': SKLEARN_AVAILABLE,
            'error_count': self.error_count,
            'last_error': self.last_error
        }
    
    def get_ai_model_info(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        try:
            model_info = {}
            
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    info = {
                        'loaded': True,
                        'device': str(next(model.parameters()).device) if hasattr(model, 'parameters') else 'unknown',
                        'type': type(model).__name__,
                        'parameters': sum(p.numel() for p in model.parameters()) if hasattr(model, 'parameters') else 0
                    }
                    model_info[model_name] = info
            
            return {
                'ai_models': model_info,
                'total_models': len(model_info),
                'torch_available': TORCH_AVAILABLE,
                'device': self.device
            }
        
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================
def create_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return QualityAssessmentStep(device=device, config=config, **kwargs)

async def create_and_initialize_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± ë° ì´ˆê¸°í™”"""
    step = QualityAssessmentStep(device=device, config=config, **kwargs)
    await step.initialize()
    return step

def create_quality_assessment_with_checkpoints(
    perceptual_model_path: Optional[str] = None,
    aesthetic_model_path: Optional[str] = None,
    device: str = "auto",
    **kwargs
) -> QualityAssessmentStep:
    """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì§€ì •í•œ í’ˆì§ˆ í‰ê°€ Step ìƒì„±"""
    config = {
        'perceptual_model_path': perceptual_model_path,
        'aesthetic_model_path': aesthetic_model_path,
        'enable_ai_models': True,
        **kwargs.get('config', {})
    }
    
    return QualityAssessmentStep(device=device, config=config, **kwargs)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
# ==============================================
__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'QualityAssessmentStep',
    
    # ë°ì´í„° êµ¬ì¡°
    'QualityMetrics',
    'QualityGrade', 
    'AssessmentMode',
    'QualityAspect',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'RealPerceptualQualityModel',
    'RealAestheticQualityModel',
    
    # ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤
    'TechnicalQualityAnalyzer',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_quality_assessment_step',
    'create_and_initialize_quality_assessment_step',
    'create_quality_assessment_with_checkpoints',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_tensor_to_numpy'
]

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================
if __name__ == "__main__":
    async def test_quality_assessment_step():
        """í’ˆì§ˆ í‰ê°€ Step í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ§ª QualityAssessmentStep v17.0 í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # Step ìƒì„±
            step = QualityAssessmentStep(device="auto")
            
            # ê¸°ë³¸ ì†ì„± í™•ì¸
            assert hasattr(step, 'logger'), "logger ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'process'), "process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'cleanup_resources'), "cleanup_resources ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'initialize'), "initialize ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            
            # Step ì •ë³´ í™•ì¸
            step_info = step.get_step_info()
            assert 'step_name' in step_info, "step_nameì´ step_infoì— ì—†ìŠµë‹ˆë‹¤!"
            
            # AI ëª¨ë¸ ì •ë³´ í™•ì¸
            ai_model_info = step.get_ai_model_info()
            assert 'ai_models' in ai_model_info, "ai_modelsê°€ ai_model_infoì— ì—†ìŠµë‹ˆë‹¤!"
            
            print("âœ… QualityAssessmentStep v17.0 í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            print(f"ğŸ“Š Step ì •ë³´: {step_info}")
            print(f"ğŸ§  AI ëª¨ë¸ ì •ë³´: {ai_model_info}")
            print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {step.device}")
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {step_info.get('memory_gb', 0)}GB")
            print(f"ğŸ M3 Max: {'âœ…' if step_info.get('is_m3_max', False) else 'âŒ'}")
            print(f"ğŸ§  BaseStepMixin: {'âœ…' if step_info.get('base_step_mixin_available', False) else 'âŒ'}")
            print(f"ğŸ”Œ DependencyManager: {'âœ…' if step_info.get('dependency_manager_available', False) else 'âŒ'}")
            print(f"ğŸ¯ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {step_info.get('pipeline_stages', 0)}")
            print(f"ğŸš€ AI ëª¨ë¸ ë¡œë“œë¨: {step_info.get('ai_models_loaded', 0)}ê°œ")
            print(f"ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
            print(f"   - PyTorch: {'âœ…' if step_info.get('torch_available', False) else 'âŒ'}")
            print(f"   - OpenCV: {'âœ…' if step_info.get('opencv_available', False) else 'âŒ'}")
            print(f"   - PIL: {'âœ…' if step_info.get('pil_available', False) else 'âŒ'}")
            print(f"   - scikit-image: {'âœ…' if step_info.get('skimage_available', False) else 'âŒ'}")
            print(f"   - scikit-learn: {'âœ…' if step_info.get('sklearn_available', False) else 'âŒ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ QualityAssessmentStep v17.0 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_quality_assessment_step())