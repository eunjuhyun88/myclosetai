# app/ai_pipeline/steps/step_08_quality_assessment.py
"""
âœ… MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„
âœ… AI ëª¨ë¸ ë¡œë”ì™€ ì™„ë²½ ì—°ë™
âœ… Pipeline Manager 100% í˜¸í™˜
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë“  í’ˆì§ˆ í‰ê°€ ê¸°ëŠ¥
âœ… í†µì¼ëœ ìƒì„±ì íŒ¨í„´

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache
import numpy as np
import base64
import io

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ - ì•ˆì „í•œ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch í•„ìˆ˜: pip install torch torchvision")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âŒ OpenCV í•„ìˆ˜: pip install opencv-python")

try:
    from PIL import Image, ImageStat, ImageEnhance, ImageFilter, ImageDraw
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âŒ Pillow í•„ìˆ˜: pip install Pillow")

try:
    from scipy import stats, ndimage, spatial
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ê¶Œì¥: pip install scipy")

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ Scikit-learn ê¶Œì¥: pip install scikit-learn")

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False
    print("âš ï¸ Scikit-image ê¶Œì¥: pip install scikit-image")

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"      # 90-100ì 
    GOOD = "good"               # 75-89ì 
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

class AssessmentMode(Enum):
    """í‰ê°€ ëª¨ë“œ"""
    FAST = "fast"              # ë¹ ë¥¸ ê¸°ë³¸ í‰ê°€
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© í‰ê°€
    DETAILED = "detailed"      # ìƒì„¸ ë¶„ì„
    NEURAL = "neural"          # AI ê¸°ë°˜ í‰ê°€

class QualityAspect(Enum):
    """í’ˆì§ˆ ì¸¡ë©´"""
    TECHNICAL = "technical"    # ê¸°ìˆ ì  í’ˆì§ˆ
    PERCEPTUAL = "perceptual"  # ì§€ê°ì  í’ˆì§ˆ
    AESTHETIC = "aesthetic"    # ë¯¸ì  í’ˆì§ˆ
    FUNCTIONAL = "functional"  # ê¸°ëŠ¥ì  í’ˆì§ˆ

# ==============================================
# ğŸ”¥ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    
    # ê¸°ìˆ ì  í’ˆì§ˆ
    sharpness: float = 0.0
    noise_level: float = 0.0
    contrast: float = 0.0
    saturation: float = 0.0
    brightness: float = 0.0
    color_accuracy: float = 0.0
    
    # ì§€ê°ì  í’ˆì§ˆ
    structural_similarity: float = 0.0
    perceptual_similarity: float = 0.0
    visual_quality: float = 0.0
    artifact_level: float = 0.0
    
    # ë¯¸ì  í’ˆì§ˆ
    composition: float = 0.0
    color_harmony: float = 0.0
    symmetry: float = 0.0
    balance: float = 0.0
    
    # ê¸°ëŠ¥ì  í’ˆì§ˆ
    fitting_quality: float = 0.0
    edge_preservation: float = 0.0
    texture_quality: float = 0.0
    detail_preservation: float = 0.0
    
    # ì „ì²´ ì ìˆ˜
    overall_score: float = 0.0
    confidence: float = 0.0
    
    def calculate_overall_score(self, weights: Optional[Dict[str, float]] = None) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        if weights is None:
            weights = {
                'technical': 0.3,
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.2
            }
        
        technical_score = np.mean([
            self.sharpness, self.contrast, self.color_accuracy,
            1.0 - self.noise_level  # ë…¸ì´ì¦ˆëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        ])
        
        perceptual_score = np.mean([
            self.structural_similarity, self.perceptual_similarity,
            self.visual_quality, 1.0 - self.artifact_level
        ])
        
        aesthetic_score = np.mean([
            self.composition, self.color_harmony,
            self.symmetry, self.balance
        ])
        
        functional_score = np.mean([
            self.fitting_quality, self.edge_preservation,
            self.texture_quality, self.detail_preservation
        ])
        
        self.overall_score = (
            technical_score * weights['technical'] +
            perceptual_score * weights['perceptual'] +
            aesthetic_score * weights['aesthetic'] +
            functional_score * weights['functional']
        )
        
        return self.overall_score
    
    def get_grade(self) -> QualityGrade:
        """ë“±ê¸‰ ë°˜í™˜"""
        score = self.overall_score * 100
        
        if score >= 90:
            return QualityGrade.EXCELLENT
        elif score >= 75:
            return QualityGrade.GOOD
        elif score >= 60:
            return QualityGrade.ACCEPTABLE
        elif score >= 40:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class PerceptualQualityModel(nn.Module):
    """ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
    
    def __init__(self):
        super().__init__()
        
        # CNN ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8))
        )
        
        # í’ˆì§ˆ ì˜ˆì¸¡ê¸°
        self.quality_predictor = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        quality = self.quality_predictor(features)
        return quality

class AestheticQualityModel(nn.Module):
    """ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
    
    def __init__(self):
        super().__init__()
        
        # ResNet ê¸°ë°˜ ë°±ë³¸
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet ë¸”ë¡ë“¤
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ë¯¸ì  ì ìˆ˜ ì˜ˆì¸¡ê¸°
        self.aesthetic_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 4),  # composition, harmony, symmetry, balance
            nn.Sigmoid()
        )
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        layers.append(self._make_resnet_block(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(self._make_resnet_block(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _make_resnet_block(self, in_channels, out_channels, stride=1):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        aesthetic_scores = self.aesthetic_head(features)
        return aesthetic_scores

class TechnicalQualityAnalyzer:
    """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
    
    def analyze_sharpness(self, image: np.ndarray) -> float:
        """ì„ ëª…ë„ ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                # ì •ê·œí™” (0-1 ë²”ìœ„)
                return min(laplacian_var / 1000.0, 1.0)
            else:
                # PIL ê¸°ë°˜ ê·¼ì‚¬ì¹˜
                pil_img = Image.fromarray(image).convert('L')
                edges = pil_img.filter(ImageFilter.FIND_EDGES)
                stat = ImageStat.Stat(edges)
                return min(stat.stddev[0] / 50.0, 1.0)
        except Exception:
            return 0.5
    
    def analyze_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
                kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                filtered = cv2.filter2D(gray, -1, kernel)
                noise_level = np.std(filtered) / 255.0
                return min(noise_level, 1.0)
            else:
                # ê°„ë‹¨í•œ í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì¶”ì •
                gray = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                return min(np.std(gray) / 128.0, 1.0)
        except Exception:
            return 0.3
    
    def analyze_contrast(self, image: np.ndarray) -> float:
        """ëŒ€ë¹„ ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                contrast = gray.std() / 128.0
            else:
                gray = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                contrast = gray.std() / 128.0
            
            return min(contrast, 1.0)
        except Exception:
            return 0.5
    
    def analyze_color_accuracy(self, original: np.ndarray, processed: np.ndarray) -> float:
        """ìƒ‰ìƒ ì •í™•ë„ ë¶„ì„"""
        try:
            # RGB íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
            hist_orig = [cv2.calcHist([original], [i], None, [256], [0, 256]) for i in range(3)] if CV2_AVAILABLE else []
            hist_proc = [cv2.calcHist([processed], [i], None, [256], [0, 256]) for i in range(3)] if CV2_AVAILABLE else []
            
            if hist_orig and hist_proc:
                correlations = [cv2.compareHist(hist_orig[i], hist_proc[i], cv2.HISTCMP_CORREL) for i in range(3)]
                return np.mean(correlations)
            else:
                # ê°„ë‹¨í•œ í‰ê·  ìƒ‰ìƒ ë¹„êµ
                mean_orig = np.mean(original, axis=(0, 1))
                mean_proc = np.mean(processed, axis=(0, 1))
                diff = np.linalg.norm(mean_orig - mean_proc) / (255 * np.sqrt(3))
                return max(0, 1.0 - diff)
        except Exception:
            return 0.7

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤
# ==============================================

class QualityAssessmentStep:
    """
    âœ… 8ë‹¨ê³„: ì™„ì „í•œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
    âœ… AI ëª¨ë¸ ë¡œë”ì™€ ì™„ë²½ ì—°ë™
    âœ… Pipeline Manager í˜¸í™˜ì„±
    âœ… M3 Max ìµœì í™”
    âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë“  ê¸°ëŠ¥
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í’ˆì§ˆ ê°€ì¤‘ì¹˜
    CLOTHING_QUALITY_WEIGHTS = {
        'shirt': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'dress': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'pants': {'fitting': 0.6, 'texture': 0.2, 'edge': 0.1, 'color': 0.1},
        'jacket': {'fitting': 0.3, 'texture': 0.4, 'edge': 0.2, 'color': 0.1},
        'skirt': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'top': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'default': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1}
    }
    
    # ì›ë‹¨ íƒ€ì…ë³„ í’ˆì§ˆ ê¸°ì¤€
    FABRIC_QUALITY_STANDARDS = {
        'cotton': {'texture_importance': 0.8, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.3},
        'silk': {'texture_importance': 0.9, 'drape_importance': 0.9, 'wrinkle_tolerance': 0.2},
        'wool': {'texture_importance': 0.7, 'drape_importance': 0.7, 'wrinkle_tolerance': 0.4},
        'polyester': {'texture_importance': 0.5, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.8},
        'denim': {'texture_importance': 0.9, 'drape_importance': 0.4, 'wrinkle_tolerance': 0.6},
        'leather': {'texture_importance': 0.95, 'drape_importance': 0.3, 'wrinkle_tolerance': 0.9},
        'default': {'texture_importance': 0.7, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.5}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… í†µì¼ëœ ìƒì„±ì íŒ¨í„´ - Pipeline Manager ì™„ë²½ í˜¸í™˜"""
        
        # 1. ê¸°ë³¸ ì„¤ì •
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # 2. ì‹œìŠ¤í…œ ì •ë³´ ì¶”ì¶œ
        self.device_type = kwargs.get('device_type', self._get_device_type())
        self.memory_gb = float(kwargs.get('memory_gb', self._get_memory_gb()))
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # 3. ì„¤ì • ì—…ë°ì´íŠ¸
        self._update_config_from_kwargs(kwargs)
        
        # 4. ì´ˆê¸°í™”
        self.is_initialized = False
        self.initialization_error = None
        self.performance_stats = {
            'total_assessments': 0,
            'total_time': 0.0,
            'average_time': 0.0,
            'last_assessment_time': 0.0,
            'average_score': 0.0,
            'peak_memory_usage': 0.0,
            'error_count': 0
        }
        
        # 5. í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self._initialize_step_specific()
            self._setup_model_loader()
            self._initialize_analyzers()
            self._setup_assessment_pipeline()
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - M3 Max: {self.is_m3_max}")
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ - M3 Max ìµœì í™”"""
        if device:
            return device
        
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        
        return "cpu"
    
    def _get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        if self.device == "mps":
            return "apple_silicon"
        elif self.device == "cuda":
            return "nvidia_gpu"
        else:
            return "cpu"
    
    def _get_memory_gb(self) -> float:
        """ë©”ëª¨ë¦¬ í¬ê¸° ê°ì§€"""
        try:
            if self.is_m3_max:
                return 128.0  # M3 Max ê¸°ë³¸ê°’
            else:
                import psutil
                return psutil.virtual_memory().total / (1024**3)
        except:
            return 16.0  # ê¸°ë³¸ê°’
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            if platform.system() == "Darwin":
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return "M3" in result.stdout and "Max" in result.stdout
        except:
            pass
        return False
    
    def _update_config_from_kwargs(self, kwargs: Dict[str, Any]):
        """kwargsì—ì„œ config ì—…ë°ì´íŠ¸"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        }
        
        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _setup_model_loader(self):
        """AI ëª¨ë¸ ë¡œë” ì—°ë™"""
        try:
            from app.ai_pipeline.utils.model_loader import BaseStepMixin, get_global_model_loader
            
            # Step ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
            model_loader = get_global_model_loader()
            self.model_interface = model_loader.create_step_interface(self.step_name)
            
            # ì¶”ì²œ ëª¨ë¸ ìë™ ë¡œë“œ
            self._load_recommended_models()
            
            self.logger.info(f"ğŸ”— {self.step_name} ëª¨ë¸ ë¡œë” ì—°ë™ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë” ì—°ë™ ì‹¤íŒ¨, ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©: {e}")
            self.model_interface = None
    
    def _initialize_step_specific(self):
        """8ë‹¨ê³„ ì „ìš© ì´ˆê¸°í™”"""
        
        # í’ˆì§ˆ í‰ê°€ ì„¤ì •
        self.assessment_config = {
            'mode': self.config.get('assessment_mode', 'comprehensive'),
            'technical_analysis_enabled': self.config.get('technical_analysis_enabled', True),
            'perceptual_analysis_enabled': self.config.get('perceptual_analysis_enabled', True),
            'aesthetic_analysis_enabled': self.config.get('aesthetic_analysis_enabled', True),
            'functional_analysis_enabled': self.config.get('functional_analysis_enabled', True),
            'detailed_analysis_enabled': self.config.get('detailed_analysis_enabled', False),
            'neural_analysis_enabled': self.config.get('neural_analysis_enabled', True)
        }
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.75,
            'acceptable': 0.6,
            'poor': 0.4,
            'minimum_acceptable': self.config.get('minimum_quality', 0.6)
        }
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        if self.is_m3_max:
            self.optimization_level = 'maximum'
            self.batch_processing = True
            self.parallel_analysis = True
        elif self.memory_gb >= 32:
            self.optimization_level = 'high'
            self.batch_processing = True
            self.parallel_analysis = False
        else:
            self.optimization_level = 'basic'
            self.batch_processing = False
            self.parallel_analysis = False
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        cache_size = min(200 if self.is_m3_max else 100, int(self.memory_gb * 3))
        self.assessment_cache = {}
        self.cache_max_size = cache_size
        
        self.logger.info(f"ğŸ“Š 8ë‹¨ê³„ ì„¤ì • ì™„ë£Œ - ëª¨ë“œ: {self.assessment_config['mode']}, ìµœì í™”: {self.optimization_level}")
    
    def _initialize_analyzers(self):
        """ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”"""
        try:
            # 1. ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(self.device)
            
            # 2. AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
            self._initialize_ai_models()
            
            # 3. ì§€ê°ì  ë¶„ì„ê¸° (AI ê¸°ë°˜)
            self.perceptual_analyzer = self._create_perceptual_analyzer()
            
            # 4. ë¯¸ì  ë¶„ì„ê¸° (AI ê¸°ë°˜)
            self.aesthetic_analyzer = self._create_aesthetic_analyzer()
            
            # 5. ê¸°ëŠ¥ì  ë¶„ì„ê¸°
            self.functional_analyzer = self._create_functional_analyzer()
            
            # 6. ì–¼êµ´ ê°ì§€ê¸° (ì–¼êµ´ í’ˆì§ˆ í‰ê°€ìš©)
            self.face_detector = self._create_face_detector()
            
            self.logger.info("ğŸ”§ ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_ai_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        self.ai_models = {}
        
        if not TORCH_AVAILABLE:
            self.logger.warning("âš ï¸ PyTorch ì—†ìŒ, AI ëª¨ë¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
        
        try:
            # ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if self.assessment_config['perceptual_analysis_enabled']:
                self.ai_models['perceptual_quality'] = PerceptualQualityModel()
                self.ai_models['perceptual_quality'].to(self.device)
                self.ai_models['perceptual_quality'].eval()
            
            # ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if self.assessment_config['aesthetic_analysis_enabled']:
                self.ai_models['aesthetic_quality'] = AestheticQualityModel()
                self.ai_models['aesthetic_quality'].to(self.device)
                self.ai_models['aesthetic_quality'].eval()
            
            # M3 Max ìµœì í™”
            if self.is_m3_max and self.device == "mps":
                for model in self.ai_models.values():
                    if hasattr(model, 'half'):
                        model.half()
            
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ {len(self.ai_models)}ê°œ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_models = {}
    
    def _setup_assessment_pipeline(self):
        """í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # í‰ê°€ ìˆœì„œ ì •ì˜
        self.assessment_pipeline = []
        
        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
        self.assessment_pipeline.append(('preprocessing', self._preprocess_for_assessment))
        
        # 2. ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„
        if self.assessment_config['technical_analysis_enabled']:
            self.assessment_pipeline.append(('technical_analysis', self._analyze_technical_quality))
        
        # 3. ì§€ê°ì  í’ˆì§ˆ ë¶„ì„
        if self.assessment_config['perceptual_analysis_enabled']:
            self.assessment_pipeline.append(('perceptual_analysis', self._analyze_perceptual_quality))
        
        # 4. ë¯¸ì  í’ˆì§ˆ ë¶„ì„
        if self.assessment_config['aesthetic_analysis_enabled']:
            self.assessment_pipeline.append(('aesthetic_analysis', self._analyze_aesthetic_quality))
        
        # 5. ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„
        if self.assessment_config['functional_analysis_enabled']:
            self.assessment_pipeline.append(('functional_analysis', self._analyze_functional_quality))
        
        # 6. ì¢…í•© ë¶„ì„
        self.assessment_pipeline.append(('comprehensive_analysis', self._perform_comprehensive_analysis))
        
        self.logger.info(f"ğŸ”„ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ - {len(self.assessment_pipeline)}ë‹¨ê³„")
    
    # =================================================================
    # ğŸš€ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ (Pipeline Manager í˜¸ì¶œ)
    # =================================================================
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… ë©”ì¸ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜ - Pipeline Manager í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            fitted_image: í›„ì²˜ë¦¬ëœ ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì´ë¯¸ì§€
            person_image: ì›ë³¸ ì¸ë¬¼ ì´ë¯¸ì§€ (ì„ íƒì )
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€ (ì„ íƒì )
            fabric_type: ì›ë‹¨ íƒ€ì…
            clothing_type: ì˜ë¥˜ íƒ€ì…
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            Dict[str, Any]: í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                raise ValueError(f"QualityAssessmentStepì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")
            
            # 2. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ fitted_imageì…ë‹ˆë‹¤")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image is not None else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image is not None else None
            
            # 3. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(fitted_img, fabric_type, clothing_type, kwargs)
            if cache_key in self.assessment_cache:
                self.logger.info("ğŸ“‹ ìºì‹œì—ì„œ í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë°˜í™˜")
                cached_result = self.assessment_cache[cache_key].copy()
                cached_result['from_cache'] = True
                return cached_result
            
            # 4. ë©”ëª¨ë¦¬ ìµœì í™”
            if self.is_m3_max:
                self._optimize_m3_max_memory()
            
            # 5. ë©”ì¸ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            quality_metrics = await self._execute_assessment_pipeline(
                fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs
            )
            
            # 6. ê°œì„  ì œì•ˆ ìƒì„±
            recommendations = await self._generate_recommendations(quality_metrics, fabric_type, clothing_type)
            
            # 7. ìƒì„¸ ë¶„ì„ (ì„ íƒì )
            detailed_analysis = {}
            if self.assessment_config['detailed_analysis_enabled']:
                detailed_analysis = await self._generate_detailed_analysis(
                    quality_metrics, fitted_img, person_img, clothing_img, fabric_type
                )
            
            # 8. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            result = self._build_final_result(
                quality_metrics, recommendations, detailed_analysis,
                processing_time, fabric_type, clothing_type
            )
            
            # 9. ìºì‹œ ì €ì¥
            self._save_to_cache(cache_key, result)
            
            # 10. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(processing_time, quality_metrics.overall_score)
            
            self.logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {quality_metrics.overall_score:.3f} ({quality_metrics.get_grade().value})")
            return result
            
        except Exception as e:
            error_msg = f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, 0.0, success=False)
            
            return {
                "success": False,
                "step_name": self.step_name,
                "error": error_msg,
                "processing_time": processing_time,
                "quality_metrics": None,
                "overall_score": 0.0,
                "grade": QualityGrade.VERY_POOR.value
            }
    
    # =================================================================
    # ğŸ”§ í’ˆì§ˆ í‰ê°€ í•µì‹¬ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _execute_assessment_pipeline(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> QualityMetrics:
        """í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        metrics = QualityMetrics()
        intermediate_results = {}
        
        self.logger.info(f"ğŸ”„ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ì˜ë¥˜: {clothing_type}, ì›ë‹¨: {fabric_type}")
        
        for step_name, analyzer_func in self.assessment_pipeline:
            try:
                step_start = time.time()
                
                # ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ê³„ë“¤ (M3 Max ìµœì í™”)
                if self.parallel_analysis and step_name in ['technical_analysis', 'perceptual_analysis']:
                    step_result = await self._process_with_m3_max_optimization(
                        fitted_img, person_img, clothing_img, analyzer_func, step_name
                    )
                else:
                    step_result = await analyzer_func(
                        fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs
                    )
                
                step_time = time.time() - step_start
                intermediate_results[step_name] = {
                    'processing_time': step_time,
                    'success': True,
                    'result': step_result
                }
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if isinstance(step_result, dict):
                    for key, value in step_result.items():
                        if hasattr(metrics, key) and isinstance(value, (int, float)):
                            setattr(metrics, key, float(value))
                
                self.logger.debug(f"  âœ“ {step_name} ì™„ë£Œ - {step_time:.3f}ì´ˆ")
                
            except Exception as e:
                self.logger.warning(f"  âš ï¸ {step_name} ì‹¤íŒ¨: {e}")
                intermediate_results[step_name] = {
                    'processing_time': 0,
                    'success': False,
                    'error': str(e)
                }
                continue
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        fabric_weights = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
        clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
        
        # ê°€ì¤‘ì¹˜ ì¡°í•©
        combined_weights = {
            'technical': 0.3 * fabric_weights['texture_importance'],
            'perceptual': 0.3,
            'aesthetic': 0.2,
            'functional': 0.2 * clothing_weights['fitting']
        }
        
        metrics.calculate_overall_score(combined_weights)
        
        self.logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - {len(intermediate_results)}ë‹¨ê³„ ì²˜ë¦¬")
        return metrics
    
    async def _process_with_m3_max_optimization(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        analyzer_func: Callable,
        step_name: str
    ) -> Dict[str, Any]:
        """M3 Max ìµœì í™” ì²˜ë¦¬"""
        
        if not self.is_m3_max or self.device != "mps":
            return await analyzer_func(fitted_img, person_img, clothing_img)
        
        try:
            # M3 Max Neural Engine í™œìš©
            if TORCH_AVAILABLE and step_name in ['perceptual_analysis', 'aesthetic_analysis']:
                return await self._process_with_neural_engine(fitted_img, step_name)
            else:
                return await analyzer_func(fitted_img, person_img, clothing_img)
                
        except Exception as e:
            self.logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨, ì¼ë°˜ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")
            return await analyzer_func(fitted_img, person_img, clothing_img)
    
    async def _process_with_neural_engine(self, image: np.ndarray, analysis_type: str) -> Dict[str, Any]:
        """Neural Engine í™œìš© ë¶„ì„"""
        
        if analysis_type not in self.ai_models:
            raise ValueError(f"AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {analysis_type}")
        
        model = self.ai_models[analysis_type]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        tensor_img = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
        tensor_img = tensor_img.unsqueeze(0).to(self.device)
        
        # ë°˜ì •ë°€ë„ ì—°ì‚° (M3 Max ìµœì í™”)
        if self.is_m3_max:
            tensor_img = tensor_img.half()
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            if self.device == "mps":
                # MPS ë°±ì—”ë“œ ìµœì í™”
                with autocast(device_type='cpu', dtype=torch.float16):
                    result = model(tensor_img)
            else:
                result = model(tensor_img)
        
        # ê²°ê³¼ ì²˜ë¦¬
        if analysis_type == 'perceptual_analysis':
            return {'perceptual_similarity': float(result.cpu().squeeze())}
        elif analysis_type == 'aesthetic_analysis':
            scores = result.cpu().squeeze().numpy()
            return {
                'composition': float(scores[0]),
                'color_harmony': float(scores[1]),
                'symmetry': float(scores[2]),
                'balance': float(scores[3])
            }
        
        return {}
    
    # =================================================================
    # ğŸ”§ ê°œë³„ ë¶„ì„ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _preprocess_for_assessment(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, Any]:
        """í‰ê°€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬"""
        
        # 1. ì´ë¯¸ì§€ ì •ê·œí™”
        if fitted_img.dtype != np.uint8:
            fitted_img = np.clip(fitted_img * 255, 0, 255).astype(np.uint8)
        
        # 2. í•´ìƒë„ í™•ì¸ ë° ì¡°ì •
        h, w = fitted_img.shape[:2]
        if max(h, w) > 1024:
            scale = 1024 / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            if CV2_AVAILABLE:
                fitted_img = cv2.resize(fitted_img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # 3. ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        basic_checks = {
            'valid_shape': fitted_img.ndim == 3 and fitted_img.shape[2] == 3,
            'valid_size': fitted_img.size > 0,
            'valid_range': np.all(fitted_img >= 0) and np.all(fitted_img <= 255),
            'not_corrupted': not np.any(np.isnan(fitted_img))
        }
        
        return {
            'preprocessing_success': all(basic_checks.values()),
            'basic_checks': basic_checks,
            'processed_shape': fitted_img.shape
        }
    
    async def _analyze_technical_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        
        try:
            results = {}
            
            # 1. ì„ ëª…ë„ ë¶„ì„
            results['sharpness'] = self.technical_analyzer.analyze_sharpness(fitted_img)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„
            results['noise_level'] = self.technical_analyzer.analyze_noise_level(fitted_img)
            
            # 3. ëŒ€ë¹„ ë¶„ì„
            results['contrast'] = self.technical_analyzer.analyze_contrast(fitted_img)
            
            # 4. ìƒ‰ìƒ ì •í™•ë„ (ì›ë³¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
            if person_img is not None:
                results['color_accuracy'] = self.technical_analyzer.analyze_color_accuracy(person_img, fitted_img)
            else:
                results['color_accuracy'] = 0.8  # ê¸°ë³¸ê°’
            
            # 5. ì±„ë„ ë¶„ì„
            results['saturation'] = self._analyze_saturation(fitted_img)
            
            # 6. ë°ê¸° ë¶„ì„
            results['brightness'] = self._analyze_brightness(fitted_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5,
                'color_accuracy': 0.5, 'saturation': 0.5, 'brightness': 0.5
            }
    
    async def _analyze_perceptual_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ì§€ê°ì  í’ˆì§ˆ ë¶„ì„"""
        
        try:
            results = {}
            
            # 1. AI ëª¨ë¸ ê¸°ë°˜ ì§€ê°ì  í’ˆì§ˆ
            if 'perceptual_quality' in self.ai_models:
                neural_result = await self._process_with_neural_engine(fitted_img, 'perceptual_analysis')
                results.update(neural_result)
            
            # 2. êµ¬ì¡°ì  ìœ ì‚¬ì„± (SSIM)
            if person_img is not None and SKIMAGE_AVAILABLE:
                # í¬ê¸° ë§ì¶”ê¸°
                if person_img.shape != fitted_img.shape:
                    person_resized = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
                else:
                    person_resized = person_img
                
                try:
                    ssim_score = ssim(person_resized, fitted_img, multichannel=True, channel_axis=2)
                    results['structural_similarity'] = max(0, ssim_score)
                except:
                    results['structural_similarity'] = 0.7
            else:
                results['structural_similarity'] = 0.7
            
            # 3. ì‹œê°ì  í’ˆì§ˆ (ì „í†µì  ë°©ë²•)
            results['visual_quality'] = self._calculate_visual_quality(fitted_img)
            
            # 4. ì•„í‹°íŒ©íŠ¸ ë ˆë²¨
            results['artifact_level'] = self._detect_artifacts(fitted_img)
            
            # 5. ì§€ê°ì  ìœ ì‚¬ì„± (ê¸°ë³¸ê°’ ì„¤ì •)
            if 'perceptual_similarity' not in results:
                results['perceptual_similarity'] = results.get('structural_similarity', 0.7)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'structural_similarity': 0.5, 'perceptual_similarity': 0.5,
                'visual_quality': 0.5, 'artifact_level': 0.5
            }
    
    async def _analyze_aesthetic_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
        
        try:
            results = {}
            
            # 1. AI ëª¨ë¸ ê¸°ë°˜ ë¯¸ì  í’ˆì§ˆ
            if 'aesthetic_quality' in self.ai_models:
                neural_result = await self._process_with_neural_engine(fitted_img, 'aesthetic_analysis')
                results.update(neural_result)
            
            # 2. ì „í†µì  ë°©ë²•ë“¤ë¡œ ë³´ì™„
            if 'composition' not in results:
                results['composition'] = self._analyze_composition(fitted_img)
            
            if 'color_harmony' not in results:
                results['color_harmony'] = self._analyze_color_harmony(fitted_img)
            
            if 'symmetry' not in results:
                results['symmetry'] = self._analyze_symmetry(fitted_img)
            
            if 'balance' not in results:
                results['balance'] = self._analyze_balance(fitted_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'composition': 0.5, 'color_harmony': 0.5,
                'symmetry': 0.5, 'balance': 0.5
            }
    
    async def _analyze_functional_quality(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„"""
        
        try:
            results = {}
            
            # 1. í”¼íŒ… í’ˆì§ˆ
            results['fitting_quality'] = self._analyze_fitting_quality(fitted_img, person_img, clothing_type)
            
            # 2. ì—£ì§€ ë³´ì¡´
            results['edge_preservation'] = self._analyze_edge_preservation(fitted_img, person_img)
            
            # 3. í…ìŠ¤ì²˜ í’ˆì§ˆ
            results['texture_quality'] = self._analyze_texture_quality(fitted_img, clothing_img, fabric_type)
            
            # 4. ë””í…Œì¼ ë³´ì¡´
            results['detail_preservation'] = self._analyze_detail_preservation(fitted_img, person_img)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'fitting_quality': 0.5, 'edge_preservation': 0.5,
                'texture_quality': 0.5, 'detail_preservation': 0.5
            }
    
    async def _perform_comprehensive_analysis(
        self,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str,
        clothing_type: str,
        **kwargs
    ) -> Dict[str, float]:
        """ì¢…í•© ë¶„ì„"""
        
        try:
            results = {}
            
            # 1. ì „ì²´ì  ì¼ê´€ì„±
            results['overall_consistency'] = self._analyze_overall_consistency(fitted_img)
            
            # 2. í˜„ì‹¤ì„±
            results['realism'] = self._analyze_realism(fitted_img, person_img)
            
            # 3. ì™„ì„±ë„
            results['completeness'] = self._analyze_completeness(fitted_img)
            
            # 4. ì‹ ë¢°ë„ ê³„ì‚°
            confidence_factors = [
                results.get('overall_consistency', 0.5),
                results.get('realism', 0.5),
                results.get('completeness', 0.5)
            ]
            results['confidence'] = np.mean(confidence_factors)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_consistency': 0.5, 'realism': 0.5,
                'completeness': 0.5, 'confidence': 0.5
            }
    
    # =================================================================
    # ğŸ”§ ê°œë³„ ë¶„ì„ ë©”ì„œë“œë“¤
    # =================================================================
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """ì±„ë„ ë¶„ì„"""
        try:
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation = hsv[:, :, 1].mean() / 255.0
            else:
                # RGBì—ì„œ ê·¼ì‚¬ ì±„ë„ ê³„ì‚°
                r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]
                max_rgb = np.maximum(np.maximum(r, g), b)
                min_rgb = np.minimum(np.minimum(r, g), b)
                saturation = np.mean((max_rgb - min_rgb) / (max_rgb + 1e-8)) 
            
            return min(saturation, 1.0)
        except:
            return 0.5
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """ë°ê¸° ë¶„ì„"""
        try:
            brightness = np.mean(image) / 255.0
            # ì ì ˆí•œ ë°ê¸° ë²”ìœ„ (0.3-0.7)ì—ì„œ 1.0ì— ê°€ê¹Œìš´ ì ìˆ˜
            if 0.3 <= brightness <= 0.7:
                return 1.0 - abs(brightness - 0.5) * 2
            else:
                return max(0, 1.0 - abs(brightness - 0.5) * 4)
        except:
            return 0.5
    
    def _calculate_visual_quality(self, image: np.ndarray) -> float:
        """ì‹œê°ì  í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ì—¬ëŸ¬ ìš”ì†Œ ì¢…í•©
            factors = []
            
            # 1. ìƒ‰ìƒ ë¶„í¬
            color_std = np.std(image, axis=(0, 1)).mean() / 255.0
            factors.append(min(color_std * 2, 1.0))
            
            # 2. ê·¸ë¼ë””ì–¸íŠ¸ ê°•ë„
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2).mean()
                factors.append(min(gradient_magnitude / 100.0, 1.0))
            
            # 3. ì—”íŠ¸ë¡œí”¼ (ì •ë³´ëŸ‰)
            if len(factors) == 0:
                factors.append(0.5)
            
            return np.mean(factors)
        except:
            return 0.5
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ ê°ì§€"""
        try:
            artifacts = 0.0
            
            # 1. ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê°ì§€
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                # DCT ê¸°ë°˜ ë¸”ë¡ ê²½ê³„ ê°ì§€
                for i in range(0, gray.shape[0]-8, 8):
                    for j in range(0, gray.shape[1]-8, 8):
                        block = gray[i:i+8, j:j+8]
                        if block.shape == (8, 8):
                            # ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ê¸‰ê²©í•œ ë³€í™” ê°ì§€
                            edge_diff = np.abs(np.diff(block, axis=0)).mean() + np.abs(np.diff(block, axis=1)).mean()
                            if edge_diff > 30:
                                artifacts += 0.1
            
            # 2. ë§ê¹… ì•„í‹°íŒ©íŠ¸ (ê³¼ë„í•œ ìƒ¤í”„ë‹)
            if CV2_AVAILABLE:
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                if laplacian_var > 2000:  # ê³¼ë„í•œ ì—£ì§€ ê°•í™”
                    artifacts += 0.2
            
            # 3. ë…¸ì´ì¦ˆ íŒ¨í„´
            noise_level = np.std(image) / 255.0
            if noise_level > 0.15:
                artifacts += 0.3
            
            return min(artifacts, 1.0)
        except:
            return 0.3
    
    def _analyze_composition(self, image: np.ndarray) -> float:
        """êµ¬ë„ ë¶„ì„"""
        try:
            # í™©ê¸ˆë¹„, 3ë¶„í•  ë²•ì¹™ ë“±ì„ ê³ ë ¤í•œ êµ¬ë„ ë¶„ì„
            h, w = image.shape[:2]
            
            # 3ë¶„í•  ì§€ì ë“¤
            thirds_h = [h//3, 2*h//3]
            thirds_w = [w//3, 2*w//3]
            
            # ê´€ì‹¬ ì˜ì—­ ê°ì§€ (ì—£ì§€ê°€ ë§ì€ ì˜ì—­)
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # 3ë¶„í•  ì§€ì  ê·¼ì²˜ì˜ ì—£ì§€ ë°€ë„
                composition_score = 0
                for th in thirds_h:
                    for tw in thirds_w:
                        region = edges[max(0, th-20):min(h, th+20), max(0, tw-20):min(w, tw+20)]
                        if region.size > 0:
                            edge_density = np.sum(region) / (region.size * 255)
                            composition_score += edge_density
                
                return min(composition_score / 4, 1.0)
            else:
                return 0.6  # ê¸°ë³¸ê°’
        except:
            return 0.5
    
    def _analyze_color_harmony(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¡°í™” ë¶„ì„"""
        try:
            if SKLEARN_AVAILABLE:
                # ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
                pixels = image.reshape(-1, 3)
                
                # ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
                if len(pixels) > 10000:
                    indices = np.random.choice(len(pixels), 10000, replace=False)
                    pixels = pixels[indices]
                
                kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                kmeans.fit(pixels)
                
                # ì£¼ìš” ìƒ‰ìƒë“¤ ê°„ì˜ ê±°ë¦¬ ë¶„ì„
                centers = kmeans.cluster_centers_
                distances = []
                for i in range(len(centers)):
                    for j in range(i+1, len(centers)):
                        dist = np.linalg.norm(centers[i] - centers[j])
                        distances.append(dist)
                
                # ì ì ˆí•œ ìƒ‰ìƒ ê°„ê²© (ë„ˆë¬´ ê°€ê¹ì§€ë„ ë©€ì§€ë„ ì•Šê²Œ)
                avg_distance = np.mean(distances)
                optimal_distance = 100  # RGB ê³µê°„ì—ì„œ ì ì ˆí•œ ê±°ë¦¬
                harmony_score = 1.0 - abs(avg_distance - optimal_distance) / optimal_distance
                
                return max(0, min(harmony_score, 1.0))
            else:
                # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ì‚° ê¸°ë°˜ ë¶„ì„
                color_std = np.std(image, axis=(0, 1))
                balance = 1.0 - np.std(color_std) / 128.0
                return max(0, min(balance, 1.0))
        except:
            return 0.6
    
    def _analyze_symmetry(self, image: np.ndarray) -> float:
        """ëŒ€ì¹­ì„± ë¶„ì„"""
        try:
            h, w = image.shape[:2]
            
            # ìˆ˜ì§ ëŒ€ì¹­ì„± (ì¢Œìš°)
            left_half = image[:, :w//2]
            right_half = np.fliplr(image[:, w//2:])
            
            # í¬ê¸° ë§ì¶”ê¸°
            min_width = min(left_half.shape[1], right_half.shape[1])
            left_half = left_half[:, :min_width]
            right_half = right_half[:, :min_width]
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            if SKIMAGE_AVAILABLE and left_half.shape == right_half.shape:
                try:
                    symmetry_score = ssim(left_half, right_half, multichannel=True, channel_axis=2)
                    return max(0, symmetry_score)
                except:
                    pass
            
            # ëŒ€ì•ˆ: MSE ê¸°ë°˜
            mse = np.mean((left_half.astype(float) - right_half.astype(float))**2)
            symmetry_score = max(0, 1.0 - mse / (255**2))
            
            return symmetry_score
        except:
            return 0.4
    
    def _analyze_balance(self, image: np.ndarray) -> float:
        """ê· í˜• ë¶„ì„"""
        try:
            h, w = image.shape[:2]
            
            # ì´ë¯¸ì§€ë¥¼ 4ë¶„í• ë¡œ ë‚˜ëˆ„ì–´ ê° ì˜ì—­ì˜ ì‹œê°ì  ë¬´ê²Œ ê³„ì‚°
            quarters = [
                image[:h//2, :w//2],      # ì¢Œìƒ
                image[:h//2, w//2:],      # ìš°ìƒ
                image[h//2:, :w//2],      # ì¢Œí•˜
                image[h//2:, w//2:]       # ìš°í•˜
            ]
            
            # ê° ì˜ì—­ì˜ ì‹œê°ì  ë¬´ê²Œ ê³„ì‚°
            weights = []
            for quarter in quarters:
                if quarter.size > 0:
                    # ë°ê¸° + ëŒ€ë¹„ + ì±„ë„ë¥¼ ì¢…í•©í•œ ì‹œê°ì  ë¬´ê²Œ
                    brightness = np.mean(quarter)
                    contrast = np.std(quarter)
                    weight = brightness * 0.5 + contrast * 0.5
                    weights.append(weight)
            
            if len(weights) == 4:
                # ëŒ€ê°ì„  ê· í˜• (ì¢Œìƒ+ìš°í•˜ vs ìš°ìƒ+ì¢Œí•˜)
                diagonal1 = weights[0] + weights[3]  # ì¢Œìƒ + ìš°í•˜
                diagonal2 = weights[1] + weights[2]  # ìš°ìƒ + ì¢Œí•˜
                diagonal_balance = 1.0 - abs(diagonal1 - diagonal2) / max(diagonal1 + diagonal2, 1)
                
                # ìˆ˜ì§ ê· í˜• (ìƒë‹¨ vs í•˜ë‹¨)
                top = weights[0] + weights[1]
                bottom = weights[2] + weights[3]
                vertical_balance = 1.0 - abs(top - bottom) / max(top + bottom, 1)
                
                # ìˆ˜í‰ ê· í˜• (ì¢Œì¸¡ vs ìš°ì¸¡)
                left = weights[0] + weights[2]
                right = weights[1] + weights[3]
                horizontal_balance = 1.0 - abs(left - right) / max(left + right, 1)
                
                # ì¢…í•© ê· í˜•
                balance_score = (diagonal_balance + vertical_balance + horizontal_balance) / 3
                return max(0, min(balance_score, 1.0))
            
            return 0.5
        except:
            return 0.5
    
    def _analyze_fitting_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_type: str) -> float:
        """í”¼íŒ… í’ˆì§ˆ ë¶„ì„"""
        try:
            if person_img is None:
                return 0.6  # ê¸°ë³¸ê°’
            
            # 1. ì‹ ì²´ ìœ¤ê³½ì„ ê³¼ ì˜ë¥˜ì˜ ì¼ì¹˜ë„
            fitting_score = 0.0
            
            if CV2_AVAILABLE:
                # ì—£ì§€ ê¸°ë°˜ ë¶„ì„
                fitted_edges = cv2.Canny(cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY), 50, 150)
                person_edges = cv2.Canny(cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY), 50, 150)
                
                # í¬ê¸° ë§ì¶”ê¸°
                if fitted_edges.shape != person_edges.shape:
                    person_edges = cv2.resize(person_edges, (fitted_edges.shape[1], fitted_edges.shape[0]))
                
                # ì—£ì§€ ì¼ì¹˜ë„
                edge_overlap = np.sum((fitted_edges > 0) & (person_edges > 0))
                total_edges = np.sum(fitted_edges > 0) + np.sum(person_edges > 0)
                if total_edges > 0:
                    fitting_score = (2 * edge_overlap) / total_edges
            
            # 2. ì˜ë¥˜ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
            fitting_weight = clothing_weights['fitting']
            
            return min(fitting_score * fitting_weight + 0.3, 1.0)
        except:
            return 0.5
    
    def _analyze_edge_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """ì—£ì§€ ë³´ì¡´ ë¶„ì„"""
        try:
            if person_img is None or not CV2_AVAILABLE:
                return 0.6
            
            # ì›ë³¸ê³¼ í”¼íŒ… ê²°ê³¼ì˜ ì—£ì§€ ë¹„êµ
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_gray.shape != person_gray.shape:
                person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
            
            # ì—£ì§€ ê°ì§€
            fitted_edges = cv2.Canny(fitted_gray, 50, 150)
            person_edges = cv2.Canny(person_gray, 50, 150)
            
            # ì—£ì§€ ë³´ì¡´ë¥  ê³„ì‚°
            preserved_edges = np.sum((fitted_edges > 0) & (person_edges > 0))
            original_edges = np.sum(person_edges > 0)
            
            if original_edges > 0:
                preservation_rate = preserved_edges / original_edges
                return min(preservation_rate, 1.0)
            
            return 0.6
        except:
            return 0.5
    
    def _analyze_texture_quality(self, fitted_img: np.ndarray, clothing_img: Optional[np.ndarray], fabric_type: str) -> float:
        """í…ìŠ¤ì²˜ í’ˆì§ˆ ë¶„ì„"""
        try:
            # 1. í…ìŠ¤ì²˜ ì¼ê´€ì„±
            texture_score = 0.0
            
            if SKIMAGE_AVAILABLE:
                # LBP (Local Binary Pattern) ê¸°ë°˜ í…ìŠ¤ì²˜ ë¶„ì„
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                # LBP íŠ¹ì§• ì¶”ì¶œ
                radius = 3
                n_points = 8 * radius
                lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')
                
                # í…ìŠ¤ì²˜ ê· ì¼ì„± (LBP íˆìŠ¤í† ê·¸ë¨ì˜ ì—”íŠ¸ë¡œí”¼)
                hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, range=(0, n_points + 2))
                hist = hist.astype(float)
                hist /= (hist.sum() + 1e-8)
                
                # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ í…ìŠ¤ì²˜ê°€ ë³µì¡)
                entropy_score = -np.sum(hist * np.log2(hist + 1e-8))
                texture_score = min(entropy_score / 8.0, 1.0)  # ì •ê·œí™”
            
            # 2. ì›ë‹¨ íƒ€ì…ë³„ ê¸°ì¤€ ì ìš©
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            texture_importance = fabric_standards['texture_importance']
            
            # 3. í…ìŠ¤ì²˜ ì„ ëª…ë„
            if CV2_AVAILABLE:
                # Sobel í•„í„°ë¡œ í…ìŠ¤ì²˜ ì„¸ë¶€ì‚¬í•­ ë¶„ì„
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                texture_sharpness = np.sqrt(sobel_x**2 + sobel_y**2).mean() / 255.0
                texture_score = (texture_score + min(texture_sharpness * 2, 1.0)) / 2
            
            return texture_score * texture_importance + (1 - texture_importance) * 0.7
        except:
            return 0.6
    
    def _analyze_detail_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """ë””í…Œì¼ ë³´ì¡´ ë¶„ì„"""
        try:
            if person_img is None:
                return 0.6
            
            # 1. ê³ ì£¼íŒŒ ì„±ë¶„ ë¹„êµ
            detail_score = 0.0
            
            if CV2_AVAILABLE:
                # ë¼í”Œë¼ì‹œì•ˆ í•„í„°ë¡œ ì„¸ë¶€ì‚¬í•­ ì¶”ì¶œ
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
                
                # í¬ê¸° ë§ì¶”ê¸°
                if fitted_gray.shape != person_gray.shape:
                    person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
                
                # ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ
                fitted_detail = cv2.Laplacian(fitted_gray, cv2.CV_64F)
                person_detail = cv2.Laplacian(person_gray, cv2.CV_64F)
                
                # ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ë¥ 
                fitted_detail_energy = np.sum(np.abs(fitted_detail))
                person_detail_energy = np.sum(np.abs(person_detail))
                
                if person_detail_energy > 0:
                    detail_score = min(fitted_detail_energy / person_detail_energy, 1.0)
                else:
                    detail_score = 0.6
            
            # 2. ì–¼êµ´ ì„¸ë¶€ì‚¬í•­ íŠ¹ë³„ ë¶„ì„
            faces = self._detect_faces_for_quality(fitted_img)
            if len(faces) > 0 and person_img is not None:
                face_detail_score = self._analyze_face_detail_preservation(fitted_img, person_img, faces)
                detail_score = (detail_score + face_detail_score) / 2
            
            return detail_score
        except:
            return 0.5
    
    def _analyze_overall_consistency(self, image: np.ndarray) -> float:
        """ì „ì²´ì  ì¼ê´€ì„± ë¶„ì„"""
        try:
            consistency_factors = []
            
            # 1. ìƒ‰ìƒ ì¼ê´€ì„±
            color_consistency = self._calculate_color_consistency(image)
            consistency_factors.append(color_consistency)
            
            # 2. ì¡°ëª… ì¼ê´€ì„±
            lighting_consistency = self._calculate_lighting_consistency(image)
            consistency_factors.append(lighting_consistency)
            
            # 3. ìŠ¤íƒ€ì¼ ì¼ê´€ì„±
            style_consistency = self._calculate_style_consistency(image)
            consistency_factors.append(style_consistency)
            
            return np.mean(consistency_factors)
        except:
            return 0.6
    
    def _analyze_realism(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        """í˜„ì‹¤ì„± ë¶„ì„"""
        try:
            realism_factors = []
            
            # 1. ìì—°ìŠ¤ëŸ¬ìš´ ì¡°ëª…
            lighting_realism = self._assess_lighting_realism(fitted_img)
            realism_factors.append(lighting_realism)
            
            # 2. ë¬¼ë¦¬ì  íƒ€ë‹¹ì„± (ë“œë ˆì´í•‘, ì£¼ë¦„ ë“±)
            physics_realism = self._assess_physics_realism(fitted_img)
            realism_factors.append(physics_realism)
            
            # 3. ì¸ì²´ ë¹„ë¡€
            if person_img is not None:
                proportion_realism = self._assess_proportion_realism(fitted_img, person_img)
                realism_factors.append(proportion_realism)
            
            return np.mean(realism_factors)
        except:
            return 0.6
    
    def _analyze_completeness(self, image: np.ndarray) -> float:
        """ì™„ì„±ë„ ë¶„ì„"""
        try:
            completeness_factors = []
            
            # 1. ì´ë¯¸ì§€ ê²½ê³„ ì™„ì„±ë„
            boundary_completeness = self._check_boundary_completeness(image)
            completeness_factors.append(boundary_completeness)
            
            # 2. ì˜ë¥˜ ì™„ì„±ë„
            clothing_completeness = self._check_clothing_completeness(image)
            completeness_factors.append(clothing_completeness)
            
            # 3. ì „ì²´ì  ì™„ì„±ë„
            overall_completeness = self._check_overall_completeness(image)
            completeness_factors.append(overall_completeness)
            
            return np.mean(completeness_factors)
        except:
            return 0.7
    
    # =================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° ë¶„ì„ ë©”ì„œë“œë“¤
    # =================================================================
    
    def _detect_faces_for_quality(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """í’ˆì§ˆ í‰ê°€ìš© ì–¼êµ´ ê°ì§€"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                return faces.tolist()
            else:
                # íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì–¼êµ´ ì˜ì—­ ì¶”ì •
                h, w = image.shape[:2]
                return [(w//4, h//6, w//2, h//3)]
        except:
            return []
    
    def _analyze_face_detail_preservation(self, fitted_img: np.ndarray, person_img: np.ndarray, faces: List) -> float:
        """ì–¼êµ´ ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ ë¶„ì„"""
        try:
            if len(faces) == 0:
                return 0.6
            
            face_scores = []
            for (x, y, w, h) in faces:
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                fitted_face = fitted_img[y:y+h, x:x+w]
                person_face = person_img[y:y+h, x:x+w] if person_img.shape[:2] == fitted_img.shape[:2] else person_img
                
                if fitted_face.size > 0 and person_face.size > 0:
                    # í¬ê¸° ë§ì¶”ê¸°
                    if fitted_face.shape != person_face.shape:
                        person_face = cv2.resize(person_face, (fitted_face.shape[1], fitted_face.shape[0])) if CV2_AVAILABLE else person_face
                    
                    # SSIMìœ¼ë¡œ ì–¼êµ´ ìœ ì‚¬ë„ ì¸¡ì •
                    if SKIMAGE_AVAILABLE and fitted_face.shape == person_face.shape:
                        try:
                            face_similarity = ssim(fitted_face, person_face, multichannel=True, channel_axis=2)
                            face_scores.append(max(0, face_similarity))
                        except:
                            face_scores.append(0.6)
                    else:
                        face_scores.append(0.6)
            
            return np.mean(face_scores) if face_scores else 0.6
        except:
            return 0.5
    
    def _calculate_color_consistency(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
            h, w = image.shape[:2]
            regions = [
                image[:h//2, :w//2],      # ì¢Œìƒ
                image[:h//2, w//2:],      # ìš°ìƒ
                image[h//2:, :w//2],      # ì¢Œí•˜
                image[h//2:, w//2:]       # ìš°í•˜
            ]
            
            # ê° ì˜ì—­ì˜ í‰ê·  ìƒ‰ìƒ
            region_colors = []
            for region in regions:
                if region.size > 0:
                    mean_color = np.mean(region, axis=(0, 1))
                    region_colors.append(mean_color)
            
            if len(region_colors) >= 2:
                # ì˜ì—­ ê°„ ìƒ‰ìƒ ì°¨ì´ ê³„ì‚°
                color_diffs = []
                for i in range(len(region_colors)):
                    for j in range(i+1, len(region_colors)):
                        diff = np.linalg.norm(region_colors[i] - region_colors[j])
                        color_diffs.append(diff)
                
                # ì ì ˆí•œ ìƒ‰ìƒ ì¼ê´€ì„± (ë„ˆë¬´ uniformí•˜ì§€ë„ diverseí•˜ì§€ë„ ì•Šê²Œ)
                avg_diff = np.mean(color_diffs)
                consistency = max(0, 1.0 - avg_diff / 128.0)
                return min(consistency, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _calculate_lighting_consistency(self, image: np.ndarray) -> float:
        """ì¡°ëª… ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ë°ê¸° ë¶„í¬ì˜ ì¼ê´€ì„± ë¶„ì„
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # ì´ë¯¸ì§€ë¥¼ ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ì–´ ê° ì˜ì—­ì˜ ë°ê¸° ë¶„ì„
            h, w = gray.shape
            grid_size = 4
            brightnesses = []
            
            for i in range(grid_size):
                for j in range(grid_size):
                    y1, y2 = i * h // grid_size, (i + 1) * h // grid_size
                    x1, x2 = j * w // grid_size, (j + 1) * w // grid_size
                    region = gray[y1:y2, x1:x2]
                    if region.size > 0:
                        brightnesses.append(np.mean(region))
            
            if len(brightnesses) > 1:
                # ë°ê¸° ë¶„í¬ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ì´ ë†’ìŒ
                brightness_std = np.std(brightnesses)
                consistency = max(0, 1.0 - brightness_std / 128.0)
                return min(consistency, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _calculate_style_consistency(self, image: np.ndarray) -> float:
        """ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # í…ìŠ¤ì²˜ì™€ íŒ¨í„´ì˜ ì¼ê´€ì„±
            if SKIMAGE_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                
                # ì—¬ëŸ¬ ì˜ì—­ì—ì„œ LBP íŠ¹ì§• ì¶”ì¶œ
                h, w = gray.shape
                regions = [
                    gray[:h//2, :w//2],
                    gray[:h//2, w//2:],
                    gray[h//2:, :w//2],
                    gray[h//2:, w//2:]
                ]
                
                lbp_histograms = []
                for region in regions:
                    if region.size > 64:  # ìµœì†Œ í¬ê¸° í™•ì¸
                        lbp = feature.local_binary_pattern(region, 8, 1, method='uniform')
                        hist, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10))
                        hist = hist.astype(float) / (hist.sum() + 1e-8)
                        lbp_histograms.append(hist)
                
                if len(lbp_histograms) >= 2:
                    # íˆìŠ¤í† ê·¸ë¨ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                    similarities = []
                    for i in range(len(lbp_histograms)):
                        for j in range(i+1, len(lbp_histograms)):
                            # Bhattacharyya distance
                            bc = np.sum(np.sqrt(lbp_histograms[i] * lbp_histograms[j]))
                            similarity = bc
                            similarities.append(similarity)
                    
                    return np.mean(similarities)
            
            return 0.7
        except:
            return 0.6
    
    def _assess_lighting_realism(self, image: np.ndarray) -> float:
        """ì¡°ëª… í˜„ì‹¤ì„± í‰ê°€"""
        try:
            # ê·¸ë¦¼ìì™€ í•˜ì´ë¼ì´íŠ¸ì˜ ìì—°ìŠ¤ëŸ¬ì›€
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            hist, _ = np.histogram(gray, bins=256, range=(0, 256))
            hist = hist.astype(float) / hist.sum()
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ì¡°ëª…ì€ ë³´í†µ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€
            # ê·¹ë‹¨ì ì¸ ê°’ë“¤ (ìˆœë°±, ìˆœí‘)ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€
            extreme_ratio = (hist[0] + hist[-1])  # 0ê³¼ 255 ê°’ì˜ ë¹„ìœ¨
            if extreme_ratio < 0.1:  # 10% ë¯¸ë§Œì´ë©´ ìì—°ìŠ¤ëŸ¬ì›€
                return 0.9
            elif extreme_ratio < 0.2:
                return 0.7
            else:
                return max(0.3, 1.0 - extreme_ratio)
        except:
            return 0.6
    
    def _assess_physics_realism(self, image: np.ndarray) -> float:
        """ë¬¼ë¦¬ì  í˜„ì‹¤ì„± í‰ê°€"""
        try:
            # ì˜ë¥˜ì˜ ë“œë ˆì´í•‘ê³¼ ì£¼ë¦„ì˜ ìì—°ìŠ¤ëŸ¬ì›€
            physics_score = 0.7  # ê¸°ë³¸ ì ìˆ˜
            
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ì—£ì§€ ë°©í–¥ì„± ë¶„ì„ (ìì—°ìŠ¤ëŸ¬ìš´ ì£¼ë¦„ì€ íŠ¹ì • ë°©í–¥ì„±ì„ ê°€ì§)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                
                # ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ ê³„ì‚°
                angles = np.arctan2(grad_y, grad_x)
                
                # ë°©í–¥ì„±ì˜ ì¼ê´€ì„± (ì™„ì „íˆ randomí•˜ì§€ ì•Šê³  ì–´ëŠì •ë„ íŒ¨í„´ì´ ìˆì–´ì•¼ í•¨)
                angle_hist, _ = np.histogram(angles, bins=8, range=(-np.pi, np.pi))
                angle_hist = angle_hist.astype(float) / (angle_hist.sum() + 1e-8)
                
                # ì—”íŠ¸ë¡œí”¼ê°€ ì ë‹¹í•´ì•¼ í•¨ (ë„ˆë¬´ uniformí•˜ì§€ë„ ë„ˆë¬´ concentratedí•˜ì§€ë„ ì•Šê²Œ)
                angle_entropy = -np.sum(angle_hist * np.log2(angle_hist + 1e-8))
                optimal_entropy = 2.5  # ê²½í—˜ì  ê°’
                entropy_score = max(0, 1.0 - abs(angle_entropy - optimal_entropy) / optimal_entropy)
                
                physics_score = (physics_score + entropy_score) / 2
            
            return physics_score
        except:
            return 0.6
    
    def _assess_proportion_realism(self, fitted_img: np.ndarray, person_img: np.ndarray) -> float:
        """ë¹„ë¡€ í˜„ì‹¤ì„± í‰ê°€"""
        try:
            # ì‹ ì²´ ë¹„ë¡€ì˜ ìœ ì§€ ì •ë„
            if fitted_img.shape != person_img.shape:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            # ìœ¤ê³½ì„  ê¸°ë°˜ ë¹„ë¡€ ë¶„ì„
            if CV2_AVAILABLE:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
                
                # ì£¼ìš” ì‹ ì²´ ë¶€ìœ„ì˜ ìœ¤ê³½ ê°ì§€
                fitted_contours, _ = cv2.findContours(cv2.Canny(fitted_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                person_contours, _ = cv2.findContours(cv2.Canny(person_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                if fitted_contours and person_contours:
                    # ê°€ì¥ í° ìœ¤ê³½ì„  (ì‹ ì²´) ë¹„êµ
                    fitted_main = max(fitted_contours, key=cv2.contourArea)
                    person_main = max(person_contours, key=cv2.contourArea)
                    
                    # ìœ¤ê³½ì„ ì˜ ëª¨ë©˜íŠ¸ ë¹„êµ (í˜•íƒœ ìœ ì‚¬ì„±)
                    fitted_moments = cv2.moments(fitted_main)
                    person_moments = cv2.moments(person_main)
                    
                    # Hu moments (í˜•íƒœ ë¶ˆë³€ íŠ¹ì§•)
                    fitted_hu = cv2.HuMoments(fitted_moments).flatten()
                    person_hu = cv2.HuMoments(person_moments).flatten()
                    
                    # ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ì •ê·œí™”
                    fitted_hu = -np.sign(fitted_hu) * np.log10(np.abs(fitted_hu) + 1e-10)
                    person_hu = -np.sign(person_hu) * np.log10(np.abs(person_hu) + 1e-10)
                    
                    # ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = np.exp(-np.sum(np.abs(fitted_hu - person_hu)))
                    return min(similarity, 1.0)
            
            return 0.7
        except:
            return 0.6
    
    def _check_boundary_completeness(self, image: np.ndarray) -> float:
        """ê²½ê³„ ì™„ì„±ë„ í™•ì¸"""
        try:
            h, w = image.shape[:2]
            
            # ì´ë¯¸ì§€ ê²½ê³„ì˜ ê¸‰ê²©í•œ ë³€í™” ê°ì§€
            boundary_issues = 0
            
            # ìƒí•˜ì¢Œìš° ê²½ê³„ ì²´í¬
            boundaries = [
                image[0, :],      # ìƒë‹¨
                image[-1, :],     # í•˜ë‹¨
                image[:, 0],      # ì¢Œì¸¡
                image[:, -1]      # ìš°ì¸¡
            ]
            
            for boundary in boundaries:
                if boundary.size > 0:
                    # ê²½ê³„ì—ì„œì˜ ê¸‰ê²©í•œ ìƒ‰ìƒ ë³€í™”
                    if len(boundary.shape) == 2:  # RGB
                        diff = np.sum(np.abs(np.diff(boundary, axis=0)))
                    else:  # 1D
                        diff = np.sum(np.abs(np.diff(boundary)))
                    
                    # ì •ê·œí™”ëœ ë³€í™”ëŸ‰
                    normalized_diff = diff / (len(boundary) * 255 * 3)
                    if normalized_diff > 0.5:  # ì„ê³„ê°’
                        boundary_issues += 1
            
            completeness = max(0, 1.0 - boundary_issues / 4.0)
            return completeness
        except:
            return 0.8
    
    def _check_clothing_completeness(self, image: np.ndarray) -> float:
        """ì˜ë¥˜ ì™„ì„±ë„ í™•ì¸"""
        try:
            # ì˜ë¥˜ ì˜ì—­ì˜ ì—°ì†ì„±ê³¼ ì™„ì„±ë„
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ì˜ë¥˜ ì˜ì—­ ì¶”ì • (ì¤‘ê°„ ë°ê¸° ì˜ì—­)
                clothing_mask = ((gray > 50) & (gray < 200)).astype(np.uint8)
                
                # ì—°ê²°ëœ êµ¬ì„±ìš”ì†Œ ë¶„ì„
                num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(clothing_mask)
                
                if num_labels > 1:
                    # ê°€ì¥ í° ì—°ê²° êµ¬ì„±ìš”ì†Œ (ì£¼ ì˜ë¥˜)
                    main_component_size = np.max(stats[1:, cv2.CC_STAT_AREA])
                    total_clothing_area = np.sum(stats[1:, cv2.CC_STAT_AREA])
                    
                    # ì£¼ ì˜ë¥˜ê°€ ì „ì²´ ì˜ë¥˜ ì˜ì—­ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨
                    main_ratio = main_component_size / (total_clothing_area + 1e-8)
                    
                    # ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì™„ì„±ë„ê°€ ì¢‹ìŒ (íŒŒí¸í™”ë˜ì§€ ì•ŠìŒ)
                    return min(main_ratio, 1.0)
            
            return 0.8
        except:
            return 0.7
    
    def _check_overall_completeness(self, image: np.ndarray) -> float:
        """ì „ì²´ì  ì™„ì„±ë„ í™•ì¸"""
        try:
            completeness_factors = []
            
            # 1. ìƒ‰ìƒ ì¼ê´€ì„±
            color_completeness = self._calculate_color_consistency(image)
            completeness_factors.append(color_completeness)
            
            # 2. ì´ë¯¸ì§€ í’ˆì§ˆ
            if not self._is_image_corrupted(image):
                completeness_factors.append(0.9)
            else:
                completeness_factors.append(0.3)
            
            # 3. í•´ìƒë„ ì ì ˆì„±
            h, w = image.shape[:2]
            if min(h, w) >= 256:
                resolution_score = min((min(h, w) / 512.0), 1.0)
            else:
                resolution_score = min(h, w) / 256.0
            completeness_factors.append(resolution_score)
            
            return np.mean(completeness_factors)
        except:
            return 0.7
    
    # =================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    # =================================================================
    
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], input_name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if isinstance(image_input, np.ndarray):
                image = image_input
            elif isinstance(image_input, (str, Path)):
                if PIL_AVAILABLE:
                    pil_img = Image.open(image_input)
                    image = np.array(pil_img.convert('RGB'))
                else:
                    raise ImportError("PILì´ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            # ê²€ì¦
            if image.ndim != 3 or image.shape[2] != 3:
                raise ValueError("RGB ì´ë¯¸ì§€ì—¬ì•¼ í•©ë‹ˆë‹¤")
            
            if image.size == 0:
                raise ValueError("ë¹ˆ ì´ë¯¸ì§€ì…ë‹ˆë‹¤")
            
            return image
            
        except Exception as e:
            self.logger.error(f"{input_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _is_image_corrupted(self, image: np.ndarray) -> bool:
        """ì´ë¯¸ì§€ ì†ìƒ ì—¬ë¶€ í™•ì¸"""
        try:
            # 1. NaN/Inf ì²´í¬
            if np.any(np.isnan(image)) or np.any(np.isinf(image)):
                return True
            
            # 2. ê°’ ë²”ìœ„ ì²´í¬
            if np.any(image < 0) or np.any(image > 255):
                return True
            
            # 3. í˜•íƒœ ì²´í¬
            if image.ndim != 3 or image.shape[2] != 3:
                return True
            
            # 4. í¬ê¸° ì²´í¬
            if image.size == 0:
                return True
            
            return False
            
        except Exception:
            return True
    
    def _generate_cache_key(self, image: np.ndarray, fabric_type: str, clothing_type: str, kwargs: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            import hashlib
            
            # ì´ë¯¸ì§€ í•´ì‹œ
            image_hash = hashlib.md5(image.tobytes()).hexdigest()[:16]
            
            # ì„¤ì • í•´ì‹œ
            config_data = {
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config.get('mode', 'comprehensive'),
                **{k: v for k, v in kwargs.items() if isinstance(v, (str, int, float, bool))}
            }
            config_str = json.dumps(config_data, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"qa_{image_hash}_{config_hash}"
            
        except Exception:
            return f"qa_fallback_{time.time()}"
    
    def _optimize_m3_max_memory(self):
        """M3 Max ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self.is_m3_max and TORCH_AVAILABLE:
            try:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def _generate_recommendations(self, metrics: QualityMetrics, fabric_type: str, clothing_type: str) -> List[Dict[str, Any]]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        recommendations = []
        
        try:
            # 1. ê¸°ìˆ ì  í’ˆì§ˆ ê°œì„  ì œì•ˆ
            if metrics.sharpness < 0.6:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'low_sharpness',
                    'description': 'ì´ë¯¸ì§€ ì„ ëª…ë„ê°€ ë‚®ìŠµë‹ˆë‹¤',
                    'suggestion': 'ìƒ¤í”„ë‹ í•„í„°ë¥¼ ì ìš©í•˜ê±°ë‚˜ ë” ë†’ì€ í•´ìƒë„ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”',
                    'priority': 'high'
                })
            
            if metrics.noise_level > 0.4:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'high_noise',
                    'description': 'ë…¸ì´ì¦ˆ ë ˆë²¨ì´ ë†’ìŠµë‹ˆë‹¤',
                    'suggestion': 'ë…¸ì´ì¦ˆ ì œê±° í•„í„°ë¥¼ ê°•í™”í•˜ê±°ë‚˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ê°œì„ í•˜ì„¸ìš”',
                    'priority': 'medium'
                })
            
            if metrics.contrast < 0.5:
                recommendations.append({
                    'category': 'technical',
                    'issue': 'low_contrast',
                    'description': 'ëŒ€ë¹„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤',
                    'suggestion': 'íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”ë‚˜ ì ì‘í˜• ëŒ€ë¹„ í–¥ìƒì„ ì ìš©í•˜ì„¸ìš”',
                    'priority': 'medium'
                })
            
            # 2. ì§€ê°ì  í’ˆì§ˆ ê°œì„  ì œì•ˆ
            if metrics.structural_similarity < 0.7:
                recommendations.append({
                    'category': 'perceptual',
                    'issue': 'low_similarity',
                    'description': 'ì›ë³¸ê³¼ì˜ êµ¬ì¡°ì  ìœ ì‚¬ì„±ì´ ë‚®ìŠµë‹ˆë‹¤',
                    'suggestion': 'ì§€ì˜¤ë©”íŠ¸ë¦­ ë§¤ì¹­ ë‹¨ê³„ë¥¼ ê°œì„ í•˜ê±°ë‚˜ ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜ì„ ì¡°ì •í•˜ì„¸ìš”',
                    'priority': 'high'
                })
            
            # 3. ë¯¸ì  í’ˆì§ˆ ê°œì„  ì œì•ˆ
            if metrics.color_harmony < 0.6:
                recommendations.append({
                    'category': 'aesthetic',
                    'issue': 'poor_color_harmony',
                    'description': 'ìƒ‰ìƒ ì¡°í™”ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤',
                    'suggestion': 'ìƒ‰ìƒ ë³´ì •ì´ë‚˜ ìƒ‰ì˜¨ë„ ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”',
                    'priority': 'low'
                })
            
            # 4. ê¸°ëŠ¥ì  í’ˆì§ˆ ê°œì„  ì œì•ˆ
            if metrics.fitting_quality < 0.7:
                recommendations.append({
                    'category': 'functional',
                    'issue': 'poor_fitting',
                    'description': f'{clothing_type} í”¼íŒ… í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤',
                    'suggestion': 'ì¸ì²´ íŒŒì‹± ì •í™•ë„ë¥¼ ë†’ì´ê±°ë‚˜ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ê°œì„ í•˜ì„¸ìš”',
                    'priority': 'high'
                })
            
            # 5. ì›ë‹¨ë³„ íŠ¹í™” ì œì•ˆ
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            if metrics.texture_quality < fabric_standards['texture_importance'] * 0.8:
                recommendations.append({
                    'category': 'fabric_specific',
                    'issue': 'texture_quality',
                    'description': f'{fabric_type} ì›ë‹¨ì˜ í…ìŠ¤ì²˜ í’ˆì§ˆì´ ê¸°ì¤€ ë¯¸ë‹¬ì…ë‹ˆë‹¤',
                    'suggestion': f'{fabric_type}ì— íŠ¹í™”ëœ í…ìŠ¤ì²˜ í–¥ìƒ ê¸°ë²•ì„ ì ìš©í•˜ì„¸ìš”',
                    'priority': 'medium'
                })
            
            # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
            priority_order = {'high': 0, 'medium': 1, 'low': 2}
            recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))
            
        except Exception as e:
            self.logger.error(f"ê°œì„  ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations.append({
                'category': 'general',
                'issue': 'analysis_error',
                'description': 'í’ˆì§ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'suggestion': 'ì…ë ¥ ì´ë¯¸ì§€ë‚˜ ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”',
                'priority': 'high'
            })
        
        return recommendations
    
    async def _generate_detailed_analysis(
        self,
        metrics: QualityMetrics,
        fitted_img: np.ndarray,
        person_img: Optional[np.ndarray],
        clothing_img: Optional[np.ndarray],
        fabric_type: str
    ) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìƒì„±"""
        detailed_analysis = {}
        
        try:
            # 1. ì´ë¯¸ì§€ í†µê³„
            detailed_analysis['image_statistics'] = {
                'mean_brightness': float(np.mean(fitted_img)),
                'std_brightness': float(np.std(fitted_img)),
                'color_distribution': {
                    'red_mean': float(np.mean(fitted_img[:, :, 0])),
                    'green_mean': float(np.mean(fitted_img[:, :, 1])),
                    'blue_mean': float(np.mean(fitted_img[:, :, 2]))
                },
                'shape': fitted_img.shape,
                'total_pixels': int(fitted_img.size)
            }
            
            # 2. í’ˆì§ˆ ë©”íŠ¸ë¦­ ìƒì„¸ ë¶„ì„
            detailed_analysis['quality_breakdown'] = {
                'technical_quality': {
                    'sharpness': float(metrics.sharpness),
                    'noise_level': float(metrics.noise_level),
                    'contrast': float(metrics.contrast),
                    'color_accuracy': float(metrics.color_accuracy)
                },
                'perceptual_quality': {
                    'structural_similarity': float(metrics.structural_similarity),
                    'visual_quality': float(metrics.visual_quality),
                    'artifact_level': float(metrics.artifact_level)
                },
                'aesthetic_quality': {
                    'composition': float(metrics.composition),
                    'color_harmony': float(metrics.color_harmony),
                    'balance': float(metrics.balance)
                },
                'functional_quality': {
                    'fitting_quality': float(metrics.fitting_quality),
                    'texture_quality': float(metrics.texture_quality),
                    'detail_preservation': float(metrics.detail_preservation)
                }
            }
            
            # 3. ì–¼êµ´ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
            faces = self._detect_faces_for_quality(fitted_img)
            if faces:
                detailed_analysis['face_analysis'] = {
                    'faces_detected': len(faces),
                    'face_regions': [{'x': int(x), 'y': int(y), 'width': int(w), 'height': int(h)} for x, y, w, h in faces],
                    'face_quality_preserved': bool(metrics.detail_preservation > 0.7)
                }
            
            # 4. ì›ë‹¨ë³„ íŠ¹ì„± ë¶„ì„
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            detailed_analysis['fabric_analysis'] = {
                'fabric_type': fabric_type,
                'texture_importance': fabric_standards['texture_importance'],
                'texture_meets_standard': bool(metrics.texture_quality >= fabric_standards['texture_importance'] * 0.8),
                'draping_quality': float(metrics.fitting_quality * fabric_standards['drape_importance'])
            }
            
            # 5. ì²˜ë¦¬ ì‹œê°„ ë° ì„±ëŠ¥
            detailed_analysis['performance_analysis'] = {
                'processing_device': self.device,
                'is_m3_max_optimized': self.is_m3_max,
                'memory_usage_gb': self.memory_gb,
                'optimization_level': self.optimization_level
            }
            
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            detailed_analysis['error'] = str(e)
        
        return detailed_analysis
    
    def _build_final_result(
        self,
        metrics: QualityMetrics,
        recommendations: List[Dict[str, Any]],
        detailed_analysis: Dict[str, Any],
        processing_time: float,
        fabric_type: str,
        clothing_type: str
    ) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        
        return {
            "success": True,
            "step_name": self.step_name,
            "processing_time": processing_time,
            
            # í•µì‹¬ í’ˆì§ˆ ë©”íŠ¸ë¦­
            "quality_metrics": asdict(metrics),
            "overall_score": float(metrics.overall_score),
            "grade": metrics.get_grade().value,
            "confidence": float(metrics.confidence),
            
            # ê°œì„  ì œì•ˆ
            "recommendations": recommendations,
            "recommendation_count": len(recommendations),
            "high_priority_issues": len([r for r in recommendations if r.get('priority') == 'high']),
            
            # ìƒì„¸ ë¶„ì„ (ì„ íƒì )
            "detailed_analysis": detailed_analysis if detailed_analysis else None,
            
            # ë©”íƒ€ë°ì´í„°
            "fabric_type": fabric_type,
            "clothing_type": clothing_type,
            "assessment_mode": self.assessment_config['mode'],
            
            # ì‹œìŠ¤í…œ ì •ë³´
            "device_info": {
                "device": self.device,
                "device_type": self.device_type,
                "is_m3_max": self.is_m3_max,
                "memory_gb": self.memory_gb,
                "optimization_level": self.optimization_level
            },
            
            # ì„±ëŠ¥ í†µê³„
            "performance_stats": self.performance_stats.copy(),
            
            # í’ˆì§ˆ í†µê³¼ ì—¬ë¶€
            "quality_passed": metrics.overall_score >= self.quality_thresholds['minimum_acceptable'],
            "quality_thresholds": self.quality_thresholds.copy(),
            
            "from_cache": False
        }
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            if len(self.assessment_cache) >= self.cache_max_size:
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = min(self.assessment_cache.keys())
                del self.assessment_cache[oldest_key]
            
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìƒì„¸ ë¶„ì„ì€ ìºì‹œì—ì„œ ì œì™¸
            cached_result = result.copy()
            if 'detailed_analysis' in cached_result:
                cached_result['detailed_analysis'] = None
            
            self.assessment_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_performance_stats(self, processing_time: float, quality_score: float, success: bool = True):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            if success:
                self.performance_stats['total_assessments'] += 1
                self.performance_stats['total_time'] += processing_time
                self.performance_stats['average_time'] = (
                    self.performance_stats['total_time'] / self.performance_stats['total_assessments']
                )
                
                # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
                current_avg = self.performance_stats.get('average_score', 0.0)
                total_assessments = self.performance_stats['total_assessments']
                self.performance_stats['average_score'] = (
                    (current_avg * (total_assessments - 1) + quality_score) / total_assessments
                )
            else:
                self.performance_stats['error_count'] += 1
            
            self.performance_stats['last_assessment_time'] = processing_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  (M3 Max)
            if self.is_m3_max:
                try:
                    import psutil
                    memory_usage = psutil.virtual_memory().percent
                    self.performance_stats['peak_memory_usage'] = max(
                        self.performance_stats.get('peak_memory_usage', 0),
                        memory_usage
                    )
                except:
                    pass
            
        except Exception as e:
            self.logger.warning(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # =================================================================
    # ğŸ”§ íŒ©í† ë¦¬ ë©”ì„œë“œë“¤ (ë¶„ì„ê¸° ìƒì„±)
    # =================================================================
    
    def _create_perceptual_analyzer(self) -> Optional[Callable]:
        """ì§€ê°ì  ë¶„ì„ê¸° ìƒì„±"""
        if 'perceptual_quality' in self.ai_models:
            return self.ai_models['perceptual_quality']
        return None
    
    def _create_aesthetic_analyzer(self) -> Optional[Callable]:
        """ë¯¸ì  ë¶„ì„ê¸° ìƒì„±"""
        if 'aesthetic_quality' in self.ai_models:
            return self.ai_models['aesthetic_quality']
        return None
    
    def _create_functional_analyzer(self) -> Callable:
        """ê¸°ëŠ¥ì  ë¶„ì„ê¸° ìƒì„±"""
        def functional_analyzer(image: np.ndarray) -> Dict[str, float]:
            # ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„
            return {
                'fitting_quality': 0.7,
                'edge_preservation': 0.7,
                'texture_quality': 0.7,
                'detail_preservation': 0.7
            }
        return functional_analyzer
    
    def _create_face_detector(self) -> Optional[Callable]:
        """ì–¼êµ´ ê°ì§€ê¸° ìƒì„±"""
        if CV2_AVAILABLE:
            return lambda img: self._detect_faces_for_quality(img)
        return None
    
    async def _load_recommended_models(self):
        """ì¶”ì²œ ëª¨ë¸ ë¡œë“œ"""
        if self.model_interface is None:
            return
        
        try:
            # í’ˆì§ˆ í‰ê°€ìš© ì¶”ì²œ ëª¨ë¸ë“¤
            recommended_models = [
                'quality_assessment_combined',
                'perceptual_quality_model',
                'aesthetic_quality_model'
            ]
            
            for model_name in recommended_models:
                try:
                    model = await self.model_interface.get_model(model_name)
                    if model:
                        self.logger.info(f"ğŸ“¦ ì¶”ì²œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¶”ì²œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶”ì²œ ëª¨ë¸ ë¡œë“œ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {e}")
    
    # =================================================================
    # ğŸ” í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œë“¤ (Pipeline Manager í˜¸í™˜)
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "QualityAssessment",
            "class_name": self.__class__.__name__,
            "version": "4.0-m3max",
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "config_keys": list(self.config.keys()),
            "performance_stats": self.performance_stats.copy(),
            "capabilities": {
                "technical_analysis": self.assessment_config['technical_analysis_enabled'],
                "perceptual_analysis": self.assessment_config['perceptual_analysis_enabled'],
                "aesthetic_analysis": self.assessment_config['aesthetic_analysis_enabled'],
                "functional_analysis": self.assessment_config['functional_analysis_enabled'],
                "detailed_analysis": self.assessment_config['detailed_analysis_enabled'],
                "neural_analysis": bool(self.ai_models) if hasattr(self, 'ai_models') else False,
                "m3_max_acceleration": self.is_m3_max and self.device == 'mps'
            },
            "supported_fabrics": list(self.FABRIC_QUALITY_STANDARDS.keys()),
            "supported_clothing_types": list(self.CLOTHING_QUALITY_WEIGHTS.keys()),
            "quality_settings": {
                "optimization_level": self.optimization_level,
                "quality_thresholds": self.quality_thresholds,
                "assessment_mode": self.assessment_config['mode']
            },
            "assessment_pipeline": [name for name, _ in self.assessment_pipeline] if hasattr(self, 'assessment_pipeline') else [],
            "cache_status": {
                "enabled": True,
                "size": len(self.assessment_cache) if hasattr(self, 'assessment_cache') else 0,
                "max_size": self.cache_max_size if hasattr(self, 'cache_max_size') else 0
            }
        }
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.ai_models.clear()
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'assessment_cache'):
                self.assessment_cache.clear()
            
            # ë¶„ì„ê¸° ì •ë¦¬
            if hasattr(self, 'technical_analyzer'):
                del self.technical_analyzer
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and self.device in ["mps", "cuda"]:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… QualityAssessmentStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup_resources()
        except:
            pass

# =================================================================
# ğŸ”¥ í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
# =================================================================

def create_quality_assessment_step(
    device: str = "mps",
    config: Optional[Dict[str, Any]] = None
) -> QualityAssessmentStep:
    """ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ ìƒì„±ì"""
    return QualityAssessmentStep(device=device, config=config)

def create_m3_max_quality_assessment_step(
    memory_gb: float = 128.0,
    assessment_mode: str = "comprehensive",
    **kwargs
) -> QualityAssessmentStep:
    """M3 Max ìµœì í™” ìƒì„±ì"""
    return QualityAssessmentStep(
        device=None,  # ìë™ ê°ì§€
        memory_gb=memory_gb,
        quality_level=assessment_mode,
        is_m3_max=True,
        optimization_enabled=True,
        assessment_mode=assessment_mode,
        **kwargs
    )

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'QualityAssessmentStep',
    'QualityMetrics',
    'QualityGrade',
    'AssessmentMode',
    'QualityAspect',
    'PerceptualQualityModel',
    'AestheticQualityModel',
    'TechnicalQualityAnalyzer',
    'create_quality_assessment_step',
    'create_m3_max_quality_assessment_step'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
logger.info("âœ… QualityAssessmentStep ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„")