# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„ v2.0
âœ… BaseStepMixin ê¸°ë°˜ ì™„ì „ ì¬ì‘ì„±
âœ… ModelLoader ì™„ë²½ ì—°ë™ + logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°
âœ… step_model_requests.py ê¸°ë°˜ ëª¨ë¸ ìë™ ë¡œë“œ
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥
âœ… Pipeline Manager 100% í˜¸í™˜

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache
import numpy as np
import base64
import io

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ - ì•ˆì „í•œ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

try:
    from PIL import Image, ImageStat, ImageEnhance, ImageFilter, ImageDraw
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

try:
    from scipy import stats, ndimage, spatial
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë‚´ë¶€ import
try:
    from ..steps.base_step_mixin import (
        BaseStepMixin, QualityAssessmentMixin, 
        safe_step_method, ensure_step_initialization, performance_monitor
    )
    from ..utils.model_loader import get_global_model_loader
    from ..utils.step_model_requests import StepModelRequestAnalyzer, get_step_request
except ImportError as e:
    print(f"âš ï¸ ë‚´ë¶€ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    # í´ë°± í´ë˜ìŠ¤ë“¤
    class BaseStepMixin:
        def __init__(self, *args, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.device = "cpu"
            self.step_name = self.__class__.__name__
            self.is_initialized = False
    
    class QualityAssessmentMixin(BaseStepMixin):
        pass
    
    def safe_step_method(func):
        return func
    def ensure_step_initialization(func):
        return func
    def performance_monitor(name):
        def decorator(func):
            return func
        return decorator

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = ("Excellent", 0.9, 1.0)
    GOOD = ("Good", 0.75, 0.9)
    FAIR = ("Fair", 0.6, 0.75)
    POOR = ("Poor", 0.4, 0.6)
    VERY_POOR = ("Very Poor", 0.0, 0.4)
    
    def __init__(self, label: str, min_score: float, max_score: float):
        self.label = label
        self.min_score = min_score
        self.max_score = max_score

class QualityMetric(Enum):
    """í’ˆì§ˆ í‰ê°€ ì§€í‘œ"""
    SHARPNESS = "sharpness"
    CONTRAST = "contrast"
    BRIGHTNESS = "brightness"
    COLOR_HARMONY = "color_harmony"
    NOISE_LEVEL = "noise_level"
    ARTIFACT_DETECTION = "artifact_detection"
    CONSISTENCY = "consistency"
    REALISM = "realism"
    FITTING_QUALITY = "fitting_quality"
    EDGE_QUALITY = "edge_quality"

# ìƒìˆ˜ ì •ì˜
QUALITY_THRESHOLDS = {
    QualityMetric.SHARPNESS: {"min": 0.3, "good": 0.7, "excellent": 0.9},
    QualityMetric.CONTRAST: {"min": 0.2, "good": 0.6, "excellent": 0.85},
    QualityMetric.BRIGHTNESS: {"min": 0.1, "good": 0.5, "excellent": 0.8},
    QualityMetric.COLOR_HARMONY: {"min": 0.4, "good": 0.7, "excellent": 0.9},
    QualityMetric.NOISE_LEVEL: {"min": 0.1, "good": 0.3, "excellent": 0.8},
    QualityMetric.ARTIFACT_DETECTION: {"min": 0.2, "good": 0.6, "excellent": 0.9},
    QualityMetric.CONSISTENCY: {"min": 0.3, "good": 0.7, "excellent": 0.9},
    QualityMetric.REALISM: {"min": 0.4, "good": 0.7, "excellent": 0.85},
    QualityMetric.FITTING_QUALITY: {"min": 0.5, "good": 0.8, "excellent": 0.95},
    QualityMetric.EDGE_QUALITY: {"min": 0.3, "good": 0.6, "excellent": 0.85}
}

# ==============================================
# ğŸ”¥ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜
# ==============================================

@dataclass
class QualityAssessmentConfig:
    """í’ˆì§ˆ í‰ê°€ ì„¤ì •"""
    assessment_mode: str = "comprehensive"  # "fast", "balanced", "comprehensive"
    technical_analysis_enabled: bool = True
    aesthetic_analysis_enabled: bool = True
    clip_analysis_enabled: bool = True
    perceptual_analysis_enabled: bool = True
    
    # ì„ê³„ê°’ ì„¤ì •
    quality_threshold: float = 0.7
    noise_threshold: float = 0.3
    artifact_threshold: float = 0.2
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    weights: Dict[str, float] = field(default_factory=lambda: {
        "technical": 0.4,
        "aesthetic": 0.3,
        "clip_score": 0.2,
        "perceptual": 0.1
    })
    
    # ì„±ëŠ¥ ì„¤ì •
    enable_gpu_acceleration: bool = True
    batch_processing: bool = False
    parallel_metrics: bool = True
    cache_enabled: bool = True

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ í‰ê°€ ê²°ê³¼"""
    overall_score: float = 0.0
    technical_score: float = 0.0
    aesthetic_score: float = 0.0
    clip_score: float = 0.0
    perceptual_score: float = 0.0
    
    # ì„¸ë¶€ ì§€í‘œ
    sharpness: float = 0.0
    contrast: float = 0.0
    brightness: float = 0.0
    color_harmony: float = 0.0
    noise_level: float = 0.0
    artifact_level: float = 0.0
    consistency: float = 0.0
    realism: float = 0.0
    fitting_quality: float = 0.0
    edge_quality: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    processing_time: float = 0.0
    confidence: float = 0.0
    grade: str = "Unknown"
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤
# ==============================================

class QualityAssessmentStep(BaseStepMixin, QualityAssessmentMixin):
    """
    ğŸ”¥ 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ Step - ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„
    âœ… BaseStepMixin ê¸°ë°˜ í‘œì¤€í™”ëœ êµ¬ì¡°
    âœ… ModelLoader ì™„ë²½ ì—°ë™
    âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥
    âœ… M3 Max ìµœì í™”
    """
    
    def __init__(self, **kwargs):
        """ğŸ”¥ í†µì¼ëœ ìƒì„±ì íŒ¨í„´"""
        
        # ğŸ”¥ BaseStepMixin ë¨¼ì € ì´ˆê¸°í™” (logger ì†ì„± í•´ê²°)
        super().__init__(**kwargs)
        
        # ğŸ”¥ Step 8 ì „ìš© ì†ì„± ì„¤ì •
        self.step_name = "QualityAssessmentStep"
        self.step_number = 8
        self.step_type = "quality_assessment"
        
        # ğŸ”¥ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.step_08.{self.__class__.__name__}")
        
        # ì„¤ì • ì´ˆê¸°í™”
        self.config = QualityAssessmentConfig(**kwargs)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._auto_detect_device()
        
        # ëª¨ë¸ ê´€ë ¨ ì†ì„±
        self.clip_model = None
        self.clip_processor = None
        self.models_loaded = False
        self.model_cache = {}
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.quality_cache = {}
        self.cache_enabled = self.config.cache_enabled
        
        # ì„±ëŠ¥ í†µê³„
        self.processing_stats = {
            "total_processed": 0,
            "average_time": 0.0,
            "last_processing_time": 0.0
        }
        
        # ğŸ”¥ ModelLoader ì—°ë™ ì„¤ì •
        self._setup_model_interface_safe()
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True
        self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - Device: {self.device}")
    
    def _auto_detect_device(self) -> str:
        """ğŸ”§ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        return "cpu"
    
    def _setup_model_interface_safe(self):
        """ğŸ”§ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì•ˆì „ ì„¤ì •"""
        try:
            # Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ í™•ì¸
            step_request = get_step_request(self.step_name)
            if step_request:
                self.logger.info(f"ğŸ” Step 8 ëª¨ë¸ ìš”êµ¬ì‚¬í•­: {step_request.model_name}")
            
            # ModelLoader ì—°ë™
            model_loader = get_global_model_loader()
            if model_loader:
                self.model_interface = model_loader.create_step_interface(self.step_name)
                self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì—°ë™ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ ModelLoader ì—†ìŒ, ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©")
                self.model_interface = None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    @safe_step_method
    @performance_monitor("model_initialization")
    async def initialize_models(self) -> bool:
        """ğŸ”§ AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} AI ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            
            # ModelLoader í†µí•´ CLIP ëª¨ë¸ ë¡œë“œ ì‹œë„
            if self.model_interface:
                try:
                    clip_model = await self.model_interface.get_model("quality_assessment_clip")
                    if clip_model:
                        self.clip_model = clip_model
                        self.logger.info("âœ… CLIP ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ModelLoader)")
                    else:
                        self.logger.info("â„¹ï¸ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©")
                        await self._load_builtin_clip_model()
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader CLIP ë¡œë“œ ì‹¤íŒ¨: {e}")
                    await self._load_builtin_clip_model()
            else:
                # ë‚´ì¥ CLIP ëª¨ë¸ ë¡œë“œ
                await self._load_builtin_clip_model()
            
            self.models_loaded = True
            self.logger.info(f"âœ… {self.step_name} AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_builtin_clip_model(self):
        """ğŸ”§ ë‚´ì¥ CLIP ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ, CLIP ë¶„ì„ ë¹„í™œì„±í™”")
                return
            
            # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            try:
                from transformers import CLIPModel, CLIPProcessor
                
                model_name = "openai/clip-vit-base-patch32"
                self.clip_processor = CLIPProcessor.from_pretrained(model_name)
                self.clip_model = CLIPModel.from_pretrained(model_name)
                
                if self.device != "cpu":
                    self.clip_model = self.clip_model.to(self.device)
                
                self.clip_model.eval()
                self.logger.info("âœ… ë‚´ì¥ CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except ImportError:
                self.logger.warning("âš ï¸ Transformers ì—†ìŒ: pip install transformers")
                self.clip_model = None
                self.clip_processor = None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë‚´ì¥ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.clip_model = None
            self.clip_processor = None
    
    @safe_step_method
    @ensure_step_initialization
    @performance_monitor("quality_assessment")
    async def process(
        self,
        fitted_image: Union[np.ndarray, Image.Image],
        original_image: Optional[Union[np.ndarray, Image.Image]] = None,
        cloth_image: Optional[Union[np.ndarray, Image.Image]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ í•¨ìˆ˜
        
        Args:
            fitted_image: ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì´ë¯¸ì§€
            original_image: ì›ë³¸ ì‚¬ëŒ ì´ë¯¸ì§€ (ì„ íƒì )
            cloth_image: ì˜· ì´ë¯¸ì§€ (ì„ íƒì )
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # ì…ë ¥ ê²€ì¦
            if fitted_image is None:
                raise ValueError("fitted_imageëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            fitted_img_array = self._preprocess_image(fitted_image)
            original_img_array = self._preprocess_image(original_image) if original_image is not None else None
            cloth_img_array = self._preprocess_image(cloth_image) if cloth_image is not None else None
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(fitted_img_array)
            if self.cache_enabled and cache_key in self.quality_cache:
                self.logger.debug("âœ… ìºì‹œì—ì„œ í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë°˜í™˜")
                return self.quality_cache[cache_key]
            
            # ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            if not self.models_loaded:
                await self.initialize_models()
            
            # Step ì´ˆê¸°í™” í™•ì¸ (BaseStepMixin v2.0)
            if not self.is_initialized:
                await self.initialize_step()
            
            # í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            metrics = await self._perform_quality_assessment(
                fitted_img_array,
                original_img_array,
                cloth_img_array,
                **kwargs
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            metrics.processing_time = processing_time
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            metrics = self._calculate_overall_score(metrics)
            
            # ë“±ê¸‰ ê²°ì •
            metrics.grade = self._determine_quality_grade(metrics.overall_score)
            
            # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
            metrics.recommendations = self._generate_recommendations(metrics)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'step_name': self.step_name,
                'quality_metrics': metrics.to_dict(),
                'overall_score': metrics.overall_score,
                'grade': metrics.grade,
                'confidence': metrics.confidence,
                'processing_time': processing_time,
                'recommendations': metrics.recommendations,
                'metadata': {
                    'assessment_mode': self.config.assessment_mode,
                    'device': self.device,
                    'models_used': self._get_models_info(),
                    'image_size': fitted_img_array.shape if fitted_img_array is not None else None,
                    'has_reference': original_img_array is not None,
                    'has_cloth': cloth_img_array is not None
                }
            }
            
            # ìºì‹œ ì €ì¥
            if self.cache_enabled:
                self.quality_cache[cache_key] = result
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(processing_time)
            
            self.logger.info(f"âœ… {self.step_name} ì™„ë£Œ - ì ìˆ˜: {metrics.overall_score:.3f}, ë“±ê¸‰: {metrics.grade}")
            return result
            
        except Exception as e:
            error_msg = f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            self.logger.error(error_msg)
            
            return {
                'success': False,
                'step_name': self.step_name,
                'error': error_msg,
                'quality_metrics': QualityMetrics().to_dict(),
                'overall_score': 0.0,
                'grade': 'ERROR',
                'confidence': 0.0,
                'processing_time': time.time() - start_time,
                'recommendations': ["ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
                'metadata': {'error_details': str(e)}
            }
    
    async def _perform_quality_assessment(
        self,
        fitted_image: np.ndarray,
        original_image: Optional[np.ndarray],
        cloth_image: Optional[np.ndarray],
        **kwargs
    ) -> QualityMetrics:
        """ğŸ”§ ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰"""
        
        metrics = QualityMetrics()
        
        try:
            # ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
            if self.config.parallel_metrics:
                tasks = []
                
                # ê¸°ìˆ ì  ë¶„ì„
                if self.config.technical_analysis_enabled:
                    tasks.append(self._assess_technical_quality(fitted_image))
                
                # ë¯¸ì  ë¶„ì„
                if self.config.aesthetic_analysis_enabled:
                    tasks.append(self._assess_aesthetic_quality(fitted_image))
                
                # CLIP ë¶„ì„
                if self.config.clip_analysis_enabled and self.clip_model:
                    tasks.append(self._assess_clip_quality(fitted_image, original_image))
                
                # ì§€ê°ì  ë¶„ì„
                if self.config.perceptual_analysis_enabled and original_image is not None:
                    tasks.append(self._assess_perceptual_quality(fitted_image, original_image))
                
                # ë³‘ë ¬ ì‹¤í–‰
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ê²°ê³¼ í†µí•©
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        self.logger.warning(f"âš ï¸ í‰ê°€ ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                    elif isinstance(result, dict):
                        for key, value in result.items():
                            if hasattr(metrics, key) and isinstance(value, (int, float)):
                                setattr(metrics, key, value)
            
            else:
                # ìˆœì°¨ ì‹¤í–‰
                if self.config.technical_analysis_enabled:
                    tech_result = await self._assess_technical_quality(fitted_image)
                    metrics.technical_score = tech_result.get('technical_score', 0.0)
                    metrics.sharpness = tech_result.get('sharpness', 0.0)
                    metrics.contrast = tech_result.get('contrast', 0.0)
                    metrics.brightness = tech_result.get('brightness', 0.0)
                    metrics.noise_level = tech_result.get('noise_level', 0.0)
                
                if self.config.aesthetic_analysis_enabled:
                    aes_result = await self._assess_aesthetic_quality(fitted_image)
                    metrics.aesthetic_score = aes_result.get('aesthetic_score', 0.0)
                    metrics.color_harmony = aes_result.get('color_harmony', 0.0)
                    metrics.consistency = aes_result.get('consistency', 0.0)
                
                if self.config.clip_analysis_enabled and self.clip_model:
                    clip_result = await self._assess_clip_quality(fitted_image, original_image)
                    metrics.clip_score = clip_result.get('clip_score', 0.0)
                    metrics.realism = clip_result.get('realism', 0.0)
                
                if self.config.perceptual_analysis_enabled and original_image is not None:
                    perc_result = await self._assess_perceptual_quality(fitted_image, original_image)
                    metrics.perceptual_score = perc_result.get('perceptual_score', 0.0)
                    metrics.fitting_quality = perc_result.get('fitting_quality', 0.0)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            metrics.confidence = self._calculate_confidence(metrics)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return QualityMetrics()
    
    async def _assess_technical_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ğŸ”§ ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€"""
        try:
            results = {}
            
            # ì„ ëª…ë„ ì¸¡ì • (Laplacian variance)
            if CV2_AVAILABLE and image is not None:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
                sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
                results['sharpness'] = min(sharpness / 1000.0, 1.0)  # ì •ê·œí™”
            else:
                results['sharpness'] = 0.5
            
            # ëŒ€ë¹„ ì¸¡ì •
            if PIL_AVAILABLE and image is not None:
                pil_img = Image.fromarray(image.astype(np.uint8))
                stat = ImageStat.Stat(pil_img)
                contrast = np.std(stat.mean) / 255.0
                results['contrast'] = contrast
            else:
                results['contrast'] = 0.5
            
            # ë°ê¸° ì¸¡ì •
            if image is not None:
                brightness = np.mean(image) / 255.0
                results['brightness'] = brightness
            else:
                results['brightness'] = 0.5
            
            # ë…¸ì´ì¦ˆ ë ˆë²¨ (ê°„ë‹¨í•œ ì¶”ì •)
            if image is not None:
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ì™€ ì›ë³¸ì˜ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
                if CV2_AVAILABLE:
                    blurred = cv2.GaussianBlur(image, (5, 5), 0)
                    noise = np.mean(np.abs(image.astype(float) - blurred.astype(float))) / 255.0
                    results['noise_level'] = 1.0 - min(noise * 10, 1.0)  # ì—­ë°©í–¥ (ë‚®ì€ ë…¸ì´ì¦ˆ = ë†’ì€ ì ìˆ˜)
                else:
                    results['noise_level'] = 0.7
            else:
                results['noise_level'] = 0.5
            
            # ê¸°ìˆ ì  ì ìˆ˜ ì¢…í•©
            tech_scores = [results.get(k, 0.5) for k in ['sharpness', 'contrast', 'brightness', 'noise_level']]
            results['technical_score'] = np.mean(tech_scores)
            
            return results
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'technical_score': 0.5,
                'sharpness': 0.5,
                'contrast': 0.5,
                'brightness': 0.5,
                'noise_level': 0.5
            }
    
    async def _assess_aesthetic_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ğŸ”§ ë¯¸ì  í’ˆì§ˆ í‰ê°€"""
        try:
            results = {}
            
            # ìƒ‰ìƒ ì¡°í™” í‰ê°€
            if image is not None and len(image.shape) == 3:
                # HSV ìƒ‰ìƒ ê³µê°„ì—ì„œ ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
                if CV2_AVAILABLE:
                    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                    hue_hist = cv2.calcHist([hsv], [0], None, [180], [0, 180])
                    
                    # ìƒ‰ìƒ ë¶„í¬ì˜ ê· ë“±ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
                    hue_hist_norm = hue_hist / (hue_hist.sum() + 1e-7)
                    entropy_val = -np.sum(hue_hist_norm * np.log(hue_hist_norm + 1e-7))
                    color_harmony = min(entropy_val / 5.0, 1.0)
                    results['color_harmony'] = color_harmony
                else:
                    results['color_harmony'] = 0.6
            else:
                results['color_harmony'] = 0.5
            
            # êµ¬ì„± ì¼ê´€ì„± (ê°„ë‹¨í•œ ëŒ€ì¹­ì„± ê²€ì‚¬)
            if image is not None:
                h, w = image.shape[:2]
                left_half = image[:, :w//2]
                right_half = image[:, w//2:]
                right_half_flipped = np.fliplr(right_half)
                
                # ì¢Œìš° ëŒ€ì¹­ì„± ê³„ì‚°
                if left_half.shape == right_half_flipped.shape:
                    symmetry = 1.0 - np.mean(np.abs(left_half.astype(float) - right_half_flipped.astype(float))) / 255.0
                    results['consistency'] = max(0.0, symmetry)
                else:
                    results['consistency'] = 0.5
            else:
                results['consistency'] = 0.5
            
            # ë¯¸ì  ì ìˆ˜ ì¢…í•©
            aes_scores = [results.get(k, 0.5) for k in ['color_harmony', 'consistency']]
            results['aesthetic_score'] = np.mean(aes_scores)
            
            return results
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'aesthetic_score': 0.5,
                'color_harmony': 0.5,
                'consistency': 0.5
            }
    
    async def _assess_clip_quality(
        self, 
        fitted_image: np.ndarray, 
        original_image: Optional[np.ndarray]
    ) -> Dict[str, float]:
        """ğŸ”§ CLIP ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        try:
            if not self.clip_model or not self.clip_processor:
                return {'clip_score': 0.5, 'realism': 0.5}
            
            results = {}
            
            # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
            pil_fitted = Image.fromarray(fitted_image.astype(np.uint8))
            
            # í’ˆì§ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            quality_prompts = [
                "a high quality realistic photo",
                "a clear and sharp image",
                "a well-fitted clothing on a person",
                "a natural looking person wearing clothes"
            ]
            
            # CLIP ìœ ì‚¬ë„ ê³„ì‚°
            with torch.no_grad():
                # ì´ë¯¸ì§€ ì¸ì½”ë”©
                image_inputs = self.clip_processor(images=pil_fitted, return_tensors="pt")
                if self.device != "cpu":
                    image_inputs = {k: v.to(self.device) for k, v in image_inputs.items()}
                
                image_features = self.clip_model.get_image_features(**image_inputs)
                
                # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
                text_inputs = self.clip_processor(text=quality_prompts, return_tensors="pt", padding=True)
                if self.device != "cpu":
                    text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
                
                text_features = self.clip_model.get_text_features(**text_inputs)
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarities = torch.cosine_similarity(image_features, text_features, dim=-1)
                clip_score = torch.mean(similarities).item()
                
                # ì •ê·œí™” (CLIP ì ìˆ˜ëŠ” ë³´í†µ -1~1 ë²”ìœ„)
                clip_score = (clip_score + 1) / 2  # 0~1 ë²”ìœ„ë¡œ ë³€í™˜
                
                results['clip_score'] = clip_score
                results['realism'] = clip_score  # í˜„ì‹¤ì„±ì€ CLIP ì ìˆ˜ì™€ ìœ ì‚¬í•˜ê²Œ ì„¤ì •
            
            return results
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ CLIP í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'clip_score': 0.5, 'realism': 0.5}
    
    async def _assess_perceptual_quality(
        self, 
        fitted_image: np.ndarray, 
        original_image: np.ndarray
    ) -> Dict[str, float]:
        """ğŸ”§ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ (ì›ë³¸ ì´ë¯¸ì§€ì™€ ë¹„êµ)"""
        try:
            results = {}
            
            # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            if fitted_image.shape != original_image.shape:
                if CV2_AVAILABLE:
                    target_shape = original_image.shape[:2]
                    fitted_resized = cv2.resize(fitted_image, (target_shape[1], target_shape[0]))
                else:
                    fitted_resized = fitted_image
                    original_image = original_image
            else:
                fitted_resized = fitted_image
            
            # SSIM (Structural Similarity Index) ê³„ì‚°
            if SKIMAGE_AVAILABLE:
                if len(fitted_resized.shape) == 3:
                    ssim_score = ssim(
                        original_image, fitted_resized, 
                        channel_axis=2, data_range=255
                    )
                else:
                    ssim_score = ssim(original_image, fitted_resized, data_range=255)
                
                results['fitting_quality'] = ssim_score
            else:
                # SSIM ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í”½ì…€ ì°¨ì´ë¡œ ëŒ€ì²´
                diff = np.mean(np.abs(fitted_resized.astype(float) - original_image.astype(float))) / 255.0
                results['fitting_quality'] = 1.0 - diff
            
            # ì§€ê°ì  ì ìˆ˜ëŠ” fitting_qualityì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
            results['perceptual_score'] = results['fitting_quality']
            
            return results
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'perceptual_score': 0.5, 'fitting_quality': 0.5}
    
    def _calculate_overall_score(self, metrics: QualityMetrics) -> QualityMetrics:
        """ğŸ”§ ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        try:
            weights = self.config.weights
            
            overall_score = (
                metrics.technical_score * weights.get('technical', 0.4) +
                metrics.aesthetic_score * weights.get('aesthetic', 0.3) +
                metrics.clip_score * weights.get('clip_score', 0.2) +
                metrics.perceptual_score * weights.get('perceptual', 0.1)
            )
            
            metrics.overall_score = max(0.0, min(1.0, overall_score))
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì „ì²´ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            metrics.overall_score = 0.5
            return metrics
    
    def _calculate_confidence(self, metrics: QualityMetrics) -> float:
        """ğŸ”§ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê° ì ìˆ˜ì˜ ë¶„ì‚°ì„ ì´ìš©í•´ ì‹ ë¢°ë„ ê³„ì‚°
            scores = [
                metrics.technical_score,
                metrics.aesthetic_score,
                metrics.clip_score,
                metrics.perceptual_score
            ]
            
            # 0ì´ ì•„ë‹Œ ì ìˆ˜ë“¤ë§Œ ì‚¬ìš©
            valid_scores = [s for s in scores if s > 0]
            
            if len(valid_scores) < 2:
                return 0.5
            
            # ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
            variance = np.var(valid_scores)
            confidence = 1.0 - min(variance * 4, 1.0)  # ë¶„ì‚°ì´ 0.25 ì´ìƒì´ë©´ ì‹ ë¢°ë„ 0
            
            return max(0.0, min(1.0, confidence))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _determine_quality_grade(self, overall_score: float) -> str:
        """ğŸ”§ í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        for grade in QualityGrade:
            if grade.min_score <= overall_score <= grade.max_score:
                return grade.label
        return "Unknown"
    
    def _generate_recommendations(self, metrics: QualityMetrics) -> List[str]:
        """ğŸ”§ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            # ê¸°ìˆ ì  ê°œì„ ì‚¬í•­
            if metrics.sharpness < QUALITY_THRESHOLDS[QualityMetric.SHARPNESS]["good"]:
                recommendations.append("ì´ë¯¸ì§€ ì„ ëª…ë„ ê°œì„  í•„ìš”")
            
            if metrics.contrast < QUALITY_THRESHOLDS[QualityMetric.CONTRAST]["good"]:
                recommendations.append("ëŒ€ë¹„ ì¡°ì • ê¶Œì¥")
            
            if metrics.noise_level < QUALITY_THRESHOLDS[QualityMetric.NOISE_LEVEL]["good"]:
                recommendations.append("ë…¸ì´ì¦ˆ ê°ì†Œ ì²˜ë¦¬ ê¶Œì¥")
            
            # ë¯¸ì  ê°œì„ ì‚¬í•­
            if metrics.color_harmony < QUALITY_THRESHOLDS[QualityMetric.COLOR_HARMONY]["good"]:
                recommendations.append("ìƒ‰ìƒ ì¡°í™” ê°œì„  ê¶Œì¥")
            
            if metrics.consistency < QUALITY_THRESHOLDS[QualityMetric.CONSISTENCY]["good"]:
                recommendations.append("êµ¬ì„± ì¼ê´€ì„± ê°œì„  í•„ìš”")
            
            # CLIP ê¸°ë°˜ ê°œì„ ì‚¬í•­
            if metrics.realism < QUALITY_THRESHOLDS[QualityMetric.REALISM]["good"]:
                recommendations.append("í˜„ì‹¤ê° ê°œì„  í•„ìš”")
            
            # í”¼íŒ… í’ˆì§ˆ ê°œì„ ì‚¬í•­
            if metrics.fitting_quality < QUALITY_THRESHOLDS[QualityMetric.FITTING_QUALITY]["good"]:
                recommendations.append("ê°€ìƒ í”¼íŒ… ì •í™•ë„ ê°œì„  ê¶Œì¥")
            
            # ì „ì²´ì ì¸ ê¶Œì¥ì‚¬í•­
            if metrics.overall_score < 0.6:
                recommendations.append("ì „ë°˜ì ì¸ í’ˆì§ˆ ê°œì„  í•„ìš”")
            elif metrics.overall_score >= 0.9:
                recommendations.append("ìš°ìˆ˜í•œ í’ˆì§ˆì…ë‹ˆë‹¤")
            
            return recommendations[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ"]
    
    def _preprocess_image(self, image: Union[np.ndarray, Image.Image, None]) -> Optional[np.ndarray]:
        """ğŸ”§ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if image is None:
            return None
        
        try:
            if isinstance(image, Image.Image):
                return np.array(image)
            elif isinstance(image, np.ndarray):
                return image
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_cache_key(self, image: Optional[np.ndarray]) -> str:
        """ğŸ”§ ìºì‹œ í‚¤ ìƒì„±"""
        if image is None:
            return "none"
        
        try:
            # ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•)
            image_hash = hash(image.tobytes())
            config_hash = hash(str(self.config.assessment_mode))
            return f"quality_{image_hash}_{config_hash}"
        except:
            return f"quality_{time.time()}"
    
    def _get_models_info(self) -> Dict[str, bool]:
        """ğŸ”§ ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
        return {
            'clip_model': self.clip_model is not None,
            'clip_processor': self.clip_processor is not None,
            'models_loaded': self.models_loaded
        }
    
    def _update_processing_stats(self, processing_time: float):
        """ğŸ”§ ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.processing_stats['total_processed'] += 1
        self.processing_stats['last_processing_time'] = processing_time
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        total = self.processing_stats['total_processed']
        current_avg = self.processing_stats['average_time']
        new_avg = (current_avg * (total - 1) + processing_time) / total
        self.processing_stats['average_time'] = new_avg
    
    @safe_step_method
    def cleanup(self):
        """ğŸ”§ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'clip_model') and self.clip_model:
                del self.clip_model
            
            if hasattr(self, 'clip_processor') and self.clip_processor:
                del self.clip_processor
            
            if hasattr(self, 'quality_cache'):
                self.quality_cache.clear()
            
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                torch.mps.empty_cache()
            
            gc.collect()
            
            self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ğŸ”§ ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'total_processed': self.processing_stats['total_processed'],
            'average_processing_time': self.processing_stats['average_time'],
            'last_processing_time': self.processing_stats['last_processing_time'],
            'cache_size': len(self.quality_cache) if hasattr(self, 'quality_cache') else 0,
            'models_loaded': self.models_loaded,
            'device': self.device
        }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'QualityAssessmentStep',
    'QualityAssessmentConfig',
    'QualityMetrics',
    'QualityGrade',
    'QualityMetric',
    'QUALITY_THRESHOLDS'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
logger.info("âœ… Step 08: Quality Assessment v2.0 ë¡œë“œ ì™„ë£Œ - ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„")