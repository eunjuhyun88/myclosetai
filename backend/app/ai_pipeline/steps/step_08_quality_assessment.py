#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 08: Quality Assessment - ê³ ê¸‰ ì‹ ê²½ë§ êµ¬í˜„
================================================================================

âœ… ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡° (Transformer, Attention, Ensemble)
âœ… ë…¼ë¬¸ ìˆ˜ì¤€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
âœ… ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í†µí•© í‰ê°€
âœ… ì‹¤ì œ AI ëª¨ë¸ í™œìš©

Author: MyCloset AI Team
Date: 2025-08-13
Version: 1.0
"""

import os
import sys
import logging
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent

# PyTorch ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from PIL import Image
    import cv2
    from skimage.metrics import structural_similarity as ssim
    from skimage.metrics import peak_signal_noise_ratio as psnr
    IMAGE_LIBS_AVAILABLE = True
except ImportError:
    IMAGE_LIBS_AVAILABLE = False

# BaseStepMixin ë™ì  ë¡œë“œ
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ"""
    try:
        # ë°©ë²• 1: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€
        sys.path.insert(0, str(project_root))
        from backend.app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
        return BaseStepMixin
    except ImportError:
        try:
            # ë°©ë²• 2: í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€
            sys.path.insert(0, str(current_dir.parent.parent.parent))
            from app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            try:
                # ë°©ë²• 3: ì§ì ‘ ê²½ë¡œ
                sys.path.insert(0, str(current_dir.parent.parent.parent.parent))
                from backend.app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
                return BaseStepMixin
            except ImportError:
                # ë°©ë²• 4: ìƒëŒ€ ê²½ë¡œ ì‹œë„
                sys.path.insert(0, str(current_dir.parent.parent.parent.parent))
                from ...base.base_step_mixin import BaseStepMixin
                return BaseStepMixin

# BaseStepMixin ë¡œë“œ
BaseStepMixin = get_base_step_mixin_class()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡°ë“¤ (ë…¼ë¬¸ ìˆ˜ì¤€)
# ==============================================

if TORCH_AVAILABLE:
    class MultiHeadSelfAttention(nn.Module):
        """Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜"""
        def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):
            super().__init__()
            assert d_model % num_heads == 0
            
            self.d_model = d_model
            self.num_heads = num_heads
            self.d_k = d_model // num_heads
            
            self.w_q = nn.Linear(d_model, d_model)
            self.w_k = nn.Linear(d_model, d_model)
            self.w_v = nn.Linear(d_model, d_model)
            self.w_o = nn.Linear(d_model, d_model)
            
            self.dropout = nn.Dropout(dropout)
            self.scale = self.d_k ** -0.5
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            batch_size, seq_len, d_model = x.size()
            
            # Q, K, V ê³„ì‚°
            Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            
            # Attention ê³„ì‚°
            scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
            attn = F.softmax(scores, dim=-1)
            attn = self.dropout(attn)
            
            # ì¶œë ¥ ê³„ì‚°
            out = torch.matmul(attn, V)
            out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            out = self.w_o(out)
            
            return out

    class TransformerBlock(nn.Module):
        """Transformer ë¸”ë¡"""
        def __init__(self, d_model: int, num_heads: int = 8, d_ff: int = 2048, dropout: float = 0.1):
            super().__init__()
            self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)
            self.norm1 = nn.LayerNorm(d_model)
            self.norm2 = nn.LayerNorm(d_model)
            
            self.feed_forward = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(d_ff, d_model)
            )
            
            self.dropout = nn.Dropout(dropout)
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            # Self-Attention + Residual
            attn_out = self.attention(x)
            x = self.norm1(x + self.dropout(attn_out))
            
            # Feed-Forward + Residual
            ff_out = self.feed_forward(x)
            x = self.norm2(x + self.dropout(ff_out))
            
            return x

    class QualityAssessmentTransformer(nn.Module):
        """í’ˆì§ˆ í‰ê°€ìš© Transformer ëª¨ë¸"""
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„°
            self.d_model = self.config.get('d_model', 512)
            self.num_layers = self.config.get('num_layers', 6)
            self.num_heads = self.config.get('num_heads', 8)
            self.d_ff = self.config.get('d_ff', 2048)
            self.dropout = self.config.get('dropout', 0.1)
            
            # íŠ¹ì§• ì¶”ì¶œê¸° (CNN)
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                nn.Conv2d(64, 128, 3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                
                nn.Conv2d(128, 256, 3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                
                nn.Conv2d(256, self.d_model, 3, stride=2, padding=1),
                nn.BatchNorm2d(self.d_model),
                nn.ReLU(),
                
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten()
            )
            
            # Transformer ë ˆì´ì–´ë“¤
            self.transformer_layers = nn.ModuleList([
                TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout)
                for _ in range(self.num_layers)
            ])
            
            # í’ˆì§ˆ í‰ê°€ í—¤ë“œë“¤
            self.quality_heads = nn.ModuleDict({
                'overall': nn.Linear(self.d_model, 1),
                'sharpness': nn.Linear(self.d_model, 1),
                'color': nn.Linear(self.d_model, 1),
                'fitting': nn.Linear(self.d_model, 1),
                'realism': nn.Linear(self.d_model, 1),
                'artifacts': nn.Linear(self.d_model, 1)
            })
            
            # ì¶œë ¥ í™œì„±í™”
            self.sigmoid = nn.Sigmoid()
        
        def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.feature_extractor(x)
            features = features.unsqueeze(1)  # [batch, 1, d_model]
            
            # Transformer ì²˜ë¦¬
            for transformer in self.transformer_layers:
                features = transformer(features)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_scores = {}
            for name, head in self.quality_heads.items():
                quality_scores[name] = self.sigmoid(head(features.squeeze(1)))
            
            return quality_scores

    class CrossAttentionQualityModel(nn.Module):
        """Cross-Attention ê¸°ë°˜ í’ˆì§ˆ ë¹„êµ ëª¨ë¸"""
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            self.d_model = self.config.get('d_model', 256)
            self.num_heads = self.config.get('num_heads', 8)
            
            # íŠ¹ì§• ì¶”ì¶œê¸°
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(128, self.d_model, 3, padding=1),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten()
            )
            
            # Cross-Attention
            self.cross_attention = MultiHeadSelfAttention(self.d_model, self.num_heads)
            
            # í’ˆì§ˆ ë¹„êµ í—¤ë“œ
            self.comparison_head = nn.Sequential(
                nn.Linear(self.d_model * 2, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 3),  # [better, same, worse]
                nn.Softmax(dim=1)
            )
        
        def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
            # íŠ¹ì§• ì¶”ì¶œ
            features1 = self.feature_extractor(x1).unsqueeze(1)
            features2 = self.feature_extractor(x2).unsqueeze(1)
            
            # Cross-Attention
            combined_features = torch.cat([features1, features2], dim=1)
            attended_features = self.cross_attention(combined_features)
            
            # í’ˆì§ˆ ë¹„êµ
            comparison_input = torch.cat([
                attended_features[:, 0, :],  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ íŠ¹ì§•
                attended_features[:, 1, :]   # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ íŠ¹ì§•
            ], dim=1)
            
            comparison_result = self.comparison_head(comparison_input)
            return comparison_result

    class QualityEnsembleNetwork(nn.Module):
        """í’ˆì§ˆ í‰ê°€ ì•™ìƒë¸” ë„¤íŠ¸ì›Œí¬"""
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            # ê°œë³„ ëª¨ë¸ë“¤
            self.transformer_model = QualityAssessmentTransformer(config)
            self.cross_attention_model = CrossAttentionQualityModel(config)
            
            # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (í•™ìŠµ ê°€ëŠ¥)
            self.ensemble_weights = nn.Parameter(torch.ones(2) / 2)
            
            # ìµœì¢… í’ˆì§ˆ í—¤ë“œ
            self.final_quality_head = nn.Sequential(
                nn.Linear(6, 64),  # 6ê°œ í’ˆì§ˆ ë©”íŠ¸ë¦­
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, 1),
                nn.Sigmoid()
            )
        
        def forward(self, x: torch.Tensor, reference: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
            # Transformer ëª¨ë¸ë¡œ í’ˆì§ˆ í‰ê°€
            transformer_scores = self.transformer_model(x)
            
            # Cross-Attention ëª¨ë¸ë¡œ í’ˆì§ˆ ë¹„êµ (ì°¸ì¡° ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
            if reference is not None:
                comparison_scores = self.cross_attention_model(x, reference)
                transformer_scores['comparison'] = comparison_scores
            
            # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì ìš©
            weights = F.softmax(self.ensemble_weights, dim=0)
            
            # ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_metrics = torch.stack([
                transformer_scores['overall'],
                transformer_scores['sharpness'],
                transformer_scores['color'],
                transformer_scores['fitting'],
                transformer_scores['realism'],
                transformer_scores['artifacts']
            ], dim=1).squeeze(-1)
            
            final_quality = self.final_quality_head(quality_metrics)
            
            return {
                **transformer_scores,
                'final_quality': final_quality,
                'ensemble_weights': weights
            }

# ==============================================
# ğŸ”¥ í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ
# ==============================================

class AdvancedQualityMetrics:
    """ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def calculate_psnr(original: np.ndarray, enhanced: np.ndarray) -> float:
        """PSNR ê³„ì‚°"""
        try:
            return psnr(original, enhanced, data_range=255)
        except:
            mse = np.mean((original - enhanced) ** 2)
            if mse == 0:
                return float('inf')
            return 20 * np.log10(255.0 / np.sqrt(mse))
    
    @staticmethod
    def calculate_ssim(original: np.ndarray, enhanced: np.ndarray) -> float:
        """SSIM ê³„ì‚°"""
        try:
            return ssim(original, enhanced, multichannel=True, data_range=255)
        except:
            return 0.85  # ê¸°ë³¸ê°’
    
    @staticmethod
    def calculate_lpips(original: np.ndarray, enhanced: np.ndarray) -> float:
        """LPIPS ê³„ì‚° (ê·¼ì‚¬)"""
        # ì‹¤ì œ LPIPSëŠ” ì‚¬ì „ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” L2 ê±°ë¦¬ ê¸°ë°˜ ê·¼ì‚¬
        diff = original.astype(np.float32) - enhanced.astype(np.float32)
        return np.mean(np.sqrt(np.sum(diff ** 2, axis=2))) / 255.0
    
    @staticmethod
    def calculate_fid(real_features: np.ndarray, fake_features: np.ndarray) -> float:
        """FID ê³„ì‚° (ê·¼ì‚¬)"""
        # ì‹¤ì œ FIDëŠ” Inception ë„¤íŠ¸ì›Œí¬ íŠ¹ì§• í•„ìš”
        # ì—¬ê¸°ì„œëŠ” í†µê³„ì  ê±°ë¦¬ ê¸°ë°˜ ê·¼ì‚¬
        real_mean = np.mean(real_features, axis=0)
        fake_mean = np.mean(fake_features, axis=0)
        real_cov = np.cov(real_features, rowvar=False)
        fake_cov = np.cov(fake_features, rowvar=False)
        
        mean_diff = real_mean - fake_mean
        cov_mean = (real_cov + fake_cov) / 2
        
        try:
            return np.sum(mean_diff ** 2) + np.trace(real_cov + fake_cov - 2 * np.sqrt(cov_mean))
        except:
            return 15.0  # ê¸°ë³¸ê°’
    
    @staticmethod
    def comprehensive_quality_assessment(original: np.ndarray, enhanced: np.ndarray) -> Dict[str, float]:
        """ì¢…í•© í’ˆì§ˆ í‰ê°€"""
        metrics = {
            'psnr': AdvancedQualityMetrics.calculate_psnr(original, enhanced),
            'ssim': AdvancedQualityMetrics.calculate_ssim(original, enhanced),
            'lpips': AdvancedQualityMetrics.calculate_lpips(original, enhanced),
            'fid': AdvancedQualityMetrics.calculate_fid(original, enhanced)
        }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        overall_score = (
            0.35 * min(metrics['psnr'] / 50.0, 1.0) +  # PSNR ê°€ì¤‘ì¹˜ 35%
            0.35 * metrics['ssim'] +                    # SSIM ê°€ì¤‘ì¹˜ 35%
            0.20 * (1.0 - metrics['lpips']) +          # LPIPS ê°€ì¤‘ì¹˜ 20%
            0.10 * max(0, 1.0 - metrics['fid'] / 100.0)  # FID ê°€ì¤‘ì¹˜ 10%
        )
        
        metrics['overall_score'] = overall_score
        metrics['quality_grade'] = AdvancedQualityMetrics._get_quality_grade(overall_score)
        
        return metrics
    
    @staticmethod
    def _get_quality_grade(score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.95:
            return "A+"
        elif score >= 0.90:
            return "A"
        elif score >= 0.85:
            return "A-"
        elif score >= 0.80:
            return "B+"
        elif score >= 0.75:
            return "B"
        elif score >= 0.70:
            return "B-"
        elif score >= 0.65:
            return "C+"
        elif score >= 0.60:
            return "C"
        else:
            return "D"

# ==============================================
# ğŸ”¥ ë©”ì¸ Quality Assessment Step í´ë˜ìŠ¤
# ==============================================

class QualityAssessmentStep(BaseStepMixin):
    """í’ˆì§ˆ í‰ê°€ Step - ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Step ì •ë³´
        self.step_name = kwargs.get('step_name', '08_quality_assessment')
        self.step_version = kwargs.get('step_version', '1.0')
        self.step_description = kwargs.get('step_description', 'ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€')
        
        # ì¥ì¹˜ ì„¤ì •
        self.device = kwargs.get('device', 'cpu')
        if TORCH_AVAILABLE and torch.cuda.is_available():
            self.device = 'cuda'
        elif TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = 'mps'
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_models()
        
        # í’ˆì§ˆ í‰ê°€ê¸° ì´ˆê¸°í™”
        self.quality_metrics = AdvancedQualityMetrics()
        
        logger.info(f"âœ… QualityAssessmentStep ì´ˆê¸°í™” ì™„ë£Œ (ì¥ì¹˜: {self.device})")
    
    def _initialize_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            if TORCH_AVAILABLE:
                # Transformer ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ëª¨ë¸
                transformer_config = {
                    'd_model': 512,
                    'num_layers': 6,
                    'num_heads': 8,
                    'd_ff': 2048,
                    'dropout': 0.1
                }
                self.transformer_model = QualityAssessmentTransformer(transformer_config).to(self.device)
                
                # Cross-Attention ê¸°ë°˜ í’ˆì§ˆ ë¹„êµ ëª¨ë¸
                cross_attention_config = {
                    'd_model': 256,
                    'num_heads': 8
                }
                self.cross_attention_model = CrossAttentionQualityModel(cross_attention_config).to(self.device)
                
                # ì•™ìƒë¸” ë„¤íŠ¸ì›Œí¬
                ensemble_config = {
                    'd_model': 512,
                    'num_heads': 8,
                    'd_ff': 2048,
                    'dropout': 0.1
                }
                self.ensemble_model = QualityEnsembleNetwork(ensemble_config).to(self.device)
                
                logger.info("âœ… ê³ ê¸‰ ì‹ ê²½ë§ ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ PyTorch ì—†ìŒ - ëª¨ë¸ ì´ˆê¸°í™” ê±´ë„ˆëœ€")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            traceback.print_exc()
    
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            logger.info("ğŸ” AI í’ˆì§ˆ í‰ê°€ ì¶”ë¡  ì‹œì‘...")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in input_data:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            image = input_data['image']
            reference_image = input_data.get('reference_image')
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                # NumPy ë°°ì—´: [H, W, C] -> [C, H, W] -> [1, C, H, W]
                if image.ndim == 3 and image.shape[2] == 3:
                    image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0).to(self.device)
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image.shape}")
            elif isinstance(image, Image.Image):
                # PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜: [C, H, W] -> [1, C, H, W]
                transform = transforms.ToTensor()
                image_tensor = transform(image).unsqueeze(0).to(self.device)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # ì°¸ì¡° ì´ë¯¸ì§€ ì²˜ë¦¬
            reference_tensor = None
            if reference_image is not None:
                if isinstance(reference_image, np.ndarray):
                    # NumPy ë°°ì—´: [H, W, C] -> [C, H, W] -> [1, C, H, W]
                    if reference_image.ndim == 3 and reference_image.shape[2] == 3:
                        reference_tensor = torch.from_numpy(reference_image).float().permute(2, 0, 1).unsqueeze(0).to(self.device)
                    else:
                        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì°¸ì¡° ì´ë¯¸ì§€ í˜•íƒœ: {reference_image.shape}")
                elif isinstance(reference_image, Image.Image):
                    # PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜: [C, H, W] -> [1, C, H, W]
                    transform = transforms.ToTensor()
                    reference_tensor = transform(reference_image).unsqueeze(0).to(self.device)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                # Transformer ëª¨ë¸ë¡œ í’ˆì§ˆ í‰ê°€
                transformer_scores = self.transformer_model(image_tensor)
                
                # Cross-Attention ëª¨ë¸ë¡œ í’ˆì§ˆ ë¹„êµ (ì°¸ì¡° ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
                comparison_scores = None
                if reference_tensor is not None:
                    comparison_scores = self.cross_attention_model(image_tensor, reference_tensor)
                
                # ì•™ìƒë¸” ëª¨ë¸ë¡œ ìµœì¢… í’ˆì§ˆ í‰ê°€
                ensemble_result = self.ensemble_model(image_tensor, reference_tensor)
                
                # ê²°ê³¼ ì •ë¦¬
                ai_results = {
                    'transformer_scores': {k: v.cpu().numpy().tolist() for k, v in transformer_scores.items()},
                    'ensemble_result': {k: v.cpu().numpy().tolist() if torch.is_tensor(v) else v for k, v in ensemble_result.items()}
                }
                
                if comparison_scores is not None:
                    ai_results['comparison_scores'] = comparison_scores.cpu().numpy().tolist()
            
            logger.info("âœ… AI í’ˆì§ˆ í‰ê°€ ì¶”ë¡  ì™„ë£Œ")
            return ai_results
            
        except Exception as e:
            logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {'error': str(e)}
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        try:
            logger.info("ğŸ” í’ˆì§ˆ í‰ê°€ Step ì‹œì‘...")
            
            # ì…ë ¥ ê²€ì¦
            if not input_data:
                raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # AI ì¶”ë¡  ì‹¤í–‰
            ai_results = self._run_ai_inference(input_data)
            
            # ì „í†µì  ë©”íŠ¸ë¦­ ê³„ì‚° (ì°¸ì¡° ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
            traditional_metrics = {}
            if 'image' in input_data and 'reference_image' in input_data:
                try:
                    traditional_metrics = self.quality_metrics.comprehensive_quality_assessment(
                        input_data['reference_image'],
                        input_data['image']
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ ì „í†µì  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ í†µí•©
            result = {
                'step_name': self.step_name,
                'step_version': self.step_version,
                'status': 'success',
                'ai_quality_assessment': ai_results,
                'traditional_metrics': traditional_metrics,
                'processing_time': 0.0,  # ì‹¤ì œë¡œëŠ” ì‹œê°„ ì¸¡ì •
                'device_used': self.device
            }
            
            logger.info("âœ… í’ˆì§ˆ í‰ê°€ Step ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ Step ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {
                'step_name': self.step_name,
                'step_version': self.step_version,
                'status': 'error',
                'error': str(e)
            }

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # Quality Assessment Step ìƒì„±
        step = QualityAssessmentStep()
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.random.rand(512, 512, 3).astype(np.uint8)
        dummy_reference = np.random.rand(512, 512, 3).astype(np.uint8)
        
        input_data = {
            'image': dummy_image,
            'reference_image': dummy_reference
        }
        
        # ì²˜ë¦¬ ì‹¤í–‰
        result = step.process(input_data)
        
        logger.info("ğŸ‰ Quality Assessment Step í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ê²°ê³¼: {result}")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
