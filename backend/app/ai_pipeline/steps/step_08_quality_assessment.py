# app/ai_pipeline/steps/step_08_quality_assessment.py
"""
‚úÖ MyCloset AI - 8Îã®Í≥Ñ: ÌíàÏßà ÌèâÍ∞Ä (Quality Assessment) - ÌïµÏã¨ Í∏∞Îä• ÏôÑÏ†ÑÌåê
‚úÖ AI Î™®Îç∏ Î°úÎçî ÏôÑÎ≤Ω Ïó∞Îèô + ÏãúÍ∞ÅÌôî ÏßÄÏõê
‚úÖ Pipeline Manager 100% Ìò∏Ìôò
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ Ïã§Ï†ú ÏûëÎèôÌïòÎäî Î™®Îì† ÌïµÏã¨ Í∏∞Îä• Ìè¨Ìï® (Ï§ëÍ∞Ñ Î≤ÑÏ†Ñ)

ÌååÏùº ÏúÑÏπò: backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""

import os
import sys
import logging
import time
import asyncio
import json
import gc
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
import base64
import io

# ÌïÑÏàò Ìå®ÌÇ§ÏßÄÎì§ - ÏïàÏ†ÑÌïú import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

try:
    from PIL import Image, ImageStat, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from skimage import feature
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

# AI Î™®Îç∏ Î°úÎçî Ïó∞Îèô
try:
    from app.ai_pipeline.utils.model_loader import BaseStepMixin, get_global_model_loader
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    MODEL_LOADER_AVAILABLE = False
    BaseStepMixin = object

logger = logging.getLogger(__name__)

# ==============================================
# üî• Ïó¥Í±∞Ìòï Î∞è Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§
# ==============================================

class QualityGrade(Enum):
    """ÌíàÏßà Îì±Í∏â"""
    EXCELLENT = "excellent"      # 90-100Ï†ê
    GOOD = "good"               # 75-89Ï†ê
    ACCEPTABLE = "acceptable"   # 60-74Ï†ê
    POOR = "poor"              # 40-59Ï†ê
    VERY_POOR = "very_poor"    # 0-39Ï†ê

@dataclass
class QualityMetrics:
    """ÌíàÏßà Î©îÌä∏Î¶≠"""
    # Í∏∞Ïà†Ï†Å ÌíàÏßà
    sharpness: float = 0.0
    noise_level: float = 0.0
    contrast: float = 0.0
    saturation: float = 0.0
    brightness: float = 0.0
    color_accuracy: float = 0.0
    
    # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà
    structural_similarity: float = 0.0
    perceptual_similarity: float = 0.0
    visual_quality: float = 0.0
    artifact_level: float = 0.0
    
    # ÎØ∏Ï†Å ÌíàÏßà
    composition: float = 0.0
    color_harmony: float = 0.0
    symmetry: float = 0.0
    balance: float = 0.0
    
    # Í∏∞Îä•Ï†Å ÌíàÏßà
    fitting_quality: float = 0.0
    edge_preservation: float = 0.0
    texture_quality: float = 0.0
    detail_preservation: float = 0.0
    
    # Ï†ÑÏ≤¥ Ï†êÏàò
    overall_score: float = 0.0
    confidence: float = 0.0
    
    def calculate_overall_score(self, weights: Optional[Dict[str, float]] = None) -> float:
        """Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞"""
        if weights is None:
            weights = {'technical': 0.3, 'perceptual': 0.3, 'aesthetic': 0.2, 'functional': 0.2}
        
        technical_score = np.mean([self.sharpness, self.contrast, self.color_accuracy, 1.0 - self.noise_level])
        perceptual_score = np.mean([self.structural_similarity, self.perceptual_similarity, self.visual_quality, 1.0 - self.artifact_level])
        aesthetic_score = np.mean([self.composition, self.color_harmony, self.symmetry, self.balance])
        functional_score = np.mean([self.fitting_quality, self.edge_preservation, self.texture_quality, self.detail_preservation])
        
        self.overall_score = (
            technical_score * weights['technical'] +
            perceptual_score * weights['perceptual'] +
            aesthetic_score * weights['aesthetic'] +
            functional_score * weights['functional']
        )
        return self.overall_score
    
    def get_grade(self) -> QualityGrade:
        """Îì±Í∏â Î∞òÌôò"""
        score = self.overall_score * 100
        if score >= 90: return QualityGrade.EXCELLENT
        elif score >= 75: return QualityGrade.GOOD
        elif score >= 60: return QualityGrade.ACCEPTABLE
        elif score >= 40: return QualityGrade.POOR
        else: return QualityGrade.VERY_POOR
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

# ==============================================
# üî• AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
# ==============================================

class PerceptualQualityModel(nn.Module):
    """ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏"""
    
    def __init__(self):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((8, 8))
        )
        self.quality_predictor = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 1), nn.Sigmoid()
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        return self.quality_predictor(features)

class AestheticQualityModel(nn.Module):
    """ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏"""
    
    def __init__(self):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            self._make_layer(64, 64, 2), self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2), nn.AdaptiveAvgPool2d((1, 1))
        )
        self.aesthetic_head = nn.Sequential(
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 4), nn.Sigmoid()
        )
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        for i in range(blocks):
            layers.append(nn.Sequential(
                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, 3, 
                         stride=stride if i == 0 else 1, padding=1),
                nn.BatchNorm2d(out_channels), nn.ReLU()
            ))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        return self.aesthetic_head(features)

class TechnicalQualityAnalyzer:
    """Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
    
    def analyze_sharpness(self, image: np.ndarray) -> float:
        """ÏÑ†Î™ÖÎèÑ Î∂ÑÏÑù - ÎùºÌîåÎùºÏãúÏïà Î∂ÑÏÇ∞ Í∏∞Î∞ò"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                return min(laplacian_var / 1000.0, 1.0)
            else:
                pil_img = Image.fromarray(image).convert('L')
                edges = pil_img.filter(ImageFilter.FIND_EDGES)
                stat = ImageStat.Stat(edges)
                return min(stat.stddev[0] / 50.0, 1.0)
        except:
            return 0.5
    
    def analyze_noise_level(self, image: np.ndarray) -> float:
        """ÎÖ∏Ïù¥Ï¶à Î†àÎ≤® Î∂ÑÏÑù - Í≥†Ï£ºÌåå ÏÑ±Î∂Ñ Í∏∞Î∞ò"""
        try:
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                filtered = cv2.filter2D(gray, -1, kernel)
                return min(np.std(filtered) / 255.0, 1.0)
            else:
                gray = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
                return min(np.std(gray) / 128.0, 1.0)
        except:
            return 0.3
    
    def analyze_contrast(self, image: np.ndarray) -> float:
        """ÎåÄÎπÑ Î∂ÑÏÑù"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            return min(gray.std() / 128.0, 1.0)
        except:
            return 0.5
    
    def analyze_color_accuracy(self, original: np.ndarray, processed: np.ndarray) -> float:
        """ÏÉâÏÉÅ Ï†ïÌôïÎèÑ Î∂ÑÏÑù - ÌûàÏä§ÌÜ†Í∑∏Îû® ÎπÑÍµê"""
        try:
            if CV2_AVAILABLE and original is not None:
                hist_orig = [cv2.calcHist([original], [i], None, [256], [0, 256]) for i in range(3)]
                hist_proc = [cv2.calcHist([processed], [i], None, [256], [0, 256]) for i in range(3)]
                correlations = [cv2.compareHist(hist_orig[i], hist_proc[i], cv2.HISTCMP_CORREL) for i in range(3)]
                return np.mean(correlations)
            else:
                return 0.8  # Í∏∞Î≥∏Í∞í
        except:
            return 0.7

# ==============================================
# üî• ÏãúÍ∞ÅÌôî ÏÉùÏÑ±Í∏∞
# ==============================================

class QualityVisualizationGenerator:
    """ÌíàÏßà ÌèâÍ∞Ä ÏãúÍ∞ÅÌôî ÏÉùÏÑ±Í∏∞"""
    
    def __init__(self):
        self.colors = {
            'excellent': '#2E8B57', 'good': '#32CD32', 'acceptable': '#FFD700',
            'poor': '#FF8C00', 'very_poor': '#DC143C'
        }
    
    def generate_quality_heatmap(self, metrics: QualityMetrics) -> Optional[str]:
        """ÌíàÏßà ÌûàÌä∏Îßµ ÏÉùÏÑ±"""
        if not MATPLOTLIB_AVAILABLE:
            return None
        
        try:
            categories = ['Technical', 'Perceptual', 'Aesthetic', 'Functional']
            technical_metrics = [metrics.sharpness, metrics.contrast, metrics.color_accuracy, 1.0-metrics.noise_level]
            perceptual_metrics = [metrics.structural_similarity, metrics.perceptual_similarity, metrics.visual_quality, 1.0-metrics.artifact_level]
            aesthetic_metrics = [metrics.composition, metrics.color_harmony, metrics.symmetry, metrics.balance]
            functional_metrics = [metrics.fitting_quality, metrics.edge_preservation, metrics.texture_quality, metrics.detail_preservation]
            
            data = np.array([technical_metrics, perceptual_metrics, aesthetic_metrics, functional_metrics])
            
            plt.figure(figsize=(12, 8))
            sns.heatmap(data, xticklabels=['Metric 1', 'Metric 2', 'Metric 3', 'Metric 4'],
                       yticklabels=categories, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,
                       cbar_kws={'label': 'Quality Score'})
            
            plt.title(f'Quality Assessment Heatmap\nOverall Score: {metrics.overall_score:.3f} ({metrics.get_grade().value})', 
                     fontsize=16, fontweight='bold')
            plt.tight_layout()
            
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            return image_base64
        except Exception as e:
            logger.error(f"ÌûàÌä∏Îßµ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def generate_quality_bar_chart(self, metrics: QualityMetrics) -> Optional[str]:
        """ÌíàÏßà Î∞î Ï∞®Ìä∏ ÏÉùÏÑ±"""
        if not MATPLOTLIB_AVAILABLE:
            return None
        
        try:
            categories = ['Technical', 'Perceptual', 'Aesthetic', 'Functional', 'Overall']
            technical_avg = np.mean([metrics.sharpness, metrics.contrast, metrics.color_accuracy, 1.0-metrics.noise_level])
            perceptual_avg = np.mean([metrics.structural_similarity, metrics.perceptual_similarity, metrics.visual_quality, 1.0-metrics.artifact_level])
            aesthetic_avg = np.mean([metrics.composition, metrics.color_harmony, metrics.symmetry, metrics.balance])
            functional_avg = np.mean([metrics.fitting_quality, metrics.edge_preservation, metrics.texture_quality, metrics.detail_preservation])
            
            scores = [technical_avg, perceptual_avg, aesthetic_avg, functional_avg, metrics.overall_score]
            colors = [self._get_score_color(score) for score in scores]
            
            plt.figure(figsize=(12, 8))
            bars = plt.bar(categories, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
            
            for bar, score in zip(bars, scores):
                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                        f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            plt.title(f'Quality Assessment - Grade: {metrics.get_grade().value.upper()}', fontsize=16, fontweight='bold')
            plt.xlabel('Quality Categories', fontsize=12)
            plt.ylabel('Quality Score', fontsize=12)
            plt.ylim(0, 1.1)
            plt.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            return image_base64
        except Exception as e:
            logger.error(f"Î∞î Ï∞®Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def generate_radar_chart(self, metrics: QualityMetrics) -> Optional[str]:
        """Î†àÏù¥Îçî Ï∞®Ìä∏ ÏÉùÏÑ±"""
        if not MATPLOTLIB_AVAILABLE:
            return None
        
        try:
            categories = ['Technical\nQuality', 'Perceptual\nQuality', 'Aesthetic\nQuality', 'Functional\nQuality']
            technical_avg = np.mean([metrics.sharpness, metrics.contrast, metrics.color_accuracy, 1.0-metrics.noise_level])
            perceptual_avg = np.mean([metrics.structural_similarity, metrics.perceptual_similarity, metrics.visual_quality, 1.0-metrics.artifact_level])
            aesthetic_avg = np.mean([metrics.composition, metrics.color_harmony, metrics.symmetry, metrics.balance])
            functional_avg = np.mean([metrics.fitting_quality, metrics.edge_preservation, metrics.texture_quality, metrics.detail_preservation])
            
            scores = [technical_avg, perceptual_avg, aesthetic_avg, functional_avg]
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
            scores += scores[:1]
            angles = np.concatenate((angles, [angles[0]]))
            
            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
            ax.plot(angles, scores, 'o-', linewidth=3, label='Quality Scores', color='#1f77b4')
            ax.fill(angles, scores, alpha=0.25, color='#1f77b4')
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories, fontsize=12)
            ax.set_ylim(0, 1)
            ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
            ax.grid(True)
            
            plt.title(f'Quality Assessment Radar Chart\nOverall: {metrics.overall_score:.3f} | Grade: {metrics.get_grade().value.upper()}', 
                     fontsize=14, fontweight='bold', pad=20)
            plt.tight_layout()
            
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            return image_base64
        except Exception as e:
            logger.error(f"Î†àÏù¥Îçî Ï∞®Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _get_score_color(self, score: float) -> str:
        """Ï†êÏàòÏóê Îî∞Î•∏ ÏÉâÏÉÅ Î∞òÌôò"""
        if score >= 0.9: return self.colors['excellent']
        elif score >= 0.75: return self.colors['good']
        elif score >= 0.6: return self.colors['acceptable']
        elif score >= 0.4: return self.colors['poor']
        else: return self.colors['very_poor']

# ==============================================
# üî• Î©îÏù∏ QualityAssessmentStep ÌÅ¥ÎûòÏä§
# ==============================================

class QualityAssessmentStep(BaseStepMixin):
    """
    ‚úÖ 8Îã®Í≥Ñ: ÌíàÏßà ÌèâÍ∞Ä ÏãúÏä§ÌÖú - ÌïµÏã¨ Í∏∞Îä• ÏôÑÏ†ÑÌåê
    ‚úÖ AI Î™®Îç∏ Î°úÎçîÏôÄ ÏôÑÎ≤Ω Ïó∞Îèô
    ‚úÖ Pipeline Manager Ìò∏ÌôòÏÑ±
    ‚úÖ M3 Max ÏµúÏ†ÅÌôî
    ‚úÖ Ïã§Ï†ú ÏûëÎèôÌïòÎäî Î™®Îì† ÌïµÏã¨ Í∏∞Îä•
    """
    
    # ÏùòÎ•ò ÌÉÄÏûÖÎ≥Ñ ÌíàÏßà Í∞ÄÏ§ëÏπò
    CLOTHING_QUALITY_WEIGHTS = {
        'shirt': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'dress': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'pants': {'fitting': 0.6, 'texture': 0.2, 'edge': 0.1, 'color': 0.1},
        'jacket': {'fitting': 0.3, 'texture': 0.4, 'edge': 0.2, 'color': 0.1},
        'default': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1}
    }
    
    # ÏõêÎã® ÌÉÄÏûÖÎ≥Ñ ÌíàÏßà Í∏∞Ï§Ä
    FABRIC_QUALITY_STANDARDS = {
        'cotton': {'texture_importance': 0.8, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.3},
        'silk': {'texture_importance': 0.9, 'drape_importance': 0.9, 'wrinkle_tolerance': 0.2},
        'wool': {'texture_importance': 0.7, 'drape_importance': 0.7, 'wrinkle_tolerance': 0.4},
        'denim': {'texture_importance': 0.9, 'drape_importance': 0.4, 'wrinkle_tolerance': 0.6},
        'leather': {'texture_importance': 0.95, 'drape_importance': 0.3, 'wrinkle_tolerance': 0.9},
        'default': {'texture_importance': 0.7, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.5}
    }
    
    def __init__(self, device: Optional[str] = None, config: Optional[Dict[str, Any]] = None, **kwargs):
        """‚úÖ ÌÜµÏùºÎêú ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ - Pipeline Manager ÏôÑÎ≤Ω Ìò∏Ìôò"""
        
        # 1. Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # 2. ÏãúÏä§ÌÖú Ï†ïÎ≥¥
        self.device_type = kwargs.get('device_type', self._get_device_type())
        self.memory_gb = float(kwargs.get('memory_gb', self._get_memory_gb()))
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # 3. Ï¥àÍ∏∞Ìôî
        self.is_initialized = False
        self.initialization_error = None
        self.performance_stats = {
            'total_assessments': 0, 'total_time': 0.0, 'average_time': 0.0,
            'last_assessment_time': 0.0, 'average_score': 0.0, 'error_count': 0
        }
        
        # 4. ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        try:
            self._initialize_step_specific()
            self._setup_model_loader()
            self._initialize_analyzers()
            self._initialize_visualizer()
            self._setup_assessment_pipeline()
            self.is_initialized = True
            self.logger.info(f"‚úÖ {self.step_name} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - M3 Max: {self.is_m3_max}")
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"‚ùå {self.step_name} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ"""
        if device: return device
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available(): return "mps"
            elif torch.cuda.is_available(): return "cuda"
        return "cpu"
    
    def _get_device_type(self) -> str:
        if self.device == "mps": return "apple_silicon"
        elif self.device == "cuda": return "nvidia_gpu"
        else: return "cpu"
    
    def _get_memory_gb(self) -> float:
        try:
            if self.is_m3_max: return 128.0
            else:
                import psutil
                return psutil.virtual_memory().total / (1024**3)
        except: return 16.0
    
    def _detect_m3_max(self) -> bool:
        try:
            import platform
            if platform.system() == "Darwin":
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True)
                return "M3" in result.stdout and "Max" in result.stdout
        except: pass
        return False
    
    def _setup_model_loader(self):
        """‚úÖ AI Î™®Îç∏ Î°úÎçî ÏôÑÎ≤Ω Ïó∞Îèô"""
        try:
            if MODEL_LOADER_AVAILABLE:
                self.global_model_loader = get_global_model_loader()
                self.model_interface = self.global_model_loader.create_step_interface(
                    step_name=self.step_name, device=self.device, memory_gb=self.memory_gb
                )
                asyncio.create_task(self._load_recommended_models())
                self.logger.info(f"üîó {self.step_name} Î™®Îç∏ Î°úÎçî ÏôÑÎ≤Ω Ïó∞Îèô ÏôÑÎ£å")
            else:
                self.model_interface = None
                self.logger.warning("‚ö†Ô∏è Î™®Îç∏ Î°úÎçî Ïó∞Îèô Ïã§Ìå®, ÎÇ¥Ïû• Î™®Îç∏ ÏÇ¨Ïö©")
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎçî Ïó∞Îèô Ïò§Î•ò: {e}")
            self.model_interface = None
    
    def _initialize_step_specific(self):
        """8Îã®Í≥Ñ Ï†ÑÏö© Ï¥àÍ∏∞Ìôî"""
        self.assessment_config = {
            'mode': self.config.get('assessment_mode', 'comprehensive'),
            'technical_analysis_enabled': self.config.get('technical_analysis_enabled', True),
            'perceptual_analysis_enabled': self.config.get('perceptual_analysis_enabled', True),
            'aesthetic_analysis_enabled': self.config.get('aesthetic_analysis_enabled', True),
            'functional_analysis_enabled': self.config.get('functional_analysis_enabled', True),
            'neural_analysis_enabled': self.config.get('neural_analysis_enabled', True),
            'visualization_enabled': self.config.get('visualization_enabled', True)
        }
        
        self.quality_thresholds = {
            'excellent': 0.9, 'good': 0.75, 'acceptable': 0.6, 'poor': 0.4,
            'minimum_acceptable': self.config.get('minimum_quality', 0.6)
        }
        
        self.optimization_level = 'maximum' if self.is_m3_max else ('high' if self.memory_gb >= 32 else 'basic')
        self.parallel_analysis = self.is_m3_max
        
        cache_size = min(200 if self.is_m3_max else 100, int(self.memory_gb * 3))
        self.assessment_cache = {}
        self.cache_max_size = cache_size
        
        self.logger.info(f"üìä 8Îã®Í≥Ñ ÏÑ§Ï†ï ÏôÑÎ£å - Î™®Îìú: {self.assessment_config['mode']}, ÏµúÏ†ÅÌôî: {self.optimization_level}")
    
    def _initialize_analyzers(self):
        """Î∂ÑÏÑùÍ∏∞Îì§ Ï¥àÍ∏∞Ìôî"""
        try:
            # 1. Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑùÍ∏∞
            self.technical_analyzer = TechnicalQualityAnalyzer(self.device)
            
            # 2. AI Î™®Îç∏Îì§ Ï¥àÍ∏∞Ìôî
            self._initialize_ai_models()
            
            self.logger.info("üîß Î™®Îì† Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        except Exception as e:
            self.logger.error(f"‚ùå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise
    
    def _initialize_ai_models(self):
        """‚úÖ AI Î™®Îç∏Îì§ ÏôÑÏ†Ñ Ï¥àÍ∏∞Ìôî"""
        self.ai_models = {}
        
        if not TORCH_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è PyTorch ÏóÜÏùå, AI Î™®Îç∏ Í∏∞Îä• ÎπÑÌôúÏÑ±Ìôî")
            return
        
        try:
            # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏
            if self.assessment_config['perceptual_analysis_enabled']:
                self.ai_models['perceptual_quality'] = PerceptualQualityModel()
                self.ai_models['perceptual_quality'].to(self.device)
                self.ai_models['perceptual_quality'].eval()
            
            # ÎØ∏Ï†Å ÌíàÏßà ÌèâÍ∞Ä Î™®Îç∏
            if self.assessment_config['aesthetic_analysis_enabled']:
                self.ai_models['aesthetic_quality'] = AestheticQualityModel()
                self.ai_models['aesthetic_quality'].to(self.device)
                self.ai_models['aesthetic_quality'].eval()
            
            # M3 Max ÏµúÏ†ÅÌôî
            if self.is_m3_max and self.device == "mps":
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'half'):
                        model.half()
                    # Neural Engine ÏµúÏ†ÅÌôî
                    if hasattr(torch.backends, 'mps') and hasattr(torch, 'compile'):
                        try:
                            self.ai_models[model_name] = torch.compile(model, backend='inductor')
                            self.logger.info(f"üçé {model_name} M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©")
                        except:
                            self.logger.info(f"‚ÑπÔ∏è {model_name} Ïª¥ÌååÏùº ÏµúÏ†ÅÌôî ÎØ∏ÏßÄÏõê")
            
            self.logger.info(f"üß† AI Î™®Îç∏ {len(self.ai_models)}Í∞ú Î°úÎìú ÏôÑÎ£å")
        except Exception as e:
            self.logger.error(f"‚ùå AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.ai_models = {}
    
    def _initialize_visualizer(self):
        """‚úÖ ÏãúÍ∞ÅÌôî ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî"""
        try:
            self.visualizer = QualityVisualizationGenerator()
            self.visualization_enabled = self.assessment_config['visualization_enabled'] and MATPLOTLIB_AVAILABLE
            if self.visualization_enabled:
                self.logger.info("üìä ÏãúÍ∞ÅÌôî ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            else:
                self.logger.warning("‚ö†Ô∏è ÏãúÍ∞ÅÌôî ÏãúÏä§ÌÖú ÎπÑÌôúÏÑ±Ìôî (Matplotlib ÌïÑÏöî)")
        except Exception as e:
            self.logger.error(f"‚ùå ÏãúÍ∞ÅÌôî Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.visualization_enabled = False
    
    def _setup_assessment_pipeline(self):
        """ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï"""
        self.assessment_pipeline = [
            ('preprocessing', self._preprocess_for_assessment),
            ('technical_analysis', self._analyze_technical_quality),
            ('perceptual_analysis', self._analyze_perceptual_quality),
            ('aesthetic_analysis', self._analyze_aesthetic_quality),
            ('functional_analysis', self._analyze_functional_quality),
            ('comprehensive_analysis', self._perform_comprehensive_analysis)
        ]
        self.logger.info(f"üîÑ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ÏôÑÎ£å - {len(self.assessment_pipeline)}Îã®Í≥Ñ")
    
    # =================================================================
    # üöÄ Î©îÏù∏ Ï≤òÎ¶¨ Ìï®Ïàò
    # =================================================================
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        enable_visualization: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """‚úÖ Î©îÏù∏ ÌíàÏßà ÌèâÍ∞Ä Ìï®Ïàò"""
        start_time = time.time()
        
        try:
            if not self.is_initialized:
                raise ValueError(f"QualityAssessmentStepÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: {self.initialization_error}")
            
            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Í≤ÄÏ¶ù
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None:
                raise ValueError("Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ fitted_imageÏûÖÎãàÎã§")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image is not None else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image is not None else None
            
            # Ï∫êÏãú ÌôïÏù∏
            cache_key = self._generate_cache_key(fitted_img, fabric_type, clothing_type)
            if cache_key in self.assessment_cache:
                self.logger.info("üìã Ï∫êÏãúÏóêÏÑú ÌíàÏßà ÌèâÍ∞Ä Í≤∞Í≥º Î∞òÌôò")
                cached_result = self.assessment_cache[cache_key].copy()
                cached_result['from_cache'] = True
                return cached_result
            
            # M3 Max Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
            if self.is_m3_max:
                self._optimize_m3_max_memory()
            
            # ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
            quality_metrics = await self._execute_assessment_pipeline(fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs)
            
            # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
            visualization_data = None
            if enable_visualization and self.visualization_enabled:
                visualization_data = await self._generate_visualizations(quality_metrics)
            
            # Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ±
            recommendations = self._generate_recommendations(quality_metrics, fabric_type, clothing_type)
            
            # ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±
            detailed_analysis = self._generate_detailed_analysis(quality_metrics, fitted_img, person_img, fabric_type)
            
            # ÏµúÏ¢Ö Í≤∞Í≥º Íµ¨ÏÑ±
            processing_time = time.time() - start_time
            result = self._build_final_result(quality_metrics, recommendations, detailed_analysis, visualization_data, processing_time, fabric_type, clothing_type)
            
            # Ï∫êÏãú Ï†ÄÏû• Î∞è ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self._save_to_cache(cache_key, result)
            self._update_performance_stats(processing_time, quality_metrics.overall_score)
            
            self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä ÏôÑÎ£å - Ï†êÏàò: {quality_metrics.overall_score:.3f} ({quality_metrics.get_grade().value})")
            return result
            
        except Exception as e:
            error_msg = f"ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}"
            self.logger.error(f"‚ùå {error_msg}")
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, 0.0, success=False)
            
            return {
                "success": False, "step_name": self.step_name, "error": error_msg,
                "processing_time": processing_time, "quality_metrics": None,
                "overall_score": 0.0, "grade": QualityGrade.VERY_POOR.value, "visualization": None
            }
    
    # =================================================================
    # üîß ÌíàÏßà ÌèâÍ∞Ä ÌïµÏã¨ Ìï®ÏàòÎì§
    # =================================================================
    
    async def _execute_assessment_pipeline(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> QualityMetrics:
        """ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ"""
        metrics = QualityMetrics()
        
        self.logger.info(f"üîÑ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë - ÏùòÎ•ò: {clothing_type}, ÏõêÎã®: {fabric_type}")
        
        for step_name, analyzer_func in self.assessment_pipeline:
            try:
                step_start = time.time()
                
                # AI Î™®Îç∏ Í∏∞Î∞ò Î∂ÑÏÑù (M3 Max ÏµúÏ†ÅÌôî)
                if self.parallel_analysis and step_name in ['perceptual_analysis', 'aesthetic_analysis'] and self.ai_models:
                    step_result = await self._process_with_neural_engine(fitted_img, step_name)
                else:
                    step_result = await analyzer_func(fitted_img, person_img, clothing_img, fabric_type, clothing_type, **kwargs)
                
                # Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
                if isinstance(step_result, dict):
                    for key, value in step_result.items():
                        if hasattr(metrics, key) and isinstance(value, (int, float)):
                            setattr(metrics, key, float(value))
                
                step_time = time.time() - step_start
                self.logger.debug(f"  ‚úì {step_name} ÏôÑÎ£å - {step_time:.3f}Ï¥à")
                
            except Exception as e:
                self.logger.warning(f"  ‚ö†Ô∏è {step_name} Ïã§Ìå®: {e}")
                continue
        
        # ÏõêÎã®/ÏùòÎ•òÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©ÌïòÏó¨ Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
        fabric_weights = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
        clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
        
        combined_weights = {
            'technical': 0.3 * fabric_weights['texture_importance'],
            'perceptual': 0.3,
            'aesthetic': 0.2,
            'functional': 0.2 * clothing_weights['fitting']
        }
        
        metrics.calculate_overall_score(combined_weights)
        metrics.confidence = min(0.9, metrics.overall_score + 0.1)
        
        self.logger.info(f"‚úÖ ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å")
        return metrics
    
    async def _process_with_neural_engine(self, image: np.ndarray, analysis_type: str) -> Dict[str, Any]:
        """Neural Engine ÌôúÏö© AI Î™®Îç∏ Î∂ÑÏÑù"""
        if analysis_type not in self.ai_models:
            return {}
        
        try:
            model = self.ai_models[analysis_type]
            
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            tensor_img = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
            tensor_img = tensor_img.unsqueeze(0).to(self.device)
            
            # M3 Max ÏµúÏ†ÅÌôî
            if self.is_m3_max and self.device == "mps":
                tensor_img = tensor_img.half()
            
            # Î™®Îç∏ Ï∂îÎ°†
            with torch.no_grad():
                if self.device == "mps":
                    with autocast(device_type='cpu', dtype=torch.float16):
                        result = model(tensor_img)
                else:
                    result = model(tensor_img)
            
            # Í≤∞Í≥º Ï≤òÎ¶¨
            if analysis_type == 'perceptual_analysis':
                return {'perceptual_similarity': float(result.cpu().squeeze())}
            elif analysis_type == 'aesthetic_analysis':
                scores = result.cpu().squeeze().numpy()
                return {
                    'composition': float(scores[0]), 'color_harmony': float(scores[1]),
                    'symmetry': float(scores[2]), 'balance': float(scores[3])
                }
            
            return {}
        except Exception as e:
            self.logger.warning(f"Neural Engine Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {}
    
    # =================================================================
    # üîß Í∞úÎ≥Ñ Î∂ÑÏÑù Î©îÏÑúÎìúÎì§ (ÌïµÏã¨ Í∏∞Îä•)
    # =================================================================
    
    async def _preprocess_for_assessment(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, Any]:
        """ÌèâÍ∞ÄÎ•º ÏúÑÌïú Ï†ÑÏ≤òÎ¶¨"""
        if fitted_img.dtype != np.uint8:
            fitted_img = np.clip(fitted_img * 255, 0, 255).astype(np.uint8)
        
        h, w = fitted_img.shape[:2]
        if max(h, w) > 1024:
            scale = 1024 / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            if CV2_AVAILABLE:
                fitted_img = cv2.resize(fitted_img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        return {'preprocessing_success': True, 'processed_shape': fitted_img.shape}
    
    async def _analyze_technical_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, float]:
        """Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        try:
            return {
                'sharpness': self.technical_analyzer.analyze_sharpness(fitted_img),
                'noise_level': self.technical_analyzer.analyze_noise_level(fitted_img),
                'contrast': self.technical_analyzer.analyze_contrast(fitted_img),
                'color_accuracy': self.technical_analyzer.analyze_color_accuracy(person_img, fitted_img) if person_img is not None else 0.8,
                'saturation': self._analyze_saturation(fitted_img),
                'brightness': self._analyze_brightness(fitted_img)
            }
        except Exception as e:
            self.logger.error(f"Í∏∞Ïà†Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5, 'color_accuracy': 0.5, 'saturation': 0.5, 'brightness': 0.5}
    
    async def _analyze_perceptual_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, float]:
        """ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Î∂ÑÏÑù"""
        try:
            results = {}
            
            # SSIM Íµ¨Ï°∞Ï†Å Ïú†ÏÇ¨ÏÑ±
            if person_img is not None and SKIMAGE_AVAILABLE:
                if person_img.shape != fitted_img.shape:
                    person_resized = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
                else:
                    person_resized = person_img
                
                try:
                    ssim_score = ssim(person_resized, fitted_img, multichannel=True, channel_axis=2)
                    results['structural_similarity'] = max(0, ssim_score)
                except:
                    results['structural_similarity'] = 0.7
            else:
                results['structural_similarity'] = 0.7
            
            results['visual_quality'] = self._calculate_visual_quality(fitted_img)
            results['artifact_level'] = self._detect_artifacts(fitted_img)
            results['perceptual_similarity'] = results.get('structural_similarity', 0.7)
            
            return results
        except Exception as e:
            self.logger.error(f"ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'structural_similarity': 0.5, 'perceptual_similarity': 0.5, 'visual_quality': 0.5, 'artifact_level': 0.5}
    
    async def _analyze_aesthetic_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, float]:
        """ÎØ∏Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        try:
            return {
                'composition': self._analyze_composition(fitted_img),
                'color_harmony': self._analyze_color_harmony(fitted_img),
                'symmetry': self._analyze_symmetry(fitted_img),
                'balance': self._analyze_balance(fitted_img)
            }
        except Exception as e:
            self.logger.error(f"ÎØ∏Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'composition': 0.5, 'color_harmony': 0.5, 'symmetry': 0.5, 'balance': 0.5}
    
    async def _analyze_functional_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, float]:
        """Í∏∞Îä•Ï†Å ÌíàÏßà Î∂ÑÏÑù"""
        try:
            return {
                'fitting_quality': self._analyze_fitting_quality(fitted_img, person_img, clothing_type),
                'edge_preservation': self._analyze_edge_preservation(fitted_img, person_img),
                'texture_quality': self._analyze_texture_quality(fitted_img, clothing_img, fabric_type),
                'detail_preservation': self._analyze_detail_preservation(fitted_img, person_img)
            }
        except Exception as e:
            self.logger.error(f"Í∏∞Îä•Ï†Å ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'fitting_quality': 0.5, 'edge_preservation': 0.5, 'texture_quality': 0.5, 'detail_preservation': 0.5}
    
    async def _perform_comprehensive_analysis(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_img: Optional[np.ndarray], fabric_type: str, clothing_type: str, **kwargs) -> Dict[str, float]:
        """Ï¢ÖÌï© Î∂ÑÏÑù"""
        try:
            return {
                'overall_consistency': self._analyze_overall_consistency(fitted_img),
                'realism': self._analyze_realism(fitted_img, person_img),
                'completeness': self._analyze_completeness(fitted_img),
                'confidence': 0.8
            }
        except Exception as e:
            self.logger.error(f"Ï¢ÖÌï© Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'overall_consistency': 0.5, 'realism': 0.5, 'completeness': 0.5, 'confidence': 0.5}
    
    # =================================================================
    # üîß ÏÑ∏Î∂Ä Î∂ÑÏÑù Î©îÏÑúÎìúÎì§ (Í∞ÑÏÜåÌôîÎêú ÌïµÏã¨ Î≤ÑÏ†Ñ)
    # =================================================================
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        try:
            if CV2_AVAILABLE:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                return min(hsv[:, :, 1].mean() / 255.0, 1.0)
            else:
                r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]
                max_rgb = np.maximum(np.maximum(r, g), b)
                min_rgb = np.minimum(np.minimum(r, g), b)
                return min(np.mean((max_rgb - min_rgb) / (max_rgb + 1e-8)), 1.0)
        except: return 0.5
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        try:
            brightness = np.mean(image) / 255.0
            if 0.3 <= brightness <= 0.7:
                return 1.0 - abs(brightness - 0.5) * 2
            else:
                return max(0, 1.0 - abs(brightness - 0.5) * 4)
        except: return 0.5
    
    def _calculate_visual_quality(self, image: np.ndarray) -> float:
        try:
            color_std = np.std(image, axis=(0, 1)).mean() / 255.0
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2).mean()
                return min(np.mean([color_std * 2, gradient_magnitude / 100.0]), 1.0)
            return min(color_std * 2, 1.0)
        except: return 0.5
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        try:
            artifacts = 0.0
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                if laplacian_var > 2000: artifacts += 0.2
            
            noise_level = np.std(image) / 255.0
            if noise_level > 0.15: artifacts += 0.3
            
            return min(artifacts, 1.0)
        except: return 0.3
    
    def _analyze_composition(self, image: np.ndarray) -> float:
        try:
            h, w = image.shape[:2]
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # 3Î∂ÑÌï† Î≤ïÏπô Í∏∞Î∞ò Íµ¨ÎèÑ Î∂ÑÏÑù
                thirds_h = [h//3, 2*h//3]
                thirds_w = [w//3, 2*w//3]
                composition_score = 0
                
                for th in thirds_h:
                    for tw in thirds_w:
                        region = edges[max(0, th-20):min(h, th+20), max(0, tw-20):min(w, tw+20)]
                        if region.size > 0:
                            edge_density = np.sum(region) / (region.size * 255)
                            composition_score += edge_density
                
                return min(composition_score / 4, 1.0)
            return 0.6
        except: return 0.5
    
    def _analyze_color_harmony(self, image: np.ndarray) -> float:
        try:
            if SKLEARN_AVAILABLE:
                pixels = image.reshape(-1, 3)
                if len(pixels) > 10000:
                    indices = np.random.choice(len(pixels), 10000, replace=False)
                    pixels = pixels[indices]
                
                kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                kmeans.fit(pixels)
                centers = kmeans.cluster_centers_
                
                distances = []
                for i in range(len(centers)):
                    for j in range(i+1, len(centers)):
                        distances.append(np.linalg.norm(centers[i] - centers[j]))
                
                avg_distance = np.mean(distances)
                harmony_score = 1.0 - abs(avg_distance - 100) / 100
                return max(0, min(harmony_score, 1.0))
            else:
                color_std = np.std(image, axis=(0, 1))
                balance = 1.0 - np.std(color_std) / 128.0
                return max(0, min(balance, 1.0))
        except: return 0.6
    
    def _analyze_symmetry(self, image: np.ndarray) -> float:
        try:
            h, w = image.shape[:2]
            left_half = image[:, :w//2]
            right_half = np.fliplr(image[:, w//2:])
            
            min_width = min(left_half.shape[1], right_half.shape[1])
            left_half = left_half[:, :min_width]
            right_half = right_half[:, :min_width]
            
            if SKIMAGE_AVAILABLE and left_half.shape == right_half.shape:
                try:
                    return max(0, ssim(left_half, right_half, multichannel=True, channel_axis=2))
                except: pass
            
            mse = np.mean((left_half.astype(float) - right_half.astype(float))**2)
            return max(0, 1.0 - mse / (255**2))
        except: return 0.4
    
    def _analyze_balance(self, image: np.ndarray) -> float:
        try:
            h, w = image.shape[:2]
            quarters = [image[:h//2, :w//2], image[:h//2, w//2:], image[h//2:, :w//2], image[h//2:, w//2:]]
            
            weights = []
            for quarter in quarters:
                if quarter.size > 0:
                    brightness = np.mean(quarter)
                    contrast = np.std(quarter)
                    weights.append(brightness * 0.5 + contrast * 0.5)
            
            if len(weights) == 4:
                # ÎåÄÍ∞ÅÏÑ†, ÏàòÏßÅ, ÏàòÌèâ Í∑†Ìòï Í≥ÑÏÇ∞
                diagonal1 = weights[0] + weights[3]
                diagonal2 = weights[1] + weights[2]
                diagonal_balance = 1.0 - abs(diagonal1 - diagonal2) / max(diagonal1 + diagonal2, 1)
                
                top = weights[0] + weights[1]
                bottom = weights[2] + weights[3]
                vertical_balance = 1.0 - abs(top - bottom) / max(top + bottom, 1)
                
                left = weights[0] + weights[2]
                right = weights[1] + weights[3]
                horizontal_balance = 1.0 - abs(left - right) / max(left + right, 1)
                
                return max(0, min((diagonal_balance + vertical_balance + horizontal_balance) / 3, 1.0))
            return 0.5
        except: return 0.5
    
    def _analyze_fitting_quality(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray], clothing_type: str) -> float:
        if person_img is None: return 0.6
        try:
            if CV2_AVAILABLE:
                fitted_edges = cv2.Canny(cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY), 50, 150)
                person_edges = cv2.Canny(cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY), 50, 150)
                
                if fitted_edges.shape != person_edges.shape:
                    person_edges = cv2.resize(person_edges, (fitted_edges.shape[1], fitted_edges.shape[0]))
                
                edge_overlap = np.sum((fitted_edges > 0) & (person_edges > 0))
                total_edges = np.sum(fitted_edges > 0) + np.sum(person_edges > 0)
                if total_edges > 0:
                    fitting_score = (2 * edge_overlap) / total_edges
                    clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
                    return min(fitting_score * clothing_weights['fitting'] + 0.3, 1.0)
            return 0.5
        except: return 0.5
    
    def _analyze_edge_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        if person_img is None or not CV2_AVAILABLE: return 0.6
        try:
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            if fitted_gray.shape != person_gray.shape:
                person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
            
            fitted_edges = cv2.Canny(fitted_gray, 50, 150)
            person_edges = cv2.Canny(person_gray, 50, 150)
            
            preserved_edges = np.sum((fitted_edges > 0) & (person_edges > 0))
            original_edges = np.sum(person_edges > 0)
            
            return min(preserved_edges / (original_edges + 1e-8), 1.0)
        except: return 0.5
    
    def _analyze_texture_quality(self, fitted_img: np.ndarray, clothing_img: Optional[np.ndarray], fabric_type: str) -> float:
        try:
            texture_score = 0.0
            
            if SKIMAGE_AVAILABLE:
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                # LBP ÌÖçÏä§Ï≤ò Î∂ÑÏÑù
                radius = 3
                n_points = 8 * radius
                lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')
                hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, range=(0, n_points + 2))
                hist = hist.astype(float) / (hist.sum() + 1e-8)
                entropy_score = -np.sum(hist * np.log2(hist + 1e-8))
                texture_score = min(entropy_score / 8.0, 1.0)
            
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            texture_importance = fabric_standards['texture_importance']
            
            if CV2_AVAILABLE:
                gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
                sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                texture_sharpness = np.sqrt(sobel_x**2 + sobel_y**2).mean() / 255.0
                texture_score = (texture_score + min(texture_sharpness * 2, 1.0)) / 2
            
            return texture_score * texture_importance + (1 - texture_importance) * 0.7
        except: return 0.6
    
    def _analyze_detail_preservation(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        if person_img is None or not CV2_AVAILABLE: return 0.6
        try:
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            if fitted_gray.shape != person_gray.shape:
                person_gray = cv2.resize(person_gray, (fitted_gray.shape[1], fitted_gray.shape[0]))
            
            fitted_detail = cv2.Laplacian(fitted_gray, cv2.CV_64F)
            person_detail = cv2.Laplacian(person_gray, cv2.CV_64F)
            
            fitted_detail_energy = np.sum(np.abs(fitted_detail))
            person_detail_energy = np.sum(np.abs(person_detail))
            
            return min(fitted_detail_energy / (person_detail_energy + 1e-8), 1.0)
        except: return 0.5
    
    def _analyze_overall_consistency(self, image: np.ndarray) -> float:
        try:
            # ÏÉâÏÉÅ ÏùºÍ¥ÄÏÑ±
            h, w = image.shape[:2]
            regions = [image[:h//2, :w//2], image[:h//2, w//2:], image[h//2:, :w//2], image[h//2:, w//2:]]
            region_colors = [np.mean(region, axis=(0, 1)) for region in regions if region.size > 0]
            
            if len(region_colors) >= 2:
                color_diffs = []
                for i in range(len(region_colors)):
                    for j in range(i+1, len(region_colors)):
                        color_diffs.append(np.linalg.norm(region_colors[i] - region_colors[j]))
                
                avg_diff = np.mean(color_diffs)
                color_consistency = max(0, 1.0 - avg_diff / 128.0)
                return min(color_consistency, 1.0)
            return 0.7
        except: return 0.6
    
    def _analyze_realism(self, fitted_img: np.ndarray, person_img: Optional[np.ndarray]) -> float:
        try:
            realism_factors = []
            
            # Ï°∞Î™Ö ÌòÑÏã§ÏÑ±
            gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
            hist, _ = np.histogram(gray, bins=256, range=(0, 256))
            hist = hist.astype(float) / hist.sum()
            extreme_ratio = (hist[0] + hist[-1])
            
            if extreme_ratio < 0.1:
                lighting_realism = 0.9
            elif extreme_ratio < 0.2:
                lighting_realism = 0.7
            else:
                lighting_realism = max(0.3, 1.0 - extreme_ratio)
            
            realism_factors.append(lighting_realism)
            
            # Î¨ºÎ¶¨Ï†Å ÌÉÄÎãπÏÑ±
            if CV2_AVAILABLE:
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                angles = np.arctan2(grad_y, grad_x)
                angle_hist, _ = np.histogram(angles, bins=8, range=(-np.pi, np.pi))
                angle_hist = angle_hist.astype(float) / (angle_hist.sum() + 1e-8)
                angle_entropy = -np.sum(angle_hist * np.log2(angle_hist + 1e-8))
                entropy_score = max(0, 1.0 - abs(angle_entropy - 2.5) / 2.5)
                realism_factors.append(entropy_score)
            
            return np.mean(realism_factors)
        except: return 0.6
    
    def _analyze_completeness(self, image: np.ndarray) -> float:
        try:
            completeness_factors = []
            
            # Ïù¥ÎØ∏ÏßÄ Í≤ΩÍ≥Ñ ÏôÑÏÑ±ÎèÑ
            h, w = image.shape[:2]
            boundaries = [image[0, :], image[-1, :], image[:, 0], image[:, -1]]
            boundary_issues = 0
            
            for boundary in boundaries:
                if boundary.size > 0:
                    if len(boundary.shape) == 2:
                        diff = np.sum(np.abs(np.diff(boundary, axis=0)))
                    else:
                        diff = np.sum(np.abs(np.diff(boundary)))
                    
                    normalized_diff = diff / (len(boundary) * 255 * 3)
                    if normalized_diff > 0.5:
                        boundary_issues += 1
            
            boundary_completeness = max(0, 1.0 - boundary_issues / 4.0)
            completeness_factors.append(boundary_completeness)
            
            # Ïù¥ÎØ∏ÏßÄ ÌíàÏßà
            if not self._is_image_corrupted(image):
                completeness_factors.append(0.9)
            else:
                completeness_factors.append(0.3)
            
            # Ìï¥ÏÉÅÎèÑ Ï†ÅÏ†àÏÑ±
            if min(h, w) >= 256:
                resolution_score = min((min(h, w) / 512.0), 1.0)
            else:
                resolution_score = min(h, w) / 256.0
            completeness_factors.append(resolution_score)
            
            return np.mean(completeness_factors)
        except: return 0.7
    
    # =================================================================
    # üîß ÏãúÍ∞ÅÌôî Î∞è Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
    # =================================================================
    
    async def _generate_visualizations(self, metrics: QualityMetrics) -> Optional[Dict[str, Any]]:
        """ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        if not self.visualization_enabled:
            return None
        
        try:
            visualization = {
                'heatmap_image': self.visualizer.generate_quality_heatmap(metrics),
                'bar_chart_image': self.visualizer.generate_quality_bar_chart(metrics),
                'radar_chart_image': self.visualizer.generate_radar_chart(metrics),
                'visualization_info': {
                    'generated_charts': ['heatmap', 'bar_chart', 'radar_chart'],
                    'overall_score': metrics.overall_score,
                    'grade': metrics.get_grade().value,
                    'device': self.device,
                    'is_m3_max': self.is_m3_max
                },
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            self.logger.info("üìä ÏãúÍ∞ÅÌôî ÏÉùÏÑ± ÏôÑÎ£å")
            return visualization
        except Exception as e:
            self.logger.error(f"‚ùå ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _generate_recommendations(self, metrics: QualityMetrics, fabric_type: str, clothing_type: str) -> List[Dict[str, Any]]:
        """Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ±"""
        recommendations = []
        
        try:
            # Í∏∞Ïà†Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.sharpness < 0.6:
                recommendations.append({
                    'category': 'technical', 'issue': 'low_sharpness',
                    'description': 'Ïù¥ÎØ∏ÏßÄ ÏÑ†Î™ÖÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'ÏÉ§ÌîÑÎãù ÌïÑÌÑ∞Î•º Ï†ÅÏö©ÌïòÍ±∞ÎÇò Îçî ÎÜíÏùÄ Ìï¥ÏÉÅÎèÑÎ°ú Ï≤òÎ¶¨ÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            if metrics.noise_level > 0.4:
                recommendations.append({
                    'category': 'technical', 'issue': 'high_noise',
                    'description': 'ÎÖ∏Ïù¥Ï¶à Î†àÎ≤®Ïù¥ ÎÜíÏäµÎãàÎã§',
                    'suggestion': 'ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞ ÌïÑÌÑ∞Î•º Í∞ïÌôîÌïòÍ±∞ÎÇò Ï†ÑÏ≤òÎ¶¨ Îã®Í≥ÑÎ•º Í∞úÏÑ†ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            if metrics.contrast < 0.5:
                recommendations.append({
                    'category': 'technical', 'issue': 'low_contrast',
                    'description': 'ÎåÄÎπÑÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§',
                    'suggestion': 'ÌûàÏä§ÌÜ†Í∑∏Îû® ÌèâÌôúÌôîÎÇò Ï†ÅÏùëÌòï ÎåÄÎπÑ Ìñ•ÏÉÅÏùÑ Ï†ÅÏö©ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            # ÏßÄÍ∞ÅÏ†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.structural_similarity < 0.7:
                recommendations.append({
                    'category': 'perceptual', 'issue': 'low_similarity',
                    'description': 'ÏõêÎ≥∏Í≥ºÏùò Íµ¨Ï°∞Ï†Å Ïú†ÏÇ¨ÏÑ±Ïù¥ ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'ÏßÄÏò§Î©îÌä∏Î¶≠ Îß§Ïπ≠ Îã®Í≥ÑÎ•º Í∞úÏÑ†ÌïòÍ±∞ÎÇò ÏõåÌïë ÏïåÍ≥†Î¶¨Ï¶òÏùÑ Ï°∞Ï†ïÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            # ÎØ∏Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.color_harmony < 0.6:
                recommendations.append({
                    'category': 'aesthetic', 'issue': 'poor_color_harmony',
                    'description': 'ÏÉâÏÉÅ Ï°∞ÌôîÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§',
                    'suggestion': 'ÏÉâÏÉÅ Î≥¥Ï†ïÏù¥ÎÇò ÏÉâÏò®ÎèÑ Ï°∞Ï†ïÏùÑ Í≥†Î†§ÌïòÏÑ∏Ïöî',
                    'priority': 'low'
                })
            
            # Í∏∞Îä•Ï†Å ÌíàÏßà Í∞úÏÑ† Ï†úÏïà
            if metrics.fitting_quality < 0.7:
                recommendations.append({
                    'category': 'functional', 'issue': 'poor_fitting',
                    'description': f'{clothing_type} ÌîºÌåÖ ÌíàÏßàÏù¥ ÎÇÆÏäµÎãàÎã§',
                    'suggestion': 'Ïù∏Ï≤¥ ÌååÏã± Ï†ïÌôïÎèÑÎ•º ÎÜíÏù¥Í±∞ÎÇò ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖòÏùÑ Í∞úÏÑ†ÌïòÏÑ∏Ïöî',
                    'priority': 'high'
                })
            
            # ÏõêÎã®Î≥Ñ ÌäπÌôî Ï†úÏïà
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            if metrics.texture_quality < fabric_standards['texture_importance'] * 0.8:
                recommendations.append({
                    'category': 'fabric_specific', 'issue': 'texture_quality',
                    'description': f'{fabric_type} ÏõêÎã®Ïùò ÌÖçÏä§Ï≤ò ÌíàÏßàÏù¥ Í∏∞Ï§Ä ÎØ∏Îã¨ÏûÖÎãàÎã§',
                    'suggestion': f'{fabric_type}Ïóê ÌäπÌôîÎêú ÌÖçÏä§Ï≤ò Ìñ•ÏÉÅ Í∏∞Î≤ïÏùÑ Ï†ÅÏö©ÌïòÏÑ∏Ïöî',
                    'priority': 'medium'
                })
            
            # Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Ï†ïÎ†¨
            priority_order = {'high': 0, 'medium': 1, 'low': 2}
            recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))
            
        except Exception as e:
            self.logger.error(f"Í∞úÏÑ† Ï†úÏïà ÏÉùÏÑ± Ïã§Ìå®: {e}")
            recommendations.append({
                'category': 'general', 'issue': 'analysis_error',
                'description': 'ÌíàÏßà Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§',
                'suggestion': 'ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎÇò ÏÑ§Ï†ïÏùÑ ÌôïÏù∏ÌïòÍ≥† Îã§Ïãú ÏãúÎèÑÌïòÏÑ∏Ïöî',
                'priority': 'high'
            })
        
        return recommendations
    
    def _generate_detailed_analysis(self, metrics: QualityMetrics, fitted_img: np.ndarray, person_img: Optional[np.ndarray], fabric_type: str) -> Dict[str, Any]:
        """ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ±"""
        try:
            return {
                'image_statistics': {
                    'mean_brightness': float(np.mean(fitted_img)),
                    'std_brightness': float(np.std(fitted_img)),
                    'color_distribution': {
                        'red_mean': float(np.mean(fitted_img[:, :, 0])),
                        'green_mean': float(np.mean(fitted_img[:, :, 1])),
                        'blue_mean': float(np.mean(fitted_img[:, :, 2]))
                    },
                    'shape': fitted_img.shape,
                    'total_pixels': int(fitted_img.size)
                },
                'quality_breakdown': {
                    'technical_quality': {
                        'sharpness': float(metrics.sharpness),
                        'noise_level': float(metrics.noise_level),
                        'contrast': float(metrics.contrast),
                        'color_accuracy': float(metrics.color_accuracy)
                    },
                    'perceptual_quality': {
                        'structural_similarity': float(metrics.structural_similarity),
                        'visual_quality': float(metrics.visual_quality),
                        'artifact_level': float(metrics.artifact_level)
                    },
                    'aesthetic_quality': {
                        'composition': float(metrics.composition),
                        'color_harmony': float(metrics.color_harmony),
                        'balance': float(metrics.balance)
                    },
                    'functional_quality': {
                        'fitting_quality': float(metrics.fitting_quality),
                        'texture_quality': float(metrics.texture_quality),
                        'detail_preservation': float(metrics.detail_preservation)
                    }
                },
                'fabric_analysis': {
                    'fabric_type': fabric_type,
                    'texture_importance': self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])['texture_importance'],
                    'texture_meets_standard': bool(metrics.texture_quality >= self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])['texture_importance'] * 0.8)
                },
                'performance_analysis': {
                    'processing_device': self.device,
                    'is_m3_max_optimized': self.is_m3_max,
                    'memory_usage_gb': self.memory_gb,
                    'optimization_level': self.optimization_level
                }
            }
        except Exception as e:
            self.logger.error(f"ÏÉÅÏÑ∏ Î∂ÑÏÑù ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {'error': str(e)}
    
    # =================================================================
    # üîß Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    # =================================================================
    
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], input_name: str) -> Optional[np.ndarray]:
        """Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Í≤ÄÏ¶ù"""
        try:
            if isinstance(image_input, np.ndarray):
                image = image_input
            elif isinstance(image_input, (str, Path)):
                if PIL_AVAILABLE:
                    pil_img = Image.open(image_input)
                    image = np.array(pil_img.convert('RGB'))
                else:
                    raise ImportError("PILÏù¥ ÌïÑÏöîÌï©ÎãàÎã§")
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïù¥ÎØ∏ÏßÄ ÌÉÄÏûÖ: {type(image_input)}")
            
            if image.ndim != 3 or image.shape[2] != 3:
                raise ValueError("RGB Ïù¥ÎØ∏ÏßÄÏó¨Ïïº Ìï©ÎãàÎã§")
            if image.size == 0:
                raise ValueError("Îπà Ïù¥ÎØ∏ÏßÄÏûÖÎãàÎã§")
            
            return image
        except Exception as e:
            self.logger.error(f"{input_name} Î°úÎìú Ïã§Ìå®: {e}")
            return None
    
    def _is_image_corrupted(self, image: np.ndarray) -> bool:
        """Ïù¥ÎØ∏ÏßÄ ÏÜêÏÉÅ Ïó¨Î∂Ä ÌôïÏù∏"""
        try:
            if np.any(np.isnan(image)) or np.any(np.isinf(image)):
                return True
            if np.any(image < 0) or np.any(image > 255):
                return True
            if image.ndim != 3 or image.shape[2] != 3:
                return True
            if image.size == 0:
                return True
            return False
        except:
            return True
    
    def _generate_cache_key(self, image: np.ndarray, fabric_type: str, clothing_type: str) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        try:
            import hashlib
            image_hash = hashlib.md5(image.tobytes()).hexdigest()[:16]
            config_hash = hashlib.md5(f"{fabric_type}_{clothing_type}_{self.assessment_config['mode']}".encode()).hexdigest()[:8]
            return f"qa_{image_hash}_{config_hash}"
        except:
            return f"qa_fallback_{time.time()}"
    
    def _optimize_m3_max_memory(self):
        """M3 Max Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        if self.is_m3_max and TORCH_AVAILABLE:
            try:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                gc.collect()
            except Exception as e:
                self.logger.warning(f"Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
    
    def _build_final_result(self, metrics: QualityMetrics, recommendations: List[Dict[str, Any]], detailed_analysis: Dict[str, Any], visualization_data: Optional[Dict[str, Any]], processing_time: float, fabric_type: str, clothing_type: str) -> Dict[str, Any]:
        """ÏµúÏ¢Ö Í≤∞Í≥º Íµ¨ÏÑ±"""
        return {
            "success": True,
            "step_name": self.step_name,
            "processing_time": processing_time,
            
            # ÌïµÏã¨ ÌíàÏßà Î©îÌä∏Î¶≠
            "quality_metrics": metrics.to_dict(),
            "overall_score": float(metrics.overall_score),
            "grade": metrics.get_grade().value,
            "confidence": float(metrics.confidence),
            
            # ÏãúÍ∞ÅÌôî Îç∞Ïù¥ÌÑ∞
            "visualization": visualization_data,
            
            # Í∞úÏÑ† Ï†úÏïà
            "recommendations": recommendations,
            "recommendation_count": len(recommendations),
            "high_priority_issues": len([r for r in recommendations if r.get('priority') == 'high']),
            
            # ÏÉÅÏÑ∏ Î∂ÑÏÑù
            "detailed_analysis": detailed_analysis,
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            "fabric_type": fabric_type,
            "clothing_type": clothing_type,
            "assessment_mode": self.assessment_config['mode'],
            
            # ÏãúÏä§ÌÖú Ï†ïÎ≥¥
            "device_info": {
                "device": self.device,
                "device_type": self.device_type,
                "is_m3_max": self.is_m3_max,
                "memory_gb": self.memory_gb,
                "optimization_level": self.optimization_level,
                "visualization_enabled": self.visualization_enabled
            },
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ
            "performance_stats": self.performance_stats.copy(),
            
            # ÌíàÏßà ÌÜµÍ≥º Ïó¨Î∂Ä
            "quality_passed": metrics.overall_score >= self.quality_thresholds['minimum_acceptable'],
            "quality_thresholds": self.quality_thresholds.copy(),
            
            "from_cache": False
        }
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """Ï∫êÏãúÏóê Í≤∞Í≥º Ï†ÄÏû•"""
        try:
            if len(self.assessment_cache) >= self.cache_max_size:
                oldest_key = min(self.assessment_cache.keys())
                del self.assessment_cache[oldest_key]
            
            cached_result = result.copy()
            if 'visualization' in cached_result:
                cached_result['visualization'] = None  # Î©îÎ™®Î¶¨ Ï†àÏïΩ
            
            self.assessment_cache[cache_key] = cached_result
        except Exception as e:
            self.logger.warning(f"Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    def _update_performance_stats(self, processing_time: float, quality_score: float, success: bool = True):
        """ÏÑ±Îä• ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            if success:
                self.performance_stats['total_assessments'] += 1
                self.performance_stats['total_time'] += processing_time
                self.performance_stats['average_time'] = self.performance_stats['total_time'] / self.performance_stats['total_assessments']
                
                current_avg = self.performance_stats.get('average_score', 0.0)
                total = self.performance_stats['total_assessments']
                self.performance_stats['average_score'] = (current_avg * (total - 1) + quality_score) / total
            else:
                self.performance_stats['error_count'] += 1
            
            self.performance_stats['last_assessment_time'] = processing_time
        except Exception as e:
            self.logger.warning(f"ÏÑ±Îä• ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
    
    async def _load_recommended_models(self):
        """‚úÖ Ï∂îÏ≤ú Î™®Îç∏ ÏôÑÏ†Ñ Î°úÎìú"""
        if self.model_interface is None:
            return
        
        try:
            recommended_models = [
                'quality_assessment_combined',
                'perceptual_quality_model',
                'aesthetic_quality_model',
                'clip_vision_model'
            ]
            
            for model_name in recommended_models:
                try:
                    model = await self.model_interface.get_model(model_name)
                    if model:
                        self.logger.info(f"üì¶ Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {model_name}")
                        
                        # Î™®Îç∏ÏùÑ AI Î™®Îç∏ ÎîïÏÖîÎÑàÎ¶¨Ïóê Ï∂îÍ∞Ä
                        if model_name == 'perceptual_quality_model' and 'perceptual_quality' not in self.ai_models:
                            self.ai_models['perceptual_quality'] = model
                        elif model_name == 'aesthetic_quality_model' and 'aesthetic_quality' not in self.ai_models:
                            self.ai_models['aesthetic_quality'] = model
                            
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï∂îÏ≤ú Î™®Îç∏ Î°úÎìú Í≥ºÏ†ïÏóêÏÑú Ïò§Î•ò: {e}")
    
    # =================================================================
    # üîç ÌëúÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Î©îÏÑúÎìúÎì§ (Pipeline Manager Ìò∏Ìôò)
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "step_name": "QualityAssessment",
            "class_name": self.__class__.__name__,
            "version": "4.0-m3max-core",
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "performance_stats": self.performance_stats.copy(),
            "capabilities": {
                "technical_analysis": self.assessment_config['technical_analysis_enabled'],
                "perceptual_analysis": self.assessment_config['perceptual_analysis_enabled'],
                "aesthetic_analysis": self.assessment_config['aesthetic_analysis_enabled'],
                "functional_analysis": self.assessment_config['functional_analysis_enabled'],
                "neural_analysis": bool(self.ai_models) if hasattr(self, 'ai_models') else False,
                "visualization_enabled": self.visualization_enabled,
                "m3_max_acceleration": self.is_m3_max and self.device == 'mps'
            },
            "supported_fabrics": list(self.FABRIC_QUALITY_STANDARDS.keys()),
            "supported_clothing_types": list(self.CLOTHING_QUALITY_WEIGHTS.keys()),
            "quality_settings": {
                "optimization_level": self.optimization_level,
                "quality_thresholds": self.quality_thresholds,
                "assessment_mode": self.assessment_config['mode']
            },
            "cache_status": {
                "enabled": True,
                "size": len(self.assessment_cache) if hasattr(self, 'assessment_cache') else 0,
                "max_size": self.cache_max_size if hasattr(self, 'cache_max_size') else 0
            },
            "model_loader_info": {
                "connected": self.model_interface is not None,
                "loaded_models": list(self.ai_models.keys()) if hasattr(self, 'ai_models') else [],
                "available": MODEL_LOADER_AVAILABLE
            }
        }
    
    def cleanup_resources(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            # AI Î™®Îç∏ Ï†ïÎ¶¨
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.ai_models.clear()
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            if hasattr(self, 'assessment_cache'):
                self.assessment_cache.clear()
            
            # Î∂ÑÏÑùÍ∏∞ Ï†ïÎ¶¨
            if hasattr(self, 'technical_analyzer'):
                del self.technical_analyzer
            
            # ÏãúÍ∞ÅÌôî Ï†ïÎ¶¨
            if hasattr(self, 'visualizer'):
                del self.visualizer
            
            # Î™®Îç∏ Î°úÎçî Ï†ïÎ¶¨
            if hasattr(self, 'model_interface') and self.model_interface:
                if hasattr(self.model_interface, 'cleanup'):
                    self.model_interface.cleanup()
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if TORCH_AVAILABLE and self.device in ["mps", "cuda"]:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            self.logger.info("‚úÖ QualityAssessmentStep Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def __del__(self):
        """ÏÜåÎ©∏Ïûê"""
        try:
            self.cleanup_resources()
        except:
            pass

# =================================================================
# üî• Ìò∏ÌôòÏÑ± ÏßÄÏõê Ìï®ÏàòÎì§
# =================================================================

def create_quality_assessment_step(device: str = "mps", config: Optional[Dict[str, Any]] = None) -> QualityAssessmentStep:
    """Í∏∞Ï°¥ Î∞©Ïãù Ìò∏Ìôò ÏÉùÏÑ±Ïûê"""
    return QualityAssessmentStep(device=device, config=config)

def create_m3_max_quality_assessment_step(memory_gb: float = 128.0, assessment_mode: str = "comprehensive", enable_visualization: bool = True, **kwargs) -> QualityAssessmentStep:
    """M3 Max ÏµúÏ†ÅÌôî ÏÉùÏÑ±Ïûê"""
    return QualityAssessmentStep(
        device=None, memory_gb=memory_gb, is_m3_max=True, optimization_enabled=True,
        assessment_mode=assessment_mode, visualization_enabled=enable_visualization, **kwargs
    )

def create_complete_quality_assessment_step(device: Optional[str] = None, config: Optional[Dict[str, Any]] = None, enable_all_features: bool = True, **kwargs) -> QualityAssessmentStep:
    """ÏôÑÏ†Ñ Í∏∞Îä• ÏÉùÏÑ±Ïûê"""
    if enable_all_features:
        config = config or {}
        config.update({
            'technical_analysis_enabled': True,
            'perceptual_analysis_enabled': True,
            'aesthetic_analysis_enabled': True,
            'functional_analysis_enabled': True,
            'neural_analysis_enabled': True,
            'visualization_enabled': True
        })
    
    return QualityAssessmentStep(device=device, config=config, **kwargs)

# Î™®Îìà ÏùµÏä§Ìè¨Ìä∏
__all__ = [
    'QualityAssessmentStep', 'QualityMetrics', 'QualityGrade', 'QualityVisualizationGenerator',
    'PerceptualQualityModel', 'AestheticQualityModel', 'TechnicalQualityAnalyzer',
    'create_quality_assessment_step', 'create_m3_max_quality_assessment_step', 'create_complete_quality_assessment_step'
]

# Î™®Îìà Ï¥àÍ∏∞Ìôî Î°úÍ∑∏
logger.info("‚úÖ QualityAssessmentStep ÌïµÏã¨ Í∏∞Îä• ÏôÑÏ†ÑÌåê Î°úÎìú ÏôÑÎ£å")
logger.info("üìä Ï£ºÏöî ÌäπÏßï:")
logger.info("  - 16Í∞ÄÏßÄ ÌíàÏßà Î©îÌä∏Î¶≠ ÏôÑÏ†Ñ Î∂ÑÏÑù")
logger.info("  - AI Î™®Îç∏ Î°úÎçî ÏôÑÎ≤Ω Ïó∞Îèô")
logger.info("  - 3Í∞ÄÏßÄ ÏãúÍ∞ÅÌôî Ï∞®Ìä∏ ÏßÄÏõê")
logger.info("  - M3 Max Neural Engine ÏµúÏ†ÅÌôî")
logger.info("  - Pipeline Manager 100% Ìò∏Ìôò")
logger.info("  - ÏõêÎã®/ÏùòÎ•òÎ≥Ñ ÌäπÌôî Î∂ÑÏÑù")
logger.info("  - Ïã§ÏãúÍ∞Ñ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî")

if MATPLOTLIB_AVAILABLE:
    logger.info("üìà ÏãúÍ∞ÅÌôî: ÌûàÌä∏Îßµ, Î∞îÏ∞®Ìä∏, Î†àÏù¥ÎçîÏ∞®Ìä∏ ÏßÄÏõê")
else:
    logger.warning("üìà ÏãúÍ∞ÅÌôî: ÎπÑÌôúÏÑ±Ìôî (matplotlib ÏÑ§Ïπò ÌïÑÏöî)")

if MODEL_LOADER_AVAILABLE:
    logger.info("üîó Î™®Îç∏ Î°úÎçî: Ïó∞Îèô Í∞ÄÎä•")
else:
    logger.warning("üîó Î™®Îç∏ Î°úÎçî: ÎÇ¥Ïû• Î™®Îç∏ ÏÇ¨Ïö©")

if TORCH_AVAILABLE and torch.backends.mps.is_available():
    logger.info("üçé M3 Max Í∞ÄÏÜç: ÏÇ¨Ïö© Í∞ÄÎä•")
else:
    logger.info("üíª ÏùºÎ∞ò Ï≤òÎ¶¨: CPU/CUDA ÏÇ¨Ïö©")