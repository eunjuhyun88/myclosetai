# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - v18.0 ì™„ì „ í˜¸í™˜ ë²„ì „
================================================================================
âœ… step_model_requests.py v8.0 ì™„ì „ í˜¸í™˜ - DetailedDataSpec ë°˜ì˜
âœ… BaseStepMixin v18.0+ ì™„ì „ í˜¸í™˜ - UnifiedDependencyManager ì—°ë™
âœ… ModelLoader v21.0 í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ì—°ì‚°
âœ… StepInterface v2.0 register_model_requirement í™œìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING íŒ¨í„´)
âœ… ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬í˜„
âœ… step_model_requests.py REAL_STEP_MODEL_REQUESTS ì™„ì „ í™œìš©
âœ… DetailedDataSpec ê¸°ë°˜ ë°ì´í„° íë¦„ ì™„ì „ êµ¬í˜„
âœ… FastAPI ë¼ìš°í„° í˜¸í™˜ì„± 100% ì§€ì›
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ìœ ì§€

ì²˜ë¦¬ íë¦„:
ğŸŒ API ìš”ì²­ â†’ ğŸ“‹ PipelineManager â†’ ğŸ¯ QualityAssessmentStep ìƒì„±
â†“
ğŸ”— BaseStepMixin.dependency_manager.auto_inject_dependencies()
â”œâ”€ ModelLoader ìë™ ì£¼ì…
â”œâ”€ StepModelInterface ìƒì„±
â””â”€ register_model_requirement í˜¸ì¶œ (step_model_requests.py ê¸°ë°˜)
â†“
ğŸš€ QualityAssessmentStep.initialize()
â”œâ”€ step_model_requests.py ìŠ¤í™ ë¡œë“œ
â”œâ”€ AI í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ (open_clip_pytorch_model.bin 5.2GB)
â”œâ”€ DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
â””â”€ M3 Max ìµœì í™” ì ìš©
â†“
ğŸ§  ì‹¤ì œ AI ì¶”ë¡  process()
â”œâ”€ step_model_requests.py ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦
â”œâ”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â†’ DetailedDataSpec ê¸°ë°˜ Tensor ë³€í™˜
â”œâ”€ AI ëª¨ë¸ ì¶”ë¡  (OpenCLIP, LPIPS, í’ˆì§ˆ í‰ê°€)
â”œâ”€ 8ê°€ì§€ í’ˆì§ˆ ë¶„ì„ â†’ ê²°ê³¼ í•´ì„
â”œâ”€ DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬
â””â”€ step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜
â†“
ğŸ“¤ ê²°ê³¼ ë°˜í™˜ (QualityMetrics ê°ì²´ + FastAPI í˜¸í™˜ ì‘ë‹µ)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache, wraps
import numpy as np
import base64
import io

# ==============================================
# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.interfaces.step_interface import StepModelInterface

# ==============================================
# ğŸ”¥ step_model_requests.py ì„í¬íŠ¸ (í•µì‹¬ í˜¸í™˜ì„±)
# ==============================================
try:
    from ..utils.step_model_requests import (
        get_enhanced_step_request,
        get_step_data_structure_info,
        get_step_preprocessing_requirements,
        get_step_postprocessing_requirements,
        get_step_data_flow,
        get_step_api_mapping,
        REAL_STEP_MODEL_REQUESTS,
        EnhancedRealModelRequest,
        DetailedDataSpec
    )
    STEP_MODEL_REQUESTS_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… step_model_requests.py v8.0 ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    STEP_MODEL_REQUESTS_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ step_model_requests.py ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ BaseStepMixin v18.0 ì„í¬íŠ¸ (í•µì‹¬)
# ==============================================
try:
    from .base_step_mixin import BaseStepMixin, QualityAssessmentMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger.info("âœ… BaseStepMixin v18.0 ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logger.warning(f"âš ï¸ BaseStepMixin ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================================
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    # OpenCV í´ë°± ì‹œìŠ¤í…œ
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    pil_img = Image.fromarray(img)
                    resized = pil_img.resize(size)
                    return np.array(resized)
            return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:
                    return img[:, :, ::-1]
            return img
        
        def imread(self, path):
            if PIL_AVAILABLE:
                from PIL import Image
                img = Image.open(path)
                return np.array(img)
            return None
        
        def imwrite(self, path, img):
            if PIL_AVAILABLE:
                from PIL import Image
                if hasattr(img, 'shape'):
                    Image.fromarray(img).save(path)
                    return True
            return False
    
    cv2 = OpenCVFallback()
    OPENCV_AVAILABLE = False

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ==============================================
# ğŸ”¥ GPU ì•ˆì „ ì—°ì‚° ìœ í‹¸ë¦¬í‹°
# ==============================================
def safe_mps_empty_cache():
    """MPS ìºì‹œ ì•ˆì „ ì •ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
        gc.collect()
        return {"success": True, "method": "mps_cache_cleared"}
    except Exception:
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

def safe_tensor_to_numpy(tensor):
    """Tensorë¥¼ ì•ˆì „í•˜ê²Œ NumPyë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            return tensor.detach().cpu().numpy()
        elif hasattr(tensor, 'numpy'):
            return tensor.numpy()
        else:
            return np.array(tensor)
    except Exception:
        return np.array(tensor)

# ==============================================
# ğŸ”¥ MRO ì•ˆì „í•œ í´ë°± í´ë˜ìŠ¤ë“¤
# ==============================================
if not BASE_STEP_MIXIN_AVAILABLE:
    class BaseStepMixin:
        """MRO ì•ˆì „í•œ í´ë°± BaseStepMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.step_name = getattr(self, 'step_name', 'quality_assessment')
            self.step_number = 8
            self.device = 'cpu'
            self.is_initialized = False
            self.dependency_manager = None
    
    class QualityAssessmentMixin(BaseStepMixin):
        """MRO ì•ˆì „í•œ í´ë°± QualityAssessmentMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.step_type = "quality_assessment"
            self.quality_threshold = 0.7

# ==============================================
# ğŸ”¥ step_model_requests.py ê¸°ë°˜ ë°ì´í„° êµ¬ì¡°ë“¤
# ==============================================
class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰ (step_model_requests.py í˜¸í™˜)"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    FAILED = "failed"

class AssessmentMode(Enum):
    """í‰ê°€ ëª¨ë“œ (step_model_requests.py í˜¸í™˜)"""
    COMPREHENSIVE = "comprehensive"
    FAST = "fast"
    DETAILED = "detailed"
    CUSTOM = "custom"

class QualityAspect(Enum):
    """í’ˆì§ˆ í‰ê°€ ì˜ì—­ (step_model_requests.py í˜¸í™˜)"""
    SHARPNESS = "sharpness"
    COLOR = "color"
    FITTING = "fitting"
    REALISM = "realism"
    ARTIFACTS = "artifacts"
    ALIGNMENT = "alignment"
    LIGHTING = "lighting"
    TEXTURE = "texture"

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° êµ¬ì¡° (step_model_requests.py DetailedDataSpec í˜¸í™˜)"""
    overall_score: float = 0.0
    confidence: float = 0.0
    
    # ì„¸ë¶€ ì ìˆ˜ë“¤ (step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)
    sharpness_score: float = 0.0
    color_score: float = 0.0
    fitting_score: float = 0.0
    realism_score: float = 0.0
    artifacts_score: float = 0.0
    alignment_score: float = 0.0
    lighting_score: float = 0.0
    texture_score: float = 0.0
    
    # step_model_requests.py API ë§¤í•‘ í˜¸í™˜
    quality_breakdown: Dict[str, float] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    processing_time: float = 0.0
    device_used: str = "cpu"
    model_version: str = "v1.0"
    
    def to_dict(self) -> Dict[str, Any]:
        """step_model_requests.py API ì¶œë ¥ ë§¤í•‘ í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        result = asdict(self)
        
        # step_model_requests.py API ì¶œë ¥ ë§¤í•‘ ì¤€ìˆ˜
        result.update({
            "overall_quality": self.overall_score,
            "quality_breakdown": {
                "sharpness": self.sharpness_score,
                "color": self.color_score,
                "fitting": self.fitting_score,
                "realism": self.realism_score,
                "artifacts": self.artifacts_score,
                "alignment": self.alignment_score,
                "lighting": self.lighting_score,
                "texture": self.texture_score
            },
            "recommendations": self.recommendations,
            "confidence": self.confidence
        })
        
        return result
    
    def to_fastapi_response(self) -> Dict[str, Any]:
        """FastAPI ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (step_model_requests.py í˜¸í™˜)"""
        return {
            "overall_quality": float(self.overall_score),
            "quality_breakdown": {k: float(v) for k, v in self.quality_breakdown.items()},
            "recommendations": list(self.recommendations),
            "confidence": float(self.confidence)
        }

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)
# ==============================================
if TORCH_AVAILABLE:
    class RealPerceptualQualityModel(nn.Module):
        """ì‹¤ì œ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            # step_model_requests.py ìŠ¤í™ì—ì„œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ë³´ ë¡œë“œ
            self.model_architecture = self.config.get('model_architecture', 'open_clip_vit')
            self.input_size = self.config.get('input_size', (224, 224))
            
            # OpenCLIP ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ (5.2GB ëª¨ë¸)
            self.feature_extractor = self._create_feature_extractor()
            
            # LPIPS ìŠ¤íƒ€ì¼ ê±°ë¦¬ ê³„ì‚°
            self.lpips_layers = nn.ModuleList([
                nn.Conv2d(768, 512, 1),  # ViT íŠ¹ì§•ì„ LPIPS í˜¸í™˜ í¬ê¸°ë¡œ
                nn.Conv2d(512, 256, 1),
                nn.Conv2d(256, 128, 1),
                nn.Conv2d(128, 64, 1)
            ])
            
            # í’ˆì§ˆ ì˜ˆì¸¡ í—¤ë“œë“¤ (step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)
            self.quality_heads = nn.ModuleDict({
                'overall': self._create_quality_head(768, 1),
                'sharpness': self._create_quality_head(768, 1),
                'color': self._create_quality_head(768, 1),
                'fitting': self._create_quality_head(768, 1),
                'realism': self._create_quality_head(768, 1),
                'artifacts': self._create_quality_head(768, 1),
                'alignment': self._create_quality_head(768, 1),
                'lighting': self._create_quality_head(768, 1),
                'texture': self._create_quality_head(768, 1)
            })
            
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        def _create_feature_extractor(self):
            """íŠ¹ì§• ì¶”ì¶œê¸° ìƒì„± (OpenCLIP ê¸°ë°˜)"""
            return nn.Sequential(
                # Vision Transformer ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
                nn.Conv2d(3, 768, kernel_size=16, stride=16),  # Patch embedding
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
                nn.LayerNorm(768),
                nn.Dropout(0.1)
            )
        
        def _create_quality_head(self, in_features: int, out_features: int):
            """í’ˆì§ˆ ì˜ˆì¸¡ í—¤ë“œ ìƒì„±"""
            return nn.Sequential(
                nn.Linear(in_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, out_features),
                nn.Sigmoid()
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (step_model_requests.py íŒŒì¼ ìŠ¤í™ ê¸°ë°˜)"""
            try:
                if Path(checkpoint_path).exists():
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    if 'state_dict' in checkpoint:
                        self.load_state_dict(checkpoint['state_dict'], strict=False)
                    elif 'model' in checkpoint:
                        self.load_state_dict(checkpoint['model'], strict=False)
                    else:
                        self.load_state_dict(checkpoint, strict=False)
                    self.logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                    return False
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        
        def forward(self, x):
            """ìˆœì „íŒŒ (step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)"""
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.feature_extractor(x)
            
            # ê° í’ˆì§ˆ ì¸¡ë©´ë³„ ì ìˆ˜ ê³„ì‚°
            quality_scores = {}
            for aspect, head in self.quality_heads.items():
                quality_scores[aspect] = head(features)
            
            return {
                'quality_scores': quality_scores,
                'features': features,
                'overall_quality': quality_scores.get('overall', torch.tensor(0.5)),
                'confidence': torch.mean(torch.stack(list(quality_scores.values())))
            }

    class RealAestheticQualityModel(nn.Module):
        """ì‹¤ì œ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            # ResNet ê¸°ë°˜ ë°±ë³¸ (ë” ê°€ë²¼ìš´ êµ¬ì¡°)
            self.backbone = self._create_lightweight_backbone()
            
            # ë¯¸ì  íŠ¹ì„± ë¶„ì„ í—¤ë“œë“¤
            self.aesthetic_heads = nn.ModuleDict({
                'composition': self._create_head(512, 1),
                'color_harmony': self._create_head(512, 1),
                'lighting': self._create_head(512, 1),
                'balance': self._create_head(512, 1),
                'symmetry': self._create_head(512, 1)
            })
            
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        def _create_lightweight_backbone(self):
            """ê²½ëŸ‰í™”ëœ ë°±ë³¸ ìƒì„±"""
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                nn.Conv2d(64, 128, 3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(128, 256, 3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(256, 512, 3, stride=2, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                
                nn.AdaptiveAvgPool2d(1)
            )
        
        def _create_head(self, in_features: int, out_features: int):
            """ë¶„ì„ í—¤ë“œ ìƒì„±"""
            return nn.Sequential(
                nn.Linear(in_features, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, out_features),
                nn.Sigmoid()
            )
        
        def load_checkpoint(self, checkpoint_path: str):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
            try:
                if Path(checkpoint_path).exists():
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    self.load_state_dict(checkpoint, strict=False)
                    self.logger.info(f"âœ… ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                    return False
            except Exception as e:
                self.logger.error(f"âŒ ë¯¸ì  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        
        def forward(self, x):
            """ìˆœì „íŒŒ"""
            features = self.backbone(x).flatten(1)
            
            results = {}
            for name, head in self.aesthetic_heads.items():
                results[name] = head(features)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall'] = torch.mean(torch.stack(list(results.values())))
            
            return results

else:
    # PyTorch ì—†ì„ ë•Œ ë”ë¯¸ í´ë˜ìŠ¤
    class RealPerceptualQualityModel:
        def __init__(self, config=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ RealPerceptualQualityModel")
            self.config = config or {}
        
        def load_checkpoint(self, checkpoint_path: str):
            return False
        
        def predict(self, x):
            return {
                'quality_scores': {'overall': 0.7},
                'overall_quality': 0.7,
                'confidence': 0.6
            }
    
    class RealAestheticQualityModel:
        def __init__(self, config=None):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ RealAestheticQualityModel")
            self.config = config or {}
        
        def load_checkpoint(self, checkpoint_path: str):
            return False
        
        def predict(self, x):
            return {
                'composition': 0.7,
                'color_harmony': 0.8,
                'lighting': 0.75,
                'balance': 0.7,
                'symmetry': 0.8,
                'overall': 0.75
            }

# ==============================================
# ğŸ”¥ ì „ë¬¸ ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€, step_model_requests.py ìŠ¤í™ ê°œì„ )
# ==============================================
class TechnicalQualityAnalyzer:
    """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸° (step_model_requests.py DetailedDataSpec ê¸°ë°˜)"""
    
    def __init__(self, device: str = "cpu", enable_gpu: bool = False, 
                 detailed_spec: DetailedDataSpec = None):
        self.device = device
        self.enable_gpu = enable_gpu
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
        
        # step_model_requests.py DetailedDataSpec í™œìš©
        self.detailed_spec = detailed_spec
        if self.detailed_spec:
            self.input_value_ranges = detailed_spec.input_value_ranges
            self.output_value_ranges = detailed_spec.output_value_ranges
            self.preprocessing_steps = detailed_spec.preprocessing_steps
            self.postprocessing_steps = detailed_spec.postprocessing_steps
        else:
            # ê¸°ë³¸ê°’
            self.input_value_ranges = {"normalized": (0.0, 1.0), "raw": (0.0, 255.0)}
            self.output_value_ranges = {"scores": (0.0, 1.0)}
            self.preprocessing_steps = ["normalize", "resize"]
            self.postprocessing_steps = ["aggregate_scores", "clip_values"]
        
        # ë¶„ì„ ìºì‹œ
        self.analysis_cache = {}
        
        # ê¸°ìˆ ì  ë¶„ì„ ì„ê³„ê°’ë“¤
        self.thresholds = {
            'sharpness_min': 100.0,
            'noise_max': 50.0,
            'contrast_min': 20.0,
            'brightness_range': (50, 200)
        }
    
    def analyze(self, image: np.ndarray) -> Dict[str, Any]:
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        try:
            if image is None or image.size == 0:
                return self._get_fallback_technical_results()
            
            results = {}
            
            # step_model_requests.py ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            processed_image = self._apply_preprocessing(image)
            
            # 1. ì„ ëª…ë„ ë¶„ì„
            results['sharpness'] = self._analyze_sharpness(processed_image)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„
            results['noise_level'] = self._analyze_noise_level(processed_image)
            
            # 3. ëŒ€ë¹„ ë¶„ì„
            results['contrast'] = self._analyze_contrast(processed_image)
            
            # 4. ë°ê¸° ë¶„ì„
            results['brightness'] = self._analyze_brightness(processed_image)
            
            # 5. í¬í™”ë„ ë¶„ì„
            results['saturation'] = self._analyze_saturation(processed_image)
            
            # 6. ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            results['artifacts'] = self._detect_artifacts(processed_image)
            
            # 7. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall_score'] = self._calculate_technical_score(results)
            
            # step_model_requests.py í›„ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            results = self._apply_postprocessing(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_technical_results()
    
    def _apply_preprocessing(self, image: np.ndarray) -> np.ndarray:
        """step_model_requests.py ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©"""
        try:
            processed = image.copy()
            
            for step in self.preprocessing_steps:
                if step == "normalize":
                    if "normalized" in self.input_value_ranges:
                        min_val, max_val = self.input_value_ranges["normalized"]
                        processed = processed.astype(np.float32) / 255.0
                        processed = processed * (max_val - min_val) + min_val
                elif step == "resize":
                    if processed.shape[:2] != (224, 224):  # step_model_requests.py ì…ë ¥ í¬ê¸°
                        processed = cv2.resize(processed, (224, 224))
                elif step == "clip_values":
                    processed = np.clip(processed, 0, 1)
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_postprocessing(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requests.py í›„ì²˜ë¦¬ ë‹¨ê³„ ì ìš©"""
        try:
            processed_results = results.copy()
            
            for step in self.postprocessing_steps:
                if step == "aggregate_scores":
                    # ì ìˆ˜ë“¤ì„ step_model_requests.py ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ì§‘ê³„
                    quality_breakdown = {}
                    for key, value in processed_results.items():
                        if key.endswith('_score') or key in ['sharpness', 'noise_level', 'contrast', 
                                                           'brightness', 'saturation', 'artifacts']:
                            quality_breakdown[key] = float(value)
                    processed_results['quality_breakdown'] = quality_breakdown
                    
                elif step == "clip_values":
                    # ì¶œë ¥ ê°’ ë²”ìœ„ ì œí•œ
                    if "scores" in self.output_value_ranges:
                        min_val, max_val = self.output_value_ranges["scores"]
                        for key, value in processed_results.items():
                            if isinstance(value, (int, float)):
                                processed_results[key] = max(min_val, min(max_val, value))
                
                elif step == "generate_quality_report":
                    # step_model_requests.py API ì¶œë ¥ ë§¤í•‘ ì¤€ìˆ˜
                    processed_results.update({
                        "overall_quality": processed_results.get('overall_score', 0.5),
                        "confidence": self._calculate_confidence(processed_results),
                        "recommendations": self._generate_recommendations(processed_results)
                    })
            
            return processed_results
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return results
    
    def _calculate_confidence(self, results: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ë¶„ì„ ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
            scores = []
            for key, value in results.items():
                if isinstance(value, (int, float)) and 0 <= value <= 1:
                    scores.append(value)
            
            if scores:
                # ì ìˆ˜ë“¤ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
                std_dev = np.std(scores)
                confidence = max(0.3, 1.0 - std_dev)
                return min(1.0, confidence)
            else:
                return 0.5
                
        except Exception:
            return 0.5
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """step_model_requests.py API ì¶œë ¥ ë§¤í•‘ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            overall_score = results.get('overall_score', 0.5)
            
            if overall_score < 0.6:
                recommendations.append("ì „ë°˜ì ì¸ ì´ë¯¸ì§€ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if results.get('sharpness', 0.5) < 0.5:
                recommendations.append("ì´ë¯¸ì§€ ì„ ëª…ë„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ì»¤ìŠ¤ë¥¼ ë‹¤ì‹œ ë§ì¶°ë³´ì„¸ìš”.")
            
            if results.get('brightness', 0.5) < 0.4:
                recommendations.append("ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì–´ë‘¡ìŠµë‹ˆë‹¤. ì¡°ëª…ì„ ê°œì„ í•´ë³´ì„¸ìš”.")
            elif results.get('brightness', 0.5) > 0.8:
                recommendations.append("ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ë°ìŠµë‹ˆë‹¤. ë…¸ì¶œì„ ì¤„ì—¬ë³´ì„¸ìš”.")
            
            if results.get('artifacts', 0.8) < 0.6:
                recommendations.append("ì´ë¯¸ì§€ì— ì•„í‹°íŒ©íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
            
            if results.get('contrast', 0.5) < 0.4:
                recommendations.append("ì´ë¯¸ì§€ ëŒ€ë¹„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ëŒ€ë¹„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”.")
                
            if not recommendations:
                if overall_score >= 0.8:
                    recommendations.append("í›Œë¥­í•œ í’ˆì§ˆì˜ ì´ë¯¸ì§€ì…ë‹ˆë‹¤!")
                elif overall_score >= 0.6:
                    recommendations.append("ì–‘í˜¸í•œ í’ˆì§ˆì˜ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.")
                else:
                    recommendations.append("ì´ë¯¸ì§€ í’ˆì§ˆì„ ê°œì„ í•  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"âŒ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["í’ˆì§ˆ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."]
    
    def _analyze_sharpness(self, image: np.ndarray) -> float:
        """ì„ ëª…ë„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if OPENCV_AVAILABLE else np.mean(image, axis=2)
            else:
                gray = image
            
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                sharpness = laplacian.var()
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜ ì„ ëª…ë„
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                sharpness = np.var(dx) + np.var(dy)
            
            # ì •ê·œí™” (0-1)
            normalized_sharpness = min(1.0, sharpness / 10000.0)
            return max(0.0, normalized_sharpness)
            
        except Exception as e:
            self.logger.error(f"ì„ ëª…ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                # ê° ì±„ë„ë³„ ë…¸ì´ì¦ˆ ë¶„ì„
                noise_levels = []
                for channel in range(3):
                    channel_data = image[:, :, channel]
                    # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
                    if OPENCV_AVAILABLE:
                        blur = cv2.GaussianBlur(channel_data.astype(np.uint8), (5, 5), 0)
                        noise = np.abs(channel_data.astype(float) - blur.astype(float))
                    else:
                        # ê°„ë‹¨í•œ í‘œì¤€í¸ì°¨ ê¸°ë°˜
                        noise = np.std(channel_data)
                    
                    noise_level = np.mean(noise) / 255.0
                    noise_levels.append(noise_level)
                
                # í‰ê·  ë…¸ì´ì¦ˆ ë ˆë²¨
                avg_noise = np.mean(noise_levels)
            else:
                avg_noise = np.std(image) / 255.0
            
            # ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ìŒ (ì—­ìˆœ)
            return max(0.0, min(1.0, 1.0 - avg_noise * 5))
            
        except Exception as e:
            self.logger.error(f"ë…¸ì´ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_contrast(self, image: np.ndarray) -> float:
        """ëŒ€ë¹„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # RMS ëŒ€ë¹„ ê³„ì‚°
            contrast = np.std(gray)
            
            # ì •ê·œí™” (ì ì ˆí•œ ëŒ€ë¹„ ë²”ìœ„: 30-80)
            if 30 <= contrast <= 80:
                contrast_score = 1.0
            elif contrast < 30:
                contrast_score = contrast / 30.0
            else:
                contrast_score = max(0.3, 1.0 - (contrast - 80) / 100.0)
            
            return max(0.0, min(1.0, contrast_score))
            
        except Exception as e:
            self.logger.error(f"ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """ë°ê¸° ë¶„ì„"""
        try:
            brightness = np.mean(image)
            
            # ì ì ˆí•œ ë°ê¸° ë²”ìœ„ (100-160)
            if 100 <= brightness <= 160:
                brightness_score = 1.0
            elif brightness < 100:
                brightness_score = brightness / 100.0
            else:
                brightness_score = max(0.3, 1.0 - (brightness - 160) / 95.0)
            
            return max(0.0, min(1.0, brightness_score))
            
        except Exception as e:
            self.logger.error(f"ë°ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """í¬í™”ë„ ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # HSV ë³€í™˜ ë° í¬í™”ë„ ë¶„ì„
            if OPENCV_AVAILABLE:
                hsv = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2HSV)
                saturation = np.mean(hsv[:, :, 1])
            else:
                # RGB ê¸°ë°˜ í¬í™”ë„ ê·¼ì‚¬
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = np.mean((max_vals - min_vals) / (max_vals + 1e-8)) * 255
            
            # ì ì ˆí•œ í¬í™”ë„ ë²”ìœ„ (80-180)
            if 80 <= saturation <= 180:
                saturation_score = 1.0
            elif saturation < 80:
                saturation_score = saturation / 80.0
            else:
                saturation_score = max(0.3, 1.0 - (saturation - 180) / 75.0)
            
            return max(0.0, min(1.0, saturation_score))
            
        except Exception as e:
            self.logger.error(f"í¬í™”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„ìœ¼ë¡œ ì•„í‹°íŒ©íŠ¸ ì¶”ì •
            if OPENCV_AVAILABLE:
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                artifact_metric = np.std(laplacian)
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                artifact_metric = np.std(dx) + np.std(dy)
            
            # ì•„í‹°íŒ©íŠ¸ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            artifact_score = max(0.0, 1.0 - artifact_metric / 1000.0)
            return min(1.0, artifact_score)
            
        except Exception as e:
            self.logger.error(f"ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _calculate_technical_score(self, results: Dict[str, Any]) -> float:
        """ê¸°ìˆ ì  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'sharpness': 0.25,
                'noise_level': 0.20,
                'contrast': 0.15,
                'brightness': 0.15,
                'saturation': 0.10,
                'artifacts': 0.15
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            # ì •ê·œí™”
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _get_fallback_technical_results(self) -> Dict[str, Any]:
        """í´ë°± ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼"""
        return {
            'sharpness': 0.5,
            'noise_level': 0.6,
            'contrast': 0.5,
            'brightness': 0.6,
            'saturation': 0.5,
            'artifacts': 0.7,
            'overall_score': 0.55,
            'quality_breakdown': {
                'sharpness': 0.5,
                'noise_level': 0.6,
                'contrast': 0.5,
                'brightness': 0.6,
                'saturation': 0.5,
                'artifacts': 0.7
            },
            'overall_quality': 0.55,
            'confidence': 0.6,
            'recommendations': ["í’ˆì§ˆ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."],
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """ë¶„ì„ê¸° ì •ë¦¬"""
        self.analysis_cache.clear()

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤ (step_model_requests.py ì™„ì „ í˜¸í™˜)
# ==============================================
class QualityAssessmentStep(BaseStepMixin):
    """í’ˆì§ˆ í‰ê°€ Step (step_model_requests.py v8.0 ì™„ì „ í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        """BaseStepMixin v18.0+ í˜¸í™˜ ìƒì„±ì (step_model_requests.py í†µí•©)"""
        super().__init__(**kwargs)
        
        # step_model_requests.py ìŠ¤í™ ë¡œë“œ
        self.step_request = None
        self.detailed_spec = None
        if STEP_MODEL_REQUESTS_AVAILABLE:
            self.step_request = get_enhanced_step_request("QualityAssessmentStep")
            if self.step_request:
                self.detailed_spec = self.step_request.data_spec
                self.logger.info("âœ… step_model_requests.py QualityAssessmentStep ìŠ¤í™ ë¡œë“œ ì„±ê³µ")
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì • (step_model_requests.py ê¸°ë°˜)
        self.step_name = "quality_assessment"
        self.step_id = 8
        self.device = kwargs.get('device', 'mps' if self._detect_m3_max() else 'cpu')
        
        # step_model_requests.py í˜¸í™˜ ì†ì„±ë“¤
        self.is_m3_max = self._detect_m3_max()
        self.is_apple_silicon = self._detect_apple_silicon()
        self.mps_available = self._check_mps_availability()
        
        # step_model_requests.py ìŠ¤í™ ê¸°ë°˜ ì„¤ì •
        if self.step_request:
            self.optimal_batch_size = self.step_request.batch_size
            self.memory_fraction = self.step_request.memory_fraction
            self.model_architecture = self.step_request.model_architecture
            self.input_size = self.step_request.input_size
            self.device = self.step_request.device if self.step_request.device != "auto" else self.device
        else:
            self.optimal_batch_size = 1
            self.memory_fraction = 0.5
            self.model_architecture = "open_clip_vit"
            self.input_size = (224, 224)
        
        # ìƒíƒœ ê´€ë¦¬
        self.status = kwargs.get('status', {})
        self.model_loaded = False
        self.initialized = False
        
        # AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self.quality_models = {}
        self.feature_extractors = {}
        self.technical_analyzer = None
        
        # ì˜ì¡´ì„± ê´€ë¦¬
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        
        # ì„¤ì • ì´ˆê¸°í™” (step_model_requests.py ê¸°ë°˜)
        self._setup_configurations(kwargs.get('config', {}))
        
        self.logger.info(f"âœ… QualityAssessmentStep ìƒì„± ì™„ë£Œ - Device: {self.device}, M3 Max: {self.is_m3_max}")
        if self.step_request:
            self.logger.info(f"ğŸ“‹ step_model_requests.py ìŠ¤í™ ì ìš© - ëª¨ë¸: {self.step_request.model_name}")

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            
            # macOS Apple Silicon ì²´í¬
            if platform.system() != 'Darwin' or platform.machine() != 'arm64':
                return False
            
            # M3 Max êµ¬ì²´ì  ê°ì§€
            try:
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                cpu_info = result.stdout.strip().lower()
                
                if 'apple m3 max' in cpu_info:
                    return True
                elif 'apple m3' in cpu_info:
                    # M3 Pro/ê¸°ë³¸ M3ë„ í¬í•¨
                    return True
                elif 'apple' in cpu_info and 'm' in cpu_info:
                    # M1, M2 ë“±ë„ ê³ ì„±ëŠ¥ìœ¼ë¡œ ê°„ì£¼
                    return True
                    
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
                pass
            
            # PyTorch MPS ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ê³ ì„±ëŠ¥ Macìœ¼ë¡œ ê°„ì£¼
            try:
                import torch
                if torch.backends.mps.is_available():
                    return True
            except ImportError:
                pass
            
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            return False

    def _detect_apple_silicon(self) -> bool:
        """Apple Silicon ê°ì§€"""
        try:
            import platform
            return platform.system() == 'Darwin' and platform.machine() == 'arm64'
        except Exception:
            return False

    def _check_mps_availability(self) -> bool:
        """MPS ê°€ìš©ì„± ì²´í¬"""
        try:
            import torch
            return torch.backends.mps.is_available()
        except ImportError:
            return False

    def _setup_configurations(self, config: dict):
        """ì„¤ì • ì´ˆê¸°í™” (step_model_requests.py í†µí•©)"""
        self.config = {
            'quality_threshold': config.get('quality_threshold', 0.8),
            'batch_size': self.optimal_batch_size,
            'use_mps': self.mps_available,
            'memory_efficient': self.is_m3_max,
            'quality_models': config.get('quality_models', {
                'perceptual_quality': True,
                'aesthetic_quality': True,
                'technical_analysis': True
            }),
            'optimization': {
                'm3_max_optimized': self.is_m3_max,
                'apple_silicon_optimized': self.is_apple_silicon,
                'mps_enabled': self.mps_available
            }
        }
        
        # step_model_requests.py ìŠ¤í™ ë³‘í•©
        if self.step_request:
            self.config.update({
                'model_name': self.step_request.model_name,
                'primary_file': self.step_request.primary_file,
                'primary_size_mb': self.step_request.primary_size_mb,
                'search_paths': self.step_request.search_paths,
                'fallback_paths': self.step_request.fallback_paths,
                'checkpoint_patterns': self.step_request.checkpoint_patterns,
                'model_architecture': self.step_request.model_architecture,
                'conda_optimized': self.step_request.conda_optimized,
                'mps_acceleration': self.step_request.mps_acceleration
            })
        
        if self.is_m3_max:
            # M3 Max íŠ¹í™” ìµœì í™”
            self.config.update({
                'max_memory_gb': 128,
                'thread_count': 16,
                'enable_metal_performance_shaders': True,
                'use_unified_memory': True
            })

    def apply_m3_max_optimizations(self):
        """M3 Max ìµœì í™” ì ìš©"""
        if not self.is_m3_max:
            return
        
        try:
            import os
            import torch
            
            # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            os.environ['OMP_NUM_THREADS'] = '16'
            os.environ['MKL_NUM_THREADS'] = '16'
            
            # PyTorch ìŠ¤ë ˆë“œ ì„¤ì •
            if hasattr(torch, 'set_num_threads'):
                torch.set_num_threads(16)
            
            self.logger.info("ğŸ M3 Max ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")

    def get_device_info(self) -> dict:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜ (step_model_requests.py í˜¸í™˜)"""
        return {
            'device': self.device,
            'is_m3_max': self.is_m3_max,
            'is_apple_silicon': self.is_apple_silicon,
            'mps_available': self.mps_available,
            'optimal_batch_size': self.optimal_batch_size,
            'memory_fraction': self.memory_fraction,
            'model_architecture': self.model_architecture,
            'step_request_loaded': self.step_request is not None,
            'detailed_spec_available': self.detailed_spec is not None
        }

    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (step_model_requests.py í˜¸í™˜)"""
        self.model_loader = model_loader
        self.logger.info("âœ… QualityAssessmentStep ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        self.memory_manager = memory_manager
        self.logger.info("âœ… QualityAssessmentStep MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        self.data_converter = data_converter
        self.logger.info("âœ… QualityAssessmentStep DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")

    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        if self.initialized:
            return True
        
        try:
            self.logger.info("ğŸ”„ QualityAssessmentStep ì´ˆê¸°í™” ì‹œì‘...")
            
            # M3 Max ìµœì í™” ì ìš©
            if self.is_m3_max:
                self.apply_m3_max_optimizations()
            
            # step_model_requests.py ìŠ¤í™ ê¸°ë°˜ ëª¨ë¸ ë¡œë”©
            await self._load_quality_models()
            
            # ê¸°ìˆ ì  ë¶„ì„ê¸° ì´ˆê¸°í™” (DetailedDataSpec í™œìš©)
            self._initialize_technical_analyzer()
            
            self.initialized = True
            self.logger.info("âœ… QualityAssessmentStep ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ QualityAssessmentStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def _load_quality_models(self):
        """í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë”© (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        try:
            self.logger.info("ğŸ¤– step_model_requests.py ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # step_model_requests.py ìŠ¤í™ì—ì„œ ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_config = {}
            if self.step_request:
                model_config = {
                    'model_architecture': self.step_request.model_architecture,
                    'input_size': self.step_request.input_size,
                    'device': self.device,
                    'precision': self.step_request.precision,
                    'memory_fraction': self.step_request.memory_fraction,
                    'batch_size': self.step_request.batch_size
                }
            
            # ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë”©
            if self.config['quality_models'].get('perceptual_quality', True):
                self.quality_models['perceptual'] = RealPerceptualQualityModel(config=model_config)
                
                # step_model_requests.py ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ë¡œë”© ì‹œë„
                if self.step_request:
                    for search_path in self.step_request.search_paths:
                        primary_path = Path(search_path) / self.step_request.primary_file
                        if primary_path.exists():
                            if self.quality_models['perceptual'].load_checkpoint(str(primary_path)):
                                self.logger.info(f"âœ… ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ: {primary_path}")
                                break
                    
                    # ëŒ€ì²´ íŒŒì¼ë“¤ë„ ì‹œë„
                    for alt_file, _ in self.step_request.alternative_files:
                        if alt_file in ["lpips_vgg.pth", "lpips_alex.pth"]:
                            for search_path in self.step_request.search_paths:
                                alt_path = Path(search_path) / alt_file
                                if alt_path.exists():
                                    if self.quality_models['perceptual'].load_checkpoint(str(alt_path)):
                                        self.logger.info(f"âœ… ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ (ëŒ€ì²´) ë¡œë“œ: {alt_path}")
                                        break
            
            # ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë”©
            if self.config['quality_models'].get('aesthetic_quality', True):
                self.quality_models['aesthetic'] = RealAestheticQualityModel(config=model_config)
                
                # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆë‹¤ë©´ ë¡œë”©
                if hasattr(self, 'config') and 'aesthetic_model_path' in self.config:
                    aesthetic_path = self.config['aesthetic_model_path']
                    if aesthetic_path and Path(aesthetic_path).exists():
                        self.quality_models['aesthetic'].load_checkpoint(aesthetic_path)
                        self.logger.info(f"âœ… ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ: {aesthetic_path}")
            
            # ëª¨ë¸ì„ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if TORCH_AVAILABLE:
                for model_name, model in self.quality_models.items():
                    if hasattr(model, 'to'):
                        model.to(self.device)
                        if hasattr(model, 'eval'):
                            model.eval()
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™ ì™„ë£Œ")
            
            self.model_loaded = True
            self.logger.info("âœ… step_model_requests.py ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°± ëª¨ë¸ ì‚¬ìš©
            self.quality_models = {
                'perceptual': RealPerceptualQualityModel(),
                'aesthetic': RealAestheticQualityModel()
            }

    def _initialize_technical_analyzer(self):
        """ê¸°ìˆ ì  ë¶„ì„ê¸° ì´ˆê¸°í™” (DetailedDataSpec í™œìš©)"""
        try:
            self.technical_analyzer = TechnicalQualityAnalyzer(
                device=self.device,
                enable_gpu=self.mps_available,
                detailed_spec=self.detailed_spec
            )
            self.logger.info("âœ… TechnicalQualityAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (DetailedDataSpec ê¸°ë°˜)")
            
        except Exception as e:
            self.logger.error(f"âŒ TechnicalQualityAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í´ë°± ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(device=self.device)

    async def process(self, input_data: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ (step_model_requests.py ì™„ì „ í˜¸í™˜)"""
        start_time = time.time()
        
        try:
            if not self.initialized:
                await self.initialize()
            
            self.logger.info("ğŸ”„ QualityAssessmentStep ì²˜ë¦¬ ì‹œì‘...")
            
            # step_model_requests.py ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦
            validated_input = self._validate_input_schema(input_data)
            
            # step_model_requests.py DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬
            processed_data = self._apply_detailed_preprocessing(validated_input)
            
            # ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            quality_results = await self._perform_quality_assessment(processed_data)
            
            # step_model_requests.py DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬
            final_results = self._apply_detailed_postprocessing(quality_results)
            
            # step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜
            output_data = self._format_output_schema(final_results)
            
            processing_time = time.time() - start_time
            
            # FastAPI í˜¸í™˜ ì‘ë‹µ ìƒì„±
            response = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': processing_time,
                'device_info': self.get_device_info(),
                **output_data
            }
            
            self.logger.info(f"âœ… QualityAssessmentStep ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time
            
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': processing_time,
                'fallback_results': self._get_fallback_quality_results()
            }

    def _validate_input_schema(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requests.py ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        try:
            validated = {}
            
            # step_model_requests.py ìŠ¤í™ì—ì„œ ì˜ˆìƒ ì…ë ¥ í™•ì¸
            if self.detailed_spec:
                expected_inputs = self.detailed_spec.accepts_from_previous_step
                
                # Step 06 (VirtualFittingStep)ì—ì„œ ì˜¤ëŠ” ë°ì´í„° ê²€ì¦
                step_06_inputs = expected_inputs.get("step_06", {})
                if "final_result" in step_06_inputs:
                    if "final_result" in input_data:
                        validated["final_result"] = input_data["final_result"]
                    elif "fitted_image" in input_data:
                        validated["final_result"] = input_data["fitted_image"]
                    elif "enhanced_image" in input_data:
                        validated["final_result"] = input_data["enhanced_image"]
                
                # Step 07 (PostProcessingStep)ì—ì„œ ì˜¤ëŠ” ë°ì´í„° ê²€ì¦
                step_07_inputs = expected_inputs.get("step_07", {})
                if "enhanced_image" in step_07_inputs:
                    if "enhanced_image" in input_data:
                        validated["enhanced_image"] = input_data["enhanced_image"]
                
                # ì°¸ì¡° ì´ë¯¸ì§€ë“¤
                if "original_person" in input_data:
                    validated["original_person"] = input_data["original_person"]
                if "original_clothing" in input_data:
                    validated["original_clothing"] = input_data["original_clothing"]
            
            # API ì…ë ¥ ë§¤í•‘ ê²€ì¦ (step_model_requests.py ê¸°ë°˜)
            if self.detailed_spec and self.detailed_spec.api_input_mapping:
                api_mapping = self.detailed_spec.api_input_mapping
                
                for api_field, data_type in api_mapping.items():
                    if api_field in input_data:
                        validated[api_field] = input_data[api_field]
            
            # ê¸°ë³¸ ì…ë ¥ì´ ì—†ìœ¼ë©´ í´ë°±
            if not validated and input_data:
                validated = input_data.copy()
            
            self.logger.debug(f"âœ… ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: {len(validated)}ê°œ í•„ë“œ")
            return validated
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return input_data

    def _apply_detailed_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requests.py DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬"""
        try:
            processed = {}
            
            # step_model_requests.py ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            if self.detailed_spec:
                preprocessing_steps = self.detailed_spec.preprocessing_steps
                input_shapes = self.detailed_spec.input_shapes
                input_value_ranges = self.detailed_spec.input_value_ranges
                normalization_mean = self.detailed_spec.normalization_mean
                normalization_std = self.detailed_spec.normalization_std
            else:
                # ê¸°ë³¸ê°’
                preprocessing_steps = ["resize_224x224", "normalize_clip", "extract_features"]
                input_shapes = {"final_result": (3, 224, 224)}
                input_value_ranges = {"clip_normalized": (-2.0, 2.0)}
                normalization_mean = (0.48145466, 0.4578275, 0.40821073)
                normalization_std = (0.26862954, 0.26130258, 0.27577711)
            
            # ê° ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ì „ì²˜ë¦¬ ì ìš©
            for key, data in input_data.items():
                if key in ["final_result", "enhanced_image", "original_person", "original_clothing"]:
                    processed_image = self._preprocess_image(
                        data, 
                        preprocessing_steps, 
                        input_shapes, 
                        input_value_ranges,
                        normalization_mean,
                        normalization_std
                    )
                    processed[key] = processed_image
                else:
                    processed[key] = data
            
            self.logger.debug(f"âœ… DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data

    def _preprocess_image(self, image_data: Any, preprocessing_steps: List[str], 
                         input_shapes: Dict[str, Tuple], input_value_ranges: Dict[str, Tuple],
                         normalization_mean: Tuple, normalization_std: Tuple) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)"""
        try:
            # ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ ì²˜ë¦¬
            if isinstance(image_data, str):
                # base64 ë¬¸ìì—´ì¸ ê²½ìš°
                if image_data.startswith('data:image'):
                    image_data = image_data.split(',')[1]
                image_bytes = base64.b64decode(image_data)
                image = Image.open(io.BytesIO(image_bytes))
                image_array = np.array(image)
            elif isinstance(image_data, np.ndarray):
                image_array = image_data
            elif hasattr(image_data, 'read'):
                # íŒŒì¼ ê°ì²´ì¸ ê²½ìš°
                image = Image.open(image_data)
                image_array = np.array(image)
            else:
                # PIL Imageì¸ ê²½ìš°
                image_array = np.array(image_data)
            
            # RGB ë³€í™˜
            if len(image_array.shape) == 3 and image_array.shape[2] == 4:
                # RGBA to RGB
                image_array = image_array[:, :, :3]
            elif len(image_array.shape) == 2:
                # Grayscale to RGB
                image_array = np.stack([image_array] * 3, axis=-1)
            
            # ì „ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            processed = image_array.astype(np.float32)
            
            for step in preprocessing_steps:
                if step == "resize_224x224":
                    processed = cv2.resize(processed, (224, 224))
                elif step == "resize_original":
                    # ì›ë³¸ í¬ê¸° ìœ ì§€
                    pass
                elif step == "normalize_clip":
                    # CLIP ì •ê·œí™”
                    processed = processed / 255.0
                    for i in range(3):
                        processed[:, :, i] = (processed[:, :, i] - normalization_mean[i]) / normalization_std[i]
                elif step == "normalize_imagenet":
                    # ImageNet ì •ê·œí™”
                    processed = processed / 255.0
                    imagenet_mean = (0.485, 0.456, 0.406)
                    imagenet_std = (0.229, 0.224, 0.225)
                    for i in range(3):
                        processed[:, :, i] = (processed[:, :, i] - imagenet_mean[i]) / imagenet_std[i]
                elif step == "to_tensor":
                    # ì±„ë„ ìˆœì„œ ë³€ê²½ (H, W, C) -> (C, H, W)
                    processed = np.transpose(processed, (2, 0, 1))
                elif step == "extract_features":
                    # íŠ¹ì§• ì¶”ì¶œ ì¤€ë¹„
                    if len(processed.shape) == 3:
                        processed = np.expand_dims(processed, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            
            # ê°’ ë²”ìœ„ í´ë¦¬í•‘
            if "clip_normalized" in input_value_ranges:
                min_val, max_val = input_value_ranges["clip_normalized"]
                processed = np.clip(processed, min_val, max_val)
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì²˜ë¦¬
            if isinstance(image_data, np.ndarray):
                return cv2.resize(image_data, (224, 224)).astype(np.float32) / 255.0
            else:
                return np.zeros((224, 224, 3), dtype=np.float32)

    async def _perform_quality_assessment(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        try:
            results = {}
            
            # ë©”ì¸ ì´ë¯¸ì§€ ì¶”ì¶œ
            main_image = None
            for key in ["final_result", "enhanced_image"]:
                if key in processed_data:
                    main_image = processed_data[key]
                    break
            
            if main_image is None:
                return self._get_fallback_quality_results()
            
            # 1. ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„
            if self.technical_analyzer:
                technical_results = self.technical_analyzer.analyze(main_image)
                results.update(technical_results)
            
            # 2. AI ëª¨ë¸ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
            if self.model_loaded and self.quality_models:
                
                # ì§€ê°ì  í’ˆì§ˆ í‰ê°€
                if 'perceptual' in self.quality_models:
                    perceptual_results = await self._run_perceptual_assessment(main_image)
                    results.update(perceptual_results)
                
                # ë¯¸ì  í’ˆì§ˆ í‰ê°€
                if 'aesthetic' in self.quality_models:
                    aesthetic_results = await self._run_aesthetic_assessment(main_image)
                    results.update(aesthetic_results)
            
            # 3. ì°¸ì¡° ì´ë¯¸ì§€ì™€ì˜ ë¹„êµ (ìˆëŠ” ê²½ìš°)
            if "original_person" in processed_data or "original_clothing" in processed_data:
                comparison_results = await self._run_comparison_assessment(
                    main_image, processed_data
                )
                results.update(comparison_results)
            
            # 4. ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            overall_score = self._calculate_overall_quality(results)
            results['overall_quality'] = overall_score
            results['overall_score'] = overall_score
            
            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_assessment_confidence(results)
            results['confidence'] = confidence
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._get_fallback_quality_results()

    async def _run_perceptual_assessment(self, image: np.ndarray) -> Dict[str, Any]:
        """ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        try:
            perceptual_model = self.quality_models['perceptual']
            results = {}
            
            if TORCH_AVAILABLE and hasattr(perceptual_model, 'forward'):
                # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                with torch.no_grad():
                    if len(image.shape) == 3:
                        image_tensor = torch.from_numpy(image).unsqueeze(0).to(self.device)
                    else:
                        image_tensor = torch.from_numpy(image).to(self.device)
                    
                    if image_tensor.shape[1] != 3:  # (B, H, W, C) -> (B, C, H, W)
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                    
                    model_output = perceptual_model(image_tensor)
                    
                    # ê²°ê³¼ ì¶”ì¶œ
                    if 'quality_scores' in model_output:
                        quality_scores = model_output['quality_scores']
                        for aspect, score_tensor in quality_scores.items():
                            if hasattr(score_tensor, 'item'):
                                results[f"{aspect}_score"] = float(score_tensor.item())
                            else:
                                results[f"{aspect}_score"] = float(score_tensor)
                    
                    if 'overall_quality' in model_output:
                        results['perceptual_quality'] = float(model_output['overall_quality'].item())
                    
                    if 'confidence' in model_output:
                        results['perceptual_confidence'] = float(model_output['confidence'].item())
            
            else:
                # ë”ë¯¸ ëª¨ë¸ì¸ ê²½ìš°
                prediction = perceptual_model.predict(image)
                if 'quality_scores' in prediction:
                    for aspect, score in prediction['quality_scores'].items():
                        results[f"{aspect}_score"] = float(score)
                
                results['perceptual_quality'] = float(prediction.get('overall_quality', 0.7))
                results['perceptual_confidence'] = float(prediction.get('confidence', 0.6))
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'perceptual_quality': 0.7,
                'perceptual_confidence': 0.6,
                'overall_score': 0.7
            }

    async def _run_aesthetic_assessment(self, image: np.ndarray) -> Dict[str, Any]:
        """ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        try:
            aesthetic_model = self.quality_models['aesthetic']
            results = {}
            
            if TORCH_AVAILABLE and hasattr(aesthetic_model, 'forward'):
                # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                with torch.no_grad():
                    if len(image.shape) == 3:
                        image_tensor = torch.from_numpy(image).unsqueeze(0).to(self.device)
                    else:
                        image_tensor = torch.from_numpy(image).to(self.device)
                    
                    if image_tensor.shape[1] != 3:  # (B, H, W, C) -> (B, C, H, W)
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                    
                    model_output = aesthetic_model(image_tensor)
                    
                    # ê²°ê³¼ ì¶”ì¶œ
                    for aspect, score_tensor in model_output.items():
                        if hasattr(score_tensor, 'item'):
                            results[f"aesthetic_{aspect}"] = float(score_tensor.item())
                        else:
                            results[f"aesthetic_{aspect}"] = float(score_tensor)
            
            else:
                # ë”ë¯¸ ëª¨ë¸ì¸ ê²½ìš°
                prediction = aesthetic_model.predict(image)
                for aspect, score in prediction.items():
                    results[f"aesthetic_{aspect}"] = float(score)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'aesthetic_composition': 0.7,
                'aesthetic_color_harmony': 0.8,
                'aesthetic_lighting': 0.75,
                'aesthetic_balance': 0.7,
                'aesthetic_symmetry': 0.8,
                'aesthetic_overall': 0.75
            }

    async def _run_comparison_assessment(self, main_image: np.ndarray, 
                                       processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì°¸ì¡° ì´ë¯¸ì§€ì™€ì˜ ë¹„êµ í‰ê°€"""
        try:
            results = {}
            
            # ì›ë³¸ ì¸ë¬¼ ì´ë¯¸ì§€ì™€ ë¹„êµ
            if "original_person" in processed_data:
                person_similarity = self._calculate_image_similarity(
                    main_image, processed_data["original_person"]
                )
                results['person_similarity'] = person_similarity
            
            # ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€ì™€ ë¹„êµ
            if "original_clothing" in processed_data:
                clothing_similarity = self._calculate_image_similarity(
                    main_image, processed_data["original_clothing"]
                )
                results['clothing_similarity'] = clothing_similarity
            
            # ì „ì²´ ì¼ì¹˜ë„ ê³„ì‚°
            if "original_person" in processed_data and "original_clothing" in processed_data:
                overall_similarity = (results.get('person_similarity', 0.5) + 
                                    results.get('clothing_similarity', 0.5)) / 2
                results['overall_similarity'] = overall_similarity
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„êµ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'person_similarity': 0.7,
                'clothing_similarity': 0.7,
                'overall_similarity': 0.7
            }

    def _calculate_image_similarity(self, image1: np.ndarray, image2: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # í¬ê¸° í†µì¼
            if image1.shape != image2.shape:
                image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]))
            
            # SSIM ê³„ì‚° (scikit-image ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if SKIMAGE_AVAILABLE:
                if len(image1.shape) == 3:
                    # ì»¬ëŸ¬ ì´ë¯¸ì§€
                    similarity = 0.0
                    for i in range(3):
                        channel_sim = ssim(image1[:, :, i], image2[:, :, i], data_range=1.0)
                        similarity += channel_sim
                    similarity /= 3
                else:
                    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                    similarity = ssim(image1, image2, data_range=1.0)
            else:
                # ê°„ë‹¨í•œ MSE ê¸°ë°˜ ìœ ì‚¬ë„
                mse = np.mean((image1 - image2) ** 2)
                similarity = max(0.0, 1.0 - mse)
            
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def _calculate_overall_quality(self, results: Dict[str, Any]) -> float:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = []
            weights = {}
            
            # ê¸°ìˆ ì  í’ˆì§ˆ (ê°€ì¤‘ì¹˜ 30%)
            if 'overall_score' in results:
                scores.append(results['overall_score'])
                weights[len(scores)-1] = 0.3
            
            # ì§€ê°ì  í’ˆì§ˆ (ê°€ì¤‘ì¹˜ 40%)
            if 'perceptual_quality' in results:
                scores.append(results['perceptual_quality'])
                weights[len(scores)-1] = 0.4
            
            # ë¯¸ì  í’ˆì§ˆ (ê°€ì¤‘ì¹˜ 20%)
            if 'aesthetic_overall' in results:
                scores.append(results['aesthetic_overall'])
                weights[len(scores)-1] = 0.2
            
            # ë¹„êµ í‰ê°€ (ê°€ì¤‘ì¹˜ 10%)
            if 'overall_similarity' in results:
                scores.append(results['overall_similarity'])
                weights[len(scores)-1] = 0.1
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            if scores:
                weighted_sum = sum(score * weights.get(i, 1.0/len(scores)) 
                                 for i, score in enumerate(scores))
                total_weight = sum(weights.values()) if weights else 1.0
                overall_score = weighted_sum / total_weight
            else:
                overall_score = 0.5
            
            return max(0.0, min(1.0, overall_score))
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def _calculate_assessment_confidence(self, results: Dict[str, Any]) -> float:
        """í‰ê°€ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_scores = []
            
            # ê° í‰ê°€ ëª¨ë“ˆì˜ ì‹ ë¢°ë„ ìˆ˜ì§‘
            if 'confidence' in results:
                confidence_scores.append(results['confidence'])
            
            if 'perceptual_confidence' in results:
                confidence_scores.append(results['perceptual_confidence'])
            
            # ì ìˆ˜ë“¤ì˜ ì¼ê´€ì„± ê¸°ë°˜ ì‹ ë¢°ë„
            quality_scores = []
            for key, value in results.items():
                if ('score' in key or 'quality' in key) and isinstance(value, (int, float)):
                    if 0 <= value <= 1:
                        quality_scores.append(value)
            
            if quality_scores:
                # ì ìˆ˜ë“¤ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
                std_dev = np.std(quality_scores)
                consistency_confidence = max(0.3, 1.0 - std_dev)
                confidence_scores.append(consistency_confidence)
            
            # í‰ê·  ì‹ ë¢°ë„
            if confidence_scores:
                final_confidence = np.mean(confidence_scores)
            else:
                final_confidence = 0.6  # ê¸°ë³¸ê°’
            
            return max(0.0, min(1.0, final_confidence))
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.6

    def _apply_detailed_postprocessing(self, quality_results: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requests.py DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬"""
        try:
            processed = quality_results.copy()
            
            # step_model_requests.py í›„ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            if self.detailed_spec:
                postprocessing_steps = self.detailed_spec.postprocessing_steps
                output_value_ranges = self.detailed_spec.output_value_ranges
            else:
                # ê¸°ë³¸ê°’
                postprocessing_steps = ["compute_lpips", "aggregate_metrics", "generate_quality_report"]
                output_value_ranges = {"scores": (0.0, 1.0)}
            
            for step in postprocessing_steps:
                if step == "compute_lpips":
                    # LPIPS ì ìˆ˜ ê³„ì‚° (ì§€ê°ì  ê±°ë¦¬)
                    if 'perceptual_quality' in processed:
                        processed['lpips_score'] = 1.0 - processed['perceptual_quality']
                
                elif step == "aggregate_metrics":
                    # ë©”íŠ¸ë¦­ ì§‘ê³„
                    quality_breakdown = {}
                    for key, value in processed.items():
                        if ('score' in key or 'quality' in key) and isinstance(value, (int, float)):
                            quality_breakdown[key] = float(value)
                    processed['quality_breakdown'] = quality_breakdown
                
                elif step == "generate_quality_report":
                    # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
                    processed['recommendations'] = self._generate_quality_recommendations(processed)
                    processed['quality_grade'] = self._determine_quality_grade(processed.get('overall_quality', 0.5))
            
            # ì¶œë ¥ ê°’ ë²”ìœ„ ì œí•œ
            if "scores" in output_value_ranges:
                min_val, max_val = output_value_ranges["scores"]
                for key, value in processed.items():
                    if isinstance(value, (int, float)) and ('score' in key or 'quality' in key):
                        processed[key] = max(min_val, min(max_val, float(value)))
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return quality_results

    def _generate_quality_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        try:
            recommendations = []
            overall_quality = results.get('overall_quality', 0.5)
            
            # ì „ì²´ í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            if overall_quality >= 0.9:
                recommendations.append("ğŸŒŸ íƒì›”í•œ í’ˆì§ˆì˜ ê°€ìƒ í”¼íŒ… ê²°ê³¼ì…ë‹ˆë‹¤!")
            elif overall_quality >= 0.8:
                recommendations.append("âœ¨ ë§¤ìš° ì¢‹ì€ í’ˆì§ˆì˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
            elif overall_quality >= 0.7:
                recommendations.append("ğŸ‘ ì–‘í˜¸í•œ í’ˆì§ˆì˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
            elif overall_quality >= 0.6:
                recommendations.append("âš ï¸ í’ˆì§ˆì„ ê°œì„ í•  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            else:
                recommendations.append("ğŸ”§ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ì„¸ë¶€ ì˜ì—­ë³„ ê¶Œì¥ì‚¬í•­
            if results.get('sharpness', 0.5) < 0.6:
                recommendations.append("â€¢ ì´ë¯¸ì§€ ì„ ëª…ë„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if results.get('color_score', 0.5) < 0.6:
                recommendations.append("â€¢ ìƒ‰ìƒ ì¡°í™”ë¥¼ ê°œì„ í•´ë³´ì„¸ìš”.")
            
            if results.get('fitting_score', 0.5) < 0.6:
                recommendations.append("â€¢ ì˜ë¥˜ í”¼íŒ… ì •í™•ë„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”.")
            
            if results.get('realism_score', 0.5) < 0.6:
                recommendations.append("â€¢ ë” ìì—°ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ìœ„í•´ ì¡°ëª…ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
            
            if results.get('artifacts_score', 0.8) < 0.7:
                recommendations.append("â€¢ ì•„í‹°íŒ©íŠ¸ ì œê±°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ë¹„êµ í‰ê°€ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            if results.get('person_similarity', 0.7) < 0.6:
                recommendations.append("â€¢ ì›ë³¸ ì¸ë¬¼ê³¼ì˜ ìœ ì‚¬ì„±ì„ ë†’ì—¬ë³´ì„¸ìš”.")
            
            if results.get('clothing_similarity', 0.7) < 0.6:
                recommendations.append("â€¢ ì˜ë¥˜ ì¬í˜„ ì •í™•ë„ë¥¼ ê°œì„ í•´ë³´ì„¸ìš”.")
            
            # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if len(recommendations) == 1:  # ì „ì²´ í‰ê°€ë§Œ ìˆëŠ” ê²½ìš°
                if overall_quality >= 0.8:
                    recommendations.append("â€¢ í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.")
                else:
                    recommendations.append("â€¢ ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
                    recommendations.append("â€¢ ì¡°ëª…ì´ ê· ì¼í•œ í™˜ê²½ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”.")
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"âŒ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."]

    def _determine_quality_grade(self, overall_quality: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if overall_quality >= 0.9:
            return QualityGrade.EXCELLENT.value
        elif overall_quality >= 0.8:
            return QualityGrade.GOOD.value
        elif overall_quality >= 0.6:
            return QualityGrade.ACCEPTABLE.value
        elif overall_quality >= 0.4:
            return QualityGrade.POOR.value
        else:
            return QualityGrade.FAILED.value

    def _format_output_schema(self, final_results: Dict[str, Any]) -> Dict[str, Any]:
        """step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ í˜•ì‹í™”"""
        try:
            # step_model_requests.py API ì¶œë ¥ ë§¤í•‘ ì¤€ìˆ˜
            output = {}
            
            if self.detailed_spec and self.detailed_spec.api_output_mapping:
                api_mapping = self.detailed_spec.api_output_mapping
                
                # API ë§¤í•‘ì— ë”°ë¥¸ ì¶œë ¥ êµ¬ì„±
                for api_field, data_type in api_mapping.items():
                    if api_field == "overall_quality":
                        output[api_field] = float(final_results.get('overall_quality', 0.5))
                    elif api_field == "quality_breakdown":
                        output[api_field] = final_results.get('quality_breakdown', {})
                    elif api_field == "recommendations":
                        output[api_field] = final_results.get('recommendations', [])
                    elif api_field == "confidence":
                        output[api_field] = float(final_results.get('confidence', 0.6))
            
            # step_model_requests.py ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜
            if self.detailed_spec and self.detailed_spec.step_output_schema:
                step_output = self.detailed_spec.step_output_schema.get("final_output", {})
                
                for field, data_type in step_output.items():
                    if field == "quality_assessment":
                        output[field] = final_results.get('quality_breakdown', {})
                    elif field == "final_score":
                        output[field] = float(final_results.get('overall_quality', 0.5))
                    elif field == "recommendations":
                        output[field] = final_results.get('recommendations', [])
            
            # ê¸°ë³¸ ì¶œë ¥ (ìŠ¤í‚¤ë§ˆê°€ ì—†ëŠ” ê²½ìš°)
            if not output:
                output = {
                    "overall_quality": float(final_results.get('overall_quality', 0.5)),
                    "quality_breakdown": final_results.get('quality_breakdown', {}),
                    "recommendations": final_results.get('recommendations', []),
                    "confidence": float(final_results.get('confidence', 0.6))
                }
            
            # QualityMetrics ê°ì²´ë¡œ ë³€í™˜
            quality_metrics = QualityMetrics(
                overall_score=output.get("overall_quality", 0.5),
                confidence=output.get("confidence", 0.6),
                quality_breakdown=output.get("quality_breakdown", {}),
                recommendations=output.get("recommendations", []),
                processing_time=final_results.get('processing_time', 0.0),
                device_used=self.device,
                model_version="v18.0"
            )
            
            # ì„¸ë¶€ ì ìˆ˜ë“¤ ì„¤ì •
            quality_breakdown = output.get("quality_breakdown", {})
            if quality_breakdown:
                quality_metrics.sharpness_score = quality_breakdown.get('sharpness', 0.5)
                quality_metrics.color_score = quality_breakdown.get('color_score', 0.5)
                quality_metrics.fitting_score = quality_breakdown.get('fitting_score', 0.5)
                quality_metrics.realism_score = quality_breakdown.get('realism_score', 0.5)
                quality_metrics.artifacts_score = quality_breakdown.get('artifacts_score', 0.8)
                quality_metrics.alignment_score = quality_breakdown.get('alignment_score', 0.7)
                quality_metrics.lighting_score = quality_breakdown.get('lighting_score', 0.7)
                quality_metrics.texture_score = quality_breakdown.get('texture_score', 0.7)
            
            # FastAPI í˜¸í™˜ ì‘ë‹µê³¼ ë‚´ë¶€ ê²°ê³¼ ëª¨ë‘ ë°˜í™˜
            return {
                **output,
                "quality_metrics": quality_metrics.to_dict(),
                "fastapi_response": quality_metrics.to_fastapi_response(),
                "quality_grade": final_results.get('quality_grade', 'acceptable')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ í˜•ì‹í™” ì‹¤íŒ¨: {e}")
            return {
                "overall_quality": 0.5,
                "quality_breakdown": {},
                "recommendations": ["í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."],
                "confidence": 0.6
            }

    def _get_fallback_quality_results(self) -> Dict[str, Any]:
        """í´ë°± í’ˆì§ˆ í‰ê°€ ê²°ê³¼"""
        return {
            'overall_quality': 0.6,
            'quality_breakdown': {
                'sharpness': 0.6,
                'color_score': 0.6,
                'fitting_score': 0.6,
                'realism_score': 0.6,
                'artifacts_score': 0.7,
                'alignment_score': 0.6,
                'lighting_score': 0.6,
                'texture_score': 0.6
            },
            'confidence': 0.5,
            'recommendations': [
                "ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.",
                "ë” ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ AI ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”."
            ],
            'quality_grade': 'acceptable',
            'analysis_method': 'fallback'
        }

    # ==============================================
    # ğŸ”¥ PipelineManager í•„ìˆ˜ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ì¶”ê°€
    # ==============================================
    
    def validate_dependencies_github_format(self, format_type: str = "boolean") -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (PipelineManager í•„ìˆ˜)"""
        try:
            if format_type == "boolean":
                return {
                    'model_loader': self.model_loader is not None,
                    'step_interface': hasattr(self, 'step_interface') and self.step_interface is not None,
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None,
                    'step_requests_available': STEP_MODEL_REQUESTS_AVAILABLE,
                    'detailed_spec_available': self.detailed_spec is not None
                }
            else:
                return {
                    "success": True,
                    "total_dependencies": 4,
                    "validated_dependencies": sum([
                        self.model_loader is not None,
                        hasattr(self, 'step_interface'),
                        self.memory_manager is not None,
                        self.data_converter is not None
                    ]),
                    "github_compatible": True,
                    "step_requests_integrated": STEP_MODEL_REQUESTS_AVAILABLE
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}

    def _force_mps_device(self):
        """MPS ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì • (PipelineManager í˜¸í™˜ì„±)"""
        try:
            if self.is_m3_max and self.mps_available:
                self.device = 'mps'
                return True
            return False
        except Exception:
            return False

    def _setup_configurations(self):
        """ì„¤ì • ì´ˆê¸°í™” (PipelineManager í˜¸í™˜ì„±)"""
        try:
            # ì´ë¯¸ _setup_configurationsê°€ __init__ì—ì„œ í˜¸ì¶œë˜ë¯€ë¡œ ì¶”ê°€ ì„¤ì •ë§Œ
            if not hasattr(self, 'assessment_config'):
                self.assessment_config = {
                    'use_clip': True,
                    'use_aesthetic': True,
                    'quality_threshold': 0.8
                }
            
            if not hasattr(self, 'optimization_enabled'):
                self.optimization_enabled = self.is_m3_max
                
            if not hasattr(self, 'analysis_depth'):
                self.analysis_depth = 'comprehensive'
                
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ QualityAssessmentStep ì¶”ê°€ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def warmup(self) -> bool:
        """ëª¨ë¸ ì›œì—… (PipelineManager í•„ìˆ˜)"""
        try:
            if not self.initialized:
                await self.initialize()
            
            # AI ëª¨ë¸ ì›œì—…
            if self.quality_models and TORCH_AVAILABLE:
                dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
                
                for model_name, model in self.quality_models.items():
                    if hasattr(model, 'forward'):
                        with torch.no_grad():
                            _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ì›œì—… ì™„ë£Œ")
            
            # ê¸°ìˆ ì  ë¶„ì„ê¸° ì›œì—…
            if self.technical_analyzer:
                dummy_image = np.random.rand(224, 224, 3).astype(np.float32)
                _ = self.technical_analyzer.analyze(dummy_image)
                self.logger.info("âœ… TechnicalQualityAnalyzer ì›œì—… ì™„ë£Œ")
            
            # ì›œì—… ì™„ë£Œ í”Œë˜ê·¸
            if not hasattr(self, 'warmup_completed'):
                self.warmup_completed = True
            
            self.logger.info("âœ… QualityAssessmentStep ì›œì—… ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì›œì—… ì‹¤íŒ¨: {e}")
            return False

    def register_model_requirement(self, model_name: str, **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (StepInterface í˜¸í™˜)"""
        try:
            if not hasattr(self, 'registered_models'):
                self.registered_models = {}
            
            self.registered_models[model_name] = {
                'timestamp': time.time(),
                'requirements': kwargs,
                'status': 'registered'
            }
            
            self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False

    def ensure_step_compatibility(self, config: Dict[str, Any] = None):
        """Step í˜¸í™˜ì„± ë³´ì¥ (PipelineManager ê¸€ë¡œë²Œ í˜¸í™˜ì„±)"""
        try:
            config = config or {}
            
            # í•„ìˆ˜ ì†ì„± ì„¤ì •
            essential_attrs = {
                'step_name': 'quality_assessment',
                'step_id': 8,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'model_loaded': self.model_loaded,
                'warmup_completed': getattr(self, 'warmup_completed', False),
                'optimization_enabled': self.is_m3_max,
                'assessment_config': getattr(self, 'assessment_config', {
                    'use_clip': True,
                    'use_aesthetic': True,
                    'quality_threshold': 0.8
                }),
                'analysis_depth': 'comprehensive',
                'quality_threshold': 0.8
            }
            
            for attr, value in essential_attrs.items():
                if not hasattr(self, attr):
                    setattr(self, attr, value)
            
            # ë¡œê±° í™•ì¸
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(f"steps.{self.__class__.__name__}")
            
            return True
            
        except Exception as e:
            logging.getLogger(__name__).error(f"âŒ Step í˜¸í™˜ì„± ë³´ì¥ ì‹¤íŒ¨: {e}")
            return False

    # ==============================================
    # ğŸ”¥ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
    # ==============================================
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
        info = {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'step_type': 'quality_assessment',
            'device': self.device,
            'initialized': self.initialized,
            'model_loaded': self.model_loaded,
            'memory_gb': getattr(self, 'memory_gb', 128 if self.is_m3_max else 16),
            'is_m3_max': self.is_m3_max,
            'base_step_mixin_available': BASE_STEP_MIXIN_AVAILABLE,
            'step_model_requests_available': STEP_MODEL_REQUESTS_AVAILABLE,
            'dependency_manager_available': hasattr(self, 'dependency_manager') and self.dependency_manager is not None,
            'pipeline_stages': 8,
            'torch_available': TORCH_AVAILABLE,
            'opencv_available': OPENCV_AVAILABLE,
            'pil_available': PIL_AVAILABLE,
            'skimage_available': SKIMAGE_AVAILABLE,
            'sklearn_available': SKLEARN_AVAILABLE
        }
        
        # step_model_requests.py ì •ë³´ ì¶”ê°€
        if self.step_request:
            info.update({
                'step_request_model_name': self.step_request.model_name,
                'step_request_model_architecture': self.step_request.model_architecture,
                'step_request_primary_file': self.step_request.primary_file,
                'step_request_primary_size_mb': self.step_request.primary_size_mb,
                'detailed_spec_available': self.detailed_spec is not None
            })
        
        return info
    
    def get_ai_model_info(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì •ë³´ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
        return {
            'ai_models': list(self.quality_models.keys()) if self.quality_models else [],
            'ai_models_loaded': len(self.quality_models) if self.quality_models else 0,
            'model_architecture': self.model_architecture,
            'primary_model_file': getattr(self.step_request, 'primary_file', None) if self.step_request else None,
            'model_size_mb': getattr(self.step_request, 'primary_size_mb', 0) if self.step_request else 0,
            'device': self.device,
            'memory_fraction': self.memory_fraction,
            'batch_size': self.optimal_batch_size,
            'conda_optimized': getattr(self.step_request, 'conda_optimized', True) if self.step_request else True,
            'mps_acceleration': getattr(self.step_request, 'mps_acceleration', True) if self.step_request else True
        }
    
    async def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (BaseStepMixin í˜¸í™˜)"""
        try:
            # AI ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
            if hasattr(self, 'quality_models') and self.quality_models:
                for model_name, model in self.quality_models.items():
                    if TORCH_AVAILABLE and hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.quality_models.clear()
            
            # ê¸°ìˆ ì  ë¶„ì„ê¸° ì •ë¦¬
            if hasattr(self, 'technical_analyzer') and self.technical_analyzer:
                self.technical_analyzer.cleanup()
            
            # MPS ìºì‹œ ì •ë¦¬
            if self.mps_available:
                safe_mps_empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.model_loaded = False
            self.initialized = False
            
            self.logger.info("âœ… QualityAssessmentStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ QualityAssessmentStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def cleanup(self):
        """cleanup ë³„ì¹­ (í˜¸í™˜ì„±)"""
        await self.cleanup_resources()

    # ==============================================
    # ğŸ”¥ ì¶”ê°€ í•„ìˆ˜ ë©”ì„œë“œë“¤ (ëˆ„ë½ëœ ê¸°ëŠ¥ë“¤)
    # ==============================================
    
    def register_step(self, step_name: str = None, step_config: Dict[str, Any] = None) -> bool:
        """Step ë“±ë¡ (StepFactory í˜¸í™˜)"""
        try:
            step_name = step_name or self.step_name
            step_config = step_config or {}
            
            if not hasattr(self, 'registered_steps'):
                self.registered_steps = {}
            
            self.registered_steps[step_name] = {
                'timestamp': time.time(),
                'config': step_config,
                'status': 'registered',
                'step_id': self.step_id,
                'class_name': self.__class__.__name__
            }
            
            self.logger.info(f"âœ… Step ë“±ë¡ ì™„ë£Œ: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False

    def get_requirements(self) -> Dict[str, Any]:
        """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (step_model_requests.py ê¸°ë°˜)"""
        try:
            requirements = {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'model_requirements': [],
                'device_requirements': {
                    'preferred_device': self.device,
                    'mps_supported': self.mps_available,
                    'm3_max_optimized': self.is_m3_max
                },
                'memory_requirements': {
                    'minimum_gb': 8,
                    'recommended_gb': 16,
                    'optimal_gb': 32 if self.is_m3_max else 16
                },
                'dependencies': {
                    'torch': TORCH_AVAILABLE,
                    'opencv': OPENCV_AVAILABLE,
                    'pil': PIL_AVAILABLE,
                    'skimage': SKIMAGE_AVAILABLE,
                    'sklearn': SKLEARN_AVAILABLE
                }
            }
            
            # step_model_requests.py ìŠ¤í™ ì¶”ê°€
            if self.step_request:
                requirements['model_requirements'] = [
                    {
                        'model_name': self.step_request.model_name,
                        'primary_file': self.step_request.primary_file,
                        'size_mb': self.step_request.primary_size_mb,
                        'architecture': self.step_request.model_architecture,
                        'search_paths': self.step_request.search_paths,
                        'alternative_files': self.step_request.alternative_files
                    }
                ]
                
                if self.detailed_spec:
                    requirements['data_requirements'] = {
                        'input_data_types': self.detailed_spec.input_data_types,
                        'output_data_types': self.detailed_spec.output_data_types,
                        'input_shapes': self.detailed_spec.input_shapes,
                        'output_shapes': self.detailed_spec.output_shapes,
                        'preprocessing_steps': self.detailed_spec.preprocessing_steps,
                        'postprocessing_steps': self.detailed_spec.postprocessing_steps
                    }
            
            return requirements
            
        except Exception as e:
            self.logger.error(f"âŒ ìš”êµ¬ì‚¬í•­ ë°˜í™˜ ì‹¤íŒ¨: {e}")
            return {'step_name': self.step_name, 'step_id': self.step_id}

    def validate_input(self, input_data: Any) -> Tuple[bool, str]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        try:
            if input_data is None:
                return False, "ì…ë ¥ ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤."
            
            # step_model_requests.py ìŠ¤í™ ê¸°ë°˜ ê²€ì¦
            if self.detailed_spec:
                expected_types = self.detailed_spec.input_data_types
                
                if isinstance(input_data, dict):
                    # ë”•ì…”ë„ˆë¦¬ ì…ë ¥ì¸ ê²½ìš°
                    required_keys = ["final_result", "enhanced_image"]
                    if not any(key in input_data for key in required_keys):
                        return False, f"í•„ìˆ˜ í‚¤ ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: {required_keys}"
                
                elif isinstance(input_data, str):
                    # base64 ë¬¸ìì—´ì¸ ê²½ìš°
                    if not input_data.startswith(('data:image', '/9j/', 'iVBOR')):
                        return False, "ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤."
                
                elif isinstance(input_data, np.ndarray):
                    # NumPy ë°°ì—´ì¸ ê²½ìš°
                    if len(input_data.shape) not in [2, 3]:
                        return False, "ì´ë¯¸ì§€ëŠ” 2D ë˜ëŠ” 3D ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                    
                elif hasattr(input_data, 'read'):
                    # íŒŒì¼ ê°ì²´ì¸ ê²½ìš°
                    try:
                        input_data.seek(0)  # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
                    except Exception:
                        return False, "ì½ì„ ìˆ˜ ì—†ëŠ” íŒŒì¼ ê°ì²´ì…ë‹ˆë‹¤."
                
                else:
                    return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(input_data)}"
            
            return True, "ì…ë ¥ ë°ì´í„°ê°€ ìœ íš¨í•©ë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"ì…ë ¥ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def validate_output(self, output_data: Any) -> Tuple[bool, str]:
        """ì¶œë ¥ ë°ì´í„° ê²€ì¦"""
        try:
            if not isinstance(output_data, dict):
                return False, "ì¶œë ¥ ë°ì´í„°ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤."
            
            # step_model_requests.py API ì¶œë ¥ ë§¤í•‘ ê²€ì¦
            if self.detailed_spec and self.detailed_spec.api_output_mapping:
                required_fields = self.detailed_spec.api_output_mapping.keys()
                missing_fields = []
                
                for field in required_fields:
                    if field not in output_data:
                        missing_fields.append(field)
                
                if missing_fields:
                    return False, f"í•„ìˆ˜ ì¶œë ¥ í•„ë“œ ëˆ„ë½: {missing_fields}"
            
            # ê¸°ë³¸ í•„ìˆ˜ í•„ë“œ ê²€ì¦
            basic_required = ['overall_quality', 'confidence']
            missing_basic = [field for field in basic_required if field not in output_data]
            
            if missing_basic:
                return False, f"ê¸°ë³¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_basic}"
            
            # ê°’ ë²”ìœ„ ê²€ì¦
            if 'overall_quality' in output_data:
                quality_score = output_data['overall_quality']
                if not isinstance(quality_score, (int, float)) or not (0 <= quality_score <= 1):
                    return False, "overall_qualityëŠ” 0-1 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤."
            
            if 'confidence' in output_data:
                confidence = output_data['confidence']
                if not isinstance(confidence, (int, float)) or not (0 <= confidence <= 1):
                    return False, "confidenceëŠ” 0-1 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤."
            
            return True, "ì¶œë ¥ ë°ì´í„°ê°€ ìœ íš¨í•©ë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"ì¶œë ¥ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        try:
            metrics = {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'device': self.device,
                'model_loaded': self.model_loaded,
                'initialized': self.initialized,
                'warmup_completed': getattr(self, 'warmup_completed', False),
                'memory_usage': {},
                'processing_stats': {},
                'optimization_stats': {
                    'm3_max_enabled': self.is_m3_max,
                    'mps_available': self.mps_available,
                    'conda_optimized': getattr(self, 'conda_optimized', True)
                }
            }
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
            if PSUTIL_AVAILABLE:
                try:
                    process = psutil.Process()
                    memory_info = process.memory_info()
                    metrics['memory_usage'] = {
                        'rss_mb': memory_info.rss / 1024 / 1024,
                        'vms_mb': memory_info.vms / 1024 / 1024,
                        'percent': process.memory_percent()
                    }
                except Exception:
                    pass
            
            # GPU ë©”ëª¨ë¦¬ (PyTorch MPS ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE and self.mps_available:
                try:
                    if hasattr(torch.mps, 'current_allocated_memory'):
                        metrics['memory_usage']['mps_allocated_mb'] = torch.mps.current_allocated_memory() / 1024 / 1024
                except Exception:
                    pass
            
            # AI ëª¨ë¸ ì •ë³´
            if self.quality_models:
                metrics['model_info'] = {
                    'loaded_models': list(self.quality_models.keys()),
                    'model_count': len(self.quality_models),
                    'model_architecture': self.model_architecture
                }
            
            # step_model_requests.py ìŠ¤í™ ì •ë³´
            if self.step_request:
                metrics['step_request_info'] = {
                    'model_name': self.step_request.model_name,
                    'primary_file': self.step_request.primary_file,
                    'size_mb': self.step_request.primary_size_mb,
                    'detailed_spec_available': self.detailed_spec is not None
                }
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'error': str(e)
            }

    def reset_state(self):
        """ìƒíƒœ ë¦¬ì…‹"""
        try:
            # ìƒíƒœ ë³€ìˆ˜ ë¦¬ì…‹
            self.model_loaded = False
            self.initialized = False
            
            if hasattr(self, 'warmup_completed'):
                self.warmup_completed = False
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'analysis_cache'):
                self.analysis_cache.clear()
            
            if self.technical_analyzer and hasattr(self.technical_analyzer, 'analysis_cache'):
                self.technical_analyzer.analysis_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.mps_available:
                safe_mps_empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… QualityAssessmentStep ìƒíƒœ ë¦¬ì…‹ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        try:
            status = {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'class_name': self.__class__.__name__,
                'initialized': self.initialized,
                'model_loaded': self.model_loaded,
                'warmup_completed': getattr(self, 'warmup_completed', False),
                'device': self.device,
                'device_available': True,
                'memory_efficient': self.is_m3_max,
                'optimization_enabled': getattr(self, 'optimization_enabled', self.is_m3_max),
                'analysis_depth': getattr(self, 'analysis_depth', 'comprehensive'),
                'quality_threshold': getattr(self, 'quality_threshold', 0.8),
                'dependencies': {
                    'model_loader': self.model_loader is not None,
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None,
                    'step_requests_available': STEP_MODEL_REQUESTS_AVAILABLE,
                    'detailed_spec_available': self.detailed_spec is not None
                },
                'capabilities': {
                    'technical_analysis': self.technical_analyzer is not None,
                    'ai_quality_assessment': len(self.quality_models) > 0 if self.quality_models else False,
                    'perceptual_quality': 'perceptual' in (self.quality_models or {}),
                    'aesthetic_quality': 'aesthetic' in (self.quality_models or {}),
                    'comparison_assessment': True,
                    'recommendation_generation': True
                },
                'last_updated': time.time()
            }
            
            # MPS ìƒíƒœ í™•ì¸
            if self.mps_available:
                try:
                    import torch
                    status['mps_status'] = {
                        'available': torch.backends.mps.is_available(),
                        'built': torch.backends.mps.is_built()
                    }
                except Exception:
                    status['mps_status'] = {'available': False, 'built': False}
            
            return status
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ë°˜í™˜ ì‹¤íŒ¨: {e}")
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'error': str(e),
                'status': 'error'
            }

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
# ==============================================
def create_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return QualityAssessmentStep(device=device, config=config, **kwargs)

async def create_and_initialize_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± ë° ì´ˆê¸°í™”"""
    step = QualityAssessmentStep(device=device, config=config, **kwargs)
    await step.initialize()
    return step

def create_quality_assessment_with_checkpoints(
    perceptual_model_path: Optional[str] = None,
    aesthetic_model_path: Optional[str] = None,
    device: str = "auto",
    **kwargs
) -> QualityAssessmentStep:
    """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì§€ì •í•œ í’ˆì§ˆ í‰ê°€ Step ìƒì„±"""
    config = {
        'perceptual_model_path': perceptual_model_path,
        'aesthetic_model_path': aesthetic_model_path,
        'enable_ai_models': True,
        **kwargs.get('config', {})
    }
    
    return QualityAssessmentStep(device=device, config=config, **kwargs)

def create_quality_assessment_with_step_requests(
    device: str = "auto",
    **kwargs
) -> QualityAssessmentStep:
    """step_model_requests.py ì™„ì „ í˜¸í™˜ Step ìƒì„±"""
    if STEP_MODEL_REQUESTS_AVAILABLE:
        step_request = get_enhanced_step_request("QualityAssessmentStep")
        if step_request:
            config = {
                'step_request': step_request,
                'use_detailed_spec': True,
                'enable_ai_models': True,
                **kwargs.get('config', {})
            }
            return QualityAssessmentStep(device=device, config=config, **kwargs)
    
    # í´ë°±
    return create_quality_assessment_step(device=device, **kwargs)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ + step_model_requests.py í˜¸í™˜ ì¶”ê°€)
# ==============================================
__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'QualityAssessmentStep',
    
    # ë°ì´í„° êµ¬ì¡° (step_model_requests.py í˜¸í™˜)
    'QualityMetrics',
    'QualityGrade', 
    'AssessmentMode',
    'QualityAspect',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (step_model_requests.py ìŠ¤í™ ê¸°ë°˜)
    'RealPerceptualQualityModel',
    'RealAestheticQualityModel',
    
    # ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (step_model_requests.py DetailedDataSpec í™œìš©)
    'TechnicalQualityAnalyzer',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ + ìƒˆë¡œìš´)
    'create_quality_assessment_step',
    'create_and_initialize_quality_assessment_step',
    'create_quality_assessment_with_checkpoints',
    'create_quality_assessment_with_step_requests',  # ìƒˆë¡œìš´ í•¨ìˆ˜
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_tensor_to_numpy'
]

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================
if __name__ == "__main__":
    async def test_quality_assessment_step():
        """í’ˆì§ˆ í‰ê°€ Step í…ŒìŠ¤íŠ¸ (step_model_requests.py í˜¸í™˜)"""
        try:
            print("ğŸ§ª QualityAssessmentStep v18.0 í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # Step ìƒì„± (step_model_requests.py í˜¸í™˜)
            step = create_quality_assessment_with_step_requests(device="auto")
            
            # ê¸°ë³¸ ì†ì„± í™•ì¸
            assert hasattr(step, 'logger'), "logger ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'process'), "process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'cleanup_resources'), "cleanup_resources ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'initialize'), "initialize ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            
            # Step ì •ë³´ í™•ì¸
            step_info = step.get_step_info()
            assert 'step_name' in step_info, "step_nameì´ step_infoì— ì—†ìŠµë‹ˆë‹¤!"
            
            # AI ëª¨ë¸ ì •ë³´ í™•ì¸
            ai_model_info = step.get_ai_model_info()
            assert 'ai_models' in ai_model_info, "ai_modelsê°€ ai_model_infoì— ì—†ìŠµë‹ˆë‹¤!"
            
            # step_model_requests.py í˜¸í™˜ì„± í™•ì¸
            if STEP_MODEL_REQUESTS_AVAILABLE:
                assert hasattr(step, 'step_request'), "step_request ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
                assert hasattr(step, 'detailed_spec'), "detailed_spec ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
                
                if step.step_request:
                    assert step.step_request.model_name, "step_request.model_nameì´ ì—†ìŠµë‹ˆë‹¤!"
                    assert step.step_request.data_spec, "step_request.data_specì´ ì—†ìŠµë‹ˆë‹¤!"
            
            print("âœ… QualityAssessmentStep v18.0 í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            print(f"ğŸ“Š Step ì •ë³´: {step_info}")
            print(f"ğŸ§  AI ëª¨ë¸ ì •ë³´: {ai_model_info}")
            print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {step.device}")
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {step_info.get('memory_gb', 0)}GB")
            print(f"ğŸ M3 Max: {'âœ…' if step_info.get('is_m3_max', False) else 'âŒ'}")
            print(f"ğŸ§  BaseStepMixin: {'âœ…' if step_info.get('base_step_mixin_available', False) else 'âŒ'}")
            print(f"ğŸ“‹ step_model_requests.py: {'âœ…' if step_info.get('step_model_requests_available', False) else 'âŒ'}")
            print(f"ğŸ”Œ DependencyManager: {'âœ…' if step_info.get('dependency_manager_available', False) else 'âŒ'}")
            print(f"ğŸ¯ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {step_info.get('pipeline_stages', 0)}")
            print(f"ğŸš€ AI ëª¨ë¸ ë¡œë“œë¨: {ai_model_info.get('ai_models_loaded', 0)}ê°œ")
            print(f"ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
            print(f"   - PyTorch: {'âœ…' if step_info.get('torch_available', False) else 'âŒ'}")
            print(f"   - OpenCV: {'âœ…' if step_info.get('opencv_available', False) else 'âŒ'}")
            print(f"   - PIL: {'âœ…' if step_info.get('pil_available', False) else 'âŒ'}")
            print(f"   - scikit-image: {'âœ…' if step_info.get('skimage_available', False) else 'âŒ'}")
            print(f"   - scikit-learn: {'âœ…' if step_info.get('sklearn_available', False) else 'âŒ'}")
            
            # step_model_requests.py ìŠ¤í™ ì •ë³´
            if STEP_MODEL_REQUESTS_AVAILABLE and step.step_request:
                print(f"ğŸ“‹ step_model_requests.py ìŠ¤í™:")
                print(f"   - ëª¨ë¸ëª…: {step.step_request.model_name}")
                print(f"   - ì•„í‚¤í…ì²˜: {step.step_request.model_architecture}")
                print(f"   - ì£¼ìš” íŒŒì¼: {step.step_request.primary_file}")
                print(f"   - íŒŒì¼ í¬ê¸°: {step.step_request.primary_size_mb}MB")
                print(f"   - DetailedDataSpec: {'âœ…' if step.detailed_spec else 'âŒ'}")
                
                if step.detailed_spec:
                    print(f"   - ì…ë ¥ ë°ì´í„° íƒ€ì…: {len(step.detailed_spec.input_data_types)}ê°œ")
                    print(f"   - ì¶œë ¥ ë°ì´í„° íƒ€ì…: {len(step.detailed_spec.output_data_types)}ê°œ")
                    print(f"   - API ì…ë ¥ ë§¤í•‘: {len(step.detailed_spec.api_input_mapping)}ê°œ")
                    print(f"   - API ì¶œë ¥ ë§¤í•‘: {len(step.detailed_spec.api_output_mapping)}ê°œ")
                    print(f"   - ì „ì²˜ë¦¬ ë‹¨ê³„: {len(step.detailed_spec.preprocessing_steps)}ê°œ")
                    print(f"   - í›„ì²˜ë¦¬ ë‹¨ê³„: {len(step.detailed_spec.postprocessing_steps)}ê°œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ QualityAssessmentStep v18.0 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_quality_assessment_step())