# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - ModelLoader ì™„ì „ ì—°ë™ ë²„ì „
âœ… BaseStepMixin ì™„ì „ ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°
âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™ìœ¼ë¡œ AI ëª¨ë¸ ê°„ì ‘ í˜¸ì¶œ
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (í•œë°©í–¥ ì°¸ì¡°)
âœ… ê¸°ì¡´ íŒŒì¼ì˜ ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ ìœ ì§€
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë° í™œìš©
âœ… Pipeline Manager 100% í˜¸í™˜
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ìœ ì§€

ì²˜ë¦¬ íë¦„:
ğŸŒ API ìš”ì²­ â†’ ğŸ“‹ PipelineManager â†’ ğŸ¯ QualityAssessmentStep ìƒì„±
â†“
ğŸ”— ModelLoader.create_step_interface() â† ModelLoader ë‹´ë‹¹
â”œâ”€ StepModelInterface ìƒì„±
â”œâ”€ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡
â””â”€ 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€
â†“
ğŸš€ QualityAssessmentStep.initialize() â† Step + ModelLoader í˜‘ì—…
â”œâ”€ AI í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ â† ModelLoaderê°€ ì‹¤ì œ ë¡œë“œ
â”œâ”€ ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” â† Step ì²˜ë¦¬
â””â”€ M3 Max ìµœì í™” ì ìš© â† Step ì ìš©
â†“
ğŸ§  ì‹¤ì œ AI ì¶”ë¡  process() â† Step íŒŒì¼ì´ ì£¼ë„
â”œâ”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â† Step ì²˜ë¦¬
â”œâ”€ AI ëª¨ë¸ ì¶”ë¡  (í’ˆì§ˆ í‰ê°€) â† ModelLoaderê°€ ì œê³µí•œ ëª¨ë¸ë¡œ Stepì´ ì¶”ë¡ 
â”œâ”€ 8ê°€ì§€ í’ˆì§ˆ ë¶„ì„ â† Step ì²˜ë¦¬
â””â”€ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° â† Step ì²˜ë¦¬
â†“
ğŸ“¤ ê²°ê³¼ ë°˜í™˜ â† Stepì´ ìµœì¢… ê²°ê³¼ ìƒì„±
â”œâ”€ ì¢…í•© í’ˆì§ˆ ì ìˆ˜
â”œâ”€ ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
â”œâ”€ í’ˆì§ˆ ë“±ê¸‰ ë° ê¶Œì¥ì‚¬í•­
â””â”€ ì‹œê°í™” ì´ë¯¸ì§€
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache, wraps
import numpy as np
import base64
import io

# ==============================================
# ğŸ”¥ í•œë°©í–¥ ì°¸ì¡° êµ¬ì¡° - ìˆœí™˜ì°¸ì¡° í•´ê²°
# ==============================================

# 1. BaseStepMixin ë° QualityAssessmentMixin ì„í¬íŠ¸ (í•µì‹¬)
try:
    from .base_step_mixin import BaseStepMixin, QualityAssessmentMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… BaseStepMixin ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ BaseStepMixin ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# 2. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„í¬íŠ¸ (í•µì‹¬)
try:
    from ..utils.model_loader import ModelLoader, get_global_model_loader
    MODEL_LOADER_AVAILABLE = True
    logger.info("âœ… ModelLoader ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logger.warning(f"âš ï¸ ModelLoader ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# 3. ì„¤ì • ë° ì½”ì–´ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from ...core.config import MODEL_CONFIG
    from ...core.gpu_config import GPUConfig
    from ...core.m3_optimizer import M3MaxOptimizer
    CORE_AVAILABLE = True
except ImportError as e:
    CORE_AVAILABLE = False
    logger.warning(f"âš ï¸ Core ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# 4. ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    import cv2
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# ==============================================
# ğŸ”¥ MRO ì•ˆì „í•œ í´ë°± í´ë˜ìŠ¤ë“¤ (import ì‹¤íŒ¨ ì‹œ)
# ==============================================

if not BASE_STEP_MIXIN_AVAILABLE:
    class BaseStepMixin:
        """MRO ì•ˆì „í•œ í´ë°± BaseStepMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()  # MRO ì•ˆì „
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.step_name = getattr(self, 'step_name', 'quality_assessment')
            self.step_number = 8
            self.device = 'cpu'
            self.is_initialized = False
    
    class QualityAssessmentMixin(BaseStepMixin):
        """MRO ì•ˆì „í•œ í´ë°± QualityAssessmentMixin"""
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.step_type = "quality_assessment"
            self.quality_threshold = 0.7

# ==============================================
# ğŸ”¥ í’ˆì§ˆ í‰ê°€ ë°ì´í„° êµ¬ì¡°ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    FAILED = "failed"

class AssessmentMode(Enum):
    """í‰ê°€ ëª¨ë“œ"""
    COMPREHENSIVE = "comprehensive"
    FAST = "fast"
    DETAILED = "detailed"
    CUSTOM = "custom"

class QualityAspect(Enum):
    """í’ˆì§ˆ í‰ê°€ ì˜ì—­"""
    SHARPNESS = "sharpness"
    COLOR = "color"
    FITTING = "fitting"
    REALISM = "realism"
    ARTIFACTS = "artifacts"
    ALIGNMENT = "alignment"
    LIGHTING = "lighting"
    TEXTURE = "texture"

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° êµ¬ì¡°"""
    overall_score: float = 0.0
    confidence: float = 0.0
    
    # ì„¸ë¶€ ì ìˆ˜ë“¤
    sharpness_score: float = 0.0
    color_score: float = 0.0
    fitting_score: float = 0.0
    realism_score: float = 0.0
    artifacts_score: float = 0.0
    alignment_score: float = 0.0
    lighting_score: float = 0.0
    texture_score: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    processing_time: float = 0.0
    device_used: str = "cpu"
    model_version: str = "v1.0"
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
# ==============================================

if TORCH_AVAILABLE:
    class EnhancedPerceptualQualityModel(nn.Module):
        """í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (ResNet ë°±ë³¸)"""
        
        def __init__(self):
            super().__init__()
            
            # ResNet ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œê¸°
            self.feature_extractor = nn.Sequential(
                # Block 1
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                # Block 2
                nn.Conv2d(64, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.MaxPool2d(2),
                
                # Block 3
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((8, 8))
            )
            
            # ë‹¤ì¤‘ í’ˆì§ˆ ì˜ˆì¸¡ê¸°
            feature_dim = 256 * 8 * 8
            self.quality_predictors = nn.ModuleDict({
                'overall': nn.Sequential(
                    nn.Linear(feature_dim, 512),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(512, 128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                ),
                'sharpness': nn.Sequential(
                    nn.Linear(feature_dim, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'artifacts': nn.Sequential(
                    nn.Linear(feature_dim, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'noise': nn.Sequential(
                    nn.Linear(feature_dim, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                )
            })
        
        def forward(self, x):
            features = self.feature_extractor(x)
            features = features.view(features.size(0), -1)
            
            results = {}
            for name, predictor in self.quality_predictors.items():
                results[name] = predictor(features)
            
            return results

    class EnhancedAestheticQualityModel(nn.Module):
        """í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (ë‹¤ì¤‘ í—¤ë“œ)"""
        
        def __init__(self):
            super().__init__()
            
            # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                self._make_layer(64, 64, 2),
                self._make_layer(64, 128, 2, stride=2),
                self._make_layer(128, 256, 2, stride=2),
                self._make_layer(256, 512, 2, stride=2),
                
                nn.AdaptiveAvgPool2d((1, 1))
            )
            
            # ë¯¸ì  ì ìˆ˜ ì˜ˆì¸¡ í—¤ë“œë“¤
            self.aesthetic_heads = nn.ModuleDict({
                'composition': nn.Sequential(
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'color_harmony': nn.Sequential(
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'lighting': nn.Sequential(
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'symmetry': nn.Sequential(
                    nn.Linear(512, 128),
                    nn.ReLU(),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                ),
                'balance': nn.Sequential(
                    nn.Linear(512, 128),
                    nn.ReLU(),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                )
            })
        
        def _make_layer(self, in_channels, out_channels, blocks, stride=1):
            """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
            layers = []
            layers.append(nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU())
            
            for _ in range(1, blocks):
                layers.append(nn.Conv2d(out_channels, out_channels, 3, padding=1))
                layers.append(nn.BatchNorm2d(out_channels))
                layers.append(nn.ReLU())
            
            return nn.Sequential(*layers)
        
        def forward(self, x):
            features = self.backbone(x)
            features = features.view(features.size(0), -1)
            
            results = {}
            for name, head in self.aesthetic_heads.items():
                results[name] = head(features)
            
            return results

else:
    # PyTorch ì—†ì„ ë•Œ ë”ë¯¸ í´ë˜ìŠ¤
    class EnhancedPerceptualQualityModel:
        def __init__(self):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ EnhancedPerceptualQualityModel")
        
        def predict(self, x):
            return {'overall': 0.7, 'sharpness': 0.8, 'artifacts': 0.9, 'noise': 0.8}
    
    class EnhancedAestheticQualityModel:
        def __init__(self):
            self.logger = logging.getLogger(__name__)
            self.logger.warning("PyTorch ì—†ìŒ - ë”ë¯¸ EnhancedAestheticQualityModel")
        
        def predict(self, x):
            return {'composition': 0.7, 'color_harmony': 0.8, 'lighting': 0.75, 'symmetry': 0.7, 'balance': 0.8}

# ==============================================
# ğŸ”¥ ì „ë¬¸ ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ì™„ì „ ì´ì‹)
# ==============================================

class TechnicalQualityAnalyzer:
    """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸° (í–¥ìƒëœ ë²„ì „)"""
    
    def __init__(self, device: str = "cpu", enable_gpu: bool = False):
        self.device = device
        self.enable_gpu = enable_gpu
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
        
        # ë¶„ì„ ìºì‹œ
        self.analysis_cache = {}
        
        # ê¸°ìˆ ì  ë¶„ì„ ì„ê³„ê°’ë“¤
        self.thresholds = {
            'sharpness_min': 100.0,
            'noise_max': 50.0,
            'contrast_min': 20.0,
            'brightness_range': (50, 200)
        }
    
    def analyze(self, image: np.ndarray) -> Dict[str, Any]:
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            results = {}
            
            # 1. ì„ ëª…ë„ ë¶„ì„
            results['sharpness'] = self._analyze_sharpness(image)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„
            results['noise_level'] = self._analyze_noise_level(image)
            
            # 3. ëŒ€ë¹„ ë¶„ì„
            results['contrast'] = self._analyze_contrast(image)
            
            # 4. ë°ê¸° ë¶„ì„
            results['brightness'] = self._analyze_brightness(image)
            
            # 5. í¬í™”ë„ ë¶„ì„
            results['saturation'] = self._analyze_saturation(image)
            
            # 6. ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            results['artifacts'] = self._detect_artifacts(image)
            
            # 7. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall_score'] = self._calculate_technical_score(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_technical_results()
    
    def _analyze_sharpness(self, image: np.ndarray) -> float:
        """ì„ ëª…ë„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if 'cv2' in globals() else np.mean(image, axis=2)
            else:
                gray = image
            
            if 'cv2' in globals():
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                sharpness = laplacian.var()
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜ ì„ ëª…ë„
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                sharpness = np.var(dx) + np.var(dy)
            
            # ì •ê·œí™” (0-1)
            normalized_sharpness = min(1.0, sharpness / 10000.0)
            return max(0.0, normalized_sharpness)
            
        except Exception as e:
            self.logger.error(f"ì„ ëª…ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                # ê° ì±„ë„ë³„ ë…¸ì´ì¦ˆ ë¶„ì„
                noise_levels = []
                for channel in range(3):
                    channel_data = image[:, :, channel]
                    # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
                    if 'cv2' in globals():
                        blur = cv2.GaussianBlur(channel_data.astype(np.uint8), (5, 5), 0)
                        noise = np.abs(channel_data.astype(float) - blur.astype(float))
                    else:
                        # ê°„ë‹¨í•œ ì´ë™í‰ê·  ê¸°ë°˜
                        kernel = np.ones((3, 3)) / 9
                        blur = np.convolve(channel_data.flatten(), kernel.flatten(), mode='same').reshape(channel_data.shape)
                        noise = np.abs(channel_data.astype(float) - blur)
                    
                    noise_level = np.mean(noise) / 255.0
                    noise_levels.append(noise_level)
                
                # í‰ê·  ë…¸ì´ì¦ˆ ë ˆë²¨
                avg_noise = np.mean(noise_levels)
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                if 'cv2' in globals():
                    blur = cv2.GaussianBlur(image.astype(np.uint8), (5, 5), 0)
                    noise = np.abs(image.astype(float) - blur.astype(float))
                else:
                    noise = np.std(image) / 255.0
                avg_noise = np.mean(noise) / 255.0 if 'cv2' in globals() else noise
            
            # ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ìŒ (ì—­ìˆœ)
            return max(0.0, min(1.0, 1.0 - avg_noise * 5))
            
        except Exception as e:
            self.logger.error(f"ë…¸ì´ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_contrast(self, image: np.ndarray) -> float:
        """ëŒ€ë¹„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # RMS ëŒ€ë¹„ ê³„ì‚°
            contrast = np.std(gray)
            
            # ì •ê·œí™” (ì ì ˆí•œ ëŒ€ë¹„ ë²”ìœ„: 30-80)
            if 30 <= contrast <= 80:
                contrast_score = 1.0
            elif contrast < 30:
                contrast_score = contrast / 30.0
            else:
                contrast_score = max(0.3, 1.0 - (contrast - 80) / 100.0)
            
            return max(0.0, min(1.0, contrast_score))
            
        except Exception as e:
            self.logger.error(f"ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_brightness(self, image: np.ndarray) -> float:
        """ë°ê¸° ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                brightness = np.mean(image)
            else:
                brightness = np.mean(image)
            
            # ì ì ˆí•œ ë°ê¸° ë²”ìœ„ (100-160)
            if 100 <= brightness <= 160:
                brightness_score = 1.0
            elif brightness < 100:
                brightness_score = brightness / 100.0
            else:
                brightness_score = max(0.3, 1.0 - (brightness - 160) / 95.0)
            
            return max(0.0, min(1.0, brightness_score))
            
        except Exception as e:
            self.logger.error(f"ë°ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_saturation(self, image: np.ndarray) -> float:
        """í¬í™”ë„ ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # HSV ë³€í™˜ ë° í¬í™”ë„ ë¶„ì„
            if 'cv2' in globals():
                hsv = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2HSV)
                saturation = np.mean(hsv[:, :, 1])
            else:
                # RGB ê¸°ë°˜ í¬í™”ë„ ê·¼ì‚¬
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = np.mean((max_vals - min_vals) / (max_vals + 1e-8)) * 255
            
            # ì ì ˆí•œ í¬í™”ë„ ë²”ìœ„ (80-180)
            if 80 <= saturation <= 180:
                saturation_score = 1.0
            elif saturation < 80:
                saturation_score = saturation / 80.0
            else:
                saturation_score = max(0.3, 1.0 - (saturation - 180) / 75.0)
            
            return max(0.0, min(1.0, saturation_score))
            
        except Exception as e:
            self.logger.error(f"í¬í™”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _detect_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            artifact_score = 1.0  # ê¸°ë³¸ê°’: ì•„í‹°íŒ©íŠ¸ ì—†ìŒ
            
            # 1. ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            blocking_score = self._detect_blocking_artifacts(image)
            
            # 2. ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            ringing_score = self._detect_ringing_artifacts(image)
            
            # 3. ì••ì¶• ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            compression_score = self._detect_compression_artifacts(image)
            
            # ì¢…í•© ì•„í‹°íŒ©íŠ¸ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì•„í‹°íŒ©íŠ¸ ì ìŒ)
            artifact_score = np.mean([blocking_score, ringing_score, compression_score])
            
            return max(0.0, min(1.0, artifact_score))
            
        except Exception as e:
            self.logger.error(f"ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _detect_blocking_artifacts(self, image: np.ndarray) -> float:
        """ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # 8x8 ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ë¶ˆì—°ì†ì„± ê²€ì‚¬
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            h, w = gray.shape
            blocking_score = 1.0
            
            # ìˆ˜ì§/ìˆ˜í‰ ë¸”ë¡ ê²½ê³„ ê²€ì‚¬
            for i in range(8, h-8, 8):
                diff = np.mean(np.abs(gray[i, :] - gray[i-1, :]))
                if diff > 10:  # ì„ê³„ê°’
                    blocking_score -= 0.1
            
            for j in range(8, w-8, 8):
                diff = np.mean(np.abs(gray[:, j] - gray[:, j-1]))
                if diff > 10:  # ì„ê³„ê°’
                    blocking_score -= 0.1
            
            return max(0.0, blocking_score)
            
        except Exception as e:
            return 0.9
    
    def _detect_ringing_artifacts(self, image: np.ndarray) -> float:
        """ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ì—ì§€ ê·¼ì²˜ì˜ ì§„ë™ íŒ¨í„´ ê²€ì¶œ
            if 'cv2' in globals() and len(image.shape) == 3:
                gray = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # ì—ì§€ ê·¼ì²˜ì˜ ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
                kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
                enhanced = cv2.filter2D(gray, -1, kernel)
                
                # ë§ì‰ ì •ë„ ê³„ì‚°
                ringing_metric = np.mean(np.abs(enhanced - gray))
                ringing_score = max(0.0, 1.0 - ringing_metric / 50.0)
            else:
                ringing_score = 0.9  # ê¸°ë³¸ê°’
            
            return ringing_score
            
        except Exception as e:
            return 0.9
    
    def _detect_compression_artifacts(self, image: np.ndarray) -> float:
        """ì••ì¶• ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # DCT ê³„ìˆ˜ ë¶„ì„ ê¸°ë°˜ ì••ì¶• ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ì†ì‹¤ ì •ë„ ë¶„ì„
            if 'cv2' in globals():
                # ë¼í”Œë¼ì‹œì•ˆ í•„í„°ë¡œ ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                high_freq_energy = np.var(laplacian)
                
                # ì••ì¶•ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ì˜ ì˜ˆìƒ ê³ ì£¼íŒŒ ì—ë„ˆì§€ì™€ ë¹„êµ
                expected_energy = np.var(gray) * 0.1  # ì¶”ì •ê°’
                compression_score = min(1.0, high_freq_energy / (expected_energy + 1e-8))
            else:
                compression_score = 0.8  # ê¸°ë³¸ê°’
            
            return max(0.0, compression_score)
            
        except Exception as e:
            return 0.8
    
    def _calculate_technical_score(self, results: Dict[str, Any]) -> float:
        """ê¸°ìˆ ì  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'sharpness': 0.25,
                'noise_level': 0.20,
                'contrast': 0.15,
                'brightness': 0.15,
                'saturation': 0.10,
                'artifacts': 0.15
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            # ì •ê·œí™”
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _get_fallback_technical_results(self) -> Dict[str, Any]:
        """í´ë°± ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼"""
        return {
            'sharpness': 0.5,
            'noise_level': 0.6,
            'contrast': 0.5,
            'brightness': 0.6,
            'saturation': 0.5,
            'artifacts': 0.7,
            'overall_score': 0.55,
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """ë¶„ì„ê¸° ì •ë¦¬"""
        self.analysis_cache.clear()

class FittingQualityAnalyzer:
    """ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ê¸° (í”¼íŒ… ì „ë¬¸)"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.FittingQualityAnalyzer")
        
        # ì˜ë¥˜ íƒ€ì…ë³„ ê¸°ëŒ€ ë¹„ìœ¨
        self.clothing_ratios = {
            'shirt': {'aspect_ratio': (0.7, 1.3), 'coverage': (0.3, 0.7)},
            'dress': {'aspect_ratio': (0.5, 1.0), 'coverage': (0.5, 0.9)},
            'pants': {'aspect_ratio': (0.8, 1.2), 'coverage': (0.4, 0.8)},
            'jacket': {'aspect_ratio': (0.6, 1.2), 'coverage': (0.4, 0.8)},
            'skirt': {'aspect_ratio': (0.8, 1.4), 'coverage': (0.2, 0.6)},
            'top': {'aspect_ratio': (0.7, 1.4), 'coverage': (0.2, 0.6)},
            'default': {'aspect_ratio': (0.5, 1.5), 'coverage': (0.2, 0.9)}
        }
    
    def analyze(self, image: np.ndarray, clothing_type: str = "default") -> Dict[str, Any]:
        """ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰"""
        try:
            results = {}
            
            # 1. í”¼íŒ… ì •í™•ë„ ë¶„ì„
            results['fitting_accuracy'] = self._analyze_fitting_accuracy(image, clothing_type)
            
            # 2. ì˜ë¥˜ ì •ë ¬ ë¶„ì„
            results['clothing_alignment'] = self._analyze_clothing_alignment(image)
            
            # 3. ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„
            results['naturalness'] = self._analyze_naturalness(image)
            
            # 4. ì—ì§€ ë³´ì¡´ë„ ë¶„ì„
            results['edge_preservation'] = self._analyze_edge_preservation(image)
            
            # 5. í…ìŠ¤ì²˜ í’ˆì§ˆ ë¶„ì„
            results['texture_quality'] = self._analyze_texture_quality(image)
            
            # 6. ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ë„ ë¶„ì„
            results['detail_preservation'] = self._analyze_detail_preservation(image)
            
            # 7. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall_score'] = self._calculate_fitting_score(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ëŠ¥ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_fitting_results()
    
    def _analyze_fitting_accuracy(self, image: np.ndarray, clothing_type: str) -> float:
        """í”¼íŒ… ì •í™•ë„ ë¶„ì„"""
        try:
            height, width = image.shape[:2]
            aspect_ratio = width / height
            
            # ì˜ë¥˜ íƒ€ì…ë³„ ê¸°ëŒ€ ë¹„ìœ¨
            expected = self.clothing_ratios.get(clothing_type, self.clothing_ratios['default'])
            min_ratio, max_ratio = expected['aspect_ratio']
            
            # ë¹„ìœ¨ ì ìˆ˜ ê³„ì‚°
            if min_ratio <= aspect_ratio <= max_ratio:
                ratio_score = 1.0
            else:
                center_ratio = (min_ratio + max_ratio) / 2
                ratio_score = max(0.0, 1.0 - abs(aspect_ratio - center_ratio) * 2)
            
            # ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (ì˜ë¥˜ê°€ ì°¨ì§€í•˜ëŠ” ì˜ì—­)
            coverage_score = self._analyze_clothing_coverage(image, expected['coverage'])
            
            # ì¢…í•© í”¼íŒ… ì •í™•ë„
            fitting_accuracy = (ratio_score * 0.6 + coverage_score * 0.4)
            
            return max(0.0, min(1.0, fitting_accuracy))
            
        except Exception as e:
            self.logger.error(f"í”¼íŒ… ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_clothing_coverage(self, image: np.ndarray, expected_coverage: Tuple[float, float]) -> float:
        """ì˜ë¥˜ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        try:
            # ì˜ë¥˜ ì˜ì—­ ì¶”ì • (ìƒ‰ìƒ ê¸°ë°˜ ê°„ë‹¨í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜)
            if len(image.shape) == 3:
                # ë°°ê²½ê³¼ ì „ê²½ êµ¬ë¶„ (ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜)
                gray = np.mean(image, axis=2)
                
                # Otsu's ë°©ë²• ê·¼ì‚¬
                if 'cv2' in globals():
                    _, binary = cv2.threshold(gray.astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                    clothing_pixels = np.sum(binary > 0)
                else:
                    # ê°„ë‹¨í•œ ì„ê³„ê°’
                    threshold = np.mean(gray)
                    clothing_pixels = np.sum(gray > threshold)
                
                total_pixels = gray.size
                coverage_ratio = clothing_pixels / total_pixels
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì˜ ê²½ìš°
                threshold = np.mean(image)
                coverage_ratio = np.sum(image > threshold) / image.size
            
            # ê¸°ëŒ€ ì»¤ë²„ë¦¬ì§€ì™€ ë¹„êµ
            min_coverage, max_coverage = expected_coverage
            
            if min_coverage <= coverage_ratio <= max_coverage:
                coverage_score = 1.0
            else:
                center_coverage = (min_coverage + max_coverage) / 2
                coverage_score = max(0.0, 1.0 - abs(coverage_ratio - center_coverage) * 3)
            
            return coverage_score
            
        except Exception as e:
            self.logger.error(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_clothing_alignment(self, image: np.ndarray) -> float:
        """ì˜ë¥˜ ì •ë ¬ ë¶„ì„"""
        try:
            height, width = image.shape[:2]
            
            # ì¢Œìš° ëŒ€ì¹­ì„± ë¶„ì„
            left_half = image[:, :width//2]
            right_half = image[:, width//2:]
            right_half_flipped = np.flip(right_half, axis=1)
            
            # í¬ê¸° ë§ì¶”ê¸°
            min_width = min(left_half.shape[1], right_half_flipped.shape[1])
            left_half = left_half[:, :min_width]
            right_half_flipped = right_half_flipped[:, :min_width]
            
            # ì°¨ì´ ê³„ì‚°
            diff = np.mean(np.abs(left_half.astype(float) - right_half_flipped.astype(float)))
            symmetry_score = max(0.0, 1.0 - diff / 128.0)  # ì •ê·œí™”
            
            # ìˆ˜ì§ ì •ë ¬ ë¶„ì„ (ì¤‘ì‹¬ì„  ê¸°ì¤€)
            center_col = width // 2
            center_line = image[:, max(0, center_col-2):min(width, center_col+3)]
            
            # ì¤‘ì‹¬ì„ ì˜ ì¼ê´€ì„± ë¶„ì„
            if center_line.size > 0:
                vertical_consistency = 1.0 - np.std(np.mean(center_line, axis=1)) / 255.0
                vertical_consistency = max(0.0, min(1.0, vertical_consistency))
            else:
                vertical_consistency = 0.5
            
            # ì¢…í•© ì •ë ¬ ì ìˆ˜
            alignment_score = (symmetry_score * 0.7 + vertical_consistency * 0.3)
            
            return max(0.0, min(1.0, alignment_score))
            
        except Exception as e:
            self.logger.error(f"ì˜ë¥˜ ì •ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_naturalness(self, image: np.ndarray) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„"""
        try:
            # ìƒ‰ìƒ ë¶„í¬ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ì²´í¬
            if len(image.shape) == 3:
                # RGB ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
                color_variance = np.mean([np.var(image[:,:,i]) for i in range(3)])
                color_naturalness = min(1.0, color_variance / (255.0 * 255.0) * 10)
            else:
                color_naturalness = 0.7
            
            # ê²½ê³„ì„ ì˜ ë¶€ë“œëŸ¬ì›€ ì²´í¬
            if 'cv2' in globals():
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
                edges = cv2.Canny(gray.astype(np.uint8), 50, 150)
                edge_density = np.sum(edges > 0) / edges.size
                smoothness = max(0.0, 1.0 - edge_density * 5)
            else:
                smoothness = 0.7
            
            naturalness = (color_naturalness + smoothness) / 2
            return max(0.0, min(1.0, naturalness))
        
        except Exception as e:
            self.logger.error(f"ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_edge_preservation(self, image: np.ndarray) -> float:
        """ì—ì§€ ë³´ì¡´ë„ ë¶„ì„"""
        try:
            # ì—ì§€ ê²€ì¶œ ë° í’ˆì§ˆ í‰ê°€
            if 'cv2' in globals():
                if len(image.shape) == 3:
                    gray = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2GRAY)
                else:
                    gray = image.astype(np.uint8)
                
                # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì—ì§€ ê²€ì¶œ
                edges_fine = cv2.Canny(gray, 50, 150)
                edges_coarse = cv2.Canny(gray, 100, 200)
                
                # ì—ì§€ì˜ ì„ ëª…ë„ ë° ì—°ì†ì„± í‰ê°€
                edge_strength = np.mean(edges_fine)
                edge_consistency = 1.0 - abs(np.mean(edges_fine) - np.mean(edges_coarse)) / 255.0
                
                preservation_score = (edge_strength / 255.0) * 0.6 + edge_consistency * 0.4
            else:
                # ê°„ë‹¨í•œ gradient ê¸°ë°˜
                if len(image.shape) == 3:
                    gray = np.mean(image, axis=2)
                else:
                    gray = image
                
                dx = np.abs(np.diff(gray, axis=1))
                dy = np.abs(np.diff(gray, axis=0))
                edge_strength = (np.mean(dx) + np.mean(dy)) / 2
                
                preservation_score = min(1.0, edge_strength / 30.0)
            
            return max(0.0, min(1.0, preservation_score))
            
        except Exception as e:
            self.logger.error(f"ì—ì§€ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_texture_quality(self, image: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ í’ˆì§ˆ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ì§€ì—­ ë°”ì´ë„ˆë¦¬ íŒ¨í„´ (LBP) ê·¼ì‚¬
            texture_features = []
            
            # ë‹¤ì–‘í•œ ë°©í–¥ì˜ í…ìŠ¤ì²˜ ë¶„ì„
            for dx, dy in [(1, 0), (0, 1), (1, 1), (-1, 1)]:
                if dx == 1 and dy == 0:  # ìˆ˜í‰
                    diff = np.abs(np.diff(gray, axis=1))
                elif dx == 0 and dy == 1:  # ìˆ˜ì§
                    diff = np.abs(np.diff(gray, axis=0))
                else:  # ëŒ€ê°ì„  (ê°„ë‹¨ ê·¼ì‚¬)
                    diff = np.abs(gray[1:, 1:] - gray[:-1, :-1])
                
                texture_strength = np.mean(diff)
                texture_features.append(texture_strength)
            
            # í…ìŠ¤ì²˜ ë³µì¡ë„
            texture_complexity = np.mean(texture_features)
            
            # ì ì ˆí•œ í…ìŠ¤ì²˜ ë³µì¡ë„ (5-25)
            if 5 <= texture_complexity <= 25:
                texture_score = 1.0
            else:
                texture_score = max(0.0, 1.0 - abs(texture_complexity - 15) / 20.0)
            
            return max(0.0, min(1.0, texture_score))
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤ì²˜ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_detail_preservation(self, image: np.ndarray) -> float:
        """ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ë„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
            if 'cv2' in globals():
                # ë¼í”Œë¼ì‹œì•ˆ í•„í„°ë¡œ ì„¸ë¶€ì‚¬í•­ ì¶”ì¶œ
                laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                detail_strength = np.var(laplacian)
                
                # ê°€ìš°ì‹œì•ˆ í”¼ë¼ë¯¸ë“œë¡œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¶„ì„
                pyramid_levels = []
                current = gray.astype(np.uint8)
                for _ in range(3):
                    current = cv2.pyrDown(current)
                    pyramid_levels.append(np.var(current))
                
                detail_preservation = detail_strength / (1 + np.mean(pyramid_levels))
                preservation_score = min(1.0, detail_preservation / 1000.0)
            else:
                # ê°„ë‹¨í•œ ë¶„ì‚° ê¸°ë°˜ ì„¸ë¶€ì‚¬í•­ ë¶„ì„
                detail_variance = np.var(gray)
                preservation_score = min(1.0, detail_variance / 2000.0)
            
            return max(0.0, min(1.0, preservation_score))
            
        except Exception as e:
            self.logger.error(f"ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_fitting_score(self, results: Dict[str, Any]) -> float:
        """ê¸°ëŠ¥ì  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'fitting_accuracy': 0.25,
                'clothing_alignment': 0.20,
                'naturalness': 0.20,
                'edge_preservation': 0.15,
                'texture_quality': 0.10,
                'detail_preservation': 0.10
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            # ì •ê·œí™”
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ê¸°ëŠ¥ì  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _get_fallback_fitting_results(self) -> Dict[str, Any]:
        """í´ë°± ê¸°ëŠ¥ì  ë¶„ì„ ê²°ê³¼"""
        return {
            'fitting_accuracy': 0.6,
            'clothing_alignment': 0.6,
            'naturalness': 0.5,
            'edge_preservation': 0.6,
            'texture_quality': 0.5,
            'detail_preservation': 0.6,
            'overall_score': 0.57,
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """ë¶„ì„ê¸° ì •ë¦¬"""
        pass

class ColorQualityAnalyzer:
    """ìƒ‰ìƒ í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.ColorQualityAnalyzer")
    
    def analyze(self, image: np.ndarray) -> Dict[str, Any]:
        """ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰"""
        try:
            results = {}
            
            # 1. ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„
            results['color_consistency'] = self._analyze_color_consistency(image)
            
            # 2. ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„
            results['color_naturalness'] = self._analyze_color_naturalness(image)
            
            # 3. ìƒ‰ìƒ ëŒ€ë¹„ ë¶„ì„
            results['color_contrast'] = self._analyze_color_contrast(image)
            
            # 4. ìƒ‰ìƒ ì¡°í™” ë¶„ì„
            results['color_harmony'] = self._analyze_color_harmony(image)
            
            # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            results['overall_score'] = self._calculate_color_score(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_color_results()
    
    def _analyze_color_consistency(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # ì˜ì—­ë³„ ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
            height, width = image.shape[:2]
            regions = [
                image[:height//2, :width//2],    # ì¢Œìƒ
                image[:height//2, width//2:],    # ìš°ìƒ
                image[height//2:, :width//2],    # ì¢Œí•˜
                image[height//2:, width//2:]     # ìš°í•˜
            ]
            
            # ê° ì˜ì—­ì˜ í‰ê·  ìƒ‰ìƒ
            region_colors = [np.mean(region, axis=(0,1)) for region in regions]
            
            # ìƒ‰ìƒ ê°„ í¸ì°¨ ê³„ì‚°
            color_std = np.std(region_colors, axis=0)
            consistency = 1.0 - np.mean(color_std) / 255.0
            
            return max(0.0, min(1.0, consistency))
        
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_color_naturalness(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # ìƒ‰ìƒ í¬í™”ë„ ë¶„ì„
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV) if 'cv2' in globals() else image
            
            if 'cv2' in globals():
                saturation = hsv[:,:,1]
                avg_saturation = np.mean(saturation) / 255.0
                
                # ì ì ˆí•œ í¬í™”ë„ ë²”ìœ„ (0.3-0.8)
                if 0.3 <= avg_saturation <= 0.8:
                    saturation_score = 1.0
                else:
                    saturation_score = max(0.0, 1.0 - abs(avg_saturation - 0.55) * 2)
            else:
                # OpenCV ì—†ì„ ë•Œ RGB ê¸°ë°˜ ê·¼ì‚¬
                max_vals = np.max(image, axis=2)
                min_vals = np.min(image, axis=2)
                saturation = (max_vals - min_vals) / (max_vals + 1e-8)
                avg_saturation = np.mean(saturation)
                saturation_score = min(1.0, avg_saturation * 1.5)
            
            return saturation_score
        
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_color_contrast(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ëŒ€ë¹„ ë¶„ì„"""
        try:
            if len(image.shape) == 3:
                # ë°ê¸° ì±„ë„ ì¶”ì¶œ
                brightness = np.mean(image, axis=2)
            else:
                brightness = image
            
            # ëŒ€ë¹„ ê³„ì‚° (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
            contrast = np.std(brightness) / 255.0
            
            # ì ì ˆí•œ ëŒ€ë¹„ ë²”ìœ„ (0.2-0.6)
            if 0.2 <= contrast <= 0.6:
                contrast_score = 1.0
            else:
                contrast_score = max(0.0, 1.0 - abs(contrast - 0.4) * 2.5)
            
            return contrast_score
        
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_color_harmony(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¡°í™” ë¶„ì„"""
        try:
            if len(image.shape) != 3:
                return 0.5
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            if SKLEARN_AVAILABLE:
                pixels = image.reshape(-1, 3)
                
                # ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”)
                if len(pixels) > 10000:
                    indices = np.random.choice(len(pixels), 10000, replace=False)
                    pixels = pixels[indices]
                
                # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
                try:
                    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                    kmeans.fit(pixels)
                    
                    centers = kmeans.cluster_centers_
                    
                    # ìƒ‰ìƒ ê°„ ê±°ë¦¬ ë¶„ì„
                    color_distances = []
                    for i in range(len(centers)):
                        for j in range(i+1, len(centers)):
                            distance = np.linalg.norm(centers[i] - centers[j])
                            color_distances.append(distance)
                    
                    # ì ì ˆí•œ ìƒ‰ìƒ ë‹¤ì–‘ì„±
                    avg_distance = np.mean(color_distances)
                    harmony_score = min(1.0, avg_distance / 100.0)
                    
                except Exception:
                    harmony_score = 0.6
            else:
                # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ì‚° ê¸°ë°˜
                color_std = np.std(image, axis=(0,1))
                harmony_score = min(1.0, np.mean(color_std) / 64.0)
            
            return max(0.0, min(1.0, harmony_score))
        
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì¡°í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_color_score(self, results: Dict[str, Any]) -> float:
        """ìƒ‰ìƒ í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'color_consistency': 0.3,
                'color_naturalness': 0.3,
                'color_contrast': 0.2,
                'color_harmony': 0.2
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in results:
                    total_score += results[metric] * weight
                    total_weight += weight
            
            # ì •ê·œí™”
            if total_weight > 0:
                final_score = total_score / total_weight
            else:
                final_score = 0.5
            
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _get_fallback_color_results(self) -> Dict[str, Any]:
        """í´ë°± ìƒ‰ìƒ ë¶„ì„ ê²°ê³¼"""
        return {
            'color_consistency': 0.6,
            'color_naturalness': 0.6,
            'color_contrast': 0.5,
            'color_harmony': 0.6,
            'overall_score': 0.58,
            'analysis_method': 'fallback'
        }
    
    def cleanup(self):
        """ë¶„ì„ê¸° ì •ë¦¬"""
        pass

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤
# ==============================================

class QualityAssessmentStep(QualityAssessmentMixin):
    """
    ğŸ”¥ 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ Step - ModelLoader ì—°ë™ ì™„ì „ ë²„ì „
    âœ… QualityAssessmentMixin ìƒì†ìœ¼ë¡œ logger ë³´ì¥
    âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ í†µí•œ AI ëª¨ë¸ í˜¸ì¶œ (í•µì‹¬)
    âœ… ê¸°ì¡´ ëª¨ë“  ë¶„ì„ ê¸°ëŠ¥ ìœ ì§€
    âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ í™œìš©
    âœ… M3 Max ìµœì í™”
    """
    
    def __init__(
        self,
        device: str = "auto",
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """í’ˆì§ˆ í‰ê°€ Step ì´ˆê¸°í™”"""
        
        # ğŸ”¥ MRO ì•ˆì „í•œ ì´ˆê¸°í™”
        super().__init__()
        
        # ğŸ”¥ Step ê¸°ë³¸ ì •ë³´ ì„¤ì •
        self.step_name = 'quality_assessment'
        self.step_number = 8
        self.step_type = "quality_assessment"
        
        # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._determine_device(device)
        self.device_type = self._get_device_type()
        
        # ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´
        self.memory_gb = self._get_system_memory()
        self.is_m3_max = self._detect_m3_max()
        
        # ğŸ”¥ ì„¤ì • ì´ˆê¸°í™”
        self.config = config or {}
        self.quality_threshold = self.config.get('quality_threshold', 0.7)
        self.assessment_mode = AssessmentMode(self.config.get('mode', 'comprehensive'))
        
        # ğŸ”¥ í’ˆì§ˆ í‰ê°€ ì„¤ì •
        self.assessment_config = {
            'mode': self.assessment_mode,
            'quality_threshold': self.quality_threshold,
            'enable_detailed_analysis': self.config.get('detailed_analysis', True),
            'enable_visualization': self.config.get('visualization', True),
            'enable_ai_models': self.config.get('enable_ai_models', True)
        }
        
        # ğŸ”¥ ModelLoader ì¸í„°í˜ì´ìŠ¤ (í•µì‹¬)
        self.model_interface = None
        self.models_loaded = {}
        
        # ğŸ”¥ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸
        self.assessment_pipeline = []
        
        # ğŸ”¥ ë¶„ì„ê¸°ë“¤ (ê¸°ì¡´ ìœ ì§€)
        self.technical_analyzer = None
        self.perceptual_analyzer = None
        self.aesthetic_analyzer = None
        self.functional_analyzer = None
        self.color_analyzer = None
        
        # ğŸ”¥ ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_error = None
        self.error_count = 0
        self.last_error = None
        
        # ğŸ”¥ ì„±ëŠ¥ ìµœì í™”
        self.optimization_enabled = self.is_m3_max and self.memory_gb >= 64
        
        self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - Device: {self.device}, Memory: {self.memory_gb}GB")
    
    # ==============================================
    # ğŸ”¥ ì´ˆê¸°í™” ë° ModelLoader ì—°ë™ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def initialize(self) -> bool:
        """í’ˆì§ˆ í‰ê°€ Step ì´ˆê¸°í™”"""
        try:
            self.logger.info(f"ğŸš€ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (í•µì‹¬)
            await self._setup_model_interface()
            
            # 2. AI ëª¨ë¸ ë¡œë“œ (ModelLoader í†µí•´)
            await self._load_quality_assessment_models()
            
            # 3. í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            self._setup_assessment_pipeline()
            
            # 4. ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self._initialize_analyzers()
            
            # 5. M3 Max ìµœì í™” ì„¤ì •
            if self.optimization_enabled:
                self._optimize_for_m3_max()
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _setup_model_interface(self):
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (í•µì‹¬)"""
        try:
            if MODEL_LOADER_AVAILABLE:
                # ê¸€ë¡œë²Œ ModelLoader ê°€ì ¸ì˜¤ê¸°
                model_loader = get_global_model_loader()
                if model_loader:
                    # Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    
                    # ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡
                    self._register_model_requirements()
                    
                    self.logger.info(f"âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ: {self.step_name}")
                    return
            
            # í´ë°±: ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì¤€ë¹„
            self.logger.warning("ModelLoader ë¯¸ì‚¬ìš©, ì§ì ‘ ë¡œë“œ ëª¨ë“œ")
            self.model_interface = None
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    def _register_model_requirements(self):
        """í’ˆì§ˆ í‰ê°€ìš© ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡"""
        try:
            if self.model_interface:
                # ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="perceptual_quality_model",
                    model_type="quality_assessment",
                    priority="high",
                    fallback_models=["lpips_model", "ssim_model"]
                )
                
                # ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="technical_quality_model", 
                    model_type="image_analysis",
                    priority="medium",
                    fallback_models=["opencv_analysis"]
                )
                
                # ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
                self.model_interface.register_model_requirement(
                    model_name="aesthetic_quality_model",
                    model_type="aesthetic_analysis", 
                    priority="medium",
                    fallback_models=["traditional_metrics"]
                )
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {self.step_name}")
        
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    async def _load_quality_assessment_models(self):
        """í’ˆì§ˆ í‰ê°€ AI ëª¨ë¸ë“¤ ë¡œë“œ (ModelLoader í†µí•´)"""
        try:
            if self.model_interface:
                # ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ
                perceptual_model = await self.model_interface.get_model("perceptual_quality_model")
                if perceptual_model:
                    self.models_loaded['perceptual'] = perceptual_model
                    self.logger.info("âœ… ì§€ê°ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                
                # ê¸°ìˆ ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ  
                technical_model = await self.model_interface.get_model("technical_quality_model")
                if technical_model:
                    self.models_loaded['technical'] = technical_model
                    self.logger.info("âœ… ê¸°ìˆ ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                
                # ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ
                aesthetic_model = await self.model_interface.get_model("aesthetic_quality_model")
                if aesthetic_model:
                    self.models_loaded['aesthetic'] = aesthetic_model
                    self.logger.info("âœ… ë¯¸ì  í’ˆì§ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                
                self.logger.info(f"âœ… {len(self.models_loaded)}ê°œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                # í´ë°±: ì „í†µì  ë¶„ì„ ë°©ë²• ì‚¬ìš©
                self.logger.warning("AI ëª¨ë¸ ì—†ìŒ, ì „í†µì  ë¶„ì„ ë°©ë²• ì‚¬ìš©")
                await self._setup_traditional_analysis()
        
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            await self._setup_traditional_analysis()
    
    async def _setup_traditional_analysis(self):
        """ì „í†µì  ë¶„ì„ ë°©ë²• ì„¤ì • (AI ëª¨ë¸ ì—†ì„ ë•Œ)"""
        try:
            # OpenCV ê¸°ë°˜ ê¸°ìˆ ì  ë¶„ì„
            self.models_loaded['opencv_technical'] = True
            
            # PIL ê¸°ë°˜ ë¯¸ì  ë¶„ì„  
            self.models_loaded['pil_aesthetic'] = True
            
            # NumPy ê¸°ë°˜ ì§€ê°ì  ë¶„ì„
            self.models_loaded['numpy_perceptual'] = True
            
            self.logger.info("âœ… ì „í†µì  ë¶„ì„ ë°©ë²• ì„¤ì • ì™„ë£Œ")
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ë¶„ì„ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_assessment_pipeline(self):
        """í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„±"""
        try:
            self.assessment_pipeline = [
                ("ê¸°ìˆ ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_technical_quality),
                ("ì§€ê°ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_perceptual_quality), 
                ("ë¯¸ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_aesthetic_quality),
                ("ê¸°ëŠ¥ì _í’ˆì§ˆ_ë¶„ì„", self._analyze_functional_quality),
                ("ìƒ‰ìƒ_í’ˆì§ˆ_ë¶„ì„", self._analyze_color_quality),
                ("ì¢…í•©_ì ìˆ˜_ê³„ì‚°", self._calculate_overall_quality),
                ("ë“±ê¸‰_ë¶€ì—¬", self._assign_quality_grade),
                ("ì‹œê°í™”_ìƒì„±", self._generate_quality_visualization)
            ]
            
            self.logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ: {len(self.assessment_pipeline)}ë‹¨ê³„")
        
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì‹¤íŒ¨: {e}")
            self.assessment_pipeline = []
    
    def _initialize_analyzers(self):
        """ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        try:
            # ê¸°ìˆ ì  ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(
                device=self.device,
                enable_gpu=TORCH_AVAILABLE and self.device != 'cpu'
            )
            
            # ê¸°ëŠ¥ì  ë¶„ì„ê¸°
            self.functional_analyzer = FittingQualityAnalyzer(
                device=self.device
            )
            
            # ìƒ‰ìƒ ë¶„ì„ê¸°
            self.color_analyzer = ColorQualityAnalyzer(
                device=self.device
            )
            
            self.logger.info("âœ… ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (process)
    # ==============================================
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬ í•¨ìˆ˜
        âœ… ModelLoader í†µí•œ AI ëª¨ë¸ í˜¸ì¶œ
        âœ… 8ê°€ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
        âœ… ì¢…í•© ì ìˆ˜ ê³„ì‚°
        """
        
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ¯ {self.step_name} í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if fitted_img is None:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ fitted_imageì…ë‹ˆë‹¤")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image else None
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            assessment_data = {
                'processed_image': fitted_img,
                'original_image': person_img,
                'clothing_image': clothing_img,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                **kwargs
            }
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if TORCH_AVAILABLE and self.optimization_enabled:
                self._optimize_memory()
            
            # 4. í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            for stage_name, stage_func in self.assessment_pipeline:
                self.logger.info(f"ğŸ”„ {stage_name} ì‹¤í–‰ ì¤‘...")
                
                stage_start = time.time()
                
                if asyncio.iscoroutinefunction(stage_func):
                    stage_result = await stage_func(assessment_data)
                else:
                    stage_result = stage_func(assessment_data)
                
                stage_duration = time.time() - stage_start
                
                # ê²°ê³¼ ë³‘í•©
                assessment_data.update(stage_result)
                
                self.logger.info(f"âœ… {stage_name} ì™„ë£Œ ({stage_duration:.2f}ì´ˆ)")
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            quality_metrics = assessment_data.get('quality_metrics')
            
            if quality_metrics is None:
                raise ValueError("í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
            
            result = {
                'success': True,
                'step_name': self.step_name,
                'overall_score': quality_metrics.overall_score,
                'confidence': quality_metrics.confidence,
                'grade': assessment_data.get('grade', 'acceptable'),
                'grade_description': assessment_data.get('grade_description', 'ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ'),
                
                # ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
                'detailed_scores': {
                    'technical': assessment_data.get('technical_results', {}),
                    'perceptual': assessment_data.get('perceptual_results', {}),
                    'aesthetic': assessment_data.get('aesthetic_results', {}),
                    'functional': assessment_data.get('functional_results', {}),
                    'color': assessment_data.get('color_results', {})
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì „ì²´
                'quality_metrics': asdict(quality_metrics),
                
                # ë©”íƒ€ë°ì´í„°
                'processing_time': processing_time,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'].value,
                
                # ì‹œìŠ¤í…œ ì •ë³´
                'device_info': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'ai_models_used': len(self.models_loaded),
                    'optimization_enabled': self.optimization_enabled
                },
                
                # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
                'warnings': assessment_data.get('warnings', []),
                'recommendations': assessment_data.get('recommendations', [])
            }
            
            self.logger.info(f"âœ… {self.step_name} í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {result['overall_score']:.3f} ({processing_time:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'step_name': self.step_name,
                'error': str(e),
                'error_type': type(e).__name__,
                'processing_time': processing_time,
                'metadata': {
                    'device': self.device,
                    'pipeline_stages': len(self.assessment_pipeline),
                    'error_location': 'main_process'
                }
            }
    
    # ==============================================
    # ğŸ”¥ í’ˆì§ˆ ë¶„ì„ ë©”ì„œë“œë“¤ (AI ëª¨ë¸ í˜¸ì¶œ)
    # ==============================================
    
    async def _analyze_technical_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ (AI ëª¨ë¸ ë˜ëŠ” ì „í†µì  ë°©ë²•)"""
        try:
            image = data['processed_image']
            
            # AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if 'technical' in self.models_loaded:
                model = self.models_loaded['technical']
                
                # ModelLoaderê°€ ì œê³µí•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
                if hasattr(model, 'predict') or hasattr(model, '__call__'):
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    processed_tensor = self._preprocess_for_ai_model(image)
                    
                    # AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                    with torch.no_grad() if TORCH_AVAILABLE else self._dummy_context():
                        if hasattr(model, 'predict'):
                            ai_result = model.predict(processed_tensor)
                        else:
                            ai_result = model(processed_tensor)
                    
                    # AI ê²°ê³¼ í•´ì„
                    technical_scores = self._interpret_technical_ai_result(ai_result)
                else:
                    # ì „í†µì  ë°©ë²•ìœ¼ë¡œ í´ë°±
                    technical_scores = self._traditional_technical_analysis(image)
            else:
                # ì „í†µì  ë¶„ì„ ë°©ë²•
                technical_scores = self._traditional_technical_analysis(image)
            
            return {
                'technical_results': technical_scores,
                'technical_score': technical_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'technical_results': {'error': str(e)},
                'technical_score': 0.3
            }
    
    async def _analyze_perceptual_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ (AI ëª¨ë¸ ë˜ëŠ” ì „í†µì  ë°©ë²•)"""
        try:
            image = data['processed_image']
            original = data.get('original_image')
            
            # AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if 'perceptual' in self.models_loaded:
                model = self.models_loaded['perceptual']
                
                # ModelLoaderê°€ ì œê³µí•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
                if hasattr(model, 'predict') or hasattr(model, '__call__'):
                    # ì´ë¯¸ì§€ ìŒ ì „ì²˜ë¦¬
                    processed_pair = self._preprocess_image_pair_for_ai(image, original)
                    
                    # AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                    with torch.no_grad() if TORCH_AVAILABLE else self._dummy_context():
                        if hasattr(model, 'predict'):
                            ai_result = model.predict(processed_pair)
                        else:
                            ai_result = model(processed_pair)
                    
                    # AI ê²°ê³¼ í•´ì„
                    perceptual_scores = self._interpret_perceptual_ai_result(ai_result)
                else:
                    # ì „í†µì  ë°©ë²•ìœ¼ë¡œ í´ë°±
                    perceptual_scores = self._traditional_perceptual_analysis(image, original)
            else:
                # ì „í†µì  ë¶„ì„ ë°©ë²•
                perceptual_scores = self._traditional_perceptual_analysis(image, original)
            
            return {
                'perceptual_results': perceptual_scores,
                'perceptual_score': perceptual_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'perceptual_results': {'error': str(e)},
                'perceptual_score': 0.3
            }
    
    async def _analyze_aesthetic_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¯¸ì  í’ˆì§ˆ ë¶„ì„ (AI ëª¨ë¸ ë˜ëŠ” ì „í†µì  ë°©ë²•)"""
        try:
            image = data['processed_image']
            
            # AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if 'aesthetic' in self.models_loaded:
                model = self.models_loaded['aesthetic']
                
                # ModelLoaderê°€ ì œê³µí•œ AI ëª¨ë¸ë¡œ ì¶”ë¡ 
                if hasattr(model, 'predict') or hasattr(model, '__call__'):
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    processed_tensor = self._preprocess_for_ai_model(image)
                    
                    # AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                    with torch.no_grad() if TORCH_AVAILABLE else self._dummy_context():
                        if hasattr(model, 'predict'):
                            ai_result = model.predict(processed_tensor)
                        else:
                            ai_result = model(processed_tensor)
                    
                    # AI ê²°ê³¼ í•´ì„
                    aesthetic_scores = self._interpret_aesthetic_ai_result(ai_result)
                else:
                    # ì „í†µì  ë°©ë²•ìœ¼ë¡œ í´ë°±
                    aesthetic_scores = self._traditional_aesthetic_analysis(image)
            else:
                # ì „í†µì  ë¶„ì„ ë°©ë²•
                aesthetic_scores = self._traditional_aesthetic_analysis(image)
            
            return {
                'aesthetic_results': aesthetic_scores,
                'aesthetic_score': aesthetic_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'aesthetic_results': {'error': str(e)},
                'aesthetic_score': 0.3
            }
    
    def _analyze_functional_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            clothing_type = data.get('clothing_type', 'default')
            
            # ì „ë¬¸ ë¶„ì„ê¸° ì‚¬ìš©
            if self.functional_analyzer:
                functional_scores = self.functional_analyzer.analyze(image, clothing_type)
            else:
                # í´ë°± ë¶„ì„
                functional_scores = self._basic_functional_analysis(image, clothing_type)
            
            return {
                'functional_results': functional_scores,
                'functional_score': functional_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'functional_results': {'error': str(e)},
                'functional_score': 0.3
            }
    
    def _analyze_color_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            image = data['processed_image']
            
            # ì „ë¬¸ ë¶„ì„ê¸° ì‚¬ìš©
            if self.color_analyzer:
                color_scores = self.color_analyzer.analyze(image)
            else:
                # í´ë°± ë¶„ì„
                color_scores = self._basic_color_analysis(image)
            
            return {
                'color_results': color_scores,
                'color_score': color_scores.get('overall_score', 0.5)
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_results': {'error': str(e)},
                'color_score': 0.3
            }
    
    def _calculate_overall_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê° ì˜ì—­ë³„ ì ìˆ˜ ìˆ˜ì§‘
            scores = {
                'technical': data.get('technical_score', 0.5),
                'perceptual': data.get('perceptual_score', 0.5),
                'aesthetic': data.get('aesthetic_score', 0.5),
                'functional': data.get('functional_score', 0.5),
                'color': data.get('color_score', 0.5)
            }
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weights = {
                'technical': 0.25,
                'perceptual': 0.25,
                'aesthetic': 0.20,
                'functional': 0.20,
                'color': 0.10
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            overall_score = sum(scores[key] * weights[key] for key in scores.keys())
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(data)
            
            # QualityMetrics ê°ì²´ ìƒì„±
            quality_metrics = QualityMetrics(
                overall_score=overall_score,
                confidence=confidence,
                sharpness_score=data.get('technical_results', {}).get('sharpness', 0.5),
                color_score=data.get('color_score', 0.5),
                fitting_score=data.get('functional_results', {}).get('fitting_accuracy', 0.5),
                realism_score=data.get('perceptual_score', 0.5),
                artifacts_score=data.get('technical_results', {}).get('artifacts', 0.5),
                alignment_score=data.get('functional_results', {}).get('clothing_alignment', 0.5),
                lighting_score=data.get('aesthetic_results', {}).get('lighting', 0.5),
                texture_score=data.get('aesthetic_results', {}).get('texture', 0.5),
                device_used=self.device,
                model_version="v2.0"
            )
            
            return {
                'quality_metrics': quality_metrics,
                'overall_score': overall_score,
                'confidence': confidence
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'quality_metrics': QualityMetrics(),
                'overall_score': 0.3,
                'confidence': 0.1
            }
    
    def _assign_quality_grade(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë“±ê¸‰ ë¶€ì—¬"""
        try:
            overall_score = data.get('overall_score', 0.5)
            
            if overall_score >= 0.9:
                grade = QualityGrade.EXCELLENT
                description = "íƒì›”í•œ í’ˆì§ˆ - ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥"
            elif overall_score >= 0.75:
                grade = QualityGrade.GOOD  
                description = "ì¢‹ì€ í’ˆì§ˆ - ì¼ë°˜ì  ì‚¬ìš© ì í•©"
            elif overall_score >= 0.6:
                grade = QualityGrade.ACCEPTABLE
                description = "ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ - ê°œì„  ê¶Œì¥"
            elif overall_score >= 0.4:
                grade = QualityGrade.POOR
                description = "ë¶ˆëŸ‰í•œ í’ˆì§ˆ - ìƒë‹¹í•œ ê°œì„  í•„ìš”"
            else:
                grade = QualityGrade.FAILED
                description = "ì‹¤íŒ¨í•œ í’ˆì§ˆ - ì¬ì²˜ë¦¬ í•„ìš”"
            
            return {
                'grade': grade.value,
                'grade_description': description
            }
        
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ë“±ê¸‰ ë¶€ì—¬ ì‹¤íŒ¨: {e}")
            return {
                'grade': QualityGrade.ACCEPTABLE.value,
                'grade_description': "ë“±ê¸‰ ê³„ì‚° ì‹¤íŒ¨"
            }
    
    def _generate_quality_visualization(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ ì‹œê°í™” ìƒì„±"""
        try:
            # ê¸°ë³¸ ì‹œê°í™” ì •ë³´ë§Œ ë°˜í™˜ (ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±ì€ ì„ íƒì‚¬í•­)
            visualization_info = {
                'has_visualization': True,
                'quality_chart': f"í’ˆì§ˆ ì ìˆ˜: {data.get('overall_score', 0.5):.3f}",
                'grade_display': data.get('grade_description', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                'detailed_breakdown': data.get('detailed_scores', {})
            }
            
            return {
                'visualization': visualization_info
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'visualization': {'error': str(e)}
            }
    
    # ==============================================
    # ğŸ”¥ AI ëª¨ë¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _preprocess_for_ai_model(self, image: np.ndarray) -> Any:
        """AI ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if TORCH_AVAILABLE:
                # NumPy to Tensor ë³€í™˜
                if len(image.shape) == 3:
                    tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                else:
                    tensor = torch.from_numpy(image).float() / 255.0
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                tensor = tensor.unsqueeze(0)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if self.device != 'cpu':
                    tensor = tensor.to(self.device)
                
                return tensor
            else:
                # PyTorch ì—†ì„ ë•ŒëŠ” NumPy ë°°ì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                return image
        
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ìš© ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _preprocess_image_pair_for_ai(self, image1: np.ndarray, image2: Optional[np.ndarray]) -> Any:
        """ì´ë¯¸ì§€ ìŒ AI ëª¨ë¸ìš© ì „ì²˜ë¦¬"""
        try:
            if image2 is None:
                return self._preprocess_for_ai_model(image1)
            
            if TORCH_AVAILABLE:
                tensor1 = self._preprocess_for_ai_model(image1)
                tensor2 = self._preprocess_for_ai_model(image2)
                
                # ìŒìœ¼ë¡œ ë¬¶ê¸°
                pair_tensor = torch.cat([tensor1, tensor2], dim=1)
                return pair_tensor
            else:
                return np.concatenate([image1, image2], axis=-1)
        
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ìŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._preprocess_for_ai_model(image1)
    
    def _dummy_context(self):
        """PyTorch ì—†ì„ ë•Œ dummy context manager"""
        class DummyContext:
            def __enter__(self):
                return self
            def __exit__(self, *args):
                pass
        return DummyContext()
    
    # ==============================================
    # ğŸ”¥ AI ê²°ê³¼ í•´ì„ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _interpret_technical_ai_result(self, ai_result: Any) -> Dict[str, Any]:
        """ê¸°ìˆ ì  í’ˆì§ˆ AI ê²°ê³¼ í•´ì„"""
        try:
            if TORCH_AVAILABLE and hasattr(ai_result, 'cpu'):
                result_data = ai_result.cpu().numpy()
            elif hasattr(ai_result, 'numpy'):
                result_data = ai_result.numpy()
            else:
                result_data = ai_result
            
            # AI ê²°ê³¼ë¥¼ í’ˆì§ˆ ì ìˆ˜ë¡œ ë³€í™˜
            if isinstance(result_data, np.ndarray):
                if result_data.size == 1:
                    overall_score = float(result_data.item())
                else:
                    overall_score = float(np.mean(result_data))
            else:
                overall_score = float(result_data) if isinstance(result_data, (int, float)) else 0.5
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'sharpness': overall_score * 0.9 + 0.1,
                'artifacts': 1.0 - overall_score * 0.3,
                'noise_level': overall_score * 0.8 + 0.2,
                'analysis_method': 'ai_model'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ ì  AI ê²°ê³¼ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._traditional_technical_analysis(None)
    
    def _interpret_perceptual_ai_result(self, ai_result: Any) -> Dict[str, Any]:
        """ì§€ê°ì  í’ˆì§ˆ AI ê²°ê³¼ í•´ì„"""
        try:
            if TORCH_AVAILABLE and hasattr(ai_result, 'cpu'):
                result_data = ai_result.cpu().numpy()
            elif hasattr(ai_result, 'numpy'):
                result_data = ai_result.numpy()
            else:
                result_data = ai_result
            
            # AI ê²°ê³¼ë¥¼ ì§€ê°ì  ì ìˆ˜ë¡œ ë³€í™˜
            if isinstance(result_data, np.ndarray):
                overall_score = float(np.mean(result_data))
            else:
                overall_score = float(result_data) if isinstance(result_data, (int, float)) else 0.5
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'visual_quality': overall_score,
                'structural_similarity': overall_score * 0.95 + 0.05,
                'perceptual_distance': 1.0 - overall_score,
                'analysis_method': 'ai_model'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì§€ê°ì  AI ê²°ê³¼ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._traditional_perceptual_analysis(None, None)
    
    def _interpret_aesthetic_ai_result(self, ai_result: Any) -> Dict[str, Any]:
        """ë¯¸ì  í’ˆì§ˆ AI ê²°ê³¼ í•´ì„"""
        try:
            if TORCH_AVAILABLE and hasattr(ai_result, 'cpu'):
                result_data = ai_result.cpu().numpy()
            elif hasattr(ai_result, 'numpy'):
                result_data = ai_result.numpy()
            else:
                result_data = ai_result
            
            # AI ê²°ê³¼ë¥¼ ë¯¸ì  ì ìˆ˜ë¡œ ë³€í™˜
            if isinstance(result_data, np.ndarray):
                overall_score = float(np.mean(result_data))
            else:
                overall_score = float(result_data) if isinstance(result_data, (int, float)) else 0.5
            
            return {
                'overall_score': max(0.0, min(1.0, overall_score)),
                'composition': overall_score * 0.9 + 0.1,
                'lighting': overall_score * 0.95 + 0.05,
                'texture': overall_score * 0.85 + 0.15,
                'color_harmony': overall_score * 0.8 + 0.2,
                'analysis_method': 'ai_model'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ì  AI ê²°ê³¼ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._traditional_aesthetic_analysis(None)
    
    # ==============================================
    # ğŸ”¥ ì „í†µì  ë¶„ì„ ë©”ì„œë“œë“¤ (AI ëª¨ë¸ ì—†ì„ ë•Œ)
    # ==============================================
    
    def _traditional_technical_analysis(self, image: Optional[np.ndarray]) -> Dict[str, Any]:
        """ì „í†µì  ê¸°ìˆ ì  ë¶„ì„ ë°©ë²•"""
        try:
            if image is None:
                return {
                    'overall_score': 0.5,
                    'sharpness': 0.5,
                    'artifacts': 0.7,
                    'noise_level': 0.6,
                    'analysis_method': 'fallback'
                }
            
            # ì „ë¬¸ ë¶„ì„ê¸° ì‚¬ìš©
            if self.technical_analyzer:
                return self.technical_analyzer.analyze(image)
            else:
                # ê°„ë‹¨í•œ ì„ ëª…ë„ ì¸¡ì •
                if len(image.shape) == 3:
                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if 'cv2' in globals() else np.mean(image, axis=2)
                else:
                    gray = image
                
                # ì„ ëª…ë„ ê³„ì‚°
                if 'cv2' in globals():
                    laplacian = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F)
                    sharpness = laplacian.var() / 10000.0  # ì •ê·œí™”
                else:
                    # OpenCV ì—†ì„ ë•Œ ê°„ë‹¨í•œ gradient ê³„ì‚°
                    dx = np.diff(gray, axis=1)
                    dy = np.diff(gray, axis=0)
                    sharpness = (np.var(dx) + np.var(dy)) / 20000.0
                
                sharpness = max(0.0, min(1.0, sharpness))
                
                return {
                    'overall_score': sharpness * 0.8 + 0.2,
                    'sharpness': sharpness,
                    'artifacts': 0.8,  # ê¸°ë³¸ê°’
                    'noise_level': 0.7,  # ê¸°ë³¸ê°’
                    'analysis_method': 'traditional'
                }
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ê¸°ìˆ  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'sharpness': 0.5,
                'artifacts': 0.7,
                'noise_level': 0.6,
                'analysis_method': 'error_fallback'
            }
    
    def _traditional_perceptual_analysis(self, image1: Optional[np.ndarray], image2: Optional[np.ndarray]) -> Dict[str, Any]:
        """ì „í†µì  ì§€ê°ì  ë¶„ì„ ë°©ë²•"""
        try:
            if image1 is None:
                return {
                    'overall_score': 0.5,
                    'visual_quality': 0.5,
                    'structural_similarity': 0.5,
                    'perceptual_distance': 0.5,
                    'analysis_method': 'fallback'
                }
            
            # SSIM ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
            if image2 is not None and SKIMAGE_AVAILABLE:
                try:
                    ssim_score = ssim(image1, image2, multichannel=True, channel_axis=2)
                    overall_score = max(0.0, ssim_score)
                except:
                    overall_score = 0.7
            else:
                # ê°„ë‹¨í•œ í†µê³„ì  ë¶„ì„
                mean_brightness = np.mean(image1) / 255.0
                brightness_score = 1.0 - abs(mean_brightness - 0.5) * 2  # 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
                
                # ëŒ€ë¹„ ë¶„ì„
                contrast = np.std(image1) / 255.0
                contrast_score = min(1.0, contrast * 2)  # ì ì ˆí•œ ëŒ€ë¹„
                
                overall_score = (brightness_score + contrast_score) / 2
            
            return {
                'overall_score': overall_score,
                'visual_quality': overall_score,
                'structural_similarity': overall_score * 0.9 + 0.1,
                'perceptual_distance': 1.0 - overall_score,
                'analysis_method': 'traditional'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ì§€ê° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'visual_quality': 0.5,
                'structural_similarity': 0.5,
                'perceptual_distance': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    def _traditional_aesthetic_analysis(self, image: Optional[np.ndarray]) -> Dict[str, Any]:
        """ì „í†µì  ë¯¸ì  ë¶„ì„ ë°©ë²•"""
        try:
            if image is None:
                return {
                    'overall_score': 0.5,
                    'composition': 0.5,
                    'lighting': 0.5,
                    'texture': 0.5,
                    'color_harmony': 0.5,
                    'analysis_method': 'fallback'
                }
            
            # ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
            if len(image.shape) == 3:
                color_std = np.mean([np.std(image[:,:,i]) for i in range(3)]) / 255.0
            else:
                color_std = np.std(image) / 255.0
            
            color_harmony = min(1.0, color_std * 1.5)
            
            # ë°ê¸° ë¶„í¬ ë¶„ì„
            brightness = np.mean(image) / 255.0
            lighting_score = 1.0 - abs(brightness - 0.5) * 1.5
            lighting_score = max(0.0, min(1.0, lighting_score))
            
            overall_score = (color_harmony + lighting_score) / 2
            
            return {
                'overall_score': overall_score,
                'composition': overall_score * 0.9 + 0.1,
                'lighting': lighting_score,
                'texture': overall_score * 0.8 + 0.2,
                'color_harmony': color_harmony,
                'analysis_method': 'traditional'
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  ë¯¸ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.5,
                'composition': 0.5,
                'lighting': 0.5,
                'texture': 0.5,
                'color_harmony': 0.5,
                'analysis_method': 'error_fallback'
            }
    
    # ==============================================
    # ğŸ”¥ í´ë°± ë¶„ì„ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _basic_functional_analysis(self, image: np.ndarray, clothing_type: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„ (í´ë°±)"""
        try:
            # ê°„ë‹¨í•œ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì²´í¬
            height, width = image.shape[:2]
            aspect_ratio = width / height
            
            # ì˜ë¥˜ íƒ€ì…ë³„ ê¸°ëŒ€ ë¹„ìœ¨
            expected_ratios = {
                'shirt': (0.7, 1.3),
                'dress': (0.6, 1.0),
                'pants': (0.8, 1.2),
                'default': (0.5, 1.5)
            }
            
            min_ratio, max_ratio = expected_ratios.get(clothing_type, expected_ratios['default'])
            
            if min_ratio <= aspect_ratio <= max_ratio:
                fitting_score = 1.0
            else:
                fitting_score = max(0.0, 1.0 - abs(aspect_ratio - (min_ratio + max_ratio) / 2) * 2)
            
            return {
                'fitting_accuracy': fitting_score,
                'clothing_alignment': 0.7,
                'naturalness': 0.6,
                'overall_score': (fitting_score + 0.7 + 0.6) / 3
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ê¸°ëŠ¥ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'fitting_accuracy': 0.5,
                'clothing_alignment': 0.5,
                'naturalness': 0.5,
                'overall_score': 0.5
            }
    
    def _basic_color_analysis(self, image: np.ndarray) -> Dict[str, Any]:
        """ê¸°ë³¸ ìƒ‰ìƒ ë¶„ì„ (í´ë°±)"""
        try:
            if len(image.shape) == 3:
                # RGB ì±„ë„ë³„ ë¶„ì„
                color_means = [np.mean(image[:,:,i]) for i in range(3)]
                color_stds = [np.std(image[:,:,i]) for i in range(3)]
                
                # ìƒ‰ìƒ ì¼ê´€ì„± (ì±„ë„ ê°„ ê· í˜•)
                consistency = 1.0 - np.std(color_means) / (np.mean(color_means) + 1e-8)
                
                # ìƒ‰ìƒ ëŒ€ë¹„
                contrast = np.mean(color_stds) / 255.0
                
                overall_score = (consistency + min(contrast * 2, 1.0)) / 2
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                overall_score = 0.6
            
            return {
                'color_consistency': consistency if len(image.shape) == 3 else 0.6,
                'color_naturalness': 0.7,
                'color_contrast': min(contrast * 2, 1.0) if len(image.shape) == 3 else 0.6,
                'overall_score': overall_score
            }
        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_consistency': 0.5,
                'color_naturalness': 0.5,
                'color_contrast': 0.5,
                'overall_score': 0.5
            }
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _load_and_validate_image(self, image_input: Union[np.ndarray, str, Path], name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if image_input is None:
                return None
            
            if isinstance(image_input, np.ndarray):
                return image_input
            elif isinstance(image_input, (str, Path)):
                image_path = Path(image_input)
                if image_path.exists():
                    from PIL import Image
                    with Image.open(image_path) as img:
                        return np.array(img)
            
            self.logger.warning(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {name}")
            return None
        
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜ {name}: {e}")
            return None
    
    def _calculate_confidence(self, data: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_factors = []
            
            # 1. AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            ai_model_count = sum(1 for key in self.models_loaded.keys() if key in ['technical', 'perceptual', 'aesthetic'])
            ai_confidence = min(1.0, ai_model_count / 3.0)  # ìµœëŒ€ 3ê°œ ëª¨ë¸
            confidence_factors.append(ai_confidence)
            
            # 2. ì…ë ¥ ë°ì´í„° í’ˆì§ˆ
            has_original = data.get('original_image') is not None
            has_clothing = data.get('clothing_image') is not None
            data_quality = (0.5 + 0.3 * has_original + 0.2 * has_clothing)
            confidence_factors.append(data_quality)
            
            # 3. ë¶„ì„ ê²°ê³¼ ì¼ê´€ì„±
            scores = [
                data.get('technical_score', 0.5),
                data.get('perceptual_score', 0.5),
                data.get('aesthetic_score', 0.5),
                data.get('functional_score', 0.5),
                data.get('color_score', 0.5)
            ]
            score_std = np.std(scores)
            consistency = max(0.0, 1.0 - score_std * 2)  # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
            confidence_factors.append(consistency)
            
            # 4. ì‹œìŠ¤í…œ ìµœì í™” ìƒíƒœ
            optimization_factor = 0.9 if self.optimization_enabled else 0.7
            confidence_factors.append(optimization_factor)
            
            return np.mean(confidence_factors)
        
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    # ==============================================
    # ğŸ”¥ ì‹œìŠ¤í…œ ìµœì í™” ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _determine_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device == "auto":
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        return device
    
    def _get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        if "mps" in self.device:
            return "Apple Silicon"
        elif "cuda" in self.device:
            return "NVIDIA GPU"
        else:
            return "CPU"
    
    def _get_system_memory(self) -> int:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)"""
        try:
            if PSUTIL_AVAILABLE:
                return int(psutil.virtual_memory().total / (1024**3))
            else:
                return 8  # ê¸°ë³¸ê°’
        except:
            return 8
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            if self.device == "mps" and self.memory_gb >= 32:
                return True
            return False
        except:
            return False
    
    def _optimize_for_m3_max(self):
        """M3 Max ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE and self.device == "mps":
                # MPS ìµœì í™” ì„¤ì •
                torch.mps.set_high_watermark_ratio(0.0)
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„¤ì •
                if hasattr(torch.backends.mps, 'set_max_memory_allocation'):
                    torch.backends.mps.set_max_memory_allocation(self.memory_gb * 0.8 * 1024**3)
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ìµœì í™” ì˜¤ë¥˜: {e}")
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if self.model_interface:
                try:
                    self.model_interface.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
            self.models_loaded.clear()
            
            # ë¶„ì„ê¸° ì •ë¦¬
            for analyzer_name in ['technical_analyzer', 'functional_analyzer', 'color_analyzer']:
                analyzer = getattr(self, analyzer_name, None)
                if analyzer and hasattr(analyzer, 'cleanup'):
                    try:
                        analyzer.cleanup()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {analyzer_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
                setattr(self, analyzer_name, None)
            
            # íŒŒì´í”„ë¼ì¸ ì •ë¦¬
            self.assessment_pipeline.clear()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory()
            
            self.is_initialized = False
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': self.step_number,
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'ai_models_loaded': len(self.models_loaded),
            'assessment_modes': [mode.value for mode in AssessmentMode],
            'quality_threshold': self.quality_threshold,
            'pipeline_stages': len(self.assessment_pipeline),
            'optimization_enabled': self.optimization_enabled,
            'is_initialized': self.is_initialized,
            'model_interface_available': self.model_interface is not None,
            'base_step_mixin_available': BASE_STEP_MIXIN_AVAILABLE,
            'model_loader_available': MODEL_LOADER_AVAILABLE
        }

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def create_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return QualityAssessmentStep(device=device, config=config, **kwargs)

async def create_and_initialize_quality_assessment_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> QualityAssessmentStep:
    """í’ˆì§ˆ í‰ê°€ Step ìƒì„± ë° ì´ˆê¸°í™”"""
    step = QualityAssessmentStep(device=device, config=config, **kwargs)
    await step.initialize()
    return step

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'QualityAssessmentStep',
    
    # ë°ì´í„° êµ¬ì¡°
    'QualityMetrics',
    'QualityGrade', 
    'AssessmentMode',
    'QualityAspect',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'EnhancedPerceptualQualityModel',
    'EnhancedAestheticQualityModel',
    
    # ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤
    'TechnicalQualityAnalyzer',
    'FittingQualityAnalyzer',
    'ColorQualityAnalyzer',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_quality_assessment_step',
    'create_and_initialize_quality_assessment_step'
]

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================

if __name__ == "__main__":
    async def test_quality_assessment_step():
        """í’ˆì§ˆ í‰ê°€ Step í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ§ª QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # Step ìƒì„±
            step = QualityAssessmentStep(device="auto")
            
            # ê¸°ë³¸ ì†ì„± í™•ì¸
            assert hasattr(step, 'logger'), "logger ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'process'), "process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            assert hasattr(step, 'cleanup_resources'), "cleanup_resources ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
            
            # Step ì •ë³´ í™•ì¸
            step_info = step.get_step_info()
            assert 'step_name' in step_info, "step_nameì´ step_infoì— ì—†ìŠµë‹ˆë‹¤!"
            
            print("âœ… QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            print(f"ğŸ“Š Step ì •ë³´: {step_info}")
            print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {step.device} ({step.device_type})")
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {step.memory_gb}GB")
            print(f"ğŸ M3 Max: {'âœ…' if step.is_m3_max else 'âŒ'}")
            print(f"ğŸ§  BaseStepMixin: {'âœ…' if BASE_STEP_MIXIN_AVAILABLE else 'âŒ'}")
            print(f"ğŸ”Œ ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
            print(f"ğŸ¯ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {len(step.assessment_pipeline)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_quality_assessment_step())