# backend/app/ai_pipeline/steps/step_08_quality_assessment.py
"""
ğŸ”¥ MyCloset AI - 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment) - ì™„ì „ í†µí•© ë²„ì „
âœ… BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ê¸°ì¡´ íŒŒì¼ì˜ ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ ëª¨ë‘ í¬í•¨
âœ… ModelLoaderì™€ ì‹¤ì œ ì—°ë™ë˜ëŠ” AI ëª¨ë¸ ì¶”ë¡ 
âœ… Pipeline Manager 100% í˜¸í™˜
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache
import numpy as np
import base64
import io

# ğŸ”¥ BaseStepMixin ìƒì† - logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°
from .base_step_mixin import QualityAssessmentMixin, ensure_step_initialization, safe_step_method, performance_monitor

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ - conda í™˜ê²½ ìµœì í™”
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch")

try:

    import cv2
    CV2_AVAILABLE = True
except:
    CV2_AVAILABLE = False
    print("âŒ OpenCV í•„ìˆ˜: conda install opencv")

try:

    from PIL import Image, ImageStat, ImageEnhance, ImageFilter, ImageDraw
    PIL_AVAILABLE = True
except:
    PIL_AVAILABLE = False
    print("âŒ Pillow í•„ìˆ˜: conda install pillow")

try:

    from scipy import stats, ndimage, spatial
    from scipy.stats import entropy
    SCIPY_AVAILABLE = True
except:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ê¶Œì¥: conda install scipy")

try:

    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ Scikit-learn ê¶Œì¥: conda install scikit-learn")

try:

    from skimage import feature, measure, filters, exposure, segmentation
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except:
    SKIMAGE_AVAILABLE = False
    print("âš ï¸ Scikit-image ê¶Œì¥: conda install scikit-image")

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜ (í†µí•©)
# ==============================================


    def _setup_model_precision:


        """M3 Max í˜¸í™˜ ì •ë°€ë„ ì„¤ì •"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float()
            elif self.device == "cuda" and hasattr(model, 'half'):
                return model.half()
            else:
                return model.float()
        except:
            self.logger.warning(f"âš ï¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model.float()

class QualityGrade:

    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"      # 90-100ì 
    GOOD = "good"               # 75-89ì 
    ACCEPTABLE = "acceptable"    # 60-74ì 
    POOR = "poor"               # 40-59ì 
    VERY_POOR = "very_poor"     # 0-39ì 

class AssessmentMode:

    """í‰ê°€ ëª¨ë“œ"""
    FAST = "fast"              # ë¹ ë¥¸ ê¸°ë³¸ í‰ê°€
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© í‰ê°€
    DETAILED = "detailed"      # ìƒì„¸ ë¶„ì„
    NEURAL = "neural"          # AI ê¸°ë°˜ í‰ê°€

class QualityAspect:

    """í’ˆì§ˆ ì¸¡ë©´"""
    TECHNICAL = "technical"    # ê¸°ìˆ ì  í’ˆì§ˆ
    PERCEPTUAL = "perceptual"  # ì§€ê°ì  í’ˆì§ˆ
    AESTHETIC = "aesthetic"    # ë¯¸ì  í’ˆì§ˆ
    FUNCTIONAL = "functional"  # ê¸°ëŠ¥ì  í’ˆì§ˆ

# ==============================================
# ğŸ”¥ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤ (í†µí•©)
# ==============================================

@dataclass
class QualityMetrics:
    """ì™„ì „í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê¸°ì¡´ + ìƒˆë¡œìš´ ê¸°ëŠ¥ í†µí•©)"""
    
    # ê¸°ìˆ ì  í’ˆì§ˆ
    sharpness: float = 0.0
    noise_level: float = 0.0
    contrast: float = 0.0
    saturation: float = 0.0
    brightness: float = 0.0
    color_accuracy: float = 0.0
    
    # ì§€ê°ì  í’ˆì§ˆ
    structural_similarity: float = 0.0
    perceptual_similarity: float = 0.0
    visual_quality: float = 0.0
    artifact_level: float = 0.0
    ssim_score: float = 0.0
    psnr_score: float = 0.0
    lpips_score: float = 0.0
    
    # ë¯¸ì  í’ˆì§ˆ
    composition: float = 0.0
    color_harmony: float = 0.0
    symmetry: float = 0.0
    balance: float = 0.0
    
    # ê¸°ëŠ¥ì  í’ˆì§ˆ
    fitting_quality: float = 0.0
    edge_preservation: float = 0.0
    texture_quality: float = 0.0
    detail_preservation: float = 0.0
    
    # ì „ì²´ ì ìˆ˜
    overall_score: float = 0.0
    confidence: float = 0.0
    
    def calculate_overall_score:
    
        """ì „ì²´ ì ìˆ˜ ê³„ì‚° (í–¥ìƒëœ ë²„ì „)"""
        if:
            weights = {
                'technical': 0.3,
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.2
            }
        
        # ê¸°ìˆ ì  í’ˆì§ˆ (ë…¸ì´ì¦ˆëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        technical_score = np.mean([
            self.sharpness, 
            1.0 - self.noise_level,  # ë…¸ì´ì¦ˆ ì—­ì „
            self.contrast, 
            self.brightness, 
            self.saturation,
            self.color_accuracy
        ])
        
        # ì§€ê°ì  í’ˆì§ˆ (ì•„í‹°íŒ©íŠ¸ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        perceptual_score = np.mean([
            self.ssim_score or self.structural_similarity,
            self.perceptual_similarity,
            self.visual_quality,
            1.0 - self.artifact_level,  # ì•„í‹°íŒ©íŠ¸ ì—­ì „
            self.psnr_score
        ])
        
        # ë¯¸ì  í’ˆì§ˆ
        aesthetic_score = np.mean([
            self.composition, 
            self.color_harmony,
            self.symmetry, 
            self.balance
        ])
        
        # ê¸°ëŠ¥ì  í’ˆì§ˆ
        functional_score = np.mean([
            self.fitting_quality, 
            self.edge_preservation,
            self.texture_quality, 
            self.detail_preservation
        ])
        
        # ê°€ì¤‘ í‰ê· 
        self.overall_score = (
            technical_score * weights['technical'] +
            perceptual_score * weights['perceptual'] +
            aesthetic_score * weights['aesthetic'] +
            functional_score * weights['functional']
        )
        
        return self.overall_score
    
    def get_grade:
    
        """ë“±ê¸‰ ë°˜í™˜"""
        score = self.overall_score * 100
        
        if:
        
            return QualityGrade.EXCELLENT
        elif score >= 75:
            return QualityGrade.GOOD
        elif score >= 60:
            return QualityGrade.ACCEPTABLE
        elif score >= 40:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (í–¥ìƒëœ ë²„ì „)
# ==============================================

class EnhancedPerceptualQualityModel:

    """í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
    
    def __init__:
    
        super().__init__()
        
        # ë” ê¹Šì€ CNN ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_extractor = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8))
        )
        
        # ë‹¤ì¤‘ í’ˆì§ˆ ì˜ˆì¸¡ê¸°
        feature_dim = 256 * 8 * 8
        self.quality_predictors = nn.ModuleDict({
            'overall': nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1),
                nn.Sigmoid()
            ),
            'sharpness': nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'artifacts': nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
        })
    
    def forward:
    
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        
        results = {}
        for name, predictor in self.quality_predictors.items():
            results[name] = predictor(features)
        
        return results

class EnhancedAestheticQualityModel:

    """í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ (ResNet ë°±ë³¸)"""
    
    def __init__:
    
        super().__init__()
        
        # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
        self.backbone = nn.Sequential(
            # Initial conv
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet blocks
            self._make_layer(64, 64, 2),
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ë¯¸ì  ì ìˆ˜ ì˜ˆì¸¡ í—¤ë“œë“¤
        self.aesthetic_heads = nn.ModuleDict({
            'composition': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'color_harmony': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'symmetry': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            ),
            'balance': nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
        })
    
    def _make_layer:
    
        layers = []
        layers.append(self._make_resnet_block(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(self._make_resnet_block(out_channels, out_channels))
        return nn.Sequential(*layers)
    
    def _make_resnet_block:
    
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )
    
    def forward:
    
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        
        results = {}
        for name, head in self.aesthetic_heads.items():
            results[name] = head(features)
        
        return results

# ==============================================
# ğŸ”¥ ì „ë¬¸ ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ í†µí•©)
# ==============================================

class TechnicalQualityAnalyzer:

    """í–¥ìƒëœ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__:
    
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.TechnicalQualityAnalyzer")
    
    def analyze_comprehensive:
    
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. ì„ ëª…ë„ ë¶„ì„ (í–¥ìƒëœ ë²„ì „)
            results['sharpness'] = self._analyze_sharpness_enhanced(image)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„ (ë‹¤ì¤‘ ë°©ë²•)
            results['noise_level'] = self._analyze_noise_multi_method(image)
            
            # 3. ëŒ€ë¹„ ë¶„ì„ (ì ì‘í˜•)
            results['contrast'] = self._analyze_contrast_adaptive(image)
            
            # 4. ë°ê¸° ë¶„ì„ (íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜)
            results['brightness'] = self._analyze_brightness_histogram(image)
            
            # 5. ì±„ë„ ë¶„ì„ (HSV ê¸°ë°˜)
            results['saturation'] = self._analyze_saturation_hsv(image)
            
            return results
            
        except:
            
            self.logger.error(f"ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'sharpness': 0.5, 'noise_level': 0.5, 'contrast': 0.5,
                'brightness': 0.5, 'saturation': 0.5
            }
    
    def _analyze_sharpness_enhanced:
    
        """í–¥ìƒëœ ì„ ëª…ë„ ë¶„ì„"""
        try:
            if:
                return 0.5
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # ë‹¤ì¤‘ ì—£ì§€ ê°ì§€ ë°©ë²• ì¡°í•©
            methods = []
            
            # 1. Laplacian variance
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            methods.append(min(laplacian_var / 1000.0, 1.0))
            
            # 2. Sobel magnitude
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2).mean()
            methods.append(min(sobel_magnitude / 100.0, 1.0))
            
            # 3. Canny edge density
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            methods.append(min(edge_density * 10, 1.0))
            
            return np.mean(methods)
            
        except:
            
            self.logger.error(f"ì„ ëª…ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_noise_multi_method:
    
        """ë‹¤ì¤‘ ë°©ë²• ë…¸ì´ì¦ˆ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            noise_levels = []
            
            # 1. ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„
            if:
                kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                filtered = cv2.filter2D(gray, -1, kernel)
                noise_levels.append(np.std(filtered) / 255.0)
            
            # 2. ê°€ìš°ì‹œì•ˆ í•„í„° ì°¨ì´
            if:
                blurred = cv2.GaussianBlur(gray, (5, 5), 0)
                noise_levels.append(np.std(gray - blurred) / 255.0)
            
            # 3. ì›¨ì´ë¸”ë¦¿ ê¸°ë°˜ (ê·¼ì‚¬)
            if:
                noise_levels.append(np.std(gray) / 255.0 * 0.3)  # í´ë°±
            
            return min(np.mean(noise_levels), 1.0)
            
        except:
            
            self.logger.error(f"ë…¸ì´ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.3
    
    def _analyze_contrast_adaptive:
    
        """ì ì‘í˜• ëŒ€ë¹„ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # 1. RMS ëŒ€ë¹„
            rms_contrast = np.std(gray) / 255.0
            
            # 2. Michelson ëŒ€ë¹„
            max_val, min_val = np.max(gray), np.min(gray)
            if:
                michelson_contrast = (max_val - min_val) / (max_val + min_val)
            else:
                michelson_contrast = 0
            
            # 3. íˆìŠ¤í† ê·¸ë¨ ë¶„ì‚°
            hist, _ = np.histogram(gray, bins=256)
            hist_normalized = hist / np.sum(hist)
            hist_entropy = -np.sum(hist_normalized * np.log2(hist_normalized + 1e-8))
            entropy_contrast = hist_entropy / 8.0  # ì •ê·œí™”
            
            # ì¢…í•© ëŒ€ë¹„ ì ìˆ˜
            contrast_score = np.mean([rms_contrast, michelson_contrast, entropy_contrast])
            return min(contrast_score, 1.0)
            
        except:
            
            self.logger.error(f"ëŒ€ë¹„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_brightness_histogram:
    
        """íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ë°ê¸° ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            
            # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            hist, bins = np.histogram(gray, bins=256, range=(0, 256))
            hist_normalized = hist / np.sum(hist)
            
            # ê°€ì¤‘ í‰ê·  ë°ê¸°
            bin_centers = (bins[:-1] + bins[1:]) / 2
            weighted_brightness = np.sum(hist_normalized * bin_centers) / 255.0
            
            # ì ì • ë°ê¸° ë²”ìœ„ í‰ê°€ (0.3-0.7)
            optimal_min, optimal_max = 0.3, 0.7
            
            if:
            
                brightness_score = 1.0
            elif weighted_brightness < optimal_min:
                brightness_score = weighted_brightness / optimal_min
            else:
                brightness_score = 1.0 - (weighted_brightness - optimal_max) / (1.0 - optimal_max)
            
            return max(0.0, min(brightness_score, 1.0))
            
        except:
            
            self.logger.error(f"ë°ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_saturation_hsv:
    
        """HSV ê¸°ë°˜ ì±„ë„ ë¶„ì„"""
        try:
            if:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation = hsv[:, :, 1]
                
                # í‰ê·  ë° ë¶„ì‚° ë¶„ì„
                mean_sat = np.mean(saturation) / 255.0
                std_sat = np.std(saturation) / 255.0
                
                # ì ì • ì±„ë„ ë²”ìœ„ (0.3-0.8) + ë¶„ì‚° ê³ ë ¤
                optimal_range = 0.3 <= mean_sat <= 0.8
                good_variance = 0.1 <= std_sat <= 0.3
                
                saturation_score = 0.0
                if:
                    saturation_score += 0.7
                else:
                    saturation_score += max(0, 0.7 - abs(mean_sat - 0.55) * 2)
                
                if:
                
                    saturation_score += 0.3
                else:
                    saturation_score += max(0, 0.3 - abs(std_sat - 0.2) * 3)
                
                return min(saturation_score, 1.0)
            else:
                # RGB ê¸°ë°˜ ê·¼ì‚¬ ì±„ë„
                r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]
                max_rgb = np.maximum(np.maximum(r, g), b)
                min_rgb = np.minimum(np.minimum(r, g), b)
                saturation = np.mean((max_rgb - min_rgb) / (max_rgb + 1e-8))
                return min(saturation, 1.0)
                
        except:
                
            self.logger.error(f"ì±„ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5

class FittingQualityAnalyzer:

    """ì˜ë¥˜ í”¼íŒ… í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°"""
    
    def __init__:
    
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.FittingQualityAnalyzer")
    
    def analyze_comprehensive:
    
        """ì¢…í•© í”¼íŒ… í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. í”¼íŒ… ì •í™•ë„
            results['fitting_accuracy'] = self._analyze_fitting_accuracy_advanced(fitted_img, person_img, clothing_type)
            
            # 2. ì—£ì§€ ë³´ì¡´
            results['edge_preservation'] = self._analyze_edge_preservation_advanced(fitted_img, person_img)
            
            # 3. í…ìŠ¤ì²˜ ë³´ì¡´
            results['texture_preservation'] = self._analyze_texture_preservation_advanced(fitted_img, person_img)
            
            # 4. í˜•íƒœ ì¼ê´€ì„±
            results['shape_consistency'] = self._analyze_shape_consistency(fitted_img, person_img)
            
            # 5. ì˜ë¥˜ë³„ íŠ¹í™” ë¶„ì„
            results['clothing_specific_quality'] = self._analyze_clothing_specific_quality(fitted_img, clothing_type)
            
            return results
            
        except:
            
            self.logger.error(f"í”¼íŒ… í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'fitting_accuracy': 0.5, 'edge_preservation': 0.5,
                'texture_preservation': 0.5, 'shape_consistency': 0.5,
                'clothing_specific_quality': 0.5
            }
    
    def _analyze_fitting_accuracy_advanced:
    
        """ê³ ê¸‰ í”¼íŒ… ì •í™•ë„ ë¶„ì„"""
        try:
            if:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0]))
            
            accuracy_factors = []
            
            # 1. ìœ¤ê³½ì„  ì¼ì¹˜ë„
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            fitted_contours, _ = cv2.findContours(cv2.Canny(fitted_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            person_contours, _ = cv2.findContours(cv2.Canny(person_gray, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if fitted_contours and person_contours:
                # ê°€ì¥ í° ìœ¤ê³½ì„  ë¹„êµ
                fitted_main = max(fitted_contours, key=cv2.contourArea)
                person_main = max(person_contours, key=cv2.contourArea)
                
                # ìœ¤ê³½ì„  ë§¤ì¹­ ì ìˆ˜
                match_score = cv2.matchShapes(fitted_main, person_main, cv2.CONTOURS_MATCH_I1, 0)
                contour_similarity = max(0, 1.0 - match_score)
                accuracy_factors.append(contour_similarity)
            
            # 2. SSIM ê¸°ë°˜ êµ¬ì¡° ìœ ì‚¬ì„±
            if:
                ssim_score = ssim(person_img, fitted_img, multichannel=True, channel_axis=2)
                accuracy_factors.append(max(0, ssim_score))
            
            # 3. ì˜ë¥˜ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            from .step_08_quality_assessment import QualityAssessmentStep
            clothing_weights = QualityAssessmentStep.CLOTHING_QUALITY_WEIGHTS.get(
                clothing_type, 
                QualityAssessmentStep.CLOTHING_QUALITY_WEIGHTS['default']
            )
            
            base_accuracy = np.mean(accuracy_factors) if accuracy_factors else 0.6
            weighted_accuracy = base_accuracy * clothing_weights['fitting'] + 0.3 * (1 - clothing_weights['fitting'])
            
            return min(weighted_accuracy, 1.0)
            
        except:
            
            self.logger.error(f"í”¼íŒ… ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_edge_preservation_advanced:
    
        """ê³ ê¸‰ ì—£ì§€ ë³´ì¡´ ë¶„ì„"""
        try:
            if:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0]))
            
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            preservation_scores = []
            
            # 1. ë‹¤ì¤‘ ì„ê³„ê°’ Canny ì—£ì§€ ë¹„êµ
            thresholds = [(50, 150), (100, 200), (30, 100)]
            
            for low, high in thresholds:
                fitted_edges = cv2.Canny(fitted_gray, low, high)
                person_edges = cv2.Canny(person_gray, low, high)
                
                # IoU ê³„ì‚°
                intersection = np.sum((fitted_edges > 0) & (person_edges > 0))
                union = np.sum((fitted_edges > 0) | (person_edges > 0))
                
                if:
                
                    iou = intersection / union
                    preservation_scores.append(iou)
            
            # 2. ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ ì¼ì¹˜ë„
            grad_x_fitted = cv2.Sobel(fitted_gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y_fitted = cv2.Sobel(fitted_gray, cv2.CV_64F, 0, 1, ksize=3)
            grad_x_person = cv2.Sobel(person_gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y_person = cv2.Sobel(person_gray, cv2.CV_64F, 0, 1, ksize=3)
            
            # ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥ ê°ë„
            angles_fitted = np.arctan2(grad_y_fitted, grad_x_fitted)
            angles_person = np.arctan2(grad_y_person, grad_x_person)
            
            # ê°ë„ ì°¨ì´ (circular distance)
            angle_diff = np.abs(angles_fitted - angles_person)
            angle_diff = np.minimum(angle_diff, 2*np.pi - angle_diff)
            
            # ê°•í•œ ì—£ì§€ì—ì„œë§Œ ê³„ì‚°
            edge_strength = np.sqrt(grad_x_fitted**2 + grad_y_fitted**2)
            strong_edges = edge_strength > np.percentile(edge_strength, 75)
            
            if:
            
                angle_similarity = np.mean(1.0 - angle_diff[strong_edges] / np.pi)
                preservation_scores.append(angle_similarity)
            
            return np.mean(preservation_scores) if preservation_scores else 0.6
            
        except:
            
            self.logger.error(f"ì—£ì§€ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_texture_preservation_advanced:
    
        """ê³ ê¸‰ í…ìŠ¤ì²˜ ë³´ì¡´ ë¶„ì„"""
        try:
            if:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            texture_scores = []
            
            # 1. LBP (Local Binary Pattern) ë¹„êµ
            if:
                fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(fitted_img[...,:3], [0.2989, 0.5870, 0.1140])
                person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(person_img[...,:3], [0.2989, 0.5870, 0.1140])
                
                # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ LBP
                for radius in [1, 2, 3]:
                    n_points = 8 * radius
                    fitted_lbp = feature.local_binary_pattern(fitted_gray, n_points, radius, method='uniform')
                    person_lbp = feature.local_binary_pattern(person_gray, n_points, radius, method='uniform')
                    
                    # íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
                    fitted_hist, _ = np.histogram(fitted_lbp.ravel(), bins=n_points + 2)
                    person_hist, _ = np.histogram(person_lbp.ravel(), bins=n_points + 2)
                    
                    # ì •ê·œí™”
                    fitted_hist = fitted_hist.astype(float) / (fitted_hist.sum() + 1e-8)
                    person_hist = person_hist.astype(float) / (person_hist.sum() + 1e-8)
                    
                    # Bhattacharyya coefficient
                    similarity = np.sum(np.sqrt(fitted_hist * person_hist))
                    texture_scores.append(similarity)
            
            # 2. ê°€ë³´ í•„í„° ì‘ë‹µ ë¹„êµ
            if:
                from skimage.filters import gabor
                
                for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:
                    for frequency in [0.1, 0.3, 0.5]:
                        fitted_gabor, _ = gabor(fitted_gray, frequency=frequency, theta=theta)
                        person_gabor, _ = gabor(person_gray, frequency=frequency, theta=theta)
                        
                        # ì‘ë‹µ ìƒê´€ê´€ê³„
                        correlation = np.corrcoef(fitted_gabor.ravel(), person_gabor.ravel())[0, 1]
                        if:
                            texture_scores.append(max(0, correlation))
            
            # 3. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¶„ì„
            if len(texture_scores) == 0:
                # í´ë°±: ê°„ë‹¨í•œ í…ìŠ¤ì²˜ ë¶„ì„
                fitted_std = np.std(fitted_img)
                person_std = np.std(person_img)
                if:
                    texture_similarity = min(fitted_std / person_std, person_std / fitted_std)
                    texture_scores.append(texture_similarity)
            
            return np.mean(texture_scores) if texture_scores else 0.6
            
        except:
            
            self.logger.error(f"í…ìŠ¤ì²˜ ë³´ì¡´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_shape_consistency:
    
        """í˜•íƒœ ì¼ê´€ì„± ë¶„ì„"""
        try:
            if:
                return 0.6
            
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0]))
            
            fitted_gray = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2GRAY)
            person_gray = cv2.cvtColor(person_img, cv2.COLOR_RGB2GRAY)
            
            # ëª¨ë©˜íŠ¸ ê¸°ë°˜ í˜•íƒœ ë¶„ì„
            fitted_moments = cv2.moments(fitted_gray)
            person_moments = cv2.moments(person_gray)
            
            # Hu ëª¨ë©˜íŠ¸ (í˜•íƒœ ë¶ˆë³€ íŠ¹ì§•)
            fitted_hu = cv2.HuMoments(fitted_moments).flatten()
            person_hu = cv2.HuMoments(person_moments).flatten()
            
            # ë¡œê·¸ ë³€í™˜
            fitted_hu = -np.sign(fitted_hu) * np.log10(np.abs(fitted_hu) + 1e-10)
            person_hu = -np.sign(person_hu) * np.log10(np.abs(person_hu) + 1e-10)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            hu_similarity = np.exp(-np.sum(np.abs(fitted_hu - person_hu)))
            
            return min(hu_similarity, 1.0)
            
        except:
            
            self.logger.error(f"í˜•íƒœ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_clothing_specific_quality:
    
        """ì˜ë¥˜ë³„ íŠ¹í™” í’ˆì§ˆ ë¶„ì„"""
        try:
            # ì˜ë¥˜ íƒ€ì…ë³„ íŠ¹í™” ë¶„ì„
            if:
                return self._analyze_shirt_quality(fitted_img)
            elif clothing_type in ['pants', 'jeans']:
                return self._analyze_pants_quality(fitted_img)
            elif clothing_type == 'dress':
                return self._analyze_dress_quality(fitted_img)
            elif clothing_type == 'jacket':
                return self._analyze_jacket_quality(fitted_img)
            else:
                return self._analyze_general_clothing_quality(fitted_img)
                
        except:
                
            self.logger.error(f"ì˜ë¥˜ë³„ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_shirt_quality:
    
        """ì…”ì¸  í’ˆì§ˆ ë¶„ì„"""
        # ì…”ì¸  íŠ¹í™”: ì£¼ë¦„, ë‹¨ì¶”, ì¹¼ë¼ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_pants_quality:
    
        """ë°”ì§€ í’ˆì§ˆ ë¶„ì„"""
        # ë°”ì§€ íŠ¹í™”: ë‹¤ë¦¬ ì„ , ì£¼ë¦„, í• ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_dress_quality:
    
        """ë“œë ˆìŠ¤ í’ˆì§ˆ ë¶„ì„"""
        # ë“œë ˆìŠ¤ íŠ¹í™”: ë“œë ˆì´í•‘, ì‹¤ë£¨ì—£ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_jacket_quality:
    
        """ì¬í‚· í’ˆì§ˆ ë¶„ì„"""
        # ì¬í‚· íŠ¹í™”: ì–´ê¹¨ì„ , ë¼í , í…ìŠ¤ì²˜ ë“±
        return 0.7  # êµ¬í˜„ ì˜ˆì‹œ
    
    def _analyze_general_clothing_quality:
    
        """ì¼ë°˜ ì˜ë¥˜ í’ˆì§ˆ ë¶„ì„"""
        return 0.6  # ê¸°ë³¸ í’ˆì§ˆ

class ColorQualityAnalyzer:

    """ìƒ‰ìƒ í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°"""
    
    def __init__:
    
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.ColorQualityAnalyzer")
    
    def analyze_comprehensive:
    
        """ì¢…í•© ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        results = {}
        
        try:
            # 1. ìƒ‰ìƒ ì •í™•ë„ (ì›ë³¸ ëŒ€ë¹„)
            if:
                results['color_accuracy'] = self._analyze_color_accuracy_advanced(fitted_img, person_img)
            else:
                results['color_accuracy'] = 0.7
            
            # 2. ìƒ‰ìƒ ì¡°í™”
            results['color_harmony'] = self._analyze_color_harmony_advanced(fitted_img)
            
            # 3. ìƒ‰ìƒ ì¼ê´€ì„±
            results['color_consistency'] = self._analyze_color_consistency_advanced(fitted_img)
            
            # 4. ìƒ‰ìƒ ìƒë™ê°
            results['color_vibrancy'] = self._analyze_color_vibrancy(fitted_img)
            
            # 5. í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤
            results['white_balance'] = self._analyze_white_balance(fitted_img)
            
            return results
            
        except:
            
            self.logger.error(f"ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'color_accuracy': 0.5, 'color_harmony': 0.5,
                'color_consistency': 0.5, 'color_vibrancy': 0.5,
                'white_balance': 0.5
            }
    
    def _analyze_color_accuracy_advanced:
    
        """ê³ ê¸‰ ìƒ‰ìƒ ì •í™•ë„ ë¶„ì„"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                person_img = cv2.resize(person_img, (fitted_img.shape[1], fitted_img.shape[0])) if CV2_AVAILABLE else person_img
            
            accuracy_scores = []
            
            # 1. LAB ìƒ‰ê³µê°„ì—ì„œ Delta E ê³„ì‚°
            if:
                fitted_lab = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2LAB)
                person_lab = cv2.cvtColor(person_img, cv2.COLOR_RGB2LAB)
                
                # í”½ì…€ë³„ Delta E
                delta_e = np.sqrt(np.sum((fitted_lab.astype(float) - person_lab.astype(float))**2, axis=2))
                mean_delta_e = np.mean(delta_e)
                
                # Delta Eë¥¼ 0-1 ìŠ¤ì½”ì–´ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                delta_e_score = max(0, 1.0 - mean_delta_e / 100.0)
                accuracy_scores.append(delta_e_score)
            
            # 2. HSV ìƒ‰ê³µê°„ ë¹„êµ
            if:
                fitted_hsv = cv2.cvtColor(fitted_img, cv2.COLOR_RGB2HSV)
                person_hsv = cv2.cvtColor(person_img, cv2.COLOR_RGB2HSV)
                
                # ìƒ‰ìƒ(H), ì±„ë„(S), ëª…ë„(V) ê°ê° ë¹„êµ
                h_diff = np.mean(np.abs(fitted_hsv[:,:,0].astype(float) - person_hsv[:,:,0].astype(float))) / 180.0
                s_diff = np.mean(np.abs(fitted_hsv[:,:,1].astype(float) - person_hsv[:,:,1].astype(float))) / 255.0
                v_diff = np.mean(np.abs(fitted_hsv[:,:,2].astype(float) - person_hsv[:,:,2].astype(float))) / 255.0
                
                hsv_score = 1.0 - np.mean([h_diff, s_diff, v_diff])
                accuracy_scores.append(max(0, hsv_score))
            
            # 3. íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
            if:
                hist_scores = []
                for i in range(3):  # RGB ê° ì±„ë„
                    fitted_hist = cv2.calcHist([fitted_img], [i], None, [256], [0, 256])
                    person_hist = cv2.calcHist([person_img], [i], None, [256], [0, 256])
                    
                    # íˆìŠ¤í† ê·¸ë¨ ìƒê´€ê´€ê³„
                    correlation = cv2.compareHist(fitted_hist, person_hist, cv2.HISTCMP_CORREL)
                    hist_scores.append(max(0, correlation))
                
                accuracy_scores.append(np.mean(hist_scores))
            
            # í´ë°±: ê°„ë‹¨í•œ í‰ê·  ìƒ‰ìƒ ë¹„êµ
            if:
                fitted_mean = np.mean(fitted_img, axis=(0, 1))
                person_mean = np.mean(person_img, axis=(0, 1))
                diff = np.linalg.norm(fitted_mean - person_mean) / (255 * np.sqrt(3))
                accuracy_scores.append(max(0, 1.0 - diff))
            
            return np.mean(accuracy_scores)
            
        except:
            
            self.logger.error(f"ìƒ‰ìƒ ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_color_harmony_advanced:
    
        """ê³ ê¸‰ ìƒ‰ìƒ ì¡°í™” ë¶„ì„"""
        try:
            harmony_scores = []
            
            # 1. ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ ë° ì¡°í™” ë¶„ì„
            if:
                pixels = image.reshape(-1, 3)
                
                # ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
                if:
                    indices = np.random.choice(len(pixels), 20000, replace=False)
                    pixels = pixels[indices]
                
                # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
                for n_colors in [3, 5, 7]:
                    try:
                        kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
                        kmeans.fit(pixels)
                        
                        centers = kmeans.cluster_centers_
                        
                        # ìƒ‰ìƒ ê°„ ê°ë„ ê´€ê³„ ë¶„ì„ (HSV ê³µê°„)
                        if:
                            hsv_centers = []
                            for center in centers:
                                center_rgb = center.reshape(1, 1, 3).astype(np.uint8)
                                center_hsv = cv2.cvtColor(center_rgb, cv2.COLOR_RGB2HSV)[0, 0]
                                hsv_centers.append(center_hsv[0])  # ìƒ‰ìƒê°’ë§Œ
                            
                            # ì¡°í™”ë¡œìš´ ìƒ‰ìƒ ê´€ê³„ í™•ì¸ (ë³´ìƒ‰, 3ìƒ‰ ì¡°í™”, ìœ ì‚¬ìƒ‰ ë“±)
                            angles = np.array(hsv_centers) * 2  # 0-360ë„ ë³€í™˜
                            angle_diffs = []
                            
                            for i in range(len(angles)):
                                for j in range(i+1, len(angles)):
                                    diff = abs(angles[i] - angles[j])
                                    diff = min(diff, 360 - diff)  # ì›í˜• ê±°ë¦¬
                                    angle_diffs.append(diff)
                            
                            # ì¡°í™”ë¡œìš´ ê°ë„ë“¤ (60ë„ ë°°ìˆ˜)
                            harmonic_angles = [60, 120, 180]
                            harmony_score = 0
                            
                            for diff in angle_diffs:
                                for harmonic in harmonic_angles:
                                    if abs(diff - harmonic) <= 15:  # 15ë„ í—ˆìš© ì˜¤ì°¨
                                        harmony_score += 1
                            
                            harmony_scores.append(min(harmony_score / len(angle_diffs), 1.0))
                    
                    except:
                    
                        continue
            
            # 2. ìƒ‰ìƒ ë¶„ì‚° ë¶„ì„
            color_std = np.std(image, axis=(0, 1))
            color_balance = 1.0 - np.std(color_std) / (np.mean(color_std) + 1e-8)
            harmony_scores.append(max(0, min(color_balance, 1.0)))
            
            # 3. ì±„ë„ ì¼ê´€ì„±
            if:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                saturation_std = np.std(hsv[:, :, 1]) / 255.0
                saturation_consistency = max(0, 1.0 - saturation_std)
                harmony_scores.append(saturation_consistency)
            
            return np.mean(harmony_scores) if harmony_scores else 0.6
            
        except:
            
            self.logger.error(f"ìƒ‰ìƒ ì¡°í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_color_consistency_advanced:
    
        """ê³ ê¸‰ ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„"""
        try:
            h, w = image.shape[:2]
            consistency_scores = []
            
            # 1. ì§€ì—­ë³„ ìƒ‰ìƒ ë¶„í¬ ë¹„êµ
            regions = [
                image[:h//2, :w//2],      # ì¢Œìƒ
                image[:h//2, w//2:],      # ìš°ìƒ
                image[h//2:, :w//2],      # ì¢Œí•˜
                image[h//2:, w//2:]       # ìš°í•˜
            ]
            
            region_stats = []
            for region in regions:
                if:
                    mean_color = np.mean(region, axis=(0, 1))
                    std_color = np.std(region, axis=(0, 1))
                    region_stats.append({'mean': mean_color, 'std': std_color})
            
            # ì§€ì—­ ê°„ ì¼ê´€ì„± ê³„ì‚°
            if:
                mean_diffs = []
                std_diffs = []
                
                for i in range(len(region_stats)):
                    for j in range(i+1, len(region_stats)):
                        mean_diff = np.linalg.norm(region_stats[i]['mean'] - region_stats[j]['mean'])
                        std_diff = np.linalg.norm(region_stats[i]['std'] - region_stats[j]['std'])
                        
                        mean_diffs.append(mean_diff)
                        std_diffs.append(std_diff)
                
                # ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ì¼ê´€ì„±ì´ ë†’ìŒ
                mean_consistency = max(0, 1.0 - np.mean(mean_diffs) / 128.0)
                std_consistency = max(0, 1.0 - np.mean(std_diffs) / 64.0)
                
                consistency_scores.extend([mean_consistency, std_consistency])
            
            # 2. ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„
            if:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                
                gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
                
                # ì ë‹¹í•œ ê·¸ë¼ë””ì–¸íŠ¸ (ë„ˆë¬´ uniformí•˜ì§€ë„ chaoticí•˜ì§€ë„ ì•Šê²Œ)
                grad_mean = np.mean(gradient_magnitude)
                grad_std = np.std(gradient_magnitude)
                
                # ì ì • ë²”ìœ„ í‰ê°€
                optimal_grad_mean = 50  # ê²½í—˜ì  ê°’
                optimal_grad_std = 30
                
                grad_score = (
                    max(0, 1.0 - abs(grad_mean - optimal_grad_mean) / optimal_grad_mean) +
                    max(0, 1.0 - abs(grad_std - optimal_grad_std) / optimal_grad_std)
                ) / 2
                
                consistency_scores.append(grad_score)
            
            return np.mean(consistency_scores) if consistency_scores else 0.6
            
        except:
            
            self.logger.error(f"ìƒ‰ìƒ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_color_vibrancy:
    
        """ìƒ‰ìƒ ìƒë™ê° ë¶„ì„"""
        try:
            if:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                
                # ì±„ë„ì™€ ëª…ë„ë¥¼ ì¢…í•©í•œ ìƒë™ê°
                saturation = hsv[:, :, 1] / 255.0
                value = hsv[:, :, 2] / 255.0
                
                # ë†’ì€ ì±„ë„ + ì ë‹¹í•œ ëª…ë„ = ìƒë™ê°
                vibrancy = saturation * value * (1.0 - np.abs(value - 0.7))  # 0.7 ì£¼ë³€ì´ ìµœì 
                
                mean_vibrancy = np.mean(vibrancy)
                return min(mean_vibrancy * 2, 1.0)  # ìŠ¤ì¼€ì¼ë§
            else:
                # RGB ê¸°ë°˜ ê·¼ì‚¬
                rgb_range = np.max(image, axis=2) - np.min(image, axis=2)
                vibrancy = np.mean(rgb_range) / 255.0
                return min(vibrancy * 1.5, 1.0)
                
        except:
                
            self.logger.error(f"ìƒ‰ìƒ ìƒë™ê° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _analyze_white_balance:
    
        """í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë¶„ì„"""
        try:
            # RGB ì±„ë„ ê°„ ê· í˜• ë¶„ì„
            r_mean = np.mean(image[:, :, 0])
            g_mean = np.mean(image[:, :, 1])
            b_mean = np.mean(image[:, :, 2])
            
            # ì´ìƒì ì¸ í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ì—ì„œëŠ” RGBê°€ ë¹„ìŠ·í•´ì•¼ í•¨
            rgb_means = np.array([r_mean, g_mean, b_mean])
            overall_mean = np.mean(rgb_means)
            
            if:
            
                deviations = np.abs(rgb_means - overall_mean) / overall_mean
                balance_score = max(0, 1.0 - np.mean(deviations))
            else:
                balance_score = 0.5
            
            return balance_score
            
        except:
            
            self.logger.error(f"í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.6

# ==============================================
# ğŸ”¥ ë©”ì¸ QualityAssessmentStep í´ë˜ìŠ¤ (í†µí•© ë²„ì „)
# ==============================================

class QualityAssessmentStep:

    """
    ğŸ”¥ 8ë‹¨ê³„: ì™„ì „ í†µí•© í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
    âœ… BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… ê¸°ì¡´ íŒŒì¼ì˜ ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨
    âœ… ModelLoaderì™€ ì‹¤ì œ ì—°ë™ë˜ëŠ” AI ëª¨ë¸ ì¶”ë¡ 
    âœ… Pipeline Manager 100% í˜¸í™˜
    âœ… M3 Max ìµœì í™” + conda í™˜ê²½ ìµœì í™”
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í’ˆì§ˆ ê°€ì¤‘ì¹˜ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
    CLOTHING_QUALITY_WEIGHTS = {
        'shirt': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'dress': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'pants': {'fitting': 0.6, 'texture': 0.2, 'edge': 0.1, 'color': 0.1},
        'jacket': {'fitting': 0.3, 'texture': 0.4, 'edge': 0.2, 'color': 0.1},
        'skirt': {'fitting': 0.5, 'texture': 0.2, 'edge': 0.2, 'color': 0.1},
        'top': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1},
        'default': {'fitting': 0.4, 'texture': 0.3, 'edge': 0.2, 'color': 0.1}
    }
    
    # ì›ë‹¨ íƒ€ì…ë³„ í’ˆì§ˆ ê¸°ì¤€ (ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
    FABRIC_QUALITY_STANDARDS = {
        'cotton': {'texture_importance': 0.8, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.3},
        'silk': {'texture_importance': 0.9, 'drape_importance': 0.9, 'wrinkle_tolerance': 0.2},
        'wool': {'texture_importance': 0.7, 'drape_importance': 0.7, 'wrinkle_tolerance': 0.4},
        'polyester': {'texture_importance': 0.5, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.8},
        'denim': {'texture_importance': 0.9, 'drape_importance': 0.4, 'wrinkle_tolerance': 0.6},
        'leather': {'texture_importance': 0.95, 'drape_importance': 0.3, 'wrinkle_tolerance': 0.9},
        'default': {'texture_importance': 0.7, 'drape_importance': 0.6, 'wrinkle_tolerance': 0.5}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°"""
        
        # ğŸ”¥ BaseStepMixin ì´ˆê¸°í™” - logger ì†ì„± ìë™ ì„¤ì •
        super().__init__(device=device, config=config, **kwargs)
        
        # í’ˆì§ˆ í‰ê°€ ì„¤ì •
        self.assessment_config = {
            'mode': self.config.get('assessment_mode', 'comprehensive'),
            'technical_analysis_enabled': self.config.get('technical_analysis_enabled', True),
            'perceptual_analysis_enabled': self.config.get('perceptual_analysis_enabled', True),
            'aesthetic_analysis_enabled': self.config.get('aesthetic_analysis_enabled', True),
            'functional_analysis_enabled': self.config.get('functional_analysis_enabled', True),
            'detailed_analysis_enabled': self.config.get('detailed_analysis_enabled', True),
            'neural_analysis_enabled': self.config.get('neural_analysis_enabled', True),
            'ai_models_enabled': TORCH_AVAILABLE,
            'quality_threshold': 0.7,
            'save_intermediate_results': False
        }
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        if:
            self.assessment_config.update(config)
        
        # M3 Max ìµœì í™” ì„¤ì •
        self._setup_m3_max_optimization()
        
        # ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
        self._initialize_professional_analyzers()
        
        # AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._initialize_enhanced_ai_models()
        
        # í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self._setup_comprehensive_assessment_pipeline()
        
        self.logger.info(f"âœ… {self.step_name} í†µí•© ì´ˆê¸°í™” ì™„ë£Œ - logger ì†ì„± ë¬¸ì œ í•´ê²°ë¨")
    
    def _setup_m3_max_optimization:
    
        """M3 Max ìµœì í™” ì„¤ì • (í–¥ìƒëœ ë²„ì „)"""
        if:
            try:
                # M3 Max íŠ¹í™” ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                if:
                    torch.backends.mps.empty_cache()
                
                # 128GB ë©”ëª¨ë¦¬ í™œìš©ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì„¤ì •
                self.batch_size = 16  # M3 Max 128GBì— ìµœì í™”
                self.use_mixed_precision = True
                self.parallel_analysis = True
                
                self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì™„ë£Œ (128GB + í–¥ìƒëœ ë¶„ì„)")
            except:
                self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
        else:
            self.batch_size = 8
            self.use_mixed_precision = False
            self.parallel_analysis = False
    
    def _initialize_professional_analyzers:
    
        """ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”"""
        try:
            # 1. í–¥ìƒëœ ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ê¸°
            self.technical_analyzer = TechnicalQualityAnalyzer(self.device)
            
            # 2. í”¼íŒ… í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°
            self.fitting_analyzer = FittingQualityAnalyzer(self.device)
            
            # 3. ìƒ‰ìƒ í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ê¸°
            self.color_analyzer = ColorQualityAnalyzer(self.device)
            
            self.logger.info("ğŸ”§ ì „ë¬¸ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ì „ë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_enhanced_ai_models:
    
        """í–¥ìƒëœ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™” - ModelLoaderì™€ ì—°ë™"""
        self.ai_models = {}
        
        if:
        
            self.logger.warning("âš ï¸ AI ëª¨ë¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
        
        try:
            # ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹œë„
            if:
                self._load_models_via_interface()
            
            # í´ë°±: ì§ì ‘ ëª¨ë¸ ìƒì„±
            self._create_fallback_models()
            
            # M3 Max ìµœì í™”
            self._optimize_models_for_m3_max()
            
            self.logger.info(f"ğŸ§  í–¥ìƒëœ AI ëª¨ë¸ {len(self.ai_models)}ê°œ ë¡œë“œ ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ í–¥ìƒëœ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ai_models = {}
    
    def _load_models_via_interface:
    
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            # ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©ì€ ë‚˜ì¤‘ì— ì²˜ë¦¬
            self.pending_model_loads = [
                'enhanced_perceptual_quality',
                'enhanced_aesthetic_quality',
                'quality_assessment_combined'
            ]
            self.logger.info("ğŸ“‹ ModelLoader ì¸í„°í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì˜ˆì•½ ì™„ë£Œ")
        except:
            self.logger.warning(f"âš ï¸ ModelLoader ì¸í„°í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _create_fallback_models:
    
        """í´ë°± ëª¨ë¸ë“¤ ì§ì ‘ ìƒì„±"""
        try:
            # í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if:
                self.ai_models['enhanced_perceptual'] = EnhancedPerceptualQualityModel()
                self.ai_models['enhanced_perceptual'].to(self.device)
                self.ai_models['enhanced_perceptual'].eval()
            
            # í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            if:
                self.ai_models['enhanced_aesthetic'] = EnhancedAestheticQualityModel()
                self.ai_models['enhanced_aesthetic'].to(self.device)
                self.ai_models['enhanced_aesthetic'].eval()
            
        except:
            
            self.logger.error(f"í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _optimize_models_for_m3_max:
    
        """M3 Maxìš© ëª¨ë¸ ìµœì í™”"""
        if:
            try:
                for model_name, model in self.ai_models.items():
                    if:
                        model.half() if self.device != "cpu" else self
                    # M3 Max Neural Engine ìµœì í™” ì„¤ì •
                    if:
                        model.eval()
                
                self.logger.info("ğŸ AI ëª¨ë¸ë“¤ M3 Max ìµœì í™” ì™„ë£Œ")
            except:
                self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _setup_comprehensive_assessment_pipeline:
    
        """ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # í‰ê°€ ìˆœì„œ ì •ì˜ (ë” ì„¸ë°€í•œ ë‹¨ê³„ë“¤)
        self.assessment_pipeline = []
        
        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
        self.assessment_pipeline.append(('preprocessing', self._preprocess_for_assessment))
        self.assessment_pipeline.append(('validation', self._validate_inputs))
        
        # 2. ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ (í–¥ìƒëœ ë²„ì „)
        if:
            self.assessment_pipeline.append(('technical_analysis', self._analyze_technical_quality_comprehensive))
        
        # 3. ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ (AI + ì „í†µì  ë°©ë²•)
        if:
            self.assessment_pipeline.append(('perceptual_analysis', self._analyze_perceptual_quality_enhanced))
        
        # 4. ë¯¸ì  í’ˆì§ˆ ë¶„ì„ (AI + ì „í†µì  ë°©ë²•)
        if:
            self.assessment_pipeline.append(('aesthetic_analysis', self._analyze_aesthetic_quality_enhanced))
        
        # 5. ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ (ì˜ë¥˜ íŠ¹í™”)
        if:
            self.assessment_pipeline.append(('functional_analysis', self._analyze_functional_quality_comprehensive))
        
        # 6. ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ (ì „ë¬¸ ë¶„ì„)
        self.assessment_pipeline.append(('color_analysis', self._analyze_color_quality_professional))
        
        # 7. ì¢…í•© í‰ê°€ ë° ì ìˆ˜ ê³„ì‚°
        self.assessment_pipeline.append(('final_assessment', self._calculate_comprehensive_final_score))
        
        self.logger.info(f"ğŸ“‹ ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ ({len(self.assessment_pipeline)}ë‹¨ê³„)")
    
    # =================================================================
    # ğŸ”¥ í–¥ìƒëœ ë¶„ì„ ë©”ì„œë“œë“¤
    # =================================================================
    
    def _preprocess_for_assessment:
    
        """í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)"""
        try:
            # ê¸°ë³¸ ì „ì²˜ë¦¬ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì™€ ë™ì¼
            result = super()._preprocess_for_assessment(data)
            
            # ì¶”ê°€ ì „ì²˜ë¦¬
            if result.get('preprocessing_successful'):
                # ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ ì „ì²˜ë¦¬
                processed_img = data.get('processed_image')
                if processed_img is not None:
                    # ë…¸ì´ì¦ˆ ì œê±° ë° ì„ ëª…í™” (ì„ íƒì )
                    if:
                        processed_img = self._enhance_image_for_assessment(processed_img)
                        result['enhanced_image'] = processed_img
            
            return result
            
        except:
            
            self.logger.error(f"âŒ í–¥ìƒëœ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'preprocessing_successful': False, 'error': str(e)}
    
    def _validate_inputs:
    
        """ì…ë ¥ ê²€ì¦"""
        try:
            validation_results = {
                'validation_successful': True,
                'warnings': [],
                'recommendations': []
            }
            
            processed_img = data.get('processed_image')
            original_img = data.get('original_image')
            
            # 1. ì´ë¯¸ì§€ í•´ìƒë„ ê²€ì¦
            if:
                h, w = processed_img.shape[:2]
                if:
                    validation_results['warnings'].append("ë‚®ì€ í•´ìƒë„ë¡œ ì¸í•´ ë¶„ì„ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                    validation_results['recommendations'].append("ìµœì†Œ 512x512 í•´ìƒë„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
                
                if:
                
                    validation_results['warnings'].append("ë§¤ìš° ë†’ì€ í•´ìƒë„ë¡œ ì¸í•´ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # 2. ì›ë³¸ ì´ë¯¸ì§€ ìœ ë¬´ í™•ì¸
            if:
                validation_results['warnings'].append("ì›ë³¸ ì´ë¯¸ì§€ê°€ ì—†ì–´ ë¹„êµ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤")
                validation_results['recommendations'].append("ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì›ë³¸ ì´ë¯¸ì§€ ì œê³µì„ ê¶Œì¥í•©ë‹ˆë‹¤")
            
            return validation_results
            
        except:
            
            self.logger.error(f"âŒ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'validation_successful': False, 'error': str(e)}
    
    def _analyze_technical_quality_comprehensive:
    
        """ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.technical_analyzer.analyze_comprehensive(processed_img)
            
            # ì¶”ê°€ ë©”íŠ¸ë¦­ë“¤
            results.update({
                'image_entropy': self._calculate_image_entropy(processed_img),
                'compression_artifacts': self._detect_compression_artifacts(processed_img),
                'blur_detection': self._detect_blur(processed_img)
            })
            
            self.logger.info(f"ğŸ“Š ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'technical_analysis_successful': True,
                'technical_results': results
            }
            
        except:
            
            self.logger.error(f"âŒ ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'technical_analysis_successful': False, 'error': str(e)}
    
    async def _analyze_perceptual_quality_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            
            results = {}
            
            # 1. í–¥ìƒëœ AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
            if:
                try:
                    ai_results = await self._run_enhanced_perceptual_model(processed_img)
                    results.update(ai_results)
                except:
                    self.logger.warning(f"âš ï¸ í–¥ìƒëœ ì§€ê°ì  AI ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # 2. ì „í†µì  ë°©ë²•ë“¤
            if original_img is not None:
                # SSIM, PSNR ë“±
                traditional_results = self._calculate_traditional_perceptual_metrics(processed_img, original_img)
                results.update(traditional_results)
            
            # 3. ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­
            no_ref_results = self._calculate_no_reference_quality_metrics(processed_img)
            results.update(no_ref_results)
            
            self.logger.info(f"ğŸ‘ í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'perceptual_analysis_successful': True,
                'perceptual_results': results
            }
            
        except:
            
            self.logger.error(f"âŒ í–¥ìƒëœ ì§€ê°ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'perceptual_analysis_successful': False, 'error': str(e)}
    
    async def _analyze_aesthetic_quality_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            
            results = {}
            
            # 1. í–¥ìƒëœ AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
            if:
                try:
                    ai_results = await self._run_enhanced_aesthetic_model(processed_img)
                    results.update(ai_results)
                except:
                    self.logger.warning(f"âš ï¸ í–¥ìƒëœ ë¯¸ì  AI ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # 2. ì „í†µì  ë¯¸ì  ë¶„ì„
            traditional_results = self._calculate_traditional_aesthetic_metrics(processed_img)
            results.update(traditional_results)
            
            # 3. ê³ ê¸‰ êµ¬ë„ ë¶„ì„
            advanced_composition = self._analyze_advanced_composition(processed_img)
            results.update(advanced_composition)
            
            self.logger.info(f"ğŸ¨ í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'aesthetic_analysis_successful': True,
                'aesthetic_results': results
            }
            
        except:
            
            self.logger.error(f"âŒ í–¥ìƒëœ ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'aesthetic_analysis_successful': False, 'error': str(e)}
    
    def _analyze_functional_quality_comprehensive:
    
        """ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            clothing_type = data.get('clothing_type', 'default')
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.fitting_analyzer.analyze_comprehensive(processed_img, original_img, clothing_type)
            
            self.logger.info(f"âš™ï¸ ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'functional_analysis_successful': True,
                'functional_results': results
            }
            
        except:
            
            self.logger.error(f"âŒ ì¢…í•© ê¸°ëŠ¥ì  í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'functional_analysis_successful': False, 'error': str(e)}
    
    def _analyze_color_quality_professional:
    
        """ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            processed_img = data['processed_image']
            original_img = data.get('original_image')
            
            # ì „ë¬¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
            results = self.color_analyzer.analyze_comprehensive(processed_img, original_img)
            
            self.logger.info(f"ğŸŒˆ ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'color_analysis_successful': True,
                'color_results': results
            }
            
        except:
            
            self.logger.error(f"âŒ ì „ë¬¸ ìƒ‰ìƒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'color_analysis_successful': False, 'error': str(e)}
    
    def _calculate_comprehensive_final_score:
    
        """ì¢…í•© ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
        try:
            # ëª¨ë“  ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
            technical_results = data.get('technical_results', {})
            perceptual_results = data.get('perceptual_results', {})
            aesthetic_results = data.get('aesthetic_results', {})
            functional_results = data.get('functional_results', {})
            color_results = data.get('color_results', {})
            
            # QualityMetrics ê°ì²´ ìƒì„±
            metrics = QualityMetrics()
            
            # ê¸°ìˆ ì  í’ˆì§ˆ ë§¤í•‘
            metrics.sharpness = technical_results.get('sharpness', 0.5)
            metrics.noise_level = technical_results.get('noise_level', 0.5)
            metrics.contrast = technical_results.get('contrast', 0.5)
            metrics.brightness = technical_results.get('brightness', 0.5)
            metrics.saturation = technical_results.get('saturation', 0.5)
            
            # ì§€ê°ì  í’ˆì§ˆ ë§¤í•‘
            metrics.structural_similarity = perceptual_results.get('ssim_score', perceptual_results.get('structural_similarity', 0.5))
            metrics.perceptual_similarity = perceptual_results.get('perceptual_similarity', 0.5)
            metrics.visual_quality = perceptual_results.get('visual_quality', 0.5)
            metrics.artifact_level = perceptual_results.get('artifact_level', 0.5)
            
            # ë¯¸ì  í’ˆì§ˆ ë§¤í•‘
            metrics.composition = aesthetic_results.get('composition', 0.5)
            metrics.color_harmony = color_results.get('color_harmony', aesthetic_results.get('color_harmony', 0.5))
            metrics.symmetry = aesthetic_results.get('symmetry', 0.5)
            metrics.balance = aesthetic_results.get('balance', 0.5)
            
            # ê¸°ëŠ¥ì  í’ˆì§ˆ ë§¤í•‘
            metrics.fitting_quality = functional_results.get('fitting_accuracy', functional_results.get('fitting_quality', 0.5))
            metrics.edge_preservation = functional_results.get('edge_preservation', 0.5)
            metrics.texture_quality = functional_results.get('texture_preservation', functional_results.get('texture_quality', 0.5))
            metrics.detail_preservation = functional_results.get('detail_preservation', 0.5)
            
            # ìƒ‰ìƒ í’ˆì§ˆ ë§¤í•‘
            metrics.color_accuracy = color_results.get('color_accuracy', 0.5)
            
            # ì˜ë¥˜/ì›ë‹¨ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            clothing_type = data.get('clothing_type', 'default')
            fabric_type = data.get('fabric_type', 'default')
            
            clothing_weights = self.CLOTHING_QUALITY_WEIGHTS.get(clothing_type, self.CLOTHING_QUALITY_WEIGHTS['default'])
            fabric_standards = self.FABRIC_QUALITY_STANDARDS.get(fabric_type, self.FABRIC_QUALITY_STANDARDS['default'])
            
            # ê°€ì¤‘ì¹˜ ì¡°í•©
            combined_weights = {
                'technical': 0.25 * fabric_standards['texture_importance'],
                'perceptual': 0.3,
                'aesthetic': 0.2,
                'functional': 0.25 * clothing_weights['fitting']
            }
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            metrics.calculate_overall_score(combined_weights)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            metrics.confidence = self._calculate_enhanced_confidence(data)
            
            # ë“±ê¸‰ ê²°ì •
            grade = metrics.get_grade()
            
            self.logger.info(f"ğŸ¯ ì¢…í•© ìµœì¢… í‰ê°€ ì™„ë£Œ (ì ìˆ˜: {metrics.overall_score:.3f}, ë“±ê¸‰: {grade.value})")
            
            return {
                'final_assessment_successful': True,
                'quality_metrics': metrics,
                'overall_score': metrics.overall_score,
                'confidence': metrics.confidence,
                'grade': grade.value,
                'grade_description': self._get_grade_description(grade)
            }
            
        except:
            
            self.logger.error(f"âŒ ì¢…í•© ìµœì¢… ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'final_assessment_successful': False, 'error': str(e)}
    
    # =================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (í–¥ìƒëœ ë²„ì „)
    # =================================================================
    
    def _enhance_image_for_assessment:
    
        """í‰ê°€ ì „ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            if:
                return image
            
            # 1. ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.bilateralFilter(image, 9, 75, 75)
            
            # 2. ì„ ëª…í™”
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(denoised, -1, kernel)
            
            # 3. ì›ë³¸ê³¼ ë¸”ë Œë”© (ê³¼ë„í•œ ì²˜ë¦¬ ë°©ì§€)
            enhanced = cv2.addWeighted(image, 0.7, sharpened, 0.3, 0)
            
            return enhanced
            
        except:
            
            self.logger.warning(f"ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_image_entropy:
    
        """ì´ë¯¸ì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])
            hist, _ = np.histogram(gray, bins=256, range=(0, 256))
            hist = hist / hist.sum()
            entropy = -np.sum(hist * np.log2(hist + 1e-8))
            return entropy / 8.0  # ì •ê·œí™”
        except:
            return 0.5
    
    def _detect_compression_artifacts:
    
        """ì••ì¶• ì•„í‹°íŒ©íŠ¸ ê°ì§€"""
        try:
            if:
                return 0.3
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # DCT ê¸°ë°˜ ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê°ì§€
            artifacts = 0.0
            for i in range(0, gray.shape[0]-8, 8):
                for j in range(0, gray.shape[1]-8, 8):
                    block = gray[i:i+8, j:j+8]
                    if block.shape == (8, 8):
                        # ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ë¶ˆì—°ì†ì„±
                        h_diff = np.mean(np.abs(np.diff(block, axis=1)))
                        v_diff = np.mean(np.abs(np.diff(block, axis=0)))
                        if:
                            artifacts += 0.01
            
            return min(artifacts, 1.0)
        except:
            return 0.3
    
    def _detect_blur:
    
        """ë¸”ëŸ¬ ê°ì§€"""
        try:
            if:
                return 0.3
            
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ë¸”ëŸ¬ ì •ë„ (ë‚®ì„ìˆ˜ë¡ ë¸”ëŸ¬ë¨)
            blur_score = min(laplacian_var / 1000.0, 1.0)
            return 1.0 - blur_score  # ë¸”ëŸ¬ ê°ì§€ëŠ” ë†’ì„ìˆ˜ë¡ ë¸”ëŸ¬ë¨
        except:
            return 0.3
    
    async def _run_enhanced_perceptual_model(self, image: np.ndarray) -> Dict[str, float]:
        """í–¥ìƒëœ ì§€ê°ì  ëª¨ë¸ ì‹¤í–‰"""
        try:
            model = self.ai_models['enhanced_perceptual']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            tensor_img = self._image_to_tensor(image)
            
            with torch.no_grad():
                if:
                    with autocast('cpu'):
                        results = model(tensor_img)
                else:
                    results = model(tensor_img)
            
            # ê²°ê³¼ ì²˜ë¦¬
            return {
                'perceptual_overall': float(results['overall'].cpu().squeeze()),
                'ai_sharpness': float(results['sharpness'].cpu().squeeze()),
                'ai_artifacts': float(results['artifacts'].cpu().squeeze())
            }
            
        except:
            
            self.logger.error(f"í–¥ìƒëœ ì§€ê°ì  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _run_enhanced_aesthetic_model(self, image: np.ndarray) -> Dict[str, float]:
        """í–¥ìƒëœ ë¯¸ì  ëª¨ë¸ ì‹¤í–‰"""
        try:
            model = self.ai_models['enhanced_aesthetic']
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            tensor_img = self._image_to_tensor(image)
            
            with torch.no_grad():
                if:
                    with autocast('cpu'):
                        results = model(tensor_img)
                else:
                    results = model(tensor_img)
            
            # ê²°ê³¼ ì²˜ë¦¬
            return {
                'ai_composition': float(results['composition'].cpu().squeeze()),
                'ai_color_harmony': float(results['color_harmony'].cpu().squeeze()),
                'ai_symmetry': float(results['symmetry'].cpu().squeeze()),
                'ai_balance': float(results['balance'].cpu().squeeze())
            }
            
        except:
            
            self.logger.error(f"í–¥ìƒëœ ë¯¸ì  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_traditional_perceptual_metrics:
    
        """ì „í†µì  ì§€ê°ì  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # í¬ê¸° ë§ì¶”ê¸°
            if:
                original_img = cv2.resize(original_img, (processed_img.shape[1], processed_img.shape[0])) if CV2_AVAILABLE else original_img
            
            # SSIM
            if:
                ssim_score = ssim(original_img, processed_img, multichannel=True, channel_axis=2)
                results['ssim_score'] = max(0, ssim_score)
            
            # PSNR
            mse = np.mean((original_img.astype(float) - processed_img.astype(float)) ** 2)
            if:
                psnr = 20 * np.log10(255.0 / np.sqrt(mse))
                results['psnr_score'] = min(1.0, max(0.0, (psnr - 20) / 30))  # 20-50 ë²”ìœ„ ì •ê·œí™”
            else:
                results['psnr_score'] = 1.0
            
            return results
            
        except:
            
            self.logger.error(f"ì „í†µì  ì§€ê°ì  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_no_reference_quality_metrics:
    
        """ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # 1. BRISQUE ìŠ¤íƒ€ì¼ ë©”íŠ¸ë¦­ (ê°„ì†Œí™”)
            if:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ í†µê³„
                mu = np.mean(gray)
                sigma = np.std(gray)
                
                # ì •ê·œí™”ëœ ì´ë¯¸ì§€
                normalized = (gray - mu) / (sigma + 1e-8)
                
                # ì™œê³¡ ì¸¡ì •
                alpha = np.mean(normalized**4) - 3  # ì²¨ë„
                beta = np.mean(normalized**3)       # ì™œë„
                
                # ì ìˆ˜ ê³„ì‚° (ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ì¼ìˆ˜ë¡ 0ì— ê°€ê¹Œì›€)
                naturalness = max(0, 1.0 - (abs(alpha) + abs(beta)) / 2.0)
                results['naturalness'] = naturalness
            
            # 2. ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€
            color_naturalness = self._assess_color_naturalness(image)
            results['color_naturalness'] = color_naturalness
            
            return results
            
        except:
            
            self.logger.error(f"ë¬´ì°¸ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_traditional_aesthetic_metrics:
    
        """ì „í†µì  ë¯¸ì  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            results = {}
            
            # 1. 3ë¶„í•  ë²•ì¹™
            rule_of_thirds = self._evaluate_rule_of_thirds(image)
            results['rule_of_thirds'] = rule_of_thirds
            
            # 2. ìƒ‰ìƒ ë¶„í¬
            color_distribution = self._evaluate_color_distribution(image)
            results['color_distribution'] = color_distribution
            
            # 3. ì‹œê°ì  ë³µì¡ì„±
            visual_complexity = self._calculate_visual_complexity(image)
            results['visual_complexity'] = visual_complexity
            
            return results
            
        except:
            
            self.logger.error(f"ì „í†µì  ë¯¸ì  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_advanced_composition:
    
        """ê³ ê¸‰ êµ¬ë„ ë¶„ì„"""
        try:
            results = {}
            
            # 1. í™©ê¸ˆë¹„ ë¶„ì„
            golden_ratio = self._evaluate_golden_ratio(image)
            results['golden_ratio'] = golden_ratio
            
            # 2. ëŒ€ê°ì„  êµ¬ë„
            diagonal_composition = self._evaluate_diagonal_composition(image)
            results['diagonal_composition'] = diagonal_composition
            
            # 3. í”„ë ˆì´ë°
            framing_quality = self._evaluate_framing(image)
            results['framing_quality'] = framing_quality
            
            return results
            
        except:
            
            self.logger.error(f"ê³ ê¸‰ êµ¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_enhanced_confidence:
    
        """í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence_factors = []
            
            # 1. ë¶„ì„ ì„±ê³µë¥ 
            success_count = sum([
                data.get('technical_analysis_successful', False),
                data.get('perceptual_analysis_successful', False),
                data.get('aesthetic_analysis_successful', False),
                data.get('functional_analysis_successful', False),
                data.get('color_analysis_successful', False)
            ])
            success_rate = success_count / 5.0
            confidence_factors.append(success_rate)
            
            # 2. AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            ai_usage = len(self.ai_models) / 2.0  # ìµœëŒ€ 2ê°œ ëª¨ë¸
            confidence_factors.append(min(ai_usage, 1.0))
            
            # 3. ë°ì´í„° í’ˆì§ˆ
            has_original = data.get('original_image') is not None
            confidence_factors.append(0.9 if has_original else 0.6)
            
            # 4. ì´ë¯¸ì§€ í’ˆì§ˆ
            validation_warnings = len(data.get('warnings', []))
            image_quality = max(0.3, 1.0 - validation_warnings * 0.2)
            confidence_factors.append(image_quality)
            
            # 5. ë¶„ì„ ì¼ê´€ì„±
            consistency = self._calculate_analysis_consistency(data)
            confidence_factors.append(consistency)
            
            return np.mean(confidence_factors)
            
        except:
            
            self.logger.error(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _calculate_analysis_consistency:
    
        """ë¶„ì„ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ì„œë¡œ ë‹¤ë¥¸ ë¶„ì„ ë°©ë²•ë“¤ì˜ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            consistency_checks = []
            
            # ê¸°ìˆ ì  vs ì§€ê°ì  ë¶„ì„ ì¼ê´€ì„±
            tech_sharpness = data.get('technical_results', {}).get('sharpness', 0.5)
            perc_quality = data.get('perceptual_results', {}).get('visual_quality', 0.5)
            if:
                consistency_checks.append(1.0)
            else:
                consistency_checks.append(0.5)
            
            # ìƒ‰ìƒ ë¶„ì„ ì¼ê´€ì„±
            color_harmony_1 = data.get('aesthetic_results', {}).get('color_harmony', 0.5)
            color_harmony_2 = data.get('color_results', {}).get('color_harmony', 0.5)
            if:
                consistency_checks.append(1.0)
            else:
                consistency_checks.append(0.7)
            
            return np.mean(consistency_checks) if consistency_checks else 0.7
            
        except:
            
            self.logger.error(f"ë¶„ì„ ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    # =================================================================
    # ğŸ”§ ìƒˆë¡œìš´ ë¶„ì„ ë©”ì„œë“œë“¤
    # =================================================================
    
    def _assess_color_naturalness:
    
        """ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
        try:
            # í”¼ë¶€ìƒ‰, í•˜ëŠ˜ìƒ‰, ë…¹ìƒ‰ ë“± ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ìƒ ë²”ìœ„ì™€ ë¹„êµ
            natural_score = 0.7  # ê¸°ë³¸ê°’
            
            if:
            
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                
                # ê³¼ë„í•œ ì±„ë„ë‚˜ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ìƒ ê°ì§€
                saturation = hsv[:, :, 1] / 255.0
                oversaturated_ratio = np.sum(saturation > 0.9) / saturation.size
                
                if:
                
                    natural_score = 0.9
                elif oversaturated_ratio < 0.3:
                    natural_score = 0.7
                else:
                    natural_score = 0.5
            
            return natural_score
            
        except:
            
            return 0.7
    
    def _evaluate_rule_of_thirds:
    
        """3ë¶„í•  ë²•ì¹™ í‰ê°€"""
        try:
            h, w = image.shape[:2]
            
            # 3ë¶„í•  ì§€ì ë“¤
            third_lines = {
                'h1': h // 3,
                'h2': 2 * h // 3,
                'w1': w // 3,
                'w2': 2 * w // 3
            }
            
            if:
            
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # 3ë¶„í•  ì§€ì  ê·¼ì²˜ì˜ ê´€ì‹¬ ì˜ì—­ ë°€ë„
                score = 0.0
                for line_name, line_pos in third_lines.items():
                    if 'h' in line_name:
                        # ìˆ˜í‰ì„ 
                        region = edges[max(0, line_pos-10):min(h, line_pos+10), :]
                    else:
                        # ìˆ˜ì§ì„ 
                        region = edges[:, max(0, line_pos-10):min(w, line_pos+10)]
                    
                    if:
                    
                        density = np.sum(region > 0) / region.size
                        score += density
                
                return min(score / 4.0, 1.0)
            
            return 0.6
            
        except:
            
            return 0.6
    
    def _evaluate_color_distribution:
    
        """ìƒ‰ìƒ ë¶„í¬ í‰ê°€"""
        try:
            # ìƒ‰ìƒì˜ ê· í˜•ì  ë¶„í¬ í‰ê°€
            if:
                pixels = image.reshape(-1, 3)
                
                # ìƒ˜í”Œë§
                if:
                    indices = np.random.choice(len(pixels), 10000, replace=False)
                    pixels = pixels[indices]
                
                # ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§
                kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                labels = kmeans.fit_predict(pixels)
                
                # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
                unique, counts = np.unique(labels, return_counts=True)
                proportions = counts / np.sum(counts)
                
                # ê· ë“±í•œ ë¶„í¬ì¼ìˆ˜ë¡ ì¢‹ì€ ì ìˆ˜
                entropy_score = entropy(proportions)
                max_entropy = np.log(len(unique))
                
                if:
                
                    distribution_score = entropy_score / max_entropy
                else:
                    distribution_score = 1.0
                
                return distribution_score
            
            return 0.6
            
        except:
            
            return 0.6
    
    def _calculate_visual_complexity:
    
        """ì‹œê°ì  ë³µì¡ì„± ê³„ì‚°"""
        try:
            if:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # ì—£ì§€ ë°€ë„
                edges = cv2.Canny(gray, 50, 150)
                edge_density = np.sum(edges > 0) / edges.size
                
                # í…ìŠ¤ì²˜ ë³µì¡ì„± (LBP ì—”íŠ¸ë¡œí”¼)
                complexity_score = edge_density
                
                if:
                
                    lbp = feature.local_binary_pattern(gray, 8, 1, method='uniform')
                    hist, _ = np.histogram(lbp.ravel(), bins=10)
                    hist = hist.astype(float) / hist.sum()
                    lbp_entropy = -np.sum(hist * np.log2(hist + 1e-8))
                    
                    complexity_score = (edge_density + lbp_entropy / 8.0) / 2
                
                # ì ë‹¹í•œ ë³µì¡ì„±ì´ ì¢‹ìŒ (ë„ˆë¬´ ë‹¨ìˆœí•˜ì§€ë„ ë³µì¡í•˜ì§€ë„ ì•Šê²Œ)
                optimal_complexity = 0.3
                return max(0, 1.0 - abs(complexity_score - optimal_complexity) / optimal_complexity)
            
            return 0.6
            
        except:
            
            return 0.6
    
    def _evaluate_golden_ratio:
    
        """í™©ê¸ˆë¹„ í‰ê°€"""
        try:
            h, w = image.shape[:2]
            
            # í™©ê¸ˆë¹„ ì§€ì ë“¤ (1.618)
            golden_ratio = 1.618
            golden_h = int(h / golden_ratio)
            golden_w = int(w / golden_ratio)
            
            if:
            
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                
                # í™©ê¸ˆë¹„ ì§€ì  ê·¼ì²˜ì˜ ê´€ì‹¬ ì˜ì—­
                score = 0.0
                points = [
                    (golden_w, golden_h),
                    (w - golden_w, golden_h),
                    (golden_w, h - golden_h),
                    (w - golden_w, h - golden_h)
                ]
                
                for x, y in points:
                    if:
                        region = edges[max(0, y-20):min(h, y+20), max(0, x-20):min(w, x+20)]
                        if:
                            density = np.sum(region > 0) / region.size
                            score += density
                
                return min(score / len(points), 1.0)
            
            return 0.6
            
        except:
            
            return 0.6
    
    def _evaluate_diagonal_composition:
    
        """ëŒ€ê°ì„  êµ¬ë„ í‰ê°€"""
        try:
            if:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                
                # Hough ë³€í™˜ìœ¼ë¡œ ì§ì„  ê°ì§€
                edges = cv2.Canny(gray, 50, 150)
                lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)
                
                if:
                
                    diagonal_score = 0.0
                    
                    for line in lines:
                        rho, theta = line[0]
                        angle = np.degrees(theta)
                        
                        # ëŒ€ê°ì„  ê°ë„ (45ë„, 135ë„ ê·¼ì²˜)
                        if:
                            diagonal_score += 1.0
                    
                    # ì •ê·œí™”
                    return min(diagonal_score / 10.0, 1.0)
                
            return 0.5
            
        except:
            
            return 0.5
    
    def _evaluate_framing:
    
        """í”„ë ˆì´ë° í’ˆì§ˆ í‰ê°€"""
        try:
            h, w = image.shape[:2]
            
            # ì´ë¯¸ì§€ ê²½ê³„ ê·¼ì²˜ì˜ ì–´ë‘ìš´ ì˜ì—­ (ìì—°ìŠ¤ëŸ¬ìš´ í”„ë ˆì´ë°)
            border_width = min(h, w) // 20
            
            # ê²½ê³„ ì˜ì—­ ì¶”ì¶œ
            top_border = image[:border_width, :]
            bottom_border = image[-border_width:, :]
            left_border = image[:, :border_width]
            right_border = image[:, -border_width:]
            
            # ê²½ê³„ì˜ í‰ê·  ë°ê¸°
            borders = [top_border, bottom_border, left_border, right_border]
            border_brightness = [np.mean(border) for border in borders if border.size > 0]
            
            if:
            
                avg_border_brightness = np.mean(border_brightness)
                center_brightness = np.mean(image[h//4:3*h//4, w//4:3*w//4])
                
                # ìì—°ìŠ¤ëŸ¬ìš´ ë¹„ë„¤íŒ… íš¨ê³¼ (ê²½ê³„ê°€ ì•½ê°„ ì–´ë‘ìš´ ê²ƒ)
                if:
                    brightness_ratio = avg_border_brightness / (center_brightness + 1e-8)
                    framing_score = min(brightness_ratio * 1.5, 1.0)
                else:
                    framing_score = 0.7
                
                return framing_score
            
            return 0.6
            
        except:
            
            return 0.6
    
    # =================================================================
    # ğŸ”§ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (BaseStepMixin ë°ì½”ë ˆì´í„° ì‚¬ìš©)
    # =================================================================
    
    @ensure_step_initialization
    @safe_step_method
    @performance_monitor("quality_assessment_comprehensive")
    async def process(
        self,
        fitted_image: Union[np.ndarray, str, Path],
        person_image: Optional[Union[np.ndarray, str, Path]] = None,
        clothing_image: Optional[Union[np.ndarray, str, Path]] = None,
        fabric_type: str = "default",
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜ - í†µí•© ë²„ì „
        âœ… BaseStepMixin ë°ì½”ë ˆì´í„°ë¡œ ì•ˆì „ì„± ë³´ì¥
        âœ… logger ì†ì„± ìë™ í™•ì¸
        âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìë™ ì ìš©
        âœ… ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨
        """
        
        start_time = time.time()
        
        try:
        
            self.logger.info(f"ğŸ¯ {self.step_name} í†µí•© í’ˆì§ˆ í‰ê°€ ì‹œì‘")
            
            # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            fitted_img = self._load_and_validate_image(fitted_image, "fitted_image")
            if:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ fitted_imageì…ë‹ˆë‹¤")
            
            person_img = self._load_and_validate_image(person_image, "person_image") if person_image is not None else None
            clothing_img = self._load_and_validate_image(clothing_image, "clothing_image") if clothing_image is not None else None
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            assessment_data = {
                'processed_image': fitted_img,
                'original_image': person_img,
                'clothing_image': clothing_img,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                **kwargs
            }
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if:
                self._optimize_m3_max_memory()
            
            # 4. ì¢…í•© í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            for stage_name, stage_func in self.assessment_pipeline:
                self.logger.info(f"ğŸ”„ {stage_name} ì‹¤í–‰ ì¤‘...")
                
                stage_start = time.time()
                
                if:
                
                    stage_result = await stage_func(assessment_data)
                else:
                    stage_result = stage_func(assessment_data)
                
                stage_duration = time.time() - stage_start
                
                # ê²°ê³¼ ë³‘í•©
                assessment_data.update(stage_result)
                
                self.logger.info(f"âœ… {stage_name} ì™„ë£Œ ({stage_duration:.2f}ì´ˆ)")
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            quality_metrics = assessment_data.get('quality_metrics')
            
            if:
            
                raise ValueError("í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
            
            result = {
                'success': True,
                'step_name': self.step_name,
                'overall_score': quality_metrics.overall_score,
                'confidence': quality_metrics.confidence,
                'grade': assessment_data.get('grade', 'acceptable'),
                'grade_description': assessment_data.get('grade_description', 'ìˆ˜ìš© ê°€ëŠ¥í•œ í’ˆì§ˆ'),
                
                # ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
                'detailed_scores': {
                    'technical': assessment_data.get('technical_results', {}),
                    'perceptual': assessment_data.get('perceptual_results', {}),
                    'aesthetic': assessment_data.get('aesthetic_results', {}),
                    'functional': assessment_data.get('functional_results', {}),
                    'color': assessment_data.get('color_results', {})
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì „ì²´
                'quality_metrics': asdict(quality_metrics),
                
                # ë©”íƒ€ë°ì´í„°
                'processing_time': processing_time,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'assessment_mode': self.assessment_config['mode'],
                
                # ì‹œìŠ¤í…œ ì •ë³´
                'device_info': {
                    'device': self.device,
                    'is_m3_max': getattr(self, 'is_m3_max', False),
                    'ai_models_used': len(self.ai_models),
                    'optimization_enabled': getattr(self, 'optimization_enabled', False)
                },
                
                # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
                'warnings': assessment_data.get('warnings', []),
                'recommendations': assessment_data.get('recommendations', [])
            }
            
            self.logger.info(f"âœ… {self.step_name} í†µí•© í‰ê°€ ì™„ë£Œ - í’ˆì§ˆ ì ìˆ˜: {result['overall_score']:.3f} ({processing_time:.2f}ì´ˆ)")
            
            return result
            
        except:
            
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ {self.step_name} í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'step_name': self.step_name,
                'error': str(e),
                'error_type': type(e).__name__,
                'processing_time': processing_time,
                'metadata': {
                    'device': self.device,
                    'pipeline_stages': len(self.assessment_pipeline) if hasattr(self, 'assessment_pipeline') else 0,
                    'error_location': 'comprehensive_quality_assessment'
                }
            }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (í†µí•© ë²„ì „)
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'QualityAssessmentStep',
    
    # ë°ì´í„° êµ¬ì¡°
    'QualityMetrics',
    'QualityGrade',
    'AssessmentMode',
    'QualityAspect',
    
    # AI ëª¨ë¸ë“¤
    'EnhancedPerceptualQualityModel',
    'EnhancedAestheticQualityModel',
    
    # ì „ë¬¸ ë¶„ì„ê¸°ë“¤
    'TechnicalQualityAnalyzer',
    'FittingQualityAnalyzer',
    'ColorQualityAnalyzer'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
logger = logging.getLogger(__name__)
logger.info("âœ… QualityAssessmentStep í†µí•© ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— BaseStepMixin ìƒì†ìœ¼ë¡œ logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°")
logger.info("ğŸ§  ëª¨ë“  ì„¸ë¶€ ë¶„ì„ ê¸°ëŠ¥ í†µí•©")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ“¦ conda í™˜ê²½ ìµœì í™” ì™„ë£Œ")

# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_comprehensive_quality_assessment:
    """í†µí•© í’ˆì§ˆ í‰ê°€ ìŠ¤í… í…ŒìŠ¤íŠ¸"""
    try:
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        step = QualityAssessmentStep()
        
        # logger ì†ì„± í™•ì¸
        assert hasattr(step, 'logger'), "logger ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!"
        assert step.logger is not None, "loggerê°€ Noneì…ë‹ˆë‹¤!"
        
        # ê¸°ë³¸ ë©”ì„œë“œ í™•ì¸
        assert hasattr(step, 'process'), "process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'cleanup_resources'), "cleanup_resources ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤!"
        
        # ì „ë¬¸ ë¶„ì„ê¸° í™•ì¸
        assert hasattr(step, 'technical_analyzer'), "technical_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'fitting_analyzer'), "fitting_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        assert hasattr(step, 'color_analyzer'), "color_analyzerê°€ ì—†ìŠµë‹ˆë‹¤!"
        
        # Step ì •ë³´ í™•ì¸
        step_info = step.get_step_info()
        assert 'step_name' in step_info, "step_nameì´ step_infoì— ì—†ìŠµë‹ˆë‹¤!"
        
        print("âœ… í†µí•© QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"ğŸ“Š Step ì •ë³´: {step_info}")
        print(f"ğŸ§  AI ëª¨ë¸ ìˆ˜: {len(step.ai_models)}")
        print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {len(step.assessment_pipeline)}")
        
        return True
        
    except:
        
        print(f"âŒ í†µí•© QualityAssessmentStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if:

    print("ğŸ§ª í†µí•© Quality Assessment Step í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    test_comprehensive_quality_assessment()