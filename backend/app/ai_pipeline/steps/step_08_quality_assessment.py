"""
ğŸ¯ ì™„ì „íˆ ì‘ë™í•˜ëŠ” 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment)
ì‹¤ì œ ë©”íŠ¸ë¦­ ê³„ì‚° + ìë™ ê°œì„  ì œì•ˆ + ìƒì„¸ ë¶„ì„
"""
import os
import time
import logging
import asyncio
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageStat
import json
from dataclasses import dataclass, asdict
from enum import Enum
import base64
import io
from scipy import stats
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# PyTorchëŠ” ì„ íƒì  ì‚¬ìš©
try:
    import torch
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‹¤í–‰")
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

class QualityGrade(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    VERY_POOR = "very_poor"

@dataclass
class QualityMetrics:
    """ì‹¤ì œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    overall_score: float
    perceptual_quality: float
    technical_quality: float
    aesthetic_quality: float
    fit_accuracy: float
    color_harmony: float
    detail_preservation: float
    edge_quality: float
    lighting_consistency: float
    artifact_level: float
    face_preservation: float
    
    def to_dict(self) -> Dict[str, float]:
        return asdict(self)
    
    def get_grade(self) -> QualityGrade:
        """ë“±ê¸‰ ê³„ì‚°"""
        if self.overall_score >= 0.9:
            return QualityGrade.EXCELLENT
        elif self.overall_score >= 0.75:
            return QualityGrade.GOOD
        elif self.overall_score >= 0.6:
            return QualityGrade.FAIR
        elif self.overall_score >= 0.4:
            return QualityGrade.POOR
        else:
            return QualityGrade.VERY_POOR

class RealQualityAssessmentStep:
    """
    ğŸ¯ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
    
    íŠ¹ì§•:
    - ì‹¤ì œ SSIM, PSNR, MSE ê³„ì‚°
    - ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ í’ˆì§ˆ ë©”íŠ¸ë¦­
    - ìë™ ê°œì„  ì œì•ˆ ìƒì„±
    - ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸
    - ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€
    - ìƒ‰ìƒ ì¡°í™” ë¶„ì„
    """
    
    def __init__(self, device: str = 'cpu', config: Dict[str, Any] = None):
        """
        Args:
            device: ë””ë°”ì´ìŠ¤ ('cpu', 'mps', 'cuda')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.device = device
        self.config = config or {}
        
        # í‰ê°€ ì„¤ì •
        self.enable_advanced_metrics = self.config.get('enable_advanced_metrics', True)
        self.enable_face_detection = self.config.get('enable_face_detection', True)
        self.enable_detailed_analysis = self.config.get('enable_detailed_analysis', True)
        
        # í’ˆì§ˆ ì„ê³„ê°’
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.75,
            'fair': 0.6,
            'poor': 0.4
        }
        
        # ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜
        self.metric_weights = {
            'perceptual_quality': 0.25,
            'technical_quality': 0.20,
            'aesthetic_quality': 0.15,
            'fit_accuracy': 0.15,
            'color_harmony': 0.10,
            'detail_preservation': 0.10,
            'face_preservation': 0.05
        }
        
        # ì–¼êµ´ ê²€ì¶œê¸°
        self.face_detector = None
        
        # PyTorch ìµœì í™”
        self.use_torch = TORCH_AVAILABLE and self.config.get('use_torch', True)
        
        self.is_initialized = False
        
        logger.info(f"ğŸ“Š ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}")
    
    async def initialize(self) -> bool:
        """ì‹¤ì œ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ë¡œë”© ì¤‘...")
            
            # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
            if self.enable_face_detection:
                await self._initialize_face_detector()
            
            self.is_initialized = True
            logger.info("âœ… í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _initialize_face_detector(self):
        """ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”"""
        try:
            # OpenCV Haar ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_detector = cv2.CascadeClassifier(cascade_path)
            
            if self.face_detector.empty():
                logger.warning("âš ï¸ ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì‹¤íŒ¨")
                self.face_detector = None
            else:
                logger.info("âœ… ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.face_detector = None
    
    async def process(
        self,
        fitted_image: Union[np.ndarray, Image.Image, str],
        original_person: Union[np.ndarray, Image.Image, str],
        original_clothing: Union[np.ndarray, Image.Image, str],
        pipeline_results: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì²˜ë¦¬
        
        Args:
            fitted_image: ìµœì¢… ê°€ìƒ í”¼íŒ… ê²°ê³¼
            original_person: ì›ë³¸ ì‚¬ìš©ì ì´ë¯¸ì§€
            original_clothing: ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€
            pipeline_results: íŒŒì´í”„ë¼ì¸ ì¤‘ê°„ ê²°ê³¼ (ì„ íƒì )
            
        Returns:
            ì¢…í•© í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        if not self.is_initialized:
            raise RuntimeError("í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # 1. ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            fitted_np = await self._prepare_image(fitted_image)
            person_np = await self._prepare_image(original_person)
            clothing_np = await self._prepare_image(original_clothing)
            
            logger.info("ğŸ“Š ì¢…í•© í’ˆì§ˆ í‰ê°€ ì‹œì‘...")
            
            # 2. ì§€ê°ì  í’ˆì§ˆ í‰ê°€ (SSIM, PSNR ë“±)
            logger.info("ğŸ‘ï¸ ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            perceptual_score = await self._evaluate_perceptual_quality(fitted_np, person_np)
            
            # 3. ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ (ì„ ëª…ë„, ë…¸ì´ì¦ˆ ë“±)
            logger.info("ğŸ”§ ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            technical_score = await self._evaluate_technical_quality(fitted_np)
            
            # 4. ë¯¸ì  í’ˆì§ˆ í‰ê°€ (ìƒ‰ìƒ ì¡°í™”, êµ¬ì„± ë“±)
            logger.info("ğŸ¨ ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì¤‘...")
            aesthetic_score = await self._evaluate_aesthetic_quality(fitted_np, person_np, clothing_np)
            
            # 5. í• ì •í™•ë„ í‰ê°€
            logger.info("ğŸ‘• í• ì •í™•ë„ í‰ê°€ ì¤‘...")
            fit_score = await self._evaluate_fit_accuracy(fitted_np, person_np, pipeline_results)
            
            # 6. ìƒ‰ìƒ ì¡°í™” í‰ê°€
            logger.info("ğŸŒˆ ìƒ‰ìƒ ì¡°í™” í‰ê°€ ì¤‘...")
            color_harmony = await self._evaluate_color_harmony(fitted_np, person_np, clothing_np)
            
            # 7. ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€
            logger.info("ğŸ” ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€ ì¤‘...")
            detail_preservation = await self._evaluate_detail_preservation(fitted_np, clothing_np)
            
            # 8. ì—£ì§€ í’ˆì§ˆ í‰ê°€
            logger.info("ğŸ“ ì—£ì§€ í’ˆì§ˆ í‰ê°€ ì¤‘...")
            edge_quality = await self._evaluate_edge_quality(fitted_np)
            
            # 9. ì¡°ëª… ì¼ê´€ì„± í‰ê°€
            logger.info("ğŸ’¡ ì¡°ëª… ì¼ê´€ì„± í‰ê°€ ì¤‘...")
            lighting_consistency = await self._evaluate_lighting_consistency(fitted_np, person_np)
            
            # 10. ì•„í‹°íŒ©íŠ¸ ë ˆë²¨ í‰ê°€
            logger.info("ğŸ” ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì¤‘...")
            artifact_level = await self._evaluate_artifacts(fitted_np)
            
            # 11. ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€
            logger.info("ğŸ˜Š ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€ ì¤‘...")
            face_preservation = await self._evaluate_face_preservation(fitted_np, person_np)
            
            # 12. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            logger.info("ğŸ“ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘...")
            overall_score = self._calculate_overall_score({
                'perceptual_quality': perceptual_score,
                'technical_quality': technical_score,
                'aesthetic_quality': aesthetic_score,
                'fit_accuracy': fit_score,
                'color_harmony': color_harmony,
                'detail_preservation': detail_preservation,
                'face_preservation': face_preservation
            })
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
            quality_metrics = QualityMetrics(
                overall_score=overall_score,
                perceptual_quality=perceptual_score,
                technical_quality=technical_score,
                aesthetic_quality=aesthetic_score,
                fit_accuracy=fit_score,
                color_harmony=color_harmony,
                detail_preservation=detail_preservation,
                edge_quality=edge_quality,
                lighting_consistency=lighting_consistency,
                artifact_level=artifact_level,
                face_preservation=face_preservation
            )
            
            # 13. ê°œì„  ì œì•ˆ ìƒì„±
            logger.info("ğŸ’¡ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘...")
            improvement_suggestions = await self._generate_improvement_suggestions(quality_metrics)
            
            # 14. ìƒì„¸ ë¶„ì„ (ì„ íƒì )
            detailed_analysis = None
            if self.enable_detailed_analysis:
                logger.info("ğŸ” ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
                detailed_analysis = await self._perform_detailed_analysis(
                    fitted_np, person_np, clothing_np, quality_metrics
                )
            
            processing_time = time.time() - start_time
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "quality_metrics": quality_metrics.to_dict(),
                "quality_grade": quality_metrics.get_grade().value,
                "letter_grade": self._get_letter_grade(overall_score),
                "score_percentage": round(overall_score * 100, 1),
                
                # ê°œì„  ì œì•ˆ
                "improvement_suggestions": improvement_suggestions,
                
                # ìƒì„¸ ë¶„ì„
                "detailed_analysis": detailed_analysis,
                
                # ê°œë³„ ë©”íŠ¸ë¦­ ìƒì„¸
                "metric_breakdown": {
                    "perceptual": {
                        "score": perceptual_score,
                        "description": "ì§€ê°ì  í’ˆì§ˆ (SSIM, ì‹œê°ì  ìœ ì‚¬ë„)",
                        "status": self._get_metric_status(perceptual_score)
                    },
                    "technical": {
                        "score": technical_score,
                        "description": "ê¸°ìˆ ì  í’ˆì§ˆ (ì„ ëª…ë„, ë…¸ì´ì¦ˆ)",
                        "status": self._get_metric_status(technical_score)
                    },
                    "aesthetic": {
                        "score": aesthetic_score,
                        "description": "ë¯¸ì  í’ˆì§ˆ (êµ¬ì„±, ì‹œê°ì  ë§¤ë ¥)",
                        "status": self._get_metric_status(aesthetic_score)
                    },
                    "fit": {
                        "score": fit_score,
                        "description": "ì°©ìš©ê° ì •í™•ë„",
                        "status": self._get_metric_status(fit_score)
                    },
                    "color_harmony": {
                        "score": color_harmony,
                        "description": "ìƒ‰ìƒ ì¡°í™”",
                        "status": self._get_metric_status(color_harmony)
                    },
                    "detail_preservation": {
                        "score": detail_preservation,
                        "description": "ë””í…Œì¼ ë³´ì¡´ë„",
                        "status": self._get_metric_status(detail_preservation)
                    }
                },
                
                # ì²˜ë¦¬ ì •ë³´
                "assessment_info": {
                    "processing_time": processing_time,
                    "evaluation_method": "comprehensive_multi_metric",
                    "metrics_computed": 11,
                    "device_used": self.device,
                    "face_detection_enabled": self.face_detector is not None,
                    "advanced_metrics_enabled": self.enable_advanced_metrics
                }
            }
            
            logger.info(
                f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - "
                f"ì¢…í•©ì ìˆ˜: {overall_score:.3f} ({quality_metrics.get_grade().value}), "
                f"ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}ì´ˆ"
            )
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time
            }
    
    async def _prepare_image(self, image: Union[np.ndarray, Image.Image, str]) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        if isinstance(image, str):
            # Base64 ë¬¸ìì—´ ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
            if image.startswith('data:image') or len(image) > 100:
                # Base64 ë””ì½”ë”©
                if 'base64,' in image:
                    image_data = image.split('base64,')[1]
                else:
                    image_data = image
                
                img_bytes = base64.b64decode(image_data)
                img = Image.open(io.BytesIO(img_bytes))
                return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            else:
                # íŒŒì¼ ê²½ë¡œ
                return cv2.imread(image)
                
        elif isinstance(image, Image.Image):
            return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
        elif isinstance(image, np.ndarray):
            return image
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
    
    async def _evaluate_perceptual_quality(self, fitted: np.ndarray, reference: np.ndarray) -> float:
        """ì§€ê°ì  í’ˆì§ˆ í‰ê°€ (SSIM ê¸°ë°˜)"""
        
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted.shape != reference.shape:
                reference = cv2.resize(reference, (fitted.shape[1], fitted.shape[0]))
            
            # SSIM ê³„ì‚°
            ssim_score = self._calculate_ssim(fitted, reference)
            
            # PSNR ê³„ì‚°
            psnr_score = self._calculate_psnr(fitted, reference)
            psnr_normalized = min(1.0, psnr_score / 40.0)  # 40dBë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
            
            # MSE ê¸°ë°˜ ìœ ì‚¬ë„
            mse = mean_squared_error(
                fitted.flatten(), 
                reference.flatten()
            )
            mse_score = 1.0 / (1.0 + mse / 1000.0)  # MSEë¥¼ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
            
            # ì¢…í•© ì§€ê°ì  í’ˆì§ˆ
            perceptual_quality = (ssim_score * 0.5 + psnr_normalized * 0.3 + mse_score * 0.2)
            
            return max(0.0, min(1.0, perceptual_quality))
            
        except Exception as e:
            logger.warning(f"ì§€ê°ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_technical_quality(self, image: np.ndarray) -> float:
        """ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€"""
        
        try:
            # 1. ì„ ëª…ë„ í‰ê°€ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(1.0, laplacian_var / 500.0)
            
            # 2. ë…¸ì´ì¦ˆ ë ˆë²¨ í‰ê°€
            noise_score = self._evaluate_noise_level(image)
            
            # 3. ëŒ€ë¹„ í‰ê°€
            contrast_score = self._evaluate_contrast(image)
            
            # 4. ë°ê¸° ì ì ˆì„±
            brightness_score = self._evaluate_brightness(image)
            
            # 5. ìƒ‰ìƒ í¬í™”ë„
            saturation_score = self._evaluate_saturation(image)
            
            # ì¢…í•© ê¸°ìˆ ì  í’ˆì§ˆ
            technical_quality = (
                sharpness_score * 0.3 +
                (1.0 - noise_score) * 0.2 +  # ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
                contrast_score * 0.2 +
                brightness_score * 0.15 +
                saturation_score * 0.15
            )
            
            return max(0.0, min(1.0, technical_quality))
            
        except Exception as e:
            logger.warning(f"ê¸°ìˆ ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_aesthetic_quality(
        self, 
        fitted: np.ndarray, 
        person: np.ndarray, 
        clothing: np.ndarray
    ) -> float:
        """ë¯¸ì  í’ˆì§ˆ í‰ê°€"""
        
        try:
            # 1. ìƒ‰ìƒ ë‹¤ì–‘ì„±
            color_diversity = self._evaluate_color_diversity(fitted)
            
            # 2. êµ¬ì„± í’ˆì§ˆ (ì‚¼ë“±ë¶„ë²•)
            composition_score = self._evaluate_composition(fitted)
            
            # 3. ì‹œê°ì  ê· í˜•
            visual_balance = self._evaluate_visual_balance(fitted)
            
            # 4. ì „ì²´ì ì¸ ë§¤ë ¥ë„
            appeal_score = self._evaluate_visual_appeal(fitted)
            
            # ì¢…í•© ë¯¸ì  í’ˆì§ˆ
            aesthetic_quality = (
                color_diversity * 0.25 +
                composition_score * 0.25 +
                visual_balance * 0.25 +
                appeal_score * 0.25
            )
            
            return max(0.0, min(1.0, aesthetic_quality))
            
        except Exception as e:
            logger.warning(f"ë¯¸ì  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_fit_accuracy(
        self, 
        fitted: np.ndarray, 
        person: np.ndarray, 
        pipeline_results: Optional[Dict[str, Any]]
    ) -> float:
        """í• ì •í™•ë„ í‰ê°€"""
        
        try:
            # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ í• ì •ë³´ ì¶”ì¶œ
            if pipeline_results:
                # í¬ì¦ˆ ì¼ê´€ì„± í™•ì¸
                pose_consistency = pipeline_results.get('pose_result', {}).get('pose_confidence', 0.8)
                
                # ì›Œí•‘ í’ˆì§ˆ í™•ì¸  
                warping_quality = pipeline_results.get('warping_result', {}).get('quality_metrics', {}).get('overall_quality', 0.8)
                
                # íŒŒì´í”„ë¼ì¸ ê¸°ë°˜ í• ì ìˆ˜
                pipeline_fit_score = (pose_consistency + warping_quality) / 2
            else:
                pipeline_fit_score = 0.8
            
            # ì‹œê°ì  í• ë¶„ì„
            visual_fit_score = self._analyze_visual_fit(fitted, person)
            
            # ì¢…í•© í• ì ìˆ˜
            fit_accuracy = (pipeline_fit_score * 0.6 + visual_fit_score * 0.4)
            
            return max(0.0, min(1.0, fit_accuracy))
            
        except Exception as e:
            logger.warning(f"í• ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_color_harmony(
        self, 
        fitted: np.ndarray, 
        person: np.ndarray, 
        clothing: np.ndarray
    ) -> float:
        """ìƒ‰ìƒ ì¡°í™” í‰ê°€"""
        
        try:
            # 1. HSV ìƒ‰ê³µê°„ì—ì„œ ë¶„ì„
            fitted_hsv = cv2.cvtColor(fitted, cv2.COLOR_BGR2HSV)
            person_hsv = cv2.cvtColor(person, cv2.COLOR_BGR2HSV)
            clothing_hsv = cv2.cvtColor(clothing, cv2.COLOR_BGR2HSV)
            
            # 2. ìƒ‰ì¡°(Hue) ì¡°í™” ë¶„ì„
            fitted_hue = np.mean(fitted_hsv[:, :, 0])
            person_hue = np.mean(person_hsv[:, :, 0])
            clothing_hue = np.mean(clothing_hsv[:, :, 0])
            
            # ìƒ‰ì¡° ì°¨ì´ ê³„ì‚° (ì›í˜• ê±°ë¦¬)
            hue_diff_person = min(abs(fitted_hue - person_hue), 180 - abs(fitted_hue - person_hue))
            hue_diff_clothing = min(abs(fitted_hue - clothing_hue), 180 - abs(fitted_hue - clothing_hue))
            
            hue_harmony = 1.0 - (hue_diff_person + hue_diff_clothing) / (2 * 90)  # 90ë„ë¥¼ ìµœëŒ€ ì°¨ì´ë¡œ
            hue_harmony = max(0.0, hue_harmony)
            
            # 3. ì±„ë„ ì¼ê´€ì„±
            fitted_sat = np.mean(fitted_hsv[:, :, 1])
            person_sat = np.mean(person_hsv[:, :, 1])
            
            sat_consistency = 1.0 - abs(fitted_sat - person_sat) / 255.0
            
            # 4. ëª…ë„ ì¼ê´€ì„±
            fitted_val = np.mean(fitted_hsv[:, :, 2])
            person_val = np.mean(person_hsv[:, :, 2])
            
            val_consistency = 1.0 - abs(fitted_val - person_val) / 255.0
            
            # ì¢…í•© ìƒ‰ìƒ ì¡°í™”
            color_harmony = (hue_harmony * 0.5 + sat_consistency * 0.3 + val_consistency * 0.2)
            
            return max(0.0, min(1.0, color_harmony))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì¡°í™” í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_detail_preservation(self, fitted: np.ndarray, clothing: np.ndarray) -> float:
        """ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€"""
        
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted.shape != clothing.shape:
                clothing = cv2.resize(clothing, (fitted.shape[1], fitted.shape[0]))
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¹„êµ
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_BGR2GRAY)
            clothing_gray = cv2.cvtColor(clothing, cv2.COLOR_BGR2GRAY)
            
            # ë¼í”Œë¼ì‹œì•ˆìœ¼ë¡œ ë””í…Œì¼ ì¶”ì¶œ
            fitted_detail = cv2.Laplacian(fitted_gray, cv2.CV_64F)
            clothing_detail = cv2.Laplacian(clothing_gray, cv2.CV_64F)
            
            # ë””í…Œì¼ ê°•ë„ ë¹„êµ
            fitted_detail_strength = np.std(fitted_detail)
            clothing_detail_strength = np.std(clothing_detail)
            
            if clothing_detail_strength > 0:
                preservation_ratio = fitted_detail_strength / clothing_detail_strength
                preservation_score = 1.0 - abs(preservation_ratio - 1.0)
            else:
                preservation_score = 0.5
            
            return max(0.0, min(1.0, preservation_score))
            
        except Exception as e:
            logger.warning(f"ë””í…Œì¼ ë³´ì¡´ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_edge_quality(self, image: np.ndarray) -> float:
        """ì—£ì§€ í’ˆì§ˆ í‰ê°€"""
        
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # Canny ì—£ì§€ ê²€ì¶œ
            edges = cv2.Canny(gray, 50, 150)
            
            # ìœ¤ê³½ì„  ì°¾ê¸°
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if not contours:
                return 0.5
            
            # ê°€ì¥ í° ìœ¤ê³½ì„ ë“¤ì˜ í’ˆì§ˆ í‰ê°€
            contour_areas = [cv2.contourArea(c) for c in contours]
            contour_areas.sort(reverse=True)
            
            # ìƒìœ„ ìœ¤ê³½ì„ ë“¤ì˜ í‰ê·  í¬ê¸°
            top_contours = contour_areas[:min(5, len(contour_areas))]
            avg_contour_size = np.mean(top_contours) if top_contours else 0
            
            # ì´ë¯¸ì§€ í¬ê¸° ëŒ€ë¹„ ì •ê·œí™”
            image_area = image.shape[0] * image.shape[1]
            normalized_size = avg_contour_size / image_area
            
            # ì ì ˆí•œ ìœ¤ê³½ì„  í¬ê¸° ë²”ìœ„ë¡œ í‰ê°€
            if 0.01 <= normalized_size <= 0.1:
                edge_quality = 1.0
            else:
                edge_quality = max(0.0, 1.0 - abs(normalized_size - 0.05) * 10)
            
            return edge_quality
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_lighting_consistency(self, fitted: np.ndarray, reference: np.ndarray) -> float:
        """ì¡°ëª… ì¼ê´€ì„± í‰ê°€"""
        
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted.shape != reference.shape:
                reference = cv2.resize(reference, (fitted.shape[1], fitted.shape[0]))
            
            # LAB ìƒ‰ê³µê°„ì—ì„œ ëª…ë„ ì±„ë„ ë¹„êµ
            fitted_lab = cv2.cvtColor(fitted, cv2.COLOR_BGR2LAB)
            ref_lab = cv2.cvtColor(reference, cv2.COLOR_BGR2LAB)
            
            fitted_l = fitted_lab[:, :, 0]
            ref_l = ref_lab[:, :, 0]
            
            # ëª…ë„ ë¶„í¬ ë¹„êµ
            fitted_mean = np.mean(fitted_l)
            ref_mean = np.mean(ref_l)
            
            fitted_std = np.std(fitted_l)
            ref_std = np.std(ref_l)
            
            # í‰ê·  ëª…ë„ ì°¨ì´
            mean_diff = abs(fitted_mean - ref_mean) / 255.0
            
            # ëª…ë„ ë¶„ì‚° ì°¨ì´
            std_diff = abs(fitted_std - ref_std) / (ref_std + 1e-6)
            std_diff = min(1.0, std_diff)
            
            # ì¡°ëª… ì¼ê´€ì„± ì ìˆ˜
            lighting_consistency = 1.0 - (mean_diff + std_diff) / 2.0
            
            return max(0.0, min(1.0, lighting_consistency))
            
        except Exception as e:
            logger.warning(f"ì¡°ëª… ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.7
    
    async def _evaluate_artifacts(self, image: np.ndarray) -> float:
        """ì•„í‹°íŒ©íŠ¸ í‰ê°€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # 1. ë¸”ë¡í‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            # DCT ê¸°ë°˜ ë¸”ë¡í‚¹ ê²€ì¶œ
            h, w = gray.shape
            block_size = 8
            
            blocking_score = 0.0
            block_count = 0
            
            for y in range(0, h - block_size, block_size):
                for x in range(0, w - block_size, block_size):
                    block = gray[y:y+block_size, x:x+block_size]
                    
                    # ë¸”ë¡ ê²½ê³„ì—ì„œì˜ ë¶ˆì—°ì†ì„± ê²€ì‚¬
                    if x + block_size < w:
                        right_diff = np.mean(np.abs(block[:, -1] - gray[y:y+block_size, x+block_size]))
                        blocking_score += right_diff
                        block_count += 1
                    
                    if y + block_size < h:
                        bottom_diff = np.mean(np.abs(block[-1, :] - gray[y+block_size, x:x+block_size]))
                        blocking_score += bottom_diff
                        block_count += 1
            
            if block_count > 0:
                blocking_score = blocking_score / block_count / 255.0
            
            # 2. ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ (ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ)
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            
            gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
            ringing_score = np.std(gradient_magnitude) / 255.0
            
            # ì¢…í•© ì•„í‹°íŒ©íŠ¸ ë ˆë²¨
            artifact_level = (blocking_score * 0.6 + ringing_score * 0.4)
            
            return max(0.0, min(1.0, artifact_level))
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.2
    
    async def _evaluate_face_preservation(self, fitted: np.ndarray, reference: np.ndarray) -> float:
        """ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€"""
        
        try:
            if self.face_detector is None:
                return 0.8  # ê¸°ë³¸ê°’
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_BGR2GRAY)
            ref_gray = cv2.cvtColor(reference, cv2.COLOR_BGR2GRAY)
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted.shape != reference.shape:
                ref_gray = cv2.resize(ref_gray, (fitted.shape[1], fitted.shape[0]))
                reference = cv2.resize(reference, (fitted.shape[1], fitted.shape[0]))
            
            # ì–¼êµ´ ê²€ì¶œ
            fitted_faces = self.face_detector.detectMultiScale(fitted_gray, 1.1, 4)
            ref_faces = self.face_detector.detectMultiScale(ref_gray, 1.1, 4)
            
            if len(fitted_faces) == 0 or len(ref_faces) == 0:
                return 0.8  # ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’
            
            # ê°€ì¥ í° ì–¼êµ´ ì˜ì—­ ë¹„êµ
            fitted_face = max(fitted_faces, key=lambda x: x[2] * x[3])
            ref_face = max(ref_faces, key=lambda x: x[2] * x[3])
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            fx, fy, fw, fh = fitted_face
            rx, ry, rw, rh = ref_face
            
            fitted_face_region = fitted[fy:fy+fh, fx:fx+fw]
            ref_face_region = reference[ry:ry+rh, rx:rx+rw]
            
            # í¬ê¸° ë§ì¶”ê¸°
            if fitted_face_region.shape != ref_face_region.shape:
                ref_face_region = cv2.resize(ref_face_region, (fw, fh))
            
            # ì–¼êµ´ ì˜ì—­ SSIM ê³„ì‚°
            face_ssim = self._calculate_ssim(fitted_face_region, ref_face_region)
            
            return face_ssim
            
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ë³´ì¡´ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        
        total_score = 0.0
        total_weight = 0.0
        
        for metric_name, weight in self.metric_weights.items():
            if metric_name in metrics:
                total_score += metrics[metric_name] * weight
                total_weight += weight
        
        if total_weight > 0:
            return total_score / total_weight
        else:
            return 0.7
    
    async def _generate_improvement_suggestions(self, metrics: QualityMetrics) -> List[Dict[str, Any]]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        # ì§€ê°ì  í’ˆì§ˆ ê°œì„ 
        if metrics.perceptual_quality < 0.7:
            suggestions.append({
                "category": "perceptual",
                "priority": "high",
                "issue": "ì‹œê°ì  í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤",
                "suggestion": "ë” ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í’ˆì§ˆ ì„¤ì •ì„ ë†’ì—¬ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ì§€ê°ì  í’ˆì§ˆ: {metrics.perceptual_quality:.2f}",
                "target_improvement": "0.8 ì´ìƒ"
            })
        
        # ê¸°ìˆ ì  í’ˆì§ˆ ê°œì„ 
        if metrics.technical_quality < 0.7:
            suggestions.append({
                "category": "technical",
                "priority": "high",
                "issue": "ì´ë¯¸ì§€ì˜ ì„ ëª…ë„ë‚˜ ë…¸ì´ì¦ˆ ë ˆë²¨ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤",
                "suggestion": "ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ì´¬ì˜í•˜ê±°ë‚˜ ë…¸ì´ì¦ˆ ì œê±° ì²˜ë¦¬ë¥¼ ì ìš©í•´ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ê¸°ìˆ ì  í’ˆì§ˆ: {metrics.technical_quality:.2f}",
                "target_improvement": "0.75 ì´ìƒ"
            })
        
        # í• ì •í™•ë„ ê°œì„ 
        if metrics.fit_accuracy < 0.6:
            suggestions.append({
                "category": "fit",
                "priority": "high",
                "issue": "ì˜ë¥˜ í•ì´ ë¶€ì •í™•í•©ë‹ˆë‹¤",
                "suggestion": "ì „ì‹ ì´ ì˜ ë³´ì´ëŠ” ì •ë©´ ì‚¬ì§„ì„ ì‚¬ìš©í•˜ê³ , ì •í™•í•œ ì‹ ì²´ ì¹˜ìˆ˜ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ í• ì •í™•ë„: {metrics.fit_accuracy:.2f}",
                "target_improvement": "0.7 ì´ìƒ"
            })
        
        # ìƒ‰ìƒ ì¡°í™” ê°œì„ 
        if metrics.color_harmony < 0.6:
            suggestions.append({
                "category": "color",
                "priority": "medium",
                "issue": "ìƒ‰ìƒ ì¡°í™”ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                "suggestion": "ìì—°ê´‘ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒ‰ìƒ ë³´ì •ì„ ì ìš©í•´ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ìƒ‰ìƒ ì¡°í™”: {metrics.color_harmony:.2f}",
                "target_improvement": "0.7 ì´ìƒ"
            })
        
        # ë””í…Œì¼ ë³´ì¡´ ê°œì„ 
        if metrics.detail_preservation < 0.5:
            suggestions.append({
                "category": "detail",
                "priority": "medium",
                "issue": "ì˜ë¥˜ ë””í…Œì¼ì´ ì†ì‹¤ë˜ì—ˆìŠµë‹ˆë‹¤",
                "suggestion": "ë” ì„ ëª…í•˜ê³  ê³ í•´ìƒë„ì˜ ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ë””í…Œì¼ ë³´ì¡´ë„: {metrics.detail_preservation:.2f}",
                "target_improvement": "0.6 ì´ìƒ"
            })
        
        # ì–¼êµ´ ë³´ì¡´ ê°œì„ 
        if metrics.face_preservation < 0.7:
            suggestions.append({
                "category": "face",
                "priority": "medium",
                "issue": "ì–¼êµ´ ì˜ì—­ì´ ë¶€ìì—°ìŠ¤ëŸ½ê²Œ ë³€í˜•ë˜ì—ˆìŠµë‹ˆë‹¤",
                "suggestion": "ì–¼êµ´ì´ ì •ë©´ì„ í–¥í•˜ê³  ëª…í™•í•˜ê²Œ ë³´ì´ëŠ” ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ì–¼êµ´ ë³´ì¡´ë„: {metrics.face_preservation:.2f}",
                "target_improvement": "0.8 ì´ìƒ"
            })
        
        # ì•„í‹°íŒ©íŠ¸ ê°œì„ 
        if metrics.artifact_level > 0.3:
            suggestions.append({
                "category": "artifact",
                "priority": "low",
                "issue": "ì´ë¯¸ì§€ì— ì¸ê³µì ì¸ í”ì ì´ ìˆìŠµë‹ˆë‹¤",
                "suggestion": "í’ˆì§ˆ ì„¤ì •ì„ ë†’ì´ê±°ë‚˜ ì²˜ë¦¬ ì‹œê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”",
                "technical_detail": f"í˜„ì¬ ì•„í‹°íŒ©íŠ¸ ë ˆë²¨: {metrics.artifact_level:.2f}",
                "target_improvement": "0.2 ì´í•˜"
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        priority_order = {"high": 0, "medium": 1, "low": 2}
        suggestions.sort(key=lambda x: priority_order[x["priority"]])
        
        # ì„±ê³µì ì¸ ê²½ìš° ê²©ë ¤ ë©”ì‹œì§€
        if not suggestions:
            suggestions.append({
                "category": "success",
                "priority": "info",
                "issue": "ìš°ìˆ˜í•œ í’ˆì§ˆì…ë‹ˆë‹¤!",
                "suggestion": "ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì¢‹ì€ ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ê°œì„ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.",
                "technical_detail": f"ì¢…í•© ì ìˆ˜: {metrics.overall_score:.2f}",
                "target_improvement": "í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€"
            })
        
        return suggestions
    
    async def _perform_detailed_analysis(
        self,
        fitted: np.ndarray,
        person: np.ndarray,
        clothing: np.ndarray,
        metrics: QualityMetrics
    ) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìˆ˜í–‰"""
        
        analysis = {}
        
        try:
            # 1. ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
            analysis["color_distribution"] = self._analyze_color_distribution(fitted)
            
            # 2. íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            analysis["histogram_analysis"] = self._analyze_histograms(fitted, person, clothing)
            
            # 3. í…ìŠ¤ì²˜ ë¶„ì„
            analysis["texture_analysis"] = self._analyze_texture_statistics(fitted)
            
            # 4. ì—£ì§€ ë¶„ì„
            analysis["edge_analysis"] = self._analyze_edge_statistics(fitted)
            
            # 5. í’ˆì§ˆ ì§„ë‹¨
            analysis["quality_diagnosis"] = self._diagnose_quality_issues(metrics)
            
            # 6. ê°œì„  ì ì¬ë ¥ ë¶„ì„
            analysis["improvement_potential"] = self._analyze_improvement_potential(metrics)
            
            return analysis
            
        except Exception as e:
            logger.warning(f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    # === í—¬í¼ ë©”ì†Œë“œë“¤ ===
    
    def _calculate_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """SSIM ê³„ì‚°"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if img1.shape != img2.shape:
                img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(img1.shape) == 3:
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            else:
                gray1 = img1
                gray2 = img2
            
            # SSIM ê³„ì‚°
            C1 = (0.01 * 255) ** 2
            C2 = (0.03 * 255) ** 2
            
            mu1 = cv2.GaussianBlur(gray1, (11, 11), 1.5)
            mu2 = cv2.GaussianBlur(gray2, (11, 11), 1.5)
            
            mu1_sq = mu1 * mu1
            mu2_sq = mu2 * mu2
            mu1_mu2 = mu1 * mu2
            
            sigma1_sq = cv2.GaussianBlur(gray1 * gray1, (11, 11), 1.5) - mu1_sq
            sigma2_sq = cv2.GaussianBlur(gray2 * gray2, (11, 11), 1.5) - mu2_sq
            sigma12 = cv2.GaussianBlur(gray1 * gray2, (11, 11), 1.5) - mu1_mu2
            
            ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
            
            return float(np.mean(ssim_map))
            
        except Exception as e:
            logger.warning(f"SSIM ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _calculate_psnr(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """PSNR ê³„ì‚°"""
        try:
            if img1.shape != img2.shape:
                img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
            
            mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2)
            
            if mse == 0:
                return 40.0  # ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°
            
            max_pixel = 255.0
            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
            
            return float(psnr)
            
        except Exception as e:
            logger.warning(f"PSNR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 30.0
    
    def _evaluate_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ í‰ê°€"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ë¯¸ë””ì•ˆ í•„í„°ì™€ ì›ë³¸ì˜ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
        median_filtered = cv2.medianBlur(gray, 5)
        noise = np.abs(gray.astype(float) - median_filtered.astype(float))
        
        noise_level = np.mean(noise) / 255.0
        return min(1.0, noise_level)
    
    def _evaluate_contrast(self, image: np.ndarray) -> float:
        """ëŒ€ë¹„ í‰ê°€"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        contrast = np.std(gray) / 255.0
        
        # ì ì ˆí•œ ëŒ€ë¹„ ë²”ìœ„ (0.1-0.3)ë¡œ í‰ê°€
        if 0.1 <= contrast <= 0.3:
            return 1.0
        else:
            return max(0.0, 1.0 - abs(contrast - 0.2) * 5)
    
    def _evaluate_brightness(self, image: np.ndarray) -> float:
        """ë°ê¸° ì ì ˆì„± í‰ê°€"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray) / 255.0
        
        # ì ì ˆí•œ ë°ê¸° ë²”ìœ„ (0.3-0.7)ë¡œ í‰ê°€
        if 0.3 <= brightness <= 0.7:
            return 1.0
        else:
            return max(0.0, 1.0 - abs(brightness - 0.5) * 2)
    
    def _evaluate_saturation(self, image: np.ndarray) -> float:
        """ì±„ë„ í‰ê°€"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        saturation = np.mean(hsv[:, :, 1]) / 255.0
        
        # ì ì ˆí•œ ì±„ë„ ë²”ìœ„ (0.2-0.6)ë¡œ í‰ê°€
        if 0.2 <= saturation <= 0.6:
            return 1.0
        else:
            return max(0.0, 1.0 - abs(saturation - 0.4) * 2.5)
    
    def _evaluate_color_diversity(self, image: np.ndarray) -> float:
        """ìƒ‰ìƒ ë‹¤ì–‘ì„± í‰ê°€"""
        # ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜í•˜ê³  ê³ ìœ  ìƒ‰ìƒ ìˆ˜ ê³„ì‚°
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚° ì†ë„ í–¥ìƒ
        small_img = cv2.resize(rgb_image, (50, 50))
        pixels = small_img.reshape(-1, 3)
        
        # ê³ ìœ  ìƒ‰ìƒ ìˆ˜
        unique_colors = len(np.unique(pixels, axis=0))
        
        # ì •ê·œí™” (ìµœëŒ€ 2500ê°œ ìƒ‰ìƒ ê¸°ì¤€)
        diversity = min(1.0, unique_colors / 1000)
        
        return diversity
    
    def _evaluate_composition(self, image: np.ndarray) -> float:
        """êµ¬ì„± í’ˆì§ˆ í‰ê°€ (ì‚¼ë“±ë¶„ë²•)"""
        h, w = image.shape[:2]
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(gray, 50, 150)
        
        # ì‚¼ë“±ë¶„ì„  ìœ„ì¹˜
        third_h1, third_h2 = h // 3, 2 * h // 3
        third_w1, third_w2 = w // 3, 2 * w // 3
        
        # ì‚¼ë“±ë¶„ì„  ê·¼ì²˜ì˜ ì—£ì§€ ë°€ë„
        edge_density = 0.0
        
        # ìˆ˜ì§ ì‚¼ë“±ë¶„ì„ 
        edge_density += np.sum(edges[:, max(0, third_w1-10):min(w, third_w1+10)])
        edge_density += np.sum(edges[:, max(0, third_w2-10):min(w, third_w2+10)])
        
        # ìˆ˜í‰ ì‚¼ë“±ë¶„ì„ 
        edge_density += np.sum(edges[max(0, third_h1-10):min(h, third_h1+10), :])
        edge_density += np.sum(edges[max(0, third_h2-10):min(h, third_h2+10), :])
        
        # ì •ê·œí™”
        composition_score = min(1.0, edge_density / (h * w * 0.01))
        
        return composition_score
    
    def _evaluate_visual_balance(self, image: np.ndarray) -> float:
        """ì‹œê°ì  ê· í˜• í‰ê°€"""
        h, w = image.shape[:2]
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì¢Œìš° ê· í˜•
        left_half = gray[:, :w//2]
        right_half = gray[:, w//2:]
        
        left_mean = np.mean(left_half)
        right_mean = np.mean(right_half)
        
        horizontal_balance = 1.0 - abs(left_mean - right_mean) / 255.0
        
        # ìƒí•˜ ê· í˜•
        top_half = gray[:h//2, :]
        bottom_half = gray[h//2:, :]
        
        top_mean = np.mean(top_half)
        bottom_mean = np.mean(bottom_half)
        
        vertical_balance = 1.0 - abs(top_mean - bottom_mean) / 255.0
        
        # ì¢…í•© ê· í˜•
        visual_balance = (horizontal_balance + vertical_balance) / 2
        
        return visual_balance
    
    def _evaluate_visual_appeal(self, image: np.ndarray) -> float:
        """ì‹œê°ì  ë§¤ë ¥ë„ í‰ê°€"""
        # ì—¬ëŸ¬ ìš”ì†Œì˜ ì¡°í•©
        contrast = self._evaluate_contrast(image)
        brightness = self._evaluate_brightness(image)
        saturation = self._evaluate_saturation(image)
        color_diversity = self._evaluate_color_diversity(image)
        
        # ì¢…í•© ë§¤ë ¥ë„
        appeal = (contrast * 0.3 + brightness * 0.2 + saturation * 0.3 + color_diversity * 0.2)
        
        return appeal
    
    def _analyze_visual_fit(self, fitted: np.ndarray, person: np.ndarray) -> float:
        """ì‹œê°ì  í• ë¶„ì„"""
        try:
            # ìœ¤ê³½ì„  ê¸°ë°˜ ë¶„ì„
            fitted_gray = cv2.cvtColor(fitted, cv2.COLOR_BGR2GRAY)
            person_gray = cv2.cvtColor(person, cv2.COLOR_BGR2GRAY)
            
            if fitted.shape != person.shape:
                person_gray = cv2.resize(person_gray, (fitted.shape[1], fitted.shape[0]))
            
            # ì—£ì§€ ê²€ì¶œ
            fitted_edges = cv2.Canny(fitted_gray, 50, 150)
            person_edges = cv2.Canny(person_gray, 50, 150)
            
            # ìœ¤ê³½ì„  ìœ ì‚¬ë„
            edge_diff = np.abs(fitted_edges.astype(float) - person_edges.astype(float))
            edge_similarity = 1.0 - np.mean(edge_diff) / 255.0
            
            return max(0.0, min(1.0, edge_similarity))
            
        except Exception as e:
            logger.warning(f"ì‹œê°ì  í• ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _analyze_color_distribution(self, image: np.ndarray) -> Dict[str, Any]:
        """ìƒ‰ìƒ ë¶„í¬ ë¶„ì„"""
        try:
            # RGB íˆìŠ¤í† ê·¸ë¨
            hist_r = cv2.calcHist([image], [2], None, [256], [0, 256])  # BGR ìˆœì„œ
            hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])
            hist_b = cv2.calcHist([image], [0], None, [256], [0, 256])
            
            return {
                "mean_rgb": [float(np.mean(image[:, :, 2])), float(np.mean(image[:, :, 1])), float(np.mean(image[:, :, 0]))],
                "std_rgb": [float(np.std(image[:, :, 2])), float(np.std(image[:, :, 1])), float(np.std(image[:, :, 0]))],
                "histogram_peaks": {
                    "red": int(np.argmax(hist_r)),
                    "green": int(np.argmax(hist_g)),
                    "blue": int(np.argmax(hist_b))
                }
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_histograms(self, fitted: np.ndarray, person: np.ndarray, clothing: np.ndarray) -> Dict[str, Any]:
        """íˆìŠ¤í† ê·¸ë¨ ë¶„ì„"""
        try:
            # ê° ì´ë¯¸ì§€ì˜ íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
            def calc_hist_similarity(img1, img2):
                if img1.shape != img2.shape:
                    img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
                
                hist1 = cv2.calcHist([img1], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
                hist2 = cv2.calcHist([img2], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
                
                return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            
            return {
                "person_similarity": float(calc_hist_similarity(fitted, person)),
                "clothing_similarity": float(calc_hist_similarity(fitted, clothing))
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_texture_statistics(self, image: np.ndarray) -> Dict[str, Any]:
        """í…ìŠ¤ì²˜ í†µê³„ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # í…ìŠ¤ì²˜ ë©”íŠ¸ë¦­
            contrast = float(np.std(gray))
            sharpness = float(cv2.Laplacian(gray, cv2.CV_64F).var())
            
            # LBP (Local Binary Pattern) ê°„ì†Œí™” ë²„ì „
            uniformity = float(np.sum(cv2.calcHist([gray], [0], None, [256], [0, 256])**2) / (gray.shape[0] * gray.shape[1])**2)
            
            return {
                "contrast": contrast,
                "sharpness": sharpness,
                "uniformity": uniformity
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_edge_statistics(self, image: np.ndarray) -> Dict[str, Any]:
        """ì—£ì§€ í†µê³„ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ë‹¤ì–‘í•œ ì—£ì§€ ê²€ì¶œ
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            canny_edges = cv2.Canny(gray, 50, 150)
            
            return {
                "sobel_x_mean": float(np.mean(np.abs(sobel_x))),
                "sobel_y_mean": float(np.mean(np.abs(sobel_y))),
                "edge_density": float(np.sum(canny_edges > 0) / (gray.shape[0] * gray.shape[1])),
                "edge_strength": float(np.mean(np.sqrt(sobel_x**2 + sobel_y**2)))
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _diagnose_quality_issues(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¬¸ì œ ì§„ë‹¨"""
        issues = []
        strengths = []
        
        metric_dict = metrics.to_dict()
        
        for metric_name, value in metric_dict.items():
            if metric_name == "overall_score":
                continue
                
            readable_name = metric_name.replace("_", " ").title()
            
            if value >= 0.8:
                strengths.append(f"{readable_name}: {value:.2f}")
            elif value < 0.6:
                issues.append(f"{readable_name}: {value:.2f}")
        
        return {
            "identified_issues": issues,
            "strengths": strengths,
            "critical_issues": len([i for i in metric_dict.values() if i < 0.5]),
            "excellent_metrics": len([i for i in metric_dict.values() if i >= 0.9])
        }
    
    def _analyze_improvement_potential(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """ê°œì„  ì ì¬ë ¥ ë¶„ì„"""
        metric_dict = metrics.to_dict()
        
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì™€ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì°¾ê¸°
        min_metric = min(metric_dict.items(), key=lambda x: x[1] if x[0] != "overall_score" else 1.0)
        max_metric = max(metric_dict.items(), key=lambda x: x[1] if x[0] != "overall_score" else 0.0)
        
        improvement_potential = max_metric[1] - min_metric[1]
        
        return {
            "weakest_metric": {
                "name": min_metric[0].replace("_", " ").title(),
                "score": min_metric[1],
                "improvement_needed": max(0, 0.8 - min_metric[1])
            },
            "strongest_metric": {
                "name": max_metric[0].replace("_", " ").title(),
                "score": max_metric[1]
            },
            "improvement_potential": improvement_potential,
            "recommended_focus": "overall" if improvement_potential < 0.2 else min_metric[0].replace("_", " ")
        }
    
    def _get_letter_grade(self, score: float) -> str:
        """ë¬¸ì ë“±ê¸‰ ë°˜í™˜"""
        if score >= 0.97:
            return "A+"
        elif score >= 0.93:
            return "A"
        elif score >= 0.90:
            return "A-"
        elif score >= 0.87:
            return "B+"
        elif score >= 0.83:
            return "B"
        elif score >= 0.80:
            return "B-"
        elif score >= 0.77:
            return "C+"
        elif score >= 0.73:
            return "C"
        elif score >= 0.70:
            return "C-"
        elif score >= 0.65:
            return "D+"
        elif score >= 0.60:
            return "D"
        elif score >= 0.50:
            return "D-"
        else:
            return "F"
    
    def _get_metric_status(self, score: float) -> str:
        """ë©”íŠ¸ë¦­ ìƒíƒœ ë°˜í™˜"""
        if score >= 0.9:
            return "excellent"
        elif score >= 0.75:
            return "good"
        elif score >= 0.6:
            return "fair"
        elif score >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "RealQualityAssessment",
            "version": "1.0",
            "device": self.device,
            "initialized": self.is_initialized,
            "torch_available": TORCH_AVAILABLE,
            "face_detection_enabled": self.face_detector is not None,
            "advanced_metrics_enabled": self.enable_advanced_metrics,
            "detailed_analysis_enabled": self.enable_detailed_analysis,
            "supported_metrics": list(QualityMetrics.__annotations__.keys()),
            "quality_thresholds": self.quality_thresholds,
            "metric_weights": self.metric_weights
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.face_detector = None
        self.is_initialized = False
        logger.info("ğŸ§¹ ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# === ì‚¬ìš© ì˜ˆì‹œ ===
async def test_real_quality_assessment():
    """ì‹¤ì œ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    quality_assessor = RealQualityAssessmentStep(
        device='cpu',  # ë˜ëŠ” 'mps' (M3 Max)
        config={
            'enable_advanced_metrics': True,
            'enable_face_detection': True,
            'enable_detailed_analysis': True,
            'use_torch': TORCH_AVAILABLE
        }
    )
    
    success = await quality_assessor.initialize()
    if not success:
        print("âŒ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ìš©)
    def create_test_image(color=(128, 128, 128), noise_level=0):
        img = np.full((400, 300, 3), color, dtype=np.uint8)
        if noise_level > 0:
            noise = np.random.randint(-noise_level, noise_level, img.shape, dtype=np.int16)
            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        return img
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤
    fitted_image = create_test_image((120, 150, 180), noise_level=10)  # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ
    person_image = create_test_image((130, 140, 160), noise_level=5)   # ì›ë³¸ ì‚¬ëŒ
    clothing_image = create_test_image((100, 160, 200), noise_level=0) # ê¹¨ë—í•œ ì˜ë¥˜
    
    # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
    pipeline_results = {
        'pose_result': {'pose_confidence': 0.85},
        'warping_result': {'quality_metrics': {'overall_quality': 0.78}},
        'parsing_result': {'confidence': 0.82}
    }
    
    # 3. í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
    result = await quality_assessor.process(
        fitted_image=fitted_image,
        original_person=person_image,
        original_clothing=clothing_image,
        pipeline_results=pipeline_results
    )
    
    if result["success"]:
        print(f"âœ… í’ˆì§ˆ í‰ê°€ ì„±ê³µ!")
        print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {result['score_percentage']}% ({result['letter_grade']})")
        print(f"ğŸ† í’ˆì§ˆ ë“±ê¸‰: {result['quality_grade']}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['assessment_info']['processing_time']:.3f}ì´ˆ")
        
        print("\nğŸ“ˆ ë©”íŠ¸ë¦­ ìƒì„¸:")
        for metric, details in result['metric_breakdown'].items():
            print(f"  {details['description']}: {details['score']:.3f} ({details['status']})")
        
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ ({len(result['improvement_suggestions'])}ê°œ):")
        for suggestion in result['improvement_suggestions'][:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            print(f"  [{suggestion['priority'].upper()}] {suggestion['issue']}")
            print(f"    â†’ {suggestion['suggestion']}")
        
        if result['detailed_analysis']:
            print(f"\nğŸ” ìƒì„¸ ë¶„ì„:")
            diagnosis = result['detailed_analysis'].get('quality_diagnosis', {})
            if diagnosis:
                print(f"  ê°•ì : {len(diagnosis.get('strengths', []))}ê°œ")
                print(f"  ê°œì„  í•„ìš”: {len(diagnosis.get('identified_issues', []))}ê°œ")
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open("quality_assessment_result.json", "w") as f:
            json.dump(result, f, indent=2, default=str)
        
        print("ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: quality_assessment_result.json")
        
    else:
        print(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {result['error']}")
    
    # 4. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await quality_assessor.cleanup()


# === ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜ ===
async def test_with_real_images():
    """ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸"""
    
    quality_assessor = RealQualityAssessmentStep(
        device='cpu',
        config={'enable_detailed_analysis': True}
    )
    
    await quality_assessor.initialize()
    
    # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë“¤
    fitted_path = "output/fitted_result.jpg"      # ê°€ìƒ í”¼íŒ… ê²°ê³¼
    person_path = "test_images/person.jpg"        # ì›ë³¸ ì‚¬ëŒ ì‚¬ì§„
    clothing_path = "test_images/clothing.jpg"    # ì›ë³¸ ì˜ë¥˜ ì‚¬ì§„
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if all(os.path.exists(path) for path in [fitted_path, person_path, clothing_path]):
        result = await quality_assessor.process(
            fitted_image=fitted_path,
            original_person=person_path,
            original_clothing=clothing_path
        )
        
        if result["success"]:
            print(f"ğŸ¯ ì‹¤ì œ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ!")
            print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {result['score_percentage']}%")
            print(f"ğŸ† ë“±ê¸‰: {result['quality_grade']} ({result['letter_grade']})")
            
            # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
            report = f"""
# ê°€ìƒ í”¼íŒ… í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸

## ì¢…í•© í‰ê°€
- **ì ìˆ˜**: {result['score_percentage']}% ({result['letter_grade']})
- **ë“±ê¸‰**: {result['quality_grade']}
- **ì²˜ë¦¬ ì‹œê°„**: {result['assessment_info']['processing_time']:.3f}ì´ˆ

## ë©”íŠ¸ë¦­ ìƒì„¸
"""
            for metric, details in result['metric_breakdown'].items():
                report += f"- **{details['description']}**: {details['score']:.3f} ({details['status']})\n"
            
            report += f"\n## ê°œì„  ì œì•ˆ\n"
            for i, suggestion in enumerate(result['improvement_suggestions'], 1):
                report += f"{i}. **[{suggestion['priority'].upper()}]** {suggestion['issue']}\n"
                report += f"   â†’ {suggestion['suggestion']}\n\n"
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            with open("quality_report.md", "w", encoding='utf-8') as f:
                f.write(report)
            
            print("ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: quality_report.md")
            
        else:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {result['error']}")
    else:
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("í•„ìš”í•œ íŒŒì¼ë“¤:")
        print(f"  - {fitted_path}")
        print(f"  - {person_path}")
        print(f"  - {clothing_path}")
    
    await quality_assessor.cleanup()


# === FastAPI í†µí•© ì˜ˆì‹œ ===
class QualityAssessmentAPI:
    """í’ˆì§ˆ í‰ê°€ API ë˜í¼"""
    
    def __init__(self):
        self.assessor = None
    
    async def initialize(self):
        """API ì´ˆê¸°í™”"""
        self.assessor = RealQualityAssessmentStep(
            device='cpu',  # ë˜ëŠ” í™˜ê²½ì— ë”°ë¼ 'mps'
            config={
                'enable_advanced_metrics': True,
                'enable_face_detection': True,
                'enable_detailed_analysis': True
            }
        )
        return await self.assessor.initialize()
    
    async def assess_quality(
        self,
        fitted_image_base64: str,
        person_image_base64: str,
        clothing_image_base64: str,
        pipeline_results: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ API ì—”ë“œí¬ì¸íŠ¸"""
        
        if not self.assessor or not self.assessor.is_initialized:
            raise RuntimeError("í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return await self.assessor.process(
            fitted_image=fitted_image_base64,
            original_person=person_image_base64,
            original_clothing=clothing_image_base64,
            pipeline_results=pipeline_results
        )
    
    async def cleanup(self):
        """API ì •ë¦¬"""
        if self.assessor:
            await self.assessor.cleanup()


if __name__ == "__main__":
    print("ğŸ¯ ì‹¤ì œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    asyncio.run(test_real_quality_assessment())
    
    print("\n" + "=" * 50)
    print("ğŸ“ ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ í…ŒìŠ¤íŠ¸")
    
    # ì‹¤ì œ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
    asyncio.run(test_with_real_images())