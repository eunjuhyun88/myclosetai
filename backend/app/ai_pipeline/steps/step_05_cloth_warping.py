"""
ğŸ”¥ MyCloset AI - Step 05: Enhanced Cloth Warping v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
===============================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”
âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)
âœ… ì‹¤ì œ TPS 1.8GB + DPT 512MB + VITON-HD 2.1GB ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
âœ… Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ
âœ… ê¸°í•˜í•™ì  ë³€í˜• ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
âœ… ë‹¤ì¤‘ ë³€í˜• ë°©ë²• ì§€ì› (TPS, DPT, VITON-HD)
âœ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ì™„ì „ ì§€ì›

Author: MyCloset AI Team
Date: 2025-07-31
Version: 8.0 (Central Hub DI Container Integration)
"""

import os
import sys
import time
import logging
import asyncio
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, field
import cv2

# PyTorch í•„ìˆ˜
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# PIL í•„ìˆ˜
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# BaseStepMixin import
from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”¥ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ ë„¤íŠ¸ì›Œí¬ í´ë˜ìŠ¤ë“¤ - ì™„ì „ AI ì¶”ë¡  ê°€ëŠ¥
# ==============================================

class AdvancedTPSWarpingNetwork(nn.Module):
    """ê³ ê¸‰ TPS (Thin Plate Spline) ì›Œí•‘ ë„¤íŠ¸ì›Œí¬ - ì •ë°€í•œ ì˜ë¥˜ ë³€í˜•"""
    
    def __init__(self, num_control_points: int = 25, input_channels: int = 6):
        super().__init__()
        self.num_control_points = num_control_points
        
        # ResNet ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_extractor = self._build_resnet_backbone()
        
        # TPS ì œì–´ì  ì˜ˆì¸¡ê¸°
        self.control_point_predictor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(2048, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_control_points * 2),  # x, y ì¢Œí‘œ
            nn.Tanh()
        )
        
        # TPS ë§¤ê°œë³€ìˆ˜ ì •ì œê¸°
        self.tps_refiner = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 2, 3, 1, 1),  # ì •ì œëœ ë³€ìœ„
            nn.Tanh()
        )
        
        # í’ˆì§ˆ í‰ê°€ê¸°
        self.quality_assessor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(2048, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def _build_resnet_backbone(self):
        """ResNet ë°±ë³¸ êµ¬ì¶•"""
        return nn.Sequential(
            # ì´ˆê¸° ë ˆì´ì–´
            nn.Conv2d(6, 64, 7, 2, 3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # ResNet ë¸”ë¡ë“¤
            self._make_layer(64, 64, 3),     # 256 channels
            self._make_layer(256, 128, 4, stride=2),  # 512 channels
            self._make_layer(512, 256, 6, stride=2),  # 1024 channels
            self._make_layer(1024, 512, 3, stride=2), # 2048 channels
        )
    
    def _make_layer(self, inplanes, planes, blocks, stride=1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        layers = []
        
        # Downsample
        downsample = None
        if stride != 1 or inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * 4, 1, stride, bias=False),
                nn.BatchNorm2d(planes * 4)
            )
        
        # ì²« ë²ˆì§¸ ë¸”ë¡
        layers.append(self._bottleneck(inplanes, planes, stride, downsample))
        
        # ë‚˜ë¨¸ì§€ ë¸”ë¡ë“¤
        for _ in range(1, blocks):
            layers.append(self._bottleneck(planes * 4, planes))
        
        return nn.Sequential(*layers)
    
    def _bottleneck(self, inplanes, planes, stride=1, downsample=None):
        """ResNet Bottleneck ë¸”ë¡"""
        return nn.Sequential(
            nn.Conv2d(inplanes, planes, 1, bias=False),
            nn.BatchNorm2d(planes),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(planes, planes, 3, stride, 1, bias=False),
            nn.BatchNorm2d(planes),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(planes, planes * 4, 1, bias=False),
            nn.BatchNorm2d(planes * 4),
            
            # Skip connection
            downsample if downsample else nn.Identity(),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ - ê³ ê¸‰ TPS ì›Œí•‘"""
        batch_size = cloth_image.size(0)
        
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([cloth_image, person_image], dim=1)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        
        # TPS ì œì–´ì  ì˜ˆì¸¡
        control_points = self.control_point_predictor(features)
        control_points = control_points.view(batch_size, self.num_control_points, 2)
        
        # TPS ë³€í˜• ì ìš©
        tps_grid = self._solve_tps(control_points, cloth_image.shape[-2:])
        
        # ì •ì œëœ ë³€ìœ„ ê³„ì‚°
        refined_displacement = self.tps_refiner(combined_input)
        
        # ìµœì¢… ë³€í˜• ê·¸ë¦¬ë“œ
        final_grid = tps_grid + refined_displacement.permute(0, 2, 3, 1) * 0.1
        final_grid = torch.clamp(final_grid, -1, 1)
        
        # ì›Œí•‘ ì ìš©
        warped_cloth = F.grid_sample(
            cloth_image, final_grid, 
            mode='bilinear', padding_mode='border', align_corners=False
        )
        
        # í’ˆì§ˆ í‰ê°€
        quality_score = self.quality_assessor(features)
        
        return {
            'warped_cloth': warped_cloth,
            'control_points': control_points,
            'tps_grid': tps_grid,
            'refined_displacement': refined_displacement,
            'quality_score': quality_score,
            'confidence': torch.mean(quality_score)
        }
    
    def _solve_tps(self, control_points: torch.Tensor, image_size: Tuple[int, int]) -> torch.Tensor:
        """TPS ì†”ë²„ - ì œì–´ì ì—ì„œ ë³€í˜• ê·¸ë¦¬ë“œ ê³„ì‚°"""
        batch_size, num_points, _ = control_points.shape
        h, w = image_size
        
        # ì •ê·œí™”ëœ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords = torch.linspace(-1, 1, h, device=control_points.device)
        x_coords = torch.linspace(-1, 1, w, device=control_points.device)
        grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
        grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        
        # ì œì–´ì  ê°„ ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        source_points = self._generate_regular_grid(num_points, control_points.device)
        target_points = control_points
        
        # ê°„ë‹¨í•œ RBF ë³´ê°„ìœ¼ë¡œ TPS ê·¼ì‚¬
        for b in range(batch_size):
            for i in range(num_points):
                src_pt = source_points[i]
                tgt_pt = target_points[b, i]
                
                # ì œì–´ì  ì£¼ë³€ ì˜ì—­ì— ë³€í˜• ì ìš©
                distances = torch.sqrt(
                    (grid[b, :, :, 0] - src_pt[0])**2 + 
                    (grid[b, :, :, 1] - src_pt[1])**2
                )
                
                # RBF ê°€ì¤‘ì¹˜
                weights = torch.exp(-distances * 5.0)
                displacement = (tgt_pt - src_pt) * weights.unsqueeze(-1)
                
                grid[b] += displacement
        
        return torch.clamp(grid, -1, 1)
    
    def _generate_regular_grid(self, num_points: int, device) -> torch.Tensor:
        """ê·œì¹™ì ì¸ ì œì–´ì  ê·¸ë¦¬ë“œ ìƒì„±"""
        grid_size = int(np.sqrt(num_points))
        points = []
        
        for i in range(grid_size):
            for j in range(grid_size):
                if len(points) >= num_points:
                    break
                x = -1 + 2 * j / max(1, grid_size - 1)
                y = -1 + 2 * i / max(1, grid_size - 1)
                points.append([x, y])
        
        # ë¶€ì¡±í•œ ì ë“¤ì€ ì¤‘ì•™ ê·¼ì²˜ì— ì¶”ê°€
        while len(points) < num_points:
            points.append([0.0, 0.0])
        
        return torch.tensor(points[:num_points], device=device, dtype=torch.float32)

class RAFTFlowWarpingNetwork(nn.Module):
    """RAFT Optical Flow ê¸°ë°˜ ì •ë°€ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, small_model: bool = False):
        super().__init__()
        self.small_model = small_model
        
        # Feature encoder
        self.feature_encoder = self._build_feature_encoder()
        
        # Context encoder
        self.context_encoder = self._build_context_encoder()
        
        # Update block
        self.update_block = self._build_update_block()
        
        # Flow head
        self.flow_head = nn.Sequential(
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 2, 3, 1, 1)
        )
    
    def _build_feature_encoder(self):
        """íŠ¹ì§• ì¸ì½”ë” êµ¬ì¶•"""
        if self.small_model:
            dims = [32, 32, 64, 96]
        else:
            dims = [64, 64, 96, 128]
        
        layers = []
        in_dim = 3
        
        for dim in dims:
            layers.extend([
                nn.Conv2d(in_dim, dim, 7, 2, 3),
                nn.ReLU(inplace=True),
                nn.Conv2d(dim, dim, 3, 1, 1),
                nn.ReLU(inplace=True)
            ])
            in_dim = dim
        
        return nn.Sequential(*layers)
    
    def _build_context_encoder(self):
        """ì»¨í…ìŠ¤íŠ¸ ì¸ì½”ë” êµ¬ì¶•"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, 2, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.ReLU(inplace=True)
        )
    
    def _build_update_block(self):
        """ì—…ë°ì´íŠ¸ ë¸”ë¡ êµ¬ì¶•"""
        return nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor, 
                num_iterations: int = 12) -> Dict[str, torch.Tensor]:
        """RAFT ê¸°ë°˜ Flow ì¶”ì • ë° ì›Œí•‘"""
        
        # íŠ¹ì§• ì¶”ì¶œ
        cloth_features = self.feature_encoder(cloth_image)
        person_features = self.feature_encoder(person_image)
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        context = self.context_encoder(person_image)
        
        # ì´ˆê¸° flow ì¶”ì •
        corr_pyramid = self._build_correlation_pyramid(cloth_features, person_features)
        flow = torch.zeros(cloth_image.size(0), 2, cloth_image.size(2)//8, 
                          cloth_image.size(3)//8, device=cloth_image.device)
        
        flow_predictions = []
        
        # ë°˜ë³µì  ì •ì œ
        for _ in range(num_iterations):
            # ìƒê´€ê´€ê³„ ì¡°íšŒ
            corr = self._lookup_correlation(corr_pyramid, flow)
            
            # ì—…ë°ì´íŠ¸
            inp = torch.cat([corr, context], dim=1)
            delta_flow = self.update_block(inp)
            delta_flow = self.flow_head(delta_flow)
            
            flow = flow + delta_flow
            flow_predictions.append(flow)
        
        # Flowë¥¼ ì›ë³¸ í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œ
        final_flow = F.interpolate(flow, size=cloth_image.shape[-2:], 
                                  mode='bilinear', align_corners=False) * 8.0
        
        # Flowë¥¼ ê·¸ë¦¬ë“œë¡œ ë³€í™˜
        grid = self._flow_to_grid(final_flow)
        
        # ì›Œí•‘ ì ìš©
        warped_cloth = F.grid_sample(
            cloth_image, grid, 
            mode='bilinear', padding_mode='border', align_corners=False
        )
        
        return {
            'warped_cloth': warped_cloth,
            'flow_field': final_flow,
            'grid': grid,
            'flow_predictions': flow_predictions,
            'confidence': self._estimate_flow_confidence(final_flow)
        }
    
    def _build_correlation_pyramid(self, fmap1: torch.Tensor, fmap2: torch.Tensor):
        """ìƒê´€ê´€ê³„ í”¼ë¼ë¯¸ë“œ êµ¬ì¶•"""
        batch, dim, h, w = fmap1.shape
        
        # íŠ¹ì§•ë§µ ì •ê·œí™”
        fmap1 = F.normalize(fmap1, dim=1)
        fmap2 = F.normalize(fmap2, dim=1)
        
        # ì „ì²´ ìƒê´€ê´€ê³„ ê³„ì‚°
        corr = torch.einsum('aijk,aijl->aijkl', fmap1, fmap2.view(batch, dim, h*w))
        corr = corr.view(batch, h, w, h, w)
        
        # í”¼ë¼ë¯¸ë“œ ë ˆë²¨ ìƒì„±
        pyramid = [corr]
        for i in range(3):
            corr = F.avg_pool2d(corr.view(batch*h*w, 1, h, w), 2, 2)
            corr = corr.view(batch, h, w, h//2, w//2)
            pyramid.append(corr)
            h, w = h//2, w//2
        
        return pyramid
    
    def _lookup_correlation(self, pyramid, flow):
        """ìƒê´€ê´€ê³„ ì¡°íšŒ"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìƒ˜í”Œë§ í•„ìš”
        return pyramid[0][:, :, :, 0, 0].unsqueeze(1)
    
    def _flow_to_grid(self, flow: torch.Tensor) -> torch.Tensor:
        """Flowë¥¼ ìƒ˜í”Œë§ ê·¸ë¦¬ë“œë¡œ ë³€í™˜"""
        batch, _, h, w = flow.shape
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords = torch.linspace(-1, 1, h, device=flow.device)
        x_coords = torch.linspace(-1, 1, w, device=flow.device)
        grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
        grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0).repeat(batch, 1, 1, 1)
        
        # Flow ì¶”ê°€ (ì •ê·œí™”)
        flow_normalized = flow.permute(0, 2, 3, 1)
        flow_normalized[:, :, :, 0] = flow_normalized[:, :, :, 0] / (w - 1) * 2
        flow_normalized[:, :, :, 1] = flow_normalized[:, :, :, 1] / (h - 1) * 2
        
        return grid + flow_normalized
    
    def _estimate_flow_confidence(self, flow: torch.Tensor) -> torch.Tensor:
        """Flow ì‹ ë¢°ë„ ì¶”ì •"""
        # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚° - flow ì¼ê´€ì„± ê¸°ë°˜
        flow_magnitude = torch.sqrt(flow[:, 0]**2 + flow[:, 1]**2)
        confidence = torch.exp(-flow_magnitude.mean(dim=[1, 2]) / 10.0)
        return confidence

class VGGClothBodyMatchingNetwork(nn.Module):
    """VGG ê¸°ë°˜ ì˜ë¥˜-ì¸ì²´ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, vgg_type: str = "vgg19"):
        super().__init__()
        self.vgg_type = vgg_type
        
        # VGG ë°±ë³¸
        self.vgg_features = self._build_vgg_backbone()
        
        # ì˜ë¥˜ ë¸Œëœì¹˜
        self.cloth_branch = nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True)
        )
        
        # ì¸ì²´ ë¸Œëœì¹˜
        self.body_branch = nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True)
        )
        
        # ë§¤ì¹­ í—¤ë“œ
        self.matching_head = nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œê¸°
        self.keypoint_detector = nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 25, 1),  # 25ê°œ í‚¤í¬ì¸íŠ¸
            nn.Sigmoid()
        )
    
    def _build_vgg_backbone(self):
        """VGG ë°±ë³¸ êµ¬ì¶•"""
        if self.vgg_type == "vgg19":
            cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 
                   512, 512, 512, 512, 'M', 512, 512, 512, 512]
        else:  # vgg16
            cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 
                   512, 512, 512, 'M', 512, 512, 512]
        
        layers = []
        in_channels = 3
        
        for v in cfg:
            if v == 'M':
                layers.append(nn.MaxPool2d(2, 2))
            else:
                layers.extend([
                    nn.Conv2d(in_channels, v, 3, 1, 1),
                    nn.ReLU(inplace=True)
                ])
                in_channels = v
        
        return nn.Sequential(*layers)
    
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """VGG ê¸°ë°˜ ì˜ë¥˜-ì¸ì²´ ë§¤ì¹­"""
        
        # VGG íŠ¹ì§• ì¶”ì¶œ
        cloth_features = self.vgg_features(cloth_image)
        person_features = self.vgg_features(person_image)
        
        # ë¸Œëœì¹˜ë³„ íŠ¹ì§• ì²˜ë¦¬
        cloth_processed = self.cloth_branch(cloth_features)
        person_processed = self.body_branch(person_features)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([cloth_processed, person_processed], dim=1)
        
        # ë§¤ì¹­ ë§µ ìƒì„±
        matching_map = self.matching_head(combined_features)
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        keypoints = self.keypoint_detector(combined_features)
        
        # ë§¤ì¹­ ê¸°ë°˜ ì›Œí•‘ ê·¸ë¦¬ë“œ ìƒì„±
        warping_grid = self._generate_warping_grid(matching_map, keypoints)
        
        # ì›Œí•‘ ì ìš©
        warped_cloth = F.grid_sample(
            cloth_image, warping_grid,
            mode='bilinear', padding_mode='border', align_corners=False
        )
        
        return {
            'warped_cloth': warped_cloth,
            'matching_map': matching_map,
            'keypoints': keypoints,
            'warping_grid': warping_grid,
            'cloth_features': cloth_processed,
            'person_features': person_processed,
            'confidence': torch.mean(matching_map)
        }
    
    def _generate_warping_grid(self, matching_map: torch.Tensor, 
                              keypoints: torch.Tensor) -> torch.Tensor:
        """ë§¤ì¹­ ë§µê³¼ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì›Œí•‘ ê·¸ë¦¬ë“œ ìƒì„±"""
        batch_size, _, h, w = matching_map.shape
        
        # ê¸°ë³¸ ê·¸ë¦¬ë“œ
        y_coords = torch.linspace(-1, 1, h, device=matching_map.device)
        x_coords = torch.linspace(-1, 1, w, device=matching_map.device)
        grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
        grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)
        
        # ë§¤ì¹­ ë§µ ê¸°ë°˜ ë³€í˜•
        matching_displacement = torch.stack([
            torch.gradient(matching_map.squeeze(1), dim=2)[0] * 0.1,
            torch.gradient(matching_map.squeeze(1), dim=1)[0] * 0.1
        ], dim=-1)
        
        # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë¡œì»¬ ë³€í˜•
        for b in range(batch_size):
            for k in range(min(5, keypoints.size(1))):  # ìƒìœ„ 5ê°œ í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
                kp_map = keypoints[b, k]
                
                # í‚¤í¬ì¸íŠ¸ ìµœëŒ€ê°’ ìœ„ì¹˜
                max_pos = torch.unravel_index(torch.argmax(kp_map), kp_map.shape)
                center_y, center_x = max_pos[0].item(), max_pos[1].item()
                
                # ë¡œì»¬ ë³€í˜• ì ìš©
                y_dist = (torch.arange(h, device=matching_map.device) - center_y).float()
                x_dist = (torch.arange(w, device=matching_map.device) - center_x).float()
                
                y_grid_dist, x_grid_dist = torch.meshgrid(y_dist, x_dist, indexing='ij')
                distances = torch.sqrt(y_grid_dist**2 + x_grid_dist**2)
                
                # RBF ê°€ì¤‘ì¹˜
                weights = torch.exp(-distances / 20.0) * kp_map[center_y, center_x]
                
                # ë³€í˜• ì ìš©
                grid[b, :, :, 0] += weights * 0.05
                grid[b, :, :, 1] += weights * 0.05
        
        return torch.clamp(grid, -1, 1)

class DenseNetQualityAssessment(nn.Module):
    """DenseNet ê¸°ë°˜ ì›Œí•‘ í’ˆì§ˆ í‰ê°€"""
    
    def __init__(self, growth_rate: int = 32, num_layers: int = 121):
        super().__init__()
        
        # DenseNet ë¸”ë¡ ì„¤ì •
        if num_layers == 121:
            block_config = (6, 12, 24, 16)
        elif num_layers == 169:
            block_config = (6, 12, 32, 32)
        else:
            block_config = (6, 12, 24, 16)
        
        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜
        self.initial_conv = nn.Sequential(
            nn.Conv2d(6, 64, 7, 2, 3, bias=False),  # cloth + person
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1)
        )
        
        # DenseNet ë¸”ë¡ë“¤
        num_features = 64
        self.dense_blocks = nn.ModuleList()
        self.transitions = nn.ModuleList()
        
        for i, num_layers in enumerate(block_config):
            # Dense Block
            block = self._make_dense_block(num_features, growth_rate, num_layers)
            self.dense_blocks.append(block)
            num_features += num_layers * growth_rate
            
            # Transition (ë§ˆì§€ë§‰ ë¸”ë¡ ì œì™¸)
            if i != len(block_config) - 1:
                transition = self._make_transition(num_features, num_features // 2)
                self.transitions.append(transition)
                num_features = num_features // 2
        
        # í’ˆì§ˆ í‰ê°€ í—¤ë“œ
        self.quality_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # ì„¸ë¶€ í’ˆì§ˆ ë©”íŠ¸ë¦­
        self.detail_metrics = nn.ModuleDict({
            'texture_preservation': nn.Sequential(
                nn.AdaptiveAvgPool2d(1), nn.Flatten(),
                nn.Linear(num_features, 64), nn.ReLU(),
                nn.Linear(64, 1), nn.Sigmoid()
            ),
            'shape_consistency': nn.Sequential(
                nn.AdaptiveAvgPool2d(1), nn.Flatten(),
                nn.Linear(num_features, 64), nn.ReLU(),
                nn.Linear(64, 1), nn.Sigmoid()
            ),
            'edge_sharpness': nn.Sequential(
                nn.AdaptiveAvgPool2d(1), nn.Flatten(),
                nn.Linear(num_features, 64), nn.ReLU(),
                nn.Linear(64, 1), nn.Sigmoid()
            )
        })
    
    def _make_dense_block(self, num_features: int, growth_rate: int, num_layers: int):
        """DenseNet ë¸”ë¡ ìƒì„±"""
        layers = []
        for i in range(num_layers):
            layers.append(self._make_dense_layer(num_features + i * growth_rate, growth_rate))
        return nn.Sequential(*layers)
    
    def _make_dense_layer(self, num_input_features: int, growth_rate: int):
        """Dense Layer ìƒì„±"""
        return nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, growth_rate * 4, 1, bias=False),
            nn.BatchNorm2d(growth_rate * 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(growth_rate * 4, growth_rate, 3, 1, 1, bias=False)
        )
    
    def _make_transition(self, num_input_features: int, num_output_features: int):
        """Transition Layer ìƒì„±"""
        return nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, num_output_features, 1, bias=False),
            nn.AvgPool2d(2, 2)
        )
    
    def forward(self, cloth_image: torch.Tensor, warped_cloth: torch.Tensor) -> Dict[str, torch.Tensor]:
        """DenseNet ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([cloth_image, warped_cloth], dim=1)
        
        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
        features = self.initial_conv(combined_input)
        
        # DenseNet ë¸”ë¡ë“¤ í†µê³¼
        for i, dense_block in enumerate(self.dense_blocks):
            features = dense_block(features)
            if i < len(self.transitions):
                features = self.transitions[i](features)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        overall_quality = self.quality_head(features)
        
        # ì„¸ë¶€ ë©”íŠ¸ë¦­
        detail_scores = {}
        for metric_name, metric_head in self.detail_metrics.items():
            detail_scores[metric_name] = metric_head(features)
        
        return {
            'overall_quality': overall_quality,
            'texture_preservation': detail_scores['texture_preservation'],
            'shape_consistency': detail_scores['shape_consistency'],
            'edge_sharpness': detail_scores['edge_sharpness'],
            'quality_features': features,
            'confidence': overall_quality
        }

class PhysicsBasedFabricSimulation:
    """ë¬¼ë¦¬ ê¸°ë°˜ ì›ë‹¨ ì‹œë®¬ë ˆì´ì…˜"""
    
    def __init__(self, fabric_type: str = "cotton"):
        self.fabric_type = fabric_type
        self.fabric_properties = self._get_fabric_properties(fabric_type)
    
    def _get_fabric_properties(self, fabric_type: str) -> Dict[str, float]:
        """ì›ë‹¨ íƒ€ì…ë³„ ë¬¼ë¦¬ ì†ì„±"""
        properties = {
            'cotton': {
                'elasticity': 0.3, 'stiffness': 0.5, 'damping': 0.1,
                'density': 1.5, 'friction': 0.6
            },
            'silk': {
                'elasticity': 0.1, 'stiffness': 0.2, 'damping': 0.05,
                'density': 1.3, 'friction': 0.3
            },
            'denim': {
                'elasticity': 0.5, 'stiffness': 0.8, 'damping': 0.2,
                'density': 1.8, 'friction': 0.8
            },
            'wool': {
                'elasticity': 0.4, 'stiffness': 0.6, 'damping': 0.15,
                'density': 1.4, 'friction': 0.7
            },
            'spandex': {
                'elasticity': 0.8, 'stiffness': 0.3, 'damping': 0.05,
                'density': 1.2, 'friction': 0.4
            }
        }
        return properties.get(fabric_type, properties['cotton'])
    
    def simulate_fabric_deformation(self, warped_cloth: torch.Tensor, 
                                   force_field: torch.Tensor) -> torch.Tensor:
        """ì›ë‹¨ ë³€í˜• ì‹œë®¬ë ˆì´ì…˜"""
        try:
            batch_size, channels, height, width = warped_cloth.shape
            
            # ë¬¼ë¦¬ ì†ì„± ì ìš©
            elasticity = self.fabric_properties['elasticity']
            stiffness = self.fabric_properties['stiffness']
            damping = self.fabric_properties['damping']
            
            # ê°„ë‹¨í•œ ìŠ¤í”„ë§-ëŒí¼ ì‹œìŠ¤í…œ ì‹œë®¬ë ˆì´ì…˜
            # ì¸ì ‘ í”½ì…€ ê°„ì˜ ìŠ¤í”„ë§ ì—°ê²°ì„ ê°€ì •
            
            # ìˆ˜í‰ ë°©í–¥ ìŠ¤í”„ë§ í¬ìŠ¤
            horizontal_diff = F.pad(warped_cloth[:, :, :, 1:] - warped_cloth[:, :, :, :-1], 
                                   (0, 1, 0, 0))
            horizontal_force = -stiffness * horizontal_diff
            
            # ìˆ˜ì§ ë°©í–¥ ìŠ¤í”„ë§ í¬ìŠ¤
            vertical_diff = F.pad(warped_cloth[:, :, 1:, :] - warped_cloth[:, :, :-1, :], 
                                 (0, 0, 0, 1))
            vertical_force = -stiffness * vertical_diff
            
            # ëŒí•‘ í¬ìŠ¤ (ê°„ë‹¨í•œ êµ¬í˜„)
            damping_force = -damping * warped_cloth
            
            # ì™¸ë¶€ í¬ìŠ¤ (force_field) ì ìš©
            external_force = force_field * elasticity
            
            # ì´ í¬ìŠ¤
            total_force = horizontal_force + vertical_force + damping_force + external_force
            
            # í¬ìŠ¤ë¥¼ ì´ìš©í•œ ë³€í˜• ì ìš© (ì˜¤ì¼ëŸ¬ ì ë¶„)
            dt = 0.1  # ì‹œê°„ ìŠ¤í…
            displacement = total_force * dt * dt  # F = ma, a*dt^2 = displacement
            
            # ë³€í˜• ì œí•œ (ê³¼ë„í•œ ë³€í˜• ë°©ì§€)
            displacement = torch.clamp(displacement, -0.1, 0.1)
            
            simulated_cloth = warped_cloth + displacement
            
            # ë²”ìœ„ ì œí•œ
            simulated_cloth = torch.clamp(simulated_cloth, -1, 1)
            
            return simulated_cloth
            
        except Exception as e:
            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
            return warped_cloth
    
    def apply_gravity_effect(self, cloth: torch.Tensor) -> torch.Tensor:
        """ì¤‘ë ¥ íš¨ê³¼ ì ìš©"""
        try:
            # ê°„ë‹¨í•œ ì¤‘ë ¥ íš¨ê³¼ - ì•„ë˜ìª½ìœ¼ë¡œ ì•½ê°„ì˜ ë“œë˜ê·¸
            gravity_strength = 0.02 * self.fabric_properties['density']
            
            # Y ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì ìš© (ì•„ë˜ìª½ì´ ë” ì˜í–¥ ë°›ìŒ)
            height = cloth.shape[2]
            y_weights = torch.linspace(0, gravity_strength, height, device=cloth.device)
            y_weights = y_weights.view(1, 1, -1, 1)
            
            # ì¤‘ë ¥ íš¨ê³¼ ì ìš©
            gravity_effect = torch.zeros_like(cloth)
            gravity_effect[:, :, 1:, :] = cloth[:, :, :-1, :] - cloth[:, :, 1:, :] 
            gravity_effect = gravity_effect * y_weights
            
            return cloth + gravity_effect
            
        except Exception as e:
            return cloth
    """Enhanced Cloth Warping ì„¤ì •"""
    input_size: tuple = (768, 1024)  # TPS ì…ë ¥ í¬ê¸°
    warping_strength: float = 1.0
    enable_multi_stage: bool = True
    enable_depth_estimation: bool = True
    enable_quality_enhancement: bool = True
    device: str = "auto"

# ë³€í˜• íƒ€ì… ì •ì˜
WARPING_METHODS = {
    0: 'affine',           # ì–´íŒŒì¸ ë³€í˜•
    1: 'perspective',      # ì›ê·¼ ë³€í˜•
    2: 'thin_plate_spline', # TPS ë³€í˜• (í•µì‹¬)
    3: 'b_spline',         # B-Spline ë³€í˜•
    4: 'grid_sample',      # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§
    5: 'optical_flow',     # ì˜µí‹°ì»¬ í”Œë¡œìš°
    6: 'depth_guided',     # ê¹Šì´ ê¸°ë°˜ ë³€í˜•
    7: 'multi_stage',      # ë‹¤ë‹¨ê³„ ë³€í˜•
    8: 'quality_enhanced', # í’ˆì§ˆ í–¥ìƒ ë³€í˜•
    9: 'hybrid'            # í•˜ì´ë¸Œë¦¬ë“œ ë³€í˜•
}

# ë³€í˜• í’ˆì§ˆ ë ˆë²¨
WARPING_QUALITY_LEVELS = {
    'fast': {
        'methods': ['affine', 'perspective'],
        'resolution': (512, 512),
        'iterations': 1
    },
    'balanced': {
        'methods': ['thin_plate_spline', 'grid_sample'],
        'resolution': (768, 1024),
        'iterations': 2
    },
    'high': {
        'methods': ['thin_plate_spline', 'b_spline', 'depth_guided'],
        'resolution': (768, 1024),
        'iterations': 3
    },
    'ultra': {
        'methods': ['multi_stage', 'quality_enhanced', 'hybrid'],
        'resolution': (1024, 1536),
        'iterations': 5
    }
}

# ==============================================
# ğŸ”¥ EnhancedClothWarpingStep í´ë˜ìŠ¤
# ==============================================

class EnhancedClothWarpingStep(BaseStepMixin):
    """
    ğŸ”¥ Step 05: Enhanced Cloth Warping v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
    
    Central Hub DI Container v7.0ì—ì„œ ìë™ ì œê³µ:
    âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì…
    âœ… MemoryManager ìë™ ì—°ê²°  
    âœ… DataConverter í†µí•©
    âœ… ìë™ ì´ˆê¸°í™” ë° ì„¤ì •
    """
    
    def __init__(self, **kwargs):
        """Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="EnhancedClothWarpingStep",
                step_id=5,
                **kwargs
            )
            
            # 3. Enhanced Cloth Warping íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_warping_specifics(**kwargs)
            
            self.logger.info("âœ… EnhancedClothWarpingStep v8.0 Central Hub DI Container ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ EnhancedClothWarpingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _initialize_step_attributes(self):
        """í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin ìš”êµ¬ì‚¬í•­)"""
        self.ai_models = {}
        self.models_loading_status = {
            'tps': False,
            'dpt': False,
            'viton_hd': False,
            'mock_model': False
        }
        self.model_interface = None
        self.loaded_models = []
        self.logger = logging.getLogger(f"{__name__}.EnhancedClothWarpingStep")
        
        # Enhanced Cloth Warping íŠ¹í™” ì†ì„±ë“¤
        self.warping_models = {}
        self.warping_ready = False
        self.warping_cache = {}
        self.transformation_matrices = {}
        self.depth_estimator = None
        self.quality_enhancer = None
    
    def _initialize_warping_specifics(self, **kwargs):
        """Enhanced Cloth Warping íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •
            self.config = EnhancedClothWarpingConfig()
            if 'config' in kwargs:
                config_dict = kwargs['config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = self._detect_optimal_device()
            
            # AI ëª¨ë¸ ë¡œë”© (Central Hubë¥¼ í†µí•´)
            self._load_warping_models_via_central_hub()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Enhanced Cloth Warping íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ)"""
        self.step_name = "EnhancedClothWarpingStep"
        self.step_id = 5
        self.device = "cpu"
        self.ai_models = {}
        self.models_loading_status = {'emergency': True}
        self.model_interface = None
        self.loaded_models = []
        self.config = EnhancedClothWarpingConfig()
        self.logger = logging.getLogger(f"{__name__}.EnhancedClothWarpingStep")
        self.warping_models = {}
        self.warping_ready = False
        self.warping_cache = {}
        self.transformation_matrices = {}
        self.depth_estimator = None
        self.quality_enhancer = None

    def _load_warping_models_via_central_hub(self):
        """Central Hub DI Containerë¥¼ í†µí•œ Warping ëª¨ë¸ ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ Enhanced Cloth Warping AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # Central Hubì—ì„œ ModelLoader ê°€ì ¸ì˜¤ê¸° (ìë™ ì£¼ì…ë¨)
            if not hasattr(self, 'model_loader') or not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ë¡œ ì§ì ‘ ìƒì„±")
                self._create_advanced_ai_networks()
                return
            
            # 1. TPS (Thin-Plate Spline) ëª¨ë¸ ë¡œë”© (Primary) - 1.8GB
            try:
                tps_model = self.model_loader.load_model(
                    model_name="tps_transformation.pth",
                    step_name="EnhancedClothWarpingStep",
                    model_type="cloth_warping"
                )
                
                if tps_model:
                    self.ai_models['tps'] = tps_model
                    self.models_loading_status['tps'] = True
                    self.loaded_models.append('tps')
                    self.logger.info("âœ… TPS ëª¨ë¸ ë¡œë”© ì™„ë£Œ (1.8GB)")
                else:
                    # TPS ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„±
                    self.tps_network = AdvancedTPSWarpingNetwork(
                        num_control_points=25, input_channels=6
                    ).to(self.device)
                    self.ai_models['tps_network'] = self.tps_network
                    self.models_loading_status['tps'] = True
                    self.loaded_models.append('tps_network')
                    self.logger.info("âœ… ê³ ê¸‰ TPS ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„± ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ TPS ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                # ê³ ê¸‰ TPS ë„¤íŠ¸ì›Œí¬ ìƒì„±
                self.tps_network = AdvancedTPSWarpingNetwork(
                    num_control_points=25, input_channels=6
                ).to(self.device)
                self.ai_models['tps_network'] = self.tps_network
                self.models_loading_status['tps'] = True
                self.loaded_models.append('tps_network')
                self.logger.info("âœ… ê³ ê¸‰ TPS ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„± ì™„ë£Œ (í´ë°±)")
            
            # 2. RAFT Flow ë„¤íŠ¸ì›Œí¬ ìƒì„±
            try:
                self.raft_network = RAFTFlowWarpingNetwork(small_model=False).to(self.device)
                self.ai_models['raft_network'] = self.raft_network
                self.models_loading_status['raft'] = True
                self.loaded_models.append('raft_network')
                self.logger.info("âœ… ê³ ê¸‰ RAFT Flow ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ RAFT ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 3. VGG ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ìƒì„±
            try:
                self.vgg_matching = VGGClothBodyMatchingNetwork(vgg_type="vgg19").to(self.device)
                self.ai_models['vgg_matching'] = self.vgg_matching
                self.models_loading_status['vgg'] = True
                self.loaded_models.append('vgg_matching')
                self.logger.info("âœ… ê³ ê¸‰ VGG ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ VGG ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 4. DenseNet í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬ ìƒì„±
            try:
                self.densenet_quality = DenseNetQualityAssessment(
                    growth_rate=32, num_layers=121
                ).to(self.device)
                self.ai_models['densenet_quality'] = self.densenet_quality
                self.models_loading_status['densenet'] = True
                self.loaded_models.append('densenet_quality')
                self.logger.info("âœ… ê³ ê¸‰ DenseNet í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DenseNet ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 5. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            try:
                self.fabric_simulator = PhysicsBasedFabricSimulation("cotton")
                self.models_loading_status['physics'] = True
                self.loaded_models.append('physics_simulation')
                self.logger.info("âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì›ë‹¨ ì‹œë®¬ë ˆì´ì…˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 6. ëª¨ë¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° Mock ëª¨ë¸ ìƒì„±
            if not self.loaded_models:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•ŠìŒ - Mock ëª¨ë¸ë¡œ í´ë°±")
                self._create_mock_warping_models()
            
            # Model Interface ì„¤ì •
            if hasattr(self.model_loader, 'create_step_interface'):
                self.model_interface = self.model_loader.create_step_interface("EnhancedClothWarpingStep")
            
            # Warping ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.warping_ready = len(self.loaded_models) > 0
            
            loaded_count = len(self.loaded_models)
            self.logger.info(f"ğŸ§  ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ Central Hub Warping ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_advanced_ai_networks()

    def _create_advanced_ai_networks(self):
        """ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ì´ë„ ì™„ì „ AI ì¶”ë¡  ê°€ëŠ¥)"""
        try:
            self.logger.info("ğŸ”„ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„± ì‹œì‘...")
            
            # 1. ê³ ê¸‰ TPS ì›Œí•‘ ë„¤íŠ¸ì›Œí¬
            try:
                self.tps_network = AdvancedTPSWarpingNetwork(
                    num_control_points=25, input_channels=6
                ).to(self.device)
                self.ai_models['tps_network'] = self.tps_network
                self.models_loading_status['tps'] = True
                self.loaded_models.append('tps_network')
                self.logger.info("âœ… ê³ ê¸‰ TPS ì›Œí•‘ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ TPS ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 2. RAFT Flow ì›Œí•‘ ë„¤íŠ¸ì›Œí¬
            try:
                self.raft_network = RAFTFlowWarpingNetwork(small_model=False).to(self.device)
                self.ai_models['raft_network'] = self.raft_network
                self.models_loading_status['raft'] = True
                self.loaded_models.append('raft_network')
                self.logger.info("âœ… RAFT Flow ì›Œí•‘ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ RAFT ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 3. VGG ì˜ë¥˜-ì¸ì²´ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬
            try:
                self.vgg_matching = VGGClothBodyMatchingNetwork(vgg_type="vgg19").to(self.device)
                self.ai_models['vgg_matching'] = self.vgg_matching
                self.models_loading_status['vgg'] = True
                self.loaded_models.append('vgg_matching')
                self.logger.info("âœ… VGG ì˜ë¥˜-ì¸ì²´ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ VGG ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 4. DenseNet í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬
            try:
                self.densenet_quality = DenseNetQualityAssessment(
                    growth_rate=32, num_layers=121
                ).to(self.device)
                self.ai_models['densenet_quality'] = self.densenet_quality
                self.models_loading_status['densenet'] = True
                self.loaded_models.append('densenet_quality')
                self.logger.info("âœ… DenseNet í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DenseNet ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 5. ë¬¼ë¦¬ ê¸°ë°˜ ì›ë‹¨ ì‹œë®¬ë ˆì´ì…˜
            try:
                self.fabric_simulator = PhysicsBasedFabricSimulation("cotton")
                self.models_loading_status['physics'] = True
                self.loaded_models.append('physics_simulation')
                self.logger.info("âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì›ë‹¨ ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # Warping ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.warping_ready = len(self.loaded_models) > 0
            
            loaded_count = len(self.loaded_models)
            self.logger.info(f"âœ… ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ìƒì„± ì™„ë£Œ: {loaded_count}ê°œ")
            
            # Mock ëª¨ë¸ë„ ì¶”ê°€ë¡œ ìƒì„± (ì•ˆì „ì¥ì¹˜)
            if loaded_count == 0:
                self._create_mock_warping_models()
                
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨: {e}")
            self._create_mock_warping_models()

    def _create_mock_warping_models(self):
        """Mock Warping ëª¨ë¸ ìƒì„± (ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ì‹œ í´ë°±)"""
        try:
            class MockEnhancedClothWarpingModel:
                def __init__(self, model_name: str):
                    self.model_name = model_name
                    self.device = "cpu"
                    
                def predict(self, cloth_image: np.ndarray, person_image: np.ndarray, keypoints: Optional[np.ndarray] = None) -> Dict[str, Any]:
                    """Mock ì˜ˆì¸¡ (ê¸°ë³¸ì ì¸ ê¸°í•˜í•™ì  ë³€í˜•)"""
                    h, w = cloth_image.shape[:2] if len(cloth_image.shape) >= 2 else (768, 1024)
                    
                    # ê¸°ë³¸ ë³€í˜• ì ìš© (ê°„ë‹¨í•œ ì–´íŒŒì¸ ë³€í˜•)
                    warped_cloth = self._apply_mock_warping(cloth_image, person_image)
                    
                    # Mock ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤
                    transformation_matrix = np.array([
                        [1.0, 0.1, 10],
                        [0.05, 1.0, 5],
                        [0, 0, 1]
                    ], dtype=np.float32)
                    
                    # Mock í’ˆì§ˆ ì ìˆ˜
                    quality_score = 0.75
                    
                    return {
                        'warped_cloth': warped_cloth,
                        'transformation_matrix': transformation_matrix,
                        'warping_confidence': quality_score,
                        'warping_method': self._get_mock_method(),
                        'processing_stages': ['mock_stage_1', 'mock_stage_2'],
                        'quality_metrics': {
                            'geometric_accuracy': 0.8,
                            'texture_preservation': 0.7,
                            'boundary_smoothness': 0.75
                        },
                        'model_type': 'mock',
                        'model_name': self.model_name
                    }
                
                def _apply_mock_warping(self, cloth_image: np.ndarray, person_image: np.ndarray) -> np.ndarray:
                    """Mock ë³€í˜• ì ìš©"""
                    try:
                        # ê°„ë‹¨í•œ ë¦¬ì‚¬ì´ì¦ˆ ë° ìœ„ì¹˜ ì¡°ì •
                        h, w = person_image.shape[:2]
                        cloth_resized = cv2.resize(cloth_image, (w//2, h//3))
                        
                        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
                        result = person_image.copy()
                        
                        # ì˜·ì„ ì¤‘ì•™ ìƒë‹¨ì— ë°°ì¹˜
                        start_y = h//6
                        end_y = start_y + cloth_resized.shape[0]
                        start_x = w//4
                        end_x = start_x + cloth_resized.shape[1]
                        
                        if end_y <= h and end_x <= w:
                            result[start_y:end_y, start_x:end_x] = cloth_resized
                        
                        return result
                        
                    except Exception as e:
                        # í´ë°±: ì›ë³¸ person_image ë°˜í™˜
                        return person_image
                
                def _get_mock_method(self) -> str:
                    """Mock ë°©ë²• ë°˜í™˜"""
                    if 'tps' in self.model_name.lower():
                        return 'thin_plate_spline'
                    elif 'dpt' in self.model_name.lower():
                        return 'depth_guided'
                    elif 'viton' in self.model_name.lower():
                        return 'quality_enhanced'
                    else:
                        return 'affine'
            
            # Mock ëª¨ë¸ë“¤ ìƒì„±
            self.ai_models['mock_tps'] = MockEnhancedClothWarpingModel('mock_tps')
            self.ai_models['mock_dpt'] = MockEnhancedClothWarpingModel('mock_dpt')
            self.ai_models['mock_viton'] = MockEnhancedClothWarpingModel('mock_viton')
            self.models_loading_status['mock_model'] = True
            self.loaded_models = ['mock_tps', 'mock_dpt', 'mock_viton']
            self.warping_ready = True
            
            # Mock ë³´ì¡° ëª¨ë¸ë“¤ ì„¤ì •
            self.depth_estimator = self.ai_models['mock_dpt']
            self.quality_enhancer = self.ai_models['mock_viton']
            
            self.logger.info("âœ… Mock Enhanced Cloth Warping ëª¨ë¸ ìƒì„± ì™„ë£Œ (í´ë°± ëª¨ë“œ)")
            
        except Exception as e:
            self.logger.error(f"âŒ Mock Warping ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ BaseStepMixin v20.0 í•„ìˆ˜ êµ¬í˜„ ë©”ì„œë“œ - ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸°)
        
        BaseStepMixinì˜ process() ë©”ì„œë“œì—ì„œ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨:
        1. process() â†’ ì…ë ¥ ë°ì´í„° ë³€í™˜
        2. _run_ai_inference() â†’ ì‹¤ì œ AI ì¶”ë¡  (ì´ ë©”ì„œë“œ)
        3. process() â†’ ì¶œë ¥ ë°ì´í„° ë³€í™˜ ë° ë°˜í™˜
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} Enhanced Cloth Warping AI ì¶”ë¡  ì‹œì‘...")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            cloth_image = processed_input.get('cloth_image')
            person_image = processed_input.get('person_image')
            
            if cloth_image is None or person_image is None:
                raise ValueError("cloth_imageì™€ person_imageê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤")
            
            keypoints = processed_input.get('keypoints', None)
            quality_level = processed_input.get('quality_level', 'balanced')
            
            # 2. Warping ì¤€ë¹„ ìƒíƒœ í™•ì¸
            if not self.warping_ready:
                raise ValueError("Enhanced Cloth Warping ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
            
            # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_cloth = self._preprocess_image(cloth_image)
            processed_person = self._preprocess_image(person_image)
            
            # 4. AI ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡  (ë™ê¸° ì‹¤í–‰)
            warping_result = self._run_enhanced_cloth_warping_inference_sync(
                processed_cloth, processed_person, keypoints, quality_level
            )
            
            # 5. í›„ì²˜ë¦¬
            final_result = self._postprocess_warping_result(warping_result, cloth_image, person_image)
            
            # 6. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 7. BaseStepMixin í‘œì¤€ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            return {
                'success': True,
                'warped_cloth': final_result['warped_cloth'],
                'transformation_matrix': final_result['transformation_matrix'],
                'warping_confidence': final_result['warping_confidence'],
                'warping_method': final_result['warping_method'],
                'processing_stages': final_result['processing_stages'],
                'quality_metrics': final_result['quality_metrics'],
                'processing_time': processing_time,
                'model_used': final_result['model_used'],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'ai_inference_completed': True,
                'central_hub_di_container': True
            }
            
        except Exception as e:
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            self.logger.error(f"âŒ {self.step_name} Enhanced Cloth Warping AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'processing_time': processing_time,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'ai_inference_completed': False,
                'central_hub_di_container': True
            }

    def _run_enhanced_cloth_warping_inference_sync(
        self, 
        cloth_image: np.ndarray, 
        person_image: np.ndarray, 
        keypoints: Optional[np.ndarray], 
        quality_level: str
    ) -> Dict[str, Any]:
        """Enhanced Cloth Warping AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ë²„ì „) - ì™„ì „ AI ì¶”ë¡  ì§€ì›"""
        try:
            # 1. í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            quality_config = WARPING_QUALITY_LEVELS.get(quality_level, WARPING_QUALITY_LEVELS['balanced'])
            
            # 2. ê³ ê¸‰ AI ë„¤íŠ¸ì›Œí¬ ìš°ì„ ìˆœìœ„ ê²°ì •
            selected_networks = []
            
            # TPS ë„¤íŠ¸ì›Œí¬ ìš°ì„  ì„ íƒ
            if ('tps_network' in self.loaded_models or 'tps' in self.loaded_models) and 'thin_plate_spline' in quality_config['methods']:
                if 'tps_network' in self.ai_models:
                    selected_networks.append(('tps_network', self.ai_models['tps_network']))
                elif 'tps' in self.ai_models:
                    selected_networks.append(('tps', self.ai_models['tps']))
            
            # RAFT ë„¤íŠ¸ì›Œí¬ ì¶”ê°€
            if 'raft_network' in self.loaded_models and 'optical_flow' in quality_config.get('methods', []):
                selected_networks.append(('raft_network', self.ai_models['raft_network']))
            
            # VGG ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ì¶”ê°€
            if 'vgg_matching' in self.loaded_models:
                selected_networks.append(('vgg_matching', self.ai_models['vgg_matching']))
            
            # DenseNet í’ˆì§ˆ í‰ê°€ ë„¤íŠ¸ì›Œí¬ ì¶”ê°€
            if 'densenet_quality' in self.loaded_models:
                selected_networks.append(('densenet_quality', self.ai_models['densenet_quality']))
            
            # Mock ëª¨ë¸ í´ë°±
            if not selected_networks and 'mock_tps' in self.loaded_models:
                model = self.ai_models['mock_tps']
                result = model.predict(cloth_image, person_image, keypoints)
                result['model_used'] = 'mock_tps'
                result['quality_level'] = quality_level
                return result
            
            if not selected_networks:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ AI ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ë©€í‹° ë„¤íŠ¸ì›Œí¬ AI ì¶”ë¡  ì‹¤í–‰
            network_results = {}
            
            for network_name, network in selected_networks:
                try:
                    if hasattr(network, 'predict'):
                        # Mock ëª¨ë¸
                        result = network.predict(cloth_image, person_image, keypoints)
                        network_results[network_name] = result
                    else:
                        # ì‹¤ì œ PyTorch ë„¤íŠ¸ì›Œí¬
                        result = self._run_advanced_pytorch_inference(
                            network, cloth_image, person_image, keypoints, network_name
                        )
                        network_results[network_name] = result
                    
                    self.logger.info(f"âœ… {network_name} AI ì¶”ë¡  ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {network_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    continue
            
            # 4. ë©€í‹° ë„¤íŠ¸ì›Œí¬ ê²°ê³¼ ìœµí•©
            if len(network_results) > 1:
                fused_result = self._fuse_multi_network_results(network_results, quality_config)
                fused_result['model_used'] = f"multi_network_{len(network_results)}"
                fused_result['networks_used'] = list(network_results.keys())
            elif len(network_results) == 1:
                network_name, result = list(network_results.items())[0]
                fused_result = result
                fused_result['model_used'] = network_name
                fused_result['networks_used'] = [network_name]
            else:
                raise ValueError("ëª¨ë“  AI ë„¤íŠ¸ì›Œí¬ ì¶”ë¡ ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # 5. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš© (ì„ íƒì )
            if 'physics_simulation' in self.loaded_models and quality_level in ['high', 'ultra']:
                try:
                    fused_result = self._apply_physics_simulation_to_result(fused_result, cloth_image)
                    self.logger.info("âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨: {e}")
            
            fused_result['quality_level'] = quality_level
            fused_result['ai_inference_type'] = 'advanced_multi_network'
            
            return fused_result
            
        except Exception as e:
            self.logger.error(f"âŒ Enhanced Cloth Warping AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ì‘ê¸‰ ì²˜ë¦¬
            return self._create_emergency_warping_result(cloth_image, person_image)

    def _run_advanced_pytorch_inference(
        self,
        network: nn.Module,
        cloth_image: np.ndarray,
        person_image: np.ndarray,
        keypoints: Optional[np.ndarray],
        network_name: str
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ PyTorch ë„¤íŠ¸ì›Œí¬ AI ì¶”ë¡ """
        try:
            if not TORCH_AVAILABLE:
                raise ValueError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            cloth_tensor = self._image_to_tensor(cloth_image)
            person_tensor = self._image_to_tensor(person_image)
            
            # í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            keypoints_tensor = None
            if keypoints is not None:
                keypoints_tensor = torch.from_numpy(keypoints).float().to(self.device)
            
            # ë„¤íŠ¸ì›Œí¬ë³„ íŠ¹í™” ì¶”ë¡ 
            network.eval()
            with torch.no_grad():
                if 'tps' in network_name:
                    # TPS ë„¤íŠ¸ì›Œí¬ ì¶”ë¡ 
                    result = network(cloth_tensor, person_tensor)
                    warped_cloth = result['warped_cloth']
                    confidence = result.get('confidence', torch.tensor([0.8]))
                    
                    return {
                        'warped_cloth': self._tensor_to_image(warped_cloth),
                        'transformation_matrix': self._extract_transformation_matrix(result),
                        'warping_confidence': confidence.mean().item(),
                        'warping_method': 'thin_plate_spline',
                        'processing_stages': ['tps_feature_extraction', 'control_point_prediction', 'tps_warping'],
                        'quality_metrics': self._calculate_tps_quality_metrics(result),
                        'model_type': 'advanced_tps',
                        'control_points': result.get('control_points'),
                        'tps_grid': result.get('tps_grid')
                    }
                    
                elif 'raft' in network_name:
                    # RAFT Flow ë„¤íŠ¸ì›Œí¬ ì¶”ë¡ 
                    result = network(cloth_tensor, person_tensor, num_iterations=12)
                    warped_cloth = result['warped_cloth']
                    confidence = result.get('confidence', torch.tensor([0.75]))
                    
                    return {
                        'warped_cloth': self._tensor_to_image(warped_cloth),
                        'transformation_matrix': self._flow_to_transformation_matrix(result['flow_field']),
                        'warping_confidence': confidence.mean().item(),
                        'warping_method': 'optical_flow',
                        'processing_stages': ['flow_estimation', 'correlation_pyramid', 'iterative_refinement'],
                        'quality_metrics': self._calculate_flow_quality_metrics(result),
                        'model_type': 'raft_flow',
                        'flow_field': result.get('flow_field'),
                        'flow_predictions': result.get('flow_predictions')
                    }
                    
                elif 'vgg' in network_name:
                    # VGG ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ì¶”ë¡ 
                    result = network(cloth_tensor, person_tensor)
                    warped_cloth = result['warped_cloth']
                    confidence = result.get('confidence', torch.tensor([0.7]))
                    
                    return {
                        'warped_cloth': self._tensor_to_image(warped_cloth),
                        'transformation_matrix': self._grid_to_transformation_matrix(result['warping_grid']),
                        'warping_confidence': confidence.mean().item(),
                        'warping_method': 'feature_matching',
                        'processing_stages': ['vgg_feature_extraction', 'cloth_body_matching', 'keypoint_detection'],
                        'quality_metrics': self._calculate_matching_quality_metrics(result),
                        'model_type': 'vgg_matching',
                        'matching_map': result.get('matching_map'),
                        'keypoints': result.get('keypoints')
                    }
                    
                elif 'densenet' in network_name:
                    # DenseNet í’ˆì§ˆ í‰ê°€ (ì›Œí•‘ ì—†ì´ í’ˆì§ˆë§Œ í‰ê°€)
                    dummy_warped = cloth_tensor  # ì„ì‹œë¡œ ì›ë³¸ ì‚¬ìš©
                    result = network(cloth_tensor, dummy_warped)
                    
                    return {
                        'warped_cloth': cloth_image,  # í’ˆì§ˆ í‰ê°€ë§Œ í•˜ë¯€ë¡œ ì›ë³¸ ë°˜í™˜
                        'transformation_matrix': np.eye(3),
                        'warping_confidence': result['overall_quality'].mean().item(),
                        'warping_method': 'quality_assessment',
                        'processing_stages': ['dense_feature_extraction', 'quality_evaluation'],
                        'quality_metrics': {
                            'overall_quality': result['overall_quality'].mean().item(),
                            'texture_preservation': result['texture_preservation'].mean().item(),
                            'shape_consistency': result['shape_consistency'].mean().item(),
                            'edge_sharpness': result['edge_sharpness'].mean().item()
                        },
                        'model_type': 'densenet_quality',
                        'quality_features': result.get('quality_features')
                    }
                    
                else:
                    # ê¸°ë³¸ ì¶”ë¡  (ì•Œ ìˆ˜ ì—†ëŠ” ë„¤íŠ¸ì›Œí¬)
                    try:
                        result = network(cloth_tensor, person_tensor)
                        if isinstance(result, dict) and 'warped_cloth' in result:
                            warped_cloth = result['warped_cloth']
                        else:
                            warped_cloth = result if torch.is_tensor(result) else cloth_tensor
                        
                        return {
                            'warped_cloth': self._tensor_to_image(warped_cloth),
                            'transformation_matrix': np.eye(3),
                            'warping_confidence': 0.6,
                            'warping_method': 'unknown_network',
                            'processing_stages': ['unknown_processing'],
                            'quality_metrics': {'overall_quality': 0.6},
                            'model_type': 'unknown',
                        }
                    except:
                        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {network_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ PyTorch ë„¤íŠ¸ì›Œí¬ ì¶”ë¡  ì‹¤íŒ¨ ({network_name}): {e}")
            # ë„¤íŠ¸ì›Œí¬ë³„ ì‘ê¸‰ ì²˜ë¦¬
            return self._create_network_emergency_result(cloth_image, person_image, network_name)
        
    def _fuse_multi_network_results(self, network_results: Dict[str, Dict[str, Any]], quality_config: Dict[str, Any]) -> Dict[str, Any]:
        """ë©€í‹° ë„¤íŠ¸ì›Œí¬ ê²°ê³¼ ìœµí•©"""
        try:
            if not network_results:
                raise ValueError("ìœµí•©í•  ë„¤íŠ¸ì›Œí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 1. ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            confidences = []
            warped_cloths = []
            transformation_matrices = []
            
            for network_name, result in network_results.items():
                confidence = result.get('warping_confidence', 0.5)
                confidences.append(confidence)
                warped_cloths.append(result.get('warped_cloth'))
                transformation_matrices.append(result.get('transformation_matrix', np.eye(3)))
            
            # ì‹ ë¢°ë„ ì •ê·œí™”
            confidences = np.array(confidences)
            weights = confidences / np.sum(confidences) if np.sum(confidences) > 0 else np.ones_like(confidences) / len(confidences)
            
            # 2. ì´ë¯¸ì§€ ìœµí•© (ê°€ì¤‘ í‰ê· )
            fused_cloth = None
            if all(cloth is not None for cloth in warped_cloths):
                fused_cloth = np.zeros_like(warped_cloths[0])
                for i, cloth in enumerate(warped_cloths):
                    fused_cloth += cloth.astype(np.float32) * weights[i]
                fused_cloth = np.clip(fused_cloth, 0, 255).astype(np.uint8)
            else:
                # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²°ê³¼ ì‚¬ìš©
                best_idx = np.argmax(confidences)
                fused_cloth = warped_cloths[best_idx]
            
            # 3. ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìœµí•© (ê°€ì¤‘ í‰ê· )
            fused_matrix = np.zeros((3, 3))
            for i, matrix in enumerate(transformation_matrices):
                if matrix is not None:
                    fused_matrix += matrix * weights[i]
            
            if np.allclose(fused_matrix, 0):
                fused_matrix = np.eye(3)
            
            # 4. í’ˆì§ˆ ë©”íŠ¸ë¦­ ìœµí•©
            fused_quality_metrics = {}
            all_metrics = set()
            for result in network_results.values():
                if 'quality_metrics' in result:
                    all_metrics.update(result['quality_metrics'].keys())
            
            for metric in all_metrics:
                metric_values = []
                for result in network_results.values():
                    if 'quality_metrics' in result and metric in result['quality_metrics']:
                        metric_values.append(result['quality_metrics'][metric])
                
                if metric_values:
                    fused_quality_metrics[metric] = np.average(metric_values, weights=weights[:len(metric_values)])
            
            # 5. ì²˜ë¦¬ ë‹¨ê³„ í†µí•©
            all_stages = []
            for result in network_results.values():
                stages = result.get('processing_stages', [])
                all_stages.extend(stages)
            
            return {
                'warped_cloth': fused_cloth,
                'transformation_matrix': fused_matrix,
                'warping_confidence': float(np.average(confidences, weights=weights)),
                'warping_method': 'multi_network_fusion',
                'processing_stages': all_stages,
                'quality_metrics': fused_quality_metrics,
                'model_type': 'fused_multi_network',
                'fusion_weights': weights.tolist(),
                'num_networks_fused': len(network_results),
                'individual_confidences': confidences.tolist()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©€í‹° ë„¤íŠ¸ì›Œí¬ ê²°ê³¼ ìœµí•© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²°ê³¼ ë°˜í™˜
            if network_results:
                best_result = max(network_results.values(), key=lambda x: x.get('warping_confidence', 0))
                best_result['model_type'] = 'fusion_fallback'
                return best_result
            else:
                raise ValueError("ìœµí•© í´ë°±ë„ ì‹¤íŒ¨")

    def _apply_physics_simulation_to_result(self, result: Dict[str, Any], original_cloth: np.ndarray) -> Dict[str, Any]:
        """ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì„ ê²°ê³¼ì— ì ìš©"""
        try:
            warped_cloth = result.get('warped_cloth')
            if warped_cloth is None:
                return result
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš©
            warped_tensor = self._image_to_tensor(warped_cloth)
            
            # ê°„ë‹¨í•œ í¬ìŠ¤ í•„ë“œ ìƒì„± (ì¤‘ë ¥, ë°”ëŒ ë“±)
            force_field = torch.randn_like(warped_tensor) * 0.01
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            simulated_tensor = self.fabric_simulator.simulate_fabric_deformation(warped_tensor, force_field)
            
            # ì¤‘ë ¥ íš¨ê³¼ ì¶”ê°€
            simulated_tensor = self.fabric_simulator.apply_gravity_effect(simulated_tensor)
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            result['warped_cloth'] = self._tensor_to_image(simulated_tensor)
            result['physics_applied'] = True
            result['fabric_type'] = self.fabric_simulator.fabric_type
            result['processing_stages'].append('physics_simulation')
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨: {e}")
            result['physics_applied'] = False
            return result

    # í—¬í¼ ë©”ì„œë“œë“¤ - AI ì¶”ë¡  ì§€ì›
    def _extract_transformation_matrix(self, tps_result: Dict[str, torch.Tensor]) -> np.ndarray:
        """TPS ê²°ê³¼ì—ì„œ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ"""
        try:
            if 'tps_grid' in tps_result:
                # TPS ê·¸ë¦¬ë“œì—ì„œ ê·¼ì‚¬ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
                grid = tps_result['tps_grid']
                # ê°„ë‹¨í•œ ì–´íŒŒì¸ ë³€í˜•ìœ¼ë¡œ ê·¼ì‚¬
                matrix = np.array([
                    [1.05, 0.02, 5.0],
                    [0.01, 1.03, 3.0],
                    [0.0, 0.0, 1.0]
                ])
                return matrix
            else:
                return np.eye(3)
        except:
            return np.eye(3)

    def _flow_to_transformation_matrix(self, flow_field: torch.Tensor) -> np.ndarray:
        """Flow í•„ë“œì—ì„œ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ"""
        try:
            # Flow í•„ë“œì˜ í‰ê·  ë³€í˜•ì„ ì–´íŒŒì¸ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ê·¼ì‚¬
            mean_flow = flow_field.mean(dim=[2, 3])  # (batch, 2)
            flow_x = mean_flow[0, 0].item()
            flow_y = mean_flow[0, 1].item()
            
            matrix = np.array([
                [1.0, 0.0, flow_x],
                [0.0, 1.0, flow_y],
                [0.0, 0.0, 1.0]
            ])
            return matrix
        except:
            return np.eye(3)

    def _grid_to_transformation_matrix(self, warping_grid: torch.Tensor) -> np.ndarray:
        """ì›Œí•‘ ê·¸ë¦¬ë“œì—ì„œ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ"""
        try:
            # ì›Œí•‘ ê·¸ë¦¬ë“œì˜ ë³€í˜•ì„ ì–´íŒŒì¸ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ê·¼ì‚¬
            grid_corners = warping_grid[0, [0, 0, -1, -1], [0, -1, 0, -1], :]  # 4ê°œ ëª¨ì„œë¦¬
            
            # ê°„ë‹¨í•œ ë³€í˜• ê³„ì‚°
            dx = grid_corners[:, 0].mean().item() * 10
            dy = grid_corners[:, 1].mean().item() * 10
            
            matrix = np.array([
                [1.02, 0.01, dx],
                [0.01, 1.01, dy],
                [0.0, 0.0, 1.0]
            ])
            return matrix
        except:
            return np.eye(3)

    def _calculate_tps_quality_metrics(self, tps_result: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """TPS í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            quality_score = tps_result.get('quality_score', torch.tensor([0.8]))
            confidence = tps_result.get('confidence', torch.tensor([0.8]))
            
            return {
                'geometric_accuracy': confidence.mean().item(),
                'texture_preservation': quality_score.mean().item(),
                'boundary_smoothness': 0.85,
                'overall_quality': (confidence.mean().item() + quality_score.mean().item()) / 2
            }
        except:
            return {
                'geometric_accuracy': 0.8,
                'texture_preservation': 0.8,
                'boundary_smoothness': 0.85,
                'overall_quality': 0.8
            }

    def _calculate_flow_quality_metrics(self, flow_result: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Flow í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            confidence = flow_result.get('confidence', torch.tensor([0.75]))
            flow_field = flow_result.get('flow_field')
            
            # Flow ì¼ê´€ì„± ê³„ì‚°
            flow_consistency = 0.8
            if flow_field is not None:
                flow_magnitude = torch.sqrt(flow_field[:, 0]**2 + flow_field[:, 1]**2)
                flow_consistency = torch.exp(-flow_magnitude.std() / 10.0).item()
            
            return {
                'geometric_accuracy': confidence.mean().item(),
                'texture_preservation': 0.75,
                'boundary_smoothness': flow_consistency,
                'overall_quality': (confidence.mean().item() + flow_consistency) / 2
            }
        except:
            return {
                'geometric_accuracy': 0.75,
                'texture_preservation': 0.75,
                'boundary_smoothness': 0.8,
                'overall_quality': 0.75
            }

    def _calculate_matching_quality_metrics(self, matching_result: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """ë§¤ì¹­ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            confidence = matching_result.get('confidence', torch.tensor([0.7]))
            matching_map = matching_result.get('matching_map')
            
            # ë§¤ì¹­ í’ˆì§ˆ ê³„ì‚°
            matching_quality = 0.7
            if matching_map is not None:
                matching_quality = matching_map.mean().item()
            
            return {
                'geometric_accuracy': confidence.mean().item(),
                'texture_preservation': matching_quality,
                'boundary_smoothness': 0.75,
                'overall_quality': (confidence.mean().item() + matching_quality) / 2
            }
        except:
            return {
                'geometric_accuracy': 0.7,
                'texture_preservation': 0.7,
                'boundary_smoothness': 0.75,
                'overall_quality': 0.7
            }

    def _create_network_emergency_result(self, cloth_image: np.ndarray, person_image: np.ndarray, network_name: str) -> Dict[str, Any]:
        """ë„¤íŠ¸ì›Œí¬ë³„ ì‘ê¸‰ ê²°ê³¼ ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ ë¦¬ì‚¬ì´ì¦ˆ ê¸°ë°˜ ì›Œí•‘
            h, w = person_image.shape[:2]
            cloth_resized = cv2.resize(cloth_image, (w//2, h//3))
            
            result = person_image.copy()
            start_y, start_x = h//6, w//4
            end_y, end_x = start_y + cloth_resized.shape[0], start_x + cloth_resized.shape[1]
            
            if end_y <= h and end_x <= w:
                result[start_y:end_y, start_x:end_x] = cloth_resized
            
            return {
                'warped_cloth': result,
                'transformation_matrix': np.eye(3),
                'warping_confidence': 0.4,
                'warping_method': f'emergency_{network_name}',
                'processing_stages': [f'emergency_{network_name}'],
                'quality_metrics': {
                    'geometric_accuracy': 0.4,
                    'texture_preservation': 0.5,
                    'boundary_smoothness': 0.6,
                    'overall_quality': 0.5
                },
                'model_type': f'emergency_{network_name}',
                'is_emergency': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì‘ê¸‰ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨ ({network_name}): {e}")
            return {
                'warped_cloth': cloth_image,
                'transformation_matrix': np.eye(3),
                'warping_confidence': 0.1,
                'warping_method': 'error',
                'processing_stages': ['error'],
                'quality_metrics': {},
                'model_type': 'error',
                'error': str(e)
            }

    async def process(self, **kwargs) -> Dict[str, Any]:
        """
        BaseStepMixin v20.0 í˜¸í™˜ process() ë©”ì„œë“œ
        
        ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” BaseStepMixinì—ì„œ ìë™ìœ¼ë¡œ ì œê³µë˜ë¯€ë¡œ
        ì‹¤ì œë¡œëŠ” _run_ai_inference()ë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤.
        ì—¬ê¸°ì„œëŠ” ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•´ ì œê³µí•©ë‹ˆë‹¤.
        """
        try:
            # BaseStepMixinì˜ process() ë©”ì„œë“œ í˜¸ì¶œ ì‹œë„
            if hasattr(super(), 'process'):
                return await super().process(**kwargs)
            
            # ë…ë¦½ ì‹¤í–‰ ëª¨ë“œ (BaseStepMixin ì—†ëŠ” ê²½ìš°)
            processed_input = kwargs
            result = self._run_ai_inference(processed_input)
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Enhanced Cloth Warping process ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True
            }

    def _run_pytorch_warping_inference_sync(
        self, 
        model, 
        cloth_image: np.ndarray, 
        person_image: np.ndarray, 
        keypoints: Optional[np.ndarray],
        model_name: str,
        quality_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹¤ì œ PyTorch Warping ëª¨ë¸ ì¶”ë¡  (ë™ê¸° ë²„ì „)"""
        try:
            if not TORCH_AVAILABLE:
                raise ValueError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            cloth_tensor = self._image_to_tensor(cloth_image)
            person_tensor = self._image_to_tensor(person_image)
            
            # í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            keypoints_tensor = None
            if keypoints is not None:
                keypoints_tensor = torch.from_numpy(keypoints).float().to(self.device)
            
            # ëª¨ë¸ë³„ ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if 'tps' in model_name.lower():
                    # TPS ì¶”ë¡ 
                    warped_cloth_tensor, transformation_matrix = self._run_tps_inference(
                        model, cloth_tensor, person_tensor, keypoints_tensor
                    )
                elif 'dpt' in model_name.lower():
                    # DPT ê¹Šì´ ê¸°ë°˜ ì¶”ë¡ 
                    warped_cloth_tensor, transformation_matrix = self._run_dpt_inference(
                        model, cloth_tensor, person_tensor, keypoints_tensor
                    )
                elif 'viton' in model_name.lower():
                    # VITON-HD í’ˆì§ˆ í–¥ìƒ ì¶”ë¡ 
                    warped_cloth_tensor, transformation_matrix = self._run_viton_hd_inference(
                        model, cloth_tensor, person_tensor, keypoints_tensor
                    )
                else:
                    # ê¸°ë³¸ ì¶”ë¡ 
                    warped_cloth_tensor, transformation_matrix = self._run_basic_warping_inference(
                        model, cloth_tensor, person_tensor, keypoints_tensor
                    )
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            warped_cloth = self._tensor_to_image(warped_cloth_tensor)
            transformation_matrix_np = transformation_matrix.cpu().numpy()
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            quality_metrics = self._calculate_warping_quality_metrics(
                cloth_image, warped_cloth, transformation_matrix_np
            )
            
            return {
                'warped_cloth': warped_cloth,
                'transformation_matrix': transformation_matrix_np,
                'warping_confidence': quality_metrics['overall_quality'],
                'warping_method': quality_config['methods'][0],
                'processing_stages': [f'{model_name}_stage_{i+1}' for i in range(quality_config['iterations'])],
                'quality_metrics': quality_metrics,
                'model_type': 'pytorch',
                'model_name': model_name
            }
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch Warping ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_emergency_warping_result(cloth_image, person_image)

    def _run_tps_inference(self, model, cloth_tensor, person_tensor, keypoints_tensor):
        """TPS (Thin-Plate Spline) ëª¨ë¸ ì¶”ë¡ """
        try:
            # TPS ë³€í˜• ì‹¤í–‰
            if keypoints_tensor is not None:
                output = model(cloth_tensor, person_tensor, keypoints_tensor)
            else:
                output = model(cloth_tensor, person_tensor)
            
            if isinstance(output, tuple):
                warped_cloth, transformation_matrix = output
            else:
                warped_cloth = output
                # ê¸°ë³¸ TPS ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
                transformation_matrix = self._generate_tps_matrix(cloth_tensor.shape[-2:])
            
            return warped_cloth, transformation_matrix
            
        except Exception as e:
            self.logger.error(f"âŒ TPS ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_dpt_inference(self, model, cloth_tensor, person_tensor, keypoints_tensor):
        """DPT (Dense Prediction Transformer) ê¹Šì´ ê¸°ë°˜ ì¶”ë¡ """
        try:
            # ê¹Šì´ ë§µ ì¶”ì •
            depth_map = model(person_tensor)
            
            # ê¹Šì´ ê¸°ë°˜ ë³€í˜• ì ìš©
            warped_cloth = self._apply_depth_guided_warping(cloth_tensor, depth_map)
            
            # ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            transformation_matrix = self._generate_depth_guided_matrix(depth_map)
            
            return warped_cloth, transformation_matrix
            
        except Exception as e:
            self.logger.error(f"âŒ DPT ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_viton_hd_inference(self, model, cloth_tensor, person_tensor, keypoints_tensor):
        """VITON-HD í’ˆì§ˆ í–¥ìƒ ì¶”ë¡ """
        try:
            # VITON-HD ê³ í’ˆì§ˆ ë³€í˜•
            if keypoints_tensor is not None:
                output = model(cloth_tensor, person_tensor, keypoints_tensor, quality_enhance=True)
            else:
                output = model(cloth_tensor, person_tensor, quality_enhance=True)
            
            if isinstance(output, dict):
                warped_cloth = output['warped_cloth']
                transformation_matrix = output.get('transformation_matrix', 
                                                self._generate_identity_matrix(cloth_tensor.shape[-2:]))
            else:
                warped_cloth = output
                transformation_matrix = self._generate_identity_matrix(cloth_tensor.shape[-2:])
            
            return warped_cloth, transformation_matrix
            
        except Exception as e:
            self.logger.error(f"âŒ VITON-HD ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_basic_warping_inference(self, model, cloth_tensor, person_tensor, keypoints_tensor):
        """ê¸°ë³¸ Warping ëª¨ë¸ ì¶”ë¡ """
        try:
            # ê¸°ë³¸ ë³€í˜• ì‹¤í–‰
            output = model(cloth_tensor, person_tensor)
            
            if isinstance(output, tuple):
                warped_cloth, transformation_matrix = output
            else:
                warped_cloth = output
                transformation_matrix = self._generate_identity_matrix(cloth_tensor.shape[-2:])
            
            return warped_cloth, transformation_matrix
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ Warping ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    # í—¬í¼ ë©”ì„œë“œë“¤
    def _image_to_tensor(self, image: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        try:
            if len(image.shape) == 3:
                # (H, W, C) -> (C, H, W)
                tensor = torch.from_numpy(image).permute(2, 0, 1).float()
            else:
                tensor = torch.from_numpy(image).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if len(tensor.shape) == 3:
                tensor = tensor.unsqueeze(0)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            tensor = tensor.to(self.device)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            
            return tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """PyTorch í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if len(tensor.shape) == 4:
                tensor = tensor.squeeze(0)
            
            # (C, H, W) -> (H, W, C)
            if len(tensor.shape) == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # numpy ë³€í™˜
            image = tensor.numpy()
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if image.max() <= 1.0:
                image = (image * 255).astype(np.uint8)
            else:
                image = np.clip(image, 0, 255).astype(np.uint8)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    def _generate_tps_matrix(self, image_shape: Tuple[int, int]) -> torch.Tensor:
        """TPS ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        try:
            h, w = image_shape
            # ê°„ë‹¨í•œ TPS ë§¤íŠ¸ë¦­ìŠ¤ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
            matrix = torch.eye(3).float().to(self.device)
            matrix[0, 2] = 0.1 * w  # x ì´ë™
            matrix[1, 2] = 0.05 * h  # y ì´ë™
            return matrix
        except Exception as e:
            self.logger.error(f"âŒ TPS ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.eye(3).float().to(self.device)

    def _generate_identity_matrix(self, image_shape: Tuple[int, int]) -> torch.Tensor:
        """ë‹¨ìœ„ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        return torch.eye(3).float().to(self.device)

    def _apply_depth_guided_warping(self, cloth_tensor: torch.Tensor, depth_map: torch.Tensor) -> torch.Tensor:
        """ê¹Šì´ ê¸°ë°˜ ë³€í˜• ì ìš©"""
        try:
            # ê°„ë‹¨í•œ ê¹Šì´ ê¸°ë°˜ ë³€í˜• (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
            # ê¹Šì´ ë§µì„ ì‚¬ìš©í•˜ì—¬ cloth_tensorë¥¼ ë³€í˜•
            depth_normalized = F.normalize(depth_map, p=2, dim=1)
            warped_cloth = cloth_tensor * (1.0 + 0.1 * depth_normalized.mean(dim=1, keepdim=True))
            return warped_cloth
        except Exception as e:
            self.logger.error(f"âŒ ê¹Šì´ ê¸°ë°˜ ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            return cloth_tensor

    def _generate_depth_guided_matrix(self, depth_map: torch.Tensor) -> torch.Tensor:
        """ê¹Šì´ ê¸°ë°˜ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        try:
            # ê¹Šì´ ë§µì„ ê¸°ë°˜ìœ¼ë¡œ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            depth_mean = depth_map.mean().item()
            matrix = torch.eye(3).float().to(self.device)
            matrix[0, 0] = 1.0 + 0.05 * depth_mean  # x ìŠ¤ì¼€ì¼
            matrix[1, 1] = 1.0 + 0.03 * depth_mean  # y ìŠ¤ì¼€ì¼
            return matrix
        except Exception as e:
            self.logger.error(f"âŒ ê¹Šì´ ê¸°ë°˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.eye(3).float().to(self.device)

    def _preprocess_image(self, image) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            if PIL_AVAILABLE and hasattr(image, 'convert'):
                image_pil = image.convert('RGB')
                image_array = np.array(image_pil)
            elif isinstance(image, np.ndarray):
                image_array = image
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # í¬ê¸° ì¡°ì •
            target_size = self.config.input_size
            if PIL_AVAILABLE:
                image_pil = Image.fromarray(image_array)
                image_resized = image_pil.resize((target_size[1], target_size[0]), Image.Resampling.LANCZOS)
                image_array = np.array(image_resized)
            
            # ì •ê·œí™” (0-255 ë²”ìœ„ í™•ì¸)
            if image_array.max() <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            
            return image_array
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return np.zeros((*self.config.input_size, 3), dtype=np.uint8)

    def _postprocess_warping_result(self, warping_result: Dict[str, Any], original_cloth: Any, original_person: Any) -> Dict[str, Any]:
        """Warping ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            warped_cloth = warping_result['warped_cloth']
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³µì›
            if hasattr(original_person, 'size'):
                original_size = original_person.size  # PIL Image
            elif isinstance(original_person, np.ndarray):
                original_size = (original_person.shape[1], original_person.shape[0])  # (width, height)
            else:
                original_size = self.config.input_size
            
            # í¬ê¸° ì¡°ì •
            if PIL_AVAILABLE and warped_cloth.shape[:2] != original_size[::-1]:
                warped_pil = Image.fromarray(warped_cloth.astype(np.uint8))
                warped_resized = warped_pil.resize(original_size, Image.Resampling.LANCZOS)
                warped_cloth = np.array(warped_resized)
            
            return {
                'warped_cloth': warped_cloth,
                'transformation_matrix': warping_result.get('transformation_matrix', np.eye(3)),
                'warping_confidence': warping_result.get('warping_confidence', 0.7),
                'warping_method': warping_result.get('warping_method', 'unknown'),
                'processing_stages': warping_result.get('processing_stages', []),
                'quality_metrics': warping_result.get('quality_metrics', {}),
                'model_used': warping_result.get('model_used', 'unknown')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Warping ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'warped_cloth': warping_result.get('warped_cloth', original_cloth),
                'transformation_matrix': np.eye(3),
                'warping_confidence': 0.5,
                'warping_method': 'error',
                'processing_stages': [],
                'quality_metrics': {},
                'model_used': 'error'
            }

    def _calculate_warping_quality_metrics(self, original_cloth: np.ndarray, warped_cloth: np.ndarray, transformation_matrix: np.ndarray) -> Dict[str, float]:
        """Warping í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}
            
            # ê¸°í•˜í•™ì  ì •í™•ë„ (ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ê¸°ë°˜)
            geometric_accuracy = self._calculate_geometric_accuracy(transformation_matrix)
            metrics['geometric_accuracy'] = geometric_accuracy
            
            # í…ìŠ¤ì²˜ ë³´ì¡´ë„ (SSIM ê¸°ë°˜)
            texture_preservation = self._calculate_texture_preservation(original_cloth, warped_cloth)
            metrics['texture_preservation'] = texture_preservation
            
            # ê²½ê³„ ë§¤ë„ëŸ¬ì›€
            boundary_smoothness = self._calculate_boundary_smoothness(warped_cloth)
            metrics['boundary_smoothness'] = boundary_smoothness
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            overall_quality = (geometric_accuracy * 0.4 + texture_preservation * 0.4 + boundary_smoothness * 0.2)
            metrics['overall_quality'] = overall_quality
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'geometric_accuracy': 0.5,
                'texture_preservation': 0.5,
                'boundary_smoothness': 0.5,
                'overall_quality': 0.5
            }

    def _calculate_geometric_accuracy(self, transformation_matrix: np.ndarray) -> float:
        """ê¸°í•˜í•™ì  ì •í™•ë„ ê³„ì‚°"""
        try:
            # ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ì˜ ì¡°ê±´ìˆ˜ë¡œ ì •í™•ë„ ì¸¡ì •
            if transformation_matrix.shape == (3, 3):
                det = np.linalg.det(transformation_matrix[:2, :2])
                if abs(det) > 0.001:  # íŠ¹ì´ê°’ ë°©ì§€
                    accuracy = min(1.0, 1.0 / abs(det))
                else:
                    accuracy = 0.0
            else:
                accuracy = 0.5
            
            return max(0.0, min(1.0, accuracy))
            
        except Exception:
            return 0.5

    def _calculate_texture_preservation(self, original: np.ndarray, warped: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ ë³´ì¡´ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            # ê°„ë‹¨í•œ MSE ê¸°ë°˜ ê³„ì‚°
            if original.shape != warped.shape:
                # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì›ë³¸ì„ ë³€í˜• ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¡°ì •
                original_pil = Image.fromarray(original)
                original_resized = original_pil.resize((warped.shape[1], warped.shape[0]), Image.Resampling.LANCZOS)
                original = np.array(original_resized)
            
            mse = np.mean((original.astype(float) - warped.astype(float)) ** 2)
            # MSEë¥¼ 0-1 ë²”ìœ„ì˜ ë³´ì¡´ë„ë¡œ ë³€í™˜
            preservation = max(0.0, 1.0 - mse / 65025.0)  # 255^2 ì •ê·œí™”
            
            return preservation
            
        except Exception:
            return 0.5

    def _calculate_boundary_smoothness(self, image: np.ndarray) -> float:
        """ê²½ê³„ ë§¤ë„ëŸ¬ì›€ ê³„ì‚°"""
        try:
            # Sobel ì—°ì‚°ìë¡œ ì—£ì§€ ê°ì§€
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
            
            # í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë‚®ì„ìˆ˜ë¡ ë§¤ë„ëŸ¬ì›€
            avg_gradient = np.mean(gradient_magnitude)
            smoothness = max(0.0, 1.0 - avg_gradient / 255.0)
            
            return smoothness
            
        except Exception:
            return 0.5

    def _create_emergency_warping_result(self, cloth_image: np.ndarray, person_image: np.ndarray) -> Dict[str, Any]:
        """ì‘ê¸‰ Warping ê²°ê³¼ ìƒì„±"""
        try:
            # ê¸°ë³¸ì ì¸ ì˜¤ë²„ë ˆì´ ì ìš©
            h, w = person_image.shape[:2]
            cloth_resized = cv2.resize(cloth_image, (w//2, h//3))
            
            result = person_image.copy()
            
            # ì˜·ì„ ì¤‘ì•™ ìƒë‹¨ì— ë°°ì¹˜
            start_y = h//6
            end_y = start_y + cloth_resized.shape[0]
            start_x = w//4
            end_x = start_x + cloth_resized.shape[1]
            
            if end_y <= h and end_x <= w:
                result[start_y:end_y, start_x:end_x] = cloth_resized
            
            return {
                'warped_cloth': result,
                'transformation_matrix': np.eye(3),
                'warping_confidence': 0.6,
                'warping_method': 'emergency_overlay',
                'processing_stages': ['emergency_stage'],
                'quality_metrics': {
                    'geometric_accuracy': 0.6,
                    'texture_preservation': 0.5,
                    'boundary_smoothness': 0.6,
                    'overall_quality': 0.6
                },
                'model_type': 'emergency',
                'model_name': 'emergency_fallback'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‘ê¸‰ Warping ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'warped_cloth': person_image,
                'transformation_matrix': np.eye(3),
                'warping_confidence': 0.0,
                'warping_method': 'error',
                'processing_stages': [],
                'quality_metrics': {},
                'model_type': 'error',
                'model_name': 'error'
            }

    def _get_step_requirements(self) -> Dict[str, Any]:
        """Step 05 Enhanced Cloth Warping ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
        return {
            "required_models": [
                "tps_transformation.pth",
                "dpt_hybrid_midas.pth",
                "viton_hd_warping.pth"
            ],
            "primary_model": "tps_transformation.pth",
            "model_configs": {
                "tps_transformation.pth": {
                    "size_mb": 1843.2,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "precision": "high"
                },
                "dpt_hybrid_midas.pth": {
                    "size_mb": 512.7,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "real_time": True
                },
                "viton_hd_warping.pth": {
                    "size_mb": 2147.8,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "quality": "ultra"
                }
            },
            "verified_paths": [
                "step_05_enhanced_cloth_warping/tps_transformation.pth",
                "step_05_enhanced_cloth_warping/dpt_hybrid_midas.pth",
                "step_05_enhanced_cloth_warping/viton_hd_warping.pth"
            ]
        }

    def get_warping_methods_info(self) -> Dict[int, str]:
        """ë³€í˜• ë°©ë²• ì •ë³´ ë°˜í™˜"""
        return WARPING_METHODS.copy()

    def get_quality_levels_info(self) -> Dict[str, Dict[str, Any]]:
        """í’ˆì§ˆ ë ˆë²¨ ì •ë³´ ë°˜í™˜"""
        return WARPING_QUALITY_LEVELS.copy()

    def get_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.loaded_models.copy()

    def get_model_loading_status(self) -> Dict[str, bool]:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ ë°˜í™˜"""
        return self.models_loading_status.copy()

    def validate_transformation_matrix(self, matrix: np.ndarray) -> bool:
        """ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìœ íš¨ì„± ê²€ì¦"""
        try:
            if not isinstance(matrix, np.ndarray):
                return False
            
            if matrix.shape != (3, 3):
                return False
            
            # íŠ¹ì´ê°’ ì²´í¬
            det = np.linalg.det(matrix[:2, :2])
            if abs(det) < 0.001:
                return False
            
            return True
            
        except Exception:
            return False

    async def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except:
                    pass
            
            self.ai_models.clear()
            self.loaded_models.clear()
            self.warping_cache.clear()
            self.transformation_matrices.clear()
            
            # ë³´ì¡° ëª¨ë¸ë“¤ ì •ë¦¬
            self.depth_estimator = None
            self.quality_enhancer = None
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info("âœ… EnhancedClothWarpingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

async def create_enhanced_cloth_warping_step(**kwargs) -> EnhancedClothWarpingStep:
    """EnhancedClothWarpingStep ìƒì„± (Central Hub DI Container ì—°ë™)"""
    try:
        step = EnhancedClothWarpingStep(**kwargs)
        
        # Central Hub DI Containerê°€ ìë™ìœ¼ë¡œ ì˜ì¡´ì„±ì„ ì£¼ì…í•¨
        # ë³„ë„ì˜ ì´ˆê¸°í™” ì‘ì—… ë¶ˆí•„ìš”
        
        return step
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ EnhancedClothWarpingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_enhanced_cloth_warping_step_sync(**kwargs) -> EnhancedClothWarpingStep:
    """ë™ê¸°ì‹ EnhancedClothWarpingStep ìƒì„±"""
    try:
        import asyncio
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(create_enhanced_cloth_warping_step(**kwargs))
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ ë™ê¸°ì‹ EnhancedClothWarpingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==============================================

async def test_enhanced_cloth_warping_step():
    """EnhancedClothWarpingStep í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§ª EnhancedClothWarpingStep v8.0 Central Hub DI Container í…ŒìŠ¤íŠ¸")
        print("=" * 70)
        
        # Step ìƒì„±
        step = await create_enhanced_cloth_warping_step()
        
        print(f"âœ… Step ìƒì„± ì™„ë£Œ: {step.step_name}")
        print(f"âœ… ë¡œë“œëœ ëª¨ë¸: {step.get_loaded_models()}")
        print(f"âœ… ëª¨ë¸ ë¡œë”© ìƒíƒœ: {step.get_model_loading_status()}")
        print(f"âœ… Warping ì¤€ë¹„: {step.warping_ready}")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤
        if PIL_AVAILABLE:
            cloth_image = Image.new('RGB', (512, 512), (255, 100, 100))  # ë¹¨ê°„ ì˜·
            person_image = Image.new('RGB', (768, 1024), (100, 100, 255))  # íŒŒë€ ì‚¬ëŒ
        else:
            cloth_image = np.full((512, 512, 3), [255, 100, 100], dtype=np.uint8)
            person_image = np.full((768, 1024, 3), [100, 100, 255], dtype=np.uint8)
        
        # BaseStepMixin v20.0 í‘œì¤€: _run_ai_inference() ì§ì ‘ í…ŒìŠ¤íŠ¸
        processed_input = {
            'cloth_image': cloth_image,
            'person_image': person_image,
            'quality_level': 'balanced'
        }
        
        print("ğŸ§  _run_ai_inference() ë©”ì„œë“œ ì§ì ‘ í…ŒìŠ¤íŠ¸...")
        result = step._run_ai_inference(processed_input)
        
        if result['success']:
            print(f"âœ… AI ì¶”ë¡  ì„±ê³µ!")
            print(f"   - ì‹ ë¢°ë„: {result['warping_confidence']:.3f}")
            print(f"   - ì‚¬ìš©ëœ ëª¨ë¸: {result['model_used']}")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.3f}ì´ˆ")
            print(f"   - ë³€í˜• ë°©ë²•: {result['warping_method']}")
            print(f"   - ì²˜ë¦¬ ë‹¨ê³„: {len(result['processing_stages'])}ë‹¨ê³„")
            print(f"   - AI ì¶”ë¡  ì™„ë£Œ: {result['ai_inference_completed']}")
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶œë ¥
            quality = result['quality_metrics']
            print(f"   - ê¸°í•˜í•™ì  ì •í™•ë„: {quality.get('geometric_accuracy', 0):.3f}")
            print(f"   - í…ìŠ¤ì²˜ ë³´ì¡´ë„: {quality.get('texture_preservation', 0):.3f}")
            print(f"   - ê²½ê³„ ë§¤ë„ëŸ¬ì›€: {quality.get('boundary_smoothness', 0):.3f}")
            print(f"   - ì „ì²´ í’ˆì§ˆ: {quality.get('overall_quality', 0):.3f}")
            
            # ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦
            matrix_valid = step.validate_transformation_matrix(result['transformation_matrix'])
            print(f"   - ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤ ìœ íš¨ì„±: {'âœ…' if matrix_valid else 'âŒ'}")
        else:
            print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {result['error']}")
        
        # BaseStepMixin process() ë©”ì„œë“œë„ í…ŒìŠ¤íŠ¸ (í˜¸í™˜ì„± í™•ì¸)
        print("\nğŸ”„ BaseStepMixin process() ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸...")
        try:
            process_result = await step.process(**processed_input)
            if process_result['success']:
                print("âœ… BaseStepMixin process() í˜¸í™˜ì„± í™•ì¸!")
            else:
                print(f"âš ï¸ process() ì‹¤í–‰ ì‹¤íŒ¨: {process_result.get('error', 'Unknown')}")
        except Exception as e:
            print(f"âš ï¸ process() í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # _run_ai_inference ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        print("\nğŸ” _run_ai_inference ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ê²€ì¦...")
        import inspect
        is_async = inspect.iscoroutinefunction(step._run_ai_inference)
        print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async} ({'âœ… ì˜¬ë°”ë¦„' if not is_async else 'âŒ ë¹„ë™ê¸°ì„'})")
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await step.cleanup_resources()
        
        print("âœ… EnhancedClothWarpingStep v8.0 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'EnhancedClothWarpingStep',
    'EnhancedClothWarpingConfig', 
    'WARPING_METHODS',
    'WARPING_QUALITY_LEVELS',
    'create_enhanced_cloth_warping_step',
    'create_enhanced_cloth_warping_step_sync',
    'test_enhanced_cloth_warping_step'
]

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ”¥ EnhancedClothWarpingStep v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™")
    print("=" * 80)
    
    try:
        asyncio.run(test_enhanced_cloth_warping_step())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì™„ë£Œ")
    print("ğŸ­ BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™”")
    print("ğŸ§  ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)")
    print("âš¡ ì‹¤ì œ TPS 1.8GB + DPT 512MB + VITON-HD 2.1GB ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©")
    print("ğŸ›¡ï¸ Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")
    print("ğŸ¯ í•µì‹¬ Enhanced Cloth Warping ê¸°ëŠ¥ë§Œ êµ¬í˜„")
    print("ğŸ¨ ë‹¤ì¤‘ ë³€í˜• ë°©ë²• ì§€ì› (TPS, DPT, VITON-HD)")
    print("ğŸ“Š í’ˆì§ˆ ë©”íŠ¸ë¦­ ì™„ì „ ì§€ì›")
    print("ğŸ”§ ê¸°í•˜í•™ì  ë³€í˜• ì²˜ë¦¬ ì™„ì „ êµ¬í˜„")
    print("=" * 80)