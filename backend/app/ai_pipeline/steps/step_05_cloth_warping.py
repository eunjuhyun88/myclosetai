# app/ai_pipeline/steps/step_05_cloth_warping.py
"""
ğŸ¯ Step 5: ì˜ë¥˜ ì›Œí•‘ (Cloth Warping) - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜
=========================================================================

âœ… BaseStepMixin v19.1 í‘œì¤€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„
âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)
âœ… AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”
âœ… ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„  
âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° í’ˆì§ˆ ë¶„ì„ ì™„ì „ êµ¬í˜„
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

ì‹¤ì œ ì‚¬ìš© ëª¨ë¸ íŒŒì¼:
- RealVisXL_V4.0.safetensors (6.6GB) - ë©”ì¸ ì›Œí•‘ ëª¨ë¸
- vgg19_warping.pth (548MB) - ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
- vgg16_warping_ultra.pth (527MB) - íŠ¹ì§• ì¶”ì¶œ 
- densenet121_ultra.pth (31MB) - ë³€í˜• ê²€ì¶œ
- diffusion_pytorch_model.bin (1.3GB) - Diffusion ì›Œí•‘

Author: MyCloset AI Team
Date: 2025-07-27
Version: 14.0 (BaseStepMixin v19.1 Standard Compliance)
"""

import os
import gc
import time
import math
import logging

import traceback
import threading
import platform
import subprocess
import base64
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, Union, List, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from functools import wraps

logger = logging.getLogger(__name__)
# ==============================================
# ğŸ”§ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”§ BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================
def import_base_step_mixin():
    """BaseStepMixin ë™ì  import"""
    try:
        import importlib
        base_module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(base_module, 'BaseStepMixin')
    except ImportError as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin import ì‹¤íŒ¨: {e}")
        # ìµœì†Œ í´ë°± í´ë˜ìŠ¤
        class BaseStepMixin:
            def __init__(self, **kwargs):
                self.step_name = kwargs.get('step_name', 'ClothWarpingStep')
                self.step_id = kwargs.get('step_id', 5)
                self.device = kwargs.get('device', 'cpu')
                self.logger = logging.getLogger(self.step_name)
        return BaseStepMixin

BaseStepMixin = import_base_step_mixin()

# ==============================================
# ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as T
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# OpenCV ì•ˆì „ import
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    cv2 = None

# SafeTensors ì•ˆì „ import
SAFETENSORS_AVAILABLE = False
try:
    from safetensors import safe_open
    from safetensors.torch import load_file as load_safetensors
    SAFETENSORS_AVAILABLE = True
except ImportError:
    pass

# ==============================================
# ğŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤
# ==============================================

class WarpingMethod(Enum):
    """ì›Œí•‘ ë°©ë²• ì—´ê±°í˜•"""
    REAL_AI_MODEL = "real_ai_model"
    REALVIS_XL = "realvis_xl"
    VGG_WARPING = "vgg_warping"
    DENSENET = "densenet"
    DIFFUSION_WARPING = "diffusion_warping"
    TPS_CLASSICAL = "tps_classical"
    HYBRID = "hybrid"

class FabricType(Enum):
    """ì›ë‹¨ íƒ€ì… ì—´ê±°í˜•"""
    COTTON = "cotton"
    SILK = "silk"
    DENIM = "denim"
    WOOL = "wool"
    POLYESTER = "polyester"
    LINEN = "linen"
    LEATHER = "leather"

class WarpingQuality(Enum):
    """ì›Œí•‘ í’ˆì§ˆ ë ˆë²¨"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class PhysicsProperties:
    """ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì†ì„±"""
    fabric_type: FabricType = FabricType.COTTON
    thickness: float = 0.001  # meters
    density: float = 1500.0  # kg/mÂ³
    elastic_modulus: float = 1000.0  # Pa
    poisson_ratio: float = 0.3
    friction_coefficient: float = 0.4
    air_resistance: float = 0.01

@dataclass
class ClothWarpingConfig:
    """ì˜ë¥˜ ì›Œí•‘ ì„¤ì •"""
    warping_method: WarpingMethod = WarpingMethod.REAL_AI_MODEL
    input_size: Tuple[int, int] = (512, 512)
    num_control_points: int = 25
    ai_model_enabled: bool = True
    physics_enabled: bool = True
    visualization_enabled: bool = True
    cache_enabled: bool = True
    cache_size: int = 50
    quality_level: str = "high"
    precision: str = "fp16"
    memory_fraction: float = 0.6
    batch_size: int = 1
    strict_mode: bool = False
    
    # ì‹¤ì œ AI ëª¨ë¸ ì„¤ì •
    use_realvis_xl: bool = True
    use_vgg19_warping: bool = True
    use_vgg16_warping: bool = True
    use_densenet: bool = True
    use_diffusion_warping: bool = False  # ë©”ëª¨ë¦¬ ì ˆì•½ìš©

# ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘
ENHANCED_STEP_05_MODEL_MAPPING = {
    'realvis_xl': {
        'filename': 'RealVisXL_V4.0.safetensors',
        'size_mb': 6616.6,
        'format': 'safetensors',
        'class': 'RealVisXLModel',
        'priority': 1
    },
    'vgg19_warping': {
        'filename': 'vgg19_warping.pth',
        'size_mb': 548.1,
        'format': 'pth',
        'class': 'RealVGG19WarpingModel',
        'priority': 2
    },
    'vgg16_warping': {
        'filename': 'vgg16_warping_ultra.pth',
        'size_mb': 527.8,
        'format': 'pth',
        'class': 'RealVGG16WarpingModel',
        'priority': 3
    },
    'densenet121': {
        'filename': 'densenet121_ultra.pth',
        'size_mb': 31.0,
        'format': 'pth',
        'class': 'RealDenseNetWarpingModel',
        'priority': 4
    },
    'diffusion_warping': {
        'filename': 'diffusion_pytorch_model.bin',
        'size_mb': 1378.2,
        'format': 'bin',
        'class': 'RealDiffusionWarpingModel',
        'priority': 5
    }
}

# ==============================================
# ğŸ§  AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ í´ë˜ìŠ¤ (OpenCV ì™„ì „ ëŒ€ì²´) - ê¸°ì¡´ íŒŒì¼ ê¸°ëŠ¥ ë³µì›
# ==============================================

class AIImageProcessor:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ í´ë˜ìŠ¤ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(__name__)
        
        # CLIP ëª¨ë¸ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
        self.clip_processor = None
        self.clip_model = None
        
        # Real-ESRGAN (ì—…ìŠ¤ì¼€ì¼ë§ìš©)
        self.esrgan_model = None
        
        # ì´ˆê¸°í™”
        self._initialize_ai_models()
        
    def _initialize_ai_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # CLIP ëª¨ë¸ ë¡œë“œ ì‹œë„
            try:
                from transformers import CLIPProcessor, CLIPModel
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                if TORCH_AVAILABLE:
                    self.clip_model.to(self.device)
                self.logger.info("âœ… CLIP ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                self.logger.debug("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            except Exception as e:
                self.logger.debug(f"CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def ai_mask_generation(self, image: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """AI ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„± (OpenCV threshold ëŒ€ì²´)"""
        try:
            # CLIP ê¸°ë°˜ ì˜ë¥˜ ì˜ì—­ ê°ì§€
            if self.clip_model and self.clip_processor:
                pil_img = Image.fromarray(image)
                inputs = self.clip_processor(images=pil_img, return_tensors="pt")
                
                with torch.no_grad():
                    image_features = self.clip_model.get_image_features(**inputs)
                    # ì˜ë¥˜ ê´€ë ¨ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
                    # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì‚¬ìš©)
                    
            # í´ë°±: ê°„ë‹¨í•œ ìƒ‰ìƒ ê¸°ë°˜ ë§ˆìŠ¤í¬
            gray = self._rgb_to_grayscale(image)
            mask = (gray > threshold * 255).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"AI ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±
            gray = self._rgb_to_grayscale(image)
            return (gray > threshold * 255).astype(np.uint8) * 255
    
    def ai_color_conversion(self, image: np.ndarray, conversion_type: str = "RGB2BGR") -> np.ndarray:
        """AI ê¸°ë°˜ ìƒ‰ìƒ ë³€í™˜ (OpenCV cvtColor ëŒ€ì²´)"""
        try:
            if conversion_type == "RGB2BGR" or conversion_type == "BGR2RGB":
                # ë‹¨ìˆœ ì±„ë„ ìˆœì„œ ë³€ê²½
                return image[:, :, ::-1]
            elif conversion_type == "RGB2GRAY" or conversion_type == "BGR2GRAY":
                return self._rgb_to_grayscale(image)
            else:
                return image
                
        except Exception as e:
            self.logger.warning(f"ìƒ‰ìƒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    def ai_edge_detection(self, image: np.ndarray, low_threshold: int = 50, high_threshold: int = 150) -> np.ndarray:
        """AI ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ (OpenCV Canny ëŒ€ì²´)"""
        try:
            # Sobel ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ
            gray = self._rgb_to_grayscale(image)
            
            if TORCH_AVAILABLE:
                # PyTorch Sobel í•„í„°
                tensor = torch.from_numpy(gray).unsqueeze(0).unsqueeze(0).float().to(self.device)
                
                # Sobel ì»¤ë„
                sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
                sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
                
                # ì—£ì§€ ê²€ì¶œ
                edges_x = F.conv2d(tensor, sobel_x, padding=1)
                edges_y = F.conv2d(tensor, sobel_y, padding=1)
                edges = torch.sqrt(edges_x**2 + edges_y**2)
                
                # ì„ê³„ê°’ ì ìš©
                edges = (edges > low_threshold).float() * 255
                
                return edges.squeeze().cpu().numpy().astype(np.uint8)
            
            # NumPy í´ë°±
            return self._simple_edge_detection(gray, low_threshold)
            
        except Exception as e:
            self.logger.warning(f"AI ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return self._simple_edge_detection(gray, low_threshold)
    
    def _rgb_to_grayscale(self, image: np.ndarray) -> np.ndarray:
        """RGBë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜"""
        if len(image.shape) == 3:
            # í‘œì¤€ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            return np.dot(image[...,:3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)
        return image
    
    def _simple_edge_detection(self, gray: np.ndarray, threshold: int) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ"""
        # ê°„ë‹¨í•œ Sobel í•„í„° êµ¬í˜„
        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
        
        # íŒ¨ë”© ì¶”ê°€
        padded = np.pad(gray, 1, mode='edge')
        
        edges = np.zeros_like(gray)
        for i in range(gray.shape[0]):
            for j in range(gray.shape[1]):
                gx = np.sum(padded[i:i+3, j:j+3] * sobel_x)
                gy = np.sum(padded[i:i+3, j:j+3] * sobel_y)
                edges[i, j] = min(255, int(np.sqrt(gx**2 + gy**2)))
        
        return (edges > threshold).astype(np.uint8) * 255
    
    def ai_resize(self, image: np.ndarray, target_size: Tuple[int, int], mode: str = "bilinear") -> np.ndarray:
        """AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•"""
        try:
            if not TORCH_AVAILABLE:
                # PIL í´ë°±
                pil_img = Image.fromarray(image)
                pil_resample = {
                    "nearest": Image.NEAREST,
                    "bilinear": Image.BILINEAR, 
                    "bicubic": Image.BICUBIC,
                    "lanczos": Image.LANCZOS
                }.get(mode.lower(), Image.LANCZOS)
                resized = pil_img.resize(target_size, pil_resample)
                return np.array(resized)
            
            # PyTorch ê¸°ë°˜ ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì§•
            if len(image.shape) == 3:
                tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0
            else:
                tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).float() / 255.0
            
            tensor = tensor.to(self.device)
            
            torch_mode = {
                "nearest": "nearest",
                "bilinear": "bilinear", 
                "bicubic": "bicubic",
                "lanczos": "bilinear"
            }.get(mode.lower(), "bilinear")
            
            resized_tensor = F.interpolate(tensor, size=target_size, mode=torch_mode, align_corners=False)
            
            if len(image.shape) == 3:
                result = resized_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
            else:
                result = resized_tensor.squeeze().cpu().numpy()
            
            return (result * 255).astype(np.uint8)
            
        except Exception as e:
            self.logger.warning(f"AI ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨, PIL í´ë°±: {e}")
            try:
                pil_img = Image.fromarray(image)
                resized = pil_img.resize(target_size, Image.LANCZOS)
                return np.array(resized)
            except Exception as e2:
                self.logger.error(f"PIL í´ë°±ë„ ì‹¤íŒ¨: {e2}")
                return image
    
    def ai_geometric_transform(self, image: np.ndarray, transform_matrix: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE:
                # PIL í´ë°±
                pil_img = Image.fromarray(image)
                return np.array(pil_img)
            
            # PyTorch ê¸°ë°˜ ë³€í™˜
            tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0
            tensor = tensor.to(self.device)
            
            # Affine grid ìƒì„±
            transform_tensor = torch.from_numpy(transform_matrix[:2]).unsqueeze(0).float().to(self.device)
            grid = F.affine_grid(transform_tensor, tensor.size(), align_corners=False)
            
            # ë³€í™˜ ì ìš©
            warped_tensor = F.grid_sample(tensor, grid, align_corners=False)
            
            # numpyë¡œ ë³€í™˜
            result = warped_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
            return (result * 255).astype(np.uint8)
            
        except Exception as e:
            self.logger.warning(f"AI ê¸°í•˜í•™ì  ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image

# ==============================================
# ğŸ”§ ê³ ê¸‰ TPS ë³€í™˜ ì‹œìŠ¤í…œ (AI ê¸°ë°˜)
# ==============================================

class AdvancedTPSTransform:
    """ê³ ê¸‰ TPS (Thin Plate Spline) ë³€í™˜ - AI ëª¨ë¸ ê¸°ë°˜"""
    
    def __init__(self, num_control_points: int = 25):
        self.num_control_points = num_control_points
        self.logger = logging.getLogger(__name__)
        self.ai_processor = AIImageProcessor()
    
    def create_adaptive_control_grid(self, width: int, height: int) -> np.ndarray:
        """ì ì‘ì  ì œì–´ì  ê·¸ë¦¬ë“œ ìƒì„±"""
        grid_size = int(np.sqrt(self.num_control_points))
        if grid_size * grid_size < self.num_control_points:
            grid_size += 1
        
        points = []
        
        for i in range(grid_size):
            for j in range(grid_size):
                if len(points) >= self.num_control_points:
                    break
                x = (width - 1) * i / max(1, grid_size - 1)
                y = (height - 1) * j / max(1, grid_size - 1)
                points.append([x, y])
        
        return np.array(points[:self.num_control_points])
    
    def apply_transform(self, image: np.ndarray, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë³€í™˜ ì ìš© (AI ê¸°ë°˜)"""
        try:
            if len(source_points) >= 3 and len(target_points) >= 3:
                # ê°„ë‹¨í•œ ì–´íŒŒì¸ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
                src_pts = source_points[:3].astype(np.float32)
                dst_pts = target_points[:3].astype(np.float32)
                
                # ì–´íŒŒì¸ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
                transform_matrix = self._calculate_affine_matrix(src_pts, dst_pts)
                
                # AI ê¸°ë°˜ ê¸°í•˜í•™ì  ë³€í™˜ ì ìš©
                return self.ai_processor.ai_geometric_transform(image, transform_matrix)
            
            return image
        except Exception as e:
            self.logger.warning(f"AI ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_affine_matrix(self, src_pts: np.ndarray, dst_pts: np.ndarray) -> np.ndarray:
        """ì–´íŒŒì¸ ë³€í™˜ í–‰ë ¬ ê³„ì‚°"""
        try:
            # 3ì ì„ ì´ìš©í•œ ì–´íŒŒì¸ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
            A = np.column_stack([src_pts, np.ones(3)])
            B = dst_pts
            
            # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
            M = np.linalg.lstsq(A, B, rcond=None)[0]
            
            # 3x3 í˜•íƒœë¡œ í™•ì¥
            transform_matrix = np.vstack([M.T, [0, 0, 1]])
            
            return transform_matrix
            
        except Exception as e:
            self.logger.warning(f"ì–´íŒŒì¸ í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(3)

# ==============================================
# ğŸ”¬ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ (AI ê°•í™”)
# ==============================================

class ClothPhysicsSimulator:
    """ì˜ë¥˜ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ - AI ê°•í™”"""
    
    def __init__(self, properties: PhysicsProperties):
        self.properties = properties
        self.mesh_vertices = None
        self.mesh_faces = None
        self.velocities = None
        self.forces = None
        self.logger = logging.getLogger(__name__)
        
    def create_cloth_mesh(self, width: int, height: int, resolution: int = 32) -> Tuple[np.ndarray, np.ndarray]:
        """ì˜ë¥˜ ë©”ì‹œ ìƒì„±"""
        try:
            x = np.linspace(0, width-1, resolution)
            y = np.linspace(0, height-1, resolution)
            xx, yy = np.meshgrid(x, y)
            
            vertices = np.column_stack([xx.flatten(), yy.flatten(), np.zeros(xx.size)])
            
            faces = []
            for i in range(resolution-1):
                for j in range(resolution-1):
                    idx = i * resolution + j
                    faces.append([idx, idx+1, idx+resolution])
                    faces.append([idx+1, idx+resolution+1, idx+resolution])
            
            self.mesh_vertices = vertices
            self.mesh_faces = np.array(faces)
            self.velocities = np.zeros_like(vertices)
            self.forces = np.zeros_like(vertices)
            
            return vertices, self.mesh_faces
            
        except Exception as e:
            self.logger.error(f"ë©”ì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë©”ì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def simulate_step(self, dt: float = 0.016):
        """ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ ì‹¤í–‰"""
        if self.mesh_vertices is None:
            raise ValueError("ë©”ì‹œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        try:
            gravity = np.array([0, 0, -9.81]) * self.properties.density * dt
            self.forces[:, 2] += gravity[2]
            
            acceleration = self.forces / self.properties.density
            self.mesh_vertices += self.velocities * dt + 0.5 * acceleration * dt * dt
            self.velocities += acceleration * dt
            
            self.velocities *= (1.0 - self.properties.friction_coefficient * dt)
            self.forces.fill(0)
            
        except Exception as e:
            self.logger.error(f"ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
    
    def get_deformed_mesh(self) -> np.ndarray:
        """ë³€í˜•ëœ ë©”ì‹œ ë°˜í™˜"""
        if self.mesh_vertices is None:
            raise ValueError("ë©”ì‹œê°€ ì—†ìŠµë‹ˆë‹¤")
        return self.mesh_vertices.copy()

# ==============================================
# ğŸ¨ ì›Œí•‘ ì‹œê°í™” ì—”ì§„ (AI ê¸°ë°˜)
# ==============================================

class WarpingVisualizer:
    """ì›Œí•‘ ê³¼ì • ì‹œê°í™” ì—”ì§„ - AI ê¸°ë°˜"""
    
    def __init__(self, quality: str = "high"):
        self.quality = quality
        self.dpi = {"low": 72, "medium": 150, "high": 300, "ultra": 600}[quality]
        self.logger = logging.getLogger(__name__)
        self.ai_processor = AIImageProcessor()
        
    def create_warping_visualization(self, 
                                   original_cloth: np.ndarray,
                                   warped_cloth: np.ndarray,
                                   control_points: np.ndarray) -> np.ndarray:
        """ì›Œí•‘ ê³¼ì • ì¢…í•© ì‹œê°í™” (AI ê¸°ë°˜)"""
        try:
            h, w = original_cloth.shape[:2]
            canvas_w = w * 2
            canvas_h = h
            
            # AI ê¸°ë°˜ ìº”ë²„ìŠ¤ ìƒì„±
            canvas = np.ones((canvas_h, canvas_w, 3), dtype=np.uint8) * 255
            
            # ì›ë³¸ (ì¢Œì¸¡) - AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§•
            original_resized = self.ai_processor.ai_resize(original_cloth, (w, h))
            canvas[0:h, 0:w] = original_resized
            
            # ì›Œí•‘ ê²°ê³¼ (ìš°ì¸¡) - AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§•
            warped_resized = self.ai_processor.ai_resize(warped_cloth, (w, h))
            canvas[0:h, w:2*w] = warped_resized
            
            # ì œì–´ì  ì‹œê°í™”
            if len(control_points) > 0:
                canvas = self._draw_control_points_ai(canvas, control_points, w, h)
            
            # êµ¬ë¶„ì„  ê·¸ë¦¬ê¸°
            canvas = self._draw_divider_line_ai(canvas, w, h)
            
            return canvas
        except Exception as e:
            self.logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            try:
                h, w = original_cloth.shape[:2]
                canvas = np.hstack([original_cloth, warped_cloth])
                return canvas
            except:
                return original_cloth
    
    def _draw_control_points_ai(self, canvas: np.ndarray, control_points: np.ndarray, w: int, h: int) -> np.ndarray:
        """AI ê¸°ë°˜ ì œì–´ì  ê·¸ë¦¬ê¸°"""
        try:
            for i, point in enumerate(control_points[:min(10, len(control_points))]):
                x, y = int(point[0]), int(point[1])
                if 0 <= x < w and 0 <= y < h:
                    # ì›í˜• ì  ê·¸ë¦¬ê¸°
                    self._draw_circle_ai(canvas, (x, y), 3, (255, 0, 0))
                    self._draw_circle_ai(canvas, (x + w, y), 3, (0, 255, 0))
            return canvas
        except Exception as e:
            self.logger.warning(f"ì œì–´ì  ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return canvas
    
    def _draw_circle_ai(self, image: np.ndarray, center: Tuple[int, int], radius: int, color: Tuple[int, int, int]):
        """AI ê¸°ë°˜ ì› ê·¸ë¦¬ê¸°"""
        try:
            x_center, y_center = center
            h, w = image.shape[:2]
            
            # ì› ì¢Œí‘œ ê³„ì‚°
            y, x = np.ogrid[:h, :w]
            mask = (x - x_center)**2 + (y - y_center)**2 <= radius**2
            
            # ìƒ‰ìƒ ì ìš©
            image[mask] = color
            
        except Exception as e:
            self.logger.warning(f"ì› ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
    
    def _draw_divider_line_ai(self, canvas: np.ndarray, w: int, h: int) -> np.ndarray:
        """AI ê¸°ë°˜ êµ¬ë¶„ì„  ê·¸ë¦¬ê¸°"""
        try:
            # ìˆ˜ì§ì„  ê·¸ë¦¬ê¸°
            canvas[:, w:w+2] = [128, 128, 128]
            return canvas
        except Exception as e:
            self.logger.warning(f"êµ¬ë¶„ì„  ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return canvas

# ==============================================
# ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì™„ì „í•œ êµ¬í˜„)
# ==============================================

class RealClothWarpingModel(nn.Module):
    """ì‹¤ì œ ì˜ë¥˜ ì›Œí•‘ AI ëª¨ë¸ (TOM/HRVITON ê¸°ë°˜)"""
    
    def __init__(self, num_control_points: int = 25, input_channels: int = 6):
        super().__init__()
        self.num_control_points = num_control_points
        
        # Feature Extractor (ResNet ê¸°ë°˜)
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 64, 7, 2, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # ResNet Block 1
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # ResNet Block 2
            nn.Conv2d(128, 256, 3, 2, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            # Global Average Pooling
            nn.AdaptiveAvgPool2d(1)
        )
        
        # TPS Parameter Regressor
        self.tps_regressor = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, num_control_points * 2)  # x, y coordinates
        )
        
        # Flow Field Generator
        self.flow_generator = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 2, 3, 1, 1),  # flow field (dx, dy)
            nn.Tanh()
        )
        
        # Warping Quality Predictor
        self.quality_predictor = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        batch_size = cloth_image.size(0)
        
        # ì…ë ¥ ì—°ê²°
        combined_input = torch.cat([cloth_image, person_image], dim=1)
        
        # Feature ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        features_flat = features.view(batch_size, -1)
        
        # TPS íŒŒë¼ë¯¸í„° ìƒì„±
        tps_params = self.tps_regressor(features_flat)
        tps_params = tps_params.view(batch_size, self.num_control_points, 2)
        
        # Flow Field ìƒì„±
        flow_field = self.flow_generator(combined_input)
        
        # í’ˆì§ˆ ì˜ˆì¸¡
        quality_score = self.quality_predictor(combined_input)
        
        # TPS ë³€í™˜ ì ìš©
        warped_cloth = self._apply_tps_transform(cloth_image, tps_params)
        
        # Flow Field ì ìš© (ì¶”ê°€ì ì¸ fine-tuning)
        final_warped = self._apply_flow_field(warped_cloth, flow_field)
        
        return {
            'warped_cloth': final_warped,
            'tps_params': tps_params,
            'flow_field': flow_field,
            'quality_score': quality_score,
            'confidence': self._calculate_confidence(cloth_image, final_warped)
        }
    
    def _apply_tps_transform(self, cloth_image: torch.Tensor, tps_params: torch.Tensor) -> torch.Tensor:
        """TPS ë³€í™˜ ì ìš©"""
        try:
            batch_size, channels, height, width = cloth_image.shape
            
            # ê°„ë‹¨í•œ ì–´íŒŒì¸ ë³€í™˜ìœ¼ë¡œ ê·¼ì‚¬
            theta = torch.zeros(batch_size, 2, 3, device=cloth_image.device)
            theta[:, 0, 0] = 1.0
            theta[:, 1, 1] = 1.0
            
            # TPS íŒŒë¼ë¯¸í„°ë¥¼ ì–´íŒŒì¸ íŒŒë¼ë¯¸í„°ë¡œ ê·¼ì‚¬ ë³€í™˜
            if tps_params.size(-1) >= 2:
                mean_params = tps_params.mean(dim=1)  # [B, 2]
                theta[:, 0, 2] = mean_params[:, 0] * 0.1  # translation x
                theta[:, 1, 2] = mean_params[:, 1] * 0.1  # translation y
            
            # Grid ìƒì„± ë° ìƒ˜í”Œë§
            grid = F.affine_grid(theta, cloth_image.size(), align_corners=False)
            warped = F.grid_sample(cloth_image, grid, align_corners=False)
            
            return warped
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"TPS ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
class RealTOMModel(nn.Module):
    """ì‹¤ì œ TOM (Try-On Model) AI ëª¨ë¸"""
    
    def __init__(self, input_size: Tuple[int, int] = (512, 384)):
        super().__init__()
        self.input_size = input_size
        
        # Encoder
        self.cloth_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
        )
        
        self.person_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
        )
        
        # Fusion Layer
        self.fusion = nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.Tanh()
        )
        
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        # Encode
        cloth_features = self.cloth_encoder(cloth_image)
        person_features = self.person_encoder(person_image)
        
        # Fuse
        combined_features = torch.cat([cloth_features, person_features], dim=1)
        fused_features = self.fusion(combined_features)
        
        # Decode
        output = self.decoder(fused_features)
        
        return output

class EnhancedImageMatchingNetwork(nn.Module):
    """ê°•í™”ëœ ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self):
        super().__init__()
        
        # íŠ¹ì§• ì¶”ì¶œê¸° (VGG ìŠ¤íƒ€ì¼)
        self.feature_extractor = nn.Sequential(
            # ë ˆë²¨ 1
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # ë ˆë²¨ 2
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # ë ˆë²¨ 3
            nn.Conv2d(128, 256, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        # ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬
        self.matcher = nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1),  # cloth + person features
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 3, 1, 1),
            nn.Sigmoid()
        )
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œê¸°
        self.keypoint_detector = nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 25, 3, 1, 1),  # 25 keypoints
            nn.Sigmoid()
        )
        
        # ë§¤ì¹­ í’ˆì§ˆ í‰ê°€ê¸°
        self.quality_assessor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ì´ë¯¸ì§€ ë§¤ì¹­ ìˆ˜í–‰"""
        # ê°ê°ì˜ íŠ¹ì§• ì¶”ì¶œ
        cloth_features = self.feature_extractor(cloth_image)
        person_features = self.feature_extractor(person_image)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([cloth_features, person_features], dim=1)
        
        # ë§¤ì¹­ ë§µ ìƒì„±
        matching_map = self.matcher(combined_features)
        
        # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        keypoints = self.keypoint_detector(cloth_features)
        
        # í’ˆì§ˆ í‰ê°€
        quality_score = self.quality_assessor(combined_features)
        
        return {
            'matching_map': matching_map,
            'keypoints': keypoints,
            'quality_score': quality_score,
            'cloth_features': cloth_features,
            'person_features': person_features
        }
    
    def _apply_flow_field(self, cloth_image: torch.Tensor, flow_field: torch.Tensor) -> torch.Tensor:
        """Flow Field ì ìš©"""
        try:
            batch_size, channels, height, width = cloth_image.shape
            
            # ì •ê·œí™”ëœ grid ìƒì„± [-1, 1]
            y_coords = torch.linspace(-1, 1, height, device=cloth_image.device)
            x_coords = torch.linspace(-1, 1, width, device=cloth_image.device)
            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
            
            # Gridë¥¼ batch ì°¨ì›ìœ¼ë¡œ í™•ì¥
            grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            # Flow field ì¶”ê°€ (ìŠ¤ì¼€ì¼ë§ ì ìš©)
            flow_scaled = flow_field * 0.1  # ë³€í˜• ì •ë„ ì¡°ì ˆ
            grid = grid + flow_scaled
            
            # Grid í˜•íƒœ ë³€ê²½: [B, H, W, 2]
            grid = grid.permute(0, 2, 3, 1)
            
            # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§
            warped = F.grid_sample(cloth_image, grid, align_corners=False)
            
            return warped
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"Flow Field ì ìš© ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            return cloth_image
    
    def _calculate_confidence(self, original: torch.Tensor, warped: torch.Tensor) -> torch.Tensor:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ MSE ê¸°ë°˜ ì‹ ë¢°ë„
            mse = F.mse_loss(original, warped, reduction='none')
            confidence = torch.exp(-mse.mean(dim=[1, 2, 3]))
            return confidence
        except:
            return torch.ones(original.size(0), device=original.device) * 0.8

class EnhancedRealVisXLModel(nn.Module):
    """ê°•í™”ëœ RealVisXL ëª¨ë¸ - ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, input_channels: int = 6):
        super().__init__()
        self.input_channels = input_channels
        
        # íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬ (ë” ê¹Šê³  ê°•í™”ë¨)
        self.feature_extractor = nn.Sequential(
            # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
            nn.Conv2d(input_channels, 64, 7, 2, 3),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            nn.Conv2d(64, 128, 3, 2, 1),
            nn.GroupNorm(16, 128),
            nn.SiLU(),
            
            # ResNet ë¸”ë¡ë“¤
            self._make_residual_block(128, 128),
            self._make_residual_block(128, 256, stride=2),
            self._make_residual_block(256, 256),
            self._make_residual_block(256, 512, stride=2),
            self._make_residual_block(512, 512),
        )
        
        # ê³ ê¸‰ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬
        self.matching_network = nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 128, 3, 1, 1),
            nn.GroupNorm(16, 128),
            nn.SiLU(),
        )
        
        # ì›Œí•‘ í•„ë“œ ìƒì„±ê¸° (ê³ í•´ìƒë„)
        self.warping_generator = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.GroupNorm(8, 32),
            nn.SiLU(),
            nn.ConvTranspose2d(32, 16, 4, 2, 1),
            nn.GroupNorm(4, 16),
            nn.SiLU(),
            nn.ConvTranspose2d(16, 2, 4, 2, 1),
            nn.Tanh()
        )
        
        # í’ˆì§ˆ ì˜ˆì¸¡ê¸°
        self.quality_predictor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def _make_residual_block(self, in_channels: int, out_channels: int, stride: int = 1):
        """ResNet ìŠ¤íƒ€ì¼ ì”ì°¨ ë¸”ë¡"""
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1))
        layers.append(nn.GroupNorm(min(32, out_channels//4), out_channels))
        layers.append(nn.SiLU())
        layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1))
        layers.append(nn.GroupNorm(min(32, out_channels//4), out_channels))
        
        return nn.Sequential(*layers)
    
    def forward(self, cloth_image: torch.Tensor, person_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ê°•í™”ëœ ìˆœì „íŒŒ - ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì¹­"""
        batch_size = cloth_image.size(0)
        
        # ì…ë ¥ ê²°í•© ë° ì „ì²˜ë¦¬
        combined_input = torch.cat([cloth_image, person_image], dim=1)
        
        # ê³„ì¸µì  íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        
        # ë§¤ì¹­ íŠ¹ì§• ê°•í™”
        matched_features = self.matching_network(features)
        
        # ê³ í•´ìƒë„ ì›Œí•‘ í•„ë“œ ìƒì„±
        warping_field = self.warping_generator(matched_features)
        
        # í’ˆì§ˆ í‰ê°€
        quality_score = self.quality_predictor(matched_features)
        
        # ê³ ê¸‰ ì›Œí•‘ ì ìš©
        warped_cloth = self._apply_advanced_warping(cloth_image, warping_field, quality_score)
        
        return {
            'warped_cloth': warped_cloth,
            'warping_field': warping_field,
            'quality_score': quality_score,
            'confidence': torch.mean(quality_score),
            'features': matched_features
        }
    
    def _apply_advanced_warping(self, cloth_image: torch.Tensor, warping_field: torch.Tensor, 
                               quality_score: torch.Tensor) -> torch.Tensor:
        """ê³ ê¸‰ ì›Œí•‘ ì ìš© - í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì ì‘ì  ì›Œí•‘"""
        try:
            batch_size, channels, height, width = cloth_image.shape
            
            # í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ì›Œí•‘ ê°•ë„ ì¡°ì ˆ
            warping_strength = quality_score.view(-1, 1, 1, 1) * 0.1
            
            # ì •ê·œí™”ëœ ê·¸ë¦¬ë“œ ìƒì„±
            y_coords = torch.linspace(-1, 1, height, device=cloth_image.device)
            x_coords = torch.linspace(-1, 1, width, device=cloth_image.device)
            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
            
            grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            # ì ì‘ì  ì›Œí•‘ í•„ë“œ ì ìš©
            scaled_warping = warping_field * warping_strength
            deformed_grid = grid + scaled_warping
            
            # ê²½ê³„ ì œì•½ ì ìš©
            deformed_grid = torch.clamp(deformed_grid, -1, 1)
            deformed_grid = deformed_grid.permute(0, 2, 3, 1)
            
            # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§
            warped = F.grid_sample(cloth_image, deformed_grid, 
                                 mode='bilinear', padding_mode='border', align_corners=False)
            
            return warped
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"ê³ ê¸‰ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return cloth_image

# ==============================================
# ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ë¡œë”
# ==============================================

class EnhancedCheckpointLoader:
    """ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë”"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(__name__)
        self.loaded_checkpoints = {}
        
        # ê²€ìƒ‰ ê²½ë¡œë“¤
        self.search_paths = [
            "step_05_cloth_warping",
            "step_05_cloth_warping/ultra_models",
            "step_05_cloth_warping/ultra_models/unet",
            "step_05_cloth_warping/ultra_models/safety_checker"
        ]
        
        self.fallback_paths = [
            "checkpoints/step_05_cloth_warping"
        ]
        
    def load_checkpoint(self, model_name: str) -> Optional[Dict[str, Any]]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model_info = ENHANCED_STEP_05_MODEL_MAPPING.get(model_name)
            if not model_info:
                self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
                return None
            
            filename = model_info['filename']
            format_type = model_info['format']
            
            # ê²€ìƒ‰ ê²½ë¡œì—ì„œ ì°¾ê¸°
            for search_path in self.search_paths:
                checkpoint_path = Path(f"{search_path}/{filename}")
                if checkpoint_path.exists():
                    self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_path}")
                    return self._load_checkpoint_file(checkpoint_path, format_type)
            
            # í´ë°± ê²½ë¡œì—ì„œ ì°¾ê¸°
            for fallback_path in self.fallback_paths:
                checkpoint_path = Path(f"{fallback_path}/{filename}")
                if checkpoint_path.exists():
                    self.logger.info(f"í´ë°± ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_path}")
                    return self._load_checkpoint_file(checkpoint_path, format_type)
            
            self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename}")
            return None
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_checkpoint_file(self, checkpoint_path: Path, format_type: str) -> Optional[Dict[str, Any]]:
        """ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”©"""
        try:
            self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘: {checkpoint_path.name} ({format_type})")
            
            if format_type == "safetensors" and SAFETENSORS_AVAILABLE:
                return self._load_safetensors(checkpoint_path)
            elif format_type in ["pth", "pt"]:
                return self._load_pytorch(checkpoint_path)
            elif format_type == "bin":
                return self._load_bin(checkpoint_path)
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§·: {format_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_safetensors(self, checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """SafeTensors í¬ë§· ë¡œë”©"""
        try:
            checkpoint = load_safetensors(str(checkpoint_path), device=self.device)
            
            try:
                with safe_open(str(checkpoint_path), framework="pt", device=self.device) as f:
                    metadata = f.metadata() if hasattr(f, 'metadata') else {}
            except:
                metadata = {}
            
            return {
                'state_dict': checkpoint,
                'metadata': metadata,
                'format': 'safetensors',
                'device': self.device,
                'path': str(checkpoint_path)
            }
            
        except Exception as e:
            self.logger.error(f"SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_pytorch(self, checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """PyTorch í¬ë§· ë¡œë”©"""
        try:
            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                safe_mode = True
            except:
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                safe_mode = False
            
            if isinstance(checkpoint, dict):
                return {
                    'checkpoint': checkpoint,
                    'format': 'pytorch',
                    'device': self.device,
                    'safe_mode': safe_mode,
                    'path': str(checkpoint_path)
                }
            else:
                return {
                    'state_dict': checkpoint,
                    'format': 'pytorch',
                    'device': self.device,
                    'safe_mode': safe_mode,
                    'path': str(checkpoint_path)
                }
                
        except Exception as e:
            self.logger.error(f"PyTorch ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_bin(self, checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """.bin í¬ë§· ë¡œë”©"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            return {
                'checkpoint': checkpoint,
                'format': 'bin',
                'device': self.device,
                'path': str(checkpoint_path)
            }
            
        except Exception as e:
            self.logger.error(f"BIN ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_checkpoint_fallback(self, model_name: str) -> Optional[Dict[str, Any]]:
        """í´ë°± ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            model_info = ENHANCED_STEP_05_MODEL_MAPPING.get(model_name)
            if not model_info:
                return None
            
            filename = model_info['filename']
            
            # ê¸°ë³¸ ê²½ë¡œë“¤ì—ì„œ ê²€ìƒ‰
            possible_paths = [
                Path(f"ai_models/step_05_cloth_warping/{filename}"),
                Path(f"ai_models/step_05_cloth_warping/ultra_models/{filename}"),
                Path(f"../ai_models/step_05_cloth_warping/{filename}"),
                Path(f"../../ai_models/step_05_cloth_warping/{filename}"),
            ]
            
            for checkpoint_path in possible_paths:
                if checkpoint_path.exists():
                    return self._load_checkpoint_file(checkpoint_path, model_info['format'])
            
            return None
            
        except Exception as e:
            self.logger.error(f"í´ë°± ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

# ==============================================
# ğŸ¤– ê°•í™”ëœ AI ëª¨ë¸ ë˜í¼
# ==============================================

class EnhancedAIModelWrapper:
    """ê°•í™”ëœ AI ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_loader=None, device: str = "cpu"):
        self.model_loader = model_loader
        self.device = device
        self.logger = logging.getLogger(__name__)
        
        # AI ëª¨ë¸ë“¤
        self.realvis_xl_model = None
        self.vgg19_warping_model = None
        self.vgg16_warping_model = None
        self.densenet_warping_model = None
        self.diffusion_warping_model = None
        
        # ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬
        self.image_matching_network = None
        
        # ë¡œë”© ìƒíƒœ
        self.models_loaded = {}
        self.checkpoint_loader = EnhancedCheckpointLoader(device)
        
        # ìš°ì„ ìˆœìœ„
        self.model_priority = ['realvis_xl', 'vgg19_warping', 'vgg16_warping', 'densenet121']
    
    def load_all_models(self) -> bool:
        """ëª¨ë“  ëª¨ë¸ ë¡œë”©"""
        try:
            self.logger.info("ğŸš€ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            load_results = {}
            
            # ëª¨ë¸ë“¤ ìˆœì°¨ ë¡œë”©
            for model_name in self.model_priority:
                try:
                    success = self._load_single_model(model_name)
                    load_results[model_name] = success
                    if success:
                        self.logger.info(f"âœ… {model_name} ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning(f"âš ï¸ {model_name} ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} ë¡œë”© ì˜ˆì™¸: {e}")
                    load_results[model_name] = False
            
            # ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ë¡œë”©
            try:
                self.image_matching_network = EnhancedImageMatchingNetwork().to(self.device)
                self.logger.info("âœ… ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            success_count = sum(load_results.values())
            total_models = len(load_results)
            
            self.logger.info(f"ğŸ¯ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}/{total_models} ì„±ê³µ")
            
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_single_model(self, model_name: str) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë¡œë”©"""
        try:
            if model_name not in ENHANCED_STEP_05_MODEL_MAPPING:
                return False
            
            # ModelLoaderë¥¼ í†µí•œ ë¡œë”© ì‹œë„
            checkpoint = None
            if self.model_loader:
                try:
                    if hasattr(self.model_loader, 'load_model'):
                        checkpoint = self.model_loader.load_model(model_name)
                    
                    if checkpoint:
                        self.logger.info(f"âœ… ModelLoaderë¡œë¶€í„° {model_name} íšë“")
                except Exception as e:
                    self.logger.warning(f"ModelLoader ì‹¤íŒ¨, ì§ì ‘ ë¡œë”© ì‹œë„: {e}")
            
            # ì§ì ‘ ë¡œë”©
            if checkpoint is None:
                checkpoint = self.checkpoint_loader.load_checkpoint(model_name)
            
            if checkpoint is None:
                self.models_loaded[model_name] = False
                return False
            
            # AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
            ai_model = self._create_ai_model(model_name, checkpoint)
            
            if ai_model is not None:
                setattr(self, f"{model_name.replace('-', '_')}_model", ai_model)
                self.models_loaded[model_name] = True
                return True
            else:
                self.models_loaded[model_name] = False
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            self.models_loaded[model_name] = False
            return False
    
    def _create_ai_model(self, model_name: str, checkpoint: Dict[str, Any]) -> Optional[nn.Module]:
        """AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±"""
        try:
            self.logger.info(f"ğŸ§  {model_name} AI ëª¨ë¸ ìƒì„± ì‹œì‘")
            
            # AI ëª¨ë¸ë³„ í´ë˜ìŠ¤ ìƒì„±
            if model_name == 'realvis_xl':
                ai_model = EnhancedRealVisXLModel().to(self.device)
            elif model_name in ['vgg19_warping', 'vgg16_warping']:
                ai_model = RealClothWarpingModel().to(self.device)
            elif model_name == 'densenet121':
                ai_model = RealClothWarpingModel().to(self.device)
            else:
                self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œë„
            try:
                if 'state_dict' in checkpoint:
                    ai_model.load_state_dict(checkpoint['state_dict'], strict=False)
                    self.logger.info(f"âœ… {model_name} state_dict ë¡œë”© ì„±ê³µ")
                elif 'checkpoint' in checkpoint:
                    if isinstance(checkpoint['checkpoint'], dict):
                        if 'state_dict' in checkpoint['checkpoint']:
                            ai_model.load_state_dict(checkpoint['checkpoint']['state_dict'], strict=False)
                        elif 'model' in checkpoint['checkpoint']:
                            ai_model.load_state_dict(checkpoint['checkpoint']['model'], strict=False)
                        else:
                            ai_model.load_state_dict(checkpoint['checkpoint'], strict=False)
                        self.logger.info(f"âœ… {model_name} checkpoint ë¡œë”© ì„±ê³µ")
                else:
                    self.logger.warning(f"âš ï¸ {model_name} ê°€ì¤‘ì¹˜ ë¡œë”© ì—†ìŒ, ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ {model_name} ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.logger.info(f"ëœë¤ ì´ˆê¸°í™”ëœ {model_name} ëª¨ë¸ ì‚¬ìš©")
            
            ai_model.eval()
            self.logger.info(f"âœ… {model_name} AI ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            return ai_model
            
        except Exception as e:
            self.logger.error(f"âŒ {model_name} AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def perform_cloth_warping(self, cloth_tensor: torch.Tensor, person_tensor: torch.Tensor, 
                             method: str = "auto") -> Dict[str, Any]:
        """ì˜ë¥˜ ì›Œí•‘ ìˆ˜í–‰"""
        try:
            # ìµœì  ëª¨ë¸ ì„ íƒ
            selected_model = self._select_best_model(method)
            
            if selected_model is None:
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ AI ì›Œí•‘ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            model_name, ai_model = selected_model
            
            self.logger.info(f"ğŸ§  {model_name} ëª¨ë¸ë¡œ AI ì¶”ë¡  ì‹œì‘")
            
            # AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                ai_model.eval()
                
                if hasattr(ai_model, 'forward') and 'cloth_image' in ai_model.forward.__code__.co_varnames:
                    result = ai_model(cloth_tensor, person_tensor)
                else:
                    # ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ì¸ ê²½ìš°
                    result = ai_model(cloth_tensor, person_tensor)
                    if 'warped_cloth' not in result:
                        # ë§¤ì¹­ ê²°ê³¼ë¥¼ ì›Œí•‘ ê²°ê³¼ë¡œ ë³€í™˜
                        result['warped_cloth'] = self._apply_matching_based_warping(
                            cloth_tensor, result
                        )
            
            # ê²°ê³¼ êµ¬ì„±
            warping_result = {
                'warped_cloth': result.get('warped_cloth', cloth_tensor),
                'confidence': result.get('confidence', torch.tensor([0.7])).mean().item(),
                'quality_score': result.get('quality_score', torch.tensor([0.7])).mean().item(),
                'model_used': model_name,
                'success': True,
                'enhanced_ai_inference': True,
                'warping_field': result.get('warping_field'),
                'features': result.get('features')
            }
            
            self.logger.info(f"âœ… {model_name} AI ì¶”ë¡  ì™„ë£Œ - ì‹ ë¢°ë„: {warping_result['confidence']:.3f}")
            
            return warping_result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì›Œí•‘ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                'warped_cloth': cloth_tensor,
                'confidence': 0.3,
                'quality_score': 0.3,
                'model_used': 'fallback',
                'success': False,
                'error': str(e),
                'enhanced_ai_inference': False
            }
    
    def _apply_matching_based_warping(self, cloth_tensor: torch.Tensor, 
                                    matching_result: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ë§¤ì¹­ ê¸°ë°˜ ì›Œí•‘ ì ìš©"""
        try:
            matching_map = matching_result.get('matching_map')
            keypoints = matching_result.get('keypoints')
            
            if matching_map is not None:
                # ë§¤ì¹­ ë§µ ê¸°ë°˜ ì›Œí•‘
                warped = self._apply_matching_map_warping(cloth_tensor, matching_map)
                return warped
            elif keypoints is not None:
                # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì›Œí•‘
                warped = self._apply_keypoint_warping(cloth_tensor, keypoints)
                return warped
            else:
                return cloth_tensor
                
        except Exception as e:
            self.logger.warning(f"ë§¤ì¹­ ê¸°ë°˜ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return cloth_tensor
    
    def _apply_matching_map_warping(self, cloth_tensor: torch.Tensor, 
                                  matching_map: torch.Tensor) -> torch.Tensor:
        """ë§¤ì¹­ ë§µ ê¸°ë°˜ ì›Œí•‘"""
        try:
            batch_size, channels, height, width = cloth_tensor.shape
            
            # ë§¤ì¹­ ë§µì„ ì›Œí•‘ í•„ë“œë¡œ ë³€í™˜
            y_coords = torch.linspace(-1, 1, height, device=cloth_tensor.device)
            x_coords = torch.linspace(-1, 1, width, device=cloth_tensor.device)
            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')
            
            grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)
            
            # ë§¤ì¹­ ë§µì—ì„œ ë³€í˜• ê³„ì‚°
            if matching_map.dim() == 4 and matching_map.size(1) == 1:
                # (B, 1, H, W) -> (B, 2, H, W) ë³€í™˜
                dx = torch.gradient(matching_map.squeeze(1), dim=2)[0] * 0.1
                dy = torch.gradient(matching_map.squeeze(1), dim=1)[0] * 0.1
                displacement = torch.stack([dx, dy], dim=1)
            else:
                displacement = torch.zeros_like(grid)
            
            deformed_grid = grid + displacement
            deformed_grid = torch.clamp(deformed_grid, -1, 1)
            deformed_grid = deformed_grid.permute(0, 2, 3, 1)
            
            # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§
            warped = F.grid_sample(cloth_tensor, deformed_grid, 
                                 mode='bilinear', padding_mode='border', align_corners=False)
            
            return warped
            
        except Exception as e:
            self.logger.warning(f"ë§¤ì¹­ ë§µ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return cloth_tensor
    
    def _apply_keypoint_warping(self, cloth_tensor: torch.Tensor, 
                              keypoints: torch.Tensor) -> torch.Tensor:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì›Œí•‘"""
        try:
            # ê°„ë‹¨í•œ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë³€í˜•
            batch_size, channels, height, width = cloth_tensor.shape
            
            # í‚¤í¬ì¸íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ local ë³€í˜•
            warped = cloth_tensor.clone()
            
            # í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ì—ì„œ radial ë³€í˜• ì ìš©
            for b in range(batch_size):
                for kp_idx in range(min(5, keypoints.size(1))):  # ìµœëŒ€ 5ê°œ í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
                    kp_map = keypoints[b, kp_idx]
                    
                    # í‚¤í¬ì¸íŠ¸ ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                    max_pos = torch.unravel_index(torch.argmax(kp_map), kp_map.shape)
                    center_y, center_x = max_pos[0].item(), max_pos[1].item()
                    
                    # ì£¼ë³€ ì˜ì—­ì— radial ë³€í˜• ì ìš©
                    radius = min(20, height//10, width//10)
                    for dy in range(-radius, radius+1):
                        for dx in range(-radius, radius+1):
                            y, x = center_y + dy, center_x + dx
                            if 0 <= y < height and 0 <= x < width:
                                dist = (dy*dy + dx*dx) ** 0.5
                                if dist < radius:
                                    factor = (1 - dist/radius) * 0.1
                                    # ê°„ë‹¨í•œ ë³€í˜• ì ìš©
                                    warped[b, :, y, x] = warped[b, :, y, x] * (1 + factor)
            
            return torch.clamp(warped, 0, 1)
            
        except Exception as e:
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return cloth_tensor
    
    def _select_best_model(self, method: str = "auto") -> Optional[Tuple[str, nn.Module]]:
        """ìµœì  ëª¨ë¸ ì„ íƒ"""
        try:
            # íŠ¹ì • ëª¨ë¸ ìš”ì²­ ì‹œ
            if method != "auto" and method in self.models_loaded:
                if self.models_loaded[method]:
                    model_attr = f"{method.replace('-', '_')}_model"
                    ai_model = getattr(self, model_attr, None)
                    if ai_model is not None:
                        return method, ai_model
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìë™ ì„ íƒ
            for model_name in self.model_priority:
                if self.models_loaded.get(model_name, False):
                    model_attr = f"{model_name.replace('-', '_')}_model"
                    ai_model = getattr(self, model_attr, None)
                    if ai_model is not None:
                        return model_name, ai_model
            
            # ì´ë¯¸ì§€ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ í´ë°±
            if self.image_matching_network is not None:
                return "image_matching", self.image_matching_network
            
            return None
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„ íƒ ì‹¤íŒ¨: {e}")
            return None
    
    def get_loaded_models_status(self) -> Dict[str, Any]:
        """ë¡œë”©ëœ ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        return {
            'loaded_models': self.models_loaded.copy(),
            'total_models': len(self.model_priority),
            'success_rate': sum(self.models_loaded.values()) / len(self.models_loaded) if self.models_loaded else 0,
            'image_matching_available': self.image_matching_network is not None,
            'model_mapping': ENHANCED_STEP_05_MODEL_MAPPING
        }
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            for model_name in self.model_priority:
                model_attr = f"{model_name.replace('-', '_')}_model"
                if hasattr(self, model_attr):
                    delattr(self, model_attr)
            
            if hasattr(self, 'image_matching_network') and self.image_matching_network:
                del self.image_matching_network
                self.image_matching_network = None
            
            self.models_loaded.clear()
            
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            gc.collect()
            self.logger.info("âœ… AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ¯ ë©”ì¸ ClothWarpingStep í´ë˜ìŠ¤ (BaseStepMixin v19.1 í‘œì¤€)
# ==============================================

class ClothWarpingStep(BaseStepMixin):
    """
    Step 5: ì˜ë¥˜ ì›Œí•‘ - BaseStepMixin v19.1 í‘œì¤€ ì¤€ìˆ˜
    
    âœ… BaseStepMixinì˜ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„
    âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)
    âœ… AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”
    âœ… ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„
    """
    
    def __init__(self, **kwargs):
        """ì´ˆê¸°í™”"""
        try:
            # ê¸°ë³¸ ì†ì„± ì„¤ì •
            kwargs.setdefault('step_name', 'ClothWarpingStep')
            kwargs.setdefault('step_id', 5)
            
            # BaseStepMixin ì´ˆê¸°í™”
            super().__init__(**kwargs)
            
            # ì›Œí•‘ ì„¤ì •
            self.warping_config = ClothWarpingConfig(**kwargs)
            
            # AI ëª¨ë¸ ë˜í¼
            self.ai_model_wrapper = None
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            self.physics_properties = PhysicsProperties()
            self.physics_simulator = None
            
            # ì‹œê°í™”
            self.visualizer = WarpingVisualizer(self.warping_config.quality_level)
            
            # TPS ë³€í™˜
            self.tps_transform = AdvancedTPSTransform(self.warping_config.num_control_points)
            
            # AI ì´ë¯¸ì§€ ì²˜ë¦¬
            self.ai_processor = AIImageProcessor(self.device)
            
            # ìºì‹œ
            self.prediction_cache = {}
            
            self.logger.info(f"âœ… ClothWarpingStep v14.0 ì´ˆê¸°í™” ì™„ë£Œ - BaseStepMixin v19.1 í‘œì¤€ ì¤€ìˆ˜")
            
        except Exception as e:
            self.logger.error(f"âŒ ClothWarpingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = 'ClothWarpingStep'
        self.step_id = 5
        self.device = kwargs.get('device', 'cpu')
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        self.warping_config = ClothWarpingConfig()
        self.ai_model_wrapper = None
        self.prediction_cache = {}
        self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ì™„ë£Œ")
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            super().set_model_loader(model_loader)
            
            # AI ëª¨ë¸ ë˜í¼ ìƒì„±
            self.ai_model_wrapper = EnhancedAIModelWrapper(model_loader, self.device)
            
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ - AI ëª¨ë¸ ë˜í¼ ìƒì„±")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info("ğŸš€ ClothWarpingStep v14.0 ì´ˆê¸°í™” ì‹œì‘")
            
            # AI ëª¨ë¸ ë¡œë”©
            if self.ai_model_wrapper and self.warping_config.ai_model_enabled:
                ai_load_success = self.ai_model_wrapper.load_all_models()
                if ai_load_success:
                    self.logger.info("âœ… AI ëª¨ë¸ë“¤ ë¡œë”© ì„±ê³µ")
                    model_status = self.ai_model_wrapper.get_loaded_models_status()
                    self.logger.info(f"ë¡œë”© ì„±ê³µë¥ : {model_status['success_rate']:.1%}")
                else:
                    self.logger.warning("âš ï¸ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    if self.warping_config.strict_mode:
                        return False
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™”
            if self.warping_config.physics_enabled:
                self.physics_simulator = ClothPhysicsSimulator(self.physics_properties)
                self.logger.info("âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
            self.is_initialized = True
            self.is_ready = True
            
            self.logger.info("âœ… ClothWarpingStep v14.0 ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v19.1 í‘œì¤€ - _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„
    # ==============================================
    
    async def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ìˆœìˆ˜ AI ë¡œì§ ì‹¤í–‰ (BaseStepMixin v19.1 í‘œì¤€)
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ AI ëª¨ë¸ ì…ë ¥
                - 'image': ì „ì²˜ë¦¬ëœ ì¸ë¬¼ ì´ë¯¸ì§€ (PIL.Image ë˜ëŠ” torch.Tensor)
                - 'cloth_image': ì „ì²˜ë¦¬ëœ ì˜ë¥˜ ì´ë¯¸ì§€ (PIL.Image ë˜ëŠ” torch.Tensor)
                - 'from_step_XX': ì´ì „ Stepì˜ ì¶œë ¥ ë°ì´í„°
                - ê¸°íƒ€ ì…ë ¥ ë°ì´í„°
        
        Returns:
            AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ (BaseStepMixinì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} AI ì¶”ë¡  ì‹œì‘")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input and 'cloth_image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„°(ì´ë¯¸ì§€)ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            person_image = processed_input.get('image')
            cloth_image = processed_input.get('cloth_image')
            cloth_mask = processed_input.get('cloth_mask')
            fabric_type = processed_input.get('fabric_type', 'cotton')
            clothing_type = processed_input.get('clothing_type', 'shirt')
            warping_method = processed_input.get('warping_method', 'auto')
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if person_image is None:
                person_image = cloth_image
            if cloth_image is None:
                cloth_image = person_image
            
            # 3. ì´ì „ Step ë°ì´í„° í™œìš©
            previous_data = {}
            for key, value in processed_input.items():
                if key.startswith('from_step_'):
                    previous_data[key] = value
                    self.logger.debug(f"ì´ì „ Step ë°ì´í„° í™œìš©: {key}")
            
            # 4. ì…ë ¥ì„ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._prepare_tensor_input(person_image)
            cloth_tensor = self._prepare_tensor_input(cloth_image)
            
            # 5. AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            if self.ai_model_wrapper:
                ai_result = self.ai_model_wrapper.perform_cloth_warping(
                    cloth_tensor, person_tensor, warping_method
                )
                
                if ai_result['success']:
                    # 6. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ê°•í™” (ì„¤ì •ëœ ê²½ìš°)
                    if self.warping_config.physics_enabled and self.physics_simulator:
                        enhanced_result = self._enhance_with_physics(ai_result, fabric_type)
                    else:
                        enhanced_result = ai_result
                    
                    # 7. í’ˆì§ˆ ë¶„ì„
                    quality_analysis = self._analyze_warping_quality(
                        cloth_tensor, enhanced_result['warped_cloth']
                    )
                    
                    # 8. ì‹œê°í™” ìƒì„± (ì„¤ì •ëœ ê²½ìš°)
                    visualization = None
                    if self.warping_config.visualization_enabled:
                        visualization = self._create_warping_visualization(
                            cloth_image, enhanced_result['warped_cloth']
                        )
                    
                    # 9. ìµœì¢… ê²°ê³¼ êµ¬ì„±
                    final_result = {
                        'warped_cloth': enhanced_result['warped_cloth'],
                        'warped_cloth_tensor': enhanced_result['warped_cloth'],
                        'confidence': enhanced_result['confidence'],
                        'quality_score': enhanced_result['quality_score'],
                        'matching_score': enhanced_result.get('matching_score', enhanced_result['confidence']),
                        'model_used': enhanced_result['model_used'],
                        'ai_success': True,
                        'enhanced_ai_inference': True,
                        
                        # í’ˆì§ˆ ë¶„ì„ ê²°ê³¼
                        'quality_analysis': quality_analysis,
                        'overall_quality': quality_analysis.get('overall_quality', 0.7),
                        'quality_grade': self._get_quality_grade(quality_analysis.get('overall_quality', 0.7)),
                        
                        # ë¬¼ë¦¬ ê°•í™” ì •ë³´
                        'physics_applied': self.warping_config.physics_enabled,
                        'fabric_type': fabric_type,
                        'clothing_type': clothing_type,
                        
                        # ì‹œê°í™”
                        'visualization': visualization,
                        'visualization_generated': visualization is not None,
                        
                        # AI ì¶”ë¡  ë©”íƒ€ë°ì´í„°
                        'warping_field': enhanced_result.get('warping_field'),
                        'features': enhanced_result.get('features'),
                        'ai_metadata': {
                            'model_used': enhanced_result['model_used'],
                            'processing_method': warping_method,
                            'input_size': self.warping_config.input_size,
                            'device': self.device,
                            'precision': self.warping_config.precision
                        }
                    }
                    
                    self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ - í’ˆì§ˆ: {final_result['quality_grade']}")
                    return final_result
                    
                else:
                    raise RuntimeError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {ai_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            # AI ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í´ë°±
            self.logger.warning("âš ï¸ AI ëª¨ë¸ ì—†ìŒ - í´ë°± ì²˜ë¦¬ ì‚¬ìš©")
            return self._fallback_warping(person_tensor, cloth_tensor, fabric_type, clothing_type)
        
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            self.logger.debug(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return self._create_error_ai_result(str(e))
    
    # ==============================================
    # ğŸ”§ AI ì¶”ë¡  ì§€ì› ë©”ì„œë“œë“¤
    # ==============================================
    
    def _prepare_tensor_input(self, image_input: Any) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì…ë ¥ì„ í…ì„œë¡œ ë³€í™˜"""
        try:
            if image_input is None:
                # ê¸°ë³¸ ë”ë¯¸ í…ì„œ ìƒì„±
                size = self.warping_config.input_size
                return torch.randn(1, 3, size[1], size[0]).to(self.device)
            
            # ì´ë¯¸ í…ì„œì¸ ê²½ìš°
            if TORCH_AVAILABLE and torch.is_tensor(image_input):
                tensor = image_input.to(self.device)
                if len(tensor.shape) == 3:
                    tensor = tensor.unsqueeze(0)
                return tensor
            
            # PIL Imageì¸ ê²½ìš°
            if PIL_AVAILABLE and isinstance(image_input, Image.Image):
                array = np.array(image_input)
                if len(array.shape) == 3:
                    array = np.transpose(array, (2, 0, 1))
                tensor = torch.from_numpy(array).float().unsqueeze(0) / 255.0
                return tensor.to(self.device)
            
            # NumPy ë°°ì—´ì¸ ê²½ìš°
            if NUMPY_AVAILABLE and isinstance(image_input, np.ndarray):
                if len(image_input.shape) == 3:
                    array = np.transpose(image_input, (2, 0, 1))
                else:
                    array = image_input
                
                if array.dtype != np.float32:
                    array = array.astype(np.float32)
                
                if array.max() > 1.0:
                    array = array / 255.0
                
                tensor = torch.from_numpy(array).unsqueeze(0)
                return tensor.to(self.device)
            
            # ê¸°ë³¸ ë”ë¯¸ í…ì„œ
            size = self.warping_config.input_size
            return torch.randn(1, 3, size[1], size[0]).to(self.device)
            
        except Exception as e:
            self.logger.warning(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            size = self.warping_config.input_size
            return torch.randn(1, 3, size[1], size[0]).to(self.device)
    
    def _enhance_with_physics(self, ai_result: Dict[str, Any], fabric_type: str) -> Dict[str, Any]:
        """ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ AI ê²°ê³¼ ê°•í™”"""
        try:
            if not self.physics_simulator:
                return ai_result
            
            warped_cloth = ai_result['warped_cloth']
            
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if TORCH_AVAILABLE and torch.is_tensor(warped_cloth):
                cloth_array = warped_cloth.squeeze().permute(1, 2, 0).cpu().numpy()
            else:
                cloth_array = warped_cloth
            
            # ì›ë‹¨ íƒ€ì…ì— ë”°ë¥¸ ë¬¼ë¦¬ ì†ì„± ì¡°ì •
            fabric_properties = {
                'cotton': {'elasticity': 0.3, 'stiffness': 0.5},
                'silk': {'elasticity': 0.1, 'stiffness': 0.2},
                'denim': {'elasticity': 0.5, 'stiffness': 0.8},
                'wool': {'elasticity': 0.4, 'stiffness': 0.6}
            }
            
            props = fabric_properties.get(fabric_type, fabric_properties['cotton'])
            
            # ê°„ë‹¨í•œ ë¬¼ë¦¬ íš¨ê³¼ ì ìš©
            if len(cloth_array.shape) >= 2:
                h, w = cloth_array.shape[:2]
                
                # ë©”ì‹œ ìƒì„±
                vertices, faces = self.physics_simulator.create_cloth_mesh(w, h, 16)
                
                # ëª‡ ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
                for _ in range(3):
                    self.physics_simulator.simulate_step(0.016)
                
                # ë¬¼ë¦¬ íš¨ê³¼ê°€ ì ìš©ëœ ê²°ê³¼ (ê°„ë‹¨í•œ ë¸”ëŸ¬ë§ìœ¼ë¡œ ê·¼ì‚¬)
                if TORCH_AVAILABLE:
                    cloth_tensor = torch.from_numpy(cloth_array).permute(2, 0, 1).unsqueeze(0).float()
                    cloth_tensor = cloth_tensor.to(self.device)
                    
                    # ê°„ë‹¨í•œ ë¸”ëŸ¬ë§ íš¨ê³¼
                    kernel_size = 3
                    blurred = F.avg_pool2d(F.pad(cloth_tensor, (1,1,1,1), mode='reflect'), kernel_size, stride=1)
                    
                    # ë¬¼ë¦¬ íš¨ê³¼ ê°•ë„ ì¡°ì ˆ
                    physics_strength = props['elasticity'] * 0.1
                    enhanced_cloth = cloth_tensor * (1 - physics_strength) + blurred * physics_strength
                    
                    ai_result['warped_cloth'] = enhanced_cloth
                    ai_result['physics_enhanced'] = True
                    ai_result['fabric_properties'] = props
            
            return ai_result
            
        except Exception as e:
            self.logger.warning(f"ë¬¼ë¦¬ ê°•í™” ì‹¤íŒ¨: {e}")
            ai_result['physics_enhanced'] = False
            return ai_result
    
    def _analyze_warping_quality(self, original_cloth: torch.Tensor, warped_cloth: torch.Tensor) -> Dict[str, Any]:
        """ì›Œí•‘ í’ˆì§ˆ ë¶„ì„"""
        try:
            quality_metrics = {}
            
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if TORCH_AVAILABLE:
                orig_np = original_cloth.squeeze().permute(1, 2, 0).cpu().numpy()
                warp_np = warped_cloth.squeeze().permute(1, 2, 0).cpu().numpy()
            else:
                orig_np = original_cloth
                warp_np = warped_cloth
            
            # í…ìŠ¤ì²˜ ë³´ì¡´ë„
            quality_metrics['texture_preservation'] = self._calculate_texture_preservation(orig_np, warp_np)
            
            # ë³€í˜• ìì—°ìŠ¤ëŸ¬ì›€
            quality_metrics['deformation_naturalness'] = self._calculate_deformation_naturalness(warp_np)
            
            # ìƒ‰ìƒ ì¼ê´€ì„±
            quality_metrics['color_consistency'] = self._calculate_color_consistency(orig_np, warp_np)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            overall_quality = np.mean(list(quality_metrics.values()))
            quality_metrics['overall_quality'] = overall_quality
            
            # ê¶Œì¥ì‚¬í•­
            recommendations = []
            if quality_metrics['texture_preservation'] < 0.7:
                recommendations.append('í…ìŠ¤ì²˜ ë³´ì¡´ ê°œì„  í•„ìš”')
            if quality_metrics['deformation_naturalness'] < 0.6:
                recommendations.append('ë³€í˜• ìì—°ìŠ¤ëŸ¬ì›€ ê°œì„  í•„ìš”')
            if quality_metrics['color_consistency'] < 0.8:
                recommendations.append('ìƒ‰ìƒ ì¼ê´€ì„± ê°œì„  í•„ìš”')
            
            quality_metrics['recommendations'] = recommendations
            
            return quality_metrics
            
        except Exception as e:
            self.logger.warning(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'texture_preservation': 0.7,
                'deformation_naturalness': 0.6,
                'color_consistency': 0.8,
                'overall_quality': 0.7,
                'recommendations': []
            }
    
    def _calculate_texture_preservation(self, original: np.ndarray, warped: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ ë³´ì¡´ë„ ê³„ì‚°"""
        try:
            if original.shape != warped.shape:
                warped = self.ai_processor.ai_resize(warped, original.shape[:2][::-1])
            
            orig_var = np.var(original)
            warp_var = np.var(warped)
            
            if orig_var == 0:
                return 1.0
            
            texture_ratio = min(warp_var / orig_var, orig_var / warp_var) if orig_var > 0 else 1.0
            return float(np.clip(texture_ratio, 0.0, 1.0))
            
        except Exception:
            return 0.7
    
    def _calculate_deformation_naturalness(self, warped_cloth: np.ndarray) -> float:
        """ë³€í˜• ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚°"""
        try:
            gray = np.mean(warped_cloth, axis=2) if len(warped_cloth.shape) == 3 else warped_cloth
            
            # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ
            if gray.shape[0] > 2 and gray.shape[1] > 2:
                dx = np.diff(gray, axis=1)
                dy = np.diff(gray, axis=0)
                
                edge_density = (np.abs(dx).mean() + np.abs(dy).mean()) / 2
                optimal_density = 0.1
                naturalness = 1.0 - min(abs(edge_density - optimal_density) / optimal_density, 1.0)
                
                return float(np.clip(naturalness, 0.0, 1.0))
            
            return 0.6
            
        except Exception:
            return 0.6
    
    def _calculate_color_consistency(self, original: np.ndarray, warped: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            if original.shape != warped.shape:
                warped = self.ai_processor.ai_resize(warped, original.shape[:2][::-1])
            
            orig_mean = np.mean(original, axis=(0, 1))
            warp_mean = np.mean(warped, axis=(0, 1))
            
            color_diff = np.mean(np.abs(orig_mean - warp_mean))
            consistency = max(0.0, 1.0 - color_diff)
            
            return float(np.clip(consistency, 0.0, 1.0))
            
        except Exception:
            return 0.8
    
    def _create_warping_visualization(self, original_cloth: Any, warped_cloth: torch.Tensor) -> Optional[np.ndarray]:
        """ì›Œí•‘ ì‹œê°í™” ìƒì„±"""
        try:
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if TORCH_AVAILABLE and torch.is_tensor(warped_cloth):
                warped_np = warped_cloth.squeeze().permute(1, 2, 0).cpu().numpy()
                warped_np = (warped_np * 255).astype(np.uint8)
            else:
                warped_np = warped_cloth
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì¤€ë¹„
            if PIL_AVAILABLE and isinstance(original_cloth, Image.Image):
                original_np = np.array(original_cloth)
            elif isinstance(original_cloth, np.ndarray):
                original_np = original_cloth
            else:
                original_np = warped_np
            
            # ì œì–´ì  ìƒì„±
            h, w = original_np.shape[:2]
            control_points = self.tps_transform.create_adaptive_control_grid(w, h)
            
            # ì‹œê°í™” ìƒì„±
            visualization = self.visualizer.create_warping_visualization(
                original_np, warped_np, control_points
            )
            
            return visualization
            
        except Exception as e:
            self.logger.warning(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_quality_grade(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 0.9:
            return "A+"
        elif score >= 0.8:
            return "A"
        elif score >= 0.7:
            return "B"
        elif score >= 0.6:
            return "C"
        elif score >= 0.5:
            return "D"
        else:
            return "F"
    
    def _fallback_warping(self, person_tensor: torch.Tensor, cloth_tensor: torch.Tensor, 
                         fabric_type: str, clothing_type: str) -> Dict[str, Any]:
        """í´ë°± ì›Œí•‘ ì²˜ë¦¬"""
        try:
            self.logger.info("ğŸ”„ í´ë°± ì›Œí•‘ ì²˜ë¦¬ ì‹œì‘")
            
            # ê°„ë‹¨í•œ ë³€í˜• ì ìš©
            warped_cloth = self._apply_simple_transformation(cloth_tensor)
            
            # ê¸°ë³¸ í’ˆì§ˆ ë¶„ì„
            quality_analysis = {
                'texture_preservation': 0.5,
                'deformation_naturalness': 0.4,
                'color_consistency': 0.6,
                'overall_quality': 0.5,
                'recommendations': ['AI ëª¨ë¸ ë¡œë”© í•„ìš”']
            }
            
            return {
                'warped_cloth': warped_cloth,
                'warped_cloth_tensor': warped_cloth,
                'confidence': 0.5,
                'quality_score': 0.5,
                'matching_score': 0.5,
                'model_used': 'fallback',
                'ai_success': False,
                'enhanced_ai_inference': False,
                'quality_analysis': quality_analysis,
                'overall_quality': 0.5,
                'quality_grade': 'C',
                'physics_applied': False,
                'fabric_type': fabric_type,
                'clothing_type': clothing_type,
                'visualization': None,
                'visualization_generated': False,
                'ai_metadata': {
                    'model_used': 'fallback',
                    'processing_method': 'simple_transform',
                    'device': self.device
                }
            }
            
        except Exception as e:
            self.logger.error(f"í´ë°± ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return self._create_error_ai_result(f"í´ë°± ì›Œí•‘ ì‹¤íŒ¨: {e}")
    
    def _apply_simple_transformation(self, cloth_tensor: torch.Tensor) -> torch.Tensor:
        """ê°„ë‹¨í•œ ë³€í˜• ì ìš©"""
        try:
            if not TORCH_AVAILABLE:
                return cloth_tensor
            
            # ê°„ë‹¨í•œ ì•„í•€ ë³€í™˜
            batch_size, channels, height, width = cloth_tensor.shape
            
            # ì‘ì€ ë³€í˜• ë§¤íŠ¸ë¦­ìŠ¤
            theta = torch.tensor([
                [1.02, 0.01, 0.01],
                [0.01, 1.01, 0.01]
            ]).unsqueeze(0).repeat(batch_size, 1, 1).to(cloth_tensor.device)
            
            grid = F.affine_grid(theta, cloth_tensor.size(), align_corners=False)
            transformed = F.grid_sample(cloth_tensor, grid, align_corners=False)
            
            return transformed
            
        except Exception as e:
            self.logger.warning(f"ê°„ë‹¨í•œ ë³€í˜• ì‹¤íŒ¨: {e}")
            return cloth_tensor
    
    def _create_error_ai_result(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ AI ê²°ê³¼ ìƒì„±"""
        size = self.warping_config.input_size
        dummy_tensor = torch.zeros(1, 3, size[1], size[0]).to(self.device)
        
        return {
            'warped_cloth': dummy_tensor,
            'warped_cloth_tensor': dummy_tensor,
            'confidence': 0.0,
            'quality_score': 0.0,
            'matching_score': 0.0,
            'model_used': 'error',
            'ai_success': False,
            'enhanced_ai_inference': False,
            'error': error_message,
            'quality_analysis': {
                'texture_preservation': 0.0,
                'deformation_naturalness': 0.0,
                'color_consistency': 0.0,
                'overall_quality': 0.0,
                'recommendations': ['ì—ëŸ¬ í•´ê²° í•„ìš”']
            },
            'overall_quality': 0.0,
            'quality_grade': 'F',
            'physics_applied': False,
            'visualization': None,
            'visualization_generated': False,
            'ai_metadata': {
                'model_used': 'error',
                'error': error_message,
                'device': self.device
            }
        }
    
    # ==============================================
    # ğŸ”§ ì‹œìŠ¤í…œ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_loaded_ai_models(self) -> Dict[str, bool]:
        """ë¡œë”©ëœ AI ëª¨ë¸ ì •ë³´"""
        try:
            if self.ai_model_wrapper:
                return self.ai_model_wrapper.get_loaded_models_status()
            return {}
        except Exception:
            return {}
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'ai_model_wrapper') and self.ai_model_wrapper:
                self.ai_model_wrapper.cleanup_models()
                del self.ai_model_wrapper
                self.ai_model_wrapper = None
            
            if hasattr(self, 'prediction_cache'):
                self.prediction_cache.clear()
            
            if hasattr(super(), 'cleanup_models'):
                super().cleanup_models()
            
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… ClothWarpingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        try:
            base_info = {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'version': '14.0',
                'base_step_mixin_version': '19.1',
                'is_initialized': getattr(self, 'is_initialized', False),
                'device': self.device
            }
            
            # AI ì„¤ì • ì •ë³´
            ai_info = {
                "warping_method": self.warping_config.warping_method.value,
                "input_size": self.warping_config.input_size,
                "ai_model_enabled": self.warping_config.ai_model_enabled,
                "use_realvis_xl": self.warping_config.use_realvis_xl,
                "use_vgg19_warping": self.warping_config.use_vgg19_warping,
                "use_vgg16_warping": self.warping_config.use_vgg16_warping,
                "use_densenet": self.warping_config.use_densenet,
                "quality_level": self.warping_config.quality_level,
                "strict_mode": self.warping_config.strict_mode
            }
            
            # ë¡œë”©ëœ AI ëª¨ë¸ ì •ë³´
            loaded_models = self.get_loaded_ai_models()
            
            # ìºì‹œ ì •ë³´
            cache_info = {
                "cache_size": len(self.prediction_cache) if hasattr(self, 'prediction_cache') else 0,
                "cache_limit": self.warping_config.cache_size
            }
            
            # ì˜ì¡´ì„± ì •ë³´
            dependencies_info = {
                "model_loader_injected": getattr(self, 'model_loader', None) is not None,
                "torch_available": TORCH_AVAILABLE,
                "mps_available": MPS_AVAILABLE,
                "safetensors_available": SAFETENSORS_AVAILABLE,
                "ai_model_wrapper_ready": self.ai_model_wrapper is not None
            }
            
            return {
                **base_info,
                "ai_config": ai_info,
                "loaded_models": loaded_models,
                "cache_info": cache_info,
                "dependencies_info": dependencies_info,
                "model_mapping": ENHANCED_STEP_05_MODEL_MAPPING
            }
            
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": f"ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}"}
    
    def __del__(self):
        try:
            if hasattr(self, 'cleanup_resources'):  # ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
                self.cleanup_resources()
        except Exception:
            pass
# ==============================================
# ğŸ†• ì›Œë°ì—… ë° ë¹„ë™ê¸° ì§€ì› ë©”ì„œë“œë“¤
# ==============================================

    def warmup_async(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰"""
        try:
            warmup_results = []
            
            # AI ëª¨ë¸ ì›Œë°ì—…
            if hasattr(self, 'ai_model_wrapper') and self.ai_model_wrapper:
                try:
                    model_status = self.ai_model_wrapper.get_loaded_models_status()
                    if model_status['success_rate'] > 0:
                        # ë”ë¯¸ í…ì„œë¡œ ì›Œë°ì—…
                        size = self.warping_config.input_size
                        dummy_tensor = torch.randn(1, 3, size[1], size[0]).to(self.device)
                        _ = self.ai_model_wrapper.perform_cloth_warping(dummy_tensor, dummy_tensor)
                        warmup_results.append("ai_model_warmup_success")
                    else:
                        warmup_results.append("ai_model_not_loaded")
                except Exception as e:
                    self.logger.debug(f"AI ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                    warmup_results.append("ai_model_warmup_failed")
            else:
                warmup_results.append("ai_model_not_available")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë” ì›Œë°ì—…
            if hasattr(self, 'ai_model_wrapper') and self.ai_model_wrapper and self.ai_model_wrapper.checkpoint_loader:
                try:
                    warmup_results.append("checkpoint_loader_warmup_success")
                except Exception as e:
                    self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë” ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                    warmup_results.append("checkpoint_loader_warmup_failed")
            else:
                warmup_results.append("checkpoint_loader_not_available")
            
            return {
                'success': True,
                'warmup_results': warmup_results,
                'warmup_success': any('success' in result for result in warmup_results)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ†• step_model_requests.py í˜¸í™˜ ë©”ì„œë“œë“¤ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)
# ==============================================

    def _load_step_model_requests_config(self, **kwargs):
        """step_model_requests.py ì„¤ì • ë¡œë“œ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)"""
        try:
            # step_model_requests.pyì—ì„œ ì„¤ì • ë¡œë”© ì‹œë„
            try:
                # ë™ì  import ì‹œë„
                import importlib
                requests_module = importlib.import_module('app.ai_pipeline.utils.step_model_requests')
                get_enhanced_step_request = getattr(requests_module, 'get_enhanced_step_request', None)
                
                if get_enhanced_step_request:
                    self.step_request = get_enhanced_step_request("ClothWarpingStep")
                    if self.step_request:
                        self.logger.info("âœ… step_model_requests.py ì„¤ì • ë¡œë“œ ì„±ê³µ")
                        return
                
            except ImportError:
                self.logger.debug("step_model_requests.py ëª¨ë“ˆ ì—†ìŒ")
            
            # í´ë°±: ê¸°ë³¸ ì„¤ì •
            self.step_request = None
            self.logger.info("ê¸°ë³¸ ì›Œí•‘ ì„¤ì • ì‚¬ìš©")
            
        except Exception as e:
            self.logger.warning(f"step_model_requests.py ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.step_request = None

    def _apply_physics_effect(self, cloth_image: np.ndarray, fabric_type: str) -> np.ndarray:
        """ë¬¼ë¦¬ íš¨ê³¼ ì ìš© (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)"""
        try:
            fabric_properties = {
                'cotton': {'gravity': 0.02, 'stiffness': 0.3},
                'silk': {'gravity': 0.01, 'stiffness': 0.1},
                'denim': {'gravity': 0.03, 'stiffness': 0.8},
                'wool': {'gravity': 0.025, 'stiffness': 0.5}
            }
            
            props = fabric_properties.get(fabric_type, fabric_properties['cotton'])
            
            if TORCH_AVAILABLE:
                # PyTorch ê¸°ë°˜ ë¬¼ë¦¬ íš¨ê³¼
                if isinstance(cloth_image, np.ndarray):
                    tensor = torch.from_numpy(cloth_image).permute(2, 0, 1).unsqueeze(0).float() / 255.0
                else:
                    tensor = cloth_image
                
                tensor = tensor.to(self.device)
                
                kernel_size = 3
                blurred = F.avg_pool2d(F.pad(tensor, (1,1,1,1), mode='reflect'), kernel_size, stride=1)
                
                result = tensor * (1 - props['gravity']) + blurred * props['gravity']
                
                if isinstance(cloth_image, np.ndarray):
                    result = result.squeeze(0).permute(1, 2, 0).cpu().numpy()
                    return (result * 255).astype(np.uint8)
                else:
                    return result
            
            return cloth_image
            
        except Exception as e:
            self.logger.warning(f"ë¬¼ë¦¬ íš¨ê³¼ ì ìš© ì‹¤íŒ¨: {e}")
            return cloth_image

    def _enhance_image_quality(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)"""
        try:
            if TORCH_AVAILABLE:
                tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0
                tensor = tensor.to(self.device)
                
                sharpen_kernel = torch.tensor([
                    [0, -1, 0],
                    [-1, 5, -1],
                    [0, -1, 0]
                ], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
                
                enhanced_channels = []
                for i in range(3):
                    channel = tensor[:, i:i+1, :, :]
                    enhanced = F.conv2d(F.pad(channel, (1,1,1,1), mode='reflect'), sharpen_kernel)
                    enhanced_channels.append(enhanced)
                
                enhanced_tensor = torch.cat(enhanced_channels, dim=1)
                enhanced_tensor = torch.clamp(enhanced_tensor, 0, 1)
                
                result = enhanced_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
                return (result * 255).astype(np.uint8)
            
            # PIL í´ë°±
            if PIL_AVAILABLE:
                pil_img = Image.fromarray(image)
                enhanced = ImageEnhance.Sharpness(pil_img).enhance(1.1)
                return np.array(enhanced)
            
            return image
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image

    def _smooth_boundaries(self, image: np.ndarray) -> np.ndarray:
        """ê²½ê³„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)"""
        try:
            if PIL_AVAILABLE:
                pil_img = Image.fromarray(image)
                blurred = pil_img.filter(ImageFilter.GaussianBlur(radius=0.5))
                return np.array(blurred)
            return image
            
        except Exception as e:
            self.logger.warning(f"ê²½ê³„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image

# ==============================================
# ğŸ†• íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)
# ==============================================

async def create_enhanced_cloth_warping_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothWarpingStep:
    """
    ê°•í™”ëœ ClothWarpingStep ìƒì„± (ë¹„ë™ê¸°)
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        if device == "auto":
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    device_param = "mps"
                elif torch.cuda.is_available():
                    device_param = "cuda"
                else:
                    device_param = "cpu"
            else:
                device_param = "cpu"
        else:
            device_param = device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['device'] = device_param
        
        # Step ìƒì„±
        step = ClothWarpingStep(**config)
        
        # ì´ˆê¸°í™”
        if not step.is_initialized:
            step.initialize()
        
        return step
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ create_enhanced_cloth_warping_step ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"ê°•í™”ëœ ClothWarpingStep ìƒì„± ì‹¤íŒ¨: {e}")

def create_enhanced_cloth_warping_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothWarpingStep:
    """ë™ê¸°ì‹ ê°•í™”ëœ ClothWarpingStep ìƒì„±"""
    try:
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_enhanced_cloth_warping_step(device, config, **kwargs)
        )
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ create_enhanced_cloth_warping_step_sync ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"ë™ê¸°ì‹ ê°•í™”ëœ ClothWarpingStep ìƒì„± ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ†• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ì›ë³¸ íŒŒì¼ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„)
# ==============================================

async def test_step_model_requests_integration():
    """step_model_requests.py í†µí•© í…ŒìŠ¤íŠ¸ (ì›ë³¸ íŒŒì¼ ê¸°ëŠ¥)"""
    print("ğŸ§ª step_model_requests.py í†µí•© ClothWarpingStep í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # Step ìƒì„±
        step = ClothWarpingStep(
            device="auto",
            ai_model_enabled=True,
            use_realvis_xl=True,
            use_vgg19_warping=True,
            use_vgg16_warping=True,
            use_densenet=True,
            quality_level="high",
            strict_mode=False
        )
        
        # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
        system_info = step.get_system_info()
        print(f"âœ… ì‹œìŠ¤í…œ ì •ë³´: {system_info['step_name']} v{system_info['version']}")
        print(f"âœ… ë””ë°”ì´ìŠ¤: {system_info['device']}")
        print(f"âœ… AI ëª¨ë¸ í™œì„±í™”: {system_info['ai_config']['ai_model_enabled']}")
        
        # ë¡œë”©ëœ AI ëª¨ë¸ í™•ì¸
        loaded_models = step.get_loaded_ai_models()
        print(f"âœ… ë¡œë”©ëœ AI ëª¨ë¸: {loaded_models}")
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        dummy_input = {
            'image': np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8),
            'cloth_image': np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8),
            'fabric_type': 'cotton',
            'clothing_type': 'shirt',
            'warping_method': 'auto'
        }
        
        print("âœ… step_model_requests.py í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"âŒ step_model_requests.py í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_cloth_warping_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> ClothWarpingStep:
    """
    ClothWarpingStep ìƒì„± (ë™ê¸°ì‹)
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        if device == "auto":
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    device_param = "mps"
                elif torch.cuda.is_available():
                    device_param = "cuda"
                else:
                    device_param = "cpu"
            else:
                device_param = "cpu"
        else:
            device_param = device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['device'] = device_param
        
        # Step ìƒì„±
        step = ClothWarpingStep(**config)
        
        # ì´ˆê¸°í™”
        if not step.is_initialized:
            step.initialize()
        
        return step
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ create_cloth_warping_step ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"ClothWarpingStep ìƒì„± ì‹¤íŒ¨: {e}")

def create_production_cloth_warping_step(
    quality_level: str = "high",
    enable_all_models: bool = True,
    **kwargs
) -> ClothWarpingStep:
    """í”„ë¡œë•ì…˜ í™˜ê²½ìš© ClothWarpingStep ìƒì„±"""
    production_config = {
        'quality_level': quality_level,
        'warping_method': WarpingMethod.REAL_AI_MODEL,
        'ai_model_enabled': True,
        'use_realvis_xl': enable_all_models,
        'use_vgg19_warping': enable_all_models,
        'use_vgg16_warping': enable_all_models,
        'use_densenet': enable_all_models,
        'use_diffusion_warping': False,  # ë©”ëª¨ë¦¬ ì ˆì•½
        'physics_enabled': True,
        'visualization_enabled': True,
        'cache_enabled': True,
        'cache_size': 50,
        'strict_mode': False
    }
    
    production_config.update(kwargs)
    
    return ClothWarpingStep(**production_config)

# ==============================================
# ğŸ†• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

def test_cloth_warping_step():
    """ClothWarpingStep í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ClothWarpingStep v14.0 í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # Step ìƒì„±
        step = ClothWarpingStep(
            device="auto",
            ai_model_enabled=True,
            use_realvis_xl=True,
            use_vgg19_warping=True,
            use_vgg16_warping=True,
            use_densenet=True,
            quality_level="high",
            strict_mode=False
        )
        
        # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
        system_info = step.get_system_info()
        print(f"âœ… ì‹œìŠ¤í…œ ì •ë³´: {system_info['step_name']} v{system_info['version']}")
        print(f"âœ… ë””ë°”ì´ìŠ¤: {system_info['device']}")
        print(f"âœ… AI ëª¨ë¸ í™œì„±í™”: {system_info['ai_config']['ai_model_enabled']}")
        
        # ë¡œë”©ëœ AI ëª¨ë¸ í™•ì¸
        loaded_models = step.get_loaded_ai_models()
        print(f"âœ… ë¡œë”©ëœ AI ëª¨ë¸: {loaded_models}")
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (_run_ai_inferenceëŠ” BaseStepMixinì—ì„œ í˜¸ì¶œë¨)
        dummy_input = {
            'image': np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8),
            'cloth_image': np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8),
            'fabric_type': 'cotton',
            'clothing_type': 'shirt',
            'warping_method': 'auto'
        }
        
        print("âœ… ClothWarpingStep v14.0 í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("ğŸ’¡ BaseStepMixin v19.1 í‘œì¤€ì— ë”°ë¼ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ë¨")
        print("ğŸ’¡ ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨")
        return True
        
    except Exception as e:
        print(f"âŒ ClothWarpingStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ†• ëª¨ë“ˆ ì •ë³´
# ==============================================

__version__ = "14.0.0"
__author__ = "MyCloset AI Team"  
__description__ = "ì˜ë¥˜ ì›Œí•‘ - BaseStepMixin v19.1 í‘œì¤€ ì¤€ìˆ˜ + ê°•í™”ëœ AI ì¶”ë¡  ë²„ì „"
__compatibility__ = "BaseStepMixin v19.1 + AI ëª¨ë¸ ì™„ì „ í™œìš© + M3 Max 128GB ìµœì í™”"

__features__ = [
    "BaseStepMixin v19.1 í‘œì¤€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„",
    "ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨",
    "ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)",
    "AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”",
    "ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„",
    "ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° í’ˆì§ˆ ë¶„ì„ ì™„ì „ êµ¬í˜„",
    "TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€",
    "M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”",
    "í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±"
]

__models__ = [
    "RealVisXL_V4.0.safetensors (6.6GB) - ê°•í™”ëœ ë©”ì¸ ì›Œí•‘ ëª¨ë¸",
    "vgg19_warping.pth (548MB) - ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ",
    "vgg16_warping_ultra.pth (527MB) - íŠ¹ì§• ì¶”ì¶œ",
    "densenet121_ultra.pth (31MB) - ë³€í˜• ê²€ì¶œ",
    "diffusion_pytorch_model.bin (1.3GB) - Diffusion ì›Œí•‘"
]

# ==============================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==============================================

if __name__ == "__main__":
    print("ğŸ¯ ClothWarpingStep v14.0 - BaseStepMixin v19.1 í‘œì¤€ ì¤€ìˆ˜")
    print("=" * 80)
    print("ğŸ”¥ ì£¼ìš” íŠ¹ì§•:")
    print("   âœ… BaseStepMixin v19.1 í‘œì¤€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„")
    print("   âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨")
    print("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)")
    print("   âœ… AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”")
    print("   âœ… ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„")
    print("   âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° í’ˆì§ˆ ë¶„ì„ ì™„ì „ êµ¬í˜„")
    print("   âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
    print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    print("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    print("")
    
    # í…ŒìŠ¤íŠ¸
    print("1ï¸âƒ£ ClothWarpingStep í…ŒìŠ¤íŠ¸")
    test_success = test_cloth_warping_step()
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"   - ClothWarpingStep í…ŒìŠ¤íŠ¸: {'âœ… ì„±ê³µ' if test_success else 'âŒ ì‹¤íŒ¨'}")
    
    if test_success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! ClothWarpingStep v14.0 ì™„ì„±!")
        print("   âœ… BaseStepMixin v19.1 í‘œì¤€ ì™„ì „ ì¤€ìˆ˜")
        print("   âœ… _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ë¨")
        print("   âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬")
        print("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš©")
        print("   âœ… ê°•í™”ëœ AI ì¶”ë¡  ì—”ì§„")
        print("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤
    print("\nğŸ¤– ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤:")
    for model_name, model_info in ENHANCED_STEP_05_MODEL_MAPPING.items():
        size_info = f"{model_info['size_mb']}"
        if model_info['size_mb'] >= 1000:
            size_info = f"{model_info['size_mb']/1000:.1f}GB"
        else:
            size_info += "MB"
        print(f"   - {model_info['filename']} ({size_info}) - {model_info['class']}")
    
    # ì‚¬ìš©ë²•
    print("\nğŸ¤– ì‚¬ìš©ë²•:")
    print("   # 1. StepFactoryë¡œ Step ìƒì„± (ê¶Œì¥)")
    print("   step_factory = StepFactory()")
    print("   step = step_factory.create_step('cloth_warping')")
    print("")
    print("   # 2. ì§ì ‘ ìƒì„±")
    print("   step = ClothWarpingStep()")
    print("   step.set_model_loader(model_loader)")
    print("   step.initialize()")
    print("")
    print("   # 3. BaseStepMixin v19.1 í‘œì¤€ ì²˜ë¦¬ ì‹¤í–‰")
    print("   result = await step.process(image=person_image, cloth_image=cloth_image)")
    print("   print('AI ì¶”ë¡  ì„±ê³µ:', result['ai_success'])")
    print("   print('í’ˆì§ˆ ë“±ê¸‰:', result['quality_grade'])")
    print("   print('ì‹ ë¢°ë„:', result['confidence'])")
    
    print(f"\nğŸ¯ BaseStepMixin v19.1 í‘œì¤€ ì²˜ë¦¬ íë¦„:")
    print("   1. BaseStepMixin.process() â†’ ì…ë ¥ ë°ì´í„° ë³€í™˜")
    print("   2. ClothWarpingStep._run_ai_inference() â†’ ìˆœìˆ˜ AI ë¡œì§")
    print("   3. BaseStepMixin â†’ ì¶œë ¥ ë°ì´í„° ë³€í™˜ â†’ í‘œì¤€ ì‘ë‹µ")
    print("   4. ì™„ì „í•œ ë°ì´í„° ë³€í™˜ ìë™í™” ë‹¬ì„±!")
    
    print("\nğŸ“ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ:")
    print("   step_05_cloth_warping/")
    print("   â”œâ”€â”€ RealVisXL_V4.0.safetensors (6.6GB) â­ ë©”ì¸ ëª¨ë¸")
    print("   â””â”€â”€ ultra_models/")
    print("       â”œâ”€â”€ vgg19_warping.pth (548MB)")
    print("       â”œâ”€â”€ vgg16_warping_ultra.pth (527MB)")
    print("       â”œâ”€â”€ densenet121_ultra.pth (31MB)")
    print("       â””â”€â”€ diffusion_pytorch_model.bin (1.3GB)")
    
    print("=" * 80)
    print("ğŸ‰ ClothWarpingStep v14.0 - BaseStepMixin v19.1 í‘œì¤€ ì™„ì „ ì¤€ìˆ˜!")
    print("ğŸ’¡ ì´ì œ BaseStepMixinì´ ëª¨ë“  ë°ì´í„° ë³€í™˜ì„ ìë™ ì²˜ë¦¬í•©ë‹ˆë‹¤!")
    print("ğŸ’¡ Step í´ë˜ìŠ¤ëŠ” ìˆœìˆ˜ AI ë¡œì§(_run_ai_inference)ë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤!")
    print("=" * 80)

# ìµœì¢… í™•ì¸ ë¡œê¹…
logger.info(f"ğŸ“¦ ClothWarpingStep v{__version__} ë¡œë“œ ì™„ë£Œ - BaseStepMixin v19.1 í‘œì¤€ ì¤€ìˆ˜")
logger.info("âœ… _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„ë¨")
logger.info("âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)")
logger.info("âœ… AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”")
logger.info("âœ… ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„")
logger.info("âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° í’ˆì§ˆ ë¶„ì„ ì™„ì „ êµ¬í˜„")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("ğŸ‰ ClothWarpingStep v14.0 ì¤€ë¹„ ì™„ë£Œ!")

# ==============================================
# ğŸ”¥ END OF FILE - BaseStepMixin v19.1 í‘œì¤€ ì™„ì „ ì¤€ìˆ˜
# ==============================================

"""
âœ¨ ClothWarpingStep v14.0 - BaseStepMixin v19.1 í‘œì¤€ ì™„ì „ ì¤€ìˆ˜ ìš”ì•½:

ğŸ¯ í•µì‹¬ ì„±ê³¼:
   âœ… BaseStepMixin v19.1 í‘œì¤€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„
   âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ ì²˜ë¦¬ë¨
   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš© (RealVisXL 6.6GB)
   âœ… AI ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°•í™”
   âœ… ê°•í™”ëœ ì˜ë¥˜ ì›Œí•‘ AI ì¶”ë¡  ì—”ì§„
   âœ… ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ë° í’ˆì§ˆ ë¶„ì„ ì™„ì „ êµ¬í˜„
   âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

ğŸ¤– ì‹¤ì œ AI ëª¨ë¸:
   - RealVisXL_V4.0.safetensors (6.6GB) - ê°•í™”ëœ ë©”ì¸ ì›Œí•‘ ëª¨ë¸
   - vgg19_warping.pth (548MB) - ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
   - vgg16_warping_ultra.pth (527MB) - íŠ¹ì§• ì¶”ì¶œ
   - densenet121_ultra.pth (31MB) - ë³€í˜• ê²€ì¶œ
   - diffusion_pytorch_model.bin (1.3GB) - Diffusion ì›Œí•‘

ğŸ”§ ì£¼ìš” êµ¬ì¡°:
   1. BaseStepMixin.process() â†’ ì…ë ¥ ë°ì´í„° ë³€í™˜ (API â†’ AI ëª¨ë¸)
   2. ClothWarpingStep._run_ai_inference() â†’ ìˆœìˆ˜ AI ë¡œì§ ì‹¤í–‰
   3. BaseStepMixin â†’ ì¶œë ¥ ë°ì´í„° ë³€í™˜ (AI ëª¨ë¸ â†’ API)
   4. ì™„ì „í•œ í‘œì¤€í™”ëœ ë°ì´í„° íë¦„

ğŸš€ ì‚¬ìš©ë²•:
   step = ClothWarpingStep()  # BaseStepMixin v19.1 í‘œì¤€
   step.set_model_loader(model_loader)  # ì˜ì¡´ì„± ì£¼ì…
   step.initialize()  # AI ëª¨ë¸ ë¡œë”©
   result = await step.process(image=person_image, cloth_image=cloth_image)
   
ğŸ¯ ê²°ê³¼: BaseStepMixin v19.1 í‘œì¤€ â†’ ì™„ì „í•œ ë°ì´í„° ë³€í™˜ ìë™í™”!
   MyCloset AI - Step 05 Cloth Warping v14.0 ì™„ì„±!
"""