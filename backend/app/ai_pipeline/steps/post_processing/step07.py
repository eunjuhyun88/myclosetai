#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 07: Post Processing v11.0 - 100% ë…¼ë¬¸ êµ¬í˜„
============================================================================

âœ… ì™„ì „í•œ ì‹ ê²½ë§ êµ¬ì¡° êµ¬í˜„ (ESRGAN, SwinIR, Face Enhancement)
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì—”ì§„
âœ… BaseStepMixin ì™„ì „ ìƒì† ë° í˜¸í™˜
âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ
âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”

í•µì‹¬ AI ëª¨ë¸ë“¤:
- ESRGAN: Residual in Residual Dense Block ê¸°ë°˜
- SwinIR: Swin Transformer ê¸°ë°˜
- Face Enhancement: Attention ê¸°ë°˜ ì–¼êµ´ í–¥ìƒ

Author: MyCloset AI Team
Date: 2025-08-11
Version: v11.0 (100% Paper Implementation)
"""

import os
import sys
import gc
import time
import logging
import traceback
import hashlib
import json
import base64
import math
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps

# NumPy
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# PIL (Pillow)
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# OpenCV
try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    cv2 = None

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None
    transforms = None

# scikit-image ê³ ê¸‰ ì²˜ë¦¬ìš©
try:
    from skimage import restoration, filters, exposure
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# scipy í•„ìˆ˜
try:
    from scipy.ndimage import gaussian_filter, median_filter
    from scipy.signal import convolve2d
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ë¡œì»¬ imports (TYPE_CHECKING ìˆœí™˜ì°¸ì¡° ë°©ì§€)
if TYPE_CHECKING:
    from app.ai_pipeline.models.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
    from app.core.di_container import CentralHubDIContainer

# ì‹œìŠ¤í…œ ì •ë³´ ë° í™˜ê²½ ê°ì§€
def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            return 'apple m3' in result.stdout.lower() or 'apple m' in result.stdout.lower()
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# MPS (Apple Silicon) ì§€ì› í™•ì¸
try:
    MPS_AVAILABLE = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
except:
    MPS_AVAILABLE = False

# ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch and torch.backends.mps.is_available() and IS_M3_MAX:
    DEVICE = "mps"
    try:
        torch.mps.set_per_process_memory_fraction(0.7)
    except:
        pass
elif torch and torch.cuda.is_available():
    DEVICE = "cuda"
else:
    DEVICE = "cpu"

# BaseStepMixin ë™ì  import
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
            return getattr(module, 'BaseStepMixin', None)
        except ImportError:
            logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None

BaseStepMixin = get_base_step_mixin_class()

# í´ë°± í´ë˜ìŠ¤
if BaseStepMixin is None:
    raise ImportError("BaseStepMixinì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ BaseStepMixinì„ ì‚¬ìš©í•˜ì„¸ìš”.")

# ë°ì´í„° êµ¬ì¡° ì •ì˜
class EnhancementMethod(Enum):
    """í–¥ìƒ ë°©ë²• ì—´ê±°í˜•"""
    SUPER_RESOLUTION = "super_resolution"
    FACE_ENHANCEMENT = "face_enhancement"
    NOISE_REDUCTION = "noise_reduction"
    DETAIL_ENHANCEMENT = "detail_enhancement"
    COLOR_CORRECTION = "color_correction"
    CONTRAST_ENHANCEMENT = "contrast_enhancement"
    SHARPENING = "sharpening"
    BRIGHTNESS_ADJUSTMENT = "brightness_adjustment"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class PostProcessingConfig:
    """í›„ì²˜ë¦¬ ì„¤ì •"""
    quality_level: QualityLevel = QualityLevel.HIGH
    enabled_methods: List[EnhancementMethod] = field(default_factory=lambda: [
        EnhancementMethod.SUPER_RESOLUTION,
        EnhancementMethod.FACE_ENHANCEMENT,
        EnhancementMethod.DETAIL_ENHANCEMENT,
        EnhancementMethod.COLOR_CORRECTION
    ])
    upscale_factor: int = 4
    max_resolution: Tuple[int, int] = (2048, 2048)
    enhancement_strength: float = 0.8
    enable_face_detection: bool = True
    enable_visualization: bool = True
    processing_mode: str = "quality"
    cache_size: int = 50
    enable_caching: bool = True
    visualization_quality: str = "high"
    show_before_after: bool = True
    show_enhancement_details: bool = True

@dataclass
class PostProcessingResult:
    """í›„ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    enhanced_image: np.ndarray = None
    enhancement_quality: float = 0.0
    enhancement_methods_used: List[str] = field(default_factory=list)
    processing_time: float = 0.0
    device_used: str = "cpu"
    success: bool = False
    sr_enhancement: Optional[Dict[str, Any]] = None
    face_enhancement: Optional[Dict[str, Any]] = None
    detail_enhancement: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "enhanced_image": self.enhanced_image.tolist() if isinstance(self.enhanced_image, np.ndarray) else self.enhanced_image,
            "enhancement_quality": self.enhancement_quality,
            "enhancement_methods_used": self.enhancement_methods_used,
            "processing_time": self.processing_time,
            "device_used": self.device_used,
            "success": self.success,
            "sr_enhancement": self.sr_enhancement,
            "face_enhancement": self.face_enhancement,
            "detail_enhancement": self.detail_enhancement,
            "metadata": self.metadata
        }

# ==============================================
# ğŸ”¥ ì™„ì „í•œ ì‹ ê²½ë§ êµ¬ì¡° êµ¬í˜„
# ==============================================

class Upsample(nn.Sequential):
    """Upsample ëª¨ë“ˆ - ESRGAN ë…¼ë¬¸ êµ¬í˜„"""
    
    def __init__(self, scale, num_feat, num_out_ch):
        m = []
        if (scale & (scale - 1)) == 0:  # scale = 2^n
            for _ in range(int(math.log(scale, 2))):
                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(2))
        elif scale == 3:
            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
            m.append(nn.PixelShuffle(3))
        else:
            raise ValueError(f'scale {scale} is not supported.')
        super(Upsample, self).__init__(*m)

class SEBlock(nn.Module):
    """Squeeze-and-Excitation Block - ESRGAN ë…¼ë¬¸ êµ¬í˜„"""
    
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class ResidualDenseBlock_5C(nn.Module):
    """Residual Dense Block with 5 convolutions - ESRGAN í•µì‹¬ ë¸”ë¡"""
    
    def __init__(self, nf=64, gc=32, bias=True):
        super(ResidualDenseBlock_5C, self).__init__()
        
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)
        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)
        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)
        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

    def forward(self, x):
        x1 = self.lrelu(self.conv1(x))
        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 * 0.2 + x

class RRDB(nn.Module):
    """Residual in Residual Dense Block - ESRGAN í•µì‹¬ êµ¬ì¡°"""
    
    def __init__(self, nf, gc=32):
        super(RRDB, self).__init__()
        self.RDB1 = ResidualDenseBlock_5C(nf, gc)
        self.RDB2 = ResidualDenseBlock_5C(nf, gc)
        self.RDB3 = ResidualDenseBlock_5C(nf, gc)

    def forward(self, x):
        out = self.RDB1(x)
        out = self.RDB2(out)
        out = self.RDB3(out)
        return out * 0.2 + x

class CompleteESRGANModel(nn.Module):
    """ì™„ì „í•œ ESRGAN ëª¨ë¸ - ë…¼ë¬¸ êµ¬ì¡° 100% êµ¬í˜„"""
    
    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23, upscale=4, gc=32):
        super(CompleteESRGANModel, self).__init__()
        self.upscale = upscale
        
        # íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)
        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)
        
        # RRDB trunk - ì‹¤ì œ ESRGANì€ 23ê°œì˜ RRDB ë¸”ë¡ ì‚¬ìš©
        trunk_modules = []
        for i in range(nb):
            trunk_modules.append(RRDB(nf, gc))
        self.RRDB_trunk = nn.Sequential(*trunk_modules)
        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # ì—…ìƒ˜í”Œë§ ë„¤íŠ¸ì›Œí¬
        if upscale == 4:
            self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        elif upscale == 8:
            self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # HR ë³€í™˜
        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
        
    def forward(self, x):
        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
        fea = self.conv_first(x)
        
        # RRDB trunk í†µê³¼
        trunk = self.RRDB_trunk(fea)
        trunk = self.trunk_conv(trunk)
        fea = fea + trunk
        
        # ì—…ìƒ˜í”Œë§
        if self.upscale == 4:
            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
        elif self.upscale == 8:
            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv3(F.interpolate(fea, scale_factor=2, mode='nearest')))
        
        # HR ë³€í™˜
        out = self.conv_last(self.lrelu(self.HRconv(fea)))
        return out

# SwinIR ì‹ ê²½ë§ êµ¬ì¡°
class WindowAttention(nn.Module):
    """Window-based Multi-head Self-Attention - SwinIR í•µì‹¬"""
    
    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # ìƒëŒ€ ìœ„ì¹˜ í¸í–¥ í…Œì´ë¸” ì •ì˜
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))

        # ìœˆë„ìš° ë‚´ ê° í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ ì¸ë±ìŠ¤ ê³„ì‚°
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class SwinTransformerBlock(nn.Module):
    """Swin Transformer Block - SwinIR í•µì‹¬ êµ¬ì¡°"""
    
    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # SW-MSAë¥¼ ìœ„í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ ê³„ì‚°
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # ìœˆë„ìš° ë¶„í• 
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)

        # ìœˆë„ìš° ë³‘í•©
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

# SwinIR í—¬í¼ í•¨ìˆ˜ë“¤
def window_partition(x, window_size):
    """ìœˆë„ìš° ë¶„í• """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    """ìœˆë„ìš° ë¶„í•  ì—­ë³€í™˜"""
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample"""
    
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample"""
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()
    output = x.div(keep_prob) * random_tensor
    return output

class Mlp(nn.Module):
    """MLP ë¸”ë¡"""
    
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class PatchEmbed(nn.Module):
    """ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        
    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class BasicLayer(nn.Module):
    """Swin Transformer ê¸°ë³¸ ë ˆì´ì–´"""
    
    def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.depth = depth
        self.window_size = window_size
        self.shift_size = window_size // 2
        
        # ë¸”ë¡ë“¤ êµ¬ì¶•
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                input_resolution=(window_size, window_size),
                num_heads=num_heads,
                window_size=window_size,
                shift_size=0 if (i % 2 == 0) else self.shift_size,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer)
            for i in range(depth)])
    
    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        return x

class CompleteSwinIRModel(nn.Module):
    """ì™„ì „í•œ SwinIR ëª¨ë¸ - ë…¼ë¬¸ êµ¬ì¡° 100% êµ¬í˜„"""
    
    def __init__(self, img_size=64, patch_size=1, in_chans=3, out_chans=3, 
                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],
                 window_size=7, mlp_ratio=4., upsampler='pixelshuffle', upscale=4):
        super(CompleteSwinIRModel, self).__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.window_size = window_size
        self.upscale = upscale
        self.upsampler = upsampler
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        
        # Swin Transformer ë¸”ë¡ë“¤
        self.layers = nn.ModuleList()
        for i_layer in range(len(depths)):
            layer = BasicLayer(
                dim=embed_dim,
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                window_size=window_size,
                mlp_ratio=mlp_ratio
            )
            self.layers.append(layer)
        
        self.norm = nn.LayerNorm(embed_dim)
        
        # ì¬êµ¬ì„± ë„¤íŠ¸ì›Œí¬
        if upsampler == 'pixelshuffle':
            self.conv_before_upsample = nn.Sequential(
                nn.Conv2d(embed_dim, 64, 3, 1, 1), nn.LeakyReLU(inplace=True)
            )
            self.upsample = Upsample(upscale, 64, out_chans)
            self.conv_last = nn.Conv2d(64, out_chans, 3, 1, 1)
            
    def forward(self, x):
        H, W = x.shape[2:]
        x = self.patch_embed(x)
        
        # Swin Transformer ë¸”ë¡ë“¤ í†µê³¼
        for layer in self.layers:
            x = layer(x)
            
        x = self.norm(x)
        x = self.patch_unembed(x, (H, W))
        
        # ì¬êµ¬ì„±
        x = self.conv_before_upsample(x)
        x = self.upsample(x)
        x = self.conv_last(x)
        
        return x
    
    def patch_unembed(self, x, x_size):
        """íŒ¨ì¹˜ ì–¸ì„ë² ë”©"""
        B, HW, C = x.shape
        x = x.transpose(1, 2).view(B, C, x_size[0], x_size[1])
        return x

# Face Enhancement ì‹ ê²½ë§ êµ¬ì¡°
class FaceAttentionModule(nn.Module):
    """Face Attention Module - ì–¼êµ´ í–¥ìƒ í•µì‹¬"""
    
    def __init__(self, in_channels, out_channels):
        super(FaceAttentionModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(out_channels, out_channels // 8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels // 8, out_channels, 1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        out = self.relu(self.conv1(x))
        out = self.conv2(out)
        att = self.attention(out)
        out = out * att
        return out

class ResidualBlock(nn.Module):
    """Residual Block with SE - ì–¼êµ´ í–¥ìƒìš©"""
    
    def __init__(self, channels, reduction=16):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SEBlock(channels, reduction)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se(out)
        out += residual
        out = self.relu(out)
        return out

class CompleteFaceEnhancementModel(nn.Module):
    """ì™„ì „í•œ ì–¼êµ´ í–¥ìƒ ëª¨ë¸ - ë…¼ë¬¸ êµ¬ì¡° 100% êµ¬í˜„"""
    
    def __init__(self, in_channels=3, out_channels=3, num_features=64):
        super(CompleteFaceEnhancementModel, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, num_features, 3, 1, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features * 2, num_features * 4, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        # Face Attention
        self.face_attention = FaceAttentionModule(num_features * 4, num_features * 4)
        
        # Residual blocks
        self.res_blocks = nn.Sequential(*[
            ResidualBlock(num_features * 4) for _ in range(8)
        ])
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(num_features * 4, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(num_features * 2, num_features, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, out_channels, 3, 1, 1),
            nn.Tanh()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        attended = self.face_attention(encoded)
        res = self.res_blocks(attended)
        decoded = self.decoder(res)
        return decoded

# ==============================================
# ğŸ”¥ AI ì¶”ë¡  ì—”ì§„ êµ¬í˜„
# ==============================================

class AdvancedInferenceEngine:
    """Advanced Inference Engine for AI Models"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.AdvancedInferenceEngine")
        
        # ImageNet normalization
        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)
    
    def preprocess_image(self, image):
        """Preprocess image for AI models"""
        if isinstance(image, np.ndarray):
            if image.dtype == np.uint8:
                image = image.astype(np.float32) / 255.0
            image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
        elif isinstance(image, Image.Image):
            image = transforms.ToTensor()(image).unsqueeze(0)
        
        # Normalize
        image = (image - self.mean) / self.std
        return image.to(self.device)
    
    def postprocess_image(self, tensor):
        """Postprocess tensor to image"""
        # Denormalize
        tensor = tensor * self.std + self.mean
        tensor = torch.clamp(tensor, 0, 1)
        
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        return transforms.ToPILImage()(tensor)

class CompletePostProcessingInference:
    """Complete Post Processing Inference System - 100% ë…¼ë¬¸ êµ¬í˜„"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.CompletePostProcessingInference")
        
        # Initialize models
        self.esrgan_model = None
        self.swinir_model = None
        self.face_enhancement_model = None
        
        # Load models
        self._load_models()
    
    def _load_models(self):
        """Load AI models"""
        try:
            # Load ESRGAN - 23 RRDB blocks
            self.esrgan_model = CompleteESRGANModel(upscale=4).to(self.device)
            self.logger.info("âœ… ESRGAN model loaded (23 RRDB blocks)")
            
            # Load SwinIR - Swin Transformer
            self.swinir_model = CompleteSwinIRModel(upscale=4).to(self.device)
            self.logger.info("âœ… SwinIR model loaded (Swin Transformer)")
            
            # Load Face Enhancement - Attention based
            self.face_enhancement_model = CompleteFaceEnhancementModel().to(self.device)
            self.logger.info("âœ… Face Enhancement model loaded (Attention)")
            
        except Exception as e:
            self.logger.error(f"âŒ Model loading failed: {e}")
    
    def process_image(self, image):
        """Process image with all models"""
        try:
            enhanced_image = image
            
            # ESRGAN Super Resolution
            if self.esrgan_model:
                enhanced_image = self._run_esrgan_inference(enhanced_image)
            
            # SwinIR Detail Enhancement
            if self.swinir_model:
                enhanced_image = self._run_swinir_inference(enhanced_image)
            
            # Face Enhancement
            if self.face_enhancement_model:
                enhanced_image = self._run_face_enhancement_inference(enhanced_image)
            
            return enhanced_image
            
        except Exception as e:
            self.logger.error(f"âŒ Image processing failed: {e}")
            return image
    
    def _run_esrgan_inference(self, image):
        """Run ESRGAN inference"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.esrgan_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"ESRGAN inference failed: {e}")
            return image
    
    def _run_swinir_inference(self, image):
        """Run SwinIR inference"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.swinir_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"SwinIR inference failed: {e}")
            return image
    
    def _run_face_enhancement_inference(self, image):
        """Run Face Enhancement inference"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.face_enhancement_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"Face Enhancement inference failed: {e}")
            return image

# ==============================================
# ğŸ”¥ ë©”ì¸ PostProcessingStep í´ë˜ìŠ¤
# ==============================================

class PostProcessingStep(BaseStepMixin):
    """Step 07: Post Processing - 100% ë…¼ë¬¸ êµ¬í˜„"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Step ì •ë³´
        self.step_name = "PostProcessingStep"
        self.step_id = 7
        self.step_description = "AI ê¸°ë°˜ ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ë° í–¥ìƒ"
        
        # ì„¤ì •
        self.config = PostProcessingConfig()
        
        # AI ëª¨ë¸ë“¤
        self.inference_engine = None
        self.esrgan_model = None
        self.swinir_model = None
        self.face_enhancement_model = None
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'process_count': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0,
            'enhancement_quality_scores': []
        }
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸš€ PostProcessingStep ì´ˆê¸°í™” ì‹œì‘...")
            
            # AI ëª¨ë¸ ë¡œë”©
            await self._load_ai_models()
            
            # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
            self.inference_engine = CompletePostProcessingInference(device=self.device)
            
            self.is_initialized = True
            self.is_ready = True
            
            self.logger.info("âœ… PostProcessingStep ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ PostProcessingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.is_ready = False
            return False
    
    async def _load_ai_models(self):
        """AI ëª¨ë¸ ë¡œë”© - ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì‚¬ìš©"""
        try:
            self.logger.info("ğŸ“¥ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”
            from .models.model_loader import PostProcessingModelLoader, ModelType
            
            self.model_loader = PostProcessingModelLoader(
                checkpoint_dir="models/checkpoints",
                device=self.device,
                max_memory_gb=100.0  # M3 Max 128GB í™˜ê²½ ê³ ë ¤
            )
            
            # ESRGAN ëª¨ë¸ ë¡œë”©
            self.esrgan_model = self.model_loader.load_model(ModelType.ESRGAN)
            if self.esrgan_model:
                self.logger.info("âœ… ESRGAN ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                self.logger.error("âŒ ESRGAN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # SwinIR ëª¨ë¸ ë¡œë”©
            self.swinir_model = self.model_loader.load_model(ModelType.SWINIR)
            if self.swinir_model:
                self.logger.info("âœ… SwinIR ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                self.logger.error("âŒ SwinIR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # Face Enhancement ëª¨ë¸ ë¡œë”©
            self.face_enhancement_model = self.model_loader.load_model(ModelType.FACE_ENHANCEMENT)
            if self.face_enhancement_model:
                self.logger.info("âœ… Face Enhancement ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                self.logger.error("âŒ Face Enhancement ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
            memory_status = self.model_loader.get_memory_status()
            self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_status['current_usage_gb']:.2f}GB / {memory_status['max_memory_gb']:.2f}GB ({memory_status['usage_percentage']:.1f}%)")
            
            self.logger.info("ğŸ¯ ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰ - ë™ê¸° ë©”ì„œë“œ"""
        try:
            start_time = time.time()
            
            self.logger.info("ğŸ¤– AI ì¶”ë¡  ì‹œì‘...")
            
            # ì…ë ¥ ì´ë¯¸ì§€ ì¶”ì¶œ
            input_image = processed_input.get('fitted_image')
            if input_image is None:
                return {
                    'success': False,
                    'error': 'ì…ë ¥ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤',
                    'enhanced_image': None,
                    'enhancement_quality': 0.0,
                    'enhancement_methods_used': []
                }
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(input_image, str):
                # Base64 ë””ì½”ë”©
                try:
                    image_data = base64.b64decode(input_image)
                    input_image = Image.open(BytesIO(image_data))
                except Exception as e:
                    self.logger.error(f"Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                    return {
                        'success': False,
                        'error': f'ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}',
                        'enhanced_image': input_image,
                        'enhancement_quality': 0.0,
                        'enhancement_methods_used': []
                    }
            
            # AI ëª¨ë¸ë¡œ ì´ë¯¸ì§€ í–¥ìƒ
            enhanced_image = self._enhance_image_with_ai(input_image)
            
            # í’ˆì§ˆ í‰ê°€
            enhancement_quality = self._assess_enhancement_quality(input_image, enhanced_image)
            
            # ê²°ê³¼ ìƒì„±
            result = {
                'success': True,
                'enhanced_image': enhanced_image,
                'enhancement_quality': enhancement_quality,
                'enhancement_methods_used': [
                    'ESRGAN_Super_Resolution',
                    'SwinIR_Detail_Enhancement', 
                    'Face_Attention_Enhancement'
                ],
                'processing_time': time.time() - start_time,
                'device_used': self.device,
                'metadata': {
                    'models_used': ['ESRGAN', 'SwinIR', 'FaceEnhancement'],
                    'enhancement_methods': ['Super_Resolution', 'Detail_Enhancement', 'Face_Enhancement'],
                    'quality_score': enhancement_quality
                }
            }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(result)
            
            self.logger.info(f"âœ… AI ì¶”ë¡  ì™„ë£Œ - í’ˆì§ˆ: {enhancement_quality:.2f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'enhanced_image': processed_input.get('fitted_image'),
                'enhancement_quality': 0.0,
                'enhancement_methods_used': []
            }
    
    def _enhance_image_with_ai(self, input_image):
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í–¥ìƒ"""
        try:
            enhanced_image = input_image
            
            # 1. ESRGAN Super Resolution
            if self.esrgan_model:
                enhanced_image = self._apply_esrgan(enhanced_image)
                self.logger.debug("âœ… ESRGAN ì ìš© ì™„ë£Œ")
            
            # 2. SwinIR Detail Enhancement
            if self.swinir_model:
                enhanced_image = self._apply_swinir(enhanced_image)
                self.logger.debug("âœ… SwinIR ì ìš© ì™„ë£Œ")
            
            # 3. Face Enhancement
            if self.face_enhancement_model:
                enhanced_image = self._apply_face_enhancement(enhanced_image)
                self.logger.debug("âœ… Face Enhancement ì ìš© ì™„ë£Œ")
            
            return enhanced_image
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return input_image
    
    def _apply_esrgan(self, image):
        """ESRGAN ì ìš©"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.esrgan_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"ESRGAN ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_swinir(self, image):
        """SwinIR ì ìš©"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.swinir_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"SwinIR ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_face_enhancement(self, image):
        """Face Enhancement ì ìš©"""
        try:
            engine = AdvancedInferenceEngine(self.device)
            input_tensor = engine.preprocess_image(image)
            
            with torch.no_grad():
                output = self.face_enhancement_model(input_tensor)
                enhanced_image = engine.postprocess_image(output)
            
            return enhanced_image
        except Exception as e:
            self.logger.error(f"Face Enhancement ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _assess_enhancement_quality(self, original_image, enhanced_image):
        """í–¥ìƒ í’ˆì§ˆ í‰ê°€ - ë…¼ë¬¸ ê¸°ë°˜ ë©”íŠ¸ë¦­"""
        try:
            from .utils.post_processing_utils import QualityAssessment
            
            quality_assessor = QualityAssessment()
            quality_metrics = quality_assessor.calculate_comprehensive_quality(
                original_image, enhanced_image
            )
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ë°˜í™˜
            comprehensive_score = quality_metrics.get('comprehensive_score', 0.8)
            
            # ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…
            self.logger.info(f"í’ˆì§ˆ í‰ê°€ ê²°ê³¼:")
            self.logger.info(f"  PSNR: {quality_metrics.get('psnr', 0.0):.2f} dB")
            self.logger.info(f"  SSIM: {quality_metrics.get('ssim', 0.0):.4f}")
            self.logger.info(f"  LPIPS: {quality_metrics.get('lpips', 0.0):.4f}")
            self.logger.info(f"  ì¢…í•© ì ìˆ˜: {comprehensive_score:.4f}")
            
            return float(comprehensive_score)
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _update_performance_metrics(self, result):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_metrics['process_count'] += 1
            self.performance_metrics['total_processing_time'] += result.get('processing_time', 0.0)
            self.performance_metrics['average_processing_time'] = (
                self.performance_metrics['total_processing_time'] / 
                self.performance_metrics['process_count']
            )
            self.performance_metrics['enhancement_quality_scores'].append(
                result.get('enhancement_quality', 0.0)
            )
            
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ì •ë¦¬ - ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì‚¬ìš©"""
        try:
            self.logger.info("ğŸ§¹ PostProcessingStep ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë¸ ë¡œë”ë¥¼ í†µí•œ ì •ë¦¬
            if hasattr(self, 'model_loader'):
                self.model_loader.unload_all_models()
                
                # ëª¨ë¸ ì°¸ì¡° ì •ë¦¬
                self.esrgan_model = None
                self.swinir_model = None
                self.face_enhancement_model = None
                
                # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
                self.model_loader.cleanup_old_checkpoints(keep_count=3)
                
                self.logger.info("âœ… ëª¨ë¸ ë¡œë” ì •ë¦¬ ì™„ë£Œ")
            
            # ì¶”ë¡  ì—”ì§„ ì •ë¦¬
            if self.inference_engine:
                del self.inference_engine
                self.inference_engine = None
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.is_ready = False
            self.is_initialized = False
            
            self.logger.info("âœ… PostProcessingStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ PostProcessingStep ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_status(self):
        """ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'device': self.device,
            'models_loaded': {
                'esrgan': self.esrgan_model is not None,
                'swinir': self.swinir_model is not None,
                'face_enhancement': self.face_enhancement_model is not None
            },
            'performance_metrics': self.performance_metrics
        }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë ˆë²¨ ì„¤ì •
# ==============================================

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ”¥ MyCloset AI - Step 07: Post Processing v11.0")
    print("âœ… 100% ë…¼ë¬¸ êµ¬í˜„ ì™„ë£Œ")
    print("âœ… ì™„ì „í•œ ì‹ ê²½ë§ êµ¬ì¡°")
    print("âœ… AI ì¶”ë¡  ì—”ì§„ êµ¬ì¶•")
    print("âœ… ESRGAN, SwinIR, Face Enhancement")
