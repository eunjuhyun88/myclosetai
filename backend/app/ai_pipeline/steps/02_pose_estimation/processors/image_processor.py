#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Pose Estimation Image Processor
===============================================

âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ ë¶„ë¦¬
âœ… ê¸°ì¡´ step.py ê¸°ëŠ¥ ë³´ì¡´
âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡° ì ìš©
"""

import logging
from app.ai_pipeline.utils.common_imports import (
    np, Image, cv2, torch, TORCH_AVAILABLE, PIL_AVAILABLE, CV2_AVAILABLE,
    Dict, Any, Optional, Tuple, List, Union
)

logger = logging.getLogger(__name__)

class ImageProcessor:
    """ì´ë¯¸ì§€ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.ImageProcessor")
    
    def preprocess_image(self, image: Union[torch.Tensor, np.ndarray, Image.Image], 
                        target_size: Tuple[int, int] = (512, 512)) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, Image.Image):
                image_np = np.array(image)
            elif isinstance(image, torch.Tensor):
                image_np = image.cpu().numpy()
            elif isinstance(image, np.ndarray):
                image_np = image.copy()
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # RGBë¡œ ë³€í™˜
            if len(image_np.shape) == 3 and image_np.shape[2] == 3:
                pass  # ì´ë¯¸ RGB
            elif len(image_np.shape) == 3 and image_np.shape[2] == 4:
                image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
            elif len(image_np.shape) == 2:
                image_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2RGB)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì±„ë„: {image_np.shape}")
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            if image_np.shape[:2] != target_size:
                image_np = cv2.resize(image_np, target_size, interpolation=cv2.INTER_LINEAR)
            
            return image_np
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def preprocess_image_with_scale(self, image: Union[torch.Tensor, np.ndarray, Image.Image], 
                                   target_size: Tuple[int, int] = (512, 512)) -> Tuple[np.ndarray, float]:
        """ìŠ¤ì¼€ì¼ ì •ë³´ì™€ í•¨ê»˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            original_size = None
            
            # ì›ë³¸ í¬ê¸° ì €ì¥
            if isinstance(image, Image.Image):
                original_size = image.size[::-1]  # (height, width)
                image_np = np.array(image)
            elif isinstance(image, torch.Tensor):
                original_size = image.shape[-2:]  # (height, width)
                image_np = image.cpu().numpy()
            elif isinstance(image, np.ndarray):
                original_size = image.shape[:2]  # (height, width)
                image_np = image.copy()
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # ì „ì²˜ë¦¬
            processed_image = self.preprocess_image(image_np, target_size)
            
            # ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚°
            if original_size:
                scale_factor = min(target_size[0] / original_size[0], target_size[1] / original_size[1])
            else:
                scale_factor = 1.0
            
            return processed_image, scale_factor
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ í¬í•¨) ì‹¤íŒ¨: {e}")
            raise
    
    def normalize_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì •ê·œí™”"""
        try:
            if image.dtype != np.float32:
                image = image.astype(np.float32)
            
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            if image.max() > 1.0:
                image = image / 255.0
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            raise
    
    def denormalize_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì—­ì •ê·œí™”"""
        try:
            if image.dtype != np.float32:
                image = image.astype(np.float32)
            
            # 0-255 ë²”ìœ„ë¡œ ì—­ì •ê·œí™”
            if image.max() <= 1.0:
                image = image * 255.0
            
            return image.astype(np.uint8)
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì—­ì •ê·œí™” ì‹¤íŒ¨: {e}")
            raise
    
    def apply_augmentation(self, image: np.ndarray, augmentation_type: str = "none") -> np.ndarray:
        """ì´ë¯¸ì§€ ì¦ê°•"""
        try:
            if augmentation_type == "none":
                return image
            
            augmented_image = image.copy()
            
            if augmentation_type == "flip_horizontal":
                augmented_image = cv2.flip(augmented_image, 1)
            elif augmentation_type == "flip_vertical":
                augmented_image = cv2.flip(augmented_image, 0)
            elif augmentation_type == "rotate_90":
                augmented_image = cv2.rotate(augmented_image, cv2.ROTATE_90_CLOCKWISE)
            elif augmentation_type == "rotate_180":
                augmented_image = cv2.rotate(augmented_image, cv2.ROTATE_180)
            elif augmentation_type == "rotate_270":
                augmented_image = cv2.rotate(augmented_image, cv2.ROTATE_90_COUNTERCLOCKWISE)
            
            return augmented_image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì¦ê°• ì‹¤íŒ¨: {e}")
            return image
    
    def extract_patches(self, image: np.ndarray, patch_size: Tuple[int, int] = (64, 64), 
                       stride: int = 32) -> List[np.ndarray]:
        """ì´ë¯¸ì§€ì—ì„œ íŒ¨ì¹˜ ì¶”ì¶œ"""
        try:
            patches = []
            height, width = image.shape[:2]
            patch_h, patch_w = patch_size
            
            for y in range(0, height - patch_h + 1, stride):
                for x in range(0, width - patch_w + 1, stride):
                    patch = image[y:y + patch_h, x:x + patch_w]
                    patches.append(patch)
            
            return patches
            
        except Exception as e:
            self.logger.error(f"âŒ íŒ¨ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def merge_patches(self, patches: List[np.ndarray], original_size: Tuple[int, int], 
                     patch_size: Tuple[int, int] = (64, 64), stride: int = 32) -> np.ndarray:
        """íŒ¨ì¹˜ë“¤ì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³‘í•©"""
        try:
            height, width = original_size
            patch_h, patch_w = patch_size
            
            # ë¹ˆ ì´ë¯¸ì§€ ìƒì„±
            merged_image = np.zeros((height, width, 3), dtype=np.float32)
            count_image = np.zeros((height, width, 3), dtype=np.float32)
            
            patch_idx = 0
            for y in range(0, height - patch_h + 1, stride):
                for x in range(0, width - patch_w + 1, stride):
                    if patch_idx < len(patches):
                        patch = patches[patch_idx]
                        merged_image[y:y + patch_h, x:x + patch_w] += patch
                        count_image[y:y + patch_h, x:x + patch_w] += 1
                        patch_idx += 1
            
            # í‰ê·  ê³„ì‚°
            count_image[count_image == 0] = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            merged_image = merged_image / count_image
            
            return merged_image
            
        except Exception as e:
            self.logger.error(f"âŒ íŒ¨ì¹˜ ë³‘í•© ì‹¤íŒ¨: {e}")
            return np.zeros(original_size + (3,), dtype=np.float32)
