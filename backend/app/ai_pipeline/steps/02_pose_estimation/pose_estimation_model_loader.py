# backend/app/ai_pipeline/steps/pose_estimation_integrated_loader.py
"""
ğŸ”¥ PoseEstimationStep í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ
================================================================================

âœ… Central Hub í†µí•©
âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ ì—°ë™
âœ… ëª¨ë¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ìƒì„±
âœ… ë‹¨ê³„ì  í´ë°± ì‹œìŠ¤í…œ
âœ… BaseStepMixin ì™„ì „ í˜¸í™˜

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0 (í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ)
"""

import os
import sys
import gc
import time
import json
import logging
import traceback
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass

# PyTorch ì•ˆì „ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None

# NumPy ì•ˆì „ import
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# MPS ì§€ì› í™•ì¸
MPS_AVAILABLE = TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()

# ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
DEFAULT_DEVICE = "mps" if MPS_AVAILABLE else ("cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu")

logger = logging.getLogger(__name__)

@dataclass
class ModelLoadingResult:
    """ëª¨ë¸ ë¡œë”© ê²°ê³¼"""
    success: bool
    model: Optional[Any] = None
    model_name: str = ""
    loading_method: str = ""
    error_message: str = ""
    processing_time: float = 0.0

class PoseEstimationModelLoader:
    """PoseEstimationStep ì „ìš© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, logger=None):
        self.device = self._setup_device(device)
        self.logger = logger or logging.getLogger(self.__class__.__name__)
        self.models = {}
        self.loaded_models = {}
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if MPS_AVAILABLE:
                return "mps"
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def load_models_integrated(self) -> bool:
        """í†µí•©ëœ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ - ë©”ì¸ ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ PoseEstimation í†µí•© ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ ì‹œì‘")
            start_time = time.time()
            
            # 1ë‹¨ê³„: Central Hub ì‹œë„
            if self._load_via_central_hub():
                self.logger.info("âœ… Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                return True
            
            # 2ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ë¡œë”©
            models_loaded = 0
            for model_name in ['hrnet', 'openpose', 'yolo_pose', 'mediapipe']:
                result = self._load_with_checkpoint_analysis(model_name)
                if result.success:
                    self.models[model_name] = result.model
                    models_loaded += 1
            
            if models_loaded > 0:
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {models_loaded}ê°œ")
                return True
            
            # 3ë‹¨ê³„: ì•„í‚¤í…ì²˜ ê¸°ë°˜ ìƒì„±
            fallback_models = {
                'hrnet': {'num_keypoints': 17, 'architecture_type': 'hrnet'},
                'openpose': {'num_keypoints': 18, 'architecture_type': 'openpose'},
                'yolo_pose': {'num_keypoints': 17, 'architecture_type': 'yolo_pose'},
                'mediapipe': {'num_keypoints': 17, 'architecture_type': 'mediapipe'}
            }
            
            for model_name, config in fallback_models.items():
                result = self._create_with_architecture(model_name, config)
                if result.success:
                    self.models[model_name] = result.model
                    models_loaded += 1
            
            if models_loaded > 0:
                self.logger.info(f"âœ… ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„± ì„±ê³µ: {models_loaded}ê°œ")
                return True
            
            self.logger.error("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ë°©ë²• ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_via_central_hub(self) -> bool:
        """Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            # Central Hubì—ì„œ ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            model_loader_service = self._get_service_from_central_hub('model_loader')
            if not model_loader_service:
                self.logger.warning("âš ï¸ Central Hubì—ì„œ ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # Stepë³„ ìµœì  ëª¨ë¸ ë¡œë“œ
            models_to_load = {
                'hrnet': 'pose_estimation',
                'openpose': 'pose_estimation', 
                'yolo_pose': 'pose_estimation',
                'mediapipe': 'pose_estimation'
            }
            
            models_loaded = 0
            for model_name, step_type in models_to_load.items():
                try:
                    model = model_loader_service.load_model_for_step(step_type, model_name)
                    if model:
                        self.models[model_name] = model
                        models_loaded += 1
                        self.logger.info(f"âœ… Central Hubë¥¼ í†µí•œ {model_name} ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Central Hubë¥¼ í†µí•œ {model_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            return models_loaded > 0
            
        except Exception as e:
            self.logger.error(f"âŒ Central Hub ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_with_checkpoint_analysis(self, model_name: str) -> ModelLoadingResult:
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”©"""
        start_time = time.time()
        try:
            self.logger.info(f"ğŸ” {model_name} ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹œì‘")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            checkpoint_analyzer = self._get_service_from_central_hub('checkpoint_analyzer')
            if not checkpoint_analyzer:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_name,
                    loading_method="checkpoint_analysis",
                    error_message="ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    processing_time=time.time() - start_time
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ëª¨ë¸ ìƒì„±
            config = checkpoint_analyzer.analyze_pose_model(model_name)
            if not config:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_name,
                    loading_method="checkpoint_analysis", 
                    error_message="ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
            
            # ëª¨ë¸ ìƒì„±
            if model_name == 'hrnet':
                result = self._load_hrnet_from_modules(config)
            elif model_name == 'openpose':
                result = self._load_openpose_from_modules(config)
            elif model_name == 'yolo_pose':
                result = self._load_yolo_pose_from_modules(config)
            elif model_name == 'mediapipe':
                result = self._load_mediapipe_from_modules(config)
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name=model_name,
                    loading_method="checkpoint_analysis",
                    error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}",
                    processing_time=time.time() - start_time
                )
            
            return result
            
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name=model_name,
                loading_method="checkpoint_analysis",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _create_with_architecture(self, model_type: str, config: Dict[str, Any]) -> ModelLoadingResult:
        """ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„±"""
        start_time = time.time()
        try:
            self.logger.info(f"ğŸ—ï¸ {model_type} ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ ìƒì„±")
            
            # ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±
            model = self._create_basic_architecture(model_type, config)
            
            return ModelLoadingResult(
                success=True,
                model=model,
                model_name=model_type,
                loading_method="architecture_based",
                processing_time=time.time() - start_time
            )
            
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name=model_type,
                loading_method="architecture_based",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_hrnet_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ HRNet ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .pose_estimation.models.openpose_model import HRNetModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = HRNetModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… HRNet ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='hrnet',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='hrnet',
                    loading_method="modular_architecture",
                    error_message="HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ HRNet ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('hrnet', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='hrnet',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_openpose_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ OpenPose ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .pose_estimation.models.openpose_model import OpenPoseModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = OpenPoseModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… OpenPose ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='openpose',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='openpose',
                    loading_method="modular_architecture",
                    error_message="OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ OpenPose ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('openpose', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='openpose',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_yolo_pose_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ YOLO Pose ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .pose_estimation.models.yolov8_model import YOLOv8PoseModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = YOLOv8PoseModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… YOLO Pose ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='yolo_pose',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='yolo_pose',
                    loading_method="modular_architecture",
                    error_message="YOLO Pose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ YOLO Pose ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('yolo_pose', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='yolo_pose',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _load_mediapipe_from_modules(self, config: Dict[str, Any]) -> ModelLoadingResult:
        """ê¸°ì¡´ ëª¨ë“ˆí™”ëœ MediaPipe ëª¨ë¸ ë¡œë“œ - ì‹¤ì œ API í˜¸í™˜"""
        start_time = time.time()
        try:
            from .pose_estimation.models.mediapipe_model import MediaPoseModel
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ ì‹¤ì œ APIì— ë§ê²Œ í˜¸ì¶œ
            model_path = config.get('model_path')
            model = MediaPoseModel(model_path=model_path)
            
            # ê¸°ì¡´ ëª¨ë“ˆì˜ load_model() ë©”ì„œë“œ í˜¸ì¶œ
            success = model.load_model()
            
            if success and model.loaded:
                self.logger.info(f"âœ… MediaPipe ëª¨ë“ˆí™”ëœ êµ¬ì¡°ì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return ModelLoadingResult(
                    success=True,
                    model=model,
                    model_name='mediapipe',
                    loading_method="modular_architecture",
                    processing_time=time.time() - start_time
                )
            else:
                return ModelLoadingResult(
                    success=False,
                    model_name='mediapipe',
                    loading_method="modular_architecture",
                    error_message="MediaPipe ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
                    processing_time=time.time() - start_time
                )
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ MediaPipe ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            return self._create_basic_architecture('mediapipe', config)
        except Exception as e:
            return ModelLoadingResult(
                success=False,
                model_name='mediapipe',
                loading_method="modular_architecture",
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _create_basic_architecture(self, model_type: str, config: Dict[str, Any]):
        """ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if model_type == 'hrnet':
            return self._create_hrnet_architecture(config)
        elif model_type == 'openpose':
            return self._create_openpose_architecture(config)
        elif model_type == 'yolo_pose':
            return self._create_yolo_pose_architecture(config)
        elif model_type == 'mediapipe':
            return self._create_mediapipe_architecture(config)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    def _create_hrnet_architecture(self, config: Dict[str, Any]):
        """HRNet ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class BasicHRNet(nn.Module):
            def __init__(self, num_keypoints=17):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2, 2)
                )
                self.head = nn.Conv2d(128, num_keypoints, 1)
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.head(features)
                return heatmaps
        
        model = BasicHRNet(config.get('num_keypoints', 17))
        model.to(self.device)
        model.eval()
        return model
    
    def _create_openpose_architecture(self, config: Dict[str, Any]):
        """OpenPose ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class BasicOpenPose(nn.Module):
            def __init__(self, num_keypoints=18):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2, 2)
                )
                self.paf_head = nn.Conv2d(128, 38, 1)  # 19 limbs * 2
                self.confidence_head = nn.Conv2d(128, num_keypoints, 1)
            
            def forward(self, x):
                features = self.backbone(x)
                pafs = self.paf_head(features)
                confidence = self.confidence_head(features)
                return pafs, confidence
        
        model = BasicOpenPose(config.get('num_keypoints', 18))
        model.to(self.device)
        model.eval()
        return model
    
    def _create_yolo_pose_architecture(self, config: Dict[str, Any]):
        """YOLO Pose ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class BasicYOLOPose(nn.Module):
            def __init__(self, num_keypoints=17):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2, 2)
                )
                self.head = nn.Conv2d(128, num_keypoints * 3, 1)  # x, y, confidence
            
            def forward(self, x):
                features = self.backbone(x)
                keypoints = self.head(features)
                return keypoints
        
        model = BasicYOLOPose(config.get('num_keypoints', 17))
        model.to(self.device)
        model.eval()
        return model
    
    def _create_mediapipe_architecture(self, config: Dict[str, Any]):
        """MediaPipe ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        # MediaPipeëŠ” ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë¯€ë¡œ ê°„ë‹¨í•œ ë˜í¼ ìƒì„±
        class MediaPipeWrapper:
            def __init__(self):
                self.loaded = False
                try:
                    import mediapipe as mp
                    self.mp = mp
                    self.model = mp.solutions.pose.Pose(
                        static_image_mode=False,
                        model_complexity=1,
                        enable_segmentation=False,
                        min_detection_confidence=0.5,
                        min_tracking_confidence=0.5
                    )
                    self.loaded = True
                except ImportError:
                    pass
            
            def detect_poses(self, image):
                if not self.loaded:
                    return {"error": "MediaPipeë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                try:
                    results = self.model.process(image)
                    if results.pose_landmarks:
                        keypoints = []
                        for landmark in results.pose_landmarks.landmark:
                            keypoints.append([landmark.x, landmark.y, landmark.z])
                        return {
                            "keypoints": keypoints,
                            "success": True
                        }
                    else:
                        return {
                            "keypoints": [],
                            "success": False
                        }
                except Exception as e:
                    return {"error": str(e), "success": False}
        
        return MediaPipeWrapper()
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            from app.ai_pipeline.utils.common_imports import _get_central_hub_container
            container = _get_central_hub_container()
            if container and hasattr(container, 'get_service'):
                return container.get_service(service_key)
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None
    
    def get_loaded_models(self) -> Dict[str, Any]:
        """ë¡œë“œëœ ëª¨ë¸ë“¤ ë°˜í™˜"""
        return self.models.copy()
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            for model_name, model in self.models.items():
                if hasattr(model, 'cleanup'):
                    model.cleanup()
                elif hasattr(model, 'close'):
                    model.close()
            
            self.models.clear()
            self.loaded_models.clear()
            
            if TORCH_AVAILABLE:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            gc.collect()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

def get_integrated_loader(device: str = DEFAULT_DEVICE, logger=None) -> PoseEstimationModelLoader:
    """í†µí•© ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return PoseEstimationModelLoader(device=device, logger=logger)
