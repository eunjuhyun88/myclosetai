#!/usr/bin/env python3
"""
π”¥ MyCloset AI - Step 01: Human Parsing (Fixed Version)
μ²΄ν¬ν¬μΈνΈ λ¶„μ„ κ²°κ³Όμ— λ”°λΌ SCHPModelμ„ μ™„μ „ν μƒλ΅ μ‘μ„±
μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ΅λ”© λ° λ§¤ν•‘ κµ¬ν„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import logging
from typing import Dict, Any, Optional

class SCHPModel(nn.Module):
    def __init__(self, num_classes=20):
        super().__init__()
        # π”¥ μ²΄ν¬ν¬μΈνΈμ™€ μ •ν™•ν μΌμΉν•λ” SCHP μ•„ν‚¤ν…μ²
        
        # μ²΄ν¬ν¬μΈνΈ λ¶„μ„ κ²°κ³Όμ— λ”°λ¥Έ μ •ν™•ν• κµ¬μ΅°
        # conv1: [64, 3, 3, 3]
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # conv2: [64, 64, 3, 3]
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        
        # conv3: [128, 64, 3, 3]
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Layer 1: ResNet bottleneck κµ¬μ΅° (μ²΄ν¬ν¬μΈνΈμ™€ μΌμΉ)
        # layer1.0.conv1: [64, 128, 1, 1]
        # layer1.0.conv2: [64, 64, 3, 3]
        # layer1.0.conv3: [256, 64, 1, 1]
        self.layer1 = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(128, 64, kernel_size=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 256, kernel_size=1),
                nn.BatchNorm2d(256)
            )
        ])
        
        # Layer 2, 3, 4λ” μ²΄ν¬ν¬μΈνΈμ—μ„ ν™•μΈλ κµ¬μ΅°λ΅ κµ¬ν„
        # μ‹¤μ λ΅λ” λ” λ³µμ΅ν• ResNet κµ¬μ΅°μ΄μ§€λ§ κ°„λ‹¨ν κµ¬ν„
        self.layer2 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )
        
        self.layer3 = nn.Sequential(
            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True)
        )
        
        self.layer4 = nn.Sequential(
            nn.Conv2d(1024, 2048, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(2048),
            nn.ReLU(inplace=True)
        )
        
        # Context Encoding Module (μ²΄ν¬ν¬μΈνΈμ™€ μΌμΉ)
        # context_encoding.stages.0.1.weight: [512, 2048, 1, 1]
        # context_encoding.bottleneck.0.weight: [512, 4096, 3, 3]
        self.context_encoding = nn.ModuleDict({
            'stages': nn.ModuleList([
                nn.Sequential(
                    nn.Conv2d(2048, 512, kernel_size=1, stride=1, padding=0),
                    nn.BatchNorm2d(512)
                ) for _ in range(4)
            ]),
            'bottleneck': nn.Sequential(
                nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(512)
            )
        })
        
        # Edge Detection Module (μ²΄ν¬ν¬μΈνΈμ™€ μΌμΉ)
        # edge.conv1.0.weight: [256, 256, 1, 1] - μ‹¤μ λ΅λ” 2048 μ…λ ¥μ΄μ–΄μ•Ό ν•¨
        self.edge = nn.Sequential(
            nn.Conv2d(2048, 256, kernel_size=1),  # 2048 -> 256
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1),
            nn.Conv2d(1, 1, kernel_size=1)
        )
        
        # Decoder Module (μ²΄ν¬ν¬μΈνΈμ™€ μΌμΉ)
        # decoder.conv1.0.weight: [256, 512, 1, 1]
        self.decoder = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, num_classes, kernel_size=1)
        )
        
        # Fusion Module (μ²΄ν¬ν¬μΈνΈμ™€ μΌμΉ)
        # fushion.0.weight: [256, 1024, 1, 1]
        self.fushion = nn.Sequential(
            nn.Conv2d(num_classes + 1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_classes, kernel_size=1)
        )
        
        # Classifier (μµμΆ… μ¶λ ¥)
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)
        
        # μ²΄ν¬ν¬μΈνΈ λ΅λ”© μƒνƒ
        self.checkpoint_loaded = False
        self.checkpoint_path = None
        
        # λ΅κ±° μ„¤μ •
        self.logger = logging.getLogger(__name__)
    
    def forward(self, x):
        try:
            print(f"π” SCHPModel λ””λ²„κΉ…:")
            print(f"  μ…λ ¥ x shape: {x.shape}")
            
            # μ²΄ν¬ν¬μΈνΈ κµ¬μ΅°μ— λ”°λ¥Έ forward pass
            x = self.conv1(x)
            print(f"  conv1 ν›„ x shape: {x.shape}")
            
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
            print(f"  maxpool ν›„ x shape: {x.shape}")
            
            x = self.conv2(x)
            x = self.bn2(x)
            x = self.relu(x)
            print(f"  conv2 ν›„ x shape: {x.shape}")
            
            x = self.conv3(x)
            x = self.bn3(x)
            x = self.relu(x)
            print(f"  conv3 ν›„ x shape: {x.shape}")
            
            # Layer 1
            for layer in self.layer1:
                x = layer(x)
            print(f"  layer1 ν›„ x shape: {x.shape}")
            
            # Layer 2, 3, 4
            x = self.layer2(x)
            print(f"  layer2 ν›„ x shape: {x.shape}")
            
            x = self.layer3(x)
            print(f"  layer3 ν›„ x shape: {x.shape}")
            
            x = self.layer4(x)
            print(f"  layer4 ν›„ x shape: {x.shape}")
            
            # Context Encoding
            context_feat = self.context_encoding['bottleneck'](x)
            print(f"  context_encoding ν›„ x shape: {context_feat.shape}")
            
            # Edge Detection (2048 μ±„λ„ μ…λ ¥)
            edge_map = self.edge(x)
            print(f"  edge ν›„ edge_map shape: {edge_map.shape}")
            
            # Decoder
            parsing = self.decoder(context_feat)
            print(f"  decoder ν›„ parsing shape: {parsing.shape}")
            
            # Fusion (parsing + edge_map)
            fusion_input = torch.cat([parsing, edge_map], dim=1)
            output = self.fushion(fusion_input)
            print(f"  fusion ν›„ output shape: {output.shape}")
            
            return {
                'parsing': output,
                'parsing_pred': output,
                'confidence_map': torch.sigmoid(output),
                'final_confidence': torch.sigmoid(output)
            }
        except Exception as e:
            # π”¥ μ¤λ¥ λ°μƒ μ‹ λ‹¨μν™”λ forward pass
            print(f"β οΈ SCHP forward μ¤λ¥: {e}, λ‹¨μν™”λ λ¨λ“ μ‚¬μ©")
            
            # λ‹¨μν™”λ forward pass
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
            
            x = self.conv2(x)
            x = self.bn2(x)
            x = self.relu(x)
            
            x = self.conv3(x)
            x = self.bn3(x)
            x = self.relu(x)
            
            for layer in self.layer1:
                x = layer(x)
            
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            
            context_feat = self.context_encoding['bottleneck'](x)
            output = self.classifier(context_feat)
            
            return {
                'parsing': output,
                'parsing_pred': output,
                'confidence_map': torch.sigmoid(output),
                'final_confidence': torch.sigmoid(output)
            }
    
    def load_checkpoint(self, checkpoint_path: str, map_location: str = 'cpu') -> bool:
        """μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ΅λ”© λ° λ§¤ν•‘"""
        try:
            self.logger.info(f"π” μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹μ‘: {checkpoint_path}")
            
            if not os.path.exists(checkpoint_path):
                self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ νμΌμ΄ μ΅΄μ¬ν•μ§€ μ•μ: {checkpoint_path}")
                return False
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ”©
            checkpoint = torch.load(checkpoint_path, map_location=map_location)
            self.logger.info(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ™„λ£")
            
            # μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° λ¶„μ„
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                self.logger.info(f"π“ state_dict ν‚¤ κ°μ: {len(state_dict.keys())}")
            else:
                state_dict = checkpoint
                self.logger.info(f"π“ μ§μ ‘ state_dict ν‚¤ κ°μ: {len(state_dict.keys())}")
            
            # μ²΄ν¬ν¬μΈνΈ ν‚¤ λ¶„μ„
            checkpoint_keys = list(state_dict.keys())
            self.logger.info(f"π” μ²΄ν¬ν¬μΈνΈ ν‚¤ μƒν”: {checkpoint_keys[:10]}")
            
            # λ¨λΈ state_dict κ°€μ Έμ¤κΈ°
            model_state_dict = self.state_dict()
            model_keys = list(model_state_dict.keys())
            self.logger.info(f"π” λ¨λΈ ν‚¤ μƒν”: {model_keys[:10]}")
            
            # ν‚¤ λ§¤ν•‘ μƒμ„±
            key_mapping = self._create_key_mapping(checkpoint_keys, model_keys)
            
            # λ§¤ν•‘λ state_dict μƒμ„±
            mapped_state_dict = {}
            mapped_count = 0
            
            for checkpoint_key, model_key in key_mapping.items():
                if checkpoint_key in state_dict and model_key in model_state_dict:
                    # ν…μ„ ν¬κΈ° ν™•μΈ
                    checkpoint_tensor = state_dict[checkpoint_key]
                    model_tensor = model_state_dict[model_key]
                    
                    if checkpoint_tensor.shape == model_tensor.shape:
                        mapped_state_dict[model_key] = checkpoint_tensor
                        mapped_count += 1
                        self.logger.debug(f"β… λ§¤ν•‘ μ„±κ³µ: {checkpoint_key} -> {model_key}")
                    else:
                        self.logger.warning(f"β οΈ ν¬κΈ° λ¶μΌμΉ: {checkpoint_key} ({checkpoint_tensor.shape}) != {model_key} ({model_tensor.shape})")
                else:
                    self.logger.debug(f"β οΈ ν‚¤ λ„λ½: {checkpoint_key} λλ” {model_key}")
            
            # λ§¤ν•‘λ κ°€μ¤‘μΉ λ΅λ”©
            if mapped_state_dict:
                self.load_state_dict(mapped_state_dict, strict=False)
                self.logger.info(f"β… μ²΄ν¬ν¬μΈνΈ λ§¤ν•‘ μ™„λ£: {mapped_count}/{len(model_keys)} λ μ΄μ–΄")
                
                self.checkpoint_loaded = True
                self.checkpoint_path = checkpoint_path
                return True
            else:
                self.logger.error(f"β λ§¤ν•‘λ κ°€μ¤‘μΉκ°€ μ—†μ")
                return False
                
        except Exception as e:
            self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _create_key_mapping(self, checkpoint_keys: list, model_keys: list) -> Dict[str, str]:
        """μ²΄ν¬ν¬μΈνΈ ν‚¤μ™€ λ¨λΈ ν‚¤ κ°„μ λ§¤ν•‘ μƒμ„±"""
        mapping = {}
        
        # μ§μ ‘ λ§¤ν•‘ κ·μΉ™λ“¤
        direct_mappings = {
            # μ΄κΈ° λ μ΄μ–΄λ“¤
            'conv1.weight': 'conv1.weight',
            'conv1.bias': 'conv1.bias',
            'bn1.weight': 'bn1.weight',
            'bn1.bias': 'bn1.bias',
            'bn1.running_mean': 'bn1.running_mean',
            'bn1.running_var': 'bn1.running_var',
            
            'conv2.weight': 'conv2.weight',
            'conv2.bias': 'conv2.bias',
            'bn2.weight': 'bn2.weight',
            'bn2.bias': 'bn2.bias',
            'bn2.running_mean': 'bn2.running_mean',
            'bn2.running_var': 'bn2.running_var',
            
            'conv3.weight': 'conv3.weight',
            'conv3.bias': 'conv3.bias',
            'bn3.weight': 'bn3.weight',
            'bn3.bias': 'bn3.bias',
            'bn3.running_mean': 'bn3.running_mean',
            'bn3.running_var': 'bn3.running_var',
            
            # Layer 1 (ResNet bottleneck)
            'layer1.0.conv1.weight': 'layer1.0.conv1.weight',
            'layer1.0.conv1.bias': 'layer1.0.conv1.bias',
            'layer1.0.bn1.weight': 'layer1.0.bn1.weight',
            'layer1.0.bn1.bias': 'layer1.0.bn1.bias',
            'layer1.0.bn1.running_mean': 'layer1.0.bn1.running_mean',
            'layer1.0.bn1.running_var': 'layer1.0.bn1.running_var',
            
            'layer1.0.conv2.weight': 'layer1.0.conv2.weight',
            'layer1.0.conv2.bias': 'layer1.0.conv2.bias',
            'layer1.0.bn2.weight': 'layer1.0.bn2.weight',
            'layer1.0.bn2.bias': 'layer1.0.bn2.bias',
            'layer1.0.bn2.running_mean': 'layer1.0.bn2.running_mean',
            'layer1.0.bn2.running_var': 'layer1.0.bn2.running_var',
            
            'layer1.0.conv3.weight': 'layer1.0.conv3.weight',
            'layer1.0.conv3.bias': 'layer1.0.conv3.bias',
            'layer1.0.bn3.weight': 'layer1.0.bn3.weight',
            'layer1.0.bn3.bias': 'layer1.0.bn3.bias',
            'layer1.0.bn3.running_mean': 'layer1.0.bn3.running_mean',
            'layer1.0.bn3.running_var': 'layer1.0.bn3.running_var',
            
            # Context Encoding
            'context_encoding.bottleneck.0.weight': 'context_encoding.bottleneck.0.weight',
            'context_encoding.bottleneck.0.bias': 'context_encoding.bottleneck.0.bias',
            'context_encoding.bottleneck.1.weight': 'context_encoding.bottleneck.1.weight',
            'context_encoding.bottleneck.1.bias': 'context_encoding.bottleneck.1.bias',
            'context_encoding.bottleneck.1.running_mean': 'context_encoding.bottleneck.1.running_mean',
            'context_encoding.bottleneck.1.running_var': 'context_encoding.bottleneck.1.running_var',
            
            # Edge Detection
            'edge.conv1.0.weight': 'edge.0.weight',
            'edge.conv1.0.bias': 'edge.0.bias',
            'edge.conv1.1.weight': 'edge.1.weight',
            'edge.conv1.1.bias': 'edge.1.bias',
            'edge.conv1.1.running_mean': 'edge.1.running_mean',
            'edge.conv1.1.running_var': 'edge.1.running_var',
            
            # Decoder
            'decoder.conv1.0.weight': 'decoder.0.weight',
            'decoder.conv1.0.bias': 'decoder.0.bias',
            'decoder.conv1.1.weight': 'decoder.1.weight',
            'decoder.conv1.1.bias': 'decoder.1.bias',
            'decoder.conv1.1.running_mean': 'decoder.1.running_mean',
            'decoder.conv1.1.running_var': 'decoder.1.running_var',
            
            # Fusion
            'fushion.0.weight': 'fushion.0.weight',
            'fushion.0.bias': 'fushion.0.bias',
            'fushion.1.weight': 'fushion.1.weight',
            'fushion.1.bias': 'fushion.1.bias',
            'fushion.1.running_mean': 'fushion.1.running_mean',
            'fushion.1.running_var': 'fushion.1.running_var',
        }
        
        # μ§μ ‘ λ§¤ν•‘ μ μ©
        for checkpoint_key, model_key in direct_mappings.items():
            if checkpoint_key in checkpoint_keys and model_key in model_keys:
                mapping[checkpoint_key] = model_key
        
        # ν¨ν„΄ λ§¤ν•‘ (λ” μ μ—°ν• λ§¤ν•‘)
        for checkpoint_key in checkpoint_keys:
            if checkpoint_key in mapping:
                continue
                
            # ν¨ν„΄ κΈ°λ° λ§¤ν•‘
            for model_key in model_keys:
                if self._keys_match_pattern(checkpoint_key, model_key):
                    mapping[checkpoint_key] = model_key
                    break
        
        return mapping
    
    def _keys_match_pattern(self, checkpoint_key: str, model_key: str) -> bool:
        """ν‚¤ ν¨ν„΄ λ§¤μΉ­"""
        # κ°„λ‹¨ν• ν¨ν„΄ λ§¤μΉ­ κ·μΉ™λ“¤
        patterns = [
            # conv -> conv
            (r'conv(\d+)\.weight', r'conv\1\.weight'),
            (r'conv(\d+)\.bias', r'conv\1\.bias'),
            
            # bn -> bn
            (r'bn(\d+)\.weight', r'bn\1\.weight'),
            (r'bn(\d+)\.bias', r'bn\1\.bias'),
            (r'bn(\d+)\.running_mean', r'bn\1\.running_mean'),
            (r'bn(\d+)\.running_var', r'bn\1\.running_var'),
            
            # layer -> layer
            (r'layer(\d+)\.(\d+)\.conv(\d+)\.weight', r'layer\1\.\2\.conv\3\.weight'),
            (r'layer(\d+)\.(\d+)\.conv(\d+)\.bias', r'layer\1\.\2\.conv\3\.bias'),
            (r'layer(\d+)\.(\d+)\.bn(\d+)\.weight', r'layer\1\.\2\.bn\3\.weight'),
            (r'layer(\d+)\.(\d+)\.bn(\d+)\.bias', r'layer\1\.\2\.bn\3\.bias'),
        ]
        
        import re
        for pattern, replacement in patterns:
            if re.match(pattern, checkpoint_key) and re.match(replacement, model_key):
                return True
        
        return False
    
    def get_checkpoint_status(self) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ λ΅λ”© μƒνƒ λ°ν™"""
        return {
            'checkpoint_loaded': self.checkpoint_loaded,
            'checkpoint_path': self.checkpoint_path,
            'model_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad)
        }

# ν…μ¤νΈ ν•¨μ
def test_schp_model():
    """SCHP λ¨λΈ ν…μ¤νΈ"""
    model = SCHPModel(num_classes=20)
    print(f"β… SCHP λ¨λΈ μƒμ„± μ™„λ£")
    
    # μ²΄ν¬ν¬μΈνΈ λ΅λ”© ν…μ¤νΈ
    checkpoint_path = "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth"
    if os.path.exists(checkpoint_path):
        print(f"π” μ²΄ν¬ν¬μΈνΈ λ΅λ”© ν…μ¤νΈ: {checkpoint_path}")
        success = model.load_checkpoint(checkpoint_path)
        if success:
            print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ„±κ³µ")
            status = model.get_checkpoint_status()
            print(f"π“ μ²΄ν¬ν¬μΈνΈ μƒνƒ: {status}")
        else:
            print(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨")
    else:
        print(f"β οΈ μ²΄ν¬ν¬μΈνΈ νμΌμ΄ μ΅΄μ¬ν•μ§€ μ•μ: {checkpoint_path}")
    
    # ν…μ¤νΈ μ…λ ¥
    x = torch.randn(1, 3, 512, 512)
    print(f"π“ ν…μ¤νΈ μ…λ ¥ shape: {x.shape}")
    
    # Forward pass
    with torch.no_grad():
        output = model(x)
    
    print(f"β… Forward pass μ„±κ³µ")
    print(f"π“ μ¶λ ¥ keys: {list(output.keys())}")
    for key, value in output.items():
        print(f"  {key}: {value.shape}")
    
    return model

if __name__ == "__main__":
    test_schp_model() 