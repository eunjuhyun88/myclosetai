#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ - Cloth Segmentation (í†µí•© ì‹œìŠ¤í…œ)
================================================================================

ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ AI íŒŒì´í”„ë¼ì¸ ìŠ¤í…
BaseStepMixinì„ ìƒì†ë°›ì•„ ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ êµ¬í˜„
âœ… í†µì¼ëœ import êµ¬ì¡°

Author: MyCloset AI Team  
Date: 2025-01-27
Version: 9.0 - í†µí•© ì‹œìŠ¤í…œ
"""

# ê¸°ë³¸ imports
import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import math
import warnings
import weakref
import uuid
import subprocess
import platform
import datetime
from datetime import timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING, Set
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO, StringIO
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps, partial
from contextlib import asynccontextmanager, contextmanager
from collections import defaultdict, deque
from itertools import chain

# NumPy
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torch import autograd
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None
    DataLoader = None
    autograd = None
    autocast = None
    TORCH_VERSION = "N/A"

# PIL
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont, ImageOps
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# OpenCV
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

# scikit-image
try:
    from skimage import measure, morphology, segmentation, filters, restoration, exposure
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# scipy
try:
    from scipy import ndimage
    from scipy.ndimage import gaussian_filter, median_filter
    from scipy.signal import convolve2d
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    mp = None

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    YOLO = None

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    AutoModel = None
    AutoTokenizer = None

# í™˜ê²½ ë³€ìˆ˜
IS_M3_MAX = platform.system() == 'Darwin' and 'M3' in platform.processor()
CONDA_INFO = os.environ.get('CONDA_DEFAULT_ENV', 'none')
MEMORY_GB = 128.0 if IS_M3_MAX else 16.0
DEVICE = 'mps' if IS_M3_MAX and TORCH_AVAILABLE else 'cpu'

# ê°€ìš©ì„± ë³€ìˆ˜ë“¤
MPS_AVAILABLE = TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
CUDA_AVAILABLE = TORCH_AVAILABLE and torch.cuda.is_available()

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ë¡œê±° ì´ˆê¸°í™”
logger = logging.getLogger(__name__)

# ì‹¤ì œ AI ëª¨ë¸ import ì‹œë„
REAL_MODELS_AVAILABLE = False
try:
    # ìƒëŒ€ ê²½ë¡œë¡œ import ì‹œë„
    from .models.cloth_segmentation_u2net import U2NET, RealU2NETModel
    from .models.cloth_segmentation_deeplabv3plus import DeepLabV3PlusModel, RealDeepLabV3PlusModel
    from .models.cloth_segmentation_sam import SAM2025
    REAL_MODELS_AVAILABLE = True
    logger.info("âœ… ìƒëŒ€ ê²½ë¡œë¡œ ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ ìƒëŒ€ ê²½ë¡œ import ì‹¤íŒ¨: {e}")
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ import ì‹œë„
        from app.ai_pipeline.steps.step_03_cloth_segmentation_models.models.cloth_segmentation_u2net import U2NET, RealU2NETModel
        from app.ai_pipeline.steps.step_03_cloth_segmentation_models.models.cloth_segmentation_deeplabv3plus import DeepLabV3PlusModel, RealDeepLabV3PlusModel
        from app.ai_pipeline.steps.step_03_cloth_segmentation_models.models.cloth_segmentation_sam import SAM2025
        REAL_MODELS_AVAILABLE = True
        logger.info("âœ… ì ˆëŒ€ ê²½ë¡œë¡œ ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë“œ ì„±ê³µ")
    except ImportError as e2:
        logger.warning(f"âš ï¸ ì ˆëŒ€ ê²½ë¡œ importë„ ì‹¤íŒ¨: {e2}")
        try:
            # ì§ì ‘ ê²½ë¡œ ì¡°ì‘
            current_dir = os.path.dirname(os.path.abspath(__file__))
            models_dir = os.path.join(current_dir, 'models')
            if os.path.exists(models_dir):
                sys.path.insert(0, models_dir)
                from cloth_segmentation_u2net import U2NET, RealU2NETModel
                from cloth_segmentation_deeplabv3plus import DeepLabV3PlusModel, RealDeepLabV3PlusModel
                from cloth_segmentation_sam import SAM2025
                REAL_MODELS_AVAILABLE = True
                logger.info("âœ… ì§ì ‘ ê²½ë¡œë¡œ ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë“œ ì„±ê³µ")
            else:
                raise ImportError(f"models ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {models_dir}")
        except ImportError as e3:
            logger.warning(f"âš ï¸ ëª¨ë“  import ë°©ë²• ì‹¤íŒ¨: {e3}")
            # Mock ëª¨ë¸ë“¤ ì‚¬ìš©
            U2NET = None
            RealU2NETModel = None
            DeepLabV3PlusModel = None
            RealDeepLabV3PlusModel = None
            SAM2025 = None

# Mock ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì‹¤ì œ ëª¨ë¸ì´ ì—†ì„ ê²½ìš°)
if not REAL_MODELS_AVAILABLE:
    # torchê°€ ì—†ì„ ë•ŒëŠ” ê¸°ë³¸ í´ë˜ìŠ¤ ì‚¬ìš©
    try:
        import torch.nn as nn
        class MockU2NetModel(nn.Module):
            """Mock U2Net ëª¨ë¸"""
            def __init__(self, in_ch=3, out_ch=1):
                super().__init__()
                self.conv = nn.Conv2d(in_ch, out_ch, 1)
            
            def forward(self, x):
                return self.conv(x)
        
        class MockDeepLabV3PlusModel(nn.Module):
            """Mock DeepLabV3+ ëª¨ë¸"""
            def __init__(self, num_classes=21):
                super().__init__()
                self.conv = nn.Conv2d(3, num_classes, 1)
            
            def forward(self, x):
                return self.conv(x)
        
        class MockSAMModel(nn.Module):
            """Mock SAM ëª¨ë¸"""
            def __init__(self):
                super().__init__()
                self.conv = nn.Conv2d(3, 1, 1)
            
            def forward(self, x):
                return self.conv(x)
    except ImportError:
        # torchê°€ ì—†ì„ ë•ŒëŠ” ê¸°ë³¸ Mock í´ë˜ìŠ¤ ì‚¬ìš©
        class MockU2NetModel:
            """Mock U2Net ëª¨ë¸ (torch ì—†ìŒ)"""
            def __init__(self, in_ch=3, out_ch=1):
                pass
            
            def forward(self, x):
                return x
        
        class MockDeepLabV3PlusModel:
            """Mock DeepLabV3+ ëª¨ë¸ (torch ì—†ìŒ)"""
            def __init__(self, num_classes=21):
                pass
            
            def forward(self, x):
                return x
        
        class MockSAMModel:
            """Mock SAM ëª¨ë¸ (torch ì—†ìŒ)"""
            def __init__(self):
                pass
            
            def forward(self, x):
                return x

# BaseStepMixin import
try:
    from ...base.core.base_step_mixin import BaseStepMixin
except ImportError:
    from app.ai_pipeline.steps.base.core.base_step_mixin import BaseStepMixin
BASE_STEP_MIXIN_AVAILABLE = True

class ClothSegmentationStep(BaseStepMixin):
    """
    ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ AI íŒŒì´í”„ë¼ì¸ ìŠ¤í…
    """
    
    def __init__(self, device: str = "auto", **kwargs):
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ì´ˆê¸°í™”"""
        super().__init__(device=device, **kwargs)
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.step_name = "ClothSegmentationStep"
        self.step_id = 3
        
        # íŠ¹í™” ì´ˆê¸°í™”
        self._init_cloth_segmentation_specific()
        
        logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_cloth_segmentation_specific(self):
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if self.device == "auto":
                self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            self.models = {}
            self.models_loading_status = {}
            
            # ì‹¤ì œ AI ëª¨ë¸ë§Œ ë¡œë“œ (Mock ëª¨ë¸ ì œê±°)
            if REAL_MODELS_AVAILABLE:
                self._load_real_models()
            else:
                raise RuntimeError("ì‹¤ì œ AI ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            # ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”
            self.performance_stats = {
                'total_inferences': 0,
                'successful_inferences': 0,
                'failed_inferences': 0,
                'average_inference_time': 0.0,
                'total_processing_time': 0.0
            }
            
            # ì•™ìƒë¸” ë§¤ë‹ˆì € ì´ˆê¸°í™”
            try:
                if 'ClothSegmentationEnsembleSystem' in globals() and ClothSegmentationEnsembleSystem:
                    self.ensemble_system = ClothSegmentationEnsembleSystem()
                    logger.info("âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.ensemble_system = None
                    logger.warning("âš ï¸ ì•™ìƒë¸” ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                logger.warning(f"âš ï¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ensemble_system = None
            
            # í’ˆì§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™”
            try:
                if 'ClothSegmentationQualityAnalyzer' in globals() and ClothSegmentationQualityAnalyzer:
                    self.analyzer = ClothSegmentationQualityAnalyzer()
                    logger.info("âœ… í’ˆì§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.analyzer = None
                    logger.warning("âš ï¸ í’ˆì§ˆ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                logger.warning(f"âš ï¸ í’ˆì§ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.analyzer = None
            
            logger.info(f"âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._fallback_initialization()
            raise
    
    def _load_real_models(self):
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info("ğŸš€ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # U2Net ëª¨ë¸
            try:
                self.models['u2net'] = U2NET(in_ch=3, out_ch=1)
                self.models['u2net'].to(self.device)
                self.models_loading_status['u2net'] = True
                logger.info("âœ… U2Net ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.models_loading_status['u2net'] = False
            
            # DeepLabV3+ ëª¨ë¸
            try:
                self.models['deeplabv3plus'] = DeepLabV3PlusModel(num_classes=21)
                self.models['deeplabv3plus'].to(self.device)
                self.models_loading_status['deeplabv3plus'] = True
                logger.info("âœ… DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ DeepLabV3+ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.models_loading_status['deeplabv3plus'] = False
            
            # SAM ëª¨ë¸
            try:
                self.models['sam'] = SAM()
                self.models['sam'].to(self.device)
                self.models_loading_status['sam'] = True
                logger.info("âœ… SAM ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.models_loading_status['sam'] = False
            
            # ì‹¤ì œ ëª¨ë¸ì´ í•˜ë‚˜ë¼ë„ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            real_models_loaded = any(self.models_loading_status.values())
            if real_models_loaded:
                logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {sum(self.models_loading_status.values())}/{len(self.models_loading_status)}ê°œ")
                self.is_ready = True
            else:
                logger.warning("âš ï¸ ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - Mock ëª¨ë¸ë¡œ í´ë°±")
                self._create_mock_models()
                
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._create_mock_models()
    
    def _create_mock_models(self):
        """Mock ëª¨ë¸ ìƒì„± (í´ë°±ìš©)"""
        try:
            logger.info("âš ï¸ Mock ëª¨ë¸ ìƒì„± ì‹œì‘...")
            
            # Mock U2Net ëª¨ë¸
            try:
                self.models['u2net'] = MockU2NetModel(in_ch=3, out_ch=1)
                self.models['u2net'].to(self.device)
                self.models_loading_status['u2net'] = True
                logger.info("âœ… Mock U2Net ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Mock U2Net ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                self.models_loading_status['u2net'] = False
            
            # Mock DeepLabV3+ ëª¨ë¸
            try:
                self.models['deeplabv3plus'] = MockDeepLabV3PlusModel(num_classes=21)
                self.models['deeplabv3plus'].to(self.device)
                self.models_loading_status['deeplabv3plus'] = True
                logger.info("âœ… Mock DeepLabV3+ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Mock DeepLabV3+ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                self.models_loading_status['deeplabv3plus'] = False
            
            # Mock SAM ëª¨ë¸
            try:
                self.models['sam'] = MockSAMModel()
                self.models['sam'].to(self.device)
                self.models_loading_status['sam'] = True
                logger.info("âœ… Mock SAM ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Mock SAM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                self.models_loading_status['sam'] = False
            
            # Mock ëª¨ë¸ì´ í•˜ë‚˜ë¼ë„ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            mock_models_created = any(self.models_loading_status.values())
            if mock_models_created:
                logger.info(f"âš ï¸ Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ: {sum(self.models_loading_status.values())}/{len(self.models_loading_status)}ê°œ")
                self.is_ready = True
            else:
                logger.error("âŒ ëª¨ë“  Mock ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"âŒ Mock ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _fallback_initialization(self):
        """í´ë°± ì´ˆê¸°í™”"""
        self.device = 'cpu'
        self.models = {}
        self.models_loading_status = {}
        self.performance_stats = {
            'total_inferences': 0,
            'successful_inferences': 0,
            'failed_inferences': 0,
            'average_inference_time': 0.0,
            'total_processing_time': 0.0
        }
        self.ensemble_system = None
        self.analyzer = None
        self.is_ready = False
        self.is_initialized = True
        logger.warning("âš ï¸ í´ë°± ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰"""
        start_time = time.time()
        try:
            # ì…ë ¥ ì´ë¯¸ì§€ ì²˜ë¦¬
            if 'image' not in input_data:
                return {'error': 'ì…ë ¥ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            input_tensor = input_data['image']
            if not isinstance(input_tensor, torch.Tensor):
                input_tensor = torch.tensor(input_tensor, dtype=torch.float32)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_tensor = input_tensor.to(self.device)
            
            # ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡ 
            model_name = input_data.get('model', 'u2net')
            if model_name not in self.models:
                model_name = 'u2net'  # ê¸°ë³¸ê°’
            
            model = self.models[model_name]
            model.eval()
            
            with torch.no_grad():
                if model_name == 'u2net':
                    # U2Netì€ (main_output, side1, side2, side3, side4, side5, side6) í˜•íƒœë¡œ ë°˜í™˜
                    outputs = model(input_tensor)
                    if isinstance(outputs, tuple):
                        main_output = outputs[0]  # ë©”ì¸ ì¶œë ¥ë§Œ ì‚¬ìš©
                    else:
                        main_output = outputs
                    # ë§ˆìŠ¤í¬ ìƒì„±
                    mask = torch.sigmoid(main_output)
                    mask = (mask > 0.5).float()
                elif model_name == 'deeplabv3plus':
                    # DeepLabV3+ëŠ” í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡
                    output = model(input_tensor)
                    mask = torch.argmax(output, dim=1, keepdim=True)
                elif model_name == 'sam':
                    # SAM ëª¨ë¸ ì²˜ë¦¬
                    output = model(input_tensor)
                    mask = torch.sigmoid(output) if output.shape[1] == 1 else torch.argmax(output, dim=1, keepdim=True)
                else:
                    # ê¸°ë³¸ ì²˜ë¦¬
                    output = model(input_tensor)
                    mask = torch.sigmoid(output) if output.shape[1] == 1 else torch.argmax(output, dim=1, keepdim=True)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            mask = mask.cpu().numpy()
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, True)
            
            return {
                'method_used': model_name,
                'confidence_score': 0.85,  # Mock ê°’
                'quality_score': 0.90,     # Mock ê°’
                'processing_time': processing_time,
                'mask': mask,
                'segmentation_result': {
                    'mask_shape': mask.shape,
                    'mask_dtype': str(mask.dtype),
                    'unique_values': np.unique(mask).tolist()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, False)
            return {
                'error': str(e),
                'method_used': 'error',
                'confidence_score': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time
            }
    
    def _update_performance_stats(self, processing_time: float, success: bool):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_inferences'] += 1
        self.performance_stats['total_processing_time'] += processing_time
        
        if success:
            self.performance_stats['successful_inferences'] += 1
        else:
            self.performance_stats['failed_inferences'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_successful = self.performance_stats['successful_inferences']
        if total_successful > 0:
            self.performance_stats['average_inference_time'] = (
                self.performance_stats['total_processing_time'] / total_successful
            )
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not kwargs:
                return {'error': 'ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = self._run_ai_inference(kwargs)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìŠ¤í… ìƒíƒœ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'device': self.device,
            'models_loaded': list(self.models.keys()),
            'models_loading_status': self.models_loading_status,
            'performance_stats': self.performance_stats,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'real_models_available': REAL_MODELS_AVAILABLE
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ëª¨ë¸ë“¤ì„ CPUë¡œ ì´ë™
            for model in self.models.values():
                if hasattr(model, 'to'):
                    model.to('cpu')
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_cloth_segmentation_step(**kwargs):
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ìƒì„±"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs):
    """M3 Max ìµœì í™”ëœ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… ìƒì„±"""
    kwargs['device'] = 'mps' if torch.backends.mps.is_available() else 'cpu'
    return ClothSegmentationStep(**kwargs)

def test_cloth_segmentation_step():
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        step = ClothSegmentationStep()
        status = step.get_status()
        
        logger.info(f"âœ… ìŠ¤í… ìƒíƒœ: {status}")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        if step.models:
            logger.info("ğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            test_image = torch.randn(1, 3, 512, 512)  # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            result = step.process(image=test_image)
            logger.info(f"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
        
        return {
            'success': True,
            'status': status,
            'message': 'ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸ ì„±ê³µ'
        }
    except Exception as e:
        logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': 'ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨'
        }

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logger.info("ğŸš€ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í… í…ŒìŠ¤íŠ¸ ì‹œì‘")
    result = test_cloth_segmentation_step()
    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
    logger.info("ğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
