#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 03: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Model Loader í†µí•© ë²„ì „)
=====================================================================

âœ… Model Loader v6.0 ì™„ì „ í†µí•© - ì¤‘ì•™ ì§‘ì¤‘ì‹ ëª¨ë¸ ê´€ë¦¬
âœ… Model Architectures ì™„ì „ í™œìš© - í‘œì¤€í™”ëœ ëª¨ë¸ êµ¬ì¡°
âœ… StepModelInterface ì™„ì „ í˜¸í™˜ - ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì§€ì› - SAM, U2Net, DeepLabV3+
âœ… ë©”ëª¨ë¦¬ ìµœì í™” - M3 Max í™˜ê²½ ìµœì í™”
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” - í†µí•©ëœ ì˜ˆì™¸ ì²˜ë¦¬

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 34.0 (Model Loader Integration)
"""

# ğŸ”¥ ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš©
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, gc, time, logging, threading, math, hashlib, json, base64, warnings, np,
    Path, Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING,
    dataclass, field, Enum, BytesIO, ThreadPoolExecutor,
    
    # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
    error_tracker, track_exception, get_error_summary, create_exception_response, convert_to_mycloset_exception,
    ErrorCodes, EXCEPTIONS_AVAILABLE,
    
    # Mock Data Diagnostic
    detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    cv2, PIL_AVAILABLE, CV2_AVAILABLE, NUMPY_AVAILABLE, Image, ImageEnhance
)

# PIL Image import ì¶”ê°€
if PIL_AVAILABLE:
    from PIL import Image as PILImage

# ì¶”ê°€ imports
import weakref
from abc import ABC, abstractmethod

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ìµœìƒë‹¨ì— ì¶”ê°€
logger = logging.getLogger(__name__)

# ğŸ”¥ PyTorch ë¡œë”© ìµœì í™”
try:
    from fix_pytorch_loading import apply_pytorch_patch
    apply_pytorch_patch()
except ImportError:
    logger.warning("âš ï¸ fix_pytorch_loading ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ PyTorch ë¡œë”© ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch ë¡œë”© íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# ğŸ”¥ PyTorch í†µí•© import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
    logger.info(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    if MPS_AVAILABLE:
        logger.info("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.error("âŒ PyTorch í•„ìˆ˜ - ì„¤ì¹˜ í•„ìš”")
    if EXCEPTIONS_AVAILABLE:
        error = ModelLoadingError("PyTorch í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨", ErrorCodes.MODEL_LOADING_FAILED)
        track_exception(error, {'library': 'torch'}, 3)
        raise error
    else:
        raise

# ğŸ”¥ Model Loader ë° Architectures import
try:
    from ..models.model_loader import (
        ModelLoader, StepModelInterface, StepModelFactory,
        get_global_model_loader, create_step_interface, initialize_all_steps
    )
    from ..models.model_architectures import (
        SAMModel, U2NetModel, DeepLabV3PlusModel,
        ModelArchitectureFactory, CompleteModelWrapper
    )
    MODEL_LOADER_AVAILABLE = True
    logger.info("âœ… Model Loader ë° Architectures ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    logger.error(f"âŒ Model Loader import ì‹¤íŒ¨: {e}")
    MODEL_LOADER_AVAILABLE = False

# ğŸ”¥ ë©”ì¸ BaseStepMixin import
try:
    from app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger.info("âœ… ë©”ì¸ BaseStepMixin import ì„±ê³µ")
except ImportError:
    try:
        from ...base.base_step_mixin import BaseStepMixin
        BASE_STEP_MIXIN_AVAILABLE = True
        logger.info("âœ… ìƒëŒ€ ê²½ë¡œë¡œ BaseStepMixin import ì„±ê³µ")
    except ImportError:
        BASE_STEP_MIXIN_AVAILABLE = False
        logger.error("âŒ BaseStepMixin import ì‹¤íŒ¨ - ë©”ì¸ íŒŒì¼ ì‚¬ìš© í•„ìš”")
        raise ImportError("BaseStepMixinì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”ì¸ BaseStepMixinì„ ì‚¬ìš©í•˜ì„¸ìš”.")

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    return BaseStepMixin
            def __init__(self, **kwargs):
                self.logger = logging.getLogger(self.__class__.__name__)
                self.initialized = False
                self.model_loader = None
                self.memory_manager = None
                self.data_converter = None
                self.di_container = None
                
                # Stepë³„ íŠ¹í™” ì†ì„±ë“¤
                self.step_type = "cloth_segmentation"
                self.step_name = "Step 03: Cloth Segmentation"
                self.step_version = "34.0"
                
                # ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤
                self.ai_models = {}
                self.segmentation_models = {}
                self.models_loading_status = {}
                self.model_paths = {}
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                self.inference_count = 0
                self.total_inference_time = 0.0
                self.last_inference_time = 0.0
                
                # ì„¤ì •
                self.config = kwargs.get('config', {})
                
            def process(self, **kwargs) -> Dict[str, Any]:
                """ë©”ì¸ ì²˜ë¦¬ ë¡œì§"""
                try:
                    # ì…ë ¥ ê²€ì¦
                    if not self._validate_input(kwargs):
                        return self._create_error_result("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
                    
                    # ëª¨ë¸ ë¡œë”© í™•ì¸
                    if not self._ensure_models_loaded():
                        return self._create_error_result("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
                    # AI ì¶”ë¡  ì‹¤í–‰
                    result = self._run_ai_inference(kwargs)
                    
                    # ê²°ê³¼ ê²€ì¦
                    if not self._validate_output(result):
                        return self._create_error_result("ì¶œë ¥ ê²€ì¦ ì‹¤íŒ¨")
                    
                    return result
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return self._create_error_result(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            def initialize(self) -> bool:
                """ì´ˆê¸°í™”"""
                try:
                    if self.initialized:
                        return True
                    
                    # Model Loader ì´ˆê¸°í™”
                    if MODEL_LOADER_AVAILABLE:
                        self.model_loader = get_global_model_loader()
                        if not self.model_loader:
                            self.logger.warning("âš ï¸ ì „ì—­ Model Loader ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
                            self.model_loader = ModelLoader()
                    
                    # ëª¨ë¸ ë¡œë”©
                    if not self._load_models():
                        self.logger.error("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                        return False
                    
                    self.initialized = True
                    self.logger.info("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
                    return True
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return False
            
            def cleanup(self):
                """ì •ë¦¬"""
                try:
                    # ëª¨ë¸ ì–¸ë¡œë“œ
                    if self.model_loader:
                        self.model_loader.cleanup()
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    if TORCH_AVAILABLE:
                        try:
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            elif MPS_AVAILABLE:
                                torch.mps.empty_cache()
                        except:
                            pass
                    
                    self.initialized = False
                    self.logger.info("âœ… ì •ë¦¬ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            def get_status(self) -> Dict[str, Any]:
                """ìƒíƒœ ì¡°íšŒ"""
                return {
                    'initialized': self.initialized,
                    'step_type': self.step_type,
                    'step_name': self.step_name,
                    'step_version': self.step_version,
                    'models_loaded': len(self.ai_models),
                    'inference_count': self.inference_count,
                    'total_inference_time': self.total_inference_time,
                    'last_inference_time': self.last_inference_time
                }
            
            def _validate_input(self, kwargs) -> bool:
                """ì…ë ¥ ê²€ì¦"""
                required_keys = ['image']
                return all(key in kwargs for key in required_keys)
            
            def _ensure_models_loaded(self) -> bool:
                """ëª¨ë¸ ë¡œë”© í™•ì¸"""
                return len(self.ai_models) > 0
            
            def _run_ai_inference(self, kwargs) -> Dict[str, Any]:
                """AI ì¶”ë¡  ì‹¤í–‰"""
                # ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
                return {'status': 'success', 'message': 'ê¸°ë³¸ ì¶”ë¡  ì™„ë£Œ'}
            
            def _validate_output(self, result) -> bool:
                """ì¶œë ¥ ê²€ì¦"""
                return isinstance(result, dict) and 'status' in result
            
            def _create_error_result(self, message: str) -> Dict[str, Any]:
                """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
                return {
                    'status': 'error',
                    'message': message,
                    'step_type': self.step_type,
                    'timestamp': time.time()
                }
            
            def _load_models(self) -> bool:
                """ëª¨ë¸ ë¡œë”©"""
                # ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
                return True
        
        return BaseStepMixin

# ğŸ”¥ ì¤‘ì•™ í—ˆë¸Œ ì»¨í…Œì´ë„ˆ ê°€ì ¸ì˜¤ê¸°
def _get_central_hub_container():
    """ì¤‘ì•™ í—ˆë¸Œ ì»¨í…Œì´ë„ˆ ê°€ì ¸ì˜¤ê¸°"""
    try:
        from app.api.central_hub import get_central_hub_container
        return get_central_hub_container()
    except ImportError:
        logger.warning("âš ï¸ ì¤‘ì•™ í—ˆë¸Œ ì»¨í…Œì´ë„ˆ ì—†ìŒ")
        return None

# ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ì•ˆì „ í•¨ìˆ˜
def _inject_dependencies_safe(step_instance):
    """ì˜ì¡´ì„± ì•ˆì „ ì£¼ì…"""
    try:
        container = _get_central_hub_container()
        if container:
            # Model Loader ì£¼ì…
            if hasattr(step_instance, 'set_model_loader'):
                model_loader = container.get_service('model_loader')
                if model_loader:
                    step_instance.set_model_loader(model_loader)
            
            # Memory Manager ì£¼ì…
            if hasattr(step_instance, 'set_memory_manager'):
                memory_manager = container.get_service('memory_manager')
                if memory_manager:
                    step_instance.set_memory_manager(memory_manager)
            
            # Data Converter ì£¼ì…
            if hasattr(step_instance, 'set_data_converter'):
                data_converter = container.get_service('data_converter')
                if data_converter:
                    step_instance.set_data_converter(data_converter)
            
            # DI Container ì£¼ì…
            if hasattr(step_instance, 'set_di_container'):
                step_instance.set_di_container(container)
                
    except Exception as e:
        logger.warning(f"âš ï¸ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

# ğŸ”¥ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì•ˆì „ í•¨ìˆ˜
def _get_service_from_central_hub(service_key: str):
    """ì¤‘ì•™ í—ˆë¸Œì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get_service(service_key)
        return None
    except Exception as e:
        logger.warning(f"âš ï¸ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({service_key}): {e}")
        return None

# ğŸ”¥ M3 Max ê°ì§€
def detect_m3_max():
    """M3 Max í™˜ê²½ ê°ì§€"""
    try:
        import platform
        if platform.system() == 'Darwin':
            # macOSì—ì„œ M3 Max ê°ì§€
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True)
            if 'M3 Max' in result.stdout:
                return True
        return False
    except:
        return False

# ğŸ”¥ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²• ì—´ê±°í˜•
class SegmentationMethod(Enum):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ë²•"""
    U2NET_CLOTH = "u2net_cloth"         # U2Net ì˜ë¥˜ íŠ¹í™” (168.1MB) - ìš°ì„ ìˆœìœ„ 1 (M3 Max ì•ˆì „)
    SAM_HUGE = "sam_huge"               # SAM ViT-Huge (2445.7MB) - ìš°ì„ ìˆœìœ„ 2 (ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ)
    DEEPLABV3_PLUS = "deeplabv3_plus"   # DeepLabV3+ (233.3MB) - ìš°ì„ ìˆœìœ„ 3 (ë‚˜ì¤‘ì—)
    MASK_RCNN = "mask_rcnn"             # Mask R-CNN (í´ë°±)
    HYBRID_AI = "hybrid_ai"             # í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”

# ğŸ”¥ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì—´ê±°í˜•
class ClothCategory(Enum):
    """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ (ë‹¤ì¤‘ í´ë˜ìŠ¤)"""
    BACKGROUND = 0
    SHIRT = 1           # ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤
    T_SHIRT = 2         # í‹°ì…”ì¸ 
    SWEATER = 3         # ìŠ¤ì›¨í„°/ë‹ˆíŠ¸
    HOODIE = 4          # í›„ë“œí‹°
    JACKET = 5          # ì¬í‚·/ì•„ìš°í„°
    COAT = 6            # ì½”íŠ¸
    DRESS = 7           # ì›í”¼ìŠ¤
    SKIRT = 8           # ìŠ¤ì»¤íŠ¸
    PANTS = 9           # ë°”ì§€
    JEANS = 10          # ì²­ë°”ì§€
    SHORTS = 11         # ë°˜ë°”ì§€
    SHOES = 12          # ì‹ ë°œ
    BOOTS = 13          # ë¶€ì¸ 
    SNEAKERS = 14       # ìš´ë™í™”
    BAG = 15            # ê°€ë°©
    HAT = 16            # ëª¨ì
    GLASSES = 17        # ì•ˆê²½
    SCARF = 18          # ìŠ¤ì¹´í”„
    BELT = 19           # ë²¨íŠ¸
    ACCESSORY = 20      # ì•¡ì„¸ì„œë¦¬

# ğŸ”¥ í’ˆì§ˆ ë ˆë²¨ ì—´ê±°í˜•
class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"           # ë¹ ë¥¸ ì²˜ë¦¬
    BALANCED = "balanced"   # ê· í˜•
    HIGH = "high"          # ê³ í’ˆì§ˆ
    ULTRA = "ultra"        # ìµœê³ í’ˆì§ˆ

# ğŸ”¥ ì„¤ì • ë°ì´í„°í´ë˜ìŠ¤
@dataclass
class ClothSegmentationConfig:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    method: SegmentationMethod = SegmentationMethod.U2NET_CLOTH  # M3 Max ì•ˆì „ ëª¨ë“œ
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (512, 512)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    
    # ì˜ë¥˜ ë¶„ë¥˜ ì„¤ì •
    enable_clothing_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # í›„ì²˜ë¦¬ ì„¤ì •
    enable_crf_postprocessing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    confidence_threshold: float = 0.5
    enable_visualization: bool = True
    
    # ìë™ ì „ì²˜ë¦¬ ì„¤ì •
    auto_preprocessing: bool = True
    
    # ìë™ í›„ì²˜ë¦¬ ì„¤ì •
    auto_postprocessing: bool = True
    
    # ë°ì´í„° ê²€ì¦ ì„¤ì •
    strict_data_validation: bool = True

# ğŸ”¥ ë©”ì¸ Step í´ë˜ìŠ¤
class ClothSegmentationStep(get_base_step_mixin_class()):
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step - Model Loader í†µí•© ë²„ì „"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # ì„¤ì • ì´ˆê¸°í™”
        self.config = kwargs.get('config', ClothSegmentationConfig())
        
        # Model Loader í†µí•©
        self.step_interface: Optional[StepModelInterface] = None
        self.model_factory: Optional[StepModelFactory] = None
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.segmentation_stats = {
            'total_segmentations': 0,
            'successful_segmentations': 0,
            'failed_segmentations': 0,
            'average_confidence': 0.0,
            'average_processing_time': 0.0
        }
        
        # ì˜ì¡´ì„± ì£¼ì…
        _inject_dependencies_safe(self)
        
        self.logger.info(f"ğŸ”¥ {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ (v{self.step_version})")
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” - Model Loader í†µí•©"""
        try:
            if self.initialized:
                return True
            
            # Model Loader ì´ˆê¸°í™”
            if not self._initialize_model_loader():
                return False
            
            # Step Interface ìƒì„±
            if not self._create_step_interface():
                return False
            
            # ëª¨ë¸ ë¡œë”©
            if not self._load_models():
                return False
            
            self.initialized = True
            self.logger.info("âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _initialize_model_loader(self) -> bool:
        """Model Loader ì´ˆê¸°í™”"""
        try:
            if not MODEL_LOADER_AVAILABLE:
                self.logger.error("âŒ Model Loader ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ì „ì—­ Model Loader ê°€ì ¸ì˜¤ê¸°
            self.model_loader = get_global_model_loader()
            if not self.model_loader:
                self.logger.warning("âš ï¸ ì „ì—­ Model Loader ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
                self.model_loader = ModelLoader()
            
            # Step Factory ìƒì„±
            self.model_factory = StepModelFactory(self.model_loader)
            
            self.logger.info("âœ… Model Loader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Model Loader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_step_interface(self) -> bool:
        """Step Interface ìƒì„±"""
        try:
            if not self.model_factory:
                self.logger.error("âŒ Model Factory ì—†ìŒ")
                return False
            
            # Step Interface ìƒì„±
            self.step_interface = self.model_factory.create_step_interface('cloth_segmentation')
            if not self.step_interface:
                self.logger.error("âŒ Step Interface ìƒì„± ì‹¤íŒ¨")
                return False
            
            self.logger.info("âœ… Step Interface ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _load_models(self) -> bool:
        """ëª¨ë¸ ë¡œë”© - Model Loader í™œìš©"""
        try:
            if not self.step_interface:
                self.logger.error("âŒ Step Interface ì—†ìŒ")
                return False
            
            # ì£¼ìš” ëª¨ë¸ ë¡œë”©
            success = self.step_interface.load_primary_model()
            if not success:
                self.logger.error("âŒ ì£¼ìš” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            # ëª¨ë¸ ì •ë³´ ì €ì¥
            model = self.step_interface.get_model()
            if model:
                self.ai_models['primary'] = model
                self.segmentation_models['primary'] = model
                self.models_loading_status['primary'] = True
                self.logger.info("âœ… ì£¼ìš” ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë¡œì§ - Model Loader í†µí•©"""
        try:
            start_time = time.time()
            
            # ì…ë ¥ ê²€ì¦
            if not self._validate_input(kwargs):
                return self._create_error_result("ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
            
            # ëª¨ë¸ ë¡œë”© í™•ì¸
            if not self._ensure_models_loaded():
                return self._create_error_result("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ì…ë ¥ ì „ì²˜ë¦¬
            processed_input = self._preprocess_input(kwargs)
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = self._run_ai_inference(processed_input)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            result = self._postprocess_output(result)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_metrics(processing_time, result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _validate_input(self, kwargs) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        try:
            required_keys = ['image']
            if not all(key in kwargs for key in required_keys):
                self.logger.error("âŒ í•„ìˆ˜ ì…ë ¥ í‚¤ ëˆ„ë½")
                return False
            
            image = kwargs['image']
            if not isinstance(image, np.ndarray):
                self.logger.error("âŒ ì´ë¯¸ì§€ê°€ numpy ë°°ì—´ì´ ì•„ë‹˜")
                return False
            
            if len(image.shape) != 3:
                self.logger.error("âŒ ì´ë¯¸ì§€ ì°¨ì›ì´ 3ì´ ì•„ë‹˜")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _preprocess_input(self, kwargs) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            image = kwargs['image']
            
            # ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
            quality_scores = self._assess_image_quality(image)
            
            # í’ˆì§ˆ ë ˆë²¨ ê²°ì •
            quality_level = self._determine_quality_level(quality_scores)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if self.config.enable_lighting_normalization:
                image = self._normalize_lighting(image)
            
            if self.config.enable_color_correction:
                image = self._correct_colors(image)
            
            return {
                'image': image,
                'quality_scores': quality_scores,
                'quality_level': quality_level,
                'person_parsing': kwargs.get('person_parsing', {}),
                'pose_info': kwargs.get('pose_info', {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return kwargs
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰ - Model Loader í™œìš©"""
        try:
            if not self.step_interface:
                return self._create_error_result("Step Interface ì—†ìŒ")
            
            # Step Interfaceë¥¼ í†µí•œ ì¶”ë¡  ì‹¤í–‰
            result = self.step_interface.run_inference(
                processed_input['image'],
                **processed_input
            )
            
            if not result:
                return self._create_error_result("ì¶”ë¡  ì‹¤íŒ¨")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    def _postprocess_output(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            if result.get('status') == 'error':
                return result
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            if 'masks' in result:
                result['masks'] = self._postprocess_masks(result['masks'])
            
            # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ê°ì§€
            if self.config.enable_clothing_classification and 'masks' in result:
                cloth_categories = self._detect_cloth_categories(result['masks'])
                result['cloth_categories'] = cloth_categories
            
            # ì‹œê°í™” ìƒì„±
            if self.config.enable_visualization and 'masks' in result:
                visualizations = self._create_segmentation_visualizations(
                    result.get('original_image', np.zeros((512, 512, 3))),
                    result['masks']
                )
                result['visualizations'] = visualizations
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['step_type'] = self.step_type
            result['step_name'] = self.step_name
            result['step_version'] = self.step_version
            result['timestamp'] = time.time()
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶œë ¥ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return result
    
    def _assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
        try:
            scores = {}
            
            # ë°ê¸° í‰ê°€
            brightness = np.mean(image)
            scores['brightness'] = brightness / 255.0
            
            # ëŒ€ë¹„ í‰ê°€
            contrast = np.std(image)
            scores['contrast'] = min(contrast / 50.0, 1.0)
            
            # ì„ ëª…ë„ í‰ê°€
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            scores['sharpness'] = min(laplacian_var / 1000.0, 1.0)
            
            # ë…¸ì´ì¦ˆ í‰ê°€
            noise_level = 1.0 - scores['sharpness']
            scores['noise'] = noise_level
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            scores['overall'] = (scores['brightness'] + scores['contrast'] + scores['sharpness']) / 3.0
            
            return scores
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall': 0.5}
    
    def _normalize_lighting(self, image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì •ê·œí™”"""
        try:
            # CLAHE ì ìš©
            if len(image.shape) == 3:
                lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                lab[:, :, 0] = clahe.apply(lab[:, :, 0])
                return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            else:
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                return clahe.apply(image)
        except Exception as e:
            self.logger.error(f"âŒ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    def _correct_colors(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            # ìë™ í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤
            if len(image.shape) == 3:
                result = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                avg_a = np.average(result[:, :, 1])
                avg_b = np.average(result[:, :, 2])
                result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
                result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
                return cv2.cvtColor(result, cv2.COLOR_LAB2RGB)
            else:
                return image
        except Exception as e:
            self.logger.error(f"âŒ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _determine_quality_level(self, quality_scores: Dict[str, float]) -> QualityLevel:
        """í’ˆì§ˆ ë ˆë²¨ ê²°ì •"""
        try:
            overall_score = quality_scores.get('overall', 0.5)
            
            if overall_score >= 0.8:
                return QualityLevel.ULTRA
            elif overall_score >= 0.6:
                return QualityLevel.HIGH
            elif overall_score >= 0.4:
                return QualityLevel.BALANCED
            else:
                return QualityLevel.FAST
                
        except Exception as e:
            self.logger.error(f"âŒ í’ˆì§ˆ ë ˆë²¨ ê²°ì • ì‹¤íŒ¨: {e}")
            return QualityLevel.BALANCED
    
    def _postprocess_masks(self, masks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬"""
        try:
            processed_masks = {}
            
            for mask_name, mask in masks.items():
                if mask is None:
                    continue
                
                # ë…¸ì´ì¦ˆ ì œê±°
                if self.config.enable_hole_filling:
                    mask = self._fill_holes_and_remove_noise(mask)
                
                # ê²½ê³„ ì •ì œ
                if self.config.enable_edge_refinement:
                    mask = self._refine_edges(mask)
                
                processed_masks[mask_name] = mask
            
            return processed_masks
            
        except Exception as e:
            self.logger.error(f"âŒ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return masks
    
    def _fill_holes_and_remove_noise(self, mask: np.ndarray) -> np.ndarray:
        """êµ¬ë© ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
            kernel = np.ones((3, 3), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            # êµ¬ë© ì±„ìš°ê¸°
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ë© ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
            return mask
    
    def _refine_edges(self, mask: np.ndarray) -> np.ndarray:
        """ê²½ê³„ ì •ì œ"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ
            blurred = cv2.GaussianBlur(mask, (3, 3), 0)
            
            # ì„ê³„ê°’ ì ìš©
            _, refined = cv2.threshold(blurred, 0.5, 1.0, cv2.THRESH_BINARY)
            
            return refined
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½ê³„ ì •ì œ ì‹¤íŒ¨: {e}")
            return mask
    
    def _detect_cloth_categories(self, masks: Dict[str, np.ndarray]) -> List[str]:
        """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ê°ì§€"""
        try:
            categories = []
            
            for mask_name, mask in masks.items():
                if mask is None or np.sum(mask) == 0:
                    continue
                
                # ë§ˆìŠ¤í¬ ì´ë¦„ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
                if 'shirt' in mask_name.lower():
                    categories.append('shirt')
                elif 'pants' in mask_name.lower():
                    categories.append('pants')
                elif 'dress' in mask_name.lower():
                    categories.append('dress')
                elif 'jacket' in mask_name.lower():
                    categories.append('jacket')
                else:
                    categories.append('unknown')
            
            return list(set(categories))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_segmentation_visualizations(self, image: np.ndarray, masks: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œê°í™” ìƒì„±"""
        try:
            visualizations = {}
            
            # ì›ë³¸ ì´ë¯¸ì§€
            visualizations['original'] = image.tolist()
            
            # ê°œë³„ ë§ˆìŠ¤í¬ ì‹œê°í™”
            for mask_name, mask in masks.items():
                if mask is None:
                    continue
                
                # ë§ˆìŠ¤í¬ë¥¼ ì»¬ëŸ¬ë¡œ ë³€í™˜
                colored_mask = np.zeros_like(image)
                colored_mask[mask > 0.5] = [255, 0, 0]  # ë¹¨ê°„ìƒ‰
                
                # ì˜¤ë²„ë ˆì´ ìƒì„±
                overlay = cv2.addWeighted(image, 0.7, colored_mask, 0.3, 0)
                visualizations[f'{mask_name}_overlay'] = overlay.tolist()
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _update_metrics(self, processing_time: float, result: Dict[str, Any]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            self.inference_count += 1
            self.total_inference_time += processing_time
            self.last_inference_time = processing_time
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„ ì—…ë°ì´íŠ¸
            self.segmentation_stats['total_segmentations'] += 1
            
            if result.get('status') == 'success':
                self.segmentation_stats['successful_segmentations'] += 1
            else:
                self.segmentation_stats['failed_segmentations'] += 1
            
            # í‰ê·  ê³„ì‚°
            if self.inference_count > 0:
                self.segmentation_stats['average_processing_time'] = self.total_inference_time / self.inference_count
            
            # ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            confidence = result.get('confidence', 0.0)
            if confidence > 0:
                current_avg = self.segmentation_stats['average_confidence']
                total_successful = self.segmentation_stats['successful_segmentations']
                self.segmentation_stats['average_confidence'] = (
                    (current_avg * (total_successful - 1) + confidence) / total_successful
                )
                
        except Exception as e:
            self.logger.error(f"âŒ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_model_info(self, model_key: str = None) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            if not self.step_interface:
                return {'error': 'Step Interface ì—†ìŒ'}
            
            model = self.step_interface.get_model()
            if not model:
                return {'error': 'ëª¨ë¸ ì—†ìŒ'}
            
            return {
                'model_type': type(model).__name__,
                'step_type': self.step_type,
                'loaded': self.step_interface.loaded,
                'inference_count': self.inference_count,
                'total_inference_time': self.total_inference_time,
                'last_inference_time': self.last_inference_time
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_segmentation_stats(self) -> Dict[str, Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„ ì¡°íšŒ"""
        return self.segmentation_stats.copy()
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            if self.model_loader:
                self.model_loader.cleanup()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if TORCH_AVAILABLE:
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif MPS_AVAILABLE:
                        torch.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def reload_models(self):
        """ëª¨ë¸ ì¬ë¡œë”©"""
        try:
            # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
            self.ai_models.clear()
            self.segmentation_models.clear()
            self.models_loading_status.clear()
            
            # ëª¨ë¸ ì¬ë¡œë”©
            if self._load_models():
                self.logger.info("âœ… ëª¨ë¸ ì¬ë¡œë”© ì™„ë£Œ")
            else:
                self.logger.error("âŒ ëª¨ë¸ ì¬ë¡œë”© ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¬ë¡œë”© ì‹¤íŒ¨: {e}")

# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step ìƒì„±"""
    return ClothSegmentationStep(**kwargs)

def create_m3_max_segmentation_step(**kwargs) -> ClothSegmentationStep:
    """M3 Max ìµœì í™” ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ Step ìƒì„±"""
    config = kwargs.get('config', ClothSegmentationConfig())
    config.method = SegmentationMethod.U2NET_CLOTH  # M3 Max ì•ˆì „ ëª¨ë“œ
    config.quality_level = QualityLevel.BALANCED  # ê· í˜• ëª¨ë“œ
    
    kwargs['config'] = config
    return ClothSegmentationStep(**kwargs)

# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_cloth_segmentation_ai():
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # Step ìƒì„±
        step = create_cloth_segmentation_step()
        
        # ì´ˆê¸°í™”
        if not step.initialize():
            logger.error("âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = step.process(image=dummy_image)
        
        if result.get('status') == 'success':
            logger.info("âœ… ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
        else:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('message')}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ğŸ”¥ ì¤‘ì•™ í—ˆë¸Œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
def test_central_hub_compatibility():
    """ì¤‘ì•™ í—ˆë¸Œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª ì¤‘ì•™ í—ˆë¸Œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì¤‘ì•™ í—ˆë¸Œ ì»¨í…Œì´ë„ˆ í™•ì¸
        container = _get_central_hub_container()
        if not container:
            logger.warning("âš ï¸ ì¤‘ì•™ í—ˆë¸Œ ì»¨í…Œì´ë„ˆ ì—†ìŒ")
            return True  # ê²½ê³ ë§Œ í•˜ê³  ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        
        # ì„œë¹„ìŠ¤ í™•ì¸
        services = ['model_loader', 'memory_manager', 'data_converter']
        for service in services:
            service_instance = container.get_service(service)
            if service_instance:
                logger.info(f"âœ… {service} ì„œë¹„ìŠ¤ í™•ì¸ë¨")
            else:
                logger.warning(f"âš ï¸ {service} ì„œë¹„ìŠ¤ ì—†ìŒ")
        
        logger.info("âœ… ì¤‘ì•™ í—ˆë¸Œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì¤‘ì•™ í—ˆë¸Œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        gc.collect()
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif MPS_AVAILABLE:
                    torch.mps.empty_cache()
            except Exception as mem_error:
                logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mem_error}")
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def safe_torch_operation(operation_func, *args, **kwargs):
    """ì•ˆì „í•œ PyTorch ì—°ì‚°"""
    try:
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorch ì‚¬ìš© ë¶ˆê°€")
        
        return operation_func(*args, **kwargs)
        
    except Exception as e:
        logger.error(f"âŒ PyTorch ì—°ì‚° ì‹¤íŒ¨: {e}")
        if EXCEPTIONS_AVAILABLE:
            error = ModelLoadingError(f"PyTorch ì—°ì‚° ì‹¤íŒ¨: {e}", ErrorCodes.MODEL_LOADING_FAILED)
            track_exception(error, {'operation': operation_func.__name__}, 3)
        raise

# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™”
if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_cloth_segmentation_ai()
    test_central_hub_compatibility()
    
    logger.info("ğŸ‰ Step 03 Cloth Segmentation ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
