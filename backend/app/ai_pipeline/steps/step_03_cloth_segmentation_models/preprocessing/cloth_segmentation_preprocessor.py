"""
ğŸ”¥ Cloth Segmentation ì „ìš© ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
========================================

ì˜ë¥˜ ë¶„í• ì— ìµœì í™”ëœ ì „ì²˜ë¦¬ ê¸°ëŠ¥ë“¤:
1. ì˜ë¥˜ ì˜ì—­ ê°•í™” ë° ì „ì²˜ë¦¬
2. íŒ¨í„´ ë³µì¡ë„ ë¶„ì„
3. ì¬ì§ˆ íŠ¹ì„± ê°ì§€
4. íˆ¬ëª…ë„ ì²˜ë¦¬
5. ì˜ë¥˜-ë°°ê²½ ë¶„ë¦¬

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0 (ì™„ì „ êµ¬í˜„)
"""

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple, List
import logging
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

logger = logging.getLogger(__name__)

class ClothSegmentationPreprocessor:
    """ì˜ë¥˜ ë¶„í• ì— ìµœì í™”ëœ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, target_size: Tuple[int, int] = (512, 512)):
        self.target_size = target_size
        self.logger = logging.getLogger(f"{__name__}.ClothSegmentationPreprocessor")
        
        # ì˜ë¥˜ ë¶„í• ìš© ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
        self.cloth_params = {
            'pattern_enhancement': True,
            'material_detection': True,
            'transparency_handling': True,
            'background_separation': True,
            'texture_preservation': True
        }
        
        # ì˜ë¥˜ íƒ€ì…ë³„ ìƒ‰ìƒ ë²”ìœ„ (HSV)
        self.cloth_color_ranges = {
            'light': {'lower': np.array([0, 0, 200]), 'upper': np.array([180, 30, 255])},
            'dark': {'lower': np.array([0, 0, 0]), 'upper': np.array([180, 255, 50])},
            'colorful': {'lower': np.array([0, 100, 100]), 'upper': np.array([180, 255, 255])}
        }
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'images_processed': 0,
            'cloth_detected': 0,
            'patterns_enhanced': 0,
            'materials_analyzed': 0
        }
    
    def preprocess_image(self, image: np.ndarray, mode: str = 'advanced') -> Dict[str, Any]:
        """ì˜ë¥˜ ë¶„í• ì„ ìœ„í•œ ì™„ì „í•œ ì „ì²˜ë¦¬"""
        try:
            self.processing_stats['images_processed'] += 1
            self.logger.info(f"ğŸ”¥ ì˜ë¥˜ ë¶„í•  ì „ì²˜ë¦¬ ì‹œì‘ (ëª¨ë“œ: {mode})")
            
            # 1. ì´ë¯¸ì§€ ê²€ì¦
            validated_image = self._validate_image(image)
            
            # 2. ì˜ë¥˜ ì˜ì—­ ê°ì§€ ë° ê°•í™”
            enhanced_image, cloth_info = self._detect_and_enhance_clothing(validated_image)
            if cloth_info['cloth_detected']:
                self.processing_stats['cloth_detected'] += 1
            
            # 3. í•´ìƒë„ í‘œì¤€í™”
            resized_image = self._standardize_resolution(enhanced_image)
            
            # 4. ì˜ë¥˜ ë¶„í•  ìµœì í™”
            if mode == 'advanced':
                optimized_image = self._optimize_for_cloth_segmentation(resized_image)
                self.processing_stats['patterns_enhanced'] += 1
                self.processing_stats['materials_analyzed'] += 1
            else:
                optimized_image = resized_image
            
            # 5. ì •ê·œí™” ë° í…ì„œ ë³€í™˜
            normalized_tensor = self._normalize_and_convert(optimized_image)
            
            # 6. ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
            preprocessing_result = {
                'processed_image': optimized_image,
                'tensor': normalized_tensor,
                'cloth_info': cloth_info,
                'target_size': self.target_size,
                'mode': mode,
                'cloth_params': self.cloth_params,
                'success': True
            }
            
            self.logger.info("âœ… ì˜ë¥˜ ë¶„í•  ì „ì²˜ë¦¬ ì™„ë£Œ")
            return preprocessing_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ë¥˜ ë¶„í•  ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'processed_image': image,
                'tensor': torch.randn(1, 3, *self.target_size)
            }
    
    def _validate_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦ ë° ë³€í™˜"""
        try:
            # PIL Imageë¥¼ NumPyë¡œ ë³€í™˜
            if isinstance(image, Image.Image):
                image = np.array(image)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ RGBë¡œ ë³€í™˜
            if len(image.shape) == 2:
                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
            elif image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            
            # ë°ì´í„° íƒ€ì… ì •ê·œí™”
            if image.dtype != np.uint8:
                image = image.astype(np.uint8)
            
            return image
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return image
    
    def _detect_and_enhance_clothing(self, image: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """ì˜ë¥˜ ì˜ì—­ ê°ì§€ ë° ê°•í™”"""
        try:
            # HSV ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            
            # ì˜ë¥˜ ìƒ‰ìƒ ë§ˆìŠ¤í¬ ìƒì„±
            cloth_masks = {}
            for cloth_type, color_range in self.cloth_color_ranges.items():
                mask = cv2.inRange(hsv, color_range['lower'], color_range['upper'])
                cloth_masks[cloth_type] = mask
            
            # í†µí•© ì˜ë¥˜ ë§ˆìŠ¤í¬
            combined_mask = np.zeros_like(cloth_masks['light'])
            for mask in cloth_masks.values():
                combined_mask = cv2.bitwise_or(combined_mask, mask)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            kernel = np.ones((5, 5), np.uint8)
            cleaned_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel)
            
            # ì˜ë¥˜ ì˜ì—­ ê°•í™”
            enhanced = image.copy()
            cloth_regions = cleaned_mask > 0
            
            if np.any(cloth_regions):
                # ì˜ë¥˜ ì˜ì—­ì—ì„œ ëŒ€ë¹„ í–¥ìƒ
                enhanced[cloth_regions] = cv2.convertScaleAbs(
                    enhanced[cloth_regions], alpha=1.2, beta=10
                )
                
                # ì˜ë¥˜ ì˜ì—­ì—ì„œ ì„ ëª…ë„ í–¥ìƒ
                kernel_sharpen = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
                enhanced[cloth_regions] = cv2.filter2D(
                    enhanced[cloth_regions], -1, kernel_sharpen
                )
            
            # ì˜ë¥˜ ì •ë³´ ìˆ˜ì§‘
            cloth_info = {
                'cloth_detected': np.any(cloth_regions),
                'cloth_coverage': np.sum(cloth_regions) / cloth_regions.size,
                'cloth_types': {k: np.sum(v > 0) / v.size for k, v in cloth_masks.items()},
                'original_size': image.shape[:2],
                'enhanced_size': enhanced.shape[:2]
            }
            
            return enhanced, cloth_info
            
        except Exception as e:
            self.logger.warning(f"ì˜ë¥˜ ê°ì§€ ë° ê°•í™” ì‹¤íŒ¨: {e}")
            return image, {
                'cloth_detected': False,
                'cloth_coverage': 0.0,
                'cloth_types': {},
                'original_size': image.shape[:2],
                'enhanced_size': image.shape[:2]
            }
    
    def _standardize_resolution(self, image: np.ndarray) -> np.ndarray:
        """í•´ìƒë„ í‘œì¤€í™”"""
        try:
            # ëª©í‘œ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LANCZOS4)
            return resized
            
        except Exception as e:
            self.logger.warning(f"í•´ìƒë„ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _optimize_for_cloth_segmentation(self, image: np.ndarray) -> np.ndarray:
        """ì˜ë¥˜ ë¶„í•  ìµœì í™”"""
        try:
            optimized = image.copy()
            
            # 1. íŒ¨í„´ ê°•í™”
            if self.cloth_params['pattern_enhancement']:
                optimized = self._enhance_patterns(optimized)
            
            # 2. ì¬ì§ˆ íŠ¹ì„± ê°ì§€
            if self.cloth_params['material_detection']:
                optimized = self._detect_material_properties(optimized)
            
            # 3. íˆ¬ëª…ë„ ì²˜ë¦¬
            if self.cloth_params['transparency_handling']:
                optimized = self._handle_transparency(optimized)
            
            # 4. ë°°ê²½ ë¶„ë¦¬
            if self.cloth_params['background_separation']:
                optimized = self._separate_background(optimized)
            
            # 5. í…ìŠ¤ì²˜ ë³´ì¡´
            if self.cloth_params['texture_preservation']:
                optimized = self._preserve_texture(optimized)
            
            return optimized
            
        except Exception as e:
            self.logger.warning(f"ì˜ë¥˜ ë¶„í•  ìµœì í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _enhance_patterns(self, image: np.ndarray) -> np.ndarray:
        """íŒ¨í„´ ê°•í™”"""
        try:
            enhanced = image.copy()
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # ì—£ì§€ ê°ì§€ (íŒ¨í„´ ê²½ê³„ ê°•í™”)
            edges = cv2.Canny(gray, 30, 100)
            
            # ì—£ì§€ë¥¼ RGBë¡œ ë³€í™˜
            edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
            
            # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì—£ì§€ í•©ì„±
            enhanced = cv2.addWeighted(enhanced, 0.8, edges_rgb, 0.2, 0)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"íŒ¨í„´ ê°•í™” ì‹¤íŒ¨: {e}")
            return image
    
    def _detect_material_properties(self, image: np.ndarray) -> np.ndarray:
        """ì¬ì§ˆ íŠ¹ì„± ê°ì§€"""
        try:
            enhanced = image.copy()
            
            # LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            
            # ì¬ì§ˆ íŠ¹ì„± ê°ì§€ë¥¼ ìœ„í•œ í•„í„°ë§
            # 1. ì§ˆê° ê°ì§€ (ë¡œì»¬ í‘œì¤€í¸ì°¨)
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            texture_kernel = np.ones((7, 7), np.float32) / 49
            mean_texture = cv2.filter2D(gray.astype(np.float32), -1, texture_kernel)
            texture_variance = cv2.filter2D((gray.astype(np.float32) - mean_texture)**2, -1, texture_kernel)
            texture_std = np.sqrt(texture_variance)
            
            # 2. ì¬ì§ˆ íŠ¹ì„±ì— ë”°ë¥¸ ê°•í™”
            # ê±°ì¹œ ì¬ì§ˆ (ë†’ì€ í‘œì¤€í¸ì°¨)
            rough_mask = texture_std > np.percentile(texture_std, 70)
            if np.any(rough_mask):
                enhanced[rough_mask] = cv2.convertScaleAbs(
                    enhanced[rough_mask], alpha=1.3, beta=15
                )
            
            # ë¶€ë“œëŸ¬ìš´ ì¬ì§ˆ (ë‚®ì€ í‘œì¤€í¸ì°¨)
            smooth_mask = texture_std < np.percentile(texture_std, 30)
            if np.any(smooth_mask):
                enhanced[smooth_mask] = cv2.GaussianBlur(enhanced[smooth_mask], (3, 3), 0)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"ì¬ì§ˆ íŠ¹ì„± ê°ì§€ ì‹¤íŒ¨: {e}")
            return image
    
    def _handle_transparency(self, image: np.ndarray) -> np.ndarray:
        """íˆ¬ëª…ë„ ì²˜ë¦¬"""
        try:
            enhanced = image.copy()
            
            # HSV ìƒ‰ê³µê°„ì—ì„œ íˆ¬ëª…ë„ ê°ì§€
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            saturation = hsv[:, :, 1]
            value = hsv[:, :, 2]
            
            # íˆ¬ëª…ë„ê°€ ë†’ì€ ì˜ì—­ ê°ì§€ (ë‚®ì€ ì±„ë„, ë†’ì€ ëª…ë„)
            transparent_mask = (saturation < 50) & (value > 200)
            
            if np.any(transparent_mask):
                # íˆ¬ëª… ì˜ì—­ì„ ë°˜íˆ¬ëª…í•˜ê²Œ ì²˜ë¦¬
                enhanced[transparent_mask] = cv2.addWeighted(
                    enhanced[transparent_mask], 0.7,
                    np.full_like(enhanced[transparent_mask], 255), 0.3, 0
                )
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"íˆ¬ëª…ë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _separate_background(self, image: np.ndarray) -> np.ndarray:
        """ë°°ê²½ ë¶„ë¦¬"""
        try:
            enhanced = image.copy()
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Otsu ì´ì§„í™”ë¡œ ë°°ê²½ ë¶„ë¦¬
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë°°ê²½ ë§ˆìŠ¤í¬ ì •ì œ
            kernel = np.ones((3, 3), np.uint8)
            background_mask = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # ë°°ê²½ ì˜ì—­ì„ ì•½ê°„ ì–´ë‘¡ê²Œ ì²˜ë¦¬
            background_regions = background_mask == 0
            if np.any(background_regions):
                enhanced[background_regions] = cv2.convertScaleAbs(
                    enhanced[background_regions], alpha=0.8, beta=-20
                )
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"ë°°ê²½ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _preserve_texture(self, image: np.ndarray) -> np.ndarray:
        """í…ìŠ¤ì²˜ ë³´ì¡´"""
        try:
            enhanced = image.copy()
            
            # ì–‘ë°©í–¥ í•„í„°ë¡œ ì—£ì§€ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆ ì œê±°
            enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            
            # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ í…ìŠ¤ì²˜ ì„ ëª…ë„ í–¥ìƒ
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            blurred = cv2.GaussianBlur(gray, (0, 0), 2.0)
            sharpened = cv2.addWeighted(gray, 1.5, blurred, -0.5, 0)
            
            # ì„ ëª…ë„ í–¥ìƒì„ RGBì— ì ìš©
            enhanced = cv2.cvtColor(sharpened, cv2.COLOR_GRAY2RGB)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"í…ìŠ¤ì²˜ ë³´ì¡´ ì‹¤íŒ¨: {e}")
            return image
    
    def _normalize_and_convert(self, image: np.ndarray) -> torch.Tensor:
        """ì •ê·œí™” ë° í…ì„œ ë³€í™˜"""
        try:
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized = image.astype(np.float32) / 255.0
            
            # ImageNet í‰ê· /í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            
            normalized = (normalized - mean) / std
            
            # í…ì„œë¡œ ë³€í™˜ [H, W, C] -> [C, H, W] -> [1, C, H, W]
            tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
            
            return tensor
            
        except Exception as e:
            self.logger.warning(f"ì •ê·œí™” ë° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.randn(1, 3, *self.target_size)
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.processing_stats.copy()
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.processing_stats = {
            'images_processed': 0,
            'cloth_detected': 0,
            'patterns_enhanced': 0,
            'materials_analyzed': 0
        }
    
    def update_cloth_params(self, **kwargs):
        """ì˜ë¥˜ ë¶„í•  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if key in self.cloth_params:
                self.cloth_params[key] = value
                self.logger.info(f"ì˜ë¥˜ ë¶„í•  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: {key} = {value}")
