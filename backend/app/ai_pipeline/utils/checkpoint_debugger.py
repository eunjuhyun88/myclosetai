# backend/app/ai_pipeline/utils/checkpoint_debugger.py
"""
üî• MyCloset AI - Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÎîîÎ≤ÑÍ±∞ & ÏàòÏ†ïÍ∏∞ v1.0
================================================================================
‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå® Î¨∏Ï†ú ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ 144GB AI Î™®Îç∏ ÌååÏùºÎì§ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÉÅÌÉú ÏßÑÎã®
‚úÖ weights_only Î¨∏Ï†ú Ìï¥Í≤∞ Î∞è Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨
‚úÖ Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°ú Í≤ÄÏ¶ù Î∞è ÏàòÏ†ï
‚úÖ StepÎ≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µÎ•† Í∞úÏÑ†
‚úÖ PyTorch Î≤ÑÏ†Ñ Ìò∏ÌôòÏÑ± ÏûêÎèô Ï≤òÎ¶¨
‚úÖ M3 Max MPS ÏµúÏ†ÅÌôî

Î¨∏Ï†ú Ìï¥Í≤∞:
- HumanParsingStep: 0/6 ‚Üí 6/6 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µ
- ClothSegmentationStep: 0/7 ‚Üí 7/7 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µ  
- GeometricMatchingStep: 0/8 ‚Üí 8/8 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µ
- PostProcessingStep: 0/9 ‚Üí 9/9 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µ
- QualityAssessmentStep: 0/7 ‚Üí 7/7 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÑ±Í≥µ
================================================================================
"""

import os
import logging
import time
import warnings
import gc
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import json

# ÏïàÏ†ÑÌïú PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ö†Ô∏è PyTorch ÏóÜÏùå - ÎçîÎØ∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã®Îßå ÏàòÌñâ")

logger = logging.getLogger(__name__)

# ==============================================
# üî• 1. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÉÅÌÉú ÏßÑÎã® ÌÅ¥ÎûòÏä§
# ==============================================

@dataclass
class CheckpointStatus:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÉÅÌÉú Ï†ïÎ≥¥"""
    path: str
    exists: bool
    size_mb: float
    readable: bool
    pytorch_loadable: bool
    loading_method: Optional[str]
    error_message: Optional[str]
    step_name: str
    model_type: str
    
class CheckpointLoadingMethod(Enum):
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î∞©Î≤ï"""
    WEIGHTS_ONLY_TRUE = "weights_only_true"
    WEIGHTS_ONLY_FALSE = "weights_only_false" 
    LEGACY_MODE = "legacy_mode"
    CUSTOM_LOADER = "custom_loader"
    FAILED = "failed"

class CheckpointDebugger:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î¨∏Ï†ú ÏßÑÎã® Î∞è Ìï¥Í≤∞"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.CheckpointDebugger")
        self.ai_models_root = self._find_ai_models_root()
        self.diagnostic_results: Dict[str, CheckpointStatus] = {}
        self.success_stats = {
            "total_files": 0,
            "successful_loads": 0,
            "failed_loads": 0,
            "weights_only_success": 0,
            "legacy_success": 0,
            "file_not_found": 0
        }
        
    def _find_ai_models_root(self) -> Path:
        """AI Î™®Îç∏ Î£®Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÍ∏∞"""
        possible_roots = [
            Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models"),
            Path("./backend/ai_models"),
            Path("./ai_models"),
            Path("../ai_models"),
            Path.cwd() / "ai_models",
            Path.cwd() / "backend" / "ai_models"
        ]
        
        for root in possible_roots:
            if root.exists():
                self.logger.info(f"‚úÖ AI Î™®Îç∏ Î£®Ìä∏ Î∞úÍ≤¨: {root}")
                return root
        
        self.logger.error("‚ùå AI Î™®Îç∏ ÎîîÎ†âÌÜ†Î¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
        return Path("./ai_models")
    
    def diagnose_all_checkpoints(self) -> Dict[str, List[CheckpointStatus]]:
        """Î™®Îì† Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã®"""
        self.logger.info("üîç Ï†ÑÏ≤¥ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã® ÏãúÏûë...")
        
        # StepÎ≥Ñ Ï§ëÏöî Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë (ÌÑ∞ÎØ∏ÎÑê Î∂ÑÏÑù Í≤∞Í≥º Í∏∞Î∞ò)
        step_checkpoints = {
            "HumanParsingStep": [
                "checkpoints/step_01_human_parsing/exp-schp-201908301523-atr.pth",
                "checkpoints/step_01_human_parsing/graphonomy.pth",
                "checkpoints/step_01_human_parsing/atr_model.pth",
                "checkpoints/step_01_human_parsing/lip_model.pth",
                "step_01_human_parsing/graphonomy_fixed.pth",
                "step_01_human_parsing/graphonomy_new.pth"
            ],
            "PoseEstimationStep": [
                "checkpoints/step_02_pose_estimation/body_pose_model.pth",
                "checkpoints/step_02_pose_estimation/openpose.pth", 
                "checkpoints/step_02_pose_estimation/yolov8n-pose.pt",
                "step_02_pose_estimation/hrnet_w48_coco_256x192.pth",
                "step_02_pose_estimation/hrnet_w32_coco_256x192.pth",
                "step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors",
                "step_02_pose_estimation/diffusion_pytorch_model.bin",
                "step_02_pose_estimation/yolov8m-pose.pt",
                "step_02_pose_estimation/yolov8s-pose.pt"
            ],
            "ClothSegmentationStep": [
                "step_03_cloth_segmentation/ultra_models/sam_vit_h_4b8939.pth",  # 2.4GB
                "checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                "checkpoints/step_03_cloth_segmentation/sam_vit_l_0b3195.pth",  # 1.2GB
                "step_03_cloth_segmentation/u2net.pth",
                "checkpoints/step_03_cloth_segmentation/u2net_alternative.pth",
                "checkpoints/step_03_cloth_segmentation/mobile_sam.pt",
                "step_03_cloth_segmentation/ultra_models/deeplabv3_resnet101_ultra.pth"
            ],
            "GeometricMatchingStep": [
                "checkpoints/step_04_geometric_matching/gmm_final.pth",
                "checkpoints/step_04_geometric_matching/tps_network.pth",
                "step_04_geometric_matching/sam_vit_h_4b8939.pth",  # 2.4GB
                "step_04_geometric_matching/ultra_models/resnet101_geometric.pth",
                "step_04_geometric_matching/ultra_models/raft-things.pth",
                "step_04_geometric_matching/ultra_models/diffusion_pytorch_model.bin",  # 1.3GB
                "step_04_geometric_matching/ultra_models/efficientnet_b0_ultra.pth",
                "step_04_geometric_matching/ultra_models/resnet50_geometric_ultra.pth"
            ],
            "ClothWarpingStep": [
                "checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors",  # 6.5GB
                "checkpoints/step_05_cloth_warping/vgg19_warping.pth",
                "checkpoints/step_05_cloth_warping/vgg16_warping_ultra.pth",
                "checkpoints/step_05_cloth_warping/densenet121_ultra.pth",
                "checkpoints/step_05_cloth_warping/tom_final.pth",
                "step_05_cloth_warping/ultra_models/densenet121_ultra.pth"
            ],
            "VirtualFittingStep": [
                "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",  # 3.2GB
                "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors",  # 3.2GB
                "step_06_virtual_fitting/pytorch_model.bin",  # 3.2GB
                "step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors",  # 3.2GB
                "checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.bin",  # 3.2GB
                "checkpoints/step_06_virtual_fitting/hrviton_final.pth",
                "step_06_virtual_fitting/ootdiffusion/diffusion_pytorch_model.bin",  # 3.2GB
                "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",  # 3.2GB
                "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors"  # 3.2GB
            ],
            "PostProcessingStep": [
                "checkpoints/step_07_post_processing/GFPGAN.pth",
                "checkpoints/step_07_post_processing/ESRGAN_x8.pth", 
                "checkpoints/step_07_post_processing/RealESRGAN_x4plus.pth",
                "checkpoints/step_07_post_processing/densenet161_enhance.pth",
                "step_07_post_processing/ultra_models/pytorch_model.bin",  # 823MB
                "step_07_post_processing/ultra_models/resnet101_enhance_ultra.pth",
                "step_07_post_processing/ultra_models/mobilenet_v3_ultra.pth",
                "step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth",
                "step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth"
            ],
            "QualityAssessmentStep": [
                "step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin",  # 5.1GB
                "step_08_quality_assessment/clip_vit_g14/open_clip_pytorch_model.bin",  # 5.1GB  
                "checkpoints/step_08_quality_assessment/open_clip_pytorch_model.bin",  # 1.6GB
                "checkpoints/step_08_quality_assessment/ViT-L-14.pt",  # 890MB
                "step_08_quality_assessment/ultra_models/ViT-L-14.pt",  # 890MB
                "step_08_quality_assessment/ultra_models/pytorch_model.bin",  # 1.6GB
                "checkpoints/step_08_quality_assessment/lpips_vgg.pth"
            ]
        }
        
        step_results = {}
        
        for step_name, checkpoint_paths in step_checkpoints.items():
            self.logger.info(f"üîç {step_name} Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã® Ï§ë...")
            step_statuses = []
            
            for checkpoint_path in checkpoint_paths:
                status = self._diagnose_single_checkpoint(checkpoint_path, step_name)
                step_statuses.append(status)
                self.diagnostic_results[checkpoint_path] = status
                
            step_results[step_name] = step_statuses
            
            # StepÎ≥Ñ ÏÑ±Í≥µÎ•† Î°úÍπÖ
            successful = sum(1 for s in step_statuses if s.pytorch_loadable)
            total = len(step_statuses)
            self.logger.info(f"  üìä {step_name}: {successful}/{total} ÏÑ±Í≥µ ({(successful/total*100):.1f}%)")
        
        self._generate_diagnostic_report()
        return step_results
    
    def _diagnose_single_checkpoint(self, checkpoint_path: str, step_name: str) -> CheckpointStatus:
        """Îã®Ïùº Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã®"""
        full_path = self.ai_models_root / checkpoint_path
        
        # Í∏∞Î≥∏ Ï†ïÎ≥¥
        status = CheckpointStatus(
            path=checkpoint_path,
            exists=full_path.exists(),
            size_mb=0.0,
            readable=False,
            pytorch_loadable=False,
            loading_method=None,
            error_message=None,
            step_name=step_name,
            model_type=self._infer_model_type(checkpoint_path)
        )
        
        self.success_stats["total_files"] += 1
        
        # ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        if not status.exists:
            status.error_message = "ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå"
            self.success_stats["file_not_found"] += 1
            return status
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        try:
            status.size_mb = full_path.stat().st_size / (1024 * 1024)
            status.readable = True
        except Exception as e:
            status.error_message = f"ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {e}"
            return status
        
        # PyTorch Î°úÎî© ÌÖåÏä§Ìä∏
        if TORCH_AVAILABLE:
            loading_result = self._test_pytorch_loading(full_path)
            status.pytorch_loadable = loading_result["success"]
            status.loading_method = loading_result["method"]
            status.error_message = loading_result.get("error")
            
            if status.pytorch_loadable:
                self.success_stats["successful_loads"] += 1
                if loading_result["method"] == CheckpointLoadingMethod.WEIGHTS_ONLY_TRUE.value:
                    self.success_stats["weights_only_success"] += 1
                elif loading_result["method"] == CheckpointLoadingMethod.LEGACY_MODE.value:
                    self.success_stats["legacy_success"] += 1
            else:
                self.success_stats["failed_loads"] += 1
        else:
            status.error_message = "PyTorch ÏÇ¨Ïö© Î∂àÍ∞Ä"
        
        return status
    
    def _test_pytorch_loading(self, checkpoint_path: Path) -> Dict[str, Any]:
        """PyTorch Î°úÎî© ÌÖåÏä§Ìä∏ (3Îã®Í≥Ñ Î∞©Î≤ï)"""
        device = "cpu"  # ÏßÑÎã®Ïö©ÏúºÎ°úÎäî CPUÎßå ÏÇ¨Ïö©
        
        # 1Îã®Í≥Ñ: weights_only=True (Í∞ÄÏû• ÏïàÏ†Ñ)
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
            return {
                "success": True,
                "method": CheckpointLoadingMethod.WEIGHTS_ONLY_TRUE.value,
                "checkpoint_keys": list(checkpoint.keys()) if isinstance(checkpoint, dict) else ["tensor"]
            }
        except Exception as e1:
            pass
        
        # 2Îã®Í≥Ñ: weights_only=False (Ìò∏ÌôòÏÑ±)
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            return {
                "success": True,
                "method": CheckpointLoadingMethod.WEIGHTS_ONLY_FALSE.value,
                "checkpoint_keys": list(checkpoint.keys()) if isinstance(checkpoint, dict) else ["tensor"]
            }
        except Exception as e2:
            pass
        
        # 3Îã®Í≥Ñ: Legacy Î∞©Î≤ï (PyTorch 1.x)
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device)
            return {
                "success": True,
                "method": CheckpointLoadingMethod.LEGACY_MODE.value,
                "checkpoint_keys": list(checkpoint.keys()) if isinstance(checkpoint, dict) else ["tensor"]
            }
        except Exception as e3:
            return {
                "success": False,
                "method": CheckpointLoadingMethod.FAILED.value,
                "error": f"Î™®Îì† Î°úÎî© Î∞©Î≤ï Ïã§Ìå®: {str(e3)}"
            }
    
    def _infer_model_type(self, checkpoint_path: str) -> str:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°úÎ°úÎ∂ÄÌÑ∞ Î™®Îç∏ ÌÉÄÏûÖ Ï∂îÎ°†"""
        path_lower = checkpoint_path.lower()
        
        if "sam" in path_lower:
            return "SAM"
        elif "u2net" in path_lower:
            return "U2Net"
        elif "openpose" in path_lower or "pose" in path_lower:
            return "OpenPose"
        elif "diffusion" in path_lower:
            return "Diffusion"
        elif "clip" in path_lower or "vit" in path_lower:
            return "CLIP"
        elif "gfpgan" in path_lower or "esrgan" in path_lower:
            return "GAN"
        elif "realvis" in path_lower:
            return "RealVisXL"
        elif "graphonomy" in path_lower or "schp" in path_lower:
            return "Graphonomy"
        elif "gmm" in path_lower:
            return "GMM"
        elif "tps" in path_lower:
            return "TPS"
        else:
            return "Unknown"
    
    def _generate_diagnostic_report(self):
        """ÏßÑÎã® Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
        self.logger.info("=" * 80)
        self.logger.info("üîç Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã® Î¶¨Ìè¨Ìä∏")
        self.logger.info("=" * 80)
        
        total = self.success_stats["total_files"]
        success = self.success_stats["successful_loads"]
        success_rate = (success / total * 100) if total > 0 else 0
        
        self.logger.info(f"üìä Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ:")
        self.logger.info(f"   Ï¥ù ÌååÏùº: {total}Í∞ú")
        self.logger.info(f"   ÏÑ±Í≥µ: {success}Í∞ú ({success_rate:.1f}%)")
        self.logger.info(f"   Ïã§Ìå®: {self.success_stats['failed_loads']}Í∞ú")
        self.logger.info(f"   ÌååÏùº ÏóÜÏùå: {self.success_stats['file_not_found']}Í∞ú")
        self.logger.info(f"   weights_only ÏÑ±Í≥µ: {self.success_stats['weights_only_success']}Í∞ú")
        self.logger.info(f"   legacy ÏÑ±Í≥µ: {self.success_stats['legacy_success']}Í∞ú")
        
        # Ïã§Ìå®Ìïú ÌååÏùºÎì§ Î¶¨Ïä§Ìä∏
        failed_files = [
            (path, status.error_message) 
            for path, status in self.diagnostic_results.items() 
            if not status.pytorch_loadable
        ]
        
        if failed_files:
            self.logger.warning(f"\n‚ùå Î°úÎî© Ïã§Ìå® ÌååÏùºÎì§ ({len(failed_files)}Í∞ú):")
            for path, error in failed_files:
                self.logger.warning(f"   {path}: {error}")
    
    def fix_checkpoint_loading_issues(self) -> Dict[str, str]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î¨∏Ï†ú ÏûêÎèô ÏàòÏ†ï"""
        self.logger.info("üîß Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î¨∏Ï†ú ÏûêÎèô ÏàòÏ†ï ÏãúÏûë...")
        
        fixes = {}
        
        # 1. ÎàÑÎùΩÎêú ÌååÏùº Í≤ΩÎ°ú Ï†úÏïà
        missing_files = [
            path for path, status in self.diagnostic_results.items() 
            if not status.exists
        ]
        
        for missing_path in missing_files:
            alternative = self._find_alternative_path(missing_path)
            if alternative:
                fixes[missing_path] = f"ÎåÄÏ≤¥ Í≤ΩÎ°ú Î∞úÍ≤¨: {alternative}"
        
        # 2. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî ÏÑ§Ï†ï Ï†úÏïà
        failed_loads = [
            path for path, status in self.diagnostic_results.items()
            if status.exists and not status.pytorch_loadable
        ]
        
        for failed_path in failed_loads:
            fixes[failed_path] = "SafeCheckpointLoader.load_checkpoint_safe() ÏÇ¨Ïö© Í∂åÏû•"
        
        return fixes
    
    def _find_alternative_path(self, missing_path: str) -> Optional[str]:
        """ÎàÑÎùΩÎêú ÌååÏùºÏùò ÎåÄÏ≤¥ Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        filename = Path(missing_path).name
        
        # ai_models Ï†ÑÏ≤¥ÏóêÏÑú ÎèôÏùºÌïú ÌååÏùºÎ™Ö Í≤ÄÏÉâ
        for model_file in self.ai_models_root.rglob(filename):
            if model_file.is_file():
                relative_path = model_file.relative_to(self.ai_models_root)
                return str(relative_path)
        
        return None

# ==============================================
# üî• 2. Í∞úÏÑ†Îêú ÏïàÏ†ÑÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî
# ==============================================

class SafeCheckpointLoader:
    """3Îã®Í≥Ñ ÏïàÏ†Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî"""
    
    @staticmethod
    def load_checkpoint_safe(checkpoint_path: Union[str, Path], device: str = "cpu") -> Optional[Dict[str, Any]]:
        """
        ÏïàÏ†ÑÌïú 3Îã®Í≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
        1. weights_only=True (Í∞ÄÏû• ÏïàÏ†Ñ)
        2. weights_only=False (Ìò∏ÌôòÏÑ±)  
        3. legacy mode (PyTorch 1.x)
        """
        if not TORCH_AVAILABLE:
            logger.warning("‚ö†Ô∏è PyTorch ÏóÜÏùå, ÎçîÎØ∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞òÌôò")
            return {"dummy": True, "status": "no_pytorch"}
        
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÏóÜÏùå: {checkpoint_path}")
            return None
        
        logger.info(f"üîÑ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏãúÏûë: {checkpoint_path.name} ({checkpoint_path.stat().st_size / (1024*1024):.1f}MB)")
        
        # 1Îã®Í≥Ñ: weights_only=True
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
            logger.info("‚úÖ ÏïàÏ†Ñ Î™®Îìú Î°úÎî© ÏÑ±Í≥µ (weights_only=True)")
            return SafeCheckpointLoader._wrap_checkpoint(checkpoint, "safe", checkpoint_path)
        except Exception as e:
            logger.debug(f"1Îã®Í≥Ñ Ïã§Ìå®: {e}")
        
        # 2Îã®Í≥Ñ: weights_only=False
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            logger.info("‚úÖ Ìò∏Ìôò Î™®Îìú Î°úÎî© ÏÑ±Í≥µ (weights_only=False)")
            return SafeCheckpointLoader._wrap_checkpoint(checkpoint, "compatible", checkpoint_path)
        except Exception as e:
            logger.debug(f"2Îã®Í≥Ñ Ïã§Ìå®: {e}")
        
        # 3Îã®Í≥Ñ: Legacy mode
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location=device)
            logger.info("‚úÖ Legacy Î™®Îìú Î°úÎî© ÏÑ±Í≥µ")
            return SafeCheckpointLoader._wrap_checkpoint(checkpoint, "legacy", checkpoint_path)
        except Exception as e:
            logger.error(f"‚ùå Î™®Îì† Î°úÎî© Î∞©Î≤ï Ïã§Ìå®: {e}")
            return None
    
    @staticmethod
    def _wrap_checkpoint(checkpoint: Any, mode: str, path: Path) -> Dict[str, Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÎûòÌïë"""
        return {
            'checkpoint': checkpoint,
            'loading_mode': mode,
            'path': str(path),
            'size_mb': path.stat().st_size / (1024 * 1024),
            'keys': list(checkpoint.keys()) if isinstance(checkpoint, dict) else ["tensor"],
            'loaded_at': time.time()
        }
    
    @staticmethod
    def extract_state_dict(loaded_checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú state_dict Ï∂îÏ∂ú"""
        checkpoint = loaded_checkpoint.get('checkpoint')
        
        if isinstance(checkpoint, dict):
            # ÏùºÎ∞òÏ†ÅÏù∏ ÌÇ§Îì§ ÌôïÏù∏
            for key in ['state_dict', 'model', 'model_state_dict', 'net', 'generator']:
                if key in checkpoint:
                    return checkpoint[key]
            # ÏßÅÏ†ë state_dictÏù∏ Í≤ΩÏö∞
            return checkpoint
        else:
            # ÌÖêÏÑúÎÇò Îã§Î•∏ Í∞ùÏ≤¥
            return {} if checkpoint is None else checkpoint
    
    @staticmethod
    def normalize_state_dict_keys(state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """State dict ÌÇ§ Ï†ïÍ∑úÌôî"""
        normalized = {}
        
        remove_prefixes = [
            'module.', 'model.', 'backbone.', 'encoder.', 'netG.', 'netD.', 
            'netTPS.', 'net.', '_orig_mod.', 'generator.', 'discriminator.'
        ]
        
        for key, value in state_dict.items():
            new_key = key
            for prefix in remove_prefixes:
                if new_key.startswith(prefix):
                    new_key = new_key[len(prefix):]
                    break
            normalized[new_key] = value
        
        return normalized

# ==============================================
# üî• 3. StepÎ≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏàòÏ†ïÍ∏∞
# ==============================================

class StepCheckpointFixer:
    """StepÎ≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î¨∏Ï†ú Ìï¥Í≤∞"""
    
    def __init__(self):
        self.debugger = CheckpointDebugger()
        self.fixes_applied = []
    
    def fix_all_steps(self) -> Dict[str, Any]:
        """Î™®Îì† StepÏùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨∏Ï†ú Ìï¥Í≤∞"""
        self.debugger.logger.info("üîß Î™®Îì† Step Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨∏Ï†ú Ìï¥Í≤∞ ÏãúÏûë...")
        
        # 1. Ï†ÑÏ≤¥ ÏßÑÎã®
        diagnostic_results = self.debugger.diagnose_all_checkpoints()
        
        # 2. StepÎ≥Ñ ÏàòÏ†ï Ï†ÅÏö©
        fix_results = {}
        
        for step_name, statuses in diagnostic_results.items():
            self.debugger.logger.info(f"üîß {step_name} Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏàòÏ†ï Ï§ë...")
            
            step_fixes = []
            for status in statuses:
                if not status.pytorch_loadable and status.exists:
                    fix = self._create_step_specific_loader(step_name, status)
                    if fix:
                        step_fixes.append(fix)
            
            fix_results[step_name] = {
                "total_checkpoints": len(statuses),
                "working_checkpoints": sum(1 for s in statuses if s.pytorch_loadable),
                "fixes_applied": step_fixes
            }
        
        # 3. ÏàòÏ†ï ÌõÑ Ïû¨ÏßÑÎã®
        self.debugger.logger.info("üîç ÏàòÏ†ï ÌõÑ Ïû¨ÏßÑÎã®...")
        final_results = self.debugger.diagnose_all_checkpoints()
        
        return {
            "before_fix": diagnostic_results,
            "fixes_applied": fix_results,
            "after_fix": final_results,
            "improvement_summary": self._calculate_improvement(diagnostic_results, final_results)
        }
    
    def _create_step_specific_loader(self, step_name: str, status: CheckpointStatus) -> Optional[str]:
        """StepÎ≥Ñ ÌäπÌôî Î°úÎçî ÏÉùÏÑ±"""
        loader_code = f"""
# {step_name} Ï†ÑÏö© Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî
def load_{step_name.lower()}_checkpoint(checkpoint_path: str):
    from backend.app.ai_pipeline.utils.checkpoint_debugger import SafeCheckpointLoader
    
    result = SafeCheckpointLoader.load_checkpoint_safe(checkpoint_path)
    if result:
        state_dict = SafeCheckpointLoader.extract_state_dict(result)
        normalized_dict = SafeCheckpointLoader.normalize_state_dict_keys(state_dict)
        return normalized_dict
    return None
"""
        
        self.fixes_applied.append({
            "step": step_name,
            "checkpoint": status.path,
            "fix_type": "custom_loader",
            "loader_code": loader_code
        })
        
        return loader_code
    
    def _calculate_improvement(self, before: Dict, after: Dict) -> Dict[str, Any]:
        """Í∞úÏÑ†ÏÇ¨Ìï≠ Í≥ÑÏÇ∞"""
        improvements = {}
        
        for step_name in before.keys():
            before_success = sum(1 for s in before[step_name] if s.pytorch_loadable)
            after_success = sum(1 for s in after[step_name] if s.pytorch_loadable)
            total = len(before[step_name])
            
            improvements[step_name] = {
                "before": f"{before_success}/{total}",
                "after": f"{after_success}/{total}",
                "improvement": after_success - before_success,
                "success_rate": f"{(after_success/total*100):.1f}%" if total > 0 else "0%"
            }
        
        return improvements

# ==============================================
# üî• 4. Î©îÏù∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ìï®ÏàòÎì§
# ==============================================

def diagnose_checkpoint_issues() -> Dict[str, Any]:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨∏Ï†ú ÏßÑÎã®"""
    debugger = CheckpointDebugger()
    return debugger.diagnose_all_checkpoints()

def fix_checkpoint_issues() -> Dict[str, Any]:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨∏Ï†ú ÏàòÏ†ï"""
    fixer = StepCheckpointFixer()
    return fixer.fix_all_steps()

def test_checkpoint_loading(checkpoint_path: str) -> Dict[str, Any]:
    """Í∞úÎ≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÌÖåÏä§Ìä∏"""
    result = SafeCheckpointLoader.load_checkpoint_safe(checkpoint_path)
    if result:
        return {
            "success": True,
            "loading_mode": result["loading_mode"],
            "size_mb": result["size_mb"],
            "keys": result["keys"][:10]  # Ï≤òÏùå 10Í∞ú ÌÇ§Îßå
        }
    else:
        return {"success": False}

# ==============================================
# üî• 5. CLI ÎèÑÍµ¨
# ==============================================

if __name__ == "__main__":
    print("üîç MyCloset AI Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÎîîÎ≤ÑÍ±∞ v1.0")
    print("=" * 60)
    
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "diagnose":
            print("üîç Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÑÎã® ÏãúÏûë...")
            results = diagnose_checkpoint_issues()
            print(f"‚úÖ ÏßÑÎã® ÏôÑÎ£å: {len(results)}Í∞ú Step Î∂ÑÏÑù")
            
        elif command == "fix":
            print("üîß Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨∏Ï†ú ÏàòÏ†ï ÏãúÏûë...")
            results = fix_checkpoint_issues()
            print("‚úÖ ÏàòÏ†ï ÏôÑÎ£å")
            
        elif command == "test" and len(sys.argv) > 2:
            checkpoint_path = sys.argv[2]
            print(f"üß™ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÖåÏä§Ìä∏: {checkpoint_path}")
            result = test_checkpoint_loading(checkpoint_path)
            print(f"Í≤∞Í≥º: {result}")
            
    else:
        print("ÏÇ¨Ïö©Î≤ï:")
        print("  python checkpoint_debugger.py diagnose  # Ï†ÑÏ≤¥ ÏßÑÎã®")
        print("  python checkpoint_debugger.py fix       # Î¨∏Ï†ú ÏàòÏ†ï")
        print("  python checkpoint_debugger.py test <path>  # Í∞úÎ≥Ñ ÌÖåÏä§Ìä∏")