# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v4.0 - ìµœì í™”ëœ í†µí•© ì‹œìŠ¤í…œ
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ + ë‹¨ë°©í–¥ ì˜ì¡´ì„± êµ¬ì¡°
âœ… Step í´ë˜ìŠ¤ ì™„ë²½ ì§€ì› + ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤
âœ… ì‹¤ì œ AI ëª¨ë¸ ìë™ íƒì§€ + ë¡œë”© + ì¶”ë¡ 
âœ… M3 Max 128GB ìµœì í™” + Neural Engine í™œìš©
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± + í™•ì¥ì„± ë³´ì¥

ì˜ì¡´ì„± íë¦„:
step_model_requests â†’ auto_model_detector â†’ model_loader â†’ __init__ â†’ Step í´ë˜ìŠ¤ë“¤
"""

import os
import sys
import time
import logging
import threading
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
from functools import lru_cache

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ (ìµœì í™”ë¨)
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (í•œë²ˆë§Œ ì‹¤í–‰)"""
    try:
        import platform
        
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3]))
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        try:
            import psutil
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        except ImportError:
            system_info["memory_gb"] = 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        device = "cpu"
        try:
            import torch
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        except ImportError:
            pass
        
        system_info["device"] = device
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8.0"
        }

# ì‹œìŠ¤í…œ ì •ë³´ ì „ì—­ ë³€ìˆ˜
SYSTEM_INFO = _get_system_info()
IS_M3_MAX = SYSTEM_INFO["is_m3_max"]
DEVICE = SYSTEM_INFO["device"]
MEMORY_GB = SYSTEM_INFO["memory_gb"]

# ==============================================
# ğŸ”¥ í•µì‹¬ ëª¨ë“ˆ ì•ˆì „ Import (ë‹¨ë°©í–¥ ì˜ì¡´ì„±)
# ==============================================

# 1. Step ëª¨ë¸ ìš”ì²­ ì •ì˜ (ìµœí•˜ìœ„ - ì˜ì¡´ì„± ì—†ìŒ)
try:
    from .step_model_requests import (
        STEP_MODEL_REQUESTS,
        ModelRequest,
        StepPriority,
        get_step_request,
        get_all_step_requests,
        get_checkpoint_patterns,
        get_model_config_for_step,
        validate_model_for_step,
        get_step_priorities,
        get_steps_by_priority
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("âœ… Step Model Requests ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"âš ï¸ Step Model Requests ë¡œë“œ ì‹¤íŒ¨: {e}")

# 2. ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ (step_model_requests ì˜ì¡´)
try:
    from .auto_model_detector import (
        AutoModelDetector,
        DetectedModel,
        DetectionStatus,
        quick_detect_models,
        detect_and_export_for_loader,
        validate_detected_models
    )
    AUTO_DETECTOR_AVAILABLE = True
    logger.info("âœ… Auto Model Detector ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    AUTO_DETECTOR_AVAILABLE = False
    logger.warning(f"âš ï¸ Auto Model Detector ë¡œë“œ ì‹¤íŒ¨: {e}")

# 3. ëª¨ë¸ ë¡œë” ì‹œìŠ¤í…œ (ìœ„ ë‘ ëª¨ë“ˆ ì˜ì¡´)
try:
    from .model_loader import (
        ModelLoader,
        ModelConfig,
        ModelType,
        ModelFormat,
        LoadedModel,
        StepModelInterface,
        BaseStepMixin,
        # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
        BaseModel,
        GraphonomyModel,
        OpenPoseModel,
        U2NetModel,
        GeometricMatchingModel,
        HRVITONModel,
        # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
        preprocess_image,
        postprocess_segmentation,
        # ì „ì—­ í•¨ìˆ˜ë“¤
        get_global_model_loader,
        initialize_global_model_loader,
        cleanup_global_loader
    )
    MODEL_LOADER_AVAILABLE = True
    logger.info("âœ… Model Loader ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logger.warning(f"âš ï¸ Model Loader ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ
# ==============================================

class PipelineUtils:
    """
    ğŸ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° í†µí•© ê´€ë¦¬ì
    Step í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•© ì œê³µ
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.logger = logging.getLogger(f"{__name__}.PipelineUtils")
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        self.model_loader = None
        
        # í†µê³„
        self.stats = {
            "total_models_detected": 0,
            "models_loaded": 0,
            "step_interfaces_created": 0,
            "total_requests": 0
        }
        
        self._initialized = True
        
        self.logger.info("ğŸ¯ PipelineUtils ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™”"""
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        try:
            start_time = time.time()
            self.logger.info("ğŸš€ PipelineUtils ì´ˆê¸°í™” ì‹œì‘...")
            
            results = {
                "step_requests": STEP_REQUESTS_AVAILABLE,
                "auto_detector": AUTO_DETECTOR_AVAILABLE,
                "model_loader": False,
                "auto_detection_count": 0,
                "errors": []
            }
            
            # 1. ModelLoader ì´ˆê¸°í™”
            if MODEL_LOADER_AVAILABLE:
                try:
                    init_result = initialize_global_model_loader(**kwargs)
                    if init_result.get("success"):
                        self.model_loader = get_global_model_loader()
                        results["model_loader"] = True
                        self.logger.info("âœ… ModelLoader ì´ˆê¸°í™” ì„±ê³µ")
                    else:
                        results["errors"].append(f"ModelLoader: {init_result.get('error', 'Unknown')}")
                except Exception as e:
                    results["errors"].append(f"ModelLoader: {e}")
                    self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. ìë™ ëª¨ë¸ íƒì§€ (ì„ íƒì )
            if AUTO_DETECTOR_AVAILABLE and kwargs.get("auto_detect", True):
                try:
                    detected_models = quick_detect_models(min_confidence=0.7)
                    results["auto_detection_count"] = len(detected_models)
                    self.stats["total_models_detected"] = len(detected_models)
                    
                    if detected_models:
                        self.logger.info(f"ğŸ” ìë™ íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
                    else:
                        self.logger.info("ğŸ” ìë™ íƒì§€ ì™„ë£Œ: íƒì§€ëœ ëª¨ë¸ ì—†ìŒ")
                        
                except Exception as e:
                    results["errors"].append(f"AutoDetector: {e}")
                    self.logger.warning(f"âš ï¸ ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
            
            # 3. í´ë°± ModelLoader ìƒì„± (ì‹¤íŒ¨ ì‹œ)
            if not self.model_loader:
                try:
                    self.model_loader = get_global_model_loader()
                    self.logger.info("âœ… í´ë°± ModelLoader ìƒì„±")
                except Exception as e:
                    results["errors"].append(f"Fallback ModelLoader: {e}")
                    self.logger.error(f"âŒ í´ë°± ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            success_count = sum([
                results["step_requests"],
                results["auto_detector"],
                results["model_loader"]
            ])
            
            self.logger.info(f"ğŸ‰ PipelineUtils ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            self.logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ: {success_count}/3")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_info": SYSTEM_INFO,
                "modules": results,
                "stats": self.stats
            }
            
        except Exception as e:
            self.logger.error(f"âŒ PipelineUtils ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "system_info": SYSTEM_INFO
            }
    
    def create_step_interface(self, step_name: str) -> Dict[str, Any]:
        """
        ğŸ”¥ Step í´ë˜ìŠ¤ìš© í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        ëª¨ë“  Step í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
        """
        try:
            self.stats["step_interfaces_created"] += 1
            
            # ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
            interface = {
                "step_name": step_name,
                "system_info": SYSTEM_INFO,
                "logger": logging.getLogger(f"steps.{step_name}"),
                "initialized": self.is_initialized
            }
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤
            if self.model_loader:
                try:
                    model_interface = self.model_loader.create_step_interface(step_name)
                    interface["model_interface"] = model_interface
                    interface["get_model"] = self._create_get_model_func(model_interface)
                    interface["has_model_loader"] = True
                except Exception as e:
                    interface["model_loader_error"] = str(e)
                    interface["has_model_loader"] = False
            else:
                interface["has_model_loader"] = False
            
            # Step ìš”ì²­ ì •ë³´
            if STEP_REQUESTS_AVAILABLE:
                step_request = get_step_request(step_name)
                if step_request:
                    interface["step_request"] = step_request
                    interface["recommended_model"] = step_request.model_name
                    interface["input_size"] = step_request.input_size
                    interface["num_classes"] = step_request.num_classes
                    interface["optimization_params"] = step_request.optimization_params
            
            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
            interface["preprocess_image"] = self._create_preprocess_func()
            interface["postprocess_output"] = self._create_postprocess_func()
            interface["optimize_memory"] = self._create_memory_func()
            
            # ë©”íƒ€ë°ì´í„°
            interface["metadata"] = {
                "creation_time": time.time(),
                "available_modules": {
                    "step_requests": STEP_REQUESTS_AVAILABLE,
                    "auto_detector": AUTO_DETECTOR_AVAILABLE,
                    "model_loader": MODEL_LOADER_AVAILABLE
                }
            }
            
            self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "step_name": step_name,
                "error": str(e),
                "system_info": SYSTEM_INFO,
                "logger": logging.getLogger(f"steps.{step_name}")
            }
    
    def _create_get_model_func(self, model_interface: Any) -> Callable:
        """ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ìƒì„±"""
        async def get_model(model_name: Optional[str] = None):
            try:
                self.stats["total_requests"] += 1
                return await model_interface.get_model(model_name)
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
        return get_model
    
    def _create_preprocess_func(self) -> Callable:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±"""
        def preprocess_func(image, target_size=(512, 512), **kwargs):
            try:
                if MODEL_LOADER_AVAILABLE:
                    return preprocess_image(image, target_size, **kwargs)
                else:
                    self.logger.warning("ModelLoader ì—†ìŒ: ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
                    return None
            except Exception as e:
                self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return None
        return preprocess_func
    
    def _create_postprocess_func(self) -> Callable:
        """í›„ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±"""
        def postprocess_func(output, output_type="segmentation", **kwargs):
            try:
                if MODEL_LOADER_AVAILABLE:
                    if output_type == "segmentation":
                        return postprocess_segmentation(output, **kwargs)
                    # ë‹¤ë¥¸ íƒ€ì…ë“¤ ì¶”ê°€ ê°€ëŠ¥
                    return output
                else:
                    self.logger.warning("ModelLoader ì—†ìŒ: í›„ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
                    return None
            except Exception as e:
                self.logger.error(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return None
        return postprocess_func
    
    def _create_memory_func(self) -> Callable:
        """ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜ ìƒì„±"""
        def optimize_memory():
            try:
                if self.model_loader and hasattr(self.model_loader, 'memory_manager'):
                    self.model_loader.memory_manager.cleanup_memory()
                    return {"success": True}
                else:
                    import gc
                    gc.collect()
                    return {"success": True, "message": "Basic cleanup"}
            except Exception as e:
                self.logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                return {"success": False, "error": str(e)}
        return optimize_memory
    
    def get_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = {
                "initialized": self.is_initialized,
                "initialization_time": self.initialization_time,
                "system_info": SYSTEM_INFO,
                "modules": {
                    "step_requests": STEP_REQUESTS_AVAILABLE,
                    "auto_detector": AUTO_DETECTOR_AVAILABLE,
                    "model_loader": MODEL_LOADER_AVAILABLE
                },
                "stats": self.stats
            }
            
            # ModelLoader ìƒíƒœ
            if self.model_loader:
                try:
                    status["model_loader_info"] = self.model_loader.get_system_info()
                except Exception as e:
                    status["model_loader_error"] = str(e)
            
            return status
            
        except Exception as e:
            return {"error": str(e), "system_info": SYSTEM_INFO}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model_loader:
                self.model_loader.cleanup()
            
            self.is_initialized = False
            self.logger.info("âœ… PipelineUtils ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ PipelineUtils ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
# ==============================================

_global_utils: Optional[PipelineUtils] = None
_utils_lock = threading.Lock()

def get_pipeline_utils() -> PipelineUtils:
    """ì „ì—­ PipelineUtils ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_utils
    
    with _utils_lock:
        if _global_utils is None:
            _global_utils = PipelineUtils()
        return _global_utils

def initialize_pipeline_utils(**kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
    main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì 
    """
    try:
        utils = get_pipeline_utils()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™”
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
            future = asyncio.create_task(utils.initialize(**kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
            result = loop.run_until_complete(utils.initialize(**kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ Step í´ë˜ìŠ¤ìš© ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    ëª¨ë“  Step í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    """
    try:
        utils = get_pipeline_utils()
        return utils.create_step_interface(step_name)
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO
        }

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        utils = get_pipeline_utils()
        return utils.get_status()
    except Exception as e:
        return {"error": str(e), "system_info": SYSTEM_INFO}

def cleanup_pipeline_utils():
    """íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì •ë¦¬"""
    global _global_utils
    
    try:
        with _utils_lock:
            if _global_utils:
                _global_utils.cleanup()
                _global_utils = None
        
        # ì „ì—­ ModelLoader ì •ë¦¬
        if MODEL_LOADER_AVAILABLE:
            cleanup_global_loader()
        
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
# ==============================================

def get_model_loader():
    """ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜)"""
    try:
        if MODEL_LOADER_AVAILABLE:
            return get_global_model_loader()
        return None
    except Exception as e:
        logger.error(f"ModelLoader ë°˜í™˜ ì‹¤íŒ¨: {e}")
        return None

def detect_models(search_paths: Optional[List[Path]] = None):
    """ìë™ ëª¨ë¸ íƒì§€ (í•˜ìœ„ í˜¸í™˜)"""
    try:
        if AUTO_DETECTOR_AVAILABLE:
            return quick_detect_models(search_paths=search_paths)
        return {}
    except Exception as e:
        logger.error(f"ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return {}

def get_step_requirements(step_name: str):
    """Step ìš”êµ¬ì‚¬í•­ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)"""
    try:
        if STEP_REQUESTS_AVAILABLE:
            return get_step_request(step_name)
        return None
    except Exception as e:
        logger.error(f"Step ìš”êµ¬ì‚¬í•­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ __all__ ì •ì˜
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í•¨ìˆ˜ë“¤ (Step í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©)
    'create_step_interface',
    'initialize_pipeline_utils',
    'get_pipeline_utils',
    'get_system_status',
    'cleanup_pipeline_utils',
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'IS_M3_MAX',
    'DEVICE',
    'MEMORY_GB',
    
    # ğŸ”§ í¸ì˜ í•¨ìˆ˜ë“¤
    'get_model_loader',
    'detect_models',
    'get_step_requirements',
    
    # ğŸ“¦ í•µì‹¬ í´ë˜ìŠ¤ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    'PipelineUtils'
]

# Step Model Requests ëª¨ë“ˆ export
if STEP_REQUESTS_AVAILABLE:
    __all__.extend([
        'STEP_MODEL_REQUESTS',
        'ModelRequest',
        'StepPriority',
        'get_step_request',
        'get_all_step_requests',
        'get_checkpoint_patterns',
        'get_model_config_for_step',
        'validate_model_for_step',
        'get_step_priorities',
        'get_steps_by_priority'
    ])

# Auto Model Detector ëª¨ë“ˆ export
if AUTO_DETECTOR_AVAILABLE:
    __all__.extend([
        'AutoModelDetector',
        'DetectedModel',
        'DetectionStatus',
        'quick_detect_models',
        'detect_and_export_for_loader',
        'validate_detected_models'
    ])

# Model Loader ëª¨ë“ˆ export
if MODEL_LOADER_AVAILABLE:
    __all__.extend([
        'ModelLoader',
        'ModelConfig',
        'ModelType',
        'ModelFormat',
        'LoadedModel',
        'StepModelInterface',
        'BaseStepMixin',
        # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
        'BaseModel',
        'GraphonomyModel',
        'OpenPoseModel',
        'U2NetModel',
        'GeometricMatchingModel',
        'HRVITONModel',
        # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
        'preprocess_image',
        'postprocess_segmentation',
        # ì „ì—­ í•¨ìˆ˜ë“¤
        'get_global_model_loader',
        'initialize_global_model_loader',
        'cleanup_global_loader'
    ])

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ìš”ì•½
# ==============================================

def _print_initialization_summary():
    """ì´ˆê¸°í™” ìš”ì•½ ì¶œë ¥"""
    available_modules = sum([
        STEP_REQUESTS_AVAILABLE,
        AUTO_DETECTOR_AVAILABLE,
        MODEL_LOADER_AVAILABLE
    ])
    
    logger.info("=" * 70)
    logger.info("ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v4.0")
    logger.info("=" * 70)
    logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ: {available_modules}/3")
    logger.info(f"   - Step Model Requests: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
    logger.info(f"   - Auto Model Detector: {'âœ…' if AUTO_DETECTOR_AVAILABLE else 'âŒ'}")
    logger.info(f"   - Model Loader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
    logger.info(f"ğŸ ì‹œìŠ¤í…œ ì •ë³´:")
    logger.info(f"   - Platform: {SYSTEM_INFO['platform']}")
    logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    logger.info(f"   - Device: {DEVICE}")
    logger.info(f"   - Memory: {MEMORY_GB}GB")
    
    if available_modules >= 2:
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        logger.info("ğŸ”¥ Step í´ë˜ìŠ¤ì—ì„œ create_step_interface() ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ ì¼ë¶€ ëª¨ë“ˆë§Œ ì‚¬ìš© ê°€ëŠ¥ - ì œí•œì  ê¸°ëŠ¥ ì œê³µ")
    
    logger.info("=" * 70)

# ì´ˆê¸°í™” ìš”ì•½ ì¶œë ¥
_print_initialization_summary()

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_pipeline_utils)

# í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì´ˆê¸°í™”
if os.getenv('AUTO_INIT_PIPELINE_UTILS', 'false').lower() in ('true', '1', 'yes'):
    try:
        result = initialize_pipeline_utils()
        if result.get('success'):
            logger.info("ğŸš€ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ìë™ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {result.get('error')}")
    except Exception as e:
        logger.warning(f"âš ï¸ ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

logger.info("ğŸ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v4.0 ë¡œë“œ ì™„ë£Œ")