# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v7.0 - GitHub ì™„ì „ í˜¸í™˜ + ì˜¤ë¥˜ í•´ê²°
================================================================================
âœ… get_step_memory_manager í•¨ìˆ˜ ì¶”ê°€ (import ì˜¤ë¥˜ í•´ê²°)
âœ… ê¸°ì¡´ main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… get_step_model_interface í•¨ìˆ˜ ì™„ë²½ êµ¬í˜„
âœ… StepModelInterface.list_available_models í¬í•¨
âœ… BaseStepMixin ì˜ì¡´ì„± ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… M3 Max 128GB ìµœì í™” (conda í™˜ê²½ ìš°ì„ )
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜
âœ… ëª¨ë“  í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥
âœ… ModelLoader coroutine ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ í¬í•¨ (ê¸°ëŠ¥ ëˆ„ë½ ì—†ìŒ)

main.py í˜¸ì¶œ íŒ¨í„´:
from app.ai_pipeline.utils import get_step_model_interface, get_step_memory_manager
interface = get_step_model_interface("HumanParsingStep")
models = interface.list_available_models()
memory_manager = get_step_memory_manager()
"""

import os
import sys
import logging
import threading
import asyncio
import time
import gc
import weakref
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from abc import ABC, abstractmethod

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° ì„¤ì • (GitHub í”„ë¡œì íŠ¸ ë°˜ì˜)
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (í•œë²ˆë§Œ ì‹¤í–‰) - conda í™˜ê²½ ìš°ì„ """
    try:
        import platform
        
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ
        }
        
        # M3 Max ê°ì§€ (GitHub í”„ë¡œì íŠ¸ ìµœì í™” ëŒ€ìƒ)
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        else:
            system_info["memory_gb"] = 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€ (M3 Max ìš°ì„ )
        device = "cpu"
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available() and is_m3_max:
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        
        # AI ëª¨ë¸ ê²½ë¡œ ê°ì§€
        project_root = Path(__file__).parent.parent.parent.parent
        ai_models_path = project_root / "ai_models"
        system_info["ai_models_path"] = str(ai_models_path)
        system_info["ai_models_exists"] = ai_models_path.exists()
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8.0",
            "conda_env": "base",
            "in_conda": False,
            "ai_models_path": "./ai_models",
            "ai_models_exists": False
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"        # ê¸°ì¡´ ë°©ì‹ (v3.0)
    UNIFIED = "unified"      # ìƒˆë¡œìš´ í†µí•© ë°©ì‹ (v6.0)
    HYBRID = "hybrid"        # í˜¼í•© ë°©ì‹
    FALLBACK = "fallback"    # í´ë°± ëª¨ë“œ

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • - conda í™˜ê²½ ìµœì í™”"""
    device: str = "auto"
    memory_gb: float = 16.0
    is_m3_max: bool = False
    optimization_enabled: bool = True
    max_workers: int = 4
    cache_enabled: bool = True
    debug_mode: bool = False
    conda_optimized: bool = True
    model_precision: str = "fp16"  # M3 Maxì—ì„œ fp16 ê¸°ë³¸
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if self.device == "auto":
            self.device = SYSTEM_INFO["device"]
        if self.is_m3_max and self.conda_optimized:
            self.model_precision = "fp16"
            self.max_workers = min(8, SYSTEM_INFO["cpu_count"])

@dataclass
class StepConfig:
    """Step ì„¤ì • (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í‘œì¤€)"""
    step_name: str
    step_number: Optional[int] = None
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_class: Optional[str] = None
    input_size: Tuple[int, int] = (512, 512)
    device: str = "auto"
    precision: str = "fp16"
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Step ë²ˆí˜¸ ìë™ ì¶”ì¶œ"""
        if self.step_number is None and "Step" in self.step_name:
            try:
                # HumanParsingStep -> step_01_human_parsing
                import re
                match = re.search(r'(\d+)', self.step_name)
                if match:
                    self.step_number = int(match.group(1))
                else:
                    # Step ì´ë¦„ì—ì„œ ìˆœì„œ ì¶”ì¶œ
                    step_mapping = {
                        "HumanParsingStep": 1,
                        "PoseEstimationStep": 2,
                        "ClothSegmentationStep": 3,
                        "GeometricMatchingStep": 4,
                        "ClothWarpingStep": 5,
                        "VirtualFittingStep": 6,
                        "PostProcessingStep": 7,
                        "QualityAssessmentStep": 8
                    }
                    self.step_number = step_mapping.get(self.step_name, 0)
            except:
                self.step_number = 0

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (GitHub ai_models í´ë” í‘œì¤€)"""
    name: str
    path: str
    model_type: str
    file_size_mb: float
    confidence_score: float = 1.0
    step_compatibility: List[str] = field(default_factory=list)
    architecture: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ ë°ì´í„° ë³€í™˜ê¸° í´ë˜ìŠ¤ (get_step_data_converter ì˜¤ë¥˜ í•´ê²°)
# ==============================================

class StepDataConverter:
    """
    ğŸ”„ Stepë³„ ë°ì´í„° ë³€í™˜ê¸° - main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í´ë˜ìŠ¤
    âœ… get_step_data_converter() í•¨ìˆ˜ë¡œ ì ‘ê·¼
    âœ… ì´ë¯¸ì§€/í…ì„œ ë³€í™˜ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    
    def __init__(self, device: str = "auto", precision: str = "fp16"):
        self.device = device if device != "auto" else SYSTEM_INFO["device"]
        self.precision = precision
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        self.logger = logging.getLogger(f"{__name__}.StepDataConverter")
        
        # ë³€í™˜ í†µê³„
        self.conversion_count = 0
        self.total_conversion_time = 0.0
        self.cache_hits = 0
        
        # ê°„ë‹¨í•œ ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self._conversion_cache = {}
        self.max_cache_size = 100
        
        self.logger.info(f"ğŸ”„ ë°ì´í„° ë³€í™˜ê¸° ì´ˆê¸°í™”: {self.device}, {self.precision}")
    
    def tensor_to_pil(self, tensor: Any) -> Any:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ - ê¸°ë³¸ ë³€í™˜")
                return tensor
            
            if hasattr(tensor, 'cpu'):
                # PyTorch í…ì„œ ì²˜ë¦¬
                if tensor.dim() == 4:
                    tensor = tensor.squeeze(0)
                if tensor.dim() == 3 and tensor.shape[0] in [1, 3]:
                    tensor = tensor.permute(1, 2, 0)
                
                tensor = tensor.cpu().detach()
                
                if tensor.dtype != torch.uint8:
                    tensor = (tensor * 255).clamp(0, 255).byte()
                
                array = tensor.numpy()
                
                if PIL_AVAILABLE:
                    from PIL import Image
                    if array.shape[-1] == 1:
                        array = array.squeeze(-1)
                        return Image.fromarray(array, mode='L')
                    elif array.shape[-1] == 3:
                        return Image.fromarray(array, mode='RGB')
                    else:
                        return Image.fromarray(array)
                
            return tensor
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return tensor
    
    def pil_to_tensor(self, image: Any, normalize: bool = True) -> Any:
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            if not PIL_AVAILABLE or not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PIL/PyTorch ì—†ìŒ - ê¸°ë³¸ ë³€í™˜")
                return image
            
            if hasattr(image, 'size'):  # PIL ì´ë¯¸ì§€
                import numpy as np
                from PIL import Image
                
                # PIL -> NumPy
                array = np.array(image)
                
                if array.ndim == 2:  # Grayscale
                    array = np.expand_dims(array, axis=2)
                
                # NumPy -> PyTorch
                tensor = torch.from_numpy(array.copy())
                
                if tensor.dim() == 3:
                    tensor = tensor.permute(2, 0, 1)  # HWC -> CHW
                
                tensor = tensor.unsqueeze(0)  # Add batch dimension
                
                if normalize:
                    tensor = tensor.float() / 255.0
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if self.device != "cpu":
                    tensor = tensor.to(self.device)
                
                return tensor
            
            return image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PIL->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    def numpy_to_tensor(self, array: Any) -> Any:
        """NumPy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE or not NUMPY_AVAILABLE:
                return array
            
            if hasattr(array, 'shape'):  # NumPy ë°°ì—´
                tensor = torch.from_numpy(array.copy())
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if self.device != "cpu":
                    tensor = tensor.to(self.device)
                
                return tensor
            
            return array
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ NumPy->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return array
    
    def tensor_to_numpy(self, tensor: Any) -> Any:
        """í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE:
                return tensor
            
            if hasattr(tensor, 'cpu'):
                return tensor.cpu().detach().numpy()
            
            return tensor
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->NumPy ë³€í™˜ ì‹¤íŒ¨: {e}")
            return tensor
    
    def preprocess_image(self, image: Any, target_size: Tuple[int, int] = (512, 512)) -> Any:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if PIL_AVAILABLE and hasattr(image, 'resize'):
                # PIL ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
                image = image.resize(target_size, Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)
            
            # í…ì„œë¡œ ë³€í™˜
            tensor = self.pil_to_tensor(image, normalize=True)
            
            self.conversion_count += 1
            return tensor
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def postprocess_output(self, output: Any, output_type: str = "image") -> Any:
        """ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            if output_type == "image":
                return self.tensor_to_pil(output)
            elif output_type == "mask":
                # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
                if TORCH_AVAILABLE and hasattr(output, 'cpu'):
                    output = output.cpu()
                    if output.dim() > 2:
                        output = output.squeeze()
                    
                    # ì´ì§„í™”
                    output = (output > 0.5).float()
                    
                return self.tensor_to_pil(output)
            elif output_type == "numpy":
                return self.tensor_to_numpy(output)
            else:
                return output
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶œë ¥ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return output
    
    def convert_batch(self, data_list: List[Any], conversion_type: str = "pil_to_tensor") -> List[Any]:
        """ë°°ì¹˜ ë°ì´í„° ë³€í™˜"""
        try:
            results = []
            
            for data in data_list:
                if conversion_type == "pil_to_tensor":
                    result = self.pil_to_tensor(data)
                elif conversion_type == "tensor_to_pil":
                    result = self.tensor_to_pil(data)
                elif conversion_type == "numpy_to_tensor":
                    result = self.numpy_to_tensor(data)
                elif conversion_type == "tensor_to_numpy":
                    result = self.tensor_to_numpy(data)
                else:
                    result = data
                
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°°ì¹˜ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return data_list
    
    def get_stats(self) -> Dict[str, Any]:
        """ë³€í™˜ í†µê³„"""
        return {
            "device": self.device,
            "precision": self.precision,
            "conversion_count": self.conversion_count,
            "total_conversion_time": self.total_conversion_time,
            "cache_hits": self.cache_hits,
            "cache_size": len(self._conversion_cache),
            "is_m3_max": self.is_m3_max
        }
    
    def cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        if len(self._conversion_cache) > self.max_cache_size:
            # ì ˆë°˜ ì •ë„ ì œê±°
            items_to_remove = list(self._conversion_cache.keys())[:self.max_cache_size // 2]
            for key in items_to_remove:
                del self._conversion_cache[key]
            
            self.logger.debug(f"ğŸ§¹ ë³€í™˜ ìºì‹œ ì •ë¦¬: {len(items_to_remove)}ê°œ ì œê±°")

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì í´ë˜ìŠ¤ (main.py ì˜¤ë¥˜ í•´ê²°)
# ==============================================

class StepMemoryManager:
    """
    ğŸ§  Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í´ë˜ìŠ¤
    âœ… get_step_memory_manager() í•¨ìˆ˜ë¡œ ì ‘ê·¼
    âœ… M3 Max 128GB ìµœì í™”
    âœ… conda í™˜ê²½ íŠ¹í™”
    """
    
    def __init__(self, device: str = "auto", memory_limit_gb: float = None):
        self.device = device if device != "auto" else SYSTEM_INFO["device"]
        self.memory_limit_gb = memory_limit_gb or SYSTEM_INFO["memory_gb"]
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        self.logger = logging.getLogger(f"{__name__}.StepMemoryManager")
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self.memory_limit_gb = min(self.memory_limit_gb, 100.0)  # 128GB ì¤‘ 100GB ì‚¬ìš©
            
        self.allocated_memory = {}  # Stepë³„ í• ë‹¹ëœ ë©”ëª¨ë¦¬ ì¶”ì 
        self.peak_usage = 0.0
        self.cleanup_threshold = 0.8  # 80% ì‚¬ìš© ì‹œ ì •ë¦¬
        
        self.logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.device}, {self.memory_limit_gb}GB")
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps" and self.is_m3_max:
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    return min(available_gb, self.memory_limit_gb)
                else:
                    return self.memory_limit_gb * 0.7  # ë³´ìˆ˜ì  ì¶”ì •
            else:
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                else:
                    return 8.0
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def allocate_memory(self, step_name: str, size_gb: float) -> bool:
        """Stepì— ë©”ëª¨ë¦¬ í• ë‹¹"""
        try:
            available = self.get_available_memory()
            if available >= size_gb:
                self.allocated_memory[step_name] = size_gb
                self.peak_usage = max(self.peak_usage, sum(self.allocated_memory.values()))
                self.logger.info(f"âœ… {step_name}: {size_gb}GB í• ë‹¹ë¨")
                return True
            else:
                self.logger.warning(f"âš ï¸ {step_name}: {size_gb}GB í• ë‹¹ ì‹¤íŒ¨ (ì‚¬ìš© ê°€ëŠ¥: {available:.1f}GB)")
                return False
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {e}")
            return False
    
    def deallocate_memory(self, step_name: str):
        """Stepì˜ ë©”ëª¨ë¦¬ í•´ì œ"""
        if step_name in self.allocated_memory:
            size = self.allocated_memory.pop(step_name)
            self.logger.info(f"ğŸ—‘ï¸ {step_name}: {size}GB í•´ì œë¨")
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            self.logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            used_memory = sum(self.allocated_memory.values())
            pressure = used_memory / self.memory_limit_gb
            return pressure > self.cleanup_threshold
        except Exception:
            return False
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        return {
            "device": self.device,
            "total_limit_gb": self.memory_limit_gb,
            "available_gb": self.get_available_memory(),
            "allocated_by_steps": self.allocated_memory.copy(),
            "total_allocated_gb": sum(self.allocated_memory.values()),
            "peak_usage_gb": self.peak_usage,
            "memory_pressure": self.check_memory_pressure(),
            "is_m3_max": self.is_m3_max
        }

# ==============================================
# ğŸ”¥ StepModelInterface í´ë˜ìŠ¤ (main.py í˜¸í™˜)
# ==============================================

class StepModelInterface:
    """
    ğŸ”— Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤)
    âœ… get_model() ë©”ì„œë“œ ì œê³µ
    âœ… list_available_models() ë©”ì„œë“œ ì œê³µ
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ë‚´ì¥
    """
    
    def __init__(self, step_name: str, model_loader_instance: Optional[Any] = None):
        self.step_name = step_name
        self.model_loader = model_loader_instance
        self.logger = logging.getLogger(f"interface.{step_name}")
        
        # ìƒíƒœ ê´€ë¦¬
        self._models_cache = {}
        self._last_request_time = None
        self._request_count = 0
        self._initialization_attempted = False
        
        # Stepë³„ ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
        self._default_models = {
            "HumanParsingStep": ["graphonomy", "human_parsing_atr", "parsing_lip"],
            "PoseEstimationStep": ["openpose", "mediapipe", "yolov8_pose"],
            "ClothSegmentationStep": ["u2net", "cloth_segmentation", "deeplabv3"],
            "GeometricMatchingStep": ["geometric_matching", "tps_transformation"],
            "ClothWarpingStep": ["cloth_warping", "spatial_transformer"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "virtual_tryon"],
            "PostProcessingStep": ["image_enhancement", "artifact_removal"],
            "QualityAssessmentStep": ["clipiqa", "quality_assessment", "brisque"]
        }
        
        self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”")
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """
        ğŸ”¥ ëª¨ë¸ ë¡œë“œ (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ)
        """
        try:
            self._request_count += 1
            self._last_request_time = time.time()
            
            # ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
            if not model_name:
                available_models = self.list_available_models()
                if available_models:
                    model_name = available_models[0]
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                    return None
            
            # ìºì‹œ í™•ì¸
            if model_name in self._models_cache:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self._models_cache[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ë¡œë“œ ì‹œë„
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                try:
                    # ModelLoaderì˜ coroutine ì˜¤ë¥˜ í•´ê²°
                    if asyncio.iscoroutinefunction(self.model_loader.load_model):
                        model = await self.model_loader.load_model(model_name)
                    else:
                        model = self.model_loader.load_model(model_name)
                    
                    if model:
                        self._models_cache[model_name] = model
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        return model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoaderë¥¼ í†µí•œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹œë„ (í´ë°±)
            model = await self._direct_model_load(model_name)
            if model:
                self._models_cache[model_name] = model
                return model
            
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ìµœì¢… í´ë°±)
            return self._create_simulation_model(model_name)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def _direct_model_load(self, model_name: str) -> Optional[Any]:
        """ì§ì ‘ ëª¨ë¸ ë¡œë“œ (ai_models í´ë”ì—ì„œ)"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            if not ai_models_path.exists():
                return None
            
            # ëª¨ë¸ íŒŒì¼ íƒìƒ‰
            model_patterns = [
                f"{model_name}.pth",
                f"{model_name}.pt",
                f"{model_name}.ckpt",
                f"{model_name}.safetensors"
            ]
            
            for pattern in model_patterns:
                model_file = ai_models_path / pattern
                if model_file.exists():
                    # ì‹¤ì œ ëª¨ë¸ ë¡œë“œëŠ” ì—¬ê¸°ì„œ êµ¬í˜„
                    self.logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_file}")
                    return ModelInfo(
                        name=model_name,
                        path=str(model_file),
                        model_type=f"{self.step_name}_model",
                        file_size_mb=model_file.stat().st_size / (1024*1024),
                        step_compatibility=[self.step_name]
                    )
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_simulation_model(self, model_name: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        return {
            "name": model_name,
            "type": "simulation",
            "step_name": self.step_name,
            "created_at": time.time(),
            "simulate": True,
            "device": SYSTEM_INFO["device"],
            "precision": "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32"
        }
    
    def list_available_models(self) -> List[str]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ)
        """
        try:
            available_models = []
            
            # 1. Stepë³„ ê¸°ë³¸ ëª¨ë¸ë“¤
            default_models = self._default_models.get(self.step_name, [])
            available_models.extend(default_models)
            
            # 2. ai_models í´ë” ìŠ¤ìº”
            try:
                ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
                if ai_models_path.exists():
                    # Stepë³„ í´ë” í™•ì¸
                    step_folder = ai_models_path / self.step_name.lower().replace("step", "")
                    if step_folder.exists():
                        for model_file in step_folder.glob("*.{pth,pt,ckpt,safetensors}"):
                            model_name = model_file.stem
                            if model_name not in available_models:
                                available_models.append(model_name)
                    
                    # ë£¨íŠ¸ í´ë”ì—ì„œë„ í™•ì¸
                    for model_file in ai_models_path.glob("*.{pth,pt,ckpt,safetensors}"):
                        model_name = model_file.stem
                        if self.step_name.lower() in model_name.lower():
                            if model_name not in available_models:
                                available_models.append(model_name)
            
            except Exception as e:
                self.logger.debug(f"ai_models í´ë” ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            
            # 3. ModelLoaderì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            if self.model_loader and hasattr(self.model_loader, 'list_models'):
                try:
                    loader_models = self.model_loader.list_models()
                    if isinstance(loader_models, dict):
                        for model_name in loader_models.keys():
                            if model_name not in available_models:
                                available_models.append(model_name)
                    elif isinstance(loader_models, list):
                        for model in loader_models:
                            if model not in available_models:
                                available_models.append(model)
                except Exception as e:
                    self.logger.debug(f"ModelLoader ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            available_models = sorted(list(set(available_models)))
            
            self.logger.info(f"ğŸ“‹ {self.step_name} ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(available_models)}ê°œ")
            return available_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._default_models.get(self.step_name, [])
    
    async def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            self._models_cache.clear()
            gc.collect()
            
            if TORCH_AVAILABLE and SYSTEM_INFO["device"] == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            elif TORCH_AVAILABLE and SYSTEM_INFO["device"] == "cuda":
                torch.cuda.empty_cache()
            
            self.logger.info(f"ğŸ—‘ï¸ {self.step_name} ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ í†µê³„"""
        return {
            "step_name": self.step_name,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "cached_models": len(self._models_cache),
            "has_model_loader": self.model_loader is not None,
            "available_models_count": len(self.list_available_models())
        }

# ==============================================
# ğŸ”¥ í†µí•© Step ì¸í„°í˜ì´ìŠ¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

class UnifiedStepInterface:
    """
    ğŸ”— í†µí•© Step ì¸í„°í˜ì´ìŠ¤
    âœ… GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì›
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    """
    
    def __init__(self, manager: 'UnifiedUtilsManager', config: StepConfig, is_fallback: bool = False):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # í†µê³„ ì¶”ì 
        self._request_count = 0
        self._last_request_time = None
        self._processing_time_total = 0.0
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            target_model = model_name or self.config.model_name
            if not target_model:
                self.logger.warning("ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•ŠìŒ")
                return None
            
            start_time = time.time()
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ë¡œì§ êµ¬í˜„
            model = {"name": target_model, "type": "unified_model", "step": self.config.step_name}
            processing_time = time.time() - start_time
            
            self._request_count += 1
            self._last_request_time = time.time()
            self._processing_time_total += processing_time
            self.manager.stats["total_requests"] += 1
            
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return await self.manager.optimize_memory()
    
    async def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (Stepë³„ íŠ¹í™”)"""
        try:
            if self.is_fallback:
                self.logger.warning(f"{self.config.step_name} í´ë°± ëª¨ë“œ - ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬")
                return {"success": True, "simulation": True, "step_number": self.config.step_number}
            
            start_time = time.time()
            self.logger.info(f"ğŸ¯ Step {self.config.step_number:02d} {self.config.step_name} ì²˜ë¦¬ ì‹œì‘")
            
            # GitHub í”„ë¡œì íŠ¸ 8ë‹¨ê³„ë³„ íŠ¹í™” ì²˜ë¦¬
            if self.config.step_number == 1:  # Human Parsing
                result = await self._process_human_parsing(image_data, **kwargs)
            elif self.config.step_number == 2:  # Pose Estimation
                result = await self._process_pose_estimation(image_data, **kwargs)
            elif self.config.step_number == 3:  # Cloth Segmentation
                result = await self._process_cloth_segmentation(image_data, **kwargs)
            elif self.config.step_number == 4:  # Geometric Matching
                result = await self._process_geometric_matching(image_data, **kwargs)
            elif self.config.step_number == 5:  # Cloth Warping
                result = await self._process_cloth_warping(image_data, **kwargs)
            elif self.config.step_number == 6:  # Virtual Fitting
                result = await self._process_virtual_fitting(image_data, **kwargs)
            elif self.config.step_number == 7:  # Post Processing
                result = await self._process_post_processing(image_data, **kwargs)
            elif self.config.step_number == 8:  # Quality Assessment
                result = await self._process_quality_assessment(image_data, **kwargs)
            else:
                result = await self._process_generic(image_data, **kwargs)
            
            processing_time = time.time() - start_time
            self._processing_time_total += processing_time
            
            if result:
                result.update({
                    "step_number": self.config.step_number,
                    "step_name": self.config.step_name,
                    "processing_time": processing_time,
                    "total_processing_time": self._processing_time_total
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _process_human_parsing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì¸ê°„ íŒŒì‹± ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "human_mask",
            "body_parts": ["head", "torso", "arms", "legs"],
            "confidence": 0.95
        }
    
    async def _process_pose_estimation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "pose_keypoints",
            "keypoints_count": 17,
            "confidence": 0.92
        }
    
    async def _process_cloth_segmentation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì˜ìƒ ë¶„í•  ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "cloth_mask",
            "cloth_types": ["shirt", "pants", "dress"],
            "confidence": 0.88
        }
    
    async def _process_geometric_matching(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "transformation_matrix",
            "matching_points": 128,
            "confidence": 0.90
        }
    
    async def _process_cloth_warping(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì˜ìƒ ë³€í˜• ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "warped_cloth",
            "warp_quality": "high",
            "confidence": 0.87
        }
    
    async def _process_virtual_fitting(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ê°€ìƒ í”¼íŒ… ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "fitted_image",
            "fitting_quality": "high",
            "confidence": 0.93
        }
    
    async def _process_post_processing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "enhanced_image",
            "enhancements": ["color_correction", "artifact_removal"],
            "confidence": 0.89
        }
    
    async def _process_quality_assessment(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€"""
        return {
            "success": True,
            "output_type": "quality_score",
            "overall_score": 8.5,
            "metrics": {"sharpness": 0.9, "realism": 0.85, "artifacts": 0.1},
            "confidence": 0.91
        }
    
    async def _process_generic(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì¼ë°˜ ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "processed_image",
            "generic_processing": True,
            "confidence": 0.8
        }
    
    def get_config(self) -> StepConfig:
        """ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            "step_name": self.config.step_name,
            "step_number": self.config.step_number,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "total_processing_time": self._processing_time_total,
            "average_processing_time": self._processing_time_total / max(self._request_count, 1),
            "is_fallback": self.is_fallback,
            "model_name": self.config.model_name
        }

# ==============================================
# ğŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

class UnifiedUtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € v7.0
    âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ ë°˜ì˜
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.system_config = SystemConfig(
            device=SYSTEM_INFO["device"],
            memory_gb=SYSTEM_INFO["memory_gb"],
            is_m3_max=SYSTEM_INFO["is_m3_max"],
            max_workers=min(SYSTEM_INFO["cpu_count"], 8),
            conda_optimized=SYSTEM_INFO["in_conda"]
        )
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ì¥ì†Œ (ì•½í•œ ì°¸ì¡°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces = {}  # StepModelInterface ì €ì¥
        self._model_cache = {}
        self._service_cache = weakref.WeakValueDictionary()
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = StepMemoryManager()
        
        # í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0,
            "conda_optimizations": 0
        }
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        
        # conda í™˜ê²½ ìµœì í™”
        if SYSTEM_INFO["in_conda"]:
            self._setup_conda_optimizations()
        
        self._initialized = True
        self.logger.info(f"ğŸ¯ UnifiedUtilsManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (conda: {SYSTEM_INFO['in_conda']})")
    
    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            # conda í™˜ê²½ì—ì„œ PyTorch ìµœì í™”
            if TORCH_AVAILABLE:
                # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
                torch.set_num_threads(self.system_config.max_workers)
                
                # M3 Max MPS ìµœì í™”
                if SYSTEM_INFO["is_m3_max"]:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                    })
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                # condaì—ì„œ ì„¤ì¹˜ëœ OpenBLAS/MKL í™œìš©
                os.environ['OMP_NUM_THREADS'] = str(self.system_config.max_workers)
                os.environ['MKL_NUM_THREADS'] = str(self.system_config.max_workers)
            
            self.stats["conda_optimizations"] += 1
            self.logger.info("âœ… conda í™˜ê²½ ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™” - GitHub í”„ë¡œì íŠ¸ ìµœì í™”"""
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        try:
            start_time = time.time()
            self.logger.info("ğŸš€ UnifiedUtilsManager ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.system_config, key):
                    setattr(self.system_config, key, value)
            
            # M3 Max + conda íŠ¹ë³„ ìµœì í™”
            if self.system_config.is_m3_max and self.system_config.conda_optimized:
                await self._optimize_m3_max_conda()
            
            # ModelLoader ì—°ë™ ì‹œë„
            await self._try_initialize_model_loader()
            
            # AI ëª¨ë¸ ê²½ë¡œ í™•ì¸
            await self._verify_ai_models_path()
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            self.logger.info(f"ğŸ‰ UnifiedUtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_config": self.system_config,
                "system_info": SYSTEM_INFO,
                "conda_optimized": self.system_config.conda_optimized
            }
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _optimize_m3_max_conda(self):
        """M3 Max + conda íŠ¹ë³„ ìµœì í™”"""
        try:
            if TORCH_AVAILABLE:
                # M3 Max MPS ë°±ì—”ë“œ ìµœì í™”
                if torch.backends.mps.is_available():
                    # 128GBì˜ 80% í™œìš©í•˜ë„ë¡ ì„¤ì •
                    if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                        torch.mps.set_per_process_memory_fraction(0.8)
                
                # FP16 ê¸°ë³¸ ì„¤ì •
                if hasattr(torch, 'set_default_dtype'):
                    torch.set_default_dtype(torch.float16)
            
            self.logger.info("âœ… M3 Max + conda íŠ¹ë³„ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def _try_initialize_model_loader(self):
        """ModelLoader ì´ˆê¸°í™” ì‹œë„"""
        try:
            # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import
            from app.ai_pipeline.utils.model_loader import get_global_model_loader
            self.model_loader = get_global_model_loader()
            
            if self.model_loader:
                self.logger.info("âœ… ModelLoader ì—°ë™ ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    async def _verify_ai_models_path(self):
        """AI ëª¨ë¸ ê²½ë¡œ í™•ì¸ ë° ìƒì„±"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            
            if not ai_models_path.exists():
                ai_models_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ AI ëª¨ë¸ í´ë” ìƒì„±: {ai_models_path}")
            
            # Stepë³„ í•˜ìœ„ í´ë” ìƒì„±
            step_folders = [
                "human_parsing", "pose_estimation", "cloth_segmentation",
                "geometric_matching", "cloth_warping", "virtual_fitting",
                "post_processing", "quality_assessment"
            ]
            
            for folder in step_folders:
                folder_path = ai_models_path / folder
                folder_path.mkdir(exist_ok=True)
            
            self.logger.info("âœ… AI ëª¨ë¸ í´ë” êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ í´ë” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str, **options) -> UnifiedStepInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹)"""
        try:
            with self._interface_lock:
                # ìºì‹œ í™•ì¸
                cache_key = f"{step_name}_{hash(str(options))}" if options else step_name
                
                if cache_key in self._step_interfaces:
                    self.logger.debug(f"ğŸ“‹ {step_name} ìºì‹œëœ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜")
                    return self._step_interfaces[cache_key]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                step_config = self._create_step_config(step_name, **options)
                interface = UnifiedStepInterface(self, step_config)
                
                # ìºì‹œ ì €ì¥
                self._step_interfaces[cache_key] = interface
                
                self.stats["interfaces_created"] += 1
                self.logger.info(f"ğŸ”— {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return self._create_fallback_interface(step_name)
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """
        ğŸ”¥ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
        """
        try:
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            interface = StepModelInterface(step_name, getattr(self, 'model_loader', None))
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return StepModelInterface(step_name, None)
    
    def _create_step_config(self, step_name: str, **options) -> StepConfig:
        """Step ì„¤ì • ìƒì„± (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê¸°ì¤€)"""
        # GitHub í”„ë¡œì íŠ¸ì˜ 8ë‹¨ê³„ë³„ ê¸°ë³¸ ì„¤ì •
        step_defaults = {
            "HumanParsingStep": {
                "model_name": "graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512),
                "step_number": 1
            },
            "PoseEstimationStep": {
                "model_name": "openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368),
                "step_number": 2
            },
            "ClothSegmentationStep": {
                "model_name": "u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320),
                "step_number": 3
            },
            "GeometricMatchingStep": {
                "model_name": "geometric_matching",
                "model_type": "GeometricMatchingModel",
                "input_size": (256, 192),
                "step_number": 4
            },
            "ClothWarpingStep": {
                "model_name": "cloth_warping",
                "model_type": "ClothWarpingModel",
                "input_size": (256, 192),
                "step_number": 5
            },
            "VirtualFittingStep": {
                "model_name": "ootdiffusion",
                "model_type": "OOTDiffusionModel",
                "input_size": (512, 512),
                "step_number": 6
            },
            "PostProcessingStep": {
                "model_name": "post_processing",
                "model_type": "PostProcessingModel",
                "input_size": (512, 512),
                "step_number": 7
            },
            "QualityAssessmentStep": {
                "model_name": "clipiqa",
                "model_type": "CLIPIQAModel",
                "input_size": (224, 224),
                "step_number": 8
            }
        }
        
        defaults = step_defaults.get(step_name, {
            "model_name": f"{step_name.lower()}_model",
            "model_type": "BaseModel",
            "input_size": (512, 512),
            "step_number": 0
        })
        
        # ì„¤ì • ë³‘í•©
        config_data = {
            "step_name": step_name,
            "device": self.system_config.device,
            "precision": self.system_config.model_precision,
            **defaults,
            **options
        }
        
        return StepConfig(**config_data)
    
    def _create_fallback_interface(self, step_name: str) -> UnifiedStepInterface:
        """í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        fallback_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(self, fallback_config, is_fallback=True)
    
    def get_memory_manager(self) -> StepMemoryManager:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜"""
        return self.memory_manager
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max 128GB íŠ¹í™”)"""
        try:
            import gc
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.system_config.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                elif self.system_config.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬ (128GBì—ì„œë„ ì£¼ê¸°ì  ì •ë¦¬)
            if len(self._model_cache) > 20:  # M3 MaxëŠ” ë” ë§ì€ ëª¨ë¸ ìºì‹œ í—ˆìš©
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ëª¨ë¸ ì œê±°
                items_to_remove = list(self._model_cache.keys())[:10]
                for key in items_to_remove:
                    del self._model_cache[key]
                    
                self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(items_to_remove)}ê°œ ì œê±°")
            
            self.stats["memory_optimizations"] += 1
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent": round(vm.percent, 1),
                    "is_m3_max_optimized": self.system_config.is_m3_max
                }
            
            return {
                "success": True,
                "memory_info": memory_info,
                "collected_objects": collected,
                "cache_cleared": len(items_to_remove) if 'items_to_remove' in locals() else 0,
                "optimization_count": self.stats["memory_optimizations"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        memory_info = {}
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            memory_info = {
                "total_gb": round(vm.total / (1024**3), 1),
                "available_gb": round(vm.available / (1024**3), 1),
                "percent": round(vm.percent, 1)
            }
        
        return {
            "initialized": self.is_initialized,
            "initialization_time": self.initialization_time,
            "system_config": self.system_config,
            "system_info": SYSTEM_INFO,
            "stats": self.stats,
            "memory_info": memory_info,
            "cache_sizes": {
                "step_interfaces": len(self._step_interfaces),
                "model_interfaces": len(self._model_interfaces),
                "models": len(self._model_cache),
                "services": len(self._service_cache)
            },
            "conda_status": {
                "in_conda": SYSTEM_INFO["in_conda"],
                "conda_env": SYSTEM_INFO["conda_env"],
                "optimized": self.system_config.conda_optimized
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë¹„ë™ê¸° ê°œì„ """
        try:
            # ëª¨ë“  ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            for interface in self._model_interfaces.values():
                try:
                    await interface.unload_models()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self._step_interfaces.clear()
            self._model_interfaces.clear()
            self._model_cache.clear()
            self._service_cache.clear()
            self.is_initialized = False
            
            self.logger.info("âœ… UnifiedUtilsManager ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (v3.0 ë°©ì‹)
    ê¸°ì¡´ Step í´ë˜ìŠ¤ë“¤ì´ ê³„ì† ì‚¬ìš© ê°€ëŠ¥
    """
    try:
        manager = get_utils_manager()
        unified_interface = manager.create_step_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v7.0-github-optimized",
            "has_unified_utils": True,
            "unified_interface": unified_interface,
            "conda_optimized": SYSTEM_INFO["in_conda"]
        }
        
        # ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ async wrapperë¡œ ì œê³µ
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        legacy_interface["get_model"] = get_model_wrapper
        legacy_interface["optimize_memory"] = unified_interface.optimize_memory
        legacy_interface["process_image"] = unified_interface.process_image
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "get_model": lambda: None,
            "optimize_memory": lambda: {"success": False},
            "process_image": lambda x, **k: None
        }

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    ğŸ”¥ main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… StepModelInterface ë°˜í™˜
    âœ… ë¹„ë™ê¸° ë©”ì„œë“œ í¬í•¨
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        if model_loader_instance is None:
            try:
                # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"âœ… ì „ì—­ ModelLoader íšë“: {step_name}")
            except ImportError as e:
                logger.warning(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ModelLoader íšë“ ì‹¤íŒ¨: {e}")
                model_loader_instance = None
        
        # UnifiedUtilsManagerë¥¼ í†µí•œ ìƒì„± ì‹œë„
        try:
            manager = get_utils_manager()
            interface = manager.create_step_model_interface(step_name)
            logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Manager)")
            return interface
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        interface = StepModelInterface(step_name, model_loader_instance)
        logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Direct)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

def get_step_data_converter(step_name: str = None, **kwargs) -> StepDataConverter:
    """
    ğŸ”¥ main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ - ë°ì´í„° ë³€í™˜ê¸° ë°˜í™˜
    âœ… import ì˜¤ë¥˜ í•´ê²°
    âœ… ì´ë¯¸ì§€/í…ì„œ ë³€í™˜ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    try:
        # ì§ì ‘ ìƒì„±
        converter = StepDataConverter(**kwargs)
        logger.info(f"ğŸ”„ ë°ì´í„° ë³€í™˜ê¸° ìƒì„±: {step_name or 'global'}")
        return converter
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë³€í™˜ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return StepDataConverter()

def get_step_memory_manager(step_name: str = None, **kwargs) -> StepMemoryManager:
    """
    ğŸ”¥ main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ - ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜
    âœ… import ì˜¤ë¥˜ í•´ê²°
    âœ… M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # UnifiedUtilsManagerë¥¼ í†µí•œ ì¡°íšŒ ì‹œë„
        try:
            manager = get_utils_manager()
            memory_manager = manager.get_memory_manager()
            logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜ (Manager): {step_name or 'global'}")
            return memory_manager
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        memory_manager = StepMemoryManager(**kwargs)
        logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì§ì ‘ ìƒì„±: {step_name or 'global'}")
        return memory_manager
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return StepMemoryManager()

# ==============================================
# ğŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì )
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    try:
        manager = get_utils_manager()
        
        # conda í™˜ê²½ íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            kwargs.setdefault("conda_optimized", True)
            kwargs.setdefault("model_precision", "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32")
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
            future = asyncio.create_task(manager.initialize(**kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
            result = loop.run_until_complete(manager.initialize(**kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {"error": str(e), "system_info": SYSTEM_INFO}

async def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ - ë¹„ë™ê¸° ê°œì„ """
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.cleanup()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¶Œì¥)"""
    manager = get_utils_manager()
    return manager.create_step_interface(step_name, **options)

async def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸°"""
    manager = get_utils_manager()
    return await manager.optimize_memory()

def get_ai_models_path() -> Path:
    """AI ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return Path(SYSTEM_INFO["ai_models_path"])

def list_available_steps() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ Step ëª©ë¡ (GitHub 8ë‹¨ê³„ ê¸°ì¤€)"""
    return [
        "HumanParsingStep",
        "PoseEstimationStep", 
        "ClothSegmentationStep",
        "GeometricMatchingStep",
        "ClothWarpingStep",
        "VirtualFittingStep",
        "PostProcessingStep",
        "QualityAssessmentStep"
    ]

def is_conda_environment() -> bool:
    """conda í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["in_conda"]

def get_conda_info() -> Dict[str, Any]:
    """conda í™˜ê²½ ì •ë³´"""
    return {
        "in_conda": SYSTEM_INFO["in_conda"],
        "conda_env": SYSTEM_INFO["conda_env"],
        "conda_prefix": os.environ.get('CONDA_PREFIX'),
        "python_path": sys.executable
    }

# ==============================================
# ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ì„± ë³´ì¥)
# ==============================================

def create_model_config(
    name: str,
    model_type: str = "BaseModel",
    device: str = "auto",
    **kwargs
) -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ìƒì„± ë„ìš°ë¯¸"""
    config = {
        "name": name,
        "model_type": model_type,
        "device": device if device != "auto" else SYSTEM_INFO["device"],
        "precision": "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32",
        "created_at": time.time(),
        **kwargs
    }
    return config

def validate_step_name(step_name: str) -> bool:
    """Step ì´ë¦„ ìœ íš¨ì„± ê²€ì¦"""
    valid_steps = [
        "HumanParsingStep", "PoseEstimationStep", "ClothSegmentationStep",
        "GeometricMatchingStep", "ClothWarpingStep", "VirtualFittingStep",
        "PostProcessingStep", "QualityAssessmentStep"
    ]
    return step_name in valid_steps

def get_step_number(step_name: str) -> int:
    """Step ë²ˆí˜¸ ë°˜í™˜"""
    step_mapping = {
        "HumanParsingStep": 1,
        "PoseEstimationStep": 2,
        "ClothSegmentationStep": 3,
        "GeometricMatchingStep": 4,
        "ClothWarpingStep": 5,
        "VirtualFittingStep": 6,
        "PostProcessingStep": 7,
        "QualityAssessmentStep": 8
    }
    return step_mapping.get(step_name, 0)

def format_memory_size(bytes_size: int) -> str:
    """ë©”ëª¨ë¦¬ í¬ê¸° í¬ë§·íŒ…"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f}{unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f}PB"

def check_device_compatibility(device: str) -> bool:
    """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬"""
    if device == "cpu":
        return True
    elif device == "mps":
        return TORCH_AVAILABLE and torch.backends.mps.is_available()
    elif device.startswith("cuda"):
        return TORCH_AVAILABLE and torch.cuda.is_available()
    else:
        return False

def get_optimal_workers() -> int:
    """ìµœì  ì›Œì»¤ ìˆ˜ ê³„ì‚°"""
    cpu_count = SYSTEM_INFO["cpu_count"]
    if SYSTEM_INFO["is_m3_max"]:
        return min(8, cpu_count)  # M3 MaxëŠ” ìµœëŒ€ 8ê°œ
    else:
        return min(4, cpu_count)  # ì¼ë°˜ì ìœ¼ë¡œëŠ” ìµœëŒ€ 4ê°œ

def create_fallback_response(error_msg: str, step_name: str = None) -> Dict[str, Any]:
    """í´ë°± ì‘ë‹µ ìƒì„±"""
    return {
        "success": False,
        "error": error_msg,
        "step_name": step_name,
        "fallback": True,
        "timestamp": time.time(),
        "system_info": {
            "device": SYSTEM_INFO["device"],
            "memory_gb": SYSTEM_INFO["memory_gb"],
            "conda": SYSTEM_INFO["in_conda"]
        }
    }

def get_environment_info() -> Dict[str, Any]:
    """í™˜ê²½ ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
    env_info = {
        "system": SYSTEM_INFO.copy(),
        "libraries": {
            "torch": TORCH_AVAILABLE,
            "numpy": NUMPY_AVAILABLE,
            "pil": PIL_AVAILABLE,
            "psutil": PSUTIL_AVAILABLE
        },
        "environment_variables": {
            "conda_prefix": os.environ.get('CONDA_PREFIX'),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV'),
            "python_path": sys.executable,
            "home": os.environ.get('HOME'),
            "user": os.environ.get('USER')
        }
    }
    
    # PyTorch ìƒì„¸ ì •ë³´
    if TORCH_AVAILABLE:
        env_info["torch_info"] = {
            "version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    
    return env_info

def safe_import(module_name: str, fallback=None):
    """ì•ˆì „í•œ ëª¨ë“ˆ import"""
    try:
        return __import__(module_name)
    except ImportError as e:
        logger.debug(f"ëª¨ë“ˆ import ì‹¤íŒ¨: {module_name} - {e}")
        return fallback

def measure_execution_time(func: Callable) -> Callable:
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start_time
        logger.debug(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ")
        return result
    return wrapper

def log_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¡œê¹…"""
    try:
        if PSUTIL_AVAILABLE:
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent:.1f}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
            logger.info(f"ğŸ”¥ CPU ì‚¬ìš©ë¥ : {cpu_percent:.1f}%")
            
            if TORCH_AVAILABLE and torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                    memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                    logger.info(f"ğŸš€ GPU {i} ë©”ëª¨ë¦¬: {memory_allocated:.1f}GB í• ë‹¹, {memory_reserved:.1f}GB ì˜ˆì•½")
        else:
            logger.info("ğŸ“Š psutil ì—†ìŒ - ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì œí•œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ë¡œê¹… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ __all__ ì •ì˜ (GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜)
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface',
    'StepModelInterface',  # main.py í•„ìˆ˜
    'StepMemoryManager',   # main.py ì˜¤ë¥˜ í•´ê²°
    'StepDataConverter',   # main.py ì˜¤ë¥˜ í•´ê²° (ìƒˆë¡œ ì¶”ê°€)
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ğŸ”§ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'reset_global_utils',
    
    # ğŸ”„ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
    'create_step_interface',          # ë ˆê±°ì‹œ í˜¸í™˜
    'create_unified_interface',       # ìƒˆë¡œìš´ ë°©ì‹
    'get_step_model_interface',       # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    'get_step_memory_manager',        # âœ… main.py ì˜¤ë¥˜ í•´ê²° í•¨ìˆ˜
    'get_step_data_converter',        # âœ… main.py ì˜¤ë¥˜ í•´ê²° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'optimize_system_memory',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° (GitHub í”„ë¡œì íŠ¸ íŠ¹í™”)
    'UtilsMode',
    'get_ai_models_path',
    'list_available_steps',
    'is_conda_environment',
    'get_conda_info',
    
    # ğŸ› ï¸ ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
    'create_model_config',
    'validate_step_name',
    'get_step_number',
    'format_memory_size',
    'check_device_compatibility',
    'get_optimal_workers',
    'create_fallback_response',
    'get_environment_info',
    'safe_import',
    'measure_execution_time',
    'log_system_resources'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
# ==============================================

# í™˜ê²½ ì •ë³´ ë¡œê¹…
logger.info("=" * 80)
logger.info("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v7.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ í˜¸í™˜")
logger.info("âœ… get_step_model_interface í•¨ìˆ˜ êµ¬í˜„ (main.py í˜¸í™˜)")
logger.info("âœ… get_step_memory_manager í•¨ìˆ˜ ì¶”ê°€ (import ì˜¤ë¥˜ í•´ê²°)")
logger.info("âœ… get_step_data_converter í•¨ìˆ˜ ì¶”ê°€ (import ì˜¤ë¥˜ í•´ê²°)")
logger.info("âœ… StepModelInterface.list_available_models í¬í•¨")
logger.info("âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›")
logger.info("âœ… conda í™˜ê²½ ìµœì í™”")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ ")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… ê¸°ì¡´ ì½”ë“œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥")
logger.info("âœ… ModelLoader coroutine ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ í¬í•¨ (ê¸°ëŠ¥ ëˆ„ë½ ì—†ìŒ)")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
logger.info(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info(f"ğŸ conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
logger.info(f"ğŸ“ AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
logger.info("=" * 80)

# conda í™˜ê²½ë³„ ì¶”ê°€ ìµœì í™”
if SYSTEM_INFO["in_conda"]:
    logger.info("ğŸ conda í™˜ê²½ ê°ì§€ - ì¶”ê°€ ìµœì í™” í™œì„±í™”")
    if SYSTEM_INFO["is_m3_max"]:
        logger.info("ğŸ M3 Max + conda ì¡°í•© - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ")

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit

def cleanup_on_exit():
    """ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(reset_global_utils())
        loop.close()
        logger.info("ğŸ§¹ ì‹œìŠ¤í…œ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

atexit.register(cleanup_on_exit)

# ==============================================
# ğŸ”¥ ê°œë°œ/ë””ë²„ê·¸ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def debug_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ë””ë²„ê·¸ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ” MyCloset AI ì‹œìŠ¤í…œ ì •ë³´")
    print("="*60)
    print(f"í”Œë«í¼: {SYSTEM_INFO['platform']}")
    print(f"ì•„í‚¤í…ì²˜: {SYSTEM_INFO['machine']}")
    print(f"M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
    print(f"ë””ë°”ì´ìŠ¤: {SYSTEM_INFO['device']}")
    print(f"ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
    print(f"CPU ì½”ì–´: {SYSTEM_INFO['cpu_count']}")
    print(f"Python: {SYSTEM_INFO['python_version']}")
    print(f"conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
    print(f"AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
    print(f"ëª¨ë¸ í´ë” ì¡´ì¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")
    print("="*60)
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    print("ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    print(f"  PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
    print(f"  NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
    print(f"  PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
    print(f"  psutil: {'âœ…' if PSUTIL_AVAILABLE else 'âŒ'}")
    print("="*60)

def test_step_interface(step_name: str = "HumanParsingStep"):
    """Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {step_name} ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        interface = get_step_model_interface(step_name)
        print(f"âœ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {type(interface).__name__}")
        
        # ëª¨ë¸ ëª©ë¡ í…ŒìŠ¤íŠ¸
        models = interface.list_available_models()
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(models)}ê°œ")
        for i, model in enumerate(models[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            print(f"   {i+1}. {model}")
        if len(models) > 3:
            print(f"   ... ë° {len(models)-3}ê°œ ë”")
        
        # í†µê³„ í™•ì¸
        stats = interface.get_stats()
        print(f"âœ… ì¸í„°í˜ì´ìŠ¤ í†µê³„: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_data_converter():
    """ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ”„ ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ë°ì´í„° ë³€í™˜ê¸° ìƒì„± í…ŒìŠ¤íŠ¸
        converter = get_step_data_converter()
        print(f"âœ… ë°ì´í„° ë³€í™˜ê¸° ìƒì„±: {type(converter).__name__}")
        
        # ë³€í™˜ í†µê³„ í™•ì¸
        stats = converter.get_stats()
        print(f"âœ… ë³€í™˜ê¸° í†µê³„:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        # ë³€í™˜ í…ŒìŠ¤íŠ¸ (ê°€ìƒ ë°ì´í„°)
        if PIL_AVAILABLE:
            from PIL import Image
            import numpy as np
            
            # ê°€ìƒ ì´ë¯¸ì§€ ìƒì„±
            test_array = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
            test_image = Image.fromarray(test_array)
            
            # PIL -> í…ì„œ ë³€í™˜ í…ŒìŠ¤íŠ¸
            tensor = converter.pil_to_tensor(test_image)
            print(f"âœ… PIL->í…ì„œ ë³€í™˜ í…ŒìŠ¤íŠ¸: {type(tensor)}")
            
            # í…ì„œ -> PIL ë³€í™˜ í…ŒìŠ¤íŠ¸
            converted_back = converter.tensor_to_pil(tensor)
            print(f"âœ… í…ì„œ->PIL ë³€í™˜ í…ŒìŠ¤íŠ¸: {type(converted_back)}")
        else:
            print("âš ï¸ PIL ì—†ìŒ - ë³€í™˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_memory_manager():
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± í…ŒìŠ¤íŠ¸
        memory_manager = get_step_memory_manager()
        print(f"âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±: {type(memory_manager).__name__}")
        
        # ë©”ëª¨ë¦¬ í†µê³„ í™•ì¸
        stats = memory_manager.get_memory_stats()
        print(f"âœ… ë©”ëª¨ë¦¬ í†µê³„:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        # ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
        success = memory_manager.allocate_memory("TestStep", 1.0)
        print(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸: {success}")
        
        # ë©”ëª¨ë¦¬ í•´ì œ í…ŒìŠ¤íŠ¸
        memory_manager.deallocate_memory("TestStep")
        print(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ í…ŒìŠ¤íŠ¸: ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def validate_github_compatibility():
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦"""
    print("\nğŸ”— GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦")
    print("-" * 50)
    
    results = {}
    
    # 1. main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸
    try:
        interface = get_step_model_interface("HumanParsingStep")
        results["get_step_model_interface"] = "âœ…"
    except Exception as e:
        results["get_step_model_interface"] = f"âŒ {e}"
    
    # 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ì í•¨ìˆ˜ í™•ì¸ (ì˜¤ë¥˜ í•´ê²°)
    try:
        memory_manager = get_step_memory_manager()
        results["get_step_memory_manager"] = "âœ…"
    except Exception as e:
        results["get_step_memory_manager"] = f"âŒ {e}"
    
    # 3. StepModelInterface ë©”ì„œë“œ í™•ì¸
    try:
        interface = get_step_model_interface("ClothSegmentationStep")
        models = interface.list_available_models()
        results["list_available_models"] = "âœ…"
    except Exception as e:
        results["list_available_models"] = f"âŒ {e}"
    
    # 4. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸
    steps = list_available_steps()
    if len(steps) == 8:
        results["8_step_pipeline"] = "âœ…"
    else:
        results["8_step_pipeline"] = f"âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›"
    
    # 5. conda í™˜ê²½ ìµœì í™” í™•ì¸
    if is_conda_environment():
        results["conda_optimization"] = "âœ…"
    else:
        results["conda_optimization"] = "âš ï¸ conda í™˜ê²½ ì•„ë‹˜"
    
    # 6. AI ëª¨ë¸ ê²½ë¡œ í™•ì¸
    ai_path = get_ai_models_path()
    if ai_path.exists():
        results["ai_models_path"] = "âœ…"
    else:
        results["ai_models_path"] = f"âš ï¸ {ai_path} ì—†ìŒ"
    
    # 8. ë°ì´í„° ë³€í™˜ê¸° í•¨ìˆ˜ í™•ì¸ (ì˜¤ë¥˜ í•´ê²°)
    try:
        data_converter = get_step_data_converter()
        results["get_step_data_converter"] = "âœ…"
    except Exception as e:
        results["get_step_data_converter"] = f"âŒ {e}"
    
    # 9. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í™•ì¸
    try:
        config = create_model_config("test_model")
        if validate_step_name("HumanParsingStep"):
            results["utility_functions"] = "âœ…"
        else:
            results["utility_functions"] = "âŒ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì˜¤ë¥˜"
    except Exception as e:
        results["utility_functions"] = f"âŒ {e}"
    
    # ê²°ê³¼ ì¶œë ¥
    for test, result in results.items():
        print(f"  {test}: {result}")
    
    # ì „ì²´ ì ìˆ˜
    success_count = sum(1 for r in results.values() if r.startswith("âœ…"))
    total_count = len(results)
    score = (success_count / total_count) * 100
    
    print(f"\nğŸ“Š í˜¸í™˜ì„± ì ìˆ˜: {score:.1f}% ({success_count}/{total_count})")
    
    return score >= 80  # 80% ì´ìƒì´ë©´ ì„±ê³µ

async def test_async_operations():
    """ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ë§¤ë‹ˆì € ì´ˆê¸°í™”
        manager = get_utils_manager()
        init_result = await manager.initialize()
        print(f"âœ… ë§¤ë‹ˆì € ì´ˆê¸°í™”: {init_result['success']}")
        
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        interface = get_step_model_interface("VirtualFittingStep")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        model = await interface.get_model()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {model is not None}")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        memory_result = await manager.optimize_memory()
        print(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™”: {memory_result['success']}")
        
        # í†µí•© ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        unified_interface = create_unified_interface("PostProcessingStep")
        unified_model = await unified_interface.get_model()
        print(f"âœ… í†µí•© ì¸í„°í˜ì´ìŠ¤: {unified_model is not None}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_all_functionality():
    """ëª¨ë“  ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ì „ì²´ ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    test_results = []
    
    # 1. ì‹œìŠ¤í…œ ì •ë³´ í…ŒìŠ¤íŠ¸
    debug_system_info()
    test_results.append(("ì‹œìŠ¤í…œ ì •ë³´", True))
    
    # 2. Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    for step in ["HumanParsingStep", "VirtualFittingStep", "PostProcessingStep"]:
        result = test_step_interface(step)
        test_results.append((f"{step} ì¸í„°í˜ì´ìŠ¤", result))
    
    # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
    memory_result = test_memory_manager()
    test_results.append(("ë©”ëª¨ë¦¬ ê´€ë¦¬ì", memory_result))
    
    # 4. ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸
    converter_result = test_data_converter()
    test_results.append(("ë°ì´í„° ë³€í™˜ê¸°", converter_result))
    
    # 5. GitHub í˜¸í™˜ì„± ê²€ì¦
    compatibility_result = validate_github_compatibility()
    test_results.append(("GitHub í˜¸í™˜ì„±", compatibility_result))
    
    # 6. ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸
    try:
        async_result = asyncio.run(test_async_operations())
        test_results.append(("ë¹„ë™ê¸° ì‘ì—…", async_result))
    except Exception as e:
        print(f"âš ï¸ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€: {e}")
        test_results.append(("ë¹„ë™ê¸° ì‘ì—…", False))
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("-" * 40)
    passed = 0
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    total_score = (passed / len(test_results)) * 100
    print(f"\nğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ì ìˆ˜: {total_score:.1f}% ({passed}/{len(test_results)})")
    
    if total_score >= 80:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        return True
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return False

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    print("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v7.0")
    print("=" * 60)
    print("ğŸ“‹ ì™„ì „í•œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    # ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = test_all_functionality()
    
    if success:
        print("\nğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! main.pyì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\nğŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
        print("from app.ai_pipeline.utils import get_step_model_interface, get_step_memory_manager, get_step_data_converter")
        print("interface = get_step_model_interface('HumanParsingStep')")
        print("memory_manager = get_step_memory_manager()")
        print("data_converter = get_step_data_converter()")
        print("models = interface.list_available_models()")
        print("\nğŸ”§ ì¶”ê°€ ê¸°ëŠ¥:")
        print("from app.ai_pipeline.utils import create_unified_interface, optimize_system_memory")
        print("unified = create_unified_interface('VirtualFittingStep')")
        print("await optimize_system_memory()")
    else:
        print("\nâš ï¸ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    return success

if __name__ == "__main__":
    main()