# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v6.2 - GitHub ì™„ì „ í˜¸í™˜
================================================================================
âœ… ê¸°ì¡´ main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… get_step_model_interface í•¨ìˆ˜ ì™„ë²½ êµ¬í˜„
âœ… StepModelInterface.list_available_models í¬í•¨
âœ… BaseStepMixin ì˜ì¡´ì„± ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… M3 Max 128GB ìµœì í™” (conda í™˜ê²½ ìš°ì„ )
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜
âœ… ëª¨ë“  í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥

main.py í˜¸ì¶œ íŒ¨í„´:
from app.ai_pipeline.utils import get_step_model_interface
interface = get_step_model_interface("HumanParsingStep")
models = interface.list_available_models()
"""

import os
import sys
import logging
import threading
import asyncio
import time
import gc
import weakref
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from abc import ABC, abstractmethod

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° ì„¤ì • (GitHub í”„ë¡œì íŠ¸ ë°˜ì˜)
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (í•œë²ˆë§Œ ì‹¤í–‰) - conda í™˜ê²½ ìš°ì„ """
    try:
        import platform
        
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ
        }
        
        # M3 Max ê°ì§€ (GitHub í”„ë¡œì íŠ¸ ìµœì í™” ëŒ€ìƒ)
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        else:
            system_info["memory_gb"] = 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€ (M3 Max ìš°ì„ )
        device = "cpu"
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available() and is_m3_max:
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        
        # AI ëª¨ë¸ ê²½ë¡œ ê°ì§€
        project_root = Path(__file__).parent.parent.parent.parent
        ai_models_path = project_root / "ai_models"
        system_info["ai_models_path"] = str(ai_models_path)
        system_info["ai_models_exists"] = ai_models_path.exists()
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8.0",
            "conda_env": "base",
            "in_conda": False,
            "ai_models_path": "./ai_models",
            "ai_models_exists": False
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"        # ê¸°ì¡´ ë°©ì‹ (v3.0)
    UNIFIED = "unified"      # ìƒˆë¡œìš´ í†µí•© ë°©ì‹ (v6.0)
    HYBRID = "hybrid"        # í˜¼í•© ë°©ì‹
    FALLBACK = "fallback"    # í´ë°± ëª¨ë“œ

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • - conda í™˜ê²½ ìµœì í™”"""
    device: str = "auto"
    memory_gb: float = 16.0
    is_m3_max: bool = False
    optimization_enabled: bool = True
    max_workers: int = 4
    cache_enabled: bool = True
    debug_mode: bool = False
    conda_optimized: bool = True
    model_precision: str = "fp16"  # M3 Maxì—ì„œ fp16 ê¸°ë³¸
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if self.device == "auto":
            self.device = SYSTEM_INFO["device"]
        if self.is_m3_max and self.conda_optimized:
            self.model_precision = "fp16"
            self.max_workers = min(8, SYSTEM_INFO["cpu_count"])

@dataclass
class StepConfig:
    """Step ì„¤ì • (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í‘œì¤€)"""
    step_name: str
    step_number: Optional[int] = None
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_class: Optional[str] = None
    input_size: Tuple[int, int] = (512, 512)
    device: str = "auto"
    precision: str = "fp16"
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Step ë²ˆí˜¸ ìë™ ì¶”ì¶œ"""
        if self.step_number is None and "Step" in self.step_name:
            try:
                # HumanParsingStep -> step_01_human_parsing
                import re
                match = re.search(r'(\d+)', self.step_name)
                if match:
                    self.step_number = int(match.group(1))
                else:
                    # Step ì´ë¦„ì—ì„œ ìˆœì„œ ì¶”ì¶œ
                    step_mapping = {
                        "HumanParsingStep": 1,
                        "PoseEstimationStep": 2,
                        "ClothSegmentationStep": 3,
                        "GeometricMatchingStep": 4,
                        "ClothWarpingStep": 5,
                        "VirtualFittingStep": 6,
                        "PostProcessingStep": 7,
                        "QualityAssessmentStep": 8
                    }
                    self.step_number = step_mapping.get(self.step_name, 0)
            except:
                self.step_number = 0

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (GitHub ai_models í´ë” í‘œì¤€)"""
    name: str
    path: str
    model_type: str
    file_size_mb: float
    confidence_score: float = 1.0
    step_compatibility: List[str] = field(default_factory=list)
    architecture: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ StepModelInterface í´ë˜ìŠ¤ (main.py í˜¸í™˜)
# ==============================================

class StepModelInterface:
    """
    ğŸ”— Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤)
    âœ… get_model() ë©”ì„œë“œ ì œê³µ
    âœ… list_available_models() ë©”ì„œë“œ ì œê³µ
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ë‚´ì¥
    """
    
    def __init__(self, step_name: str, model_loader_instance: Optional[Any] = None):
        self.step_name = step_name
        self.model_loader = model_loader_instance
        self.logger = logging.getLogger(f"interface.{step_name}")
        
        # ìƒíƒœ ê´€ë¦¬
        self._models_cache = {}
        self._last_request_time = None
        self._request_count = 0
        self._initialization_attempted = False
        
        # Stepë³„ ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
        self._default_models = {
            "HumanParsingStep": ["graphonomy", "human_parsing_atr", "parsing_lip"],
            "PoseEstimationStep": ["openpose", "mediapipe", "yolov8_pose"],
            "ClothSegmentationStep": ["u2net", "cloth_segmentation", "deeplabv3"],
            "GeometricMatchingStep": ["geometric_matching", "tps_transformation"],
            "ClothWarpingStep": ["cloth_warping", "spatial_transformer"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "virtual_tryon"],
            "PostProcessingStep": ["image_enhancement", "artifact_removal"],
            "QualityAssessmentStep": ["clipiqa", "quality_assessment", "brisque"]
        }
        
        self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”")
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """
        ğŸ”¥ ëª¨ë¸ ë¡œë“œ (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ)
        """
        try:
            self._request_count += 1
            self._last_request_time = time.time()
            
            # ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
            if not model_name:
                available_models = self.list_available_models()
                if available_models:
                    model_name = available_models[0]
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                    return None
            
            # ìºì‹œ í™•ì¸
            if model_name in self._models_cache:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self._models_cache[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ë¡œë“œ ì‹œë„
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                try:
                    model = await self._safe_model_load(model_name)
                    if model:
                        self._models_cache[model_name] = model
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        return model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoaderë¥¼ í†µí•œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹œë„ (í´ë°±)
            model = await self._direct_model_load(model_name)
            if model:
                self._models_cache[model_name] = model
                return model
            
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ìµœì¢… í´ë°±)
            return self._create_simulation_model(model_name)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def _safe_model_load(self, model_name: str) -> Optional[Any]:
        """ModelLoaderë¥¼ í†µí•œ ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if hasattr(self.model_loader, 'get_model'):
                # ë™ê¸° ë©”ì„œë“œì¸ ê²½ìš°
                if asyncio.iscoroutinefunction(self.model_loader.get_model):
                    return await self.model_loader.get_model(model_name)
                else:
                    return self.model_loader.get_model(model_name)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _direct_model_load(self, model_name: str) -> Optional[Any]:
        """ì§ì ‘ ëª¨ë¸ ë¡œë“œ (ai_models í´ë”ì—ì„œ)"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            if not ai_models_path.exists():
                return None
            
            # ëª¨ë¸ íŒŒì¼ íƒìƒ‰
            model_patterns = [
                f"{model_name}.pth",
                f"{model_name}.pt",
                f"{model_name}.ckpt",
                f"{model_name}.safetensors"
            ]
            
            for pattern in model_patterns:
                model_file = ai_models_path / pattern
                if model_file.exists():
                    # ì‹¤ì œ ëª¨ë¸ ë¡œë“œëŠ” ì—¬ê¸°ì„œ êµ¬í˜„
                    self.logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_file}")
                    return ModelInfo(
                        name=model_name,
                        path=str(model_file),
                        model_type=f"{self.step_name}_model",
                        file_size_mb=model_file.stat().st_size / (1024*1024),
                        step_compatibility=[self.step_name]
                    )
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_simulation_model(self, model_name: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        return {
            "name": model_name,
            "type": "simulation",
            "step_name": self.step_name,
            "created_at": time.time(),
            "simulate": True,
            "device": SYSTEM_INFO["device"],
            "precision": "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32"
        }
    
    def list_available_models(self) -> List[str]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ)
        """
        try:
            available_models = []
            
            # 1. Stepë³„ ê¸°ë³¸ ëª¨ë¸ë“¤
            default_models = self._default_models.get(self.step_name, [])
            available_models.extend(default_models)
            
            # 2. ai_models í´ë” ìŠ¤ìº”
            try:
                ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
                if ai_models_path.exists():
                    # Stepë³„ í´ë” í™•ì¸
                    step_folder = ai_models_path / self.step_name.lower().replace("step", "")
                    if step_folder.exists():
                        for model_file in step_folder.glob("*.{pth,pt,ckpt,safetensors}"):
                            model_name = model_file.stem
                            if model_name not in available_models:
                                available_models.append(model_name)
                    
                    # ë£¨íŠ¸ í´ë”ì—ì„œë„ í™•ì¸
                    for model_file in ai_models_path.glob("*.{pth,pt,ckpt,safetensors}"):
                        model_name = model_file.stem
                        if self.step_name.lower() in model_name.lower():
                            if model_name not in available_models:
                                available_models.append(model_name)
            
            except Exception as e:
                self.logger.debug(f"ai_models í´ë” ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            
            # 3. ModelLoaderì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            if self.model_loader and hasattr(self.model_loader, 'list_models'):
                try:
                    loader_models = self.model_loader.list_models(self.step_name)
                    if loader_models:
                        for model in loader_models:
                            if model not in available_models:
                                available_models.append(model)
                except Exception as e:
                    self.logger.debug(f"ModelLoader ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            available_models = sorted(list(set(available_models)))
            
            self.logger.info(f"ğŸ“‹ {self.step_name} ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(available_models)}ê°œ")
            return available_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._default_models.get(self.step_name, [])
    
    async def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            self._models_cache.clear()
            gc.collect()
            
            if TORCH_AVAILABLE and SYSTEM_INFO["device"] == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            elif TORCH_AVAILABLE and SYSTEM_INFO["device"] == "cuda":
                torch.cuda.empty_cache()
            
            self.logger.info(f"ğŸ—‘ï¸ {self.step_name} ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ í†µê³„"""
        return {
            "step_name": self.step_name,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "cached_models": len(self._models_cache),
            "has_model_loader": self.model_loader is not None,
            "available_models_count": len(self.list_available_models())
        }

# ==============================================
# ğŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

class UnifiedUtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € v6.2
    âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ ë°˜ì˜
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.system_config = SystemConfig(
            device=SYSTEM_INFO["device"],
            memory_gb=SYSTEM_INFO["memory_gb"],
            is_m3_max=SYSTEM_INFO["is_m3_max"],
            max_workers=min(SYSTEM_INFO["cpu_count"], 8),
            conda_optimized=SYSTEM_INFO["in_conda"]
        )
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ì¥ì†Œ (ì•½í•œ ì°¸ì¡°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces = {}  # StepModelInterface ì €ì¥
        self._model_cache = {}
        self._service_cache = weakref.WeakValueDictionary()
        
        # í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0,
            "conda_optimizations": 0
        }
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        
        # conda í™˜ê²½ ìµœì í™”
        if SYSTEM_INFO["in_conda"]:
            self._setup_conda_optimizations()
        
        self._initialized = True
        self.logger.info(f"ğŸ¯ UnifiedUtilsManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (conda: {SYSTEM_INFO['in_conda']})")
    
    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            # conda í™˜ê²½ì—ì„œ PyTorch ìµœì í™”
            if TORCH_AVAILABLE:
                # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
                torch.set_num_threads(self.system_config.max_workers)
                
                # M3 Max MPS ìµœì í™”
                if SYSTEM_INFO["is_m3_max"]:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                    })
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                # condaì—ì„œ ì„¤ì¹˜ëœ OpenBLAS/MKL í™œìš©
                os.environ['OMP_NUM_THREADS'] = str(self.system_config.max_workers)
                os.environ['MKL_NUM_THREADS'] = str(self.system_config.max_workers)
            
            self.stats["conda_optimizations"] += 1
            self.logger.info("âœ… conda í™˜ê²½ ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™” - GitHub í”„ë¡œì íŠ¸ ìµœì í™”"""
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        try:
            start_time = time.time()
            self.logger.info("ğŸš€ UnifiedUtilsManager ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.system_config, key):
                    setattr(self.system_config, key, value)
            
            # M3 Max + conda íŠ¹ë³„ ìµœì í™”
            if self.system_config.is_m3_max and self.system_config.conda_optimized:
                await self._optimize_m3_max_conda()
            
            # ModelLoader ì—°ë™ ì‹œë„
            await self._try_initialize_model_loader()
            
            # AI ëª¨ë¸ ê²½ë¡œ í™•ì¸
            await self._verify_ai_models_path()
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            self.logger.info(f"ğŸ‰ UnifiedUtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_config": self.system_config,
                "system_info": SYSTEM_INFO,
                "conda_optimized": self.system_config.conda_optimized
            }
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _optimize_m3_max_conda(self):
        """M3 Max + conda íŠ¹ë³„ ìµœì í™”"""
        try:
            if TORCH_AVAILABLE:
                # M3 Max MPS ë°±ì—”ë“œ ìµœì í™”
                if torch.backends.mps.is_available():
                    torch.mps.set_per_process_memory_fraction(0.8)  # 128GBì˜ 80% í™œìš©
                
                # FP16 ê¸°ë³¸ ì„¤ì •
                if hasattr(torch, 'set_default_dtype'):
                    torch.set_default_dtype(torch.float16)
            
            self.logger.info("âœ… M3 Max + conda íŠ¹ë³„ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def _try_initialize_model_loader(self):
        """ModelLoader ì´ˆê¸°í™” ì‹œë„"""
        try:
            # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import
            from app.ai_pipeline.utils.model_loader import get_global_model_loader
            self.model_loader = get_global_model_loader()
            
            if self.model_loader:
                self.logger.info("âœ… ModelLoader ì—°ë™ ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    async def _verify_ai_models_path(self):
        """AI ëª¨ë¸ ê²½ë¡œ í™•ì¸ ë° ìƒì„±"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            
            if not ai_models_path.exists():
                ai_models_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ AI ëª¨ë¸ í´ë” ìƒì„±: {ai_models_path}")
            
            # Stepë³„ í•˜ìœ„ í´ë” ìƒì„±
            step_folders = [
                "human_parsing", "pose_estimation", "cloth_segmentation",
                "geometric_matching", "cloth_warping", "virtual_fitting",
                "post_processing", "quality_assessment"
            ]
            
            for folder in step_folders:
                folder_path = ai_models_path / folder
                folder_path.mkdir(exist_ok=True)
            
            self.logger.info("âœ… AI ëª¨ë¸ í´ë” êµ¬ì¡° í™•ì¸ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ í´ë” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str, **options) -> 'UnifiedStepInterface':
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹)"""
        try:
            with self._interface_lock:
                # ìºì‹œ í™•ì¸
                cache_key = f"{step_name}_{hash(str(options))}" if options else step_name
                
                if cache_key in self._step_interfaces:
                    self.logger.debug(f"ğŸ“‹ {step_name} ìºì‹œëœ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜")
                    return self._step_interfaces[cache_key]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                step_config = self._create_step_config(step_name, **options)
                interface = UnifiedStepInterface(self, step_config)
                
                # ìºì‹œ ì €ì¥
                self._step_interfaces[cache_key] = interface
                
                self.stats["interfaces_created"] += 1
                self.logger.info(f"ğŸ”— {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return self._create_fallback_interface(step_name)
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """
        ğŸ”¥ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
        """
        try:
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            interface = StepModelInterface(step_name, getattr(self, 'model_loader', None))
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return StepModelInterface(step_name, None)
    
    def _create_step_config(self, step_name: str, **options) -> StepConfig:
        """Step ì„¤ì • ìƒì„± (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê¸°ì¤€)"""
        # GitHub í”„ë¡œì íŠ¸ì˜ 8ë‹¨ê³„ë³„ ê¸°ë³¸ ì„¤ì •
        step_defaults = {
            "HumanParsingStep": {
                "model_name": "graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512),
                "step_number": 1
            },
            "PoseEstimationStep": {
                "model_name": "openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368),
                "step_number": 2
            },
            "ClothSegmentationStep": {
                "model_name": "u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320),
                "step_number": 3
            },
            "GeometricMatchingStep": {
                "model_name": "geometric_matching",
                "model_type": "GeometricMatchingModel",
                "input_size": (256, 192),
                "step_number": 4
            },
            "ClothWarpingStep": {
                "model_name": "cloth_warping",
                "model_type": "ClothWarpingModel",
                "input_size": (256, 192),
                "step_number": 5
            },
            "VirtualFittingStep": {
                "model_name": "ootdiffusion",
                "model_type": "OOTDiffusionModel",
                "input_size": (512, 512),
                "step_number": 6
            },
            "PostProcessingStep": {
                "model_name": "post_processing",
                "model_type": "PostProcessingModel",
                "input_size": (512, 512),
                "step_number": 7
            },
            "QualityAssessmentStep": {
                "model_name": "clipiqa",
                "model_type": "CLIPIQAModel",
                "input_size": (224, 224),
                "step_number": 8
            }
        }
        
        defaults = step_defaults.get(step_name, {
            "model_name": f"{step_name.lower()}_model",
            "model_type": "BaseModel",
            "input_size": (512, 512),
            "step_number": 0
        })
        
        # ì„¤ì • ë³‘í•©
        config_data = {
            "step_name": step_name,
            "device": self.system_config.device,
            "precision": self.system_config.model_precision,
            **defaults,
            **options
        }
        
        return StepConfig(**config_data)
    
    def _create_fallback_interface(self, step_name: str) -> 'UnifiedStepInterface':
        """í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        fallback_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(self, fallback_config, is_fallback=True)
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max 128GB íŠ¹í™”)"""
        try:
            import gc
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.system_config.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                elif self.system_config.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬ (128GBì—ì„œë„ ì£¼ê¸°ì  ì •ë¦¬)
            if len(self._model_cache) > 20:  # M3 MaxëŠ” ë” ë§ì€ ëª¨ë¸ ìºì‹œ í—ˆìš©
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ëª¨ë¸ ì œê±°
                items_to_remove = list(self._model_cache.keys())[:10]
                for key in items_to_remove:
                    del self._model_cache[key]
                    
                self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(items_to_remove)}ê°œ ì œê±°")
            
            self.stats["memory_optimizations"] += 1
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent": round(vm.percent, 1),
                    "is_m3_max_optimized": self.system_config.is_m3_max
                }
            
            return {
                "success": True,
                "memory_info": memory_info,
                "collected_objects": collected,
                "cache_cleared": len(items_to_remove) if 'items_to_remove' in locals() else 0,
                "optimization_count": self.stats["memory_optimizations"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        memory_info = {}
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            memory_info = {
                "total_gb": round(vm.total / (1024**3), 1),
                "available_gb": round(vm.available / (1024**3), 1),
                "percent": round(vm.percent, 1)
            }
        
        return {
            "initialized": self.is_initialized,
            "initialization_time": self.initialization_time,
            "system_config": self.system_config,
            "system_info": SYSTEM_INFO,
            "stats": self.stats,
            "memory_info": memory_info,
            "cache_sizes": {
                "step_interfaces": len(self._step_interfaces),
                "model_interfaces": len(self._model_interfaces),
                "models": len(self._model_cache),
                "services": len(self._service_cache)
            },
            "conda_status": {
                "in_conda": SYSTEM_INFO["in_conda"],
                "conda_env": SYSTEM_INFO["conda_env"],
                "optimized": self.system_config.conda_optimized
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë¹„ë™ê¸° ê°œì„ """
        try:
            # ëª¨ë“  ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            for interface in self._model_interfaces.values():
                try:
                    await interface.unload_models()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self._step_interfaces.clear()
            self._model_interfaces.clear()
            self._model_cache.clear()
            self._service_cache.clear()
            self.is_initialized = False
            
            self.logger.info("âœ… UnifiedUtilsManager ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í†µí•© Step ì¸í„°í˜ì´ìŠ¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

class UnifiedStepInterface:
    """
    ğŸ”— í†µí•© Step ì¸í„°í˜ì´ìŠ¤
    âœ… GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì›
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    """
    
    def __init__(self, manager: UnifiedUtilsManager, config: StepConfig, is_fallback: bool = False):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # í†µê³„ ì¶”ì 
        self._request_count = 0
        self._last_request_time = None
        self._processing_time_total = 0.0
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            target_model = model_name or self.config.model_name
            if not target_model:
                self.logger.warning("ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•ŠìŒ")
                return None
            
            start_time = time.time()
            model = self.manager.get_or_load_model(target_model, self.config)
            processing_time = time.time() - start_time
            
            self._request_count += 1
            self._last_request_time = time.time()
            self._processing_time_total += processing_time
            self.manager.stats["total_requests"] += 1
            
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return await self.manager.optimize_memory()
    
    async def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (Stepë³„ íŠ¹í™”)"""
        try:
            if self.is_fallback:
                self.logger.warning(f"{self.config.step_name} í´ë°± ëª¨ë“œ - ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬")
                return {"success": True, "simulation": True, "step_number": self.config.step_number}
            
            start_time = time.time()
            self.logger.info(f"ğŸ¯ Step {self.config.step_number:02d} {self.config.step_name} ì²˜ë¦¬ ì‹œì‘")
            
            # GitHub í”„ë¡œì íŠ¸ 8ë‹¨ê³„ë³„ íŠ¹í™” ì²˜ë¦¬
            if self.config.step_number == 1:  # Human Parsing
                result = await self._process_human_parsing(image_data, **kwargs)
            elif self.config.step_number == 2:  # Pose Estimation
                result = await self._process_pose_estimation(image_data, **kwargs)
            elif self.config.step_number == 3:  # Cloth Segmentation
                result = await self._process_cloth_segmentation(image_data, **kwargs)
            elif self.config.step_number == 4:  # Geometric Matching
                result = await self._process_geometric_matching(image_data, **kwargs)
            elif self.config.step_number == 5:  # Cloth Warping
                result = await self._process_cloth_warping(image_data, **kwargs)
            elif self.config.step_number == 6:  # Virtual Fitting
                result = await self._process_virtual_fitting(image_data, **kwargs)
            elif self.config.step_number == 7:  # Post Processing
                result = await self._process_post_processing(image_data, **kwargs)
            elif self.config.step_number == 8:  # Quality Assessment
                result = await self._process_quality_assessment(image_data, **kwargs)
            else:
                result = await self._process_generic(image_data, **kwargs)
            
            processing_time = time.time() - start_time
            self._processing_time_total += processing_time
            
            if result:
                result.update({
                    "step_number": self.config.step_number,
                    "step_name": self.config.step_name,
                    "processing_time": processing_time,
                    "total_processing_time": self._processing_time_total
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _process_human_parsing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì¸ê°„ íŒŒì‹± ì²˜ë¦¬"""
        # ì‹¤ì œ êµ¬í˜„ì€ ê° Step í´ë˜ìŠ¤ì—ì„œ
        return {
            "success": True,
            "output_type": "human_mask",
            "body_parts": ["head", "torso", "arms", "legs"],
            "confidence": 0.95
        }
    
    async def _process_pose_estimation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "pose_keypoints",
            "keypoints_count": 17,
            "confidence": 0.92
        }
    
    async def _process_cloth_segmentation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì˜ìƒ ë¶„í•  ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "cloth_mask",
            "cloth_types": ["shirt", "pants", "dress"],
            "confidence": 0.88
        }
    
    async def _process_geometric_matching(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "transformation_matrix",
            "matching_points": 128,
            "confidence": 0.90
        }
    
    async def _process_cloth_warping(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì˜ìƒ ë³€í˜• ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "warped_cloth",
            "warp_quality": "high",
            "confidence": 0.87
        }
    
    async def _process_virtual_fitting(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ê°€ìƒ í”¼íŒ… ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "fitted_image",
            "fitting_quality": "high",
            "confidence": 0.93
        }
    
    async def _process_post_processing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "enhanced_image",
            "enhancements": ["color_correction", "artifact_removal"],
            "confidence": 0.89
        }
    
    async def _process_quality_assessment(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€"""
        return {
            "success": True,
            "output_type": "quality_score",
            "overall_score": 8.5,
            "metrics": {"sharpness": 0.9, "realism": 0.85, "artifacts": 0.1},
            "confidence": 0.91
        }
    
    async def _process_generic(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì¼ë°˜ ì²˜ë¦¬"""
        return {
            "success": True,
            "output_type": "processed_image",
            "generic_processing": True,
            "confidence": 0.8
        }
    
    def get_config(self) -> StepConfig:
        """ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            "step_name": self.config.step_name,
            "step_number": self.config.step_number,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "total_processing_time": self._processing_time_total,
            "average_processing_time": self._processing_time_total / max(self._request_count, 1),
            "is_fallback": self.is_fallback,
            "model_name": self.config.model_name
        }

# ==============================================
# ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (v3.0 ë°©ì‹)
    ê¸°ì¡´ Step í´ë˜ìŠ¤ë“¤ì´ ê³„ì† ì‚¬ìš© ê°€ëŠ¥
    """
    try:
        manager = get_utils_manager()
        unified_interface = manager.create_step_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v6.2-github-optimized",
            "has_unified_utils": True,
            "unified_interface": unified_interface,
            "conda_optimized": SYSTEM_INFO["in_conda"]
        }
        
        # ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ async wrapperë¡œ ì œê³µ
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        legacy_interface["get_model"] = get_model_wrapper
        legacy_interface["optimize_memory"] = unified_interface.optimize_memory
        legacy_interface["process_image"] = unified_interface.process_image
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "get_model": lambda: None,
            "optimize_memory": lambda: {"success": False},
            "process_image": lambda x, **k: None
        }

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    ğŸ”¥ main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… StepModelInterface ë°˜í™˜
    âœ… ë¹„ë™ê¸° ë©”ì„œë“œ í¬í•¨
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        if model_loader_instance is None:
            try:
                # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"âœ… ì „ì—­ ModelLoader íšë“: {step_name}")
            except ImportError as e:
                logger.warning(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ModelLoader íšë“ ì‹¤íŒ¨: {e}")
                model_loader_instance = None
        
        # UnifiedUtilsManagerë¥¼ í†µí•œ ìƒì„± ì‹œë„
        try:
            manager = get_utils_manager()
            interface = manager.create_step_model_interface(step_name)
            logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Manager)")
            return interface
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        interface = StepModelInterface(step_name, model_loader_instance)
        logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Direct)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

# ==============================================
# ğŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” (main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì )
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    try:
        manager = get_utils_manager()
        
        # conda í™˜ê²½ íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            kwargs.setdefault("conda_optimized", True)
            kwargs.setdefault("model_precision", "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32")
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
            future = asyncio.create_task(manager.initialize(**kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
            result = loop.run_until_complete(manager.initialize(**kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {"error": str(e), "system_info": SYSTEM_INFO}

async def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ - ë¹„ë™ê¸° ê°œì„ """
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.cleanup()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¶Œì¥)"""
    manager = get_utils_manager()
    return manager.create_step_interface(step_name, **options)

async def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸°"""
    manager = get_utils_manager()
    return await manager.optimize_memory()

def get_ai_models_path() -> Path:
    """AI ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return Path(SYSTEM_INFO["ai_models_path"])

def list_available_steps() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ Step ëª©ë¡ (GitHub 8ë‹¨ê³„ ê¸°ì¤€)"""
    return [
        "HumanParsingStep",
        "PoseEstimationStep", 
        "ClothSegmentationStep",
        "GeometricMatchingStep",
        "ClothWarpingStep",
        "VirtualFittingStep",
        "PostProcessingStep",
        "QualityAssessmentStep"
    ]

def is_conda_environment() -> bool:
    """conda í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["in_conda"]

def get_conda_info() -> Dict[str, Any]:
    """conda í™˜ê²½ ì •ë³´"""
    return {
        "in_conda": SYSTEM_INFO["in_conda"],
        "conda_env": SYSTEM_INFO["conda_env"],
        "conda_prefix": os.environ.get('CONDA_PREFIX'),
        "python_path": sys.executable
    }

# ==============================================
# ğŸ”¥ __all__ ì •ì˜ (GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜)
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface',
    'StepModelInterface',  # main.py í•„ìˆ˜
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ğŸ”§ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'reset_global_utils',
    
    # ğŸ”„ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
    'create_step_interface',          # ë ˆê±°ì‹œ í˜¸í™˜
    'create_unified_interface',       # ìƒˆë¡œìš´ ë°©ì‹
    'get_step_model_interface',       # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'optimize_system_memory',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° (GitHub í”„ë¡œì íŠ¸ íŠ¹í™”)
    'UtilsMode',
    'get_ai_models_path',
    'list_available_steps',
    'is_conda_environment',
    'get_conda_info'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ (GitHub í”„ë¡œì íŠ¸ í‘œì¤€)
# ==============================================

# í™˜ê²½ ì •ë³´ ë¡œê¹…
logger.info("=" * 80)
logger.info("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v6.2 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ í˜¸í™˜")
logger.info("âœ… get_step_model_interface í•¨ìˆ˜ êµ¬í˜„ (main.py í˜¸í™˜)")
logger.info("âœ… StepModelInterface.list_available_models í¬í•¨")
logger.info("âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›")
logger.info("âœ… conda í™˜ê²½ ìµœì í™”")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ ")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… ê¸°ì¡´ ì½”ë“œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
logger.info(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info(f"ğŸ conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
logger.info(f"ğŸ“ AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
logger.info("=" * 80)

# conda í™˜ê²½ë³„ ì¶”ê°€ ìµœì í™”
if SYSTEM_INFO["in_conda"]:
    logger.info("ğŸ conda í™˜ê²½ ê°ì§€ - ì¶”ê°€ ìµœì í™” í™œì„±í™”")
    if SYSTEM_INFO["is_m3_max"]:
        logger.info("ğŸ M3 Max + conda ì¡°í•© - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ")

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit

def cleanup_on_exit():
    """ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(reset_global_utils())
        loop.close()
        logger.info("ğŸ§¹ ì‹œìŠ¤í…œ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

atexit.register(cleanup_on_exit)

# ==============================================
# ğŸ”¥ ê°œë°œ/ë””ë²„ê·¸ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def debug_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ë””ë²„ê·¸ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ” MyCloset AI ì‹œìŠ¤í…œ ì •ë³´")
    print("="*60)
    print(f"í”Œë«í¼: {SYSTEM_INFO['platform']}")
    print(f"ì•„í‚¤í…ì²˜: {SYSTEM_INFO['machine']}")
    print(f"M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
    print(f"ë””ë°”ì´ìŠ¤: {SYSTEM_INFO['device']}")
    print(f"ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
    print(f"CPU ì½”ì–´: {SYSTEM_INFO['cpu_count']}")
    print(f"Python: {SYSTEM_INFO['python_version']}")
    print(f"conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
    print(f"AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
    print(f"ëª¨ë¸ í´ë” ì¡´ì¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")
    print("="*60)
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    print("ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    print(f"  PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
    print(f"  NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
    print(f"  PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
    print(f"  psutil: {'âœ…' if PSUTIL_AVAILABLE else 'âŒ'}")
    print("="*60)

def test_step_interface(step_name: str = "HumanParsingStep"):
    """Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {step_name} ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        interface = get_step_model_interface(step_name)
        print(f"âœ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {type(interface).__name__}")
        
        # ëª¨ë¸ ëª©ë¡ í…ŒìŠ¤íŠ¸
        models = interface.list_available_models()
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(models)}ê°œ")
        for i, model in enumerate(models[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            print(f"   {i+1}. {model}")
        if len(models) > 3:
            print(f"   ... ë° {len(models)-3}ê°œ ë”")
        
        # í†µê³„ í™•ì¸
        stats = interface.get_stats()
        print(f"âœ… ì¸í„°í˜ì´ìŠ¤ í†µê³„: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

async def test_async_operations():
    """ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        # ë§¤ë‹ˆì € ì´ˆê¸°í™”
        manager = get_utils_manager()
        init_result = await manager.initialize()
        print(f"âœ… ë§¤ë‹ˆì € ì´ˆê¸°í™”: {init_result['success']}")
        
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        interface = get_step_model_interface("VirtualFittingStep")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        model = await interface.get_model()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {model is not None}")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        memory_result = await manager.optimize_memory()
        print(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™”: {memory_result['success']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def validate_github_compatibility():
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦"""
    print("\nğŸ”— GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦")
    print("-" * 50)
    
    results = {}
    
    # 1. main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸
    try:
        interface = get_step_model_interface("HumanParsingStep")
        results["get_step_model_interface"] = "âœ…"
    except Exception as e:
        results["get_step_model_interface"] = f"âŒ {e}"
    
    # 2. StepModelInterface ë©”ì„œë“œ í™•ì¸
    try:
        interface = get_step_model_interface("ClothSegmentationStep")
        models = interface.list_available_models()
        results["list_available_models"] = "âœ…"
    except Exception as e:
        results["list_available_models"] = f"âŒ {e}"
    
    # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸
    steps = list_available_steps()
    if len(steps) == 8:
        results["8_step_pipeline"] = "âœ…"
    else:
        results["8_step_pipeline"] = f"âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›"
    
    # 4. conda í™˜ê²½ ìµœì í™” í™•ì¸
    if is_conda_environment():
        results["conda_optimization"] = "âœ…"
    else:
        results["conda_optimization"] = "âš ï¸ conda í™˜ê²½ ì•„ë‹˜"
    
    # 5. AI ëª¨ë¸ ê²½ë¡œ í™•ì¸
    ai_path = get_ai_models_path()
    if ai_path.exists():
        results["ai_models_path"] = "âœ…"
    else:
        results["ai_models_path"] = f"âš ï¸ {ai_path} ì—†ìŒ"
    
    # ê²°ê³¼ ì¶œë ¥
    for test, result in results.items():
        print(f"  {test}: {result}")
    
    # ì „ì²´ ì ìˆ˜
    success_count = sum(1 for r in results.values() if r.startswith("âœ…"))
    total_count = len(results)
    score = (success_count / total_count) * 100
    
    print(f"\nğŸ“Š í˜¸í™˜ì„± ì ìˆ˜: {score:.1f}% ({success_count}/{total_count})")
    
    return score >= 80  # 80% ì´ìƒì´ë©´ ì„±ê³µ

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    print("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v6.2")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    debug_system_info()
    
    # Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    test_step_interface("HumanParsingStep")
    test_step_interface("VirtualFittingStep")
    
    # GitHub í˜¸í™˜ì„± ê²€ì¦
    compatibility_ok = validate_github_compatibility()
    
    if compatibility_ok:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! GitHub í”„ë¡œì íŠ¸ì™€ ì™„ì „ í˜¸í™˜ë©ë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    try:
        import asyncio
        print("\nğŸ”„ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        asyncio.run(test_async_operations())
    except Exception as e:
        print(f"âš ï¸ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€: {e}")

if __name__ == "__main__":
    main()