# backend/app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v7.0 - ë‹¨ìˆœí™”ëœ í†µí•©
================================================================

âœ… ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… conda í™˜ê²½ 100% ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
âœ… ì‹¤íŒ¨ í—ˆìš©ì  ì„¤ê³„ (Fault Tolerant)
âœ… main.py í˜¸ì¶œ íŒ¨í„´ ì™„ì „ í˜¸í™˜

ì£¼ìš” ê¸°ëŠ¥:
- get_step_model_interface: Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- get_step_memory_manager: Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì œê³µ  
- get_step_data_converter: Stepë³„ ë°ì´í„° ë³€í™˜ê¸° ì œê³µ
- preprocess_image_for_step: Stepë³„ ì´ë¯¸ì§€ ì „ì²˜ë¦¬

ì‘ì„±ì: MyCloset AI Team
ë‚ ì§œ: 2025-07-23
ë²„ì „: v7.0.0 (Simplified Utility Integration)
"""

import logging
import threading
import sys
from typing import Dict, Any, Optional, List, Union, Callable, Type
from pathlib import Path
from functools import lru_cache
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# =============================================================================
# ğŸ”¥ ê¸°ë³¸ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# =============================================================================

logger = logging.getLogger(__name__)

# ìƒìœ„ íŒ¨í‚¤ì§€ì—ì„œ ì‹œìŠ¤í…œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
try:
    from ... import get_system_info, is_conda_environment, is_m3_max, get_device
    SYSTEM_INFO = get_system_info()
    IS_CONDA = is_conda_environment()
    IS_M3_MAX = is_m3_max()
    DEVICE = get_device()
    logger.info("âœ… ìƒìœ„ íŒ¨í‚¤ì§€ì—ì„œ ì‹œìŠ¤í…œ ì •ë³´ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ ìƒìœ„ íŒ¨í‚¤ì§€ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    SYSTEM_INFO = {'device': 'cpu', 'is_m3_max': False, 'memory_gb': 16.0}
    IS_CONDA = False
    IS_M3_MAX = False
    DEVICE = 'cpu'

# ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    import torch
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    np = None
    NUMPY_VERSION = "not_available"

try:
    from PIL import Image
    PIL_AVAILABLE = True
    PIL_VERSION = Image.__version__ if hasattr(Image, '__version__') else "unknown"
except ImportError:
    PIL_AVAILABLE = False
    Image = None
    PIL_VERSION = "not_available"

# =============================================================================
# ğŸ”¥ ë‹¨ìˆœí™”ëœ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
# =============================================================================

class SimpleStepModelInterface:
    """ë‹¨ìˆœí™”ëœ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"utils.model_interface.{step_name}")
        self._models_cache = {}
        self._lock = threading.Lock()
        
    def list_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” model_loaderì—ì„œ ê°€ì ¸ì˜´
            # í˜„ì¬ëŠ” ê¸°ë³¸ ëª¨ë¸ë“¤ ë°˜í™˜
            default_models = {
                'HumanParsingStep': [
                    {'name': 'SCHP', 'type': 'human_parsing', 'available': True},
                    {'name': 'Graphonomy', 'type': 'human_parsing', 'available': True}
                ],
                'PoseEstimationStep': [
                    {'name': 'OpenPose', 'type': 'pose_estimation', 'available': True},
                    {'name': 'YOLO-Pose', 'type': 'pose_estimation', 'available': True}
                ],
                'ClothSegmentationStep': [
                    {'name': 'U2Net', 'type': 'segmentation', 'available': True},
                    {'name': 'SAM', 'type': 'segmentation', 'available': True}
                ],
                'VirtualFittingStep': [
                    {'name': 'OOTDiffusion', 'type': 'diffusion', 'available': True},
                    {'name': 'IDM-VTON', 'type': 'virtual_tryon', 'available': True}
                ]
            }
            
            models = default_models.get(self.step_name, [])
            self.logger.debug(f"ğŸ“‹ {self.step_name} ëª¨ë¸ ëª©ë¡: {len(models)}ê°œ")
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def load_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        with self._lock:
            if model_name in self._models_cache:
                return self._models_cache[model_name]
            
            try:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì„œ ëª¨ë¸ì„ ë¡œë“œ
                # í˜„ì¬ëŠ” Mock ê°ì²´ ë°˜í™˜
                mock_model = {
                    'name': model_name,
                    'step': self.step_name,
                    'device': DEVICE,
                    'loaded': True
                }
                
                self._models_cache[model_name] = mock_model
                self.logger.info(f"âœ… {self.step_name} ëª¨ë¸ ë¡œë“œ: {model_name}")
                return mock_model
                
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({model_name}): {e}")
                return None
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        with self._lock:
            if model_name in self._models_cache:
                del self._models_cache[model_name]
                self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
                return True
            return False

# =============================================================================
# ğŸ”¥ ë‹¨ìˆœí™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
# =============================================================================

class SimpleStepMemoryManager:
    """ë‹¨ìˆœí™”ëœ Step ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"utils.memory_manager.{step_name}")
        self.memory_limit = SYSTEM_INFO.get('memory_gb', 16) * 0.8  # 80% ì‚¬ìš©
        
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            
            return {
                'rss_mb': memory_info.rss / 1024 / 1024,
                'vms_mb': memory_info.vms / 1024 / 1024,
                'percent': process.memory_percent()
            }
        except ImportError:
            self.logger.warning("âš ï¸ psutil ì—†ìŒ, ë©”ëª¨ë¦¬ ì •ë³´ ì‚¬ìš© ë¶ˆê°€")
            return {'error': 'psutil_not_available'}
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def optimize(self, aggressive: bool = False) -> bool:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            import gc
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            self.logger.debug(f"ğŸ§¹ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE and aggressive:
                if DEVICE == 'cuda' and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    self.logger.debug("ğŸ§¹ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬")
                elif DEVICE == 'mps' and torch.backends.mps.is_available():
                    # M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ (ì•ˆì „í•˜ê²Œ)
                    gc.collect()
                    self.logger.debug("ğŸ§¹ MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return False
    
    def check_memory_limit(self) -> bool:
        """ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸"""
        try:
            memory_usage = self.get_memory_usage()
            if 'percent' in memory_usage:
                return memory_usage['percent'] < (self.memory_limit * 10)  # 80% -> 8.0
            return True
        except:
            return True

# =============================================================================
# ğŸ”¥ ë‹¨ìˆœí™”ëœ ë°ì´í„° ë³€í™˜ê¸°
# =============================================================================

class SimpleStepDataConverter:
    """ë‹¨ìˆœí™”ëœ Step ë°ì´í„° ë³€í™˜ê¸°"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"utils.data_converter.{step_name}")
        
    def convert_image_format(self, image_data: Any, target_format: str = "RGB") -> Optional[Any]:
        """ì´ë¯¸ì§€ í¬ë§· ë³€í™˜"""
        try:
            if not PIL_AVAILABLE:
                self.logger.warning("âš ï¸ PIL ì—†ìŒ, ì´ë¯¸ì§€ ë³€í™˜ ë¶ˆê°€")
                return image_data
            
            # PIL Image ê°ì²´ì¸ ê²½ìš°
            if hasattr(image_data, 'convert'):
                return image_data.convert(target_format)
            
            # numpy ë°°ì—´ì¸ ê²½ìš°
            if NUMPY_AVAILABLE and isinstance(image_data, np.ndarray):
                if len(image_data.shape) == 3:
                    pil_image = Image.fromarray(image_data)
                    return pil_image.convert(target_format)
            
            # ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return image_data
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í¬ë§· ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image_data
    
    def resize_image(self, image_data: Any, size: tuple = (512, 512)) -> Optional[Any]:
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        try:
            if not PIL_AVAILABLE:
                return image_data
            
            if hasattr(image_data, 'resize'):
                return image_data.resize(size, Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)
            
            return image_data
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {e}")
            return image_data
    
    def normalize_data(self, data: Any) -> Optional[Any]:
        """ë°ì´í„° ì •ê·œí™”"""
        try:
            if NUMPY_AVAILABLE and isinstance(data, np.ndarray):
                # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                if data.dtype == np.uint8:
                    return data.astype(np.float32) / 255.0
                return data
            
            return data
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return data

# =============================================================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (main.py í˜¸ì¶œ íŒ¨í„´ ì™„ì „ í˜¸í™˜)
# =============================================================================

_interface_cache = {}
_memory_manager_cache = {}
_data_converter_cache = {}
_cache_lock = threading.Lock()

@lru_cache(maxsize=8)
def get_step_model_interface(step_name: str) -> SimpleStepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜ (main.py í˜¸ì¶œìš©)"""
    with _cache_lock:
        if step_name not in _interface_cache:
            _interface_cache[step_name] = SimpleStepModelInterface(step_name)
            logger.debug(f"ğŸ”§ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±")
        
        return _interface_cache[step_name]

@lru_cache(maxsize=8)
def get_step_memory_manager(step_name: str) -> SimpleStepMemoryManager:
    """Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜ (main.py í˜¸ì¶œìš©)"""
    with _cache_lock:
        if step_name not in _memory_manager_cache:
            _memory_manager_cache[step_name] = SimpleStepMemoryManager(step_name)
            logger.debug(f"ğŸ”§ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±")
        
        return _memory_manager_cache[step_name]

@lru_cache(maxsize=8)
def get_step_data_converter(step_name: str) -> SimpleStepDataConverter:
    """Stepë³„ ë°ì´í„° ë³€í™˜ê¸° ë°˜í™˜ (main.py í˜¸ì¶œìš©)"""
    with _cache_lock:
        if step_name not in _data_converter_cache:
            _data_converter_cache[step_name] = SimpleStepDataConverter(step_name)
            logger.debug(f"ğŸ”§ {step_name} ë°ì´í„° ë³€í™˜ê¸° ìƒì„±")
        
        return _data_converter_cache[step_name]

def preprocess_image_for_step(image_data: Any, step_name: str, **kwargs) -> Optional[Any]:
    """Stepë³„ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (main.py í˜¸ì¶œìš©)"""
    try:
        converter = get_step_data_converter(step_name)
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        processed_image = image_data
        
        # 1. í¬ë§· ë³€í™˜
        target_format = kwargs.get('format', 'RGB')
        processed_image = converter.convert_image_format(processed_image, target_format)
        
        # 2. í¬ê¸° ì¡°ì •
        target_size = kwargs.get('size', (512, 512))
        processed_image = converter.resize_image(processed_image, target_size)
        
        # 3. ì •ê·œí™” (ì˜µì…˜)
        if kwargs.get('normalize', False):
            processed_image = converter.normalize_data(processed_image)
        
        logger.debug(f"âœ… {step_name} ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ")
        return processed_image
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return image_data

# =============================================================================
# ğŸ”¥ ê³ ê¸‰ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì¶”ê°€ ê¸°ëŠ¥)
# =============================================================================

def clear_all_caches():
    """ëª¨ë“  ìºì‹œ ì´ˆê¸°í™”"""
    global _interface_cache, _memory_manager_cache, _data_converter_cache
    
    with _cache_lock:
        _interface_cache.clear()
        _memory_manager_cache.clear()  
        _data_converter_cache.clear()
        
        # @lru_cache ìºì‹œë„ ì´ˆê¸°í™”
        get_step_model_interface.cache_clear()
        get_step_memory_manager.cache_clear()
        get_step_data_converter.cache_clear()
        
        logger.info("ğŸ§¹ ëª¨ë“  ìœ í‹¸ë¦¬í‹° ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
    return {
        'system_info': SYSTEM_INFO,
        'conda_optimized': IS_CONDA,
        'm3_max_optimized': IS_M3_MAX,
        'device': DEVICE,
        'libraries': {
            'torch': {'available': TORCH_AVAILABLE, 'version': TORCH_VERSION},
            'numpy': {'available': NUMPY_AVAILABLE, 'version': NUMPY_VERSION},
            'pil': {'available': PIL_AVAILABLE, 'version': PIL_VERSION}
        },
        'cache_status': {
            'model_interfaces': len(_interface_cache),
            'memory_managers': len(_memory_manager_cache),
            'data_converters': len(_data_converter_cache)
        }
    }

def optimize_system_memory(aggressive: bool = False) -> bool:
    """ì‹œìŠ¤í…œ ì „ì²´ ë©”ëª¨ë¦¬ ìµœì í™”"""
    try:
        import gc
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        
        # ëª¨ë“  ë©”ëª¨ë¦¬ ê´€ë¦¬ìì—ì„œ ìµœì í™” ì‹¤í–‰
        success_count = 0
        for manager in _memory_manager_cache.values():
            if manager.optimize(aggressive):
                success_count += 1
        
        logger.info(f"ğŸ§¹ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ (ê°€ë¹„ì§€: {collected}, ê´€ë¦¬ì: {success_count})")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ”¥ ì•ˆì „í•œ ëª¨ë“ˆ ë¡œë”© (ê³ ê¸‰ ê¸°ëŠ¥ë“¤)
# =============================================================================

def _try_import_advanced_modules():
    """ê³ ê¸‰ ëª¨ë“ˆë“¤ ì•ˆì „í•˜ê²Œ import ì‹œë„"""
    advanced_status = {
        'model_loader': False,
        'auto_detector': False,
        'step_requirements': False
    }
    
    # ModelLoader ì‹œë„
    try:
        from .model_loader import ModelLoader
        globals()['ModelLoader'] = ModelLoader
        advanced_status['model_loader'] = True
        logger.info("âœ… ê³ ê¸‰ ModelLoader ë¡œë“œ ì„±ê³µ")
    except ImportError:
        logger.debug("ğŸ“‹ ê³ ê¸‰ ModelLoader ì—†ìŒ (ì •ìƒ)")
    
    # auto_model_detector ì‹œë„  
    try:
        from .auto_model_detector import detect_available_models
        globals()['detect_available_models'] = detect_available_models
        advanced_status['auto_detector'] = True
        logger.info("âœ… auto_model_detector ë¡œë“œ ì„±ê³µ")
    except ImportError:
        logger.debug("ğŸ“‹ auto_model_detector ì—†ìŒ (ì •ìƒ)")
    
    # step_model_requirements ì‹œë„
    try:
        from .step_model_requirements import StepModelRequestAnalyzer
        globals()['StepModelRequestAnalyzer'] = StepModelRequestAnalyzer
        advanced_status['step_requirements'] = True
        logger.info("âœ… step_model_requirements ë¡œë“œ ì„±ê³µ")
    except ImportError:
        logger.debug("ğŸ“‹ step_model_requirements ì—†ìŒ (ì •ìƒ)")
    
    return advanced_status

# ê³ ê¸‰ ëª¨ë“ˆë“¤ ë¡œë”© ì‹œë„
ADVANCED_STATUS = _try_import_advanced_modules()

# =============================================================================
# ğŸ”¥ Export ëª©ë¡ (main.py ì™„ì „ í˜¸í™˜)
# =============================================================================

__all__ = [
    # ğŸ¯ í•µì‹¬ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (main.pyì—ì„œ í˜¸ì¶œ)
    'get_step_model_interface',
    'get_step_memory_manager', 
    'get_step_data_converter',
    'preprocess_image_for_step',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
    'SimpleStepModelInterface',
    'SimpleStepMemoryManager',
    'SimpleStepDataConverter',
    
    # ğŸ› ï¸ ì‹œìŠ¤í…œ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'clear_all_caches',
    'get_system_status',
    'optimize_system_memory',
    
    # ğŸ“Š ìƒíƒœ ì •ë³´
    'SYSTEM_INFO',
    'IS_CONDA',
    'IS_M3_MAX', 
    'DEVICE',
    'TORCH_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'ADVANCED_STATUS'
]

# ê³ ê¸‰ ëª¨ë“ˆë“¤ ë™ì  ì¶”ê°€
if ADVANCED_STATUS['model_loader']:
    __all__.append('ModelLoader')
if ADVANCED_STATUS['auto_detector']:
    __all__.append('detect_available_models')  
if ADVANCED_STATUS['step_requirements']:
    __all__.append('StepModelRequestAnalyzer')

# =============================================================================
# ğŸ”¥ ì´ˆê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

def _print_initialization_summary():
    """ì´ˆê¸°í™” ìš”ì•½ ì¶œë ¥"""
    basic_utils = ['model_interface', 'memory_manager', 'data_converter', 'image_preprocessor']
    basic_count = len(basic_utils)
    
    advanced_count = sum(ADVANCED_STATUS.values())
    library_count = sum([TORCH_AVAILABLE, NUMPY_AVAILABLE, PIL_AVAILABLE])
    
    print(f"\nğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v7.0 ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"ğŸ”§ ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°: {basic_count}/4ê°œ âœ…")
    print(f"ğŸš€ ê³ ê¸‰ ëª¨ë“ˆ: {advanced_count}/3ê°œ")
    print(f"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬: {library_count}/3ê°œ (torch, numpy, PIL)")
    print(f"ğŸ conda í™˜ê²½: {'âœ…' if IS_CONDA else 'âŒ'}")
    print(f"ğŸ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {DEVICE}")
    
    # ê³ ê¸‰ ëª¨ë“ˆ ìƒíƒœ
    if advanced_count > 0:
        available_modules = [k for k, v in ADVANCED_STATUS.items() if v]
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ê³ ê¸‰ ëª¨ë“ˆ: {', '.join(available_modules)}")
    
    print("ğŸš€ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")

# ì´ˆê¸°í™” ìƒíƒœ ì¶œë ¥ (í•œ ë²ˆë§Œ)
if not hasattr(sys, '_mycloset_utils_initialized'):
    _print_initialization_summary()
    sys._mycloset_utils_initialized = True

logger.info("ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")