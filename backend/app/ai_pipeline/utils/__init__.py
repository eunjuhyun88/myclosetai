# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v5.0 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± (Dependency Injection)
âœ… ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • ì—†ì´ í•´ê²°
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›

ì‚¬ìš©ë²•:
1. ìƒˆë¡œìš´ Step: UnifiedStepInterface ì‚¬ìš©
2. ê¸°ì¡´ Step: ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜)
3. ì„œë¹„ìŠ¤: UnifiedServiceManager ì‚¬ìš©
"""

import os
import sys
import logging
import threading
import asyncio
import time
from typing import Dict, Any, Optional, List, Union, Callable, Type
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
import weakref

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° ì„¤ì •
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (í•œë²ˆë§Œ ì‹¤í–‰)"""
    try:
        import platform
        
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3]))
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        else:
            system_info["memory_gb"] = 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        device = "cpu"
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8.0"
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ğŸ”¥ í†µí•© ë°ì´í„° êµ¬ì¡° (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"        # ê¸°ì¡´ ë°©ì‹ (v3.0)
    UNIFIED = "unified"      # ìƒˆë¡œìš´ í†µí•© ë°©ì‹ (v5.0)
    HYBRID = "hybrid"        # í˜¼í•© ë°©ì‹

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì •"""
    device: str = "auto"
    memory_gb: float = 16.0
    is_m3_max: bool = False
    optimization_enabled: bool = True
    max_workers: int = 4
    cache_enabled: bool = True
    debug_mode: bool = False

@dataclass
class StepConfig:
    """Step ì„¤ì • (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë°ì´í„° ì „ìš©)"""
    step_name: str
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_class: Optional[str] = None
    input_size: tuple = (512, 512)
    device: str = "auto"
    precision: str = "fp16"
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë°ì´í„° ì „ìš©)"""
    name: str
    path: str
    model_type: str
    file_size_mb: float
    confidence_score: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (ì˜ì¡´ì„± ì£¼ì…)
# ==============================================

class UnifiedUtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì €
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
    âœ… ëª¨ë“  ê¸°ëŠ¥ í†µí•© ê´€ë¦¬
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ê¸°ë³¸ ì„¤ì •
        self.system_config = SystemConfig(
            device=SYSTEM_INFO["device"],
            memory_gb=SYSTEM_INFO["memory_gb"],
            is_m3_max=SYSTEM_INFO["is_m3_max"],
            max_workers=min(SYSTEM_INFO["cpu_count"], 8)
        )
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ì¥ì†Œ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_cache = {}
        self._service_cache = weakref.WeakValueDictionary()
        
        # í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0
        }
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        
        self._initialized = True
        self.logger.info("ğŸ¯ UnifiedUtilsManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™”"""
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        try:
            start_time = time.time()
            self.logger.info("ğŸš€ UnifiedUtilsManager ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.system_config, key):
                    setattr(self.system_config, key, value)
            
            # GPU ìµœì í™” ì„¤ì •
            if self.system_config.is_m3_max and TORCH_AVAILABLE:
                try:
                    # M3 Max ìµœì í™” í™˜ê²½ë³€ìˆ˜
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'OMP_NUM_THREADS': str(min(self.system_config.max_workers * 2, 16))
                    })
                    self.logger.info("âœ… M3 Max GPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            self.logger.info(f"ğŸ‰ UnifiedUtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_config": self.system_config,
                "system_info": SYSTEM_INFO
            }
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def create_step_interface(self, step_name: str, **options) -> 'UnifiedStepInterface':
        """
        Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹)
        ìˆœí™˜ì°¸ì¡° ì—†ì´ ëª¨ë“  ê¸°ëŠ¥ ì œê³µ
        """
        try:
            with self._interface_lock:
                # ìºì‹œ í™•ì¸
                cache_key = f"{step_name}_{hash(str(options))}" if options else step_name
                
                if cache_key in self._step_interfaces:
                    self.logger.debug(f"ğŸ“‹ {step_name} ìºì‹œëœ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜")
                    return self._step_interfaces[cache_key]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                step_config = self._create_step_config(step_name, **options)
                interface = UnifiedStepInterface(self, step_config)
                
                # ìºì‹œ ì €ì¥
                self._step_interfaces[cache_key] = interface
                
                self.stats["interfaces_created"] += 1
                self.logger.info(f"ğŸ”— {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return self._create_fallback_interface(step_name)
    
    def _create_step_config(self, step_name: str, **options) -> StepConfig:
        """Step ì„¤ì • ìƒì„±"""
        # Stepë³„ ê¸°ë³¸ ì„¤ì • (í•˜ë“œì½”ë”©ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        step_defaults = {
            "HumanParsingStep": {
                "model_name": "human_parsing_graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512)
            },
            "PoseEstimationStep": {
                "model_name": "pose_estimation_openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368)
            },
            "ClothSegmentationStep": {
                "model_name": "cloth_segmentation_u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320)
            },
            "VirtualFittingStep": {
                "model_name": "virtual_fitting_stable_diffusion",
                "model_type": "StableDiffusionPipeline",
                "input_size": (512, 512)
            }
        }
        
        defaults = step_defaults.get(step_name, {
            "model_name": f"{step_name.lower()}_model",
            "model_type": "BaseModel",
            "input_size": (512, 512)
        })
        
        # ì„¤ì • ë³‘í•©
        config_data = {
            "step_name": step_name,
            "device": self.system_config.device,
            "precision": "fp16" if self.system_config.is_m3_max else "fp32",
            **defaults,
            **options
        }
        
        return StepConfig(**config_data)
    
    def _create_fallback_interface(self, step_name: str) -> 'UnifiedStepInterface':
        """í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        fallback_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(self, fallback_config, is_fallback=True)
    
    def get_or_load_model(self, model_name: str, step_config: StepConfig) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ìºì‹œ í™œìš©)"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self._model_cache:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self._model_cache[model_name]
            
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œëŠ” ì—¬ê¸°ì„œ êµ¬í˜„
            # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
            model_info = ModelInfo(
                name=model_name,
                path=f"ai_models/{model_name}.pth",
                model_type=step_config.model_type or "BaseModel",
                file_size_mb=150.0
            )
            
            # ìºì‹œ ì €ì¥
            self._model_cache[model_name] = model_info
            self.stats["models_loaded"] += 1
            
            self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            return model_info
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            import gc
            gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.system_config.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                elif self.system_config.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ìºì‹œ ì •ë¦¬ (ì˜¤ë˜ëœ í•­ëª©)
            if len(self._model_cache) > 10:
                # ê°„ë‹¨í•œ LRU êµ¬í˜„
                items_to_remove = list(self._model_cache.keys())[:5]
                for key in items_to_remove:
                    del self._model_cache[key]
            
            self.stats["memory_optimizations"] += 1
            
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent": round(vm.percent, 1)
                }
            
            return {
                "success": True,
                "memory_info": memory_info,
                "cache_cleared": len(items_to_remove) if 'items_to_remove' in locals() else 0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        memory_info = {}
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            memory_info = {
                "total_gb": round(vm.total / (1024**3), 1),
                "available_gb": round(vm.available / (1024**3), 1),
                "percent": round(vm.percent, 1)
            }
        
        return {
            "initialized": self.is_initialized,
            "initialization_time": self.initialization_time,
            "system_config": self.system_config,
            "system_info": SYSTEM_INFO,
            "stats": self.stats,
            "memory_info": memory_info,
            "cache_sizes": {
                "step_interfaces": len(self._step_interfaces),
                "models": len(self._model_cache),
                "services": len(self._service_cache)
            }
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self._step_interfaces.clear()
            self._model_cache.clear()
            self._service_cache.clear()
            self.is_initialized = False
            self.logger.info("âœ… UnifiedUtilsManager ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í†µí•© Step ì¸í„°í˜ì´ìŠ¤
# ==============================================

class UnifiedStepInterface:
    """
    ğŸ”— í†µí•© Step ì¸í„°í˜ì´ìŠ¤
    âœ… ìˆœí™˜ì°¸ì¡° ì—†ìŒ
    âœ… ëª¨ë“  ê¸°ëŠ¥ ì œê³µ
    âœ… ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜
    """
    
    def __init__(self, manager: UnifiedUtilsManager, config: StepConfig, is_fallback: bool = False):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # í†µê³„ ì¶”ì 
        self._request_count = 0
        self._last_request_time = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            target_model = model_name or self.config.model_name
            if not target_model:
                self.logger.warning("ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•ŠìŒ")
                return None
            
            model = self.manager.get_or_load_model(target_model, self.config)
            
            self._request_count += 1
            self._last_request_time = time.time()
            self.manager.stats["total_requests"] += 1
            
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return self.manager.optimize_memory()
    
    def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (ê¸°ë³¸ êµ¬í˜„)"""
        try:
            if self.is_fallback:
                self.logger.warning(f"{self.config.step_name} í´ë°± ëª¨ë“œ - ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬")
                return {"success": True, "simulation": True}
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§ì€ ê° Stepì—ì„œ êµ¬í˜„
            self.logger.info(f"{self.config.step_name} ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬ (í¬ê¸° ì¡°ì • ë“±)
            if hasattr(image_data, 'resize'):
                processed_image = image_data.resize(self.config.input_size)
            else:
                processed_image = image_data
            
            return {
                "success": True,
                "processed_image": processed_image,
                "step_name": self.config.step_name,
                "processing_time": 0.1
            }
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def get_config(self) -> StepConfig:
        """ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            "step_name": self.config.step_name,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "is_fallback": self.is_fallback,
            "model_name": self.config.model_name
        }

# ==============================================
# ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (v3.0 ë°©ì‹)
    ê¸°ì¡´ Step í´ë˜ìŠ¤ë“¤ì´ ê³„ì† ì‚¬ìš© ê°€ëŠ¥
    """
    try:
        manager = get_utils_manager()
        unified_interface = manager.create_step_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v5.0-legacy-compatible",
            "has_unified_utils": True,
            "unified_interface": unified_interface
        }
        
        # ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ async wrapperë¡œ ì œê³µ
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        legacy_interface["get_model"] = get_model_wrapper
        legacy_interface["optimize_memory"] = unified_interface.optimize_memory
        legacy_interface["process_image"] = unified_interface.process_image
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "get_model": lambda: None,
            "optimize_memory": lambda: {"success": False},
            "process_image": lambda x, **k: None
        }

# ==============================================
# ğŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
    main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì 
    """
    try:
        manager = get_utils_manager()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
            future = asyncio.create_task(manager.initialize(**kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
            result = loop.run_until_complete(manager.initialize(**kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {"error": str(e), "system_info": SYSTEM_INFO}

def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹"""
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                _global_manager.cleanup()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¶Œì¥)"""
    manager = get_utils_manager()
    return manager.create_step_interface(step_name, **options)

def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™”"""
    manager = get_utils_manager()
    return manager.optimize_memory()

# ==============================================
# ğŸ”¥ __all__ ì •ì˜
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface',
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ğŸ”§ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'reset_global_utils',
    
    # ğŸ”„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    'create_step_interface',          # ë ˆê±°ì‹œ í˜¸í™˜
    'create_unified_interface',       # ìƒˆë¡œìš´ ë°©ì‹
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'optimize_system_memory',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹°
    'UtilsMode'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ
# ==============================================

logger.info("=" * 70)
logger.info("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v5.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… ê¸°ì¡´ ì½”ë“œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥")
logger.info("âœ… ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ì œê³µ")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
logger.info(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info("=" * 70)

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(reset_global_utils)