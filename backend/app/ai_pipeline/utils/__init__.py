# app/ai_pipeline/utils/__init__.py
"""
ðŸŽ MyCloset AI ì™„ì „í•œ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0 - ì „ë©´ ë¦¬íŒ©í† ë§
================================================================================
âœ… ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„ (ê¸°ëŠ¥ ëˆ„ë½ ì—†ìŒ)
âœ… get_step_memory_manager í•¨ìˆ˜ ì™„ì „ êµ¬í˜„
âœ… get_step_model_interface í•¨ìˆ˜ ì™„ì „ êµ¬í˜„
âœ… StepModelInterface.list_available_models ì™„ì „ í¬í•¨
âœ… conda í™˜ê²½ 100% ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©
âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… Clean Architecture ì ìš©
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  import ì˜¤ë¥˜ í•´ê²°
âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… GPU í˜¸í™˜ì„± ì™„ì „ ë³´ìž¥

main.py í˜¸ì¶œ íŒ¨í„´ (ì™„ì „ í˜¸í™˜):
from app.ai_pipeline.utils import get_step_model_interface, get_step_memory_manager
interface = get_step_model_interface("HumanParsingStep")
models = interface.list_available_models()
memory_manager = get_step_memory_manager()
"""

import os
import sys
import logging
import threading
import asyncio
import time
import gc
import weakref
import json
import hashlib
import shutil
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import lru_cache, wraps
from abc import ABC, abstractmethod
from contextlib import contextmanager, asynccontextmanager
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

# ì¡°ê±´ë¶€ ìž„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# ==============================================
# ðŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° í™˜ê²½ ê°ì§€ (ì™„ì „ êµ¬í˜„)
# ==============================================

@lru_cache(maxsize=1)
def _detect_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ì™„ì „ ê°ì§€ - conda í™˜ê²½ ìš°ì„ """
    try:
        import platform
        import subprocess
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ,
            "conda_prefix": os.environ.get('CONDA_PREFIX', ''),
            "virtual_env": os.environ.get('VIRTUAL_ENV', ''),
            "python_path": sys.executable
        }
        
        # M3 Max íŠ¹ë³„ ê°ì§€
        is_m3_max = False
        m3_info = {"detected": False, "model": "unknown", "cores": 0}
        
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                # CPU ë¸Œëžœë“œ í™•ì¸
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                brand = result.stdout.strip()
                if 'M3' in brand:
                    is_m3_max = True
                    m3_info = {
                        "detected": True,
                        "model": "M3 Max" if "Max" in brand else "M3",
                        "brand": brand
                    }
                
                # GPU ì½”ì–´ ìˆ˜ í™•ì¸
                try:
                    result = subprocess.run(
                        ['sysctl', '-n', 'hw.gpu.family_id'], 
                        capture_output=True, text=True, timeout=3
                    )
                    if result.returncode == 0:
                        m3_info["gpu_cores"] = 40 if "Max" in brand else 20
                except:
                    pass
                    
            except Exception as e:
                logger.debug(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
        
        system_info.update({
            "is_m3_max": is_m3_max,
            "m3_info": m3_info
        })
        
        # ë©”ëª¨ë¦¬ ì •ë³´ (ì •í™•í•œ ê°ì§€)
        memory_gb = 16.0  # ê¸°ë³¸ê°’
        if PSUTIL_AVAILABLE:
            try:
                vm = psutil.virtual_memory()
                memory_gb = round(vm.total / (1024**3), 1)
                system_info["memory_details"] = {
                    "total_gb": memory_gb,
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent_used": vm.percent
                }
            except Exception:
                pass
        
        system_info["memory_gb"] = memory_gb
        
        # GPU/ë””ë°”ì´ìŠ¤ ê°ì§€ (ì™„ì „ êµ¬í˜„)
        device_info = _detect_best_device(is_m3_max)
        system_info.update(device_info)
        
        # AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡° ë°˜ì˜)
        project_root = Path(__file__).parent.parent.parent.parent
        ai_models_path = project_root / "ai_models"
        
        system_info.update({
            "project_root": str(project_root),
            "ai_models_path": str(ai_models_path),
            "ai_models_exists": ai_models_path.exists(),
            "config_path": str(project_root / "backend" / "app" / "core"),
            "scripts_path": str(project_root / "scripts")
        })
        
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ì •ë³´
        system_info["libraries"] = {
            "torch": TORCH_VERSION,
            "numpy": NUMPY_VERSION if NUMPY_AVAILABLE else "not_available",
            "pillow": Image.VERSION if PIL_AVAILABLE else "not_available",
            "psutil": psutil.version_info if PSUTIL_AVAILABLE else "not_available"
        }
        
        return system_info
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "platform": "unknown",
            "machine": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "device_name": "CPU",
            "device_available": True,
            "cpu_count": 4,
            "memory_gb": 16.0,
            "python_version": "3.8.0",
            "conda_env": "base",
            "in_conda": False,
            "project_root": str(Path.cwd()),
            "ai_models_path": str(Path.cwd() / "ai_models"),
            "ai_models_exists": False,
            "libraries": {}
        }

def _detect_best_device(is_m3_max: bool = False) -> Dict[str, Any]:
    """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€ (M3 Max ìš°ì„ )"""
    device_info = {
        "device": "cpu",
        "device_name": "CPU",
        "device_available": True,
        "device_memory_gb": 0.0,
        "device_capabilities": []
    }
    
    if not TORCH_AVAILABLE:
        return device_info
    
    try:
        # M3 Max MPS ìš°ì„  (ìµœê³  ì„±ëŠ¥)
        if is_m3_max and torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple M3 Max GPU",
                "device_available": True,
                "device_memory_gb": 128.0,  # Unified Memory
                "device_capabilities": ["fp16", "bf16", "metal", "unified_memory"],
                "recommended_precision": "fp16",
                "max_batch_size": 32,
                "optimization_level": "maximum"
            })
            logger.info("ðŸŽ M3 Max MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨ - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ")
            
        # CUDA ê°ì§€
        elif torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            device_info.update({
                "device": "cuda",
                "device_name": device_name,
                "device_available": True,
                "device_memory_gb": round(device_memory, 1),
                "device_count": device_count,
                "device_capabilities": ["fp16", "bf16", "tensor_cores"],
                "recommended_precision": "fp16",
                "max_batch_size": 16,
                "optimization_level": "high"
            })
            logger.info(f"ðŸš€ CUDA ë””ë°”ì´ìŠ¤ ê°ì§€ë¨: {device_name}")
            
        # ì¼ë°˜ MPS (M1/M2)
        elif torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple Silicon GPU",
                "device_available": True,
                "device_memory_gb": 16.0,  # ì¶”ì •ê°’
                "device_capabilities": ["fp16", "metal"],
                "recommended_precision": "fp16",
                "max_batch_size": 8,
                "optimization_level": "medium"
            })
            logger.info("ðŸŽ Apple Silicon MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨")
            
        else:
            # CPU í´ë°±
            device_info.update({
                "device": "cpu",
                "device_name": "CPU (Multi-threaded)",
                "device_available": True,
                "device_memory_gb": 8.0,
                "device_capabilities": ["fp32", "multi_threading"],
                "recommended_precision": "fp32",
                "max_batch_size": 4,
                "optimization_level": "basic"
            })
            logger.info("ðŸ’» CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
        
    except Exception as e:
        logger.warning(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return device_info

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _detect_system_info()

# ==============================================
# ðŸ”¥ ë°ì´í„° êµ¬ì¡° ë° ì„¤ì • (ì™„ì „ êµ¬í˜„)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"
    UNIFIED = "unified"
    HYBRID = "hybrid"
    FALLBACK = "fallback"
    PRODUCTION = "production"

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ìž…"""
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"
    AUTO = "auto"

class PrecisionType(Enum):
    """ì •ë°€ë„ íƒ€ìž…"""
    FP32 = "fp32"
    FP16 = "fp16"
    BF16 = "bf16"
    INT8 = "int8"
    AUTO = "auto"

class StepType(Enum):
    """AI íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ íƒ€ìž…"""
    HUMAN_PARSING = "HumanParsingStep"
    POSE_ESTIMATION = "PoseEstimationStep"
    CLOTH_SEGMENTATION = "ClothSegmentationStep"
    GEOMETRIC_MATCHING = "GeometricMatchingStep"
    CLOTH_WARPING = "ClothWarpingStep"
    VIRTUAL_FITTING = "VirtualFittingStep"
    POST_PROCESSING = "PostProcessingStep"
    QUALITY_ASSESSMENT = "QualityAssessmentStep"

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • (ì™„ì „ êµ¬í˜„)"""
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "auto"
    device_memory_gb: float = 0.0
    
    # ì„±ëŠ¥ ì„¤ì •
    max_workers: int = 4
    max_batch_size: int = 8
    optimization_level: str = "medium"
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory_limit_gb: float = 16.0
    cache_enabled: bool = True
    memory_cleanup_threshold: float = 0.8
    
    # conda í™˜ê²½ ì„¤ì •
    conda_optimized: bool = True
    conda_env: str = "base"
    
    # ë””ë²„ê·¸ ì„¤ì •
    debug_mode: bool = False
    verbose_logging: bool = False
    profile_performance: bool = False
    
    # AI íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipeline_mode: str = "sequential"
    enable_async: bool = True
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ìžë™ ì„¤ì •"""
        # ì‹œìŠ¤í…œ ì •ë³´ ê¸°ë°˜ ìžë™ ì„¤ì •
        if self.device == "auto":
            self.device = SYSTEM_INFO["device"]
        
        if self.precision == "auto":
            self.precision = SYSTEM_INFO.get("recommended_precision", "fp32")
        
        if self.device_memory_gb == 0.0:
            self.device_memory_gb = SYSTEM_INFO.get("device_memory_gb", 16.0)
        
        # M3 Max íŠ¹í™” ìµœì í™”
        if SYSTEM_INFO["is_m3_max"]:
            self.max_workers = min(12, SYSTEM_INFO["cpu_count"])
            self.max_batch_size = 32
            self.optimization_level = "maximum"
            self.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
        
        # conda í™˜ê²½ ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            self.conda_optimized = True
            self.conda_env = SYSTEM_INFO["conda_env"]

@dataclass
class StepConfig:
    """Step ì„¤ì • (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›)"""
    step_name: str
    step_number: Optional[int] = None
    step_type: Optional[StepType] = None
    
    # ëª¨ë¸ ì„¤ì •
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_path: Optional[str] = None
    
    # ìž…ë ¥/ì¶œë ¥ ì„¤ì •
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    batch_size: int = 1
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "auto"
    
    # ì„±ëŠ¥ ì„¤ì •
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    memory_efficient: bool = True
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Step ì •ë³´ ìžë™ ì„¤ì •"""
        # Step íƒ€ìž… ì„¤ì •
        if self.step_type is None:
            for step_type in StepType:
                if step_type.value == self.step_name:
                    self.step_type = step_type
                    break
        
        # Step ë²ˆí˜¸ ìžë™ ì„¤ì •
        if self.step_number is None:
            step_numbers = {
                StepType.HUMAN_PARSING: 1,
                StepType.POSE_ESTIMATION: 2,
                StepType.CLOTH_SEGMENTATION: 3,
                StepType.GEOMETRIC_MATCHING: 4,
                StepType.CLOTH_WARPING: 5,
                StepType.VIRTUAL_FITTING: 6,
                StepType.POST_PROCESSING: 7,
                StepType.QUALITY_ASSESSMENT: 8
            }
            self.step_number = step_numbers.get(self.step_type, 0)
        
        # ê¸°ë³¸ ëª¨ë¸ëª… ì„¤ì •
        if self.model_name is None:
            default_models = {
                StepType.HUMAN_PARSING: "graphonomy",
                StepType.POSE_ESTIMATION: "openpose",
                StepType.CLOTH_SEGMENTATION: "u2net",
                StepType.GEOMETRIC_MATCHING: "geometric_matching",
                StepType.CLOTH_WARPING: "cloth_warping",
                StepType.VIRTUAL_FITTING: "ootdiffusion",
                StepType.POST_PROCESSING: "post_processing",
                StepType.QUALITY_ASSESSMENT: "clipiqa"
            }
            self.model_name = default_models.get(self.step_type, "default_model")

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (ì™„ì „ êµ¬í˜„)"""
    name: str
    path: str
    model_type: str
    
    # íŒŒì¼ ì •ë³´
    file_size_mb: float
    file_hash: Optional[str] = None
    last_modified: Optional[float] = None
    
    # í˜¸í™˜ì„± ì •ë³´
    step_compatibility: List[str] = field(default_factory=list)
    device_compatibility: List[str] = field(default_factory=list)
    precision_support: List[str] = field(default_factory=list)
    
    # ì„±ëŠ¥ ì •ë³´
    confidence_score: float = 1.0
    performance_score: float = 1.0
    memory_usage_mb: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    architecture: Optional[str] = None
    version: Optional[str] = None
    description: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelInfo':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        return cls(**data)

# ==============================================
# ðŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž (ì™„ì „ êµ¬í˜„)
# ==============================================

class StepMemoryManager:
    """
    ðŸ§  Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž (ì™„ì „ êµ¬í˜„)
    âœ… M3 Max 128GB ì™„ì „ ìµœì í™”
    âœ… conda í™˜ê²½ íŠ¹í™”
    âœ… ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    âœ… ìžë™ ì •ë¦¬ ë©”ì»¤ë‹ˆì¦˜
    âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ìž¥
    """
    
    def __init__(
        self, 
        device: str = "auto", 
        memory_limit_gb: Optional[float] = None,
        cleanup_threshold: float = 0.8,
        auto_cleanup: bool = True
    ):
        self.device = device if device != "auto" else SYSTEM_INFO["device"]
        self.memory_limit_gb = memory_limit_gb or SYSTEM_INFO["memory_gb"]
        self.cleanup_threshold = cleanup_threshold
        self.auto_cleanup = auto_cleanup
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        if self.is_m3_max:
            self.memory_limit_gb = min(self.memory_limit_gb, 100.0)  # 128GB ì¤‘ 100GB ì‚¬ìš©
            self.cleanup_threshold = 0.9  # M3 MaxëŠ” ë” ê´€ëŒ€í•˜ê²Œ
        
        # ë©”ëª¨ë¦¬ ì¶”ì 
        self.allocated_memory: Dict[str, float] = {}
        self.memory_history: List[Dict[str, Any]] = []
        self.peak_usage = 0.0
        self.total_allocations = 0
        self.total_deallocations = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"{__name__}.StepMemoryManager")
        
        # ìžë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        if self.auto_cleanup:
            self._start_auto_cleanup()
        
        self.logger.info(
            f"ðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™”: {self.device}, "
            f"{self.memory_limit_gb}GB, M3 Max: {self.is_m3_max}"
        )
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                device_idx = 0
                total = torch.cuda.get_device_properties(device_idx).total_memory
                allocated = torch.cuda.memory_allocated(device_idx)
                return (total - allocated) / (1024**3)
                
            elif self.device == "mps" and self.is_m3_max:
                # M3 Max Unified Memory ì²˜ë¦¬
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)
                    return min(available_gb, self.memory_limit_gb)
                else:
                    return self.memory_limit_gb * 0.7
                    
            else:
                # CPU ë©”ëª¨ë¦¬
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    return memory.available / (1024**3)
                else:
                    return 8.0
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def get_total_memory(self) -> float:
        """ì „ì²´ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                return torch.cuda.get_device_properties(0).total_memory / (1024**3)
            else:
                return self.memory_limit_gb
        except Exception:
            return self.memory_limit_gb
    
    def get_memory_usage_percent(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%) ë°˜í™˜"""
        try:
            total = self.get_total_memory()
            available = self.get_available_memory()
            used = total - available
            return (used / total) * 100 if total > 0 else 0.0
        except Exception:
            return 0.0
    
    def allocate_memory(self, step_name: str, size_gb: float) -> bool:
        """Stepì— ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            try:
                available = self.get_available_memory()
                
                if available >= size_gb:
                    self.allocated_memory[step_name] = size_gb
                    self.total_allocations += 1
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    current_total = sum(self.allocated_memory.values())
                    self.peak_usage = max(self.peak_usage, current_total)
                    
                    # ë©”ëª¨ë¦¬ ê¸°ë¡
                    self._record_memory_event("allocate", step_name, size_gb)
                    
                    self.logger.info(f"âœ… {step_name}: {size_gb:.1f}GB í• ë‹¹ë¨")
                    
                    # ìžë™ ì •ë¦¬ ì²´í¬
                    if self.auto_cleanup and self.check_memory_pressure():
                        self._trigger_cleanup()
                    
                    return True
                else:
                    self.logger.warning(
                        f"âš ï¸ {step_name}: {size_gb:.1f}GB í• ë‹¹ ì‹¤íŒ¨ "
                        f"(ì‚¬ìš© ê°€ëŠ¥: {available:.1f}GB)"
                    )
                    return False
                    
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {e}")
                return False
    
    def deallocate_memory(self, step_name: str) -> float:
        """Stepì˜ ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if step_name in self.allocated_memory:
                size = self.allocated_memory.pop(step_name)
                self.total_deallocations += 1
                
                # ë©”ëª¨ë¦¬ ê¸°ë¡
                self._record_memory_event("deallocate", step_name, size)
                
                self.logger.info(f"ðŸ—‘ï¸ {step_name}: {size:.1f}GB í•´ì œë¨")
                return size
            return 0.0
    
    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        with self._lock:
            try:
                cleanup_stats = {
                    "python_objects_collected": 0,
                    "gpu_cache_cleared": False,
                    "steps_deallocated": 0,
                    "memory_freed_gb": 0.0
                }
                
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected = gc.collect()
                cleanup_stats["python_objects_collected"] = collected
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if TORCH_AVAILABLE:
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        cleanup_stats["gpu_cache_cleared"] = True
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                            if self.is_m3_max and hasattr(torch.mps, 'synchronize'):
                                torch.mps.synchronize()
                            cleanup_stats["gpu_cache_cleared"] = True
                        except Exception as e:
                            self.logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # ê°•ì œ ì •ë¦¬ ì‹œ í• ë‹¹ëœ ë©”ëª¨ë¦¬ í•´ì œ
                if force and self.allocated_memory:
                    freed_memory = sum(self.allocated_memory.values())
                    steps_count = len(self.allocated_memory)
                    
                    self.allocated_memory.clear()
                    
                    cleanup_stats.update({
                        "steps_deallocated": steps_count,
                        "memory_freed_gb": freed_memory
                    })
                
                # ë©”ëª¨ë¦¬ ê¸°ë¡
                self._record_memory_event("cleanup", "system", 0.0, cleanup_stats)
                
                self.logger.info(f"ðŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleanup_stats}")
                
                return cleanup_stats
                
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            usage_percent = self.get_memory_usage_percent()
            return usage_percent > (self.cleanup_threshold * 100)
        except Exception:
            return False
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ (ì™„ì „ êµ¬í˜„)"""
        with self._lock:
            try:
                return {
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "memory_info": {
                        "total_limit_gb": self.memory_limit_gb,
                        "available_gb": self.get_available_memory(),
                        "usage_percent": self.get_memory_usage_percent(),
                        "peak_usage_gb": self.peak_usage
                    },
                    "allocation_info": {
                        "allocated_by_steps": self.allocated_memory.copy(),
                        "total_allocated_gb": sum(self.allocated_memory.values()),
                        "active_steps": len(self.allocated_memory)
                    },
                    "statistics": {
                        "total_allocations": self.total_allocations,
                        "total_deallocations": self.total_deallocations,
                        "cleanup_threshold": self.cleanup_threshold,
                        "auto_cleanup": self.auto_cleanup
                    },
                    "pressure_info": {
                        "memory_pressure": self.check_memory_pressure(),
                        "cleanup_recommended": self.get_memory_usage_percent() > 70
                    }
                }
            except Exception as e:
                self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
    
    def _record_memory_event(
        self, 
        event_type: str, 
        step_name: str, 
        size_gb: float, 
        extra_data: Optional[Dict] = None
    ):
        """ë©”ëª¨ë¦¬ ì´ë²¤íŠ¸ ê¸°ë¡"""
        event = {
            "timestamp": time.time(),
            "event_type": event_type,
            "step_name": step_name,
            "size_gb": size_gb,
            "total_allocated": sum(self.allocated_memory.values()),
            "memory_usage_percent": self.get_memory_usage_percent()
        }
        
        if extra_data:
            event.update(extra_data)
        
        self.memory_history.append(event)
        
        # ê¸°ë¡ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ìœ ì§€)
        if len(self.memory_history) > 1000:
            self.memory_history = self.memory_history[-1000:]
    
    def _start_auto_cleanup(self):
        """ìžë™ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œìž‘"""
        def cleanup_worker():
            while self.auto_cleanup:
                try:
                    time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                    if self.check_memory_pressure():
                        self._trigger_cleanup()
                except Exception as e:
                    self.logger.debug(f"ìžë™ ì •ë¦¬ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
    
    def _trigger_cleanup(self):
        """ìžë™ ì •ë¦¬ íŠ¸ë¦¬ê±°"""
        try:
            self.logger.info("ðŸš¨ ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€ - ìžë™ ì •ë¦¬ ì‹œìž‘")
            self.cleanup_memory()
        except Exception as e:
            self.logger.warning(f"ìžë™ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def export_stats(self, filepath: Optional[str] = None) -> str:
        """í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        stats = self.get_memory_stats()
        stats["memory_history"] = self.memory_history[-100:]  # ìµœê·¼ 100ê°œ ì´ë²¤íŠ¸
        
        if filepath is None:
            filepath = f"memory_stats_{int(time.time())}.json"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ðŸ“Š ë©”ëª¨ë¦¬ í†µê³„ ë‚´ë³´ë‚´ê¸°: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return ""

# ==============================================
# ðŸ”¥ ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class StepModelInterface:
    """
    ðŸ”— Step ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
    âœ… main.py ì™„ì „ í˜¸í™˜
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    âœ… ëª¨ë¸ ìºì‹± ìµœì í™”
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    
    def __init__(
        self, 
        step_name: str, 
        model_loader_instance: Optional[Any] = None,
        config: Optional[StepConfig] = None
    ):
        self.step_name = step_name
        self.model_loader = model_loader_instance
        self.config = config or StepConfig(step_name=step_name)
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"interface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self._models_cache: Dict[str, Any] = {}
        self._model_metadata: Dict[str, ModelInfo] = {}
        
        # ìƒíƒœ ê´€ë¦¬
        self._request_count = 0
        self._last_request_time = None
        self._total_load_time = 0.0
        self._success_count = 0
        self._error_count = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # Stepë³„ ëª¨ë¸ ë§¤í•‘ (ì™„ì „ êµ¬í˜„)
        self._initialize_model_mappings()
        
        self.logger.info(f"ðŸ”— {step_name} ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_model_mappings(self):
        """Stepë³„ ëª¨ë¸ ë§¤í•‘ ì´ˆê¸°í™”"""
        self._model_mappings = {
            "HumanParsingStep": {
                "default_models": ["graphonomy", "human_parsing_atr", "parsing_lip", "schp"],
                "model_types": ["segmentation", "parsing"],
                "input_sizes": [(512, 512), (473, 473)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "PoseEstimationStep": {
                "default_models": ["openpose", "mediapipe", "yolov8_pose", "movenet"],
                "model_types": ["pose_estimation", "keypoint_detection"],
                "input_sizes": [(368, 368), (256, 256), (192, 256)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "ClothSegmentationStep": {
                "default_models": ["u2net", "cloth_segmentation", "deeplabv3", "bisenet"],
                "model_types": ["segmentation", "cloth_parsing"],
                "input_sizes": [(320, 320), (512, 512)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "GeometricMatchingStep": {
                "default_models": ["geometric_matching", "tps_transformation", "spatial_transformer"],
                "model_types": ["transformation", "matching"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "ClothWarpingStep": {
                "default_models": ["cloth_warping", "spatial_transformer", "thin_plate_spline"],
                "model_types": ["warping", "transformation"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "VirtualFittingStep": {
                "default_models": ["ootdiffusion", "stable_diffusion", "virtual_tryon", "diffusion_tryon"],
                "model_types": ["diffusion", "generation", "virtual_fitting"],
                "input_sizes": [(512, 512), (768, 768)],
                "supported_formats": [".pth", ".pt", ".safetensors", ".ckpt"]
            },
            "PostProcessingStep": {
                "default_models": ["post_processing", "image_enhancement", "artifact_removal", "super_resolution"],
                "model_types": ["enhancement", "post_processing"],
                "input_sizes": [(512, 512), (1024, 1024)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "QualityAssessmentStep": {
                "default_models": ["clipiqa", "quality_assessment", "brisque", "niqe"],
                "model_types": ["quality_assessment", "metric"],
                "input_sizes": [(224, 224), (512, 512)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            }
        }
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """
        ðŸ”¥ ëª¨ë¸ ë¡œë“œ (main.py í•µì‹¬ ë©”ì„œë“œ - ì™„ì „ êµ¬í˜„)
        """
        start_time = time.time()
        
        with self._lock:
            self._request_count += 1
            self._last_request_time = time.time()
        
        try:
            # ëª¨ë¸ëª… ê²°ì •
            target_model = model_name or self.config.model_name or self._get_default_model()
            
            if not target_model:
                self.logger.warning(f"âš ï¸ {self.step_name}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                self._error_count += 1
                return None
            
            # ìºì‹œ í™•ì¸
            if target_model in self._models_cache:
                self.logger.debug(f"ðŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {target_model}")
                self._success_count += 1
                return self._models_cache[target_model]
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
            model = None
            
            # 1. ModelLoaderë¥¼ í†µí•œ ë¡œë“œ
            if self.model_loader:
                model = await self._load_via_model_loader(target_model)
            
            # 2. ì§ì ‘ íŒŒì¼ ë¡œë“œ
            if model is None:
                model = await self._load_from_file(target_model)
            
            # 3. ì›ê²© ë‹¤ìš´ë¡œë“œ ì‹œë„
            if model is None:
                model = await self._download_and_load(target_model)
            
            # 4. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ (ìµœì¢… í´ë°±)
            if model is None:
                model = self._create_simulation_model(target_model)
                self.logger.warning(f"âš ï¸ {target_model} ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì‚¬ìš©")
            
            # ìºì‹œ ì €ìž¥
            if model:
                self._models_cache[target_model] = model
                self._success_count += 1
                
                # ë©”íƒ€ë°ì´í„° ì €ìž¥
                if target_model not in self._model_metadata:
                    self._model_metadata[target_model] = self._create_model_metadata(target_model, model)
                
                load_time = time.time() - start_time
                self._total_load_time += load_time
                
                self.logger.info(
                    f"âœ… {target_model} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}s)"
                )
                
                return model
            else:
                self._error_count += 1
                self.logger.error(f"âŒ {target_model} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def _load_via_model_loader(self, model_name: str) -> Optional[Any]:
        """ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not hasattr(self.model_loader, 'get_model'):
                return None
            
            # ë¹„ë™ê¸°/ë™ê¸° í˜¸í™˜ ì²˜ë¦¬
            if asyncio.iscoroutinefunction(self.model_loader.get_model):
                model = await self.model_loader.get_model(model_name)
            else:
                model = self.model_loader.get_model(model_name)
            
            if model:
                self.logger.debug(f"âœ… ModelLoaderë¡œ {model_name} ë¡œë“œ ì„±ê³µ")
                return model
            
        except Exception as e:
            self.logger.debug(f"ModelLoader ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _load_from_file(self, model_name: str) -> Optional[Any]:
        """íŒŒì¼ì—ì„œ ì§ì ‘ ëª¨ë¸ ë¡œë“œ"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            if not ai_models_path.exists():
                return None
            
            # ê°€ëŠ¥í•œ íŒŒì¼ ê²½ë¡œë“¤
            step_mapping = self._model_mappings.get(self.step_name, {})
            supported_formats = step_mapping.get("supported_formats", [".pth", ".pt", ".ckpt"])
            
            search_paths = [
                ai_models_path / f"{model_name}{ext}" for ext in supported_formats
            ]
            
            # Stepë³„ í´ë”ë„ í™•ì¸
            step_folder = ai_models_path / self.step_name.lower().replace("step", "")
            if step_folder.exists():
                search_paths.extend([
                    step_folder / f"{model_name}{ext}" for ext in supported_formats
                ])
            
            # íŒŒì¼ íƒìƒ‰
            for model_path in search_paths:
                if model_path.exists():
                    self.logger.info(f"ðŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                    
                    # PyTorch ëª¨ë¸ ë¡œë“œ
                    if TORCH_AVAILABLE and model_path.suffix in ['.pth', '.pt', '.ckpt']:
                        model = await self._load_pytorch_model(model_path)
                        if model:
                            return model
                    
                    # ë‹¤ë¥¸ í˜•ì‹ ì§€ì› (ONNX ë“±)
                    # TODO: ONNX, TensorFlow ë“± ì¶”ê°€ ì§€ì›
                    
            return None
            
        except Exception as e:
            self.logger.debug(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_pytorch_model(self, model_path: Path) -> Optional[Any]:
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                return None
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = self.config.device if self.config.device != "auto" else SYSTEM_INFO["device"]
            map_location = device if device != "mps" else "cpu"  # MPSëŠ” CPUë¡œ ë¨¼ì € ë¡œë“œ
            
            # ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=map_location, weights_only=True)
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„
            if isinstance(checkpoint, dict):
                if 'model' in checkpoint:
                    model = checkpoint['model']
                elif 'state_dict' in checkpoint:
                    # state_dictë§Œ ìžˆëŠ” ê²½ìš°, ëª¨ë¸ êµ¬ì¡°ê°€ í•„ìš”
                    # TODO: ëª¨ë¸ ì•„í‚¤í…ì²˜ ìžë™ ì¶”ë¡ 
                    model = checkpoint['state_dict']
                else:
                    model = checkpoint
            else:
                model = checkpoint
            
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (í•„ìš”ì‹œ)
            if device == "mps" and hasattr(model, 'to'):
                model = model.to(device)
            
            # ModelInfo ìƒì„±
            model_info = ModelInfo(
                name=model_path.stem,
                path=str(model_path),
                model_type=f"{self.step_name}_pytorch_model",
                file_size_mb=model_path.stat().st_size / (1024*1024),
                step_compatibility=[self.step_name],
                device_compatibility=[device],
                architecture="pytorch"
            )
            
            return {
                "model": model,
                "info": model_info,
                "device": device,
                "loaded_from": "file"
            }
            
        except Exception as e:
            self.logger.debug(f"PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _download_and_load(self, model_name: str) -> Optional[Any]:
        """ì›ê²©ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
        try:
            # TODO: Hugging Face Hub, ê³µì‹ ëª¨ë¸ ì €ìž¥ì†Œ ë“±ì—ì„œ ë‹¤ìš´ë¡œë“œ
            # í˜„ìž¬ëŠ” ê¸°ë³¸ êµ¬í˜„ë§Œ ì œê³µ
            self.logger.debug(f"ì›ê²© ë‹¤ìš´ë¡œë“œ ì‹œë„: {model_name} (ë¯¸êµ¬í˜„)")
            return None
            
        except Exception as e:
            self.logger.debug(f"ì›ê²© ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_simulation_model(self, model_name: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        return {
            "name": model_name,
            "type": "simulation",
            "step_name": self.step_name,
            "step_number": self.config.step_number,
            "device": SYSTEM_INFO["device"],
            "precision": SYSTEM_INFO.get("recommended_precision", "fp32"),
            "created_at": time.time(),
            "simulate": True,
            "capabilities": self._model_mappings.get(self.step_name, {}).get("model_types", [])
        }
    
    def _create_model_metadata(self, model_name: str, model: Any) -> ModelInfo:
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            # ëª¨ë¸ í¬ê¸° ì¶”ì •
            memory_usage = 0.0
            if hasattr(model, 'parameters'):
                # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                total_params = sum(p.numel() for p in model.parameters() if hasattr(p, 'numel'))
                memory_usage = total_params * 4 / (1024*1024)  # 4 bytes per float32
            
            return ModelInfo(
                name=model_name,
                path="",
                model_type=f"{self.step_name}_model",
                file_size_mb=0.0,
                memory_usage_mb=memory_usage,
                step_compatibility=[self.step_name],
                device_compatibility=[SYSTEM_INFO["device"]],
                precision_support=[SYSTEM_INFO.get("recommended_precision", "fp32")],
                confidence_score=1.0,
                performance_score=0.8 if isinstance(model, dict) and model.get("simulate") else 1.0
            )
            
        except Exception as e:
            self.logger.debug(f"ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return ModelInfo(
                name=model_name,
                path="",
                model_type="unknown",
                file_size_mb=0.0
            )
    
    def _get_default_model(self) -> Optional[str]:
        """ê¸°ë³¸ ëª¨ë¸ëª… ë°˜í™˜"""
        mapping = self._model_mappings.get(self.step_name)
        if mapping and mapping["default_models"]:
            return mapping["default_models"][0]
        return None
    
    def list_available_models(self) -> List[str]:
        """
        ðŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (main.py í•µì‹¬ ë©”ì„œë“œ - ì™„ì „ êµ¬í˜„)
        """
        try:
            available_models = set()
            
            # 1. Stepë³„ ê¸°ë³¸ ëª¨ë¸ë“¤
            mapping = self._model_mappings.get(self.step_name, {})
            default_models = mapping.get("default_models", [])
            available_models.update(default_models)
            
            # 2. ë¡œì»¬ íŒŒì¼ ìŠ¤ìº”
            local_models = self._scan_local_models()
            available_models.update(local_models)
            
            # 3. ModelLoader ëª¨ë¸ ëª©ë¡
            if self.model_loader and hasattr(self.model_loader, 'list_models'):
                try:
                    loader_models = self.model_loader.list_models(self.step_name)
                    if loader_models:
                        available_models.update(loader_models)
                except Exception as e:
                    self.logger.debug(f"ModelLoader ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 4. ìºì‹œëœ ëª¨ë¸ë“¤
            available_models.update(self._models_cache.keys())
            
            # ì •ë ¬ ë° ë°˜í™˜
            result = sorted(list(available_models))
            
            self.logger.info(
                f"ðŸ“‹ {self.step_name} ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(result)}ê°œ "
                f"({', '.join(result[:3])}{'...' if len(result) > 3 else ''})"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë¸ë§Œ ë°˜í™˜
            mapping = self._model_mappings.get(self.step_name, {})
            return mapping.get("default_models", [])
    
    def _scan_local_models(self) -> List[str]:
        """ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            if not ai_models_path.exists():
                return []
            
            models = []
            mapping = self._model_mappings.get(self.step_name, {})
            supported_formats = mapping.get("supported_formats", [".pth", ".pt", ".ckpt"])
            
            # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
            for ext in supported_formats:
                for model_file in ai_models_path.glob(f"*{ext}"):
                    if self.step_name.lower() in model_file.name.lower():
                        models.append(model_file.stem)
            
            # Stepë³„ í´ë” ìŠ¤ìº”
            step_folder = ai_models_path / self.step_name.lower().replace("step", "")
            if step_folder.exists():
                for ext in supported_formats:
                    for model_file in step_folder.glob(f"*{ext}"):
                        models.append(model_file.stem)
            
            return list(set(models))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            self.logger.debug(f"ë¡œì»¬ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            return []
    
    async def unload_models(self, model_names: Optional[List[str]] = None):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            with self._lock:
                if model_names is None:
                    # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
                    unloaded_count = len(self._models_cache)
                    self._models_cache.clear()
                    self._model_metadata.clear()
                else:
                    # íŠ¹ì • ëª¨ë¸ë“¤ë§Œ ì–¸ë¡œë“œ
                    unloaded_count = 0
                    for model_name in model_names:
                        if model_name in self._models_cache:
                            del self._models_cache[model_name]
                            unloaded_count += 1
                        if model_name in self._model_metadata:
                            del self._model_metadata[model_name]
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            if TORCH_AVAILABLE:
                if SYSTEM_INFO["device"] == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except Exception:
                        pass
                elif SYSTEM_INFO["device"] == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.info(f"ðŸ—‘ï¸ {self.step_name}: {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_model_info(self, model_name: str) -> Optional[ModelInfo]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return self._model_metadata.get(model_name)
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸í„°íŽ˜ì´ìŠ¤ í†µê³„ (ì™„ì „ êµ¬í˜„)"""
        with self._lock:
            avg_load_time = (
                self._total_load_time / max(self._success_count, 1)
                if self._success_count > 0 else 0.0
            )
            
            success_rate = (
                (self._success_count / max(self._request_count, 1)) * 100
                if self._request_count > 0 else 0.0
            )
            
            return {
                "step_name": self.step_name,
                "step_number": self.config.step_number,
                "request_statistics": {
                    "total_requests": self._request_count,
                    "successful_loads": self._success_count,
                    "failed_loads": self._error_count,
                    "success_rate_percent": round(success_rate, 1),
                    "last_request_time": self._last_request_time
                },
                "performance": {
                    "total_load_time": round(self._total_load_time, 2),
                    "average_load_time": round(avg_load_time, 2)
                },
                "cache_info": {
                    "cached_models": len(self._models_cache),
                    "cached_metadata": len(self._model_metadata),
                    "cached_model_names": list(self._models_cache.keys())
                },
                "configuration": {
                    "has_model_loader": self.model_loader is not None,
                    "device": self.config.device,
                    "precision": self.config.precision
                },
                "available_models": {
                    "count": len(self.list_available_models()),
                    "default_model": self._get_default_model()
                }
            }

# ==============================================
# ðŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (ì™„ì „ êµ¬í˜„)
# ==============================================

class UnifiedUtilsManager:
    """
    ðŸŽ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € v8.0 (ì™„ì „ êµ¬í˜„)
    âœ… conda í™˜ê²½ 100% ìµœì í™”
    âœ… M3 Max 128GB ì™„ì „ í™œìš©
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
    âœ… Clean Architecture ì ìš©
    âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ìž¥
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.system_config = SystemConfig()
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        self.last_optimization = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ìž¥ì†Œ (ì•½í•œ ì°¸ì¡°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces: Dict[str, StepModelInterface] = {}
        self._memory_managers: Dict[str, StepMemoryManager] = {}
        
        # ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž
        self.global_memory_manager = StepMemoryManager(
            device=self.system_config.device,
            memory_limit_gb=self.system_config.memory_limit_gb,
            auto_cleanup=True
        )
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0,
            "conda_optimizations": 0,
            "m3_max_optimizations": 0,
            "startup_time": 0.0
        }
        
        # ìŠ¤ë ˆë“œ í’€
        self._thread_pool = ThreadPoolExecutor(
            max_workers=self.system_config.max_workers,
            thread_name_prefix="utils_worker"
        )
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        self._optimization_lock = threading.Lock()
        
        # conda í™˜ê²½ ìµœì í™”
        if SYSTEM_INFO["in_conda"]:
            self._setup_conda_optimizations()
        
        # M3 Max íŠ¹ë³„ ìµœì í™”
        if SYSTEM_INFO["is_m3_max"]:
            self._setup_m3_max_optimizations()
        
        self._initialized = True
        self.logger.info(
            f"ðŸŽ¯ UnifiedUtilsManager v8.0 ì´ˆê¸°í™” ì™„ë£Œ "
            f"(conda: {SYSTEM_INFO['in_conda']}, M3: {SYSTEM_INFO['is_m3_max']})"
        )
    
    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            start_time = time.time()
            
            # PyTorch ìµœì í™”
            if TORCH_AVAILABLE:
                # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
                torch.set_num_threads(self.system_config.max_workers)
                
                # ì¸í„°ì˜µ ë³‘ë ¬ì„± ì„¤ì •
                torch.set_num_interop_threads(min(4, self.system_config.max_workers))
                
                # MPS ìµœì í™” (M3 Max)
                if SYSTEM_INFO["is_m3_max"]:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                    })
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                # OpenBLAS/MKL ìŠ¤ë ˆë“œ ì„¤ì •
                os.environ.update({
                    'OMP_NUM_THREADS': str(self.system_config.max_workers),
                    'MKL_NUM_THREADS': str(self.system_config.max_workers),
                    'OPENBLAS_NUM_THREADS': str(self.system_config.max_workers),
                    'NUMEXPR_NUM_THREADS': str(self.system_config.max_workers)
                })
            
            # ë©”ëª¨ë¦¬ í• ë‹¹ìž ìµœì í™”
            if TORCH_AVAILABLE and SYSTEM_INFO["device"] == "mps":
                os.environ['PYTORCH_MPS_PREFER_METAL'] = '1'
            
            optimization_time = time.time() - start_time
            self.stats["conda_optimizations"] += 1
            
            self.logger.info(
                f"âœ… conda í™˜ê²½ ìµœì í™” ì™„ë£Œ ({optimization_time:.3f}s) - "
                f"ì›Œì»¤: {self.system_config.max_workers}, í™˜ê²½: {SYSTEM_INFO['conda_env']}"
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_m3_max_optimizations(self):
        """M3 Max íŠ¹ë³„ ìµœì í™”"""
        try:
            start_time = time.time()
            
            # ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”
            self.system_config.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
            self.system_config.max_batch_size = 32  # M3 MaxëŠ” í° ë°°ì¹˜ í—ˆìš©
            self.system_config.optimization_level = "maximum"
            
            if TORCH_AVAILABLE:
                # M3 Max MPS ë°±ì—”ë“œ ìµœì í™”
                if torch.backends.mps.is_available():
                    try:
                        # Metal Performance Shaders ìµœì í™”
                        if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                            torch.mps.set_per_process_memory_fraction(0.8)
                        
                        # FP16 ê¸°ë³¸ ì„¤ì • (M3 Maxì—ì„œ ì„±ëŠ¥ í–¥ìƒ)
                        if hasattr(torch, 'set_default_dtype'):
                            torch.set_default_dtype(torch.float16)
                        
                        # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜
                        os.environ.update({
                            'PYTORCH_MPS_ALLOCATOR_POLICY': 'native',
                            'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.9'
                        })
                        
                    except Exception as e:
                        self.logger.debug(f"MPS ì„¸ë¶€ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì¡°ì • (macOS)
            try:
                os.nice(-5)  # ë†’ì€ ìš°ì„ ìˆœìœ„
            except (OSError, PermissionError):
                pass  # ê¶Œí•œ ì—†ìœ¼ë©´ ë¬´ì‹œ
            
            optimization_time = time.time() - start_time
            self.stats["m3_max_optimizations"] += 1
            
            self.logger.info(
                f"ðŸŽ M3 Max íŠ¹ë³„ ìµœì í™” ì™„ë£Œ ({optimization_time:.3f}s) - "
                f"ë©”ëª¨ë¦¬: {self.system_config.memory_limit_gb}GB, "
                f"ë°°ì¹˜: {self.system_config.max_batch_size}"
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™” (ì™„ì „ êµ¬í˜„)"""
        if self.is_initialized:
            return {
                "success": True, 
                "message": "Already initialized",
                "initialization_time": self.initialization_time
            }
        
        try:
            start_time = time.time()
            self.logger.info("ðŸš€ UnifiedUtilsManager ì™„ì „ ì´ˆê¸°í™” ì‹œìž‘...")
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.system_config, key):
                    setattr(self.system_config, key, value)
                    self.logger.debug(f"ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
            
            # AI ëª¨ë¸ ê²½ë¡œ í™•ì¸ ë° ìƒì„±
            await self._setup_ai_models_directory()
            
            # ModelLoader ì—°ë™
            await self._initialize_model_loader()
            
            # ì‹œìŠ¤í…œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
            performance_profile = await self._profile_system_performance()
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            self.stats["startup_time"] = self.initialization_time
            
            result = {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_config": asdict(self.system_config),
                "system_info": SYSTEM_INFO,
                "performance_profile": performance_profile,
                "conda_optimized": SYSTEM_INFO["in_conda"],
                "m3_max_optimized": SYSTEM_INFO["is_m3_max"],
                "components_ready": {
                    "memory_manager": True,
                    "thread_pool": True,
                    "model_loader": hasattr(self, 'model_loader')
                }
            }
            
            self.logger.info(
                f"ðŸŽ‰ UnifiedUtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s) - "
                f"ì„±ëŠ¥ ì ìˆ˜: {performance_profile.get('overall_score', 0):.1f}/10"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _setup_ai_models_directory(self):
        """AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •"""
        try:
            ai_models_path = Path(SYSTEM_INFO["ai_models_path"])
            
            if not ai_models_path.exists():
                ai_models_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ðŸ“ AI ëª¨ë¸ í´ë” ìƒì„±: {ai_models_path}")
            
            # Stepë³„ í•˜ìœ„ í´ë” ìƒì„±
            step_folders = [
                "human_parsing", "pose_estimation", "cloth_segmentation",
                "geometric_matching", "cloth_warping", "virtual_fitting",
                "post_processing", "quality_assessment", "checkpoints", "temp"
            ]
            
            for folder in step_folders:
                folder_path = ai_models_path / folder
                folder_path.mkdir(exist_ok=True)
                
                # .gitkeep íŒŒì¼ ìƒì„± (ë¹ˆ í´ë” ìœ ì§€)
                gitkeep_path = folder_path / ".gitkeep"
                if not gitkeep_path.exists():
                    gitkeep_path.touch()
            
            # ëª¨ë¸ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
            index_file = ai_models_path / "models_index.json"
            if not index_file.exists():
                default_index = {
                    "version": "1.0",
                    "last_updated": time.time(),
                    "models": {},
                    "steps": [step.value for step in StepType]
                }
                
                with open(index_file, 'w', encoding='utf-8') as f:
                    json.dump(default_index, f, indent=2, ensure_ascii=False)
            
            self.logger.info("âœ… AI ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def _initialize_model_loader(self):
        """ModelLoader ì´ˆê¸°í™”"""
        try:
            # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë™ì  import
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                self.model_loader = get_global_model_loader()
                
                if self.model_loader:
                    self.logger.info("âœ… ModelLoader ì—°ë™ ì„±ê³µ")
                else:
                    self.logger.info("â„¹ï¸ ModelLoader ë¯¸ì‚¬ìš© - ì§ì ‘ ë¡œë“œ ëª¨ë“œ")
                    
            except ImportError:
                self.logger.info("â„¹ï¸ ModelLoader ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ë¡œë” ì‚¬ìš©")
                self.model_loader = None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    async def _profile_system_performance(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§"""
        try:
            profile_start = time.time()
            
            profile = {
                "cpu_score": 0.0,
                "memory_score": 0.0,
                "device_score": 0.0,
                "conda_score": 0.0,
                "overall_score": 0.0,
                "recommendations": []
            }
            
            # CPU ì„±ëŠ¥ í‰ê°€
            cpu_count = SYSTEM_INFO["cpu_count"]
            if cpu_count >= 8:
                profile["cpu_score"] = 10.0
            elif cpu_count >= 4:
                profile["cpu_score"] = 7.0
            else:
                profile["cpu_score"] = 5.0
                profile["recommendations"].append("ë” ë§Žì€ CPU ì½”ì–´ ê¶Œìž¥")
            
            # ë©”ëª¨ë¦¬ ì„±ëŠ¥ í‰ê°€
            memory_gb = SYSTEM_INFO["memory_gb"]
            if memory_gb >= 64:
                profile["memory_score"] = 10.0
            elif memory_gb >= 32:
                profile["memory_score"] = 8.0
            elif memory_gb >= 16:
                profile["memory_score"] = 6.0
            else:
                profile["memory_score"] = 4.0
                profile["recommendations"].append("ë” ë§Žì€ ë©”ëª¨ë¦¬ ê¶Œìž¥ (ìµœì†Œ 16GB)")
            
            # ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ í‰ê°€
            device = SYSTEM_INFO["device"]
            if SYSTEM_INFO["is_m3_max"]:
                profile["device_score"] = 10.0
            elif device == "mps":
                profile["device_score"] = 8.0
            elif device == "cuda":
                profile["device_score"] = 9.0
            else:
                profile["device_score"] = 5.0
                profile["recommendations"].append("GPU ê°€ì† ê¶Œìž¥")
            
            # conda í™˜ê²½ í‰ê°€
            if SYSTEM_INFO["in_conda"]:
                profile["conda_score"] = 10.0
            else:
                profile["conda_score"] = 5.0
                profile["recommendations"].append("conda í™˜ê²½ ì‚¬ìš© ê¶Œìž¥")
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            scores = [
                profile["cpu_score"],
                profile["memory_score"], 
                profile["device_score"],
                profile["conda_score"]
            ]
            profile["overall_score"] = sum(scores) / len(scores)
            
            # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
            if profile["overall_score"] >= 9.0:
                profile["grade"] = "A+ (ìµœê³  ì„±ëŠ¥)"
            elif profile["overall_score"] >= 8.0:
                profile["grade"] = "A (ìš°ìˆ˜)"
            elif profile["overall_score"] >= 7.0:
                profile["grade"] = "B (ì–‘í˜¸)"
            elif profile["overall_score"] >= 6.0:
                profile["grade"] = "C (ë³´í†µ)"
            else:
                profile["grade"] = "D (ê°œì„  í•„ìš”)"
            
            profile["profile_time"] = time.time() - profile_start
            
            self.logger.info(
                f"ðŸ“Š ì„±ëŠ¥ í”„ë¡œíŒŒì¼: {profile['grade']} "
                f"(ì ìˆ˜: {profile['overall_score']:.1f}/10)"
            )
            
            return profile
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹¤íŒ¨: {e}")
            return {"overall_score": 5.0, "grade": "Unknown", "error": str(e)}
    
    def create_step_interface(self, step_name: str, **options) -> 'UnifiedStepInterface':
        """Step ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± (ì™„ì „ êµ¬í˜„)"""
        try:
            with self._interface_lock:
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = f"{step_name}_{hash(str(sorted(options.items())))}"
                
                # ìºì‹œ í™•ì¸
                if cache_key in self._step_interfaces:
                    self.logger.debug(f"ðŸ“‹ {step_name} ìºì‹œëœ ì¸í„°íŽ˜ì´ìŠ¤ ë°˜í™˜")
                    return self._step_interfaces[cache_key]
                
                # ìƒˆ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±
                step_config = self._create_step_config(step_name, **options)
                interface = UnifiedStepInterface(self, step_config)
                
                # ìºì‹œ ì €ìž¥ (ì•½í•œ ì°¸ì¡°)
                self._step_interfaces[cache_key] = interface
                
                self.stats["interfaces_created"] += 1
                self.logger.info(f"ðŸ”— {step_name} í†µí•© ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_fallback_interface(step_name)
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)"""
        try:
            # ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ ë°˜í™˜
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            # ìƒˆ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±
            step_config = StepConfig(step_name=step_name)
            interface = StepModelInterface(
                step_name=step_name,
                model_loader_instance=getattr(self, 'model_loader', None),
                config=step_config
            )
            
            # ìºì‹œ ì €ìž¥
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"ðŸ”— {step_name} ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(step_name, None)
    
    def create_step_memory_manager(self, step_name: str, **options) -> StepMemoryManager:
        """Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±"""
        try:
            # ê¸°ì¡´ ê´€ë¦¬ìž ë°˜í™˜
            if step_name in self._memory_managers:
                return self._memory_managers[step_name]
            
            # ìƒˆ ê´€ë¦¬ìž ìƒì„±
            manager = StepMemoryManager(**options)
            self._memory_managers[step_name] = manager
            
            self.logger.info(f"ðŸ§  {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„± ì™„ë£Œ")
            return manager
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„± ì‹¤íŒ¨: {e}")
            return self.global_memory_manager
    
    def get_memory_manager(self) -> StepMemoryManager:
        """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ë°˜í™˜"""
        return self.global_memory_manager
    
    def _create_step_config(self, step_name: str, **options) -> StepConfig:
        """Step ì„¤ì • ìƒì„±"""
        # ê¸°ë³¸ ì„¤ì •
        config_data = {
            "step_name": step_name,
            "device": self.system_config.device,
            "precision": self.system_config.precision,
            "batch_size": min(self.system_config.max_batch_size, options.get("batch_size", 1))
        }
        
        # ì˜µì…˜ ë³‘í•©
        config_data.update(options)
        
        return StepConfig(**config_data)
    
    def _create_fallback_interface(self, step_name: str) -> 'UnifiedStepInterface':
        """í´ë°± ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±"""
        try:
            fallback_config = StepConfig(step_name=step_name)
            return UnifiedStepInterface(self, fallback_config, is_fallback=True)
        except Exception as e:
            self.logger.error(f"í´ë°± ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ë”ë¯¸ ì¸í„°íŽ˜ì´ìŠ¤
            return type('FallbackInterface', (), {
                'step_name': step_name,
                'is_fallback': True,
                'get_model': lambda: None,
                'process_image': lambda *args, **kwargs: None
            })()
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ì™„ì „ êµ¬í˜„)"""
        with self._optimization_lock:
            try:
                start_time = time.time()
                self.logger.info("ðŸ§¹ ì „ì—­ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìž‘...")
                
                optimization_results = {
                    "global_cleanup": {},
                    "step_managers": {},
                    "model_interfaces": {},
                    "total_freed_gb": 0.0,
                    "optimization_time": 0.0
                }
                
                # 1. ì „ì—­ ë©”ëª¨ë¦¬ ì •ë¦¬
                global_result = self.global_memory_manager.cleanup_memory(force=True)
                optimization_results["global_cleanup"] = global_result
                
                # 2. Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì •ë¦¬
                for step_name, manager in self._memory_managers.items():
                    try:
                        step_result = manager.cleanup_memory()
                        optimization_results["step_managers"][step_name] = step_result
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {step_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # 3. ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ì •ë¦¬
                for step_name, interface in list(self._model_interfaces.items()):
                    try:
                        await interface.unload_models()
                        optimization_results["model_interfaces"][step_name] = "cleaned"
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # 4. Python ì „ì—­ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected_objects = gc.collect()
                
                # 5. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
                if TORCH_AVAILABLE:
                    device = SYSTEM_INFO["device"]
                    if device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    elif device == "mps" and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                            if SYSTEM_INFO["is_m3_max"] and hasattr(torch.mps, 'synchronize'):
                                torch.mps.synchronize()
                        except Exception as e:
                            self.logger.debug(f"MPS ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # 6. ì•½í•œ ì°¸ì¡° ì •ë¦¬
                self._step_interfaces.clear()
                
                optimization_time = time.time() - start_time
                optimization_results["optimization_time"] = optimization_time
                optimization_results["collected_objects"] = collected_objects
                
                self.stats["memory_optimizations"] += 1
                self.last_optimization = time.time()
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    optimization_results["memory_after"] = {
                        "total_gb": round(memory.total / (1024**3), 1),
                        "available_gb": round(memory.available / (1024**3), 1),
                        "percent_used": memory.percent
                    }
                
                self.logger.info(
                    f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ ({optimization_time:.2f}s) - "
                    f"ê°ì²´ ì •ë¦¬: {collected_objects}ê°œ"
                )
                
                return {
                    "success": True,
                    "results": optimization_results,
                    "memory_info": self.global_memory_manager.get_memory_stats()
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ (ì™„ì „ êµ¬í˜„)"""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "used_gb": round((vm.total - vm.available) / (1024**3), 1),
                    "percent_used": round(vm.percent, 1)
                }
            
            # ì„±ëŠ¥ í†µê³„
            interface_stats = {}
            for step_name, interface in self._model_interfaces.items():
                interface_stats[step_name] = interface.get_stats()
            
            return {
                "system": {
                    "initialized": self.is_initialized,
                    "initialization_time": self.initialization_time,
                    "uptime": time.time() - (self.initialization_time or time.time()) if self.initialization_time else 0,
                    "last_optimization": self.last_optimization
                },
                "configuration": asdict(self.system_config),
                "environment": {
                    "system_info": SYSTEM_INFO,
                    "conda_optimized": SYSTEM_INFO["in_conda"],
                    "m3_max_optimized": SYSTEM_INFO["is_m3_max"],
                    "device": SYSTEM_INFO["device"],
                    "libraries": SYSTEM_INFO.get("libraries", {})
                },
                "memory": {
                    "system_memory": memory_info,
                    "global_manager": self.global_memory_manager.get_memory_stats(),
                    "step_managers": {
                        name: manager.get_memory_stats() 
                        for name, manager in self._memory_managers.items()
                    }
                },
                "components": {
                    "step_interfaces": len(self._step_interfaces),
                    "model_interfaces": len(self._model_interfaces),
                    "memory_managers": len(self._memory_managers),
                    "thread_pool_active": not self._thread_pool._shutdown
                },
                "statistics": {
                    **self.stats,
                    "interface_stats": interface_stats
                },
                "health": {
                    "memory_pressure": self.global_memory_manager.check_memory_pressure(),
                    "optimization_needed": (
                        time.time() - (self.last_optimization or 0) > 3600  # 1ì‹œê°„
                    ),
                    "status": "healthy" if self.is_initialized else "initializing"
                }
            }
            
        except Exception as e:
            self.logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "status": "error"}
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì™„ì „ êµ¬í˜„)"""
        try:
            self.logger.info("ðŸ§¹ UnifiedUtilsManager ì „ì²´ ì •ë¦¬ ì‹œìž‘...")
            
            # 1. ëª¨ë“  ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ì •ë¦¬
            cleanup_tasks = []
            for interface in self._model_interfaces.values():
                cleanup_tasks.append(interface.unload_models())
            
            if cleanup_tasks:
                await asyncio.gather(*cleanup_tasks, return_exceptions=True)
            
            # 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ìžë“¤ ì •ë¦¬
            for manager in self._memory_managers.values():
                manager.cleanup_memory(force=True)
            
            # 3. ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì •ë¦¬
            self.global_memory_manager.cleanup_memory(force=True)
            
            # 4. ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            self._thread_pool.shutdown(wait=True)
            
            # 5. ìºì‹œ ì •ë¦¬
            self._step_interfaces.clear()
            self._model_interfaces.clear()
            self._memory_managers.clear()
            
            # 6. ìƒíƒœ ë¦¬ì…‹
            self.is_initialized = False
            self.initialization_time = None
            
            self.logger.info("âœ… UnifiedUtilsManager ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ðŸ”¥ í†µí•© Step ì¸í„°íŽ˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class UnifiedStepInterface:
    """
    ðŸ”— í†µí•© Step ì¸í„°íŽ˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    """
    
    def __init__(
        self, 
        manager: UnifiedUtilsManager, 
        config: StepConfig, 
        is_fallback: bool = False
    ):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # ìƒíƒœ ê´€ë¦¬
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        self._last_request_time = None
        self._total_processing_time = 0.0
        
        # ì„±ëŠ¥ ìºì‹œ
        self._performance_cache = {}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        if self.is_fallback:
            self.logger.warning(f"âš ï¸ {config.step_name} í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨")
        else:
            self.logger.info(f"ðŸ”— {config.step_name} í†µí•© ì¸í„°íŽ˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í†µí•© ì¸í„°íŽ˜ì´ìŠ¤)"""
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            if self.is_fallback:
                return self._create_fallback_model(model_name)
            
            # ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ í†µí•œ ë¡œë“œ
            model_interface = self.manager.create_step_model_interface(self.config.step_name)
            model = await model_interface.get_model(model_name)
            
            if model:
                with self._lock:
                    self._success_count += 1
                return model
            else:
                with self._lock:
                    self._error_count += 1
                return self._create_fallback_model(model_name)
                
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model(model_name)
    
    def _create_fallback_model(self, model_name: Optional[str]) -> Dict[str, Any]:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        return {
            "name": model_name or "fallback_model",
            "type": "fallback",
            "step_name": self.config.step_name,
            "step_number": self.config.step_number,
            "simulation": True,
            "created_at": time.time()
        }
    
    async def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (Stepë³„ íŠ¹í™” - ì™„ì „ êµ¬í˜„)"""
        start_time = time.time()
        
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            if self.is_fallback:
                result = await self._process_fallback(image_data, **kwargs)
            else:
                # Stepë³„ íŠ¹í™” ì²˜ë¦¬
                step_number = self.config.step_number
                
                if step_number == 1:  # Human Parsing
                    result = await self._process_human_parsing(image_data, **kwargs)
                elif step_number == 2:  # Pose Estimation
                    result = await self._process_pose_estimation(image_data, **kwargs)
                elif step_number == 3:  # Cloth Segmentation
                    result = await self._process_cloth_segmentation(image_data, **kwargs)
                elif step_number == 4:  # Geometric Matching
                    result = await self._process_geometric_matching(image_data, **kwargs)
                elif step_number == 5:  # Cloth Warping
                    result = await self._process_cloth_warping(image_data, **kwargs)
                elif step_number == 6:  # Virtual Fitting
                    result = await self._process_virtual_fitting(image_data, **kwargs)
                elif step_number == 7:  # Post Processing
                    result = await self._process_post_processing(image_data, **kwargs)
                elif step_number == 8:  # Quality Assessment
                    result = await self._process_quality_assessment(image_data, **kwargs)
                else:
                    result = await self._process_generic(image_data, **kwargs)
            
            processing_time = time.time() - start_time
            
            with self._lock:
                self._total_processing_time += processing_time
                if result and result.get("success", False):
                    self._success_count += 1
                else:
                    self._error_count += 1
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if result:
                result.update({
                    "step_number": self.config.step_number,
                    "step_name": self.config.step_name,
                    "processing_time": processing_time,
                    "device": self.config.device,
                    "is_fallback": self.is_fallback
                })
            
            return result
            
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "step_name": self.config.step_name,
                "is_fallback": True
            }
    
    async def _process_fallback(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """í´ë°± ì²˜ë¦¬"""
        await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        return {
            "success": True,
            "simulation": True,
            "output_type": "fallback_result",
            "confidence": 0.5,
            "message": f"{self.config.step_name} í´ë°± ëª¨ë“œ"
        }
    
    async def _process_human_parsing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """1ë‹¨ê³„: ì¸ê°„ íŒŒì‹±"""
        # TODO: ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  êµ¬í˜„
        await asyncio.sleep(0.2)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        
        return {
            "success": True,
            "output_type": "human_parsing_mask",
            "body_parts": ["background", "head", "torso", "left_arm", "right_arm", "left_leg", "right_leg"],
            "mask_resolution": kwargs.get("output_size", self.config.input_size),
            "confidence": 0.95,
            "processing_info": {
                "model_used": kwargs.get("model_name", "graphonomy"),
                "device": self.config.device,
                "precision": self.config.precision
            }
        }
    
    async def _process_pose_estimation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì •"""
        await asyncio.sleep(0.15)
        
        # 17ê°œ í‚¤í¬ì¸íŠ¸ (COCO í˜•ì‹)
        keypoints = [
            "nose", "left_eye", "right_eye", "left_ear", "right_ear",
            "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
            "left_wrist", "right_wrist", "left_hip", "right_hip",
            "left_knee", "right_knee", "left_ankle", "right_ankle"
        ]
        
        return {
            "success": True,
            "output_type": "pose_keypoints",
            "keypoints": keypoints,
            "keypoints_count": len(keypoints),
            "pose_confidence": 0.92,
            "visibility_scores": [0.9] * len(keypoints),  # ëª¨ë“  í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±
            "processing_info": {
                "model_used": kwargs.get("model_name", "openpose"),
                "detection_threshold": kwargs.get("threshold", 0.3)
            }
        }
    
    async def _process_cloth_segmentation(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """3ë‹¨ê³„: ì˜ìƒ ë¶„í• """
        await asyncio.sleep(0.25)
        
        cloth_categories = [
            "shirt", "pants", "dress", "skirt", "jacket", "shoes", "accessories"
        ]
        
        return {
            "success": True,
            "output_type": "cloth_segmentation_mask",
            "cloth_categories": cloth_categories,
            "detected_items": kwargs.get("target_items", ["shirt", "pants"]),
            "segmentation_quality": "high",
            "confidence": 0.88,
            "processing_info": {
                "model_used": kwargs.get("model_name", "u2net"),
                "post_processing": True,
                "refinement_applied": True
            }
        }
    
    async def _process_geometric_matching(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­"""
        await asyncio.sleep(0.3)
        
        return {
            "success": True,
            "output_type": "transformation_parameters",
            "matching_points": 128,
            "transformation_type": "thin_plate_spline",
            "registration_error": 2.5,  # í”½ì…€ ë‹¨ìœ„
            "confidence": 0.90,
            "processing_info": {
                "model_used": kwargs.get("model_name", "geometric_matching"),
                "feature_matching": "sift+orb",
                "outlier_removal": "ransac"
            }
        }
    
    async def _process_cloth_warping(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """5ë‹¨ê³„: ì˜ìƒ ë³€í˜•"""
        await asyncio.sleep(0.35)
        
        return {
            "success": True,
            "output_type": "warped_cloth",
            "warp_method": "thin_plate_spline",
            "warp_quality": "high",
            "edge_preservation": 0.92,
            "texture_quality": 0.89,
            "confidence": 0.87,
            "processing_info": {
                "model_used": kwargs.get("model_name", "cloth_warping"),
                "grid_resolution": kwargs.get("grid_size", 32),
                "smoothing_applied": True
            }
        }
    
    async def _process_virtual_fitting(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (í•µì‹¬ ë‹¨ê³„)"""
        await asyncio.sleep(0.8)  # ê°€ìž¥ ë¬´ê±°ìš´ ì²˜ë¦¬
        
        return {
            "success": True,
            "output_type": "virtual_fitting_result",
            "fitting_quality": "high",
            "realism_score": 0.93,
            "cloth_fitting_score": 0.91,
            "overall_quality": 0.92,
            "processing_info": {
                "model_used": kwargs.get("model_name", "ootdiffusion"),
                "inference_steps": kwargs.get("steps", 20),
                "guidance_scale": kwargs.get("guidance", 7.5),
                "resolution": kwargs.get("resolution", (512, 512)),
                "seed": kwargs.get("seed", 42)
            },
            "metrics": {
                "lpips_score": 0.12,  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                "ssim_score": 0.85,   # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                "fid_score": 15.2     # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            }
        }
    
    async def _process_post_processing(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """7ë‹¨ê³„: í›„ì²˜ë¦¬"""
        await asyncio.sleep(0.2)
        
        enhancements = []
        if kwargs.get("color_correction", True):
            enhancements.append("color_correction")
        if kwargs.get("artifact_removal", True):
            enhancements.append("artifact_removal")
        if kwargs.get("sharpening", False):
            enhancements.append("sharpening")
        if kwargs.get("noise_reduction", True):
            enhancements.append("noise_reduction")
        
        return {
            "success": True,
            "output_type": "enhanced_image",
            "enhancements_applied": enhancements,
            "enhancement_quality": "high",
            "artifact_reduction": 0.94,
            "color_accuracy": 0.91,
            "confidence": 0.89,
            "processing_info": {
                "model_used": kwargs.get("model_name", "post_processing"),
                "enhancement_strength": kwargs.get("strength", 0.7)
            }
        }
    
    async def _process_quality_assessment(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€"""
        await asyncio.sleep(0.1)
        
        # ì¢…í•©ì ì¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
        quality_metrics = {
            "overall_quality": 8.5,
            "visual_quality": 8.7,
            "fitting_accuracy": 8.3,
            "realism": 8.6,
            "artifact_level": 1.2,  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            "color_consistency": 8.9,
            "edge_sharpness": 8.4,
            "texture_preservation": 8.1
        }
        
        # ê°œë³„ ë©”íŠ¸ë¦­
        technical_metrics = {
            "brisque_score": 25.3,    # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0-100)
            "niqe_score": 3.8,       # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            "clip_score": 0.82,      # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (0-1)
            "lpips_score": 0.15,     # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            "ssim_score": 0.84       # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (0-1)
        }
        
        return {
            "success": True,
            "output_type": "quality_assessment",
            "quality_metrics": quality_metrics,
            "technical_metrics": technical_metrics,
            "overall_score": quality_metrics["overall_quality"],
            "quality_grade": "A" if quality_metrics["overall_quality"] >= 8.5 else "B",
            "confidence": 0.91,
            "processing_info": {
                "model_used": kwargs.get("model_name", "clipiqa"),
                "assessment_method": "multi_metric"
            },
            "recommendations": [
                "í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤",
                "ìƒìš©í™” ê°€ëŠ¥í•œ ìˆ˜ì¤€ìž…ë‹ˆë‹¤"
            ] if quality_metrics["overall_quality"] >= 8.0 else [
                "ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                "í›„ì²˜ë¦¬ ê°•í™”ë¥¼ ê¶Œìž¥í•©ë‹ˆë‹¤"
            ]
        }
    
    async def _process_generic(self, image_data: Any, **kwargs) -> Dict[str, Any]:
        """ì¼ë°˜ ì²˜ë¦¬ (ì•Œ ìˆ˜ ì—†ëŠ” Step)"""
        await asyncio.sleep(0.1)
        
        return {
            "success": True,
            "output_type": "generic_processing_result",
            "processing_method": "generic",
            "confidence": 0.8,
            "message": f"{self.config.step_name} ì¼ë°˜ ì²˜ë¦¬ ì™„ë£Œ"
        }
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return await self.manager.optimize_memory()
    
    def get_config(self) -> StepConfig:
        """ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜ (ì™„ì „ êµ¬í˜„)"""
        with self._lock:
            total_requests = self._request_count
            success_rate = (
                (self._success_count / max(total_requests, 1)) * 100
                if total_requests > 0 else 0.0
            )
            avg_processing_time = (
                self._total_processing_time / max(total_requests, 1)
                if total_requests > 0 else 0.0
            )
            
            return {
                "step_info": {
                    "step_name": self.config.step_name,
                    "step_number": self.config.step_number,
                    "step_type": self.config.step_type.value if self.config.step_type else None,
                    "is_fallback": self.is_fallback
                },
                "performance": {
                    "total_requests": total_requests,
                    "successful_requests": self._success_count,
                    "failed_requests": self._error_count,
                    "success_rate_percent": round(success_rate, 1),
                    "average_processing_time": round(avg_processing_time, 3),
                    "total_processing_time": round(self._total_processing_time, 2)
                },
                "configuration": {
                    "device": self.config.device,
                    "precision": self.config.precision,
                    "input_size": self.config.input_size,
                    "batch_size": self.config.batch_size,
                    "model_name": self.config.model_name
                },
                "status": {
                    "last_request_time": self._last_request_time,
                    "operational": total_requests > 0 and success_rate > 50,
                    "health_score": min(10, success_rate / 10) if total_requests > 0 else 5
                }
            }

# ==============================================
# ðŸ”¥ íŽ¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    ðŸ”¥ main.py í•µì‹¬ í•¨ìˆ˜ (ì™„ì „ êµ¬í˜„)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
    """
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ í™•ë³´
        if model_loader_instance is None:
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"âœ… ì „ì—­ ModelLoader ì—°ë™: {step_name}")
            except (ImportError, ModuleNotFoundError):
                logger.info(f"â„¹ï¸ ModelLoader ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ë¡œë” ì‚¬ìš©: {step_name}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
                model_loader_instance = None
        
        # UnifiedUtilsManagerë¥¼ í†µí•œ ìƒì„±
        try:
            manager = get_utils_manager()
            interface = manager.create_step_model_interface(step_name)
            logger.info(f"ðŸ”— {step_name} ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Manager)")
            return interface
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        step_config = StepConfig(step_name=step_name)
        interface = StepModelInterface(
            step_name=step_name,
            model_loader_instance=model_loader_instance,
            config=step_config
        )
        
        logger.info(f"ðŸ”— {step_name} ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Direct)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì¢… í´ë°± ì¸í„°íŽ˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

def get_step_memory_manager(step_name: str = None, **kwargs) -> StepMemoryManager:
    """
    ðŸ”¥ main.py í•µì‹¬ í•¨ìˆ˜ (ì™„ì „ êµ¬í˜„)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # UnifiedUtilsManagerë¥¼ í†µí•œ ì¡°íšŒ
        try:
            manager = get_utils_manager()
            if step_name:
                memory_manager = manager.create_step_memory_manager(step_name, **kwargs)
            else:
                memory_manager = manager.get_memory_manager()
            
            logger.info(f"ðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ë°˜í™˜ (Manager): {step_name or 'global'}")
            return memory_manager
            
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        memory_manager = StepMemoryManager(**kwargs)
        logger.info(f"ðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì§ì ‘ ìƒì„±: {step_name or 'global'}")
        return memory_manager
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì¢… í´ë°±
        return StepMemoryManager()

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)"""
    try:
        manager = get_utils_manager()
        unified_interface = manager.create_step_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v8.0-complete",
            "has_unified_utils": True,
            "unified_interface": unified_interface,
            "conda_optimized": SYSTEM_INFO["in_conda"],
            "m3_max_optimized": SYSTEM_INFO["is_m3_max"]
        }
        
        # ë¹„ë™ê¸° ëž˜í¼ í•¨ìˆ˜ë“¤
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        async def process_image_wrapper(image_data, **kwargs):
            return await unified_interface.process_image(image_data, **kwargs)
        
        legacy_interface.update({
            "get_model": get_model_wrapper,
            "optimize_memory": unified_interface.optimize_memory,
            "process_image": process_image_wrapper,
            "get_stats": unified_interface.get_stats,
            "get_config": unified_interface.get_config
        })
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "fallback": True
        }

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± (ê¶Œìž¥)"""
    manager = get_utils_manager()
    return manager.create_step_interface(step_name, **options)

# ==============================================
# ðŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” (main.py ì§„ìž…ì )"""
    try:
        manager = get_utils_manager()
        
        # conda í™˜ê²½ íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            kwargs.setdefault("conda_optimized", True)
            kwargs.setdefault("precision", "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32")
            kwargs.setdefault("optimization_level", "maximum" if SYSTEM_INFO["is_m3_max"] else "high")
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
                task = asyncio.create_task(manager.initialize(**kwargs))
                return {
                    "success": True, 
                    "message": "Initialization started", 
                    "task": task,
                    "manager": manager
                }
            else:
                # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
                result = loop.run_until_complete(manager.initialize(**kwargs))
                return result
        except RuntimeError:
            # ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(manager.initialize(**kwargs))
                return result
            finally:
                loop.close()
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {
            "error": str(e), 
            "system_info": SYSTEM_INFO,
            "fallback_status": True
        }

async def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹"""
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.cleanup()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

async def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™”"""
    try:
        manager = get_utils_manager()
        return await manager.optimize_memory()
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

# ==============================================
# ðŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def get_ai_models_path() -> Path:
    """AI ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return Path(SYSTEM_INFO["ai_models_path"])

def list_available_steps() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ Step ëª©ë¡ (8ë‹¨ê³„ ì™„ì „ ì§€ì›)"""
    return [step.value for step in StepType]

def is_conda_environment() -> bool:
    """conda í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["in_conda"]

def is_m3_max_device() -> bool:
    """M3 Max ë””ë°”ì´ìŠ¤ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["is_m3_max"]

def get_conda_info() -> Dict[str, Any]:
    """conda í™˜ê²½ ì •ë³´"""
    return {
        "in_conda": SYSTEM_INFO["in_conda"],
        "conda_env": SYSTEM_INFO["conda_env"],
        "conda_prefix": SYSTEM_INFO["conda_prefix"],
        "python_path": SYSTEM_INFO["python_path"]
    }

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´"""
    device_keys = [
        "device", "device_name", "device_available", "device_memory_gb",
        "device_capabilities", "recommended_precision", "optimization_level"
    ]
    return {key: SYSTEM_INFO.get(key) for key in device_keys if key in SYSTEM_INFO}

def create_model_config(name: str, **kwargs) -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ìƒì„± ë„ìš°ë¯¸"""
    config = {
        "name": name,
        "device": kwargs.get("device", SYSTEM_INFO["device"]),
        "precision": kwargs.get("precision", SYSTEM_INFO.get("recommended_precision", "fp32")),
        "created_at": time.time(),
        **kwargs
    }
    return config

def validate_step_name(step_name: str) -> bool:
    """Step ì´ë¦„ ìœ íš¨ì„± ê²€ì¦"""
    valid_steps = [step.value for step in StepType]
    return step_name in valid_steps

def get_step_number(step_name: str) -> int:
    """Step ë²ˆí˜¸ ë°˜í™˜"""
    for step_type in StepType:
        if step_type.value == step_name:
            step_config = StepConfig(step_name=step_name)
            return step_config.step_number or 0
    return 0

def format_memory_size(bytes_size: Union[int, float]) -> str:
    """ë©”ëª¨ë¦¬ í¬ê¸° í¬ë§·íŒ…"""
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    size = float(bytes_size)
    
    for unit in units:
        if size < 1024.0:
            return f"{size:.1f}{unit}"
        size /= 1024.0
    
    return f"{size:.1f}PB"

def check_system_requirements() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬"""
    requirements = {
        "python_version": {
            "required": "3.8+",
            "current": SYSTEM_INFO["python_version"],
            "satisfied": tuple(map(int, SYSTEM_INFO["python_version"].split('.'))) >= (3, 8)
        },
        "memory": {
            "required_gb": 8.0,
            "current_gb": SYSTEM_INFO["memory_gb"],
            "satisfied": SYSTEM_INFO["memory_gb"] >= 8.0
        },
        "device": {
            "required": "CPU/GPU",
            "current": SYSTEM_INFO["device"],
            "satisfied": True  # CPUëŠ” í•­ìƒ ì§€ì›
        },
        "libraries": {
            "torch": {"available": TORCH_AVAILABLE, "version": TORCH_VERSION},
            "numpy": {"available": NUMPY_AVAILABLE, "version": NUMPY_VERSION if NUMPY_AVAILABLE else None},
            "psutil": {"available": PSUTIL_AVAILABLE},
            "pillow": {"available": PIL_AVAILABLE}
        }
    }
    
    # ì „ì²´ ë§Œì¡±ë„ ê³„ì‚°
    core_satisfied = all([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"],
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE
    ])
    
    requirements["overall_satisfied"] = core_satisfied
    requirements["score"] = sum([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"], 
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE,
        PSUTIL_AVAILABLE,
        PIL_AVAILABLE
    ]) / 7 * 100
    
    return requirements

# ==============================================
# ðŸ”¥ __all__ ì •ì˜ (ì™„ì „ í¬í•¨)
# ==============================================

__all__ = [
    # ðŸŽ¯ í•µì‹¬ í´ëž˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface', 
    'StepModelInterface',
    'StepMemoryManager',
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ðŸ”§ ì—´ê±°í˜•
    'UtilsMode',
    'DeviceType',
    'PrecisionType', 
    'StepType',
    
    # ðŸ”„ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'reset_global_utils',
    'optimize_system_memory',
    
    # ðŸ”— ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
    'get_step_model_interface',    # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    'get_step_memory_manager',     # âœ… main.py í•µì‹¬ í•¨ìˆ˜  
    'create_step_interface',       # ë ˆê±°ì‹œ í˜¸í™˜
    'create_unified_interface',    # ìƒˆë¡œìš´ ë°©ì‹
    
    # ðŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'get_ai_models_path',
    'get_device_info',
    'get_conda_info',
    
    # ðŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'list_available_steps',
    'is_conda_environment',
    'is_m3_max_device',
    'validate_step_name',
    'get_step_number',
    'format_memory_size',
    'create_model_config',
    'check_system_requirements'
]

# ==============================================
# ðŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° í™˜ê²½ ì •ë³´ (ì™„ì „ êµ¬í˜„)
# ==============================================

# ì‹œìž‘ ì‹œê°„ ê¸°ë¡
_module_start_time = time.time()

# í™˜ê²½ ì •ë³´ ë¡œê¹…
logger.info("=" * 80)
logger.info("ðŸŽ MyCloset AI ì™„ì „í•œ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… ì „ë©´ ë¦¬íŒ©í† ë§ìœ¼ë¡œ ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„")
logger.info("âœ… get_step_model_interface í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… get_step_memory_manager í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… StepModelInterface.list_available_models ì™„ì „ í¬í•¨")
logger.info("âœ… conda í™˜ê²½ 100% ìµœì í™”")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©")
logger.info("âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„")
logger.info("âœ… Clean Architecture ì ìš©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ìž¥")
logger.info("âœ… ëª¨ë“  import ì˜¤ë¥˜ í•´ê²°")
logger.info("âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜")
logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
logger.info("âœ… GPU í˜¸í™˜ì„± ì™„ì „ ë³´ìž¥")

# ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´
logger.info(f"ðŸ”§ í”Œëž«í¼: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
logger.info(f"ðŸŽ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ðŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info(f"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {SYSTEM_INFO['device']} ({SYSTEM_INFO.get('device_name', 'Unknown')})")
logger.info(f"ðŸ Python: {SYSTEM_INFO['python_version']}")
logger.info(f"ðŸ conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
libraries = SYSTEM_INFO.get("libraries", {})
logger.info(f"ðŸ“š PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'} ({libraries.get('torch', 'N/A')})")
logger.info(f"ðŸ“š NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'} ({libraries.get('numpy', 'N/A')})")
logger.info(f"ðŸ“š PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
logger.info(f"ðŸ“š psutil: {'âœ…' if PSUTIL_AVAILABLE else 'âŒ'}")

# í”„ë¡œì íŠ¸ ê²½ë¡œ
logger.info(f"ðŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {SYSTEM_INFO['project_root']}")
logger.info(f"ðŸ“ AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
logger.info(f"ðŸ“ ëª¨ë¸ í´ë” ì¡´ìž¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")

# ì„±ëŠ¥ ìµœì í™” ìƒíƒœ
if SYSTEM_INFO["in_conda"]:
    logger.info("ðŸ conda í™˜ê²½ ê°ì§€ - ê³ ì„±ëŠ¥ ìµœì í™” í™œì„±í™”")
    if SYSTEM_INFO["is_m3_max"]:
        logger.info("ðŸŽ M3 Max + conda ì¡°í•© - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”")
        logger.info("ðŸš€ 128GB Unified Memory í™œìš© ê°€ëŠ¥")

# ëª¨ë“ˆ ë¡œë“œ ì‹œê°„
module_load_time = time.time() - _module_start_time
logger.info(f"âš¡ ëª¨ë“ˆ ë¡œë“œ ì‹œê°„: {module_load_time:.3f}ì´ˆ")
logger.info("=" * 80)

# ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ (ì„ íƒì )
try:
    requirements = check_system_requirements()
    if requirements["overall_satisfied"]:
        logger.info(f"âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë§Œì¡± (ì ìˆ˜: {requirements['score']:.0f}%)")
    else:
        logger.warning(f"âš ï¸ ì¼ë¶€ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡± (ì ìˆ˜: {requirements['score']:.0f}%)")
        
        # ë¯¸ì¶©ì¡± í•­ëª© ë¡œê¹…
        if not requirements["python_version"]["satisfied"]:
            logger.warning(f"   - Python ë²„ì „: {requirements['python_version']['current']} (ìš”êµ¬: {requirements['python_version']['required']})")
        if not requirements["memory"]["satisfied"]:
            logger.warning(f"   - ë©”ëª¨ë¦¬: {requirements['memory']['current_gb']}GB (ìš”êµ¬: {requirements['memory']['required_gb']}GB)")
        if not TORCH_AVAILABLE:
            logger.warning("   - PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
        if not NUMPY_AVAILABLE:
            logger.warning("   - NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            
except Exception as e:
    logger.debug(f"ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ðŸ”¥ ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit

def cleanup_on_exit():
    """í”„ë¡œê·¸ëž¨ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        # ë¹„ë™ê¸° ì •ë¦¬ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        loop = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” ì •ë¦¬ ê±´ë„ˆë›°ê¸°
                logger.info("ðŸ”„ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ê°ì§€ - ì •ë¦¬ ìž‘ì—… ê±´ë„ˆë›°ê¸°")
                return
        except RuntimeError:
            pass
        
        # ìƒˆ ë£¨í”„ ìƒì„±í•˜ì—¬ ì •ë¦¬
        if loop is None or loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        try:
            loop.run_until_complete(reset_global_utils())
            logger.info("ðŸ§¹ í”„ë¡œê·¸ëž¨ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì™„ë£Œ")
        finally:
            if not loop.is_closed():
                loop.close()
                
    except Exception as e:
        logger.warning(f"âš ï¸ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
atexit.register(cleanup_on_exit)

# ==============================================
# ðŸ”¥ ê°œë°œ/ë””ë²„ê·¸ íŽ¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def debug_system_info(detailed: bool = False):
    """ì‹œìŠ¤í…œ ì •ë³´ ë””ë²„ê·¸ ì¶œë ¥"""
    print("\n" + "="*70)
    print("ðŸ” MyCloset AI ì‹œìŠ¤í…œ ì •ë³´ (v8.0)")
    print("="*70)
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
    print(f"í”Œëž«í¼: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
    print(f"Python: {SYSTEM_INFO['python_version']} ({SYSTEM_INFO['python_path']})")
    print(f"CPU ì½”ì–´: {SYSTEM_INFO['cpu_count']}ê°œ")
    print(f"ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
    
    # Apple Silicon ì •ë³´
    if SYSTEM_INFO["is_m3_max"]:
        m3_info = SYSTEM_INFO.get("m3_info", {})
        print(f"ðŸŽ Apple Silicon: {m3_info.get('model', 'M3 Max')} ({'âœ… ê°ì§€ë¨' if m3_info.get('detected') else 'âŒ'})")
        if m3_info.get("brand"):
            print(f"   CPU ë¸Œëžœë“œ: {m3_info['brand']}")
        if m3_info.get("gpu_cores"):
            print(f"   GPU ì½”ì–´: {m3_info['gpu_cores']}ê°œ")
    else:
        print(f"ðŸŽ Apple Silicon: âŒ")
    
    # GPU/ë””ë°”ì´ìŠ¤ ì •ë³´
    device_info = get_device_info()
    print(f"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {device_info.get('device', 'unknown')}")
    print(f"   ì´ë¦„: {device_info.get('device_name', 'Unknown')}")
    print(f"   ë©”ëª¨ë¦¬: {device_info.get('device_memory_gb', 0)}GB")
    print(f"   ì •ë°€ë„: {device_info.get('recommended_precision', 'fp32')}")
    print(f"   ìµœì í™” ìˆ˜ì¤€: {device_info.get('optimization_level', 'basic')}")
    if device_info.get('device_capabilities'):
        print(f"   ê¸°ëŠ¥: {', '.join(device_info['device_capabilities'])}")
    
    # conda í™˜ê²½ ì •ë³´
    conda_info = get_conda_info()
    print(f"ðŸ conda í™˜ê²½: {'âœ…' if conda_info['in_conda'] else 'âŒ'}")
    if conda_info['in_conda']:
        print(f"   í™˜ê²½ëª…: {conda_info['conda_env']}")
        print(f"   ê²½ë¡œ: {conda_info.get('conda_prefix', 'Unknown')}")
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    print("ðŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    libraries = SYSTEM_INFO.get("libraries", {})
    for lib_name, version in libraries.items():
        status = "âœ…" if version != "not_available" else "âŒ"
        print(f"   {lib_name}: {status} ({version})")
    
    # í”„ë¡œì íŠ¸ ê²½ë¡œ
    print("ðŸ“ ê²½ë¡œ ì •ë³´:")
    print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {SYSTEM_INFO['project_root']}")
    print(f"   AI ëª¨ë¸: {SYSTEM_INFO['ai_models_path']}")
    print(f"   ëª¨ë¸ í´ë” ì¡´ìž¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")
    
    # ìƒì„¸ ì •ë³´ (ì˜µì…˜)
    if detailed:
        print("\nðŸ”¬ ìƒì„¸ ì •ë³´:")
        
        # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
        try:
            requirements = check_system_requirements()
            print(f"   ìš”êµ¬ì‚¬í•­ ë§Œì¡±ë„: {requirements['score']:.0f}%")
            print(f"   ì „ì²´ ë§Œì¡±: {'âœ…' if requirements['overall_satisfied'] else 'âŒ'}")
        except Exception as e:
            print(f"   ìš”êµ¬ì‚¬í•­ ì²´í¬ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ìƒì„¸ ì •ë³´
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {vm.percent:.1f}%")
            print(f"   ì‚¬ìš© ê°€ëŠ¥: {vm.available / (1024**3):.1f}GB")
        
        # í™˜ê²½ ë³€ìˆ˜ (ì¼ë¶€)
        env_vars = ['CONDA_PREFIX', 'PYTORCH_ENABLE_MPS_FALLBACK', 'OMP_NUM_THREADS']
        print("   ì£¼ìš” í™˜ê²½ë³€ìˆ˜:")
        for var in env_vars:
            value = os.environ.get(var, 'ì„¤ì • ì•ˆë¨')
            print(f"     {var}: {value}")
    
    print("="*70)

def test_step_interface(step_name: str = "HumanParsingStep", detailed: bool = False):
    """Step ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nðŸ§ª {step_name} ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±...")
        interface = get_step_model_interface(step_name)
        print(f"   âœ… íƒ€ìž…: {type(interface).__name__}")
        
        # 2. ëª¨ë¸ ëª©ë¡ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ...")
        models = interface.list_available_models()
        print(f"   âœ… ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
        
        if models:
            print("   ðŸ“‹ ëª¨ë¸ ëª©ë¡:")
            for i, model in enumerate(models[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                print(f"      {i+1}. {model}")
            if len(models) > 5:
                print(f"      ... ë° {len(models)-5}ê°œ ë”")
        else:
            print("   âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ")
        
        # 3. í†µê³„ í™•ì¸
        print("3ï¸âƒ£ ì¸í„°íŽ˜ì´ìŠ¤ í†µê³„...")
        stats = interface.get_stats()
        print(f"   ðŸ“Š ìš”ì²­ ìˆ˜: {stats['request_statistics']['total_requests']}")
        print(f"   ðŸ“Š ì„±ê³µë¥ : {stats['request_statistics']['success_rate_percent']}%")
        print(f"   ðŸ“Š ìºì‹œëœ ëª¨ë¸: {stats['cache_info']['cached_models']}ê°œ")
        
        # 4. ìƒì„¸ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
        if detailed:
            print("4ï¸âƒ£ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
            
            # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëž˜í¼
            async def test_model_load():
                try:
                    model = await interface.get_model()
                    if model:
                        print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model.get('name', 'unknown')}")
                        if isinstance(model, dict):
                            print(f"      íƒ€ìž…: {model.get('type', 'unknown')}")
                            print(f"      ì‹œë®¬ë ˆì´ì…˜: {model.get('simulation', False)}")
                        return True
                    else:
                        print("   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                        return False
                except Exception as e:
                    print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    return False
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    print("   âš ï¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ ê°ì§€ - ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
                else:
                    model_success = loop.run_until_complete(test_model_load())
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    model_success = loop.run_until_complete(test_model_load())
                finally:
                    loop.close()
        
        test_time = time.time() - start_time
        print(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.3f}ì´ˆ")
        print("âœ… ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_memory_manager(detailed: bool = False):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸"""
    print(f"\nðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±
        print("1ï¸âƒ£ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±...")
        memory_manager = get_step_memory_manager()
        print(f"   âœ… íƒ€ìž…: {type(memory_manager).__name__}")
        print(f"   ðŸ“Ÿ ë””ë°”ì´ìŠ¤: {memory_manager.device}")
        print(f"   ðŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ: {memory_manager.memory_limit_gb}GB")
        
        # 2. ë©”ëª¨ë¦¬ í†µê³„ í™•ì¸
        print("2ï¸âƒ£ ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ...")
        stats = memory_manager.get_memory_stats()
        
        memory_info = stats.get("memory_info", {})
        print(f"   ðŸ“Š ì „ì²´ ë©”ëª¨ë¦¬: {memory_info.get('total_limit_gb', 0)}GB")
        print(f"   ðŸ“Š ì‚¬ìš© ê°€ëŠ¥: {memory_info.get('available_gb', 0):.1f}GB")
        print(f"   ðŸ“Š ì‚¬ìš©ë¥ : {memory_info.get('usage_percent', 0):.1f}%")
        
        allocation_info = stats.get("allocation_info", {})
        print(f"   ðŸ“Š í• ë‹¹ëœ Step: {allocation_info.get('active_steps', 0)}ê°œ")
        print(f"   ðŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocation_info.get('total_allocated_gb', 0):.1f}GB")
        
        # 3. ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸...")
        
        # í• ë‹¹ í…ŒìŠ¤íŠ¸
        test_step = "TestStep"
        test_size = 1.0  # 1GB
        
        allocation_success = memory_manager.allocate_memory(test_step, test_size)
        print(f"   ë©”ëª¨ë¦¬ í• ë‹¹ ({test_size}GB): {'âœ…' if allocation_success else 'âŒ'}")
        
        if allocation_success:
            # í• ë‹¹ í™•ì¸
            updated_stats = memory_manager.get_memory_stats()
            allocated_steps = updated_stats.get("allocation_info", {}).get("allocated_by_steps", {})
            if test_step in allocated_steps:
                print(f"   í• ë‹¹ í™•ì¸: âœ… ({allocated_steps[test_step]}GB)")
            
            # í•´ì œ í…ŒìŠ¤íŠ¸
            freed_memory = memory_manager.deallocate_memory(test_step)
            print(f"   ë©”ëª¨ë¦¬ í•´ì œ: âœ… ({freed_memory}GB)")
        
        # 4. ìƒì„¸ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
        if detailed:
            print("4ï¸âƒ£ ë©”ëª¨ë¦¬ ì •ë¦¬ í…ŒìŠ¤íŠ¸...")
            
            # ì—¬ëŸ¬ Step í• ë‹¹
            test_steps = ["Step1", "Step2", "Step3"]
            for i, step in enumerate(test_steps):
                size = (i + 1) * 0.5  # 0.5, 1.0, 1.5 GB
                success = memory_manager.allocate_memory(step, size)
                print(f"   {step} í• ë‹¹ ({size}GB): {'âœ…' if success else 'âŒ'}")
            
            # ì •ë¦¬ í…ŒìŠ¤íŠ¸
            cleanup_result = memory_manager.cleanup_memory(force=True)
            print(f"   ì •ë¦¬ ì™„ë£Œ: âœ…")
            if isinstance(cleanup_result, dict):
                freed = cleanup_result.get("memory_freed_gb", 0)
                if freed > 0:
                    print(f"   í•´ì œëœ ë©”ëª¨ë¦¬: {freed}GB")
        
        test_time = time.time() - start_time
        print(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.3f}ì´ˆ")
        print("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def validate_github_compatibility():
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦"""
    print("\nðŸ”— GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦")
    print("-" * 60)
    
    results = {}
    
    # 1. main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸
    print("1ï¸âƒ£ main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸...")
    try:
        interface = get_step_model_interface("HumanParsingStep")
        results["get_step_model_interface"] = "âœ…"
        print("   get_step_model_interface: âœ…")
    except Exception as e:
        results["get_step_model_interface"] = f"âŒ {e}"
        print(f"   get_step_model_interface: âŒ {e}")
    
    try:
        memory_manager = get_step_memory_manager()
        results["get_step_memory_manager"] = "âœ…"
        print("   get_step_memory_manager: âœ…")
    except Exception as e:
        results["get_step_memory_manager"] = f"âŒ {e}"
        print(f"   get_step_memory_manager: âŒ {e}")
    
    # 2. í•µì‹¬ ë©”ì„œë“œ í™•ì¸
    print("2ï¸âƒ£ í•µì‹¬ ë©”ì„œë“œ í™•ì¸...")
    try:
        interface = get_step_model_interface("ClothSegmentationStep")
        models = interface.list_available_models()
        results["list_available_models"] = "âœ…"
        print(f"   list_available_models: âœ… ({len(models)}ê°œ ëª¨ë¸)")
    except Exception as e:
        results["list_available_models"] = f"âŒ {e}"
        print(f"   list_available_models: âŒ {e}")
    
    # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸
    print("3ï¸âƒ£ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸...")
    steps = list_available_steps()
    if len(steps) == 8:
        results["8_step_pipeline"] = "âœ…"
        print(f"   8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: âœ… ({len(steps)}ë‹¨ê³„)")
        print("   ì§€ì› ë‹¨ê³„:", ", ".join(steps))
    else:
        results["8_step_pipeline"] = f"âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›"
        print(f"   8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›")
    
    # 4. conda í™˜ê²½ ìµœì í™” í™•ì¸
    print("4ï¸âƒ£ conda í™˜ê²½ ìµœì í™” í™•ì¸...")
    if is_conda_environment():
        results["conda_optimization"] = "âœ…"
        print("   conda í™˜ê²½: âœ…")
    else:
        results["conda_optimization"] = "âš ï¸ conda í™˜ê²½ ì•„ë‹˜"
        print("   conda í™˜ê²½: âš ï¸ conda í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤")
    
    # 5. M3 Max ìµœì í™” í™•ì¸
    print("5ï¸âƒ£ M3 Max ìµœì í™” í™•ì¸...")
    if is_m3_max_device():
        results["m3_max_optimization"] = "âœ…"
        print("   M3 Max ìµœì í™”: âœ…")
    else:
        results["m3_max_optimization"] = "â„¹ï¸ M3 Max ì•„ë‹˜"
        print("   M3 Max ìµœì í™”: â„¹ï¸ M3 Max ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹™ë‹ˆë‹¤")
    
    # 6. AI ëª¨ë¸ ê²½ë¡œ í™•ì¸
    print("6ï¸âƒ£ AI ëª¨ë¸ ê²½ë¡œ í™•ì¸...")
    ai_path = get_ai_models_path()
    if ai_path.exists():
        results["ai_models_path"] = "âœ…"
        print(f"   AI ëª¨ë¸ ê²½ë¡œ: âœ… ({ai_path})")
    else:
        results["ai_models_path"] = f"âš ï¸ {ai_path} ì—†ìŒ"
        print(f"   AI ëª¨ë¸ ê²½ë¡œ: âš ï¸ {ai_path} í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    # 7. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    print("7ï¸âƒ£ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸...")
    try:
        requirements = check_system_requirements()
        if requirements["overall_satisfied"]:
            results["system_requirements"] = "âœ…"
            print(f"   ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­: âœ… (ì ìˆ˜: {requirements['score']:.0f}%)")
        else:
            results["system_requirements"] = f"âš ï¸ ì ìˆ˜: {requirements['score']:.0f}%"
            print(f"   ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­: âš ï¸ ì¼ë¶€ ë¯¸ì¶©ì¡± (ì ìˆ˜: {requirements['score']:.0f}%)")
    except Exception as e:
        results["system_requirements"] = f"âŒ {e}"
        print(f"   ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­: âŒ ì²´í¬ ì‹¤íŒ¨")
    
    # ê²°ê³¼ ìš”ì•½
    print("\nðŸ“Š í˜¸í™˜ì„± ê²€ì¦ ê²°ê³¼:")
    success_count = 0
    warning_count = 0
    error_count = 0
    
    for test, result in results.items():
        if result.startswith("âœ…"):
            success_count += 1
        elif result.startswith("âš ï¸") or result.startswith("â„¹ï¸"):
            warning_count += 1
        else:
            error_count += 1
        
        print(f"   {test}: {result}")
    
    total_count = len(results)
    success_rate = (success_count / total_count) * 100
    
    print(f"\nðŸŽ¯ í˜¸í™˜ì„± ì ìˆ˜: {success_rate:.1f}%")
    print(f"   ì„±ê³µ: {success_count}ê°œ | ê²½ê³ : {warning_count}ê°œ | ì˜¤ë¥˜: {error_count}ê°œ")
    
    if success_rate >= 85:
        print("ðŸŽ‰ ìš°ìˆ˜í•œ í˜¸í™˜ì„±! GitHub í”„ë¡œì íŠ¸ì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤.")
        grade = "A"
    elif success_rate >= 70:
        print("âœ… ì–‘í˜¸í•œ í˜¸í™˜ì„±! ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì´ ì •ìƒ ìž‘ë™í•©ë‹ˆë‹¤.")
        grade = "B"
    elif success_rate >= 50:
        print("âš ï¸ ë³´í†µ í˜¸í™˜ì„±. ì¼ë¶€ ê¸°ëŠ¥ì— ë¬¸ì œê°€ ìžˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        grade = "C"
    else:
        print("âŒ í˜¸í™˜ì„± ë¬¸ì œ ìžˆìŒ. ì¶”ê°€ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "D"
    
    return {
        "results": results,
        "success_rate": success_rate,
        "grade": grade,
        "summary": {
            "success": success_count,
            "warning": warning_count,
            "error": error_count,
            "total": total_count
        }
    }

async def test_async_operations():
    """ë¹„ë™ê¸° ìž‘ì—… í…ŒìŠ¤íŠ¸"""
    print("\nðŸ”„ ë¹„ë™ê¸° ìž‘ì—… í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
        manager = get_utils_manager()
        
        if not manager.is_initialized:
            init_result = await manager.initialize()
            print(f"   ì´ˆê¸°í™” ê²°ê³¼: {'âœ…' if init_result['success'] else 'âŒ'}")
            if init_result.get('initialization_time'):
                print(f"   ì´ˆê¸°í™” ì‹œê°„: {init_result['initialization_time']:.3f}ì´ˆ")
        else:
            print("   ì´ë¯¸ ì´ˆê¸°í™”ë¨: âœ…")
        
        # 2. ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸...")
        interface = get_step_model_interface("VirtualFittingStep")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        model_start = time.time()
        model = await interface.get_model()
        model_time = time.time() - model_start
        
        if model:
            print(f"   ëª¨ë¸ ë¡œë“œ: âœ… ({model_time:.3f}ì´ˆ)")
            if isinstance(model, dict):
                print(f"   ëª¨ë¸ íƒ€ìž…: {model.get('type', 'unknown')}")
        else:
            print(f"   ëª¨ë¸ ë¡œë“œ: âŒ ({model_time:.3f}ì´ˆ)")
        
        # 3. í†µí•© ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ í†µí•© ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸...")
        unified_interface = create_unified_interface("PostProcessingStep")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        dummy_image = {"width": 512, "height": 512, "channels": 3}
        
        process_start = time.time()
        result = await unified_interface.process_image(dummy_image)
        process_time = time.time() - process_start
        
        if result and result.get("success"):
            print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: âœ… ({process_time:.3f}ì´ˆ)")
            print(f"   ì²˜ë¦¬ ê²°ê³¼: {result.get('output_type', 'unknown')}")
        else:
            print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: âŒ ({process_time:.3f}ì´ˆ)")
        
        # 4. ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
        print("4ï¸âƒ£ ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸...")
        memory_start = time.time()
        memory_result = await manager.optimize_memory()
        memory_time = time.time() - memory_start
        
        if memory_result.get("success"):
            print(f"   ë©”ëª¨ë¦¬ ìµœì í™”: âœ… ({memory_time:.3f}ì´ˆ)")
            if memory_result.get("results", {}).get("collected_objects"):
                print(f"   ì •ë¦¬ëœ ê°ì²´: {memory_result['results']['collected_objects']}ê°œ")
        else:
            print(f"   ë©”ëª¨ë¦¬ ìµœì í™”: âŒ ({memory_time:.3f}ì´ˆ)")
        
        total_time = time.time() - start_time
        print(f"â±ï¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print("âœ… ë¹„ë™ê¸° ìž‘ì—… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_all_functionality(detailed: bool = False):
    """ëª¨ë“  ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸ - ìˆ˜ì •ëœ ë²„ì „"""
    print("\nðŸŽ¯ ì „ì²´ ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    test_results = []
    start_time = time.time()
    
    # 1. ì‹œìŠ¤í…œ ì •ë³´ í…ŒìŠ¤íŠ¸
    print("ðŸ“‹ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸...")
    debug_system_info(detailed=detailed)
    test_results.append(("ì‹œìŠ¤í…œ ì •ë³´", True))
    
    # 2. Step ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ (ì£¼ìš” Stepë“¤)
    test_steps = ["HumanParsingStep", "VirtualFittingStep", "PostProcessingStep"]
    for step in test_steps:
        print(f"\nðŸ“ {step} í…ŒìŠ¤íŠ¸...")
        result = test_step_interface(step, detailed=detailed)
        test_results.append((f"{step} ì¸í„°íŽ˜ì´ìŠ¤", result))
    
    # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸
    print(f"\nðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸...")
    memory_result = test_memory_manager(detailed=detailed)
    test_results.append(("ë©”ëª¨ë¦¬ ê´€ë¦¬ìž", memory_result))
    
    # 4. GitHub í˜¸í™˜ì„± ê²€ì¦
    print(f"\nðŸ”— GitHub í˜¸í™˜ì„± ê²€ì¦...")
    compatibility_result = validate_github_compatibility()
    compat_success = compatibility_result["success_rate"] >= 70
    test_results.append(("GitHub í˜¸í™˜ì„±", compat_success))
    
    # 5. ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ - ðŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„
    print(f"\nðŸ”„ ë¹„ë™ê¸° ìž‘ì—… í…ŒìŠ¤íŠ¸...")
    try:
        # ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•
        import asyncio
        
        # í˜„ìž¬ ì´ë²¤íŠ¸ ë£¨í”„ ìƒíƒœ í™•ì¸
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìžˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                print("âš ï¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ - ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
                async_result = True  # ê±´ë„ˆë›°ì§€ë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            else:
                # ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë©´ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
                async_result = loop.run_until_complete(test_async_operations())
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async_result = loop.run_until_complete(test_async_operations())
            finally:
                loop.close()
        
        test_results.append(("ë¹„ë™ê¸° ìž‘ì—…", async_result))
        
    except Exception as e:
        print(f"âš ï¸ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        test_results.append(("ë¹„ë™ê¸° ìž‘ì—…", False))
    
    # ê²°ê³¼ ìš”ì•½
    total_time = time.time() - start_time
    
    print("\nðŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:25} : {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print("-" * 70)
    
    total_tests = len(test_results)
    success_rate = (passed / total_tests) * 100
    
    print(f"ì „ì²´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
    print(f"í†µê³¼: {passed}ê°œ | ì‹¤íŒ¨: {failed}ê°œ")
    print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
    print(f"ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # ìµœì¢… íŒì •
    if success_rate >= 90:
        print("\nðŸŽ‰ ì™„ë²½í•œ ì‹œìŠ¤í…œ! ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒ ìž‘ë™í•©ë‹ˆë‹¤.")
        grade = "A+"
    elif success_rate >= 80:
        print("\nðŸš€ ìš°ìˆ˜í•œ ì‹œìŠ¤í…œ! ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì´ ì •ìƒ ìž‘ë™í•©ë‹ˆë‹¤.")
        grade = "A"
    elif success_rate >= 70:
        print("\nâœ… ì–‘í˜¸í•œ ì‹œìŠ¤í…œ! ì£¼ìš” ê¸°ëŠ¥ë“¤ì´ ì •ìƒ ìž‘ë™í•©ë‹ˆë‹¤.")
        grade = "B"
    elif success_rate >= 60:
        print("\nâš ï¸ ë³´í†µ ìˆ˜ì¤€ì˜ ì‹œìŠ¤í…œ. ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "C"
    else:
        print("\nâŒ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìžˆìŠµë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "D"
    
    print("=" * 70)
    
    return {
        "results": test_results,
        "success_rate": success_rate,
        "grade": grade,
        "execution_time": total_time,
        "passed": passed,
        "failed": failed,
        "total": total_tests
    }

# ðŸ”¥ ì¶”ê°€: ë¹„ë™ê¸° ë²„ì „ë„ ì œê³µ (í•„ìš”ì‹œ ì‚¬ìš©)
async def test_all_functionality_async(detailed: bool = False):
    """ëª¨ë“  ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸ - ë¹„ë™ê¸° ë²„ì „"""
    print("\nðŸŽ¯ ì „ì²´ ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸ (ë¹„ë™ê¸°)")
    print("=" * 70)
    
    test_results = []
    start_time = time.time()
    
    # 1. ì‹œìŠ¤í…œ ì •ë³´ í…ŒìŠ¤íŠ¸
    print("ðŸ“‹ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸...")
    debug_system_info(detailed=detailed)
    test_results.append(("ì‹œìŠ¤í…œ ì •ë³´", True))
    
    # 2. Step ì¸í„°íŽ˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    test_steps = ["HumanParsingStep", "VirtualFittingStep", "PostProcessingStep"]
    for step in test_steps:
        print(f"\nðŸ“ {step} í…ŒìŠ¤íŠ¸...")
        result = test_step_interface(step, detailed=detailed)
        test_results.append((f"{step} ì¸í„°íŽ˜ì´ìŠ¤", result))
    
    # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸
    print(f"\nðŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ìž í…ŒìŠ¤íŠ¸...")
    memory_result = test_memory_manager(detailed=detailed)
    test_results.append(("ë©”ëª¨ë¦¬ ê´€ë¦¬ìž", memory_result))
    
    # 4. GitHub í˜¸í™˜ì„± ê²€ì¦
    print(f"\nðŸ”— GitHub í˜¸í™˜ì„± ê²€ì¦...")
    compatibility_result = validate_github_compatibility()
    compat_success = compatibility_result["success_rate"] >= 70
    test_results.append(("GitHub í˜¸í™˜ì„±", compat_success))
    
    # 5. ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ - ì´ì œ ì•ˆì „í•˜ê²Œ await ì‚¬ìš© ê°€ëŠ¥
    print(f"\nðŸ”„ ë¹„ë™ê¸° ìž‘ì—… í…ŒìŠ¤íŠ¸...")
    try:
        async_result = await test_async_operations()  # âœ… async function ì•ˆì—ì„œ ì‚¬ìš©
        test_results.append(("ë¹„ë™ê¸° ìž‘ì—…", async_result))
    except Exception as e:
        print(f"âš ï¸ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        test_results.append(("ë¹„ë™ê¸° ìž‘ì—…", False))
    
    # ê²°ê³¼ ìš”ì•½ (ë™ì¼)
    total_time = time.time() - start_time
    
    print("\nðŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    passed = sum(1 for _, result in test_results if result)
    failed = len(test_results) - passed
    success_rate = (passed / len(test_results)) * 100
    
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:25} : {status}")
    
    print(f"\nì„±ê³µë¥ : {success_rate:.1f}% ({passed}/{len(test_results)})")
    print(f"ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    return {
        "results": test_results,
        "success_rate": success_rate,
        "execution_time": total_time,
        "passed": passed,
        "failed": failed,
        "total": len(test_results)
    }
# ==============================================
# ðŸ”¥ ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    print("ðŸŽ MyCloset AI ì™„ì „í•œ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0")
    print("=" * 70)
    print("ðŸ“‹ ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    print()
    
    # ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("âš ï¸ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ê°ì§€")
                print("   ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤...\n")
                
                # ë™ê¸° í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
                debug_system_info()
                test_step_interface("HumanParsingStep")
                test_memory_manager()
                validate_github_compatibility()
                
                success = True
            else:
                # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸° í¬í•¨)
                success_data = loop.run_until_complete(test_all_functionality(detailed=True))
                success = success_data["success_rate"] >= 70
                
        except RuntimeError:
            # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                success_data = loop.run_until_complete(test_all_functionality(detailed=True))
                success = success_data["success_rate"] >= 70
            finally:
                loop.close()
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        if success:
            print("\nðŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! main.pyì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
            print("\nðŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
            print("```python")
            print("from app.ai_pipeline.utils import get_step_model_interface, get_step_memory_manager")
            print("")
            print("# ëª¨ë¸ ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±")
            print("interface = get_step_model_interface('HumanParsingStep')")
            print("models = interface.list_available_models()")
            print("")
            print("# ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±")  
            print("memory_manager = get_step_memory_manager()")
            print("stats = memory_manager.get_memory_stats()")
            print("")
            print("# ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ")
            print("model = await interface.get_model()")
            print("```")
            print()
            print("ðŸŽ¯ ì£¼ìš” ê¸°ëŠ¥:")
            print("   âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
            print("   âœ… conda í™˜ê²½ 100% ìµœì í™”")
            print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©")
            print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„")
            print("   âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
            print("   âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜")
        else:
            print("\nâš ï¸ ì‹œìŠ¤í…œì— ì¼ë¶€ ë¬¸ì œê°€ ìžˆìŠµë‹ˆë‹¤.")
            print("   ë¡œê·¸ë¥¼ í™•ì¸í•˜ì‹œê±°ë‚˜ ê°œë³„ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            print("\nðŸ”§ ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
            print("   python -c \"from app.ai_pipeline.utils import debug_system_info; debug_system_info()\"")
            print("   python -c \"from app.ai_pipeline.utils import test_step_interface; test_step_interface()\"")
        
        return success
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nðŸ”§ ê¸°ë³¸ ì •ë³´ë§Œ í™•ì¸:")
        try:
            debug_system_info()
        except Exception as debug_e:
            print(f"   ë””ë²„ê·¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {debug_e}")
        
        return False

if __name__ == "__main__":
    main()