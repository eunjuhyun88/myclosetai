# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ v3.0 - ì™„ì „ ì¬êµ¬ì„±
âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš© + ì‹¤ì œ ëª¨ë¸ ìë™ íƒì§€
âœ… M3 Max 128GB ìµœì í™” ì„¤ê³„
âœ… Step í´ë˜ìŠ¤ ì™„ë²½ í˜¸í™˜ + í”„ë¡œë•ì…˜ ì•ˆì •ì„±
âœ… ë‹¨ìˆœí•¨ + í¸ì˜ì„± + í™•ì¥ì„± + ì¼ê´€ì„±
ğŸ”¥ í•µì‹¬: ëª¨ë“  Step í´ë˜ìŠ¤ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í†µí•© ì‹œìŠ¤í…œ
"""

import os
import gc
import sys
import time
import logging
import asyncio
import threading
from typing import Dict, Any, Optional, Union, List, Callable, Tuple
from pathlib import Path
from functools import lru_cache
import weakref

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€ ë° ì„¤ì •
# ==============================================

@lru_cache(maxsize=1)
def _detect_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ (ìºì‹œë¨)"""
    try:
        import platform
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": sys.version_info[:3]
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        try:
            if platform.system() == 'Darwin':
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                is_m3_max = 'M3' in result.stdout
        except:
            pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # GPU ê°ì§€
        gpu_type = "ì—†ìŒ"
        try:
            import torch
            if torch.backends.mps.is_available():
                gpu_type = "MPS (Apple Silicon)"
            elif torch.cuda.is_available():
                gpu_type = f"CUDA ({torch.cuda.get_device_name(0)})"
        except:
            pass
        
        system_info["gpu_type"] = gpu_type
        
        # ë©”ëª¨ë¦¬ ê°ì§€
        try:
            import psutil
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        except:
            system_info["memory_gb"] = 16  # ê¸°ë³¸ê°’
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "Unknown",
            "is_m3_max": False,
            "gpu_type": "ì—†ìŒ",
            "memory_gb": 16
        }

# ì‹œìŠ¤í…œ ì •ë³´ ì „ì—­ ë³€ìˆ˜
SYSTEM_INFO = _detect_system_info()
IS_M3_MAX = SYSTEM_INFO["is_m3_max"]
MEMORY_GB = SYSTEM_INFO["memory_gb"]

# ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
@lru_cache(maxsize=1)
def _detect_default_device() -> str:
    """ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ê°ì§€"""
    try:
        import torch
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    except ImportError:
        return "cpu"

DEFAULT_DEVICE = _detect_default_device()

# PyTorch ê°€ìš©ì„± í™•ì¸
try:
    import torch
    TORCH_AVAILABLE = True
    logger.info(f"âœ… PyTorch ì‚¬ìš© ê°€ëŠ¥ - ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€")

# ==============================================
# ğŸ”¥ í•µì‹¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆë“¤ ì•ˆì „í•œ Import
# ==============================================

# 1. MemoryManager - ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
try:
    from .memory_manager import (
        MemoryManager,
        MemoryStats,
        create_memory_manager,
        get_memory_manager,
        get_global_memory_manager,
        initialize_global_memory_manager,
        optimize_memory_usage,
        optimize_memory,
        check_memory,
        check_memory_available,
        get_memory_info,
        memory_efficient
    )
    MEMORY_MANAGER_AVAILABLE = True
    logger.info("âœ… MemoryManager ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    MEMORY_MANAGER_AVAILABLE = False
    logger.warning(f"âš ï¸ MemoryManager import ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ í•µì‹¬: í´ë°± MemoryManager (ê¸°ë³¸ ê¸°ëŠ¥ ì œê³µ)
    class MemoryManager:
        def __init__(self, device="auto", **kwargs):
            self.device = device if device != "auto" else DEFAULT_DEVICE
            self.logger = logging.getLogger(f"{__name__}.FallbackMemoryManager")
            
        async def initialize(self) -> bool:
            return True
            
        def get_memory_stats(self):
            return {"device": self.device, "status": "fallback"}
            
        def check_memory_pressure(self):
            return {"status": "normal", "message": "fallback mode"}
            
        def clear_cache(self, aggressive=False):
            if TORCH_AVAILABLE:
                import gc
                gc.collect()
                
        async def cleanup(self):
            self.clear_cache()
    
    # í´ë°± í•¨ìˆ˜ë“¤
    def create_memory_manager(device="auto", **kwargs):
        return MemoryManager(device=device, **kwargs)
    
    def get_memory_manager(**kwargs):
        return create_memory_manager(**kwargs)
    
    def get_global_memory_manager(**kwargs):
        return get_memory_manager(**kwargs)
    
    def initialize_global_memory_manager(device="mps", **kwargs):
        return create_memory_manager(device=device, **kwargs)
    
    def optimize_memory_usage(device=None, aggressive=False):
        manager = create_memory_manager(device=device or DEFAULT_DEVICE)
        manager.clear_cache(aggressive=aggressive)
        return {"success": True, "device": manager.device}
    
    def check_memory_available(min_gb=1.0):
        return True  # í´ë°±ì—ì„œëŠ” í•­ìƒ True
    
    def get_memory_info():
        return {"device": DEFAULT_DEVICE, "fallback": True}

# 2. ModelLoader - AI ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ
try:
    from .model_loader import (
        ModelLoader,
        ModelConfig,
        ModelFormat,
        ModelType,
        ModelPriority,
        LoadedModel,
        StepModelInterface,
        BaseStepMixin,
        create_model_loader,
        get_global_model_loader,
        initialize_global_model_loader,
        cleanup_global_loader,
        load_model_async,
        load_model_sync,
        preprocess_image,
        postprocess_segmentation,
        postprocess_pose,
        # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
        BaseModel,
        GraphonomyModel,
        OpenPoseModel,
        U2NetModel,
        GeometricMatchingModel,
        EnhancementModel,
        CLIPModel
    )
    MODEL_LOADER_AVAILABLE = True
    logger.info("âœ… ModelLoader ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logger.warning(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ í•µì‹¬: í´ë°± ModelLoader (ê¸°ë³¸ ê¸°ëŠ¥ ì œê³µ)
    from enum import Enum
    from dataclasses import dataclass
    
    class ModelType(Enum):
        HUMAN_PARSING = "human_parsing"
        POSE_ESTIMATION = "pose_estimation"
        CLOTH_SEGMENTATION = "cloth_segmentation"
        GEOMETRIC_MATCHING = "geometric_matching"
        CLOTH_WARPING = "cloth_warping"
        VIRTUAL_FITTING = "virtual_fitting"
        POST_PROCESSING = "post_processing"
        QUALITY_ASSESSMENT = "quality_assessment"
    
    class ModelFormat(Enum):
        PYTORCH = "pytorch"
        SAFETENSORS = "safetensors"
        DIFFUSERS = "diffusers"
        ONNX = "onnx"
    
    class ModelPriority(Enum):
        CRITICAL = 1
        HIGH = 2
        MEDIUM = 3
        LOW = 4
    
    @dataclass
    class ModelConfig:
        name: str
        model_type: ModelType
        model_class: str
        checkpoint_path: Optional[str] = None
        device: str = "auto"
        precision: str = "fp16"
        input_size: Tuple[int, int] = (512, 512)
        priority: ModelPriority = ModelPriority.MEDIUM
        metadata: Dict[str, Any] = None
    
    class ModelLoader:
        def __init__(self, device=None, **kwargs):
            self.device = device if device else DEFAULT_DEVICE
            self.logger = logging.getLogger(f"{__name__}.FallbackModelLoader")
            self.model_configs = {}
            self.loaded_models = {}
            
        async def initialize(self) -> bool:
            return True
            
        def register_model(self, name: str, config: ModelConfig) -> bool:
            self.model_configs[name] = config
            return True
            
        async def load_model(self, name: str, force_reload=False):
            self.logger.warning(f"í´ë°± ëª¨ë“œ: {name} ëª¨ë¸ ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜")
            return None
            
        def list_models(self):
            return list(self.model_configs.keys())
        
        def cleanup(self):
            self.loaded_models.clear()
    
    class StepModelInterface:
        def __init__(self, model_loader, step_name):
            self.model_loader = model_loader
            self.step_name = step_name
            
        async def get_model(self, model_name=None):
            return None
            
        def cleanup(self):
            pass
    
    class BaseStepMixin:
        def _setup_model_interface(self, model_loader=None):
            self.model_interface = StepModelInterface(model_loader or ModelLoader(), self.__class__.__name__)
            
        async def get_model(self, model_name=None):
            if hasattr(self, 'model_interface'):
                return await self.model_interface.get_model(model_name)
            return None
    
    # í´ë°± íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    def create_model_loader(device="auto", **kwargs):
        return ModelLoader(device=device, **kwargs)
    
    def get_global_model_loader():
        return create_model_loader()
    
    def initialize_global_model_loader(**kwargs):
        return {"success": True, "message": "Fallback mode initialized"}
    
    def cleanup_global_loader():
        pass
    
    async def load_model_async(model_name):
        return None
    
    def load_model_sync(model_name):
        return None
    
    def preprocess_image(image, target_size=(512, 512), normalize=True, device="cpu"):
        logger.warning("í´ë°± ëª¨ë“œ: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
        return None
    
    def postprocess_segmentation(output, original_size, threshold=0.5):
        logger.warning("í´ë°± ëª¨ë“œ: ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
        return None
    
    def postprocess_pose(output, original_size, confidence_threshold=0.3):
        logger.warning("í´ë°± ëª¨ë“œ: í¬ì¦ˆ í›„ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
        return {"keypoints": [], "num_keypoints": 0}

# 3. DataConverter - ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ
try:
    from .data_converter import (
        DataConverter,
        create_data_converter,
        get_global_data_converter,
        initialize_global_data_converter,
        quick_image_to_tensor,
        quick_tensor_to_image
    )
    DATA_CONVERTER_AVAILABLE = True
    logger.info("âœ… DataConverter ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    DATA_CONVERTER_AVAILABLE = False
    logger.warning(f"âš ï¸ DataConverter import ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ í•µì‹¬: í´ë°± DataConverter (ê¸°ë³¸ ê¸°ëŠ¥ ì œê³µ)
    class DataConverter:
        def __init__(self, device=None, **kwargs):
            self.device = device if device else DEFAULT_DEVICE
            self.logger = logging.getLogger(f"{__name__}.FallbackDataConverter")
            self.default_size = kwargs.get('default_size', (512, 512))
            
        async def initialize(self) -> bool:
            return True
            
        def image_to_tensor(self, image, size=None, normalize=False, **kwargs):
            self.logger.warning("í´ë°± ëª¨ë“œ: ì´ë¯¸ì§€â†’í…ì„œ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜")
            return None
            
        def tensor_to_image(self, tensor, denormalize=False, format="PIL"):
            self.logger.warning("í´ë°± ëª¨ë“œ: í…ì„œâ†’ì´ë¯¸ì§€ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜")
            return None
            
        def batch_convert_images(self, images, target_format="tensor", **kwargs):
            return [None] * len(images)
            
        def resize_image(self, image, size, method="bilinear", preserve_aspect_ratio=False):
            self.logger.warning("í´ë°± ëª¨ë“œ: ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜")
            return image
    
    # í´ë°± íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    def create_data_converter(default_size=(512, 512), device="auto", **kwargs):
        return DataConverter(device=device, default_size=default_size, **kwargs)
    
    def get_global_data_converter():
        return create_data_converter()
    
    def initialize_global_data_converter(**kwargs):
        return create_data_converter(**kwargs)
    
    def quick_image_to_tensor(image, size=(512, 512)):
        return None
    
    def quick_tensor_to_image(tensor):
        return None

# 4. ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ (ì„ íƒì )
try:
    from .auto_model_detector import (
        AdvancedModelDetector,
        AdvancedModelLoaderAdapter,
        DetectedModel,
        ModelCategory,
        create_advanced_detector,
        quick_model_detection,
        detect_and_integrate_with_model_loader,
        export_model_registry_code,
        validate_model_paths,
        benchmark_model_loading
    )
    AUTO_MODEL_DETECTOR_AVAILABLE = True
    logger.info("âœ… AutoModelDetector ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    AUTO_MODEL_DETECTOR_AVAILABLE = False
    logger.warning(f"âš ï¸ AutoModelDetector import ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ í´ë°± (ìë™ íƒì§€ ì—†ì´)
    def quick_model_detection(**kwargs):
        return {"total_models": 0, "message": "Auto detection not available"}

# ==============================================
# ğŸ”¥ í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ
# ==============================================

class UtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ê´€ë¦¬ì - ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
    âœ… ëª¨ë“  ìœ í‹¸ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©
    âœ… Step í´ë˜ìŠ¤ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self.logger = logging.getLogger(f"{__name__}.UtilsManager")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.memory_manager = None
        self.model_loader = None
        self.data_converter = None
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        self._initialized = True
        
        self.logger.info("ğŸ¯ UtilsManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    async def initialize(
        self,
        device: Optional[str] = None,
        memory_gb: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™”"""
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        start_time = time.time()
        device = device or DEFAULT_DEVICE
        memory_gb = memory_gb or MEMORY_GB
        
        self.logger.info(f"ğŸš€ UtilsManager ì´ˆê¸°í™” ì‹œì‘ - ë””ë°”ì´ìŠ¤: {device}")
        
        results = {
            "memory_manager": False,
            "model_loader": False,
            "data_converter": False,
            "errors": []
        }
        
        try:
            # 1. MemoryManager ì´ˆê¸°í™”
            if MEMORY_MANAGER_AVAILABLE:
                try:
                    self.memory_manager = create_memory_manager(
                        device=device,
                        memory_gb=memory_gb,
                        is_m3_max=IS_M3_MAX,
                        optimization_enabled=True,
                        **kwargs
                    )
                    await self.memory_manager.initialize()
                    results["memory_manager"] = True
                    self.logger.info("âœ… MemoryManager ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    results["errors"].append(f"MemoryManager: {e}")
                    self.logger.error(f"âŒ MemoryManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. ModelLoader ì´ˆê¸°í™”
            if MODEL_LOADER_AVAILABLE:
                try:
                    self.model_loader = create_model_loader(
                        device=device,
                        memory_limit_gb=memory_gb,
                        auto_scan=True,
                        **kwargs
                    )
                    await self.model_loader.initialize()
                    results["model_loader"] = True
                    self.logger.info("âœ… ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    results["errors"].append(f"ModelLoader: {e}")
                    self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 3. DataConverter ì´ˆê¸°í™”
            if DATA_CONVERTER_AVAILABLE:
                try:
                    self.data_converter = create_data_converter(
                        device=device,
                        default_size=kwargs.get('default_size', (512, 512)),
                        is_m3_max=IS_M3_MAX,
                        **kwargs
                    )
                    await self.data_converter.initialize()
                    results["data_converter"] = True
                    self.logger.info("âœ… DataConverter ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    results["errors"].append(f"DataConverter: {e}")
                    self.logger.error(f"âŒ DataConverter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 4. í´ë°± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            if not self.memory_manager:
                self.memory_manager = create_memory_manager(device=device)
            if not self.model_loader:
                self.model_loader = create_model_loader(device=device)
            if not self.data_converter:
                self.data_converter = create_data_converter(device=device)
            
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            success_count = sum(results[key] for key in ["memory_manager", "model_loader", "data_converter"])
            
            self.logger.info(f"ğŸ‰ UtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            self.logger.info(f"ğŸ“Š ì„±ê³µí•œ ì»´í¬ë„ŒíŠ¸: {success_count}/3")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "device": device,
                "components": results,
                "system_info": SYSTEM_INFO
            }
            
        except Exception as e:
            self.logger.error(f"âŒ UtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "components": results
            }
    
    def create_step_interface(self, step_name: str) -> Dict[str, Any]:
        """Step í´ë˜ìŠ¤ìš© í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            interface = {
                "step_name": step_name,
                "memory_manager": self.memory_manager,
                "model_loader": self.model_loader,
                "data_converter": self.data_converter,
                "get_model": self._create_get_model_func(step_name),
                "process_image": self._create_process_image_func(),
                "optimize_memory": self._create_optimize_memory_func(),
                "logger": logging.getLogger(f"steps.{step_name}")
            }
            
            self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _create_get_model_func(self, step_name: str) -> Callable:
        """ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ìƒì„±"""
        async def get_model(model_name: Optional[str] = None):
            try:
                if self.model_loader:
                    if hasattr(self.model_loader, 'create_step_interface'):
                        interface = self.model_loader.create_step_interface(step_name)
                        return await interface.get_model(model_name)
                    else:
                        return await self.model_loader.load_model(model_name)
                return None
            except Exception as e:
                self.logger.error(f"âŒ {step_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
        return get_model
    
    def _create_process_image_func(self) -> Callable:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±"""
        def process_image(image, operation="to_tensor", **kwargs):
            try:
                if self.data_converter:
                    if operation == "to_tensor":
                        return self.data_converter.image_to_tensor(image, **kwargs)
                    elif operation == "from_tensor":
                        return self.data_converter.tensor_to_image(image, **kwargs)
                    elif operation == "resize":
                        return self.data_converter.resize_image(image, **kwargs)
                return None
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return None
        return process_image
    
    def _create_optimize_memory_func(self) -> Callable:
        """ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜ ìƒì„±"""
        async def optimize_memory(aggressive: bool = False):
            try:
                if self.memory_manager:
                    if hasattr(self.memory_manager, 'smart_cleanup'):
                        self.memory_manager.smart_cleanup()
                    elif hasattr(self.memory_manager, 'clear_cache'):
                        self.memory_manager.clear_cache(aggressive=aggressive)
                    return {"success": True}
                return {"success": False, "message": "MemoryManager not available"}
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                return {"success": False, "error": str(e)}
        return optimize_memory
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = {
                "is_initialized": self.is_initialized,
                "initialization_time": self.initialization_time,
                "system_info": SYSTEM_INFO,
                "components": {
                    "memory_manager": {
                        "available": MEMORY_MANAGER_AVAILABLE,
                        "initialized": self.memory_manager is not None
                    },
                    "model_loader": {
                        "available": MODEL_LOADER_AVAILABLE,
                        "initialized": self.model_loader is not None
                    },
                    "data_converter": {
                        "available": DATA_CONVERTER_AVAILABLE,
                        "initialized": self.data_converter is not None
                    }
                }
            }
            
            # ìƒì„¸ ì •ë³´ ì¶”ê°€
            if self.memory_manager and hasattr(self.memory_manager, 'get_memory_stats'):
                try:
                    status["memory_stats"] = self.memory_manager.get_memory_stats().__dict__
                except:
                    pass
            
            if self.model_loader and hasattr(self.model_loader, 'get_system_info'):
                try:
                    status["model_loader_info"] = self.model_loader.get_system_info()
                except:
                    pass
            
            return status
            
        except Exception as e:
            return {"error": str(e)}
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'cleanup'):
                self.model_loader.cleanup()
            
            if self.memory_manager and hasattr(self.memory_manager, 'cleanup'):
                await self.memory_manager.cleanup()
            
            self.is_initialized = False
            self.logger.info("âœ… UtilsManager ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ UtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ í•¨ìˆ˜ë“¤ - Step í´ë˜ìŠ¤ì—ì„œ ë°”ë¡œ ì‚¬ìš©
# ==============================================

# ì „ì—­ UtilsManager ì¸ìŠ¤í„´ìŠ¤
_global_utils_manager: Optional[UtilsManager] = None
_utils_lock = threading.Lock()

def get_utils_manager() -> UtilsManager:
    """ì „ì—­ UtilsManager ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_utils_manager
    
    with _utils_lock:
        if _global_utils_manager is None:
            _global_utils_manager = UtilsManager()
        return _global_utils_manager

def initialize_global_utils(device: Optional[str] = None, **kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì´ˆê¸°í™” - main.pyì—ì„œ ì‚¬ìš©
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('auto', 'mps', 'cuda', 'cpu')
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        ì´ˆê¸°í™” ê²°ê³¼ ì •ë³´
    """
    try:
        manager = get_utils_manager()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            future = asyncio.create_task(manager.initialize(device=device, **kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            result = loop.run_until_complete(manager.initialize(device=device, **kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ Step í´ë˜ìŠ¤ìš© ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ëª¨ë“  Stepì—ì„œ ì‚¬ìš©
    
    Args:
        step_name: Step í´ë˜ìŠ¤ ì´ë¦„
    
    Returns:
        Stepìš© í†µí•© ì¸í„°í˜ì´ìŠ¤
    """
    try:
        manager = get_utils_manager()
        return manager.create_step_interface(step_name)
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_system_status()
    except Exception as e:
        return {"error": str(e)}

def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹"""
    global _global_utils_manager
    
    try:
        with _utils_lock:
            if _global_utils_manager:
                asyncio.create_task(_global_utils_manager.cleanup())
                _global_utils_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜)
def get_memory_manager_instance(**kwargs):
    """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    manager = get_utils_manager()
    return manager.memory_manager or create_memory_manager(**kwargs)

def get_model_loader_instance(**kwargs):
    """ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    manager = get_utils_manager()
    return manager.model_loader or create_model_loader(**kwargs)

def get_data_converter_instance(**kwargs):
    """ë°ì´í„° ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    manager = get_utils_manager()
    return manager.data_converter or create_data_converter(**kwargs)

# ==============================================
# ğŸ”¥ __all__ ì •ì˜ - ëª¨ë“  export ì •ë¦¬
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UtilsManager',
    
    # ğŸ”¥ ì£¼ìš” ì „ì—­ í•¨ìˆ˜ë“¤ (Stepì—ì„œ ì‚¬ìš©)
    'get_utils_manager',
    'initialize_global_utils',
    'create_step_interface',
    'get_system_status',
    'reset_global_utils',
    
    # ğŸ“¦ ì»´í¬ë„ŒíŠ¸ ì¸ìŠ¤í„´ìŠ¤ í•¨ìˆ˜ë“¤
    'get_memory_manager_instance',
    'get_model_loader_instance', 
    'get_data_converter_instance',
    
    # ğŸ ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'DEFAULT_DEVICE',
    'TORCH_AVAILABLE',
]

# MemoryManager ëª¨ë“ˆì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€
if MEMORY_MANAGER_AVAILABLE:
    __all__.extend([
        'MemoryManager',
        'MemoryStats',
        'create_memory_manager',
        'get_memory_manager',
        'get_global_memory_manager',
        'initialize_global_memory_manager',
        'optimize_memory_usage',
        'optimize_memory',
        'check_memory',
        'check_memory_available',
        'get_memory_info',
        'memory_efficient'
    ])

# ModelLoader ëª¨ë“ˆì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€
if MODEL_LOADER_AVAILABLE:
    __all__.extend([
        'ModelLoader',
        'ModelConfig',
        'ModelFormat',
        'ModelType',
        'ModelPriority',
        'LoadedModel',
        'StepModelInterface',
        'BaseStepMixin',
        'create_model_loader',
        'get_global_model_loader',
        'initialize_global_model_loader',
        'cleanup_global_loader',
        'load_model_async',
        'load_model_sync',
        'preprocess_image',
        'postprocess_segmentation',
        'postprocess_pose',
        # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
        'BaseModel',
        'GraphonomyModel',
        'OpenPoseModel',
        'U2NetModel',
        'GeometricMatchingModel',
        'EnhancementModel',
        'CLIPModel'
    ])

# DataConverter ëª¨ë“ˆì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€
if DATA_CONVERTER_AVAILABLE:
    __all__.extend([
        'DataConverter',
        'create_data_converter',
        'get_global_data_converter',
        'initialize_global_data_converter',
        'quick_image_to_tensor',
        'quick_tensor_to_image'
    ])

# AutoModelDetector ëª¨ë“ˆì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€
if AUTO_MODEL_DETECTOR_AVAILABLE:
    __all__.extend([
        'AdvancedModelDetector',
        'AdvancedModelLoaderAdapter',
        'DetectedModel',
        'ModelCategory',
        'create_advanced_detector',
        'quick_model_detection',
        'detect_and_integrate_with_model_loader',
        'export_model_registry_code',
        'validate_model_paths',
        'benchmark_model_loading'
    ])

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹… ë° ìµœì¢… ì„¤ì •
# ==============================================

def _log_initialization_summary():
    """ì´ˆê¸°í™” ìš”ì•½ ë¡œê¹…"""
    available_count = sum([
        MEMORY_MANAGER_AVAILABLE,
        MODEL_LOADER_AVAILABLE, 
        DATA_CONVERTER_AVAILABLE,
        AUTO_MODEL_DETECTOR_AVAILABLE
    ])
    
    total_count = 4
    
    logger.info("=" * 70)
    logger.info("ğŸ MyCloset AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v3.0 - ì™„ì „ ì¬êµ¬ì„±")
    logger.info("=" * 70)
    logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ìœ í‹¸ë¦¬í‹°: {available_count}/{total_count}")
    logger.info(f"   - MemoryManager: {'âœ…' if MEMORY_MANAGER_AVAILABLE else 'âŒ (í´ë°± ì‚¬ìš©)'}")
    logger.info(f"   - ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ (í´ë°± ì‚¬ìš©)'}")
    logger.info(f"   - DataConverter: {'âœ…' if DATA_CONVERTER_AVAILABLE else 'âŒ (í´ë°± ì‚¬ìš©)'}")
    logger.info(f"   - AutoModelDetector: {'âœ…' if AUTO_MODEL_DETECTOR_AVAILABLE else 'âŒ'}")
    logger.info(f"ğŸ ì‹œìŠ¤í…œ ì •ë³´:")
    logger.info(f"   - Platform: {SYSTEM_INFO['platform']}")
    logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    logger.info(f"   - Memory: {MEMORY_GB}GB")
    logger.info(f"   - GPU: {SYSTEM_INFO['gpu_type']}")
    logger.info(f"   - ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")
    logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
    
    if available_count >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒ ì‚¬ìš© ê°€ëŠ¥
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    elif available_count > 0:
        logger.warning(f"âš ï¸ ì¼ë¶€ ìœ í‹¸ë¦¬í‹°ë§Œ ì‚¬ìš© ê°€ëŠ¥ ({available_count}/{total_count}) - í´ë°± ëª¨ë“œë¡œ ë™ì‘")
    else:
        logger.error("âŒ ëª¨ë“  ìœ í‹¸ë¦¬í‹° ì‚¬ìš© ë¶ˆê°€ - ì˜ì¡´ì„± ì„¤ì¹˜ í•„ìš”")
    
    logger.info("=" * 70)

# ì´ˆê¸°í™” ìš”ì•½ ì¶œë ¥
_log_initialization_summary()

# ìë™ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
try:
    auto_init = os.getenv('AUTO_INIT_PIPELINE_UTILS', 'false').lower() in ('true', '1', 'yes', 'on')
    if auto_init:
        device = os.getenv('PIPELINE_DEVICE', DEFAULT_DEVICE)
        result = initialize_global_utils(device=device)
        if result.get('success'):
            logger.info("ğŸš€ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ìë™ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
except Exception as e:
    logger.warning(f"âš ï¸ ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit

def _cleanup_on_exit():
    """í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        reset_global_utils()
        logger.info("ğŸ”š íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ì¢…ë£Œ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass

atexit.register(_cleanup_on_exit)

# ìµœì¢… í™•ì¸ ë©”ì‹œì§€
if MEMORY_MANAGER_AVAILABLE or MODEL_LOADER_AVAILABLE or DATA_CONVERTER_AVAILABLE:
    logger.info("ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´ AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° v3.0 ì¤€ë¹„ ì™„ë£Œ")
    logger.info("ğŸ”¥ Step í´ë˜ìŠ¤ì—ì„œ create_step_interface() í•¨ìˆ˜ë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥")
else:
    logger.error("ğŸ’¥ ëª¨ë“  ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - í´ë°± ëª¨ë“œë¡œë§Œ ë™ì‘")

logger.info("ğŸ AI íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ v3.0 ë¡œë”© ì™„ë£Œ")

# ë””ë²„ê·¸ ì •ë³´ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if os.getenv('DEBUG_PIPELINE_UTILS', 'false').lower() in ('true', '1'):
    logger.debug(f"ğŸ› DEBUG: ì „ì²´ export ëª©ë¡ ({len(__all__)}ê°œ)")
    for i, item in enumerate(__all__, 1):
        logger.debug(f"   {i:2d}. {item}")