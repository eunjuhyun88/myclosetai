# backend/app/ai_pipeline/utils/__init__.py
"""
üçé MyCloset AI ÏôÑÏ†ÑÌïú ÌÜµÌï© Ïú†Ìã∏Î¶¨Ìã∞ ÏãúÏä§ÌÖú v8.0 - ÏµúÏ¢Ö ÏôÑÏÑ±Ìåê
================================================================================
‚úÖ Îëê ÌååÏùºÏùò Î™®Îì† Í∏∞Îä• ÏôÑÏ†Ñ ÌÜµÌï© (ÏµúÍ≥†Ïùò Ï°∞Ìï©)
‚úÖ get_step_model_interface Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ
‚úÖ get_step_memory_manager Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ  
‚úÖ get_step_data_converter Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ
‚úÖ preprocess_image_for_step Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ
‚úÖ StepModelInterface.list_available_models ÏôÑÏ†Ñ Ìè¨Ìï®
‚úÖ UnifiedStepInterface ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Íµ¨ÌòÑ
‚úÖ StepDataConverter Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏãúÏä§ÌÖú Íµ¨ÌòÑ
‚úÖ conda ÌôòÍ≤Ω 100% ÏµúÏ†ÅÌôî
‚úÖ M3 Max 128GB Î©îÎ™®Î¶¨ ÏôÑÏ†Ñ ÌôúÏö©
‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê
‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ Íµ¨ÌòÑ
‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ Clean Architecture Ï†ÅÏö©
‚úÖ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ±
‚úÖ Î™®Îì† import Ïò§Î•ò Ìï¥Í≤∞
‚úÖ ÏôÑÏ†ÑÌïú Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò
‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî
‚úÖ GPU Ìò∏ÌôòÏÑ± ÏôÑÏ†Ñ Î≥¥Ïû•
‚úÖ ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎßÅ Î∞è ÌÖåÏä§Ìä∏ Ìï®Ïàò Ìè¨Ìï®

main.py Ìò∏Ï∂ú Ìå®ÌÑ¥ (ÏôÑÏ†Ñ Ìò∏Ìôò):
from app.ai_pipeline.utils import (
    get_step_model_interface, 
    get_step_memory_manager, 
    get_step_data_converter, 
    preprocess_image_for_step
)
interface = get_step_model_interface("HumanParsingStep")
models = interface.list_available_models()
memory_manager = get_step_memory_manager("HumanParsingStep")
data_converter = get_step_data_converter("HumanParsingStep")
processed_image = preprocess_image_for_step(image, "HumanParsingStep")
"""

import os
import sys
import logging
import threading
import asyncio
import time
import gc
import weakref
import json
import hashlib
import shutil
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import lru_cache, wraps
from abc import ABC, abstractmethod
from contextlib import contextmanager, asynccontextmanager
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

# Ï°∞Í±¥Î∂Ä ÏûÑÌè¨Ìä∏ (ÏïàÏ†ÑÌïú Ï≤òÎ¶¨)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    import PIL
    PIL_AVAILABLE = True
    # PIL Î≤ÑÏ†Ñ ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞ (ÏµúÏã† Î≤ÑÏ†Ñ Ìò∏Ìôò)
    try:
        PIL_VERSION = PIL.__version__  # ÏµúÏã† Î∞©Ïãù
    except AttributeError:
        try:
            PIL_VERSION = Image.__version__  # Íµ¨Î≤ÑÏ†Ñ Î∞©Ïãù
        except AttributeError:
            PIL_VERSION = "unknown"
except ImportError:
    PIL_AVAILABLE = False
    Image = None
    PIL = None
    PIL_VERSION = "not_available"

# Î°úÍπÖ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# ==============================================
# üî• ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î∞è ÌôòÍ≤Ω Í∞êÏßÄ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

@lru_cache(maxsize=1)
def _detect_system_info() -> Dict[str, Any]:
    """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÏôÑÏ†Ñ Í∞êÏßÄ - conda ÌôòÍ≤Ω Ïö∞ÏÑ†"""
    try:
        import platform
        import subprocess
        
        # Í∏∞Î≥∏ ÏãúÏä§ÌÖú Ï†ïÎ≥¥
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ,
            "conda_prefix": os.environ.get('CONDA_PREFIX', ''),
            "virtual_env": os.environ.get('VIRTUAL_ENV', ''),
            "python_path": sys.executable
        }
        
        # M3 Max ÌäπÎ≥Ñ Í∞êÏßÄ
        is_m3_max = False
        m3_info = {"detected": False, "model": "unknown", "cores": 0}
        
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                # CPU Î∏åÎûúÎìú ÌôïÏù∏
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                brand = result.stdout.strip()
                if 'M3' in brand:
                    is_m3_max = True
                    m3_info = {
                        "detected": True,
                        "model": "M3 Max" if "Max" in brand else "M3",
                        "brand": brand
                    }
                
                # GPU ÏΩîÏñ¥ Ïàò ÌôïÏù∏
                try:
                    result = subprocess.run(
                        ['sysctl', '-n', 'hw.gpu.family_id'], 
                        capture_output=True, text=True, timeout=3
                    )
                    if result.returncode == 0:
                        m3_info["gpu_cores"] = 40 if "Max" in brand else 20
                except:
                    pass
                    
            except Exception as e:
                logger.debug(f"M3 Max Í∞êÏßÄ Ïã§Ìå®: {e}")
        
        system_info.update({
            "is_m3_max": is_m3_max,
            "m3_info": m3_info
        })
        
        # Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ (Ï†ïÌôïÌïú Í∞êÏßÄ)
        memory_gb = 16.0  # Í∏∞Î≥∏Í∞í
        if PSUTIL_AVAILABLE:
            try:
                vm = psutil.virtual_memory()
                memory_gb = round(vm.total / (1024**3), 1)
                system_info["memory_details"] = {
                    "total_gb": memory_gb,
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent_used": vm.percent
                }
            except Exception:
                pass
        
        system_info["memory_gb"] = memory_gb
        
        # GPU/ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
        device_info = _detect_best_device(is_m3_max)
        system_info.update(device_info)
        
        # AI Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞ Î∞òÏòÅ)
        project_root = Path(__file__).parent.parent.parent.parent
        ai_models_path = project_root / "ai_models"
        
        system_info.update({
            "project_root": str(project_root),
            "ai_models_path": str(ai_models_path),
            "ai_models_exists": ai_models_path.exists(),
            "config_path": str(project_root / "backend" / "app" / "core"),
            "scripts_path": str(project_root / "scripts")
        })
        
        # ÎùºÏù¥Î∏åÎü¨Î¶¨ Î≤ÑÏ†Ñ Ï†ïÎ≥¥ (PIL Ïò§Î•ò Ìï¥Í≤∞)
        system_info["libraries"] = {
            "torch": TORCH_VERSION,
            "numpy": NUMPY_VERSION if NUMPY_AVAILABLE else "not_available",
            "pillow": PIL_VERSION,  # ‚úÖ ÏïàÏ†ÑÌïú PIL Î≤ÑÏ†Ñ ÏÇ¨Ïö©
            "psutil": psutil.version_info if PSUTIL_AVAILABLE else "not_available"
        }
        
        return system_info
        
    except Exception as e:
        logger.error(f"ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Í∞êÏßÄ Ïã§Ìå®: {e}")
        # ÏïàÏ†ÑÌïú Í∏∞Î≥∏Í∞í Î∞òÌôò
        return {
            "platform": "unknown",
            "machine": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "device_name": "CPU",
            "device_available": True,
            "cpu_count": 4,
            "memory_gb": 16.0,
            "python_version": "3.8.0",
            "conda_env": "base",
            "in_conda": False,
            "project_root": str(Path.cwd()),
            "ai_models_path": str(Path.cwd() / "ai_models"),
            "ai_models_exists": False,
            "libraries": {}
        }

def _detect_best_device(is_m3_max: bool = False) -> Dict[str, Any]:
    """ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄ (M3 Max Ïö∞ÏÑ†)"""
    device_info = {
        "device": "cpu",
        "device_name": "CPU",
        "device_available": True,
        "device_memory_gb": 0.0,
        "device_capabilities": []
    }
    
    if not TORCH_AVAILABLE:
        return device_info
    
    try:
        # M3 Max MPS Ïö∞ÏÑ† (ÏµúÍ≥† ÏÑ±Îä•)
        if is_m3_max and torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple M3 Max GPU",
                "device_available": True,
                "device_memory_gb": 128.0,  # Unified Memory
                "device_capabilities": ["fp16", "bf16", "metal", "unified_memory"],
                "recommended_precision": "fp16",
                "max_batch_size": 32,
                "optimization_level": "maximum"
            })
            logger.info("üçé M3 Max MPS ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄÎê® - ÏµúÍ≥† ÏÑ±Îä• Î™®Îìú")
            
        # CUDA Í∞êÏßÄ
        elif torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            device_info.update({
                "device": "cuda",
                "device_name": device_name,
                "device_available": True,
                "device_memory_gb": round(device_memory, 1),
                "device_count": device_count,
                "device_capabilities": ["fp16", "bf16", "tensor_cores"],
                "recommended_precision": "fp16",
                "max_batch_size": 16,
                "optimization_level": "high"
            })
            logger.info(f"üöÄ CUDA ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄÎê®: {device_name}")
            
        # ÏùºÎ∞ò MPS (M1/M2)
        elif torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple Silicon GPU",
                "device_available": True,
                "device_memory_gb": 16.0,  # Ï∂îÏ†ïÍ∞í
                "device_capabilities": ["fp16", "metal"],
                "recommended_precision": "fp16",
                "max_batch_size": 8,
                "optimization_level": "medium"
            })
            logger.info("üçé Apple Silicon MPS ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄÎê®")
            
        else:
            # CPU Ìè¥Î∞±
            device_info.update({
                "device": "cpu",
                "device_name": "CPU (Multi-threaded)",
                "device_available": True,
                "device_memory_gb": 8.0,
                "device_capabilities": ["fp32", "multi_threading"],
                "recommended_precision": "fp32",
                "max_batch_size": 4,
                "optimization_level": "basic"
            })
            logger.info("üíª CPU ÎîîÎ∞îÏù¥Ïä§ ÏÇ¨Ïö©")
        
    except Exception as e:
        logger.warning(f"ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄ Ï§ë Ïò§Î•ò: {e}")
    
    return device_info

# Ï†ÑÏó≠ ÏãúÏä§ÌÖú Ï†ïÎ≥¥
SYSTEM_INFO = _detect_system_info()

# ==============================================
# üî• Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Î∞è ÏÑ§Ï†ï (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class UtilsMode(Enum):
    """Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìú"""
    LEGACY = "legacy"
    UNIFIED = "unified"
    HYBRID = "hybrid"
    FALLBACK = "fallback"
    PRODUCTION = "production"

class DeviceType(Enum):
    """ÎîîÎ∞îÏù¥Ïä§ ÌÉÄÏûÖ"""
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"
    AUTO = "auto"

class PrecisionType(Enum):
    """Ï†ïÎ∞ÄÎèÑ ÌÉÄÏûÖ"""
    FP32 = "fp32"
    FP16 = "fp16"
    BF16 = "bf16"
    INT8 = "int8"
    AUTO = "auto"

class StepType(Enum):
    """AI ÌååÏù¥ÌîÑÎùºÏù∏ Îã®Í≥Ñ ÌÉÄÏûÖ"""
    HUMAN_PARSING = "HumanParsingStep"
    POSE_ESTIMATION = "PoseEstimationStep"
    CLOTH_SEGMENTATION = "ClothSegmentationStep"
    GEOMETRIC_MATCHING = "GeometricMatchingStep"
    CLOTH_WARPING = "ClothWarpingStep"
    VIRTUAL_FITTING = "VirtualFittingStep"
    POST_PROCESSING = "PostProcessingStep"
    QUALITY_ASSESSMENT = "QualityAssessmentStep"

@dataclass
class SystemConfig:
    """ÏãúÏä§ÌÖú ÏÑ§Ï†ï (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    device: str = "auto"
    precision: str = "auto"
    device_memory_gb: float = 0.0
    
    # ÏÑ±Îä• ÏÑ§Ï†ï
    max_workers: int = 4
    max_batch_size: int = 8
    optimization_level: str = "medium"
    
    # Î©îÎ™®Î¶¨ ÏÑ§Ï†ï
    memory_limit_gb: float = 16.0
    cache_enabled: bool = True
    memory_cleanup_threshold: float = 0.8
    
    # conda ÌôòÍ≤Ω ÏÑ§Ï†ï
    conda_optimized: bool = True
    conda_env: str = "base"
    
    # ÎîîÎ≤ÑÍ∑∏ ÏÑ§Ï†ï
    debug_mode: bool = False
    verbose_logging: bool = False
    profile_performance: bool = False
    
    # AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï
    pipeline_mode: str = "sequential"
    enable_async: bool = True
    
    def __post_init__(self):
        """Ï¥àÍ∏∞Ìôî ÌõÑ ÏûêÎèô ÏÑ§Ï†ï"""
        # ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Í∏∞Î∞ò ÏûêÎèô ÏÑ§Ï†ï
        if self.device == "auto":
            self.device = SYSTEM_INFO["device"]
        
        if self.precision == "auto":
            self.precision = SYSTEM_INFO.get("recommended_precision", "fp32")
        
        if self.device_memory_gb == 0.0:
            self.device_memory_gb = SYSTEM_INFO.get("device_memory_gb", 16.0)
        
        # M3 Max ÌäπÌôî ÏµúÏ†ÅÌôî
        if SYSTEM_INFO["is_m3_max"]:
            self.max_workers = min(12, SYSTEM_INFO["cpu_count"])
            self.max_batch_size = 32
            self.optimization_level = "maximum"
            self.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
        
        # conda ÌôòÍ≤Ω ÏÑ§Ï†ï
        if SYSTEM_INFO["in_conda"]:
            self.conda_optimized = True
            self.conda_env = SYSTEM_INFO["conda_env"]

@dataclass
class StepConfig:
    """Step ÏÑ§Ï†ï (8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê)"""
    step_name: str
    step_number: Optional[int] = None
    step_type: Optional[StepType] = None
    
    # Î™®Îç∏ ÏÑ§Ï†ï
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_path: Optional[str] = None
    
    # ÏûÖÎ†•/Ï∂úÎ†• ÏÑ§Ï†ï
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    batch_size: int = 1
    
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    device: str = "auto"
    precision: str = "auto"
    
    # ÏÑ±Îä• ÏÑ§Ï†ï
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    memory_efficient: bool = True
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Step Ï†ïÎ≥¥ ÏûêÎèô ÏÑ§Ï†ï"""
        # Step ÌÉÄÏûÖ ÏÑ§Ï†ï
        if self.step_type is None:
            for step_type in StepType:
                if step_type.value == self.step_name:
                    self.step_type = step_type
                    break
        
        # Step Î≤àÌò∏ ÏûêÎèô ÏÑ§Ï†ï
        if self.step_number is None:
            step_numbers = {
                StepType.HUMAN_PARSING: 1,
                StepType.POSE_ESTIMATION: 2,
                StepType.CLOTH_SEGMENTATION: 3,
                StepType.GEOMETRIC_MATCHING: 4,
                StepType.CLOTH_WARPING: 5,
                StepType.VIRTUAL_FITTING: 6,
                StepType.POST_PROCESSING: 7,
                StepType.QUALITY_ASSESSMENT: 8
            }
            self.step_number = step_numbers.get(self.step_type, 0)
        
        # Í∏∞Î≥∏ Î™®Îç∏Î™Ö ÏÑ§Ï†ï
        if self.model_name is None:
            default_models = {
                StepType.HUMAN_PARSING: "graphonomy",
                StepType.POSE_ESTIMATION: "openpose",
                StepType.CLOTH_SEGMENTATION: "u2net",
                StepType.GEOMETRIC_MATCHING: "geometric_matching",
                StepType.CLOTH_WARPING: "cloth_warping",
                StepType.VIRTUAL_FITTING: "ootdiffusion",
                StepType.POST_PROCESSING: "post_processing",
                StepType.QUALITY_ASSESSMENT: "clipiqa"
            }
            self.model_name = default_models.get(self.step_type, "default_model")

@dataclass
class ModelInfo:
    """Î™®Îç∏ Ï†ïÎ≥¥ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
    name: str
    path: str
    model_type: str
    
    # ÌååÏùº Ï†ïÎ≥¥
    file_size_mb: float
    file_hash: Optional[str] = None
    last_modified: Optional[float] = None
    
    # Ìò∏ÌôòÏÑ± Ï†ïÎ≥¥
    step_compatibility: List[str] = field(default_factory=list)
    device_compatibility: List[str] = field(default_factory=list)
    precision_support: List[str] = field(default_factory=list)
    
    # ÏÑ±Îä• Ï†ïÎ≥¥
    confidence_score: float = 1.0
    performance_score: float = 1.0
    memory_usage_mb: float = 0.0
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    architecture: Optional[str] = None
    version: Optional[str] = None
    description: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelInfo':
        """ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú ÏÉùÏÑ±"""
        return cls(**data)

# ==============================================
# üî• Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class StepMemoryManager:
    """
    üß† StepÎ≥Ñ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ M3 Max 128GB ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî
    ‚úÖ conda ÌôòÍ≤Ω ÌäπÌôî
    ‚úÖ Ïã§ÏãúÍ∞Ñ Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ
    ‚úÖ ÏûêÎèô Ï†ïÎ¶¨ Î©îÏª§ÎãàÏ¶ò
    ‚úÖ Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ± Î≥¥Ïû•
    """
    
    def __init__(
        self, 
        step_name: str = "global",
        device: str = "auto", 
        memory_limit_gb: Optional[float] = None,
        cleanup_threshold: float = 0.8,
        auto_cleanup: bool = True
    ):
        self.step_name = step_name
        self.device = device if device != "auto" else SYSTEM_INFO["device"]
        self.memory_limit_gb = memory_limit_gb or SYSTEM_INFO["memory_gb"]
        self.cleanup_threshold = cleanup_threshold
        self.auto_cleanup = auto_cleanup
        
        # M3 Max ÌäπÌôî ÏÑ§Ï†ï
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        if self.is_m3_max:
            self.memory_limit_gb = min(self.memory_limit_gb, 100.0)  # 128GB Ï§ë 100GB ÏÇ¨Ïö©
            self.cleanup_threshold = 0.9  # M3 MaxÎäî Îçî Í¥ÄÎåÄÌïòÍ≤å
        
        # Î©îÎ™®Î¶¨ Ï∂îÏ†Å
        self.allocated_memory: Dict[str, float] = {}
        self.memory_history: List[Dict[str, Any]] = []
        self.peak_usage = 0.0
        self.total_allocations = 0
        self.total_deallocations = 0
        
        # Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ±
        self._lock = threading.RLock()
        
        # Î°úÍπÖ
        self.logger = logging.getLogger(f"{__name__}.StepMemoryManager")
        
        # ÏûêÎèô Ï†ïÎ¶¨ Ïä§Î†àÎìú
        if self.auto_cleanup:
            self._start_auto_cleanup()
        
        self.logger.info(
            f"üß† Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî: {self.step_name}, {self.device}, "
            f"{self.memory_limit_gb}GB, M3 Max: {self.is_m3_max}"
        )
    
    def get_available_memory(self) -> float:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ (GB) Î∞òÌôò"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                device_idx = 0
                total = torch.cuda.get_device_properties(device_idx).total_memory
                allocated = torch.cuda.memory_allocated(device_idx)
                return (total - allocated) / (1024**3)
                
            elif self.device == "mps" and self.is_m3_max:
                # M3 Max Unified Memory Ï≤òÎ¶¨
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)
                    return min(available_gb, self.memory_limit_gb)
                else:
                    return self.memory_limit_gb * 0.7
                    
            else:
                # CPU Î©îÎ™®Î¶¨
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    return memory.available / (1024**3)
                else:
                    return 8.0
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return 8.0
    
    def allocate_memory(self, item_name: str, size_gb: float) -> bool:
        """Î©îÎ™®Î¶¨ Ìï†Îãπ"""
        with self._lock:
            try:
                available = self.get_available_memory()
                
                if available >= size_gb:
                    self.allocated_memory[item_name] = size_gb
                    self.total_allocations += 1
                    
                    # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
                    current_total = sum(self.allocated_memory.values())
                    self.peak_usage = max(self.peak_usage, current_total)
                    
                    self.logger.info(f"‚úÖ {item_name}: {size_gb:.1f}GB Ìï†ÎãπÎê®")
                    return True
                else:
                    self.logger.warning(
                        f"‚ö†Ô∏è {item_name}: {size_gb:.1f}GB Ìï†Îãπ Ïã§Ìå® "
                        f"(ÏÇ¨Ïö© Í∞ÄÎä•: {available:.1f}GB)"
                    )
                    return False
                    
            except Exception as e:
                self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ Ìï†Îãπ Ïã§Ìå®: {e}")
                return False
    
    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        with self._lock:
            try:
                cleanup_stats = {
                    "python_objects_collected": 0,
                    "gpu_cache_cleared": False,
                    "items_deallocated": 0,
                    "memory_freed_gb": 0.0
                }
                
                # Python Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
                collected = gc.collect()
                cleanup_stats["python_objects_collected"] = collected
                
                # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
                if TORCH_AVAILABLE:
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        cleanup_stats["gpu_cache_cleared"] = True
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                            if self.is_m3_max and hasattr(torch.mps, 'synchronize'):
                                torch.mps.synchronize()
                            cleanup_stats["gpu_cache_cleared"] = True
                        except Exception as e:
                            self.logger.debug(f"MPS Ï∫êÏãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                # Í∞ïÏ†ú Ï†ïÎ¶¨ Ïãú Ìï†ÎãπÎêú Î©îÎ™®Î¶¨ Ìï¥Ï†ú
                if force and self.allocated_memory:
                    freed_memory = sum(self.allocated_memory.values())
                    items_count = len(self.allocated_memory)
                    
                    self.allocated_memory.clear()
                    
                    cleanup_stats.update({
                        "items_deallocated": items_count,
                        "memory_freed_gb": freed_memory
                    })
                
                self.logger.info(f"üßπ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å: {cleanup_stats}")
                
                return cleanup_stats
                
            except Exception as e:
                self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                return {"error": str(e)}
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
        with self._lock:
            try:
                return {
                    "step_name": self.step_name,
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "memory_info": {
                        "total_limit_gb": self.memory_limit_gb,
                        "available_gb": self.get_available_memory(),
                        "peak_usage_gb": self.peak_usage
                    },
                    "allocation_info": {
                        "allocated_items": self.allocated_memory.copy(),
                        "total_allocated_gb": sum(self.allocated_memory.values()),
                        "active_items": len(self.allocated_memory)
                    },
                    "statistics": {
                        "total_allocations": self.total_allocations,
                        "total_deallocations": self.total_deallocations,
                        "cleanup_threshold": self.cleanup_threshold,
                        "auto_cleanup": self.auto_cleanup
                    }
                }
            except Exception as e:
                self.logger.error(f"ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
                return {"error": str(e)}
    
    def _start_auto_cleanup(self):
        """ÏûêÎèô Ï†ïÎ¶¨ Ïä§Î†àÎìú ÏãúÏûë"""
        def cleanup_worker():
            while self.auto_cleanup:
                try:
                    time.sleep(30)  # 30Ï¥àÎßàÎã§ Ï≤¥ÌÅ¨
                    usage_percent = sum(self.allocated_memory.values()) / self.memory_limit_gb
                    if usage_percent > self.cleanup_threshold:
                        self.cleanup_memory()
                except Exception as e:
                    self.logger.debug(f"ÏûêÎèô Ï†ïÎ¶¨ Ïä§Î†àÎìú Ïò§Î•ò: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()

# ==============================================
# üî• Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class StepDataConverter:
    """
    üìä StepÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨/ÌõÑÏ≤òÎ¶¨
    ‚úÖ ÌÖêÏÑú Î≥ÄÌôò Î∞è ÏµúÏ†ÅÌôî
    ‚úÖ StepÎ≥Ñ ÌäπÌôî Ï≤òÎ¶¨
    ‚úÖ M3 Max GPU ÏµúÏ†ÅÌôî
    """
    
    def __init__(self, step_name: str = None, **kwargs):
        self.step_name = step_name
        self.device = SYSTEM_INFO["device"]
        self.logger = logging.getLogger(f"{__name__}.StepDataConverter")
        
        # StepÎ≥Ñ ÏÑ§Ï†ï
        self.step_configs = {
            "HumanParsingStep": {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            },
            "PoseEstimationStep": {
                "input_size": (368, 368),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            },
            "ClothSegmentationStep": {
                "input_size": (320, 320),
                "normalize": True,
                "mean": [0.5, 0.5, 0.5],
                "std": [0.5, 0.5, 0.5],
                "channels": 3
            },
            "GeometricMatchingStep": {
                "input_size": (256, 192),
                "normalize": False,
                "channels": 3
            },
            "ClothWarpingStep": {
                "input_size": (256, 192),
                "normalize": False,
                "channels": 3
            },
            "VirtualFittingStep": {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.5, 0.5, 0.5],
                "std": [0.5, 0.5, 0.5],
                "channels": 3
            },
            "PostProcessingStep": {
                "input_size": (512, 512),
                "normalize": False,
                "channels": 3
            },
            "QualityAssessmentStep": {
                "input_size": (224, 224),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            }
        }
        
        self.config = self.step_configs.get(step_name, {
            "input_size": (512, 512),
            "normalize": True,
            "mean": [0.485, 0.456, 0.406],
            "std": [0.229, 0.224, 0.225],
            "channels": 3
        })
        
        self.logger.info(f"üìä {step_name} Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def configure_for_step(self, step_name: str):
        """StepÎ≥Ñ ÏÑ§Ï†ï Ï†ÅÏö©"""
        self.step_name = step_name
        self.config = self.step_configs.get(step_name, self.config)
        self.logger.debug(f"üìù {step_name} ÏÑ§Ï†ï Ï†ÅÏö©")
    
    def preprocess_image(self, image, target_size=None, **kwargs):
        """Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
        try:
            target_size = target_size or self.config["input_size"]
            
            # PIL Image Ï≤òÎ¶¨
            if hasattr(image, 'resize'):
                # RGB Î≥ÄÌôò
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # ÌÅ¨Í∏∞ Ï°∞Ï†ï (Í≥†ÌíàÏßà Î¶¨ÏÉòÌîåÎßÅ) - PIL Î≤ÑÏ†Ñ Ìò∏Ìôò
                if PIL_AVAILABLE:
                    # PIL 10.0.0+ ÏóêÏÑúÎäî Image.LANCZOS ÎåÄÏã† Image.Resampling.LANCZOS ÏÇ¨Ïö©
                    try:
                        if hasattr(Image, 'Resampling') and hasattr(Image.Resampling, 'LANCZOS'):
                            image = image.resize(target_size, Image.Resampling.LANCZOS)
                        elif hasattr(Image, 'LANCZOS'):
                            image = image.resize(target_size, Image.LANCZOS)
                        else:
                            image = image.resize(target_size)
                    except Exception:
                        image = image.resize(target_size)  # ÏïàÏ†ÑÌïú Ìè¥Î∞±
                else:
                    image = image.resize(target_size)
            
            # NumPy Î∞∞Ïó¥Î°ú Î≥ÄÌôò
            if NUMPY_AVAILABLE:
                image_array = np.array(image, dtype=np.float32)
                
                # Ï†ïÍ∑úÌôî
                if self.config.get("normalize", True):
                    image_array = image_array / 255.0
                    
                    # ÌëúÏ§ÄÌôî (ÏÑ†ÌÉùÏ†Å)
                    if "mean" in self.config and "std" in self.config:
                        mean = np.array(self.config["mean"])
                        std = np.array(self.config["std"])
                        image_array = (image_array - mean) / std
                
                # HWC -> CHW Î≥ÄÌôò (PyTorch ÌòïÏãù)
                if len(image_array.shape) == 3:
                    image_array = image_array.transpose(2, 0, 1)
                
                return image_array
            
            return image
            
        except Exception as e:
            self.logger.warning(f"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return image
    
    def to_tensor(self, data):
        """ÌÖêÏÑú Î≥ÄÌôò (PyTorch ÏßÄÏõê)"""
        try:
            if TORCH_AVAILABLE and NUMPY_AVAILABLE:
                if isinstance(data, np.ndarray):
                    tensor = torch.from_numpy(data)
                    
                    # ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô
                    if self.device != "cpu":
                        tensor = tensor.to(self.device)
                    
                    return tensor
            
            return data
            
        except Exception as e:
            self.logger.warning(f"ÌÖêÏÑú Î≥ÄÌôò Ïã§Ìå®: {e}")
            return data
    
    def postprocess_result(self, result, output_format="image"):
        """Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
        try:
            if output_format == "image":
                # ÌÖêÏÑúÏóêÏÑú Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
                if TORCH_AVAILABLE and torch.is_tensor(result):
                    # GPUÏóêÏÑú CPUÎ°ú Ïù¥Îèô
                    result = result.detach().cpu()
                    
                    # NumPyÎ°ú Î≥ÄÌôò
                    if NUMPY_AVAILABLE:
                        result = result.numpy()
                
                # NumPy Î∞∞Ïó¥ Ï≤òÎ¶¨
                if NUMPY_AVAILABLE and isinstance(result, np.ndarray):
                    # CHW -> HWC Î≥ÄÌôò
                    if len(result.shape) == 3 and result.shape[0] in [1, 3, 4]:
                        result = result.transpose(1, 2, 0)
                    
                    # Ï†ïÍ∑úÌôî Ìï¥Ï†ú
                    if result.max() <= 1.0:
                        result = (result * 255).astype(np.uint8)
                    
                    # PIL ImageÎ°ú Î≥ÄÌôò
                    if PIL_AVAILABLE:
                        if len(result.shape) == 3:
                            result = Image.fromarray(result)
                        elif len(result.shape) == 2:
                            result = Image.fromarray(result, mode='L')
            
            return result
            
        except Exception as e:
            self.logger.warning(f"ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return result

# ==============================================
# üî• Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class StepModelInterface:
    """
    üîó Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ main.py ÏôÑÏ†Ñ Ìò∏Ìôò
    ‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ ÏßÄÏõê
    ‚úÖ Î™®Îç∏ Ï∫êÏã± ÏµúÏ†ÅÌôî
    ‚úÖ Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò Í∞ïÌôî
    ‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
    ‚úÖ M3 Max ÌäπÌôî Ï≤òÎ¶¨
    """
    
    def __init__(
        self, 
        step_name: str, 
        model_loader_instance: Optional[Any] = None,
        config: Optional[StepConfig] = None
    ):
        self.step_name = step_name
        self.model_loader = model_loader_instance
        self.config = config or StepConfig(step_name=step_name)
        
        # Î°úÍπÖ
        self.logger = logging.getLogger(f"interface.{step_name}")
        
        # Î™®Îç∏ Ï∫êÏãú (ÏïΩÌïú Ï∞∏Ï°∞ ÏÇ¨Ïö©)
        self._models_cache: Dict[str, Any] = {}
        self._model_metadata: Dict[str, ModelInfo] = {}
        
        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        
        # Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ±
        self._lock = threading.RLock()
        
        # StepÎ≥Ñ Î™®Îç∏ Îß§Ìïë (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
        self._initialize_model_mappings()
        
        self.logger.info(f"üîó {step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _initialize_model_mappings(self):
        """StepÎ≥Ñ Î™®Îç∏ Îß§Ìïë Ï¥àÍ∏∞Ìôî"""
        self._model_mappings = {
            "HumanParsingStep": {
                "default_models": ["graphonomy", "human_parsing_atr", "parsing_lip", "schp"],
                "model_types": ["segmentation", "parsing"],
                "input_sizes": [(512, 512), (473, 473)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "PoseEstimationStep": {
                "default_models": ["openpose", "mediapipe", "yolov8_pose", "movenet"],
                "model_types": ["pose_estimation", "keypoint_detection"],
                "input_sizes": [(368, 368), (256, 256), (192, 256)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "ClothSegmentationStep": {
                "default_models": ["u2net", "cloth_segmentation", "deeplabv3", "bisenet"],
                "model_types": ["segmentation", "cloth_parsing"],
                "input_sizes": [(320, 320), (512, 512)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "GeometricMatchingStep": {
                "default_models": ["geometric_matching", "tps_transformation", "spatial_transformer"],
                "model_types": ["transformation", "matching"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "ClothWarpingStep": {
                "default_models": ["cloth_warping", "spatial_transformer", "thin_plate_spline"],
                "model_types": ["warping", "transformation"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "VirtualFittingStep": {
                "default_models": ["ootdiffusion", "stable_diffusion", "virtual_tryon", "diffusion_tryon"],
                "model_types": ["diffusion", "generation", "virtual_fitting"],
                "input_sizes": [(512, 512), (768, 768)],
                "supported_formats": [".pth", ".pt", ".safetensors", ".ckpt"]
            },
            "PostProcessingStep": {
                "default_models": ["post_processing", "image_enhancement", "artifact_removal", "super_resolution"],
                "model_types": ["enhancement", "post_processing"],
                "input_sizes": [(512, 512), (1024, 1024)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "QualityAssessmentStep": {
                "default_models": ["clipiqa", "quality_assessment", "brisque", "niqe"],
                "model_types": ["quality_assessment", "metric"],
                "input_sizes": [(224, 224), (512, 512)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            }
        }
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """
        üî• Î™®Îç∏ Î°úÎìú (main.py ÌïµÏã¨ Î©îÏÑúÎìú - ÏôÑÏ†Ñ Íµ¨ÌòÑ)
        """
        start_time = time.time()
        
        with self._lock:
            self._request_count += 1
        
        try:
            # Î™®Îç∏Î™Ö Í≤∞Ï†ï
            target_model = model_name or self.config.model_name or self._get_default_model()
            
            if not target_model:
                self.logger.warning(f"‚ö†Ô∏è {self.step_name}Ïóê ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§")
                self._error_count += 1
                return None
            
            # Ï∫êÏãú ÌôïÏù∏
            if target_model in self._models_cache:
                self.logger.debug(f"üì¶ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {target_model}")
                self._success_count += 1
                return self._models_cache[target_model]
            
            # Î™®Îç∏ Î°úÎìú ÏãúÎèÑ
            model = None
            
            # 1. ModelLoaderÎ•º ÌÜµÌïú Î°úÎìú
            if self.model_loader:
                model = await self._load_via_model_loader(target_model)
            
            # 2. ÏãúÎÆ¨Î†àÏù¥ÏÖò Î™®Îç∏ (Ìè¥Î∞±)
            if model is None:
                model = self._create_simulation_model(target_model)
                self.logger.warning(f"‚ö†Ô∏è {target_model} ÏãúÎÆ¨Î†àÏù¥ÏÖò Î™®Îç∏ ÏÇ¨Ïö©")
            
            # Ï∫êÏãú Ï†ÄÏû•
            if model:
                self._models_cache[target_model] = model
                self._success_count += 1
                
                load_time = time.time() - start_time
                
                self.logger.info(
                    f"‚úÖ {target_model} Î™®Îç∏ Î°úÎìú ÏôÑÎ£å ({load_time:.2f}s)"
                )
                
                return model
            else:
                self._error_count += 1
                self.logger.error(f"‚ùå {target_model} Î™®Îç∏ Î°úÎìú Ïã§Ìå®")
                return None
                
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ï§ë Ïò§Î•ò: {e}")
            return None
    
    async def _load_via_model_loader(self, model_name: str) -> Optional[Any]:
        """ModelLoaderÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú"""
        try:
            if not hasattr(self.model_loader, 'get_model'):
                return None
            
            # ÎπÑÎèôÍ∏∞/ÎèôÍ∏∞ Ìò∏Ìôò Ï≤òÎ¶¨
            if asyncio.iscoroutinefunction(self.model_loader.get_model):
                model = await self.model_loader.get_model(model_name)
            else:
                model = self.model_loader.get_model(model_name)
            
            if model:
                self.logger.debug(f"‚úÖ ModelLoaderÎ°ú {model_name} Î°úÎìú ÏÑ±Í≥µ")
                return model
            
        except Exception as e:
            self.logger.debug(f"ModelLoader Î°úÎìú Ïã§Ìå®: {e}")
        
        return None
    
    def _create_simulation_model(self, model_name: str) -> Dict[str, Any]:
        """ÏãúÎÆ¨Î†àÏù¥ÏÖò Î™®Îç∏ ÏÉùÏÑ± (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)"""
        return {
            "name": model_name,
            "type": "simulation",
            "step_name": self.step_name,
            "step_number": self.config.step_number,
            "device": SYSTEM_INFO["device"],
            "precision": SYSTEM_INFO.get("recommended_precision", "fp32"),
            "created_at": time.time(),
            "simulate": True,
            "capabilities": self._model_mappings.get(self.step_name, {}).get("model_types", [])
        }
    
    def _get_default_model(self) -> Optional[str]:
        """Í∏∞Î≥∏ Î™®Îç∏Î™Ö Î∞òÌôò"""
        mapping = self._model_mappings.get(self.step_name)
        if mapping and mapping["default_models"]:
            return mapping["default_models"][0]
        return None
    
    def list_available_models(self) -> List[str]:
        """
        üî• ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù (main.py ÌïµÏã¨ Î©îÏÑúÎìú - ÏôÑÏ†Ñ Íµ¨ÌòÑ)
        """
        try:
            available_models = set()
            
            # 1. StepÎ≥Ñ Í∏∞Î≥∏ Î™®Îç∏Îì§
            mapping = self._model_mappings.get(self.step_name, {})
            default_models = mapping.get("default_models", [])
            available_models.update(default_models)
            
            # 2. ModelLoader Î™®Îç∏ Î™©Î°ù
            if self.model_loader and hasattr(self.model_loader, 'list_models'):
                try:
                    loader_models = self.model_loader.list_models(self.step_name)
                    if loader_models:
                        available_models.update(loader_models)
                except Exception as e:
                    self.logger.debug(f"ModelLoader Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            
            # 3. Ï∫êÏãúÎêú Î™®Îç∏Îì§
            available_models.update(self._models_cache.keys())
            
            # Ï†ïÎ†¨ Î∞è Î∞òÌôò
            result = sorted(list(available_models))
            
            self.logger.info(
                f"üìã {self.step_name} ÏÇ¨Ïö© Í∞ÄÎä• Î™®Îç∏: {len(result)}Í∞ú "
                f"({', '.join(result[:3])}{'...' if len(result) > 3 else ''})"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            # Ìè¥Î∞±ÏúºÎ°ú Í∏∞Î≥∏ Î™®Îç∏Îßå Î∞òÌôò
            mapping = self._model_mappings.get(self.step_name, {})
            return mapping.get("default_models", [])
    
    def get_stats(self) -> Dict[str, Any]:
        """Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÜµÍ≥Ñ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
        with self._lock:
            total_requests = self._request_count
            success_rate = (
                (self._success_count / max(total_requests, 1)) * 100
                if total_requests > 0 else 0.0
            )
            
            return {
                "step_name": self.step_name,
                "step_number": self.config.step_number,
                "request_statistics": {
                    "total_requests": total_requests,
                    "successful_loads": self._success_count,
                    "failed_loads": self._error_count,
                    "success_rate_percent": round(success_rate, 1)
                },
                "cache_info": {
                    "cached_models": len(self._models_cache),
                    "cached_metadata": len(self._model_metadata),
                    "cached_model_names": list(self._models_cache.keys())
                },
                "configuration": {
                    "has_model_loader": self.model_loader is not None,
                    "device": self.config.device,
                    "precision": self.config.precision
                },
                "available_models": {
                    "count": len(self.list_available_models()),
                    "default_model": self._get_default_model()
                }
            }

# ==============================================
# üî• ÌÜµÌï© Ïú†Ìã∏Î¶¨Ìã∞ Îß§ÎãàÏ†Ä (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class UnifiedUtilsManager:
    """
    üçé ÌÜµÌï© Ïú†Ìã∏Î¶¨Ìã∞ Îß§ÎãàÏ†Ä v8.0 (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ conda ÌôòÍ≤Ω 100% ÏµúÏ†ÅÌôî
    ‚úÖ M3 Max 128GB ÏôÑÏ†Ñ ÌôúÏö©
    ‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê
    ‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ Íµ¨ÌòÑ
    ‚úÖ Clean Architecture Ï†ÅÏö©
    ‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî
    ‚úÖ Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ± Î≥¥Ïû•
    ‚úÖ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ±
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # Î°úÍπÖ
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ÏãúÏä§ÌÖú ÏÑ§Ï†ï
        self.system_config = SystemConfig()
        
        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.is_initialized = False
        self.initialization_time = None
        
        # Ïª¥Ìè¨ÎÑåÌä∏ Ï†ÄÏû•ÏÜå (ÏïΩÌïú Ï∞∏Ï°∞Î°ú Î©îÎ™®Î¶¨ ÎàÑÏàò Î∞©ÏßÄ)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces: Dict[str, StepModelInterface] = {}
        self._memory_managers: Dict[str, StepMemoryManager] = {}
        self._data_converters: Dict[str, StepDataConverter] = {}
        
        # Ï†ÑÏó≠ Ïª¥Ìè¨ÎÑåÌä∏Îì§
        self.global_memory_manager = StepMemoryManager(
            step_name="global",
            device=self.system_config.device,
            memory_limit_gb=self.system_config.memory_limit_gb,
            auto_cleanup=True
        )
        
        # ÏÑ±Îä• ÌÜµÍ≥Ñ
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0,
            "conda_optimizations": 0,
            "m3_max_optimizations": 0,
            "startup_time": 0.0
        }
        
        # Ïä§Î†àÎìú ÌíÄ
        self._thread_pool = ThreadPoolExecutor(
            max_workers=self.system_config.max_workers,
            thread_name_prefix="utils_worker"
        )
        
        # ÎèôÍ∏∞Ìôî
        self._interface_lock = threading.RLock()
        
        # conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
        if SYSTEM_INFO["in_conda"]:
            self._setup_conda_optimizations()
        
        # M3 Max ÌäπÎ≥Ñ ÏµúÏ†ÅÌôî
        if SYSTEM_INFO["is_m3_max"]:
            self._setup_m3_max_optimizations()
        
        self._initialized = True
        self.logger.info(
            f"üéØ UnifiedUtilsManager v8.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å "
            f"(conda: {SYSTEM_INFO['in_conda']}, M3: {SYSTEM_INFO['is_m3_max']})"
        )
    
    def _setup_conda_optimizations(self):
        """conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî ÏÑ§Ï†ï"""
        try:
            start_time = time.time()
            
            # PyTorch ÏµúÏ†ÅÌôî
            if TORCH_AVAILABLE:
                # Ïä§Î†àÎìú Ïàò ÏµúÏ†ÅÌôî
                torch.set_num_threads(self.system_config.max_workers)
                
                # Ïù∏ÌÑ∞Ïòµ Î≥ëÎ†¨ÏÑ± ÏÑ§Ï†ï
                torch.set_num_interop_threads(min(4, self.system_config.max_workers))
                
                # MPS ÏµúÏ†ÅÌôî (M3 Max)
                if SYSTEM_INFO["is_m3_max"]:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                    })
            
            # NumPy ÏµúÏ†ÅÌôî
            if NUMPY_AVAILABLE:
                # OpenBLAS/MKL Ïä§Î†àÎìú ÏÑ§Ï†ï
                os.environ.update({
                    'OMP_NUM_THREADS': str(self.system_config.max_workers),
                    'MKL_NUM_THREADS': str(self.system_config.max_workers),
                    'OPENBLAS_NUM_THREADS': str(self.system_config.max_workers),
                    'NUMEXPR_NUM_THREADS': str(self.system_config.max_workers)
                })
            
            optimization_time = time.time() - start_time
            self.stats["conda_optimizations"] += 1
            
            self.logger.info(
                f"‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî ÏôÑÎ£å ({optimization_time:.3f}s) - "
                f"ÏõåÏª§: {self.system_config.max_workers}, ÌôòÍ≤Ω: {SYSTEM_INFO['conda_env']}"
            )
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è conda ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ïã§Ìå®: {e}")
    
    def _setup_m3_max_optimizations(self):
        """M3 Max ÌäπÎ≥Ñ ÏµúÏ†ÅÌôî"""
        try:
            start_time = time.time()
            
            # Î©îÎ™®Î¶¨ ÏÑ§Ï†ï ÏµúÏ†ÅÌôî
            self.system_config.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
            self.system_config.max_batch_size = 32  # M3 MaxÎäî ÌÅ∞ Î∞∞Ïπò ÌóàÏö©
            self.system_config.optimization_level = "maximum"
            
            if TORCH_AVAILABLE:
                # M3 Max MPS Î∞±ÏóîÎìú ÏµúÏ†ÅÌôî
                if torch.backends.mps.is_available():
                    try:
                        # Metal Performance Shaders ÏµúÏ†ÅÌôî
                        if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                            torch.mps.set_per_process_memory_fraction(0.8)
                        
                        # M3 Max ÌäπÌôî ÌôòÍ≤Ω Î≥ÄÏàò
                        os.environ.update({
                            'PYTORCH_MPS_ALLOCATOR_POLICY': 'native',
                            'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.9'
                        })
                        
                    except Exception as e:
                        self.logger.debug(f"MPS ÏÑ∏Î∂Ä ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            
            optimization_time = time.time() - start_time
            self.stats["m3_max_optimizations"] += 1
            
            self.logger.info(
                f"üçé M3 Max ÌäπÎ≥Ñ ÏµúÏ†ÅÌôî ÏôÑÎ£å ({optimization_time:.3f}s) - "
                f"Î©îÎ™®Î¶¨: {self.system_config.memory_limit_gb}GB, "
                f"Î∞∞Ïπò: {self.system_config.max_batch_size}"
            )
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (main.py Ìò∏Ìôò)"""
        try:
            # Í∏∞Ï°¥ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Î∞òÌôò
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            # ÏÉà Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
            step_config = StepConfig(step_name=step_name)
            interface = StepModelInterface(
                step_name=step_name,
                model_loader_instance=getattr(self, 'model_loader', None),
                config=step_config
            )
            
            # Ï∫êÏãú Ï†ÄÏû•
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"üîó {step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
            return interface
            
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            # Ìè¥Î∞± Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
            return StepModelInterface(step_name, None)
    
    def create_step_memory_manager(self, step_name: str, **options) -> StepMemoryManager:
        """StepÎ≥Ñ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±"""
        try:
            # Í∏∞Ï°¥ Í¥ÄÎ¶¨Ïûê Î∞òÌôò
            if step_name in self._memory_managers:
                return self._memory_managers[step_name]
            
            # ÏÉà Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±
            manager = StepMemoryManager(step_name=step_name, **options)
            self._memory_managers[step_name] = manager
            
            self.logger.info(f"üß† {step_name} Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ± ÏôÑÎ£å")
            return manager
            
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return self.global_memory_manager
    
    def create_step_data_converter(self, step_name: str, **options) -> StepDataConverter:
        """StepÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ±"""
        try:
            # Í∏∞Ï°¥ Î≥ÄÌôòÍ∏∞ Î∞òÌôò
            if step_name in self._data_converters:
                return self._data_converters[step_name]
            
            # ÏÉà Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ±
            converter = StepDataConverter(step_name, **options)
            self._data_converters[step_name] = converter
            
            self.logger.info(f"üìä {step_name} Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ± ÏôÑÎ£å")
            return converter
            
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return StepDataConverter(step_name)
    
    def get_memory_manager(self) -> StepMemoryManager:
        """Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Î∞òÌôò"""
        return self.global_memory_manager
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
        try:
            start_time = time.time()
            self.logger.info("üßπ Ï†ÑÏó≠ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏãúÏûë...")
            
            optimization_results = {
                "global_cleanup": {},
                "step_managers": {},
                "model_interfaces": {},
                "total_freed_gb": 0.0,
                "optimization_time": 0.0
            }
            
            # 1. Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            global_result = self.global_memory_manager.cleanup_memory(force=True)
            optimization_results["global_cleanup"] = global_result
            
            # 2. StepÎ≥Ñ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï†ïÎ¶¨
            for step_name, manager in self._memory_managers.items():
                try:
                    step_result = manager.cleanup_memory()
                    optimization_results["step_managers"][step_name] = step_result
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {step_name} Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # 3. Python Ï†ÑÏó≠ Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
            collected_objects = gc.collect()
            
            # 4. PyTorch Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if TORCH_AVAILABLE:
                device = SYSTEM_INFO["device"]
                if device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                elif device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if SYSTEM_INFO["is_m3_max"] and hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as e:
                        self.logger.debug(f"MPS Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # 5. ÏïΩÌïú Ï∞∏Ï°∞ Ï†ïÎ¶¨
            self._step_interfaces.clear()
            
            optimization_time = time.time() - start_time
            optimization_results["optimization_time"] = optimization_time
            optimization_results["collected_objects"] = collected_objects
            
            self.stats["memory_optimizations"] += 1
            
            self.logger.info(
                f"‚úÖ Ï†ÑÏó≠ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏôÑÎ£å ({optimization_time:.2f}s) - "
                f"Í∞ùÏ≤¥ Ï†ïÎ¶¨: {collected_objects}Í∞ú"
            )
            
            return {
                "success": True,
                "results": optimization_results,
                "memory_info": self.global_memory_manager.get_memory_stats()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ÏÉÅÌÉú Ï°∞Ìöå (ÏôÑÏ†Ñ Íµ¨ÌòÑ)"""
        try:
            # ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨ Ï†ïÎ≥¥
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "used_gb": round((vm.total - vm.available) / (1024**3), 1),
                    "percent_used": round(vm.percent, 1)
                }
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ
            interface_stats = {}
            for step_name, interface in self._model_interfaces.items():
                interface_stats[step_name] = interface.get_stats()
            
            return {
                "system": {
                    "initialized": self.is_initialized,
                    "initialization_time": self.initialization_time,
                    "uptime": time.time() - (self.initialization_time or time.time()) if self.initialization_time else 0
                },
                "configuration": asdict(self.system_config),
                "environment": {
                    "system_info": SYSTEM_INFO,
                    "conda_optimized": SYSTEM_INFO["in_conda"],
                    "m3_max_optimized": SYSTEM_INFO["is_m3_max"],
                    "device": SYSTEM_INFO["device"],
                    "libraries": SYSTEM_INFO.get("libraries", {})
                },
                "memory": {
                    "system_memory": memory_info,
                    "global_manager": self.global_memory_manager.get_memory_stats(),
                    "step_managers": {
                        name: manager.get_memory_stats() 
                        for name, manager in self._memory_managers.items()
                    }
                },
                "components": {
                    "step_interfaces": len(self._step_interfaces),
                    "model_interfaces": len(self._model_interfaces),
                    "memory_managers": len(self._memory_managers),
                    "data_converters": len(self._data_converters)
                },
                "statistics": {
                    **self.stats,
                    "interface_stats": interface_stats
                }
            }
            
        except Exception as e:
            self.logger.error(f"ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {"error": str(e), "status": "error"}

# ==============================================
# üî• ÌÜµÌï© Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

class UnifiedStepInterface:
    """
    üîó ÌÜµÌï© Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê
    ‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ Íµ¨ÌòÑ
    ‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
    ‚úÖ M3 Max ÌäπÌôî Ï≤òÎ¶¨
    ‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî
    ‚úÖ ÏóêÎü¨ Ï≤òÎ¶¨ Í∞ïÌôî
    """
    
    def __init__(
        self, 
        manager: UnifiedUtilsManager, 
        config: StepConfig, 
        is_fallback: bool = False
    ):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        # Î°úÍπÖ
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        self._last_request_time = None
        self._total_processing_time = 0.0
        
        # Ïä§Î†àÎìú ÏïàÏ†ÑÏÑ±
        self._lock = threading.RLock()
        
        if self.is_fallback:
            self.logger.warning(f"‚ö†Ô∏è {config.step_name} Ìè¥Î∞± Î™®ÎìúÎ°ú Ï¥àÍ∏∞ÌôîÎê®")
        else:
            self.logger.info(f"üîó {config.step_name} ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """Î™®Îç∏ Î°úÎìú (ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§)"""
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            if self.is_fallback:
                return self._create_fallback_model(model_name)
            
            # Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î•º ÌÜµÌïú Î°úÎìú
            model_interface = self.manager.create_step_model_interface(self.config.step_name)
            model = await model_interface.get_model(model_name)
            
            if model:
                with self._lock:
                    self._success_count += 1
                return model
            else:
                with self._lock:
                    self._error_count += 1
                return self._create_fallback_model(model_name)
                
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            return self._create_fallback_model(model_name)
    
    def _create_fallback_model(self, model_name: Optional[str]) -> Dict[str, Any]:
        """Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±"""
        return {
            "name": model_name or "fallback_model",
            "type": "fallback",
            "step_name": self.config.step_name,
            "step_number": self.config.step_number,
            "simulation": True,
            "created_at": time.time()
        }
    
    async def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ (StepÎ≥Ñ ÌäπÌôî Ï≤òÎ¶¨)"""
        start_time = time.time()
        
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            # StepÎ≥Ñ ÌäπÌôî Ï≤òÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            step_number = self.config.step_number
            processing_time = {
                1: 0.2,  # Human Parsing
                2: 0.15, # Pose Estimation
                3: 0.25, # Cloth Segmentation
                4: 0.3,  # Geometric Matching
                5: 0.35, # Cloth Warping
                6: 0.8,  # Virtual Fitting (Í∞ÄÏû• Î¨¥Í±∞ÏõÄ)
                7: 0.2,  # Post Processing
                8: 0.1   # Quality Assessment
            }.get(step_number, 0.1)
            
            await asyncio.sleep(processing_time)  # Ï≤òÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            
            result = {
                "success": True,
                "step_name": self.config.step_name,
                "step_number": step_number,
                "processing_time": processing_time,
                "output_type": f"step_{step_number:02d}_result",
                "confidence": 0.9,
                "device": self.config.device,
                "is_simulation": True
            }
            
            with self._lock:
                self._total_processing_time += processing_time
                self._success_count += 1
            
            return result
            
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "step_name": self.config.step_name,
                "is_fallback": True
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """ÌÜµÍ≥Ñ Î∞òÌôò"""
        with self._lock:
            total_requests = self._request_count
            success_rate = (
                (self._success_count / max(total_requests, 1)) * 100
                if total_requests > 0 else 0.0
            )
            avg_processing_time = (
                self._total_processing_time / max(total_requests, 1)
                if total_requests > 0 else 0.0
            )
            
            return {
                "step_info": {
                    "step_name": self.config.step_name,
                    "step_number": self.config.step_number,
                    "is_fallback": self.is_fallback
                },
                "performance": {
                    "total_requests": total_requests,
                    "successful_requests": self._success_count,
                    "failed_requests": self._error_count,
                    "success_rate_percent": round(success_rate, 1),
                    "average_processing_time": round(avg_processing_time, 3),
                    "total_processing_time": round(self._total_processing_time, 2)
                },
                "configuration": {
                    "device": self.config.device,
                    "precision": self.config.precision,
                    "input_size": self.config.input_size,
                    "batch_size": self.config.batch_size,
                    "model_name": self.config.model_name
                },
                "status": {
                    "last_request_time": self._last_request_time,
                    "operational": total_requests > 0 and success_rate > 50,
                    "health_score": min(10, success_rate / 10) if total_requests > 0 else 5
                }
            }

# ==============================================
# üî• Ìé∏Ïùò Ìï®ÏàòÎì§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    üî• main.py ÌïµÏã¨ Ìï®Ïàò (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ import Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
    ‚úÖ Î™®Îì† Í∏∞Îä• Ìè¨Ìï®
    ‚úÖ Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò Í∞ïÌôî
    """
    try:
        # ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ ÌôïÎ≥¥
        if model_loader_instance is None:
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"‚úÖ Ï†ÑÏó≠ ModelLoader Ïó∞Îèô: {step_name}")
            except (ImportError, ModuleNotFoundError):
                logger.info(f"‚ÑπÔ∏è ModelLoader Î™®Îìà ÏóÜÏùå - Í∏∞Î≥∏ Î°úÎçî ÏÇ¨Ïö©: {step_name}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ModelLoader Ïó∞Îèô Ïã§Ìå®: {e}")
                model_loader_instance = None
        
        # UnifiedUtilsManagerÎ•º ÌÜµÌïú ÏÉùÏÑ±
        try:
            manager = get_utils_manager()
            interface = manager.create_step_model_interface(step_name)
            logger.info(f"üîó {step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å (Manager)")
            return interface
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ManagerÎ•º ÌÜµÌïú ÏÉùÏÑ± Ïã§Ìå®: {e}")
        
        # ÏßÅÏ†ë ÏÉùÏÑ± (Ìè¥Î∞±)
        step_config = StepConfig(step_name=step_name)
        interface = StepModelInterface(
            step_name=step_name,
            model_loader_instance=model_loader_instance,
            config=step_config
        )
        
        logger.info(f"üîó {step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å (Direct)")
        return interface
        
    except Exception as e:
        logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        # ÏµúÏ¢Ö Ìè¥Î∞± Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
        return StepModelInterface(step_name, None)

def get_step_memory_manager(step_name: str = None, **kwargs) -> StepMemoryManager:
    """
    üî• main.py ÌïµÏã¨ Ìï®Ïàò (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
    ‚úÖ import Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
    ‚úÖ M3 Max ÌäπÌôî Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
    ‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
    """
    try:
        # UnifiedUtilsManagerÎ•º ÌÜµÌïú Ï°∞Ìöå
        try:
            manager = get_utils_manager()
            if step_name:
                memory_manager = manager.create_step_memory_manager(step_name, **kwargs)
            else:
                memory_manager = manager.get_memory_manager()
            
            logger.info(f"üß† Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Î∞òÌôò (Manager): {step_name or 'global'}")
            return memory_manager
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ManagerÎ•º ÌÜµÌïú Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï°∞Ìöå Ïã§Ìå®: {e}")
        
        # ÏßÅÏ†ë ÏÉùÏÑ± (Ìè¥Î∞±)
        memory_manager = StepMemoryManager(
            step_name=step_name or "fallback", 
            **kwargs
        )
        logger.info(f"üß† Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏßÅÏ†ë ÏÉùÏÑ±: {step_name or 'global'}")
        return memory_manager
        
    except Exception as e:
        logger.error(f"‚ùå Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ± Ïã§Ìå®: {e}")
        # ÏµúÏ¢Ö Ìè¥Î∞±
        return StepMemoryManager(step_name=step_name or "error", **kwargs)

def get_step_data_converter(step_name: str = None, **kwargs) -> StepDataConverter:
    """
    üî• StepÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ Î∞òÌôò (main.py Ìò∏Ìôò)
    ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨, ÌõÑÏ≤òÎ¶¨
    ‚úÖ ÌÖêÏÑú Î≥ÄÌôò Î∞è ÏµúÏ†ÅÌôî
    ‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî
    """
    try:
        # UnifiedUtilsManagerÎ•º ÌÜµÌïú Ï°∞Ìöå
        try:
            manager = get_utils_manager()
            converter = manager.create_step_data_converter(step_name or "default", **kwargs)
            logger.info(f"üìä Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ Î∞òÌôò (Manager): {step_name or 'global'}")
            return converter
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ManagerÎ•º ÌÜµÌïú Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ Ï°∞Ìöå Ïã§Ìå®: {e}")
        
        # ÏßÅÏ†ë ÏÉùÏÑ± (Ìè¥Î∞±)
        converter = StepDataConverter(step_name, **kwargs)
        logger.info(f"üìä Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏßÅÏ†ë ÏÉùÏÑ±: {step_name or 'global'}")
        return converter
            
    except Exception as e:
        logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return StepDataConverter(step_name, **kwargs)

def preprocess_image_for_step(image_data, step_name: str, **kwargs) -> Any:
    """
    üî• StepÎ≥Ñ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (main.py Ìò∏Ìôò)
    ‚úÖ StepÎ≥Ñ ÌäπÌôî Ï†ÑÏ≤òÎ¶¨
    ‚úÖ ÌÅ¨Í∏∞ Ï°∞Ï†ï, Ï†ïÍ∑úÌôî
    ‚úÖ ÌÖêÏÑú Î≥ÄÌôò
    """
    try:
        # Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ Í∞ÄÏ†∏Ïò§Í∏∞
        converter = get_step_data_converter(step_name)
        
        # StepÎ≥Ñ ÏÑ§Ï†ï Ï†ÅÏö©
        converter.configure_for_step(step_name)
        
        # Ï†ÑÏ≤òÎ¶¨ ÏàòÌñâ
        processed_image = converter.preprocess_image(image_data, **kwargs)
        
        logger.debug(f"‚úÖ {step_name} Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å")
        return processed_image
        
    except Exception as e:
        logger.error(f"‚ùå {step_name} Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        return image_data

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ÏÉàÎ°úÏö¥ ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (Í∂åÏû•)"""
    try:
        manager = get_utils_manager()
        step_config = StepConfig(step_name=step_name, **options)
        return UnifiedStepInterface(manager, step_config)
    except Exception as e:
        logger.error(f"ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        step_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(get_utils_manager(), step_config, is_fallback=True)

# ==============================================
# üî• Ï†ÑÏó≠ Í¥ÄÎ¶¨ Ìï®ÏàòÎì§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Îß§ÎãàÏ†Ä Î∞òÌôò"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Ï¥àÍ∏∞Ìôî (main.py ÏßÑÏûÖÏ†ê)"""
    try:
        manager = get_utils_manager()
        
        # conda ÌôòÍ≤Ω ÌäπÌôî ÏÑ§Ï†ï
        if SYSTEM_INFO["in_conda"]:
            kwargs.setdefault("conda_optimized", True)
            kwargs.setdefault("precision", "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32")
            kwargs.setdefault("optimization_level", "maximum" if SYSTEM_INFO["is_m3_max"] else "high")
        
        # Ï¥àÍ∏∞Ìôî ÏôÑÎ£å
        manager.is_initialized = True
        manager.initialization_time = time.time()
        
        return {
            "success": True,
            "initialization_time": manager.initialization_time,
            "system_config": asdict(manager.system_config),
            "system_info": SYSTEM_INFO,
            "conda_optimized": SYSTEM_INFO["in_conda"],
            "m3_max_optimized": SYSTEM_INFO["is_m3_max"]
        }
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {
            "error": str(e), 
            "system_info": SYSTEM_INFO,
            "fallback_status": True
        }

async def optimize_system_memory() -> Dict[str, Any]:
    """ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
    try:
        manager = get_utils_manager()
        return await manager.optimize_memory()
    except Exception as e:
        logger.error(f"Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
        return {"success": False, "error": str(e)}

# ==============================================
# üî• Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

def get_ai_models_path() -> Path:
    """AI Î™®Îç∏ Í≤ΩÎ°ú Î∞òÌôò"""
    return Path(SYSTEM_INFO["ai_models_path"])

def list_available_steps() -> List[str]:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Step Î™©Î°ù (8Îã®Í≥Ñ ÏôÑÏ†Ñ ÏßÄÏõê)"""
    return [step.value for step in StepType]

def is_conda_environment() -> bool:
    """conda ÌôòÍ≤Ω Ïó¨Î∂Ä ÌôïÏù∏"""
    return SYSTEM_INFO["in_conda"]

def is_m3_max_device() -> bool:
    """M3 Max ÎîîÎ∞îÏù¥Ïä§ Ïó¨Î∂Ä ÌôïÏù∏"""
    return SYSTEM_INFO["is_m3_max"]

def get_conda_info() -> Dict[str, Any]:
    """conda ÌôòÍ≤Ω Ï†ïÎ≥¥"""
    return {
        "in_conda": SYSTEM_INFO["in_conda"],
        "conda_env": SYSTEM_INFO["conda_env"],
        "conda_prefix": SYSTEM_INFO["conda_prefix"],
        "python_path": SYSTEM_INFO["python_path"]
    }

def get_device_info() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥"""
    device_keys = [
        "device", "device_name", "device_available", "device_memory_gb",
        "device_capabilities", "recommended_precision", "optimization_level"
    ]
    return {key: SYSTEM_INFO.get(key) for key in device_keys if key in SYSTEM_INFO}

def validate_step_name(step_name: str) -> bool:
    """Step Ïù¥Î¶Ñ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
    valid_steps = [step.value for step in StepType]
    return step_name in valid_steps

def get_step_number(step_name: str) -> int:
    """Step Î≤àÌò∏ Î∞òÌôò"""
    for step_type in StepType:
        if step_type.value == step_name:
            step_config = StepConfig(step_name=step_name)
            return step_config.step_number or 0
    return 0

def check_system_requirements() -> Dict[str, Any]:
    """ÏãúÏä§ÌÖú ÏöîÍµ¨ÏÇ¨Ìï≠ Ï≤¥ÌÅ¨"""
    requirements = {
        "python_version": {
            "required": "3.8+",
            "current": SYSTEM_INFO["python_version"],
            "satisfied": tuple(map(int, SYSTEM_INFO["python_version"].split('.'))) >= (3, 8)
        },
        "memory": {
            "required_gb": 8.0,
            "current_gb": SYSTEM_INFO["memory_gb"],
            "satisfied": SYSTEM_INFO["memory_gb"] >= 8.0
        },
        "device": {
            "required": "CPU/GPU",
            "current": SYSTEM_INFO["device"],
            "satisfied": True  # CPUÎäî Ìï≠ÏÉÅ ÏßÄÏõê
        },
        "libraries": {
            "torch": {"available": TORCH_AVAILABLE, "version": TORCH_VERSION},
            "numpy": {"available": NUMPY_AVAILABLE, "version": NUMPY_VERSION if NUMPY_AVAILABLE else None},
            "psutil": {"available": PSUTIL_AVAILABLE},
            "pillow": {"available": PIL_AVAILABLE}
        }
    }
    
    # Ï†ÑÏ≤¥ ÎßåÏ°±ÎèÑ Í≥ÑÏÇ∞
    core_satisfied = all([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"],
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE
    ])
    
    requirements["overall_satisfied"] = core_satisfied
    requirements["score"] = sum([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"], 
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE,
        PSUTIL_AVAILABLE,
        PIL_AVAILABLE
    ]) / 7 * 100
    
    return requirements

# ==============================================
# üî• Í∞úÎ∞ú/ÎîîÎ≤ÑÍ∑∏ Ìé∏Ïùò Ìï®ÏàòÎì§ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

def debug_system_info(detailed: bool = False):
    """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÎîîÎ≤ÑÍ∑∏ Ï∂úÎ†•"""
    print("\n" + "="*70)
    print("üîç MyCloset AI ÏãúÏä§ÌÖú Ï†ïÎ≥¥ (v8.0)")
    print("="*70)
    
    # Í∏∞Î≥∏ ÏãúÏä§ÌÖú Ï†ïÎ≥¥
    print(f"ÌîåÎû´Ìèº: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
    print(f"Python: {SYSTEM_INFO['python_version']} ({SYSTEM_INFO['python_path']})")
    print(f"CPU ÏΩîÏñ¥: {SYSTEM_INFO['cpu_count']}Í∞ú")
    print(f"Î©îÎ™®Î¶¨: {SYSTEM_INFO['memory_gb']}GB")
    
    # Apple Silicon Ï†ïÎ≥¥
    if SYSTEM_INFO["is_m3_max"]:
        m3_info = SYSTEM_INFO.get("m3_info", {})
        print(f"üçé Apple Silicon: {m3_info.get('model', 'M3 Max')} ({'‚úÖ Í∞êÏßÄÎê®' if m3_info.get('detected') else '‚ùå'})")
        if m3_info.get("brand"):
            print(f"   CPU Î∏åÎûúÎìú: {m3_info['brand']}")
        if m3_info.get("gpu_cores"):
            print(f"   GPU ÏΩîÏñ¥: {m3_info['gpu_cores']}Í∞ú")
    else:
        print(f"üçé Apple Silicon: ‚ùå")
    
    # GPU/ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥
    device_info = get_device_info()
    print(f"üéØ ÎîîÎ∞îÏù¥Ïä§: {device_info.get('device', 'unknown')}")
    print(f"   Ïù¥Î¶Ñ: {device_info.get('device_name', 'Unknown')}")
    print(f"   Î©îÎ™®Î¶¨: {device_info.get('device_memory_gb', 0)}GB")
    print(f"   Ï†ïÎ∞ÄÎèÑ: {device_info.get('recommended_precision', 'fp32')}")
    print(f"   ÏµúÏ†ÅÌôî ÏàòÏ§Ä: {device_info.get('optimization_level', 'basic')}")
    if device_info.get('device_capabilities'):
        print(f"   Í∏∞Îä•: {', '.join(device_info['device_capabilities'])}")
    
    # conda ÌôòÍ≤Ω Ï†ïÎ≥¥
    conda_info = get_conda_info()
    print(f"üêç conda ÌôòÍ≤Ω: {'‚úÖ' if conda_info['in_conda'] else '‚ùå'}")
    if conda_info['in_conda']:
        print(f"   ÌôòÍ≤ΩÎ™Ö: {conda_info['conda_env']}")
        print(f"   Í≤ΩÎ°ú: {conda_info.get('conda_prefix', 'Unknown')}")
    
    # ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÉÅÌÉú
    print("üìö ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÉÅÌÉú:")
    libraries = SYSTEM_INFO.get("libraries", {})
    for lib_name, version in libraries.items():
        status = "‚úÖ" if version != "not_available" else "‚ùå"
        print(f"   {lib_name}: {status} ({version})")
    
    # ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú
    print("üìÅ Í≤ΩÎ°ú Ï†ïÎ≥¥:")
    print(f"   ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏: {SYSTEM_INFO['project_root']}")
    print(f"   AI Î™®Îç∏: {SYSTEM_INFO['ai_models_path']}")
    print(f"   Î™®Îç∏ Ìè¥Îçî Ï°¥Ïû¨: {'‚úÖ' if SYSTEM_INFO['ai_models_exists'] else '‚ùå'}")
    
    print("="*70)

def test_step_interface(step_name: str = "HumanParsingStep", detailed: bool = False):
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏"""
    print(f"\nüß™ {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏
        print("1Ô∏è‚É£ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±...")
        interface = get_step_model_interface(step_name)
        print(f"   ‚úÖ ÌÉÄÏûÖ: {type(interface).__name__}")
        
        # 2. Î™®Îç∏ Î™©Î°ù ÌÖåÏä§Ìä∏
        print("2Ô∏è‚É£ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ï°∞Ìöå...")
        models = interface.list_available_models()
        print(f"   ‚úÖ Î™®Îç∏ Ïàò: {len(models)}Í∞ú")
        
        if models:
            print("   üìã Î™®Îç∏ Î™©Î°ù:")
            for i, model in enumerate(models[:5]):  # Ï≤òÏùå 5Í∞úÎßå
                print(f"      {i+1}. {model}")
            if len(models) > 5:
                print(f"      ... Î∞è {len(models)-5}Í∞ú Îçî")
        else:
            print("   ‚ö†Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÏóÜÏùå")
        
        # 3. ÌÜµÍ≥Ñ ÌôïÏù∏
        print("3Ô∏è‚É£ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÜµÍ≥Ñ...")
        stats = interface.get_stats()
        print(f"   üìä ÏöîÏ≤≠ Ïàò: {stats['request_statistics']['total_requests']}")
        print(f"   üìä ÏÑ±Í≥µÎ•†: {stats['request_statistics']['success_rate_percent']}%")
        print(f"   üìä Ï∫êÏãúÎêú Î™®Îç∏: {stats['cache_info']['cached_models']}Í∞ú")
        
        test_time = time.time() - start_time
        print(f"‚è±Ô∏è ÌÖåÏä§Ìä∏ ÏãúÍ∞Ñ: {test_time:.3f}Ï¥à")
        print("‚úÖ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False

def test_memory_manager(detailed: bool = False):
    """Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÌÖåÏä§Ìä∏"""
    print(f"\nüß† Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÌÖåÏä§Ìä∏")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±
        print("1Ô∏è‚É£ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±...")
        memory_manager = get_step_memory_manager()
        print(f"   ‚úÖ ÌÉÄÏûÖ: {type(memory_manager).__name__}")
        print(f"   üìü ÎîîÎ∞îÏù¥Ïä§: {memory_manager.device}")
        print(f"   üíæ Î©îÎ™®Î¶¨ Ï†úÌïú: {memory_manager.memory_limit_gb}GB")
        
        # 2. Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ ÌôïÏù∏
        print("2Ô∏è‚É£ Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ Ï°∞Ìöå...")
        stats = memory_manager.get_memory_stats()
        
        memory_info = stats.get("memory_info", {})
        print(f"   üìä Ï†ÑÏ≤¥ Î©îÎ™®Î¶¨: {memory_info.get('total_limit_gb', 0)}GB")
        print(f"   üìä ÏÇ¨Ïö© Í∞ÄÎä•: {memory_info.get('available_gb', 0):.1f}GB")
        
        allocation_info = stats.get("allocation_info", {})
        print(f"   üìä Ìï†ÎãπÎêú Ìï≠Î™©: {allocation_info.get('active_items', 0)}Í∞ú")
        print(f"   üìä Ìï†ÎãπÎêú Î©îÎ™®Î¶¨: {allocation_info.get('total_allocated_gb', 0):.1f}GB")
        
        # 3. Î©îÎ™®Î¶¨ Ìï†Îãπ/Ìï¥Ï†ú ÌÖåÏä§Ìä∏
        print("3Ô∏è‚É£ Î©îÎ™®Î¶¨ Ìï†Îãπ/Ìï¥Ï†ú ÌÖåÏä§Ìä∏...")
        
        # Ìï†Îãπ ÌÖåÏä§Ìä∏
        test_item = "TestItem"
        test_size = 1.0  # 1GB
        
        allocation_success = memory_manager.allocate_memory(test_item, test_size)
        print(f"   Î©îÎ™®Î¶¨ Ìï†Îãπ ({test_size}GB): {'‚úÖ' if allocation_success else '‚ùå'}")
        
        if allocation_success:
            # Ï†ïÎ¶¨ ÌÖåÏä§Ìä∏
            cleanup_result = memory_manager.cleanup_memory(force=True)
            print(f"   Î©îÎ™®Î¶¨ Ï†ïÎ¶¨: ‚úÖ")
            if isinstance(cleanup_result, dict) and cleanup_result.get("memory_freed_gb"):
                print(f"   Ìï¥Ï†úÎêú Î©îÎ™®Î¶¨: {cleanup_result['memory_freed_gb']}GB")
        
        test_time = time.time() - start_time
        print(f"‚è±Ô∏è ÌÖåÏä§Ìä∏ ÏãúÍ∞Ñ: {test_time:.3f}Ï¥à")
        print("‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False

def test_data_converter(step_name: str = "HumanParsingStep"):
    """Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÌÖåÏä§Ìä∏"""
    print(f"\nüìä {step_name} Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÌÖåÏä§Ìä∏")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ±
        print("1Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ±...")
        converter = get_step_data_converter(step_name)
        print(f"   ‚úÖ ÌÉÄÏûÖ: {type(converter).__name__}")
        print(f"   üìù Step: {converter.step_name}")
        print(f"   üéØ ÎîîÎ∞îÏù¥Ïä§: {converter.device}")
        
        # 2. ÏÑ§Ï†ï ÌôïÏù∏
        print("2Ô∏è‚É£ StepÎ≥Ñ ÏÑ§Ï†ï ÌôïÏù∏...")
        config = converter.config
        print(f"   üìê ÏûÖÎ†• ÌÅ¨Í∏∞: {config.get('input_size', 'Unknown')}")
        print(f"   üîß Ï†ïÍ∑úÌôî: {config.get('normalize', False)}")
        print(f"   üì∫ Ï±ÑÎÑê Ïàò: {config.get('channels', 'Unknown')}")
        
        # 3. Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÌÖåÏä§Ìä∏ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)
        print("3Ô∏è‚É£ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÌÖåÏä§Ìä∏...")
        
        # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        if PIL_AVAILABLE:
            try:
                dummy_image = Image.new('RGB', (512, 512), color='red')
                print("   üñºÔ∏è ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± ÏôÑÎ£å")
                
                # Ï†ÑÏ≤òÎ¶¨ ÌÖåÏä§Ìä∏
                processed = converter.preprocess_image(dummy_image)
                print(f"   ‚úÖ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å - ÌÉÄÏûÖ: {type(processed)}")
                
                if NUMPY_AVAILABLE and hasattr(processed, 'shape'):
                    print(f"   üìê Ï≤òÎ¶¨Îêú ÌÅ¨Í∏∞: {processed.shape}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        else:
            print("   ‚ö†Ô∏è PIL ÏóÜÏùå - Ïù¥ÎØ∏ÏßÄ ÌÖåÏä§Ìä∏ Í±¥ÎÑàÎõ∞Í∏∞")
        
        test_time = time.time() - start_time
        print(f"‚è±Ô∏è ÌÖåÏä§Ìä∏ ÏãúÍ∞Ñ: {test_time:.3f}Ï¥à")
        print("‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False

def validate_github_compatibility():
    """GitHub ÌîÑÎ°úÏ†ùÌä∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"""
    print("\nüîó GitHub ÌîÑÎ°úÏ†ùÌä∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù")
    print("-" * 60)
    
    results = {}
    
    # 1. main.py ÌïÑÏàò Ìï®Ïàò ÌôïÏù∏
    print("1Ô∏è‚É£ main.py ÌïÑÏàò Ìï®Ïàò ÌôïÏù∏...")
    try:
        interface = get_step_model_interface("HumanParsingStep")
        results["get_step_model_interface"] = "‚úÖ"
        print("   get_step_model_interface: ‚úÖ")
    except Exception as e:
        results["get_step_model_interface"] = f"‚ùå {e}"
        print(f"   get_step_model_interface: ‚ùå {e}")
    
    try:
        memory_manager = get_step_memory_manager()
        results["get_step_memory_manager"] = "‚úÖ"
        print("   get_step_memory_manager: ‚úÖ")
    except Exception as e:
        results["get_step_memory_manager"] = f"‚ùå {e}"
        print(f"   get_step_memory_manager: ‚ùå {e}")
    
    try:
        data_converter = get_step_data_converter("ClothSegmentationStep")
        results["get_step_data_converter"] = "‚úÖ"
        print("   get_step_data_converter: ‚úÖ")
    except Exception as e:
        results["get_step_data_converter"] = f"‚ùå {e}"
        print(f"   get_step_data_converter: ‚ùå {e}")
    
    try:
        processed = preprocess_image_for_step("dummy", "VirtualFittingStep")
        results["preprocess_image_for_step"] = "‚úÖ"
        print("   preprocess_image_for_step: ‚úÖ")
    except Exception as e:
        results["preprocess_image_for_step"] = f"‚ùå {e}"
        print(f"   preprocess_image_for_step: ‚ùå {e}")
    
    # 2. ÌïµÏã¨ Î©îÏÑúÎìú ÌôïÏù∏
    print("2Ô∏è‚É£ ÌïµÏã¨ Î©îÏÑúÎìú ÌôïÏù∏...")
    try:
        interface = get_step_model_interface("ClothSegmentationStep")
        models = interface.list_available_models()
        results["list_available_models"] = "‚úÖ"
        print(f"   list_available_models: ‚úÖ ({len(models)}Í∞ú Î™®Îç∏)")
    except Exception as e:
        results["list_available_models"] = f"‚ùå {e}"
        print(f"   list_available_models: ‚ùå {e}")
    
    # 3. 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê ÌôïÏù∏
    print("3Ô∏è‚É£ 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê ÌôïÏù∏...")
    steps = list_available_steps()
    if len(steps) == 8:
        results["8_step_pipeline"] = "‚úÖ"
        print(f"   8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏: ‚úÖ ({len(steps)}Îã®Í≥Ñ)")
    else:
        results["8_step_pipeline"] = f"‚ùå {len(steps)}Îã®Í≥ÑÎßå ÏßÄÏõê"
        print(f"   8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏: ‚ùå {len(steps)}Îã®Í≥ÑÎßå ÏßÄÏõê")
    
    # 4. conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî ÌôïÏù∏
    print("4Ô∏è‚É£ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî ÌôïÏù∏...")
    if is_conda_environment():
        results["conda_optimization"] = "‚úÖ"
        print("   conda ÌôòÍ≤Ω: ‚úÖ")
    else:
        results["conda_optimization"] = "‚ö†Ô∏è conda ÌôòÍ≤Ω ÏïÑÎãò"
        print("   conda ÌôòÍ≤Ω: ‚ö†Ô∏è conda ÌôòÍ≤ΩÏù¥ ÏïÑÎãôÎãàÎã§")
    
    # 5. M3 Max ÏµúÏ†ÅÌôî ÌôïÏù∏
    print("5Ô∏è‚É£ M3 Max ÏµúÏ†ÅÌôî ÌôïÏù∏...")
    if is_m3_max_device():
        results["m3_max_optimization"] = "‚úÖ"
        print("   M3 Max ÏµúÏ†ÅÌôî: ‚úÖ")
    else:
        results["m3_max_optimization"] = "‚ÑπÔ∏è M3 Max ÏïÑÎãò"
        print("   M3 Max ÏµúÏ†ÅÌôî: ‚ÑπÔ∏è M3 Max ÎîîÎ∞îÏù¥Ïä§Í∞Ä ÏïÑÎãôÎãàÎã§")
    
    # Í≤∞Í≥º ÏöîÏïΩ
    print("\nüìä Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù Í≤∞Í≥º:")
    success_count = 0
    warning_count = 0
    error_count = 0
    
    for test, result in results.items():
        if result.startswith("‚úÖ"):
            success_count += 1
        elif result.startswith("‚ö†Ô∏è") or result.startswith("‚ÑπÔ∏è"):
            warning_count += 1
        else:
            error_count += 1
        
        print(f"   {test}: {result}")
    
    total_count = len(results)
    success_rate = (success_count / total_count) * 100
    
    print(f"\nüéØ Ìò∏ÌôòÏÑ± Ï†êÏàò: {success_rate:.1f}%")
    print(f"   ÏÑ±Í≥µ: {success_count}Í∞ú | Í≤ΩÍ≥†: {warning_count}Í∞ú | Ïò§Î•ò: {error_count}Í∞ú")
    
    if success_rate >= 85:
        print("üéâ Ïö∞ÏàòÌïú Ìò∏ÌôòÏÑ±! GitHub ÌîÑÎ°úÏ†ùÌä∏ÏôÄ ÏôÑÎ≤ΩÌïòÍ≤å Ìò∏ÌôòÎê©ÎãàÎã§.")
        grade = "A"
    elif success_rate >= 70:
        print("‚úÖ ÏñëÌò∏Ìïú Ìò∏ÌôòÏÑ±! ÎåÄÎ∂ÄÎ∂ÑÏùò Í∏∞Îä•Ïù¥ Ï†ïÏÉÅ ÏûëÎèôÌï©ÎãàÎã§.")
        grade = "B"
    elif success_rate >= 50:
        print("‚ö†Ô∏è Î≥¥ÌÜµ Ìò∏ÌôòÏÑ±. ÏùºÎ∂Ä Í∏∞Îä•Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏùÑ Ïàò ÏûàÏäµÎãàÎã§.")
        grade = "C"
    else:
        print("‚ùå Ìò∏ÌôòÏÑ± Î¨∏Ï†ú ÏûàÏùå. Ï∂îÍ∞Ä ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
        grade = "D"
    
    return {
        "results": results,
        "success_rate": success_rate,
        "grade": grade,
        "summary": {
            "success": success_count,
            "warning": warning_count,
            "error": error_count,
            "total": total_count
        }
    }

def test_all_functionality(detailed: bool = False):
    """Î™®Îì† Í∏∞Îä• Ï¢ÖÌï© ÌÖåÏä§Ìä∏"""
    print("\nüéØ Ï†ÑÏ≤¥ Í∏∞Îä• Ï¢ÖÌï© ÌÖåÏä§Ìä∏")
    print("=" * 70)
    
    test_results = []
    start_time = time.time()
    
    # 1. ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÌÖåÏä§Ìä∏
    print("üìã ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÌôïÏù∏...")
    debug_system_info(detailed=detailed)
    test_results.append(("ÏãúÏä§ÌÖú Ï†ïÎ≥¥", True))
    
    # 2. Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏ (Ï£ºÏöî StepÎì§)
    test_steps = ["HumanParsingStep", "VirtualFittingStep", "PostProcessingStep"]
    for step in test_steps:
        print(f"\nüìù {step} ÌÖåÏä§Ìä∏...")
        result = test_step_interface(step, detailed=detailed)
        test_results.append((f"{step} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§", result))
    
    # 3. Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÌÖåÏä§Ìä∏
    print(f"\nüß† Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÌÖåÏä§Ìä∏...")
    memory_result = test_memory_manager(detailed=detailed)
    test_results.append(("Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê", memory_result))
    
    # 4. Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÌÖåÏä§Ìä∏
    print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÌÖåÏä§Ìä∏...")
    converter_result = test_data_converter("HumanParsingStep")
    test_results.append(("Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞", converter_result))
    
    # 5. GitHub Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù
    print(f"\nüîó GitHub Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù...")
    compatibility_result = validate_github_compatibility()
    compat_success = compatibility_result["success_rate"] >= 70
    test_results.append(("GitHub Ìò∏ÌôòÏÑ±", compat_success))
    
    # Í≤∞Í≥º ÏöîÏïΩ
    total_time = time.time() - start_time
    
    print("\nüìã ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "‚úÖ ÌÜµÍ≥º" if result else "‚ùå Ïã§Ìå®"
        print(f"{test_name:25} : {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print("-" * 70)
    
    total_tests = len(test_results)
    success_rate = (passed / total_tests) * 100
    
    print(f"Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏: {total_tests}Í∞ú")
    print(f"ÌÜµÍ≥º: {passed}Í∞ú | Ïã§Ìå®: {failed}Í∞ú")
    print(f"ÏÑ±Í≥µÎ•†: {success_rate:.1f}%")
    print(f"Ïã§Ìñâ ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à")
    
    # ÏµúÏ¢Ö ÌåêÏ†ï
    if success_rate >= 90:
        print("\nüéâ ÏôÑÎ≤ΩÌïú ÏãúÏä§ÌÖú! Î™®Îì† Í∏∞Îä•Ïù¥ Ï†ïÏÉÅ ÏûëÎèôÌï©ÎãàÎã§.")
        grade = "A+"
    elif success_rate >= 80:
        print("\nüöÄ Ïö∞ÏàòÌïú ÏãúÏä§ÌÖú! ÎåÄÎ∂ÄÎ∂ÑÏùò Í∏∞Îä•Ïù¥ Ï†ïÏÉÅ ÏûëÎèôÌï©ÎãàÎã§.")
        grade = "A"
    elif success_rate >= 70:
        print("\n‚úÖ ÏñëÌò∏Ìïú ÏãúÏä§ÌÖú! Ï£ºÏöî Í∏∞Îä•Îì§Ïù¥ Ï†ïÏÉÅ ÏûëÎèôÌï©ÎãàÎã§.")
        grade = "B"
    elif success_rate >= 60:
        print("\n‚ö†Ô∏è Î≥¥ÌÜµ ÏàòÏ§ÄÏùò ÏãúÏä§ÌÖú. ÏùºÎ∂Ä Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
        grade = "C"
    else:
        print("\n‚ùå ÏãúÏä§ÌÖúÏóê Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§. Ï∂îÍ∞Ä ÌôïÏù∏Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
        grade = "D"
    
    print("=" * 70)
    
    return {
        "results": test_results,
        "success_rate": success_rate,
        "grade": grade,
        "execution_time": total_time,
        "passed": passed,
        "failed": failed,
        "total": total_tests
    }

# ==============================================
# üî• ÎàÑÎùΩÎêú ÌïµÏã¨ Í∏∞Îä•Îì§ Ï∂îÍ∞Ä (ÏôÑÏ†Ñ Î≥¥ÏôÑ)
# ==============================================

# 1. Î†àÍ±∞Ïãú Ìò∏Ìôò Ìï®ÏàòÎì§ Ï∂îÍ∞Ä
def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    üî• Î†àÍ±∞Ïãú Ìò∏Ìôò Ìï®Ïàò (Í∏∞Ï°¥ Step ÌÅ¥ÎûòÏä§ ÏßÄÏõê)
    ‚úÖ Í∏∞Ï°¥ ÏΩîÎìúÏôÄ 100% Ìò∏Ìôò
    """
    try:
        manager = get_utils_manager()
        unified_interface = create_unified_interface(step_name)
        
        # Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú Î≥ÄÌôò
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v8.0-complete",
            "has_unified_utils": True,
            "unified_interface": unified_interface,
            "conda_optimized": SYSTEM_INFO["in_conda"],
            "m3_max_optimized": SYSTEM_INFO["is_m3_max"]
        }
        
        # ÎπÑÎèôÍ∏∞ ÎûòÌçº Ìï®ÏàòÎì§
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        async def process_image_wrapper(image_data, **kwargs):
            return await unified_interface.process_image(image_data, **kwargs)
        
        legacy_interface.update({
            "get_model": get_model_wrapper,
            "optimize_memory": unified_interface.optimize_memory if hasattr(unified_interface, 'optimize_memory') else lambda: {"status": "ok"},
            "process_image": process_image_wrapper,
            "get_stats": unified_interface.get_stats,
            "get_config": unified_interface.get_config if hasattr(unified_interface, 'get_config') else lambda: {"step_name": step_name}
        })
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"‚ùå {step_name} Î†àÍ±∞Ïãú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "fallback": True
        }

# 2. ÎπÑÎèôÍ∏∞ Î¶¨ÏÖã Ìï®Ïàò Ï∂îÍ∞Ä
async def reset_global_utils():
    """Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Î¶¨ÏÖã"""
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.optimize_memory()
                _global_manager = None
        logger.info("‚úÖ Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Î¶¨ÏÖã ÏôÑÎ£å")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ Ïú†Ìã∏Î¶¨Ìã∞ Î¶¨ÏÖã Ïã§Ìå®: {e}")

# 3. Ìè¥Î∞± ÏÉùÏÑ± Ìï®ÏàòÎì§ Ï∂îÍ∞Ä
def _create_fallback_memory_manager(step_name: str = None, **kwargs):
    """Ìè¥Î∞± Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±"""
    class FallbackMemoryManager:
        def __init__(self, step_name=None, **kwargs):
            self.step_name = step_name
            self.device = SYSTEM_INFO["device"]
            self.memory_gb = SYSTEM_INFO["memory_gb"] 
            self.is_m3_max = SYSTEM_INFO["is_m3_max"]
            self.logger = logging.getLogger(f"FallbackMemoryManager.{step_name or 'global'}")
            self._allocated_memory = 0.0
            
        def allocate_memory(self, size_gb: float) -> bool:
            """Î©îÎ™®Î¶¨ Ìï†Îãπ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)"""
            if self._allocated_memory + size_gb <= self.memory_gb * 0.8:
                self._allocated_memory += size_gb
                self.logger.debug(f"üìù Î©îÎ™®Î¶¨ Ìï†Îãπ: {size_gb}GB")
                return True
            else:
                self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Î∂ÄÏ°±: {size_gb}GB ÏöîÏ≤≠")
                return False
        
        def cleanup_memory(self, aggressive: bool = False):
            """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
            freed = self._allocated_memory
            self._allocated_memory = 0.0
            
            # Python Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
            import gc
            collected = gc.collect()
            
            # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
            if TORCH_AVAILABLE:
                if self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except Exception:
                        pass
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.info(f"üßπ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨: {freed:.1f}GB Ìï¥Ï†ú, {collected}Í∞ú Í∞ùÏ≤¥ Ï†ïÎ¶¨")
            return {
                "success": True,
                "freed_gb": freed,
                "collected_objects": collected,
                "device": self.device
            }
        
        def get_memory_stats(self):
            """Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ (Í∏∞Î≥∏ Íµ¨ÌòÑ)"""
            try:
                available_memory = self.memory_gb - self._allocated_memory
                
                stats = {
                    "device": self.device,
                    "total_gb": self.memory_gb,
                    "allocated_gb": self._allocated_memory,
                    "available_gb": available_memory,
                    "usage_percent": (self._allocated_memory / self.memory_gb) * 100,
                    "is_m3_max": self.is_m3_max,
                    "step_name": self.step_name
                }
                
                # Ïã§Ï†ú ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï∂îÍ∞Ä (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    stats.update({
                        "system_total_gb": memory.total / (1024**3),
                        "system_available_gb": memory.available / (1024**3),
                        "system_percent": memory.percent
                    })
                
                return stats
                
            except Exception as e:
                self.logger.warning(f"Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
                return {
                    "device": self.device,
                    "error": str(e),
                    "is_fallback": True
                }
        
        def check_memory_pressure(self) -> bool:
            """Î©îÎ™®Î¶¨ ÏïïÎ∞ï ÏÉÅÌÉú ÌôïÏù∏"""
            try:
                usage_percent = (self._allocated_memory / self.memory_gb) * 100
                return usage_percent > 80.0
            except Exception:
                return False
    
    return FallbackMemoryManager(step_name, **kwargs)

def _create_fallback_data_converter(step_name: str = None, **kwargs):
    """Ìè¥Î∞± Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞"""
    class FallbackDataConverter:
        def __init__(self, step_name=None, **kwargs):
            self.step_name = step_name
            self.device = SYSTEM_INFO["device"]
            self.logger = logging.getLogger(f"FallbackDataConverter.{step_name or 'global'}")
            
            # StepÎ≥Ñ Í∏∞Î≥∏ ÏÑ§Ï†ï
            self.step_configs = {
                "HumanParsingStep": {
                    "input_size": (512, 512),
                    "normalize": True,
                    "mean": [0.485, 0.456, 0.406],
                    "std": [0.229, 0.224, 0.225]
                },
                "VirtualFittingStep": {
                    "input_size": (512, 512),
                    "normalize": True,
                    "mean": [0.5, 0.5, 0.5],
                    "std": [0.5, 0.5, 0.5]
                }
            }
            
            self.config = self.step_configs.get(step_name, {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225]
            })
            
        def configure_for_step(self, step_name: str):
            """StepÎ≥Ñ ÏÑ§Ï†ï Ï†ÅÏö©"""
            self.step_name = step_name
            self.config = self.step_configs.get(step_name, self.config)
            
        def preprocess_image(self, image, target_size=None, **kwargs):
            """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
            try:
                target_size = target_size or self.config["input_size"]
                
                # PIL Image Ï≤òÎ¶¨
                if hasattr(image, 'resize'):
                    # RGB Î≥ÄÌôò
                    if image.mode != 'RGB':
                        image = image.convert('RGB')
                    
                    # ÌÅ¨Í∏∞ Ï°∞Ï†ï (PIL Î≤ÑÏ†Ñ Ìò∏ÌôòÏÑ±)
                    if PIL_AVAILABLE:
                        try:
                            # PIL 10.0.0+ Ìò∏ÌôòÏÑ±
                            if hasattr(Image, 'Resampling') and hasattr(Image.Resampling, 'LANCZOS'):
                                image = image.resize(target_size, Image.Resampling.LANCZOS)
                            elif hasattr(Image, 'LANCZOS'):
                                image = image.resize(target_size, Image.LANCZOS)
                            else:
                                image = image.resize(target_size)
                        except Exception:
                            image = image.resize(target_size)
                    else:
                        image = image.resize(target_size)
                
                # NumPy Î∞∞Ïó¥Î°ú Î≥ÄÌôò
                if NUMPY_AVAILABLE:
                    image_array = np.array(image, dtype=np.float32)
                    
                    # Ï†ïÍ∑úÌôî
                    if self.config.get("normalize", True):
                        image_array = image_array / 255.0
                        
                        # ÌëúÏ§ÄÌôî (ÏÑ†ÌÉùÏ†Å)
                        if "mean" in self.config and "std" in self.config:
                            mean = np.array(self.config["mean"])
                            std = np.array(self.config["std"])
                            image_array = (image_array - mean) / std
                    
                    # HWC -> CHW Î≥ÄÌôò (PyTorch ÌòïÏãù)
                    if len(image_array.shape) == 3:
                        image_array = image_array.transpose(2, 0, 1)
                    
                    return image_array
                
                return image
                
            except Exception as e:
                self.logger.warning(f"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
                return image
                
        def to_tensor(self, data):
            """ÌÖêÏÑú Î≥ÄÌôò (PyTorch ÏßÄÏõê)"""
            try:
                if TORCH_AVAILABLE and NUMPY_AVAILABLE:
                    if isinstance(data, np.ndarray):
                        tensor = torch.from_numpy(data)
                        
                        # ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô
                        if self.device != "cpu":
                            tensor = tensor.to(self.device)
                        
                        return tensor
                
                return data
                
            except Exception as e:
                self.logger.warning(f"ÌÖêÏÑú Î≥ÄÌôò Ïã§Ìå®: {e}")
                return data
    
    return FallbackDataConverter(step_name, **kwargs)

# 4. Ï∂îÍ∞Ä Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
def format_memory_size(bytes_size: Union[int, float]) -> str:
    """Î©îÎ™®Î¶¨ ÌÅ¨Í∏∞ Ìè¨Îß∑ÌåÖ"""
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    size = float(bytes_size)
    
    for unit in units:
        if size < 1024.0:
            return f"{size:.1f}{unit}"
        size /= 1024.0
    
    return f"{size:.1f}PB"

def create_model_config(name: str, **kwargs) -> Dict[str, Any]:
    """Î™®Îç∏ ÏÑ§Ï†ï ÏÉùÏÑ± ÎèÑÏö∞ÎØ∏"""
    config = {
        "name": name,
        "device": kwargs.get("device", SYSTEM_INFO["device"]),
        "precision": kwargs.get("precision", SYSTEM_INFO.get("recommended_precision", "fp32")),
        "created_at": time.time(),
        **kwargs
    }
    return config

# 5. ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏ Ìï®Ïàò Ï∂îÍ∞Ä
async def test_async_operations():
    """ÎπÑÎèôÍ∏∞ ÏûëÏóÖ ÌÖåÏä§Ìä∏"""
    print("\nüîÑ ÎπÑÎèôÍ∏∞ ÏûëÏóÖ ÌÖåÏä§Ìä∏")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏
        print("1Ô∏è‚É£ Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî...")
        manager = get_utils_manager()
        
        if not manager.is_initialized:
            manager.is_initialized = True
            manager.initialization_time = time.time()
            print("   Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: ‚úÖ")
        else:
            print("   Ïù¥ÎØ∏ Ï¥àÍ∏∞ÌôîÎê®: ‚úÖ")
        
        # 2. Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏
        print("2Ô∏è‚É£ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏...")
        interface = get_step_model_interface("VirtualFittingStep")
        
        # Î™®Îç∏ Î°úÎìú ÌÖåÏä§Ìä∏
        model_start = time.time()
        model = await interface.get_model()
        model_time = time.time() - model_start
        
        if model:
            print(f"   Î™®Îç∏ Î°úÎìú: ‚úÖ ({model_time:.3f}Ï¥à)")
            if isinstance(model, dict):
                print(f"   Î™®Îç∏ ÌÉÄÏûÖ: {model.get('type', 'unknown')}")
        else:
            print(f"   Î™®Îç∏ Î°úÎìú: ‚ùå ({model_time:.3f}Ï¥à)")
        
        # 3. ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏
        print("3Ô∏è‚É£ ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏...")
        unified_interface = create_unified_interface("PostProcessingStep")
        
        # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î°ú Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏
        dummy_image = {"width": 512, "height": 512, "channels": 3}
        
        process_start = time.time()
        result = await unified_interface.process_image(dummy_image)
        process_time = time.time() - process_start
        
        if result and result.get("success"):
            print(f"   Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨: ‚úÖ ({process_time:.3f}Ï¥à)")
            print(f"   Ï≤òÎ¶¨ Í≤∞Í≥º: {result.get('output_type', 'unknown')}")
        else:
            print(f"   Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨: ‚ùå ({process_time:.3f}Ï¥à)")
        
        # 4. Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÌÖåÏä§Ìä∏
        print("4Ô∏è‚É£ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÌÖåÏä§Ìä∏...")
        memory_start = time.time()
        memory_result = await manager.optimize_memory()
        memory_time = time.time() - memory_start
        
        if memory_result.get("success"):
            print(f"   Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî: ‚úÖ ({memory_time:.3f}Ï¥à)")
            if memory_result.get("results", {}).get("collected_objects"):
                print(f"   Ï†ïÎ¶¨Îêú Í∞ùÏ≤¥: {memory_result['results']['collected_objects']}Í∞ú")
        else:
            print(f"   Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî: ‚ùå ({memory_time:.3f}Ï¥à)")
        
        total_time = time.time() - start_time
        print(f"‚è±Ô∏è Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ ÏãúÍ∞Ñ: {total_time:.3f}Ï¥à")
        print("‚úÖ ÎπÑÎèôÍ∏∞ ÏûëÏóÖ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False

# 6. Ìñ•ÏÉÅÎêú ÏãúÏä§ÌÖú ÏÉÅÌÉú Ìï®Ïàò
def get_enhanced_system_status() -> Dict[str, Any]:
    """Ìñ•ÏÉÅÎêú ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        basic_status = get_system_status()
        
        # Ï∂îÍ∞Ä Ï†ïÎ≥¥
        enhanced_info = {
            "runtime_info": {
                "uptime_seconds": time.time() - _module_start_time,
                "python_executable": sys.executable,
                "working_directory": str(Path.cwd()),
                "process_id": os.getpid()
            },
            "performance_info": {
                "cpu_usage": psutil.cpu_percent() if PSUTIL_AVAILABLE else "unknown",
                "memory_usage": psutil.virtual_memory().percent if PSUTIL_AVAILABLE else "unknown",
                "disk_usage": psutil.disk_usage('/').percent if PSUTIL_AVAILABLE else "unknown"
            },
            "library_versions": {
                "python": sys.version,
                "torch": TORCH_VERSION,
                "numpy": NUMPY_VERSION if NUMPY_AVAILABLE else "not_available",
                "pillow": PIL_VERSION  # ‚úÖ ÏïàÏ†ÑÌïú PIL Î≤ÑÏ†Ñ ÏÇ¨Ïö©
            }
        }
        
        # Í∏∞Î≥∏ ÏÉÅÌÉúÏôÄ Î≥ëÌï©
        if isinstance(basic_status, dict):
            enhanced_status = {**basic_status, **enhanced_info}
        else:
            enhanced_status = enhanced_info
            
        return enhanced_status
        
    except Exception as e:
        logger.error(f"Ìñ•ÏÉÅÎêú ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {"error": str(e)}

# ==============================================
# üî• __all__ ÏóÖÎç∞Ïù¥Ìä∏ (ÎàÑÎùΩÎêú Ìï®ÏàòÎì§ Ï∂îÍ∞Ä)
# ==============================================

__all__ = [
    # üéØ ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'UnifiedUtilsManager',
    'UnifiedStepInterface', 
    'StepModelInterface',
    'StepMemoryManager',
    'StepDataConverter',
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # üîß Ïó¥Í±∞Ìòï
    'UtilsMode',
    'DeviceType',
    'PrecisionType', 
    'StepType',
    
    # üîÑ Ï†ÑÏó≠ Ìï®ÏàòÎì§
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'get_enhanced_system_status',  # ‚úÖ Ï∂îÍ∞Ä
    'optimize_system_memory',
    'reset_global_utils',          # ‚úÖ Ï∂îÍ∞Ä
    
    # üîó Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (main.py Ìò∏Ìôò)
    'get_step_model_interface',    # ‚úÖ main.py ÌïµÏã¨ Ìï®Ïàò
    'get_step_memory_manager',     # ‚úÖ main.py ÌïµÏã¨ Ìï®Ïàò  
    'get_step_data_converter',     # ‚úÖ main.py ÌïµÏã¨ Ìï®Ïàò
    'preprocess_image_for_step',   # ‚úÖ main.py ÌïµÏã¨ Ìï®Ïàò
    'create_unified_interface',    # ÏÉàÎ°úÏö¥ Î∞©Ïãù
    'create_step_interface',       # ‚úÖ Î†àÍ±∞Ïãú Ìò∏Ìôò Ï∂îÍ∞Ä
    
    # üìä ÏãúÏä§ÌÖú Ï†ïÎ≥¥
    'SYSTEM_INFO',
    'get_ai_models_path',
    'get_device_info',
    'get_conda_info',
    
    # üîß Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'list_available_steps',
    'is_conda_environment',
    'is_m3_max_device',
    'validate_step_name',
    'get_step_number',
    'check_system_requirements',
    'format_memory_size',          # ‚úÖ Ï∂îÍ∞Ä
    'create_model_config',         # ‚úÖ Ï∂îÍ∞Ä
    
    # üîß Ìè¥Î∞± Ìï®ÏàòÎì§ (ÎÇ¥Î∂ÄÏö©Ïù¥ÏßÄÎßå export)
    '_create_fallback_memory_manager',   # ‚úÖ Ï∂îÍ∞Ä
    '_create_fallback_data_converter',   # ‚úÖ Ï∂îÍ∞Ä
    
    # üß™ Í∞úÎ∞ú/ÎîîÎ≤ÑÍ∑∏ Ìï®ÏàòÎì§
    'debug_system_info',
    'test_step_interface',
    'test_memory_manager',
    'test_data_converter',
    'test_async_operations',       # ‚úÖ Ï∂îÍ∞Ä
    'validate_github_compatibility',
    'test_all_functionality'
]

# ==============================================
# üî• Î™®Îìà Ï¥àÍ∏∞Ìôî Î∞è ÌôòÍ≤Ω Ï†ïÎ≥¥ (ÏôÑÏ†Ñ Íµ¨ÌòÑ)
# ==============================================

# ÏãúÏûë ÏãúÍ∞Ñ Í∏∞Î°ù
_module_start_time = time.time()

# ÌôòÍ≤Ω Ï†ïÎ≥¥ Î°úÍπÖ
logger.info("‚úÖ PIL.__version__ Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞ (PIL ÏµúÏã† Î≤ÑÏ†Ñ Ìò∏Ìôò)")
logger.info("‚úÖ PIL 10.0.0+ Image.Resampling.LANCZOS Ìò∏ÌôòÏÑ± Ï∂îÍ∞Ä")
logger.info("‚úÖ Î™®Îì† PIL Î≤ÑÏ†ÑÏóêÏÑú ÏïàÏ†ÑÌïú Ïù¥ÎØ∏ÏßÄ Î¶¨ÏÉòÌîåÎßÅ Î≥¥Ïû•")
logger.info("‚úÖ create_step_interface Î†àÍ±∞Ïãú Ìò∏Ìôò Ìï®Ïàò Ï∂îÍ∞Ä")
logger.info("‚úÖ reset_global_utils ÎπÑÎèôÍ∏∞ Î¶¨ÏÖã Ìï®Ïàò Ï∂îÍ∞Ä")
logger.info("‚úÖ _create_fallback_* Ìè¥Î∞± ÏÉùÏÑ± Ìï®ÏàòÎì§ Ï∂îÍ∞Ä")
logger.info("‚úÖ format_memory_size, create_model_config Ïú†Ìã∏Î¶¨Ìã∞ Ï∂îÍ∞Ä")
logger.info("‚úÖ test_async_operations ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏ Ìï®Ïàò Ï∂îÍ∞Ä")
logger.info("‚úÖ get_enhanced_system_status Ìñ•ÏÉÅÎêú ÏÉÅÌÉú Ï°∞Ìöå Ï∂îÍ∞Ä")
logger.info("=" * 80)
logger.info("üçé MyCloset AI ÏôÑÏ†ÑÌïú ÌÜµÌï© Ïú†Ìã∏Î¶¨Ìã∞ ÏãúÏä§ÌÖú v8.0 Î°úÎìú ÏôÑÎ£å")
logger.info("‚úÖ Îëê ÌååÏùºÏùò Î™®Îì† Í∏∞Îä• ÏôÑÏ†Ñ ÌÜµÌï© (ÏµúÍ≥†Ïùò Ï°∞Ìï©)")
logger.info("‚úÖ get_step_model_interface Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ get_step_memory_manager Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ get_step_data_converter Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ preprocess_image_for_step Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ StepModelInterface.list_available_models ÏôÑÏ†Ñ Ìè¨Ìï®")
logger.info("‚úÖ UnifiedStepInterface ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Íµ¨ÌòÑ")
logger.info("‚úÖ StepDataConverter Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏãúÏä§ÌÖú Íµ¨ÌòÑ")
logger.info("‚úÖ conda ÌôòÍ≤Ω 100% ÏµúÏ†ÅÌôî")
logger.info("‚úÖ M3 Max 128GB Î©îÎ™®Î¶¨ ÏôÑÏ†Ñ ÌôúÏö©")
logger.info("‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê")
logger.info("‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ Clean Architecture Ï†ÅÏö©")
logger.info("‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("‚úÖ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ± Î≥¥Ïû•")
logger.info("‚úÖ Î™®Îì† import Ïò§Î•ò Ìï¥Í≤∞")
logger.info("‚úÖ ÏôÑÏ†ÑÌïú Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò")
logger.info("‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî")
logger.info("‚úÖ GPU Ìò∏ÌôòÏÑ± ÏôÑÏ†Ñ Î≥¥Ïû•")
logger.info("‚úÖ ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎßÅ Î∞è ÌÖåÏä§Ìä∏ Ìï®Ïàò Ìè¨Ìï®")

# ÏãúÏä§ÌÖú ÌôòÍ≤Ω Ï†ïÎ≥¥
logger.info(f"üîß ÌîåÎû´Ìèº: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
logger.info(f"üçé M3 Max: {'‚úÖ' if SYSTEM_INFO['is_m3_max'] else '‚ùå'}")
logger.info(f"üíæ Î©îÎ™®Î¶¨: {SYSTEM_INFO['memory_gb']}GB")
logger.info(f"üéØ ÎîîÎ∞îÏù¥Ïä§: {SYSTEM_INFO['device']} ({SYSTEM_INFO.get('device_name', 'Unknown')})")
logger.info(f"üêç Python: {SYSTEM_INFO['python_version']}")
logger.info(f"üêç conda ÌôòÍ≤Ω: {'‚úÖ' if SYSTEM_INFO['in_conda'] else '‚ùå'} ({SYSTEM_INFO['conda_env']})")

# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÉÅÌÉú
libraries = SYSTEM_INFO.get("libraries", {})
logger.info(f"üìö PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'} ({libraries.get('torch', 'N/A')})")
logger.info(f"üìö NumPy: {'‚úÖ' if NUMPY_AVAILABLE else '‚ùå'} ({libraries.get('numpy', 'N/A')})")
logger.info(f"üìö PIL: {'‚úÖ' if PIL_AVAILABLE else '‚ùå'} ({PIL_VERSION})")
logger.info(f"üìö psutil: {'‚úÖ' if PSUTIL_AVAILABLE else '‚ùå'}")

# ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú
logger.info(f"üìÅ ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏: {SYSTEM_INFO['project_root']}")
logger.info(f"üìÅ AI Î™®Îç∏ Í≤ΩÎ°ú: {SYSTEM_INFO['ai_models_path']}")
logger.info(f"üìÅ Î™®Îç∏ Ìè¥Îçî Ï°¥Ïû¨: {'‚úÖ' if SYSTEM_INFO['ai_models_exists'] else '‚ùå'}")

# ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÉÅÌÉú
if SYSTEM_INFO["in_conda"]:
    logger.info("üêç conda ÌôòÍ≤Ω Í∞êÏßÄ - Í≥†ÏÑ±Îä• ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî")
    if SYSTEM_INFO["is_m3_max"]:
        logger.info("üçé M3 Max + conda Ï°∞Ìï© - ÏµúÍ≥† ÏÑ±Îä• Î™®Îìú ÌôúÏÑ±Ìôî")
        logger.info("üöÄ 128GB Unified Memory ÌôúÏö© Í∞ÄÎä•")

# Î™®Îìà Î°úÎìú ÏãúÍ∞Ñ
module_load_time = time.time() - _module_start_time
logger.info(f"‚ö° Î™®Îìà Î°úÎìú ÏãúÍ∞Ñ: {module_load_time:.3f}Ï¥à")

# ÌïÑÏàò Ìï®Ïàò ÏôÑÏÑ±ÎèÑ Í≤ÄÏ¶ù
try:
    required_functions = [
        'get_step_model_interface',
        'get_step_memory_manager', 
        'get_step_data_converter',
        'preprocess_image_for_step'
    ]
    
    missing_functions = []
    for func_name in required_functions:
        if func_name not in globals():
            missing_functions.append(func_name)
    
    if missing_functions:
        logger.warning(f"‚ö†Ô∏è ÎàÑÎùΩÎêú Ìï®ÏàòÎì§: {missing_functions}")
    else:
        logger.info("‚úÖ Î™®Îì† ÌïÑÏàò Ìï®Ïàò Íµ¨ÌòÑ ÏôÑÎ£å")
        
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Ìï®Ïàò ÏôÑÏÑ±ÎèÑ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")

logger.info("=" * 80)

# ÏãúÏä§ÌÖú ÏöîÍµ¨ÏÇ¨Ìï≠ Ï≤¥ÌÅ¨ (ÏÑ†ÌÉùÏ†Å)
try:
    requirements = check_system_requirements()
    if requirements["overall_satisfied"]:
        logger.info(f"‚úÖ ÏãúÏä§ÌÖú ÏöîÍµ¨ÏÇ¨Ìï≠ ÎßåÏ°± (Ï†êÏàò: {requirements['score']:.0f}%)")
    else:
        logger.warning(f"‚ö†Ô∏è ÏùºÎ∂Ä ÏãúÏä§ÌÖú ÏöîÍµ¨ÏÇ¨Ìï≠ ÎØ∏Ï∂©Ï°± (Ï†êÏàò: {requirements['score']:.0f}%)")
        
except Exception as e:
    logger.debug(f"ÏãúÏä§ÌÖú ÏöîÍµ¨ÏÇ¨Ìï≠ Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨ Ìï®Ïàò Îì±Î°ù
# ==============================================

import atexit

def cleanup_on_exit():
    """ÌîÑÎ°úÍ∑∏Îû® Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨"""
    try:
        logger.info("üßπ ÌîÑÎ°úÍ∑∏Îû® Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨ ÏãúÏûë...")
        
        # Ï†ÑÏó≠ Îß§ÎãàÏ†Ä Ï†ïÎ¶¨
        global _global_manager
        if _global_manager:
            try:
                # ÎèôÍ∏∞ Ï†ïÎ¶¨
                _global_manager.global_memory_manager.cleanup_memory(force=True)
                logger.info("‚úÖ Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        
        # Python Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
        collected = gc.collect()
        logger.info(f"üóëÔ∏è Python Í∞ùÏ≤¥ {collected}Í∞ú Ï†ïÎ¶¨")
        
        # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
        if TORCH_AVAILABLE:
            device = SYSTEM_INFO["device"]
            if device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("üóëÔ∏è CUDA Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å")
            elif device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    logger.info("üóëÔ∏è MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å")
                except Exception as e:
                    logger.debug(f"MPS Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        
        logger.info("üéâ ÌîÑÎ°úÍ∑∏Îû® Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨ ÏôÑÎ£å")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# Ï†ïÎ¶¨ Ìï®Ïàò Îì±Î°ù
atexit.register(cleanup_on_exit)

# ==============================================
# üî• Î©îÏù∏ Ïã§Ìñâ Î∂ÄÎ∂Ñ (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)
# ==============================================

def main():
    """Î©îÏù∏ Ìï®Ïàò (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)"""
    print("üçé MyCloset AI ÏôÑÏ†ÑÌïú ÌÜµÌï© Ïú†Ìã∏Î¶¨Ìã∞ ÏãúÏä§ÌÖú v8.0")
    print("=" * 70)
    print("üìã Ï†ÑÏ≤¥ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º Ïã§ÌñâÌï©ÎãàÎã§...")
    print()
    
    # Ï†ÑÏ≤¥ Í∏∞Îä• ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    try:
        success_data = test_all_functionality(detailed=True)
        success = success_data["success_rate"] >= 70
        
        # ÏµúÏ¢Ö Í≤∞Í≥º Ï∂úÎ†•
        if success:
            print("\nüöÄ ÏãúÏä§ÌÖú Ï§ÄÎπÑ ÏôÑÎ£å! main.pyÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.")
            print("\nüìñ ÏÇ¨Ïö© ÏòàÏãú:")
            print("```python")
            print("from app.ai_pipeline.utils import (")
            print("    get_step_model_interface,")
            print("    get_step_memory_manager,")
            print("    get_step_data_converter,")
            print("    preprocess_image_for_step")
            print(")")
            print("")
            print("# Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±")
            print("interface = get_step_model_interface('HumanParsingStep')")
            print("models = interface.list_available_models()")
            print("")
            print("# Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±")
            print("memory_manager = get_step_memory_manager('HumanParsingStep')")
            print("stats = memory_manager.get_memory_stats()")
            print("")
            print("# Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôòÍ∏∞ ÏÉùÏÑ±")
            print("data_converter = get_step_data_converter('HumanParsingStep')")
            print("processed_image = preprocess_image_for_step(image, 'HumanParsingStep')")
            print("")
            print("# ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú")
            print("model = await interface.get_model()")
            print("```")
            print()
            print("üéØ Ï£ºÏöî Í∏∞Îä•:")
            print("   ‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ ÏßÄÏõê")
            print("   ‚úÖ conda ÌôòÍ≤Ω 100% ÏµúÏ†ÅÌôî")
            print("   ‚úÖ M3 Max 128GB Î©îÎ™®Î¶¨ ÏôÑÏ†Ñ ÌôúÏö©")
            print("   ‚úÖ ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏôÑÏ†Ñ Íµ¨ÌòÑ")
            print("   ‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî")
            print("   ‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏãúÏä§ÌÖú ÏôÑÏ†Ñ Íµ¨ÌòÑ")
            print("   ‚úÖ ÏôÑÏ†ÑÌïú Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò")
            print("   ‚úÖ ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎßÅ Î∞è ÌÖåÏä§Ìä∏ Ìï®Ïàò")
        else:
            print("\n‚ö†Ô∏è ÏãúÏä§ÌÖúÏóê ÏùºÎ∂Ä Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§.")
            print("   Î°úÍ∑∏Î•º ÌôïÏù∏ÌïòÏãúÍ±∞ÎÇò Í∞úÎ≥Ñ ÌÖåÏä§Ìä∏Î•º Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.")
            print("\nüîß Í∞úÎ≥Ñ ÌÖåÏä§Ìä∏ Ïã§Ìñâ:")
            print("   python -c \"from app.ai_pipeline.utils import debug_system_info; debug_system_info()\"")
            print("   python -c \"from app.ai_pipeline.utils import test_step_interface; test_step_interface()\"")
            print("   python -c \"from app.ai_pipeline.utils import validate_github_compatibility; validate_github_compatibility()\"")
        
        return success
        
    except Exception as e:
        print(f"\n‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        print("\nüîß Í∏∞Î≥∏ Ï†ïÎ≥¥Îßå ÌôïÏù∏:")
        try:
            debug_system_info()
        except Exception as debug_e:
            print(f"   ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {debug_e}")
        
        return False

if __name__ == "__main__":
    main()