# backend/app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI ì™„ì „í•œ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0 - ìµœì¢… ì™„ì„±íŒ
================================================================================
âœ… ë‘ íŒŒì¼ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ í†µí•© (ìµœê³ ì˜ ì¡°í•©)
âœ… get_step_model_interface í•¨ìˆ˜ ì™„ì „ êµ¬í˜„
âœ… get_step_memory_manager í•¨ìˆ˜ ì™„ì „ êµ¬í˜„  
âœ… get_step_data_converter í•¨ìˆ˜ ì™„ì „ êµ¬í˜„
âœ… preprocess_image_for_step í•¨ìˆ˜ ì™„ì „ êµ¬í˜„
âœ… StepModelInterface.list_available_models ì™„ì „ í¬í•¨
âœ… UnifiedStepInterface í†µí•© ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
âœ… StepDataConverter ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ êµ¬í˜„
âœ… conda í™˜ê²½ 100% ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©
âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… Clean Architecture ì ìš©
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  import ì˜¤ë¥˜ í•´ê²°
âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… GPU í˜¸í™˜ì„± ì™„ì „ ë³´ì¥
âœ… ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í¬í•¨

main.py í˜¸ì¶œ íŒ¨í„´ (ì™„ì „ í˜¸í™˜):
from app.ai_pipeline.utils import (
    get_step_model_interface, 
    get_step_memory_manager, 
    get_step_data_converter, 
    preprocess_image_for_step
)
interface = get_step_model_interface("HumanParsingStep")
models = interface.list_available_models()
memory_manager = get_step_memory_manager("HumanParsingStep")
data_converter = get_step_data_converter("HumanParsingStep")
processed_image = preprocess_image_for_step(image, "HumanParsingStep")
"""

import os
import sys
import logging
import threading
import asyncio
import time
import gc
import weakref
import json
import hashlib
import shutil
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from functools import lru_cache, wraps
from abc import ABC, abstractmethod
from contextlib import contextmanager, asynccontextmanager
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

# ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    import PIL
    PIL_AVAILABLE = True
    # PIL ë²„ì „ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (ìµœì‹  ë²„ì „ í˜¸í™˜)
    try:
        PIL_VERSION = PIL.__version__  # ìµœì‹  ë°©ì‹
    except AttributeError:
        try:
            PIL_VERSION = Image.__version__  # êµ¬ë²„ì „ ë°©ì‹
        except AttributeError:
            PIL_VERSION = "unknown"
except ImportError:
    PIL_AVAILABLE = False
    Image = None
    PIL = None
    PIL_VERSION = "not_available"

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° í™˜ê²½ ê°ì§€ (ì™„ì „ êµ¬í˜„)
# ==============================================

@lru_cache(maxsize=1)
def _detect_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ì™„ì „ ê°ì§€ - conda í™˜ê²½ ìš°ì„ """
    try:
        import platform
        import subprocess
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ,
            "conda_prefix": os.environ.get('CONDA_PREFIX', ''),
            "virtual_env": os.environ.get('VIRTUAL_ENV', ''),
            "python_path": sys.executable
        }
        
        # M3 Max íŠ¹ë³„ ê°ì§€
        is_m3_max = False
        m3_info = {"detected": False, "model": "unknown", "cores": 0}
        
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                # CPU ë¸Œëœë“œ í™•ì¸
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                brand = result.stdout.strip()
                if 'M3' in brand:
                    is_m3_max = True
                    m3_info = {
                        "detected": True,
                        "model": "M3 Max" if "Max" in brand else "M3",
                        "brand": brand
                    }
                
                # GPU ì½”ì–´ ìˆ˜ í™•ì¸
                try:
                    result = subprocess.run(
                        ['sysctl', '-n', 'hw.gpu.family_id'], 
                        capture_output=True, text=True, timeout=3
                    )
                    if result.returncode == 0:
                        m3_info["gpu_cores"] = 40 if "Max" in brand else 20
                except:
                    pass
                    
            except Exception as e:
                logger.debug(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
        
        system_info.update({
            "is_m3_max": is_m3_max,
            "m3_info": m3_info
        })
        
        # ë©”ëª¨ë¦¬ ì •ë³´ (ì •í™•í•œ ê°ì§€)
        memory_gb = 16.0  # ê¸°ë³¸ê°’
        if PSUTIL_AVAILABLE:
            try:
                vm = psutil.virtual_memory()
                memory_gb = round(vm.total / (1024**3), 1)
                system_info["memory_details"] = {
                    "total_gb": memory_gb,
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent_used": vm.percent
                }
            except Exception:
                pass
        
        system_info["memory_gb"] = memory_gb
        
        # GPU/ë””ë°”ì´ìŠ¤ ê°ì§€ (ì™„ì „ êµ¬í˜„)
        device_info = _detect_best_device(is_m3_max)
        system_info.update(device_info)
        
        # AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡° ë°˜ì˜)
        project_root = Path(__file__).parent.parent.parent.parent
        ai_models_path = project_root / "ai_models"
        
        system_info.update({
            "project_root": str(project_root),
            "ai_models_path": str(ai_models_path),
            "ai_models_exists": ai_models_path.exists(),
            "config_path": str(project_root / "backend" / "app" / "core"),
            "scripts_path": str(project_root / "scripts")
        })
        
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ì •ë³´ (PIL ì˜¤ë¥˜ í•´ê²°)
        system_info["libraries"] = {
            "torch": TORCH_VERSION,
            "numpy": NUMPY_VERSION if NUMPY_AVAILABLE else "not_available",
            "pillow": PIL_VERSION,  # âœ… ì•ˆì „í•œ PIL ë²„ì „ ì‚¬ìš©
            "psutil": psutil.version_info if PSUTIL_AVAILABLE else "not_available"
        }
        
        return system_info
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "platform": "unknown",
            "machine": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "device_name": "CPU",
            "device_available": True,
            "cpu_count": 4,
            "memory_gb": 16.0,
            "python_version": "3.8.0",
            "conda_env": "base",
            "in_conda": False,
            "project_root": str(Path.cwd()),
            "ai_models_path": str(Path.cwd() / "ai_models"),
            "ai_models_exists": False,
            "libraries": {}
        }

def _detect_best_device(is_m3_max: bool = False) -> Dict[str, Any]:
    """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€ (M3 Max ìš°ì„ )"""
    device_info = {
        "device": "cpu",
        "device_name": "CPU",
        "device_available": True,
        "device_memory_gb": 0.0,
        "device_capabilities": []
    }
    
    if not TORCH_AVAILABLE:
        return device_info
    
    try:
        # M3 Max MPS ìš°ì„  (ìµœê³  ì„±ëŠ¥)
        if is_m3_max and torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple M3 Max GPU",
                "device_available": True,
                "device_memory_gb": 128.0,  # Unified Memory
                "device_capabilities": ["fp16", "bf16", "metal", "unified_memory"],
                "recommended_precision": "fp16",
                "max_batch_size": 32,
                "optimization_level": "maximum"
            })
            logger.info("ğŸ M3 Max MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨ - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ")
            
        # CUDA ê°ì§€
        elif torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            device_info.update({
                "device": "cuda",
                "device_name": device_name,
                "device_available": True,
                "device_memory_gb": round(device_memory, 1),
                "device_count": device_count,
                "device_capabilities": ["fp16", "bf16", "tensor_cores"],
                "recommended_precision": "fp16",
                "max_batch_size": 16,
                "optimization_level": "high"
            })
            logger.info(f"ğŸš€ CUDA ë””ë°”ì´ìŠ¤ ê°ì§€ë¨: {device_name}")
            
        # ì¼ë°˜ MPS (M1/M2)
        elif torch.backends.mps.is_available():
            device_info.update({
                "device": "mps",
                "device_name": "Apple Silicon GPU",
                "device_available": True,
                "device_memory_gb": 16.0,  # ì¶”ì •ê°’
                "device_capabilities": ["fp16", "metal"],
                "recommended_precision": "fp16",
                "max_batch_size": 8,
                "optimization_level": "medium"
            })
            logger.info("ğŸ Apple Silicon MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨")
            
        else:
            # CPU í´ë°±
            device_info.update({
                "device": "cpu",
                "device_name": "CPU (Multi-threaded)",
                "device_available": True,
                "device_memory_gb": 8.0,
                "device_capabilities": ["fp32", "multi_threading"],
                "recommended_precision": "fp32",
                "max_batch_size": 4,
                "optimization_level": "basic"
            })
            logger.info("ğŸ’» CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
        
    except Exception as e:
        logger.warning(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return device_info

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _detect_system_info()

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ë° ì„¤ì • (ì™„ì „ êµ¬í˜„)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"
    UNIFIED = "unified"
    HYBRID = "hybrid"
    FALLBACK = "fallback"
    PRODUCTION = "production"

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"
    AUTO = "auto"

class PrecisionType(Enum):
    """ì •ë°€ë„ íƒ€ì…"""
    FP32 = "fp32"
    FP16 = "fp16"
    BF16 = "bf16"
    INT8 = "int8"
    AUTO = "auto"

class StepType(Enum):
    """AI íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ íƒ€ì…"""
    HUMAN_PARSING = "HumanParsingStep"
    POSE_ESTIMATION = "PoseEstimationStep"
    CLOTH_SEGMENTATION = "ClothSegmentationStep"
    GEOMETRIC_MATCHING = "GeometricMatchingStep"
    CLOTH_WARPING = "ClothWarpingStep"
    VIRTUAL_FITTING = "VirtualFittingStep"
    POST_PROCESSING = "PostProcessingStep"
    QUALITY_ASSESSMENT = "QualityAssessmentStep"

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • (ì™„ì „ êµ¬í˜„)"""
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "auto"
    device_memory_gb: float = 0.0
    
    # ì„±ëŠ¥ ì„¤ì •
    max_workers: int = 4
    max_batch_size: int = 8
    optimization_level: str = "medium"
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory_limit_gb: float = 16.0
    cache_enabled: bool = True
    memory_cleanup_threshold: float = 0.8
    
    # conda í™˜ê²½ ì„¤ì •
    conda_optimized: bool = True
    conda_env: str = "base"
    
    # ë””ë²„ê·¸ ì„¤ì •
    debug_mode: bool = False
    verbose_logging: bool = False
    profile_performance: bool = False
    
    # AI íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipeline_mode: str = "sequential"
    enable_async: bool = True
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ìë™ ì„¤ì •"""
        # ì‹œìŠ¤í…œ ì •ë³´ ê¸°ë°˜ ìë™ ì„¤ì •
        if self.device == "auto":
            self.device = SYSTEM_INFO["device"]
        
        if self.precision == "auto":
            self.precision = SYSTEM_INFO.get("recommended_precision", "fp32")
        
        if self.device_memory_gb == 0.0:
            self.device_memory_gb = SYSTEM_INFO.get("device_memory_gb", 16.0)
        
        # M3 Max íŠ¹í™” ìµœì í™”
        if SYSTEM_INFO["is_m3_max"]:
            self.max_workers = min(12, SYSTEM_INFO["cpu_count"])
            self.max_batch_size = 32
            self.optimization_level = "maximum"
            self.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
        
        # conda í™˜ê²½ ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            self.conda_optimized = True
            self.conda_env = SYSTEM_INFO["conda_env"]

@dataclass
class StepConfig:
    """Step ì„¤ì • (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›)"""
    step_name: str
    step_number: Optional[int] = None
    step_type: Optional[StepType] = None
    
    # ëª¨ë¸ ì„¤ì •
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_path: Optional[str] = None
    
    # ì…ë ¥/ì¶œë ¥ ì„¤ì •
    input_size: Tuple[int, int] = (512, 512)
    output_size: Optional[Tuple[int, int]] = None
    batch_size: int = 1
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "auto"
    
    # ì„±ëŠ¥ ì„¤ì •
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    memory_efficient: bool = True
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Step ì •ë³´ ìë™ ì„¤ì •"""
        # Step íƒ€ì… ì„¤ì •
        if self.step_type is None:
            for step_type in StepType:
                if step_type.value == self.step_name:
                    self.step_type = step_type
                    break
        
        # Step ë²ˆí˜¸ ìë™ ì„¤ì •
        if self.step_number is None:
            step_numbers = {
                StepType.HUMAN_PARSING: 1,
                StepType.POSE_ESTIMATION: 2,
                StepType.CLOTH_SEGMENTATION: 3,
                StepType.GEOMETRIC_MATCHING: 4,
                StepType.CLOTH_WARPING: 5,
                StepType.VIRTUAL_FITTING: 6,
                StepType.POST_PROCESSING: 7,
                StepType.QUALITY_ASSESSMENT: 8
            }
            self.step_number = step_numbers.get(self.step_type, 0)
        
        # ê¸°ë³¸ ëª¨ë¸ëª… ì„¤ì •
        if self.model_name is None:
            default_models = {
                StepType.HUMAN_PARSING: "graphonomy",
                StepType.POSE_ESTIMATION: "openpose",
                StepType.CLOTH_SEGMENTATION: "u2net",
                StepType.GEOMETRIC_MATCHING: "geometric_matching",
                StepType.CLOTH_WARPING: "cloth_warping",
                StepType.VIRTUAL_FITTING: "ootdiffusion",
                StepType.POST_PROCESSING: "post_processing",
                StepType.QUALITY_ASSESSMENT: "clipiqa"
            }
            self.model_name = default_models.get(self.step_type, "default_model")

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (ì™„ì „ êµ¬í˜„)"""
    name: str
    path: str
    model_type: str
    
    # íŒŒì¼ ì •ë³´
    file_size_mb: float
    file_hash: Optional[str] = None
    last_modified: Optional[float] = None
    
    # í˜¸í™˜ì„± ì •ë³´
    step_compatibility: List[str] = field(default_factory=list)
    device_compatibility: List[str] = field(default_factory=list)
    precision_support: List[str] = field(default_factory=list)
    
    # ì„±ëŠ¥ ì •ë³´
    confidence_score: float = 1.0
    performance_score: float = 1.0
    memory_usage_mb: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    architecture: Optional[str] = None
    version: Optional[str] = None
    description: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelInfo':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        return cls(**data)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ì™„ì „ êµ¬í˜„)
# ==============================================

class StepMemoryManager:
    """
    ğŸ§  Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ì™„ì „ êµ¬í˜„)
    âœ… M3 Max 128GB ì™„ì „ ìµœì í™”
    âœ… conda í™˜ê²½ íŠ¹í™”
    âœ… ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    âœ… ìë™ ì •ë¦¬ ë©”ì»¤ë‹ˆì¦˜
    âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
    """
    
    def __init__(
        self, 
        step_name: str = "global",
        device: str = "auto", 
        memory_limit_gb: Optional[float] = None,
        cleanup_threshold: float = 0.8,
        auto_cleanup: bool = True
    ):
        self.step_name = step_name
        self.device = device if device != "auto" else SYSTEM_INFO["device"]
        self.memory_limit_gb = memory_limit_gb or SYSTEM_INFO["memory_gb"]
        self.cleanup_threshold = cleanup_threshold
        self.auto_cleanup = auto_cleanup
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        if self.is_m3_max:
            self.memory_limit_gb = min(self.memory_limit_gb, 100.0)  # 128GB ì¤‘ 100GB ì‚¬ìš©
            self.cleanup_threshold = 0.9  # M3 MaxëŠ” ë” ê´€ëŒ€í•˜ê²Œ
        
        # ë©”ëª¨ë¦¬ ì¶”ì 
        self.allocated_memory: Dict[str, float] = {}
        self.memory_history: List[Dict[str, Any]] = []
        self.peak_usage = 0.0
        self.total_allocations = 0
        self.total_deallocations = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"{__name__}.StepMemoryManager")
        
        # ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        if self.auto_cleanup:
            self._start_auto_cleanup()
        
        self.logger.info(
            f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.step_name}, {self.device}, "
            f"{self.memory_limit_gb}GB, M3 Max: {self.is_m3_max}"
        )
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                device_idx = 0
                total = torch.cuda.get_device_properties(device_idx).total_memory
                allocated = torch.cuda.memory_allocated(device_idx)
                return (total - allocated) / (1024**3)
                
            elif self.device == "mps" and self.is_m3_max:
                # M3 Max Unified Memory ì²˜ë¦¬
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)
                    return min(available_gb, self.memory_limit_gb)
                else:
                    return self.memory_limit_gb * 0.7
                    
            else:
                # CPU ë©”ëª¨ë¦¬
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    return memory.available / (1024**3)
                else:
                    return 8.0
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def allocate_memory(self, item_name: str, size_gb: float) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            try:
                available = self.get_available_memory()
                
                if available >= size_gb:
                    self.allocated_memory[item_name] = size_gb
                    self.total_allocations += 1
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    current_total = sum(self.allocated_memory.values())
                    self.peak_usage = max(self.peak_usage, current_total)
                    
                    self.logger.info(f"âœ… {item_name}: {size_gb:.1f}GB í• ë‹¹ë¨")
                    return True
                else:
                    self.logger.warning(
                        f"âš ï¸ {item_name}: {size_gb:.1f}GB í• ë‹¹ ì‹¤íŒ¨ "
                        f"(ì‚¬ìš© ê°€ëŠ¥: {available:.1f}GB)"
                    )
                    return False
                    
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {e}")
                return False
    
    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        with self._lock:
            try:
                cleanup_stats = {
                    "python_objects_collected": 0,
                    "gpu_cache_cleared": False,
                    "items_deallocated": 0,
                    "memory_freed_gb": 0.0
                }
                
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected = gc.collect()
                cleanup_stats["python_objects_collected"] = collected
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if TORCH_AVAILABLE:
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        cleanup_stats["gpu_cache_cleared"] = True
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                            if self.is_m3_max and hasattr(torch.mps, 'synchronize'):
                                torch.mps.synchronize()
                            cleanup_stats["gpu_cache_cleared"] = True
                        except Exception as e:
                            self.logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # ê°•ì œ ì •ë¦¬ ì‹œ í• ë‹¹ëœ ë©”ëª¨ë¦¬ í•´ì œ
                if force and self.allocated_memory:
                    freed_memory = sum(self.allocated_memory.values())
                    items_count = len(self.allocated_memory)
                    
                    self.allocated_memory.clear()
                    
                    cleanup_stats.update({
                        "items_deallocated": items_count,
                        "memory_freed_gb": freed_memory
                    })
                
                self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleanup_stats}")
                
                return cleanup_stats
                
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ (ì™„ì „ êµ¬í˜„)"""
        with self._lock:
            try:
                return {
                    "step_name": self.step_name,
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "memory_info": {
                        "total_limit_gb": self.memory_limit_gb,
                        "available_gb": self.get_available_memory(),
                        "peak_usage_gb": self.peak_usage
                    },
                    "allocation_info": {
                        "allocated_items": self.allocated_memory.copy(),
                        "total_allocated_gb": sum(self.allocated_memory.values()),
                        "active_items": len(self.allocated_memory)
                    },
                    "statistics": {
                        "total_allocations": self.total_allocations,
                        "total_deallocations": self.total_deallocations,
                        "cleanup_threshold": self.cleanup_threshold,
                        "auto_cleanup": self.auto_cleanup
                    }
                }
            except Exception as e:
                self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
    
    def _start_auto_cleanup(self):
        """ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def cleanup_worker():
            while self.auto_cleanup:
                try:
                    time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                    usage_percent = sum(self.allocated_memory.values()) / self.memory_limit_gb
                    if usage_percent > self.cleanup_threshold:
                        self.cleanup_memory()
                except Exception as e:
                    self.logger.debug(f"ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()

# ==============================================
# ğŸ”¥ ë°ì´í„° ë³€í™˜ê¸° (ì™„ì „ êµ¬í˜„)
# ==============================================

class StepDataConverter:
    """
    ğŸ“Š Stepë³„ ë°ì´í„° ë³€í™˜ê¸° (ì™„ì „ êµ¬í˜„)
    âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬
    âœ… í…ì„œ ë³€í™˜ ë° ìµœì í™”
    âœ… Stepë³„ íŠ¹í™” ì²˜ë¦¬
    âœ… M3 Max GPU ìµœì í™”
    """
    
    def __init__(self, step_name: str = None, **kwargs):
        self.step_name = step_name
        self.device = SYSTEM_INFO["device"]
        self.logger = logging.getLogger(f"{__name__}.StepDataConverter")
        
        # Stepë³„ ì„¤ì •
        self.step_configs = {
            "HumanParsingStep": {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            },
            "PoseEstimationStep": {
                "input_size": (368, 368),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            },
            "ClothSegmentationStep": {
                "input_size": (320, 320),
                "normalize": True,
                "mean": [0.5, 0.5, 0.5],
                "std": [0.5, 0.5, 0.5],
                "channels": 3
            },
            "GeometricMatchingStep": {
                "input_size": (256, 192),
                "normalize": False,
                "channels": 3
            },
            "ClothWarpingStep": {
                "input_size": (256, 192),
                "normalize": False,
                "channels": 3
            },
            "VirtualFittingStep": {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.5, 0.5, 0.5],
                "std": [0.5, 0.5, 0.5],
                "channels": 3
            },
            "PostProcessingStep": {
                "input_size": (512, 512),
                "normalize": False,
                "channels": 3
            },
            "QualityAssessmentStep": {
                "input_size": (224, 224),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "channels": 3
            }
        }
        
        self.config = self.step_configs.get(step_name, {
            "input_size": (512, 512),
            "normalize": True,
            "mean": [0.485, 0.456, 0.406],
            "std": [0.229, 0.224, 0.225],
            "channels": 3
        })
        
        self.logger.info(f"ğŸ“Š {step_name} ë°ì´í„° ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def configure_for_step(self, step_name: str):
        """Stepë³„ ì„¤ì • ì ìš©"""
        self.step_name = step_name
        self.config = self.step_configs.get(step_name, self.config)
        self.logger.debug(f"ğŸ“ {step_name} ì„¤ì • ì ìš©")
    
    def preprocess_image(self, image, target_size=None, **kwargs):
        """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            target_size = target_size or self.config["input_size"]
            
            # PIL Image ì²˜ë¦¬
            if hasattr(image, 'resize'):
                # RGB ë³€í™˜
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # í¬ê¸° ì¡°ì • (ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§) - PIL ë²„ì „ í˜¸í™˜
                if PIL_AVAILABLE:
                    # PIL 10.0.0+ ì—ì„œëŠ” Image.LANCZOS ëŒ€ì‹  Image.Resampling.LANCZOS ì‚¬ìš©
                    try:
                        if hasattr(Image, 'Resampling') and hasattr(Image.Resampling, 'LANCZOS'):
                            image = image.resize(target_size, Image.Resampling.LANCZOS)
                        elif hasattr(Image, 'LANCZOS'):
                            image = image.resize(target_size, Image.LANCZOS)
                        else:
                            image = image.resize(target_size)
                    except Exception:
                        image = image.resize(target_size)  # ì•ˆì „í•œ í´ë°±
                else:
                    image = image.resize(target_size)
            
            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            if NUMPY_AVAILABLE:
                image_array = np.array(image, dtype=np.float32)
                
                # ì •ê·œí™”
                if self.config.get("normalize", True):
                    image_array = image_array / 255.0
                    
                    # í‘œì¤€í™” (ì„ íƒì )
                    if "mean" in self.config and "std" in self.config:
                        mean = np.array(self.config["mean"])
                        std = np.array(self.config["std"])
                        image_array = (image_array - mean) / std
                
                # HWC -> CHW ë³€í™˜ (PyTorch í˜•ì‹)
                if len(image_array.shape) == 3:
                    image_array = image_array.transpose(2, 0, 1)
                
                return image_array
            
            return image
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def to_tensor(self, data):
        """í…ì„œ ë³€í™˜ (PyTorch ì§€ì›)"""
        try:
            if TORCH_AVAILABLE and NUMPY_AVAILABLE:
                if isinstance(data, np.ndarray):
                    tensor = torch.from_numpy(data)
                    
                    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device != "cpu":
                        tensor = tensor.to(self.device)
                    
                    return tensor
            
            return data
            
        except Exception as e:
            self.logger.warning(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return data
    
    def postprocess_result(self, result, output_format="image"):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            if output_format == "image":
                # í…ì„œì—ì„œ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if TORCH_AVAILABLE and torch.is_tensor(result):
                    # GPUì—ì„œ CPUë¡œ ì´ë™
                    result = result.detach().cpu()
                    
                    # NumPyë¡œ ë³€í™˜
                    if NUMPY_AVAILABLE:
                        result = result.numpy()
                
                # NumPy ë°°ì—´ ì²˜ë¦¬
                if NUMPY_AVAILABLE and isinstance(result, np.ndarray):
                    # CHW -> HWC ë³€í™˜
                    if len(result.shape) == 3 and result.shape[0] in [1, 3, 4]:
                        result = result.transpose(1, 2, 0)
                    
                    # ì •ê·œí™” í•´ì œ
                    if result.max() <= 1.0:
                        result = (result * 255).astype(np.uint8)
                    
                    # PIL Imageë¡œ ë³€í™˜
                    if PIL_AVAILABLE:
                        if len(result.shape) == 3:
                            result = Image.fromarray(result)
                        elif len(result.shape) == 2:
                            result = Image.fromarray(result, mode='L')
            
            return result
            
        except Exception as e:
            self.logger.warning(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return result

# ==============================================
# ğŸ”¥ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class StepModelInterface:
    """
    ğŸ”— Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
    âœ… main.py ì™„ì „ í˜¸í™˜
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    âœ… ëª¨ë¸ ìºì‹± ìµœì í™”
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    """
    
    def __init__(
        self, 
        step_name: str, 
        model_loader_instance: Optional[Any] = None,
        config: Optional[StepConfig] = None
    ):
        self.step_name = step_name
        self.model_loader = model_loader_instance
        self.config = config or StepConfig(step_name=step_name)
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"interface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self._models_cache: Dict[str, Any] = {}
        self._model_metadata: Dict[str, ModelInfo] = {}
        
        # ìƒíƒœ ê´€ë¦¬
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # Stepë³„ ëª¨ë¸ ë§¤í•‘ (ì™„ì „ êµ¬í˜„)
        self._initialize_model_mappings()
        
        self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_model_mappings(self):
        """Stepë³„ ëª¨ë¸ ë§¤í•‘ ì´ˆê¸°í™”"""
        self._model_mappings = {
            "HumanParsingStep": {
                "default_models": ["graphonomy", "human_parsing_atr", "parsing_lip", "schp"],
                "model_types": ["segmentation", "parsing"],
                "input_sizes": [(512, 512), (473, 473)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "PoseEstimationStep": {
                "default_models": ["openpose", "mediapipe", "yolov8_pose", "movenet"],
                "model_types": ["pose_estimation", "keypoint_detection"],
                "input_sizes": [(368, 368), (256, 256), (192, 256)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "ClothSegmentationStep": {
                "default_models": ["u2net", "cloth_segmentation", "deeplabv3", "bisenet"],
                "model_types": ["segmentation", "cloth_parsing"],
                "input_sizes": [(320, 320), (512, 512)],
                "supported_formats": [".pth", ".pt", ".ckpt"]
            },
            "GeometricMatchingStep": {
                "default_models": ["geometric_matching", "tps_transformation", "spatial_transformer"],
                "model_types": ["transformation", "matching"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "ClothWarpingStep": {
                "default_models": ["cloth_warping", "spatial_transformer", "thin_plate_spline"],
                "model_types": ["warping", "transformation"],
                "input_sizes": [(256, 192), (512, 384)],
                "supported_formats": [".pth", ".pt"]
            },
            "VirtualFittingStep": {
                "default_models": ["ootdiffusion", "stable_diffusion", "virtual_tryon", "diffusion_tryon"],
                "model_types": ["diffusion", "generation", "virtual_fitting"],
                "input_sizes": [(512, 512), (768, 768)],
                "supported_formats": [".pth", ".pt", ".safetensors", ".ckpt"]
            },
            "PostProcessingStep": {
                "default_models": ["post_processing", "image_enhancement", "artifact_removal", "super_resolution"],
                "model_types": ["enhancement", "post_processing"],
                "input_sizes": [(512, 512), (1024, 1024)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            },
            "QualityAssessmentStep": {
                "default_models": ["clipiqa", "quality_assessment", "brisque", "niqe"],
                "model_types": ["quality_assessment", "metric"],
                "input_sizes": [(224, 224), (512, 512)],
                "supported_formats": [".pth", ".pt", ".onnx"]
            }
        }
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """
        ğŸ”¥ ëª¨ë¸ ë¡œë“œ (main.py í•µì‹¬ ë©”ì„œë“œ - ì™„ì „ êµ¬í˜„)
        """
        start_time = time.time()
        
        with self._lock:
            self._request_count += 1
        
        try:
            # ëª¨ë¸ëª… ê²°ì •
            target_model = model_name or self.config.model_name or self._get_default_model()
            
            if not target_model:
                self.logger.warning(f"âš ï¸ {self.step_name}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                self._error_count += 1
                return None
            
            # ìºì‹œ í™•ì¸
            if target_model in self._models_cache:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {target_model}")
                self._success_count += 1
                return self._models_cache[target_model]
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            model = None
            
            # 1. ModelLoaderë¥¼ í†µí•œ ë¡œë“œ
            if self.model_loader:
                model = await self._load_via_model_loader(target_model)
            
            # 2. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ (í´ë°±)
            if model is None:
                model = self._create_simulation_model(target_model)
                self.logger.warning(f"âš ï¸ {target_model} ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì‚¬ìš©")
            
            # ìºì‹œ ì €ì¥
            if model:
                self._models_cache[target_model] = model
                self._success_count += 1
                
                load_time = time.time() - start_time
                
                self.logger.info(
                    f"âœ… {target_model} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}s)"
                )
                
                return model
            else:
                self._error_count += 1
                self.logger.error(f"âŒ {target_model} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def _load_via_model_loader(self, model_name: str) -> Optional[Any]:
        """ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not hasattr(self.model_loader, 'get_model'):
                return None
            
            # ë¹„ë™ê¸°/ë™ê¸° í˜¸í™˜ ì²˜ë¦¬
            if asyncio.iscoroutinefunction(self.model_loader.get_model):
                model = await self.model_loader.get_model(model_name)
            else:
                model = self.model_loader.get_model(model_name)
            
            if model:
                self.logger.debug(f"âœ… ModelLoaderë¡œ {model_name} ë¡œë“œ ì„±ê³µ")
                return model
            
        except Exception as e:
            self.logger.debug(f"ModelLoader ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _create_simulation_model(self, model_name: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        return {
            "name": model_name,
            "type": "simulation",
            "step_name": self.step_name,
            "step_number": self.config.step_number,
            "device": SYSTEM_INFO["device"],
            "precision": SYSTEM_INFO.get("recommended_precision", "fp32"),
            "created_at": time.time(),
            "simulate": True,
            "capabilities": self._model_mappings.get(self.step_name, {}).get("model_types", [])
        }
    
    def _get_default_model(self) -> Optional[str]:
        """ê¸°ë³¸ ëª¨ë¸ëª… ë°˜í™˜"""
        mapping = self._model_mappings.get(self.step_name)
        if mapping and mapping["default_models"]:
            return mapping["default_models"][0]
        return None
    
    def list_available_models(self) -> List[str]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (main.py í•µì‹¬ ë©”ì„œë“œ - ì™„ì „ êµ¬í˜„)
        """
        try:
            available_models = set()
            
            # 1. Stepë³„ ê¸°ë³¸ ëª¨ë¸ë“¤
            mapping = self._model_mappings.get(self.step_name, {})
            default_models = mapping.get("default_models", [])
            available_models.update(default_models)
            
            # 2. ModelLoader ëª¨ë¸ ëª©ë¡
            if self.model_loader and hasattr(self.model_loader, 'list_models'):
                try:
                    loader_models = self.model_loader.list_models(self.step_name)
                    if loader_models:
                        available_models.update(loader_models)
                except Exception as e:
                    self.logger.debug(f"ModelLoader ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 3. ìºì‹œëœ ëª¨ë¸ë“¤
            available_models.update(self._models_cache.keys())
            
            # ì •ë ¬ ë° ë°˜í™˜
            result = sorted(list(available_models))
            
            self.logger.info(
                f"ğŸ“‹ {self.step_name} ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(result)}ê°œ "
                f"({', '.join(result[:3])}{'...' if len(result) > 3 else ''})"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë¸ë§Œ ë°˜í™˜
            mapping = self._model_mappings.get(self.step_name, {})
            return mapping.get("default_models", [])
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ í†µê³„ (ì™„ì „ êµ¬í˜„)"""
        with self._lock:
            total_requests = self._request_count
            success_rate = (
                (self._success_count / max(total_requests, 1)) * 100
                if total_requests > 0 else 0.0
            )
            
            return {
                "step_name": self.step_name,
                "step_number": self.config.step_number,
                "request_statistics": {
                    "total_requests": total_requests,
                    "successful_loads": self._success_count,
                    "failed_loads": self._error_count,
                    "success_rate_percent": round(success_rate, 1)
                },
                "cache_info": {
                    "cached_models": len(self._models_cache),
                    "cached_metadata": len(self._model_metadata),
                    "cached_model_names": list(self._models_cache.keys())
                },
                "configuration": {
                    "has_model_loader": self.model_loader is not None,
                    "device": self.config.device,
                    "precision": self.config.precision
                },
                "available_models": {
                    "count": len(self.list_available_models()),
                    "default_model": self._get_default_model()
                }
            }

# ==============================================
# ğŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (ì™„ì „ êµ¬í˜„)
# ==============================================

class UnifiedUtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € v8.0 (ì™„ì „ êµ¬í˜„)
    âœ… conda í™˜ê²½ 100% ìµœì í™”
    âœ… M3 Max 128GB ì™„ì „ í™œìš©
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
    âœ… Clean Architecture ì ìš©
    âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.system_config = SystemConfig()
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ì¥ì†Œ (ì•½í•œ ì°¸ì¡°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces: Dict[str, StepModelInterface] = {}
        self._memory_managers: Dict[str, StepMemoryManager] = {}
        self._data_converters: Dict[str, StepDataConverter] = {}
        
        # ì „ì—­ ì»´í¬ë„ŒíŠ¸ë“¤
        self.global_memory_manager = StepMemoryManager(
            step_name="global",
            device=self.system_config.device,
            memory_limit_gb=self.system_config.memory_limit_gb,
            auto_cleanup=True
        )
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0,
            "conda_optimizations": 0,
            "m3_max_optimizations": 0,
            "startup_time": 0.0
        }
        
        # ìŠ¤ë ˆë“œ í’€
        self._thread_pool = ThreadPoolExecutor(
            max_workers=self.system_config.max_workers,
            thread_name_prefix="utils_worker"
        )
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        
        # conda í™˜ê²½ ìµœì í™”
        if SYSTEM_INFO["in_conda"]:
            self._setup_conda_optimizations()
        
        # M3 Max íŠ¹ë³„ ìµœì í™”
        if SYSTEM_INFO["is_m3_max"]:
            self._setup_m3_max_optimizations()
        
        self._initialized = True
        self.logger.info(
            f"ğŸ¯ UnifiedUtilsManager v8.0 ì´ˆê¸°í™” ì™„ë£Œ "
            f"(conda: {SYSTEM_INFO['in_conda']}, M3: {SYSTEM_INFO['is_m3_max']})"
        )
    
    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            start_time = time.time()
            
            # PyTorch ìµœì í™”
            if TORCH_AVAILABLE:
                # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
                torch.set_num_threads(self.system_config.max_workers)
                
                # ì¸í„°ì˜µ ë³‘ë ¬ì„± ì„¤ì •
                torch.set_num_interop_threads(min(4, self.system_config.max_workers))
                
                # MPS ìµœì í™” (M3 Max)
                if SYSTEM_INFO["is_m3_max"]:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                    })
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                # OpenBLAS/MKL ìŠ¤ë ˆë“œ ì„¤ì •
                os.environ.update({
                    'OMP_NUM_THREADS': str(self.system_config.max_workers),
                    'MKL_NUM_THREADS': str(self.system_config.max_workers),
                    'OPENBLAS_NUM_THREADS': str(self.system_config.max_workers),
                    'NUMEXPR_NUM_THREADS': str(self.system_config.max_workers)
                })
            
            optimization_time = time.time() - start_time
            self.stats["conda_optimizations"] += 1
            
            self.logger.info(
                f"âœ… conda í™˜ê²½ ìµœì í™” ì™„ë£Œ ({optimization_time:.3f}s) - "
                f"ì›Œì»¤: {self.system_config.max_workers}, í™˜ê²½: {SYSTEM_INFO['conda_env']}"
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_m3_max_optimizations(self):
        """M3 Max íŠ¹ë³„ ìµœì í™”"""
        try:
            start_time = time.time()
            
            # ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”
            self.system_config.memory_limit_gb = min(100.0, SYSTEM_INFO["memory_gb"] * 0.8)
            self.system_config.max_batch_size = 32  # M3 MaxëŠ” í° ë°°ì¹˜ í—ˆìš©
            self.system_config.optimization_level = "maximum"
            
            if TORCH_AVAILABLE:
                # M3 Max MPS ë°±ì—”ë“œ ìµœì í™”
                if torch.backends.mps.is_available():
                    try:
                        # Metal Performance Shaders ìµœì í™”
                        if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                            torch.mps.set_per_process_memory_fraction(0.8)
                        
                        # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜
                        os.environ.update({
                            'PYTORCH_MPS_ALLOCATOR_POLICY': 'native',
                            'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.9'
                        })
                        
                    except Exception as e:
                        self.logger.debug(f"MPS ì„¸ë¶€ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            optimization_time = time.time() - start_time
            self.stats["m3_max_optimizations"] += 1
            
            self.logger.info(
                f"ğŸ M3 Max íŠ¹ë³„ ìµœì í™” ì™„ë£Œ ({optimization_time:.3f}s) - "
                f"ë©”ëª¨ë¦¬: {self.system_config.memory_limit_gb}GB, "
                f"ë°°ì¹˜: {self.system_config.max_batch_size}"
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)"""
        try:
            # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            step_config = StepConfig(step_name=step_name)
            interface = StepModelInterface(
                step_name=step_name,
                model_loader_instance=getattr(self, 'model_loader', None),
                config=step_config
            )
            
            # ìºì‹œ ì €ì¥
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(step_name, None)
    
    def create_step_memory_manager(self, step_name: str, **options) -> StepMemoryManager:
        """Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
        try:
            # ê¸°ì¡´ ê´€ë¦¬ì ë°˜í™˜
            if step_name in self._memory_managers:
                return self._memory_managers[step_name]
            
            # ìƒˆ ê´€ë¦¬ì ìƒì„±
            manager = StepMemoryManager(step_name=step_name, **options)
            self._memory_managers[step_name] = manager
            
            self.logger.info(f"ğŸ§  {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì™„ë£Œ")
            return manager
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì‹¤íŒ¨: {e}")
            return self.global_memory_manager
    
    def create_step_data_converter(self, step_name: str, **options) -> StepDataConverter:
        """Stepë³„ ë°ì´í„° ë³€í™˜ê¸° ìƒì„±"""
        try:
            # ê¸°ì¡´ ë³€í™˜ê¸° ë°˜í™˜
            if step_name in self._data_converters:
                return self._data_converters[step_name]
            
            # ìƒˆ ë³€í™˜ê¸° ìƒì„±
            converter = StepDataConverter(step_name, **options)
            self._data_converters[step_name] = converter
            
            self.logger.info(f"ğŸ“Š {step_name} ë°ì´í„° ë³€í™˜ê¸° ìƒì„± ì™„ë£Œ")
            return converter
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë°ì´í„° ë³€í™˜ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return StepDataConverter(step_name)
    
    def get_memory_manager(self) -> StepMemoryManager:
        """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜"""
        return self.global_memory_manager
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ì™„ì „ êµ¬í˜„)"""
        try:
            start_time = time.time()
            self.logger.info("ğŸ§¹ ì „ì—­ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘...")
            
            optimization_results = {
                "global_cleanup": {},
                "step_managers": {},
                "model_interfaces": {},
                "total_freed_gb": 0.0,
                "optimization_time": 0.0
            }
            
            # 1. ì „ì—­ ë©”ëª¨ë¦¬ ì •ë¦¬
            global_result = self.global_memory_manager.cleanup_memory(force=True)
            optimization_results["global_cleanup"] = global_result
            
            # 2. Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬
            for step_name, manager in self._memory_managers.items():
                try:
                    step_result = manager.cleanup_memory()
                    optimization_results["step_managers"][step_name] = step_result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 3. Python ì „ì—­ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = gc.collect()
            
            # 4. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                device = SYSTEM_INFO["device"]
                if device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                elif device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if SYSTEM_INFO["is_m3_max"] and hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as e:
                        self.logger.debug(f"MPS ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 5. ì•½í•œ ì°¸ì¡° ì •ë¦¬
            self._step_interfaces.clear()
            
            optimization_time = time.time() - start_time
            optimization_results["optimization_time"] = optimization_time
            optimization_results["collected_objects"] = collected_objects
            
            self.stats["memory_optimizations"] += 1
            
            self.logger.info(
                f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ ({optimization_time:.2f}s) - "
                f"ê°ì²´ ì •ë¦¬: {collected_objects}ê°œ"
            )
            
            return {
                "success": True,
                "results": optimization_results,
                "memory_info": self.global_memory_manager.get_memory_stats()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ (ì™„ì „ êµ¬í˜„)"""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "used_gb": round((vm.total - vm.available) / (1024**3), 1),
                    "percent_used": round(vm.percent, 1)
                }
            
            # ì„±ëŠ¥ í†µê³„
            interface_stats = {}
            for step_name, interface in self._model_interfaces.items():
                interface_stats[step_name] = interface.get_stats()
            
            return {
                "system": {
                    "initialized": self.is_initialized,
                    "initialization_time": self.initialization_time,
                    "uptime": time.time() - (self.initialization_time or time.time()) if self.initialization_time else 0
                },
                "configuration": asdict(self.system_config),
                "environment": {
                    "system_info": SYSTEM_INFO,
                    "conda_optimized": SYSTEM_INFO["in_conda"],
                    "m3_max_optimized": SYSTEM_INFO["is_m3_max"],
                    "device": SYSTEM_INFO["device"],
                    "libraries": SYSTEM_INFO.get("libraries", {})
                },
                "memory": {
                    "system_memory": memory_info,
                    "global_manager": self.global_memory_manager.get_memory_stats(),
                    "step_managers": {
                        name: manager.get_memory_stats() 
                        for name, manager in self._memory_managers.items()
                    }
                },
                "components": {
                    "step_interfaces": len(self._step_interfaces),
                    "model_interfaces": len(self._model_interfaces),
                    "memory_managers": len(self._memory_managers),
                    "data_converters": len(self._data_converters)
                },
                "statistics": {
                    **self.stats,
                    "interface_stats": interface_stats
                }
            }
            
        except Exception as e:
            self.logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "status": "error"}

# ==============================================
# ğŸ”¥ í†µí•© Step ì¸í„°í˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class UnifiedStepInterface:
    """
    ğŸ”— í†µí•© Step ì¸í„°í˜ì´ìŠ¤ (ì™„ì „ êµ¬í˜„)
    âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
    âœ… conda í™˜ê²½ ìµœì í™”
    âœ… M3 Max íŠ¹í™” ì²˜ë¦¬
    âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    """
    
    def __init__(
        self, 
        manager: UnifiedUtilsManager, 
        config: StepConfig, 
        is_fallback: bool = False
    ):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        # ë¡œê¹…
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # ìƒíƒœ ê´€ë¦¬
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        self._last_request_time = None
        self._total_processing_time = 0.0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        if self.is_fallback:
            self.logger.warning(f"âš ï¸ {config.step_name} í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨")
        else:
            self.logger.info(f"ğŸ”— {config.step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            if self.is_fallback:
                return self._create_fallback_model(model_name)
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ë¡œë“œ
            model_interface = self.manager.create_step_model_interface(self.config.step_name)
            model = await model_interface.get_model(model_name)
            
            if model:
                with self._lock:
                    self._success_count += 1
                return model
            else:
                with self._lock:
                    self._error_count += 1
                return self._create_fallback_model(model_name)
                
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model(model_name)
    
    def _create_fallback_model(self, model_name: Optional[str]) -> Dict[str, Any]:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        return {
            "name": model_name or "fallback_model",
            "type": "fallback",
            "step_name": self.config.step_name,
            "step_number": self.config.step_number,
            "simulation": True,
            "created_at": time.time()
        }
    
    async def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (Stepë³„ íŠ¹í™” ì²˜ë¦¬)"""
        start_time = time.time()
        
        try:
            with self._lock:
                self._request_count += 1
                self._last_request_time = time.time()
            
            # Stepë³„ íŠ¹í™” ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            step_number = self.config.step_number
            processing_time = {
                1: 0.2,  # Human Parsing
                2: 0.15, # Pose Estimation
                3: 0.25, # Cloth Segmentation
                4: 0.3,  # Geometric Matching
                5: 0.35, # Cloth Warping
                6: 0.8,  # Virtual Fitting (ê°€ì¥ ë¬´ê±°ì›€)
                7: 0.2,  # Post Processing
                8: 0.1   # Quality Assessment
            }.get(step_number, 0.1)
            
            await asyncio.sleep(processing_time)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            
            result = {
                "success": True,
                "step_name": self.config.step_name,
                "step_number": step_number,
                "processing_time": processing_time,
                "output_type": f"step_{step_number:02d}_result",
                "confidence": 0.9,
                "device": self.config.device,
                "is_simulation": True
            }
            
            with self._lock:
                self._total_processing_time += processing_time
                self._success_count += 1
            
            return result
            
        except Exception as e:
            with self._lock:
                self._error_count += 1
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "step_name": self.config.step_name,
                "is_fallback": True
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        with self._lock:
            total_requests = self._request_count
            success_rate = (
                (self._success_count / max(total_requests, 1)) * 100
                if total_requests > 0 else 0.0
            )
            avg_processing_time = (
                self._total_processing_time / max(total_requests, 1)
                if total_requests > 0 else 0.0
            )
            
            return {
                "step_info": {
                    "step_name": self.config.step_name,
                    "step_number": self.config.step_number,
                    "is_fallback": self.is_fallback
                },
                "performance": {
                    "total_requests": total_requests,
                    "successful_requests": self._success_count,
                    "failed_requests": self._error_count,
                    "success_rate_percent": round(success_rate, 1),
                    "average_processing_time": round(avg_processing_time, 3),
                    "total_processing_time": round(self._total_processing_time, 2)
                },
                "configuration": {
                    "device": self.config.device,
                    "precision": self.config.precision,
                    "input_size": self.config.input_size,
                    "batch_size": self.config.batch_size,
                    "model_name": self.config.model_name
                },
                "status": {
                    "last_request_time": self._last_request_time,
                    "operational": total_requests > 0 and success_rate > 50,
                    "health_score": min(10, success_rate / 10) if total_requests > 0 else 5
                }
            }

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    ğŸ”¥ main.py í•µì‹¬ í•¨ìˆ˜ (ì™„ì „ êµ¬í˜„)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
    âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
    """
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ í™•ë³´
        if model_loader_instance is None:
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"âœ… ì „ì—­ ModelLoader ì—°ë™: {step_name}")
            except (ImportError, ModuleNotFoundError):
                logger.info(f"â„¹ï¸ ModelLoader ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ë¡œë” ì‚¬ìš©: {step_name}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
                model_loader_instance = None
        
        # UnifiedUtilsManagerë¥¼ í†µí•œ ìƒì„±
        try:
            manager = get_utils_manager()
            interface = manager.create_step_model_interface(step_name)
            logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Manager)")
            return interface
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        step_config = StepConfig(step_name=step_name)
        interface = StepModelInterface(
            step_name=step_name,
            model_loader_instance=model_loader_instance,
            config=step_config
        )
        
        logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (Direct)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì¢… í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

def get_step_memory_manager(step_name: str = None, **kwargs) -> StepMemoryManager:
    """
    ğŸ”¥ main.py í•µì‹¬ í•¨ìˆ˜ (ì™„ì „ êµ¬í˜„)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # UnifiedUtilsManagerë¥¼ í†µí•œ ì¡°íšŒ
        try:
            manager = get_utils_manager()
            if step_name:
                memory_manager = manager.create_step_memory_manager(step_name, **kwargs)
            else:
                memory_manager = manager.get_memory_manager()
            
            logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜ (Manager): {step_name or 'global'}")
            return memory_manager
            
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        memory_manager = StepMemoryManager(
            step_name=step_name or "fallback", 
            **kwargs
        )
        logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì§ì ‘ ìƒì„±: {step_name or 'global'}")
        return memory_manager
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì¢… í´ë°±
        return StepMemoryManager(step_name=step_name or "error", **kwargs)

def get_step_data_converter(step_name: str = None, **kwargs) -> StepDataConverter:
    """
    ğŸ”¥ Stepë³„ ë°ì´í„° ë³€í™˜ê¸° ë°˜í™˜ (main.py í˜¸í™˜)
    âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬, í›„ì²˜ë¦¬
    âœ… í…ì„œ ë³€í™˜ ë° ìµœì í™”
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    try:
        # UnifiedUtilsManagerë¥¼ í†µí•œ ì¡°íšŒ
        try:
            manager = get_utils_manager()
            converter = manager.create_step_data_converter(step_name or "default", **kwargs)
            logger.info(f"ğŸ“Š ë°ì´í„° ë³€í™˜ê¸° ë°˜í™˜ (Manager): {step_name or 'global'}")
            return converter
            
        except Exception as e:
            logger.warning(f"âš ï¸ Managerë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜ê¸° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ì§ì ‘ ìƒì„± (í´ë°±)
        converter = StepDataConverter(step_name, **kwargs)
        logger.info(f"ğŸ“Š ë°ì´í„° ë³€í™˜ê¸° ì§ì ‘ ìƒì„±: {step_name or 'global'}")
        return converter
            
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë³€í™˜ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
        return StepDataConverter(step_name, **kwargs)

def preprocess_image_for_step(image_data, step_name: str, **kwargs) -> Any:
    """
    ğŸ”¥ Stepë³„ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (main.py í˜¸í™˜)
    âœ… Stepë³„ íŠ¹í™” ì „ì²˜ë¦¬
    âœ… í¬ê¸° ì¡°ì •, ì •ê·œí™”
    âœ… í…ì„œ ë³€í™˜
    """
    try:
        # ë°ì´í„° ë³€í™˜ê¸° ê°€ì ¸ì˜¤ê¸°
        converter = get_step_data_converter(step_name)
        
        # Stepë³„ ì„¤ì • ì ìš©
        converter.configure_for_step(step_name)
        
        # ì „ì²˜ë¦¬ ìˆ˜í–‰
        processed_image = converter.preprocess_image(image_data, **kwargs)
        
        logger.debug(f"âœ… {step_name} ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ")
        return processed_image
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return image_data

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¶Œì¥)"""
    try:
        manager = get_utils_manager()
        step_config = StepConfig(step_name=step_name, **options)
        return UnifiedStepInterface(manager, step_config)
    except Exception as e:
        logger.error(f"í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        step_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(get_utils_manager(), step_config, is_fallback=True)

# ==============================================
# ğŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” (main.py ì§„ì…ì )"""
    try:
        manager = get_utils_manager()
        
        # conda í™˜ê²½ íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["in_conda"]:
            kwargs.setdefault("conda_optimized", True)
            kwargs.setdefault("precision", "fp16" if SYSTEM_INFO["is_m3_max"] else "fp32")
            kwargs.setdefault("optimization_level", "maximum" if SYSTEM_INFO["is_m3_max"] else "high")
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        manager.is_initialized = True
        manager.initialization_time = time.time()
        
        return {
            "success": True,
            "initialization_time": manager.initialization_time,
            "system_config": asdict(manager.system_config),
            "system_info": SYSTEM_INFO,
            "conda_optimized": SYSTEM_INFO["in_conda"],
            "m3_max_optimized": SYSTEM_INFO["is_m3_max"]
        }
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {
            "error": str(e), 
            "system_info": SYSTEM_INFO,
            "fallback_status": True
        }

async def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™”"""
    try:
        manager = get_utils_manager()
        return await manager.optimize_memory()
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def get_ai_models_path() -> Path:
    """AI ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return Path(SYSTEM_INFO["ai_models_path"])

def list_available_steps() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ Step ëª©ë¡ (8ë‹¨ê³„ ì™„ì „ ì§€ì›)"""
    return [step.value for step in StepType]

def is_conda_environment() -> bool:
    """conda í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["in_conda"]

def is_m3_max_device() -> bool:
    """M3 Max ë””ë°”ì´ìŠ¤ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO["is_m3_max"]

def get_conda_info() -> Dict[str, Any]:
    """conda í™˜ê²½ ì •ë³´"""
    return {
        "in_conda": SYSTEM_INFO["in_conda"],
        "conda_env": SYSTEM_INFO["conda_env"],
        "conda_prefix": SYSTEM_INFO["conda_prefix"],
        "python_path": SYSTEM_INFO["python_path"]
    }

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´"""
    device_keys = [
        "device", "device_name", "device_available", "device_memory_gb",
        "device_capabilities", "recommended_precision", "optimization_level"
    ]
    return {key: SYSTEM_INFO.get(key) for key in device_keys if key in SYSTEM_INFO}

def validate_step_name(step_name: str) -> bool:
    """Step ì´ë¦„ ìœ íš¨ì„± ê²€ì¦"""
    valid_steps = [step.value for step in StepType]
    return step_name in valid_steps

def get_step_number(step_name: str) -> int:
    """Step ë²ˆí˜¸ ë°˜í™˜"""
    for step_type in StepType:
        if step_type.value == step_name:
            step_config = StepConfig(step_name=step_name)
            return step_config.step_number or 0
    return 0

def check_system_requirements() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬"""
    requirements = {
        "python_version": {
            "required": "3.8+",
            "current": SYSTEM_INFO["python_version"],
            "satisfied": tuple(map(int, SYSTEM_INFO["python_version"].split('.'))) >= (3, 8)
        },
        "memory": {
            "required_gb": 8.0,
            "current_gb": SYSTEM_INFO["memory_gb"],
            "satisfied": SYSTEM_INFO["memory_gb"] >= 8.0
        },
        "device": {
            "required": "CPU/GPU",
            "current": SYSTEM_INFO["device"],
            "satisfied": True  # CPUëŠ” í•­ìƒ ì§€ì›
        },
        "libraries": {
            "torch": {"available": TORCH_AVAILABLE, "version": TORCH_VERSION},
            "numpy": {"available": NUMPY_AVAILABLE, "version": NUMPY_VERSION if NUMPY_AVAILABLE else None},
            "psutil": {"available": PSUTIL_AVAILABLE},
            "pillow": {"available": PIL_AVAILABLE}
        }
    }
    
    # ì „ì²´ ë§Œì¡±ë„ ê³„ì‚°
    core_satisfied = all([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"],
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE
    ])
    
    requirements["overall_satisfied"] = core_satisfied
    requirements["score"] = sum([
        requirements["python_version"]["satisfied"],
        requirements["memory"]["satisfied"], 
        requirements["device"]["satisfied"],
        TORCH_AVAILABLE,
        NUMPY_AVAILABLE,
        PSUTIL_AVAILABLE,
        PIL_AVAILABLE
    ]) / 7 * 100
    
    return requirements

# ==============================================
# ğŸ”¥ ê°œë°œ/ë””ë²„ê·¸ í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

def debug_system_info(detailed: bool = False):
    """ì‹œìŠ¤í…œ ì •ë³´ ë””ë²„ê·¸ ì¶œë ¥"""
    print("\n" + "="*70)
    print("ğŸ” MyCloset AI ì‹œìŠ¤í…œ ì •ë³´ (v8.0)")
    print("="*70)
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
    print(f"í”Œë«í¼: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
    print(f"Python: {SYSTEM_INFO['python_version']} ({SYSTEM_INFO['python_path']})")
    print(f"CPU ì½”ì–´: {SYSTEM_INFO['cpu_count']}ê°œ")
    print(f"ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
    
    # Apple Silicon ì •ë³´
    if SYSTEM_INFO["is_m3_max"]:
        m3_info = SYSTEM_INFO.get("m3_info", {})
        print(f"ğŸ Apple Silicon: {m3_info.get('model', 'M3 Max')} ({'âœ… ê°ì§€ë¨' if m3_info.get('detected') else 'âŒ'})")
        if m3_info.get("brand"):
            print(f"   CPU ë¸Œëœë“œ: {m3_info['brand']}")
        if m3_info.get("gpu_cores"):
            print(f"   GPU ì½”ì–´: {m3_info['gpu_cores']}ê°œ")
    else:
        print(f"ğŸ Apple Silicon: âŒ")
    
    # GPU/ë””ë°”ì´ìŠ¤ ì •ë³´
    device_info = get_device_info()
    print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {device_info.get('device', 'unknown')}")
    print(f"   ì´ë¦„: {device_info.get('device_name', 'Unknown')}")
    print(f"   ë©”ëª¨ë¦¬: {device_info.get('device_memory_gb', 0)}GB")
    print(f"   ì •ë°€ë„: {device_info.get('recommended_precision', 'fp32')}")
    print(f"   ìµœì í™” ìˆ˜ì¤€: {device_info.get('optimization_level', 'basic')}")
    if device_info.get('device_capabilities'):
        print(f"   ê¸°ëŠ¥: {', '.join(device_info['device_capabilities'])}")
    
    # conda í™˜ê²½ ì •ë³´
    conda_info = get_conda_info()
    print(f"ğŸ conda í™˜ê²½: {'âœ…' if conda_info['in_conda'] else 'âŒ'}")
    if conda_info['in_conda']:
        print(f"   í™˜ê²½ëª…: {conda_info['conda_env']}")
        print(f"   ê²½ë¡œ: {conda_info.get('conda_prefix', 'Unknown')}")
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
    print("ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    libraries = SYSTEM_INFO.get("libraries", {})
    for lib_name, version in libraries.items():
        status = "âœ…" if version != "not_available" else "âŒ"
        print(f"   {lib_name}: {status} ({version})")
    
    # í”„ë¡œì íŠ¸ ê²½ë¡œ
    print("ğŸ“ ê²½ë¡œ ì •ë³´:")
    print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {SYSTEM_INFO['project_root']}")
    print(f"   AI ëª¨ë¸: {SYSTEM_INFO['ai_models_path']}")
    print(f"   ëª¨ë¸ í´ë” ì¡´ì¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")
    
    print("="*70)

def test_step_interface(step_name: str = "HumanParsingStep", detailed: bool = False):
    """Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {step_name} ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±...")
        interface = get_step_model_interface(step_name)
        print(f"   âœ… íƒ€ì…: {type(interface).__name__}")
        
        # 2. ëª¨ë¸ ëª©ë¡ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ...")
        models = interface.list_available_models()
        print(f"   âœ… ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
        
        if models:
            print("   ğŸ“‹ ëª¨ë¸ ëª©ë¡:")
            for i, model in enumerate(models[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                print(f"      {i+1}. {model}")
            if len(models) > 5:
                print(f"      ... ë° {len(models)-5}ê°œ ë”")
        else:
            print("   âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ")
        
        # 3. í†µê³„ í™•ì¸
        print("3ï¸âƒ£ ì¸í„°í˜ì´ìŠ¤ í†µê³„...")
        stats = interface.get_stats()
        print(f"   ğŸ“Š ìš”ì²­ ìˆ˜: {stats['request_statistics']['total_requests']}")
        print(f"   ğŸ“Š ì„±ê³µë¥ : {stats['request_statistics']['success_rate_percent']}%")
        print(f"   ğŸ“Š ìºì‹œëœ ëª¨ë¸: {stats['cache_info']['cached_models']}ê°œ")
        
        test_time = time.time() - start_time
        print(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.3f}ì´ˆ")
        print("âœ… ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_memory_manager(detailed: bool = False):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±
        print("1ï¸âƒ£ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±...")
        memory_manager = get_step_memory_manager()
        print(f"   âœ… íƒ€ì…: {type(memory_manager).__name__}")
        print(f"   ğŸ“Ÿ ë””ë°”ì´ìŠ¤: {memory_manager.device}")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ: {memory_manager.memory_limit_gb}GB")
        
        # 2. ë©”ëª¨ë¦¬ í†µê³„ í™•ì¸
        print("2ï¸âƒ£ ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ...")
        stats = memory_manager.get_memory_stats()
        
        memory_info = stats.get("memory_info", {})
        print(f"   ğŸ“Š ì „ì²´ ë©”ëª¨ë¦¬: {memory_info.get('total_limit_gb', 0)}GB")
        print(f"   ğŸ“Š ì‚¬ìš© ê°€ëŠ¥: {memory_info.get('available_gb', 0):.1f}GB")
        
        allocation_info = stats.get("allocation_info", {})
        print(f"   ğŸ“Š í• ë‹¹ëœ í•­ëª©: {allocation_info.get('active_items', 0)}ê°œ")
        print(f"   ğŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocation_info.get('total_allocated_gb', 0):.1f}GB")
        
        # 3. ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸...")
        
        # í• ë‹¹ í…ŒìŠ¤íŠ¸
        test_item = "TestItem"
        test_size = 1.0  # 1GB
        
        allocation_success = memory_manager.allocate_memory(test_item, test_size)
        print(f"   ë©”ëª¨ë¦¬ í• ë‹¹ ({test_size}GB): {'âœ…' if allocation_success else 'âŒ'}")
        
        if allocation_success:
            # ì •ë¦¬ í…ŒìŠ¤íŠ¸
            cleanup_result = memory_manager.cleanup_memory(force=True)
            print(f"   ë©”ëª¨ë¦¬ ì •ë¦¬: âœ…")
            if isinstance(cleanup_result, dict) and cleanup_result.get("memory_freed_gb"):
                print(f"   í•´ì œëœ ë©”ëª¨ë¦¬: {cleanup_result['memory_freed_gb']}GB")
        
        test_time = time.time() - start_time
        print(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.3f}ì´ˆ")
        print("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_data_converter(step_name: str = "HumanParsingStep"):
    """ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ“Š {step_name} ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ë°ì´í„° ë³€í™˜ê¸° ìƒì„±
        print("1ï¸âƒ£ ë°ì´í„° ë³€í™˜ê¸° ìƒì„±...")
        converter = get_step_data_converter(step_name)
        print(f"   âœ… íƒ€ì…: {type(converter).__name__}")
        print(f"   ğŸ“ Step: {converter.step_name}")
        print(f"   ğŸ¯ ë””ë°”ì´ìŠ¤: {converter.device}")
        
        # 2. ì„¤ì • í™•ì¸
        print("2ï¸âƒ£ Stepë³„ ì„¤ì • í™•ì¸...")
        config = converter.config
        print(f"   ğŸ“ ì…ë ¥ í¬ê¸°: {config.get('input_size', 'Unknown')}")
        print(f"   ğŸ”§ ì •ê·œí™”: {config.get('normalize', False)}")
        print(f"   ğŸ“º ì±„ë„ ìˆ˜: {config.get('channels', 'Unknown')}")
        
        # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
        print("3ï¸âƒ£ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
        if PIL_AVAILABLE:
            try:
                dummy_image = Image.new('RGB', (512, 512), color='red')
                print("   ğŸ–¼ï¸ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                
                # ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
                processed = converter.preprocess_image(dummy_image)
                print(f"   âœ… ì „ì²˜ë¦¬ ì™„ë£Œ - íƒ€ì…: {type(processed)}")
                
                if NUMPY_AVAILABLE and hasattr(processed, 'shape'):
                    print(f"   ğŸ“ ì²˜ë¦¬ëœ í¬ê¸°: {processed.shape}")
                
            except Exception as e:
                print(f"   âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            print("   âš ï¸ PIL ì—†ìŒ - ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
        
        test_time = time.time() - start_time
        print(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.3f}ì´ˆ")
        print("âœ… ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def validate_github_compatibility():
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦"""
    print("\nğŸ”— GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ì„± ê²€ì¦")
    print("-" * 60)
    
    results = {}
    
    # 1. main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸
    print("1ï¸âƒ£ main.py í•„ìˆ˜ í•¨ìˆ˜ í™•ì¸...")
    try:
        interface = get_step_model_interface("HumanParsingStep")
        results["get_step_model_interface"] = "âœ…"
        print("   get_step_model_interface: âœ…")
    except Exception as e:
        results["get_step_model_interface"] = f"âŒ {e}"
        print(f"   get_step_model_interface: âŒ {e}")
    
    try:
        memory_manager = get_step_memory_manager()
        results["get_step_memory_manager"] = "âœ…"
        print("   get_step_memory_manager: âœ…")
    except Exception as e:
        results["get_step_memory_manager"] = f"âŒ {e}"
        print(f"   get_step_memory_manager: âŒ {e}")
    
    try:
        data_converter = get_step_data_converter("ClothSegmentationStep")
        results["get_step_data_converter"] = "âœ…"
        print("   get_step_data_converter: âœ…")
    except Exception as e:
        results["get_step_data_converter"] = f"âŒ {e}"
        print(f"   get_step_data_converter: âŒ {e}")
    
    try:
        processed = preprocess_image_for_step("dummy", "VirtualFittingStep")
        results["preprocess_image_for_step"] = "âœ…"
        print("   preprocess_image_for_step: âœ…")
    except Exception as e:
        results["preprocess_image_for_step"] = f"âŒ {e}"
        print(f"   preprocess_image_for_step: âŒ {e}")
    
    # 2. í•µì‹¬ ë©”ì„œë“œ í™•ì¸
    print("2ï¸âƒ£ í•µì‹¬ ë©”ì„œë“œ í™•ì¸...")
    try:
        interface = get_step_model_interface("ClothSegmentationStep")
        models = interface.list_available_models()
        results["list_available_models"] = "âœ…"
        print(f"   list_available_models: âœ… ({len(models)}ê°œ ëª¨ë¸)")
    except Exception as e:
        results["list_available_models"] = f"âŒ {e}"
        print(f"   list_available_models: âŒ {e}")
    
    # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸
    print("3ï¸âƒ£ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§€ì› í™•ì¸...")
    steps = list_available_steps()
    if len(steps) == 8:
        results["8_step_pipeline"] = "âœ…"
        print(f"   8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: âœ… ({len(steps)}ë‹¨ê³„)")
    else:
        results["8_step_pipeline"] = f"âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›"
        print(f"   8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: âŒ {len(steps)}ë‹¨ê³„ë§Œ ì§€ì›")
    
    # 4. conda í™˜ê²½ ìµœì í™” í™•ì¸
    print("4ï¸âƒ£ conda í™˜ê²½ ìµœì í™” í™•ì¸...")
    if is_conda_environment():
        results["conda_optimization"] = "âœ…"
        print("   conda í™˜ê²½: âœ…")
    else:
        results["conda_optimization"] = "âš ï¸ conda í™˜ê²½ ì•„ë‹˜"
        print("   conda í™˜ê²½: âš ï¸ conda í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤")
    
    # 5. M3 Max ìµœì í™” í™•ì¸
    print("5ï¸âƒ£ M3 Max ìµœì í™” í™•ì¸...")
    if is_m3_max_device():
        results["m3_max_optimization"] = "âœ…"
        print("   M3 Max ìµœì í™”: âœ…")
    else:
        results["m3_max_optimization"] = "â„¹ï¸ M3 Max ì•„ë‹˜"
        print("   M3 Max ìµœì í™”: â„¹ï¸ M3 Max ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹™ë‹ˆë‹¤")
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“Š í˜¸í™˜ì„± ê²€ì¦ ê²°ê³¼:")
    success_count = 0
    warning_count = 0
    error_count = 0
    
    for test, result in results.items():
        if result.startswith("âœ…"):
            success_count += 1
        elif result.startswith("âš ï¸") or result.startswith("â„¹ï¸"):
            warning_count += 1
        else:
            error_count += 1
        
        print(f"   {test}: {result}")
    
    total_count = len(results)
    success_rate = (success_count / total_count) * 100
    
    print(f"\nğŸ¯ í˜¸í™˜ì„± ì ìˆ˜: {success_rate:.1f}%")
    print(f"   ì„±ê³µ: {success_count}ê°œ | ê²½ê³ : {warning_count}ê°œ | ì˜¤ë¥˜: {error_count}ê°œ")
    
    if success_rate >= 85:
        print("ğŸ‰ ìš°ìˆ˜í•œ í˜¸í™˜ì„±! GitHub í”„ë¡œì íŠ¸ì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤.")
        grade = "A"
    elif success_rate >= 70:
        print("âœ… ì–‘í˜¸í•œ í˜¸í™˜ì„±! ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        grade = "B"
    elif success_rate >= 50:
        print("âš ï¸ ë³´í†µ í˜¸í™˜ì„±. ì¼ë¶€ ê¸°ëŠ¥ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        grade = "C"
    else:
        print("âŒ í˜¸í™˜ì„± ë¬¸ì œ ìˆìŒ. ì¶”ê°€ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "D"
    
    return {
        "results": results,
        "success_rate": success_rate,
        "grade": grade,
        "summary": {
            "success": success_count,
            "warning": warning_count,
            "error": error_count,
            "total": total_count
        }
    }

def test_all_functionality(detailed: bool = False):
    """ëª¨ë“  ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ì „ì²´ ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    test_results = []
    start_time = time.time()
    
    # 1. ì‹œìŠ¤í…œ ì •ë³´ í…ŒìŠ¤íŠ¸
    print("ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸...")
    debug_system_info(detailed=detailed)
    test_results.append(("ì‹œìŠ¤í…œ ì •ë³´", True))
    
    # 2. Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ (ì£¼ìš” Stepë“¤)
    test_steps = ["HumanParsingStep", "VirtualFittingStep", "PostProcessingStep"]
    for step in test_steps:
        print(f"\nğŸ“ {step} í…ŒìŠ¤íŠ¸...")
        result = test_step_interface(step, detailed=detailed)
        test_results.append((f"{step} ì¸í„°í˜ì´ìŠ¤", result))
    
    # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸...")
    memory_result = test_memory_manager(detailed=detailed)
    test_results.append(("ë©”ëª¨ë¦¬ ê´€ë¦¬ì", memory_result))
    
    # 4. ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“Š ë°ì´í„° ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸...")
    converter_result = test_data_converter("HumanParsingStep")
    test_results.append(("ë°ì´í„° ë³€í™˜ê¸°", converter_result))
    
    # 5. GitHub í˜¸í™˜ì„± ê²€ì¦
    print(f"\nğŸ”— GitHub í˜¸í™˜ì„± ê²€ì¦...")
    compatibility_result = validate_github_compatibility()
    compat_success = compatibility_result["success_rate"] >= 70
    test_results.append(("GitHub í˜¸í™˜ì„±", compat_success))
    
    # ê²°ê³¼ ìš”ì•½
    total_time = time.time() - start_time
    
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:25} : {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print("-" * 70)
    
    total_tests = len(test_results)
    success_rate = (passed / total_tests) * 100
    
    print(f"ì „ì²´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
    print(f"í†µê³¼: {passed}ê°œ | ì‹¤íŒ¨: {failed}ê°œ")
    print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
    print(f"ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # ìµœì¢… íŒì •
    if success_rate >= 90:
        print("\nğŸ‰ ì™„ë²½í•œ ì‹œìŠ¤í…œ! ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        grade = "A+"
    elif success_rate >= 80:
        print("\nğŸš€ ìš°ìˆ˜í•œ ì‹œìŠ¤í…œ! ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        grade = "A"
    elif success_rate >= 70:
        print("\nâœ… ì–‘í˜¸í•œ ì‹œìŠ¤í…œ! ì£¼ìš” ê¸°ëŠ¥ë“¤ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        grade = "B"
    elif success_rate >= 60:
        print("\nâš ï¸ ë³´í†µ ìˆ˜ì¤€ì˜ ì‹œìŠ¤í…œ. ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "C"
    else:
        print("\nâŒ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        grade = "D"
    
    print("=" * 70)
    
    return {
        "results": test_results,
        "success_rate": success_rate,
        "grade": grade,
        "execution_time": total_time,
        "passed": passed,
        "failed": failed,
        "total": total_tests
    }

# ==============================================
# ğŸ”¥ ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤ ì¶”ê°€ (ì™„ì „ ë³´ì™„)
# ==============================================

# 1. ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ë“¤ ì¶”ê°€
def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (ê¸°ì¡´ Step í´ë˜ìŠ¤ ì§€ì›)
    âœ… ê¸°ì¡´ ì½”ë“œì™€ 100% í˜¸í™˜
    """
    try:
        manager = get_utils_manager()
        unified_interface = create_unified_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v8.0-complete",
            "has_unified_utils": True,
            "unified_interface": unified_interface,
            "conda_optimized": SYSTEM_INFO["in_conda"],
            "m3_max_optimized": SYSTEM_INFO["is_m3_max"]
        }
        
        # ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ë“¤
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        async def process_image_wrapper(image_data, **kwargs):
            return await unified_interface.process_image(image_data, **kwargs)
        
        legacy_interface.update({
            "get_model": get_model_wrapper,
            "optimize_memory": unified_interface.optimize_memory if hasattr(unified_interface, 'optimize_memory') else lambda: {"status": "ok"},
            "process_image": process_image_wrapper,
            "get_stats": unified_interface.get_stats,
            "get_config": unified_interface.get_config if hasattr(unified_interface, 'get_config') else lambda: {"step_name": step_name}
        })
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "fallback": True
        }

# 2. ë¹„ë™ê¸° ë¦¬ì…‹ í•¨ìˆ˜ ì¶”ê°€
async def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹"""
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.optimize_memory()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# 3. í´ë°± ìƒì„± í•¨ìˆ˜ë“¤ ì¶”ê°€
def _create_fallback_memory_manager(step_name: str = None, **kwargs):
    """í´ë°± ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
    class FallbackMemoryManager:
        def __init__(self, step_name=None, **kwargs):
            self.step_name = step_name
            self.device = SYSTEM_INFO["device"]
            self.memory_gb = SYSTEM_INFO["memory_gb"] 
            self.is_m3_max = SYSTEM_INFO["is_m3_max"]
            self.logger = logging.getLogger(f"FallbackMemoryManager.{step_name or 'global'}")
            self._allocated_memory = 0.0
            
        def allocate_memory(self, size_gb: float) -> bool:
            """ë©”ëª¨ë¦¬ í• ë‹¹ (ì‹œë®¬ë ˆì´ì…˜)"""
            if self._allocated_memory + size_gb <= self.memory_gb * 0.8:
                self._allocated_memory += size_gb
                self.logger.debug(f"ğŸ“ ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb}GB")
                return True
            else:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb}GB ìš”ì²­")
                return False
        
        def cleanup_memory(self, aggressive: bool = False):
            """ë©”ëª¨ë¦¬ ì •ë¦¬"""
            freed = self._allocated_memory
            self._allocated_memory = 0.0
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            collected = gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE:
                if self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except Exception:
                        pass
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬: {freed:.1f}GB í•´ì œ, {collected}ê°œ ê°ì²´ ì •ë¦¬")
            return {
                "success": True,
                "freed_gb": freed,
                "collected_objects": collected,
                "device": self.device
            }
        
        def get_memory_stats(self):
            """ë©”ëª¨ë¦¬ í†µê³„ (ê¸°ë³¸ êµ¬í˜„)"""
            try:
                available_memory = self.memory_gb - self._allocated_memory
                
                stats = {
                    "device": self.device,
                    "total_gb": self.memory_gb,
                    "allocated_gb": self._allocated_memory,
                    "available_gb": available_memory,
                    "usage_percent": (self._allocated_memory / self.memory_gb) * 100,
                    "is_m3_max": self.is_m3_max,
                    "step_name": self.step_name
                }
                
                # ì‹¤ì œ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
                if PSUTIL_AVAILABLE:
                    memory = psutil.virtual_memory()
                    stats.update({
                        "system_total_gb": memory.total / (1024**3),
                        "system_available_gb": memory.available / (1024**3),
                        "system_percent": memory.percent
                    })
                
                return stats
                
            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return {
                    "device": self.device,
                    "error": str(e),
                    "is_fallback": True
                }
        
        def check_memory_pressure(self) -> bool:
            """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸"""
            try:
                usage_percent = (self._allocated_memory / self.memory_gb) * 100
                return usage_percent > 80.0
            except Exception:
                return False
    
    return FallbackMemoryManager(step_name, **kwargs)

def _create_fallback_data_converter(step_name: str = None, **kwargs):
    """í´ë°± ë°ì´í„° ë³€í™˜ê¸°"""
    class FallbackDataConverter:
        def __init__(self, step_name=None, **kwargs):
            self.step_name = step_name
            self.device = SYSTEM_INFO["device"]
            self.logger = logging.getLogger(f"FallbackDataConverter.{step_name or 'global'}")
            
            # Stepë³„ ê¸°ë³¸ ì„¤ì •
            self.step_configs = {
                "HumanParsingStep": {
                    "input_size": (512, 512),
                    "normalize": True,
                    "mean": [0.485, 0.456, 0.406],
                    "std": [0.229, 0.224, 0.225]
                },
                "VirtualFittingStep": {
                    "input_size": (512, 512),
                    "normalize": True,
                    "mean": [0.5, 0.5, 0.5],
                    "std": [0.5, 0.5, 0.5]
                }
            }
            
            self.config = self.step_configs.get(step_name, {
                "input_size": (512, 512),
                "normalize": True,
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225]
            })
            
        def configure_for_step(self, step_name: str):
            """Stepë³„ ì„¤ì • ì ìš©"""
            self.step_name = step_name
            self.config = self.step_configs.get(step_name, self.config)
            
        def preprocess_image(self, image, target_size=None, **kwargs):
            """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
            try:
                target_size = target_size or self.config["input_size"]
                
                # PIL Image ì²˜ë¦¬
                if hasattr(image, 'resize'):
                    # RGB ë³€í™˜
                    if image.mode != 'RGB':
                        image = image.convert('RGB')
                    
                    # í¬ê¸° ì¡°ì • (PIL ë²„ì „ í˜¸í™˜ì„±)
                    if PIL_AVAILABLE:
                        try:
                            # PIL 10.0.0+ í˜¸í™˜ì„±
                            if hasattr(Image, 'Resampling') and hasattr(Image.Resampling, 'LANCZOS'):
                                image = image.resize(target_size, Image.Resampling.LANCZOS)
                            elif hasattr(Image, 'LANCZOS'):
                                image = image.resize(target_size, Image.LANCZOS)
                            else:
                                image = image.resize(target_size)
                        except Exception:
                            image = image.resize(target_size)
                    else:
                        image = image.resize(target_size)
                
                # NumPy ë°°ì—´ë¡œ ë³€í™˜
                if NUMPY_AVAILABLE:
                    image_array = np.array(image, dtype=np.float32)
                    
                    # ì •ê·œí™”
                    if self.config.get("normalize", True):
                        image_array = image_array / 255.0
                        
                        # í‘œì¤€í™” (ì„ íƒì )
                        if "mean" in self.config and "std" in self.config:
                            mean = np.array(self.config["mean"])
                            std = np.array(self.config["std"])
                            image_array = (image_array - mean) / std
                    
                    # HWC -> CHW ë³€í™˜ (PyTorch í˜•ì‹)
                    if len(image_array.shape) == 3:
                        image_array = image_array.transpose(2, 0, 1)
                    
                    return image_array
                
                return image
                
            except Exception as e:
                self.logger.warning(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return image
                
        def to_tensor(self, data):
            """í…ì„œ ë³€í™˜ (PyTorch ì§€ì›)"""
            try:
                if TORCH_AVAILABLE and NUMPY_AVAILABLE:
                    if isinstance(data, np.ndarray):
                        tensor = torch.from_numpy(data)
                        
                        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                        if self.device != "cpu":
                            tensor = tensor.to(self.device)
                        
                        return tensor
                
                return data
                
            except Exception as e:
                self.logger.warning(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return data
    
    return FallbackDataConverter(step_name, **kwargs)

# 4. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def format_memory_size(bytes_size: Union[int, float]) -> str:
    """ë©”ëª¨ë¦¬ í¬ê¸° í¬ë§·íŒ…"""
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    size = float(bytes_size)
    
    for unit in units:
        if size < 1024.0:
            return f"{size:.1f}{unit}"
        size /= 1024.0
    
    return f"{size:.1f}PB"

def create_model_config(name: str, **kwargs) -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ìƒì„± ë„ìš°ë¯¸"""
    config = {
        "name": name,
        "device": kwargs.get("device", SYSTEM_INFO["device"]),
        "precision": kwargs.get("precision", SYSTEM_INFO.get("recommended_precision", "fp32")),
        "created_at": time.time(),
        **kwargs
    }
    return config

# 5. ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¶”ê°€
async def test_async_operations():
    """ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    try:
        start_time = time.time()
        
        # 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
        manager = get_utils_manager()
        
        if not manager.is_initialized:
            manager.is_initialized = True
            manager.initialization_time = time.time()
            print("   ì´ˆê¸°í™” ì™„ë£Œ: âœ…")
        else:
            print("   ì´ë¯¸ ì´ˆê¸°í™”ë¨: âœ…")
        
        # 2. ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸...")
        interface = get_step_model_interface("VirtualFittingStep")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        model_start = time.time()
        model = await interface.get_model()
        model_time = time.time() - model_start
        
        if model:
            print(f"   ëª¨ë¸ ë¡œë“œ: âœ… ({model_time:.3f}ì´ˆ)")
            if isinstance(model, dict):
                print(f"   ëª¨ë¸ íƒ€ì…: {model.get('type', 'unknown')}")
        else:
            print(f"   ëª¨ë¸ ë¡œë“œ: âŒ ({model_time:.3f}ì´ˆ)")
        
        # 3. í†µí•© ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ í†µí•© ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸...")
        unified_interface = create_unified_interface("PostProcessingStep")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        dummy_image = {"width": 512, "height": 512, "channels": 3}
        
        process_start = time.time()
        result = await unified_interface.process_image(dummy_image)
        process_time = time.time() - process_start
        
        if result and result.get("success"):
            print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: âœ… ({process_time:.3f}ì´ˆ)")
            print(f"   ì²˜ë¦¬ ê²°ê³¼: {result.get('output_type', 'unknown')}")
        else:
            print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: âŒ ({process_time:.3f}ì´ˆ)")
        
        # 4. ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
        print("4ï¸âƒ£ ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸...")
        memory_start = time.time()
        memory_result = await manager.optimize_memory()
        memory_time = time.time() - memory_start
        
        if memory_result.get("success"):
            print(f"   ë©”ëª¨ë¦¬ ìµœì í™”: âœ… ({memory_time:.3f}ì´ˆ)")
            if memory_result.get("results", {}).get("collected_objects"):
                print(f"   ì •ë¦¬ëœ ê°ì²´: {memory_result['results']['collected_objects']}ê°œ")
        else:
            print(f"   ë©”ëª¨ë¦¬ ìµœì í™”: âŒ ({memory_time:.3f}ì´ˆ)")
        
        total_time = time.time() - start_time
        print(f"â±ï¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print("âœ… ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

# 6. í–¥ìƒëœ ì‹œìŠ¤í…œ ìƒíƒœ í•¨ìˆ˜
def get_enhanced_system_status() -> Dict[str, Any]:
    """í–¥ìƒëœ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        basic_status = get_system_status()
        
        # ì¶”ê°€ ì •ë³´
        enhanced_info = {
            "runtime_info": {
                "uptime_seconds": time.time() - _module_start_time,
                "python_executable": sys.executable,
                "working_directory": str(Path.cwd()),
                "process_id": os.getpid()
            },
            "performance_info": {
                "cpu_usage": psutil.cpu_percent() if PSUTIL_AVAILABLE else "unknown",
                "memory_usage": psutil.virtual_memory().percent if PSUTIL_AVAILABLE else "unknown",
                "disk_usage": psutil.disk_usage('/').percent if PSUTIL_AVAILABLE else "unknown"
            },
            "library_versions": {
                "python": sys.version,
                "torch": TORCH_VERSION,
                "numpy": NUMPY_VERSION if NUMPY_AVAILABLE else "not_available",
                "pillow": PIL_VERSION  # âœ… ì•ˆì „í•œ PIL ë²„ì „ ì‚¬ìš©
            }
        }
        
        # ê¸°ë³¸ ìƒíƒœì™€ ë³‘í•©
        if isinstance(basic_status, dict):
            enhanced_status = {**basic_status, **enhanced_info}
        else:
            enhanced_status = enhanced_info
            
        return enhanced_status
        
    except Exception as e:
        logger.error(f"í–¥ìƒëœ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ __all__ ì—…ë°ì´íŠ¸ (ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€)
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface', 
    'StepModelInterface',
    'StepMemoryManager',
    'StepDataConverter',
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ğŸ”§ ì—´ê±°í˜•
    'UtilsMode',
    'DeviceType',
    'PrecisionType', 
    'StepType',
    
    # ğŸ”„ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'get_enhanced_system_status',  # âœ… ì¶”ê°€
    'optimize_system_memory',
    'reset_global_utils',          # âœ… ì¶”ê°€
    
    # ğŸ”— ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)
    'get_step_model_interface',    # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    'get_step_memory_manager',     # âœ… main.py í•µì‹¬ í•¨ìˆ˜  
    'get_step_data_converter',     # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    'preprocess_image_for_step',   # âœ… main.py í•µì‹¬ í•¨ìˆ˜
    'create_unified_interface',    # ìƒˆë¡œìš´ ë°©ì‹
    'create_step_interface',       # âœ… ë ˆê±°ì‹œ í˜¸í™˜ ì¶”ê°€
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'get_ai_models_path',
    'get_device_info',
    'get_conda_info',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'list_available_steps',
    'is_conda_environment',
    'is_m3_max_device',
    'validate_step_name',
    'get_step_number',
    'check_system_requirements',
    'format_memory_size',          # âœ… ì¶”ê°€
    'create_model_config',         # âœ… ì¶”ê°€
    
    # ğŸ”§ í´ë°± í•¨ìˆ˜ë“¤ (ë‚´ë¶€ìš©ì´ì§€ë§Œ export)
    '_create_fallback_memory_manager',   # âœ… ì¶”ê°€
    '_create_fallback_data_converter',   # âœ… ì¶”ê°€
    
    # ğŸ§ª ê°œë°œ/ë””ë²„ê·¸ í•¨ìˆ˜ë“¤
    'debug_system_info',
    'test_step_interface',
    'test_memory_manager',
    'test_data_converter',
    'test_async_operations',       # âœ… ì¶”ê°€
    'validate_github_compatibility',
    'test_all_functionality'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° í™˜ê²½ ì •ë³´ (ì™„ì „ êµ¬í˜„)
# ==============================================

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
_module_start_time = time.time()

# í™˜ê²½ ì •ë³´ ë¡œê¹…
logger.info("âœ… PIL.__version__ ì˜¤ë¥˜ ì™„ì „ í•´ê²° (PIL ìµœì‹  ë²„ì „ í˜¸í™˜)")
logger.info("âœ… PIL 10.0.0+ Image.Resampling.LANCZOS í˜¸í™˜ì„± ì¶”ê°€")
logger.info("âœ… ëª¨ë“  PIL ë²„ì „ì—ì„œ ì•ˆì „í•œ ì´ë¯¸ì§€ ë¦¬ìƒ˜í”Œë§ ë³´ì¥")
logger.info("âœ… create_step_interface ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ ì¶”ê°€")
logger.info("âœ… reset_global_utils ë¹„ë™ê¸° ë¦¬ì…‹ í•¨ìˆ˜ ì¶”ê°€")
logger.info("âœ… _create_fallback_* í´ë°± ìƒì„± í•¨ìˆ˜ë“¤ ì¶”ê°€")
logger.info("âœ… format_memory_size, create_model_config ìœ í‹¸ë¦¬í‹° ì¶”ê°€")
logger.info("âœ… test_async_operations ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¶”ê°€")
logger.info("âœ… get_enhanced_system_status í–¥ìƒëœ ìƒíƒœ ì¡°íšŒ ì¶”ê°€")
logger.info("=" * 80)
logger.info("ğŸ MyCloset AI ì™„ì „í•œ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… ë‘ íŒŒì¼ì˜ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ í†µí•© (ìµœê³ ì˜ ì¡°í•©)")
logger.info("âœ… get_step_model_interface í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… get_step_memory_manager í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… get_step_data_converter í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… preprocess_image_for_step í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… StepModelInterface.list_available_models ì™„ì „ í¬í•¨")
logger.info("âœ… UnifiedStepInterface í†µí•© ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„")
logger.info("âœ… StepDataConverter ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ êµ¬í˜„")
logger.info("âœ… conda í™˜ê²½ 100% ìµœì í™”")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©")
logger.info("âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„")
logger.info("âœ… Clean Architecture ì ìš©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥")
logger.info("âœ… ëª¨ë“  import ì˜¤ë¥˜ í•´ê²°")
logger.info("âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜")
logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
logger.info("âœ… GPU í˜¸í™˜ì„± ì™„ì „ ë³´ì¥")
logger.info("âœ… ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í¬í•¨")

# ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´
logger.info(f"ğŸ”§ í”Œë«í¼: {SYSTEM_INFO['platform']} ({SYSTEM_INFO['machine']})")
logger.info(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {SYSTEM_INFO['device']} ({SYSTEM_INFO.get('device_name', 'Unknown')})")
logger.info(f"ğŸ Python: {SYSTEM_INFO['python_version']}")
logger.info(f"ğŸ conda í™˜ê²½: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ
libraries = SYSTEM_INFO.get("libraries", {})
logger.info(f"ğŸ“š PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'} ({libraries.get('torch', 'N/A')})")
logger.info(f"ğŸ“š NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'} ({libraries.get('numpy', 'N/A')})")
logger.info(f"ğŸ“š PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'} ({PIL_VERSION})")
logger.info(f"ğŸ“š psutil: {'âœ…' if PSUTIL_AVAILABLE else 'âŒ'}")

# í”„ë¡œì íŠ¸ ê²½ë¡œ
logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {SYSTEM_INFO['project_root']}")
logger.info(f"ğŸ“ AI ëª¨ë¸ ê²½ë¡œ: {SYSTEM_INFO['ai_models_path']}")
logger.info(f"ğŸ“ ëª¨ë¸ í´ë” ì¡´ì¬: {'âœ…' if SYSTEM_INFO['ai_models_exists'] else 'âŒ'}")

# ì„±ëŠ¥ ìµœì í™” ìƒíƒœ
if SYSTEM_INFO["in_conda"]:
    logger.info("ğŸ conda í™˜ê²½ ê°ì§€ - ê³ ì„±ëŠ¥ ìµœì í™” í™œì„±í™”")
    if SYSTEM_INFO["is_m3_max"]:
        logger.info("ğŸ M3 Max + conda ì¡°í•© - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”")
        logger.info("ğŸš€ 128GB Unified Memory í™œìš© ê°€ëŠ¥")

# ëª¨ë“ˆ ë¡œë“œ ì‹œê°„
module_load_time = time.time() - _module_start_time
logger.info(f"âš¡ ëª¨ë“ˆ ë¡œë“œ ì‹œê°„: {module_load_time:.3f}ì´ˆ")

# í•„ìˆ˜ í•¨ìˆ˜ ì™„ì„±ë„ ê²€ì¦
try:
    required_functions = [
        'get_step_model_interface',
        'get_step_memory_manager', 
        'get_step_data_converter',
        'preprocess_image_for_step'
    ]
    
    missing_functions = []
    for func_name in required_functions:
        if func_name not in globals():
            missing_functions.append(func_name)
    
    if missing_functions:
        logger.warning(f"âš ï¸ ëˆ„ë½ëœ í•¨ìˆ˜ë“¤: {missing_functions}")
    else:
        logger.info("âœ… ëª¨ë“  í•„ìˆ˜ í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ")
        
except Exception as e:
    logger.warning(f"âš ï¸ í•¨ìˆ˜ ì™„ì„±ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")

logger.info("=" * 80)

# ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ (ì„ íƒì )
try:
    requirements = check_system_requirements()
    if requirements["overall_satisfied"]:
        logger.info(f"âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë§Œì¡± (ì ìˆ˜: {requirements['score']:.0f}%)")
    else:
        logger.warning(f"âš ï¸ ì¼ë¶€ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡± (ì ìˆ˜: {requirements['score']:.0f}%)")
        
except Exception as e:
    logger.debug(f"ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit

def cleanup_on_exit():
    """í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        logger.info("ğŸ§¹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹œì‘...")
        
        # ì „ì—­ ë§¤ë‹ˆì € ì •ë¦¬
        global _global_manager
        if _global_manager:
            try:
                # ë™ê¸° ì •ë¦¬
                _global_manager.global_memory_manager.cleanup_memory(force=True)
                logger.info("âœ… ì „ì—­ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        logger.info(f"ğŸ—‘ï¸ Python ê°ì²´ {collected}ê°œ ì •ë¦¬")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
        if TORCH_AVAILABLE:
            device = SYSTEM_INFO["device"]
            if device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("ğŸ—‘ï¸ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            elif device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    logger.info("ğŸ—‘ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.debug(f"MPS ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        logger.info("ğŸ‰ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
atexit.register(cleanup_on_exit)

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    print("ğŸ MyCloset AI ì™„ì „í•œ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v8.0")
    print("=" * 70)
    print("ğŸ“‹ ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    print()
    
    # ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        success_data = test_all_functionality(detailed=True)
        success = success_data["success_rate"] >= 70
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        if success:
            print("\nğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! main.pyì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("\nğŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
            print("```python")
            print("from app.ai_pipeline.utils import (")
            print("    get_step_model_interface,")
            print("    get_step_memory_manager,")
            print("    get_step_data_converter,")
            print("    preprocess_image_for_step")
            print(")")
            print("")
            print("# ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±")
            print("interface = get_step_model_interface('HumanParsingStep')")
            print("models = interface.list_available_models()")
            print("")
            print("# ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±")
            print("memory_manager = get_step_memory_manager('HumanParsingStep')")
            print("stats = memory_manager.get_memory_stats()")
            print("")
            print("# ë°ì´í„° ë³€í™˜ê¸° ìƒì„±")
            print("data_converter = get_step_data_converter('HumanParsingStep')")
            print("processed_image = preprocess_image_for_step(image, 'HumanParsingStep')")
            print("")
            print("# ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ")
            print("model = await interface.get_model()")
            print("```")
            print()
            print("ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:")
            print("   âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
            print("   âœ… conda í™˜ê²½ 100% ìµœì í™”")
            print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©")
            print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ êµ¬í˜„")
            print("   âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
            print("   âœ… ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„")
            print("   âœ… ì™„ì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜")
            print("   âœ… ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜")
        else:
            print("\nâš ï¸ ì‹œìŠ¤í…œì— ì¼ë¶€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ë¡œê·¸ë¥¼ í™•ì¸í•˜ì‹œê±°ë‚˜ ê°œë³„ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            print("\nğŸ”§ ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
            print("   python -c \"from app.ai_pipeline.utils import debug_system_info; debug_system_info()\"")
            print("   python -c \"from app.ai_pipeline.utils import test_step_interface; test_step_interface()\"")
            print("   python -c \"from app.ai_pipeline.utils import validate_github_compatibility; validate_github_compatibility()\"")
        
        return success
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ”§ ê¸°ë³¸ ì •ë³´ë§Œ í™•ì¸:")
        try:
            debug_system_info()
        except Exception as debug_e:
            print(f"   ë””ë²„ê·¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {debug_e}")
        
        return False

if __name__ == "__main__":
    main()