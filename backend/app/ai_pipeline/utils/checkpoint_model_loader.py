# app/ai_pipeline/utils/checkpoint_model_loader.py
"""
ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ModelLoader ì™„ì „ ì—°ë™
ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ 80GB ì²´í¬í¬ì¸íŠ¸ë“¤ í™œìš©
"""

from app.ai_pipeline.utils.model_loader import ModelLoader, ModelConfig, ModelType
from app.core.optimized_model_paths import (
    ANALYZED_MODELS, get_optimal_model_for_step, 
    get_checkpoint_path, get_largest_checkpoint
)
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class CheckpointModelLoader(ModelLoader):
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ í™•ì¥ ModelLoader"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._register_analyzed_models()
    
    def _register_analyzed_models(self):
        """ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ìë™ ë“±ë¡"""
        logger.info("ğŸ“¦ ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ë“±ë¡ ì¤‘...")
        
        registered_count = 0
        
        for model_name, model_info in ANALYZED_MODELS.items():
            if not model_info["ready"]:
                continue
                
            try:
                # ModelType ë§¤í•‘
                model_type = self._map_to_model_type(model_info["type"])
                if not model_type:
                    continue
                
                # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
                main_checkpoint = get_largest_checkpoint(model_name)
                checkpoint_path = get_checkpoint_path(model_name, main_checkpoint) if main_checkpoint else None
                
                # ëª¨ë¸ ì„¤ì • ìƒì„±
                model_config = ModelConfig(
                    name=model_info["name"],
                    model_type=model_type,
                    model_class=self._get_model_class(model_info["type"]),
                    checkpoint_path=str(checkpoint_path) if checkpoint_path else None,
                    input_size=(512, 512),
                    device=self.device
                )
                
                # ëª¨ë¸ ë“±ë¡
                self.register_model(model_name, model_config)
                registered_count += 1
                
                logger.info(f"   âœ… {model_name}: {model_info['name']}")
                
            except Exception as e:
                logger.warning(f"   âš ï¸ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ“¦ ì´ {registered_count}ê°œ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    
    def _map_to_model_type(self, analysis_type: str) -> Optional[ModelType]:
        """ë¶„ì„ íƒ€ì…ì„ ModelTypeìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            'diffusion': ModelType.DIFFUSION,
            'virtual_tryon': ModelType.VIRTUAL_FITTING,
            'human_parsing': ModelType.HUMAN_PARSING,
            'pose_estimation': ModelType.POSE_ESTIMATION,
            'cloth_segmentation': ModelType.CLOTH_SEGMENTATION,
            'geometric_matching': ModelType.GEOMETRIC_MATCHING,
            'cloth_warping': ModelType.CLOTH_WARPING,
            'detection': ModelType.SEGMENTATION,
            'text_image': ModelType.DIFFUSION
        }
        return mapping.get(analysis_type)
    
    def _get_model_class(self, analysis_type: str) -> str:
        """ë¶„ì„ íƒ€ì…ì—ì„œ ëª¨ë¸ í´ë˜ìŠ¤ëª… ì¶”ì¶œ"""
        mapping = {
            'diffusion': 'StableDiffusionPipeline',
            'virtual_tryon': 'HRVITONModel',
            'human_parsing': 'GraphonomyModel',
            'pose_estimation': 'OpenPoseModel',
            'cloth_segmentation': 'U2NetModel',
            'geometric_matching': 'GeometricMatchingModel',
            'cloth_warping': 'HRVITONModel',
            'detection': 'DetectronModel',
            'text_image': 'CLIPModel'
        }
        return mapping.get(analysis_type, 'BaseModel')
    
    async def load_optimal_model_for_step(self, step: str, **kwargs):
        """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë¡œë“œ"""
        optimal_model = get_optimal_model_for_step(step)
        if not optimal_model:
            logger.warning(f"âš ï¸ {step}ì— ëŒ€í•œ ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        logger.info(f"ğŸ¯ {step} ìµœì  ëª¨ë¸ ë¡œë“œ: {optimal_model}")
        return await self.load_model(optimal_model, **kwargs)

# ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë”
_global_checkpoint_loader: Optional[CheckpointModelLoader] = None

def get_checkpoint_model_loader(**kwargs) -> CheckpointModelLoader:
    """ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_checkpoint_loader
    if _global_checkpoint_loader is None:
        _global_checkpoint_loader = CheckpointModelLoader(**kwargs)
    return _global_checkpoint_loader

async def load_best_model_for_step(step: str, **kwargs):
    """ë‹¨ê³„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
    loader = get_checkpoint_model_loader()
    return await loader.load_optimal_model_for_step(step, **kwargs)

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
async def load_best_diffusion_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_06_virtual_fitting", **kwargs)

async def load_best_human_parsing_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_01_human_parsing", **kwargs)

async def load_best_pose_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_02_pose_estimation", **kwargs)
