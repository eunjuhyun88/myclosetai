# app/ai_pipeline/utils/checkpoint_model_loader.py
"""
ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ModelLoader ì™„ì „ ì—°ë™
ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ 127.2GB ì²´í¬í¬ì¸íŠ¸ë“¤ í™œìš©
"""

import os
import torch
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

try:
    from app.core.optimized_model_paths import (
        ANALYZED_MODELS, get_optimal_model_for_step, 
        get_checkpoint_path, get_largest_checkpoint
    )
    OPTIMIZED_PATHS_AVAILABLE = True
except ImportError:
    OPTIMIZED_PATHS_AVAILABLE = False

logger = logging.getLogger(__name__)

class CheckpointModelLoader:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        self.models = {}
        self.loaded_models = {}
        
        if OPTIMIZED_PATHS_AVAILABLE:
            self._register_analyzed_models()
        else:
            logger.warning("âš ï¸ ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def _register_analyzed_models(self):
        """ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ë“±ë¡"""
        if not OPTIMIZED_PATHS_AVAILABLE:
            return
            
        registered_count = 0
        
        for model_name, model_info in ANALYZED_MODELS.items():
            if not model_info["ready"]:
                continue
            
            try:
                # ëª¨ë¸ ì •ë³´ ë“±ë¡
                self.models[model_name] = {
                    "name": model_info["name"],
                    "type": model_info["type"],
                    "step": model_info["step"],
                    "path": model_info["path"],
                    "checkpoints": model_info["checkpoints"],
                    "size_mb": model_info["size_mb"],
                    "priority": model_info["priority"]
                }
                
                registered_count += 1
                
            except Exception as e:
                logger.warning(f"   âš ï¸ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ“¦ {registered_count}ê°œ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    
    async def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        if model_name not in self.models:
            logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            return None
        
        try:
            model_info = self.models[model_name]
            
            # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            largest_checkpoint = get_largest_checkpoint(model_name)
            if not largest_checkpoint:
                logger.warning(f"âš ï¸ {model_name}ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            checkpoint_path = get_checkpoint_path(model_name, largest_checkpoint)
            
            if not checkpoint_path or not checkpoint_path.exists():
                logger.warning(f"âš ï¸ {model_name}ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
                return None
            
            # PyTorch ëª¨ë¸ ë¡œë“œ
            logger.info(f"ğŸ”§ {model_name} ë¡œë”© ì¤‘... ({checkpoint_path})")
            
            # ì•ˆì „í•œ ë¡œë“œ
            try:
                model = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            except:
                # weights_onlyê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° í´ë°±
                model = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ì •ë¦¬ ë° ë””ë°”ì´ìŠ¤ ì´ë™
            if isinstance(model, dict):
                if 'model' in model:
                    model = model['model']
                elif 'state_dict' in model:
                    model = model['state_dict']
            
            # ìºì‹œì— ì €ì¥
            self.loaded_models[model_name] = model
            
            logger.info(f"âœ… {model_name} ë¡œë”© ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def load_optimal_model_for_step(self, step: str, **kwargs) -> Optional[Any]:
        """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë¡œë“œ"""
        optimal_model = get_optimal_model_for_step(step)
        if not optimal_model:
            logger.warning(f"âš ï¸ {step}ì— ëŒ€í•œ ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        logger.info(f"ğŸ¯ {step} ìµœì  ëª¨ë¸ ë¡œë“œ: {optimal_model}")
        return await self.load_model(optimal_model, **kwargs)
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return self.models.get(model_name)
    
    def list_models(self) -> Dict[str, Dict]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        return self.models.copy()
    
    def clear_cache(self):
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        self.loaded_models.clear()
        
        if self.device == "mps" and torch.backends.mps.is_available():
            safe_mps_empty_cache()
        elif self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ëª¨ë¸ ë¡œë”
_global_checkpoint_loader: Optional[CheckpointModelLoader] = None

def get_checkpoint_model_loader(**kwargs) -> CheckpointModelLoader:
    """ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_checkpoint_loader
    if _global_checkpoint_loader is None:
        _global_checkpoint_loader = CheckpointModelLoader(**kwargs)
    return _global_checkpoint_loader

async def load_best_model_for_step(step: str, **kwargs) -> Optional[Any]:
    """ë‹¨ê³„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
    loader = get_checkpoint_model_loader()
    return await loader.load_optimal_model_for_step(step, **kwargs)

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
async def load_best_diffusion_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_06_virtual_fitting", **kwargs)

async def load_best_human_parsing_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_01_human_parsing", **kwargs)

async def load_best_pose_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_02_pose_estimation", **kwargs)

async def load_best_cloth_segmentation_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ ì˜ë¥˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_03_cloth_segmentation", **kwargs)
