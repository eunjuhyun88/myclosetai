#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Unified Common Imports for ALL AI Pipeline Steps
=================================================================

ëª¨ë“  AI pipeline step íŒŒì¼ë“¤ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” import ë¸”ë¡ë“¤ì„ ì™„ì „íˆ í†µí•©í•œ ëª¨ë“ˆì…ë‹ˆë‹¤.
ì´ ëª¨ë“ˆ í•˜ë‚˜ë§Œ importí•˜ë©´ ëª¨ë“  Stepì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from app.ai_pipeline.utils.common_imports import *
    
    ë˜ëŠ”
    
    from app.ai_pipeline.utils.common_imports import (
        torch, nn, F, transforms, Image, np, cv2,
        TORCH_AVAILABLE, MPS_AVAILABLE, PIL_AVAILABLE,
        MyClosetAIException, track_exception, ErrorCodes
    )

Author: MyCloset AI Team
Date: 2025-08-03
Version: 8.0 (Unified for All Steps + Duplicate Import Prevention)
"""

# ==============================================
# ğŸ”¥ ì¤‘ë³µ import ë°©ì§€ ì‹œìŠ¤í…œ
# ==============================================
if 'COMMON_IMPORTS_LOADED' in globals():
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ê¸°ì¡´ ê°ì²´ë“¤ ë°˜í™˜
    pass
else:
    # ìµœì´ˆ ë¡œë“œ ì‹œì—ë§Œ ì‹¤í–‰
    globals()['COMMON_IMPORTS_LOADED'] = True
    
    # ==============================================
    # ğŸ”¥ 1ë‹¨ê³„: í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ imports
    # ==============================================
    import os
    import sys
    import gc
    import time
    import asyncio
    import logging
    import threading
    import traceback
    import hashlib
    import json
    import base64
    import math
    import warnings
    import weakref
    import uuid
    import subprocess
    import platform
    from datetime import datetime, timedelta
    
    from pathlib import Path
    from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING, Set
    from dataclasses import dataclass, field
    from enum import Enum, IntEnum
    from io import BytesIO, StringIO
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from functools import lru_cache, wraps, partial
    from contextlib import asynccontextmanager, contextmanager
    from collections import defaultdict, deque
    from itertools import chain
    
    # ABCëŠ” ë³„ë„ë¡œ import
    from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 2ë‹¨ê³„: AI Pipeline ìœ í‹¸ë¦¬í‹° imports
# ==============================================

# Model Loader System
try:
    from app.ai_pipeline.models.auto_model_detector import AutoModelDetector as auto_model_detector
except ImportError:
    auto_model_detector = None

try:
    from app.ai_pipeline.models.dynamic_model_detector import DynamicModelDetector as dynamic_model_detector
except ImportError:
    dynamic_model_detector = None

# Memory Management System
try:
    from app.ai_pipeline.utils.memory_manager import MemoryManager as memory_manager
except ImportError:
    memory_manager = None

try:
    from app.ai_pipeline.utils.memory_monitor import MemoryMonitor as memory_monitor
except ImportError:
    memory_monitor = None

try:
    from app.ai_pipeline.utils.performance_optimizer import PerformanceOptimizer as performance_optimizer
except ImportError:
    performance_optimizer = None

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: Human Parsing ëª¨ë“ˆ imports (ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°)
# ==============================================

# Human Parsing ëª¨ë“ˆë“¤ì€ ê° Step íŒŒì¼ì—ì„œ ì§ì ‘ importí•˜ë„ë¡ ë³€ê²½
# ìˆœí™˜ì°¸ì¡° ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ common_importsì—ì„œëŠ” ì œì™¸

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: í™˜ê²½ ê°ì§€ ë° ë””ë°”ì´ìŠ¤ ì„¤ì • (ìµœìš°ì„ )
# ==============================================

# PyTorch imports
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.utils.data as data
    from torch.utils.data import DataLoader, Dataset
    from torch.autograd import Variable
    import torch.autograd as autograd
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    import torchvision.models as models
    TORCH_AVAILABLE = True
    print("âœ… PyTorch ë¡œë“œ ì™„ë£Œ")
except ImportError:
    torch = None
    nn = None
    F = None
    data = None
    DataLoader = None
    Dataset = None
    Variable = None
    autograd = None
    autocast = None
    transforms = None
    models = None
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ì œí•œëœ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥")

# MPS (Apple Silicon) ì§€ì› í™•ì¸
try:
    if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
    else:
        MPS_AVAILABLE = False
        print("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€")
except:
    MPS_AVAILABLE = False
    print("âš ï¸ MPS í™•ì¸ ì‹¤íŒ¨")

# NumPy
try:
    import numpy as np
    NP_AVAILABLE = True
    print("âœ… NumPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    np = None
    NP_AVAILABLE = False
    print("âš ï¸ NumPy ì—†ìŒ")

# PIL (Pillow)
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
    print("âœ… PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    Image = None
    ImageDraw = None
    ImageFont = None
    ImageFilter = None
    ImageEnhance = None
    PIL_AVAILABLE = False
    print("âš ï¸ PIL ì—†ìŒ")

# OpenCV
try:
    import cv2
    CV2_AVAILABLE = True
    print("âœ… OpenCV ë¡œë“œ ì™„ë£Œ")
except ImportError:
    cv2 = None
    CV2_AVAILABLE = False
    print("âš ï¸ OpenCV ì—†ìŒ")

# SciPy
try:
    import scipy
    from scipy import ndimage, signal, optimize, stats
    SCIPY_AVAILABLE = True
    print("âœ… SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    scipy = None
    ndimage = None
    signal = None
    optimize = None
    stats = None
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ì—†ìŒ")

def detect_m3_max() -> bool:
    """M3 Max ì¹©ì…‹ ê°ì§€"""
    try:
        if platform.system() != "Darwin":
            return False
        result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            brand = result.stdout.strip().lower()
            return any(chip in brand for chip in ['m3 max', 'm3 pro', 'm3', 'm2 max', 'm2 pro', 'm1 max'])
        return False
    except Exception:
        return False

def detect_conda_env() -> Dict[str, Any]:
    """Conda í™˜ê²½ ê°ì§€"""
    try:
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'unknown')
        conda_prefix = os.environ.get('CONDA_PREFIX', '')
        in_conda = 'CONDA_DEFAULT_ENV' in os.environ
        
        # ë©”ëª¨ë¦¬ ê°ì§€
        memory_gb = 16.0
        try:
            if platform.system() == "Darwin":
                result = subprocess.run(['sysctl', '-n', 'hw.memsize'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    memory_bytes = int(result.stdout.strip())
                    memory_gb = memory_bytes / (1024**3)
        except:
            pass
        
        return {
            'in_conda': in_conda,
            'conda_env': conda_env,
            'conda_prefix': conda_prefix,
            'is_target_env': conda_env in ['myclosetlast', 'mycloset-ai-clean'],
            'memory_gb': memory_gb
        }
    except Exception:
        return {
            'in_conda': False,
            'conda_env': 'unknown',
            'conda_prefix': '',
            'is_target_env': False,
            'memory_gb': 16.0
        }

# í™˜ê²½ ì •ë³´ ì „ì—­ ë³€ìˆ˜
IS_M3_MAX = detect_m3_max()
CONDA_INFO = detect_conda_env()
MEMORY_GB = CONDA_INFO['memory_gb']

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬ imports (ê°€ìš©ì„± ë³€ìˆ˜ ë¨¼ì € ì •ì˜)
# ==============================================

# âœ… ê°€ìš©ì„± ë³€ìˆ˜ë“¤ì„ ë¨¼ì € ì´ˆê¸°í™” (ë§¤ìš° ì¤‘ìš”!)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
CUDA_AVAILABLE = False
PIL_AVAILABLE = False
NUMPY_AVAILABLE = False
CV2_AVAILABLE = False
SCIPY_AVAILABLE = False
SKIMAGE_AVAILABLE = False
DENSECRF_AVAILABLE = False
MEDIAPIPE_AVAILABLE = False
ULTRALYTICS_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False
SAM_AVAILABLE = False
SAFETENSORS_AVAILABLE = False

# ë””ë°”ì´ìŠ¤ ë° í™˜ê²½
DEVICE = "cpu"
TORCH_VERSION = "unknown"

# ==============================================
# NumPy (í•„ìˆ˜ - ëª¨ë“  Stepì—ì„œ ì‚¬ìš©)
# ==============================================
try:
    import numpy as np
    NUMPY_AVAILABLE = True
    print("âœ… NumPy ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    np = None

# ==============================================
# PyTorch (í•µì‹¬ AI ë¼ì´ë¸ŒëŸ¬ë¦¬)
# ==============================================
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torch import autocast
    from torchvision import transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    
    # autogradëŠ” torchì—ì„œ ì§ì ‘ import
    import torch.autograd as autograd
    
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    print(f"âœ… PyTorch {TORCH_VERSION} ë¡œë“œ ì™„ë£Œ")
    
    # MPS ì§€ì› í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ MPS ì‚¬ìš© ê°€ëŠ¥")
    
    # CUDA ì§€ì› í™•ì¸
    if torch.cuda.is_available():
        CUDA_AVAILABLE = True
        print("ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥")
    
    # ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
    if MPS_AVAILABLE:
        DEVICE = "mps"
    elif CUDA_AVAILABLE:
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
    
    print(f"ğŸ¯ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {DEVICE}")
    
except ImportError as e:
    print(f"âŒ PyTorch import ì‹¤íŒ¨: {e}")
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None
    DataLoader = None
    autocast = None
    transforms = None
    resize = None
    to_pil_image = None
    to_tensor = None
    autograd = None

# ==============================================
# PIL (í•„ìˆ˜ - ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ Stepì—ì„œ ì‚¬ìš©)
# ==============================================
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont, ImageOps
    PIL_AVAILABLE = True
    print("âœ… PIL ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"âŒ PIL import ì‹¤íŒ¨: {e}")
    Image = None

# ==============================================
# OpenCV (ì¤‘ìš” - ëŒ€ë¶€ë¶„ Stepì—ì„œ ì‚¬ìš©)
# ==============================================
try:
    import cv2
    CV2_AVAILABLE = True
    print("âœ… OpenCV ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ OpenCV ì—†ìŒ - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
    cv2 = None

# ==============================================
# SciPy (ê³ ê¸‰ í›„ì²˜ë¦¬ìš©)
# ==============================================
try:
    import scipy
    from scipy import ndimage
    from scipy.ndimage import gaussian_filter, median_filter
    from scipy.signal import convolve2d
    from scipy.optimize import curve_fit
    SCIPY_AVAILABLE = True
    print("âœ… SciPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ SciPy ì—†ìŒ - ê³ ê¸‰ í›„ì²˜ë¦¬ ì œí•œ")
    scipy = None
    ndimage = None

# ==============================================
# Scikit-image (ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬)
# ==============================================
try:
    from skimage import measure, morphology, segmentation, filters, restoration, exposure
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
    print("âœ… scikit-image ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ scikit-image ì—†ìŒ - ì¼ë¶€ ì´ë¯¸ì§€ ì²˜ë¦¬ ì œí•œ")
    measure = None
    morphology = None
    segmentation = None
    filters = None

# ==============================================
# DenseCRF (CRF í›„ì²˜ë¦¬ìš©)
# ==============================================
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
    print("âœ… DenseCRF ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ DenseCRF ì—†ìŒ - CRF í›„ì²˜ë¦¬ ì œí•œ")
    dcrf = None
    unary_from_softmax = None

# ==============================================
# MediaPipe (Pose Estimationìš©)
# ==============================================
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("âœ… MediaPipe ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ MediaPipe ì—†ìŒ - Pose Estimation ì¼ë¶€ ì œí•œ")
    mp = None

# ==============================================
# Ultralytics YOLO (Pose Estimationìš©)
# ==============================================
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
    print("âœ… Ultralytics YOLO ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ Ultralytics ì—†ìŒ - YOLO ê¸°ë°˜ ê¸°ëŠ¥ ì œí•œ")
    YOLO = None

# ==============================================
# Transformers (ì¼ë¶€ Stepì—ì„œ ì‚¬ìš©)
# ==============================================
try:
    from transformers import AutoModel, AutoTokenizer, AutoProcessor
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ Transformers ì—†ìŒ - ì¼ë¶€ ëª¨ë¸ ì œí•œ")
    AutoModel = None
    AutoTokenizer = None

# ==============================================
# Segment Anything Model (SAM)
# ==============================================
try:
    import segment_anything as sam
    SAM_AVAILABLE = True
    print("âœ… SAM ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ SAM ì—†ìŒ - Segment Anything ê¸°ëŠ¥ ì œí•œ")
    sam = None

# ==============================================
# SafeTensors (ëª¨ë¸ ë¡œë”©ìš©)
# ==============================================
try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
    print("âœ… SafeTensors ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ SafeTensors ì—†ìŒ - ì¼ë¶€ ëª¨ë¸ ë¡œë”© ì œí•œ")
    st = None

# ==============================================
# ğŸ”¥ 4ë‹¨ê³„: í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ imports (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
# ==============================================

# âœ… ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê°€ìš©ì„± ë³€ìˆ˜
EXCEPTIONS_AVAILABLE = False
MOCK_DIAGNOSTIC_AVAILABLE = True

# ==============================================
# í†µí•©ëœ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
# ==============================================
try:
    from app.core.exceptions import (
        MyClosetAIException, 
        MockDataDetectionError, 
        DataQualityError, 
        ModelInferenceError,
        ModelLoadingError,
        ImageProcessingError,
        DataValidationError,
        ConfigurationError,
        detect_mock_data,
        log_detailed_error,
        create_mock_data_diagnosis_response,
        track_exception,
        get_error_summary,
        create_exception_response,
        convert_to_mycloset_exception,
        ErrorCodes
    )
    # error_trackerëŠ” ë³„ë„ë¡œ import
    try:
        from app.core.exceptions import error_tracker
    except ImportError:
        error_tracker = None
    EXCEPTIONS_AVAILABLE = True
    print("âœ… í†µí•© ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ í†µí•© ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì—†ìŒ - ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬ë§Œ ì‚¬ìš©")
    # í´ë°± í´ë˜ìŠ¤ë“¤
    class MyClosetAIException(Exception):
        pass
    class MockDataDetectionError(Exception):
        pass
    class DataQualityError(Exception):
        pass
    class ModelInferenceError(Exception):
        pass
    class ModelLoadingError(Exception):
        pass
    class ImageProcessingError(Exception):
        pass
    class DataValidationError(Exception):
        pass
    class ConfigurationError(Exception):
        pass
    
    # í´ë°± í•¨ìˆ˜ë“¤
    def detect_mock_data(data):
        return {'is_mock': False}
    def track_exception(error, context=None, step_id=None):
        """í´ë°± track_exception í•¨ìˆ˜"""
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Track Exception (í´ë°±): {error}")
        return None
    
    def get_error_summary():
        """í´ë°± get_error_summary í•¨ìˆ˜"""
        return {
            'total_errors': 0,
            'error_types': {},
            'recent_errors': [],
            'system_status': 'healthy'
        }
    
    def log_detailed_error(error, context, step_id):
        pass
    def create_exception_response(error, step_name, step_id, session_id):
        return {'success': False, 'error': str(error)}
    def convert_to_mycloset_exception(error, context):
        return MyClosetAIException(str(error))
    
    def create_mock_data_diagnosis_response(data, step_name, step_id, session_id):
        """í´ë°± create_mock_data_diagnosis_response í•¨ìˆ˜"""
        return {
            'success': False,
            'is_mock_data': True,
            'step_name': step_name,
            'step_id': step_id,
            'session_id': session_id,
            'diagnosis': {'is_mock': True, 'confidence': 0.0}
        }
    
    # error_tracker í´ë°±
    error_tracker = None
    
    class ErrorCodes:
        DATA_VALIDATION_FAILED = "DATA_VALIDATION_FAILED"
        MODEL_LOADING_FAILED = "MODEL_LOADING_FAILED"
        AI_INFERENCE_FAILED = "AI_INFERENCE_FAILED"
        CONFIGURATION_ERROR = "CONFIGURATION_ERROR"

# ==============================================
# Mock Data Diagnostic System
# ==============================================
try:
    from app.core.mock_data_diagnostic import (
        MockDataDiagnostic,
        diagnose_step_data,
        get_diagnostic_summary,
        diagnostic_decorator
    )
    MOCK_DIAGNOSTIC_AVAILABLE = True
    print("âœ… Mock ì§„ë‹¨ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ Mock ì§„ë‹¨ ì‹œìŠ¤í…œ ì—†ìŒ")
    # í´ë°± í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë“¤
    class MockDataDiagnostic:
        pass
    def diagnose_step_data(data):
        return {'is_mock': False}
    def get_diagnostic_summary():
        return {}
    def diagnostic_decorator(func):
        return func

# ==============================================
# ğŸ”¥ 5ë‹¨ê³„: Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

if TYPE_CHECKING:
    from app.core.di_container import CentralHubDIContainer
    from app.ai_pipeline.models.model_loader import ModelLoader
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            # í´ë°±: ìƒëŒ€ ê²½ë¡œ
            from .base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì…"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

# ==============================================
# ğŸ”¥ 6ë‹¨ê³„: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def get_available_libraries() -> Dict[str, bool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡ ë°˜í™˜"""
    return {
        'numpy': NUMPY_AVAILABLE,
        'torch': TORCH_AVAILABLE,
        'mps': MPS_AVAILABLE,
        'cuda': CUDA_AVAILABLE,
        'pil': PIL_AVAILABLE,
        'cv2': CV2_AVAILABLE,
        'scipy': SCIPY_AVAILABLE,
        'densecrf': DENSECRF_AVAILABLE,
        'skimage': SKIMAGE_AVAILABLE,
        'mediapipe': MEDIAPIPE_AVAILABLE,
        'ultralytics': ULTRALYTICS_AVAILABLE,
        'transformers': TRANSFORMERS_AVAILABLE,
        'sam': SAM_AVAILABLE,
        'safetensors': SAFETENSORS_AVAILABLE,
        'exceptions': EXCEPTIONS_AVAILABLE,
        'mock_diagnostic': MOCK_DIAGNOSTIC_AVAILABLE
    }

def log_library_status(logger: logging.Logger):
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœë¥¼ ë¡œê·¸ì— ì¶œë ¥"""
    libraries = get_available_libraries()
    logger.info("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:")
    for lib, available in libraries.items():
        status = "âœ…" if available else "âŒ"
        logger.info(f"  {status} {lib}")

def safe_copy(obj: Any) -> Any:
    """ì•ˆì „í•œ ë³µì‚¬ í•¨ìˆ˜ - DetailedDataSpec ì—ëŸ¬ í•´ê²°"""
    try:
        # ê¸°ë³¸ íƒ€ì…ë“¤ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if obj is None or isinstance(obj, (bool, int, float, str)):
            return obj
        
        # ë¦¬ìŠ¤íŠ¸ë‚˜ íŠœí”Œ
        elif isinstance(obj, (list, tuple)):
            return type(obj)(safe_copy(item) for item in obj)
        
        # ë”•ì…”ë„ˆë¦¬
        elif isinstance(obj, dict):
            return {key: safe_copy(value) for key, value in obj.items()}
        
        # ì§‘í•©
        elif isinstance(obj, set):
            return {safe_copy(item) for item in obj}
        
        # copy ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        else:
            try:
                import copy
                return copy.deepcopy(obj)
            except:
                try:
                    import copy
                    return copy.copy(obj)
                except:
                    # ë³µì‚¬í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°˜í™˜ (ì˜ˆ: í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±)
                    return obj
                    
    except Exception:
        # ëª¨ë“  ì‹¤íŒ¨ ì¼€ì´ìŠ¤ì—ì„œ ì›ë³¸ ë°˜í™˜
        return obj

def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max íŠ¹í™”)"""
    try:
        # Python GC
        gc.collect()
        
        # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except:
                    pass
            elif CUDA_AVAILABLE:
                try:
                    torch.cuda.empty_cache()
                except:
                    pass
                    
    except Exception:
        pass

def get_optimal_device():
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if TORCH_AVAILABLE:
        if MPS_AVAILABLE and IS_M3_MAX:
            return "mps"
        elif CUDA_AVAILABLE:
            return "cuda"
    return "cpu"

# ==============================================
# ğŸ”¥ 7ë‹¨ê³„: ê³µí†µ ìƒìˆ˜ë“¤
# ==============================================

# ë””ë°”ì´ìŠ¤ ìƒìˆ˜
DEVICE_CPU = "cpu"
DEVICE_CUDA = "cuda"
DEVICE_MPS = "mps"

# ê¸°ë³¸ ì„¤ì •ê°’ë“¤
DEFAULT_INPUT_SIZE = (512, 512)
DEFAULT_CONFIDENCE_THRESHOLD = 0.7
DEFAULT_QUALITY_THRESHOLD = 0.8
DEFAULT_BATCH_SIZE = 1

# Step ì •ë³´
STEP_NAMES = [
    "HumanParsingStep",
    "PoseEstimationStep", 
    "ClothSegmentationStep",
    "GeometricMatchingStep",
    "ClothWarpingStep",
    "VirtualFittingStep",
    "PostProcessingStep",
    "QualityAssessmentStep"
]

# ì—ëŸ¬ ë©”ì‹œì§€ í…œí”Œë¦¿
ERROR_TEMPLATES = {
    'model_loading': "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}",
    'inference': "ì¶”ë¡  ì‹¤íŒ¨: {step_name}",
    'preprocessing': "ì „ì²˜ë¦¬ ì‹¤íŒ¨: {step_name}",
    'postprocessing': "í›„ì²˜ë¦¬ ì‹¤íŒ¨: {step_name}",
    'validation': "ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {step_name}",
    'torch_unavailable': "PyTorchê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {error}",
    'device_error': "ë””ë°”ì´ìŠ¤ ì„¤ì • ì˜¤ë¥˜: {device}"
}

# COCO Keypoints (Pose Estimationìš©)
COCO_KEYPOINTS = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

# Human Parsing Classes (20 classes)
HUMAN_PARSING_CLASSES = {
    0: 'background', 1: 'hat', 2: 'hair', 3: 'glove', 4: 'sunglasses',
    5: 'upper_clothes', 6: 'dress', 7: 'coat', 8: 'socks', 9: 'pants',
    10: 'torso_skin', 11: 'scarf', 12: 'skirt', 13: 'face',
    14: 'left_arm', 15: 'right_arm', 16: 'left_leg', 17: 'right_leg',
    18: 'left_shoe', 19: 'right_shoe'
}

# ==============================================
# ğŸ”¥ 8ë‹¨ê³„: ì´ˆê¸°í™” ë° í™˜ê²½ ì •ë³´ ì¶œë ¥
# ==============================================

# í™˜ê²½ ì •ë³´ ì¶œë ¥
print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

# ì „ì—­ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
globals()['safe_copy'] = safe_copy

# ==============================================
# ğŸ”¥ 9ë‹¨ê³„: __all__ ì •ì˜ (ëª…ì‹œì  export)
# ==============================================

__all__ = [
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    'os', 'sys', 'gc', 'time', 'asyncio', 'logging', 'threading', 'traceback',
    'hashlib', 'json', 'base64', 'math', 'warnings', 'weakref', 'uuid',
    'subprocess', 'platform', 'datetime', 'timedelta',
    'Path', 'Dict', 'Any', 'Optional', 'Tuple', 'List', 'Union', 'Callable', 'TYPE_CHECKING', 'Set',
    'dataclass', 'field', 'Enum', 'IntEnum', 'BytesIO', 'StringIO', 'ThreadPoolExecutor',
    'lru_cache', 'wraps', 'partial', 'asynccontextmanager', 'contextmanager',
    'defaultdict', 'deque', 'chain', 'ABC', 'abstractmethod',
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    'np', 'torch', 'nn', 'F', 'DataLoader', 'autograd', 'autocast', 'transforms',
    'resize', 'to_pil_image', 'to_tensor',
    'Image', 'ImageEnhance', 'ImageFilter', 'ImageDraw', 'ImageFont', 'ImageOps',
    'cv2', 'scipy', 'ndimage', 'gaussian_filter', 'median_filter', 'convolve2d',
    'measure', 'morphology', 'segmentation', 'filters', 'restoration', 'exposure', 'ssim',
    'dcrf', 'unary_from_softmax', 'mp', 'YOLO', 'AutoModel', 'AutoTokenizer', 'sam', 'st',
    
    # ê°€ìš©ì„± ë³€ìˆ˜ë“¤
    'TORCH_AVAILABLE', 'MPS_AVAILABLE', 'CUDA_AVAILABLE', 'PIL_AVAILABLE', 'NUMPY_AVAILABLE',
    'CV2_AVAILABLE', 'SCIPY_AVAILABLE', 'SKIMAGE_AVAILABLE', 'DENSECRF_AVAILABLE',
    'MEDIAPIPE_AVAILABLE', 'ULTRALYTICS_AVAILABLE', 'TRANSFORMERS_AVAILABLE',
    'SAM_AVAILABLE', 'SAFETENSORS_AVAILABLE', 'EXCEPTIONS_AVAILABLE', 'MOCK_DIAGNOSTIC_AVAILABLE',
    
    # í™˜ê²½ ë³€ìˆ˜
    'IS_M3_MAX', 'CONDA_INFO', 'MEMORY_GB', 'DEVICE', 'TORCH_VERSION',
    
    # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    'MyClosetAIException', 'MockDataDetectionError', 'DataQualityError', 'ModelInferenceError',
    'ModelLoadingError', 'ImageProcessingError', 'DataValidationError', 'ConfigurationError',
    'error_tracker', 'detect_mock_data', 'log_detailed_error', 'create_mock_data_diagnosis_response',
    'track_exception', 'get_error_summary', 'create_exception_response', 'convert_to_mycloset_exception',
    'ErrorCodes',
    
    # Mock ì§„ë‹¨ ì‹œìŠ¤í…œ
    'MockDataDiagnostic', 'diagnose_step_data', 'get_diagnostic_summary', 'diagnostic_decorator',
    
    # Central Hub í•¨ìˆ˜ë“¤
    '_get_central_hub_container', 'get_base_step_mixin_class', '_inject_dependencies_safe',
    '_get_service_from_central_hub',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'detect_m3_max', 'detect_conda_env', 'get_available_libraries', 'log_library_status',
    'safe_copy', 'optimize_memory', 'get_optimal_device',
    
    # ê³µí†µ ìƒìˆ˜ë“¤
    'DEVICE_CPU', 'DEVICE_CUDA', 'DEVICE_MPS',
    'DEFAULT_INPUT_SIZE', 'DEFAULT_CONFIDENCE_THRESHOLD', 'DEFAULT_QUALITY_THRESHOLD', 'DEFAULT_BATCH_SIZE',
    'STEP_NAMES', 'ERROR_TEMPLATES', 'COCO_KEYPOINTS', 'HUMAN_PARSING_CLASSES'
]