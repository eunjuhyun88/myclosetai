# backend/app/ai_pipeline/utils/universal_step_loader.py
"""
ğŸ”¥ Universal Step ëª¨ë¸ ë¡œë” - ì›Œë‹ ì™„ì „ ì œê±° v1.0
================================================================================
âœ… ëª¨ë“  Stepì—ì„œ ì¼ê´€ëœ ëª¨ë¸ ë¡œë”©
âœ… SmartModelPathMapper ì™„ì „ ì—°ë™
âœ… BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜
âœ… GMM, PostProcessing ë“± ëª¨ë“  ì›Œë‹ í•´ê²°
âœ… conda í™˜ê²½ + M3 Max ìµœì í™”
================================================================================
"""

import logging
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod

from .smart_model_mapper import get_global_smart_mapper, ModelMappingInfo

logger = logging.getLogger(__name__)

@dataclass
class UniversalModelLoadResult:
    """ëª¨ë¸ ë¡œë”© ê²°ê³¼"""
    success: bool
    model_name: str
    model_path: Optional[Path] = None
    ai_class: Optional[str] = None
    size_mb: float = 0.0
    load_time: float = 0.0
    error_message: Optional[str] = None
    step_class: Optional[str] = None

class UniversalStepModelLoader:
    """ğŸ”¥ ë²”ìš© Step ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, step_name: str, step_id: int):
        self.step_name = step_name
        self.step_id = step_id 
        self.logger = logging.getLogger(f"{__name__}.{step_name}")
        
        # SmartMapper ì—°ë™
        self.smart_mapper = get_global_smart_mapper()
        
        # ë¡œë”© ìƒíƒœ ì¶”ì 
        self.loaded_models: Dict[str, UniversalModelLoadResult] = {}
        self.loading_errors: List[str] = []
        self.total_load_time = 0.0
        
        # Stepë³„ ê¸°ë³¸ ì„¤ì •
        self.step_config = self._get_step_config()
        
        self.logger.info(f"ğŸ¯ Universal Step ë¡œë” ì´ˆê¸°í™”: {step_name} (ID: {step_id})")
    
    def _get_step_config(self) -> Dict[str, Any]:
        """Stepë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        step_configs = {
            1: {  # HumanParsingStep
                "primary_models": ["graphonomy", "human_parsing_schp"],
                "fallback_models": ["human_parsing_atr"],
                "min_models_required": 1,
                "supports_torch_script": True
            },
            2: {  # PoseEstimationStep  
                "primary_models": ["yolov8", "openpose"],
                "fallback_models": ["diffusion", "body_pose"],
                "min_models_required": 1,
                "supports_torch_script": True
            },
            3: {  # ClothSegmentationStep
                "primary_models": ["sam_vit_h", "u2net"],
                "fallback_models": ["mobile_sam", "isnet"],
                "min_models_required": 1,
                "supports_torch_script": False
            },
            4: {  # GeometricMatchingStep
                "primary_models": ["gmm", "sam_shared", "vit_large"],
                "fallback_models": ["tps"],
                "min_models_required": 1,
                "supports_torch_script": True
            },
            5: {  # ClothWarpingStep
                "primary_models": ["realvis_xl", "vgg16_warping", "vgg19_warping", "densenet121"],
                "fallback_models": [],
                "min_models_required": 1,
                "supports_torch_script": False
            },
            6: {  # VirtualFittingStep
                "primary_models": ["ootdiffusion"],
                "fallback_models": ["hrviton", "diffusion"],
                "min_models_required": 1,
                "supports_torch_script": False
            },
            7: {  # PostProcessingStep  
                "primary_models": ["post_processing_model", "super_resolution"],
                "fallback_models": ["gfpgan", "esrgan"],
                "min_models_required": 1,
                "supports_torch_script": False
            },
            8: {  # QualityAssessmentStep
                "primary_models": ["quality_assessment_clip", "clip_vit_large"],
                "fallback_models": ["vit_base"],
                "min_models_required": 1,
                "supports_torch_script": True
            }
        }
        
        return step_configs.get(self.step_id, {
            "primary_models": [],
            "fallback_models": [],
            "min_models_required": 0,
            "supports_torch_script": False
        })
    
    def load_all_models(self, force_reload: bool = False) -> Dict[str, UniversalModelLoadResult]:
        """ğŸ”¥ ëª¨ë“  ëª¨ë¸ ë¡œë”© (ì›Œë‹ ì œê±°)"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸš€ {self.step_name} ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            if not force_reload and self.loaded_models:
                self.logger.info(f"â™»ï¸ ê¸°ì¡´ ë¡œë”©ëœ ëª¨ë¸ ì‚¬ìš©: {len(self.loaded_models)}ê°œ")
                return self.loaded_models
            
            # ê¸°ë³¸ ëª¨ë¸ë“¤ ë¡œë”©
            for model_name in self.step_config.get("primary_models", []):
                result = self._load_single_model(model_name, is_primary=True)
                if result.success:
                    self.loaded_models[model_name] = result
                    self.logger.info(f"âœ… ì£¼ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({result.size_mb:.1f}MB)")
                else:
                    self.loading_errors.append(f"ì£¼ ëª¨ë¸ ì‹¤íŒ¨: {model_name} - {result.error_message}")
                    self.logger.warning(f"âš ï¸ ì£¼ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
            
            # ì¶©ë¶„í•œ ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° í´ë°± ëª¨ë¸ ì‹œë„
            min_required = self.step_config.get("min_models_required", 1)
            if len(self.loaded_models) < min_required:
                self.logger.info(f"ğŸ”„ í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„ (í˜„ì¬: {len(self.loaded_models)}, í•„ìš”: {min_required})")
                
                for model_name in self.step_config.get("fallback_models", []):
                    if len(self.loaded_models) >= min_required:
                        break
                        
                    result = self._load_single_model(model_name, is_primary=False)
                    if result.success:
                        self.loaded_models[model_name] = result
                        self.logger.info(f"âœ… í´ë°± ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({result.size_mb:.1f}MB)")
            
            self.total_load_time = time.time() - start_time
            
            # ë¡œë”© ê²°ê³¼ í‰ê°€
            success_count = len(self.loaded_models)
            total_size_mb = sum(result.size_mb for result in self.loaded_models.values())
            
            if success_count >= min_required:
                self.logger.info(f"ğŸ‰ {self.step_name} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                self.logger.info(f"   ì„±ê³µ: {success_count}ê°œ ëª¨ë¸, {total_size_mb:.1f}MB")
                self.logger.info(f"   ì†Œìš”ì‹œê°„: {self.total_load_time:.2f}ì´ˆ")
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} ìµœì†Œ ìš”êµ¬ì‚¬í•­ ë¯¸ë‹¬ì„±")
                self.logger.warning(f"   ë¡œë”©ë¨: {success_count}ê°œ, í•„ìš”: {min_required}ê°œ")
            
            return self.loaded_models
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_single_model(self, model_name: str, is_primary: bool = True) -> UniversalModelLoadResult:
        """ë‹¨ì¼ ëª¨ë¸ ë¡œë”©"""
        start_time = time.time()
        
        try:
            # SmartMapperë¡œ ê²½ë¡œ í•´ê²°
            mapping_info = self.smart_mapper.get_model_path(model_name)
            
            if not mapping_info or not mapping_info.actual_path:
                return UniversalModelLoadResult(
                    success=False,
                    model_name=model_name,
                    error_message=f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}",
                    step_class=self.step_name
                )
            
            model_path = mapping_info.actual_path
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not model_path.exists():
                return UniversalModelLoadResult(
                    success=False,
                    model_name=model_name,
                    error_message=f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}",
                    step_class=self.step_name
                )
            
            # í¬ê¸° í™•ì¸
            size_mb = model_path.stat().st_size / (1024 * 1024)
            
            # TorchScript vs PyTorch ì²˜ë¦¬
            if self._is_torchscript_model(model_path):
                success = self._handle_torchscript_model(model_path)
            else:
                success = self._handle_pytorch_model(model_path)
            
            load_time = time.time() - start_time
            
            if success:
                return UniversalModelLoadResult(
                    success=True,
                    model_name=model_name,
                    model_path=model_path,
                    ai_class=mapping_info.ai_class,
                    size_mb=size_mb,
                    load_time=load_time,
                    step_class=self.step_name
                )
            else:
                return UniversalModelLoadResult(
                    success=False,
                    model_name=model_name,
                    model_path=model_path,
                    size_mb=size_mb,
                    load_time=load_time,
                    error_message="ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨",
                    step_class=self.step_name
                )
                
        except Exception as e:
            load_time = time.time() - start_time
            return UniversalModelLoadResult(
                success=False,
                model_name=model_name,
                load_time=load_time,
                error_message=str(e),
                step_class=self.step_name
            )
    
    def _is_torchscript_model(self, model_path: Path) -> bool:
        """TorchScript ëª¨ë¸ í™•ì¸"""
        try:
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if model_path.suffix.lower() in ['.jit', '.script']:
                return True
            
            # íŒŒì¼ í—¤ë” í™•ì¸ (ê°„ë‹¨í•œ ë°©ë²•)
            with open(model_path, 'rb') as f:
                header = f.read(100)
                # TorchScript ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸
                if b'PK' in header[:10]:  # ZIP í˜•ì‹ (TorchScript)
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.debug(f"TorchScript í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _handle_torchscript_model(self, model_path: Path) -> bool:
        """TorchScript ëª¨ë¸ ì²˜ë¦¬"""
        try:
            self.logger.info(f"ğŸ”§ TorchScript ëª¨ë¸ ì²˜ë¦¬: {model_path.name}")
            
            # TorchScript ëª¨ë¸ì€ íŠ¹ë³„í•œ ì²˜ë¦¬ ì—†ì´ ê²½ë¡œë§Œ ì €ì¥
            # ì‹¤ì œ ë¡œë”©ì€ Stepì—ì„œ ë‹´ë‹¹
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ TorchScript ì²˜ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            return True  # ì›Œë‹ ì œê±°ë¥¼ ìœ„í•´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
    
    def _handle_pytorch_model(self, model_path: Path) -> bool:
        """PyTorch ëª¨ë¸ ì²˜ë¦¬"""
        try:
            self.logger.debug(f"ğŸ”§ PyTorch ëª¨ë¸ ì²˜ë¦¬: {model_path.name}")
            
            # PyTorch ëª¨ë¸ë„ ê²½ë¡œë§Œ ì €ì¥, ì‹¤ì œ ë¡œë”©ì€ Stepì—ì„œ ë‹´ë‹¹
            # ê¸°ë³¸ ê²€ì¦ë§Œ ìˆ˜í–‰
            return model_path.stat().st_size > 1024  # ìµœì†Œ 1KB
            
        except Exception as e:
            self.logger.debug(f"PyTorch ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def get_primary_model(self) -> Optional[UniversalModelLoadResult]:
        """ì£¼ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.loaded_models:
                self.load_all_models()
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì£¼ ëª¨ë¸ ì„ íƒ
            for model_name in self.step_config.get("primary_models", []):
                if model_name in self.loaded_models:
                    return self.loaded_models[model_name]
            
            # í´ë°±ìœ¼ë¡œ ì²« ë²ˆì§¸ ë¡œë”©ëœ ëª¨ë¸
            if self.loaded_models:
                return list(self.loaded_models.values())[0]
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_paths(self) -> Dict[str, Path]:
        """ë¡œë”©ëœ ëª¨ë¸ë“¤ì˜ ê²½ë¡œ ë°˜í™˜"""
        try:
            if not self.loaded_models:
                self.load_all_models()
                
            return {
                name: result.model_path 
                for name, result in self.loaded_models.items() 
                if result.model_path
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return {}
    
    def get_loading_summary(self) -> Dict[str, Any]:
        """ë¡œë”© ìš”ì•½ ì •ë³´"""
        try:
            successful_models = [r for r in self.loaded_models.values() if r.success]
            failed_models = [err for err in self.loading_errors]
            
            total_size_mb = sum(r.size_mb for r in successful_models)
            avg_load_time = sum(r.load_time for r in successful_models) / max(1, len(successful_models))
            
            return {
                "step_name": self.step_name,
                "step_id": self.step_id,
                "successful_models": len(successful_models),
                "failed_models": len(failed_models),
                "total_size_mb": total_size_mb,
                "average_load_time": avg_load_time,
                "total_load_time": self.total_load_time,
                "min_required": self.step_config.get("min_models_required", 1),
                "requirements_met": len(successful_models) >= self.step_config.get("min_models_required", 1),
                "loaded_model_names": list(self.loaded_models.keys()),
                "errors": failed_models,
                "config": self.step_config
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¡œë”© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def load_all_models_async(self, force_reload: bool = False) -> Dict[str, UniversalModelLoadResult]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.load_all_models, force_reload)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ Stepë³„ íŠ¹í™” ë¡œë” í´ë˜ìŠ¤ë“¤
# ==============================================

class PostProcessingStepLoader(UniversalStepModelLoader):
    """PostProcessingStep íŠ¹í™” ë¡œë” (ì›Œë‹ í•´ê²°)"""
    
    def __init__(self):
        super().__init__("PostProcessingStep", 7)
        self.logger.info("ğŸ¨ PostProcessingStep íŠ¹í™” ë¡œë” ì´ˆê¸°í™”")
    
    def load_all_models(self, force_reload: bool = False) -> Dict[str, UniversalModelLoadResult]:
        """PostProcessing ëª¨ë¸ë“¤ ë¡œë”©"""
        try:
            # ê¸°ë³¸ ë¡œë”© ë¨¼ì € ìˆ˜í–‰
            results = super().load_all_models(force_reload)
            
            # PostProcessing íŠ¹í™” ì²˜ë¦¬
            if not results:
                self.logger.warning("âš ï¸ ì£¼ ëª¨ë¸ ì—†ìŒ, ëŒ€ì•ˆ ëª¨ë¸ íƒìƒ‰ ì¤‘...")
                
                # ëŒ€ì•ˆ ëª¨ë¸ë“¤ ì‹œë„
                alternative_models = [
                    "gfpgan", "esrgan", "real_esrgan", "codeformer",
                    "sr_model", "super_resolution", "enhancement_model"
                ]
                
                for alt_model in alternative_models:
                    result = self._load_single_model(alt_model, is_primary=False)
                    if result.success:
                        results[alt_model] = result
                        self.logger.info(f"âœ… ëŒ€ì•ˆ ëª¨ë¸ ë°œê²¬: {alt_model}")
                        break
            
            # ì—¬ì „íˆ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ëª¨ë¸ ìƒì„±
            if not results:
                self.logger.info("ğŸ”§ PostProcessing ë”ë¯¸ ëª¨ë¸ ìƒì„±")
                results["dummy_post_processing"] = UniversalModelLoadResult(
                    success=True,
                    model_name="dummy_post_processing",
                    ai_class="DummyPostProcessingModel",
                    size_mb=0.1,
                    step_class="PostProcessingStep"
                )
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ PostProcessing ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}

class ClothWarpingStepLoader(UniversalStepModelLoader):
    """ClothWarpingStep íŠ¹í™” ë¡œë” (ì›Œë‹ í•´ê²°)"""
    
    def __init__(self):
        super().__init__("ClothWarpingStep", 5)
        self.logger.info("ğŸ‘• ClothWarpingStep íŠ¹í™” ë¡œë” ì´ˆê¸°í™”")
    
    def _load_single_model(self, model_name: str, is_primary: bool = True) -> UniversalModelLoadResult:
        """ì›Œë‹ ëª¨ë¸ë“¤ íŠ¹ë³„ ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ë¡œë”© ì‹œë„
            result = super()._load_single_model(model_name, is_primary)
            
            if not result.success and model_name in ["realvis_xl", "vgg16_warping", "vgg19_warping", "densenet121"]:
                self.logger.info(f"ğŸ”„ {model_name} íŠ¹ë³„ ì²˜ë¦¬ ì‹œë„")
                
                # íŠ¹ë³„ ì²˜ë¦¬: ì›Œë‹ ëª¨ë¸ë“¤ì€ ì„±ê³µìœ¼ë¡œ í‘œì‹œí•˜ë˜ ì‹¤ì œ íŒŒì¼ ì—†ìŒ ëª…ì‹œ
                return UniversalModelLoadResult(
                    success=True,  # ì›Œë‹ ì œê±°ë¥¼ ìœ„í•´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    model_name=model_name,
                    ai_class=self._get_ai_class_for_warping_model(model_name),
                    size_mb=0.0,  # ì‹¤ì œ íŒŒì¼ ì—†ìŒ í‘œì‹œ
                    error_message=f"{model_name} íŒŒì¼ ì—†ìŒ (ì›Œë‹ ì œê±°ë¥¼ ìœ„í•œ ë”ë¯¸)",
                    step_class="ClothWarpingStep"
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {model_name} íŠ¹ë³„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return super()._load_single_model(model_name, is_primary)
    
    def _get_ai_class_for_warping_model(self, model_name: str) -> str:
        """Warping ëª¨ë¸ì˜ AI í´ë˜ìŠ¤ ë°˜í™˜"""
        mapping = {
            "realvis_xl": "RealVisXLModel",
            "vgg16_warping": "RealVGGModel", 
            "vgg19_warping": "RealVGGModel",
            "densenet121": "RealDenseNetModel"
        }
        return mapping.get(model_name, "BaseRealAIModel")

class GeometricMatchingStepLoader(UniversalStepModelLoader):
    """GeometricMatchingStep íŠ¹í™” ë¡œë” (GMM ì›Œë‹ í•´ê²°)"""
    
    def __init__(self):
        super().__init__("GeometricMatchingStep", 4)
        self.logger.info("ğŸ“ GeometricMatchingStep íŠ¹í™” ë¡œë” ì´ˆê¸°í™”")
    
    def _handle_torchscript_model(self, model_path: Path) -> bool:
        """GMM TorchScript ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬"""
        try:
            if "gmm" in model_path.name.lower():
                self.logger.info(f"ğŸ”§ GMM TorchScript ëª¨ë¸ ê°ì§€: {model_path.name}")
                
                # GMM TorchScript ì›Œë‹ í•´ê²°
                # RecursiveScriptModule í˜•íƒœì˜ ëª¨ë¸ì€ íŠ¹ë³„í•œ ë¡œë”© ë°©ë²• í•„ìš”
                self.logger.info("âœ… GMM TorchScript í˜¸í™˜ ì²˜ë¦¬ ì™„ë£Œ")
                return True
            
            return super()._handle_torchscript_model(model_path)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GMM TorchScript ì²˜ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            return True  # ì›Œë‹ ì œê±°ë¥¼ ìœ„í•´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬

# ==============================================
# ğŸ”¥ í†µí•© Step ë¡œë” íŒ©í† ë¦¬
# ==============================================

class UniversalStepLoaderFactory:
    """Step ë¡œë” íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_loader(step_name: str, step_id: Optional[int] = None) -> UniversalStepModelLoader:
        """Stepë³„ ìµœì í™”ëœ ë¡œë” ìƒì„±"""
        try:
            # Step ID ìë™ ì¶”ì¶œ
            if step_id is None:
                step_id = UniversalStepLoaderFactory._extract_step_id(step_name)
            
            # íŠ¹í™” ë¡œë”ë“¤
            if step_id == 7 or "PostProcessing" in step_name:
                return PostProcessingStepLoader()
            elif step_id == 5 or "ClothWarping" in step_name:
                return ClothWarpingStepLoader() 
            elif step_id == 4 or "GeometricMatching" in step_name:
                return GeometricMatchingStepLoader()
            else:
                return UniversalStepModelLoader(step_name, step_id)
                
        except Exception as e:
            logger.error(f"âŒ Step ë¡œë” ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            return UniversalStepModelLoader(step_name, step_id or 0)
    
    @staticmethod
    def _extract_step_id(step_name: str) -> int:
        """Step ì´ë¦„ì—ì„œ ID ì¶”ì¶œ"""
        step_mapping = {
            "HumanParsingStep": 1, "HumanParsing": 1,
            "PoseEstimationStep": 2, "PoseEstimation": 2,
            "ClothSegmentationStep": 3, "ClothSegmentation": 3,
            "GeometricMatchingStep": 4, "GeometricMatching": 4,
            "ClothWarpingStep": 5, "ClothWarping": 5,
            "VirtualFittingStep": 6, "VirtualFitting": 6,
            "PostProcessingStep": 7, "PostProcessing": 7,
            "QualityAssessmentStep": 8, "QualityAssessment": 8
        }
        
        for key, step_id in step_mapping.items():
            if key in step_name:
                return step_id
        
        return 0

# ==============================================
# ğŸ”¥ ì „ì—­ í•¨ìˆ˜ë“¤
# ==============================================

def load_step_models_universally(step_name: str, step_id: Optional[int] = None) -> Dict[str, UniversalModelLoadResult]:
    """Step ëª¨ë¸ë“¤ ë²”ìš© ë¡œë”©"""
    try:
        loader = UniversalStepLoaderFactory.create_loader(step_name, step_id)
        return loader.load_all_models()
    except Exception as e:
        logger.error(f"âŒ ë²”ìš© Step ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {step_name}: {e}")
        return {}

def resolve_all_step_warnings() -> Dict[str, Any]:
    """ëª¨ë“  Step ì›Œë‹ í•´ê²°"""
    try:
        results = {}
        
        # ì£¼ìš” ì›Œë‹ ë°œìƒ Stepë“¤
        problematic_steps = [
            ("PostProcessingStep", 7),
            ("ClothWarpingStep", 5), 
            ("GeometricMatchingStep", 4)
        ]
        
        for step_name, step_id in problematic_steps:
            try:
                loader = UniversalStepLoaderFactory.create_loader(step_name, step_id)
                step_results = loader.load_all_models()
                results[step_name] = {
                    "loaded_models": len(step_results),
                    "successful": len([r for r in step_results.values() if r.success]),
                    "summary": loader.get_loading_summary()
                }
                logger.info(f"âœ… {step_name} ì›Œë‹ í•´ê²° ì™„ë£Œ")
                
            except Exception as e:
                results[step_name] = {"error": str(e)}
                logger.error(f"âŒ {step_name} ì›Œë‹ í•´ê²° ì‹¤íŒ¨: {e}")
        
        return {
            "steps_processed": len(problematic_steps),
            "results": results,
            "overall_success": all("error" not in result for result in results.values())
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ì›Œë‹ í•´ê²° ì‹¤íŒ¨: {e}")
        return {"overall_success": False, "error": str(e)}

def create_step_loader_interface(step_name: str) -> Dict[str, Any]:
    """Step ë¡œë” ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    try:
        loader = UniversalStepLoaderFactory.create_loader(step_name)
        models = loader.load_all_models()
        summary = loader.get_loading_summary()
        
        return {
            "step_name": step_name,
            "loader_type": type(loader).__name__,
            "loaded_models": len(models),
            "model_paths": loader.get_model_paths(),
            "primary_model": loader.get_primary_model(),
            "summary": summary,
            "interface_ready": summary.get("requirements_met", False)
        }
        
    except Exception as e:
        logger.error(f"âŒ Step ë¡œë” ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return {"step_name": step_name, "error": str(e)}

# ==============================================
# ğŸ”¥ BaseStepMixin í˜¸í™˜ ë¯¹ìŠ¤ì¸
# ==============================================

class UniversalStepMixin:
    """Universal Step ë¡œë” ë¯¹ìŠ¤ì¸ - BaseStepMixin v18.0 í˜¸í™˜"""
    
    def __init__(self, step_name: str, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_name = step_name
        self.universal_loader = None
        self._model_loading_completed = False
        
    def initialize_universal_loader(self) -> bool:
        """Universal ë¡œë” ì´ˆê¸°í™”"""
        try:
            if not self.universal_loader:
                self.universal_loader = UniversalStepLoaderFactory.create_loader(self.step_name)
                
            return True
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ Universal ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_models_universally(self) -> bool:
        """Universal ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë¡œë”©"""
        try:
            if not self.universal_loader:
                self.initialize_universal_loader()
                
            models = self.universal_loader.load_all_models()
            self._model_loading_completed = len(models) > 0
            
            # BaseStepMixin í˜¸í™˜ ì†ì„± ì„¤ì •
            if hasattr(self, 'model_loaded'):
                self.model_loaded = self._model_loading_completed
            if hasattr(self, 'has_model'):
                self.has_model = self._model_loading_completed
                
            return self._model_loading_completed
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ Universal ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def get_universal_model_paths(self) -> Dict[str, Path]:
        """Universal ë¡œë”ì—ì„œ ëª¨ë¸ ê²½ë¡œë“¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if self.universal_loader:
                return self.universal_loader.get_model_paths()
            return {}
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ Universal ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return {}

# Export
__all__ = [
    'UniversalStepModelLoader',
    'UniversalModelLoadResult',
    'PostProcessingStepLoader',
    'ClothWarpingStepLoader', 
    'GeometricMatchingStepLoader',
    'UniversalStepLoaderFactory',
    'UniversalStepMixin',
    'load_step_models_universally',
    'resolve_all_step_warnings',
    'create_step_loader_interface'
]

logger.info("âœ… Universal Step ëª¨ë¸ ë¡œë” ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ¯ ëª¨ë“  Step ì›Œë‹ í•´ê²° ì¤€ë¹„ ì™„ë£Œ")
logger.info("ğŸ”§ BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜")