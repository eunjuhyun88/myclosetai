# backend/app/ai_pipeline/utils/pytorch_safe_ops.py
"""
ğŸ”¥ PyTorch Safe Operations v2.0 - M3 Max ìµœì í™” ì™„ì„±íŒ
=========================================================
âœ… M3 Max MPS ì™„ë²½ ìµœì í™” - 128GB í†µí•© ë©”ëª¨ë¦¬ í™œìš©
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì› - ì•ˆì •ì„± ê·¹ëŒ€í™”
âœ… ì•ˆì „í•œ PyTorch ì—°ì‚° - í´ë°± ë° ì˜¤ë¥˜ ì²˜ë¦¬ ì™„ë²½
âœ… í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ìµœì í™” - Pose/Human Parsingìš©
âœ… ì´ë¯¸ì§€ ë³€í™˜ ìµœì í™” - conda PIL ìš°ì„ 
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ì™„ë²½ - MPS ìºì‹œ ìë™ ì •ë¦¬
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± - ëª¨ë“  ì—£ì§€ì¼€ì´ìŠ¤ ì²˜ë¦¬
âœ… Step íŒŒì¼ë“¤ 100% ì§€ì› - ëª¨ë“  í•„ìˆ˜ í•¨ìˆ˜ ì œê³µ
âœ… MPS Border Padding ëª¨ë“œ í˜¸í™˜ì„± íŒ¨ì¹˜ ì¶”ê°€

í•µì‹¬ ì² í•™:
- ì•ˆì „í•¨ì´ ìµœìš°ì„  (Safety First)
- M3 Max ì„±ëŠ¥ ê·¹ëŒ€í™”
- conda í™˜ê²½ ì™„ë²½ í˜¸í™˜
- í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì™„ë²½ êµ¬ë¹„

Author: MyCloset AI Team  
Date: 2025-07-22
Version: 2.1 (M3 Max Optimized Complete + MPS Padding Fix)
"""

import os
import gc
import time
import logging
import traceback
import platform
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Callable
from dataclasses import dataclass
from functools import wraps, lru_cache
from contextlib import contextmanager
import warnings

# ==============================================
# ğŸ”¥ 1. ê¸°ë³¸ ì„¤ì • ë° ë¡œê¹…
# ==============================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ê²½ê³  í•„í„°ë§ (conda í™˜ê²½ì—ì„œ ë¶ˆí•„ìš”í•œ ê²½ê³  ì œê±°)
warnings.filterwarnings('ignore', category=UserWarning, module='torch')
warnings.filterwarnings('ignore', category=FutureWarning, module='torch')

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° M3 Max ê°ì§€
# ==============================================

@lru_cache(maxsize=1)
def detect_conda_environment() -> Dict[str, str]:
    """conda í™˜ê²½ ê°ì§€ ë° ì •ë³´ ìˆ˜ì§‘"""
    return {
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
        'python_path': os.path.dirname(os.__file__)
    }

@lru_cache(maxsize=1)
def detect_m3_max() -> bool:
    """M3 Max ì¹©ì…‹ ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout.upper()
    except Exception:
        pass
    return False

# í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
CONDA_INFO = detect_conda_environment()
IS_M3_MAX = detect_m3_max()
IS_CONDA_ENV = CONDA_INFO['conda_env'] != 'none'

# ==============================================
# ğŸ”¥ 3. MPS Border Padding ëª¨ë“œ í˜¸í™˜ì„± íŒ¨ì¹˜
# ==============================================

def apply_mps_padding_patch():
    """MPS Border Padding ëª¨ë“œ í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©"""
    try:
        import torch
        import torch.nn.functional as F
        
        if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
            return False
        
        # ì›ë³¸ F.pad í•¨ìˆ˜ ì €ì¥
        if not hasattr(F, '_original_pad'):
            F._original_pad = F.pad
        
        def safe_pad(input, pad, mode='constant', value=0):
            """MPS í˜¸í™˜ ì•ˆì „í•œ padding í•¨ìˆ˜"""
            try:
                # MPSì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” Border padding ëª¨ë“œë¥¼ constantë¡œ ëŒ€ì²´
                if mode == 'border':
                    mode = 'constant'
                    logger.debug("ğŸ”„ MPS Border paddingì„ constantë¡œ ëŒ€ì²´")
                
                # MPSì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” reflect padding ëª¨ë“œë¥¼ constantë¡œ ëŒ€ì²´
                if mode == 'reflect':
                    mode = 'constant'
                    logger.debug("ğŸ”„ MPS Reflect paddingì„ constantë¡œ ëŒ€ì²´")
                
                return F._original_pad(input, pad, mode=mode, value=value)
                
            except Exception as e:
                if "Unsupported Border padding mode" in str(e) or "Unsupported padding mode" in str(e):
                    logger.warning(f"âš ï¸ MPS padding ëª¨ë“œ ì˜¤ë¥˜ ê°ì§€, constantë¡œ ëŒ€ì²´: {e}")
                    return F._original_pad(input, pad, mode='constant', value=value)
                else:
                    raise e
        
        # íŒ¨ì¹˜ ì ìš©
        F.pad = safe_pad
        logger.info("âœ… MPS Border Padding ëª¨ë“œ í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.warning(f"âš ï¸ MPS Padding íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False

def apply_mps_conv_padding_patch():
    """MPS Conv2d Padding í˜¸í™˜ì„± íŒ¨ì¹˜"""
    try:
        import torch
        import torch.nn as nn
        
        if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
            return False
        
        # ì›ë³¸ Conv2d ì €ì¥
        if not hasattr(nn, '_original_Conv2d'):
            nn._original_Conv2d = nn.Conv2d
        
        class SafeConv2d(nn._original_Conv2d):
            """MPS í˜¸í™˜ ì•ˆì „í•œ Conv2d"""
            
            def forward(self, input):
                try:
                    return super().forward(input)
                except Exception as e:
                    if "Unsupported Border padding mode" in str(e) or "Unsupported padding mode" in str(e):
                        logger.warning(f"âš ï¸ MPS Conv2d padding ì˜¤ë¥˜ ê°ì§€, íŒ¨ë”© ëª¨ë“œ ì¡°ì •: {e}")
                        # íŒ¨ë”© ëª¨ë“œë¥¼ 'zeros'ë¡œ ê°•ì œ ë³€ê²½
                        self.padding_mode = 'zeros'
                        return super().forward(input)
                    else:
                        raise e
        
        # íŒ¨ì¹˜ ì ìš©
        nn.Conv2d = SafeConv2d
        logger.info("âœ… MPS Conv2d Padding í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.warning(f"âš ï¸ MPS Conv2d íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False

# íŒ¨ì¹˜ ìë™ ì ìš©
MPS_PADDING_PATCH_APPLIED = apply_mps_padding_patch()
MPS_CONV_PADDING_PATCH_APPLIED = apply_mps_conv_padding_patch()

# ==============================================
# ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì
# ==============================================

class LibraryManager:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ ë¡œë”© ë° í˜¸í™˜ì„± ê´€ë¦¬"""
    
    def __init__(self):
        self.torch_available = False
        self.mps_available = False
        self.numpy_available = False
        self.pil_available = False
        self.device_type = "cpu"
        self.torch_version = ""
        
        self._initialize_libraries()
    
    def _initialize_libraries(self):
        """ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” ë° ìµœì í™” ì„¤ì •"""
        # PyTorch MPS í™˜ê²½ ë³€ìˆ˜ ì‚¬ì „ ì„¤ì • (M3 Max ìµœì í™”)
        if IS_M3_MAX:
            os.environ.update({
                'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                'METAL_DEVICE_WRAPPER_TYPE': '1',
                'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                'OMP_NUM_THREADS': '16',  # M3 Max 16ì½”ì–´ í™œìš©
            })
        
        # NumPy ë¡œë”© (conda ìš°ì„ )
        self._load_numpy()
        
        # PyTorch ë¡œë”© (conda ìš°ì„ , MPS ìµœì í™”)
        self._load_pytorch()
        
        # PIL ë¡œë”© (conda ìš°ì„ )
        self._load_pil()
        
        # MPS íŒ¨ì¹˜ ìƒíƒœ ë¡œê¹…
        if self.mps_available:
            logger.info(f"ğŸ MPS ì‚¬ìš© ê°€ëŠ¥ - Padding íŒ¨ì¹˜: {MPS_PADDING_PATCH_APPLIED}, Conv íŒ¨ì¹˜: {MPS_CONV_PADDING_PATCH_APPLIED}")
    
    def _load_numpy(self):
        """NumPy ì•ˆì „ ë¡œë”©"""
        try:
            import numpy as np
            self.numpy_available = True
            globals()['np'] = np
            logger.info("âœ… NumPy ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)" if IS_CONDA_ENV else "âœ… NumPy ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            self.numpy_available = False
            logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")
    
    def _load_pytorch(self):
        """PyTorch ì•ˆì „ ë¡œë”© ë° MPS ì„¤ì •"""
        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.torch_version = torch.__version__
            
            # ê¸€ë¡œë²Œ ìŠ¤ì½”í”„ì— ì¶”ê°€
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
            # ì¥ì¹˜ ì„¤ì •
            self._setup_device()
            
            # M3 Max íŠ¹í™” ìµœì í™”
            if IS_M3_MAX and self.mps_available:
                self._optimize_for_m3_max()
            
            logger.info(f"âœ… PyTorch {self.torch_version} ë¡œë“œ ì™„ë£Œ (Device: {self.device_type})")
            
        except ImportError:
            self.torch_available = False
            logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch")
    
    def _setup_device(self):
        """ìµœì  ì¥ì¹˜ ì„¤ì •"""
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.mps_available = True
            self.device_type = "mps"
            logger.info("ğŸ MPS ë°±ì—”ë“œ í™œì„±í™” (M3 Max ìµœì í™”)")
        elif torch.cuda.is_available():
            self.device_type = "cuda"
            logger.info("ğŸ”¥ CUDA ë°±ì—”ë“œ í™œì„±í™”")
        else:
            self.device_type = "cpu"
            logger.info("ğŸ’» CPU ë°±ì—”ë“œ ì‚¬ìš©")
    
    def _optimize_for_m3_max(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì„¤ì •"""
        try:
            # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™” (M3 Max 16ì½”ì–´)
            torch.set_num_threads(16)
            
            # MPS ìºì‹œ ì´ˆê¸°í™”
            if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            
            logger.info("ğŸ M3 Max ìµœì í™” ì™„ë£Œ (16 threads, MPS cache cleared)")
            
        except Exception as e:
            logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì¼ë¶€ ì‹¤íŒ¨: {e}")
    
    def _load_pil(self):
        """PIL ì•ˆì „ ë¡œë”©"""
        try:
            from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
            self.pil_available = True
            
            # ê¸€ë¡œë²Œ ìŠ¤ì½”í”„ì— ì¶”ê°€
            globals()['Image'] = Image
            globals()['ImageEnhance'] = ImageEnhance
            globals()['ImageFilter'] = ImageFilter
            globals()['ImageOps'] = ImageOps
            globals()['ImageDraw'] = ImageDraw
            globals()['ImageFont'] = ImageFont
            
            logger.info("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)" if IS_CONDA_ENV else "ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
            
        except ImportError:
            self.pil_available = False
            logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
_lib_manager = LibraryManager()

# ì „ì—­ ìƒìˆ˜ ì„¤ì •
TORCH_AVAILABLE = _lib_manager.torch_available
MPS_AVAILABLE = _lib_manager.mps_available
NUMPY_AVAILABLE = _lib_manager.numpy_available
PIL_AVAILABLE = _lib_manager.pil_available
DEFAULT_DEVICE = _lib_manager.device_type
TORCH_VERSION = _lib_manager.torch_version

# ==============================================
# ğŸ”¥ 5. ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™” í•¨ìˆ˜ë“¤
# ==============================================

def safe_mps_empty_cache() -> bool:
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (M3 Max ìµœì í™”)"""
    if not (TORCH_AVAILABLE and MPS_AVAILABLE):
        return False
    
    try:
        # PyTorch 2.x ìŠ¤íƒ€ì¼
        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
            return True
        
        # PyTorch 1.x ìŠ¤íƒ€ì¼
        elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
                return True
        
        return False
        
    except (AttributeError, RuntimeError) as e:
        logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ì •ìƒ): {e}")
        return False

def safe_torch_cleanup() -> Dict[str, bool]:
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬"""
    results = {
        'gc_collected': False,
        'cuda_cleared': False,
        'mps_cleared': False,
        'success': False
    }
    
    try:
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        results['gc_collected'] = collected > 0
        
        if TORCH_AVAILABLE:
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                results['cuda_cleared'] = True
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max)
            if MPS_AVAILABLE:
                results['mps_cleared'] = safe_mps_empty_cache()
        
        results['success'] = True
        logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {results}")
        
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        results['error'] = str(e)
    
    return results

def get_memory_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ (M3 Max íŠ¹í™”)"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        
        info = {
            "total_gb": round(memory.total / (1024**3), 2),
            "available_gb": round(memory.available / (1024**3), 2),
            "used_gb": round(memory.used / (1024**3), 2),
            "percent": round(memory.percent, 1),
            "is_m3_max": IS_M3_MAX,
            "device_type": DEFAULT_DEVICE,
            "conda_env": CONDA_INFO['conda_env']
        }
        
        # PyTorch ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                # MPSëŠ” í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©
                info["mps_unified_memory"] = True
                info["mps_available"] = True
            elif torch.cuda.is_available():
                info["cuda_memory_allocated"] = torch.cuda.memory_allocated() / (1024**3)
                info["cuda_memory_cached"] = torch.cuda.memory_reserved() / (1024**3)
        
        return info
        
    except ImportError:
        # psutil ì—†ì„ ê²½ìš° ì¶”ì •ê°’ ë°˜í™˜
        return {
            "total_gb": 128.0 if IS_M3_MAX else 16.0,
            "available_gb": 100.0 if IS_M3_MAX else 12.0,
            "used_gb": 28.0 if IS_M3_MAX else 4.0,
            "percent": 22.0 if IS_M3_MAX else 25.0,
            "is_m3_max": IS_M3_MAX,
            "device_type": DEFAULT_DEVICE,
            "conda_env": CONDA_INFO['conda_env'],
            "estimated": True
        }

@contextmanager
def memory_efficient_context():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    initial_memory = get_memory_info()
    
    try:
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì‹œì‘
        safe_torch_cleanup()
        yield
    finally:
        # ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
        final_memory = get_memory_info()
        safe_torch_cleanup()
        
        logger.debug(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™”: "
                    f"{initial_memory['used_gb']:.1f}GB â†’ "
                    f"{final_memory['used_gb']:.1f}GB")

# ==============================================
# ğŸ”¥ 6. ì•ˆì „í•œ PyTorch ì—°ì‚° í•¨ìˆ˜ë“¤
# ==============================================

def safe_max(tensor: Any, dim: Optional[int] = None, keepdim: bool = False) -> Any:
    """ì•ˆì „í•œ torch.max ì—°ì‚° (MPS ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - NumPy í´ë°± ì‚¬ìš©")
        if NUMPY_AVAILABLE and hasattr(tensor, 'numpy'):
            np_array = tensor.numpy() if hasattr(tensor, 'numpy') else tensor
            return np.max(np_array, axis=dim, keepdims=keepdim)
        return tensor
    
    try:
        # ì¥ì¹˜ë³„ ìµœì í™”
        if hasattr(tensor, 'device'):
            if tensor.device.type == 'mps':
                # MPSì—ì„œ ì•ˆì „í•œ ì—°ì‚°
                with torch.no_grad():
                    result = torch.max(tensor, dim=dim, keepdim=keepdim)
                    return result
            elif tensor.device.type == 'cuda':
                # CUDA ìµœì í™”
                return torch.max(tensor, dim=dim, keepdim=keepdim)
        
        # ì¼ë°˜ì ì¸ ê²½ìš°
        return torch.max(tensor, dim=dim, keepdim=keepdim)
        
    except (RuntimeError, AttributeError) as e:
        logger.warning(f"safe_max ì‹¤íŒ¨, CPU í´ë°± ì‚¬ìš©: {e}")
        try:
            if hasattr(tensor, 'cpu'):
                cpu_tensor = tensor.cpu()
                return torch.max(cpu_tensor, dim=dim, keepdim=keepdim)
            else:
                return torch.max(tensor, dim=dim, keepdim=keepdim)
        except Exception as e2:
            logger.error(f"safe_max CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
            return tensor

def safe_amax(tensor: Any, dim: Optional[Union[int, List[int]]] = None, keepdim: bool = False) -> Any:
    """ì•ˆì „í•œ torch.amax ì—°ì‚° (MPS ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - NumPy í´ë°± ì‚¬ìš©")
        if NUMPY_AVAILABLE and hasattr(tensor, 'numpy'):
            np_array = tensor.numpy() if hasattr(tensor, 'numpy') else tensor
            return np.amax(np_array, axis=dim, keepdims=keepdim)
        return tensor
    
    try:
        # torch.amax ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if hasattr(torch, 'amax'):
            return torch.amax(tensor, dim=dim, keepdim=keepdim)
        else:
            # êµ¬ë²„ì „ PyTorch - torch.max ì‚¬ìš©
            if dim is None:
                return torch.max(tensor)
            else:
                values, _ = torch.max(tensor, dim=dim, keepdim=keepdim)
                return values
                
    except (RuntimeError, AttributeError) as e:
        logger.warning(f"safe_amax ì‹¤íŒ¨, safe_max í´ë°±: {e}")
        return safe_max(tensor, dim=dim, keepdim=keepdim)

def safe_argmax(tensor: Any, dim: Optional[int] = None, keepdim: bool = False) -> Any:
    """ì•ˆì „í•œ torch.argmax ì—°ì‚° (MPS ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - NumPy í´ë°± ì‚¬ìš©")
        if NUMPY_AVAILABLE and hasattr(tensor, 'numpy'):
            np_array = tensor.numpy() if hasattr(tensor, 'numpy') else tensor
            return np.argmax(np_array, axis=dim, keepdims=keepdim)
        return tensor
    
    try:
        # MPSì—ì„œ ì•ˆì „í•œ ì—°ì‚°
        if hasattr(tensor, 'device') and tensor.device.type == 'mps':
            with torch.no_grad():
                return torch.argmax(tensor, dim=dim, keepdim=keepdim)
        
        return torch.argmax(tensor, dim=dim, keepdim=keepdim)
        
    except (RuntimeError, AttributeError) as e:
        logger.warning(f"safe_argmax ì‹¤íŒ¨, CPU í´ë°±: {e}")
        try:
            if hasattr(tensor, 'cpu'):
                cpu_tensor = tensor.cpu()
                return torch.argmax(cpu_tensor, dim=dim, keepdim=keepdim)
            else:
                return torch.argmax(tensor, dim=dim, keepdim=keepdim)
        except Exception as e2:
            logger.error(f"safe_argmax CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
            return tensor

def safe_softmax(tensor: Any, dim: int = -1) -> Any:
    """ì•ˆì „í•œ softmax ì—°ì‚° (MPS ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - ì…ë ¥ í…ì„œ ë°˜í™˜")
        return tensor
    
    try:
        # MPS ìµœì í™”
        if hasattr(tensor, 'device') and tensor.device.type == 'mps':
            with torch.no_grad():
                return F.softmax(tensor, dim=dim)
        
        return F.softmax(tensor, dim=dim)
        
    except (RuntimeError, AttributeError) as e:
        logger.warning(f"safe_softmax ì‹¤íŒ¨, CPU í´ë°±: {e}")
        try:
            if hasattr(tensor, 'cpu'):
                cpu_tensor = tensor.cpu()
                result = F.softmax(cpu_tensor, dim=dim)
                # ì›ë˜ ì¥ì¹˜ë¡œ ë‹¤ì‹œ ì´ë™
                if hasattr(tensor, 'device'):
                    return result.to(tensor.device)
                return result
            else:
                return F.softmax(tensor, dim=dim)
        except Exception as e2:
            logger.error(f"safe_softmax CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
            return tensor

def safe_interpolate(input_tensor: Any, size: Optional[Tuple[int, int]] = None, 
                    scale_factor: Optional[float] = None, mode: str = 'bilinear', 
                    align_corners: bool = False) -> Any:
    """ì•ˆì „í•œ interpolation ì—°ì‚° (MPS ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - ì…ë ¥ í…ì„œ ë°˜í™˜")
        return input_tensor
    
    try:
        # MPS ìµœì í™”
        if hasattr(input_tensor, 'device') and input_tensor.device.type == 'mps':
            with torch.no_grad():
                return F.interpolate(input_tensor, size=size, scale_factor=scale_factor, 
                                   mode=mode, align_corners=align_corners)
        
        return F.interpolate(input_tensor, size=size, scale_factor=scale_factor, 
                           mode=mode, align_corners=align_corners)
        
    except (RuntimeError, AttributeError) as e:
        logger.warning(f"safe_interpolate ì‹¤íŒ¨, CPU í´ë°±: {e}")
        try:
            if hasattr(input_tensor, 'cpu'):
                cpu_tensor = input_tensor.cpu()
                result = F.interpolate(cpu_tensor, size=size, scale_factor=scale_factor, 
                                     mode=mode, align_corners=align_corners)
                # ì›ë˜ ì¥ì¹˜ë¡œ ë‹¤ì‹œ ì´ë™
                if hasattr(input_tensor, 'device'):
                    return result.to(input_tensor.device)
                return result
            else:
                return F.interpolate(input_tensor, size=size, scale_factor=scale_factor, 
                                   mode=mode, align_corners=align_corners)
        except Exception as e2:
            logger.error(f"safe_interpolate CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
            return input_tensor

# ==============================================
# ğŸ”¥ 7. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ìµœì í™” í•¨ìˆ˜ë“¤
# ==============================================

def extract_keypoints_from_heatmaps(heatmaps: Any, threshold: float = 0.1) -> List[Tuple[int, int, float]]:
    """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (M3 Max ìµœì í™”)"""
    if not TORCH_AVAILABLE:
        logger.warning("PyTorch ì—†ìŒ - ë¹ˆ í‚¤í¬ì¸íŠ¸ ë°˜í™˜")
        return []
    
    try:
        keypoints = []
        
        # ì…ë ¥ ê²€ì¦
        if not hasattr(heatmaps, 'shape') or len(heatmaps.shape) < 3:
            logger.warning("ì˜ëª»ëœ íˆíŠ¸ë§µ í˜•íƒœ")
            return []
        
        # ë°°ì¹˜ ì°¨ì›ì´ ìˆëŠ” ê²½ìš° ì œê±°
        if len(heatmaps.shape) == 4:
            heatmaps = heatmaps[0]  # [1, C, H, W] -> [C, H, W]
        
        num_keypoints = heatmaps.shape[0]
        
        with memory_efficient_context():
            for i in range(num_keypoints):
                heatmap = heatmaps[i]
                
                # ìµœëŒ€ê°’ê³¼ ìœ„ì¹˜ ì°¾ê¸° (ì•ˆì „í•œ ì—°ì‚° ì‚¬ìš©)
                max_val = safe_amax(heatmap)
                if max_val < threshold:
                    keypoints.append((0, 0, 0.0))
                    continue
                
                # argmaxë¡œ ìœ„ì¹˜ ì°¾ê¸°
                flat_idx = safe_argmax(heatmap.view(-1))
                
                if hasattr(flat_idx, 'item'):
                    flat_idx = flat_idx.item()
                
                # 2D ì¢Œí‘œë¡œ ë³€í™˜
                h, w = heatmap.shape
                y = flat_idx // w
                x = flat_idx % w
                
                # ì‹ ë¢°ë„ ê°’
                confidence = max_val.item() if hasattr(max_val, 'item') else float(max_val)
                
                keypoints.append((int(x), int(y), float(confidence)))
        
        logger.debug(f"í‚¤í¬ì¸íŠ¸ {len(keypoints)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
        return keypoints
        
    except Exception as e:
        logger.error(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        logger.error(f"íˆíŠ¸ë§µ ì •ë³´: {type(heatmaps)}, shape: {getattr(heatmaps, 'shape', 'N/A')}")
        return []

def extract_pose_keypoints(pose_output: Any, confidence_threshold: float = 0.3) -> Dict[str, Any]:
    """í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° í›„ì²˜ë¦¬"""
    if not TORCH_AVAILABLE:
        return {"keypoints": [], "valid": False}
    
    try:
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = extract_keypoints_from_heatmaps(pose_output, confidence_threshold)
        
        # ìœ íš¨ì„± ê²€ì‚¬
        valid_keypoints = sum(1 for kp in keypoints if kp[2] > confidence_threshold)
        is_valid = valid_keypoints >= 5  # ìµœì†Œ 5ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš”
        
        return {
            "keypoints": keypoints,
            "valid": is_valid,
            "num_valid": valid_keypoints,
            "confidence_threshold": confidence_threshold
        }
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {"keypoints": [], "valid": False, "error": str(e)}

def extract_human_parsing_regions(parsing_output: Any, num_classes: int = 20) -> Dict[int, Any]:
    """íœ´ë¨¼ íŒŒì‹± ì˜ì—­ ì¶”ì¶œ"""
    if not TORCH_AVAILABLE:
        return {}
    
    try:
        regions = {}
        
        # argmaxë¡œ í´ë˜ìŠ¤ ì˜ˆì¸¡
        if len(parsing_output.shape) > 2:
            # [C, H, W] -> [H, W]
            parsed = safe_argmax(parsing_output, dim=0)
        else:
            parsed = parsing_output
        
        # ê° í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬ ìƒì„±
        for class_id in range(num_classes):
            if TORCH_AVAILABLE:
                mask = (parsed == class_id)
                pixel_count = torch.sum(mask).item()
                
                if pixel_count > 0:
                    regions[class_id] = {
                        'mask': mask,
                        'pixel_count': pixel_count,
                        'area_ratio': pixel_count / (parsed.shape[0] * parsed.shape[1])
                    }
        
        logger.debug(f"íŒŒì‹± ì˜ì—­ {len(regions)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
        return regions
        
    except Exception as e:
        logger.error(f"íœ´ë¨¼ íŒŒì‹± ì˜ì—­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {}

# ==============================================
# ğŸ”¥ 8. ì´ë¯¸ì§€ ë³€í™˜ ìµœì í™” í•¨ìˆ˜ë“¤
# ==============================================

def tensor_to_pil(tensor: Any) -> Optional[Any]:
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ê¸°ë³¸)"""
    if not (TORCH_AVAILABLE and PIL_AVAILABLE):
        logger.warning("PyTorch ë˜ëŠ” PIL ì—†ìŒ")
        return None
    
    try:
        # í…ì„œ í˜•íƒœ ì •ê·œí™”
        if hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        if hasattr(tensor, 'detach'):
            tensor = tensor.detach()
        
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        while len(tensor.shape) > 3:
            tensor = tensor[0]
        
        # ì±„ë„ ìˆœì„œ ë³€ê²½ [C, H, W] -> [H, W, C]
        if len(tensor.shape) == 3 and tensor.shape[0] in [1, 3, 4]:
            tensor = tensor.permute(1, 2, 0)
        
        # NumPyë¡œ ë³€í™˜
        if hasattr(tensor, 'numpy'):
            np_array = tensor.numpy()
        else:
            np_array = tensor
        
        # ê°’ ë²”ìœ„ ì •ê·œí™” [0, 1] -> [0, 255]
        if np_array.max() <= 1.0:
            np_array = (np_array * 255).astype('uint8')
        else:
            np_array = np_array.astype('uint8')
        
        # ë‹¨ì¼ ì±„ë„ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
        if len(np_array.shape) == 2:
            np_array = np.stack([np_array] * 3, axis=-1)
        elif len(np_array.shape) == 3 and np_array.shape[2] == 1:
            np_array = np.repeat(np_array, 3, axis=2)
        
        return Image.fromarray(np_array)
        
    except Exception as e:
        logger.error(f"tensor_to_pil ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def tensor_to_pil_conda_optimized(tensor: Any, normalize: bool = True, 
                                 quality_optimization: bool = True) -> Optional[Any]:
    """conda í™˜ê²½ ìµœì í™”ëœ í…ì„œ->PIL ë³€í™˜ (M3 Max ìµœì í™”)"""
    if not (TORCH_AVAILABLE and PIL_AVAILABLE):
        logger.warning("PyTorch ë˜ëŠ” PIL ì—†ìŒ - conda í™˜ê²½ í™•ì¸ í•„ìš”")
        return None
    
    try:
        with memory_efficient_context():
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë³€í™˜
            if hasattr(tensor, 'cpu'):
                tensor = tensor.cpu()
            
            if hasattr(tensor, 'detach'):
                tensor = tensor.detach()
            
            # ë°°ì¹˜ ì°¨ì› ì²˜ë¦¬
            original_shape = tensor.shape
            while len(tensor.shape) > 3:
                tensor = tensor[0]
            
            logger.debug(f"í…ì„œ í˜•íƒœ ë³€í™˜: {original_shape} -> {tensor.shape}")
            
            # ì±„ë„ ìˆœì„œ ìµœì í™”
            if len(tensor.shape) == 3:
                if tensor.shape[0] in [1, 3, 4]:  # [C, H, W]
                    tensor = tensor.permute(1, 2, 0)  # -> [H, W, C]
            
            # NumPy ë³€í™˜ (conda ìµœì í™”)
            if NUMPY_AVAILABLE:
                if hasattr(tensor, 'numpy'):
                    np_array = tensor.numpy()
                else:
                    np_array = np.array(tensor)
            else:
                logger.warning("NumPy ì—†ìŒ - ê¸°ë³¸ ë³€í™˜ ì‚¬ìš©")
                np_array = tensor
            
            # ê°’ ë²”ìœ„ ì •ê·œí™”
            if normalize:
                if np_array.max() <= 1.0:
                    np_array = np.clip(np_array * 255, 0, 255).astype(np.uint8)
                else:
                    np_array = np.clip(np_array, 0, 255).astype(np.uint8)
            else:
                np_array = np_array.astype(np.uint8)
            
            # ì±„ë„ ìˆ˜ ì •ê·œí™”
            if len(np_array.shape) == 2:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ -> RGB
                np_array = np.stack([np_array] * 3, axis=-1)
            elif len(np_array.shape) == 3:
                if np_array.shape[2] == 1:
                    # ë‹¨ì¼ ì±„ë„ -> RGB
                    np_array = np.repeat(np_array, 3, axis=2)
                elif np_array.shape[2] == 4:
                    # RGBA -> RGB
                    np_array = np_array[:, :, :3]
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            pil_image = Image.fromarray(np_array)
            
            # í’ˆì§ˆ ìµœì í™” (M3 Maxì—ì„œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬)
            if quality_optimization and IS_M3_MAX:
                # M3 Maxì—ì„œëŠ” ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©
                if pil_image.size[0] > 1024 or pil_image.size[1] > 1024:
                    # í° ì´ë¯¸ì§€ëŠ” Lanczosë¡œ ìµœì í™”
                    pass  # í¬ê¸° ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì ìš©
            
            logger.debug(f"PIL ë³€í™˜ ì™„ë£Œ: {np_array.shape} -> {pil_image.size}")
            return pil_image
            
    except Exception as e:
        logger.error(f"conda ìµœì í™” ë³€í™˜ ì‹¤íŒ¨, ê¸°ë³¸ ë³€í™˜ ì‹œë„: {e}")
        return tensor_to_pil(tensor)

def pil_to_tensor(pil_image: Any, device: Optional[str] = None) -> Any:
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜ (conda ìµœì í™”)"""
    if not (TORCH_AVAILABLE and PIL_AVAILABLE):
        logger.warning("PyTorch ë˜ëŠ” PIL ì—†ìŒ")
        return None
    
    try:
        device = device or DEFAULT_DEVICE
        
        # NumPyë¡œ ë³€í™˜
        if NUMPY_AVAILABLE:
            np_array = np.array(pil_image)
        else:
            logger.warning("NumPy ì—†ìŒ - ê¸°ë³¸ ë³€í™˜ ì‚¬ìš©")
            np_array = pil_image
        
        # ê°’ ë²”ìœ„ ì •ê·œí™” [0, 255] -> [0, 1]
        if np_array.dtype == np.uint8:
            np_array = np_array.astype(np.float32) / 255.0
        
        # ì±„ë„ ìˆœì„œ ë³€ê²½ [H, W, C] -> [C, H, W]
        if len(np_array.shape) == 3:
            np_array = np.transpose(np_array, (2, 0, 1))
        elif len(np_array.shape) == 2:
            np_array = np.expand_dims(np_array, axis=0)
        
        # í…ì„œë¡œ ë³€í™˜
        tensor = torch.from_numpy(np_array)
        
        # ì¥ì¹˜ë¡œ ì´ë™
        if device != 'cpu':
            try:
                tensor = tensor.to(device)
            except Exception as e:
                logger.warning(f"ì¥ì¹˜ ì´ë™ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                tensor = tensor.to('cpu')
        
        return tensor
        
    except Exception as e:
        logger.error(f"pil_to_tensor ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def preprocess_image(image: Any, target_size: Tuple[int, int] = (512, 512), 
                    normalize: bool = True, device: Optional[str] = None) -> Any:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (conda ìµœì í™”)"""
    try:
        device = device or DEFAULT_DEVICE
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if not hasattr(image, 'resize'):
            if TORCH_AVAILABLE and hasattr(image, 'cpu'):
                image = tensor_to_pil_conda_optimized(image)
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                image = Image.fromarray((image * 255).astype(np.uint8))
        
        if image is None:
            logger.error("ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
            return None
        
        # í¬ê¸° ì¡°ì •
        image = image.resize(target_size, Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)
        
        # í…ì„œë¡œ ë³€í™˜
        tensor = pil_to_tensor(image, device)
        
        # ì •ê·œí™”
        if normalize and tensor is not None:
            # ImageNet ì •ê·œí™”
            mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
            std = torch.tensor([0.229, 0.224, 0.225]).to(device)
            
            if len(tensor.shape) == 3:
                tensor = (tensor - mean.view(3, 1, 1)) / std.view(3, 1, 1)
        
        return tensor
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 9. Stepë³„ íŠ¹í™” ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def preprocess_pose_input(image: Any, target_size: Tuple[int, int] = (256, 192)) -> Optional[Any]:
    """í¬ì¦ˆ ì¶”ì • ì…ë ¥ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True)

def preprocess_human_parsing_input(image: Any, target_size: Tuple[int, int] = (473, 473)) -> Optional[Any]:
    """íœ´ë¨¼ íŒŒì‹± ì…ë ¥ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True)

def preprocess_cloth_segmentation_input(image: Any, target_size: Tuple[int, int] = (512, 512)) -> Optional[Any]:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì…ë ¥ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True)

def preprocess_virtual_fitting_input(person_image: Any, cloth_image: Any, 
                                   target_size: Tuple[int, int] = (512, 512)) -> Tuple[Optional[Any], Optional[Any]]:
    """ê°€ìƒ í”¼íŒ… ì…ë ¥ ì „ì²˜ë¦¬"""
    person_tensor = preprocess_image(person_image, target_size, normalize=True)
    cloth_tensor = preprocess_image(cloth_image, target_size, normalize=True)
    return person_tensor, cloth_tensor

def postprocess_segmentation(output: Any, original_size: Optional[Tuple[int, int]] = None) -> Optional[Any]:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶œë ¥ í›„ì²˜ë¦¬"""
    if not TORCH_AVAILABLE:
        return None
    
    try:
        # Softmax ì ìš©
        if len(output.shape) > 2:
            output = safe_softmax(output, dim=0 if len(output.shape) == 3 else 1)
        
        # ìµœëŒ€ê°’ í´ë˜ìŠ¤ ì„ íƒ
        segmentation = safe_argmax(output, dim=0 if len(output.shape) == 3 else 1)
        
        # í¬ê¸° ë³µì›
        if original_size is not None:
            segmentation = safe_interpolate(
                segmentation.unsqueeze(0).unsqueeze(0).float(),
                size=original_size,
                mode='nearest'
            ).squeeze().long()
        
        return segmentation
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 10. ì¥ì¹˜ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def get_optimal_device() -> str:
    """ìµœì  ì¥ì¹˜ ë°˜í™˜"""
    return DEFAULT_DEVICE

def move_to_device(tensor: Any, device: Optional[str] = None) -> Any:
    """í…ì„œë¥¼ ì§€ì •ëœ ì¥ì¹˜ë¡œ ì´ë™"""
    if not TORCH_AVAILABLE:
        return tensor
    
    device = device or DEFAULT_DEVICE
    
    try:
        if hasattr(tensor, 'to'):
            return tensor.to(device)
        else:
            return tensor
    except Exception as e:
        logger.warning(f"ì¥ì¹˜ ì´ë™ ì‹¤íŒ¨: {e}")
        return tensor

def ensure_tensor_device(tensor: Any, target_device: str) -> Any:
    """í…ì„œê°€ ì˜¬ë°”ë¥¸ ì¥ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸"""
    if not TORCH_AVAILABLE:
        return tensor
    
    try:
        if hasattr(tensor, 'device') and tensor.device.type != target_device:
            return tensor.to(target_device)
        return tensor
    except Exception as e:
        logger.warning(f"ì¥ì¹˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        return tensor

# ==============================================
# ğŸ”¥ 11. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ í•¨ìˆ˜ë“¤
# ==============================================

def safe_operation_wrapper(operation: Callable, *args, **kwargs) -> Any:
    """ì•ˆì „í•œ ì—°ì‚° ë˜í¼"""
    try:
        return operation(*args, **kwargs)
    except RuntimeError as e:
        if 'MPS' in str(e) or 'Metal' in str(e):
            logger.warning(f"MPS ì˜¤ë¥˜ ê°ì§€, CPU í´ë°±: {e}")
            # CPUë¡œ í´ë°±
            cpu_args = []
            for arg in args:
                if hasattr(arg, 'cpu'):
                    cpu_args.append(arg.cpu())
                else:
                    cpu_args.append(arg)
            
            cpu_kwargs = {}
            for key, value in kwargs.items():
                if hasattr(value, 'cpu'):
                    cpu_kwargs[key] = value.cpu()
                else:
                    cpu_kwargs[key] = value
            
            try:
                result = operation(*cpu_args, **cpu_kwargs)
                # ê²°ê³¼ë¥¼ ì›ë˜ ì¥ì¹˜ë¡œ ë‹¤ì‹œ ì´ë™
                if hasattr(result, 'to') and len(args) > 0 and hasattr(args[0], 'device'):
                    result = result.to(args[0].device)
                return result
            except Exception as e2:
                logger.error(f"CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
                return None
        else:
            logger.error(f"ì—°ì‚° ì‹¤íŒ¨: {e}")
            return None
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return None

# ==============================================
# ğŸ”¥ 12. ëª¨ë“ˆ ì •ë³´ ë° ìƒíƒœ ì²´í¬
# ==============================================

def get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    info = {
        "conda_env": CONDA_INFO['conda_env'],
        "is_conda": IS_CONDA_ENV,
        "is_m3_max": IS_M3_MAX,
        "torch_available": TORCH_AVAILABLE,
        "torch_version": TORCH_VERSION,
        "mps_available": MPS_AVAILABLE,
        "numpy_available": NUMPY_AVAILABLE,
        "pil_available": PIL_AVAILABLE,
        "default_device": DEFAULT_DEVICE,
        "platform": platform.system(),
    }
    
    # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
    info.update(get_memory_info())
    
    return info

def print_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥"""
    info = get_system_info()
    
    print("=" * 60)
    print("ğŸ”¥ PyTorch Safe Operations v2.0 - ì‹œìŠ¤í…œ ìƒíƒœ")
    print("=" * 60)
    print(f"ğŸ conda í™˜ê²½: {'âœ…' if info['is_conda'] else 'âŒ'} {info['conda_env']}")
    print(f"ğŸ M3 Max: {'âœ…' if info['is_m3_max'] else 'âŒ'}")
    print(f"ğŸ”¥ PyTorch: {'âœ…' if info['torch_available'] else 'âŒ'} {info.get('torch_version', 'N/A')}")
    print(f"ğŸ MPS: {'âœ…' if info['mps_available'] else 'âŒ'}")
    print(f"ğŸ”¢ NumPy: {'âœ…' if info['numpy_available'] else 'âŒ'}")
    print(f"ğŸ–¼ï¸ PIL: {'âœ…' if info['pil_available'] else 'âŒ'}")
    print(f"ğŸ“± ì¥ì¹˜: {info['default_device']}")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {info['used_gb']:.1f}GB / {info['total_gb']:.1f}GB ({info['percent']:.1f}%)")
    print("=" * 60)

# ==============================================
# ğŸ”¥ 13. ëª¨ë“ˆ ì´ˆê¸°í™” ë° ìƒíƒœ ì²´í¬
# ==============================================

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
logger.info("ğŸ”¥ PyTorch Safe Operations v2.0 ë¡œë“œ ì™„ë£Œ")
if IS_CONDA_ENV:
    logger.info(f"âœ… conda í™˜ê²½: {CONDA_INFO['conda_env']}")
if IS_M3_MAX:
    logger.info("ğŸ M3 Max ìµœì í™” í™œì„±í™”")
logger.info(f"ğŸ“± ê¸°ë³¸ ì¥ì¹˜: {DEFAULT_DEVICE}")

# ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
memory_info = get_memory_info()
if memory_info['percent'] > 80:
    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_info['percent']:.1f}%")
    safe_torch_cleanup()

# ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥ (DEBUG ë ˆë²¨ì—ì„œ)
if logger.level <= logging.DEBUG:
    print_system_status()

logger.info("ğŸš€ PyTorch Safe Operations ì¤€ë¹„ ì™„ë£Œ!")