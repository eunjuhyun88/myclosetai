# app/ai_pipeline/utils/step_based_detector.py
"""
ğŸ”¥ MyCloset AI - Step ê¸°ë°˜ ìë™ íƒì§€ ì‹œìŠ¤í…œ v1.0
âœ… Step í´ë˜ìŠ¤ë“¤ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ íƒì§€
âœ… Step = AI ëª¨ë¸ + ì²˜ë¦¬ ë¡œì§ í†µí•© êµ¬ì¡° ì™„ë²½ ì§€ì›
âœ… PipelineManagerì™€ ì™„ì „ ì—°ë™
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ íŠ¹í™” ìŠ¤ìº”

ğŸ¯ í•µì‹¬ íŠ¹ì§•:
- Step í´ë˜ìŠ¤ë“¤ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ìë™ íƒì§€
- Stepë³„ ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ì„± ê²€ì¦
- ì‹¤ì œ PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ê²€ì¦
- PipelineManager í˜¸í™˜ ì„¤ì • ìë™ ìƒì„±
- ê¸°ì¡´ ModelLoader í˜¸í™˜ì„± ì™„ì „ ìœ ì§€
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import weakref

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì•ˆì „í•œ import)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ Step ê¸°ë°˜ ë°ì´í„° êµ¬ì¡°
# ==============================================

class StepStatus(Enum):
    """Step ìƒíƒœ"""
    AVAILABLE = "available"          # Step í´ë˜ìŠ¤ ë¡œë“œ ê°€ëŠ¥
    CHECKPOINT_MISSING = "checkpoint_missing"  # ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ
    CORRUPTED = "corrupted"         # ì²´í¬í¬ì¸íŠ¸ ì†ìƒ
    LOADING_FAILED = "loading_failed"  # ë¡œë”© ì‹¤íŒ¨
    NOT_FOUND = "not_found"         # Step í´ë˜ìŠ¤ ì—†ìŒ

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•„ìˆ˜ (Human Parsing, Virtual Fitting)
    HIGH = 2          # ì¤‘ìš” (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3        # ì¼ë°˜ (Cloth Warping, Geometric Matching)
    LOW = 4           # ë³´ì¡° (Post Processing, Quality Assessment)

@dataclass
class StepCheckpointInfo:
    """Step ì²´í¬í¬ì¸íŠ¸ ì •ë³´"""
    step_name: str
    step_class_name: str
    checkpoint_path: Optional[Path] = None
    checkpoint_size_mb: float = 0.0
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    step_available: bool = False
    status: StepStatus = StepStatus.NOT_FOUND
    priority: StepPriority = StepPriority.MEDIUM
    alternative_checkpoints: List[Path] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation_info: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0

# ==============================================
# ğŸ”¥ Stepë³„ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ ì •ì˜
# ==============================================

STEP_CHECKPOINT_PATTERNS = {
    "step_01_human_parsing": {
        "class_name": "HumanParsingStep",
        "priority": StepPriority.CRITICAL,
        "checkpoint_patterns": [
            r".*human.*parsing.*\.pth$",
            r".*schp.*atr.*\.pth$", 
            r".*graphonomy.*\.pth$",
            r".*lip_model.*\.pth$",
            r".*atr_model.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/human_parsing",
            "checkpoints/step_01",
            "ai_models/human_parsing",
            "models/schp",
            "Self-Correction-Human-Parsing"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (1, 500),
        "expected_parameters": (25000000, 70000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["backbone", "classifier", "conv"]
    },
    
    "step_02_pose_estimation": {
        "class_name": "PoseEstimationStep",
        "priority": StepPriority.HIGH,
        "checkpoint_patterns": [
            r".*pose.*estimation.*\.pth$",
            r".*openpose.*\.pth$",
            r".*body.*25.*\.pth$",
            r".*pose_iter.*\.caffemodel$"
        ],
        "expected_directories": [
            "checkpoints/pose_estimation",
            "checkpoints/step_02",
            "ai_models/openpose",
            "models/openpose",
            "openpose/models"
        ],
        "file_extensions": [".pth", ".pt", ".caffemodel"],
        "size_range_mb": (10, 1000),
        "expected_parameters": (10000000, 200000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["stage", "paf", "heatmap"]
    },
    
    "step_03_cloth_segmentation": {
        "class_name": "ClothSegmentationStep", 
        "priority": StepPriority.HIGH,
        "checkpoint_patterns": [
            r".*cloth.*segmentation.*\.pth$",
            r".*u2net.*\.pth$",
            r".*sam.*vit.*\.pth$",
            r".*mask.*anything.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/cloth_segmentation",
            "checkpoints/step_03",
            "ai_models/u2net",
            "models/sam",
            "segment-anything"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (10, 3000),
        "expected_parameters": (4000000, 650000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["encoder", "decoder", "outconv"]
    },
    
    "step_04_geometric_matching": {
        "class_name": "GeometricMatchingStep",
        "priority": StepPriority.MEDIUM,
        "checkpoint_patterns": [
            r".*geometric.*matching.*\.pth$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$",
            r".*matching.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/geometric_matching",
            "checkpoints/step_04",
            "ai_models/gmm",
            "models/geometric"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (1, 100),
        "expected_parameters": (500000, 50000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["correlation", "regression", "flow"]
    },
    
    "step_05_cloth_warping": {
        "class_name": "ClothWarpingStep",
        "priority": StepPriority.MEDIUM,
        "checkpoint_patterns": [
            r".*cloth.*warping.*\.pth$",
            r".*tom.*\.pth$",
            r".*warping.*\.pth$",
            r".*viton.*warp.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/cloth_warping",
            "checkpoints/step_05",
            "ai_models/tom",
            "models/warping"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (10, 4000),
        "expected_parameters": (10000000, 1000000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["generator", "discriminator", "warp"]
    },
    
    "step_06_virtual_fitting": {
        "class_name": "VirtualFittingStep",
        "priority": StepPriority.CRITICAL,
        "checkpoint_patterns": [
            r".*virtual.*fitting.*\.bin$",
            r".*ootd.*unet.*\.bin$",
            r".*stable.*diffusion.*\.bin$",
            r".*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.bin$"
        ],
        "expected_directories": [
            "checkpoints/virtual_fitting",
            "checkpoints/step_06",
            "ai_models/ootdiffusion",
            "ai_models/OOTDiffusion",
            "models/diffusion",
            "stable-diffusion-v1-5"
        ],
        "file_extensions": [".bin", ".pth", ".pt", ".safetensors"],
        "size_range_mb": (100, 8000),
        "expected_parameters": (100000000, 2000000000),
        "required_keys": ["state_dict", "model", "unet"],
        "expected_layers": ["unet", "vae", "text_encoder"]
    },
    
    "step_07_post_processing": {
        "class_name": "PostProcessingStep",
        "priority": StepPriority.LOW,
        "checkpoint_patterns": [
            r".*post.*processing.*\.pth$",
            r".*enhance.*\.pth$",
            r".*refine.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/post_processing",
            "checkpoints/step_07",
            "ai_models/enhancement"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (1, 1000),
        "expected_parameters": (1000000, 100000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["enhance", "refine"]
    },
    
    "step_08_quality_assessment": {
        "class_name": "QualityAssessmentStep",
        "priority": StepPriority.LOW,
        "checkpoint_patterns": [
            r".*quality.*assessment.*\.pth$",
            r".*quality.*\.pth$",
            r".*scorer.*\.pth$"
        ],
        "expected_directories": [
            "checkpoints/quality_assessment",
            "checkpoints/step_08",
            "ai_models/quality"
        ],
        "file_extensions": [".pth", ".pt"],
        "size_range_mb": (1, 500),
        "expected_parameters": (1000000, 50000000),
        "required_keys": ["state_dict", "model"],
        "expected_layers": ["classifier", "scorer"]
    }
}

# ==============================================
# ğŸ”¥ Step ê¸°ë°˜ ìë™ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class StepBasedDetector:
    """
    ğŸ¯ Step ê¸°ë°˜ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v1.0
    - Step í´ë˜ìŠ¤ë“¤ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ íƒì§€
    - Stepë³„ ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ì„± ê²€ì¦
    - PipelineManager í˜¸í™˜ ì„¤ì • ìƒì„±
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_pytorch_validation: bool = True,
        enable_step_loading: bool = True,
        enable_caching: bool = True,
        max_workers: int = 4,
        scan_timeout: int = 300,
        device: Optional[str] = None  # ğŸ”¥ ì„ íƒì  ë””ë°”ì´ìŠ¤ ì„¤ì •
    ):
        """Step ê¸°ë°˜ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.StepBasedDetector")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (ìë™ ê°ì§€ ë˜ëŠ” ì‚¬ìš©ì ì§€ì •)
        self.device = self._auto_detect_device(device)
        self.device_info = self._get_device_info()
        
        # ê²€ìƒ‰ ê²½ë¡œ ì„¤ì •
        if search_paths is None:
            self.search_paths = self._get_default_search_paths()
        else:
            self.search_paths = search_paths
        
        # ì„¤ì •
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_step_loading = enable_step_loading
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_steps: Dict[str, StepCheckpointInfo] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "checkpoint_files_found": 0,
            "valid_checkpoints": 0,
            "steps_available": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0,
            "errors_encountered": 0,
            "pytorch_validation_errors": 0
        }
        
        # ìºì‹œ ê´€ë¦¬
        self.cache_db_path = Path("step_detection_cache.db")
        self.cache_ttl = 86400  # 24ì‹œê°„
        self._cache_lock = threading.RLock()
        
        self.logger.info(f"ğŸ¯ Step ê¸°ë°˜ íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device} ({self.device_info['type']})")
        self.logger.info(f"ğŸ“ ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
        # ìºì‹œ DB ì´ˆê¸°í™”
        if self.enable_caching:
            self._init_cache_db()

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (Step íŒŒì¼ë“¤ê³¼ ë™ì¼í•œ ë¡œì§)"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            return 'cpu'

        try:
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except:
            return 'cpu'

    def _get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        device_info = {
            "type": self.device,
            "available": True,
            "memory_gb": 16,  # ê¸°ë³¸ê°’
            "is_m3_max": False,
            "supports_fp16": False,
            "max_batch_size": 1
        }
        
        try:
            if self.device == 'mps':
                device_info.update({
                    "is_m3_max": self._detect_m3_max(),
                    "supports_fp16": True,
                    "memory_gb": self._get_available_memory(),
                    "max_batch_size": 8 if self._detect_m3_max() else 4
                })
            elif self.device == 'cuda':
                if TORCH_AVAILABLE:
                    device_info.update({
                        "supports_fp16": torch.cuda.is_available(),
                        "memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 8,
                        "max_batch_size": 4
                    })
            else:  # CPU
                device_info.update({
                    "memory_gb": self._get_system_memory(),
                    "max_batch_size": 1
                })
                
        except Exception as e:
            self.logger.debug(f"ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return device_info

    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€ (Step íŒŒì¼ë“¤ê³¼ ë™ì¼í•œ ë¡œì§)"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                cpu_info = result.stdout.strip()
                return 'M3 Max' in cpu_info or 'M3' in cpu_info
        except:
            pass
        return False

    def _get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ê°ì§€"""
        try:
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                return memory.total / (1024**3)  # GBë¡œ ë³€í™˜
            else:
                return 16.0  # ê¸°ë³¸ê°’
        except Exception:
            return 16.0

    def _get_system_memory(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê°ì§€"""
        return self._get_available_memory()

    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜ (ê³µê°œ ë©”ì„œë“œ)"""
        return self.device_info.copy()

    def get_optimal_config_for_device(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ì— ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        config = {
            "device": self.device,
            "precision": "fp16" if self.device_info["supports_fp16"] else "fp32",
            "batch_size": self.device_info["max_batch_size"],
            "memory_limit_gb": self.device_info["memory_gb"] * 0.8,  # 80% ì‚¬ìš©
            "enable_optimization": self.device_info["is_m3_max"]
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.device_info["is_m3_max"]:
            config.update({
                "enable_neural_engine": True,
                "memory_pool_size": min(64, self.device_info["memory_gb"]),
                "concurrent_sessions": 4,
                "quality_priority": True
            })
        
        return config

    def _get_default_search_paths(self) -> List[Path]:
        """ê¸°ë³¸ ê²€ìƒ‰ ê²½ë¡œë“¤ ë°˜í™˜"""
        current_file = Path(__file__).resolve()
        backend_dir = current_file.parents[3]  # backend ë””ë ‰í† ë¦¬
        
        potential_paths = [
            # í”„ë¡œì íŠ¸ ë‚´ë¶€ ê²½ë¡œë“¤
            backend_dir / "ai_models",
            backend_dir / "app" / "ai_pipeline" / "models",
            backend_dir / "app" / "models",
            backend_dir / "checkpoints",
            backend_dir / "models",
            backend_dir / "weights",
            
            # ìƒìœ„ ë””ë ‰í† ë¦¬
            backend_dir.parent / "ai_models",
            backend_dir.parent / "models",
            
            # conda ë° ì‚¬ìš©ì ìºì‹œ ê²½ë¡œë“¤
            *self._get_conda_model_paths(),
            Path.home() / ".cache" / "huggingface",
            Path.home() / ".cache" / "torch",
            Path.home() / "Downloads"
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ê³  ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œë§Œ ë°˜í™˜
        real_paths = []
        for path in potential_paths:
            try:
                if path.exists() and path.is_dir() and os.access(path, os.R_OK):
                    real_paths.append(path)
                    self.logger.debug(f"âœ… ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {path}")
            except Exception as e:
                self.logger.debug(f"ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨ {path}: {e}")
                continue
        
        return real_paths

    def _get_conda_model_paths(self) -> List[Path]:
        """conda í™˜ê²½ ëª¨ë¸ ê²½ë¡œë“¤ íƒì§€"""
        conda_paths = []
        
        try:
            # í˜„ì¬ conda í™˜ê²½
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix and Path(conda_prefix).exists():
                conda_base = Path(conda_prefix)
                conda_paths.extend([
                    conda_base / "lib" / "python3.11" / "site-packages",
                    conda_base / "share" / "models",
                    conda_base / "models",
                    conda_base / "checkpoints"
                ])
            
            # conda ë£¨íŠ¸ íƒì§€
            conda_root = os.environ.get('CONDA_ROOT')
            if not conda_root:
                possible_roots = [
                    Path.home() / "miniforge3",
                    Path.home() / "miniconda3", 
                    Path.home() / "anaconda3",
                    Path("/opt/conda"),
                    Path("/usr/local/conda")
                ]
                for root in possible_roots:
                    if root.exists():
                        conda_root = str(root)
                        break
            
            if conda_root and Path(conda_root).exists():
                conda_paths.extend([
                    Path(conda_root) / "pkgs",
                    Path(conda_root) / "envs"
                ])
                
        except Exception as e:
            self.logger.debug(f"conda ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return conda_paths

    def _init_cache_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS step_detection_cache (
                        step_name TEXT PRIMARY KEY,
                        checkpoint_path TEXT,
                        checkpoint_size INTEGER,
                        checkpoint_mtime REAL,
                        pytorch_valid INTEGER,
                        parameter_count INTEGER,
                        step_available INTEGER,
                        status TEXT,
                        detection_data TEXT,
                        created_at REAL,
                        accessed_at REAL
                    )
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_step_accessed_at ON step_detection_cache(accessed_at)
                """)
                
                conn.commit()
                
            self.logger.debug("âœ… Step íƒì§€ ìºì‹œ DB ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step íƒì§€ ìºì‹œ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enable_caching = False

    def detect_all_steps(
        self, 
        force_rescan: bool = False,
        step_filter: Optional[List[str]] = None,
        min_confidence: float = 0.3
    ) -> Dict[str, StepCheckpointInfo]:
        """
        ëª¨ë“  Stepì˜ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            step_filter: íŠ¹ì • Stepë§Œ íƒì§€
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            
        Returns:
            Dict[str, StepCheckpointInfo]: íƒì§€ëœ Step ì •ë³´ë“¤
        """
        try:
            self.logger.info("ğŸ¯ Step ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.enable_caching:
                cached_results = self._load_from_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ Step")
                    self.scan_stats["cache_hits"] = len(cached_results)
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self._reset_scan_stats()
            
            # Step í•„í„°ë§
            if step_filter:
                filtered_patterns = {k: v for k, v in STEP_CHECKPOINT_PATTERNS.items() 
                                   if k in step_filter}
            else:
                filtered_patterns = STEP_CHECKPOINT_PATTERNS
            
            # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
            if self.max_workers > 1:
                self._parallel_scan_steps(filtered_patterns, min_confidence)
            else:
                self._sequential_scan_steps(filtered_patterns, min_confidence)
            
            # Step í´ë˜ìŠ¤ ë¡œë“œ ê°€ëŠ¥ì„± ê²€ì¦
            if self.enable_step_loading:
                self._validate_step_loading()
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["steps_available"] = len([s for s in self.detected_steps.values() if s.step_available])
            self.scan_stats["scan_duration"] = time.time() - start_time
            self.scan_stats["last_scan_time"] = time.time()
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            self._post_process_step_results(min_confidence)
            
            # ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache()
            
            self.logger.info(f"âœ… Step íƒì§€ ì™„ë£Œ: {len(self.detected_steps)}ê°œ Step ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_step_detection_summary()
            
            return self.detected_steps
            
        except Exception as e:
            self.logger.error(f"âŒ Step íƒì§€ ì‹¤íŒ¨: {e}")
            self.scan_stats["errors_encountered"] += 1
            raise

    def _parallel_scan_steps(self, step_patterns: Dict, min_confidence: float):
        """Stepë“¤ ë³‘ë ¬ ìŠ¤ìº”"""
        try:
            # ìŠ¤ìº” íƒœìŠ¤í¬ ìƒì„±
            scan_tasks = []
            for step_name, pattern_info in step_patterns.items():
                for search_path in self.search_paths:
                    if search_path.exists():
                        scan_tasks.append((step_name, pattern_info, search_path))
            
            if not scan_tasks:
                self.logger.warning("âš ï¸ ìŠ¤ìº”í•  ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {
                    executor.submit(
                        self._scan_path_for_step, 
                        step_name, 
                        pattern_info, 
                        search_path, 
                        min_confidence
                    ): (step_name, search_path)
                    for step_name, pattern_info, search_path in scan_tasks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                completed_count = 0
                for future in as_completed(future_to_task, timeout=self.scan_timeout):
                    step_name, search_path = future_to_task[future]
                    try:
                        step_info = future.result()
                        if step_info:
                            # ê²°ê³¼ ë³‘í•© (ìŠ¤ë ˆë“œ ì•ˆì „)
                            with threading.Lock():
                                self._register_step_safe(step_info)
                        
                        completed_count += 1
                        self.logger.debug(f"âœ… {step_name} @ {search_path} ìŠ¤ìº” ì™„ë£Œ ({completed_count}/{len(scan_tasks)})")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ {step_name} @ {search_path} ìŠ¤ìº” ì‹¤íŒ¨: {e}")
                        self.scan_stats["errors_encountered"] += 1
                        
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆœì°¨ ìŠ¤ìº”
            self._sequential_scan_steps(step_patterns, min_confidence)

    def _sequential_scan_steps(self, step_patterns: Dict, min_confidence: float):
        """Stepë“¤ ìˆœì°¨ ìŠ¤ìº”"""
        try:
            for step_name, pattern_info in step_patterns.items():
                self.logger.debug(f"ğŸ¯ {step_name} ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì¤‘...")
                
                best_checkpoint = None
                best_confidence = 0.0
                
                for search_path in self.search_paths:
                    if search_path.exists():
                        step_info = self._scan_path_for_step(
                            step_name, pattern_info, search_path, min_confidence
                        )
                        if step_info and step_info.confidence_score > best_confidence:
                            best_checkpoint = step_info
                            best_confidence = step_info.confidence_score
                
                if best_checkpoint:
                    self._register_step_safe(best_checkpoint)
                        
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ìŠ¤ìº” ì‹¤íŒ¨: {e}")

    def _scan_path_for_step(
        self, 
        step_name: str, 
        pattern_info: Dict, 
        search_path: Path, 
        min_confidence: float,
        max_depth: int = 6,
        current_depth: int = 0
    ) -> Optional[StepCheckpointInfo]:
        """íŠ¹ì • ê²½ë¡œì—ì„œ Step ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº”"""
        try:
            if current_depth > max_depth:
                return None
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ë‚˜ì—´
            try:
                items = list(search_path.iterdir())
            except PermissionError:
                self.logger.debug(f"ê¶Œí•œ ì—†ìŒ: {search_path}")
                return None
            
            # íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë¶„ë¦¬
            files = [item for item in items if item.is_file()]
            subdirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]
            
            best_checkpoint = None
            best_confidence = 0.0
            
            # íŒŒì¼ë“¤ ë¶„ì„
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    # Step ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
                    if self._is_step_checkpoint_file(file_path, pattern_info):
                        self.scan_stats["checkpoint_files_found"] += 1
                        
                        # Step ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
                        step_info = self._analyze_step_checkpoint(
                            file_path, step_name, pattern_info, min_confidence
                        )
                        
                        if step_info and step_info.confidence_score > best_confidence:
                            best_checkpoint = step_info
                            best_confidence = step_info.confidence_score
                        
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
            if current_depth < max_depth:
                for subdir in subdirs:
                    # ì œì™¸í•  ë””ë ‰í† ë¦¬ íŒ¨í„´
                    if subdir.name in ['__pycache__', '.git', 'node_modules', '.vscode', '.idea']:
                        continue
                    
                    try:
                        subdir_result = self._scan_path_for_step(
                            step_name, pattern_info, subdir, min_confidence, 
                            max_depth, current_depth + 1
                        )
                        if subdir_result and subdir_result.confidence_score > best_confidence:
                            best_checkpoint = subdir_result
                            best_confidence = subdir_result.confidence_score
                    except Exception as e:
                        self.logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                        continue
            
            return best_checkpoint
            
        except Exception as e:
            self.logger.debug(f"ê²½ë¡œ ìŠ¤ìº” ì˜¤ë¥˜ {search_path}: {e}")
            return None

    def _is_step_checkpoint_file(self, file_path: Path, pattern_info: Dict) -> bool:
        """Step ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸"""
        try:
            # í™•ì¥ì ì²´í¬
            if file_path.suffix.lower() not in pattern_info.get("file_extensions", [".pth", ".pt"]):
                return False
            
            # íŒŒì¼ í¬ê¸° ì²´í¬
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            size_min, size_max = pattern_info.get("size_range_mb", (1, 10000))
            
            if not (size_min <= file_size_mb <= size_max):
                return False
            
            # íŒ¨í„´ ë§¤ì¹­
            file_path_str = str(file_path).lower()
            for pattern in pattern_info.get("checkpoint_patterns", []):
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    return True
            
            # ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë§¤ì¹­
            path_parts = [part.lower() for part in file_path.parts]
            for expected_dir in pattern_info.get("expected_directories", []):
                if any(expected_dir.lower() in part for part in path_parts):
                    return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False

    def _analyze_step_checkpoint(
        self, 
        file_path: Path, 
        step_name: str,
        pattern_info: Dict,
        min_confidence: float
    ) -> Optional[StepCheckpointInfo]:
        """Step ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¶„ì„"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            last_modified = file_stat.st_mtime
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_step_confidence(file_path, pattern_info, file_size_mb)
            
            if confidence_score < min_confidence:
                return None
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            pytorch_valid = False
            parameter_count = 0
            validation_info = {}
            
            if self.enable_pytorch_validation and file_path.suffix.lower() in ['.pth', '.pt']:
                pytorch_valid, parameter_count, validation_info = self._validate_pytorch_checkpoint(
                    file_path, pattern_info
                )
                
                if pytorch_valid:
                    self.scan_stats["valid_checkpoints"] += 1
                    # PyTorch ê²€ì¦ ì„±ê³µí•˜ë©´ ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                    confidence_score = min(confidence_score + 0.2, 1.0)
                else:
                    self.scan_stats["pytorch_validation_errors"] += 1
                    # ê²€ì¦ ì‹¤íŒ¨í•˜ë©´ ì‹ ë¢°ë„ ê°ì†Œ
                    confidence_score = max(confidence_score - 0.3, 0.0)
            
            # Step ì •ë³´ ìƒì„±
            step_info = StepCheckpointInfo(
                step_name=step_name,
                step_class_name=pattern_info.get("class_name", ""),
                checkpoint_path=file_path,
                checkpoint_size_mb=file_size_mb,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count,
                last_modified=last_modified,
                step_available=False,  # ë‚˜ì¤‘ì— ê²€ì¦
                status=StepStatus.CHECKPOINT_MISSING,
                priority=pattern_info.get("priority", StepPriority.MEDIUM),
                confidence_score=confidence_score,
                metadata={
                    "file_name": file_path.name,
                    "detected_at": time.time(),
                    "auto_detected": True,
                    "pattern_matched": True,
                    **validation_info
                },
                validation_info=validation_info
            )
            
            return step_info
            
        except Exception as e:
            self.logger.debug(f"Step ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _validate_pytorch_checkpoint(self, file_path: Path, pattern_info: Dict) -> Tuple[bool, int, Dict[str, Any]]:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦"""
        try:
            if not TORCH_AVAILABLE:
                return False, 0, {"error": "PyTorch not available"}
            
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            try:
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
            except Exception as e:
                try:
                    checkpoint = torch.load(file_path, map_location='cpu')
                except Exception as e2:
                    return False, 0, {"load_error": str(e2)}
            
            validation_info = {}
            parameter_count = 0
            
            if isinstance(checkpoint, dict):
                # state_dict í™•ì¸
                state_dict = None
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    validation_info["has_state_dict"] = True
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                    validation_info["has_model"] = True
                else:
                    # ì²´í¬í¬ì¸íŠ¸ ìì²´ê°€ state_dictì¼ ìˆ˜ ìˆìŒ
                    state_dict = checkpoint
                    validation_info["is_direct_state_dict"] = True
                
                if state_dict and isinstance(state_dict, dict):
                    # ë ˆì´ì–´ ì •ë³´ ë¶„ì„
                    layers_info = self._analyze_checkpoint_layers(state_dict, pattern_info)
                    validation_info.update(layers_info)
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                    parameter_count = self._count_checkpoint_parameters(state_dict)
                    validation_info["parameter_count"] = parameter_count
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ë²”ìœ„ ê²€ì¦
                    expected_params = pattern_info.get("expected_parameters", (0, float('inf')))
                    if expected_params[0] <= parameter_count <= expected_params[1]:
                        validation_info["parameter_range_valid"] = True
                    else:
                        validation_info["parameter_range_valid"] = False
                
                # ë©”íƒ€ë°ì´í„°
                for key in ['epoch', 'version', 'arch', 'model_name']:
                    if key in checkpoint:
                        validation_info[f'checkpoint_{key}'] = str(checkpoint[key])[:100]
                
                return True, parameter_count, validation_info
            
            else:
                # ë‹¨ìˆœ í…ì„œë‚˜ ëª¨ë¸ ê°ì²´
                if hasattr(checkpoint, 'state_dict'):
                    state_dict = checkpoint.state_dict()
                    parameter_count = self._count_checkpoint_parameters(state_dict)
                    return True, parameter_count, {"model_object": True}
                elif torch.is_tensor(checkpoint):
                    return True, checkpoint.numel(), {"single_tensor": True}
                else:
                    return False, 0, {"unknown_format": type(checkpoint).__name__}
            
        except Exception as e:
            return False, 0, {"validation_error": str(e)[:200]}

    def _analyze_checkpoint_layers(self, state_dict: Dict, pattern_info: Dict) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ë ˆì´ì–´ ë¶„ì„"""
        try:
            layers_info = {
                "total_layers": len(state_dict),
                "layer_types": {},
                "layer_names": list(state_dict.keys())[:10]  # ì²˜ìŒ 10ê°œë§Œ
            }
            
            # ë ˆì´ì–´ íƒ€ì… ë¶„ì„
            layer_type_counts = {}
            for key in state_dict.keys():
                if 'conv' in key.lower():
                    layer_type_counts['conv'] = layer_type_counts.get('conv', 0) + 1
                elif 'bn' in key.lower() or 'batch' in key.lower():
                    layer_type_counts['batch_norm'] = layer_type_counts.get('batch_norm', 0) + 1
                elif 'linear' in key.lower() or 'fc' in key.lower():
                    layer_type_counts['linear'] = layer_type_counts.get('linear', 0) + 1
                elif 'attention' in key.lower() or 'attn' in key.lower():
                    layer_type_counts['attention'] = layer_type_counts.get('attention', 0) + 1
            
            layers_info["layer_types"] = layer_type_counts
            
            # ì˜ˆìƒ ë ˆì´ì–´ í™•ì¸
            expected_layers = pattern_info.get("expected_layers", [])
            found_layers = 0
            for expected_layer in expected_layers:
                if any(expected_layer in key.lower() for key in state_dict.keys()):
                    found_layers += 1
            
            layers_info["expected_layers_found"] = found_layers
            layers_info["expected_layers_total"] = len(expected_layers)
            layers_info["layer_match_rate"] = found_layers / len(expected_layers) if expected_layers else 1.0
            
            return layers_info
            
        except Exception as e:
            return {"layer_analysis_error": str(e)[:100]}

    def _count_checkpoint_parameters(self, state_dict: Dict) -> int:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        try:
            total_params = 0
            for tensor in state_dict.values():
                if torch.is_tensor(tensor):
                    total_params += tensor.numel()
            return total_params
        except Exception:
            return 0

    def _calculate_step_confidence(self, file_path: Path, pattern_info: Dict, file_size_mb: float) -> float:
        """Step ì²´í¬í¬ì¸íŠ¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            score = 0.0
            file_name = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
            for pattern in pattern_info.get("checkpoint_patterns", []):
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    score += 30.0
                    break
            
            # ë””ë ‰í† ë¦¬ ë§¤ì¹­
            path_parts = [part.lower() for part in file_path.parts]
            for expected_dir in pattern_info.get("expected_directories", []):
                if any(expected_dir.lower() in part for part in path_parts):
                    score += 25.0
                    break
            
            # íŒŒì¼ í¬ê¸° ì ì •ì„±
            size_min, size_max = pattern_info.get("size_range_mb", (1, 10000))
            size_mid = (size_min + size_max) / 2
            
            if size_min <= file_size_mb <= size_max:
                if abs(file_size_mb - size_mid) / size_mid < 0.5:
                    score += 20.0
                else:
                    score += 15.0
            
            # íŒŒì¼ í™•ì¥ì
            if file_path.suffix in pattern_info.get("file_extensions", []):
                score += 10.0
            
            # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤
            priority = pattern_info.get("priority", StepPriority.MEDIUM)
            if priority == StepPriority.CRITICAL:
                score += 10.0
            elif priority == StepPriority.HIGH:
                score += 5.0
            
            # ì •ê·œí™” (0.0 ~ 1.0)
            confidence = min(score / 100.0, 1.0)
            return max(confidence, 0.0)
            
        except Exception as e:
            self.logger.debug(f"ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

    def _validate_step_loading(self):
        """Step í´ë˜ìŠ¤ ë¡œë“œ ê°€ëŠ¥ì„± ê²€ì¦"""
        try:
            self.logger.info("ğŸ” Step í´ë˜ìŠ¤ ë¡œë“œ ê°€ëŠ¥ì„± ê²€ì¦ ì¤‘...")
            
            for step_name, step_info in self.detected_steps.items():
                try:
                    # Step í´ë˜ìŠ¤ ë¡œë“œ ì‹œë„
                    step_available = self._test_step_class_loading(step_name, step_info.step_class_name)
                    
                    step_info.step_available = step_available
                    
                    if step_available:
                        if step_info.pytorch_valid:
                            step_info.status = StepStatus.AVAILABLE
                        else:
                            step_info.status = StepStatus.CHECKPOINT_MISSING
                    else:
                        step_info.status = StepStatus.LOADING_FAILED
                        
                except Exception as e:
                    self.logger.debug(f"Step {step_name} ë¡œë“œ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    step_info.step_available = False
                    step_info.status = StepStatus.NOT_FOUND
            
        except Exception as e:
            self.logger.error(f"âŒ Step ë¡œë“œ ê²€ì¦ ì‹¤íŒ¨: {e}")

    def _test_step_class_loading(self, step_name: str, step_class_name: str) -> bool:
        """Step í´ë˜ìŠ¤ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        try:
            # steps ëª¨ë“ˆì—ì„œ Step í´ë˜ìŠ¤ ë¡œë“œ ì‹œë„
            from ..steps import get_step_class
from app.utils.safe_caller import safe_call, safe_warmup
            
            step_class = get_step_class(step_name)
            
            if step_class is None:
                self.logger.debug(f"âŒ {step_name} Step í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # í´ë˜ìŠ¤ëª… í™•ì¸
            if step_class.__name__ != step_class_name:
                self.logger.debug(f"âš ï¸ {step_name} í´ë˜ìŠ¤ëª… ë¶ˆì¼ì¹˜: {step_class.__name__} != {step_class_name}")
            
            self.logger.debug(f"âœ… {step_name} Step í´ë˜ìŠ¤ ë¡œë“œ ê°€ëŠ¥")
            return True
            
        except Exception as e:
            self.logger.debug(f"âŒ {step_name} Step í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _register_step_safe(self, step_info: StepCheckpointInfo):
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ Step ë“±ë¡"""
        with threading.Lock():
            self._register_step(step_info)

    def _register_step(self, step_info: StepCheckpointInfo):
        """Step ì •ë³´ ë“±ë¡ (ì¤‘ë³µ ì²˜ë¦¬)"""
        try:
            step_name = step_info.step_name
            
            if step_name in self.detected_steps:
                existing_step = self.detected_steps[step_name]
                
                # ë” ë‚˜ì€ ì²´í¬í¬ì¸íŠ¸ë¡œ êµì²´í• ì§€ ê²°ì •
                if self._is_better_checkpoint(step_info, existing_step):
                    step_info.alternative_checkpoints.append(existing_step.checkpoint_path)
                    step_info.alternative_checkpoints.extend(existing_step.alternative_checkpoints)
                    self.detected_steps[step_name] = step_info
                    self.logger.debug(f"ğŸ”„ Step êµì²´: {step_name}")
                else:
                    existing_step.alternative_checkpoints.append(step_info.checkpoint_path)
                    self.logger.debug(f"ğŸ“ ëŒ€ì²´ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€: {step_name}")
            else:
                self.detected_steps[step_name] = step_info
                self.logger.debug(f"âœ… ìƒˆ Step ë“±ë¡: {step_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ Step ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _is_better_checkpoint(self, new_step: StepCheckpointInfo, existing_step: StepCheckpointInfo) -> bool:
        """ìƒˆ ì²´í¬í¬ì¸íŠ¸ê°€ ê¸°ì¡´ ê²ƒë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨"""
        try:
            # 1. PyTorch ê²€ì¦ ìƒíƒœ ìš°ì„ 
            if new_step.pytorch_valid and not existing_step.pytorch_valid:
                return True
            elif not new_step.pytorch_valid and existing_step.pytorch_valid:
                return False
            
            # 2. ìš°ì„ ìˆœìœ„ ë¹„êµ (ë‚®ì€ ê°’ì´ ë†’ì€ ìš°ì„ ìˆœìœ„)
            if new_step.priority.value < existing_step.priority.value:
                return True
            elif new_step.priority.value > existing_step.priority.value:
                return False
            
            # 3. ì‹ ë¢°ë„ ë¹„êµ
            if abs(new_step.confidence_score - existing_step.confidence_score) > 0.1:
                return new_step.confidence_score > existing_step.confidence_score
            
            # 4. íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
            if new_step.parameter_count > 0 and existing_step.parameter_count > 0:
                if abs(new_step.parameter_count - existing_step.parameter_count) / max(new_step.parameter_count, existing_step.parameter_count) > 0.2:
                    return new_step.parameter_count > existing_step.parameter_count
            
            # 5. ìµœì‹ ì„± ë¹„êµ
            if abs(new_step.last_modified - existing_step.last_modified) > 86400:  # 1ì¼ ì°¨ì´
                return new_step.last_modified > existing_step.last_modified
            
            # 6. íŒŒì¼ í¬ê¸° ë¹„êµ
            return new_step.checkpoint_size_mb > existing_step.checkpoint_size_mb
            
        except Exception as e:
            self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë¹„êµ ì˜¤ë¥˜: {e}")
            return new_step.checkpoint_size_mb > existing_step.checkpoint_size_mb

    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ë¦¬ì…‹"""
        self.scan_stats.update({
            "total_files_scanned": 0,
            "checkpoint_files_found": 0,
            "valid_checkpoints": 0,
            "steps_available": 0,
            "scan_duration": 0.0,
            "errors_encountered": 0,
            "pytorch_validation_errors": 0
        })

    def _post_process_step_results(self, min_confidence: float):
        """Step ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ì‹ ë¢°ë„ í•„í„°ë§
            filtered_steps = {
                name: step for name, step in self.detected_steps.items()
                if step.confidence_score >= min_confidence
            }
            self.detected_steps = filtered_steps
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
            sorted_steps = sorted(
                self.detected_steps.items(),
                key=lambda x: (
                    x[1].priority.value, 
                    -x[1].confidence_score, 
                    -x[1].parameter_count, 
                    -x[1].checkpoint_size_mb
                )
            )
            
            self.detected_steps = {name: step for name, step in sorted_steps}
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _print_step_detection_summary(self):
        """Step íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            self.logger.info("=" * 70)
            self.logger.info("ğŸ¯ Step ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 70)
            
            total_size_gb = sum(step.checkpoint_size_mb for step in self.detected_steps.values()) / 1024
            total_params = sum(step.parameter_count for step in self.detected_steps.values())
            avg_confidence = sum(step.confidence_score for step in self.detected_steps.values()) / len(self.detected_steps) if self.detected_steps else 0
            available_count = sum(1 for step in self.detected_steps.values() if step.step_available)
            pytorch_valid_count = sum(1 for step in self.detected_steps.values() if step.pytorch_valid)
            
            self.logger.info(f"ğŸ“Š íƒì§€ëœ Step: {len(self.detected_steps)}ê°œ")
            self.logger.info(f"âœ… ë¡œë“œ ê°€ëŠ¥í•œ Step: {available_count}ê°œ")
            self.logger.info(f"ğŸ” ê²€ì¦ëœ ì²´í¬í¬ì¸íŠ¸: {pytorch_valid_count}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº” íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {self.scan_stats['checkpoint_files_found']}ê°œ")
            self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            self.logger.info(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            self.logger.info(f"ğŸ§® ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
            
            # Stepë³„ ìƒíƒœ ë¶„í¬
            status_distribution = {}
            for step in self.detected_steps.values():
                status = step.status.value
                if status not in status_distribution:
                    status_distribution[status] = 0
                status_distribution[status] += 1
            
            if status_distribution:
                self.logger.info("\nğŸ“‹ Step ìƒíƒœë³„ ë¶„í¬:")
                for status, count in status_distribution.items():
                    self.logger.info(f"  {status}: {count}ê°œ")
            
            # ì£¼ìš” Stepë“¤
            if self.detected_steps:
                self.logger.info("\nğŸ† íƒì§€ëœ ì£¼ìš” Stepë“¤:")
                for i, (name, step) in enumerate(list(self.detected_steps.items())[:8]):
                    status_icon = "âœ…" if step.step_available else ("ğŸ”" if step.pytorch_valid else "â“")
                    self.logger.info(f"  {i+1}. {name}")
                    self.logger.info(f"     ìƒíƒœ: {status_icon} {step.status.value}")
                    self.logger.info(f"     ì²´í¬í¬ì¸íŠ¸: {step.checkpoint_path.name if step.checkpoint_path else 'None'}")
                    self.logger.info(f"     í¬ê¸°: {step.checkpoint_size_mb:.1f}MB, ì‹ ë¢°ë„: {step.confidence_score:.3f}")
                    if step.parameter_count > 0:
                        self.logger.info(f"     íŒŒë¼ë¯¸í„°: {step.parameter_count:,}ê°œ")
            
            self.logger.info("=" * 70)
                
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ ìºì‹œ ê´€ë ¨ ë©”ì„œë“œë“¤
    # ==============================================

    def _load_from_cache(self) -> Optional[Dict[str, StepCheckpointInfo]]:
        """ìºì‹œì—ì„œ ë¡œë“œ"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    cutoff_time = time.time() - self.cache_ttl
                    cursor.execute("DELETE FROM step_detection_cache WHERE created_at < ?", (cutoff_time,))
                    
                    # ìºì‹œ ì¡°íšŒ
                    cursor.execute("""
                        SELECT step_name, detection_data
                        FROM step_detection_cache 
                        WHERE created_at > ?
                    """, (cutoff_time,))
                    
                    cached_steps = {}
                    for step_name, detection_data in cursor.fetchall():
                        try:
                            step_data = json.loads(detection_data)
                            step_info = self._deserialize_step_info(step_data)
                            if step_info:
                                # íŒŒì¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                                if step_info.checkpoint_path and step_info.checkpoint_path.exists():
                                    cached_steps[step_name] = step_info
                        except Exception as e:
                            self.logger.debug(f"ìºì‹œ í•­ëª© ë¡œë“œ ì‹¤íŒ¨ {step_name}: {e}")
                    
                    if cached_steps:
                        # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                        cursor.execute("UPDATE step_detection_cache SET accessed_at = ?", (time.time(),))
                        conn.commit()
                        
                        self.detected_steps = cached_steps
                        return cached_steps
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_cache(self):
        """ìºì‹œì— ì €ì¥"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    current_time = time.time()
                    
                    for step in self.detected_steps.values():
                        try:
                            detection_data = json.dumps(self._serialize_step_info(step))
                            
                            cursor.execute("""
                                INSERT OR REPLACE INTO step_detection_cache 
                                (step_name, checkpoint_path, checkpoint_size, checkpoint_mtime, 
                                 pytorch_valid, parameter_count, step_available, status, 
                                 detection_data, created_at, accessed_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                step.step_name,
                                str(step.checkpoint_path) if step.checkpoint_path else None,
                                int(step.checkpoint_size_mb * 1024 * 1024),
                                step.last_modified,
                                int(step.pytorch_valid),
                                step.parameter_count,
                                int(step.step_available),
                                step.status.value,
                                detection_data,
                                current_time,
                                current_time
                            ))
                        except Exception as e:
                            self.logger.debug(f"Step ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {step.step_name}: {e}")
                    
                    conn.commit()
                    
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _serialize_step_info(self, step_info: StepCheckpointInfo) -> Dict[str, Any]:
        """StepCheckpointInfoë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™”"""
        return {
            "step_name": step_info.step_name,
            "step_class_name": step_info.step_class_name,
            "checkpoint_path": str(step_info.checkpoint_path) if step_info.checkpoint_path else None,
            "checkpoint_size_mb": step_info.checkpoint_size_mb,
            "pytorch_valid": step_info.pytorch_valid,
            "parameter_count": step_info.parameter_count,
            "last_modified": step_info.last_modified,
            "step_available": step_info.step_available,
            "status": step_info.status.value,
            "priority": step_info.priority.value,
            "alternative_checkpoints": [str(p) for p in step_info.alternative_checkpoints],
            "metadata": step_info.metadata,
            "validation_info": step_info.validation_info,
            "confidence_score": step_info.confidence_score
        }

    def _deserialize_step_info(self, data: Dict[str, Any]) -> Optional[StepCheckpointInfo]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ StepCheckpointInfoë¡œ ì—­ì§ë ¬í™”"""
        try:
            return StepCheckpointInfo(
                step_name=data["step_name"],
                step_class_name=data["step_class_name"],
                checkpoint_path=Path(data["checkpoint_path"]) if data.get("checkpoint_path") else None,
                checkpoint_size_mb=data["checkpoint_size_mb"],
                pytorch_valid=data["pytorch_valid"],
                parameter_count=data["parameter_count"],
                last_modified=data["last_modified"],
                step_available=data["step_available"],
                status=StepStatus(data["status"]),
                priority=StepPriority(data["priority"]),
                alternative_checkpoints=[Path(p) for p in data.get("alternative_checkpoints", [])],
                metadata=data.get("metadata", {}),
                validation_info=data.get("validation_info", {}),
                confidence_score=data["confidence_score"]
            )
        except Exception as e:
            self.logger.debug(f"Step ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return None

    # ==============================================
    # ğŸ”¥ ê³µê°œ ì¡°íšŒ ë©”ì„œë“œë“¤
    # ==============================================

    def get_available_steps(self) -> List[StepCheckpointInfo]:
        """ë¡œë“œ ê°€ëŠ¥í•œ Stepë“¤ ë°˜í™˜"""
        return [step for step in self.detected_steps.values() if step.step_available]

    def get_steps_by_priority(self, priority: StepPriority) -> List[StepCheckpointInfo]:
        """ìš°ì„ ìˆœìœ„ë³„ Stepë“¤ ë°˜í™˜"""
        return [step for step in self.detected_steps.values() if step.priority == priority]

    def get_step_by_name(self, step_name: str) -> Optional[StepCheckpointInfo]:
        """ì´ë¦„ìœ¼ë¡œ Step ì¡°íšŒ"""
        return self.detected_steps.get(step_name)

    def get_critical_steps(self) -> List[StepCheckpointInfo]:
        """í•„ìˆ˜ Stepë“¤ ë°˜í™˜"""
        return self.get_steps_by_priority(StepPriority.CRITICAL)

    def get_validated_steps(self) -> Dict[str, StepCheckpointInfo]:
        """PyTorch ê²€ì¦ëœ Stepë“¤ë§Œ ë°˜í™˜"""
        return {name: step for name, step in self.detected_steps.items() if step.pytorch_valid}

    def get_all_step_paths(self) -> Dict[str, Path]:
        """ëª¨ë“  Stepì˜ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {name: step.checkpoint_path for name, step in self.detected_steps.items() 
                if step.checkpoint_path}

    def check_pipeline_readiness(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        try:
            critical_steps = self.get_critical_steps()
            available_critical = [step for step in critical_steps if step.step_available]
            
            total_steps = len(STEP_CHECKPOINT_PATTERNS)
            available_steps = len(self.get_available_steps())
            validated_steps = len(self.get_validated_steps())
            
            readiness = {
                "pipeline_ready": len(available_critical) >= len(critical_steps),
                "critical_steps_ready": len(available_critical),
                "critical_steps_total": len(critical_steps),
                "total_steps_available": available_steps,
                "total_steps_possible": total_steps,
                "validated_steps": validated_steps,
                "readiness_score": available_steps / total_steps if total_steps > 0 else 0,
                "missing_critical_steps": [step.step_name for step in critical_steps if not step.step_available],
                "summary": f"{available_steps}/{total_steps} Steps available, {validated_steps} validated"
            }
            
            return readiness
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {"pipeline_ready": False, "error": str(e)}

# ==============================================
# ğŸ”¥ PipelineManager ì—°ë™ ì„¤ì • ìƒì„±ê¸°
# ==============================================

class StepPipelineConfigGenerator:
    """
    ğŸ”— Step íƒì§€ ê²°ê³¼ë¥¼ PipelineManager ì„¤ì •ìœ¼ë¡œ ë³€í™˜
    âœ… PipelineManager ì™„ì „ í˜¸í™˜
    âœ… Step í´ë˜ìŠ¤ ê¸°ë°˜ ì„¤ì • ìƒì„±
    âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€
    """
    
    def __init__(self, detector: StepBasedDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.StepPipelineConfigGenerator")
    
    def generate_pipeline_config(self) -> Dict[str, Any]:
        """PipelineManagerìš© ì™„ì „í•œ ì„¤ì • ìƒì„±"""
        try:
            config = {
                "steps": [],
                "step_mappings": {},
                "checkpoint_paths": {},
                "priority_rankings": {},
                "pipeline_metadata": {
                    "total_steps": len(self.detector.detected_steps),
                    "available_steps": len(self.detector.get_available_steps()),
                    "validated_steps": len(self.detector.get_validated_steps()),
                    "generation_time": time.time(),
                    "detector_version": "1.0",
                    "scan_stats": self.detector.scan_stats
                },
                "readiness_check": self.detector.check_pipeline_readiness()
            }
            
            for step_name, step_info in self.detector.detected_steps.items():
                # Step ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±
                step_config = {
                    "step_name": step_name,
                    "step_class": step_info.step_class_name,
                    "checkpoint_path": str(step_info.checkpoint_path) if step_info.checkpoint_path else None,
                    "device": self.detector.device,  # ğŸ”¥ íƒì§€ê¸°ì˜ ë””ë°”ì´ìŠ¤ ì‚¬ìš©
                    "precision": "fp16" if self.detector.device_info["supports_fp16"] else "fp32",
                    "step_available": step_info.step_available,
                    "pytorch_validated": step_info.pytorch_valid,
                    "parameter_count": step_info.parameter_count,
                    "priority": step_info.priority.name,
                    "status": step_info.status.value,
                    "confidence_score": step_info.confidence_score,
                    "input_size": self._get_input_size_for_step(step_name),
                    "device_optimized": True,  # ğŸ”¥ ë””ë°”ì´ìŠ¤ ìµœì í™” ì ìš©ë¨
                    "metadata": {
                        **step_info.metadata,
                        "auto_detected": True,
                        "checkpoint_size_mb": step_info.checkpoint_size_mb,
                        "alternative_checkpoints": [str(p) for p in step_info.alternative_checkpoints],
                        "validation_info": step_info.validation_info,
                        "device_config": self.detector.get_optimal_config_for_device()  # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì„¤ì • í¬í•¨
                    }
                }
                config["steps"].append(step_config)
                
                # Step ë§¤í•‘
                config["step_mappings"][step_name] = {
                    "class_name": step_info.step_class_name,
                    "available": step_info.step_available,
                    "priority": step_info.priority.value
                }
                
                # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
                if step_info.checkpoint_path:
                    config["checkpoint_paths"][step_name] = {
                        "primary": str(step_info.checkpoint_path),
                        "alternatives": [str(p) for p in step_info.alternative_checkpoints],
                        "size_mb": step_info.checkpoint_size_mb,
                        "validated": step_info.pytorch_valid
                    }
                
                # ìš°ì„ ìˆœìœ„ ë­í‚¹
                config["priority_rankings"][step_name] = {
                    "priority_level": step_info.priority.value,
                    "priority_name": step_info.priority.name,
                    "confidence_score": step_info.confidence_score,
                    "step_available": step_info.step_available,
                    "parameter_count": step_info.parameter_count
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ PipelineManager ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _get_input_size_for_step(self, step_name: str) -> Tuple[int, int]:
        """Stepë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸°"""
        size_mapping = {
            "step_01_human_parsing": (512, 512),
            "step_02_pose_estimation": (368, 368),
            "step_03_cloth_segmentation": (320, 320),
            "step_04_geometric_matching": (512, 384),
            "step_05_cloth_warping": (512, 384),
            "step_06_virtual_fitting": (512, 512),
            "step_07_post_processing": (512, 512),
            "step_08_quality_assessment": (224, 224)
        }
        return size_mapping.get(step_name, (512, 512))

    def generate_model_loader_compatible_config(self) -> Dict[str, Any]:
        """ê¸°ì¡´ ModelLoader í˜¸í™˜ ì„¤ì • ìƒì„±"""
        try:
            model_configs = []
            
            for step_name, step_info in self.detector.detected_steps.items():
                if not step_info.step_available:
                    continue
                
                # ModelLoader í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜
                model_config = {
                    "name": f"{step_name}_model",
                    "model_type": step_info.step_class_name,
                    "checkpoint_path": str(step_info.checkpoint_path) if step_info.checkpoint_path else None,
                    "device": "auto",
                    "precision": "fp16",
                    "step_name": step_name,
                    "step_class": step_info.step_class_name,
                    "pytorch_validated": step_info.pytorch_valid,
                    "parameter_count": step_info.parameter_count,
                    "priority": step_info.priority.name,
                    "metadata": {
                        **step_info.metadata,
                        "step_based": True,
                        "auto_detected": True
                    }
                }
                model_configs.append(model_config)
            
            return {
                "model_configs": model_configs,
                "total_models": len(model_configs),
                "step_based_detection": True,
                "generation_time": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader í˜¸í™˜ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ ë° íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_step_based_detector(
    search_paths: Optional[List[Path]] = None,
    enable_pytorch_validation: bool = True,
    enable_step_loading: bool = True,
    max_workers: int = 4,
    device: Optional[str] = None,  # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì„ íƒì  ì§€ì •
    **kwargs
) -> StepBasedDetector:
    """Step ê¸°ë°˜ íƒì§€ê¸° ìƒì„± íŒ©í† ë¦¬"""
    return StepBasedDetector(
        search_paths=search_paths,
        enable_pytorch_validation=enable_pytorch_validation,
        enable_step_loading=enable_step_loading,
        max_workers=max_workers,
        device=device,  # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì „ë‹¬
        **kwargs
    )

def quick_step_detection(
    step_filter: Optional[List[str]] = None,
    min_confidence: float = 0.5,
    force_rescan: bool = False,
    validated_only: bool = False
) -> Dict[str, Any]:
    """ë¹ ë¥¸ Step íƒì§€ ë° ê²°ê³¼ ë°˜í™˜"""
    try:
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_step_based_detector()
        detected_steps = detector.detect_all_steps(
            force_rescan=force_rescan,
            step_filter=step_filter,
            min_confidence=min_confidence
        )
        
        # ê²€ì¦ëœ Stepë§Œ í•„í„°ë§ (ì˜µì…˜)
        if validated_only:
            detected_steps = detector.get_validated_steps()
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "total_steps": len(detected_steps),
            "available_steps": len([s for s in detected_steps.values() if s.step_available]),
            "validated_steps": len([s for s in detected_steps.values() if s.pytorch_valid]),
            "steps_by_priority": {},
            "steps_by_status": {},
            "top_steps": {},
            "pipeline_readiness": detector.check_pipeline_readiness(),
            "scan_stats": detector.scan_stats
        }
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        for step in detected_steps.values():
            priority = step.priority.name
            if priority not in summary["steps_by_priority"]:
                summary["steps_by_priority"][priority] = []
            summary["steps_by_priority"][priority].append({
                "name": step.step_name,
                "class": step.step_class_name,
                "available": step.step_available,
                "confidence": step.confidence_score,
                "checkpoint_path": str(step.checkpoint_path) if step.checkpoint_path else None
            })
        
        # ìƒíƒœë³„ ë¶„ë¥˜
        for step in detected_steps.values():
            status = step.status.value
            if status not in summary["steps_by_status"]:
                summary["steps_by_status"][status] = []
            summary["steps_by_status"][status].append(step.step_name)
        
        # ìš°ì„ ìˆœìœ„ë³„ ìµœê³  Step
        priorities = set(step.priority for step in detected_steps.values())
        for priority in priorities:
            priority_steps = [s for s in detected_steps.values() if s.priority == priority]
            if priority_steps:
                best_step = max(priority_steps, key=lambda s: (s.step_available, s.pytorch_valid, s.confidence_score))
                summary["top_steps"][priority.name] = {
                    "name": best_step.step_name,
                    "class": best_step.step_class_name,
                    "available": best_step.step_available,
                    "confidence": best_step.confidence_score,
                    "parameter_count": best_step.parameter_count,
                    "checkpoint_path": str(best_step.checkpoint_path) if best_step.checkpoint_path else None
                }
        
        return summary
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ Step íƒì§€ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def generate_pipeline_config_from_steps(
    detector: Optional[StepBasedDetector] = None,
    **detection_kwargs
) -> Dict[str, Any]:
    """
    Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° PipelineManager ì„¤ì • ìƒì„±
    ìˆœí™˜ì°¸ì¡° ë°©ì§€í•˜ë©° ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì¶œë ¥
    """
    try:
        logger.info("ğŸ¯ Step ê¸°ë°˜ PipelineManager ì„¤ì • ìƒì„± ì‹œì‘...")
        
        # íƒì§€ê¸°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if detector is None:
            detector = create_step_based_detector(**detection_kwargs)
            detected_steps = detector.detect_all_steps()
        else:
            detected_steps = detector.detected_steps
        
        if not detected_steps:
            logger.warning("âš ï¸ íƒì§€ëœ Stepì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No steps detected"}
        
        # ì„¤ì • ìƒì„±ê¸° ì‚¬ìš©
        config_generator = StepPipelineConfigGenerator(detector)
        pipeline_config = config_generator.generate_pipeline_safe_call(config)
        model_loader_config = config_generator.generate_model_loader_compatible_safe_call(config)
        
        # ìµœì¢… ê²°ê³¼
        available_count = len(detector.get_available_steps())
        validated_count = len(detector.get_validated_steps())
        
        result = {
            "success": True,
            "pipeline_config": pipeline_config,
            "model_loader_config": model_loader_config,
            "detection_summary": {
                "total_steps": len(detected_steps),
                "available_steps": available_count,
                "validated_steps": validated_count,
                "availability_rate": available_count / len(detected_steps) if detected_steps else 0,
                "validation_rate": validated_count / len(detected_steps) if detected_steps else 0,
                "scan_duration": detector.scan_stats["scan_duration"],
                "confidence_avg": sum(s.confidence_score for s in detected_steps.values()) / len(detected_steps),
                "total_parameters": sum(s.parameter_count for s in detected_steps.values())
            },
            "readiness_check": detector.check_pipeline_readiness()
        }
        
        logger.info(f"âœ… Step ê¸°ë°˜ ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_steps)}ê°œ Step ({available_count}ê°œ ì‚¬ìš© ê°€ëŠ¥)")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Step ê¸°ë°˜ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def validate_step_checkpoints(detected_steps: Dict[str, StepCheckpointInfo]) -> Dict[str, Any]:
    """Step ì²´í¬í¬ì¸íŠ¸ë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
    try:
        validation_result = {
            "valid_steps": [],
            "invalid_steps": [],
            "missing_checkpoints": [],
            "corrupted_checkpoints": [],
            "available_steps": [],
            "unavailable_steps": [],
            "total_size_gb": 0.0,
            "total_parameters": 0
        }
        
        for step_name, step_info in detected_steps.items():
            try:
                # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
                if step_info.checkpoint_path and step_info.checkpoint_path.exists():
                    validation_result["valid_steps"].append(step_name)
                    validation_result["total_size_gb"] += step_info.checkpoint_size_mb / 1024
                    validation_result["total_parameters"] += step_info.parameter_count
                    
                    # Step í´ë˜ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
                    if step_info.step_available:
                        validation_result["available_steps"].append(step_name)
                    else:
                        validation_result["unavailable_steps"].append(step_name)
                        
                    # PyTorch ê²€ì¦ ìƒíƒœ í™•ì¸
                    if not step_info.pytorch_valid:
                        validation_result["corrupted_checkpoints"].append({
                            "step": step_name,
                            "path": str(step_info.checkpoint_path),
                            "reason": "PyTorch validation failed"
                        })
                else:
                    validation_result["missing_checkpoints"].append({
                        "step": step_name,
                        "expected_path": str(step_info.checkpoint_path) if step_info.checkpoint_path else "Unknown"
                    })
                
            except Exception as e:
                validation_result["invalid_steps"].append({
                    "step": step_name,
                    "error": str(e)
                })
        
        validation_result["summary"] = {
            "total_steps": len(detected_steps),
            "valid_count": len(validation_result["valid_steps"]),
            "invalid_count": len(validation_result["invalid_steps"]),
            "missing_count": len(validation_result["missing_checkpoints"]),
            "corrupted_count": len(validation_result["corrupted_checkpoints"]),
            "available_count": len(validation_result["available_steps"]),
            "validation_rate": len(validation_result["valid_steps"]) / len(detected_steps) if detected_steps else 0,
            "availability_rate": len(validation_result["available_steps"]) / len(detected_steps) if detected_steps else 0
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"Step ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def integrate_with_pipeline_manager(
    pipeline_manager_instance = None,
    auto_configure: bool = True,
    **detection_kwargs
) -> Dict[str, Any]:
    """Step íƒì§€ ë° PipelineManager í†µí•© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        logger.info("ğŸ¯ Step íƒì§€ ë° PipelineManager í†µí•© ì‹œì‘...")
        
        # íƒì§€ ì‹¤í–‰
        detector = create_step_based_detector(**detection_kwargs)
        detected_steps = detector.detect_all_steps()
        
        if not detected_steps:
            logger.warning("âš ï¸ íƒì§€ëœ Stepì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No steps detected"}
        
        # ì„¤ì • ìƒì„±ê¸°
        config_generator = StepPipelineConfigGenerator(detector)
        
        # PipelineManager ì„¤ì • ìƒì„±
        pipeline_config = config_generator.generate_pipeline_safe_call(config)
        
        # PipelineManagerì™€ í†µí•© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        integration_result = {}
        if auto_configure and pipeline_manager_instance:
            try:
                # PipelineManager ì„¤ì • ì ìš©
                if hasattr(pipeline_manager_instance, 'configure_from_detection'):
                    pipeline_manager_instance.configure_from_detection(pipeline_config)
                    integration_result["configuration_applied"] = True
                elif hasattr(pipeline_manager_instance, 'update_config'):
                    pipeline_manager_instance.update_config(pipeline_config)
                    integration_result["configuration_updated"] = True
                else:
                    logger.warning("âš ï¸ PipelineManagerì— ì„¤ì • ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                    integration_result["configuration_method_missing"] = True
                
            except Exception as e:
                logger.warning(f"âš ï¸ PipelineManager í†µí•© ì‹¤íŒ¨: {e}")
                integration_result["integration_error"] = str(e)
        
        readiness_check = detector.check_pipeline_readiness()
        
        return {
            "success": True,
            "detected_count": len(detected_steps),
            "available_count": len(detector.get_available_steps()),
            "step_names": list(detected_steps.keys()),
            "integration": integration_result,
            "config": pipeline_config,
            "readiness": readiness_check,
            "pipeline_ready": readiness_check.get("pipeline_ready", False)
        }
        
    except Exception as e:
        logger.error(f"âŒ íƒì§€ ë° í†µí•© ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„° í´ë˜ìŠ¤ë“¤
# ==============================================

class StepToModelLoaderAdapter:
    """
    ê¸°ì¡´ ModelLoaderì™€ í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„°
    Step íƒì§€ ê²°ê³¼ë¥¼ ModelLoader í˜•íƒœë¡œ ë³€í™˜
    """
    
    def __init__(self, detector: StepBasedDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.StepToModelLoaderAdapter")
    
    def get_model_configs(self) -> List[Dict[str, Any]]:
        """ModelLoaderê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ì„¤ì • ë°˜í™˜"""
        try:
            config_generator = StepPipelineConfigGenerator(self.detector)
            model_loader_config = config_generator.generate_model_loader_compatible_safe_call(config)
            return model_loader_config.get("model_configs", [])
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def register_steps_to_loader(self, model_loader):
        """íƒì§€ëœ Stepë“¤ì„ ModelLoaderì— ë“±ë¡"""
        try:
            detected_steps = self.detector.detected_steps
            registered_count = 0
            
            for step_name, step_info in detected_steps.items():
                if not step_info.step_available:
                    continue
                
                try:
                    # ModelLoader í˜¸í™˜ ì„¤ì • ìƒì„±
                    model_config = {
                        "name": f"{step_name}_model",
                        "model_type": step_info.step_class_name,
                        "checkpoint_path": str(step_info.checkpoint_path) if step_info.checkpoint_path else None,
                        "device": "auto",
                        "precision": "fp16",
                        "step_based": True,
                        "step_name": step_name,
                        "pytorch_validated": step_info.pytorch_valid,
                        "parameter_count": step_info.parameter_count,
                        "confidence_score": step_info.confidence_score
                    }
                    
                    # ModelLoaderì— ë“±ë¡
                    if hasattr(model_loader, 'register_model'):
                        model_loader.register_model(f"{step_name}_model", model_config)
                        registered_count += 1
                    elif hasattr(model_loader, '_register_model'):
                        model_loader._register_model(f"{step_name}_model", model_config)
                        registered_count += 1
                        
                except Exception as e:
                    self.logger.warning(f"Step {step_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"âœ… {registered_count}ê°œ Step ë“±ë¡ ì™„ë£Œ")
            return registered_count
            
        except Exception as e:
            self.logger.error(f"Step ë“±ë¡ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return 0

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤
    'StepBasedDetector',
    'StepPipelineConfigGenerator', 
    'StepToModelLoaderAdapter',
    
    # ë°ì´í„° êµ¬ì¡°
    'StepCheckpointInfo',
    'StepStatus',
    'StepPriority',
    
    # ì„¤ì • íŒ¨í„´
    'STEP_CHECKPOINT_PATTERNS',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_step_based_detector',
    'quick_step_detection',
    'generate_pipeline_config_from_steps',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_step_checkpoints',
    'integrate_with_pipeline_manager',
    
    # í•˜ìœ„ í˜¸í™˜ì„± ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±)
    'StepDetector',
    'create_step_detector',
    'quick_detection',
    'generate_config_from_steps'
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
StepDetector = StepBasedDetector
create_step_detector = create_step_based_detector
quick_detection = quick_step_detection
generate_config_from_steps = generate_pipeline_config_from_steps

logger.info("âœ… Step ê¸°ë°˜ ìë™ íƒì§€ ì‹œìŠ¤í…œ v1.0 ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ¯ Step í´ë˜ìŠ¤ = AI ëª¨ë¸ + ì²˜ë¦¬ ë¡œì§ í†µí•© êµ¬ì¡° ì™„ë²½ ì§€ì›")
logger.info("ğŸ”— PipelineManager ì™„ì „ ì—°ë™ ë° ê¸°ì¡´ ModelLoader í˜¸í™˜ì„± ìœ ì§€")