# backend/app/ai_pipeline/utils/model_loader_complete.py
"""
ğŸ”¥ MyCloset AI - ì™„ì „í•œ ModelLoader êµ¬í˜„ (ëˆ„ë½ëœ ë©”ì„œë“œ ëª¨ë‘ ì¶”ê°€)
================================================================================
âœ… list_available_models() ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… register_step_requirements() ë©”ì„œë“œ ì™„ì „ êµ¬í˜„ 
âœ… ì‹¤ì œ 370GB ëª¨ë¸ íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™
âœ… BaseStepMixin ìš”êµ¬ì‚¬í•­ 100% ì¶©ì¡±
âœ… M3 Max ìµœì í™” ì§€ì›
âœ… ì—ëŸ¬ ë³µêµ¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
================================================================================
"""

import os
import gc
import asyncio
import logging
import threading
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Type, Union, Tuple, Set
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import json
import hashlib
import time

# ì•ˆì „í•œ ì„í¬íŠ¸
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# GPU ì„¤ì • ì„í¬íŠ¸
try:
    from app.core.gpu_config import safe_mps_empty_cache, GPUConfig
    GPU_CONFIG_AVAILABLE = True
except ImportError:
    GPU_CONFIG_AVAILABLE = False
    def safe_mps_empty_cache():
        return {"success": False, "error": "GPU config not available"}

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ í´ë˜ìŠ¤"""
    name: str
    path: str
    size_mb: float
    model_type: str
    step_class: str
    loaded: bool = False
    device: str = "cpu"
    precision: str = "fp32"
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepRequirement:
    """Step ìš”êµ¬ì‚¬í•­ í´ë˜ìŠ¤"""
    step_name: str
    model_name: str
    model_class: str
    input_size: Tuple[int, int]
    required: bool = True
    alternatives: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

class CompleteModelLoader:
    """ì™„ì „í•œ ModelLoader êµ¬í˜„"""
    
    def __init__(self, models_dir: str = "backend/ai_models", device: str = "auto"):
        self.models_dir = Path(models_dir)
        self.device = self._setup_device(device)
        self.loaded_models: Dict[str, Any] = {}
        self.available_models: Dict[str, ModelInfo] = {}
        self.step_requirements: Dict[str, List[StepRequirement]] = {}
        self.model_cache: Dict[str, Any] = {}
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        # GPU ì„¤ì •
        if GPU_CONFIG_AVAILABLE:
            self.gpu_config = GPUConfig()
        else:
            self.gpu_config = None
            
        # ì´ˆê¸°í™”
        self._scan_available_models()
        self._load_model_registry()
        
        logger.info(f"âœ… CompleteModelLoader ì´ˆê¸°í™” ì™„ë£Œ (device: {self.device})")
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if TORCH_AVAILABLE:
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        return device
        
    def _scan_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ìŠ¤ìº”"""
        logger.info("ğŸ” ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        if not self.models_dir.exists():
            logger.warning(f"âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.models_dir}")
            return
            
        scanned_count = 0
        
        # ì§€ì›í•˜ëŠ” í™•ì¥ì
        extensions = [".pth", ".bin", ".pkl", ".ckpt"]
        
        for ext in extensions:
            for model_file in self.models_dir.rglob(f"*{ext}"):
                if "cleanup_backup" in str(model_file):
                    continue  # ë°±ì—… íŒŒì¼ ì œì™¸
                    
                try:
                    # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    size_mb = model_file.stat().st_size / (1024 * 1024)
                    relative_path = model_file.relative_to(self.models_dir)
                    
                    # ëª¨ë¸ íƒ€ì… ì¶”ì •
                    model_type = self._detect_model_type(model_file)
                    step_class = self._detect_step_class(model_file)
                    
                    model_info = ModelInfo(
                        name=model_file.stem,
                        path=str(relative_path),
                        size_mb=round(size_mb, 2),
                        model_type=model_type,
                        step_class=step_class,
                        metadata={
                            "extension": ext,
                            "parent_dir": model_file.parent.name,
                            "full_path": str(model_file)
                        }
                    )
                    
                    self.available_models[model_info.name] = model_info
                    scanned_count += 1
                    
                    if size_mb > 100:  # 100MB ì´ìƒë§Œ ë¡œê¹…
                        logger.debug(f"ğŸ“¦ ëª¨ë¸ ë°œê²¬: {model_info.name} ({size_mb:.1f}MB)")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨ {model_file}: {e}")
                    
        logger.info(f"âœ… ëª¨ë¸ ìŠ¤ìº” ì™„ë£Œ: {scanned_count}ê°œ ë°œê²¬")
        
    def _detect_model_type(self, model_file: Path) -> str:
        """ëª¨ë¸ íƒ€ì… ê°ì§€"""
        filename = model_file.name.lower()
        
        type_keywords = {
            "human_parsing": ["schp", "atr", "lip", "graphonomy", "parsing"],
            "pose_estimation": ["pose", "openpose", "body_pose", "hand_pose"],
            "cloth_segmentation": ["u2net", "sam", "segment", "cloth"],
            "geometric_matching": ["gmm", "geometric", "matching", "tps"],
            "cloth_warping": ["warp", "tps", "deformation"],
            "virtual_fitting": ["viton", "hrviton", "ootd", "diffusion", "vae"],
            "post_processing": ["esrgan", "enhancement", "super_resolution"],
            "quality_assessment": ["lpips", "quality", "metric", "clip"]
        }
        
        for model_type, keywords in type_keywords.items():
            if any(keyword in filename for keyword in keywords):
                return model_type
                
        return "unknown"
        
    def _detect_step_class(self, model_file: Path) -> str:
        """Step í´ë˜ìŠ¤ ê°ì§€"""
        parent_dir = model_file.parent.name.lower()
        
        if parent_dir.startswith("step_"):
            step_mapping = {
                "step_01": "HumanParsingStep",
                "step_02": "PoseEstimationStep", 
                "step_03": "ClothSegmentationStep",
                "step_04": "GeometricMatchingStep",
                "step_05": "ClothWarpingStep",
                "step_06": "VirtualFittingStep",
                "step_07": "PostProcessingStep",
                "step_08": "QualityAssessmentStep"
            }
            
            for prefix, step_class in step_mapping.items():
                if parent_dir.startswith(prefix):
                    return step_class
                    
        return "UnknownStep"
        
    def _load_model_registry(self):
        """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ"""
        registry_file = self.models_dir / "model_registry.json"
        
        if registry_file.exists():
            try:
                with open(registry_file, 'r') as f:
                    registry_data = json.load(f)
                    
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë°ì´í„°ë¡œ ëª¨ë¸ ì •ë³´ ë³´ê°•
                for model_name, model_info in self.available_models.items():
                    if model_name in registry_data:
                        model_info.metadata.update(registry_data[model_name])
                        
                logger.info(f"âœ… ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(registry_data)}ê°œ í•­ëª©")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
    # ================================================================
    # ğŸ”¥ BaseStepMixinì—ì„œ ìš”êµ¬í•˜ëŠ” í•„ìˆ˜ ë©”ì„œë“œë“¤
    # ================================================================
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (BaseStepMixin í•„ìˆ˜ ë©”ì„œë“œ)"""
        models = []
        
        for model_name, model_info in self.available_models.items():
            # í•„í„°ë§
            if step_class and model_info.step_class != step_class:
                continue
            if model_type and model_info.model_type != model_type:
                continue
                
            models.append({
                "name": model_info.name,
                "path": model_info.path,
                "size_mb": model_info.size_mb,
                "model_type": model_info.model_type,
                "step_class": model_info.step_class,
                "loaded": model_info.loaded,
                "device": model_info.device,
                "metadata": model_info.metadata
            })
            
        # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
        models.sort(key=lambda x: x["size_mb"], reverse=True)
        
        logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ìš”ì²­: {len(models)}ê°œ ë°˜í™˜ (step={step_class}, type={model_type})")
        return models
        
    def register_step_requirements(self, step_name: str, requirements: List[Dict[str, Any]]) -> bool:
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (BaseStepMixin í•„ìˆ˜ ë©”ì„œë“œ)"""
        try:
            step_reqs = []
            
            for req_data in requirements:
                step_req = StepRequirement(
                    step_name=step_name,
                    model_name=req_data.get("model_name", ""),
                    model_class=req_data.get("model_class", ""),
                    input_size=tuple(req_data.get("input_size", (512, 512))),
                    required=req_data.get("required", True),
                    alternatives=req_data.get("alternatives", []),
                    metadata=req_data.get("metadata", {})
                )
                step_reqs.append(step_req)
                
            self.step_requirements[step_name] = step_reqs
            
            logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_name} ({len(step_reqs)}ê°œ)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
            
    def get_step_requirements(self, step_name: str) -> List[Dict[str, Any]]:
        """Step ìš”êµ¬ì‚¬í•­ ì¡°íšŒ"""
        if step_name not in self.step_requirements:
            return []
            
        requirements = []
        for req in self.step_requirements[step_name]:
            requirements.append({
                "model_name": req.model_name,
                "model_class": req.model_class,
                "input_size": req.input_size,
                "required": req.required,
                "alternatives": req.alternatives,
                "metadata": req.metadata
            })
            
        return requirements
        
    # ================================================================
    # ğŸ”¥ ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ================================================================
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        if model_name in self.loaded_models:
            logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
            return self.loaded_models[model_name]
            
        if model_name not in self.available_models:
            logger.error(f"âŒ ëª¨ë¸ ì—†ìŒ: {model_name}")
            return None
            
        try:
            # ë¹„ë™ê¸°ë¡œ ëª¨ë¸ ë¡œë”© ì‹¤í–‰
            loop = asyncio.get_event_loop()
            model = await loop.run_in_executor(
                self.executor, 
                self._load_model_sync,
                model_name,
                kwargs
            )
            
            if model is not None:
                self.loaded_models[model_name] = model
                self.available_models[model_name].loaded = True
                self.available_models[model_name].device = self.device
                
                logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
            
    def _load_model_sync(self, model_name: str, kwargs: Dict[str, Any]) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë”© (ì‹¤ì œ êµ¬í˜„)"""
        if not TORCH_AVAILABLE:
            logger.error("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return None
            
        model_info = self.available_models[model_name]
        model_path = self.models_dir / model_info.path
        
        if not model_path.exists():
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            return None
            
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device in ["mps", "cuda"]:
                safe_mps_empty_cache()
                
            # í™•ì¥ìë³„ ë¡œë”© ë°©ì‹
            if model_path.suffix == ".pth":
                model = torch.load(model_path, map_location=self.device)
            elif model_path.suffix == ".bin":
                model = torch.load(model_path, map_location=self.device)
            elif model_path.suffix == ".pkl":
                import pickle
                with open(model_path, 'rb') as f:
                    model = pickle.load(f)
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {model_path.suffix}")
                return None
                
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(model, 'to'):
                model = model.to(self.device)
                
            # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            if hasattr(model, 'eval'):
                model.eval()
                
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({model_info.size_mb:.1f}MB)")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
            
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë”© (BaseStepMixin í˜¸í™˜)"""
        return asyncio.run(self.load_model_async(model_name, **kwargs))
        
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if model_name in self.loaded_models:
            try:
                del self.loaded_models[model_name]
                self.available_models[model_name].loaded = False
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device in ["mps", "cuda"]:
                    safe_mps_empty_cache()
                    
                gc.collect()
                
                logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return False
                
        return True
        
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if model_name in self.available_models:
            model_info = self.available_models[model_name]
            return {
                "name": model_info.name,
                "path": model_info.path,
                "size_mb": model_info.size_mb,
                "model_type": model_info.model_type,
                "step_class": model_info.step_class,
                "loaded": model_info.loaded,
                "device": model_info.device,
                "metadata": model_info.metadata
            }
        return None
        
    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        memory_info = {
            "loaded_models": len(self.loaded_models),
            "total_models": len(self.available_models),
            "device": self.device
        }
        
        if TORCH_AVAILABLE and self.device == "cuda":
            memory_info.update({
                "gpu_allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                "gpu_reserved_mb": torch.cuda.memory_reserved() / (1024**2)
            })
        elif TORCH_AVAILABLE and self.device == "mps":
            memory_info.update({
                "mps_allocated_mb": torch.mps.current_allocated_memory() / (1024**2) if hasattr(torch.mps, 'current_allocated_memory') else 0
            })
            
        return memory_info
        
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
        for model_name in list(self.loaded_models.keys()):
            self.unload_model(model_name)
            
        # ìºì‹œ ì •ë¦¬
        self.model_cache.clear()
        
        # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
        self.executor.shutdown(wait=True)
        
        logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ================================================================
# ğŸ”¥ ê¸°ì¡´ ModelLoader í´ë˜ìŠ¤ í™•ì¥
# ================================================================

def patch_existing_model_loader():
    """ê¸°ì¡´ ModelLoaderì— ëˆ„ë½ëœ ë©”ì„œë“œë“¤ ì¶”ê°€"""
    try:
        from app.ai_pipeline.utils.model_loader import ModelLoader
        
        # ëˆ„ë½ëœ ë©”ì„œë“œë“¤ ì¶”ê°€
        if not hasattr(ModelLoader, 'list_available_models'):
            def list_available_models(self, step_class=None, model_type=None):
                # CompleteModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‚¬ìš©
                complete_loader = CompleteModelLoader()
                return complete_loader.list_available_models(step_class, model_type)
                
            ModelLoader.list_available_models = list_available_models
            
        if not hasattr(ModelLoader, 'register_step_requirements'):
            def register_step_requirements(self, step_name, requirements):
                complete_loader = CompleteModelLoader()
                return complete_loader.register_step_requirements(step_name, requirements)
                
            ModelLoader.register_step_requirements = register_step_requirements
            
        logger.info("âœ… ê¸°ì¡´ ModelLoader íŒ¨ì¹˜ ì™„ë£Œ")
        return True
        
    except ImportError:
        logger.warning("âš ï¸ ê¸°ì¡´ ModelLoader ì—†ìŒ, íŒ¨ì¹˜ ìŠ¤í‚µ")
        return False

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_model_loader = None

def get_global_model_loader() -> CompleteModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    if _global_model_loader is None:
        _global_model_loader = CompleteModelLoader()
        
    return _global_model_loader

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    loader = CompleteModelLoader()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    models = loader.list_available_models()
    print(f"ğŸ” ì´ {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
    
    for model in models[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
        print(f"ğŸ“¦ {model['name']}: {model['size_mb']:.1f}MB ({model['step_class']})")
        
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    memory = loader.get_memory_usage()
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ í˜„í™©: {memory}")
    
    loader.cleanup()