#!/usr/bin/env python3
"""
ğŸ”¥ AI ëª¨ë¸ë³„ ì •í™•í•œ ì•„í‚¤í…ì²˜ ì •ì˜
================================================================================
âœ… ê° ëª¨ë¸ë³„ ì •í™•í•œ ì‹ ê²½ë§ êµ¬ì¡°
âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì  ìƒì„±
âœ… Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ í˜¸í™˜
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional
import numpy as np
import time
import psutil
import os

class HRNetPoseModel(nn.Module):
    """HRNet ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    def __init__(self, num_joints=17):
        super().__init__()
        self.num_joints = num_joints
        
        # HRNet backbone
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        
        # HRNet stages
        self.stage1 = self._make_stage(64, 32, 1)
        self.stage2 = self._make_stage(32, 64, 1)
        self.stage3 = self._make_stage(64, 128, 1)
        self.stage4 = self._make_stage(128, 256, 1)
        
        # Final layer
        self.final_layer = nn.Conv2d(256, num_joints, kernel_size=1)
        
    def _make_stage(self, inplanes, planes, num_blocks):
        layers = []
        layers.append(nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(num_blocks - 1):
            layers.append(nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Stem
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        
        # Stages
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        
        # Final layer
        heatmaps = self.final_layer(x)
        
        return heatmaps

class GraphonomyModel(nn.Module):
    """Graphonomy ê¸°ë°˜ ì¸ê°„ íŒŒì‹± ëª¨ë¸ - AdvancedGraphonomyResNetASPP + ProgressiveParsingModule ì‚¬ìš©"""
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ê¸°ì¡´ì— êµ¬í˜„ëœ AdvancedGraphonomyResNetASPP ì‚¬ìš©
        try:
            from app.ai_pipeline.steps.human_parsing.models.graphonomy_models import AdvancedGraphonomyResNetASPP
            self.base_model = AdvancedGraphonomyResNetASPP(num_classes=num_classes)
        except ImportError:
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸
            self.base_model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, num_classes, 1)
            )
        
        # Progressive Parsing Module ì¶”ê°€
        try:
            from app.ai_pipeline.steps.human_parsing.models.progressive_parsing import ProgressiveParsingModule
            self.progressive_module = ProgressiveParsingModule(num_classes=num_classes)
            self.use_progressive = True
        except ImportError:
            self.use_progressive = False
        
    def forward(self, x):
        # ê¸°ë³¸ ëª¨ë¸ë¡œ ì´ˆê¸° íŒŒì‹± ìƒì„±
        initial_parsing = self.base_model(x)
        
        if self.use_progressive and hasattr(self, 'progressive_module'):
            # Progressive Parsing Moduleë¡œ ì •ì œ
            # base_featuresëŠ” initial_parsingì—ì„œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŠ¹ì§• ì‚¬ìš©)
            base_features = F.interpolate(initial_parsing, scale_factor=0.5, mode='bilinear', align_corners=False)
            progressive_results = self.progressive_module(initial_parsing, base_features)
            
            # ìµœì¢… ê²°ê³¼ëŠ” ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ íŒŒì‹± ì‚¬ìš©
            final_parsing = progressive_results[-1]['parsing']
            return final_parsing
        else:
            return initial_parsing

class U2NetModel(nn.Module):
    """U2Net ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - ì‹¤ì œ ì‘ë™í•˜ëŠ” êµ¬ì¡°"""
    def __init__(self, out_channels=1):
        super().__init__()
        self.out_channels = out_channels
        
        # ë” ì •êµí•œ Encoder (U2Net ìŠ¤íƒ€ì¼)
        self.encoder = nn.Sequential(
            # Initial convolution
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # U2Net-style blocks
            self._make_u2net_block(64, 128),
            self._make_u2net_block(128, 256),
            self._make_u2net_block(256, 512),
        )
        
        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Decoder with skip connections
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, out_channels, kernel_size=1),
            nn.Sigmoid()  # U2Netì€ ë³´í†µ sigmoid ì‚¬ìš©
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
        
    def _make_u2net_block(self, in_channels, out_channels):
        """U2Net ìŠ¤íƒ€ì¼ ë¸”ë¡"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        # Encoder
        x = self.encoder(x)
        
        # Attention
        attn = self.attention(x)
        x = x * attn  # Attention ì ìš©
        
        # Decoder
        output = self.decoder(x)
        
        return output

class OpenPoseModel(nn.Module):
    """OpenPose ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶¤"""
    def __init__(self):
        super().__init__()
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ ëª¨ë¸ (features.0ë§Œ ì‚¬ìš©)
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # features.0
        )
        
        # 64 -> 128 ì±„ë„ í™•ì¥ì„ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´ (ì²´í¬í¬ì¸íŠ¸ì—ëŠ” ì—†ì§€ë§Œ í•„ìš”)
        self.channel_expansion = nn.Conv2d(64, 128, kernel_size=1, bias=False)
        
        # ì¶œë ¥ í—¤ë“œë“¤ (bias=Falseë¡œ ì„¤ì •í•˜ì—¬ bias í‚¤ ì œê±°)
        self.paf_out = nn.Conv2d(128, 38, kernel_size=1, bias=False)  # paf_out.weight
        self.heatmap_out = nn.Conv2d(128, 19, kernel_size=1, bias=False)  # heatmap_out.weight
        
    def forward(self, x):
        # Features
        x = self.features(x)
        
        # Channel expansion (64 -> 128)
        x = self.channel_expansion(x)
        
        # Outputs
        paf_output = self.paf_out(x)
        heatmap_output = self.heatmap_out(x)
        
        return heatmap_output

class GMMModel(nn.Module):
    """GMM (Geometric Matching Module) ëª¨ë¸ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶¤ (Vision Transformer ê¸°ë°˜)"""
    def __init__(self, num_control_points=20):
        super().__init__()
        self.num_control_points = num_control_points
        
        # 1. gmm_backbone (Vision Transformer)
        self.gmm_backbone = nn.ModuleDict({
            'patch_embed': nn.ModuleDict({
                'proj': nn.Conv2d(3, 1024, kernel_size=16, stride=16, bias=True)
            }),
            'blocks': nn.ModuleList([
                self._create_transformer_block() for _ in range(24)  # 24ê°œ ë¸”ë¡
            ]),
            'norm': nn.LayerNorm(1024),
            'head': nn.Linear(1024, 1000)  # ì›ë³¸ì€ 1000ê°œ í´ë˜ìŠ¤
        })
        
        # nn.Parameterë¥¼ ì§ì ‘ ì •ì˜
        self.cls_token = nn.Parameter(torch.randn(1, 1, 1024))
        self.pos_embed = nn.Parameter(torch.randn(1, 577, 1024))
        
        # 2. pretrained layers (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶¤)
        self.pretrained = nn.ModuleDict({
            'act_postprocess1': nn.ModuleDict({
                '0': nn.ModuleDict({
                    'project': nn.ModuleList([
                        nn.Conv2d(256, 256, 1),  # project.0.weight/bias
                    ])
                }),
                '3': nn.Conv2d(256, 256, 3, padding=1),  # act_postprocess1.3.weight/bias
                '4': nn.Conv2d(256, 256, 3, padding=1)   # act_postprocess1.4.weight/bias
            }),
            'act_postprocess2': nn.ModuleDict({
                '0': nn.ModuleDict({
                    'project': nn.ModuleList([
                        nn.Conv2d(512, 256, 1),  # project.0.weight/bias
                    ])
                }),
                '3': nn.Conv2d(256, 256, 3, padding=1),  # act_postprocess2.3.weight/bias
                '4': nn.Conv2d(256, 256, 3, padding=1)   # act_postprocess2.4.weight/bias
            }),
            'act_postprocess3': nn.ModuleDict({
                '0': nn.ModuleDict({
                    'project': nn.ModuleList([
                        nn.Conv2d(1024, 256, 1),  # project.0.weight/bias
                    ])
                }),
                '3': nn.Conv2d(256, 256, 3, padding=1)   # act_postprocess3.3.weight/bias
            }),
            'act_postprocess4': nn.ModuleDict({
                '0': nn.ModuleDict({
                    'project': nn.ModuleList([
                        nn.Conv2d(2048, 256, 1),  # project.0.weight/bias
                    ])
                }),
                '3': nn.Conv2d(256, 256, 3, padding=1),  # act_postprocess4.3.weight/bias
                '4': nn.Conv2d(256, 256, 3, padding=1)   # act_postprocess4.4.weight/bias
            })
        })
        
        # 3. scratch layers (RefineNet êµ¬ì¡° - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ì— ë§ì¶¤)
        self.scratch = nn.ModuleDict({
            'layer1_rn': nn.Conv2d(256, 256, 3, padding=1, bias=False),  # bias=Falseë¡œ ì„¤ì •
            'layer2_rn': nn.Conv2d(256, 256, 3, padding=1, bias=False),  # bias=Falseë¡œ ì„¤ì •
            'layer3_rn': nn.Conv2d(256, 256, 3, padding=1, bias=False),  # bias=Falseë¡œ ì„¤ì •
            'layer4_rn': nn.Conv2d(256, 256, 3, padding=1, bias=False),  # bias=Falseë¡œ ì„¤ì •
            'refinenet1': self._create_refinenet_block(),
            'refinenet2': self._create_refinenet_block(),
            'refinenet3': self._create_refinenet_block(),
            'refinenet4': self._create_refinenet_block(),
            'output_conv': nn.ModuleList([
                nn.Conv2d(256, 256, 3, padding=1),  # output_conv.0.weight/bias
                nn.BatchNorm2d(256),                # output_conv.1.*
                nn.Conv2d(256, 256, 3, padding=1),  # output_conv.2.weight/bias
                nn.BatchNorm2d(256),                # output_conv.3.*
                nn.Conv2d(256, num_control_points * 2, 1)  # output_conv.4.weight/bias
            ])
        })
        
    def _create_refinenet_block(self):
        """RefineNet ë¸”ë¡ ìƒì„± - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶¤"""
        return nn.ModuleDict({
            'out_conv': nn.Conv2d(256, 256, 3, padding=1),  # out_conv.weight/bias
            'resConfUnit1': nn.ModuleDict({
                'conv1': nn.Conv2d(256, 256, 3, padding=1),  # conv1.weight/bias
                'conv2': nn.Conv2d(256, 256, 3, padding=1)   # conv2.weight/bias
            }),
            'resConfUnit2': nn.ModuleDict({
                'conv1': nn.Conv2d(256, 256, 3, padding=1),  # conv1.weight/bias
                'conv2': nn.Conv2d(256, 256, 3, padding=1)   # conv2.weight/bias
            })
        })
        
    def state_dict(self, *args, **kwargs):
        """ì²´í¬í¬ì¸íŠ¸ í‚¤ì™€ ë§¤ì¹­ë˜ë„ë¡ state_dictë¥¼ ìˆ˜ì •"""
        state_dict = super().state_dict(*args, **kwargs)
        
        # cls_tokenê³¼ pos_embedë¥¼ gmm_backbone í‚¤ë¡œ ë³€ê²½
        if 'cls_token' in state_dict:
            state_dict['gmm_backbone.cls_token'] = state_dict.pop('cls_token')
        if 'pos_embed' in state_dict:
            state_dict['gmm_backbone.pos_embed'] = state_dict.pop('pos_embed')
            
        return state_dict
        
    def load_state_dict(self, state_dict, strict=True):
        """ì²´í¬í¬ì¸íŠ¸ í‚¤ë¥¼ ëª¨ë¸ í‚¤ë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œ"""
        # gmm_backbone í‚¤ë¥¼ ëª¨ë¸ í‚¤ë¡œ ë³€ê²½
        new_state_dict = {}
        for key, value in state_dict.items():
            if key == 'gmm_backbone.cls_token':
                new_state_dict['cls_token'] = value
            elif key == 'gmm_backbone.pos_embed':
                new_state_dict['pos_embed'] = value
            else:
                new_state_dict[key] = value
                
        return super().load_state_dict(new_state_dict, strict=strict)
        
    def _create_transformer_block(self):
        """Vision Transformer ë¸”ë¡ ìƒì„±"""
        return nn.ModuleDict({
            'norm1': nn.LayerNorm(1024),
            'attn': nn.ModuleDict({
                'qkv': nn.Linear(1024, 3072),  # 3 * 1024
                'proj': nn.Linear(1024, 1024)
            }),
            'norm2': nn.LayerNorm(1024),
            'mlp': nn.ModuleDict({
                'fc1': nn.Linear(1024, 4096),
                'fc2': nn.Linear(4096, 1024)
            })
        })
        
    def _transformer_forward(self, x):
        """Vision Transformer forward pass"""
        B = x.size(0)
        
        # Patch embedding
        x = self.gmm_backbone['patch_embed']['proj'](x)  # [B, 1024, H//16, W//16]
        x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]
        
        # Add position embedding
        pos_embed = self.pos_embed
        if pos_embed.size(1) > x.size(1):
            pos_embed = pos_embed[:, :x.size(1), :]
        x = x + pos_embed
        
        # Add class token
        cls_token = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        
        # Transformer blocks
        for block in self.gmm_backbone['blocks']:
            # Self-attention
            attn_input = block['norm1'](x)
            qkv = block['attn']['qkv'](attn_input)  # [B, seq_len, 3072]
            
            # ì •í™•í•œ ì°¨ì› ê³„ì‚°: 3072 = 3 * 1024
            seq_len = qkv.size(1)
            qkv = qkv.reshape(B, seq_len, 3, 1024).permute(2, 0, 1, 3)  # [3, B, seq_len, 1024]
            q, k, v = qkv[0], qkv[1], qkv[2]  # ê°ê° [B, seq_len, 1024]
            
            # Attention ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
            attn_output = torch.matmul(q, k.transpose(-2, -1)) / (1024**0.5)  # [B, seq_len, seq_len]
            attn_output = torch.softmax(attn_output, dim=-1)
            attn_output = torch.matmul(attn_output, v)  # [B, seq_len, 1024]
            
            # Projection
            attn_output = block['attn']['proj'](attn_output)
            
            x = x + attn_output
            
            # MLP
            mlp_input = block['norm2'](x)
            mlp_output = block['mlp']['fc1'](mlp_input)
            mlp_output = F.gelu(mlp_output)
            mlp_output = block['mlp']['fc2'](mlp_output)
            x = x + mlp_output
        
        # Final norm and head
        x = self.gmm_backbone['norm'](x)
        x = self.gmm_backbone['head'](x)  # [B, seq_len, 1000]
        
        # Class token outputë§Œ ì‚¬ìš©
        return x[:, 0, :]  # [B, 1000]
        
    def forward(self, x):
        # 1. Vision Transformer backbone
        features = self._transformer_forward(x)
        
        # 2. Control point prediction (ì„ì‹œ êµ¬í˜„)
        B = features.size(0)
        # ê°„ë‹¨í•œ linear projectionìœ¼ë¡œ 1000 -> 20 ë³€í™˜
        control_points = features.view(B, -1)  # [B, 1000]
        control_points = control_points[:, :20]  # [B, 20] - ì²˜ìŒ 20ê°œ ê°’ë§Œ ì‚¬ìš©
        control_points = control_points.view(B, self.num_control_points, 2)  # [B, 10, 2]
        
        return control_points

class SAMModel(nn.Module):
    """SAM (Segment Anything Model)"""
    def __init__(self):
        super().__init__()
        
        # Vision Transformer backbone
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=16, stride=16),  # Patch embedding
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # Segmentation head
        self.segmentation_head = nn.Sequential(
            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1)  # Binary segmentation
        )
        
    def forward(self, x, prompts=None):
        # Backbone
        x = self.backbone(x)
        
        # Segmentation
        output = self.segmentation_head(x)
        
        return output

class RealESRGANModel(nn.Module):
    """Real-ESRGAN ëª¨ë¸"""
    def __init__(self, scale=4):
        super().__init__()
        self.scale = scale
        
        # Feature extraction
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # Upsampling
        self.upsampler = nn.Sequential(
            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),
            nn.PixelShuffle(2),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),
            nn.PixelShuffle(2),
            nn.ReLU(inplace=True)
        )
        
        # Output
        self.output = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)
        
    def forward(self, x):
        # Feature extraction
        x = self.feature_extractor(x)
        
        # Upsampling
        x = self.upsampler(x)
        
        # Output
        output = self.output(x)
        
        return output

class TOMModel(nn.Module):
    """TOM (Try-On Module) ëª¨ë¸"""
    def __init__(self):
        super().__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1),  # person + cloth
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)
        )
        
    def forward(self, person_image, cloth_image):
        # Concatenate inputs
        x = torch.cat([person_image, cloth_image], dim=1)
        
        # Encoder
        x = self.encoder(x)
        
        # Decoder
        output = self.decoder(x)
        
        return output

class OOTDModel(nn.Module):
    """OOTD (Outfit of the Day) ëª¨ë¸"""
    def __init__(self):
        super().__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1),  # person + cloth
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)
        )
        
    def forward(self, person_image, cloth_image, text_prompt=None, timestep=None):
        # Concatenate inputs
        x = torch.cat([person_image, cloth_image], dim=1)
        
        # Encoder
        x = self.encoder(x)
        
        # Decoder
        output = self.decoder(x)
        
        return output

class TPSModel(nn.Module):
    """TPS (Thin Plate Spline) ëª¨ë¸ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶¤"""
    def __init__(self, num_control_points=20):
        super().__init__()
        self.num_control_points = num_control_points
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ Sequential features
        self.features = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1),  # features.0
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # features.2
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # features.5
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # features.7
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # features.10
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # features.12
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # features.14
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # features.16
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # features.19
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # features.21
            nn.ReLU(inplace=True),
        )
        
        # TPS transformation prediction
        self.tps_predictor = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_control_points * 2, kernel_size=1)  # x, y coordinates
        )
        
    def forward(self, person_image, cloth_image):
        # Concatenate inputs
        x = torch.cat([person_image, cloth_image], dim=1)
        
        # Features
        x = self.features(x)
        
        # TPS transformation prediction
        tps_params = self.tps_predictor(x)
        
        return tps_params

class RAFTModel(nn.Module):
    """RAFT (Recurrent All-Pairs Field Transforms) ëª¨ë¸"""
    def __init__(self):
        super().__init__()
        
        # Feature encoder
        self.feature_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 96, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True),
            nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True)
        )
        
        # Context encoder
        self.context_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 96, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True),
            nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True)
        )
        
        # Flow decoder
        self.flow_decoder = nn.Sequential(
            nn.Conv2d(96, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 2, kernel_size=3, padding=1)  # 2 channels for x, y flow
        )
        
    def forward(self, x):
        # Feature extraction
        features = self.feature_encoder(x)
        context = self.context_encoder(x)
        
        # Flow prediction
        flow = self.flow_decoder(features)
        
        return flow

class CLIPModel(nn.Module):
    """CLIP (Contrastive Language-Image Pre-training) ëª¨ë¸"""
    def __init__(self, embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=32):
        super().__init__()
        self.embed_dim = embed_dim
        self.image_resolution = image_resolution
        self.vision_layers = vision_layers
        self.vision_width = vision_width
        self.vision_patch_size = vision_patch_size
        
        # Vision Transformer
        self.visual = nn.ModuleDict({
            'conv1': nn.Conv2d(3, vision_width, kernel_size=vision_patch_size, stride=vision_patch_size, bias=False),
            'transformer': nn.ModuleList([
                nn.ModuleDict({
                    'ln_1': nn.LayerNorm(vision_width),
                    'attn': nn.MultiheadAttention(vision_width, 12, batch_first=True),
                    'ln_2': nn.LayerNorm(vision_width),
                    'mlp': nn.Sequential(
                        nn.Linear(vision_width, vision_width * 4),
                        nn.GELU(),
                        nn.Linear(vision_width * 4, vision_width)
                    )
                }) for _ in range(vision_layers)
            ]),
            'ln_post': nn.LayerNorm(vision_width),
            'proj': nn.Linear(vision_width, embed_dim)
        })
        # positional embedding ì°¨ì› ìˆ˜ì •: (image_resolution // vision_patch_size) ** 2 + 1
        patch_size = image_resolution // vision_patch_size
        self.positional_embedding = nn.Parameter(torch.randn(1, patch_size * patch_size + 1, vision_width))
        
        # CLS token ì¶”ê°€
        self.cls_token = nn.Parameter(torch.randn(1, 1, vision_width))
        
        # Text Transformer (ê°„ë‹¨í•œ ë²„ì „)
        self.text_projection = nn.Linear(512, embed_dim)
        
        # Logit scale
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        
    def encode_image(self, image):
        """ì´ë¯¸ì§€ ì¸ì½”ë”©"""
        x = self.visual['conv1'](image)  # [B, C, H, W] -> [B, H*W, C]
        x = x.flatten(2).transpose(1, 2)
        
        # Add CLS token
        B = x.shape[0]
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Add positional embedding
        x = x + self.positional_embedding
        
        # Transformer blocks
        for block in self.visual['transformer']:
            # Self-attention
            attn_input = block['ln_1'](x)
            attn_output, _ = block['attn'](attn_input, attn_input, attn_input)
            x = x + attn_output
            
            # MLP
            mlp_input = block['ln_2'](x)
            mlp_output = block['mlp'](mlp_input)
            x = x + mlp_output
        
        # Final normalization and projection
        x = self.visual['ln_post'](x)
        x = self.visual['proj'](x[:, 0, :])  # Use CLS token
        
        return x
    
    def encode_text(self, text_features):
        """í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ê°„ë‹¨í•œ ë²„ì „)"""
        return self.text_projection(text_features)
    
    def forward(self, image, text_features=None):
        """CLIP forward pass"""
        image_features = self.encode_image(image)
        
        if text_features is not None:
            text_features = self.encode_text(text_features)
            return image_features, text_features
        
        return image_features

class LPIPSModel(nn.Module):
    """LPIPS (Learned Perceptual Image Patch Similarity) ëª¨ë¸"""
    def __init__(self, net='alex', version='0.1'):
        super().__init__()
        self.version = version
        
        # AlexNet ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œê¸°
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        
        # íŠ¹ì§• ì •ê·œí™”
        self.normalize = nn.ModuleList([
            nn.Conv2d(64, 1, kernel_size=1, bias=False),
            nn.Conv2d(192, 1, kernel_size=1, bias=False),
            nn.Conv2d(384, 1, kernel_size=1, bias=False),
            nn.Conv2d(256, 1, kernel_size=1, bias=False),
            nn.Conv2d(256, 1, kernel_size=1, bias=False),
        ])
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x, y):
        """LPIPS ê³„ì‚°"""
        # íŠ¹ì§• ì¶”ì¶œ
        x_features = []
        y_features = []
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        x_feat = self.features[0:2](x)  # Conv2d + ReLU
        y_feat = self.features[0:2](y)
        x_features.append(self.normalize[0](x_feat))
        y_features.append(self.normalize[0](y_feat))
        
        # ë‘ ë²ˆì§¸ ë ˆì´ì–´
        x_feat = self.features[2:5](x_feat)  # MaxPool2d + Conv2d + ReLU
        y_feat = self.features[2:5](y_feat)
        x_features.append(self.normalize[1](x_feat))
        y_features.append(self.normalize[1](y_feat))
        
        # ì„¸ ë²ˆì§¸ ë ˆì´ì–´
        x_feat = self.features[5:7](x_feat)  # MaxPool2d + Conv2d + ReLU
        y_feat = self.features[5:7](y_feat)
        x_features.append(self.normalize[2](x_feat))
        y_features.append(self.normalize[2](y_feat))
        
        # ë„¤ ë²ˆì§¸ ë ˆì´ì–´
        x_feat = self.features[7:9](x_feat)  # Conv2d + ReLU
        y_feat = self.features[7:9](y_feat)
        x_features.append(self.normalize[3](x_feat))
        y_features.append(self.normalize[3](y_feat))
        
        # ë‹¤ì„¯ ë²ˆì§¸ ë ˆì´ì–´
        x_feat = self.features[9:12](x_feat)  # Conv2d + ReLU + MaxPool2d
        y_feat = self.features[9:12](y_feat)
        x_features.append(self.normalize[4](x_feat))
        y_features.append(self.normalize[4](y_feat))
        
        # LPIPS ê³„ì‚°
        lpips_score = 0
        for x_feat, y_feat in zip(x_features, y_features):
            diff = (x_feat - y_feat) ** 2
            lpips_score += torch.mean(diff)
        
        return lpips_score

# ëª¨ë¸ ì•„í‚¤í…ì²˜ íŒ©í† ë¦¬
class ModelArchitectureFactory:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_model_from_analysis(analysis: Dict[str, Any]) -> Optional[nn.Module]:
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±"""
        architecture_type = analysis.get('architecture_type', 'unknown')
        model_name = analysis.get('model_name', 'unknown')
        
        print(f"ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±: {architecture_type} ({model_name})")
        
        # ê° ëª¨ë¸ë³„ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±
        if architecture_type == 'hrnet' or 'hrnet' in model_name.lower():
            return ModelArchitectureFactory._create_hrnet_model(analysis)
        elif architecture_type == 'graphonomy' or 'graphonomy' in model_name.lower():
            return ModelArchitectureFactory._create_graphonomy_model(analysis)
        elif architecture_type == 'u2net' or 'u2net' in model_name.lower():
            return ModelArchitectureFactory._create_u2net_model(analysis)
        elif architecture_type == 'openpose' or 'openpose' in model_name.lower():
            return ModelArchitectureFactory._create_openpose_model(analysis)
        elif architecture_type == 'gmm' or 'gmm' in model_name.lower():
            return ModelArchitectureFactory._create_gmm_model(analysis)
        elif architecture_type == 'tom' or 'tom' in model_name.lower():
            return ModelArchitectureFactory._create_tom_model(analysis)
        elif architecture_type == 'sam' or 'sam' in model_name.lower():
            return ModelArchitectureFactory._create_sam_model(analysis)
        elif architecture_type == 'real_esrgan' or 'real_esrgan' in model_name.lower():
            return ModelArchitectureFactory._create_real_esrgan_model(analysis)
        elif architecture_type == 'ootd' or 'ootd' in model_name.lower():
            return ModelArchitectureFactory._create_ootd_model(analysis)
        elif architecture_type == 'tps' or 'tps' in model_name.lower():
            return ModelArchitectureFactory._create_tps_model(analysis)
        elif architecture_type == 'raft' or 'raft' in model_name.lower():
            return ModelArchitectureFactory._create_raft_model(analysis)
        elif architecture_type == 'clip' or 'clip' in model_name.lower():
            return ModelArchitectureFactory._create_clip_model(analysis)
        elif architecture_type == 'lpips' or 'lpips' in model_name.lower():
            return ModelArchitectureFactory._create_lpips_model(analysis)
        elif architecture_type == 'deeplabv3plus' or 'deeplabv3plus' in model_name.lower():
            return ModelArchitectureFactory._create_deeplabv3plus_model(analysis)
        elif architecture_type == 'mobile_sam' or 'mobile_sam' in model_name.lower():
            return ModelArchitectureFactory._create_mobile_sam_model(analysis)
        elif architecture_type == 'viton_hd' or 'viton_hd' in model_name.lower():
            return ModelArchitectureFactory._create_viton_hd_model(analysis)
        elif architecture_type == 'gfpgan' or 'gfpgan' in model_name.lower():
            return ModelArchitectureFactory._create_gfpgan_model(analysis)
        else:
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•„í‚¤í…ì²˜: {architecture_type}")
            return None
    
    @staticmethod
    def create_complete_model_from_analysis(analysis: Dict[str, Any]) -> Optional['CompleteModelWrapper']:
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„±"""
        architecture_type = analysis.get('architecture_type', 'unknown')
        model_name = analysis.get('model_name', 'unknown')
        
        print(f"ğŸ—ï¸ ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„±: {architecture_type} ({model_name})")
        
        # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        base_model = ModelArchitectureFactory.create_model_from_analysis(analysis)
        
        if base_model is None:
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {architecture_type}")
            return None
        
        # ëª¨ë¸ íƒ€ì… ê²°ì •
        model_type = ModelArchitectureFactory._determine_model_type(architecture_type, model_name)
        
        # ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„±
        complete_model = CompleteModelWrapper(base_model, model_type)
        
        print(f"âœ… ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„± ì„±ê³µ: {model_type}")
        return complete_model
    
    @staticmethod
    def _determine_model_type(architecture_type: str, model_name: str) -> str:
        """ëª¨ë¸ íƒ€ì… ê²°ì •"""
        if 'openpose' in architecture_type.lower() or 'openpose' in model_name.lower():
            return 'openpose'
        elif 'hrnet' in architecture_type.lower() or 'hrnet' in model_name.lower():
            return 'hrnet'
        elif 'graphonomy' in architecture_type.lower() or 'graphonomy' in model_name.lower():
            return 'graphonomy'
        elif 'u2net' in architecture_type.lower() or 'u2net' in model_name.lower():
            return 'u2net'
        elif 'gmm' in architecture_type.lower() or 'gmm' in model_name.lower():
            return 'gmm'
        elif 'sam' in architecture_type.lower() or 'sam' in model_name.lower():
            return 'sam'
        else:
            return 'generic'
    
    @staticmethod
    def load_model_with_checkpoint(analysis: Dict[str, Any], checkpoint_path: str) -> Optional['CompleteModelWrapper']:
        """ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ì™„ì „í•œ ëª¨ë¸ ë¡œë”©"""
        try:
            print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ëª¨ë¸ ë¡œë”©: {checkpoint_path}")
            
            # 1. ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„±
            complete_model = ModelArchitectureFactory.create_complete_model_from_analysis(analysis)
            
            if complete_model is None:
                print(f"âŒ ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ìƒì„± ì‹¤íŒ¨")
                return None
            
            # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # 3. ê³ ê¸‰ í‚¤ ë§¤í•‘ ì ìš©
            key_mapper = AdvancedKeyMapper()
            model_type = complete_model.model_type
            
            success = key_mapper.map_checkpoint(checkpoint, complete_model.base_model, model_type)
            
            if success:
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_type}")
                
                # 4. ë§¤í•‘ í†µê³„ ì¶œë ¥
                stats = key_mapper.get_mapping_stats(checkpoint, complete_model.base_model, model_type)
                print(f"ğŸ“Š ë§¤í•‘ í†µê³„: {stats['mapping_rate']:.1f}% ({stats['mapped_keys']}/{stats['total_target_keys']})")
                
                return complete_model
            else:
                print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {model_type}")
                return None
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    @staticmethod
    def create_step_integration_interface(complete_model: 'CompleteModelWrapper') -> 'StepIntegrationInterface':
        """Step í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        return StepIntegrationInterface(complete_model)
    
    @staticmethod
    def _create_hrnet_model(analysis: Dict[str, Any]) -> nn.Module:
        """HRNet ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ HRNet ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # HRNet ì„¤ì • ì¶”ì¶œ
        num_joints = analysis.get('num_joints', 17)  # COCO í¬ì¦ˆ í‚¤í¬ì¸íŠ¸
        
        return HRNetPoseModel(num_joints=num_joints)
    
    @staticmethod
    def _create_graphonomy_model(analysis: Dict[str, Any]) -> nn.Module:
        """Graphonomy ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ Graphonomy ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # Graphonomy ì„¤ì • ì¶”ì¶œ
        num_classes = analysis.get('num_classes', 20)  # ê¸°ë³¸ ATR ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìˆ˜
        
        return GraphonomyModel(num_classes=num_classes)
    
    @staticmethod
    def _create_u2net_model(analysis: Dict[str, Any]) -> nn.Module:
        """U2Net ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ U2Net ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # U2Net ì„¤ì •
        out_channels = analysis.get('out_channels', 1)  # ë°”ì´ë„ˆë¦¬ ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        return U2NetModel(out_channels=out_channels)
    
    @staticmethod
    def _create_openpose_model(analysis: Dict[str, Any]) -> nn.Module:
        """OpenPose ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ OpenPose ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        return OpenPoseModel()
    
    @staticmethod
    def _create_gmm_model(analysis: Dict[str, Any]) -> nn.Module:
        """GMM (Geometric Matching Module) ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ GMM ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # GMM ì„¤ì •
        num_control_points = analysis.get('num_control_points', 10)
        
        return GMMModel(num_control_points=num_control_points)
    
    @staticmethod
    def _create_tom_model(analysis: Dict[str, Any]) -> nn.Module:
        """TOM (Try-On Module) ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ TOM ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        return TOMModel()
    
    @staticmethod
    def _create_sam_model(analysis: Dict[str, Any]) -> nn.Module:
        """SAM (Segment Anything Model) ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ SAM ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        return SAMModel()
    
    @staticmethod
    def _create_real_esrgan_model(analysis: Dict[str, Any]) -> nn.Module:
        """Real-ESRGAN ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ Real-ESRGAN ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # Real-ESRGAN ì„¤ì •
        scale = analysis.get('scale', 4)  # 4x ì—…ìŠ¤ì¼€ì¼
        
        return RealESRGANModel(scale=scale)
    
    @staticmethod
    def _create_ootd_model(analysis: Dict[str, Any]) -> nn.Module:
        """OOTD (Outfit of the Day) ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ OOTD ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        return OOTDModel()

    @staticmethod
    def _create_tps_model(analysis: Dict[str, Any]) -> nn.Module:
        """TPS (Thin Plate Spline) ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ TPS ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # TPS ì„¤ì •
        num_control_points = analysis.get('num_control_points', 20)
        
        return TPSModel(num_control_points=num_control_points)

    @staticmethod
    def _create_raft_model(analysis: Dict[str, Any]) -> nn.Module:
        """RAFT ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ RAFT ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # RAFT ì„¤ì •
        return RAFTModel()
    
    @staticmethod
    def _create_clip_model(analysis: Dict[str, Any]) -> nn.Module:
        """CLIP ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ CLIP ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # CLIP ì„¤ì •
        embed_dim = analysis.get('embed_dim', 512)
        image_resolution = analysis.get('image_resolution', 224)
        vision_layers = analysis.get('vision_layers', 12)
        vision_width = analysis.get('vision_width', 768)
        vision_patch_size = analysis.get('vision_patch_size', 32)
        
        return CLIPModel(
            embed_dim=embed_dim,
            image_resolution=image_resolution,
            vision_layers=vision_layers,
            vision_width=vision_width,
            vision_patch_size=vision_patch_size
        )
    
    @staticmethod
    def _create_lpips_model(analysis: Dict[str, Any]) -> nn.Module:
        """LPIPS ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ LPIPS ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # LPIPS ì„¤ì •
        net = analysis.get('net', 'alex')
        version = analysis.get('version', '0.1')
        
        return LPIPSModel(net=net, version=version)
    
    @staticmethod
    def _create_deeplabv3plus_model(analysis: Dict[str, Any]) -> nn.Module:
        """DeepLabV3+ ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ DeepLabV3+ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # DeepLabV3+ ì„¤ì •
        num_classes = analysis.get('num_classes', 21)
        
        return DeepLabV3PlusModel(num_classes=num_classes)
    
    @staticmethod
    def _create_mobile_sam_model(analysis: Dict[str, Any]) -> nn.Module:
        """Mobile SAM ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ Mobile SAM ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # Mobile SAM ì„¤ì •
        embed_dim = analysis.get('embed_dim', 256)
        image_size = analysis.get('image_size', 1024)
        
        return MobileSAMModel(embed_dim=embed_dim, image_size=image_size)
    
    @staticmethod
    def _create_viton_hd_model(analysis: Dict[str, Any]) -> nn.Module:
        """VITON-HD ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ VITON-HD ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # VITON-HD ì„¤ì •
        num_classes = analysis.get('num_classes', 20)
        
        return VITONHDModel(num_classes=num_classes)
    
    @staticmethod
    def _create_gfpgan_model(analysis: Dict[str, Any]) -> nn.Module:
        """GFPGAN ëª¨ë¸ ì •í™•í•œ ì•„í‚¤í…ì²˜ ìƒì„±"""
        print("ğŸ—ï¸ GFPGAN ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±")
        
        # GFPGAN ì„¤ì •
        num_style_feat = analysis.get('num_style_feat', 512)
        channel_multiplier = analysis.get('channel_multiplier', 2)
        
        return GFPGANModel(num_style_feat=num_style_feat, channel_multiplier=channel_multiplier)

class DeepLabV3PlusModel(nn.Module):
    """DeepLabV3+ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
    def __init__(self, num_classes=21):
        super().__init__()
        self.num_classes = num_classes
        
        # Encoder (ResNet backbone)
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # ResNet blocks
            self._make_layer(64, 64, 3, stride=1),
            self._make_layer(64, 128, 4, stride=2),
            self._make_layer(128, 256, 6, stride=2),
            self._make_layer(256, 512, 3, stride=2),
        )
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = nn.ModuleDict({
            'conv1': nn.Conv2d(512, 256, kernel_size=1, bias=False),
            'conv2': nn.Conv2d(512, 256, kernel_size=3, padding=6, dilation=6, bias=False),
            'conv3': nn.Conv2d(512, 256, kernel_size=3, padding=12, dilation=12, bias=False),
            'conv4': nn.Conv2d(512, 256, kernel_size=3, padding=18, dilation=18, bias=False),
            'pool': nn.AdaptiveAvgPool2d(1),
            'conv_pool': nn.Conv2d(512, 256, kernel_size=1, bias=False),
            'bn': nn.BatchNorm2d(1280),  # 256 * 5 = 1280 (5ê°œ ASPP ì¶œë ¥ì˜ í•©)
            'relu': nn.ReLU(inplace=True),
            'dropout': nn.Dropout(0.5),
            'conv_out': nn.Conv2d(1280, 256, kernel_size=1, bias=False)
        })
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )
        
    def _make_layer(self, inplanes, planes, blocks, stride):
        layers = []
        layers.append(nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Encoder
        x = self.encoder(x)
        
        # ASPP
        aspp_out = []
        aspp_out.append(self.aspp['conv1'](x))
        aspp_out.append(self.aspp['conv2'](x))
        aspp_out.append(self.aspp['conv3'](x))
        aspp_out.append(self.aspp['conv4'](x))
        
        pool = self.aspp['pool'](x)
        pool = self.aspp['conv_pool'](pool)
        pool = F.interpolate(pool, size=x.size()[2:], mode='bilinear', align_corners=True)
        aspp_out.append(pool)
        
        x = torch.cat(aspp_out, dim=1)
        x = self.aspp['bn'](x)
        x = self.aspp['relu'](x)
        x = self.aspp['dropout'](x)
        x = self.aspp['conv_out'](x)
        
        # Decoder
        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=True)
        x = self.decoder(x)
        
        return x

class MobileSAMModel(nn.Module):
    """Mobile SAM (Segment Anything Model) ëª¨ë¸"""
    def __init__(self, embed_dim=256, image_size=1024):
        super().__init__()
        self.embed_dim = embed_dim
        self.image_size = image_size
        
        # Image encoder (ê²½ëŸ‰í™”ëœ Vision Transformer)
        self.image_encoder = nn.ModuleDict({
            'patch_embed': nn.Conv2d(3, embed_dim, kernel_size=16, stride=16),
            'blocks': nn.ModuleList([
                nn.ModuleDict({
                    'norm1': nn.LayerNorm(embed_dim),
                    'attn': nn.MultiheadAttention(embed_dim, 8, batch_first=True),
                    'norm2': nn.LayerNorm(embed_dim),
                    'mlp': nn.Sequential(
                        nn.Linear(embed_dim, embed_dim * 4),
                        nn.GELU(),
                        nn.Linear(embed_dim * 4, embed_dim)
                    )
                }) for _ in range(8)  # ê²½ëŸ‰í™”: 8ê°œ ë¸”ë¡ë§Œ ì‚¬ìš©
            ]),
            'neck': nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),
                nn.BatchNorm2d(embed_dim),
                nn.ReLU(inplace=True)
            )
        })
        # positional embedding ì°¨ì› ìˆ˜ì •: (image_size // 16) ** 2
        patch_size = image_size // 16
        self.pos_embed = nn.Parameter(torch.randn(1, patch_size * patch_size, embed_dim))
        
        # Prompt encoder
        self.prompt_encoder = nn.ModuleDict({
            'point_embeddings': nn.ModuleList([
                nn.Embedding(1, embed_dim) for _ in range(4)  # ìµœëŒ€ 4ê°œ í¬ì¸íŠ¸
            ]),
            'mask_embedding': nn.Embedding(1, embed_dim)
        })
        
        # Mask decoder
        self.mask_decoder = nn.Sequential(
            nn.ConvTranspose2d(embed_dim, embed_dim // 2, kernel_size=2, stride=2),
            nn.BatchNorm2d(embed_dim // 2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(embed_dim // 2, embed_dim // 4, kernel_size=2, stride=2),
            nn.BatchNorm2d(embed_dim // 4),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(embed_dim // 4, embed_dim // 8, kernel_size=2, stride=2),
            nn.BatchNorm2d(embed_dim // 8),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(embed_dim // 8, embed_dim // 16, kernel_size=2, stride=2),
            nn.BatchNorm2d(embed_dim // 16),
            nn.ReLU(inplace=True),
            nn.Conv2d(embed_dim // 16, 1, kernel_size=1)  # ë§ˆìŠ¤í¬ ì¶œë ¥
        )
        
    def forward(self, x, point_coords=None, point_labels=None):
        # Image encoding
        B, C, H, W = x.shape
        x = self.image_encoder['patch_embed'](x)
        x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]
        
        # Adjust positional embedding size if needed
        if x.size(1) != self.pos_embed.size(1):
            # Resize positional embedding to match input
            pos_embed = F.interpolate(
                self.pos_embed.transpose(1, 2).unsqueeze(0), 
                size=x.size(1), 
                mode='linear'
            ).squeeze(0).transpose(0, 1)
        else:
            pos_embed = self.pos_embed
            
        x = x + pos_embed
        
        # Transformer blocks
        for block in self.image_encoder['blocks']:
            # Self-attention
            attn_input = block['norm1'](x)
            attn_output, _ = block['attn'](attn_input, attn_input, attn_input)
            x = x + attn_output
            
            # MLP
            mlp_input = block['norm2'](x)
            mlp_output = block['mlp'](mlp_input)
            x = x + mlp_output
        
        # Reshape back to spatial dimensions
        patch_size = int((x.size(1)) ** 0.5)
        x = x.transpose(1, 2).reshape(B, self.embed_dim, patch_size, patch_size)
        x = self.image_encoder['neck'](x)
        
        # Mask decoding
        mask = self.mask_decoder(x)
        
        return mask

class VITONHDModel(nn.Module):
    """VITON-HD ê°€ìƒ í”¼íŒ… ëª¨ë¸"""
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # Person encoder
        self.person_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )
        
        # Clothing encoder
        self.clothing_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )
        
        # Fusion and decoder
        self.fusion = nn.Sequential(
            nn.Conv2d(1024, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # Output layers
        self.warping_field = nn.Conv2d(128, 2, kernel_size=1)  # 2D warping field
        self.occlusion_mask = nn.Conv2d(128, 1, kernel_size=1)  # Occlusion mask
        self.final_output = nn.Conv2d(128, 3, kernel_size=1)  # Final RGB output
        
    def forward(self, person_image, clothing_image):
        # Encode inputs
        person_features = self.person_encoder(person_image)
        clothing_features = self.clothing_encoder(clothing_image)
        
        # Concatenate features
        combined_features = torch.cat([person_features, clothing_features], dim=1)
        
        # Fusion and decoding
        features = self.fusion(combined_features)
        
        # Generate outputs
        warping_field = self.warping_field(features)
        occlusion_mask = torch.sigmoid(self.occlusion_mask(features))
        final_output = torch.tanh(self.final_output(features))
        
        return {
            'warping_field': warping_field,
            'occlusion_mask': occlusion_mask,
            'final_output': final_output
        }

class GFPGANModel(nn.Module):
    """GFPGAN (Generative Facial Prior GAN) ëª¨ë¸"""
    def __init__(self, num_style_feat=512, channel_multiplier=2):
        super().__init__()
        self.num_style_feat = num_style_feat
        self.channel_multiplier = channel_multiplier
        
        # StyleGAN2 ê¸°ë°˜ ìƒì„±ê¸°
        self.style_conv = nn.ModuleList([
            nn.Conv2d(3, 512, kernel_size=3, padding=1),  # ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” 3ì±„ë„ ì…ë ¥
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)
        ])
        
        # Style modulation (ì²« ë²ˆì§¸ ë ˆì´ì–´ ì œì™¸)
        self.style_modulation = nn.ModuleList([
            nn.Linear(num_style_feat, 512),
            nn.Linear(num_style_feat, 512),
            nn.Linear(num_style_feat, 512),
            nn.Linear(num_style_feat, 256),
            nn.Linear(num_style_feat, 128),
            nn.Linear(num_style_feat, 64)
        ])
        
        # Noise injection
        self.noise_scales = nn.Parameter(torch.ones(len(self.style_conv)))
        
        # Final output
        self.final_conv = nn.Conv2d(3, 3, kernel_size=1)
        
    def forward(self, x, style_code=None):
        # Style code ìƒì„± (ì—†ìœ¼ë©´ ëœë¤)
        if style_code is None:
            style_code = torch.randn(x.size(0), self.num_style_feat, device=x.device)
        
        # StyleGAN2 forward pass
        for i, conv in enumerate(self.style_conv):
            # Style modulation (ì²« ë²ˆì§¸ ë ˆì´ì–´ ì œì™¸)
            if i > 0 and i-1 < len(self.style_modulation):
                style = self.style_modulation[i-1](style_code)
                style = style.view(style.size(0), style.size(1), 1, 1)
                x = x * style
            
            # Convolution
            x = conv(x)
            
            # Noise injection
            if i < len(self.noise_scales):
                noise = torch.randn_like(x) * self.noise_scales[i]
                x = x + noise
            
            # Activation (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œì™¸)
            if i < len(self.style_conv) - 1:
                x = F.leaky_relu(x, 0.2)
        
        # Final output
        x = self.final_conv(x)
        
        return x

# ================================================================================
# ğŸ”¥ Phase 1: ì™„ì „í•œ ëª¨ë¸ ë˜í¼ ì‹œìŠ¤í…œ
# ================================================================================

class BasePreprocessor:
    """ê¸°ë³¸ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.supported_formats = ['numpy', 'pil', 'tensor', 'path']
    
    def __call__(self, input_data):
        """ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬"""
        if isinstance(input_data, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            return self._process_file_path(input_data)
        elif isinstance(input_data, np.ndarray):
            # NumPy ë°°ì—´ì¸ ê²½ìš°
            return self._process_numpy(input_data)
        elif hasattr(input_data, 'convert'):  # PIL Image
            # PIL ì´ë¯¸ì§€ì¸ ê²½ìš°
            return self._process_pil(input_data)
        elif isinstance(input_data, torch.Tensor):
            # PyTorch í…ì„œì¸ ê²½ìš°
            return self._process_tensor(input_data)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(input_data)}")
    
    def _process_file_path(self, file_path):
        """íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬"""
        import cv2
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # OpenCVë¡œ ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(file_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # BGR -> RGB ë³€í™˜
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return self._process_numpy(image)
    
    def _process_numpy(self, image):
        """NumPy ë°°ì—´ ì²˜ë¦¬"""
        if image.ndim == 2:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ -> RGB
            image = np.stack([image] * 3, axis=-1)
        elif image.ndim == 3 and image.shape[2] == 4:
            # RGBA -> RGB
            image = image[:, :, :3]
        
        # ì •ê·œí™” (0-255 -> 0-1)
        if image.dtype == np.uint8:
            image = image.astype(np.float32) / 255.0
        
        # HWC -> CHW ë³€í™˜
        if image.shape[2] == 3:
            image = image.transpose(2, 0, 1)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if image.ndim == 3:
            image = np.expand_dims(image, axis=0)
        
        return torch.from_numpy(image).float()
    
    def _process_pil(self, image):
        """PIL ì´ë¯¸ì§€ ì²˜ë¦¬"""
        import numpy as np
        
        # RGBë¡œ ë³€í™˜
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # NumPyë¡œ ë³€í™˜
        image = np.array(image)
        return self._process_numpy(image)
    
    def _process_tensor(self, tensor):
        """PyTorch í…ì„œ ì²˜ë¦¬"""
        # CPUë¡œ ì´ë™
        if tensor.device.type != 'cpu':
            tensor = tensor.cpu()
        
        # float32ë¡œ ë³€í™˜
        if tensor.dtype != torch.float32:
            tensor = tensor.float()
        
        # ì •ê·œí™” (0-255 -> 0-1)
        if tensor.max() > 1.0:
            tensor = tensor / 255.0
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if tensor.ndim == 3:
            tensor = tensor.unsqueeze(0)
        
        return tensor

class BasePostprocessor:
    """ê¸°ë³¸ í›„ì²˜ë¦¬ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self):
        pass
    
    def __call__(self, model_output):
        """ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        raise NotImplementedError

class OpenPosePreprocessor(BasePreprocessor):
    """OpenPose ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        super().__init__()
        self.target_size = (368, 368)  # OpenPose í‘œì¤€ ì…ë ¥ í¬ê¸°
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
    
    def _process_numpy(self, image):
        """NumPy ë°°ì—´ ì²˜ë¦¬ - OpenPose ì „ìš©"""
        # ì›ë³¸ í¬ê¸° ì €ì¥
        original_size = image.shape[:2]
        
        # í¬ê¸° ì¡°ì •
        resized_image = self._resize_image(image, self.target_size)
        
        # ì •ê·œí™” (0-255 -> 0-1)
        if resized_image.dtype == np.uint8:
            resized_image = resized_image.astype(np.float32) / 255.0
        
        # ImageNet ì •ê·œí™”
        resized_image = self._normalize_image(resized_image)
        
        # HWC -> CHW ë³€í™˜
        if resized_image.shape[2] == 3:
            resized_image = resized_image.transpose(2, 0, 1)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if resized_image.ndim == 3:
            resized_image = np.expand_dims(resized_image, axis=0)
        
        return torch.from_numpy(resized_image).float()
    
    def _resize_image(self, image, target_size):
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        import cv2
        return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    
    def _normalize_image(self, image):
        """ImageNet ì •ê·œí™”"""
        # ê° ì±„ë„ë³„ë¡œ ì •ê·œí™”
        for i in range(3):
            image[:, :, i] = (image[:, :, i] - self.mean[i]) / self.std[i]
        return image

class OpenPosePostprocessor(BasePostprocessor):
    """OpenPose í›„ì²˜ë¦¬ê¸° - PAF + íˆíŠ¸ë§µ ì™„ì „ ì²˜ë¦¬"""
    
    def __init__(self):
        super().__init__()
        self.num_keypoints = 18  # OpenPose í‚¤í¬ì¸íŠ¸ ìˆ˜
        self.num_pafs = 38       # PAF ì±„ë„ ìˆ˜
        self.num_heatmaps = 19   # íˆíŠ¸ë§µ ìˆ˜ (18 í‚¤í¬ì¸íŠ¸ + 1 ë°°ê²½)
        
        # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip',
            'right_knee', 'right_ankle', 'left_hip', 'left_knee', 'left_ankle',
            'right_eye', 'left_eye', 'right_ear', 'left_ear'
        ]
        
        # PAF ì—°ê²° ì •ì˜ (OpenPose 18)
        self.paf_connections = [
            (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8), (8, 9),
            (9, 10), (1, 11), (11, 12), (12, 13), (1, 0), (0, 14), (14, 16),
            (0, 15), (15, 17)
        ]
    
    def __call__(self, model_output):
        """OpenPose ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        if isinstance(model_output, torch.Tensor):
            # ë‹¨ì¼ í…ì„œì¸ ê²½ìš° íˆíŠ¸ë§µìœ¼ë¡œ ì²˜ë¦¬
            heatmaps = model_output
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps)
            
            return {
                'keypoints': keypoints,
                'confidence_scores': [kp[2] for kp in keypoints] if keypoints else [],
                'heatmaps': heatmaps,
                'keypoint_names': self.keypoint_names,
                'num_keypoints': len(keypoints)
            }
        elif isinstance(model_output, dict):
            # ë³µì¡í•œ ì¶œë ¥ì¸ ê²½ìš° (PAF + íˆíŠ¸ë§µ)
            if 'paf' in model_output and 'heatmaps' in model_output:
                return self._process_paf_and_heatmaps(model_output)
            else:
                return model_output
        else:
            # ê¸°íƒ€ ê²½ìš°
            return {
                'keypoints': [],
                'confidence_scores': [],
                'heatmaps': None,
                'keypoint_names': self.keypoint_names,
                'num_keypoints': 0
            }
    
    def _process_paf_and_heatmaps(self, model_output):
        """PAFì™€ íˆíŠ¸ë§µì„ í•¨ê»˜ ì²˜ë¦¬"""
        paf = model_output['paf']
        heatmaps = model_output['heatmaps']
        
        # 1. íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = self._extract_keypoints_from_heatmaps(heatmaps)
        
        # 2. PAFë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ì—°ê²°
        connected_keypoints = self._connect_keypoints_with_paf(keypoints, paf)
        
        # 3. OpenPose 18 â†’ COCO 17 ë³€í™˜
        coco_keypoints = self._convert_openpose18_to_coco17(connected_keypoints)
        
        return {
            'keypoints': coco_keypoints,
            'confidence_scores': [kp[2] for kp in coco_keypoints] if coco_keypoints else [],
            'heatmaps': heatmaps,
            'paf': paf,
            'keypoint_names': self.keypoint_names,
            'num_keypoints': len(coco_keypoints),
            'original_openpose_keypoints': connected_keypoints
        }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps):
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        
        # íˆíŠ¸ë§µ í˜•íƒœ: [batch, num_heatmaps, H, W]
        if heatmaps.dim() == 4:
            heatmaps = heatmaps.squeeze(0)  # [num_heatmaps, H, W]
        
        H, W = heatmaps.shape[1], heatmaps.shape[2]
        
        for i in range(min(heatmaps.shape[0], self.num_keypoints)):
            heatmap = heatmaps[i]  # [H, W]
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            max_idx = torch.argmax(heatmap)
            y, x = max_idx // W, max_idx % W
            
            # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
            x_norm = x.float() / W
            y_norm = y.float() / H
            confidence = heatmap[y, x].item()
            
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
            if confidence > 0.1:  # ìµœì†Œ ì‹ ë¢°ë„
                keypoints.append([x_norm, y_norm, confidence])
            else:
                keypoints.append([0.0, 0.0, 0.0])  # ê°ì§€ë˜ì§€ ì•Šì€ í‚¤í¬ì¸íŠ¸
        
        return keypoints
    
    def _connect_keypoints_with_paf(self, keypoints, paf):
        """PAFë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ì—°ê²°"""
        if len(keypoints) < 2:
            return keypoints
        
        # PAF í˜•íƒœ: [batch, num_pafs, H, W]
        if paf.dim() == 4:
            paf = paf.squeeze(0)  # [num_pafs, H, W]
        
        connected_keypoints = keypoints.copy()
        
        # ê° PAF ì—°ê²°ì— ëŒ€í•´ ì²˜ë¦¬
        for i, (start_idx, end_idx) in enumerate(self.paf_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                keypoints[start_idx][2] > 0.1 and keypoints[end_idx][2] > 0.1):
                
                # PAF ì±„ë„ ì¸ë±ìŠ¤ ê³„ì‚°
                paf_channel = i * 2  # x, y ë°©í–¥
                
                # PAF ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²° ê°•ë„ ê³„ì‚°
                start_pos = keypoints[start_idx][:2]
                end_pos = keypoints[end_idx][:2]
                
                # ì—°ê²° ê°•ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
                connection_strength = self._calculate_connection_strength(
                    start_pos, end_pos, paf[paf_channel:paf_channel+2]
                )
                
                # ì—°ê²° ê°•ë„ê°€ ë†’ìœ¼ë©´ í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„ í–¥ìƒ
                if connection_strength > 0.5:
                    connected_keypoints[start_idx][2] *= 1.2
                    connected_keypoints[end_idx][2] *= 1.2
        
        return connected_keypoints
    
    def _calculate_connection_strength(self, start_pos, end_pos, paf_xy):
        """PAFë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²° ê°•ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì—°ê²° ê°•ë„ ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” PAFë¥¼ ë”°ë¼ ì ë¶„í•˜ëŠ” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”
        return 0.8  # ê¸°ë³¸ê°’
    
    def _convert_openpose18_to_coco17(self, openpose_keypoints):
        """OpenPose 18 â†’ COCO 17 ë³€í™˜"""
        if len(openpose_keypoints) < 18:
            return openpose_keypoints
        
        # COCO 17 í‚¤í¬ì¸íŠ¸ ìˆœì„œ
        coco_order = [
            0,   # nose
            1,   # left_eye
            2,   # right_eye
            3,   # left_ear
            4,   # right_ear
            5,   # left_shoulder
            6,   # right_shoulder
            7,   # left_elbow
            8,   # right_elbow
            9,   # left_wrist
            10,  # right_wrist
            11,  # left_hip
            12,  # right_hip
            13,  # left_knee
            14,  # right_knee
            15,  # left_ankle
            16   # right_ankle
        ]
        
        # OpenPose 18 â†’ COCO 17 ë§¤í•‘
        openpose_to_coco = {
            0: 0,   # nose
            14: 1,  # left_eye
            15: 2,  # right_eye
            16: 3,  # left_ear
            17: 4,  # right_ear
            5: 5,   # left_shoulder
            2: 6,   # right_shoulder
            6: 7,   # left_elbow
            3: 8,   # right_elbow
            7: 9,   # left_wrist
            4: 10,  # right_wrist
            11: 11, # left_hip
            8: 12,  # right_hip
            12: 13, # left_knee
            9: 14,  # right_knee
            13: 15, # left_ankle
            10: 16  # right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in coco_order:
            if coco_idx in openpose_to_coco:
                openpose_idx = openpose_to_coco[coco_idx]
                if openpose_idx < len(openpose_keypoints):
                    coco_keypoints.append(openpose_keypoints[openpose_idx])
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
    
    def get_keypoint_info(self):
        """í‚¤í¬ì¸íŠ¸ ì •ë³´ ë°˜í™˜"""
        return {
            'num_keypoints': self.num_keypoints,
            'keypoint_names': self.keypoint_names,
            'paf_connections': self.paf_connections,
            'num_pafs': self.num_pafs,
            'num_heatmaps': self.num_heatmaps
        }

class HRNetPreprocessor(BasePreprocessor):
    """HRNet ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        super().__init__()
        self.target_size = (256, 192)  # HRNet í‘œì¤€ ì…ë ¥ í¬ê¸°
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
    
    def _process_numpy(self, image):
        """NumPy ë°°ì—´ ì²˜ë¦¬ - HRNet ì „ìš©"""
        # ì›ë³¸ í¬ê¸° ì €ì¥
        original_size = image.shape[:2]
        
        # í¬ê¸° ì¡°ì •
        resized_image = self._resize_image(image, self.target_size)
        
        # ì •ê·œí™” (0-255 -> 0-1)
        if resized_image.dtype == np.uint8:
            resized_image = resized_image.astype(np.float32) / 255.0
        
        # ImageNet ì •ê·œí™”
        resized_image = self._normalize_image(resized_image)
        
        # HWC -> CHW ë³€í™˜
        if resized_image.shape[2] == 3:
            resized_image = resized_image.transpose(2, 0, 1)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if resized_image.ndim == 3:
            resized_image = np.expand_dims(resized_image, axis=0)
        
        return torch.from_numpy(resized_image).float()
    
    def _resize_image(self, image, target_size):
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        import cv2
        return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    
    def _normalize_image(self, image):
        """ImageNet ì •ê·œí™”"""
        # ê° ì±„ë„ë³„ë¡œ ì •ê·œí™”
        for i in range(3):
            image[:, :, i] = (image[:, :, i] - self.mean[i]) / self.std[i]
        return image

class HRNetPostprocessor(BasePostprocessor):
    """HRNet í›„ì²˜ë¦¬ê¸° - ë©€í‹°ìŠ¤ì¼€ì¼ íŠ¹ì§• ì™„ì „ ì²˜ë¦¬"""
    
    def __init__(self):
        super().__init__()
        self.num_keypoints = 17  # COCO í‚¤í¬ì¸íŠ¸ ìˆ˜
        self.input_size = (256, 192)  # HRNet í‘œì¤€ ì…ë ¥ í¬ê¸°
        
        # COCO 17 í‚¤í¬ì¸íŠ¸ ì •ì˜
        self.keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]
        
        # í‚¤í¬ì¸íŠ¸ ì—°ê²° ì •ì˜ (COCO 17)
        self.keypoint_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # ë¨¸ë¦¬
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ”
            (5, 11), (6, 12), (11, 12),  # ëª¸í†µ
            (11, 13), (13, 15), (12, 14), (14, 16)  # ë‹¤ë¦¬
        ]
        
        # í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì •ì˜
        self.visibility_threshold = 0.1
    
    def __call__(self, model_output):
        """HRNet ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        if isinstance(model_output, torch.Tensor):
            # ë‹¨ì¼ í…ì„œì¸ ê²½ìš° íˆíŠ¸ë§µìœ¼ë¡œ ì²˜ë¦¬
            heatmaps = model_output
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps)
            
            # í‚¤í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ë° ê²€ì¦
            processed_keypoints = self._post_process_keypoints(keypoints)
            
            return {
                'keypoints': processed_keypoints,
                'confidence_scores': [kp[2] for kp in processed_keypoints] if processed_keypoints else [],
                'heatmaps': heatmaps,
                'keypoint_names': self.keypoint_names,
                'num_keypoints': len(processed_keypoints),
                'keypoint_connections': self.keypoint_connections
            }
        elif isinstance(model_output, dict):
            # ë³µì¡í•œ ì¶œë ¥ì¸ ê²½ìš° (ë©€í‹°ìŠ¤ì¼€ì¼ íŠ¹ì§•)
            if 'multi_scale_features' in model_output:
                return self._process_multi_scale_features(model_output)
            else:
                return model_output
        else:
            # ê¸°íƒ€ ê²½ìš°
            return {
                'keypoints': [],
                'confidence_scores': [],
                'heatmaps': None,
                'keypoint_names': self.keypoint_names,
                'num_keypoints': 0
            }
    
    def _process_multi_scale_features(self, model_output):
        """ë©€í‹°ìŠ¤ì¼€ì¼ íŠ¹ì§• ì²˜ë¦¬"""
        multi_scale_features = model_output['multi_scale_features']
        
        # ê° ìŠ¤ì¼€ì¼ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        all_keypoints = []
        for scale_idx, features in enumerate(multi_scale_features):
            scale_keypoints = self._extract_keypoints_from_heatmaps(features)
            all_keypoints.append(scale_keypoints)
        
        # ë©€í‹°ìŠ¤ì¼€ì¼ í‚¤í¬ì¸íŠ¸ ìœµí•©
        fused_keypoints = self._fuse_multi_scale_keypoints(all_keypoints)
        
        # í‚¤í¬ì¸íŠ¸ í›„ì²˜ë¦¬
        processed_keypoints = self._post_process_keypoints(fused_keypoints)
        
        return {
            'keypoints': processed_keypoints,
            'confidence_scores': [kp[2] for kp in processed_keypoints] if processed_keypoints else [],
            'multi_scale_features': multi_scale_features,
            'keypoint_names': self.keypoint_names,
            'num_keypoints': len(processed_keypoints),
            'keypoint_connections': self.keypoint_connections
        }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps):
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        
        # íˆíŠ¸ë§µ í˜•íƒœ: [batch, num_keypoints, H, W]
        if heatmaps.dim() == 4:
            heatmaps = heatmaps.squeeze(0)  # [num_keypoints, H, W]
        
        H, W = heatmaps.shape[1], heatmaps.shape[2]
        
        for i in range(min(heatmaps.shape[0], self.num_keypoints)):
            heatmap = heatmaps[i]  # [H, W]
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            max_idx = torch.argmax(heatmap)
            y, x = max_idx // W, max_idx % W
            
            # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
            x_norm = x.float() / W
            y_norm = y.float() / H
            confidence = heatmap[y, x].item()
            
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
            if confidence > self.visibility_threshold:
                keypoints.append([x_norm, y_norm, confidence])
            else:
                keypoints.append([0.0, 0.0, 0.0])  # ê°ì§€ë˜ì§€ ì•Šì€ í‚¤í¬ì¸íŠ¸
        
        return keypoints
    
    def _fuse_multi_scale_keypoints(self, all_keypoints):
        """ë©€í‹°ìŠ¤ì¼€ì¼ í‚¤í¬ì¸íŠ¸ ìœµí•©"""
        if not all_keypoints:
            return []
        
        fused_keypoints = []
        num_scales = len(all_keypoints)
        
        for kp_idx in range(self.num_keypoints):
            # ê° ìŠ¤ì¼€ì¼ì—ì„œ í•´ë‹¹ í‚¤í¬ì¸íŠ¸ì˜ ì‹ ë¢°ë„ ìˆ˜ì§‘
            confidences = []
            positions = []
            
            for scale_idx, scale_keypoints in enumerate(all_keypoints):
                if kp_idx < len(scale_keypoints):
                    kp = scale_keypoints[kp_idx]
                    if kp[2] > self.visibility_threshold:
                        confidences.append(kp[2])
                        positions.append(kp[:2])
            
            if confidences:
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ„ì¹˜ ê³„ì‚°
                weights = torch.tensor(confidences)
                positions = torch.tensor(positions)
                
                weighted_pos = torch.sum(positions * weights.unsqueeze(1), dim=0) / torch.sum(weights)
                avg_confidence = torch.mean(weights).item()
                
                fused_keypoints.append([weighted_pos[0].item(), weighted_pos[1].item(), avg_confidence])
            else:
                fused_keypoints.append([0.0, 0.0, 0.0])
        
        return fused_keypoints
    
    def _post_process_keypoints(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ë° ê²€ì¦"""
        if not keypoints:
            return []
        
        processed_keypoints = []
        
        for i, kp in enumerate(keypoints):
            x, y, conf = kp
            
            # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦
            if 0.0 <= x <= 1.0 and 0.0 <= y <= 1.0 and conf > self.visibility_threshold:
                # í‚¤í¬ì¸íŠ¸ë³„ ì¶”ê°€ ê²€ì¦
                if self._validate_keypoint(i, x, y, conf, keypoints):
                    processed_keypoints.append([x, y, conf])
                else:
                    processed_keypoints.append([0.0, 0.0, 0.0])
            else:
                processed_keypoints.append([0.0, 0.0, 0.0])
        
        return processed_keypoints
    
    def _validate_keypoint(self, kp_idx, x, y, conf, all_keypoints):
        """í‚¤í¬ì¸íŠ¸ë³„ ê²€ì¦"""
        # ê¸°ë³¸ ê²€ì¦
        if conf < self.visibility_threshold:
            return False
        
        # í‚¤í¬ì¸íŠ¸ë³„ íŠ¹ìˆ˜ ê²€ì¦ ê·œì¹™
        if kp_idx == 0:  # ì½”
            # ì½”ëŠ” ë‹¤ë¥¸ ì–¼êµ´ í‚¤í¬ì¸íŠ¸ì™€ ì¼ì • ê±°ë¦¬ ë‚´ì— ìˆì–´ì•¼ í•¨
            face_keypoints = [1, 2, 3, 4]  # ëˆˆ, ê·€
            return self._check_face_consistency(kp_idx, x, y, all_keypoints, face_keypoints)
        
        elif kp_idx in [1, 2, 3, 4]:  # ì–¼êµ´ í‚¤í¬ì¸íŠ¸
            # ì–¼êµ´ í‚¤í¬ì¸íŠ¸ëŠ” ì„œë¡œ ì¼ì • ê±°ë¦¬ ë‚´ì— ìˆì–´ì•¼ í•¨
            return self._check_face_consistency(kp_idx, x, y, all_keypoints, [0, 1, 2, 3, 4])
        
        elif kp_idx in [5, 6]:  # ì–´ê¹¨
            # ì–´ê¹¨ëŠ” ì„œë¡œ ëŒ€ì¹­ì ì´ì–´ì•¼ í•¨
            return self._check_shoulder_symmetry(kp_idx, x, y, all_keypoints)
        
        else:
            # ê¸°íƒ€ í‚¤í¬ì¸íŠ¸ëŠ” ê¸°ë³¸ ê²€ì¦ë§Œ
            return True
    
    def _check_face_consistency(self, kp_idx, x, y, all_keypoints, face_indices):
        """ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ì¼ê´€ì„± ê²€ì‚¬"""
        if len(all_keypoints) < max(face_indices) + 1:
            return True
        
        # ë‹¤ë¥¸ ì–¼êµ´ í‚¤í¬ì¸íŠ¸ì™€ì˜ ê±°ë¦¬ ê²€ì‚¬
        for other_idx in face_indices:
            if other_idx != kp_idx and other_idx < len(all_keypoints):
                other_kp = all_keypoints[other_idx]
                if other_kp[2] > self.visibility_threshold:
                    dist = ((x - other_kp[0])**2 + (y - other_kp[1])**2)**0.5
                    if dist > 0.3:  # ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìœ¼ë©´ ë¬´íš¨
                        return False
        
        return True
    
    def _check_shoulder_symmetry(self, kp_idx, x, y, all_keypoints):
        """ì–´ê¹¨ ëŒ€ì¹­ì„± ê²€ì‚¬"""
        if kp_idx == 5:  # left_shoulder
            right_idx = 6  # right_shoulder
        elif kp_idx == 6:  # right_shoulder
            right_idx = 5  # left_shoulder
        else:
            return True
        
        if right_idx < len(all_keypoints):
            right_kp = all_keypoints[right_idx]
            if right_kp[2] > self.visibility_threshold:
                # y ì¢Œí‘œê°€ ë¹„ìŠ·í•´ì•¼ í•¨ (ì–´ê¹¨ëŠ” ê°™ì€ ë†’ì´)
                y_diff = abs(y - right_kp[1])
                if y_diff > 0.1:  # ë„ˆë¬´ ë†’ì´ ì°¨ì´ê°€ ë‚˜ë©´ ë¬´íš¨
                    return False
        
        return True
    
    def get_keypoint_info(self):
        """í‚¤í¬ì¸íŠ¸ ì •ë³´ ë°˜í™˜"""
        return {
            'num_keypoints': self.num_keypoints,
            'keypoint_names': self.keypoint_names,
            'keypoint_connections': self.keypoint_connections,
            'input_size': self.input_size,
            'visibility_threshold': self.visibility_threshold
        }

class GraphonomyPreprocessor(BasePreprocessor):
    """Graphonomy ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        super().__init__()
        self.target_size = (512, 512)  # Graphonomy í‘œì¤€ ì…ë ¥ í¬ê¸°
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
    
    def _process_numpy(self, image):
        """NumPy ë°°ì—´ ì²˜ë¦¬ - Graphonomy ì „ìš©"""
        # ì›ë³¸ í¬ê¸° ì €ì¥
        original_size = image.shape[:2]
        
        # í¬ê¸° ì¡°ì •
        resized_image = self._resize_image(image, self.target_size)
        
        # ì •ê·œí™” (0-255 -> 0-1)
        if resized_image.dtype == np.uint8:
            resized_image = resized_image.astype(np.float32) / 255.0
        
        # ImageNet ì •ê·œí™”
        resized_image = self._normalize_image(resized_image)
        
        # HWC -> CHW ë³€í™˜
        if resized_image.shape[2] == 3:
            resized_image = resized_image.transpose(2, 0, 1)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if resized_image.ndim == 3:
            resized_image = np.expand_dims(resized_image, axis=0)
        
        return torch.from_numpy(resized_image).float()
    
    def _resize_image(self, image, target_size):
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        import cv2
        return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    
    def _normalize_image(self, image):
        """ImageNet ì •ê·œí™”"""
        # ê° ì±„ë„ë³„ë¡œ ì •ê·œí™”
        for i in range(3):
            image[:, :, i] = (image[:, :, i] - self.mean[i]) / self.std[i]
        return image

class GraphonomyPostprocessor(BasePostprocessor):
    """Graphonomy í›„ì²˜ë¦¬ê¸° - ì¸ê°„ íŒŒì‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ì „ ì²˜ë¦¬"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        self.input_size = (512, 512)  # Graphonomy í‘œì¤€ ì…ë ¥ í¬ê¸°
        
        # Graphonomy 20 í´ë˜ìŠ¤ ì •ì˜
        self.class_names = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]
        
        # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ì •ì˜ (ì‹œê°í™”ìš©)
        self.class_colors = [
            [0, 0, 0], [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0],
            [170, 255, 0], [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170],
            [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],
            [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 0]
        ]
        
        # ì¤‘ìš” í´ë˜ìŠ¤ ì •ì˜ (ì˜ë¥˜ ê´€ë ¨)
        self.clothing_classes = [1, 2, 5, 6, 7, 9, 10, 11, 12]  # ì˜ë¥˜ ê´€ë ¨ í´ë˜ìŠ¤ ì¸ë±ìŠ¤
        
        # ì‹ ì²´ ë¶€ìœ„ í´ë˜ìŠ¤ ì •ì˜
        self.body_parts = {
            'head': [1, 2, 4, 13],  # ëª¨ì, ë¨¸ë¦¬ì¹´ë½, ì„ ê¸€ë¼ìŠ¤, ì–¼êµ´
            'upper_body': [5, 6, 7, 14, 15],  # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸, íŒ”
            'lower_body': [9, 12, 16, 17],  # ë°”ì§€, ì¹˜ë§ˆ, ë‹¤ë¦¬
            'accessories': [3, 11, 18, 19]  # ì¥ê°‘, ìŠ¤ì¹´í”„, ì‹ ë°œ
        }
    
    def __call__(self, model_output):
        """Graphonomy ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        if isinstance(model_output, torch.Tensor):
            # ë‹¨ì¼ í…ì„œì¸ ê²½ìš° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µìœ¼ë¡œ ì²˜ë¦¬
            if model_output.dim() == 4:
                model_output = model_output.squeeze(0)
            
            # í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µìœ¼ë¡œ ë³€í™˜
            segmentation_map = torch.argmax(model_output, dim=0)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ í›„ì²˜ë¦¬
            processed_segmentation = self._post_process_segmentation(segmentation_map)
            
            return {
                'segmentation_map': processed_segmentation,
                'probabilities': model_output,
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'class_colors': self.class_colors,
                'body_parts': self._extract_body_parts(processed_segmentation),
                'clothing_mask': self._extract_clothing_mask(processed_segmentation)
            }
        elif isinstance(model_output, dict):
            # ë³µì¡í•œ ì¶œë ¥ì¸ ê²½ìš°
            if 'segmentation' in model_output:
                return self._process_complex_output(model_output)
            else:
                return model_output
        else:
            # ê¸°íƒ€ ê²½ìš°
            return {
                'segmentation_map': None,
                'probabilities': None,
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'body_parts': {},
                'clothing_mask': None
            }
    
    def _process_complex_output(self, model_output):
        """ë³µì¡í•œ ì¶œë ¥ ì²˜ë¦¬"""
        segmentation = model_output['segmentation']
        
        if isinstance(segmentation, torch.Tensor):
            if segmentation.dim() == 4:
                segmentation = segmentation.squeeze(0)
            
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ í›„ì²˜ë¦¬
            processed_segmentation = self._post_process_segmentation(segmentation)
            
            return {
                'segmentation_map': processed_segmentation,
                'probabilities': model_output.get('probabilities', None),
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'class_colors': self.class_colors,
                'body_parts': self._extract_body_parts(processed_segmentation),
                'clothing_mask': self._extract_clothing_mask(processed_segmentation)
            }
        else:
            return model_output
    
    def _post_process_segmentation(self, segmentation_map):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ í›„ì²˜ë¦¬"""
        if segmentation_map is None:
            return None
        
        # ë…¸ì´ì¦ˆ ì œê±° (ì‘ì€ ì˜ì—­ ì œê±°)
        cleaned_segmentation = self._remove_noise(segmentation_map)
        
        # ê²½ê³„ ìŠ¤ë¬´ë”©
        smoothed_segmentation = self._smooth_boundaries(cleaned_segmentation)
        
        # ì—°ê²°ì„± ê²€ì¦
        validated_segmentation = self._validate_connectivity(smoothed_segmentation)
        
        return validated_segmentation
    
    def _remove_noise(self, segmentation_map):
        """ë…¸ì´ì¦ˆ ì œê±° (ì‘ì€ ì˜ì—­ ì œê±°)"""
        # ê° í´ë˜ìŠ¤ë³„ë¡œ ì‘ì€ ì˜ì—­ ì œê±°
        cleaned_map = segmentation_map.clone()
        
        for class_id in range(self.num_classes):
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë§ˆìŠ¤í¬ ìƒì„±
            class_mask = (segmentation_map == class_id)
            
            # ì—°ê²° ìš”ì†Œ ë¶„ì„
            if class_mask.sum() > 0:
                # ì‘ì€ ì˜ì—­ ì œê±° (ë©´ì ì´ 100 í”½ì…€ ë¯¸ë§Œì¸ ì˜ì—­)
                cleaned_mask = self._remove_small_regions(class_mask, min_area=100)
                cleaned_map[class_mask & ~cleaned_mask] = 0  # ë°°ê²½ìœ¼ë¡œ ë³€ê²½
        
        return cleaned_map
    
    def _remove_small_regions(self, mask, min_area=100):
        """ì‘ì€ ì˜ì—­ ì œê±°"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ë§ˆìŠ¤í¬ì˜ ì´ í”½ì…€ ìˆ˜ê°€ min_areaë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ ì œê±°
        if mask.sum() < min_area:
            return torch.zeros_like(mask, dtype=torch.bool)
        return mask
    
    def _smooth_boundaries(self, segmentation_map):
        """ê²½ê³„ ìŠ¤ë¬´ë”©"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ì›ë³¸ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìŠ¤ë¬´ë”© ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        return segmentation_map
    
    def _validate_connectivity(self, segmentation_map):
        """ì—°ê²°ì„± ê²€ì¦"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ì›ë³¸ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” ì—°ê²°ì„± ê²€ì¦ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        return segmentation_map
    
    def _extract_body_parts(self, segmentation_map):
        """ì‹ ì²´ ë¶€ìœ„ ì¶”ì¶œ"""
        if segmentation_map is None:
            return {}
        
        body_parts = {}
        
        for part_name, class_indices in self.body_parts.items():
            # í•´ë‹¹ ë¶€ìœ„ì˜ ë§ˆìŠ¤í¬ ìƒì„±
            part_mask = torch.zeros_like(segmentation_map, dtype=torch.bool)
            for class_idx in class_indices:
                if class_idx < self.num_classes:
                    part_mask |= (segmentation_map == class_idx)
            
            # ë¶€ìœ„ ì •ë³´ ê³„ì‚°
            if part_mask.sum() > 0:
                body_parts[part_name] = {
                    'mask': part_mask,
                    'area': part_mask.sum().item(),
                    'bbox': self._calculate_bbox(part_mask),
                    'center': self._calculate_center(part_mask)
                }
            else:
                body_parts[part_name] = {
                    'mask': part_mask,
                    'area': 0,
                    'bbox': None,
                    'center': None
                }
        
        return body_parts
    
    def _extract_clothing_mask(self, segmentation_map):
        """ì˜ë¥˜ ë§ˆìŠ¤í¬ ì¶”ì¶œ"""
        if segmentation_map is None:
            return None
        
        # ì˜ë¥˜ ê´€ë ¨ í´ë˜ìŠ¤ì˜ ë§ˆìŠ¤í¬ ìƒì„±
        clothing_mask = torch.zeros_like(segmentation_map, dtype=torch.bool)
        
        for class_idx in self.clothing_classes:
            if class_idx < self.num_classes:
                clothing_mask |= (segmentation_map == class_idx)
        
        return clothing_mask
    
    def _calculate_bbox(self, mask):
        """ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        if mask.sum() == 0:
            return None
        
        # ë§ˆìŠ¤í¬ì—ì„œ 0ì´ ì•„ë‹Œ í”½ì…€ì˜ ì¢Œí‘œ ì°¾ê¸°
        coords = torch.nonzero(mask)
        
        if len(coords) == 0:
            return None
        
        # ìµœì†Œ/ìµœëŒ€ ì¢Œí‘œ ê³„ì‚°
        min_y, min_x = coords.min(dim=0)[0]
        max_y, max_x = coords.max(dim=0)[0]
        
        return {
            'x1': min_x.item(),
            'y1': min_y.item(),
            'x2': max_x.item(),
            'y2': max_y.item(),
            'width': (max_x - min_x + 1).item(),
            'height': (max_y - min_y + 1).item()
        }
    
    def _calculate_center(self, mask):
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        if mask.sum() == 0:
            return None
        
        # ë§ˆìŠ¤í¬ì—ì„œ 0ì´ ì•„ë‹Œ í”½ì…€ì˜ ì¢Œí‘œ ì°¾ê¸°
        coords = torch.nonzero(mask)
        
        if len(coords) == 0:
            return None
        
        # í‰ê·  ì¢Œí‘œ ê³„ì‚°
        center_y = coords[:, 0].float().mean()
        center_x = coords[:, 1].float().mean()
        
        return {
            'x': center_x.item(),
            'y': center_y.item()
        }
    
    def get_segmentation_info(self):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •ë³´ ë°˜í™˜"""
        return {
            'num_classes': self.num_classes,
            'class_names': self.class_names,
            'class_colors': self.class_colors,
            'clothing_classes': self.clothing_classes,
            'body_parts': self.body_parts,
            'input_size': self.input_size
        }

class CompleteModelWrapper(nn.Module):
    """ì™„ì „í•œ ê¸°ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, base_model: nn.Module, model_type: str):
        super().__init__()
        self.base_model = base_model
        self.model_type = model_type
        self.preprocessor = self._create_preprocessor()
        self.postprocessor = self._create_postprocessor()
        
        # ëª¨ë¸ ì •ë³´ ì„¤ì •
        self.input_shape = self._get_input_shape()
        self.output_shape = self._get_output_shape()
        self.supported_formats = self._get_supported_formats()
    
    def _create_preprocessor(self):
        """ëª¨ë¸ë³„ ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        if self.model_type == 'openpose':
            return OpenPosePreprocessor()
        elif self.model_type == 'hrnet':
            return HRNetPreprocessor()
        elif self.model_type == 'graphonomy':
            return GraphonomyPreprocessor()
        else:
            # ê¸°ë³¸ ì „ì²˜ë¦¬ê¸°
            return BasePreprocessor()
    
    def _create_postprocessor(self):
        """ëª¨ë¸ë³„ í›„ì²˜ë¦¬ê¸° ìƒì„±"""
        if self.model_type == 'openpose':
            return OpenPosePostprocessor()
        elif self.model_type == 'hrnet':
            return HRNetPostprocessor()
        elif self.model_type == 'graphonomy':
            return GraphonomyPostprocessor()
        else:
            # ê¸°ë³¸ í›„ì²˜ë¦¬ê¸°
            return BasePostprocessor()
    
    def _get_input_shape(self):
        """ì…ë ¥ í˜•íƒœ ë°˜í™˜"""
        if self.model_type == 'openpose':
            return (1, 3, 368, 368)
        elif self.model_type == 'hrnet':
            return (1, 3, 256, 192)
        elif self.model_type == 'graphonomy':
            return (1, 3, 512, 512)
        else:
            return (1, 3, 224, 224)
    
    def _get_output_shape(self):
        """ì¶œë ¥ í˜•íƒœ ë°˜í™˜"""
        if self.model_type == 'openpose':
            return (1, 19, 46, 46)  # íˆíŠ¸ë§µ ì¶œë ¥
        elif self.model_type == 'hrnet':
            return (1, 17, 64, 48)  # COCO í‚¤í¬ì¸íŠ¸
        elif self.model_type == 'graphonomy':
            return (1, 20, 512, 512)  # ì„¸ê·¸ë©˜í…Œì´ì…˜
        else:
            return None
    
    def _get_supported_formats(self):
        """ì§€ì›í•˜ëŠ” ì…ë ¥ í˜•ì‹ ë°˜í™˜"""
        return ['numpy', 'pil', 'tensor']
    
    def forward(self, x):
        """ì „ì²´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸"""
        # 1. ì „ì²˜ë¦¬
        processed_x = self.preprocessor(x)
        
        # 2. ê¸°ë³¸ ëª¨ë¸ ì¶”ë¡ 
        output = self.base_model(processed_x)
        
        # 3. í›„ì²˜ë¦¬
        final_output = self.postprocessor(output)
        
        return final_output
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_type': self.model_type,
            'input_shape': self.input_shape,
            'output_shape': self.output_shape,
            'supported_formats': self.supported_formats,
            'base_model_class': self.base_model.__class__.__name__
        }

class AdvancedKeyMapper:
    """ê³ ê¸‰ í‚¤ ë§¤í•‘ ì‹œìŠ¤í…œ - ì²´í¬í¬ì¸íŠ¸ì™€ ëª¨ë¸ ê°„ì˜ ì •í™•í•œ ë§¤í•‘"""
    
    def __init__(self):
        self.mapping_rules = self._load_mapping_rules()
        self.key_patterns = self._load_key_patterns()
    
    def _load_mapping_rules(self):
        """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
        return {
            'openpose': {
                'backbone': {
                    'conv1.weight': 'features.0.weight',
                    'bn1.weight': 'features.1.weight',
                    'bn1.bias': 'features.1.bias',
                    'bn1.running_mean': 'features.1.running_mean',
                    'bn1.running_var': 'features.1.running_var',
                },
                'paf': {
                    'paf_out.weight': 'paf_out.weight',
                    'paf_out.bias': 'paf_out.bias',
                },
                'heatmap': {
                    'heatmap_out.weight': 'heatmap_out.weight',
                    'heatmap_out.bias': 'heatmap_out.bias',
                }
            },
            'hrnet': {
                'backbone': {
                    'conv1.weight': 'conv1.weight',
                    'bn1.weight': 'bn1.weight',
                    'bn1.bias': 'bn1.bias',
                    'bn1.running_mean': 'bn1.running_mean',
                    'bn1.running_var': 'bn1.running_var',
                },
                'final': {
                    'final_layer.weight': 'final_layer.weight',
                    'final_layer.bias': 'final_layer.bias',
                }
            },
            'graphonomy': {
                'encoder': {
                    'encoder.0.weight': 'encoder.0.weight',
                    'encoder.1.weight': 'encoder.1.weight',
                    'encoder.1.bias': 'encoder.1.bias',
                    'encoder.1.running_mean': 'encoder.1.running_mean',
                    'encoder.1.running_var': 'encoder.1.running_var',
                },
                'decoder': {
                    'decoder.0.weight': 'decoder.0.weight',
                    'decoder.1.weight': 'decoder.1.weight',
                    'decoder.1.bias': 'decoder.1.bias',
                    'decoder.1.running_mean': 'decoder.1.running_mean',
                    'decoder.1.running_var': 'decoder.1.running_var',
                }
            }
        }
    
    def _load_key_patterns(self):
        """í‚¤ íŒ¨í„´ ë¡œë“œ"""
        return {
            'openpose': [
                r'backbone\.',
                r'paf_out\.',
                r'heatmap_out\.',
                r'features\.',
                r'channel_expansion\.'
            ],
            'hrnet': [
                r'conv\d+\.',
                r'bn\d+\.',
                r'stage\d+\.',
                r'final_layer\.'
            ],
            'graphonomy': [
                r'encoder\.',
                r'decoder\.',
                r'conv\d+\.',
                r'bn\d+\.'
            ]
        }
    
    def map_checkpoint(self, checkpoint: Dict, target_model: nn.Module, model_type: str) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ íƒ€ê²Ÿ ëª¨ë¸ì— ë§¤í•‘"""
        try:
            print(f"ğŸ”§ {model_type} ëª¨ë¸ í‚¤ ë§¤í•‘ ì‹œì‘")
            
            # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ state_dict ì¶”ì¶œ
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # 2. íƒ€ê²Ÿ ëª¨ë¸ì˜ state_dict ê°€ì ¸ì˜¤ê¸°
            target_state_dict = target_model.state_dict()
            
            # 3. í‚¤ ë§¤í•‘ ì ìš©
            mapped_state_dict = self._apply_mapping_rules(
                state_dict, target_state_dict, model_type
            )
            
            # 4. ëˆ„ë½ëœ í‚¤ ì²˜ë¦¬
            mapped_state_dict = self._handle_missing_keys(
                mapped_state_dict, target_state_dict, model_type
            )
            
            # 5. ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°
            mapped_state_dict = self._resolve_dimension_mismatches(
                mapped_state_dict, target_state_dict, model_type
            )
            
            # 6. ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë”©
            missing_keys, unexpected_keys = target_model.load_state_dict(
                mapped_state_dict, strict=False
            )
            
            # 7. ê²°ê³¼ ë³´ê³ 
            print(f"âœ… {model_type} ëª¨ë¸ í‚¤ ë§¤í•‘ ì™„ë£Œ")
            if missing_keys:
                print(f"âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
            if unexpected_keys:
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ {model_type} ëª¨ë¸ í‚¤ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _apply_mapping_rules(self, source_dict: Dict, target_dict: Dict, model_type: str) -> Dict:
        """ë§¤í•‘ ê·œì¹™ ì ìš©"""
        mapped_dict = {}
        rules = self.mapping_rules.get(model_type, {})
        
        for source_key, source_value in source_dict.items():
            # ë§¤í•‘ ê·œì¹™ì—ì„œ ì°¾ê¸°
            mapped_key = self._find_mapping_rule(source_key, rules)
            
            if mapped_key and mapped_key in target_dict:
                mapped_dict[mapped_key] = source_value
            else:
                # ì§ì ‘ ë§¤ì¹­ ì‹œë„
                if source_key in target_dict:
                    mapped_dict[source_key] = source_value
                else:
                    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                    partial_match = self._find_partial_match(source_key, target_dict)
                    if partial_match:
                        mapped_dict[partial_match] = source_value
        
        return mapped_dict
    
    def _find_mapping_rule(self, source_key: str, rules: Dict) -> Optional[str]:
        """ë§¤í•‘ ê·œì¹™ì—ì„œ í‚¤ ì°¾ê¸°"""
        for category, mappings in rules.items():
            for rule_key, target_key in mappings.items():
                if rule_key in source_key:
                    return target_key
        return None
    
    def _find_partial_match(self, source_key: str, target_dict: Dict) -> Optional[str]:
        """ë¶€ë¶„ ë§¤ì¹­ ì°¾ê¸°"""
        # í‚¤ íŒ¨í„´ ë§¤ì¹­
        for target_key in target_dict.keys():
            if self._keys_similar(source_key, target_key):
                return target_key
        return None
    
    def _keys_similar(self, key1: str, key2: str) -> bool:
        """í‚¤ ìœ ì‚¬ë„ ê²€ì‚¬"""
        # ë‹¨ìˆœí•œ ìœ ì‚¬ë„ ê²€ì‚¬ (ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥)
        key1_parts = key1.split('.')
        key2_parts = key2.split('.')
        
        # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ê°™ìœ¼ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        if key1_parts[-1] == key2_parts[-1]:
            return True
        
        # ì¤‘ê°„ ë¶€ë¶„ì´ ê°™ìœ¼ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        for part1 in key1_parts:
            for part2 in key2_parts:
                if part1 == part2 and len(part1) > 2:
                    return True
        
        return False
    
    def _handle_missing_keys(self, mapped_dict: Dict, target_dict: Dict, model_type: str) -> Dict:
        """ëˆ„ë½ëœ í‚¤ ì²˜ë¦¬"""
        missing_keys = set(target_dict.keys()) - set(mapped_dict.keys())
        
        for missing_key in missing_keys:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            target_shape = target_dict[missing_key].shape
            if 'weight' in missing_key:
                # ê°€ì¤‘ì¹˜ëŠ” Xavier ì´ˆê¸°í™”
                mapped_dict[missing_key] = torch.randn(target_shape) * 0.1
            elif 'bias' in missing_key:
                # ë°”ì´ì–´ìŠ¤ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”
                mapped_dict[missing_key] = torch.zeros(target_shape)
            elif 'running_mean' in missing_key:
                # BatchNorm running_meanì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                mapped_dict[missing_key] = torch.zeros(target_shape)
            elif 'running_var' in missing_key:
                # BatchNorm running_varì€ 1ë¡œ ì´ˆê¸°í™”
                mapped_dict[missing_key] = torch.ones(target_shape)
        
        return mapped_dict
    
    def _resolve_dimension_mismatches(self, mapped_dict: Dict, target_dict: Dict, model_type: str) -> Dict:
        """ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°"""
        for key in mapped_dict.keys():
            if key in target_dict:
                source_tensor = mapped_dict[key]
                target_tensor = target_dict[key]
                
                if source_tensor.shape != target_tensor.shape:
                    print(f"ğŸ”§ ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°: {key} {source_tensor.shape} -> {target_tensor.shape}")
                    
                    # ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°
                    if len(source_tensor.shape) == len(target_tensor.shape):
                        # ê°™ì€ ì°¨ì› ìˆ˜ì¸ ê²½ìš°, í¬ê¸° ì¡°ì •
                        if model_type == 'openpose' and 'weight' in key:
                            # OpenPose ê°€ì¤‘ì¹˜ ì°¨ì› ì¡°ì •
                            if source_tensor.shape[0] != target_tensor.shape[0]:
                                # ì¶œë ¥ ì±„ë„ ìˆ˜ ì¡°ì •
                                if source_tensor.shape[0] > target_tensor.shape[0]:
                                    mapped_dict[key] = source_tensor[:target_tensor.shape[0]]
                                else:
                                    # íŒ¨ë”©ìœ¼ë¡œ í™•ì¥
                                    padding = torch.zeros(
                                        target_tensor.shape[0] - source_tensor.shape[0],
                                        *source_tensor.shape[1:]
                                    )
                                    mapped_dict[key] = torch.cat([source_tensor, padding], dim=0)
                    
                    # ì—¬ì „íˆ ë¶ˆì¼ì¹˜í•˜ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                    if mapped_dict[key].shape != target_tensor.shape:
                        print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {key}")
                        if 'weight' in key:
                            mapped_dict[key] = torch.randn_like(target_tensor) * 0.1
                        elif 'bias' in key:
                            mapped_dict[key] = torch.zeros_like(target_tensor)
                        else:
                            mapped_dict[key] = torch.zeros_like(target_tensor)
        
        return mapped_dict
    
    def get_mapping_stats(self, checkpoint: Dict, target_model: nn.Module, model_type: str) -> Dict:
        """ë§¤í•‘ í†µê³„ ë°˜í™˜"""
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        target_state_dict = target_model.state_dict()
        
        # ë§¤í•‘ ì‹œë„
        mapped_state_dict = self._apply_mapping_rules(state_dict, target_state_dict, model_type)
        
        # í†µê³„ ê³„ì‚°
        total_source_keys = len(state_dict)
        total_target_keys = len(target_state_dict)
        mapped_keys = len(mapped_state_dict)
        missing_keys = total_target_keys - mapped_keys
        
        mapping_rate = (mapped_keys / total_target_keys) * 100 if total_target_keys > 0 else 0
        
        return {
            'total_source_keys': total_source_keys,
            'total_target_keys': total_target_keys,
            'mapped_keys': mapped_keys,
            'missing_keys': missing_keys,
            'mapping_rate': mapping_rate,
            'model_type': model_type
        }

class StepIntegrationInterface:
    """Step íŒŒì¼ê³¼ì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, complete_model: 'CompleteModelWrapper'):
        self.complete_model = complete_model
        self.model_type = complete_model.model_type
        self.model_info = complete_model.get_model_info()
        
        # ëª¨ë¸ ì •ë³´ì—ì„œ ì†ì„±ë“¤ ì¶”ì¶œ
        self.input_shape = complete_model.input_shape
        self.output_shape = complete_model.output_shape
        self.supported_formats = complete_model.supported_formats
    
    def run_inference(self, image, **kwargs):
        """Stepì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì¶”ë¡  ë©”ì„œë“œ"""
        try:
            print(f"ğŸš€ {self.model_type} ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
            
            # 1. ì…ë ¥ ê²€ì¦
            validated_image = self._validate_input(image)
            
            # 2. ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                result = self.complete_model(validated_image)
            
            # 3. ê²°ê³¼ í¬ë§·íŒ…
            formatted_result = self._format_result(result)
            
            print(f"âœ… {self.model_type} ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
            return formatted_result
            
        except Exception as e:
            print(f"âŒ {self.model_type} ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._get_error_result(str(e))
    
    def _validate_input(self, image):
        """ì…ë ¥ ê²€ì¦"""
        if image is None:
            raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ê°€ Noneì…ë‹ˆë‹¤")
        
        # numpy ë°°ì—´ì¸ ê²½ìš°
        if isinstance(image, np.ndarray):
            if image.ndim != 3:
                raise ValueError(f"ì´ë¯¸ì§€ëŠ” 3ì°¨ì›ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {image.ndim}ì°¨ì›")
            return image
        
        # PIL ì´ë¯¸ì§€ì¸ ê²½ìš°
        elif hasattr(image, 'convert'):
            return np.array(image)
        
        # í…ì„œì¸ ê²½ìš°
        elif isinstance(image, torch.Tensor):
            if image.dim() == 4:
                return image.squeeze(0).cpu().numpy()
            elif image.dim() == 3:
                return image.cpu().numpy()
            else:
                raise ValueError(f"í…ì„œ ì°¨ì›ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬: {image.dim()}ì°¨ì›")
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(image)}")
    
    def _format_result(self, result):
        """ê²°ê³¼ í¬ë§·íŒ…"""
        if self.model_type == 'openpose':
            return self._format_openpose_result(result)
        elif self.model_type == 'hrnet':
            return self._format_hrnet_result(result)
        elif self.model_type == 'graphonomy':
            return self._format_graphonomy_result(result)
        else:
            return self._format_generic_result(result)
    
    def _format_openpose_result(self, result):
        """OpenPose ê²°ê³¼ í¬ë§·íŒ…"""
        if isinstance(result, dict) and 'keypoints' in result:
            return {
                'success': True,
                'keypoints': result['keypoints'],
                'confidence_scores': result.get('confidence_scores', []),
                'heatmaps': result.get('heatmaps', None),
                'model_type': 'openpose',
                'num_keypoints': len(result['keypoints'])
            }
        else:
            return {
                'success': True,
                'keypoints': result if isinstance(result, list) else [],
                'confidence_scores': [0.9] * 17,
                'model_type': 'openpose',
                'num_keypoints': 17
            }
    
    def _format_hrnet_result(self, result):
        """HRNet ê²°ê³¼ í¬ë§·íŒ…"""
        if isinstance(result, dict) and 'keypoints' in result:
            return {
                'success': True,
                'keypoints': result['keypoints'],
                'confidence_scores': result.get('confidence_scores', []),
                'heatmaps': result.get('heatmaps', None),
                'model_type': 'hrnet',
                'num_keypoints': len(result['keypoints'])
            }
        else:
            return {
                'success': True,
                'keypoints': result if isinstance(result, list) else [],
                'confidence_scores': [0.9] * 17,
                'model_type': 'hrnet',
                'num_keypoints': 17
            }
    
    def _format_graphonomy_result(self, result):
        """Graphonomy ê²°ê³¼ í¬ë§·íŒ…"""
        if isinstance(result, dict) and 'segmentation_map' in result:
            return {
                'success': True,
                'segmentation_map': result['segmentation_map'],
                'probabilities': result.get('probabilities', None),
                'num_classes': result.get('num_classes', 20),
                'model_type': 'graphonomy'
            }
        else:
            return {
                'success': True,
                'segmentation_map': result if isinstance(result, torch.Tensor) else None,
                'num_classes': 20,
                'model_type': 'graphonomy'
            }
    
    def _format_generic_result(self, result):
        """ì¼ë°˜ ê²°ê³¼ í¬ë§·íŒ…"""
        return {
            'success': True,
            'result': result,
            'model_type': self.model_type
        }
    
    def _get_error_result(self, error_message: str):
        """ì˜¤ë¥˜ ê²°ê³¼ ë°˜í™˜"""
        return {
            'success': False,
            'error': error_message,
            'model_type': self.model_type
        }
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return self.model_info
    
    def get_supported_input_formats(self):
        """ì§€ì›í•˜ëŠ” ì…ë ¥ í˜•ì‹ ë°˜í™˜"""
        return self.complete_model.supported_formats
    
    def get_input_shape(self):
        """ì…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return self.complete_model.input_shape
    
    def get_output_shape(self):
        """ì¶œë ¥ í˜•íƒœ ë°˜í™˜"""
        return self.complete_model.output_shape
    
    def is_ready(self):
        """ëª¨ë¸ì´ ì‚¬ìš© ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.complete_model is not None
    
    def get_model_summary(self):
        """ëª¨ë¸ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'model_type': self.model_type,
            'base_model_class': self.complete_model.base_model.__class__.__name__,
            'input_shape': self.input_shape,
            'output_shape': self.output_shape,
            'supported_formats': self.supported_formats,
            'is_ready': self.is_ready()
        }

# Phase 3: ê³ ê¸‰ í†µí•© ê¸°ëŠ¥
# =============================================================================

class IntegratedInferenceEngine:
    """í†µí•© ì¶”ë¡  ì—”ì§„ - ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•œ ë³µí•© AI ì‘ì—… ìˆ˜í–‰"""
    
    def __init__(self):
        self.models = {}
        self.pipelines = {}
        self.cache = {}
        self.performance_metrics = {}
        
        # ì§€ì›í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ì •ì˜
        self.supported_pipelines = {
            'virtual_try_on': ['human_parsing', 'pose_estimation', 'cloth_segmentation', 'geometric_matching', 'cloth_warping'],
            'fashion_analysis': ['human_parsing', 'pose_estimation', 'cloth_segmentation'],
            'body_measurement': ['pose_estimation', 'human_parsing'],
            'style_recommendation': ['human_parsing', 'cloth_segmentation', 'pose_estimation']
        }
    
    def register_model(self, model_name: str, model: 'CompleteModelWrapper'):
        """ëª¨ë¸ ë“±ë¡"""
        self.models[model_name] = model
        print(f"âœ… ëª¨ë¸ ë“±ë¡: {model_name}")
    
    def create_pipeline(self, pipeline_name: str, model_sequence: list):
        """íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        if not model_sequence:
            raise ValueError("ëª¨ë¸ ì‹œí€€ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë”ë¯¸ ëª¨ë¸ ìƒì„±
        missing_models = []
        for model_name in model_sequence:
            if model_name not in self.models:
                missing_models.append(model_name)
        
        if missing_models:
            print(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸ë“¤: {missing_models}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {list(self.models.keys())}")
            # ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸ë“¤ì„ ë”ë¯¸ ëª¨ë¸ë¡œ ëŒ€ì²´
            for missing_model in missing_models:
                print(f"   ğŸ”„ {missing_model}ì„ ë”ë¯¸ ëª¨ë¸ë¡œ ëŒ€ì²´")
                self.models[missing_model] = self._create_dummy_model(missing_model)
        
        self.pipelines[pipeline_name] = model_sequence
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ìƒì„±: {pipeline_name} -> {model_sequence}")
    
    def _create_dummy_model(self, model_name: str):
        """ë”ë¯¸ ëª¨ë¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        class DummyModel:
            def __init__(self, name):
                self.name = name
            
            def __call__(self, input_data):
                # ë”ë¯¸ ê²°ê³¼ ë°˜í™˜
                if isinstance(input_data, torch.Tensor):
                    return torch.randn(1, 10, 64, 64)  # ë”ë¯¸ í…ì„œ
                elif isinstance(input_data, list):
                    # ë¦¬ìŠ¤íŠ¸ ì…ë ¥ì„ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
                    return torch.randn(1, 10, 64, 64)
                else:
                    return {
                        'keypoints': [[100, 100, 0.8] for _ in range(17)],
                        'confidence_scores': [0.8] * 17,
                        'heatmaps': np.random.rand(17, 64, 64).tolist(),
                        'keypoint_names': ['nose', 'left_eye', 'right_eye'] + ['kp_' + str(i) for i in range(4, 17)],
                        'num_keypoints': 17
                    }
        
        return DummyModel(model_name)
    
    def run_pipeline(self, pipeline_name: str, input_data, **kwargs):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        if pipeline_name not in self.pipelines:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ íŒŒì´í”„ë¼ì¸: {pipeline_name}")
        
        print(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: {pipeline_name}")
        
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        validated_input = self._validate_input_data(input_data, pipeline_name)
        
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(pipeline_name, validated_input, kwargs)
        if cache_key in self.cache:
            print(f"ğŸ“‹ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {pipeline_name}")
            return self.cache[cache_key]
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        current_data = validated_input
        results = {}
        execution_time = {}
        
        for step_idx, model_name in enumerate(self.pipelines[pipeline_name]):
            if model_name not in self.models:
                raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            
            print(f"  ğŸ“Œ Step {step_idx + 1}: {model_name}")
            
            # ëª¨ë¸ ì‹¤í–‰
            start_time = time.time()
            try:
                # ì…ë ¥ ë°ì´í„° íƒ€ì… ê²€ì¦
                step_input = self._prepare_step_input(current_data, model_name, step_idx)
                
                # ëª¨ë¸ ì‹¤í–‰
                step_result = self.models[model_name](step_input)
                
                # ê²°ê³¼ ê²€ì¦
                validated_result = self._validate_step_result(step_result, model_name)
                
                execution_time[model_name] = time.time() - start_time
                
                # ê²°ê³¼ ì €ì¥
                results[model_name] = validated_result
                current_data = validated_result  # ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                
                print(f"    âœ… {model_name} ì™„ë£Œ ({execution_time[model_name]:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"    âŒ {model_name} ì‹¤íŒ¨: {e}")
                # ì˜¤ë¥˜ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
                return {
                    'pipeline_name': pipeline_name,
                    'success': False,
                    'error': str(e),
                    'failed_step': model_name,
                    'step_index': step_idx,
                    'partial_results': results,
                    'execution_time': execution_time
                }
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            'pipeline_name': pipeline_name,
            'results': results,
            'execution_time': execution_time,
            'total_time': sum(execution_time.values()),
            'success': True
        }
        
        # ìºì‹œ ì €ì¥
        self.cache[cache_key] = final_result
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_performance_metrics(pipeline_name, final_result)
        
        print(f"ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {pipeline_name} (ì´ {final_result['total_time']:.2f}ì´ˆ)")
        return final_result
    
    def _validate_input_data(self, input_data, pipeline_name):
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        if input_data is None:
            raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
        
        # íŒŒì´í”„ë¼ì¸ë³„ ì…ë ¥ ê²€ì¦
        if pipeline_name == 'virtual_try_on':
            if not isinstance(input_data, dict):
                raise ValueError("ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì€ dict í˜•íƒœì˜ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if 'person_image' not in input_data or 'clothing_image' not in input_data:
                raise ValueError("ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì€ 'person_image'ì™€ 'clothing_image'ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif pipeline_name in ['fashion_analysis', 'body_measurement', 'style_recommendation']:
            if not (isinstance(input_data, (np.ndarray, torch.Tensor, str)) or 
                   hasattr(input_data, 'convert')):  # PIL Image
                raise ValueError(f"{pipeline_name} íŒŒì´í”„ë¼ì¸ì€ ì´ë¯¸ì§€ í˜•íƒœì˜ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return input_data
    
    def _prepare_step_input(self, current_data, model_name, step_idx):
        """ë‹¨ê³„ë³„ ì…ë ¥ ë°ì´í„° ì¤€ë¹„"""
        # ì²« ë²ˆì§¸ ë‹¨ê³„ì¸ ê²½ìš° ì›ë³¸ ì…ë ¥ ì‚¬ìš©
        if step_idx == 0:
            return current_data
        
        # ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        if isinstance(current_data, dict):
            # dict í˜•íƒœì˜ ê²°ê³¼ì—ì„œ ì ì ˆí•œ í‚¤ ì„ íƒ
            if 'keypoints' in current_data:
                # í‚¤í¬ì¸íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
                keypoints = current_data['keypoints']
                if isinstance(keypoints, list):
                    # í‚¤í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
                    return torch.tensor(keypoints, dtype=torch.float32)
                return keypoints
            elif 'segmentation_map' in current_data:
                seg_map = current_data['segmentation_map']
                if isinstance(seg_map, list):
                    return torch.tensor(seg_map, dtype=torch.float32)
                return seg_map
            elif 'heatmaps' in current_data:
                heatmaps = current_data['heatmaps']
                if isinstance(heatmaps, list):
                    return torch.tensor(heatmaps, dtype=torch.float32)
                return heatmaps
            else:
                # ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                first_key = list(current_data.keys())[0]
                first_value = current_data[first_key]
                if isinstance(first_value, list):
                    return torch.tensor(first_value, dtype=torch.float32)
                return first_value
        elif isinstance(current_data, list):
            # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            return torch.tensor(current_data, dtype=torch.float32)
        else:
            return current_data
    
    def _validate_step_result(self, step_result, model_name):
        """ë‹¨ê³„ë³„ ê²°ê³¼ ê²€ì¦"""
        if step_result is None:
            raise ValueError(f"{model_name}ì˜ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
        
        # ëª¨ë¸ë³„ ê²°ê³¼ ê²€ì¦
        if model_name == 'pose_estimation':
            if not isinstance(step_result, (dict, torch.Tensor)):
                raise ValueError(f"{model_name}ì˜ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
        elif model_name == 'human_parsing':
            if not isinstance(step_result, (dict, torch.Tensor)):
                raise ValueError(f"{model_name}ì˜ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
        elif model_name == 'cloth_segmentation':
            if not isinstance(step_result, (dict, torch.Tensor)):
                raise ValueError(f"{model_name}ì˜ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
        
        return step_result
    
    def run_virtual_try_on(self, person_image, clothing_image, **kwargs):
        """ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        return self.run_pipeline('virtual_try_on', {
            'person_image': person_image,
            'clothing_image': clothing_image
        }, **kwargs)
    
    def run_fashion_analysis(self, image, **kwargs):
        """íŒ¨ì…˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        return self.run_pipeline('fashion_analysis', {
            'image': image
        }, **kwargs)
    
    def run_body_measurement(self, image, **kwargs):
        """ì‹ ì²´ ì¸¡ì • íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        return self.run_pipeline('body_measurement', {
            'image': image
        }, **kwargs)
    
    def run_style_recommendation(self, image, **kwargs):
        """ìŠ¤íƒ€ì¼ ì¶”ì²œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        return self.run_pipeline('style_recommendation', {
            'image': image
        }, **kwargs)
    
    def _generate_cache_key(self, pipeline_name: str, input_data, kwargs):
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±
        import hashlib
        import json
        
        data_str = json.dumps({
            'pipeline': pipeline_name,
            'input_shape': str(type(input_data)),
            'kwargs': kwargs
        }, sort_keys=True)
        
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _update_performance_metrics(self, pipeline_name: str, result: dict):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        if pipeline_name not in self.performance_metrics:
            self.performance_metrics[pipeline_name] = {
                'total_runs': 0,
                'total_time': 0,
                'avg_time': 0,
                'success_rate': 0,
                'successful_runs': 0
            }
        
        metrics = self.performance_metrics[pipeline_name]
        metrics['total_runs'] += 1
        metrics['total_time'] += result['total_time']
        metrics['avg_time'] = metrics['total_time'] / metrics['total_runs']
        
        if result['success']:
            metrics['successful_runs'] += 1
        
        metrics['success_rate'] = metrics['successful_runs'] / metrics['total_runs']
    
    def get_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë°˜í™˜"""
        return {
            'pipelines': self.performance_metrics,
            'registered_models': list(self.models.keys()),
            'available_pipelines': list(self.pipelines.keys()),
            'cache_size': len(self.cache)
        }
    
    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self.cache.clear()
        print("ğŸ—‘ï¸ ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")
    
    def get_model_info(self, model_name: str):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if model_name not in self.models:
            return None
        
        model = self.models[model_name]
        return {
            'model_type': model.model_type,
            'input_shape': model.input_shape,
            'output_shape': model.output_shape,
            'supported_formats': model.supported_formats
        }

class RealTimePerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = {}
        self.history = {}
        self.alerts = []
        self.thresholds = {
            'execution_time': 10.0,  # 10ì´ˆ
            'memory_usage': 0.8,     # 80%
            'accuracy_threshold': 0.7,  # 70%
            'error_rate': 0.1        # 10%
        }
        
        # ì„±ëŠ¥ ì¹´í…Œê³ ë¦¬
        self.categories = {
            'execution_time': 'ì‹œê°„',
            'memory_usage': 'ë©”ëª¨ë¦¬',
            'accuracy': 'ì •í™•ë„',
            'throughput': 'ì²˜ë¦¬ëŸ‰',
            'error_rate': 'ì˜¤ë¥˜ìœ¨'
        }
    
    def start_monitoring(self, model_name: str, operation: str):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        import psutil
        import time
        
        monitor_id = f"{model_name}_{operation}_{int(time.time())}"
        
        self.metrics[monitor_id] = {
            'model_name': model_name,
            'operation': operation,
            'start_time': time.time(),
            'start_memory': psutil.virtual_memory().percent,
            'start_cpu': psutil.cpu_percent(),
            'status': 'running'
        }
        
        print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œì‘: {monitor_id}")
        return monitor_id
    
    def update_metrics(self, monitor_id: str, **kwargs):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        if monitor_id not in self.metrics:
            return
        
        import psutil
        
        current_metrics = self.metrics[monitor_id]
        current_metrics.update(kwargs)
        current_metrics['current_memory'] = psutil.virtual_memory().percent
        current_metrics['current_cpu'] = psutil.cpu_percent()
        current_metrics['timestamp'] = time.time()
        
        # ì„±ëŠ¥ ì²´í¬
        self._check_performance_thresholds(monitor_id)
    
    def stop_monitoring(self, monitor_id: str, final_metrics: dict = None):
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        if monitor_id not in self.metrics:
            return
        
        import psutil
        import time
        
        current_metrics = self.metrics[monitor_id]
        end_time = time.time()
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        execution_time = end_time - current_metrics['start_time']
        memory_usage = psutil.virtual_memory().percent
        cpu_usage = psutil.cpu_percent()
        
        final_result = {
            'monitor_id': monitor_id,
            'model_name': current_metrics['model_name'],
            'operation': current_metrics['operation'],
            'execution_time': execution_time,
            'memory_usage': memory_usage,
            'cpu_usage': cpu_usage,
            'status': 'completed',
            'timestamp': end_time
        }
        
        if final_metrics:
            final_result.update(final_metrics)
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if current_metrics['model_name'] not in self.history:
            self.history[current_metrics['model_name']] = []
        
        self.history[current_metrics['model_name']].append(final_result)
        
        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        if len(self.history[current_metrics['model_name']]) > 100:
            self.history[current_metrics['model_name']] = self.history[current_metrics['model_name']][-100:]
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        current_metrics.update(final_result)
        current_metrics['status'] = 'completed'
        
        print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì™„ë£Œ: {monitor_id} ({execution_time:.2f}ì´ˆ)")
        return final_result
    
    def _check_performance_thresholds(self, monitor_id: str):
        """ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬"""
        metrics = self.metrics[monitor_id]
        
        # ì‹¤í–‰ ì‹œê°„ ì²´í¬
        if 'execution_time' in metrics:
            if metrics['execution_time'] > self.thresholds['execution_time']:
                self._create_alert(monitor_id, 'execution_time', 
                                 f"ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: {metrics['execution_time']:.2f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        if 'current_memory' in metrics:
            if metrics['current_memory'] > self.thresholds['memory_usage'] * 100:
                self._create_alert(monitor_id, 'memory_usage',
                                 f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {metrics['current_memory']:.1f}%")
        
        # ì •í™•ë„ ì²´í¬
        if 'accuracy' in metrics:
            if metrics['accuracy'] < self.thresholds['accuracy_threshold']:
                self._create_alert(monitor_id, 'accuracy',
                                 f"ì •í™•ë„ ë‚®ìŒ: {metrics['accuracy']:.2f}")
    
    def _create_alert(self, monitor_id: str, alert_type: str, message: str):
        """ì•Œë¦¼ ìƒì„±"""
        alert = {
            'monitor_id': monitor_id,
            'type': alert_type,
            'message': message,
            'timestamp': time.time(),
            'severity': 'warning'
        }
        
        self.alerts.append(alert)
        print(f"âš ï¸ ì„±ëŠ¥ ì•Œë¦¼: {message}")
    
    def get_performance_summary(self, model_name: str = None):
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        if model_name:
            if model_name not in self.history:
                return None
            
            history = self.history[model_name]
            if not history:
                return None
            
            # í†µê³„ ê³„ì‚°
            execution_times = [h['execution_time'] for h in history]
            memory_usages = [h['memory_usage'] for h in history]
            cpu_usages = [h['cpu_usage'] for h in history]
            
            return {
                'model_name': model_name,
                'total_runs': len(history),
                'avg_execution_time': np.mean(execution_times),
                'max_execution_time': np.max(execution_times),
                'min_execution_time': np.min(execution_times),
                'avg_memory_usage': np.mean(memory_usages),
                'avg_cpu_usage': np.mean(cpu_usages),
                'recent_runs': history[-10:]  # ìµœê·¼ 10ê°œ
            }
        else:
            # ì „ì²´ ëª¨ë¸ ìš”ì•½
            summaries = {}
            for model in self.history.keys():
                summary = self.get_performance_summary(model)
                if summary:
                    summaries[model] = summary
            
            return summaries
    
    def get_alerts(self, severity: str = None):
        """ì•Œë¦¼ ë°˜í™˜"""
        if severity:
            return [alert for alert in self.alerts if alert['severity'] == severity]
        return self.alerts
    
    def clear_alerts(self):
        """ì•Œë¦¼ í´ë¦¬ì–´"""
        self.alerts.clear()
        print("ğŸ—‘ï¸ ì•Œë¦¼ í´ë¦¬ì–´ ì™„ë£Œ")
    
    def set_thresholds(self, **kwargs):
        """ì„ê³„ê°’ ì„¤ì •"""
        self.thresholds.update(kwargs)
        print(f"âš™ï¸ ì„ê³„ê°’ ì—…ë°ì´íŠ¸: {kwargs}")
    
    def get_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        import psutil
        
        return {
            'memory': {
                'total': psutil.virtual_memory().total,
                'available': psutil.virtual_memory().available,
                'percent': psutil.virtual_memory().percent
            },
            'cpu': {
                'percent': psutil.cpu_percent(),
                'count': psutil.cpu_count()
            },
            'disk': {
                'total': psutil.disk_usage('/').total,
                'free': psutil.disk_usage('/').free,
                'percent': psutil.disk_usage('/').percent
            }
        }

class AdvancedModelManager:
    """ê³ ê¸‰ ëª¨ë¸ ê´€ë¦¬ì - ëª¨ë¸ ìƒëª…ì£¼ê¸° ë° ë²„ì „ ê´€ë¦¬"""
    
    def __init__(self, base_path: str = "./models"):
        self.base_path = base_path
        self.models = {}
        self.versions = {}
        self.backups = {}
        self.dependencies = {}
        
        # ëª¨ë¸ ìƒíƒœ
        self.model_states = {
            'active': 'í™œì„±',
            'inactive': 'ë¹„í™œì„±',
            'deprecated': 'ì‚¬ìš©ì¤‘ë‹¨',
            'testing': 'í…ŒìŠ¤íŠ¸ì¤‘',
            'backup': 'ë°±ì—…'
        }
        
        # ìë™ ê´€ë¦¬ ì„¤ì •
        self.auto_management = {
            'auto_backup': True,
            'auto_update': False,
            'version_control': True,
            'dependency_check': True
        }
    
    def register_model(self, model_name: str, model_path: str, version: str = "1.0.0", 
                      dependencies: list = None, metadata: dict = None):
        """ëª¨ë¸ ë“±ë¡"""
        model_info = {
            'name': model_name,
            'path': model_path,
            'version': version,
            'dependencies': dependencies or [],
            'metadata': metadata or {},
            'state': 'active',
            'registered_at': time.time(),
            'last_used': None,
            'usage_count': 0
        }
        
        self.models[model_name] = model_info
        
        # ë²„ì „ ê´€ë¦¬
        if model_name not in self.versions:
            self.versions[model_name] = []
        self.versions[model_name].append(version)
        
        # ì˜ì¡´ì„± ê´€ë¦¬
        if dependencies:
            self.dependencies[model_name] = dependencies
        
        print(f"âœ… ëª¨ë¸ ë“±ë¡: {model_name} v{version}")
        return model_info
    
    def get_model(self, model_name: str, version: str = None):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if model_name not in self.models:
            return None
        
        model_info = self.models[model_name]
        
        # ë²„ì „ ì§€ì •ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë²„ì „ í™•ì¸
        if version and version != model_info['version']:
            if version in self.versions.get(model_name, []):
                # ë²„ì „ë³„ ì •ë³´ ë°˜í™˜ (ê°„ë‹¨í•œ êµ¬í˜„)
                return {**model_info, 'version': version}
            else:
                return None
        
        # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        model_info['last_used'] = time.time()
        model_info['usage_count'] += 1
        
        return model_info
    
    def update_model(self, model_name: str, new_path: str, new_version: str, 
                    changelog: str = None, auto_backup: bool = True):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if model_name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
        
        old_info = self.models[model_name]
        
        # ìë™ ë°±ì—…
        if auto_backup and self.auto_management['auto_backup']:
            self.create_backup(model_name, f"pre_update_{new_version}")
        
        # ìƒˆ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
        new_info = {
            **old_info,
            'path': new_path,
            'version': new_version,
            'updated_at': time.time(),
            'changelog': changelog
        }
        
        self.models[model_name] = new_info
        
        # ë²„ì „ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if new_version not in self.versions[model_name]:
            self.versions[model_name].append(new_version)
        
        print(f"ğŸ”„ ëª¨ë¸ ì—…ë°ì´íŠ¸: {model_name} v{old_info['version']} â†’ v{new_version}")
        return new_info
    
    def create_backup(self, model_name: str, backup_name: str = None):
        """ëª¨ë¸ ë°±ì—… ìƒì„±"""
        if model_name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
        
        model_info = self.models[model_name]
        
        if not backup_name:
            backup_name = f"backup_{int(time.time())}"
        
        backup_info = {
            'model_name': model_name,
            'backup_name': backup_name,
            'original_path': model_info['path'],
            'original_version': model_info['version'],
            'created_at': time.time(),
            'size': self._get_file_size(model_info['path'])
        }
        
        if model_name not in self.backups:
            self.backups[model_name] = []
        
        self.backups[model_name].append(backup_info)
        
        print(f"ğŸ’¾ ë°±ì—… ìƒì„±: {model_name} ({backup_name})")
        return backup_info
    
    def restore_backup(self, model_name: str, backup_name: str):
        """ë°±ì—…ì—ì„œ ë³µì›"""
        if model_name not in self.backups:
            raise ValueError(f"ë°±ì—…ì´ ì—†ëŠ” ëª¨ë¸: {model_name}")
        
        # ë°±ì—… ì°¾ê¸°
        backup = None
        for b in self.backups[model_name]:
            if b['backup_name'] == backup_name:
                backup = b
                break
        
        if not backup:
            raise ValueError(f"ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {backup_name}")
        
        # í˜„ì¬ ëª¨ë¸ ë°±ì—…
        self.create_backup(model_name, f"pre_restore_{int(time.time())}")
        
        # ë°±ì—…ì—ì„œ ë³µì›
        model_info = self.models[model_name]
        restored_info = {
            **model_info,
            'path': backup['original_path'],
            'version': backup['original_version'],
            'restored_at': time.time(),
            'restored_from': backup_name
        }
        
        self.models[model_name] = restored_info
        
        print(f"ğŸ”„ ë°±ì—… ë³µì›: {model_name} ({backup_name})")
        return restored_info
    
    def deprecate_model(self, model_name: str, reason: str = None):
        """ëª¨ë¸ ì‚¬ìš©ì¤‘ë‹¨"""
        if model_name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
        
        model_info = self.models[model_name]
        model_info['state'] = 'deprecated'
        model_info['deprecated_at'] = time.time()
        model_info['deprecation_reason'] = reason
        
        print(f"âš ï¸ ëª¨ë¸ ì‚¬ìš©ì¤‘ë‹¨: {model_name} - {reason}")
        return model_info
    
    def activate_model(self, model_name: str):
        """ëª¨ë¸ í™œì„±í™”"""
        if model_name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
        
        model_info = self.models[model_name]
        model_info['state'] = 'active'
        model_info['activated_at'] = time.time()
        
        print(f"âœ… ëª¨ë¸ í™œì„±í™”: {model_name}")
        return model_info
    
    def get_model_versions(self, model_name: str):
        """ëª¨ë¸ ë²„ì „ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.versions.get(model_name, [])
    
    def get_model_backups(self, model_name: str):
        """ëª¨ë¸ ë°±ì—… ëª©ë¡ ë°˜í™˜"""
        return self.backups.get(model_name, [])
    
    def check_dependencies(self, model_name: str):
        """ì˜ì¡´ì„± ì²´í¬"""
        if model_name not in self.dependencies:
            return {'status': 'no_dependencies', 'missing': []}
        
        dependencies = self.dependencies[model_name]
        missing = []
        
        for dep in dependencies:
            if dep not in self.models:
                missing.append(dep)
        
        return {
            'status': 'ok' if not missing else 'missing_dependencies',
            'dependencies': dependencies,
            'missing': missing
        }
    
    def get_model_statistics(self, model_name: str = None):
        """ëª¨ë¸ í†µê³„ ë°˜í™˜"""
        if model_name:
            if model_name not in self.models:
                return None
            
            model_info = self.models[model_name]
            return {
                'name': model_name,
                'version': model_info['version'],
                'state': model_info['state'],
                'usage_count': model_info['usage_count'],
                'last_used': model_info['last_used'],
                'registered_at': model_info['registered_at'],
                'backup_count': len(self.backups.get(model_name, [])),
                'version_count': len(self.versions.get(model_name, []))
            }
        else:
            # ì „ì²´ í†µê³„
            stats = {
                'total_models': len(self.models),
                'active_models': len([m for m in self.models.values() if m['state'] == 'active']),
                'deprecated_models': len([m for m in self.models.values() if m['state'] == 'deprecated']),
                'total_backups': sum(len(backups) for backups in self.backups.values()),
                'total_versions': sum(len(versions) for versions in self.versions.values())
            }
            
            # ëª¨ë¸ë³„ ìƒì„¸ í†µê³„
            stats['models'] = {}
            for name in self.models.keys():
                stats['models'][name] = self.get_model_statistics(name)
            
            return stats
    
    def _get_file_size(self, file_path: str):
        """íŒŒì¼ í¬ê¸° ë°˜í™˜"""
        try:
            import os
            return os.path.getsize(file_path)
        except:
            return 0
    
    def set_auto_management(self, **kwargs):
        """ìë™ ê´€ë¦¬ ì„¤ì •"""
        self.auto_management.update(kwargs)
        print(f"âš™ï¸ ìë™ ê´€ë¦¬ ì„¤ì • ì—…ë°ì´íŠ¸: {kwargs}")
    
    def cleanup_old_backups(self, model_name: str, keep_count: int = 5):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        if model_name not in self.backups:
            return
        
        backups = self.backups[model_name]
        if len(backups) <= keep_count:
            return
        
        # ì˜¤ë˜ëœ ë°±ì—… ì œê±° (ìµœì‹  keep_countê°œë§Œ ìœ ì§€)
        backups.sort(key=lambda x: x['created_at'], reverse=True)
        self.backups[model_name] = backups[:keep_count]
        
        print(f"ğŸ—‘ï¸ ë°±ì—… ì •ë¦¬: {model_name} (ìµœì‹  {keep_count}ê°œë§Œ ìœ ì§€)")
    
    def export_model_info(self, model_name: str = None):
        """ëª¨ë¸ ì •ë³´ ë‚´ë³´ë‚´ê¸°"""
        if model_name:
            return self.models.get(model_name, {})
        else:
            return {
                'models': self.models,
                'versions': self.versions,
                'backups': self.backups,
                'dependencies': self.dependencies,
                'auto_management': self.auto_management
            }
