# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - ìµœì í™”ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° v4.1 (í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜)
================================================================================
âœ… í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ ì™„ì „ ë°˜ì˜ - 155ê°œ ì‹¤ì œ íŒŒì¼ ì •í™• ë§¤í•‘
âœ… ê²½ë¡œ ìš°ì„ ìˆœìœ„ ìµœì í™” - ì¤‘ë³µ íŒŒì¼ ì œê±° ë° ì •í™•í•œ ê²½ë¡œ ë§¤í•‘
âœ… í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì™„ì „ ì •í™• ì ìš© (50MB+ í•„í„°ë§)
âœ… ModelLoader v5.1ê³¼ ì™„ì „ ì—°ë™ (AI í´ë˜ìŠ¤ ìë™ í• ë‹¹)
âœ… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì™„ì „ ê²€ì¦ ë° ìš°ì„ ìˆœìœ„ ì ìš©
âœ… M3 Max + conda í™˜ê²½ ìµœì í™”
âœ… ì‹¤ì œ íŒŒì¼ í¬ê¸° ì •í™• ë°˜ì˜ ë° ìš°ì„ ìˆœìœ„ ì •ë ¬

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ğŸ¯ í„°ë¯¸ë„ ì¶œë ¥ 155ê°œ íŒŒì¼ ì™„ì „ ë§¤í•‘
2. ğŸ”§ ì¤‘ë³µ íŒŒì¼ ì œê±° (sam_vit_h_4b8939.pth ë“± ì—¬ëŸ¬ ìœ„ì¹˜ ì¡´ì¬)
3. ğŸš€ ê²½ë¡œ ìš°ì„ ìˆœìœ„: checkpoints > step_XX > ultra_models
4. ğŸ§  AI í´ë˜ìŠ¤ ìë™ ì¶”ë¡  ê°•í™”
5. ğŸ“Š í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì •í™• ì ìš©

ì‹¤ì œ í™•ì¸ëœ ëŒ€ìš©ëŸ‰ ëª¨ë¸ë“¤:
- v1-5-pruned.safetensors (7.2GB)
- RealVisXL_V4.0.safetensors (6.5GB) 
- open_clip_pytorch_model.bin (5.1GB)
- sam_vit_h_4b8939.pth (2.4GB)
- diffusion_pytorch_model.safetensors (3.2GBÃ—4)
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ì •í™•í•œ íŒŒì¼ ë§¤í•‘ ì‹œìŠ¤í…œ
# ==============================================

class OptimizedFileMapper:
    """í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”ëœ íŒŒì¼ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.OptimizedFileMapper")
        
        # ğŸ”¥ í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì‹¤ì œ íŒŒì¼ ë§¤í•‘ (ì¤‘ë³µ ì œê±° + ìš°ì„ ìˆœìœ„)
        self.step_file_mappings = {
            # Step 01: Human Parsing (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜)
            "human_parsing_schp": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("ai_models/checkpoints/step_01_human_parsing/exp-schp-201908301523-atr.pth", 1),
                    ("ai_models/checkpoints/step_01_human_parsing/exp-schp-201908261155-lip.pth", 2),
                    ("ai_models/checkpoints/step_01_human_parsing/atr_model.pth", 3),
                    ("ai_models/checkpoints/step_01_human_parsing/graphonomy.pth", 4),
                    ("ai_models/checkpoints/step_01_human_parsing/graphonomy_alternative.pth", 5),
                    ("ai_models/checkpoints/step_01_human_parsing/lip_model.pth", 6),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_01_human_parsing/ultra_models/fcn_resnet101_ultra.pth", 7),
                    ("step_01_human_parsing/graphonomy_fixed.pth", 8),
                    # ê¸°íƒ€ ìœ„ì¹˜
                    ("Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth", 6),
                    ("Graphonomy/inference.pth", 7)
                ],
                "search_patterns": [r".*exp-schp.*atr.*\.pth$", r".*graphonomy.*\.pth$"],
                "size_range": (80, 1200),
                "min_size_mb": 80,
                "step_class": "HumanParsingStep",
                "ai_class": "RealGraphonomyModel"
            },
            
            # Step 02: Pose Estimation (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜)
            "pose_estimation_openpose": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("ai_models/checkpoints/step_02_pose_estimation/body_pose_model.pth", 1),
                    ("ai_models/checkpoints/step_02_pose_estimation/openpose.pth", 2),
                    ("ai_models/checkpoints/step_02_pose_estimation/yolov8n-pose.pt", 3),
                    # âœ… OOTD ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/ootdiffusion/checkpoints/ootd/feature_extractor/preprocessor_config.json", 4),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_02_pose_estimation/hrnet_w48_coco_384x288.pth", 5),
                    ("step_02_pose_estimation/yolov8m-pose.pt", 6)
                ],
                "search_patterns": [r".*body_pose.*\.pth$", r".*openpose.*\.pth$", r".*yolov8.*pose.*\.pt$"],
                "size_range": (6, 1400),
                "min_size_mb": 6,
                "step_class": "PoseEstimationStep", 
                "ai_class": "RealOpenPoseModel"
            },
            
            # Step 03: Cloth Segmentation (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜)
            "cloth_segmentation_sam": {
                "priority_files": [
                    # âœ… SAM ëª¨ë¸ë“¤ (2.4GB) - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ê²½ë¡œ
                    ("checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth", 1),
                    ("checkpoints/step_03_cloth_segmentation/sam_vit_l_0b3195.pth", 2),
                    ("checkpoints/step_04_geometric_matching/sam_vit_h_4b8939.pth", 3),
                    # âœ… U2Net ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/step_03_cloth_segmentation/u2net_alternative.pth", 4),
                    ("checkpoints/step_03_cloth_segmentation/u2net_fallback.pth", 5),
                    # âœ… ê¸°íƒ€ segmentation ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth", 6),
                    ("checkpoints/step_03_cloth_segmentation/mobile_sam.pt", 7),
                    ("checkpoints/step_03_cloth_segmentation/mobile_sam_alternative.pt", 8),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_03_cloth_segmentation/u2net.pth", 9),
                    ("step_06_virtual_fitting/u2net_fixed.pth", 10)
                ],
                "search_patterns": [r".*sam_vit.*\.pth$", r".*u2net.*\.pth$", r".*mobile_sam.*\.pt$"],
                "size_range": (100, 2500),
                "min_size_mb": 100,
                "step_class": "ClothSegmentationStep",
                "ai_class": "RealSAMModel"
            },
            
            # Step 04: Geometric Matching (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜)
            "geometric_matching_gmm": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("checkpoints/step_04_geometric_matching/gmm_final.pth", 1),
                    ("checkpoints/step_04_geometric_matching/tps_network.pth", 2),
                    ("checkpoints/step_04_geometric_matching/sam_vit_h_4b8939.pth", 3),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_04_geometric_matching/gmm_final.pth", 4),
                    ("step_04_geometric_matching/tps_network.pth", 5),
                    ("step_04_geometric_matching/ultra_models/resnet101_geometric.pth", 6),
                    ("step_04_geometric_matching/ultra_models/raft-things.pth", 7)
                ],
                "search_patterns": [r".*gmm.*\.pth$", r".*tps.*\.pth$", r".*geometric.*\.pth$"],
                "size_range": (10, 2500),
                "min_size_mb": 10,
                "step_class": "GeometricMatchingStep",
                "ai_class": "RealGMMModel"
            },
            
            # Step 05: Cloth Warping (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜ - RealVisXL 6.5GB!)
            "cloth_warping_realvisxl": {
                "priority_files": [
                    # âœ… RealVisXL (6.5GB) - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ê²½ë¡œ
                    ("checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors", 1),
                    # âœ… VGG ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/step_05_cloth_warping/vgg19_warping.pth", 2),
                    ("checkpoints/step_05_cloth_warping/vgg16_warping_ultra.pth", 3),
                    # âœ… ê¸°íƒ€ warping ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/step_05_cloth_warping/densenet121_ultra.pth", 4),
                    ("checkpoints/step_05_cloth_warping/tom_final.pth", 5),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_05_cloth_warping/RealVisXL_V4.0.safetensors", 6),
                    ("step_05_cloth_warping/ultra_models/vgg19_warping.pth", 7)
                ],
                "search_patterns": [r".*RealVis.*\.safetensors$", r".*vgg.*warp.*\.pth$", r".*densenet.*\.pth$"],
                "size_range": (30, 7000),
                "min_size_mb": 30,
                "step_class": "ClothWarpingStep", 
                "ai_class": "RealVisXLModel"
            },
            
            # Step 06: Virtual Fitting (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜ - OOTD Diffusion)
            "virtual_fitting_ootd": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.safetensors", 1),
                    ("checkpoints/step_06_virtual_fitting/hrviton_final.pth", 2),
                    ("checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.bin", 3),
                    ("checkpoints/step_06_virtual_fitting/pytorch_model.bin", 4),  # ì‹¬ë³¼ë¦­ ë§í¬
                    # âœ… OOTD Diffusion checkpoints (3.2GBÃ—4) - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ë³µì¡í•œ ê²½ë¡œ
                    ("checkpoints/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors", 5),
                    ("checkpoints/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors", 6),
                    ("checkpoints/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors", 7),
                    ("checkpoints/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors", 8),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors", 9)
                ],
                "search_patterns": [r".*diffusion_pytorch_model\.safetensors$", r".*hrviton.*\.pth$", r".*ootd.*\.safetensors$"],
                "size_range": (100, 3300),
                "min_size_mb": 100,
                "step_class": "VirtualFittingStep",
                "ai_class": "RealOOTDDiffusionModel"
            },
            
            # Step 07: Post Processing (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜)
            "post_processing_gfpgan": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("checkpoints/step_07_post_processing/GFPGAN.pth", 1),
                    ("checkpoints/step_07_post_processing/RealESRGAN_x4plus.pth", 2),
                    ("checkpoints/step_07_post_processing/ESRGAN_x8.pth", 3),
                    ("checkpoints/step_07_post_processing/densenet161_enhance.pth", 4),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth", 5),
                    ("step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth", 6),
                    ("step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth", 7)
                ],
                "search_patterns": [r".*GFPGAN.*\.pth$", r".*ESRGAN.*\.pth$", r".*RealESRGAN.*\.pth$"],
                "size_range": (30, 350),
                "min_size_mb": 30,
                "step_class": "PostProcessingStep",
                "ai_class": "RealGFPGANModel"
            },
            
            # Step 08: Quality Assessment (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜ - CLIP 5.1GB!)
            "quality_assessment_clip": {
                "priority_files": [
                    # âœ… checkpoints ë””ë ‰í† ë¦¬ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤
                    ("checkpoints/step_08_quality_assessment/open_clip_pytorch_model.bin", 1),  # 5.1GB!
                    ("checkpoints/step_08_quality_assessment/ViT-L-14.pt", 2),
                    ("checkpoints/step_08_quality_assessment/ViT-B-32.pt", 3),
                    ("checkpoints/step_08_quality_assessment/lpips_vgg.pth", 4),
                    ("checkpoints/step_08_quality_assessment/lpips_alex.pth", 5),
                    # step ë””ë ‰í† ë¦¬ ë³´ì¡°
                    ("step_08_quality_assessment/ultra_models/ViT-L-14.pt", 6),
                    ("step_08_quality_assessment/ultra_models/alex.pth", 7),
                    ("step_04_geometric_matching/ultra_models/ViT-L-14.pt", 8)
                ],
                "search_patterns": [r".*open_clip.*\.bin$", r".*ViT-.*\.pt$", r".*clip.*\.pth$", r".*lpips.*\.pth$"],
                "size_range": (50, 5300),
                "min_size_mb": 50,
                "step_class": "QualityAssessmentStep",
                "ai_class": "RealCLIPModel"
            },
            
            # Stable Diffusion Models (ğŸ”¥ í„°ë¯¸ë„ tree ì¶œë ¥ ì™„ì „ ë°˜ì˜ - 7.2GB!)
            "stable_diffusion_v15": {
                "priority_files": [
                    # âœ… v1-5 ëª¨ë¸ë“¤ (7.2GB) - í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ê²½ë¡œ
                    ("checkpoints/stable-diffusion-v1-5/v1-5-pruned.safetensors", 1),
                    ("checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors", 2),
                    # âœ… UNet ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.safetensors", 3),
                    ("checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.fp16.safetensors", 4),
                    ("checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.non_ema.safetensors", 5),
                    # âœ… VAE ëª¨ë¸ë“¤ - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/stable-diffusion-v1-5/vae/diffusion_pytorch_model.safetensors", 6),
                    ("checkpoints/stable-diffusion-v1-5/vae/diffusion_pytorch_model.fp16.safetensors", 7),
                    # âœ… Text Encoder - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/stable-diffusion-v1-5/text_encoder/model.safetensors", 8),
                    ("checkpoints/stable-diffusion-v1-5/text_encoder/model.fp16.safetensors", 9),
                    # âœ… Safety Checker - í„°ë¯¸ë„ì—ì„œ í™•ì¸ë¨
                    ("checkpoints/stable-diffusion-v1-5/safety_checker/model.safetensors", 10),
                    ("checkpoints/stable-diffusion-v1-5/safety_checker/model.fp16.safetensors", 11)
                ],
                "search_patterns": [r".*v1-5-pruned.*\.safetensors$", r".*diffusion_pytorch_model.*\.safetensors$"],
                "size_range": (1000, 8000),
                "min_size_mb": 1000,
                "step_class": "StableDiffusionStep",
                "ai_class": "RealStableDiffusionModel"
            }
        }
        
        # í¬ê¸° ìš°ì„ ìˆœìœ„ ì„ê³„ê°’
        self.size_priority_threshold = 50  # 50MB ì´ìƒë§Œ
        
        self.logger.info(f"âœ… í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ ë§¤í•‘ ì´ˆê¸°í™”: {len(self.step_file_mappings)}ê°œ íŒ¨í„´")

    def find_best_model_file(self, request_name: str, ai_models_root: Path) -> Optional[Tuple[Path, int]]:
        """ìµœì ì˜ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        try:
            if request_name not in self.step_file_mappings:
                return None
            
            mapping = self.step_file_mappings[request_name]
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰
            for file_path, priority in mapping["priority_files"]:
                full_path = ai_models_root / file_path
                
                if full_path.exists() and full_path.is_file():
                    file_size_mb = full_path.stat().st_size / (1024 * 1024)
                    
                    # í¬ê¸° ê²€ì¦
                    min_size, max_size = mapping["size_range"]
                    if min_size <= file_size_mb <= max_size:
                        self.logger.info(f"âœ… ìš°ì„ ìˆœìœ„ ë§¤ì¹­: {request_name} â†’ {full_path} (ìš°ì„ ìˆœìœ„: {priority}, í¬ê¸°: {file_size_mb:.1f}MB)")
                        return full_path, priority
            
            # í´ë°±: íŒ¨í„´ ê¸°ë°˜ ê²€ìƒ‰
            return self._pattern_based_search(request_name, ai_models_root, mapping)
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _pattern_based_search(self, request_name: str, ai_models_root: Path, mapping: Dict) -> Optional[Tuple[Path, int]]:
        """íŒ¨í„´ ê¸°ë°˜ í´ë°± ê²€ìƒ‰"""
        try:
            candidates = []
            extensions = ['.pth', '.bin', '.safetensors', '.pt', '.ckpt']
            
            for ext in extensions:
                for model_file in ai_models_root.rglob(f"*{ext}"):
                    if model_file.is_file():
                        file_size_mb = model_file.stat().st_size / (1024 * 1024)
                        
                        # í¬ê¸° í•„í„°ë§
                        min_size, max_size = mapping["size_range"]
                        if not (min_size <= file_size_mb <= max_size):
                            continue
                        
                        # íŒ¨í„´ ë§¤ì¹­
                        filename_lower = model_file.name.lower()
                        for pattern in mapping["search_patterns"]:
                            if re.match(pattern, filename_lower):
                                candidates.append((model_file, file_size_mb, 100))  # ë‚®ì€ ìš°ì„ ìˆœìœ„
                                break
            
            if candidates:
                # í¬ê¸° ê¸°ì¤€ ì •ë ¬ (í° ê²ƒë¶€í„°)
                candidates.sort(key=lambda x: x[1], reverse=True)
                best_file, size_mb, priority = candidates[0]
                self.logger.info(f"ğŸ” íŒ¨í„´ ê¸°ë°˜ ë§¤ì¹­: {request_name} â†’ {best_file} (í¬ê¸°: {size_mb:.1f}MB)")
                return best_file, priority
                
            return None
            
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def get_step_info(self, request_name: str) -> Optional[Dict[str, Any]]:
        """Step êµ¬í˜„ì²´ ì •ë³´ ë°˜í™˜"""
        if request_name in self.step_file_mappings:
            mapping = self.step_file_mappings[request_name]
            return {
                "step_class": mapping.get("step_class"),
                "ai_class": mapping.get("ai_class"),
                "model_load_method": "load_models",
                "priority": 1,
                "patterns": mapping.get("search_patterns", []),
                "min_size_mb": mapping.get("min_size_mb", self.size_priority_threshold),
                "priority_file_count": len(mapping.get("priority_files", [])),
                "size_range": mapping.get("size_range", (50, 10000))
            }
        return None

    def validate_all_models(self, ai_models_root: Path) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦"""
        validation_results = {
            "total_models": len(self.step_file_mappings),
            "found_models": 0,
            "missing_models": [],
            "model_details": {},
            "total_size_gb": 0.0,
            "largest_models": []
        }
        
        for model_name, mapping in self.step_file_mappings.items():
            found_file_info = self.find_best_model_file(model_name, ai_models_root)
            
            if found_file_info:
                file_path, priority = found_file_info
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                file_size_gb = file_size_mb / 1024
                
                validation_results["found_models"] += 1
                validation_results["total_size_gb"] += file_size_gb
                validation_results["largest_models"].append((model_name, file_size_gb, str(file_path)))
                
                validation_results["model_details"][model_name] = {
                    "found": True,
                    "path": str(file_path),
                    "size_mb": file_size_mb,
                    "size_gb": file_size_gb,
                    "priority": priority,
                    "step_class": mapping.get("step_class"),
                    "ai_class": mapping.get("ai_class")
                }
            else:
                validation_results["missing_models"].append(model_name)
                validation_results["model_details"][model_name] = {
                    "found": False,
                    "step_class": mapping.get("step_class"),
                    "ai_class": mapping.get("ai_class")
                }
        
        # í° ëª¨ë¸ìˆœìœ¼ë¡œ ì •ë ¬
        validation_results["largest_models"].sort(key=lambda x: x[1], reverse=True)
        
        return validation_results

# ==============================================
# ğŸ”¥ 2. ìµœì í™”ëœ DetectedModel í´ë˜ìŠ¤
# ==============================================

@dataclass
class OptimizedDetectedModel:
    """ìµœì í™”ëœ íƒì§€ ëª¨ë¸ ì •ë³´"""
    name: str
    path: Path
    step_name: str
    model_type: str
    file_size_mb: float
    file_size_gb: float
    confidence_score: float
    priority_rank: int
    
    # ModelLoader ì—°ë™ ì •ë³´
    step_class_name: Optional[str] = None
    ai_class: Optional[str] = None
    model_load_method: str = "load_models"
    step_can_load: bool = False
    
    # ìš°ì„ ìˆœìœ„ ì •ë³´
    priority_score: float = 0.0
    is_ultra_large: bool = False  # 5GB+
    is_large_model: bool = False  # 1GB+
    meets_size_requirement: bool = False
    size_category: str = ""
    
    # ë””ë°”ì´ìŠ¤ ì •ë³´
    checkpoint_path: Optional[str] = None
    device_compatible: bool = True
    recommended_device: str = "cpu"
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ìë™ ê³„ì‚°"""
        self.file_size_gb = self.file_size_mb / 1024
        self.priority_score = self._calculate_priority_score()
        self.is_ultra_large = self.file_size_mb > 5000  # 5GB+
        self.is_large_model = self.file_size_mb > 1000  # 1GB+
        self.meets_size_requirement = self.file_size_mb >= 50  # 50MB+
        self.size_category = self._get_size_category()
    
    def _calculate_priority_score(self) -> float:
        """ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        if self.file_size_mb > 0:
            import math
            score += math.log10(max(self.file_size_mb, 1)) * 100
        
        # í¬ê¸°ë³„ ë³´ë„ˆìŠ¤ ì ìˆ˜
        if self.file_size_mb >= 7000:    # 7GB+ (v1-5-pruned)
            score += 1000
        elif self.file_size_mb >= 6000:  # 6GB+ (RealVisXL)
            score += 900
        elif self.file_size_mb >= 5000:  # 5GB+ (CLIP)
            score += 800
        elif self.file_size_mb >= 3000:  # 3GB+ (OOTD)
            score += 700
        elif self.file_size_mb >= 2000:  # 2GB+ (SAM)
            score += 600
        elif self.file_size_mb >= 1000:  # 1GB+
            score += 500
        elif self.file_size_mb >= 500:   # 500MB+
            score += 400
        elif self.file_size_mb >= 200:   # 200MB+
            score += 300
        elif self.file_size_mb >= 100:   # 100MB+
            score += 200
        elif self.file_size_mb >= 50:    # 50MB+
            score += 100
        else:
            score -= 500  # 50MB ë¯¸ë§Œì€ í° ê°ì 
        
        # ìš°ì„ ìˆœìœ„ ë­í¬ ë³´ë„ˆìŠ¤ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        score += max(0, 200 - (self.priority_rank * 10))
        
        # ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
        score += self.confidence_score * 50
        
        # Step ë¡œë“œ ê°€ëŠ¥ ë³´ë„ˆìŠ¤
        if self.step_can_load:
            score += 100
        
        # AI í´ë˜ìŠ¤ ë³´ë„ˆìŠ¤
        if self.ai_class and self.ai_class != "BaseRealAIModel":
            score += 50
        
        return score
    
    def _get_size_category(self) -> str:
        """í¬ê¸° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if self.file_size_mb >= 7000:
            return "ultra_massive"  # 7GB+ (v1-5-pruned)
        elif self.file_size_mb >= 6000:
            return "ultra_large"    # 6GB+ (RealVisXL)
        elif self.file_size_mb >= 5000:
            return "very_large"     # 5GB+ (CLIP)
        elif self.file_size_mb >= 3000:
            return "large"          # 3GB+ (OOTD)
        elif self.file_size_mb >= 2000:
            return "medium_large"   # 2GB+ (SAM)
        elif self.file_size_mb >= 1000:
            return "medium"         # 1GB+
        elif self.file_size_mb >= 500:
            return "small_large"    # 500MB+
        elif self.file_size_mb >= 200:
            return "small_medium"   # 200MB+
        elif self.file_size_mb >= 100:
            return "small"          # 100MB+
        elif self.file_size_mb >= 50:
            return "valid_small"    # 50MB+
        else:
            return "too_small"      # 50MB ë¯¸ë§Œ
    
    def to_dict(self) -> Dict[str, Any]:
        """ModelLoader í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "step_class": self.step_name,
            "model_type": self.model_type,
            "size_mb": self.file_size_mb,
            "size_gb": self.file_size_gb,
            "confidence": self.confidence_score,
            "priority_rank": self.priority_rank,
            
            # ModelLoader í˜¸í™˜ AI ëª¨ë¸ ì •ë³´
            "ai_model_info": {
                "ai_class": self.ai_class or "BaseRealAIModel",
                "can_create_ai_model": bool(self.ai_class),
                "device_compatible": self.device_compatible,
                "recommended_device": self.recommended_device
            },
            
            # Step ì—°ë™ ì •ë³´
            "step_implementation": {
                "step_class_name": self.step_class_name,
                "model_load_method": self.model_load_method,
                "step_can_load": self.step_can_load,
                "load_ready": self.step_can_load and self.checkpoint_path is not None
            },
            
            # ìš°ì„ ìˆœìœ„ ì •ë³´
            "priority_info": {
                "priority_score": self.priority_score,
                "priority_rank": self.priority_rank,
                "is_ultra_large": self.is_ultra_large,
                "is_large_model": self.is_large_model,
                "meets_size_requirement": self.meets_size_requirement,
                "size_category": self.size_category
            },
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device_config": {
                "recommended_device": self.recommended_device,
                "device_compatible": self.device_compatible
            },
            
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.path.suffix,
                "detector_version": "v4.1_terminal_optimized"
            }
        }
    
    def can_be_loaded_by_step(self) -> bool:
        """Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return (self.step_can_load and 
                self.step_class_name is not None and 
                self.model_load_method is not None and
                self.checkpoint_path is not None and
                self.meets_size_requirement and
                self.ai_class is not None)

# ==============================================
# ğŸ”¥ 3. ìµœì í™”ëœ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class OptimizedModelDetector:
    """í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ íƒì§€ê¸° v4.1"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.OptimizedModelDetector")
        self.file_mapper = OptimizedFileMapper()
        self.ai_models_root = self._find_ai_models_root()
        self.detected_models: Dict[str, OptimizedDetectedModel] = {}
        
        # ì„¤ì •
        self.min_model_size_mb = 50  # 50MB ë¯¸ë§Œ ì œì™¸
        self.prioritize_large_models = True
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.is_m3_max = self._detect_m3_max()
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # í†µê³„ ì •ë³´
        self.detection_stats = {
            "total_files_scanned": 0,
            "models_found": 0,
            "ultra_large_models": 0,  # 5GB+
            "large_models_found": 0,  # 1GB+
            "medium_models": 0,       # 100MB-1GB
            "small_models_filtered": 0,
            "step_loadable_models": 0,
            "ai_class_assigned": 0,
            "total_size_gb": 0.0,
            "scan_duration": 0.0
        }
        
        self.logger.info(f"ğŸ”§ í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ íƒì§€ê¸° v4.1 ì´ˆê¸°í™”")
        self.logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        self.logger.info(f"   ìµœì†Œ í¬ê¸°: {self.min_model_size_mb}MB")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ì—ì„œ backend ì°¾ê¸°
        current = Path(__file__).parent.absolute()
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ë©´ì„œ backend ì°¾ê¸°
        for _ in range(10):
            if current.name == 'backend':
                ai_models_path = current / 'ai_models'
                self.logger.info(f"âœ… AI ëª¨ë¸ ê²½ë¡œ: {ai_models_path}")
                return ai_models_path
            
            if current.name == 'mycloset-ai':
                ai_models_path = current / 'backend' / 'ai_models'
                self.logger.info(f"âœ… AI ëª¨ë¸ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸): {ai_models_path}")
                return ai_models_path
            
            if current.parent == current:  # ë£¨íŠ¸ì— ë„ë‹¬
                break
            current = current.parent
        
        # í„°ë¯¸ë„ ì¶œë ¥ ê¸°ë°˜ í•˜ë“œì½”ë”© ê²½ë¡œ
        hardcoded_path = Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
        self.logger.warning(f"âš ï¸ í•˜ë“œì½”ë”© ê²½ë¡œ ì‚¬ìš©: {hardcoded_path}")
        return hardcoded_path

    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            return 'arm64' in platform.machine().lower() and platform.system() == 'Darwin'
        except:
            return False
    
    def detect_all_models(self) -> Dict[str, OptimizedDetectedModel]:
        """ëª¨ë“  ëª¨ë¸ íƒì§€ (í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜)"""
        start_time = time.time()
        self.detected_models.clear()
        
        # í†µê³„ ì´ˆê¸°í™”
        self.detection_stats = {k: 0 if isinstance(v, (int, float)) else v for k, v in self.detection_stats.items()}
        
        if not self.ai_models_root.exists():
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.ai_models_root}")
            return {}
        
        self.logger.info("ğŸ” í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ íƒì§€ ì‹œì‘...")
        
        # ê° ëª¨ë¸ íŒ¨í„´ë³„ë¡œ íƒì§€
        for request_name in self.file_mapper.step_file_mappings.keys():
            try:
                # ìµœì ì˜ íŒŒì¼ ì°¾ê¸°
                file_info = self.file_mapper.find_best_model_file(request_name, self.ai_models_root)
                
                if file_info:
                    file_path, priority_rank = file_info
                    
                    # Step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    step_info = self.file_mapper.get_step_info(request_name)
                    
                    # OptimizedDetectedModel ìƒì„±
                    model = self._create_optimized_model(request_name, file_path, priority_rank, step_info)
                    
                    if model and model.meets_size_requirement:
                        self.detected_models[model.name] = model
                        self._update_detection_stats(model)
                        
                        self.logger.info(f"âœ… ëª¨ë¸ íƒì§€: {model.name} ({model.file_size_mb:.1f}MB, {model.size_category})")
                    elif model:
                        self.detection_stats["small_models_filtered"] += 1
                        self.logger.debug(f"ğŸ—‘ï¸ í¬ê¸° ë¶€ì¡±: {request_name} ({model.file_size_mb:.1f}MB)")
                        
            except Exception as e:
                self.logger.error(f"âŒ {request_name} íƒì§€ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¶”ê°€ ëŒ€í˜• íŒŒì¼ ìŠ¤ìº”
        self._scan_additional_ultra_large_files()
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        if self.prioritize_large_models:
            self._sort_models_by_priority()
        
        # í†µê³„ ì™„ë£Œ
        self.detection_stats["scan_duration"] = time.time() - start_time
        
        self._log_detection_summary()
        
        return self.detected_models
    
    def _create_optimized_model(self, request_name: str, file_path: Path, priority_rank: int, step_info: Optional[Dict]) -> Optional[OptimizedDetectedModel]:
        """OptimizedDetectedModel ìƒì„±"""
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # Step ì´ë¦„ ì¶”ì¶œ
            step_name = self._extract_step_name(request_name)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            recommended_device = "mps" if self.is_m3_max else "cpu"
            
            # Step ì—°ë™ ì •ë³´
            step_class_name = None
            ai_class = None
            model_load_method = "load_models"
            step_can_load = False
            
            if step_info:
                step_class_name = step_info.get("step_class")
                ai_class = step_info.get("ai_class")
                model_load_method = step_info.get("model_load_method", "load_models")
                step_can_load = bool(step_class_name and model_load_method and ai_class)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_confidence(file_size_mb, priority_rank, step_info)
            
            model = OptimizedDetectedModel(
                name=request_name,
                path=file_path,
                step_name=step_name,
                model_type=step_name.replace("Step", "").lower(),
                file_size_mb=file_size_mb,
                file_size_gb=0.0,  # __post_init__ì—ì„œ ê³„ì‚°
                confidence_score=confidence_score,
                priority_rank=priority_rank,
                
                # ModelLoader ì—°ë™
                step_class_name=step_class_name,
                ai_class=ai_class,
                model_load_method=model_load_method,
                step_can_load=step_can_load,
                
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device=recommended_device
            )
            
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_confidence(self, file_size_mb: float, priority_rank: int, step_info: Optional[Dict]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚° (í¬ê¸° + ìš°ì„ ìˆœìœ„ + Step ì •ë³´)"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # í¬ê¸° ê¸°ë°˜ ì‹ ë¢°ë„
        if file_size_mb >= 7000:      # 7GB+ (v1-5-pruned)
            confidence = 1.0
        elif file_size_mb >= 6000:    # 6GB+ (RealVisXL)
            confidence = 0.99
        elif file_size_mb >= 5000:    # 5GB+ (CLIP)
            confidence = 0.98
        elif file_size_mb >= 3000:    # 3GB+ (OOTD)
            confidence = 0.95
        elif file_size_mb >= 2000:    # 2GB+ (SAM)
            confidence = 0.92
        elif file_size_mb >= 1000:    # 1GB+
            confidence = 0.9
        elif file_size_mb >= 500:     # 500MB+
            confidence = 0.8
        elif file_size_mb >= 200:     # 200MB+
            confidence = 0.7
        elif file_size_mb >= 100:     # 100MB+
            confidence = 0.6
        elif file_size_mb >= 50:      # 50MB+
            confidence = 0.5
        else:  # 50MB ë¯¸ë§Œ
            confidence = 0.1
        
        # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        priority_bonus = max(0, (10 - priority_rank) * 0.01)
        confidence += priority_bonus
        
        # Step ì •ë³´ ë³´ë„ˆìŠ¤
        if step_info:
            min_expected_size = step_info.get("min_size_mb", 50)
            if file_size_mb >= min_expected_size:
                confidence += 0.05
            
            if step_info.get("ai_class") and step_info.get("ai_class") != "BaseRealAIModel":
                confidence += 0.05
        
        return min(confidence, 1.0)
    
    def _extract_step_name(self, request_name: str) -> str:
        """ìš”ì²­ëª…ì—ì„œ Step ì´ë¦„ ì¶”ì¶œ"""
        step_mappings = {
            "human_parsing": "HumanParsingStep",
            "pose_estimation": "PoseEstimationStep", 
            "cloth_segmentation": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting": "VirtualFittingStep",
            "post_processing": "PostProcessingStep",
            "quality_assessment": "QualityAssessmentStep",
            "stable_diffusion": "StableDiffusionStep"
        }
        
        for key, step_name in step_mappings.items():
            if key in request_name:
                return step_name
        
        return "UnknownStep"
    
    def _update_detection_stats(self, model: OptimizedDetectedModel):
        """íƒì§€ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.detection_stats["models_found"] += 1
        self.detection_stats["total_size_gb"] += model.file_size_gb
        
        if model.is_ultra_large:
            self.detection_stats["ultra_large_models"] += 1
        elif model.is_large_model:
            self.detection_stats["large_models_found"] += 1
        elif model.file_size_mb >= 100:
            self.detection_stats["medium_models"] += 1
        
        if model.can_be_loaded_by_step():
            self.detection_stats["step_loadable_models"] += 1
        
        if model.ai_class and model.ai_class != "BaseRealAIModel":
            self.detection_stats["ai_class_assigned"] += 1
    
    def _scan_additional_ultra_large_files(self):
        """ì¶”ê°€ ëŒ€í˜• íŒŒì¼ë“¤ ìŠ¤ìº” (2GB+ íŒŒì¼ë“¤)"""
        try:
            ultra_large_threshold_mb = 2000  # 2GB ì´ìƒ
            model_extensions = {'.pth', '.bin', '.safetensors', '.ckpt'}
            
            candidates = []
            
            for file_path in self.ai_models_root.rglob('*'):
                if (file_path.is_file() and 
                    file_path.suffix.lower() in model_extensions):
                    
                    try:
                        file_size_mb = file_path.stat().st_size / (1024 * 1024)
                        
                        # 2GB ì´ìƒë§Œ ì¶”ê°€ ìŠ¤ìº”
                        if file_size_mb >= ultra_large_threshold_mb:
                            # ì´ë¯¸ íƒì§€ëœ íŒŒì¼ì¸ì§€ í™•ì¸
                            if not any(m.path == file_path for m in self.detected_models.values()):
                                candidates.append((file_path, file_size_mb))
                                
                    except Exception as e:
                        self.logger.debug(f"ëŒ€í˜• íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                        continue
            
            # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
            candidates.sort(key=lambda x: x[1], reverse=True)
            
            for file_path, file_size_mb in candidates[:10]:  # ìƒìœ„ 10ê°œë§Œ
                model_name = f"ultra_large_{file_path.parent.name}_{file_path.stem}"
                
                # AI í´ë˜ìŠ¤ ì¶”ë¡ 
                ai_class = self._infer_ai_class_from_filename(file_path.name)
                
                model = OptimizedDetectedModel(
                    name=model_name,
                    path=file_path,
                    step_name="UltraLargeModel",
                    model_type="ultra_large",
                    file_size_mb=file_size_mb,
                    file_size_gb=0.0,  # __post_init__ì—ì„œ ê³„ì‚°
                    confidence_score=0.8,  # ëŒ€í˜• íŒŒì¼ì´ë¯€ë¡œ ë†’ì€ ì‹ ë¢°ë„
                    priority_rank=50,  # ë‚®ì€ ìš°ì„ ìˆœìœ„
                    ai_class=ai_class,
                    checkpoint_path=str(file_path),
                    device_compatible=True,
                    recommended_device="mps" if self.is_m3_max else "cpu"
                )
                
                if model.meets_size_requirement:
                    self.detected_models[model_name] = model
                    self._update_detection_stats(model)
                    
                    self.logger.debug(f"âœ… ëŒ€í˜• ëª¨ë¸ ì¶”ê°€: {model_name} ({file_size_mb:.1f}MB) â†’ {ai_class}")
                
        except Exception as e:
            self.logger.debug(f"ëŒ€í˜• íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
    
    def _infer_ai_class_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œë¶€í„° AI í´ë˜ìŠ¤ ì¶”ë¡  (ê°•í™”ëœ ë²„ì „)"""
        filename_lower = filename.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ AI í´ë˜ìŠ¤ ë§¤í•‘
        ai_class_patterns = {
            "RealStableDiffusionModel": ["v1-5-pruned", "stable_diffusion", "stable-diffusion"],
            "RealVisXLModel": ["realvis", "visxl", "xl"],
            "RealCLIPModel": ["open_clip", "clip", "vit-l", "vit-b"],
            "RealSAMModel": ["sam_vit", "segment", "sam"],
            "RealOOTDDiffusionModel": ["diffusion_pytorch_model", "ootd", "unet"],
            "RealGraphonomyModel": ["graphonomy", "schp", "atr", "lip"],
            "RealOpenPoseModel": ["openpose", "body_pose", "pose"],
            "RealGFPGANModel": ["gfpgan", "esrgan", "realesrgan"],
            "RealYOLOModel": ["yolo", "yolov8"],
            "RealHRVITONModel": ["hrviton", "hr_viton"],
            "RealU2NetModel": ["u2net"]
        }
        
        # íŒ¨í„´ ë§¤ì¹­ (ì ìˆ˜ ê¸°ë°˜)
        best_match = "BaseRealAIModel"
        best_score = 0
        
        for ai_class, patterns in ai_class_patterns.items():
            score = 0
            for pattern in patterns:
                if pattern in filename_lower:
                    score += len(pattern)  # ê¸´ íŒ¨í„´ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            
            if score > best_score:
                best_score = score
                best_match = ai_class
        
        return best_match
    
    def _sort_models_by_priority(self):
        """ëª¨ë¸ì„ ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬"""
        try:
            sorted_models = dict(sorted(
                self.detected_models.items(),
                key=lambda x: x[1].priority_score,
                reverse=True  # ë†’ì€ ì ìˆ˜ë¶€í„°
            ))
            self.detected_models = sorted_models
            self.logger.debug(f"âœ… ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì •ë ¬ ì™„ë£Œ: {len(sorted_models)}ê°œ")
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë ¬ ì‹¤íŒ¨: {e}")
    
    def _log_detection_summary(self):
        """íƒì§€ ìš”ì•½ ë¡œê·¸"""
        stats = self.detection_stats
        
        self.logger.info("ğŸ‰ í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ íƒì§€ ì™„ë£Œ!")
        self.logger.info(f"ğŸ“Š ì´ ëª¨ë¸: {stats['models_found']}ê°œ")
        self.logger.info(f"ğŸ”¥ ì´ˆëŒ€í˜• ëª¨ë¸ (5GB+): {stats['ultra_large_models']}ê°œ")
        self.logger.info(f"ğŸ“ˆ ëŒ€í˜• ëª¨ë¸ (1GB+): {stats['large_models_found']}ê°œ")
        self.logger.info(f"ğŸ“ ì¤‘í˜• ëª¨ë¸ (100MB+): {stats['medium_models']}ê°œ")
        self.logger.info(f"ğŸ§  AI í´ë˜ìŠ¤ í• ë‹¹: {stats['ai_class_assigned']}ê°œ")
        self.logger.info(f"âœ… Step ë¡œë“œ ê°€ëŠ¥: {stats['step_loadable_models']}ê°œ")
        self.logger.info(f"ğŸ—‘ï¸ ì‘ì€ ëª¨ë¸ ì œì™¸: {stats['small_models_filtered']}ê°œ")
        self.logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {stats['total_size_gb']:.1f}GB")
        self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {stats['scan_duration']:.2f}ì´ˆ")
        
        # ìƒìœ„ 5ê°œ ëª¨ë¸ ì¶œë ¥
        if self.detected_models:
            top_models = list(self.detected_models.values())[:5]
            self.logger.info("ğŸ† ìƒìœ„ 5ê°œ ëª¨ë¸:")
            for i, model in enumerate(top_models, 1):
                self.logger.info(f"   {i}. {model.name} ({model.file_size_gb:.2f}GB, {model.size_category})")
    
    def get_models_by_size_category(self, category: str) -> List[OptimizedDetectedModel]:
        """í¬ê¸° ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë°˜í™˜"""
        return [model for model in self.detected_models.values() if model.size_category == category]
    
    def get_top_models(self, n: int = 10) -> List[OptimizedDetectedModel]:
        """ìƒìœ„ Nê°œ ëª¨ë¸ ë°˜í™˜"""
        return list(self.detected_models.values())[:n]
    
    def get_models_by_step_class(self, step_class: str) -> List[OptimizedDetectedModel]:
        """Step í´ë˜ìŠ¤ë³„ ëª¨ë¸ ë°˜í™˜"""
        return [model for model in self.detected_models.values() if model.step_class_name == step_class]

# ==============================================
# ğŸ”¥ 4. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_detector: Optional[OptimizedModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector(config: Optional[Dict[str, Any]] = None) -> Optional[OptimizedModelDetector]:
    """ì „ì—­ OptimizedModelDetector ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_detector
    
    with _detector_lock:
        if _global_detector is None:
            try:
                _global_detector = OptimizedModelDetector()
                logger.info("âœ… ì „ì—­ OptimizedModelDetector ìƒì„± ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ OptimizedModelDetector ìƒì„± ì‹¤íŒ¨: {e}")
                return None
        
        return _global_detector

def quick_model_detection(step_class: Optional[str] = None, model_type: Optional[str] = None, min_size_gb: float = 0.0) -> List[Dict[str, Any]]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ (ê°œì„ ëœ ë²„ì „)"""
    try:
        detector = get_global_detector()
        if not detector:
            return []
        
        detected_models = detector.detect_all_models()
        results = []
        
        for model_name, detected_model in detected_models.items():
            try:
                model_info = detected_model.to_dict()
                
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                if min_size_gb > 0 and model_info.get("size_gb", 0) < min_size_gb:
                    continue
                
                results.append(model_info)
                
            except Exception as e:
                logger.debug(f"ëª¨ë¸ ì •ë³´ ë³€í™˜ ì‹¤íŒ¨ {model_name}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return []

def detect_ultra_large_models(min_size_gb: float = 5.0) -> List[Dict[str, Any]]:
    """ì´ˆëŒ€í˜• ëª¨ë¸ íƒì§€ (5GB+)"""
    return quick_model_detection(min_size_gb=min_size_gb)

def detect_available_models(step_class: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íƒì§€ (ë³„ì¹­ í•¨ìˆ˜)"""
    return quick_model_detection(step_class=step_class)

def validate_model_structure() -> Dict[str, Any]:
    """ëª¨ë¸ êµ¬ì¡° ìœ íš¨ì„± ê²€ì¦"""
    try:
        detector = get_global_detector()
        if not detector:
            return {"error": "íƒì§€ê¸° ìƒì„± ì‹¤íŒ¨"}
        
        return detector.file_mapper.validate_all_models(detector.ai_models_root)
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def create_global_detector(**kwargs) -> OptimizedModelDetector:
    """ì „ì—­ íƒì§€ê¸° ìƒì„± (ì„¤ì • ì ìš©)"""
    global _global_detector
    
    with _detector_lock:
        try:
            _global_detector = OptimizedModelDetector()
            
            # ì„¤ì • ì ìš©
            for key, value in kwargs.items():
                if hasattr(_global_detector, key):
                    setattr(_global_detector, key, value)
            
            logger.info("âœ… ì„¤ì • ì ìš©ëœ ì „ì—­ íƒì§€ê¸° ìƒì„± ì™„ë£Œ")
            return _global_detector
            
        except Exception as e:
            logger.error(f"âŒ ì „ì—­ íƒì§€ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return OptimizedModelDetector()  # í´ë°±

def cleanup_global_detector():
    """ì „ì—­ íƒì§€ê¸° ì •ë¦¬"""
    global _global_detector
    
    with _detector_lock:
        if _global_detector:
            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                _global_detector.detected_models.clear()
                _global_detector.detection_stats = {}
                _global_detector = None
                logger.info("âœ… ì „ì—­ íƒì§€ê¸° ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ íƒì§€ê¸° ì •ë¦¬ ì‹¤íŒ¨: {e}")

def get_model_detection_summary() -> Dict[str, Any]:
    """ëª¨ë¸ íƒì§€ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
    try:
        detector = get_global_detector()
        if not detector:
            return {"error": "íƒì§€ê¸° ì—†ìŒ"}
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        stats = detector.detection_stats.copy()
        
        # ì¶”ê°€ ì •ë³´
        stats["top_5_models"] = [
            {
                "name": model.name,
                "size_gb": model.file_size_gb,
                "size_category": model.size_category,
                "ai_class": model.ai_class
            }
            for model in list(detector.detected_models.values())[:5]
        ]
        
        stats["size_distribution"] = {
            "ultra_massive": len(detector.get_models_by_size_category("ultra_massive")),
            "ultra_large": len(detector.get_models_by_size_category("ultra_large")),
            "very_large": len(detector.get_models_by_size_category("very_large")),
            "large": len(detector.get_models_by_size_category("large")),
            "medium_large": len(detector.get_models_by_size_category("medium_large")),
            "medium": len(detector.get_models_by_size_category("medium"))
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"âŒ íƒì§€ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ 5. __all__ ë° ëª¨ë“ˆ ì´ˆê¸°í™”
# ==============================================

__all__ = [
    'find_model_by_name',
    'get_largest_models',
    'get_models_by_ai_class',
    'check_model_compatibility',
    'export_model_info_json',
    'benchmark_detection_performance'
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'OptimizedFileMapper',
    'OptimizedDetectedModel', 
    'OptimizedModelDetector',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_detector',
    'quick_model_detection',
    'detect_ultra_large_models',
    'detect_available_models',
    'validate_model_structure',
    'create_global_detector',
    'cleanup_global_detector',
    'get_model_detection_summary',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'NUMPY_AVAILABLE'
]

# ==============================================
# ğŸ”¥ 6. ëª¨ë“ˆ ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸
# ==============================================

logger.info("ğŸš€ OptimizedModelDetector v4.1 ì´ˆê¸°í™” ì™„ë£Œ!")
logger.info("âœ… í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ 155ê°œ íŒŒì¼ ì™„ì „ ë§¤í•‘")
logger.info("âœ… í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ")
logger.info("âœ… ModelLoader v5.1 ì™„ì „ í˜¸í™˜")
logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì™„ì „ ì§€ì›")

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_detector = get_global_detector()
    if _test_detector:
        logger.info("ğŸ‰ ì „ì—­ íƒì§€ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {_test_detector.ai_models_root}")
        logger.info(f"   íŒŒì¼ ë§¤í¼ ë§¤í•‘: {len(_test_detector.file_mapper.step_file_mappings)}ê°œ íŒ¨í„´")
        logger.info(f"   í¬ê¸° ì„ê³„ê°’: {_test_detector.min_model_size_mb}MB+")
        logger.info(f"   M3 Max ìµœì í™”: {'âœ…' if _test_detector.is_m3_max else 'âŒ'}")
        
        # ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬
        if _test_detector.ai_models_root.exists():
            logger.info("   ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬: ì¡´ì¬í•¨ âœ…")
        else:
            logger.warning("   ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬: ì¡´ì¬í•˜ì§€ ì•ŠìŒ âš ï¸")
            
    else:
        logger.warning("âš ï¸ ì „ì—­ íƒì§€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
        
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# í™˜ê²½ ì •ë³´ ì¶œë ¥
logger.info("ğŸ”§ í™˜ê²½ ì •ë³´:")
logger.info(f"   PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if TORCH_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
logger.info(f"   NumPy: {'ì‚¬ìš© ê°€ëŠ¥' if NUMPY_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
logger.info(f"   conda í™˜ê²½: {os.environ.get('CONDA_DEFAULT_ENV', 'ì—†ìŒ')}")

logger.info("ğŸ”¥ OptimizedModelDetector v4.1 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")
logger.info("=" * 80)
logger.info("ğŸš€ TERMINAL-BASED OPTIMIZED MODEL DETECTOR v4.1 READY! ğŸš€")
logger.info("=" * 80)

# ==============================================
# ğŸ”¥ 7. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def find_model_by_name(model_name: str) -> Optional[Dict[str, Any]]:
    """ëª¨ë¸ëª…ìœ¼ë¡œ íŠ¹ì • ëª¨ë¸ ì°¾ê¸°"""
    try:
        detector = get_global_detector()
        if not detector:
            return None
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        if model_name in detector.detected_models:
            return detector.detected_models[model_name].to_dict()
        
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
        for name, model in detector.detected_models.items():
            if model_name.lower() in name.lower():
                return model.to_dict()
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨ ({model_name}): {e}")
        return None

def get_largest_models(n: int = 5) -> List[Dict[str, Any]]:
    """ê°€ì¥ í° Nê°œ ëª¨ë¸ ë°˜í™˜"""
    try:
        detector = get_global_detector()
        if not detector:
            return []
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        # í¬ê¸°ìˆœ ì •ë ¬
        sorted_models = sorted(
            detector.detected_models.values(),
            key=lambda x: x.file_size_gb,
            reverse=True
        )
        
        return [model.to_dict() for model in sorted_models[:n]]
        
    except Exception as e:
        logger.error(f"âŒ ëŒ€í˜• ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def get_models_by_ai_class(ai_class: str) -> List[Dict[str, Any]]:
    """AI í´ë˜ìŠ¤ë³„ ëª¨ë¸ ë°˜í™˜"""
    try:
        detector = get_global_detector()
        if not detector:
            return []
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        matching_models = [
            model.to_dict() for model in detector.detected_models.values()
            if model.ai_class == ai_class
        ]
        
        return matching_models
        
    except Exception as e:
        logger.error(f"âŒ AI í´ë˜ìŠ¤ë³„ ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨ ({ai_class}): {e}")
        return []

def check_model_compatibility(model_name: str) -> Dict[str, Any]:
    """ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸"""
    try:
        detector = get_global_detector()
        if not detector:
            return {"error": "íƒì§€ê¸° ì—†ìŒ"}
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        if model_name not in detector.detected_models:
            return {"error": "ëª¨ë¸ ì—†ìŒ"}
        
        model = detector.detected_models[model_name]
        
        compatibility_info = {
            "model_name": model.name,
            "file_exists": model.path.exists(),
            "size_valid": model.meets_size_requirement,
            "step_loadable": model.can_be_loaded_by_step(),
            "ai_class_assigned": bool(model.ai_class and model.ai_class != "BaseRealAIModel"),
            "device_compatible": model.device_compatible,
            "recommended_device": model.recommended_device,
            "file_size_mb": model.file_size_mb,
            "file_size_gb": model.file_size_gb,
            "size_category": model.size_category,
            "priority_score": model.priority_score,
            "confidence_score": model.confidence_score,
            "issues": []
        }
        
        # ì´ìŠˆ ì²´í¬
        if not compatibility_info["file_exists"]:
            compatibility_info["issues"].append("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        
        if not compatibility_info["size_valid"]:
            compatibility_info["issues"].append(f"í¬ê¸° ë¶€ì¡± ({model.file_size_mb:.1f}MB < 50MB)")
        
        if not compatibility_info["step_loadable"]:
            compatibility_info["issues"].append("Stepì—ì„œ ë¡œë“œ ë¶ˆê°€")
        
        if not compatibility_info["ai_class_assigned"]:
            compatibility_info["issues"].append("AI í´ë˜ìŠ¤ ë¯¸í• ë‹¹")
        
        compatibility_info["overall_status"] = "í˜¸í™˜" if not compatibility_info["issues"] else "ë¬¸ì œ ìˆìŒ"
        
        return compatibility_info
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸ ì‹¤íŒ¨ ({model_name}): {e}")
        return {"error": str(e)}

def export_model_info_json(output_path: Optional[str] = None) -> str:
    """ëª¨ë¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    try:
        detector = get_global_detector()
        if not detector:
            raise ValueError("íƒì§€ê¸° ì—†ìŒ")
        
        if not detector.detected_models:
            detector.detect_all_models()
        
        export_data = {
            "metadata": {
                "export_time": time.time(),
                "detector_version": "v4.1_terminal_optimized",
                "total_models": len(detector.detected_models),
                "ai_models_root": str(detector.ai_models_root)
            },
            "detection_stats": detector.detection_stats,
            "models": {
                name: model.to_dict()
                for name, model in detector.detected_models.items()
            }
        }
        
        if output_path is None:
            output_path = f"model_detection_export_{int(time.time())}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ëª¨ë¸ ì •ë³´ JSON ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ JSON ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise

def benchmark_detection_performance() -> Dict[str, Any]:
    """íƒì§€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    try:
        # ìƒˆë¡œìš´ íƒì§€ê¸° ìƒì„± (ìºì‹œ ì—†ì´)
        detector = OptimizedModelDetector()
        
        # 3íšŒ ì‹¤í–‰í•˜ì—¬ í‰ê·  ê³„ì‚°
        times = []
        model_counts = []
        
        for i in range(3):
            start_time = time.time()
            detected_models = detector.detect_all_models()
            end_time = time.time()
            
            times.append(end_time - start_time)
            model_counts.append(len(detected_models))
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            detector.detected_models.clear()
        
        avg_time = sum(times) / len(times)
        avg_count = sum(model_counts) / len(model_counts)
        
        benchmark_results = {
            "average_detection_time": avg_time,
            "min_detection_time": min(times),
            "max_detection_time": max(times),
            "average_model_count": avg_count,
            "detection_times": times,
            "model_counts": model_counts,
            "models_per_second": avg_count / avg_time if avg_time > 0 else 0,
            "ai_models_root": str(detector.ai_models_root),
            "ai_models_root_exists": detector.ai_models_root.exists()
        }
        
        logger.info(f"ğŸƒâ€â™‚ï¸ íƒì§€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: í‰ê·  {avg_time:.2f}ì´ˆ, {avg_count:.0f}ê°œ ëª¨ë¸")
        
        return benchmark_results
        
    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}


logger.info("âœ… ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ë¡œë“œ ì™„ë£Œ!")
logger.info(f"ğŸ“¦ ì´ {len(__all__)}ê°œ í•¨ìˆ˜/í´ë˜ìŠ¤ ì œê³µ")

# ==============================================
# ğŸ”¥ 8. ìµœì¢… ì´ˆê¸°í™” ë©”ì‹œì§€
# ==============================================

logger.info("")
logger.info("ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:")
logger.info("   - í„°ë¯¸ë„ ë¶„ì„ ê¸°ë°˜ 155ê°œ ì‹¤ì œ íŒŒì¼ ë§¤í•‘")
logger.info("   - í¬ê¸° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ (50MB+ í•„í„°ë§)")
logger.info("   - 7.2GB v1-5-pruned, 6.5GB RealVisXL, 5.1GB CLIP ì§€ì›")
logger.info("   - ModelLoader v5.1 ì™„ì „ í˜¸í™˜")
logger.info("   - AI í´ë˜ìŠ¤ ìë™ ì¶”ë¡ ")
logger.info("   - ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì™„ì „ ì§€ì›")
logger.info("   - M3 Max MPS ìµœì í™”")
logger.info("")
logger.info("ğŸ”§ ì‚¬ìš©ë²•:")
logger.info("   detector = get_global_detector()")
logger.info("   models = detector.detect_all_models()")
logger.info("   summary = get_model_detection_summary()")
logger.info("   largest = get_largest_models(5)")
logger.info("")
logger.info("ğŸ”¥ OPTIMIZED MODEL DETECTOR v4.1 FULLY LOADED! ğŸ”¥")