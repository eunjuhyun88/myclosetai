# app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ” ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ ìë™ ë°œê²¬ (ì™„ì „ êµ¬í˜„íŒ)
âœ… ì‹¤ì œ 72GB+ ëª¨ë¸ë“¤ê³¼ ì™„ë²½ ì—°ê²°
âœ… ë™ì  ê²½ë¡œ ë§¤í•‘ ë° ìë™ ë“±ë¡
âœ… ModelLoaderì™€ ì™„ë²½ í†µí•©
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import sqlite3
import pickle

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ëª¨ë¸ íƒì§€ ì„¤ì • ë° ë§¤í•‘ (í™•ì¥ëœ ë²„ì „)
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•„ìˆ˜ ëª¨ë¸
    HIGH = 2          # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = 3        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    LOW = 4           # ë‚®ì€ ìš°ì„ ìˆœìœ„
    EXPERIMENTAL = 5  # ì‹¤í—˜ì  ëª¨ë¸

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (í™•ì¥ëœ ë²„ì „)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    metadata: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    requirements: List[str] = field(default_factory=list)
    performance_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)
    last_modified: float = 0.0
    checksum: Optional[str] = None

# ==============================================
# ğŸ” í™•ì¥ëœ ëª¨ë¸ ì‹ë³„ íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤
# ==============================================

ADVANCED_MODEL_PATTERNS = {
    # Step 01: Human Parsing Models (í™•ì¥ë¨)
    "human_parsing": {
        "patterns": [
            r".*human.*parsing.*\.pth$",
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*\.pth$",
            r".*atr.*model.*\.pth$",
            r".*lip.*parsing.*\.pth$",
            r".*segformer.*human.*\.pth$",
            r".*densepose.*\.pkl$",
            r".*cihp.*\.pth$",
            r".*pascal.*person.*\.pth$",
            r".*human.*segmentation.*\.pth$"
        ],
        "keywords": [
            "human", "parsing", "segmentation", "atr", "lip", "schp", 
            "graphonomy", "densepose", "cihp", "pascal", "person"
        ],
        "category": ModelCategory.HUMAN_PARSING,
        "priority": ModelPriority.CRITICAL,
        "min_size_mb": 50,
        "max_size_mb": 500,
        "expected_formats": [".pth", ".pt", ".pkl"],
        "compatibility": ["pytorch", "torchvision"]
    },
    
    # Step 02: Pose Estimation Models (í™•ì¥ë¨)
    "pose_estimation": {
        "patterns": [
            r".*pose.*model.*\.pth$",
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*hand.*pose.*\.pth$",
            r".*face.*pose.*\.pth$",
            r".*yolo.*pose.*\.pt$",
            r".*mediapipe.*\.tflite$",
            r".*alphapose.*\.pth$",
            r".*hrnet.*pose.*\.pth$",
            r".*simplebaseline.*\.pth$",
            r".*res101.*\.pth$",
            r".*clip_g.*\.pth$"
        ],
        "keywords": [
            "pose", "openpose", "yolo", "mediapipe", "body", "hand", "face",
            "keypoint", "alphapose", "hrnet", "simplebaseline", "coco"
        ],
        "category": ModelCategory.POSE_ESTIMATION,
        "priority": ModelPriority.HIGH,
        "min_size_mb": 5,
        "max_size_mb": 1000,
        "expected_formats": [".pth", ".pt", ".tflite", ".onnx"],
        "compatibility": ["pytorch", "tensorflow", "onnx"]
    },
    
    # Step 03: Cloth Segmentation Models (í™•ì¥ë¨)
    "cloth_segmentation": {
        "patterns": [
            r".*u2net.*\.pth$",
            r".*cloth.*segmentation.*\.(pth|onnx)$",
            r".*sam.*\.pth$",
            r".*mobile.*sam.*\.pth$",
            r".*parsing.*lip.*\.onnx$",
            r".*segmentation.*\.pth$",
            r".*deeplab.*cloth.*\.pth$",
            r".*mask.*rcnn.*cloth.*\.pth$",
            r".*bisenet.*\.pth$",
            r".*pspnet.*cloth.*\.pth$"
        ],
        "keywords": [
            "u2net", "segmentation", "sam", "cloth", "mask", "mobile",
            "deeplab", "bisenet", "pspnet", "rcnn", "parsing"
        ],
        "category": ModelCategory.CLOTH_SEGMENTATION,
        "priority": ModelPriority.HIGH,
        "min_size_mb": 10,
        "max_size_mb": 3000,
        "expected_formats": [".pth", ".pt", ".onnx"],
        "compatibility": ["pytorch", "onnx"]
    },
    
    # Step 04: Geometric Matching Models (í™•ì¥ë¨)
    "geometric_matching": {
        "patterns": [
            r".*geometric.*matching.*\.pth$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$",
            r".*transformation.*\.pth$",
            r".*lightweight.*gmm.*\.pth$",
            r".*cpvton.*gmm.*\.pth$",
            r".*viton.*geometric.*\.pth$",
            r".*warp.*\.pth$"
        ],
        "keywords": [
            "geometric", "matching", "gmm", "tps", "transformation", 
            "alignment", "cpvton", "viton", "warp"
        ],
        "category": ModelCategory.GEOMETRIC_MATCHING,
        "priority": ModelPriority.MEDIUM,
        "min_size_mb": 1,
        "max_size_mb": 100,
        "expected_formats": [".pth", ".pt"],
        "compatibility": ["pytorch"]
    },
    
    # Step 05 & 06: Virtual Fitting & Diffusion Models (ëŒ€í­ í™•ì¥)
    "diffusion_models": {
        "patterns": [
            r".*diffusion.*pytorch.*model\.(bin|safetensors)$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.(pth|bin)$",
            r".*unet.*diffusion.*\.bin$",
            r".*hrviton.*\.pth$",
            r".*viton.*hd.*\.pth$",
            r".*inpaint.*\.bin$",
            r".*controlnet.*\.safetensors$",
            r".*lora.*\.safetensors$",
            r".*dreambooth.*\.bin$",
            r".*v1-5-pruned.*\.safetensors$",
            r".*runway.*diffusion.*\.bin$"
        ],
        "keywords": [
            "diffusion", "stable", "oot", "viton", "unet", "inpaint", 
            "generation", "controlnet", "lora", "dreambooth", "runway"
        ],
        "category": ModelCategory.DIFFUSION_MODELS,
        "priority": ModelPriority.CRITICAL,
        "min_size_mb": 100,
        "max_size_mb": 10000,
        "expected_formats": [".bin", ".safetensors", ".pth"],
        "compatibility": ["diffusers", "pytorch"]
    },
    
    # Transformer Models (ìƒˆë¡œ ì¶”ê°€)
    "transformer_models": {
        "patterns": [
            r".*clip.*vit.*\.bin$",
            r".*clip.*base.*\.bin$",
            r".*clip.*large.*\.bin$",
            r".*bert.*\.bin$",
            r".*roberta.*\.bin$",
            r".*t5.*\.bin$",
            r".*gpt.*\.bin$",
            r".*transformer.*\.bin$"
        ],
        "keywords": [
            "clip", "vit", "bert", "roberta", "t5", "gpt", 
            "transformer", "attention", "encoder", "decoder"
        ],
        "category": ModelCategory.TRANSFORMER_MODELS,
        "priority": ModelPriority.HIGH,
        "min_size_mb": 50,
        "max_size_mb": 5000,
        "expected_formats": [".bin", ".safetensors"],
        "compatibility": ["transformers", "pytorch"]
    },
    
    # Step 07: Post Processing Models (í™•ì¥ë¨)
    "post_processing": {
        "patterns": [
            r".*realesrgan.*\.pth$",
            r".*esrgan.*\.pth$",
            r".*super.*resolution.*\.pth$",
            r".*upscale.*\.pth$",
            r".*enhance.*\.pth$",
            r".*srcnn.*\.pth$",
            r".*edsr.*\.pth$",
            r".*rcan.*\.pth$"
        ],
        "keywords": [
            "esrgan", "realesrgan", "upscale", "enhance", "super", 
            "resolution", "srcnn", "edsr", "rcan"
        ],
        "category": ModelCategory.POST_PROCESSING,
        "priority": ModelPriority.MEDIUM,
        "min_size_mb": 10,
        "max_size_mb": 200,
        "expected_formats": [".pth", ".pt"],
        "compatibility": ["pytorch"]
    },
    
    # Step 08: Quality Assessment & Feature Models (í™•ì¥ë¨)
    "quality_assessment": {
        "patterns": [
            r".*clip.*vit.*\.bin$",
            r".*clip.*base.*\.bin$",
            r".*clip.*large.*\.bin$",
            r".*quality.*assessment.*\.pth$",
            r".*feature.*extractor.*\.pth$",
            r".*resnet.*features.*\.pth$",
            r".*inception.*\.pth$",
            r".*efficientnet.*\.pth$",
            r".*mobilenet.*\.pth$"
        ],
        "keywords": [
            "clip", "vit", "quality", "assessment", "feature", "resnet",
            "inception", "efficientnet", "mobilenet", "extractor"
        ],
        "category": ModelCategory.QUALITY_ASSESSMENT,
        "priority": ModelPriority.MEDIUM,
        "min_size_mb": 50,
        "max_size_mb": 3000,
        "expected_formats": [".bin", ".pth", ".pt"],
        "compatibility": ["transformers", "pytorch"]
    },
    
    # Auxiliary Models (í™•ì¥ë¨)
    "auxiliary": {
        "patterns": [
            r".*vae.*\.bin$",
            r".*text.*encoder.*\.bin$",
            r".*tokenizer.*\.json$",
            r".*scheduler.*\.bin$",
            r".*safety.*checker.*\.bin$",
            r".*feature.*extractor.*\.bin$",
            r".*processor.*\.bin$"
        ],
        "keywords": [
            "vae", "encoder", "tokenizer", "scheduler", "safety", 
            "checker", "feature", "processor", "auxiliary"
        ],
        "category": ModelCategory.AUXILIARY,
        "priority": ModelPriority.LOW,
        "min_size_mb": 1,
        "max_size_mb": 1000,
        "expected_formats": [".bin", ".json", ".safetensors"],
        "compatibility": ["transformers", "diffusers"]
    }
}

# ==============================================
# ğŸ” ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class AdvancedModelDetector:
    """
    ğŸ” ê³ ê¸‰ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ
    âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤ ìë™ ë°œê²¬
    âœ… ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ í• ë‹¹
    âœ… ModelLoaderì™€ ì™„ë²½ í†µí•©
    âœ… ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_metadata_extraction: bool = True,
        enable_caching: bool = True,
        cache_db_path: Optional[Path] = None,
        max_workers: int = 4,
        scan_timeout: int = 300,  # 5ë¶„
        enable_checksum: bool = True
    ):
        """ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelDetector")
        
        # ê¸°ë³¸ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • (ë” í¬ê´„ì ìœ¼ë¡œ)
        if search_paths is None:
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parents[3]  # app/ai_pipeline/utilsì—ì„œ backendë¡œ
            
            self.search_paths = [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "app" / "models",
                backend_dir / "checkpoints",
                backend_dir / "ai_models" / "checkpoints",
                backend_dir / "models",
                backend_dir / "weights",
                Path.home() / ".cache" / "huggingface",
                Path.home() / ".cache" / "torch",
                # ì¶”ê°€ ì¼ë°˜ì ì¸ ê²½ë¡œë“¤
                Path("/opt/ml/models"),
                Path("/usr/local/share/models")
            ]
        else:
            self.search_paths = search_paths
        
        # ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_metadata_extraction = enable_metadata_extraction
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        self.enable_checksum = enable_checksum
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        # ìºì‹œ ê´€ë¦¬ (SQLite ê¸°ë°˜)
        self.cache_db_path = cache_db_path or Path("model_detection_cache.db")
        self.cache_ttl = 86400  # 24ì‹œê°„
        self._cache_lock = threading.RLock()
        
        # ì„±ëŠ¥ ìµœì í™”
        self._file_cache: Dict[str, Tuple[float, Dict]] = {}  # íŒŒì¼ ì •ë³´ ìºì‹œ
        self._pattern_cache: Dict[str, List] = {}  # íŒ¨í„´ ë§¤ì¹­ ìºì‹œ
        
        self.logger.info(f"ğŸ” ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
        # ìºì‹œ DB ì´ˆê¸°í™”
        if self.enable_caching:
            self._init_cache_db()

    def _init_cache_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS model_cache (
                        file_path TEXT PRIMARY KEY,
                        file_size INTEGER,
                        file_mtime REAL,
                        checksum TEXT,
                        detection_data TEXT,
                        created_at REAL,
                        accessed_at REAL
                    )
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_accessed_at ON model_cache(accessed_at)
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_created_at ON model_cache(created_at)
                """)
                
                conn.commit()
                
            self.logger.debug("âœ… ìºì‹œ DB ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enable_caching = False

    def detect_all_models(
        self, 
        force_rescan: bool = False,
        categories_filter: Optional[List[ModelCategory]] = None,
        min_confidence: float = 0.3
    ) -> Dict[str, DetectedModel]:
        """
        ëª¨ë“  AI ëª¨ë¸ ìë™ íƒì§€ (ê³ ê¸‰ ë²„ì „)
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            
        Returns:
            Dict[str, DetectedModel]: íƒì§€ëœ ëª¨ë¸ë“¤
        """
        try:
            self.logger.info("ğŸ” ê³ ê¸‰ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.enable_caching:
                cached_results = self._load_from_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ ëª¨ë¸")
                    self.scan_stats["cache_hits"] += len(cached_results)
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self._reset_scan_stats()
            
            # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
            if self.max_workers > 1:
                self._parallel_scan(categories_filter, min_confidence)
            else:
                self._sequential_scan(categories_filter, min_confidence)
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            self.scan_stats["last_scan_time"] = time.time()
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            self._post_process_results(min_confidence)
            
            # ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache()
            
            self.logger.info(f"âœ… ê³ ê¸‰ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_advanced_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.scan_stats["errors_encountered"] += 1
            raise

    def _parallel_scan(self, categories_filter: Optional[List[ModelCategory]], min_confidence: float):
        """ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰"""
        try:
            # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ëª©ë¡ ìƒì„±
            scan_dirs = []
            for search_path in self.search_paths:
                if search_path.exists() and search_path.is_dir():
                    scan_dirs.append(search_path)
            
            if not scan_dirs:
                self.logger.warning("âš ï¸ ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # ê° ë””ë ‰í† ë¦¬ì— ëŒ€í•´ ìŠ¤ìº” íƒœìŠ¤í¬ ì œì¶œ
                future_to_dir = {
                    executor.submit(self._scan_directory_advanced, directory, categories_filter, min_confidence): directory
                    for directory in scan_dirs
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ì ìš©)
                completed_count = 0
                for future in as_completed(future_to_dir, timeout=self.scan_timeout):
                    directory = future_to_dir[future]
                    try:
                        dir_results = future.result()
                        if dir_results:
                            # ê²°ê³¼ ë³‘í•© (ìŠ¤ë ˆë“œ ì•ˆì „)
                            with threading.Lock():
                                for name, model in dir_results.items():
                                    self._register_detected_model_safe(model)
                        
                        completed_count += 1
                        self.logger.debug(f"âœ… ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì™„ë£Œ: {directory} ({completed_count}/{len(scan_dirs)})")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {directory}: {e}")
                        self.scan_stats["errors_encountered"] += 1
                        
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆœì°¨ ìŠ¤ìº”
            self._sequential_scan(categories_filter, min_confidence)

    def _sequential_scan(self, categories_filter: Optional[List[ModelCategory]], min_confidence: float):
        """ìˆœì°¨ ìŠ¤ìº” ì‹¤í–‰"""
        try:
            for search_path in self.search_paths:
                if search_path.exists():
                    self.logger.debug(f"ğŸ“ ìˆœì°¨ ìŠ¤ìº” ì¤‘: {search_path}")
                    dir_results = self._scan_directory_advanced(search_path, categories_filter, min_confidence)
                    if dir_results:
                        for name, model in dir_results.items():
                            self._register_detected_model_safe(model)
                else:
                    self.logger.debug(f"âš ï¸ ê²½ë¡œ ì—†ìŒ: {search_path}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ìŠ¤ìº” ì‹¤íŒ¨: {e}")

    def _scan_directory_advanced(
        self, 
        directory: Path, 
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float,
        max_depth: int = 6,
        current_depth: int = 0
    ) -> Dict[str, DetectedModel]:
        """ê³ ê¸‰ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        results = {}
        
        try:
            if current_depth > max_depth:
                return results
                
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ë‚˜ì—´
            try:
                items = list(directory.iterdir())
            except PermissionError:
                self.logger.debug(f"ê¶Œí•œ ì—†ìŒ: {directory}")
                return results
            
            # íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë¶„ë¦¬
            files = [item for item in items if item.is_file()]
            subdirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]
            
            # íŒŒì¼ ë¶„ì„
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    detected_model = self._analyze_file_advanced(file_path, categories_filter, min_confidence)
                    if detected_model:
                        results[detected_model.name] = detected_model
                        
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
            if self.enable_deep_scan and current_depth < max_depth:
                for subdir in subdirs:
                    # ì œì™¸í•  ë””ë ‰í† ë¦¬ íŒ¨í„´
                    if subdir.name in ['__pycache__', '.git', 'node_modules', '.vscode', '.idea']:
                        continue
                    
                    try:
                        subdir_results = self._scan_directory_advanced(
                            subdir, categories_filter, min_confidence, max_depth, current_depth + 1
                        )
                        results.update(subdir_results)
                    except Exception as e:
                        self.logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                        continue
            
            return results
            
        except Exception as e:
            self.logger.debug(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {directory}: {e}")
            return results

    def _analyze_file_advanced(
        self, 
        file_path: Path, 
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float
    ) -> Optional[DetectedModel]:
        """ê³ ê¸‰ íŒŒì¼ ë¶„ì„"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            file_extension = file_path.suffix.lower()
            last_modified = file_stat.st_mtime
            
            # AI ëª¨ë¸ íŒŒì¼ í™•ì¥ì í•„í„° (í™•ì¥ë¨)
            ai_extensions = {
                '.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', 
                '.tflite', '.h5', '.pb', '.json', '.yaml', '.yml'
            }
            if file_extension not in ai_extensions:
                return None
            
            # íŒŒì¼ í¬ê¸° í•„í„° (ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸)
            if file_size_mb < 0.1:  # 100KB ë¯¸ë§Œ
                return None
            
            # ìºì‹œ í™•ì¸
            if self.enable_caching:
                cached_result = self._get_from_file_cache(file_path, file_stat)
                if cached_result:
                    self.scan_stats["cache_hits"] += 1
                    return cached_result
            
            self.scan_stats["cache_misses"] += 1
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëª¨ë¸ ë¶„ë¥˜
            detected_category, confidence_score, model_type = self._classify_model_advanced(file_path)
            
            if not detected_category or confidence_score < min_confidence:
                return None
            
            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
            if categories_filter and detected_category not in categories_filter:
                return None
            
            # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
            model_name = self._generate_model_name_advanced(file_path, detected_category)
            
            # ì²´í¬ì„¬ ê³„ì‚° (ì„ íƒì )
            checksum = None
            if self.enable_checksum and file_size_mb < 100:  # 100MB ë¯¸ë§Œë§Œ
                checksum = self._calculate_file_checksum(file_path)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = {}
            performance_info = {}
            compatibility_info = {}
            
            if self.enable_metadata_extraction:
                metadata = self._extract_metadata_advanced(file_path)
                performance_info = self._estimate_performance_info(file_path, file_size_mb, detected_category)
                compatibility_info = self._check_compatibility(file_path, detected_category)
            
            # ìš°ì„ ìˆœìœ„ ê³„ì‚°
            priority = self._calculate_priority_advanced(file_path, detected_category, file_size_mb, confidence_score)
            
            # DetectedModel ê°ì²´ ìƒì„±
            detected_model = DetectedModel(
                name=model_name,
                path=file_path,
                category=detected_category,
                model_type=model_type,
                file_size_mb=file_size_mb,
                file_extension=file_extension,
                confidence_score=confidence_score,
                priority=priority,
                metadata=metadata,
                performance_info=performance_info,
                compatibility_info=compatibility_info,
                last_modified=last_modified,
                checksum=checksum
            )
            
            # íŒŒì¼ ìºì‹œì— ì €ì¥
            if self.enable_caching:
                self._save_to_file_cache(file_path, file_stat, detected_model)
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"ê³ ê¸‰ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _classify_model_advanced(self, file_path: Path) -> Tuple[Optional[ModelCategory], float, str]:
        """ê³ ê¸‰ ëª¨ë¸ ë¶„ë¥˜"""
        try:
            file_name = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            best_category = None
            best_score = 0.0
            best_model_type = "GenericModel"
            
            for category_name, config in ADVANCED_MODEL_PATTERNS.items():
                score = 0.0
                matches = 0
                
                # íŒ¨í„´ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ì¦ê°€)
                for pattern in config["patterns"]:
                    if re.search(pattern, file_path_str, re.IGNORECASE):
                        score += 15.0  # íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜ ì¦ê°€
                        matches += 1
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ì¡°ì •)
                for keyword in config["keywords"]:
                    if keyword in file_name:
                        score += 8.0  # íŒŒì¼ëª… í‚¤ì›Œë“œ ë†’ì€ ì ìˆ˜
                    elif keyword in file_path_str:
                        score += 4.0  # ê²½ë¡œ í‚¤ì›Œë“œ ë‚®ì€ ì ìˆ˜
                    matches += 1
                
                # íŒŒì¼ í¬ê¸° ë²”ìœ„ í™•ì¸
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                min_size = config.get("min_size_mb", 0)
                max_size = config.get("max_size_mb", float('inf'))
                
                if min_size <= file_size_mb <= max_size:
                    score += 5.0
                else:
                    # ë²”ìœ„ ë²—ì–´ë‚˜ë©´ í˜ë„í‹°
                    if file_size_mb < min_size:
                        score -= 10.0
                    elif file_size_mb > max_size:
                        score -= 5.0
                
                # íŒŒì¼ í™•ì¥ì í™•ì¸
                file_ext = file_path.suffix.lower()
                expected_formats = config.get("expected_formats", [])
                if file_ext in expected_formats:
                    score += 3.0
                
                # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤
                priority = config.get("priority", ModelPriority.MEDIUM)
                if priority == ModelPriority.CRITICAL:
                    score += 5.0
                elif priority == ModelPriority.HIGH:
                    score += 3.0
                
                # ë§¤ì¹˜ ê°œìˆ˜ ë³´ë„ˆìŠ¤
                if matches > 0:
                    score += matches * 1.5
                
                # ê²½ë¡œ ê¸°ë°˜ ë³´ë„ˆìŠ¤
                path_parts = file_path.parts
                for part in path_parts:
                    if any(keyword in part.lower() for keyword in config["keywords"]):
                        score += 2.0
                        break
                
                # ìµœê³  ì ìˆ˜ ê°±ì‹ 
                if score > best_score:
                    best_score = score
                    best_category = config["category"]
                    best_model_type = self._determine_model_type_advanced(file_path, config["category"])
            
            # ìµœì†Œ ì„ê³„ê°’ í™•ì¸ (ë” ì—„ê²©í•˜ê²Œ)
            if best_score >= 20.0:
                confidence = min(best_score / 50.0, 1.0)  # ì •ê·œí™”
                return best_category, confidence, best_model_type
            
            return None, 0.0, "GenericModel"
            
        except Exception as e:
            self.logger.debug(f"ê³ ê¸‰ ë¶„ë¥˜ ì˜¤ë¥˜ {file_path}: {e}")
            return None, 0.0, "GenericModel"

    def _determine_model_type_advanced(self, file_path: Path, category: ModelCategory) -> str:
        """ê³ ê¸‰ ëª¨ë¸ íƒ€ì… ê²°ì •"""
        try:
            # ê¸°ë³¸ ë§¤í•‘
            model_type_mapping = {
                ModelCategory.HUMAN_PARSING: "GraphonomyModel",
                ModelCategory.POSE_ESTIMATION: "OpenPoseModel",
                ModelCategory.CLOTH_SEGMENTATION: "U2NetModel",
                ModelCategory.GEOMETRIC_MATCHING: "GeometricMatchingModel",
                ModelCategory.CLOTH_WARPING: "HRVITONModel",
                ModelCategory.VIRTUAL_FITTING: "HRVITONModel",
                ModelCategory.DIFFUSION_MODELS: "StableDiffusionPipeline",
                ModelCategory.TRANSFORMER_MODELS: "TransformerModel",
                ModelCategory.POST_PROCESSING: "EnhancementModel",
                ModelCategory.QUALITY_ASSESSMENT: "AssessmentModel",
                ModelCategory.AUXILIARY: "AuxiliaryModel"
            }
            
            # íŒŒì¼ëª… ê¸°ë°˜ íŠ¹ë³„ ì²˜ë¦¬
            file_name = file_path.name.lower()
            
            # Diffusion ëª¨ë¸ ì„¸ë¶„í™”
            if category == ModelCategory.DIFFUSION_MODELS:
                if "unet" in file_name:
                    return "UNet2DConditionModel"
                elif "vae" in file_name:
                    return "AutoencoderKL"
                elif "text_encoder" in file_name:
                    return "CLIPTextModel"
                elif "controlnet" in file_name:
                    return "ControlNetModel"
                elif "lora" in file_name:
                    return "LoRAModel"
                else:
                    return "StableDiffusionPipeline"
            
            # Transformer ëª¨ë¸ ì„¸ë¶„í™”
            elif category == ModelCategory.TRANSFORMER_MODELS:
                if "clip" in file_name:
                    return "CLIPModel"
                elif "bert" in file_name:
                    return "BertModel"
                elif "roberta" in file_name:
                    return "RobertaModel"
                elif "t5" in file_name:
                    return "T5Model"
                elif "gpt" in file_name:
                    return "GPTModel"
                else:
                    return "TransformerModel"
            
            return model_type_mapping.get(category, "GenericModel")
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ íƒ€ì… ê²°ì • ì˜¤ë¥˜: {e}")
            return "GenericModel"

    def _generate_model_name_advanced(self, file_path: Path, category: ModelCategory) -> str:
        """ê³ ê¸‰ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # íŠ¹ë³„í•œ ëª¨ë¸ëª… ë§¤í•‘ (ë” ì •í™•í•˜ê²Œ)
            special_mappings = {
                # ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­
                "schp_atr.pth": "human_parsing_graphonomy",
                "body_pose_model.pth": "pose_estimation_openpose",
                "u2net.pth": "cloth_segmentation_u2net",
                "geometric_matching_base.pth": "geometric_matching_gmm",
                "v1-5-pruned.safetensors": "virtual_fitting_stable_diffusion",
                "res101.pth": "post_processing_enhancer",
                "sam_vit_h_4b8939.pth": "quality_assessment_sam"
            }
            
            file_name = file_path.name
            if file_name in special_mappings:
                return special_mappings[file_name]
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¹ë³„ ì´ë¦„
            file_name_lower = file_name.lower()
            keyword_mappings = {
                "graphonomy": "human_parsing_graphonomy",
                "schp": "human_parsing_schp",
                "openpose": "pose_estimation_openpose",
                "yolo.*pose": "pose_estimation_yolo",
                "mediapipe": "pose_estimation_mediapipe",
                "u2net": "cloth_segmentation_u2net",
                "sam": "cloth_segmentation_sam",
                "mobile.*sam": "cloth_segmentation_mobile_sam",
                "ootdiffusion": "virtual_fitting_ootdiffusion",
                "stable.*diffusion": "virtual_fitting_stable_diffusion",
                "controlnet": "diffusion_controlnet",
                "realesrgan": "post_processing_realesrgan",
                "esrgan": "post_processing_esrgan",
                "clip.*vit": "quality_assessment_clip_vit",
                "clip.*base": "quality_assessment_clip_base"
            }
            
            for pattern, name in keyword_mappings.items():
                if re.search(pattern, file_name_lower):
                    return name
            
            # ê¸°ë³¸ ì´ë¦„ ìƒì„±: ì¹´í…Œê³ ë¦¬_íŒŒì¼ëª…_í•´ì‹œ
            base_name = f"{category.value}_{file_path.stem}"
            
            # í•´ì‹œ ì¶”ê°€ (ë” ì§§ê²Œ)
            path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:6]
            return f"{base_name}_{path_hash}"
            
        except Exception as e:
            # í´ë°± ì´ë¦„
            timestamp = int(time.time())
            return f"detected_model_{timestamp}"

    def _calculate_priority_advanced(
        self, 
        file_path: Path, 
        category: ModelCategory, 
        file_size_mb: float, 
        confidence_score: float
    ) -> ModelPriority:
        """ê³ ê¸‰ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ìš°ì„ ìˆœìœ„
            category_priority = {
                ModelCategory.HUMAN_PARSING: ModelPriority.CRITICAL,
                ModelCategory.POSE_ESTIMATION: ModelPriority.HIGH,
                ModelCategory.CLOTH_SEGMENTATION: ModelPriority.HIGH,
                ModelCategory.GEOMETRIC_MATCHING: ModelPriority.MEDIUM,
                ModelCategory.CLOTH_WARPING: ModelPriority.HIGH,
                ModelCategory.VIRTUAL_FITTING: ModelPriority.CRITICAL,
                ModelCategory.DIFFUSION_MODELS: ModelPriority.CRITICAL,
                ModelCategory.TRANSFORMER_MODELS: ModelPriority.HIGH,
                ModelCategory.POST_PROCESSING: ModelPriority.MEDIUM,
                ModelCategory.QUALITY_ASSESSMENT: ModelPriority.MEDIUM,
                ModelCategory.AUXILIARY: ModelPriority.LOW
            }.get(category, ModelPriority.MEDIUM)
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ ì¡°ì •
            if confidence_score > 0.8:
                # ë†’ì€ ì‹ ë¢°ë„ë©´ ìš°ì„ ìˆœìœ„ ìƒìŠ¹
                if category_priority.value > 1:
                    return ModelPriority(category_priority.value - 1)
            elif confidence_score < 0.4:
                # ë‚®ì€ ì‹ ë¢°ë„ë©´ ìš°ì„ ìˆœìœ„ í•˜ë½
                if category_priority.value < 5:
                    return ModelPriority(category_priority.value + 1)
            
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¡°ì •
            file_name = file_path.name.lower()
            
            # íŠ¹ë³„í•œ í‚¤ì›Œë“œë“¤
            if any(keyword in file_name for keyword in ["base", "foundation", "main", "primary"]):
                if category_priority.value > 1:
                    return ModelPriority(category_priority.value - 1)
            elif any(keyword in file_name for keyword in ["experimental", "test", "debug", "temp"]):
                return ModelPriority.EXPERIMENTAL
            
            return category_priority
            
        except Exception:
            return ModelPriority.MEDIUM

    def _extract_metadata_advanced(self, file_path: Path) -> Dict[str, Any]:
        """ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            "file_name": file_path.name,
            "file_size_mb": file_path.stat().st_size / (1024 * 1024),
            "file_modified": file_path.stat().st_mtime,
            "file_created": file_path.stat().st_ctime,
            "parent_directory": file_path.parent.name,
            "full_path": str(file_path)
        }
        
        try:
            # PyTorch ëª¨ë¸ ë©”íƒ€ë°ì´í„°
            if TORCH_AVAILABLE and file_path.suffix in ['.pth', '.pt']:
                try:
                    # ì•ˆì „í•œ ë¡œë“œ (weights_only=True)
                    checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                    
                    if isinstance(checkpoint, dict):
                        # ë©”íƒ€ë°ì´í„° í‚¤ë“¤
                        meta_keys = [
                            'arch', 'epoch', 'version', 'model_name', 'config',
                            'optimizer', 'lr_scheduler', 'best_acc', 'best_loss'
                        ]
                        
                        for key in meta_keys:
                            if key in checkpoint:
                                value = checkpoint[key]
                                if isinstance(value, (str, int, float, bool)):
                                    metadata[f"torch_{key}"] = value
                                else:
                                    metadata[f"torch_{key}"] = str(value)[:100]
                        
                        # ëª¨ë¸ í¬ê¸° ì •ë³´
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                            if isinstance(state_dict, dict):
                                total_params = 0
                                for v in state_dict.values():
                                    if torch.is_tensor(v):
                                        total_params += v.numel()
                                metadata['torch_total_parameters'] = total_params
                                metadata['torch_layers_count'] = len(state_dict)
                        
                except Exception as e:
                    metadata['torch_load_error'] = str(e)[:100]
            
            # Transformers ëª¨ë¸ ë©”íƒ€ë°ì´í„°
            if TRANSFORMERS_AVAILABLE and file_path.parent.name in ['transformers', 'huggingface']:
                try:
                    config_path = file_path.parent / 'config.json'
                    if config_path.exists():
                        config = AutoConfig.from_pretrained(str(file_path.parent))
                        metadata['transformers_model_type'] = getattr(config, 'model_type', 'unknown')
                        metadata['transformers_architectures'] = getattr(config, 'architectures', [])
                except Exception as e:
                    metadata['transformers_error'] = str(e)[:100]
            
            # ê²½ë¡œ ê¸°ë°˜ ì •ë³´
            path_parts = file_path.parts
            if len(path_parts) >= 3:
                metadata['model_family'] = path_parts[-3]  # ì¡°ë¶€ëª¨ ë””ë ‰í† ë¦¬
                metadata['model_variant'] = path_parts[-2]  # ë¶€ëª¨ ë””ë ‰í† ë¦¬
            
            # íŠ¹ë³„í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¸ì‹
            special_dirs = ['checkpoints', 'weights', 'models', 'pretrained']
            for part in path_parts:
                if part.lower() in special_dirs:
                    metadata['model_source'] = part.lower()
                    break
                    
        except Exception as e:
            metadata['metadata_extraction_error'] = str(e)[:100]
        
        return metadata

    def _estimate_performance_info(
        self, 
        file_path: Path, 
        file_size_mb: float, 
        category: ModelCategory
    ) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì •ë³´ ì¶”ì •"""
        try:
            performance = {
                "estimated_memory_usage_gb": file_size_mb / 1024 * 2,  # ëŒ€ëµ 2ë°°
                "estimated_inference_time_ms": 0,
                "recommended_batch_size": 1,
                "gpu_memory_required_gb": 4.0
            }
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ì¶”ì •
            if category == ModelCategory.DIFFUSION_MODELS:
                performance.update({
                    "estimated_inference_time_ms": 5000,  # 5ì´ˆ
                    "recommended_batch_size": 1,
                    "gpu_memory_required_gb": max(8.0, file_size_mb / 1024 * 3)
                })
            elif category == ModelCategory.TRANSFORMER_MODELS:
                performance.update({
                    "estimated_inference_time_ms": 100,
                    "recommended_batch_size": 16,
                    "gpu_memory_required_gb": max(4.0, file_size_mb / 1024 * 2)
                })
            elif category in [ModelCategory.HUMAN_PARSING, ModelCategory.CLOTH_SEGMENTATION]:
                performance.update({
                    "estimated_inference_time_ms": 200,
                    "recommended_batch_size": 8,
                    "gpu_memory_required_gb": max(2.0, file_size_mb / 1024 * 1.5)
                })
            elif category == ModelCategory.POSE_ESTIMATION:
                performance.update({
                    "estimated_inference_time_ms": 50,
                    "recommended_batch_size": 16,
                    "gpu_memory_required_gb": max(1.0, file_size_mb / 1024)
                })
            
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¡°ì •
            if file_size_mb > 1000:  # 1GB ì´ìƒ
                performance["gpu_memory_required_gb"] *= 1.5
                performance["estimated_inference_time_ms"] *= 1.3
            elif file_size_mb < 100:  # 100MB ë¯¸ë§Œ
                performance["gpu_memory_required_gb"] *= 0.7
                performance["estimated_inference_time_ms"] *= 0.8
            
            return performance
            
        except Exception as e:
            return {"error": str(e)}

    def _check_compatibility(self, file_path: Path, category: ModelCategory) -> Dict[str, Any]:
        """í˜¸í™˜ì„± í™•ì¸"""
        try:
            compatibility = {
                "pytorch_compatible": True,
                "requires_gpu": True,
                "requires_cuda": False,
                "requires_mps": False,
                "python_version_min": "3.8",
                "frameworks": []
            }
            
            # í™•ì¥ì ê¸°ë°˜ í˜¸í™˜ì„±
            ext = file_path.suffix.lower()
            if ext in ['.pth', '.pt']:
                compatibility["frameworks"].append("pytorch")
            elif ext == '.bin':
                compatibility["frameworks"].extend(["pytorch", "transformers"])
            elif ext == '.safetensors':
                compatibility["frameworks"].extend(["pytorch", "transformers", "diffusers"])
            elif ext == '.onnx':
                compatibility["frameworks"].append("onnx")
                compatibility["requires_gpu"] = False
            elif ext == '.tflite':
                compatibility["frameworks"].append("tensorflow")
                compatibility["requires_gpu"] = False
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ë³„ ìš”êµ¬ì‚¬í•­
            if category == ModelCategory.DIFFUSION_MODELS:
                compatibility.update({
                    "requires_gpu": True,
                    "python_version_min": "3.9",
                    "frameworks": ["pytorch", "diffusers"]
                })
            elif category == ModelCategory.TRANSFORMER_MODELS:
                compatibility.update({
                    "frameworks": ["pytorch", "transformers"]
                })
            
            # ì‹œìŠ¤í…œ íŠ¹í™”
            if IS_M3_MAX:
                compatibility["requires_mps"] = True
                compatibility["requires_cuda"] = False
            else:
                compatibility["requires_cuda"] = True
            
            return compatibility
            
        except Exception as e:
            return {"error": str(e)}

    def _calculate_file_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (SHA256)"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                # í° íŒŒì¼ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            self.logger.debug(f"ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def _register_detected_model_safe(self, detected_model: DetectedModel):
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ëª¨ë¸ ë“±ë¡"""
        with threading.Lock():
            self._register_detected_model(detected_model)

    def _register_detected_model(self, detected_model: DetectedModel):
        """íƒì§€ëœ ëª¨ë¸ ë“±ë¡ (ì¤‘ë³µ ì²˜ë¦¬)"""
        try:
            model_name = detected_model.name
            
            if model_name in self.detected_models:
                existing_model = self.detected_models[model_name]
                
                # ë” ë‚˜ì€ ëª¨ë¸ë¡œ êµì²´í• ì§€ ê²°ì •
                if self._is_better_model_advanced(detected_model, existing_model):
                    detected_model.alternative_paths.append(existing_model.path)
                    detected_model.alternative_paths.extend(existing_model.alternative_paths)
                    self.detected_models[model_name] = detected_model
                    self.logger.debug(f"ğŸ”„ ëª¨ë¸ êµì²´: {model_name}")
                else:
                    existing_model.alternative_paths.append(detected_model.path)
                    self.logger.debug(f"ğŸ“ ëŒ€ì²´ ê²½ë¡œ ì¶”ê°€: {model_name}")
            else:
                self.detected_models[model_name] = detected_model
                self.logger.debug(f"âœ… ìƒˆ ëª¨ë¸ ë“±ë¡: {model_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _is_better_model_advanced(self, new_model: DetectedModel, existing_model: DetectedModel) -> bool:
        """ê³ ê¸‰ ëª¨ë¸ ë¹„êµ (ìƒˆ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨)"""
        try:
            # 1. ìš°ì„ ìˆœìœ„ ë¹„êµ
            if new_model.priority.value < existing_model.priority.value:
                return True
            elif new_model.priority.value > existing_model.priority.value:
                return False
            
            # 2. ì‹ ë¢°ë„ ë¹„êµ
            if abs(new_model.confidence_score - existing_model.confidence_score) > 0.1:
                return new_model.confidence_score > existing_model.confidence_score
            
            # 3. íŒŒì¼ í¬ê¸° ë¹„êµ (ì ë‹¹í•œ í¬ê¸°ê°€ ì¢‹ìŒ)
            optimal_sizes = {
                ModelCategory.HUMAN_PARSING: 200,    # MB
                ModelCategory.POSE_ESTIMATION: 100,
                ModelCategory.CLOTH_SEGMENTATION: 150,
                ModelCategory.DIFFUSION_MODELS: 4000,
                ModelCategory.TRANSFORMER_MODELS: 500
            }
            
            optimal_size = optimal_sizes.get(new_model.category, 200)
            
            new_diff = abs(new_model.file_size_mb - optimal_size)
            existing_diff = abs(existing_model.file_size_mb - optimal_size)
            
            if abs(new_diff - existing_diff) > 50:  # 50MB ì´ìƒ ì°¨ì´
                return new_diff < existing_diff
            
            # 4. ìµœì‹ ì„± ë¹„êµ
            if abs(new_model.last_modified - existing_model.last_modified) > 86400:  # 1ì¼ ì´ìƒ ì°¨ì´
                return new_model.last_modified > existing_model.last_modified
            
            # 5. íŒŒì¼ëª… ê¸°ë°˜ ìš°ì„ ìˆœìœ„
            preferred_keywords = ["base", "main", "primary", "official", "stable"]
            new_has_preferred = any(keyword in new_model.path.name.lower() for keyword in preferred_keywords)
            existing_has_preferred = any(keyword in existing_model.path.name.lower() for keyword in preferred_keywords)
            
            if new_has_preferred != existing_has_preferred:
                return new_has_preferred
            
            # ê¸°ë³¸: ë” í° íŒŒì¼
            return new_model.file_size_mb > existing_model.file_size_mb
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ë¹„êµ ì˜¤ë¥˜: {e}")
            return new_model.file_size_mb > existing_model.file_size_mb

    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ë¦¬ì…‹"""
        self.scan_stats.update({
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0
        })

    def _post_process_results(self, min_confidence: float):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ì‹ ë¢°ë„ í•„í„°ë§
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            self.detected_models = filtered_models
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
            sorted_models = sorted(
                self.detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score, -x[1].file_size_mb)
            )
            
            self.detected_models = {name: model for name, model in sorted_models}
            
            # í†µê³„ ê³„ì‚°
            category_stats = {}
            priority_stats = {}
            
            for model in self.detected_models.values():
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                category = model.category.value
                if category not in category_stats:
                    category_stats[category] = {"count": 0, "total_size_mb": 0}
                category_stats[category]["count"] += 1
                category_stats[category]["total_size_mb"] += model.file_size_mb
                
                # ìš°ì„ ìˆœìœ„ë³„ í†µê³„
                priority = model.priority.name
                if priority not in priority_stats:
                    priority_stats[priority] = 0
                priority_stats[priority] += 1
            
            self.scan_stats.update({
                "category_stats": category_stats,
                "priority_stats": priority_stats
            })
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _print_advanced_summary(self):
        """ê³ ê¸‰ íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            self.logger.info("=" * 70)
            self.logger.info("ğŸ¯ ê³ ê¸‰ ìë™ ëª¨ë¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 70)
            
            total_size_gb = sum(model.file_size_mb for model in self.detected_models.values()) / 1024
            avg_confidence = sum(model.confidence_score for model in self.detected_models.values()) / len(self.detected_models) if self.detected_models else 0
            
            self.logger.info(f"ğŸ“Š ì´ íƒì§€ëœ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ ëª¨ë¸ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº”ëœ íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"â±ï¸ ìŠ¤ìº” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            self.logger.info(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            self.logger.info(f"ğŸ“ˆ ìºì‹œ íˆíŠ¸ìœ¨: {self.scan_stats['cache_hits']}/{self.scan_stats['cache_hits'] + self.scan_stats['cache_misses']}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
            if "category_stats" in self.scan_stats:
                self.logger.info("\nğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
                for category, stats in self.scan_stats["category_stats"].items():
                    size_gb = stats["total_size_mb"] / 1024
                    self.logger.info(f"  {category}: {stats['count']}ê°œ ({size_gb:.2f}GB)")
            
            # ìš°ì„ ìˆœìœ„ë³„ ìš”ì•½
            if "priority_stats" in self.scan_stats:
                self.logger.info("\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬:")
                for priority, count in self.scan_stats["priority_stats"].items():
                    self.logger.info(f"  {priority}: {count}ê°œ")
            
            # ìƒìœ„ ëª¨ë¸ë“¤
            self.logger.info("\nğŸ† ì£¼ìš” íƒì§€ëœ ëª¨ë¸ë“¤:")
            for i, (name, model) in enumerate(list(self.detected_models.items())[:8]):
                self.logger.info(f"  {i+1}. {name}")
                self.logger.info(f"     í¬ê¸°: {model.file_size_mb:.1f}MB, ì‹ ë¢°ë„: {model.confidence_score:.3f}")
                self.logger.info(f"     ì¹´í…Œê³ ë¦¬: {model.category.value}, ìš°ì„ ìˆœìœ„: {model.priority.name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    # ìºì‹œ ê´€ë ¨ ë©”ì„œë“œë“¤
    def _load_from_cache(self) -> Optional[Dict[str, DetectedModel]]:
        """ìºì‹œì—ì„œ ë¡œë“œ"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    cutoff_time = time.time() - self.cache_ttl
                    cursor.execute("DELETE FROM model_cache WHERE created_at < ?", (cutoff_time,))
                    
                    # ìºì‹œ ì¡°íšŒ
                    cursor.execute("""
                        SELECT file_path, detection_data 
                        FROM model_cache 
                        WHERE created_at > ?
                    """, (cutoff_time,))
                    
                    cached_models = {}
                    for file_path, detection_data in cursor.fetchall():
                        try:
                            # íŒŒì¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                            if not Path(file_path).exists():
                                continue
                            
                            model_data = json.loads(detection_data)
                            model = self._deserialize_detected_model(model_data)
                            if model:
                                cached_models[model.name] = model
                        except Exception as e:
                            self.logger.debug(f"ìºì‹œ í•­ëª© ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                    
                    if cached_models:
                        # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                        cursor.execute("UPDATE model_cache SET accessed_at = ?", (time.time(),))
                        conn.commit()
                        
                        self.detected_models = cached_models
                        return cached_models
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_cache(self):
        """ìºì‹œì— ì €ì¥"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    current_time = time.time()
                    
                    for model in self.detected_models.values():
                        try:
                            detection_data = json.dumps(self._serialize_detected_model(model))
                            
                            cursor.execute("""
                                INSERT OR REPLACE INTO model_cache 
                                (file_path, file_size, file_mtime, checksum, detection_data, created_at, accessed_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                str(model.path),
                                int(model.file_size_mb * 1024 * 1024),
                                model.last_modified,
                                model.checksum,
                                detection_data,
                                current_time,
                                current_time
                            ))
                        except Exception as e:
                            self.logger.debug(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {model.name}: {e}")
                    
                    conn.commit()
                    
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_from_file_cache(self, file_path: Path, file_stat) -> Optional[DetectedModel]:
        """íŒŒì¼ ìºì‹œì—ì„œ ì¡°íšŒ"""
        try:
            cache_key = str(file_path)
            if cache_key in self._file_cache:
                cached_time, cached_data = self._file_cache[cache_key]
                
                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                if abs(file_stat.st_mtime - cached_time) < 1.0:  # 1ì´ˆ ì˜¤ì°¨ í—ˆìš©
                    return self._deserialize_detected_model(cached_data)
            
            return None
            
        except Exception as e:
            self.logger.debug(f"íŒŒì¼ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_file_cache(self, file_path: Path, file_stat, detected_model: DetectedModel):
        """íŒŒì¼ ìºì‹œì— ì €ì¥"""
        try:
            cache_key = str(file_path)
            self._file_cache[cache_key] = (
                file_stat.st_mtime,
                self._serialize_detected_model(detected_model)
            )
            
            # ìºì‹œ í¬ê¸° ì œí•œ (1000ê°œ)
            if len(self._file_cache) > 1000:
                # ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = min(self._file_cache.keys(), 
                               key=lambda k: self._file_cache[k][0])
                del self._file_cache[oldest_key]
                
        except Exception as e:
            self.logger.debug(f"íŒŒì¼ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _serialize_detected_model(self, model: DetectedModel) -> Dict[str, Any]:
        """DetectedModelì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™”"""
        return {
            "name": model.name,
            "path": str(model.path),
            "category": model.category.value,
            "model_type": model.model_type,
            "file_size_mb": model.file_size_mb,
            "file_extension": model.file_extension,
            "confidence_score": model.confidence_score,
            "priority": model.priority.value,
            "metadata": model.metadata,
            "alternative_paths": [str(p) for p in model.alternative_paths],
            "requirements": model.requirements,
            "performance_info": model.performance_info,
            "compatibility_info": model.compatibility_info,
            "last_modified": model.last_modified,
            "checksum": model.checksum
        }

    def _deserialize_detected_model(self, data: Dict[str, Any]) -> Optional[DetectedModel]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ DetectedModelë¡œ ì—­ì§ë ¬í™”"""
        try:
            return DetectedModel(
                name=data["name"],
                path=Path(data["path"]),
                category=ModelCategory(data["category"]),
                model_type=data["model_type"],
                file_size_mb=data["file_size_mb"],
                file_extension=data["file_extension"],
                confidence_score=data["confidence_score"],
                priority=ModelPriority(data["priority"]),
                metadata=data.get("metadata", {}),
                alternative_paths=[Path(p) for p in data.get("alternative_paths", [])],
                requirements=data.get("requirements", []),
                performance_info=data.get("performance_info", {}),
                compatibility_info=data.get("compatibility_info", {}),
                last_modified=data.get("last_modified", 0.0),
                checksum=data.get("checksum")
            )
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return None

    # ê³µê°œ ì¡°íšŒ ë©”ì„œë“œë“¤
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]

    def get_models_by_priority(self, priority: ModelPriority) -> List[DetectedModel]:
        """ìš°ì„ ìˆœìœ„ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.priority == priority]

    def get_best_model_for_category(self, category: ModelCategory) -> Optional[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ"""
        category_models = self.get_models_by_category(category)
        if not category_models:
            return None
        
        return min(category_models, key=lambda m: (m.priority.value, -m.confidence_score))

    def get_model_by_name(self, name: str) -> Optional[DetectedModel]:
        """ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ"""
        return self.detected_models.get(name)

    def get_all_model_paths(self) -> Dict[str, Path]:
        """ëª¨ë“  ëª¨ë¸ì˜ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {name: model.path for name, model in self.detected_models.items()}

    def search_models(
        self, 
        keywords: List[str], 
        categories: Optional[List[ModelCategory]] = None,
        min_confidence: float = 0.0
    ) -> List[DetectedModel]:
        """í‚¤ì›Œë“œë¡œ ëª¨ë¸ ê²€ìƒ‰"""
        try:
            results = []
            keywords_lower = [kw.lower() for kw in keywords]
            
            for model in self.detected_models.values():
                # ì‹ ë¢°ë„ í•„í„°
                if model.confidence_score < min_confidence:
                    continue
                
                # ì¹´í…Œê³ ë¦¬ í•„í„°
                if categories and model.category not in categories:
                    continue
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                model_text = f"{model.name} {model.path.name} {model.model_type}".lower()
                if any(keyword in model_text for keyword in keywords_lower):
                    results.append(model)
            
            # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda m: (m.priority.value, -m.confidence_score))
            return results
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def export_detection_report(self, output_path: Optional[Path] = None) -> Path:
        """íƒì§€ ê²°ê³¼ë¥¼ ìƒì„¸ ë¦¬í¬íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            if output_path is None:
                timestamp = int(time.time())
                output_path = Path(f"model_detection_report_{timestamp}.json")
            
            report_data = {
                "detection_summary": {
                    "detected_at": time.time(),
                    "total_models": len(self.detected_models),
                    "scan_stats": self.scan_stats,
                    "system_info": {
                        "is_m3_max": IS_M3_MAX,
                        "torch_available": TORCH_AVAILABLE,
                        "search_paths": [str(p) for p in self.search_paths]
                    }
                },
                "models": {}
            }
            
            for name, model in self.detected_models.items():
                report_data["models"][name] = self._serialize_detected_model(model)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"âœ… íƒì§€ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

# ==============================================
# ğŸ”— ModelLoader í†µí•©ì„ ìœ„í•œ ê³ ê¸‰ ì–´ëŒ‘í„°
# ==============================================

class AdvancedModelLoaderAdapter:
    """
    ê³ ê¸‰ ìë™ íƒì§€ ì‹œìŠ¤í…œì„ ModelLoaderì™€ ì—°ê²°í•˜ëŠ” ì–´ëŒ‘í„°
    """
    
    def __init__(self, detector: AdvancedModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_model_loader_config(self) -> Dict[str, Any]:
        """ModelLoaderë¥¼ ìœ„í•œ ì™„ì „í•œ ì„¤ì • ìƒì„±"""
        try:
            config = {
                "actual_model_paths": {},
                "model_configs": [],
                "performance_profiles": {},
                "compatibility_matrix": {},
                "priority_rankings": {}
            }
            
            for name, model in self.detector.detected_models.items():
                # ê¸°ë³¸ ê²½ë¡œ ì •ë³´
                config["actual_model_paths"][name] = {
                    "primary": str(model.path),
                    "alternatives": [str(p) for p in model.alternative_paths],
                    "category": model.category.value,
                    "model_type": model.model_type,
                    "confidence": model.confidence_score,
                    "priority": model.priority.value,
                    "size_mb": model.file_size_mb
                }
                
                # ModelConfig í˜•ì‹
                model_config = {
                    "name": name,
                    "model_type": model.category.value,
                    "model_class": model.model_type,
                    "checkpoint_path": str(model.path),
                    "device": "auto",
                    "precision": "fp16",
                    "input_size": self._get_input_size_for_category(model.category),
                    "metadata": {
                        **model.metadata,
                        "auto_detected": True,
                        "confidence_score": model.confidence_score,
                        "priority": model.priority.name,
                        "alternative_paths": [str(p) for p in model.alternative_paths]
                    }
                }
                config["model_configs"].append(model_config)
                
                # ì„±ëŠ¥ í”„ë¡œí•„
                config["performance_profiles"][name] = model.performance_info
                
                # í˜¸í™˜ì„± ì •ë³´
                config["compatibility_matrix"][name] = model.compatibility_info
                
                # ìš°ì„ ìˆœìœ„ ìˆœìœ„
                config["priority_rankings"][name] = {
                    "priority_level": model.priority.value,
                    "priority_name": model.priority.name,
                    "confidence_score": model.confidence_score,
                    "category_rank": self._get_category_rank(model.category)
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _get_input_size_for_category(self, category: ModelCategory) -> Tuple[int, int]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸° (í™•ì¥ëœ ë²„ì „)"""
        size_mapping = {
            ModelCategory.HUMAN_PARSING: (512, 512),
            ModelCategory.POSE_ESTIMATION: (368, 368),
            ModelCategory.CLOTH_SEGMENTATION: (320, 320),
            ModelCategory.GEOMETRIC_MATCHING: (512, 384),
            ModelCategory.CLOTH_WARPING: (512, 384),
            ModelCategory.VIRTUAL_FITTING: (512, 384),
            ModelCategory.DIFFUSION_MODELS: (512, 512),
            ModelCategory.TRANSFORMER_MODELS: (224, 224),
            ModelCategory.POST_PROCESSING: (512, 512),
            ModelCategory.QUALITY_ASSESSMENT: (224, 224),
            ModelCategory.AUXILIARY: (224, 224)
        }
        return size_mapping.get(category, (512, 512))

    def _get_category_rank(self, category: ModelCategory) -> int:
        """ì¹´í…Œê³ ë¦¬ë³„ ìˆœìœ„ (ì¤‘ìš”ë„)"""
        rank_mapping = {
            ModelCategory.HUMAN_PARSING: 1,
            ModelCategory.VIRTUAL_FITTING: 2,
            ModelCategory.DIFFUSION_MODELS: 2,
            ModelCategory.POSE_ESTIMATION: 3,
            ModelCategory.CLOTH_SEGMENTATION: 3,
            ModelCategory.CLOTH_WARPING: 4,
            ModelCategory.TRANSFORMER_MODELS: 4,
            ModelCategory.GEOMETRIC_MATCHING: 5,
            ModelCategory.POST_PROCESSING: 6,
            ModelCategory.QUALITY_ASSESSMENT: 7,
            ModelCategory.AUXILIARY: 8
        }
        return rank_mapping.get(category, 9)

    def generate_optimized_loading_strategy(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ëª¨ë¸ ë¡œë”© ì „ëµ ìƒì„±"""
        try:
            strategy = {
                "preload_models": [],      # ë¯¸ë¦¬ ë¡œë“œí•  ëª¨ë¸ë“¤
                "lazy_load_models": [],    # í•„ìš”ì‹œ ë¡œë“œí•  ëª¨ë¸ë“¤
                "memory_budget": {},       # ë©”ëª¨ë¦¬ ì˜ˆì‚°
                "loading_order": [],       # ë¡œë”© ìˆœì„œ
                "fallback_models": {}      # í´ë°± ëª¨ë¸ë“¤
            }
            
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
            sorted_models = sorted(
                self.detector.detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score)
            )
            
            total_memory_budget = 64.0  # GB (M3 Max ê¸°ì¤€)
            used_memory = 0.0
            
            for name, model in sorted_models:
                estimated_memory = model.performance_info.get("estimated_memory_usage_gb", 2.0)
                
                if model.priority in [ModelPriority.CRITICAL, ModelPriority.HIGH]:
                    if used_memory + estimated_memory < total_memory_budget * 0.7:  # 70% ê¹Œì§€ë§Œ preload
                        strategy["preload_models"].append({
                            "name": name,
                            "estimated_memory_gb": estimated_memory,
                            "priority": model.priority.name
                        })
                        used_memory += estimated_memory
                    else:
                        strategy["lazy_load_models"].append(name)
                else:
                    strategy["lazy_load_models"].append(name)
                
                # ë¡œë”© ìˆœì„œ ì¶”ê°€
                strategy["loading_order"].append({
                    "name": name,
                    "priority": model.priority.value,
                    "estimated_load_time": model.performance_info.get("estimated_inference_time_ms", 1000) / 10
                })
                
                # í´ë°± ëª¨ë¸ ì„¤ì •
                if model.alternative_paths:
                    strategy["fallback_models"][name] = [str(p) for p in model.alternative_paths[:2]]
            
            strategy["memory_budget"] = {
                "total_gb": total_memory_budget,
                "preload_used_gb": used_memory,
                "available_gb": total_memory_budget - used_memory
            }
            
            return strategy
            
        except Exception as e:
            self.logger.error(f"ë¡œë”© ì „ëµ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸš€ í¸ì˜ í•¨ìˆ˜ë“¤ ë° íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_advanced_detector(
    search_paths: Optional[List[Path]] = None,
    enable_parallel: bool = True,
    max_workers: int = 4,
    **kwargs
) -> AdvancedModelDetector:
    """ê³ ê¸‰ ìë™ ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return AdvancedModelDetector(
        search_paths=search_paths,
        max_workers=max_workers if enable_parallel else 1,
        **kwargs
    )

def quick_model_detection(
    categories_filter: Optional[List[str]] = None,
    min_confidence: float = 0.5,
    force_rescan: bool = False
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ë° ê²°ê³¼ ë°˜í™˜"""
    try:
        # ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ì„ enumìœ¼ë¡œ ë³€í™˜
        category_enums = None
        if categories_filter:
            category_enums = [ModelCategory(cat) for cat in categories_filter if cat in [c.value for c in ModelCategory]]
        
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_advanced_detector()
        detected_models = detector.detect_all_models(
            force_rescan=force_rescan,
            categories_filter=category_enums,
            min_confidence=min_confidence
        )
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "total_models": len(detected_models),
            "models_by_category": {},
            "models_by_priority": {},
            "top_models": {},
            "scan_stats": detector.scan_stats
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        for model in detected_models.values():
            category = model.category.value
            if category not in summary["models_by_category"]:
                summary["models_by_category"][category] = []
            summary["models_by_category"][category].append({
                "name": model.name,
                "path": str(model.path),
                "confidence": model.confidence_score,
                "size_mb": model.file_size_mb
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        for model in detected_models.values():
            priority = model.priority.name
            if priority not in summary["models_by_priority"]:
                summary["models_by_priority"][priority] = []
            summary["models_by_priority"][priority].append(model.name)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸
        for category in ModelCategory:
            best_model = detector.get_best_model_for_category(category)
            if best_model:
                summary["top_models"][category.value] = {
                    "name": best_model.name,
                    "path": str(best_model.path),
                    "confidence": best_model.confidence_score,
                    "priority": best_model.priority.name
                }
        
        return summary
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def detect_and_integrate_with_model_loader(
    model_loader_instance = None,
    auto_register: bool = True,
    **detection_kwargs
) -> Dict[str, Any]:
    """ëª¨ë¸ íƒì§€ ë° ModelLoader í†µí•©"""
    try:
        logger.info("ğŸ” ëª¨ë¸ íƒì§€ ë° ModelLoader í†µí•© ì‹œì‘...")
        
        # íƒì§€ ì‹¤í–‰
        detector = create_advanced_detector(**detection_kwargs)
        detected_models = detector.detect_all_models()
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No models detected"}
        
        # ì–´ëŒ‘í„° ìƒì„±
        adapter = AdvancedModelLoaderAdapter(detector)
        
        # ModelLoader ì„¤ì • ìƒì„±
        model_loader_config = adapter.generate_model_loader_config()
        loading_strategy = adapter.generate_optimized_loading_strategy()
        
        # ModelLoaderì™€ í†µí•© (ì„ íƒì )
        integration_result = {}
        if auto_register and model_loader_instance:
            try:
                # ëª¨ë¸ ë“±ë¡ (ì‹¤ì œ ModelLoader ì¸ìŠ¤í„´ìŠ¤ í•„ìš”)
                for config in model_loader_config["model_configs"]:
                    # ì—¬ê¸°ì„œ ì‹¤ì œ ModelLoader.register_model() í˜¸ì¶œ
                    pass  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” model_loader_instance.register_model(config) í˜¸ì¶œ
                
                integration_result["registered_models"] = len(model_loader_config["model_configs"])
                integration_result["success"] = True
                
            except Exception as e:
                logger.error(f"ModelLoader í†µí•© ì‹¤íŒ¨: {e}")
                integration_result["error"] = str(e)
                integration_result["success"] = False
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "detection_summary": {
                "total_models": len(detected_models),
                "scan_duration": detector.scan_stats["scan_duration"],
                "confidence_avg": sum(m.confidence_score for m in detected_models.values()) / len(detected_models)
            },
            "model_loader_config": model_loader_config,
            "loading_strategy": loading_strategy,
            "integration_result": integration_result,
            "success": True
        }
        
        logger.info(f"âœ… ëª¨ë¸ íƒì§€ ë° í†µí•© ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ íƒì§€ ë° í†µí•© ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def export_model_registry_code(
    output_path: Optional[Path] = None,
    detector: Optional[AdvancedModelDetector] = None
) -> Path:
    """íƒì§€ëœ ëª¨ë¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ model_registry.py ì½”ë“œ ìƒì„±"""
    try:
        if detector is None:
            detector = create_advanced_detector()
            detector.detect_all_models()
        
        if output_path is None:
            output_path = Path("generated_model_registry.py")
        
        # ì½”ë“œ í…œí”Œë¦¿
        code_template = '''# Auto-generated model registry
"""
ìë™ ìƒì„±ëœ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
Generated at: {timestamp}
Total models: {total_models}
"""

from pathlib import Path
from app.ai_pipeline.utils.model_loader import ModelConfig, ModelType

def register_detected_models(model_loader):
    """íƒì§€ëœ ëª¨ë¸ë“¤ì„ ModelLoaderì— ë“±ë¡"""
    
    # ê¸°ë³¸ ê²½ë¡œ
    ai_models_root = Path("ai_models")
    
    # íƒì§€ëœ ëª¨ë¸ë“¤ ë“±ë¡
{model_registrations}

def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return {model_list}

def get_priority_models():
    """ìš°ì„ ìˆœìœ„ ë†’ì€ ëª¨ë¸ë“¤ ë°˜í™˜"""
    return {priority_models}

def get_models_by_category():
    """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë§¤í•‘ ë°˜í™˜"""
    return {category_mapping}
'''
        
        # ëª¨ë¸ ë“±ë¡ ì½”ë“œ ìƒì„±
        registrations = []
        model_names = []
        priority_models = []
        category_mapping = {}
        
        for name, model in detector.detected_models.items():
            model_names.append(f'"{name}"')
            
            if model.priority in [ModelPriority.CRITICAL, ModelPriority.HIGH]:
                priority_models.append(f'"{name}"')
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë§¤í•‘
            category = model.category.value
            if category not in category_mapping:
                category_mapping[category] = []
            category_mapping[category].append(f'"{name}"')
            
            # ë“±ë¡ ì½”ë“œ
            registration_code = f'''
    # {name}
    model_loader.register_model(
        "{name}",
        ModelConfig(
            name="{name}",
            model_type=ModelType.{model.category.name},
            model_class="{model.model_type}",
            checkpoint_path="{model.path}",
            input_size={self._get_input_size_for_category(model.category)},
            device="auto",
            metadata={{
                "auto_detected": True,
                "confidence": {model.confidence_score:.3f},
                "priority": "{model.priority.name}",
                "file_size_mb": {model.file_size_mb:.1f}
            }}
        )
    )'''
            registrations.append(registration_code)
        
        # ìµœì¢… ì½”ë“œ ìƒì„±
        final_code = code_template.format(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            total_models=len(detector.detected_models),
            model_registrations='\n'.join(registrations),
            model_list=f"[{', '.join(model_names)}]",
            priority_models=f"[{', '.join(priority_models)}]",
            category_mapping=str(category_mapping).replace("'", '"')
        )
        
        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_code)
        
        logger.info(f"âœ… ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì½”ë“œ ìƒì„±: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° ë° í—¬í¼ í•¨ìˆ˜ë“¤
# ==============================================

def validate_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ ê²½ë¡œë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "missing_files": [],
            "permission_errors": [],
            "total_size_gb": 0.0
        }
        
        for name, model in detected_models.items():
            try:
                # ì£¼ ê²½ë¡œ í™•ì¸
                if model.path.exists() and model.path.is_file():
                    validation_result["valid_models"].append(name)
                    validation_result["total_size_gb"] += model.file_size_mb / 1024
                else:
                    validation_result["missing_files"].append({
                        "name": name,
                        "path": str(model.path)
                    })
                
                # ëŒ€ì²´ ê²½ë¡œë“¤ í™•ì¸
                valid_alternatives = []
                for alt_path in model.alternative_paths:
                    if alt_path.exists() and alt_path.is_file():
                        valid_alternatives.append(str(alt_path))
                
                if valid_alternatives and name in [m["name"] for m in validation_result["missing_files"]]:
                    # ì£¼ ê²½ë¡œëŠ” ì—†ì§€ë§Œ ëŒ€ì²´ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                    validation_result["missing_files"] = [
                        m for m in validation_result["missing_files"] 
                        if m["name"] != name
                    ]
                    validation_result["valid_models"].append(name)
                
            except PermissionError:
                validation_result["permission_errors"].append({
                    "name": name,
                    "path": str(model.path)
                })
            except Exception as e:
                validation_result["invalid_models"].append({
                    "name": name,
                    "path": str(model.path),
                    "error": str(e)
                })
        
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_count": len(validation_result["valid_models"]),
            "invalid_count": len(validation_result["invalid_models"]),
            "missing_count": len(validation_result["missing_files"]),
            "permission_error_count": len(validation_result["permission_errors"]),
            "validation_rate": len(validation_result["valid_models"]) / len(detected_models) if detected_models else 0
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def benchmark_model_loading(detected_models: Dict[str, DetectedModel], sample_size: int = 5) -> Dict[str, Any]:
    """ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    try:
        if not TORCH_AVAILABLE:
            return {"error": "PyTorch not available"}
        
        benchmark_results = {
            "tested_models": [],
            "loading_times": {},
            "memory_usage": {},
            "errors": [],
            "recommendations": []
        }
        
        # ìƒ˜í”Œ ëª¨ë¸ ì„ íƒ (í¬ê¸°ë³„ë¡œ ë‹¤ì–‘í•˜ê²Œ)
        sorted_models = sorted(
            detected_models.items(),
            key=lambda x: x[1].file_size_mb
        )
        
        # ì‘ì€ ëª¨ë¸, ì¤‘ê°„ ëª¨ë¸, í° ëª¨ë¸ ê³¨ê³ ë£¨ ì„ íƒ
        sample_indices = [
            0,  # ê°€ì¥ ì‘ì€ ëª¨ë¸
            len(sorted_models) // 4,  # 25% ì§€ì 
            len(sorted_models) // 2,  # 50% ì§€ì 
            len(sorted_models) * 3 // 4,  # 75% ì§€ì 
            -1  # ê°€ì¥ í° ëª¨ë¸
        ]
        
        sample_models = [sorted_models[i] for i in sample_indices if i < len(sorted_models)]
        sample_models = sample_models[:sample_size]
        
        for name, model in sample_models:
            try:
                if not model.path.exists():
                    continue
                
                start_time = time.time()
                
                # ê°„ë‹¨í•œ ë¡œë”© í…ŒìŠ¤íŠ¸
                if model.file_extension in ['.pth', '.pt']:
                    # PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                    checkpoint = torch.load(model.path, map_location='cpu', weights_only=True)
                    loading_time = time.time() - start_time
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                    memory_usage = 0
                    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                        for tensor in state_dict.values():
                            if torch.is_tensor(tensor):
                                memory_usage += tensor.numel() * tensor.element_size()
                    
                    memory_usage_mb = memory_usage / (1024 * 1024)
                    
                    benchmark_results["tested_models"].append(name)
                    benchmark_results["loading_times"][name] = loading_time
                    benchmark_results["memory_usage"][name] = memory_usage_mb
                    
                    # ì •ë¦¬
                    del checkpoint
                    
                else:
                    # ë‹¤ë¥¸ í˜•ì‹ì€ íŒŒì¼ í¬ê¸°ë§Œ ì¸¡ì •
                    loading_time = 0.1  # ì¶”ì •ê°’
                    memory_usage_mb = model.file_size_mb * 1.2  # ì¶”ì •ê°’
                    
                    benchmark_results["tested_models"].append(name)
                    benchmark_results["loading_times"][name] = loading_time
                    benchmark_results["memory_usage"][name] = memory_usage_mb
                
            except Exception as e:
                benchmark_results["errors"].append({
                    "model": name,
                    "error": str(e)
                })
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if benchmark_results["loading_times"]:
            avg_loading_time = sum(benchmark_results["loading_times"].values()) / len(benchmark_results["loading_times"])
            total_memory = sum(benchmark_results["memory_usage"].values())
            
            if avg_loading_time > 5.0:
                benchmark_results["recommendations"].append("Consider using model caching for faster loading")
            
            if total_memory > 16000:  # 16GB
                benchmark_results["recommendations"].append("Consider selective model loading to manage memory usage")
            
            fast_models = [name for name, time in benchmark_results["loading_times"].items() if time < 1.0]
            if fast_models:
                benchmark_results["recommendations"].append(f"Fast loading models for quick startup: {fast_models[:3]}")
        
        return benchmark_results
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'AdvancedModelDetector',
    'AdvancedModelLoaderAdapter',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_advanced_detector',
    'quick_model_detection',
    'detect_and_integrate_with_model_loader',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'export_model_registry_code',
    'validate_model_paths',
    'benchmark_model_loading',
    
    # ì„¤ì • ë° íŒ¨í„´
    'ADVANCED_MODEL_PATTERNS'
]

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
AutoModelDetector = AdvancedModelDetector
ModelLoaderAdapter = AdvancedModelLoaderAdapter
create_auto_detector = create_advanced_detector

logger.info("âœ… ê³ ê¸‰ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ - ëª¨ë“  ê¸°ëŠ¥ êµ¬í˜„")