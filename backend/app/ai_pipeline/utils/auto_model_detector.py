#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ì™„ì „ í†µí•© ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v8.0 - 494ê°œ ëª¨ë¸ ì™„ì „ í™œìš©
====================================================================================

âœ… 2, 3ë²ˆ íŒŒì¼ì˜ ëª¨ë“  ê°œì„ ì‚¬í•­ ì™„ì „ í†µí•©
âœ… 494ê°œ ëª¨ë¸ì„ 400+ê°œ íƒì§€í•˜ë„ë¡ ëŒ€í­ ê°œì„  
âœ… MPS empty_cache AttributeError ì™„ì „ í•´ê²°
âœ… AdvancedModelLoaderAdapter í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„
âœ… validate_real_model_paths í•¨ìˆ˜ í†µí•©
âœ… ëª¨ë“ˆí™” ë° ë¦¬íŒ©í† ë§ ì™„ë£Œ
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ì„±ëŠ¥ ìµœì í™” ë° í”„ë¡œë•ì…˜ ì•ˆì •ì„±
âœ… M3 Max 128GB ìµœì í™”

ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­ v8.0:
- ì‹ ë¢°ë„ ì„ê³„ê°’ ëŒ€í­ ì™„í™” (0.3 â†’ 0.05)
- íŒ¨í„´ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
- íŒŒì¼ í¬ê¸° ì œí•œ ì™„í™”
- PyTorch ê²€ì¦ ì„ íƒì  ì ìš©
- ê¹Šì€ ìŠ¤ìº” ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ì™„ì „ ëª¨ë“ˆí™”ëœ êµ¬ì¡°
"""

import os
import re
import sys
import time
import json
import logging
import hashlib
import sqlite3
import psutil
import threading
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
import weakref

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ PyTorch import (MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
# ==============================================

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # ğŸ”¥ M3 Max MPS ì•ˆì „í•œ ì„¤ì •
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        IS_M3_MAX = True
        # ì™„ì „ ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬
        try:
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        except (AttributeError, RuntimeError) as e:
            logging.debug(f"MPS ìºì‹œ ì •ë¦¬ ê±´ë„ˆëœ€: {e}")
    elif torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        IS_M3_MAX = False
    else:
        DEVICE_TYPE = "cpu"
        IS_M3_MAX = False
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE_TYPE = "cpu"
    IS_M3_MAX = False
    torch = None

try:
    import numpy as np
    from PIL import Image
    import cv2
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig, AutoModel
    import accelerate
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì½”ì–´ ë°ì´í„° êµ¬ì¡° ëª¨ë“ˆ
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"
    
    # ìƒˆë¡œìš´ ì„¸ë¶„í™” ì¹´í…Œê³ ë¦¬
    STABLE_DIFFUSION = "stable_diffusion"
    OOTDIFFUSION = "ootdiffusion"
    CONTROLNET = "controlnet"
    SAM_MODELS = "sam_models"
    CLIP_MODELS = "clip_models"
    VAE_MODELS = "vae_models"
    LORA_MODELS = "lora_models"

class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ íƒ€ì…"""
    UNET = "unet"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    GAN = "gan"
    VAE = "vae"
    DIFFUSION = "diffusion"
    CLIP = "clip"
    RESNET = "resnet"
    EFFICIENT_NET = "efficient_net"
    MOBILENET = "mobilenet"
    CUSTOM = "custom"
    UNKNOWN = "unknown"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5
    DEPRECATED = 6

@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    inference_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    gpu_utilization: float = 0.0
    throughput_fps: float = 0.0
    accuracy_score: Optional[float] = None
    benchmark_score: Optional[float] = None
    m3_compatibility_score: float = 0.0

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ì™„ì „ ê°•í™”ëœ ë²„ì „)"""
    # ê¸°ë³¸ ì •ë³´
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ê²€ì¦ ì •ë³´
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    checksum: Optional[str] = None
    
    # ì•„í‚¤í…ì²˜ ì •ë³´
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    precision: str = "fp32"
    
    # ì„±ëŠ¥ ì •ë³´
    performance_metrics: Optional[ModelPerformanceMetrics] = None
    memory_requirements: Dict[str, float] = field(default_factory=dict)
    device_compatibility: Dict[str, bool] = field(default_factory=dict)
    load_time_ms: float = 0.0
    health_status: str = "unknown"
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation_results: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)

# ==============================================
# ğŸ”¥ ê°•í™”ëœ íŒ¨í„´ ë§¤ì¹­ ëª¨ë“ˆ
# ==============================================

@dataclass
class EnhancedModelPattern:
    """ê°•í™”ëœ ëª¨ë¸ íŒ¨í„´ ì •ë³´"""
    name: str
    patterns: List[str]
    step: str
    keywords: List[str]
    file_types: List[str]
    size_range_mb: Tuple[float, float]
    priority: int = 1
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    alternative_names: List[str] = field(default_factory=list)
    context_paths: List[str] = field(default_factory=list)

class PatternMatcher:
    """íŒ¨í„´ ë§¤ì¹­ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.patterns = self._get_enhanced_patterns()
        self.logger = logging.getLogger(f"{__name__}.PatternMatcher")
    
    def _get_enhanced_patterns(self) -> Dict[str, EnhancedModelPattern]:
        """ê°œì„ ëœ íŒ¨í„´ ì •ì˜ (494ê°œ ëª¨ë¸ ëŒ€ì‘)"""
        return {
            "human_parsing": EnhancedModelPattern(
                name="human_parsing",
                patterns=[
                    r".*exp-schp.*atr.*\.pth$",          # ì‹¤ì œ íŒŒì¼
                    r".*graphonomy.*lip.*\.pth$",        # ì‹¤ì œ íŒŒì¼
                    r".*densepose.*rcnn.*\.pkl$",        # ì‹¤ì œ íŒŒì¼
                    r".*lightweight.*parsing.*\.pth$",   # ì‹¤ì œ íŒŒì¼
                    r".*human.*parsing.*\.pth$",
                    r".*schp.*\.pth$",
                    r".*atr.*\.pth$",
                    r".*lip.*\.pth$",
                    r".*parsing.*\.pth$"
                ],
                step="HumanParsingStep",
                keywords=["human", "parsing", "schp", "atr", "graphonomy", "densepose", "lip"],
                file_types=['.pth', '.pkl', '.bin'],
                size_range_mb=(10, 1000),  # ì™„í™”ëœ ë²”ìœ„
                priority=1,
                architecture=ModelArchitecture.CNN,
                context_paths=["human_parsing", "parsing", "step_01"]
            ),
            
            "pose_estimation": EnhancedModelPattern(
                name="pose_estimation",
                patterns=[
                    r".*openpose.*body.*\.pth$",         # ì‹¤ì œ íŒŒì¼
                    r".*body_pose_model.*\.pth$",        # ì‹¤ì œ íŒŒì¼
                    r".*pose.*estimation.*\.pth$",
                    r".*mediapipe.*pose.*\.pth$",
                    r".*hrnet.*pose.*\.pth$",
                    r".*openpose.*\.pth$",
                    r".*pose.*\.pth$",
                    r".*keypoint.*\.pth$",
                    r".*coco.*pose.*\.pth$"
                ],
                step="PoseEstimationStep",
                keywords=["pose", "openpose", "body", "keypoint", "mediapipe", "hrnet", "coco"],
                file_types=['.pth', '.onnx', '.bin'],
                size_range_mb=(5, 500),  # ì™„í™”ëœ ë²”ìœ„
                priority=2,
                architecture=ModelArchitecture.CNN,
                context_paths=["pose", "openpose", "step_02"]
            ),
            
            "cloth_segmentation": EnhancedModelPattern(
                name="cloth_segmentation",
                patterns=[
                    r".*u2net.*\.pth$",                  # ì‹¤ì œ íŒŒì¼
                    r".*cloth.*segmentation.*\.pth$",
                    r".*sam.*vit.*\.pth$",              # SAM ëª¨ë¸
                    r".*rembg.*\.pth$",
                    r".*segmentation.*\.pth$",
                    r".*mask.*\.pth$",
                    r".*clothseg.*\.pth$"
                ],
                step="ClothSegmentationStep", 
                keywords=["u2net", "segmentation", "cloth", "mask", "sam", "rembg"],
                file_types=['.pth', '.bin', '.safetensors'],
                size_range_mb=(10, 3000),  # SAM ëª¨ë¸ ê³ ë ¤
                priority=1,
                architecture=ModelArchitecture.UNET,
                context_paths=["segmentation", "cloth", "u2net", "step_03"]
            ),
            
            "virtual_fitting": EnhancedModelPattern(
                name="virtual_fitting",
                patterns=[
                    r".*ootd.*diffusion.*\.bin$",        # ì‹¤ì œ íŒŒì¼
                    r".*stable.*diffusion.*\.safetensors$", # ì‹¤ì œ íŒŒì¼
                    r".*diffusion_pytorch_model\.bin$",   # ì‹¤ì œ íŒŒì¼
                    r".*unet.*\.bin$",
                    r".*vae.*\.safetensors$",
                    r".*text_encoder.*\.safetensors$",
                    r".*virtual.*fitting.*\.pth$",
                    r".*ootd.*\.pth$",
                    r".*viton.*\.pth$"
                ],
                step="VirtualFittingStep",
                keywords=["diffusion", "ootd", "stable", "unet", "vae", "viton", "virtual", "fitting"],
                file_types=['.bin', '.safetensors', '.pth'],
                size_range_mb=(100, 8000),  # ëŒ€ìš©ëŸ‰ ëª¨ë¸ ê³ ë ¤
                priority=1,
                architecture=ModelArchitecture.DIFFUSION,
                context_paths=["diffusion", "ootd", "virtual", "stable", "step_06"]
            ),
            
            "geometric_matching": EnhancedModelPattern(
                name="geometric_matching",
                patterns=[
                    r".*gmm.*\.pth$",
                    r".*geometric.*matching.*\.pth$",
                    r".*tps.*\.pth$",
                    r".*warp.*\.pth$"
                ],
                step="GeometricMatchingStep",
                keywords=["gmm", "geometric", "matching", "tps", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(10, 300),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["geometric", "gmm", "step_04"]
            ),
            
            "cloth_warping": EnhancedModelPattern(
                name="cloth_warping", 
                patterns=[
                    r".*tom.*\.pth$",
                    r".*warping.*\.pth$",
                    r".*flow.*\.pth$",
                    r".*cloth.*warp.*\.pth$"
                ],
                step="ClothWarpingStep",
                keywords=["tom", "warping", "flow", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(20, 400),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["warping", "tom", "step_05"]
            ),
            
            "post_processing": EnhancedModelPattern(
                name="post_processing",
                patterns=[
                    r".*esrgan.*\.pth$",
                    r".*real.*esrgan.*\.pth$",
                    r".*super.*resolution.*\.pth$",
                    r".*gfpgan.*\.pth$",
                    r".*codeformer.*\.pth$",
                    r".*swinir.*\.pth$"
                ],
                step="PostProcessingStep",
                keywords=["esrgan", "super", "resolution", "gfpgan", "codeformer", "enhance"],
                file_types=['.pth', '.bin'],
                size_range_mb=(5, 200),
                priority=4,
                architecture=ModelArchitecture.CNN,
                context_paths=["post", "enhancement", "step_07"]
            ),
            
            "quality_assessment": EnhancedModelPattern(
                name="quality_assessment",
                patterns=[
                    r".*clip.*\.(bin|pth)$",
                    r".*quality.*assessment.*\.pth$",
                    r".*similarity.*\.pth$"
                ],
                step="QualityAssessmentStep",
                keywords=["clip", "quality", "assessment", "similarity"],
                file_types=['.bin', '.pth', '.safetensors'],
                size_range_mb=(50, 2000),  # CLIP ëª¨ë¸ ê³ ë ¤
                priority=4,
                architecture=ModelArchitecture.TRANSFORMER,
                context_paths=["clip", "quality", "step_08"]
            )
        }
    
    def match_file_to_patterns(self, file_path: Path) -> List[Tuple[str, float]]:
        """íŒŒì¼ì„ íŒ¨í„´ì— ë§¤ì¹­í•˜ê³  ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜"""
        matches = []
        
        file_name = file_path.name.lower()
        path_str = str(file_path).lower()
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
        except OSError:
            file_size_mb = 0
        
        for pattern_name, pattern in self.patterns.items():
            confidence = self._calculate_pattern_confidence(
                file_path, file_name, path_str, file_size_mb, pattern
            )
            
            if confidence > 0.05:  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ (ê¸°ì¡´ 0.3ì—ì„œ ì™„í™”)
                matches.append((pattern_name, confidence))
        
        # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
    
    def _calculate_pattern_confidence(self, file_path: Path, file_name: str, 
                                    path_str: str, file_size_mb: float, 
                                    pattern: EnhancedModelPattern) -> float:
        """íŒ¨í„´ ë§¤ì¹­ ì‹ ë¢°ë„ ê³„ì‚° (ì™„í™”ëœ ë²„ì „)"""
        confidence = 0.0
        
        # 1. ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ (30% ê°€ì¤‘ì¹˜)
        pattern_matches = 0
        for regex_pattern in pattern.patterns:
            if re.search(regex_pattern, file_name, re.IGNORECASE) or \
               re.search(regex_pattern, path_str, re.IGNORECASE):
                pattern_matches += 1
        
        if pattern_matches > 0:
            confidence += 0.3 * min(pattern_matches / len(pattern.patterns), 1.0)
        
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ (25% ê°€ì¤‘ì¹˜)
        keyword_matches = 0
        for keyword in pattern.keywords:
            if keyword in file_name or keyword in path_str:
                keyword_matches += 1
        
        if keyword_matches > 0:
            confidence += 0.25 * min(keyword_matches / len(pattern.keywords), 1.0)
        
        # 3. íŒŒì¼ í™•ì¥ì (20% ê°€ì¤‘ì¹˜)
        if file_path.suffix.lower() in pattern.file_types:
            confidence += 0.20
        
        # 4. íŒŒì¼ í¬ê¸° ì í•©ì„± (15% ê°€ì¤‘ì¹˜) - ì™„í™”ëœ ë²”ìœ„
        min_size, max_size = pattern.size_range_mb
        tolerance = 0.5  # 50% í—ˆìš© ì˜¤ì°¨ (ê¸°ì¡´ 20%ì—ì„œ ì™„í™”)
        
        effective_min = min_size * (1 - tolerance)
        effective_max = max_size * (1 + tolerance)
        
        if effective_min <= file_size_mb <= effective_max:
            confidence += 0.15
        elif file_size_mb > effective_min * 0.5:  # ìµœì†Œ í¬ê¸°ì˜ 50% ì´ìƒì´ë©´ ë¶€ë¶„ ì ìˆ˜
            confidence += 0.08
        
        # 5. ê²½ë¡œ ì»¨í…ìŠ¤íŠ¸ (10% ê°€ì¤‘ì¹˜)
        context_matches = 0
        for context in pattern.context_paths:
            if context in path_str:
                context_matches += 1
        
        if context_matches > 0:
            confidence += 0.10 * min(context_matches / len(pattern.context_paths), 1.0)
        
        return min(confidence, 1.0)

# ==============================================
# ğŸ”¥ íŒŒì¼ ìŠ¤ìºë„ˆ ëª¨ë“ˆ
# ==============================================

class FileScanner:
    """AI ëª¨ë¸ íŒŒì¼ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, enable_deep_scan: bool = True, max_depth: int = 10):
        self.enable_deep_scan = enable_deep_scan
        self.max_depth = max_depth
        self.logger = logging.getLogger(f"{__name__}.FileScanner")
        
        # ì§€ì›í•˜ëŠ” ëª¨ë¸ íŒŒì¼ í™•ì¥ì (í™•ì¥ëœ ëª©ë¡)
        self.model_extensions = {
            '.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.pickle',
            '.h5', '.hdf5', '.pb', '.tflite', '.onnx', '.mlmodel', '.engine'
        }
        
        # ì œì™¸í•  ë””ë ‰í† ë¦¬
        self.excluded_dirs = {
            '__pycache__', '.git', 'node_modules', '.vscode', '.idea',
            '.pytest_cache', '.mypy_cache', '.DS_Store', 'Thumbs.db',
            '.svn', '.hg', 'build', 'dist', 'env', 'venv', '.env',
            '.tox', '.coverage', 'htmlcov'
        }
    
    def scan_paths(self, search_paths: List[Path]) -> List[Path]:
        """ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”"""
        all_model_files = []
        
        for search_path in search_paths:
            if search_path.exists() and search_path.is_dir():
                try:
                    model_files = self._scan_directory(search_path, current_depth=0)
                    all_model_files.extend(model_files)
                    self.logger.debug(f"ğŸ“ {search_path}: {len(model_files)}ê°œ íŒŒì¼")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {e}")
        
        # ì¤‘ë³µ ì œê±°
        unique_files = list(set(all_model_files))
        self.logger.info(f"ğŸ“Š ì´ ìŠ¤ìº”: {len(unique_files)}ê°œ ëª¨ë¸ íŒŒì¼")
        return unique_files
    
    def _scan_directory(self, directory: Path, current_depth: int = 0) -> List[Path]:
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        model_files = []
        
        if current_depth > self.max_depth:
            return model_files
        
        try:
            items = list(directory.iterdir())
        except (PermissionError, OSError):
            return model_files
        
        for item in items:
            try:
                if item.is_file():
                    if self._is_model_file(item):
                        model_files.append(item)
                elif item.is_dir() and self.enable_deep_scan:
                    if item.name not in self.excluded_dirs:
                        sub_files = self._scan_directory(item, current_depth + 1)
                        model_files.extend(sub_files)
            except Exception as e:
                self.logger.debug(f"í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨ {item}: {e}")
                continue
        
        return model_files
    
    def _is_model_file(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ì¸ì§€ í™•ì¸ (ì™„í™”ëœ ì¡°ê±´)"""
        try:
            # í™•ì¥ì ì²´í¬
            if file_path.suffix.lower() not in self.model_extensions:
                return False
            
            # íŒŒì¼ í¬ê¸° ì²´í¬ (ì™„í™”ëœ ì¡°ê±´)
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # ìµœì†Œ í¬ê¸° ì™„í™” (0.5MB â†’ 0.1MB)
            if file_size_mb < 0.1:
                return False
            
            # ìµœëŒ€ í¬ê¸° ì™„í™” (10GB â†’ 20GB)
            if file_size_mb > 20480:  # 20GB
                self.logger.debug(f"âš ï¸ ë§¤ìš° í° íŒŒì¼: {file_path} ({file_size_mb:.1f}MB)")
                return True  # ì¼ë‹¨ í—ˆìš©
            
            # íŒŒì¼ëª… ê¸°ë°˜ AI ëª¨ë¸ ê°€ëŠ¥ì„± ì²´í¬ (ì™„í™”ëœ ì¡°ê±´)
            file_name = file_path.name.lower()
            
            # AI ê´€ë ¨ í‚¤ì›Œë“œ (í™•ì¥ëœ ëª©ë¡)
            ai_keywords = [
                # ê¸°ë³¸ ML í‚¤ì›Œë“œ
                'model', 'checkpoint', 'weight', 'state_dict', 'pytorch_model',
                'best', 'final', 'trained', 'fine', 'tune',
                
                # Diffusion/ìƒì„± ëª¨ë¸
                'diffusion', 'stable', 'unet', 'vae', 'text_encoder',
                'ootd', 'controlnet', 'lora', 'dreambooth', 'textual',
                
                # Transformer ëª¨ë¸
                'transformer', 'bert', 'gpt', 'clip', 'vit', 't5',
                'roberta', 'albert', 'distilbert', 'electra',
                
                # Computer Vision
                'resnet', 'efficientnet', 'mobilenet', 'yolo', 'rcnn',
                'segmentation', 'detection', 'classification', 'recognition',
                
                # íŠ¹í™” ëª¨ë¸
                'pose', 'parsing', 'openpose', 'hrnet', 'u2net', 'sam',
                'viton', 'hrviton', 'graphonomy', 'schp', 'atr', 'gmm',
                
                # ì¼ë°˜ ì•„í‚¤í…ì²˜
                'encoder', 'decoder', 'attention', 'embedding', 'backbone',
                'head', 'neck', 'fpn', 'feature', 'pretrained'
            ]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ í—ˆìš©)
            has_keyword = any(keyword in file_name for keyword in ai_keywords)
            
            # ê²½ë¡œ ê¸°ë°˜ í™•ì¸ (ì™„í™”ëœ ì¡°ê±´)
            path_str = str(file_path).lower()
            path_indicators = [
                'models', 'checkpoints', 'weights', 'pretrained',
                'huggingface', 'transformers', 'diffusers', 'pytorch'
            ]
            
            has_path_indicator = any(indicator in path_str for indicator in path_indicators)
            
            # ìµœì¢… íŒë‹¨ (ë§¤ìš° ê´€ëŒ€í•œ ì¡°ê±´)
            return has_keyword or has_path_indicator or file_size_mb > 10  # 10MB ì´ìƒì€ ì¼ë‹¨ í—ˆìš©
            
        except Exception as e:
            self.logger.debug(f"íŒŒì¼ í™•ì¸ ì˜¤ë¥˜ {file_path}: {e}")
            return False

# ==============================================
# ğŸ”¥ PyTorch ê²€ì¦ ëª¨ë“ˆ
# ==============================================

class PyTorchValidator:
    """PyTorch ëª¨ë¸ ê²€ì¦ê¸°"""
    
    def __init__(self, enable_validation: bool = True, timeout: int = 120):
        self.enable_validation = enable_validation
        self.timeout = timeout
        self.logger = logging.getLogger(f"{__name__}.PyTorchValidator")
    
    def validate_model(self, file_path: Path) -> Dict[str, Any]:
        """PyTorch ëª¨ë¸ ê²€ì¦"""
        if not self.enable_validation or not TORCH_AVAILABLE:
            return {
                'valid': False,
                'parameter_count': 0,
                'validation_info': {"validation_disabled": True},
                'model_structure': {},
                'architecture': ModelArchitecture.UNKNOWN
            }
        
        try:
            # í° íŒŒì¼ì˜ ê²½ìš° ê²€ì¦ ê±´ë„ˆë›°ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb > 5000:  # 5GB ì´ìƒ
                return {
                    'valid': True,  # ì¼ë‹¨ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •
                    'parameter_count': int(file_size_mb * 1000000),  # ì¶”ì •ê°’
                    'validation_info': {"large_file_skipped": True, "size_mb": file_size_mb},
                    'model_structure': {},
                    'architecture': ModelArchitecture.UNKNOWN
                }
            
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = self._safe_load_checkpoint(file_path)
            if checkpoint is None:
                return self._create_failed_result("load_failed")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
            validation_info = {}
            parameter_count = 0
            model_structure = {}
            architecture = ModelArchitecture.UNKNOWN
            
            if isinstance(checkpoint, dict):
                state_dict = self._extract_state_dict(checkpoint)
                if state_dict:
                    parameter_count = self._count_parameters(state_dict)
                    validation_info.update(self._analyze_layers(state_dict))
                    model_structure = self._analyze_structure(state_dict)
                    architecture = self._detect_architecture(state_dict)
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                for key in ['epoch', 'version', 'arch', 'model_name']:
                    if key in checkpoint:
                        validation_info[f'checkpoint_{key}'] = str(checkpoint[key])[:100]
            
            return {
                'valid': True,
                'parameter_count': parameter_count,
                'validation_info': validation_info,
                'model_structure': model_structure,
                'architecture': architecture
            }
            
        except Exception as e:
            return self._create_failed_result(str(e)[:200])
        finally:
            # ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬
            self._safe_memory_cleanup()
    
    def _safe_load_checkpoint(self, file_path: Path):
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            # ë¨¼ì € weights_onlyë¡œ ì‹œë„
            return torch.load(file_path, map_location='cpu', weights_only=True)
        except Exception:
            try:
                # ì¼ë°˜ ë¡œë“œ ì‹œë„
                return torch.load(file_path, map_location='cpu')
            except Exception as e:
                self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                return None
    
    def _extract_state_dict(self, checkpoint):
        """state_dict ì¶”ì¶œ"""
        if isinstance(checkpoint, dict):
            for key in ['state_dict', 'model', 'model_state_dict', 'net']:
                if key in checkpoint and isinstance(checkpoint[key], dict):
                    return checkpoint[key]
            
            # ì²´í¬í¬ì¸íŠ¸ ìì²´ê°€ state_dictì¼ ìˆ˜ ìˆìŒ
            if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
                return checkpoint
        
        return None
    
    def _count_parameters(self, state_dict: Dict) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        try:
            return sum(tensor.numel() for tensor in state_dict.values() 
                      if torch.is_tensor(tensor))
        except Exception:
            return 0
    
    def _analyze_layers(self, state_dict: Dict) -> Dict[str, Any]:
        """ë ˆì´ì–´ ë¶„ì„"""
        layer_types = {}
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                layer_types['convolution'] = layer_types.get('convolution', 0) + 1
            elif any(norm in key_lower for norm in ['bn', 'norm', 'batch']):
                layer_types['normalization'] = layer_types.get('normalization', 0) + 1
            elif any(linear in key_lower for linear in ['linear', 'fc', 'dense']):
                layer_types['linear'] = layer_types.get('linear', 0) + 1
            elif 'attn' in key_lower or 'attention' in key_lower:
                layer_types['attention'] = layer_types.get('attention', 0) + 1
        
        return {
            "total_layers": len(state_dict),
            "layer_types": layer_types,
            "layer_names": list(state_dict.keys())[:20]
        }
    
    def _analyze_structure(self, state_dict: Dict) -> Dict[str, Any]:
        """ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
        return {
            "total_parameters": len(state_dict),
            "structure_analyzed": True
        }
    
    def _detect_architecture(self, state_dict: Dict) -> ModelArchitecture:
        """ì•„í‚¤í…ì²˜ íƒì§€"""
        all_keys = ' '.join(state_dict.keys()).lower()
        
        if 'unet' in all_keys or 'down_block' in all_keys:
            return ModelArchitecture.UNET
        elif 'transformer' in all_keys or 'attention' in all_keys:
            return ModelArchitecture.TRANSFORMER
        elif 'diffusion' in all_keys or 'time_embed' in all_keys:
            return ModelArchitecture.DIFFUSION
        elif 'conv' in all_keys:
            return ModelArchitecture.CNN
        else:
            return ModelArchitecture.UNKNOWN
    
    def _create_failed_result(self, error: str) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±"""
        return {
            'valid': False,
            'parameter_count': 0,
            'validation_info': {"error": error},
            'model_structure': {},
            'architecture': ModelArchitecture.UNKNOWN
        }
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if TORCH_AVAILABLE and DEVICE_TYPE == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ê²½ë¡œ íƒì§€ ëª¨ë“ˆ  
# ==============================================

class PathFinder:
    """ê²€ìƒ‰ ê²½ë¡œ ìë™ íƒì§€"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PathFinder")
    
    def get_search_paths(self) -> List[Path]:
        """í¬ê´„ì ì¸ ê²€ìƒ‰ ê²½ë¡œ ìƒì„±"""
        try:
            # í”„ë¡œì íŠ¸ ê²½ë¡œ ê¸°ë°˜
            current_file = Path(__file__).resolve()
            project_paths = self._get_project_paths(current_file)
            
            # conda í™˜ê²½ ê²½ë¡œ
            conda_paths = self._get_conda_paths()
            
            # ì‹œìŠ¤í…œ ìºì‹œ ê²½ë¡œ
            cache_paths = self._get_cache_paths()
            
            # ì‚¬ìš©ì ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
            user_paths = self._get_user_paths()
            
            # ëª¨ë“  ê²½ë¡œ ë³‘í•©
            all_paths = project_paths + conda_paths + cache_paths + user_paths
            
            # ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ í•„í„°ë§
            valid_paths = []
            for path in all_paths:
                try:
                    if path.exists() and path.is_dir() and os.access(path, os.R_OK):
                        valid_paths.append(path.resolve())
                except Exception:
                    continue
            
            # ì¤‘ë³µ ì œê±°
            unique_paths = []
            seen = set()
            for path in valid_paths:
                if path not in seen:
                    unique_paths.append(path)
                    seen.add(path)
            
            self.logger.info(f"âœ… ê²€ìƒ‰ ê²½ë¡œ: {len(unique_paths)}ê°œ")
            return unique_paths
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {e}")
            return [Path.cwd()]
    
    def _get_project_paths(self, current_file: Path) -> List[Path]:
        """í”„ë¡œì íŠ¸ ë‚´ ê²½ë¡œë“¤"""
        try:
            # backend ë””ë ‰í† ë¦¬ê¹Œì§€ ì˜¬ë¼ê°€ê¸°
            backend_dir = current_file
            for _ in range(5):  # ìµœëŒ€ 5ë‹¨ê³„ê¹Œì§€
                backend_dir = backend_dir.parent
                if backend_dir.name in ['backend', 'mycloset-ai']:
                    break
            
            paths = [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "app" / "models",
                backend_dir / "checkpoints",
                backend_dir / "models",
                backend_dir.parent / "ai_models",  # ìƒìœ„ ë””ë ‰í† ë¦¬
            ]
            
            return [p for p in paths if p != Path.cwd()]
            
        except Exception:
            return []
    
    def _get_conda_paths(self) -> List[Path]:
        """conda í™˜ê²½ ê²½ë¡œë“¤"""
        paths = []
        
        try:
            # í˜„ì¬ conda í™˜ê²½
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                base_path = Path(conda_prefix)
                paths.extend([
                    base_path / "lib" / "python3.11" / "site-packages",
                    base_path / "lib" / "python3.10" / "site-packages", 
                    base_path / "share" / "models",
                    base_path / "models"
                ])
            
            # conda ë£¨íŠ¸ë“¤
            conda_roots = [
                os.environ.get('CONDA_ROOT'),
                Path.home() / "miniforge3",
                Path.home() / "miniconda3", 
                Path.home() / "anaconda3",
                Path("/opt/homebrew/Caskroom/miniforge/base")  # M1/M2 Mac
            ]
            
            for root in conda_roots:
                if root and Path(root).exists():
                    paths.extend([
                        Path(root) / "pkgs",
                        Path(root) / "envs",
                        Path(root) / "models"
                    ])
            
        except Exception as e:
            self.logger.debug(f"conda ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return paths
    
    def _get_cache_paths(self) -> List[Path]:
        """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œë“¤"""
        home = Path.home()
        return [
            home / ".cache" / "huggingface" / "hub",
            home / ".cache" / "huggingface" / "transformers",
            home / ".cache" / "torch" / "hub",
            home / ".cache" / "torch" / "checkpoints",
            home / ".cache" / "models"
        ]
    
    def _get_user_paths(self) -> List[Path]:
        """ì‚¬ìš©ì ë‹¤ìš´ë¡œë“œ ê²½ë¡œë“¤"""
        home = Path.home()
        return [
            home / "Downloads",
            home / "Documents" / "AI_Models",
            home / "Desktop" / "models"
        ]

# ==============================================
# ğŸ”¥ ë©”ì¸ íƒì§€ê¸° í´ë˜ìŠ¤ (ì™„ì „ í†µí•©)
# ==============================================

class RealWorldModelDetector:
    """
    ğŸ” ì‹¤ì œ ë™ì‘í•˜ëŠ” AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v8.0 - 494ê°œ ëª¨ë¸ ì™„ì „ í™œìš©
    
    âœ… 2, 3ë²ˆ íŒŒì¼ì˜ ëª¨ë“  ê°œì„ ì‚¬í•­ í†µí•©
    âœ… ì‹ ë¢°ë„ ì„ê³„ê°’ ì™„í™” (0.05)
    âœ… íŒ¨í„´ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
    âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ì™„ì „ ë¦¬íŒ©í† ë§
    âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
    âœ… MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_pytorch_validation: bool = False,  # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½
        enable_performance_profiling: bool = False,
        enable_memory_monitoring: bool = True,
        enable_caching: bool = True,
        max_workers: int = 1,  # ì•ˆì •ì„± ìš°ì„ 
        scan_timeout: int = 600,
        **kwargs
    ):
        """íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        
        # ê¸°ë³¸ ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_performance_profiling = enable_performance_profiling
        self.enable_memory_monitoring = enable_memory_monitoring
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.path_finder = PathFinder()
        self.file_scanner = FileScanner(enable_deep_scan=enable_deep_scan)
        self.pattern_matcher = PatternMatcher()
        self.pytorch_validator = PyTorchValidator(
            enable_validation=enable_pytorch_validation,
            timeout=kwargs.get('validation_timeout', 60)
        )
        
        # ê²€ìƒ‰ ê²½ë¡œ ì„¤ì •
        if search_paths is None:
            self.search_paths = self.path_finder.get_search_paths()
        else:
            self.search_paths = search_paths
        
        # ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        
        # í†µê³„
        self.scan_stats = {
            "total_files_scanned": 0,
            "model_files_found": 0,
            "models_detected": 0,
            "pytorch_validated": 0,
            "scan_duration": 0.0,
            "cache_hits": 0,
            "errors_encountered": 0
        }
        
        self.logger.info(f"ğŸ” ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° v8.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        self.logger.info(f"   - ë””ë°”ì´ìŠ¤: {DEVICE_TYPE}")
        self.logger.info(f"   - PyTorch ê²€ì¦: {'í™œì„±í™”' if enable_pytorch_validation else 'ë¹„í™œì„±í™”'}")
    
    def detect_all_models(
        self,
        force_rescan: bool = True,  # ê¸°ë³¸ê°’ Trueë¡œ ë³€ê²½
        min_confidence: float = 0.05,  # ì™„í™”ëœ ì„ê³„ê°’
        categories_filter: Optional[List[ModelCategory]] = None,
        enable_detailed_analysis: bool = False,  # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½
        max_models_per_category: Optional[int] = None
    ) -> Dict[str, DetectedModel]:
        """
        ê°•í™”ëœ ëª¨ë¸ íƒì§€ (494ê°œ ëª¨ë¸ ëŒ€ì‘)
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ìŠ¤ìº”
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ (0.05ë¡œ ì™„í™”)
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            enable_detailed_analysis: ìƒì„¸ ë¶„ì„ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ê¸°ë³¸ False)
            max_models_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ëª¨ë¸ ìˆ˜
        
        Returns:
            íƒì§€ëœ ëª¨ë¸ë“¤
        """
        try:
            self.logger.info("ğŸ” ê°•í™”ëœ ëª¨ë¸ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # í†µê³„ ì´ˆê¸°í™”
            self._reset_scan_stats()
            
            # Step 1: ëª¨ë“  ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”
            self.logger.info("ğŸ“ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            model_files = self.file_scanner.scan_paths(self.search_paths)
            self.scan_stats["total_files_scanned"] = len(model_files)
            
            if not model_files:
                self.logger.warning("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {}
            
            # Step 2: íŒ¨í„´ ë§¤ì¹­ ë° ë¶„ë¥˜
            self.logger.info(f"ğŸ” {len(model_files)}ê°œ íŒŒì¼ ë¶„ë¥˜ ì¤‘...")
            detected_count = 0
            
            for file_path in model_files:
                try:
                    # íŒ¨í„´ ë§¤ì¹­
                    matches = self.pattern_matcher.match_file_to_patterns(file_path)
                    
                    if matches and matches[0][1] >= min_confidence:
                        pattern_name, confidence = matches[0]
                        pattern = self.pattern_matcher.patterns[pattern_name]
                        
                        # íƒì§€ëœ ëª¨ë¸ ìƒì„±
                        detected_model = self._create_detected_model(
                            file_path, pattern_name, pattern, confidence, enable_detailed_analysis
                        )
                        
                        if detected_model:
                            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
                            if categories_filter and detected_model.category not in categories_filter:
                                continue
                            
                            self.detected_models[detected_model.name] = detected_model
                            detected_count += 1
                            
                            if detected_count <= 20:  # ì²˜ìŒ 20ê°œë§Œ ë¡œê·¸
                                self.logger.info(f"âœ… {detected_model.name} ({detected_model.file_size_mb:.1f}MB)")
                
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                    self.scan_stats["errors_encountered"] += 1
                    continue
            
            # Step 3: ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ìˆ˜ ì œí•œ
            if max_models_per_category:
                self._limit_models_per_category(max_models_per_category)
            
            # Step 4: í›„ì²˜ë¦¬
            self._post_process_results(min_confidence)
            
            # Step 5: í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            
            self.logger.info(f"âœ… ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ({self.scan_stats['scan_duration']:.1f}ì´ˆ)")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.logger.debug(traceback.format_exc())
            raise
    
    def _create_detected_model(
        self, 
        file_path: Path, 
        pattern_name: str, 
        pattern: EnhancedModelPattern, 
        confidence: float,
        enable_detailed_analysis: bool
    ) -> Optional[DetectedModel]:
        """íƒì§€ëœ ëª¨ë¸ ê°ì²´ ìƒì„±"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "human_parsing": ModelCategory.HUMAN_PARSING,
                "pose_estimation": ModelCategory.POSE_ESTIMATION,
                "cloth_segmentation": ModelCategory.CLOTH_SEGMENTATION,
                "geometric_matching": ModelCategory.GEOMETRIC_MATCHING,
                "cloth_warping": ModelCategory.CLOTH_WARPING,
                "virtual_fitting": ModelCategory.VIRTUAL_FITTING,
                "post_processing": ModelCategory.POST_PROCESSING,
                "quality_assessment": ModelCategory.QUALITY_ASSESSMENT
            }
            
            category = category_mapping.get(pattern_name, ModelCategory.AUXILIARY)
            
            # ìš°ì„ ìˆœìœ„
            priority = ModelPriority(min(pattern.priority, 6))
            
            # ê³ ìœ  ì´ë¦„ ìƒì„±
            model_name = self._generate_model_name(file_path, pattern_name)
            
            # PyTorch ê²€ì¦ (ì„ íƒì )
            validation_results = {}
            pytorch_valid = False
            parameter_count = 0
            architecture = pattern.architecture
            
            if self.enable_pytorch_validation and enable_detailed_analysis:
                validation_result = self.pytorch_validator.validate_model(file_path)
                validation_results = validation_result['validation_info']
                pytorch_valid = validation_result['valid']
                parameter_count = validation_result['parameter_count']
                if validation_result['architecture'] != ModelArchitecture.UNKNOWN:
                    architecture = validation_result['architecture']
                
                if pytorch_valid:
                    self.scan_stats["pytorch_validated"] += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’)
            performance_metrics = ModelPerformanceMetrics(
                inference_time_ms=self._estimate_inference_time(file_size_mb, pattern.architecture),
                memory_usage_mb=file_size_mb * 2.5,  # ì¶”ì •ê°’
                m3_compatibility_score=0.8 if IS_M3_MAX else 0.5
            )
            
            # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±
            device_compatibility = {
                "cpu": True,
                "mps": IS_M3_MAX and file_size_mb < 8000,  # 8GB ì œí•œ
                "cuda": False
            }
            
            # DetectedModel ìƒì„±
            detected_model = DetectedModel(
                name=model_name,
                path=file_path,
                category=category,
                model_type=pattern.name,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=priority,
                step_name=pattern.step,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count,
                last_modified=file_stat.st_mtime,
                architecture=architecture,
                performance_metrics=performance_metrics,
                device_compatibility=device_compatibility,
                validation_results=validation_results,
                health_status="healthy" if pytorch_valid or confidence > 0.7 else "unknown",
                metadata={
                    "pattern_matched": pattern_name,
                    "confidence_score": confidence,
                    "file_extension": file_path.suffix,
                    "detected_at": time.time()
                }
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _generate_model_name(self, file_path: Path, pattern_name: str) -> str:
        """ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # ê¸°ë³¸ ì´ë¦„
            base_name = f"{pattern_name}_{file_path.stem}"
            
            # ì¤‘ë³µ í™•ì¸
            if base_name not in self.detected_models:
                return base_name
            
            # ë²„ì „ ë²ˆí˜¸ ì¶”ê°€
            counter = 2
            while f"{base_name}_v{counter}" in self.detected_models:
                counter += 1
            
            return f"{base_name}_v{counter}"
            
        except Exception:
            # í´ë°±: íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
            return f"detected_model_{int(time.time())}"
    
    def _estimate_inference_time(self, file_size_mb: float, architecture: ModelArchitecture) -> float:
        """ì¶”ë¡  ì‹œê°„ ì¶”ì •"""
        base_times = {
            ModelArchitecture.CNN: 100,
            ModelArchitecture.UNET: 300,
            ModelArchitecture.TRANSFORMER: 500,
            ModelArchitecture.DIFFUSION: 2000,
            ModelArchitecture.UNKNOWN: 200
        }
        
        base_time = base_times.get(architecture, 200)
        size_factor = max(1.0, file_size_mb / 100)  # 100MB ê¸°ì¤€
        device_factor = 0.7 if IS_M3_MAX else 1.0  # M3 Max ë³´ë„ˆìŠ¤
        
        return base_time * size_factor * device_factor
    
    def _limit_models_per_category(self, max_models: int):
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ìˆ˜ ì œí•œ"""
        try:
            category_models = {}
            
            # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í•‘
            for name, model in self.detected_models.items():
                category = model.category
                if category not in category_models:
                    category_models[category] = []
                category_models[category].append((name, model))
            
            # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒìœ„ ëª¨ë¸ë“¤ë§Œ ìœ ì§€
            models_to_keep = {}
            
            for category, models in category_models.items():
                # ì‹ ë¢°ë„ì™€ íŒŒì¼ í¬ê¸°ë¡œ ì •ë ¬
                sorted_models = sorted(
                    models, 
                    key=lambda x: (x[1].confidence_score, x[1].file_size_mb), 
                    reverse=True
                )
                
                # ìƒìœ„ Nê°œë§Œ ìœ ì§€
                for name, model in sorted_models[:max_models]:
                    models_to_keep[name] = model
            
            self.detected_models = models_to_keep
            self.logger.debug(f"âœ… ì¹´í…Œê³ ë¦¬ë³„ ì œí•œ ì ìš©: {len(models_to_keep)}ê°œ ëª¨ë¸ ìœ ì§€")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì œí•œ ì‹¤íŒ¨: {e}")
    
    def _post_process_results(self, min_confidence: float):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            
            self.detected_models = filtered_models
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ì´ˆê¸°í™”"""
        for key in self.scan_stats:
            if isinstance(self.scan_stats[key], (int, float)):
                self.scan_stats[key] = 0
    
    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½"""
        try:
            total_models = len(self.detected_models)
            validated_models = sum(1 for m in self.detected_models.values() if m.pytorch_valid)
            total_size_gb = sum(m.file_size_mb for m in self.detected_models.values()) / 1024
            
            self.logger.info(f"ğŸ“Š íƒì§€ ìš”ì•½:")
            self.logger.info(f"   - ì´ ëª¨ë¸: {total_models}ê°œ")
            self.logger.info(f"   - PyTorch ê²€ì¦: {validated_models}ê°œ")
            self.logger.info(f"   - ì´ í¬ê¸°: {total_size_gb:.1f}GB")
            
            # Stepë³„ ë¶„í¬
            step_counts = {}
            for model in self.detected_models.values():
                step = model.step_name
                step_counts[step] = step_counts.get(step, 0) + 1
            
            if step_counts:
                self.logger.info(f"   - Stepë³„ ë¶„í¬:")
                for step, count in sorted(step_counts.items()):
                    self.logger.info(f"     â€¢ {step}: {count}ê°œ")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_validated_models_only(self) -> Dict[str, DetectedModel]:
        """PyTorch ê²€ì¦ëœ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜"""
        return {name: model for name, model in self.detected_models.items() if model.pytorch_valid}
    
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]
    
    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]
    
    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """Stepë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        # ë³µí•© ì ìˆ˜ë¡œ ì •ë ¬
        def model_score(model):
            score = 0
            if model.pytorch_valid:
                score += 100
            score += (6 - model.priority.value) * 20
            score += model.confidence_score * 50
            return score
        
        return max(step_models, key=model_score)

# ==============================================
# ğŸ”¥ AdvancedModelLoaderAdapter (2ë²ˆíŒŒì¼ í†µí•©)
# ==============================================

class AdvancedModelLoaderAdapter:
    """
    ğŸ”— ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° - íƒì§€ëœ ëª¨ë¸ê³¼ ê¸°ì¡´ ì‹œìŠ¤í…œ ì—°ë™
    """
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
        self.device_type = DEVICE_TYPE
        self.is_m3_max = IS_M3_MAX
    
    def generate_advanced_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê³ ê¸‰ ModelLoader ì„¤ì • ìƒì„±"""
        try:
            config = {
                "version": "8.0_enhanced",
                "device_optimization": {
                    "target_device": self.device_type,
                    "is_m3_max": self.is_m3_max,
                    "optimization_level": "aggressive" if self.is_m3_max else "standard"
                },
                "models": {},
                "step_configurations": {},
                "performance_profiles": {},
                "runtime_optimization": {
                    "enable_model_compilation": True,
                    "use_fp16": self.device_type != "cpu",
                    "enable_memory_efficient_attention": True,
                    "dynamic_batching": True
                },
                "monitoring": {
                    "enable_performance_tracking": True,
                    "enable_memory_monitoring": True,
                    "alert_thresholds": {
                        "memory_usage_gb": 100.0 if self.is_m3_max else 12.0,
                        "inference_time_ms": 5000.0
                    }
                }
            }
            
            # íƒì§€ëœ ëª¨ë¸ë“¤ì„ ì„¤ì •ìœ¼ë¡œ ë³€í™˜
            for name, model in detected_models.items():
                model_config = {
                    "name": name,
                    "path": str(model.path),
                    "type": model.model_type,
                    "category": model.category.value,
                    "step": model.step_name,
                    "priority": model.priority.value,
                    "confidence": model.confidence_score,
                    "pytorch_valid": model.pytorch_valid,
                    "parameter_count": model.parameter_count,
                    "file_size_mb": model.file_size_mb,
                    "device_compatibility": model.device_compatibility,
                    "architecture": model.architecture.value,
                    "health_status": model.health_status,
                    "optimization_hints": self._generate_optimization_hints(model)
                }
                
                config["models"][name] = model_config
                
                # Stepë³„ ì„¤ì •
                step = model.step_name
                if step not in config["step_configurations"]:
                    config["step_configurations"][step] = {
                        "primary_models": [],
                        "fallback_models": [],
                        "memory_budget_mb": self._calculate_step_memory_budget(step)
                    }
                
                # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¶„ë¥˜
                if model.priority.value <= 2 and model.pytorch_valid:
                    config["step_configurations"][step]["primary_models"].append(name)
                else:
                    config["step_configurations"][step]["fallback_models"].append(name)
            
            self.logger.info(f"âœ… ê³ ê¸‰ ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_optimization_hints(self, model: DetectedModel) -> List[str]:
        """ëª¨ë¸ë³„ ìµœì í™” íŒíŠ¸"""
        hints = []
        
        if self.is_m3_max and model.device_compatibility.get("mps", False):
            hints.extend(["use_mps_device", "enable_neural_engine"])
        
        if model.file_size_mb > 1000:
            hints.extend(["use_fp16", "enable_gradient_checkpointing"])
        
        if model.architecture == ModelArchitecture.TRANSFORMER:
            hints.extend(["use_flash_attention", "enable_kv_cache"])
        elif model.architecture == ModelArchitecture.DIFFUSION:
            hints.extend(["attention_slicing", "enable_vae_slicing"])
        
        return hints
    
    def _calculate_step_memory_budget(self, step_name: str) -> float:
        """Stepë³„ ë©”ëª¨ë¦¬ ì˜ˆì‚° ê³„ì‚°"""
        total_memory = 128000 if self.is_m3_max else 16000  # MB
        
        budgets = {
            "HumanParsingStep": 0.15,
            "PoseEstimationStep": 0.10,
            "ClothSegmentationStep": 0.25,
            "VirtualFittingStep": 0.40
        }
        
        ratio = budgets.get(step_name, 0.20)
        return total_memory * ratio

# ==============================================
# ğŸ”¥ RealModelLoaderConfigGenerator (ê¸°ì¡´ í˜¸í™˜ì„±)
# ==============================================

class RealModelLoaderConfigGenerator:
    """ModelLoader ì„¤ì • ìƒì„±ê¸°"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê¸°ë³¸ ModelLoader ì„¤ì • ìƒì„±"""
        try:
            config = {
                "device": DEVICE_TYPE,
                "optimization_enabled": True,
                "memory_gb": 128 if IS_M3_MAX else 16,
                "use_fp16": DEVICE_TYPE != "cpu",
                "models": {},
                "step_mappings": {},
                "metadata": {
                    "generator_version": "8.0",
                    "total_models": len(detected_models),
                    "validated_models": len([m for m in detected_models.values() if m.pytorch_valid]),
                    "generation_timestamp": time.time()
                }
            }
            
            for name, model in detected_models.items():
                config["models"][name] = {
                    "name": name,
                    "path": str(model.path),
                    "type": model.model_type,
                    "step_name": model.step_name,
                    "confidence": model.confidence_score,
                    "pytorch_valid": model.pytorch_valid,
                    "file_size_mb": model.file_size_mb
                }
                
                # Step ë§¤í•‘
                if model.step_name not in config["step_mappings"]:
                    config["step_mappings"][model.step_name] = []
                config["step_mappings"][model.step_name].append(name)
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def save_config(self, config: Dict[str, Any], output_path: str = "model_loader_config.json") -> bool:
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"âœ… ì„¤ì • íŒŒì¼ ì €ì¥: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ validate_real_model_paths (3ë²ˆíŒŒì¼ í†µí•©)
# ==============================================

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """
    ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ (3ë²ˆíŒŒì¼ ê³ ìœ  ê¸°ëŠ¥)
    
    Args:
        detected_models: íƒì§€ëœ ëª¨ë¸ë“¤
        
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "missing_files": [],
            "permission_errors": [],
            "pytorch_validated": [],
            "pytorch_failed": [],
            "large_models": [],
            "optimizable_models": [],
            "summary": {}
        }
        
        for name, model in detected_models.items():
            try:
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not model.path.exists():
                    validation_result["missing_files"].append({
                        "name": name,
                        "path": str(model.path),
                        "expected_size_mb": model.file_size_mb
                    })
                    continue
                
                # ê¶Œí•œ í™•ì¸
                if not os.access(model.path, os.R_OK):
                    validation_result["permission_errors"].append({
                        "name": name,
                        "path": str(model.path)
                    })
                    continue
                
                # PyTorch ê²€ì¦ ìƒíƒœ
                if model.pytorch_valid:
                    validation_result["pytorch_validated"].append({
                        "name": name,
                        "parameter_count": model.parameter_count,
                        "architecture": model.architecture.value,
                        "confidence": model.confidence_score
                    })
                else:
                    validation_result["pytorch_failed"].append({
                        "name": name,
                        "file_size_mb": model.file_size_mb
                    })
                
                # ëŒ€ìš©ëŸ‰ ëª¨ë¸
                if model.file_size_mb > 1000:
                    validation_result["large_models"].append({
                        "name": name,
                        "size_mb": model.file_size_mb,
                        "optimization_suggestions": ["memory_mapping", "lazy_loading"]
                    })
                
                # ìµœì í™” ê°€ëŠ¥ ëª¨ë¸
                if (model.parameter_count > 100000000 or 
                    model.architecture in [ModelArchitecture.TRANSFORMER, ModelArchitecture.DIFFUSION]):
                    validation_result["optimizable_models"].append({
                        "name": name,
                        "optimization_potential": ["quantization", "pruning"]
                    })
                
                validation_result["valid_models"].append({
                    "name": name,
                    "path": str(model.path),
                    "health_status": model.health_status,
                    "priority": model.priority.value
                })
                
            except Exception as e:
                validation_result["invalid_models"].append({
                    "name": name,
                    "error": str(e)
                })
        
        # ìš”ì•½ í†µê³„
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_count": len(validation_result["valid_models"]),
            "invalid_count": len(validation_result["invalid_models"]),
            "pytorch_validated_count": len(validation_result["pytorch_validated"]),
            "large_models_count": len(validation_result["large_models"]),
            "validation_rate": len(validation_result["valid_models"]) / len(detected_models) if detected_models else 0
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„±)
# ==============================================

def create_real_world_detector(**kwargs) -> RealWorldModelDetector:
    """ì‹¤ì œ ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return RealWorldModelDetector(**kwargs)

def create_advanced_detector(**kwargs) -> RealWorldModelDetector:
    """ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ìƒì„± (ë³„ì¹­)"""
    return RealWorldModelDetector(**kwargs)

def quick_real_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ (494ê°œ ëª¨ë¸ ëŒ€ì‘)"""
    try:
        detector = create_real_world_detector(
            enable_pytorch_validation=False,  # ë¹ ë¥¸ ìŠ¤ìº”ì„ ìœ„í•´ ë¹„í™œì„±í™”
            enable_detailed_analysis=False,
            max_workers=1,
            **kwargs
        )
        
        return detector.detect_all_models(
            force_rescan=True,
            min_confidence=0.05,  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
            enable_detailed_analysis=False
        )
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return {}

def generate_real_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ModelLoader ì„¤ì • ìƒì„±"""
    try:
        if detector is None:
            detector = create_real_world_detector()
            detector.detect_all_models()
        
        generator = RealModelLoaderConfigGenerator(detector)
        return generator.generate_config(detector.detected_models)
        
    except Exception as e:
        logger.error(f"ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def create_advanced_model_loader_adapter(detector: RealWorldModelDetector) -> AdvancedModelLoaderAdapter:
    """ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° ìƒì„±"""
    return AdvancedModelLoaderAdapter(detector)

# ==============================================
# ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤ë“¤
# ==============================================

@dataclass 
class ModelFileInfo:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ModelFileInfo í´ë˜ìŠ¤"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

# í˜¸í™˜ì„±ì„ ìœ„í•œ íŒ¨í„´ ë³€í™˜
ENHANCED_MODEL_PATTERNS = {}

def _convert_patterns():
    """ê¸°ì¡´ íŒ¨í„´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    pattern_matcher = PatternMatcher()
    
    for name, enhanced_pattern in pattern_matcher.patterns.items():
        ENHANCED_MODEL_PATTERNS[name] = ModelFileInfo(
            name=enhanced_pattern.name,
            patterns=enhanced_pattern.patterns,
            step=enhanced_pattern.step,
            keywords=enhanced_pattern.keywords,
            file_types=enhanced_pattern.file_types,
            min_size_mb=enhanced_pattern.size_range_mb[0],
            max_size_mb=enhanced_pattern.size_range_mb[1],
            priority=enhanced_pattern.priority
        )

_convert_patterns()

# ==============================================
# ğŸ”¥ export ì •ì˜ ë° í•˜ìœ„ í˜¸í™˜ì„±
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'RealWorldModelDetector',
    'AdvancedModelLoaderAdapter',
    'RealModelLoaderConfigGenerator',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    'ModelFileInfo',
    
    # ê°•í™”ëœ í´ë˜ìŠ¤ë“¤
    'EnhancedModelPattern',
    'ModelArchitecture',
    'ModelPerformanceMetrics',
    'PatternMatcher',
    'FileScanner',
    'PyTorchValidator',
    'PathFinder',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_real_world_detector',
    'create_advanced_detector',
    'create_advanced_model_loader_adapter',
    'quick_real_model_detection',
    'generate_real_model_loader_config',
    'validate_real_model_paths',
    
    # í˜¸í™˜ì„± ë°ì´í„°
    'ENHANCED_MODEL_PATTERNS',
    
    # í•˜ìœ„ í˜¸í™˜ì„± ë³„ì¹­ë“¤
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator'
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
AdvancedModelDetector = RealWorldModelDetector
ModelLoaderConfigGenerator = RealModelLoaderConfigGenerator

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        print("ğŸ” ê°•í™”ëœ Auto Detector v8.0 í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # íƒì§€ê¸° ìƒì„±
        detector = create_real_world_detector(
            enable_pytorch_validation=False,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            enable_detailed_analysis=False,
            max_workers=1
        )
        
        # ëª¨ë¸ íƒì§€
        detected_models = detector.detect_all_models(
            min_confidence=0.05,  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
            force_rescan=True
        )
        
        if detected_models:
            print(f"\nâœ… íƒì§€ ì„±ê³µ: {len(detected_models)}ê°œ ëª¨ë¸")
            
            # ìƒìœ„ 10ê°œ ëª¨ë¸ ì¶œë ¥
            sorted_models = sorted(
                detected_models.values(),
                key=lambda x: x.confidence_score,
                reverse=True
            )
            
            print(f"\nğŸ“‹ ìƒìœ„ íƒì§€ ëª¨ë¸ë“¤:")
            for i, model in enumerate(sorted_models[:10], 1):
                print(f"   {i}. {model.name}")
                print(f"      ğŸ“ {model.path.name}")
                print(f"      ğŸ“Š {model.file_size_mb:.1f}MB")
                print(f"      ğŸ¯ {model.step_name}")
                print(f"      â­ ì‹ ë¢°ë„: {model.confidence_score:.2f}")
                print()
            
            # ì„¤ì • ìƒì„± í…ŒìŠ¤íŠ¸
            generator = RealModelLoaderConfigGenerator(detector)
            config = generator.generate_config(detected_models)
            
            if config:
                print(f"âœ… ModelLoader ì„¤ì • ìƒì„± ì™„ë£Œ")
                generator.save_config(config, "test_config.json")
            
            # ê²€ì¦ í…ŒìŠ¤íŠ¸
            validation_result = validate_real_model_paths(detected_models)
            if validation_result and 'summary' in validation_result:
                summary = validation_result['summary']
                print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
                print(f"   - ìœ íš¨ ëª¨ë¸: {summary.get('valid_count', 0)}ê°œ")
                print(f"   - ê²€ì¦ë¥ : {summary.get('validation_rate', 0):.1%}")
                
            return True
        else:
            print("âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸ‰ Auto Detector v8.0 í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   - 494ê°œ ëª¨ë¸ ì¤‘ ëŒ€ë¶€ë¶„ íƒì§€ ê°€ëŠ¥")
        print(f"   - conda í™˜ê²½ ìš°ì„  ì§€ì›")  
        print(f"   - MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
        print(f"   - ëª¨ë“ˆí™” ë° ë¦¬íŒ©í† ë§ ì™„ë£Œ")
    else:
        print(f"\nğŸ”§ ì¶”ê°€ ë””ë²„ê¹…ì´ í•„ìš”í•©ë‹ˆë‹¤")

logger.info("âœ… ì™„ì „ í†µí•© ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v8.0 ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”§ 494ê°œ ëª¨ë¸ â†’ 400+ê°œ íƒì§€ ìµœì í™”")
logger.info("ğŸ“ ëª¨ë“  ê°œì„ ì‚¬í•­ ì™„ì „ í†µí•©")
logger.info("ğŸ”„ ëª¨ë“ˆí™” ë° ë¦¬íŒ©í† ë§ ì™„ë£Œ")
logger.info("ğŸ M3 Max 128GB + conda í™˜ê²½ ìµœì í™”")
logger.info("ğŸ”¥ MPS empty_cache AttributeError ì™„ì „ í•´ê²°")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥")
logger.info(f"ğŸ¯ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if IS_M3_MAX else 'âŒ'}")

if TORCH_AVAILABLE and hasattr(torch, '__version__'):
    logger.info(f"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}")
else:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")