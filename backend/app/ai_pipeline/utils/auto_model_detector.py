# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ë™ì  ìë™ ëª¨ë¸ íƒì§€ê¸° v3.0 (ì™„ì „ ì¬ì„¤ê³„)
================================================================================
âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ 100% ë™ì  íƒì§€
âœ… ultra_models í´ë”ê¹Œì§€ ì™„ì „ ì»¤ë²„
âœ… conda í™˜ê²½ íŠ¹í™” ìºì‹œ ì „ëµ
âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜ - ë‹¤ë¥¸ íŒŒì¼ ìˆ˜ì • ë¶ˆí•„ìš”
âœ… Step01ì—ì„œ ìš”ì²­í•˜ëŠ” ëª…ì¹­ ê·¸ëŒ€ë¡œ ë§¤í•‘
âœ… í•˜ë“œì½”ë”© ì œê±°, ì™„ì „ ë™ì  ë§¤í•‘
âœ… M3 Max 128GB ìµœì í™”
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # ë¡œê·¸ ë…¸ì´ì¦ˆ ìµœì†Œí™”

# ==============================================
# ğŸ”¥ 1. í•µì‹¬ ë°ì´í„° êµ¬ì¡° (ê¸°ì¡´ í˜¸í™˜ì„± 100% ìœ ì§€)
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    AUXILIARY = "auxiliary"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± + ë™ì  ë§¤í•‘ ì •ë³´)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ModelLoader í•µì‹¬ ìš”êµ¬ì‚¬í•­
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    
    # ğŸ”¥ ë™ì  ë§¤í•‘ ì •ë³´
    checkpoint_path: Optional[str] = None
    checkpoint_validated: bool = False
    original_filename: str = ""
    matched_patterns: List[str] = field(default_factory=list)
    step_mapping_confidence: float = 0.0
    alternative_step_assignments: List[str] = field(default_factory=list)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device_compatible: bool = True
    recommended_device: str = "cpu"
    precision: str = "fp32"
    
    # Stepë³„ ì„¤ì •
    step_config: Dict[str, Any] = field(default_factory=dict)
    loading_config: Dict[str, Any] = field(default_factory=dict)
    optimization_config: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ModelLoader í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            # ê¸°ë³¸ ì •ë³´
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "size_mb": self.file_size_mb,
            "model_type": self.model_type,
            "step_class": self.step_name,
            "confidence": self.confidence_score,
            "loaded": False,
            
            # ğŸ”¥ ë™ì  ë§¤í•‘ ì •ë³´
            "original_filename": self.original_filename,
            "matched_patterns": self.matched_patterns,
            "step_mapping_confidence": self.step_mapping_confidence,
            "alternative_step_assignments": self.alternative_step_assignments,
            
            # ê²€ì¦ ì •ë³´
            "pytorch_valid": self.pytorch_valid,
            "parameter_count": self.parameter_count,
            "checkpoint_validated": self.checkpoint_validated,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device_config": {
                "recommended_device": self.recommended_device,
                "precision": self.precision,
                "device_compatible": self.device_compatible
            },
            
            # Stepë³„ ì„¤ì •
            "step_config": self.step_config,
            "loading_config": self.loading_config,
            "optimization_config": self.optimization_config,
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.file_extension,
                "last_modified": self.last_modified
            }
        }

# ==============================================
# ğŸ”¥ 2. ì™„ì „ ë™ì  ê²½ë¡œ íƒì§€ ì‹œìŠ¤í…œ
# ==============================================

class DynamicPathDiscovery:
    """ğŸ”¥ ì™„ì „ ë™ì  ê²½ë¡œ íƒì§€ê¸° - ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DynamicPathDiscovery")
        self.project_root = self._find_project_root()
        self.ai_models_root = self.project_root / "backend" / "ai_models"
        
    def _find_project_root(self) -> Path:
        """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë™ì  íƒì§€"""
        current = Path(__file__).resolve()
        
        # mycloset-ai ë””ë ‰í† ë¦¬ ì°¾ê¸°
        for _ in range(10):
            if current.name == 'mycloset-ai' or (current / 'backend').exists():
                return current
            if current.parent == current:
                break
            current = current.parent
        
        # í´ë°±: backend ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •
        current = Path(__file__).resolve()
        for _ in range(10):
            if current.name == 'backend':
                return current.parent
            if current.parent == current:
                break
            current = current.parent
        
        # ìµœì¢… í´ë°±: í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒìœ„ ê²½ë¡œ
        return Path(__file__).resolve().parent.parent.parent.parent
    
    def discover_all_paths(self) -> List[Path]:
        """ğŸ”¥ ëª¨ë“  AI ëª¨ë¸ ê²½ë¡œ ë™ì  íƒì§€"""
        paths = set()
        
        if not self.ai_models_root.exists():
            self.logger.warning(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ ì—†ìŒ: {self.ai_models_root}")
            return []
        
        self.logger.info(f"ğŸ” AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        
        # ğŸ”¥ 1ë‹¨ê³„: ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒì§€
        for item in self.ai_models_root.rglob("*"):
            if item.is_dir():
                paths.add(item)
        
        # ğŸ”¥ 2ë‹¨ê³„: íŠ¹ë³„íˆ ì¤‘ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        priority_dirs = [
            "step_01_human_parsing", "step_02_pose_estimation", "step_03_cloth_segmentation",
            "step_04_geometric_matching", "step_05_cloth_warping", "step_06_virtual_fitting",
            "step_07_post_processing", "step_08_quality_assessment",
            "ultra_models", "checkpoints", "organized", "ai_models2",
            "Graphonomy", "openpose", "OOTDiffusion", "HR-VITON", "u2net",
            "clip_vit_large", "idm_vton", "fashion_clip", "sam2_large"
        ]
        
        for priority_dir in priority_dirs:
            potential_path = self.ai_models_root / priority_dir
            if potential_path.exists():
                paths.add(potential_path)
                # í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ì¶”ê°€
                for sub_item in potential_path.rglob("*"):
                    if sub_item.is_dir():
                        paths.add(sub_item)
        
        # ğŸ”¥ 3ë‹¨ê³„: conda í™˜ê²½ë³„ ì¶”ê°€ ê²½ë¡œ
        conda_prefix = os.environ.get('CONDA_PREFIX')
        if conda_prefix:
            conda_models = Path(conda_prefix) / 'models'
            if conda_models.exists():
                paths.add(conda_models)
        
        # ğŸ”¥ 4ë‹¨ê³„: ìºì‹œ ë””ë ‰í† ë¦¬ë“¤
        cache_dirs = [
            Path.home() / ".cache" / "huggingface" / "hub",
            Path.home() / ".cache" / "torch" / "hub",
            self.ai_models_root / "cache",
            self.ai_models_root / "huggingface_cache"
        ]
        
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                paths.add(cache_dir)
        
        sorted_paths = sorted(paths)
        self.logger.info(f"âœ… íƒì§€ëœ ê²½ë¡œ: {len(sorted_paths)}ê°œ")
        
        # ìƒìœ„ 10ê°œ ê²½ë¡œ ë¡œê¹…
        for i, path in enumerate(sorted_paths[:10]):
            self.logger.debug(f"   {i+1:2d}. {path}")
        
        return sorted_paths

# ==============================================
# ğŸ”¥ 3. ë™ì  Step ë§¤í•‘ ì‹œìŠ¤í…œ (í•˜ë“œì½”ë”© ì™„ì „ ì œê±°)
# ==============================================

class DynamicStepMapper:
    """ğŸ”¥ ë™ì  Step ë§¤í•‘ ì‹œìŠ¤í…œ - Step01 ìš”ì²­ëª… â†’ ì‹¤ì œ íŒŒì¼ ì—°ê²°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DynamicStepMapper")
        
        # ğŸ”¥ Step01 ë“±ì—ì„œ ìš”ì²­í•˜ëŠ” ì‹¤ì œ ëª…ì¹­ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„±)
        self.step_request_patterns = self._build_dynamic_patterns()
        
    def _build_dynamic_patterns(self) -> Dict[str, Dict]:
        """ë™ì ìœ¼ë¡œ íŒ¨í„´ êµ¬ì¶•"""
        return {
            "HumanParsingStep": {
                "category": ModelCategory.HUMAN_PARSING,
                "priority": ModelPriority.CRITICAL,
                
                # Step01ì—ì„œ ì‹¤ì œ ìš”ì²­í•˜ëŠ” ëª…ì¹­ë“¤
                "request_names": [
                    "human_parsing_graphonomy",
                    "human_parsing_schp_atr", 
                    "graphonomy",
                    "schp_atr"
                ],
                
                # ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ë“¤ (ë™ì  íƒì§€)
                "filename_patterns": [
                    r".*graphonomy.*\.pth$",
                    r".*exp-schp.*atr.*\.pth$",
                    r".*schp.*\.pth$",
                    r".*atr.*\.pth$",
                    r".*parsing.*\.pth$"
                ],
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                "keywords": ["graphonomy", "schp", "atr", "parsing", "human"],
                "size_range": (50, 4000),
                "step_config": {
                    "input_size": [3, 512, 512],
                    "num_classes": 20,
                    "preprocessing": "normalize"
                }
            },
            
            "PoseEstimationStep": {
                "category": ModelCategory.POSE_ESTIMATION,
                "priority": ModelPriority.HIGH,
                
                "request_names": [
                    "pose_estimation_openpose",
                    "openpose",
                    "body_pose_model"
                ],
                
                "filename_patterns": [
                    r".*openpose.*\.pth$",
                    r".*body.*pose.*\.pth$",
                    r".*pose.*model.*\.pth$",
                    r".*pose.*\.pth$"
                ],
                
                "keywords": ["openpose", "pose", "body", "keypoint"],
                "size_range": (100, 4000),
                "step_config": {
                    "input_size": [3, 256, 192],
                    "num_keypoints": 17,
                    "preprocessing": "pose_normalize"
                }
            },
            
            "ClothSegmentationStep": {
                "category": ModelCategory.CLOTH_SEGMENTATION,
                "priority": ModelPriority.CRITICAL,
                
                "request_names": [
                    "cloth_segmentation_u2net",
                    "u2net",
                    "sam_vit_h",
                    "segment_anything"
                ],
                
                "filename_patterns": [
                    r".*u2net.*\.pth$",
                    r".*sam.*vit.*\.pth$",
                    r".*sam_vit_h_4b8939\.pth$",
                    r".*segment.*\.pth$"
                ],
                
                "keywords": ["u2net", "sam", "segmentation", "cloth", "segment"],
                "size_range": (100, 3000),
                "step_config": {
                    "input_size": [3, 320, 320],
                    "mask_threshold": 0.5,
                    "preprocessing": "u2net_normalize"
                }
            },
            
            "VirtualFittingStep": {
                "category": ModelCategory.VIRTUAL_FITTING,
                "priority": ModelPriority.CRITICAL,
                
                "request_names": [
                    "virtual_fitting_diffusion",
                    "pytorch_model",
                    "stable_diffusion",
                    "ootd_diffusion",
                    "hrviton"
                ],
                
                "filename_patterns": [
                    r".*pytorch_model\.bin$",
                    r".*diffusion.*\.bin$",
                    r".*diffusion.*\.safetensors$",
                    r".*v1-5-pruned.*\.ckpt$",
                    r".*unet.*\.bin$",
                    r".*vae.*\.safetensors$",
                    r".*ootd.*\.pth$",
                    r".*hrviton.*\.pth$"
                ],
                
                "keywords": ["pytorch_model", "diffusion", "stable", "ootd", "hrviton", "unet", "vae"],
                "size_range": (500, 8000),
                "step_config": {
                    "input_size": [3, 512, 512],
                    "guidance_scale": 7.5,
                    "num_inference_steps": 20,
                    "enable_attention_slicing": True
                }
            },
            
            "GeometricMatchingStep": {
                "category": ModelCategory.GEOMETRIC_MATCHING,
                "priority": ModelPriority.MEDIUM,
                
                "request_names": [
                    "geometric_matching_gmm",
                    "gmm",
                    "geometric_matching"
                ],
                
                "filename_patterns": [
                    r".*gmm.*\.pth$",
                    r".*geometric.*\.pth$",
                    r".*tps.*\.pth$",
                    r".*matching.*\.pth$"
                ],
                
                "keywords": ["gmm", "geometric", "matching", "tps"],
                "size_range": (20, 500),
                "step_config": {"input_size": [6, 256, 192]}
            },
            
            "ClothWarpingStep": {
                "category": ModelCategory.CLOTH_WARPING,
                "priority": ModelPriority.MEDIUM,
                
                "request_names": [
                    "cloth_warping_tom",
                    "tom",
                    "cloth_warping"
                ],
                
                "filename_patterns": [
                    r".*tom.*\.pth$",
                    r".*warping.*\.pth$",
                    r".*cloth.*warp.*\.pth$"
                ],
                
                "keywords": ["tom", "warping", "cloth", "warp"],
                "size_range": (50, 1000),
                "step_config": {"input_size": [6, 256, 192]}
            },
            
            "PostProcessingStep": {
                "category": ModelCategory.POST_PROCESSING,
                "priority": ModelPriority.LOW,
                
                "request_names": [
                    "post_processing_enhance",
                    "enhancement",
                    "super_resolution"
                ],
                
                "filename_patterns": [
                    r".*enhancement.*\.pth$",
                    r".*post.*process.*\.pth$",
                    r".*super.*resolution.*\.pth$",
                    r".*esrgan.*\.pth$"
                ],
                
                "keywords": ["enhancement", "post", "process", "super", "resolution", "esrgan"],
                "size_range": (10, 500),
                "step_config": {"input_size": [3, 512, 512]}
            },
            
            "QualityAssessmentStep": {
                "category": ModelCategory.QUALITY_ASSESSMENT,
                "priority": ModelPriority.HIGH,
                
                "request_names": [
                    "quality_assessment_clip",
                    "clip_g",
                    "perceptual_quality_model",
                    "aesthetic_quality_model"
                ],
                
                "filename_patterns": [
                    r".*clip.*\.pth$",
                    r".*clip.*\.bin$",
                    r".*quality.*\.pth$",
                    r".*assessment.*\.pth$",
                    r".*perceptual.*\.pth$",
                    r".*aesthetic.*\.pth$"
                ],
                
                "keywords": ["clip", "quality", "assessment", "perceptual", "aesthetic"],
                "size_range": (50, 6000),
                "step_config": {
                    "input_size": [3, 224, 224],
                    "quality_metrics": ["lpips", "fid", "clip_score"]
                }
            }
        }
    
    def match_file_to_step(self, file_path: Path) -> Optional[Tuple[str, float, Dict, List[str]]]:
        """ğŸ”¥ íŒŒì¼ì„ Stepì— ë™ì  ë§¤í•‘"""
        file_name = file_path.name.lower()
        path_str = str(file_path).lower()
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
        except:
            file_size_mb = 0
        
        best_match = None
        best_confidence = 0
        
        for step_name, config in self.step_request_patterns.items():
            confidence, matched_patterns = self._calculate_confidence(
                file_path, file_name, path_str, file_size_mb, config
            )
            
            if confidence > best_confidence and confidence > 0.2:  # ê´€ëŒ€í•œ ì„ê³„ê°’
                best_match = (step_name, confidence, config, matched_patterns)
                best_confidence = confidence
        
        return best_match
    
    def _calculate_confidence(self, file_path: Path, file_name: str, path_str: str, 
                            file_size_mb: float, config: Dict) -> Tuple[float, List[str]]:
        """ğŸ”¥ ë™ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        matched_patterns = []
        
        # ğŸ”¥ 1. ìš”ì²­ëª… ì§ì ‘ ë§¤ì¹­ (90% ê°€ì¤‘ì¹˜)
        request_names = config.get("request_names", [])
        for request_name in request_names:
            if request_name.lower() in file_name or request_name.lower() in path_str:
                confidence += 0.9
                matched_patterns.append(f"request_name:{request_name}")
                self.logger.debug(f"ğŸ¯ ìš”ì²­ëª… ë§¤ì¹­: {file_name} â†’ {request_name}")
                break
        
        # ğŸ”¥ 2. íŒŒì¼ëª… íŒ¨í„´ ë§¤ì¹­ (70% ê°€ì¤‘ì¹˜)
        filename_patterns = config.get("filename_patterns", [])
        for pattern in filename_patterns:
            try:
                if re.search(pattern, file_name, re.IGNORECASE):
                    confidence += 0.7
                    matched_patterns.append(f"pattern:{pattern}")
                    break
            except:
                continue
        
        # ğŸ”¥ 3. í‚¤ì›Œë“œ ë§¤ì¹­ (50% ê°€ì¤‘ì¹˜)
        keywords = config.get("keywords", [])
        keyword_matches = sum(1 for keyword in keywords 
                             if keyword in file_name or keyword in path_str)
        if keywords:
            keyword_score = 0.5 * (keyword_matches / len(keywords))
            confidence += keyword_score
            if keyword_matches > 0:
                matched_patterns.append(f"keywords:{keyword_matches}/{len(keywords)}")
        
        # ğŸ”¥ 4. íŒŒì¼ í¬ê¸° ê²€ì¦ (30% ê°€ì¤‘ì¹˜)
        size_range = config.get("size_range", (1, 10000))
        min_size, max_size = size_range
        if min_size <= file_size_mb <= max_size:
            confidence += 0.3
            matched_patterns.append(f"size:{file_size_mb:.1f}MB")
        elif file_size_mb > min_size * 0.3:  # ë§¤ìš° ê´€ëŒ€í•œ í¬ê¸° ì²´í¬
            confidence += 0.15
            matched_patterns.append(f"size_partial:{file_size_mb:.1f}MB")
        
        # ğŸ”¥ 5. ê²½ë¡œ íŒíŠ¸ (20% ë³´ë„ˆìŠ¤)
        if 'backend' in path_str and 'ai_models' in path_str:
            confidence += 0.2
            matched_patterns.append("path:backend/ai_models")
        
        # ğŸ”¥ 6. Step í´ë” íŒíŠ¸ (15% ë³´ë„ˆìŠ¤)
        step_indicators = ['step_01', 'step_02', 'step_03', 'step_04', 
                          'step_05', 'step_06', 'step_07', 'step_08']
        for indicator in step_indicators:
            if indicator in path_str:
                confidence += 0.15
                matched_patterns.append(f"step_folder:{indicator}")
                break
        
        # ğŸ”¥ 7. Ultra ëª¨ë¸ ë³´ë„ˆìŠ¤ (10% ë³´ë„ˆìŠ¤)
        if 'ultra_models' in path_str:
            confidence += 0.1
            matched_patterns.append("ultra_models")
        
        return min(confidence, 1.0), matched_patterns

# ==============================================
# ğŸ”¥ 4. ë©”ì¸ ë™ì  íƒì§€ê¸° í´ë˜ìŠ¤ (ì™„ì „ ì¬ì„¤ê³„)
# ==============================================

class RealWorldModelDetector:
    """ğŸ”¥ ì™„ì „ ë™ì  ëª¨ë¸ íƒì§€ê¸° v3.0 - ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜"""
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        self.detected_models: Dict[str, DetectedModel] = {}
        
        # ğŸ”¥ ë™ì  êµ¬ì„± ìš”ì†Œë“¤
        self.path_discovery = DynamicPathDiscovery()
        self.step_mapper = DynamicStepMapper()
        
        # íƒì§€ ì„¤ì •
        self.search_paths = kwargs.get('search_paths') or self.path_discovery.discover_all_paths()
        self.enable_pytorch_validation = kwargs.get('enable_pytorch_validation', False)
        self.include_ultra_models = kwargs.get('include_ultra_models', True)
        
        # ì‹œìŠ¤í…œ ê°ì§€
        self.is_m3_max = self._detect_m3_max()
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # ë™ì  ë§¤í•‘ í†µê³„
        self.mapping_stats = {
            "total_files_scanned": 0,
            "request_name_mappings": 0,
            "pattern_mappings": 0,
            "keyword_mappings": 0,
            "ultra_models_found": 0,
            "unmapped_files": 0,
            "dynamic_discoveries": 0
        }
        
        self.logger.info(f"ğŸ” ë™ì  RealWorldModelDetector v3.0 ì´ˆê¸°í™”")
        self.logger.info(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.path_discovery.project_root}")
        self.logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {self.path_discovery.ai_models_root}")
        self.logger.info(f"   ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            return 'arm64' in str(os.uname()) if hasattr(os, 'uname') else False
        except:
            return False
    
    def detect_all_models(self, **kwargs) -> Dict[str, DetectedModel]:
        """ğŸ”¥ ì™„ì „ ë™ì  ëª¨ë“  ëª¨ë¸ íƒì§€"""
        start_time = time.time()
        self.detected_models.clear()
        self.mapping_stats = {k: 0 for k in self.mapping_stats.keys()}
        
        # ğŸ”¥ 1ë‹¨ê³„: ëª¨ë“  íŒŒì¼ ìŠ¤ìº”
        model_files = self._dynamic_scan_files()
        self.logger.info(f"ğŸ“¦ ë™ì  ìŠ¤ìº” ì™„ë£Œ: {len(model_files)}ê°œ íŒŒì¼")
        
        if not model_files:
            self.logger.warning("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # ğŸ”¥ 2ë‹¨ê³„: ë™ì  ë§¤í•‘ ë° ëª¨ë¸ ìƒì„±
        detected_count = 0
        for file_path in model_files:
            try:
                self.mapping_stats["total_files_scanned"] += 1
                
                # Step ë§¤í•‘ ì‹œë„
                match_result = self.step_mapper.match_file_to_step(file_path)
                if match_result:
                    step_name, confidence, config, matched_patterns = match_result
                    
                    # DetectedModel ìƒì„±
                    model = self._create_dynamic_detected_model(
                        file_path, step_name, confidence, config, matched_patterns
                    )
                    
                    if model:
                        self.detected_models[model.name] = model
                        detected_count += 1
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self._update_mapping_stats(matched_patterns)
                        
                        # Ultra ëª¨ë¸ ì¹´ìš´íŠ¸
                        if 'ultra_models' in str(file_path).lower():
                            self.mapping_stats["ultra_models_found"] += 1
                        
                        if detected_count <= 15:  # ìƒìœ„ 15ê°œë§Œ ë¡œê¹…
                            patterns_str = ", ".join(matched_patterns[:2])
                            self.logger.info(f"âœ… {model.name} â†’ {step_name} ({confidence:.2f}, {patterns_str})")
                else:
                    self.mapping_stats["unmapped_files"] += 1
                    if self.mapping_stats["unmapped_files"] <= 5:  # ìƒìœ„ 5ê°œ ë¯¸ë§¤í•‘ íŒŒì¼ë§Œ ë¡œê¹…
                        self.logger.debug(f"â“ ë¯¸ë§¤í•‘: {file_path.name}")
                        
            except Exception as e:
                self.logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        duration = time.time() - start_time
        
        # ğŸ”¥ ìµœì¢… í†µê³„ ë¡œê¹…
        self._log_dynamic_stats()
        
        self.logger.info(f"ğŸ‰ ë™ì  íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ({duration:.1f}ì´ˆ)")
        
        return self.detected_models
    
    def _dynamic_scan_files(self) -> List[Path]:
        """ğŸ”¥ ë™ì  íŒŒì¼ ìŠ¤ìº”"""
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.onnx'}
        model_files = []
        scanned_files = set()
        
        for search_path in self.search_paths:
            if not search_path.exists():
                continue
                
            try:
                for file_path in search_path.rglob('*'):
                    if (file_path.is_file() and 
                        file_path.suffix.lower() in model_extensions and
                        str(file_path) not in scanned_files):
                        
                        # ì¤‘ë³µ ë°©ì§€
                        scanned_files.add(str(file_path))
                        
                        # AI ëª¨ë¸ íŒŒì¼ ê²€ì¦
                        if self._is_valid_ai_model_file(file_path):
                            model_files.append(file_path)
                            
            except Exception as e:
                self.logger.debug(f"ìŠ¤ìº” ì˜¤ë¥˜ {search_path}: {e}")
                continue
        
        # í¬ê¸°ìˆœ ì •ë ¬ (í° íŒŒì¼ ìš°ì„ )
        def sort_key(file_path):
            try:
                return file_path.stat().st_size
            except:
                return 0
        
        model_files.sort(key=sort_key, reverse=True)
        
        # Ultra ëª¨ë¸ê³¼ ì¼ë°˜ ëª¨ë¸ ë¶„ë¦¬ ë¡œê¹…
        ultra_count = sum(1 for f in model_files if 'ultra_models' in str(f).lower())
        self.logger.info(f"   ì¼ë°˜ ëª¨ë¸: {len(model_files) - ultra_count}ê°œ")
        self.logger.info(f"   Ultra ëª¨ë¸: {ultra_count}ê°œ")
        
        return model_files
    
    def _is_valid_ai_model_file(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ë™ì  ê°œì„ )"""
        try:
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            # í¬ê¸° í•„í„° (ë” ê´€ëŒ€í•˜ê²Œ)
            if file_size_mb < 5:  # 5MB ë¯¸ë§Œ ì œì™¸
                return False
            
            file_name = file_path.name.lower()
            
            # AI í‚¤ì›Œë“œ ì²´í¬ (í™•ì¥)
            ai_keywords = [
                # ê¸°ë³¸ í‚¤ì›Œë“œ
                'model', 'checkpoint', 'weight', 'pytorch_model', 'diffusion',
                # êµ¬ì²´ì  ëª¨ë¸ëª…
                'openpose', 'u2net', 'sam', 'clip', 'graphonomy', 'schp',
                'hrviton', 'ootd', 'gmm', 'tom', 'vae', 'unet',
                # í™•ì¥ìë³„
                'safetensors', 'bin', 'ckpt'
            ]
            
            if any(keyword in file_name for keyword in ai_keywords):
                return True
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ë¬´ì¡°ê±´ í¬í•¨ (Ultra ëª¨ë¸ë“¤)
            if file_size_mb > 50:
                return True
            
            # íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤
            path_str = str(file_path).lower()
            priority_paths = ['step_', 'ultra_models', 'checkpoints', 'graphonomy', 'openpose']
            if any(priority in path_str for priority in priority_paths):
                return True
            
            return False
            
        except Exception:
            return False
    
    def _create_dynamic_detected_model(self, file_path: Path, step_name: str, 
                                     confidence: float, config: Dict, 
                                     matched_patterns: List[str]) -> Optional[DetectedModel]:
        """ğŸ”¥ ë™ì  DetectedModel ìƒì„±"""
        try:
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # ë™ì  ì´ë¦„ ìƒì„± (Step01 ë“±ì—ì„œ ìš”ì²­í•˜ëŠ” ëª…ì¹­ ìš°ì„ )
            request_names = config.get("request_names", [])
            base_name = request_names[0] if request_names else file_path.stem.lower()
            
            # ì¤‘ë³µ ë°©ì§€
            model_name = base_name
            counter = 1
            while model_name in self.detected_models:
                counter += 1
                model_name = f"{base_name}_v{counter}"
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
            recommended_device = "mps" if self.is_m3_max else "cpu"
            precision = "fp16" if self.is_m3_max and file_size_mb > 100 else "fp32"
            
            # ğŸ”¥ ë™ì  DetectedModel ìƒì„±
            model = DetectedModel(
                name=model_name,
                path=file_path,
                category=config["category"],
                model_type=config["category"].value,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=config["priority"],
                step_name=step_name,
                
                # ê²€ì¦ ì •ë³´
                pytorch_valid=False,  # í•„ìš”ì‹œ ê²€ì¦
                parameter_count=0,
                last_modified=file_stat.st_mtime,
                
                # ğŸ”¥ ë™ì  ë§¤í•‘ ì •ë³´
                checkpoint_path=str(file_path),
                checkpoint_validated=False,
                original_filename=file_path.name,
                matched_patterns=matched_patterns,
                step_mapping_confidence=confidence,
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                device_compatible=True,
                recommended_device=recommended_device,
                precision=precision,
                
                # Stepë³„ ì„¤ì •
                step_config=config.get("step_config", {}),
                loading_config={
                    "lazy_loading": file_size_mb > 500,  # Ultra ëª¨ë¸ ê³ ë ¤
                    "memory_mapping": file_size_mb > 2000,
                    "batch_size": 1,
                    "enable_offload": file_size_mb > 4000  # ë§¤ìš° í° ëª¨ë¸ CPU ì˜¤í”„ë¡œë“œ
                },
                optimization_config={
                    "enable_compile": False,
                    "attention_slicing": file_size_mb > 1000,
                    "precision": precision,
                    "enable_xformers": self.is_m3_max and file_size_mb > 2000
                }
            )
            
            return model
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _update_mapping_stats(self, matched_patterns: List[str]):
        """ë§¤í•‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        for pattern in matched_patterns:
            if pattern.startswith("request_name:"):
                self.mapping_stats["request_name_mappings"] += 1
            elif pattern.startswith("pattern:"):
                self.mapping_stats["pattern_mappings"] += 1
            elif pattern.startswith("keywords:"):
                self.mapping_stats["keyword_mappings"] += 1
            elif "ultra_models" in pattern:
                self.mapping_stats["dynamic_discoveries"] += 1
    
    def _log_dynamic_stats(self):
        """ë™ì  ë§¤í•‘ í†µê³„ ë¡œê·¸"""
        stats = self.mapping_stats
        self.logger.info("ğŸ” ë™ì  ë§¤í•‘ í†µê³„:")
        self.logger.info(f"   ğŸ“ ìŠ¤ìº” íŒŒì¼: {stats['total_files_scanned']}ê°œ")
        self.logger.info(f"   ğŸ¯ ìš”ì²­ëª… ë§¤ì¹­: {stats['request_name_mappings']}ê°œ")
        self.logger.info(f"   ğŸ” íŒ¨í„´ ë§¤ì¹­: {stats['pattern_mappings']}ê°œ")
        self.logger.info(f"   ğŸ·ï¸ í‚¤ì›Œë“œ ë§¤ì¹­: {stats['keyword_mappings']}ê°œ")
        self.logger.info(f"   ğŸš€ Ultra ëª¨ë¸: {stats['ultra_models_found']}ê°œ")
        self.logger.info(f"   ğŸ”„ ë™ì  ë°œê²¬: {stats['dynamic_discoveries']}ê°œ")
        self.logger.info(f"   â“ ë¯¸ë§¤í•‘: {stats['unmapped_files']}ê°œ")

# ==============================================
# ğŸ”¥ 5. ë¹ ì§„ í•µì‹¬ í´ë˜ìŠ¤ë“¤ ì¶”ê°€ (ì›ë³¸ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„)
# ==============================================

@dataclass 
class ModelFileInfo:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ModelFileInfo í´ë˜ìŠ¤"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ë¥˜"""
    RESNET = "resnet"
    EFFICIENTNET = "efficientnet"
    VIT = "vision_transformer"
    DIFFUSION = "diffusion"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    RNN = "rnn"
    UNET = "unet"
    GAN = "gan"
    AUTOENCODER = "autoencoder"
    UNKNOWN = "unknown"

@dataclass
class ModelMetadata:
    """ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    framework: str = "pytorch"
    version: str = ""
    training_dataset: str = ""
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    optimization_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)

class AdvancedModelLoaderAdapter:
    """ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_comprehensive_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ ì„¤ì • ìƒì„±"""
        return generate_advanced_model_loader_config(self.detector)

class RealModelLoaderConfigGenerator:
    """ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„±ê¸° (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        config = {
            "version": "real_detector_v1.0",
            "models": {},
            "device": "mps" if self.detector.is_m3_max else "cpu",
            "optimization_enabled": True
        }
        
        for name, model in detected_models.items():
            config["models"][name] = model.to_dict()
        
        return config

# ==============================================
# ğŸ”¥ 6. ModelLoader ë“±ë¡ ê¸°ëŠ¥ë“¤ (í•µì‹¬! - ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)
# ==============================================

def register_detected_models_to_loader(model_loader_instance=None) -> int:
    """íƒì§€ëœ ëª¨ë“  ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        if model_loader_instance is None:
            try:
                # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° import
                from . import model_loader as ml_module
                model_loader_instance = ml_module.get_global_model_loader()
            except ImportError:
                logger.error("âŒ ModelLoader import ì‹¤íŒ¨")
                return 0
        
        # ëª¨ë¸ íƒì§€
        detector = get_global_detector()
        detected_models = detector.detect_all_models()
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return 0
        
        registered_count = 0
        
        for model_name, model_info in detected_models.items():
            try:
                # ModelLoaderìš© ì„¤ì • ìƒì„±
                model_config = create_model_config_for_loader(model_info)
                
                # ModelLoaderì— ë“±ë¡
                if register_single_model_to_loader(model_loader_instance, model_name, model_config):
                    registered_count += 1
                    logger.debug(f"âœ… {model_name} ModelLoader ë“±ë¡ ì„±ê³µ")
                else:
                    logger.warning(f"âš ï¸ {model_name} ModelLoader ë“±ë¡ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {model_name} ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ğŸ‰ ModelLoader ë“±ë¡ ì™„ë£Œ: {registered_count}/{len(detected_models)}ê°œ ëª¨ë¸")
        return registered_count
        
    except Exception as e:
        logger.error(f"âŒ ModelLoader ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return 0

def register_single_model_to_loader(model_loader, model_name: str, model_config: Dict[str, Any]) -> bool:
    """ë‹¨ì¼ ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
    try:
        # ModelLoaderê°€ ê°€ì§€ê³  ìˆëŠ” ë“±ë¡ ë©”ì„œë“œ ì‹œë„
        registration_methods = [
            'register_model',
            'register_model_config', 
            'add_model',
            'load_model_config',
            'set_model_config'
        ]
        
        for method_name in registration_methods:
            if hasattr(model_loader, method_name):
                method = getattr(model_loader, method_name)
                try:
                    # ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ì— ë”°ë¼ í˜¸ì¶œ
                    if method_name == 'register_model_config':
                        result = method(model_name, model_config)
                    else:
                        result = method(model_name, model_config)
                    
                    if result:
                        logger.debug(f"âœ… {model_name} ë“±ë¡ ì„±ê³µ (ë©”ì„œë“œ: {method_name})")
                        return True
                        
                except Exception as e:
                    logger.debug(f"âš ï¸ {method_name} ì‹œë„ ì‹¤íŒ¨: {e}")
                    continue
        
        # ì§ì ‘ ì†ì„± ì„¤ì • ì‹œë„
        if hasattr(model_loader, 'model_configs'):
            model_loader.model_configs[model_name] = model_config
            logger.debug(f"âœ… {model_name} ì§ì ‘ ë“±ë¡ ì„±ê³µ")
            return True
        
        if hasattr(model_loader, 'models'):
            model_loader.models[model_name] = model_config
            logger.debug(f"âœ… {model_name} models ì†ì„± ë“±ë¡ ì„±ê³µ")
            return True
        
        logger.warning(f"âš ï¸ {model_name} ë“±ë¡ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False
        
    except Exception as e:
        logger.error(f"âŒ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def create_model_config_for_loader(model_info: DetectedModel) -> Dict[str, Any]:
    """ModelLoaderìš© ëª¨ë¸ ì„¤ì • ìƒì„±"""
    try:
        # ê¸°ë³¸ ModelConfig êµ¬ì¡°
        config = {
            # ê¸°ë³¸ ì •ë³´
            "name": model_info.name,
            "model_type": model_info.model_type,
            "model_class": f"{model_info.step_name}Model",  # í´ë˜ìŠ¤ëª… ìƒì„±
            "step_name": model_info.step_name,
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ (í•µì‹¬!)
            "checkpoint_path": str(model_info.path),
            "checkpoint_validated": model_info.checkpoint_validated,
            "file_size_mb": model_info.file_size_mb,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device": model_info.recommended_device,
            "precision": model_info.precision,
            "device_compatible": model_info.device_compatible,
            
            # ì…ë ¥/ì¶œë ¥ ì„¤ì •
            "input_size": model_info.step_config.get("input_size", [3, 512, 512]),
            "preprocessing": model_info.step_config.get("preprocessing", "standard"),
            
            # ë¡œë”© ì„¤ì •
            "lazy_loading": model_info.loading_config.get("lazy_loading", False),
            "memory_mapping": model_info.loading_config.get("memory_mapping", False),
            "batch_size": model_info.loading_config.get("batch_size", 1),
            
            # ìµœì í™” ì„¤ì •
            "optimization": model_info.optimization_config,
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "auto_detected": True,
                "confidence": model_info.confidence_score,
                "detection_time": time.time(),
                "priority": model_info.priority.value,
                "pytorch_valid": model_info.pytorch_valid,
                "parameter_count": model_info.parameter_count
            }
        }
        
        # Stepë³„ íŠ¹í™” ì„¤ì • ì¶”ê°€
        step_specific = get_step_specific_loader_config(model_info.step_name, model_info)
        config.update(step_specific)
        
        return config
        
    except Exception as e:
        logger.error(f"âŒ {model_info.name} ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {}

def get_step_specific_loader_config(step_name: str, model_info: DetectedModel) -> Dict[str, Any]:
    """Stepë³„ ModelLoader íŠ¹í™” ì„¤ì •"""
    
    configs = {
        "HumanParsingStep": {
            "num_classes": 20,
            "output_channels": 20,
            "task_type": "segmentation",
            "loss_function": "cross_entropy",
            "metrics": ["accuracy", "iou"]
        },
        
        "PoseEstimationStep": {
            "num_keypoints": 17,
            "heatmap_size": [64, 48],
            "task_type": "keypoint_detection", 
            "sigma": 2.0,
            "metrics": ["pck", "accuracy"]
        },
        
        "ClothSegmentationStep": {
            "num_classes": 2,
            "task_type": "binary_segmentation",
            "threshold": 0.5,
            "apply_morphology": True,
            "metrics": ["iou", "dice"]
        },
        
        "VirtualFittingStep": {
            "model_architecture": "diffusion",
            "scheduler_type": "DDIM",
            "guidance_scale": 7.5,
            "num_inference_steps": 20,
            "enable_attention_slicing": model_info.file_size_mb > 2000,
            "enable_vae_slicing": model_info.file_size_mb > 4000,
            "enable_cpu_offload": model_info.file_size_mb > 8000,
            "metrics": ["fid", "lpips", "quality_score"]
        },
        
        "GeometricMatchingStep": {
            "transformation_type": "TPS",
            "grid_size": [5, 5],
            "task_type": "geometric_transformation",
            "metrics": ["geometric_error", "warping_quality"]
        },
        
        "ClothWarpingStep": {
            "warping_method": "TOM",
            "blending_enabled": True,
            "task_type": "image_warping",
            "metrics": ["warping_error", "visual_quality"]
        }
    }
    
    return configs.get(step_name, {
        "task_type": "general",
        "metrics": ["accuracy"]
    })

# ==============================================
# ğŸ”¥ 7. ê²€ì¦ ë° ì„¤ì • ìƒì„± í•¨ìˆ˜ë“¤ (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)
# ==============================================

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """ëª¨ë¸ ê²½ë¡œ ê²€ì¦"""
    valid_models = []
    invalid_models = []
    
    for name, model in detected_models.items():
        if model.path.exists() and os.access(model.path, os.R_OK):
            valid_models.append({
                "name": name,
                "path": str(model.path),
                "size_mb": model.file_size_mb,
                "step": model.step_name
            })
        else:
            invalid_models.append({
                "name": name,
                "path": str(model.path),
                "error": "File not found or not readable"
            })
    
    return {
        "valid_models": valid_models,
        "invalid_models": invalid_models,
        "summary": {
            "total_models": len(detected_models),
            "valid_count": len(valid_models),
            "invalid_count": len(invalid_models),
            "validation_rate": len(valid_models) / len(detected_models) if detected_models else 0
        }
    }

def generate_real_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ModelLoader ì„¤ì • ìƒì„± (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    if detector is None:
        detector = get_global_detector()
        detector.detect_all_models()
    
    config = {
        "device": "mps" if detector.is_m3_max else "cpu",
        "optimization_enabled": True,
        "use_fp16": detector.is_m3_max,
        "models": {},
        "step_mappings": {},
        "metadata": {
            "generator_version": "core_detector_v1.0",
            "total_models": len(detector.detected_models),
            "generation_timestamp": time.time(),
            "conda_env": detector.conda_env,
            "is_m3_max": detector.is_m3_max
        }
    }
    
    for name, model in detector.detected_models.items():
        config["models"][name] = model.to_dict()
        
        # Step ë§¤í•‘
        if model.step_name not in config["step_mappings"]:
            config["step_mappings"][model.step_name] = []
        config["step_mappings"][model.step_name].append(name)
    
    return config

def create_advanced_model_loader_adapter(detector: RealWorldModelDetector) -> AdvancedModelLoaderAdapter:
    """ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° ìƒì„± (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    return AdvancedModelLoaderAdapter(detector)

# ==============================================
# ğŸ”¥ 8. ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ëª¨ë“  í•¨ìˆ˜ ìœ ì§€)
# ==============================================

def list_available_models(step_class: Optional[str] = None, 
                         model_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    result = []
    for model in models.values():
        model_dict = model.to_dict()
        
        # í•„í„°ë§
        if step_class and model_dict["step_class"] != step_class:
            continue
        if model_type and model_dict["model_type"] != model_type:
            continue
        
        result.append(model_dict)
    
    # ì‹ ë¢°ë„ ìˆœ ì •ë ¬
    result.sort(key=lambda x: x["confidence"], reverse=True)
    return result

def register_step_requirements(step_name: str, requirements: Dict[str, Any]) -> bool:
    """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    try:
        detector = get_global_detector()
        if not hasattr(detector, 'step_requirements'):
            detector.step_requirements = {}
        
        detector.step_requirements[step_name] = requirements
        logger.debug(f"âœ… {step_name} ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"âŒ {step_name} ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def create_step_interface(step_name: str, config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    try:
        models = get_models_for_step(step_name)
        if not models:
            return None
        
        best_model = models[0]
        
        return {
            "step_name": step_name,
            "primary_model": best_model,
            "fallback_models": models[1:3],
            "config": config or {},
            "device": best_model.get("device_config", {}).get("recommended_device", "cpu"),
            "optimization": best_model.get("optimization_config", {}),
            "created_at": time.time()
        }
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def get_models_for_step(step_name: str) -> List[Dict[str, Any]]:
    """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
    models = list_available_models(step_class=step_name)
    return sorted(models, key=lambda x: x["confidence"], reverse=True)

def validate_model_exists(model_name: str) -> bool:
    """ëª¨ë¸ ì¡´ì¬ í™•ì¸"""
    detector = get_global_detector()
    return model_name in detector.detected_models

def generate_advanced_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ğŸ”¥ ë™ì  ModelLoader ì„¤ì • ìƒì„±"""
    try:
        if detector is None:
            detector = get_global_detector()
            detector.detect_all_models()
        
        detected_models = detector.detected_models
        
        # M3 Max ê°ì§€
        is_m3_max = detector.is_m3_max
        device_type = "mps" if is_m3_max else "cpu"
        
        config = {
            # ê¸°ë³¸ ì •ë³´
            "version": "dynamic_detector_v3.0",
            "generated_at": time.time(),
            "device": device_type,
            "is_m3_max": is_m3_max,
            "conda_env": detector.conda_env,
            
            # ì „ì—­ ì„¤ì •
            "optimization_enabled": True,
            "use_fp16": device_type != "cpu",
            "enable_compilation": is_m3_max,
            "memory_efficient": True,
            
            # ğŸ”¥ ë™ì  ë§¤í•‘ ì •ë³´ í¬í•¨
            "mapping_stats": detector.mapping_stats,
            "project_root": str(detector.path_discovery.project_root),
            "ai_models_root": str(detector.path_discovery.ai_models_root),
            
            # ëª¨ë¸ ì„¤ì •ë“¤
            "models": {},
            "step_mappings": {},
            "ultra_models": {},
            "device_optimization": {
                "target_device": device_type,
                "precision": "fp16" if device_type != "cpu" else "fp32",
                "enable_attention_slicing": True,
                "enable_vae_slicing": True,
                "enable_cpu_offload": is_m3_max,
                "memory_fraction": 0.8
            },
            
            # ì„±ëŠ¥ ìµœì í™”
            "performance_config": {
                "lazy_loading": True,
                "memory_mapping": True,
                "concurrent_loading": False,
                "cache_models": True,
                "preload_critical": True,
                "ultra_model_optimization": True
            }
        }
        
        # ê° ëª¨ë¸ë³„ ì„¤ì • ìƒì„±
        for model_name, model in detected_models.items():
            model_config = model.to_dict()
            config["models"][model_name] = model_config
            
            # Step ë§¤í•‘ ì¶”ê°€
            step_name = model.step_name
            if step_name not in config["step_mappings"]:
                config["step_mappings"][step_name] = []
            config["step_mappings"][step_name].append(model_name)
            
            # Ultra ëª¨ë¸ ë¶„ë¥˜
            if model.file_size_mb > 1000:  # 1GB ì´ìƒì€ Ultra ëª¨ë¸
                config["ultra_models"][model_name] = {
                    "size_gb": model.file_size_mb / 1024,
                    "requires_optimization": True,
                    "memory_offload": True
                }
        
        # Stepë³„ ì„¤ì • ìƒì„±
        config["step_configurations"] = {}
        for step_name, model_names in config["step_mappings"].items():
            step_models = [config["models"][name] for name in model_names]
            primary_model = max(step_models, key=lambda x: x["confidence"]) if step_models else None
            
            config["step_configurations"][step_name] = {
                "primary_model": primary_model["name"] if primary_model else None,
                "fallback_models": [m["name"] for m in sorted(step_models, key=lambda x: x["confidence"], reverse=True)[1:3]],
                "model_count": len(step_models),
                "total_size_mb": sum(m["size_mb"] for m in step_models),
                "has_ultra_models": any(m["size_mb"] > 1000 for m in step_models),
                "step_ready": len(step_models) > 0
            }
        
        # ì „ì²´ í†µê³„
        total_size_gb = sum(m["size_mb"] for m in config["models"].values()) / 1024
        ultra_count = len(config["ultra_models"])
        
        config["summary"] = {
            "total_models": len(config["models"]),
            "ultra_models_count": ultra_count,
            "total_steps": len(config["step_configurations"]),
            "ready_steps": sum(1 for s in config["step_configurations"].values() if s["step_ready"]),
            "total_size_gb": total_size_gb,
            "ultra_size_gb": sum(info["size_gb"] for info in config["ultra_models"].values()),
            "device_optimized": device_type != "cpu",
            "ready_for_production": len(config["models"]) > 0,
            "dynamic_detection": True
        }
        
        logger.info(f"âœ… ë™ì  ModelLoader ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
        logger.info(f"   Ultra ëª¨ë¸: {ultra_count}ê°œ ({sum(info['size_gb'] for info in config['ultra_models'].values()):.1f}GB)")
        
        return config
        
    except Exception as e:
        logger.error(f"âŒ ë™ì  ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "error": str(e),
            "version": "dynamic_detector_v3.0_error",
            "generated_at": time.time(),
            "models": {},
            "step_mappings": {},
            "success": False
        }

def quick_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€"""
    detector = get_global_detector()
    return detector.detect_all_models(**kwargs)

def comprehensive_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """í¬ê´„ì ì¸ ëª¨ë¸ íƒì§€"""
    kwargs['enable_pytorch_validation'] = kwargs.get('enable_pytorch_validation', True)
    kwargs['include_ultra_models'] = kwargs.get('include_ultra_models', True)
    return quick_model_detection(**kwargs)

def create_real_world_detector(**kwargs) -> RealWorldModelDetector:
    """íƒì§€ê¸° ìƒì„±"""
    return RealWorldModelDetector(**kwargs)

# ==============================================
# ğŸ”¥ 6. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

_global_detector: Optional[RealWorldModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector() -> RealWorldModelDetector:
    """ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤ (ë™ì )"""
    global _global_detector
    if _global_detector is None:
        with _detector_lock:
            if _global_detector is None:
                _global_detector = RealWorldModelDetector()
    return _global_detector

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤ (ì›ë³¸ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„)
create_advanced_detector = create_real_world_detector

# ğŸ”¥ ì›ë³¸ì—ì„œ ì‚¬ìš©ë˜ë˜ í•¨ìˆ˜ëª…ë“¤ ì¶”ê°€ (í•˜ë“œì½”ë”© ë°©ì§€)
enhanced_match_file_to_step = lambda file_path: get_global_detector().step_mapper.match_file_to_step(file_path)
enhanced_calculate_confidence = lambda file_path, file_name, path_str, file_size_mb, config: get_global_detector().step_mapper._calculate_confidence(file_path, file_name, path_str, file_size_mb, config)
enhanced_find_ai_models_paths = lambda: get_global_detector().path_discovery.discover_all_paths()

# ì›ë³¸ í˜¸í™˜ì„±ì„ ìœ„í•œ íŒ¨í„´ ë§¤í•‘
ENHANCED_STEP_MODEL_PATTERNS = property(lambda: get_global_detector().step_mapper.step_request_patterns)

# ==============================================
# ğŸ”¥ 9. ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„± 100% ìœ ì§€ + ë¹ ì§„ í•¨ìˆ˜ë“¤ ì¶”ê°€)
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'RealWorldModelDetector',
    'DetectedModel', 
    'ModelCategory',
    'ModelPriority',
    'DynamicPathDiscovery',
    'DynamicStepMapper',
    
    # ğŸ”¥ ë¹ ì§„ í•µì‹¬ í´ë˜ìŠ¤ë“¤ ì¶”ê°€
    'ModelFileInfo',
    'ModelArchitecture', 
    'ModelMetadata',
    'AdvancedModelLoaderAdapter',
    'RealModelLoaderConfigGenerator',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_real_world_detector',
    'create_advanced_detector',
    'quick_model_detection',
    'comprehensive_model_detection',
    
    # ModelLoader ì¸í„°í˜ì´ìŠ¤ (í•„ìˆ˜!)
    'list_available_models',
    'register_step_requirements',
    'create_step_interface', 
    'get_models_for_step',
    'validate_model_exists',
    'generate_advanced_model_loader_config',
    
    # ğŸ”¥ ë¹ ì§„ ModelLoader ë“±ë¡ í•¨ìˆ˜ë“¤ ì¶”ê°€ (í•µì‹¬!)
    'register_detected_models_to_loader',
    'register_single_model_to_loader',
    'create_model_config_for_loader',
    'get_step_specific_loader_config',
    
    # ğŸ”¥ ë¹ ì§„ ê²€ì¦ ë° ì„¤ì • í•¨ìˆ˜ë“¤ ì¶”ê°€
    'validate_real_model_paths',
    'generate_real_model_loader_config',
    'create_advanced_model_loader_adapter',
    
    # ì „ì—­ í•¨ìˆ˜
    'get_global_detector'
]

# ==============================================
# ğŸ”¥ 8. ì´ˆê¸°í™” ë° ë¡œê¹…
# ==============================================

logger.info("=" * 80)
logger.info("âœ… ì™„ì „ ë™ì  ìë™ ëª¨ë¸ íƒì§€ê¸° v3.0 ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ¯ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ 100% ë™ì  íƒì§€")
logger.info("ğŸ”¥ ultra_models í´ë”ê¹Œì§€ ì™„ì „ ì»¤ë²„")
logger.info("ğŸš€ conda í™˜ê²½ íŠ¹í™” ìºì‹œ ì „ëµ")
logger.info("âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜")
logger.info("âœ… Step01 ìš”ì²­ëª… â†’ ì‹¤ì œ íŒŒì¼ ì™„ë²½ ë§¤í•‘")
logger.info("âœ… í•˜ë“œì½”ë”© ì™„ì „ ì œê±°, ìˆœìˆ˜ ë™ì  íƒì§€")
logger.info("âœ… M3 Max 128GB ìµœì í™”")
logger.info("=" * 80)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
try:
    _test_detector = get_global_detector()
    logger.info("ğŸš€ ë™ì  íƒì§€ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {_test_detector.path_discovery.project_root}")
    logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {_test_detector.path_discovery.ai_models_root}")
    logger.info(f"   ê²€ìƒ‰ ê²½ë¡œ: {len(_test_detector.search_paths)}ê°œ")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

if __name__ == "__main__":
    print("ğŸ” ì™„ì „ ë™ì  ìë™ ëª¨ë¸ íƒì§€ê¸° v3.0 í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # 1. ë™ì  íƒì§€ í…ŒìŠ¤íŠ¸
    print("ğŸ“ 1ë‹¨ê³„: ë™ì  ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸")
    models = quick_model_detection()
    print(f"   íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    if models:
        # Stepë³„ ë¶„ë¥˜
        step_groups = {}
        ultra_models = []
        
        for model in models.values():
            step = model.step_name
            if step not in step_groups:
                step_groups[step] = []
            step_groups[step].append(model)
            
            # Ultra ëª¨ë¸ ì²´í¬
            if model.file_size_mb > 1000:
                ultra_models.append(model)
        
        print(f"   Stepë³„ ë¶„ë¥˜:")
        for step, step_models in step_groups.items():
            total_size = sum(m.file_size_mb for m in step_models) / 1024
            print(f"   {step}: {len(step_models)}ê°œ ({total_size:.1f}GB)")
            for model in step_models[:2]:  # ê° Stepì—ì„œ ìƒìœ„ 2ê°œë§Œ
                patterns = ", ".join(model.matched_patterns[:2])
                print(f"     - {model.name} ({model.confidence_score:.2f}, {patterns})")
        
        print(f"\nğŸš€ Ultra ëª¨ë¸: {len(ultra_models)}ê°œ")
        for model in ultra_models[:3]:  # ìƒìœ„ 3ê°œë§Œ
            size_gb = model.file_size_mb / 1024
            print(f"   - {model.name}: {size_gb:.1f}GB")
    
    # 2. ë™ì  ë§¤í•‘ í†µê³„
    print(f"\nğŸ“Š 2ë‹¨ê³„: ë™ì  ë§¤í•‘ í†µê³„")
    detector = get_global_detector()
    if hasattr(detector, 'mapping_stats'):
        stats = detector.mapping_stats
        print(f"   ìš”ì²­ëª… ë§¤ì¹­: {stats.get('request_name_mappings', 0)}ê°œ")
        print(f"   íŒ¨í„´ ë§¤ì¹­: {stats.get('pattern_mappings', 0)}ê°œ")
        print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {stats.get('keyword_mappings', 0)}ê°œ")
        print(f"   Ultra ëª¨ë¸: {stats.get('ultra_models_found', 0)}ê°œ")
        print(f"   ë™ì  ë°œê²¬: {stats.get('dynamic_discoveries', 0)}ê°œ")
        print(f"   ë¯¸ë§¤í•‘: {stats.get('unmapped_files', 0)}ê°œ")
    
    print("\nğŸ‰ ì™„ì „ ë™ì  íƒì§€ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("âœ… ê¸°ì¡´ Step01 ë“±ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥")
    print("âœ… ë‹¤ë¥¸ íŒŒì¼ ìˆ˜ì • ë¶ˆí•„ìš”")
    print("=" * 80)