#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ì™„ì „í•œ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v9.0 - ModelLoader ì—°ë™ í†µí•©
==================================================================================

âœ… íƒì§€ëœ ëª¨ë¸ì„ ModelLoaderì— ìë™ ë“±ë¡í•˜ëŠ” ì—°ê²° ê³ ë¦¬ ì™„ì„±
âœ… 574ê°œ ëª¨ë¸ íƒì§€ â†’ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë“±ë¡ê¹Œì§€ ì™„ë£Œ
âœ… PipelineManagerì™€ ì™„ì „ ì—°ë™
âœ… Stepë³„ ëª¨ë¸ ìë™ í• ë‹¹ ë° ë“±ë¡
âœ… MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ìœ ì§€

ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­ v9.0:
- ModelLoaderBridge í´ë˜ìŠ¤ ì¶”ê°€ (íƒì§€ â†’ ë“±ë¡ ì—°ê²°)
- AutoRegistrationManager í´ë˜ìŠ¤ ì¶”ê°€ (ìë™ ë“±ë¡ ì‹œìŠ¤í…œ)
- StepModelMatcher í´ë˜ìŠ¤ ì¶”ê°€ (Stepë³„ ëª¨ë¸ ë§¤ì¹­)
- ì‹¤ì‹œê°„ ëª¨ë¸ ê°€ìš©ì„± ê²€ì¦
- ìë™ í´ë°± ëª¨ë¸ ì„¤ì •
- ì„±ëŠ¥ ìµœì í™”ëœ ë“±ë¡ í”„ë¡œì„¸ìŠ¤
- ì™„ì „ ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜
"""

import os
import re
import sys
import time
import json
import logging
import hashlib
import sqlite3
import psutil
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ PyTorch import (MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
# ==============================================

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ì•ˆì „í•œ ì„¤ì •
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        IS_M3_MAX = True
        # ì™„ì „ ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬
        try:
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        except (AttributeError, RuntimeError) as e:
            logging.debug(f"MPS ìºì‹œ ì •ë¦¬ ê±´ë„ˆëœ€: {e}")
    elif torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        IS_M3_MAX = False
    else:
        DEVICE_TYPE = "cpu"
        IS_M3_MAX = False
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE_TYPE = "cpu"
    IS_M3_MAX = False
    torch = None

try:
    import numpy as np
    from PIL import Image
    import cv2
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

# ModelLoader ì—°ë™ì„ ìœ„í•œ import
try:
    from .model_loader import (
        ModelLoader, get_global_model_loader, 
        StepModelInterface, SafeModelService,
        ModelConfig, StepModelConfig
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    MODEL_LOADER_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ëª¨ë“ˆ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"
    
    # ì¶”ê°€ ì„¸ë¶„í™”
    STABLE_DIFFUSION = "stable_diffusion"
    OOTDIFFUSION = "ootdiffusion"
    CONTROLNET = "controlnet"
    SAM_MODELS = "sam_models"
    CLIP_MODELS = "clip_models"

class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ íƒ€ì…"""
    UNET = "unet"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    GAN = "gan"
    VAE = "vae"
    DIFFUSION = "diffusion"
    CLIP = "clip"
    RESNET = "resnet"
    CUSTOM = "custom"
    UNKNOWN = "unknown"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    inference_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    throughput_fps: float = 0.0
    accuracy_score: Optional[float] = None
    m3_compatibility_score: float = 0.0

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ì™„ì „ ê°•í™”ëœ ë²„ì „)"""
    # ê¸°ë³¸ ì •ë³´
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ê²€ì¦ ì •ë³´
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    checksum: Optional[str] = None
    
    # ì•„í‚¤í…ì²˜ ì •ë³´
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    precision: str = "fp32"
    
    # ì„±ëŠ¥ ì •ë³´
    performance_metrics: Optional[ModelPerformanceMetrics] = None
    memory_requirements: Dict[str, float] = field(default_factory=dict)
    device_compatibility: Dict[str, bool] = field(default_factory=dict)
    load_time_ms: float = 0.0
    health_status: str = "unknown"
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation_results: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    
    # ğŸ”¥ NEW: ModelLoader ì—°ë™ ì •ë³´
    model_loader_registered: bool = False
    model_loader_name: Optional[str] = None
    step_interface_assigned: bool = False
    registration_timestamp: Optional[float] = None

# ==============================================
# ğŸ”¥ ê°•í™”ëœ íŒ¨í„´ ë§¤ì¹­ ëª¨ë“ˆ
# ==============================================

@dataclass
class EnhancedModelPattern:
    """ê°•í™”ëœ ëª¨ë¸ íŒ¨í„´ ì •ë³´"""
    name: str
    patterns: List[str]
    step: str
    keywords: List[str]
    file_types: List[str]
    size_range_mb: Tuple[float, float]
    priority: int = 1
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    alternative_names: List[str] = field(default_factory=list)
    context_paths: List[str] = field(default_factory=list)
    
    # ğŸ”¥ NEW: ModelLoader ì—°ë™ ì •ë³´
    model_class: str = "BaseModel"
    loader_config: Dict[str, Any] = field(default_factory=dict)
    step_requirements: Dict[str, Any] = field(default_factory=dict)

class PatternMatcher:
    """íŒ¨í„´ ë§¤ì¹­ ì „ìš© í´ë˜ìŠ¤ (ê°•í™”ëœ ë²„ì „)"""
    
    def __init__(self):
        self.patterns = self._get_enhanced_patterns()
        self.logger = logging.getLogger(f"{__name__}.PatternMatcher")
    
    def _get_enhanced_patterns(self) -> Dict[str, EnhancedModelPattern]:
        """ê°œì„ ëœ íŒ¨í„´ ì •ì˜ (ModelLoader ì—°ë™ ì •ë³´ í¬í•¨)"""
        return {
            "human_parsing": EnhancedModelPattern(
                name="human_parsing",
                patterns=[
                    r".*exp-schp.*atr.*\.pth$",
                    r".*graphonomy.*lip.*\.pth$",
                    r".*densepose.*rcnn.*\.pkl$",
                    r".*human.*parsing.*\.pth$",
                    r".*schp.*\.pth$",
                    r".*atr.*\.pth$",
                    r".*lip.*\.pth$"
                ],
                step="HumanParsingStep",
                keywords=["human", "parsing", "schp", "atr", "graphonomy", "densepose", "lip"],
                file_types=['.pth', '.pkl', '.bin'],
                size_range_mb=(10, 1000),
                priority=1,
                architecture=ModelArchitecture.CNN,
                context_paths=["human_parsing", "parsing", "step_01"],
                model_class="GraphonomyModel",
                loader_config={
                    "input_size": (512, 512),
                    "num_classes": 20,
                    "device": "auto",
                    "precision": "fp16"
                },
                step_requirements={
                    "primary_model": True,
                    "required": True,
                    "fallback_enabled": True
                }
            ),
            
            "pose_estimation": EnhancedModelPattern(
                name="pose_estimation",
                patterns=[
                    r".*openpose.*body.*\.pth$",
                    r".*body_pose_model.*\.pth$",
                    r".*pose.*estimation.*\.pth$",
                    r".*mediapipe.*pose.*\.pth$",
                    r".*hrnet.*pose.*\.pth$",
                    r".*openpose.*\.pth$",
                    r".*pose.*\.pth$"
                ],
                step="PoseEstimationStep",
                keywords=["pose", "openpose", "body", "keypoint", "mediapipe", "hrnet"],
                file_types=['.pth', '.onnx', '.bin'],
                size_range_mb=(5, 500),
                priority=2,
                architecture=ModelArchitecture.CNN,
                context_paths=["pose", "openpose", "step_02"],
                model_class="OpenPoseModel",
                loader_config={
                    "input_size": (368, 368),
                    "num_classes": 18,
                    "device": "auto",
                    "precision": "fp16"
                }
            ),
            
            "cloth_segmentation": EnhancedModelPattern(
                name="cloth_segmentation",
                patterns=[
                    r".*u2net.*\.pth$",
                    r".*cloth.*segmentation.*\.pth$",
                    r".*sam.*vit.*\.pth$",
                    r".*rembg.*\.pth$",
                    r".*segmentation.*\.pth$",
                    r".*mask.*\.pth$"
                ],
                step="ClothSegmentationStep",
                keywords=["u2net", "segmentation", "cloth", "mask", "sam", "rembg"],
                file_types=['.pth', '.bin', '.safetensors'],
                size_range_mb=(10, 3000),
                priority=1,
                architecture=ModelArchitecture.UNET,
                context_paths=["segmentation", "cloth", "u2net", "step_03"],
                model_class="U2NetModel",
                loader_config={
                    "input_size": (320, 320),
                    "device": "auto",
                    "precision": "fp16"
                }
            ),
            
            "virtual_fitting": EnhancedModelPattern(
                name="virtual_fitting",
                patterns=[
                    r".*ootd.*diffusion.*\.bin$",
                    r".*stable.*diffusion.*\.safetensors$",
                    r".*diffusion_pytorch_model\.bin$",
                    r".*unet.*\.bin$",
                    r".*vae.*\.safetensors$",
                    r".*virtual.*fitting.*\.pth$"
                ],
                step="VirtualFittingStep",
                keywords=["diffusion", "ootd", "stable", "unet", "vae", "viton", "virtual"],
                file_types=['.bin', '.safetensors', '.pth'],
                size_range_mb=(100, 8000),
                priority=1,
                architecture=ModelArchitecture.DIFFUSION,
                context_paths=["diffusion", "ootd", "virtual", "stable", "step_06"],
                model_class="StableDiffusionPipeline",
                loader_config={
                    "input_size": (512, 512),
                    "device": "auto",
                    "precision": "fp16",
                    "enable_attention_slicing": True
                }
            ),
            
            "geometric_matching": EnhancedModelPattern(
                name="geometric_matching",
                patterns=[
                    r".*gmm.*\.pth$",
                    r".*geometric.*matching.*\.pth$",
                    r".*tps.*\.pth$",
                    r".*warp.*\.pth$"
                ],
                step="GeometricMatchingStep",
                keywords=["gmm", "geometric", "matching", "tps", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(10, 300),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["geometric", "gmm", "step_04"],
                model_class="GeometricMatchingModel"
            ),
            
            "cloth_warping": EnhancedModelPattern(
                name="cloth_warping",
                patterns=[
                    r".*tom.*\.pth$",
                    r".*warping.*\.pth$",
                    r".*flow.*\.pth$",
                    r".*cloth.*warp.*\.pth$"
                ],
                step="ClothWarpingStep",
                keywords=["tom", "warping", "flow", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(20, 400),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["warping", "tom", "step_05"],
                model_class="ClothWarpingModel"
            )
        }
    
    def match_file_to_patterns(self, file_path: Path) -> List[Tuple[str, float]]:
        """íŒŒì¼ì„ íŒ¨í„´ì— ë§¤ì¹­í•˜ê³  ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜"""
        matches = []
        
        file_name = file_path.name.lower()
        path_str = str(file_path).lower()
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
        except OSError:
            file_size_mb = 0
        
        for pattern_name, pattern in self.patterns.items():
            confidence = self._calculate_pattern_confidence(
                file_path, file_name, path_str, file_size_mb, pattern
            )
            
            if confidence > 0.05:  # ë‚®ì€ ì„ê³„ê°’
                matches.append((pattern_name, confidence))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
    
    def _calculate_pattern_confidence(self, file_path: Path, file_name: str, 
                                    path_str: str, file_size_mb: float, 
                                    pattern: EnhancedModelPattern) -> float:
        """íŒ¨í„´ ë§¤ì¹­ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        
        # 1. ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ (30%)
        pattern_matches = sum(1 for regex_pattern in pattern.patterns 
                            if re.search(regex_pattern, file_name, re.IGNORECASE) or 
                               re.search(regex_pattern, path_str, re.IGNORECASE))
        if pattern_matches > 0:
            confidence += 0.3 * min(pattern_matches / len(pattern.patterns), 1.0)
        
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ (25%)
        keyword_matches = sum(1 for keyword in pattern.keywords 
                            if keyword in file_name or keyword in path_str)
        if keyword_matches > 0:
            confidence += 0.25 * min(keyword_matches / len(pattern.keywords), 1.0)
        
        # 3. íŒŒì¼ í™•ì¥ì (20%)
        if file_path.suffix.lower() in pattern.file_types:
            confidence += 0.20
        
        # 4. íŒŒì¼ í¬ê¸° ì í•©ì„± (15%)
        min_size, max_size = pattern.size_range_mb
        tolerance = 0.5  # 50% í—ˆìš© ì˜¤ì°¨
        effective_min = min_size * (1 - tolerance)
        effective_max = max_size * (1 + tolerance)
        
        if effective_min <= file_size_mb <= effective_max:
            confidence += 0.15
        elif file_size_mb > effective_min * 0.5:
            confidence += 0.08
        
        # 5. ê²½ë¡œ ì»¨í…ìŠ¤íŠ¸ (10%)
        context_matches = sum(1 for context in pattern.context_paths 
                            if context in path_str)
        if context_matches > 0:
            confidence += 0.10 * min(context_matches / len(pattern.context_paths), 1.0)
        
        return min(confidence, 1.0)

# ==============================================
# ğŸ”¥ íŒŒì¼ ìŠ¤ìºë„ˆ ëª¨ë“ˆ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class FileScanner:
    """AI ëª¨ë¸ íŒŒì¼ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, enable_deep_scan: bool = True, max_depth: int = 10):
        self.enable_deep_scan = enable_deep_scan
        self.max_depth = max_depth
        self.logger = logging.getLogger(f"{__name__}.FileScanner")
        
        self.model_extensions = {
            '.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.pickle',
            '.h5', '.hdf5', '.pb', '.tflite', '.onnx', '.mlmodel', '.engine'
        }
        
        self.excluded_dirs = {
            '__pycache__', '.git', 'node_modules', '.vscode', '.idea',
            '.pytest_cache', '.mypy_cache', '.DS_Store', 'build', 'dist'
        }
    
    def scan_paths(self, search_paths: List[Path]) -> List[Path]:
        """ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”"""
        all_model_files = []
        
        for search_path in search_paths:
            if search_path.exists() and search_path.is_dir():
                try:
                    model_files = self._scan_directory(search_path, current_depth=0)
                    all_model_files.extend(model_files)
                    self.logger.debug(f"ğŸ“ {search_path}: {len(model_files)}ê°œ íŒŒì¼")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {e}")
        
        unique_files = list(set(all_model_files))
        self.logger.info(f"ğŸ“Š ì´ ìŠ¤ìº”: {len(unique_files)}ê°œ ëª¨ë¸ íŒŒì¼")
        return unique_files
    
    def _scan_directory(self, directory: Path, current_depth: int = 0) -> List[Path]:
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        model_files = []
        
        if current_depth > self.max_depth:
            return model_files
        
        try:
            items = list(directory.iterdir())
        except (PermissionError, OSError):
            return model_files
        
        for item in items:
            try:
                if item.is_file():
                    if self._is_model_file(item):
                        model_files.append(item)
                elif item.is_dir() and self.enable_deep_scan:
                    if item.name not in self.excluded_dirs:
                        sub_files = self._scan_directory(item, current_depth + 1)
                        model_files.extend(sub_files)
            except Exception:
                continue
        
        return model_files
    
    def _is_model_file(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ì¸ì§€ í™•ì¸"""
        try:
            # í™•ì¥ì ì²´í¬
            if file_path.suffix.lower() not in self.model_extensions:
                return False
            
            # íŒŒì¼ í¬ê¸° ì²´í¬
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb < 0.1 or file_size_mb > 20480:  # 0.1MB ~ 20GB
                return False
            
            # AI ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
            file_name = file_path.name.lower()
            path_str = str(file_path).lower()
            
            ai_keywords = [
                'model', 'checkpoint', 'weight', 'pytorch_model', 'diffusion',
                'stable', 'unet', 'transformer', 'bert', 'clip', 'pose',
                'parsing', 'segmentation', 'virtual', 'fitting'
            ]
            
            has_keyword = any(keyword in file_name for keyword in ai_keywords)
            path_indicators = ['models', 'checkpoints', 'weights', 'huggingface']
            has_path_indicator = any(indicator in path_str for indicator in path_indicators)
            
            return has_keyword or has_path_indicator or file_size_mb > 10
            
        except Exception:
            return False

# ==============================================
# ğŸ”¥ PyTorch ê²€ì¦ ëª¨ë“ˆ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class PyTorchValidator:
    """PyTorch ëª¨ë¸ ê²€ì¦ê¸°"""
    
    def __init__(self, enable_validation: bool = True, timeout: int = 120):
        self.enable_validation = enable_validation
        self.timeout = timeout
        self.logger = logging.getLogger(f"{__name__}.PyTorchValidator")
    
    def validate_model(self, file_path: Path) -> Dict[str, Any]:
        """PyTorch ëª¨ë¸ ê²€ì¦"""
        if not self.enable_validation or not TORCH_AVAILABLE:
            return {
                'valid': False,
                'parameter_count': 0,
                'validation_info': {"validation_disabled": True},
                'model_structure': {},
                'architecture': ModelArchitecture.UNKNOWN
            }
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb > 5000:  # 5GB ì´ìƒ ê±´ë„ˆë›°ê¸°
                return {
                    'valid': True,
                    'parameter_count': int(file_size_mb * 1000000),
                    'validation_info': {"large_file_skipped": True},
                    'model_structure': {},
                    'architecture': ModelArchitecture.UNKNOWN
                }
            
            checkpoint = self._safe_load_checkpoint(file_path)
            if checkpoint is None:
                return self._create_failed_result("load_failed")
            
            validation_info = {}
            parameter_count = 0
            architecture = ModelArchitecture.UNKNOWN
            
            if isinstance(checkpoint, dict):
                state_dict = self._extract_state_dict(checkpoint)
                if state_dict:
                    parameter_count = self._count_parameters(state_dict)
                    validation_info.update(self._analyze_layers(state_dict))
                    architecture = self._detect_architecture(state_dict)
            
            return {
                'valid': True,
                'parameter_count': parameter_count,
                'validation_info': validation_info,
                'model_structure': {},
                'architecture': architecture
            }
            
        except Exception as e:
            return self._create_failed_result(str(e)[:200])
        finally:
            self._safe_memory_cleanup()
    
    def _safe_load_checkpoint(self, file_path: Path):
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            return torch.load(file_path, map_location='cpu', weights_only=True)
        except Exception:
            try:
                return torch.load(file_path, map_location='cpu')
            except Exception:
                return None
    
    def _extract_state_dict(self, checkpoint):
        """state_dict ì¶”ì¶œ"""
        if isinstance(checkpoint, dict):
            for key in ['state_dict', 'model', 'model_state_dict', 'net']:
                if key in checkpoint and isinstance(checkpoint[key], dict):
                    return checkpoint[key]
            if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
                return checkpoint
        return None
    
    def _count_parameters(self, state_dict: Dict) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        try:
            return sum(tensor.numel() for tensor in state_dict.values() 
                      if torch.is_tensor(tensor))
        except Exception:
            return 0
    
    def _analyze_layers(self, state_dict: Dict) -> Dict[str, Any]:
        """ë ˆì´ì–´ ë¶„ì„"""
        layer_types = {}
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                layer_types['convolution'] = layer_types.get('convolution', 0) + 1
            elif 'norm' in key_lower or 'bn' in key_lower:
                layer_types['normalization'] = layer_types.get('normalization', 0) + 1
            elif 'linear' in key_lower or 'fc' in key_lower:
                layer_types['linear'] = layer_types.get('linear', 0) + 1
        
        return {
            "total_layers": len(state_dict),
            "layer_types": layer_types
        }
    
    def _detect_architecture(self, state_dict: Dict) -> ModelArchitecture:
        """ì•„í‚¤í…ì²˜ íƒì§€"""
        all_keys = ' '.join(state_dict.keys()).lower()
        
        if 'unet' in all_keys or 'down_block' in all_keys:
            return ModelArchitecture.UNET
        elif 'transformer' in all_keys or 'attention' in all_keys:
            return ModelArchitecture.TRANSFORMER
        elif 'diffusion' in all_keys:
            return ModelArchitecture.DIFFUSION
        elif 'conv' in all_keys:
            return ModelArchitecture.CNN
        else:
            return ModelArchitecture.UNKNOWN
    
    def _create_failed_result(self, error: str) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±"""
        return {
            'valid': False,
            'parameter_count': 0,
            'validation_info': {"error": error},
            'model_structure': {},
            'architecture': ModelArchitecture.UNKNOWN
        }
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if TORCH_AVAILABLE and DEVICE_TYPE == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass

# ==============================================
# ğŸ”¥ ê²½ë¡œ íƒì§€ ëª¨ë“ˆ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class PathFinder:
    """ê²€ìƒ‰ ê²½ë¡œ ìë™ íƒì§€"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PathFinder")
    
    def get_search_paths(self) -> List[Path]:
        """í¬ê´„ì ì¸ ê²€ìƒ‰ ê²½ë¡œ ìƒì„±"""
        try:
            current_file = Path(__file__).resolve()
            project_paths = self._get_project_paths(current_file)
            conda_paths = self._get_conda_paths()
            cache_paths = self._get_cache_paths()
            user_paths = self._get_user_paths()
            
            all_paths = project_paths + conda_paths + cache_paths + user_paths
            
            valid_paths = []
            for path in all_paths:
                try:
                    if path.exists() and path.is_dir() and os.access(path, os.R_OK):
                        valid_paths.append(path.resolve())
                except Exception:
                    continue
            
            unique_paths = []
            seen = set()
            for path in valid_paths:
                if path not in seen:
                    unique_paths.append(path)
                    seen.add(path)
            
            self.logger.info(f"âœ… ê²€ìƒ‰ ê²½ë¡œ: {len(unique_paths)}ê°œ")
            return unique_paths
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {e}")
            return [Path.cwd()]
    
    def _get_project_paths(self, current_file: Path) -> List[Path]:
        """í”„ë¡œì íŠ¸ ë‚´ ê²½ë¡œë“¤"""
        try:
            backend_dir = current_file
            for _ in range(5):
                backend_dir = backend_dir.parent
                if backend_dir.name in ['backend', 'mycloset-ai']:
                    break
            
            return [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "checkpoints",
                backend_dir / "models",
                backend_dir.parent / "ai_models"
            ]
        except Exception:
            return []
    
    def _get_conda_paths(self) -> List[Path]:
        """conda í™˜ê²½ ê²½ë¡œë“¤"""
        paths = []
        try:
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                base_path = Path(conda_prefix)
                paths.extend([
                    base_path / "lib" / "python3.11" / "site-packages",
                    base_path / "lib" / "python3.10" / "site-packages",
                    base_path / "models"
                ])
            
            conda_roots = [
                Path.home() / "miniforge3",
                Path.home() / "miniconda3",
                Path.home() / "anaconda3"
            ]
            
            for root in conda_roots:
                if root.exists():
                    paths.extend([
                        root / "pkgs",
                        root / "envs",
                        root / "models"
                    ])
        except Exception:
            pass
        
        return paths
    
    def _get_cache_paths(self) -> List[Path]:
        """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œë“¤"""
        home = Path.home()
        return [
            home / ".cache" / "huggingface" / "hub",
            home / ".cache" / "torch" / "hub",
            home / ".cache" / "models"
        ]
    
    def _get_user_paths(self) -> List[Path]:
        """ì‚¬ìš©ì ë‹¤ìš´ë¡œë“œ ê²½ë¡œë“¤"""
        home = Path.home()
        return [
            home / "Downloads",
            home / "Documents" / "AI_Models",
            home / "Desktop" / "models"
        ]

# ==============================================
# ğŸ”¥ NEW: ModelLoader ë¸Œë¦¬ì§€ í´ë˜ìŠ¤ (í•µì‹¬ ì—°ê²°ê³ ë¦¬)
# ==============================================

class ModelLoaderBridge:
    """
    ğŸ”— íƒì§€ëœ ëª¨ë¸ê³¼ ModelLoader ì—°ê²° ë¸Œë¦¬ì§€ (í•µì‹¬ ì—°ê²°ê³ ë¦¬)
    
    574ê°œ ëª¨ë¸ íƒì§€ â†’ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë“±ë¡í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤
    """
    
    def __init__(self, model_loader: Optional[Any] = None):
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderBridge")
        self.model_loader = model_loader
        self.registration_stats = {
            "attempted": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0
        }
        
        # ModelLoader ê°€ì ¸ì˜¤ê¸°
        if model_loader is None and MODEL_LOADER_AVAILABLE:
            try:
                self.model_loader = get_global_model_loader()
                self.logger.info("âœ… ì „ì—­ ModelLoader ì—°ê²° ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì „ì—­ ModelLoader ì—°ê²° ì‹¤íŒ¨: {e}")
                self.model_loader = None
        
        self.available = self.model_loader is not None
    
    def register_detected_models(
        self, 
        detected_models: Dict[str, DetectedModel],
        force_registration: bool = False,
        max_registrations: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥: íƒì§€ëœ ëª¨ë¸ë“¤ì„ ModelLoaderì— ë“±ë¡
        
        Args:
            detected_models: íƒì§€ëœ ëª¨ë¸ë“¤
            force_registration: ê°•ì œ ë“±ë¡ ì—¬ë¶€
            max_registrations: ìµœëŒ€ ë“±ë¡ ìˆ˜ ì œí•œ
            
        Returns:
            ë“±ë¡ ê²°ê³¼ í†µê³„
        """
        if not self.available:
            self.logger.warning("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€ëŠ¥ - ë“±ë¡ ê±´ë„ˆëœ€")
            return {"error": "ModelLoader not available"}
        
        try:
            self.logger.info(f"ğŸ”— ModelLoaderì— {len(detected_models)}ê°œ ëª¨ë¸ ë“±ë¡ ì‹œì‘...")
            
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
            sorted_models = sorted(
                detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score)
            )
            
            # ë“±ë¡ ì œí•œ ì ìš©
            if max_registrations:
                sorted_models = sorted_models[:max_registrations]
            
            registered_models = []
            
            for model_name, detected_model in sorted_models:
                try:
                    self.registration_stats["attempted"] += 1
                    
                    # ì´ë¯¸ ë“±ë¡ëœ ëª¨ë¸ ì²´í¬
                    if (detected_model.model_loader_registered and 
                        not force_registration):
                        self.registration_stats["skipped"] += 1
                        continue
                    
                    # ModelLoaderìš© ì„¤ì • ìƒì„±
                    model_config = self._create_model_config(detected_model)
                    
                    # ModelLoaderì— ë“±ë¡
                    registration_success = self._register_to_model_loader(
                        model_name, model_config, detected_model
                    )
                    
                    if registration_success:
                        # ë“±ë¡ ì„±ê³µ ë§ˆí‚¹
                        detected_model.model_loader_registered = True
                        detected_model.model_loader_name = model_name
                        detected_model.registration_timestamp = time.time()
                        
                        registered_models.append(model_name)
                        self.registration_stats["successful"] += 1
                        
                        self.logger.info(f"âœ… ë“±ë¡ ì„±ê³µ: {model_name}")
                    else:
                        self.registration_stats["failed"] += 1
                        self.logger.warning(f"âŒ ë“±ë¡ ì‹¤íŒ¨: {model_name}")
                
                except Exception as e:
                    self.registration_stats["failed"] += 1
                    self.logger.warning(f"âŒ {model_name} ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ë“±ë¡ ê²°ê³¼ ë°˜í™˜
            result = {
                "success": True,
                "registered_models": registered_models,
                "statistics": self.registration_stats.copy(),
                "total_detected": len(detected_models),
                "total_registered": len(registered_models)
            }
            
            self.logger.info(f"ğŸ¯ ë“±ë¡ ì™„ë£Œ: {len(registered_models)}/{len(detected_models)}ê°œ ì„±ê³µ")
            self.logger.info(f"ğŸ“Š ì„±ê³µë¥ : {(len(registered_models)/len(detected_models)*100):.1f}%")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "statistics": self.registration_stats}
    
    def _create_model_config(self, detected_model: DetectedModel) -> Dict[str, Any]:
        """DetectedModelì„ ModelLoader ì„¤ì •ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ ì„¤ì •
            config = {
                "name": detected_model.name,
                "path": str(detected_model.path),
                "model_type": detected_model.model_type,
                "step_name": detected_model.step_name,
                "device": "auto",
                "precision": "fp16" if DEVICE_TYPE != "cpu" else "fp32",
                
                # íƒì§€ ì •ë³´
                "confidence_score": detected_model.confidence_score,
                "pytorch_valid": detected_model.pytorch_valid,
                "parameter_count": detected_model.parameter_count,
                "file_size_mb": detected_model.file_size_mb,
                "architecture": detected_model.architecture.value,
                "priority": detected_model.priority.value,
                
                # ë©”íƒ€ë°ì´í„°
                "detected_by": "auto_model_detector_v9",
                "detection_timestamp": time.time(),
                "health_status": detected_model.health_status,
                "device_compatibility": detected_model.device_compatibility,
                "memory_requirements": detected_model.memory_requirements,
                
                # ModelLoader íŠ¹í™” ì„¤ì •
                "enable_caching": True,
                "lazy_loading": detected_model.file_size_mb > 1000,  # 1GB ì´ìƒ
                "optimization_hints": self._generate_optimization_hints(detected_model)
            }
            
            # ì•„í‚¤í…ì²˜ë³„ íŠ¹í™” ì„¤ì •
            if detected_model.architecture == ModelArchitecture.DIFFUSION:
                config.update({
                    "enable_attention_slicing": True,
                    "enable_vae_slicing": True,
                    "use_memory_efficient_attention": True
                })
            elif detected_model.architecture == ModelArchitecture.TRANSFORMER:
                config.update({
                    "use_flash_attention": True,
                    "enable_kv_cache": True
                })
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {"name": detected_model.name, "path": str(detected_model.path)}
    
    def _generate_optimization_hints(self, detected_model: DetectedModel) -> List[str]:
        """ëª¨ë¸ë³„ ìµœì í™” íŒíŠ¸ ìƒì„±"""
        hints = []
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX and detected_model.device_compatibility.get("mps", False):
            hints.extend(["use_mps_device", "enable_neural_engine"])
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        if detected_model.file_size_mb > 1000:
            hints.extend(["use_fp16", "enable_gradient_checkpointing", "memory_mapping"])
        
        # ì•„í‚¤í…ì²˜ë³„ ìµœì í™”
        if detected_model.architecture == ModelArchitecture.DIFFUSION:
            hints.extend(["attention_slicing", "vae_slicing"])
        elif detected_model.architecture == ModelArchitecture.TRANSFORMER:
            hints.extend(["flash_attention", "kv_caching"])
        
        return hints
    
    def _register_to_model_loader(
        self, 
        model_name: str, 
        model_config: Dict[str, Any], 
        detected_model: DetectedModel
    ) -> bool:
        """ì‹¤ì œ ModelLoaderì— ë“±ë¡"""
        try:
            if not self.model_loader:
                return False
            
            # ModelLoaderì˜ register_model ë©”ì„œë“œ ì‚¬ìš©
            if hasattr(self.model_loader, 'register_model'):
                success = self.model_loader.register_model(model_name, model_config)
                if success:
                    self.logger.debug(f"âœ… register_model ì„±ê³µ: {model_name}")
                    return True
            
            # ModelLoaderì˜ register_model_config ë©”ì„œë“œ ì‚¬ìš©
            if hasattr(self.model_loader, 'register_model_config'):
                success = self.model_loader.register_model_config(model_name, model_config)
                if success:
                    self.logger.debug(f"âœ… register_model_config ì„±ê³µ: {model_name}")
                    return True
            
            # SafeModelService ì§ì ‘ ì‚¬ìš©
            if hasattr(self.model_loader, 'safe_model_service'):
                success = self.model_loader.safe_model_service.register_model(model_name, model_config)
                if success:
                    self.logger.debug(f"âœ… safe_model_service ë“±ë¡ ì„±ê³µ: {model_name}")
                    return True
            
            self.logger.warning(f"âš ï¸ {model_name}: ì‚¬ìš© ê°€ëŠ¥í•œ ë“±ë¡ ë©”ì„œë“œ ì—†ìŒ")
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {model_name} ModelLoader ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def get_registration_stats(self) -> Dict[str, Any]:
        """ë“±ë¡ í†µê³„ ë°˜í™˜"""
        return {
            "statistics": self.registration_stats.copy(),
            "success_rate": (self.registration_stats["successful"] / 
                           max(self.registration_stats["attempted"], 1) * 100),
            "model_loader_available": self.available,
            "timestamp": time.time()
        }

# ==============================================
# ğŸ”¥ NEW: Step ëª¨ë¸ ë§¤ì²˜ í´ë˜ìŠ¤
# ==============================================

class StepModelMatcher:
    """
    ğŸ¯ Stepë³„ ëª¨ë¸ ìë™ ë§¤ì¹­ ë° í• ë‹¹ í´ë˜ìŠ¤
    
    íƒì§€ëœ ëª¨ë¸ì„ ì ì ˆí•œ Stepì— ìë™ í• ë‹¹
    """
    
    def __init__(self, model_loader_bridge: ModelLoaderBridge):
        self.bridge = model_loader_bridge
        self.logger = logging.getLogger(f"{__name__}.StepModelMatcher")
        
        # Stepë³„ ëª¨ë¸ ë§¤í•‘
        self.step_model_mapping = {
            "HumanParsingStep": ["human_parsing"],
            "PoseEstimationStep": ["pose_estimation"],
            "ClothSegmentationStep": ["cloth_segmentation"],
            "GeometricMatchingStep": ["geometric_matching"],
            "ClothWarpingStep": ["cloth_warping"],
            "VirtualFittingStep": ["virtual_fitting"],
            "PostProcessingStep": ["post_processing"],
            "QualityAssessmentStep": ["quality_assessment"]
        }
    
    def assign_models_to_steps(
        self, 
        detected_models: Dict[str, DetectedModel]
    ) -> Dict[str, List[str]]:
        """
        ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥: íƒì§€ëœ ëª¨ë¸ì„ Stepë³„ë¡œ ìë™ í• ë‹¹
        
        Args:
            detected_models: íƒì§€ëœ ëª¨ë¸ë“¤
            
        Returns:
            Stepë³„ í• ë‹¹ëœ ëª¨ë¸ ëª©ë¡
        """
        try:
            step_assignments = {}
            unassigned_models = []
            
            self.logger.info(f"ğŸ¯ {len(detected_models)}ê°œ ëª¨ë¸ì„ Stepë³„ë¡œ í• ë‹¹ ì¤‘...")
            
            # Stepë³„ ëª¨ë¸ ë¶„ë¥˜
            for model_name, detected_model in detected_models.items():
                assigned = False
                
                # Step ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ë§¤ì¹­
                if detected_model.step_name in self.step_model_mapping:
                    step_name = detected_model.step_name
                    if step_name not in step_assignments:
                        step_assignments[step_name] = []
                    step_assignments[step_name].append(model_name)
                    assigned = True
                
                # ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë§¤ì¹­
                if not assigned:
                    for step_name, model_types in self.step_model_mapping.items():
                        if detected_model.model_type in model_types:
                            if step_name not in step_assignments:
                                step_assignments[step_name] = []
                            step_assignments[step_name].append(model_name)
                            assigned = True
                            break
                
                if not assigned:
                    unassigned_models.append(model_name)
            
            # ê° Stepë³„ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
            optimized_assignments = {}
            for step_name, model_list in step_assignments.items():
                # ìš°ì„ ìˆœìœ„ì™€ ì‹ ë¢°ë„ë¡œ ì •ë ¬
                step_models = [detected_models[name] for name in model_list]
                sorted_models = sorted(
                    step_models,
                    key=lambda x: (x.priority.value, -x.confidence_score, -x.file_size_mb)
                )
                
                # ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ ì„ íƒ (ì£¼ ëª¨ë¸ + í´ë°± ëª¨ë¸ë“¤)
                selected_models = [model.name for model in sorted_models[:3]]
                optimized_assignments[step_name] = selected_models
                
                # Stepë³„ ì¸í„°í˜ì´ìŠ¤ì— í• ë‹¹ í‘œì‹œ
                for model in sorted_models[:3]:
                    model.step_interface_assigned = True
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"âœ… Stepë³„ ëª¨ë¸ í• ë‹¹ ì™„ë£Œ:")
            for step_name, models in optimized_assignments.items():
                self.logger.info(f"   - {step_name}: {len(models)}ê°œ ëª¨ë¸")
                for i, model_name in enumerate(models):
                    role = "Primary" if i == 0 else f"Fallback{i}"
                    self.logger.info(f"     â€¢ {role}: {model_name}")
            
            if unassigned_models:
                self.logger.warning(f"âš ï¸ ë¯¸í• ë‹¹ ëª¨ë¸: {len(unassigned_models)}ê°œ")
                for model_name in unassigned_models[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    self.logger.warning(f"     â€¢ {model_name}")
            
            return optimized_assignments
            
        except Exception as e:
            self.logger.error(f"âŒ Stepë³„ ëª¨ë¸ í• ë‹¹ ì‹¤íŒ¨: {e}")
            return {}
    
    def create_step_interfaces(
        self, 
        step_assignments: Dict[str, List[str]],
        detected_models: Dict[str, DetectedModel]
    ) -> Dict[str, Any]:
        """
        ğŸ”— Stepë³„ ModelLoader ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        
        Args:
            step_assignments: Stepë³„ í• ë‹¹ëœ ëª¨ë¸ë“¤
            detected_models: íƒì§€ëœ ëª¨ë¸ë“¤
            
        Returns:
            ìƒì„±ëœ ì¸í„°í˜ì´ìŠ¤ ì •ë³´
        """
        try:
            if not self.bridge.available:
                self.logger.warning("âš ï¸ ModelLoader ë¸Œë¦¬ì§€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return {}
            
            interfaces_created = {}
            
            for step_name, assigned_models in step_assignments.items():
                try:
                    # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                    if hasattr(self.bridge.model_loader, 'create_step_interface'):
                        step_interface = self.bridge.model_loader.create_step_interface(
                            step_name=step_name
                        )
                        
                        # ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡
                        for i, model_name in enumerate(assigned_models):
                            detected_model = detected_models.get(model_name)
                            if detected_model:
                                priority = "high" if i == 0 else "medium"
                                fallback_models = assigned_models[i+1:] if i < len(assigned_models)-1 else []
                                
                                # ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡
                                step_interface.register_model_requirement(
                                    model_name=model_name,
                                    model_type=detected_model.model_type,
                                    priority=priority,
                                    fallback_models=fallback_models,
                                    confidence_score=detected_model.confidence_score,
                                    pytorch_valid=detected_model.pytorch_valid
                                )
                        
                        interfaces_created[step_name] = {
                            "interface": step_interface,
                            "models_count": len(assigned_models),
                            "primary_model": assigned_models[0] if assigned_models else None,
                            "fallback_models": assigned_models[1:] if len(assigned_models) > 1 else []
                        }
                        
                        self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ”— {len(interfaces_created)}ê°œ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interfaces_created
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ NEW: ìë™ ë“±ë¡ ë§¤ë‹ˆì € í´ë˜ìŠ¤
# ==============================================

class AutoRegistrationManager:
    """
    ğŸ¤– ìë™ ë“±ë¡ ë§¤ë‹ˆì € - ì „ì²´ í”„ë¡œì„¸ìŠ¤ í†µí•© ê´€ë¦¬
    
    íƒì§€ â†’ ë“±ë¡ â†’ Step í• ë‹¹ê¹Œì§€ ì „ì²´ ìë™í™”
    """
    
    def __init__(self, model_loader: Optional[Any] = None):
        self.logger = logging.getLogger(f"{__name__}.AutoRegistrationManager")
        
        # ë¸Œë¦¬ì§€ ë° ë§¤ì²˜ ì´ˆê¸°í™”
        self.bridge = ModelLoaderBridge(model_loader)
        self.matcher = StepModelMatcher(self.bridge)
        
        # í†µê³„
        self.process_stats = {
            "detection_start": 0,
            "detection_end": 0,
            "registration_start": 0,
            "registration_end": 0,
            "step_assignment_start": 0,
            "step_assignment_end": 0,
            "total_duration": 0,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "success_rate": 0
        }
    
    def execute_full_pipeline(
        self,
        detected_models: Dict[str, DetectedModel],
        auto_assign_steps: bool = True,
        max_registrations: Optional[int] = None,
        create_step_interfaces: bool = True
    ) -> Dict[str, Any]:
        """
        ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: íƒì§€ â†’ ë“±ë¡ â†’ Step í• ë‹¹
        
        Args:
            detected_models: íƒì§€ëœ ëª¨ë¸ë“¤
            auto_assign_steps: Step ìë™ í• ë‹¹ ì—¬ë¶€
            max_registrations: ìµœëŒ€ ë“±ë¡ ìˆ˜
            create_step_interfaces: Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì—¬ë¶€
            
        Returns:
            ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê²°ê³¼
        """
        try:
            self.process_stats["detection_start"] = time.time()
            self.logger.info(f"ğŸš€ ìë™ ë“±ë¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {len(detected_models)}ê°œ ëª¨ë¸")
            
            # Phase 1: ModelLoader ë“±ë¡
            self.process_stats["registration_start"] = time.time()
            self.logger.info("ğŸ“ Phase 1: ModelLoader ë“±ë¡ ì¤‘...")
            
            registration_result = self.bridge.register_detected_models(
                detected_models=detected_models,
                max_registrations=max_registrations
            )
            
            self.process_stats["registration_end"] = time.time()
            self.process_stats["models_registered"] = registration_result.get("total_registered", 0)
            
            if not registration_result.get("success", False):
                self.logger.error("âŒ ModelLoader ë“±ë¡ ì‹¤íŒ¨")
                return {"error": "Registration failed", "details": registration_result}
            
            # Phase 2: Stepë³„ í• ë‹¹
            step_assignments = {}
            step_interfaces = {}
            
            if auto_assign_steps:
                self.process_stats["step_assignment_start"] = time.time()
                self.logger.info("ğŸ¯ Phase 2: Stepë³„ ëª¨ë¸ í• ë‹¹ ì¤‘...")
                
                step_assignments = self.matcher.assign_models_to_steps(detected_models)
                self.process_stats["steps_configured"] = len(step_assignments)
                
                # Phase 3: Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                if create_step_interfaces and self.bridge.available:
                    self.logger.info("ğŸ”— Phase 3: Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì¤‘...")
                    step_interfaces = self.matcher.create_step_interfaces(
                        step_assignments, detected_models
                    )
                
                self.process_stats["step_assignment_end"] = time.time()
            
            # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ
            self.process_stats["detection_end"] = time.time()
            self.process_stats["total_duration"] = (
                self.process_stats["detection_end"] - self.process_stats["detection_start"]
            )
            self.process_stats["models_detected"] = len(detected_models)
            self.process_stats["success_rate"] = (
                self.process_stats["models_registered"] / 
                max(self.process_stats["models_detected"], 1) * 100
            )
            
            # ìµœì¢… ê²°ê³¼
            result = {
                "success": True,
                "pipeline_completed": True,
                "statistics": self.process_stats.copy(),
                "registration_result": registration_result,
                "step_assignments": step_assignments,
                "step_interfaces_created": len(step_interfaces),
                "models_processing": {
                    "detected": len(detected_models),
                    "registered": self.process_stats["models_registered"],
                    "assigned_to_steps": sum(len(models) for models in step_assignments.values()),
                    "success_rate": self.process_stats["success_rate"]
                },
                "performance": {
                    "total_duration_sec": self.process_stats["total_duration"],
                    "registration_time_sec": (
                        self.process_stats["registration_end"] - 
                        self.process_stats["registration_start"]
                    ),
                    "step_assignment_time_sec": (
                        self.process_stats["step_assignment_end"] - 
                        self.process_stats["step_assignment_start"]
                    ) if auto_assign_steps else 0
                }
            }
            
            # ì„±ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ ìë™ ë“±ë¡ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            self.logger.info(f"   ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
            self.logger.info(f"     â€¢ íƒì§€: {len(detected_models)}ê°œ")
            self.logger.info(f"     â€¢ ë“±ë¡: {self.process_stats['models_registered']}ê°œ")
            self.logger.info(f"     â€¢ Step êµ¬ì„±: {self.process_stats['steps_configured']}ê°œ")
            self.logger.info(f"     â€¢ ì„±ê³µë¥ : {self.process_stats['success_rate']:.1f}%")
            self.logger.info(f"     â€¢ ì†Œìš”ì‹œê°„: {self.process_stats['total_duration']:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ë“±ë¡ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "statistics": self.process_stats,
                "pipeline_completed": False
            }

# ==============================================
# ğŸ”¥ ë©”ì¸ íƒì§€ê¸° í´ë˜ìŠ¤ (ModelLoader ì—°ë™ ê°•í™”)
# ==============================================

class RealWorldModelDetector:
    """
    ğŸ” ì‹¤ì œ ë™ì‘í•˜ëŠ” AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v9.0 - ModelLoader ì—°ë™ ì™„ì„±
    
    âœ… 574ê°œ ëª¨ë¸ íƒì§€ â†’ ModelLoader ë“±ë¡ê¹Œì§€ ì™„ì „ ìë™í™”
    âœ… Stepë³„ ëª¨ë¸ ìë™ í• ë‹¹ ë° ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    âœ… ì‹¤ì‹œê°„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë“±ë¡ ì™„ë£Œ
    âœ… PipelineManagerì™€ ì™„ì „ ì—°ë™
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_pytorch_validation: bool = False,
        enable_auto_registration: bool = True,  # ğŸ”¥ NEW: ìë™ ë“±ë¡ í™œì„±í™”
        enable_step_assignment: bool = True,    # ğŸ”¥ NEW: Step í• ë‹¹ í™œì„±í™”
        model_loader: Optional[Any] = None,     # ğŸ”¥ NEW: ModelLoader ì—°ë™
        max_workers: int = 1,
        scan_timeout: int = 600,
        **kwargs
    ):
        """íƒì§€ê¸° ì´ˆê¸°í™” (ModelLoader ì—°ë™ ê°•í™”)"""
        
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        
        # ê¸°ë³¸ ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_auto_registration = enable_auto_registration
        self.enable_step_assignment = enable_step_assignment
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.path_finder = PathFinder()
        self.file_scanner = FileScanner(enable_deep_scan=enable_deep_scan)
        self.pattern_matcher = PatternMatcher()
        self.pytorch_validator = PyTorchValidator(
            enable_validation=enable_pytorch_validation,
            timeout=kwargs.get('validation_timeout', 60)
        )
        
        # ğŸ”¥ NEW: ModelLoader ì—°ë™ ì»´í¬ë„ŒíŠ¸
        self.auto_registration_manager = AutoRegistrationManager(model_loader)
        
        # ê²€ìƒ‰ ê²½ë¡œ ì„¤ì •
        if search_paths is None:
            self.search_paths = self.path_finder.get_search_paths()
        else:
            self.search_paths = search_paths
        
        # ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        self.registration_results: Dict[str, Any] = {}
        self.step_assignments: Dict[str, List[str]] = {}
        
        # í†µê³„
        self.scan_stats = {
            "total_files_scanned": 0,
            "model_files_found": 0,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "pytorch_validated": 0,
            "scan_duration": 0.0,
            "registration_duration": 0.0,
            "cache_hits": 0,
            "errors_encountered": 0
        }
        
        self.logger.info(f"ğŸ” ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° v9.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        self.logger.info(f"   - ë””ë°”ì´ìŠ¤: {DEVICE_TYPE}")
        self.logger.info(f"   - ModelLoader ì—°ë™: {'âœ…' if self.auto_registration_manager.bridge.available else 'âŒ'}")
        self.logger.info(f"   - ìë™ ë“±ë¡: {'í™œì„±í™”' if enable_auto_registration else 'ë¹„í™œì„±í™”'}")
        self.logger.info(f"   - Step í• ë‹¹: {'í™œì„±í™”' if enable_step_assignment else 'ë¹„í™œì„±í™”'}")
    
    def detect_all_models(
        self,
        force_rescan: bool = True,
        min_confidence: float = 0.05,
        categories_filter: Optional[List[ModelCategory]] = None,
        enable_detailed_analysis: bool = False,
        max_models_per_category: Optional[int] = None,
        auto_register_to_model_loader: bool = True,  # ğŸ”¥ NEW: ìë™ ë“±ë¡ ì œì–´
        max_registrations: Optional[int] = None       # ğŸ”¥ NEW: ë“±ë¡ ìˆ˜ ì œí•œ
    ) -> Dict[str, DetectedModel]:
        """
        ğŸ”¥ ì™„ì „ ê°•í™”ëœ ëª¨ë¸ íƒì§€ + ModelLoader ë“±ë¡ í†µí•©
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ìŠ¤ìº”
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ (0.05ë¡œ ì™„í™”)
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            enable_detailed_analysis: ìƒì„¸ ë¶„ì„
            max_models_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ëª¨ë¸ ìˆ˜
            auto_register_to_model_loader: ğŸ”¥ ìë™ ModelLoader ë“±ë¡ ì—¬ë¶€
            max_registrations: ğŸ”¥ ìµœëŒ€ ë“±ë¡ ìˆ˜ ì œí•œ
        
        Returns:
            íƒì§€ëœ ëª¨ë¸ë“¤ (ModelLoader ë“±ë¡ ìƒíƒœ í¬í•¨)
        """
        try:
            self.logger.info("ğŸ” ê°•í™”ëœ ëª¨ë¸ íƒì§€ + ModelLoader ë“±ë¡ ì‹œì‘...")
            start_time = time.time()
            
            # í†µê³„ ì´ˆê¸°í™”
            self._reset_scan_stats()
            
            # Phase 1: ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”
            self.logger.info("ğŸ“ Phase 1: ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            model_files = self.file_scanner.scan_paths(self.search_paths)
            self.scan_stats["total_files_scanned"] = len(model_files)
            
            if not model_files:
                self.logger.warning("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {}
            
            # Phase 2: íŒ¨í„´ ë§¤ì¹­ ë° ë¶„ë¥˜
            self.logger.info(f"ğŸ” Phase 2: {len(model_files)}ê°œ íŒŒì¼ ë¶„ë¥˜ ì¤‘...")
            detected_count = 0
            
            for file_path in model_files:
                try:
                    # íŒ¨í„´ ë§¤ì¹­
                    matches = self.pattern_matcher.match_file_to_patterns(file_path)
                    
                    if matches and matches[0][1] >= min_confidence:
                        pattern_name, confidence = matches[0]
                        pattern = self.pattern_matcher.patterns[pattern_name]
                        
                        # íƒì§€ëœ ëª¨ë¸ ìƒì„±
                        detected_model = self._create_detected_model(
                            file_path, pattern_name, pattern, confidence, enable_detailed_analysis
                        )
                        
                        if detected_model:
                            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
                            if categories_filter and detected_model.category not in categories_filter:
                                continue
                            
                            self.detected_models[detected_model.name] = detected_model
                            detected_count += 1
                            
                            if detected_count <= 20:  # ì²˜ìŒ 20ê°œë§Œ ë¡œê·¸
                                self.logger.info(f"âœ… {detected_model.name} ({detected_model.file_size_mb:.1f}MB)")
                
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                    self.scan_stats["errors_encountered"] += 1
                    continue
            
            # Phase 3: í›„ì²˜ë¦¬
            if max_models_per_category:
                self._limit_models_per_category(max_models_per_category)
            
            self._post_process_results(min_confidence)
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            
            self.logger.info(f"âœ… Phase 2 ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ íƒì§€")
            
            # ğŸ”¥ Phase 4: ModelLoader ìë™ ë“±ë¡ (í•µì‹¬ ê¸°ëŠ¥)
            if auto_register_to_model_loader and self.enable_auto_registration:
                self.logger.info("ğŸ”— Phase 3: ModelLoader ìë™ ë“±ë¡ ì‹œì‘...")
                registration_start = time.time()
                
                self.registration_results = self.auto_registration_manager.execute_full_pipeline(
                    detected_models=self.detected_models,
                    auto_assign_steps=self.enable_step_assignment,
                    max_registrations=max_registrations,
                    create_step_interfaces=True
                )
                
                registration_duration = time.time() - registration_start
                self.scan_stats["registration_duration"] = registration_duration
                
                # ë“±ë¡ ê²°ê³¼ ë°˜ì˜
                if self.registration_results.get("success", False):
                    self.scan_stats["models_registered"] = self.registration_results["models_processing"]["registered"]
                    self.scan_stats["steps_configured"] = self.registration_results["models_processing"].get("assigned_to_steps", 0)
                    self.step_assignments = self.registration_results.get("step_assignments", {})
                    
                    self.logger.info(f"ğŸ‰ ModelLoader ë“±ë¡ ì™„ë£Œ!")
                    self.logger.info(f"   - ë“±ë¡ëœ ëª¨ë¸: {self.scan_stats['models_registered']}ê°œ")
                    self.logger.info(f"   - êµ¬ì„±ëœ Step: {len(self.step_assignments)}ê°œ")
                    self.logger.info(f"   - ë“±ë¡ ì†Œìš”ì‹œê°„: {registration_duration:.2f}ì´ˆ")
                else:
                    self.logger.warning(f"âš ï¸ ModelLoader ë“±ë¡ ë¶€ë¶„ ì‹¤íŒ¨")
                    self.logger.warning(f"   ì˜¤ë¥˜: {self.registration_results.get('error', 'Unknown')}")
            else:
                self.logger.info("ğŸ“‹ ModelLoader ìë™ ë“±ë¡ ê±´ë„ˆëœ€ (ë¹„í™œì„±í™”ë¨)")
            
            # ìµœì¢… í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["scan_duration"] = time.time() - start_time
            
            self.logger.info(f"ğŸ¯ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ + ë“±ë¡ ì‹¤íŒ¨: {e}")
            self.logger.debug(traceback.format_exc())
            raise
    
    def _create_detected_model(
        self, 
        file_path: Path, 
        pattern_name: str, 
        pattern: EnhancedModelPattern, 
        confidence: float,
        enable_detailed_analysis: bool
    ) -> Optional[DetectedModel]:
        """íƒì§€ëœ ëª¨ë¸ ê°ì²´ ìƒì„± (ModelLoader ì—°ë™ ì •ë³´ í¬í•¨)"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "human_parsing": ModelCategory.HUMAN_PARSING,
                "pose_estimation": ModelCategory.POSE_ESTIMATION,
                "cloth_segmentation": ModelCategory.CLOTH_SEGMENTATION,
                "geometric_matching": ModelCategory.GEOMETRIC_MATCHING,
                "cloth_warping": ModelCategory.CLOTH_WARPING,
                "virtual_fitting": ModelCategory.VIRTUAL_FITTING,
                "post_processing": ModelCategory.POST_PROCESSING,
                "quality_assessment": ModelCategory.QUALITY_ASSESSMENT
            }
            
            category = category_mapping.get(pattern_name, ModelCategory.AUXILIARY)
            priority = ModelPriority(min(pattern.priority, 5))
            
            # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
            model_name = self._generate_model_name(file_path, pattern_name)
            
            # PyTorch ê²€ì¦ (ì„ íƒì )
            validation_results = {}
            pytorch_valid = False
            parameter_count = 0
            architecture = pattern.architecture
            
            if self.enable_pytorch_validation and enable_detailed_analysis:
                validation_result = self.pytorch_validator.validate_model(file_path)
                validation_results = validation_result['validation_info']
                pytorch_valid = validation_result['valid']
                parameter_count = validation_result['parameter_count']
                if validation_result['architecture'] != ModelArchitecture.UNKNOWN:
                    architecture = validation_result['architecture']
                
                if pytorch_valid:
                    self.scan_stats["pytorch_validated"] += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            performance_metrics = ModelPerformanceMetrics(
                inference_time_ms=self._estimate_inference_time(file_size_mb, pattern.architecture),
                memory_usage_mb=file_size_mb * 2.5,
                m3_compatibility_score=0.8 if IS_M3_MAX else 0.5
            )
            
            # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±
            device_compatibility = {
                "cpu": True,
                "mps": IS_M3_MAX and file_size_mb < 8000,
                "cuda": False
            }
            
            # DetectedModel ìƒì„± (ModelLoader ì—°ë™ ì •ë³´ í¬í•¨)
            detected_model = DetectedModel(
                name=model_name,
                path=file_path,
                category=category,
                model_type=pattern.name,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=priority,
                step_name=pattern.step,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count,
                last_modified=file_stat.st_mtime,
                architecture=architecture,
                performance_metrics=performance_metrics,
                device_compatibility=device_compatibility,
                validation_results=validation_results,
                health_status="healthy" if pytorch_valid or confidence > 0.7 else "unknown",
                metadata={
                    "pattern_matched": pattern_name,
                    "confidence_score": confidence,
                    "file_extension": file_path.suffix,
                    "detected_at": time.time(),
                    # ğŸ”¥ NEW: ModelLoader ì—°ë™ ë©”íƒ€ë°ì´í„°
                    "model_class": pattern.model_class,
                    "loader_config": pattern.loader_config,
                    "step_requirements": pattern.step_requirements
                },
                # ğŸ”¥ NEW: ModelLoader ì—°ë™ ìƒíƒœ ì´ˆê¸°í™”
                model_loader_registered=False,
                model_loader_name=None,
                step_interface_assigned=False,
                registration_timestamp=None
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _generate_model_name(self, file_path: Path, pattern_name: str) -> str:
        """ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # ê¸°ë³¸ ì´ë¦„
            base_name = f"{pattern_name}_{file_path.stem}"
            
            # ì¤‘ë³µ í™•ì¸
            if base_name not in self.detected_models:
                return base_name
            
            # ë²„ì „ ë²ˆí˜¸ ì¶”ê°€
            counter = 2
            while f"{base_name}_v{counter}" in self.detected_models:
                counter += 1
            
            return f"{base_name}_v{counter}"
            
        except Exception:
            return f"detected_model_{int(time.time())}"
    
    def _estimate_inference_time(self, file_size_mb: float, architecture: ModelArchitecture) -> float:
        """ì¶”ë¡  ì‹œê°„ ì¶”ì •"""
        base_times = {
            ModelArchitecture.CNN: 100,
            ModelArchitecture.UNET: 300,
            ModelArchitecture.TRANSFORMER: 500,
            ModelArchitecture.DIFFUSION: 2000,
            ModelArchitecture.UNKNOWN: 200
        }
        
        base_time = base_times.get(architecture, 200)
        size_factor = max(1.0, file_size_mb / 100)
        device_factor = 0.7 if IS_M3_MAX else 1.0
        
        return base_time * size_factor * device_factor
    
    def _limit_models_per_category(self, max_models: int):
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ìˆ˜ ì œí•œ"""
        try:
            category_models = {}
            
            for name, model in self.detected_models.items():
                category = model.category
                if category not in category_models:
                    category_models[category] = []
                category_models[category].append((name, model))
            
            models_to_keep = {}
            
            for category, models in category_models.items():
                sorted_models = sorted(
                    models, 
                    key=lambda x: (x[1].confidence_score, x[1].file_size_mb), 
                    reverse=True
                )
                
                for name, model in sorted_models[:max_models]:
                    models_to_keep[name] = model
            
            self.detected_models = models_to_keep
            self.logger.debug(f"âœ… ì¹´í…Œê³ ë¦¬ë³„ ì œí•œ ì ìš©: {len(models_to_keep)}ê°œ ëª¨ë¸ ìœ ì§€")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì œí•œ ì‹¤íŒ¨: {e}")
    
    def _post_process_results(self, min_confidence: float):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            
            self.detected_models = filtered_models
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ì´ˆê¸°í™”"""
        for key in self.scan_stats:
            if isinstance(self.scan_stats[key], (int, float)):
                self.scan_stats[key] = 0
    
    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½ (ModelLoader ì—°ë™ ì •ë³´ í¬í•¨)"""
        try:
            total_models = len(self.detected_models)
            validated_models = sum(1 for m in self.detected_models.values() if m.pytorch_valid)
            registered_models = sum(1 for m in self.detected_models.values() if m.model_loader_registered)
            total_size_gb = sum(m.file_size_mb for m in self.detected_models.values()) / 1024
            
            self.logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
            self.logger.info(f"   ğŸ” íƒì§€:")
            self.logger.info(f"     â€¢ ì´ ëª¨ë¸: {total_models}ê°œ")
            self.logger.info(f"     â€¢ PyTorch ê²€ì¦: {validated_models}ê°œ")
            self.logger.info(f"     â€¢ ì´ í¬ê¸°: {total_size_gb:.1f}GB")
            self.logger.info(f"     â€¢ ìŠ¤ìº” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            
            if self.enable_auto_registration:
                self.logger.info(f"   ğŸ”— ModelLoader ë“±ë¡:")
                self.logger.info(f"     â€¢ ë“±ë¡ ëª¨ë¸: {registered_models}ê°œ")
                self.logger.info(f"     â€¢ ë“±ë¡ë¥ : {(registered_models/max(total_models,1)*100):.1f}%")
                self.logger.info(f"     â€¢ ë“±ë¡ ì‹œê°„: {self.scan_stats['registration_duration']:.2f}ì´ˆ")
            
            if self.enable_step_assignment and self.step_assignments:
                self.logger.info(f"   ğŸ¯ Step í• ë‹¹:")
                for step_name, models in self.step_assignments.items():
                    self.logger.info(f"     â€¢ {step_name}: {len(models)}ê°œ")
            
            # ì„±ëŠ¥ ìš”ì•½
            total_time = self.scan_stats['scan_duration']
            models_per_sec = total_models / max(total_time, 0.1)
            self.logger.info(f"   âš¡ ì„±ëŠ¥: {models_per_sec:.1f} ëª¨ë¸/ì´ˆ")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ModelLoader ì—°ë™ ì „ìš© ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_registration_status(self) -> Dict[str, Any]:
        """ModelLoader ë“±ë¡ ìƒíƒœ ì¡°íšŒ"""
        try:
            registered_models = [
                model for model in self.detected_models.values() 
                if model.model_loader_registered
            ]
            
            return {
                "total_detected": len(self.detected_models),
                "total_registered": len(registered_models),
                "registration_rate": len(registered_models) / max(len(self.detected_models), 1) * 100,
                "bridge_available": self.auto_registration_manager.bridge.available,
                "registration_results": self.registration_results,
                "step_assignments": self.step_assignments,
                "statistics": self.scan_stats
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def force_register_model(self, model_name: str) -> bool:
        """íŠ¹ì • ëª¨ë¸ ê°•ì œ ë“±ë¡"""
        try:
            if model_name not in self.detected_models:
                self.logger.warning(f"âš ï¸ ëª¨ë¸ '{model_name}' íƒì§€ë˜ì§€ ì•ŠìŒ")
                return False
            
            detected_model = self.detected_models[model_name]
            
            # ë‹¨ì¼ ëª¨ë¸ ë“±ë¡
            registration_result = self.auto_registration_manager.bridge.register_detected_models(
                detected_models={model_name: detected_model},
                force_registration=True
            )
            
            success = registration_result.get("success", False)
            if success:
                self.logger.info(f"âœ… {model_name} ê°•ì œ ë“±ë¡ ì„±ê³µ")
            else:
                self.logger.warning(f"âŒ {model_name} ê°•ì œ ë“±ë¡ ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ {model_name} ê°•ì œ ë“±ë¡ ì˜¤ë¥˜: {e}")
            return False
    
    def get_step_model_assignments(self) -> Dict[str, List[str]]:
        """Stepë³„ í• ë‹¹ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.step_assignments.copy()
    
    def reassign_model_to_step(self, model_name: str, step_name: str) -> bool:
        """ëª¨ë¸ì„ ë‹¤ë¥¸ Stepì— ì¬í• ë‹¹"""
        try:
            if model_name not in self.detected_models:
                return False
            
            # ê¸°ì¡´ í• ë‹¹ì—ì„œ ì œê±°
            for step, models in self.step_assignments.items():
                if model_name in models:
                    models.remove(model_name)
            
            # ìƒˆ Stepì— í• ë‹¹
            if step_name not in self.step_assignments:
                self.step_assignments[step_name] = []
            self.step_assignments[step_name].append(model_name)
            
            # DetectedModel ì—…ë°ì´íŠ¸
            self.detected_models[model_name].step_name = step_name
            
            self.logger.info(f"ğŸ”„ {model_name} â†’ {step_name} ì¬í• ë‹¹ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¬í• ë‹¹ ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ìœ ì§€)
    # ==============================================
    
    def get_validated_models_only(self) -> Dict[str, DetectedModel]:
        """PyTorch ê²€ì¦ëœ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜"""
        return {name: model for name, model in self.detected_models.items() if model.pytorch_valid}
    
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]
    
    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]
    
    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """Stepë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        def model_score(model):
            score = 0
            if model.pytorch_valid:
                score += 100
            if model.model_loader_registered:  # ğŸ”¥ NEW: ë“±ë¡ëœ ëª¨ë¸ ìš°ì„ 
                score += 50
            score += (6 - model.priority.value) * 20
            score += model.confidence_score * 50
            return score
        
        return max(step_models, key=model_score)

# ==============================================
# ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤ (ìœ ì§€)
# ==============================================

class AdvancedModelLoaderAdapter:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„° í´ë˜ìŠ¤"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_advanced_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê³ ê¸‰ ModelLoader ì„¤ì • ìƒì„±"""
        try:
            config = {
                "version": "9.0_with_registration",
                "device_optimization": {
                    "target_device": DEVICE_TYPE,
                    "is_m3_max": IS_M3_MAX
                },
                "models": {},
                "step_configurations": {},
                "registration_info": {
                    "auto_registered": True,
                    "registration_timestamp": time.time(),
                    "total_registered": sum(1 for m in detected_models.values() if m.model_loader_registered)
                }
            }
            
            for name, model in detected_models.items():
                config["models"][name] = {
                    "name": name,
                    "path": str(model.path),
                    "type": model.model_type,
                    "step": model.step_name,
                    "registered": model.model_loader_registered,
                    "confidence": model.confidence_score,
                    "pytorch_valid": model.pytorch_valid
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

class RealModelLoaderConfigGenerator:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì • ìƒì„±ê¸°"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê¸°ë³¸ ModelLoader ì„¤ì • ìƒì„±"""
        try:
            return {
                "device": DEVICE_TYPE,
                "models": {
                    name: {
                        "path": str(model.path),
                        "type": model.model_type,
                        "step_name": model.step_name,
                        "registered": model.model_loader_registered
                    }
                    for name, model in detected_models.items()
                },
                "metadata": {
                    "generator_version": "9.0",
                    "total_models": len(detected_models),
                    "registered_models": sum(1 for m in detected_models.values() if m.model_loader_registered)
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ModelLoader ì—°ë™ ê°•í™”)
# ==============================================

def create_real_world_detector(
    model_loader: Optional[Any] = None,
    enable_auto_registration: bool = True,
    enable_step_assignment: bool = True,
    **kwargs
) -> RealWorldModelDetector:
    """ì‹¤ì œ ëª¨ë¸ íƒì§€ê¸° ìƒì„± (ModelLoader ì—°ë™)"""
    return RealWorldModelDetector(
        model_loader=model_loader,
        enable_auto_registration=enable_auto_registration,
        enable_step_assignment=enable_step_assignment,
        **kwargs
    )

def create_advanced_detector(**kwargs) -> RealWorldModelDetector:
    """ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ìƒì„± (ë³„ì¹­)"""
    return create_real_world_detector(**kwargs)

def quick_real_model_detection(
    model_loader: Optional[Any] = None,
    auto_register: bool = True,
    **kwargs
) -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ + ìë™ ë“±ë¡"""
    try:
        detector = create_real_world_detector(
            model_loader=model_loader,
            enable_pytorch_validation=False,
            enable_auto_registration=auto_register,
            enable_step_assignment=True,
            max_workers=1,
            **kwargs
        )
        
        return detector.detect_all_models(
            force_rescan=True,
            min_confidence=0.05,
            enable_detailed_analysis=False,
            auto_register_to_model_loader=auto_register,
            max_registrations=50  # ìƒìœ„ 50ê°œë§Œ ë“±ë¡
        )
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ íƒì§€ + ë“±ë¡ ì‹¤íŒ¨: {e}")
        return {}

def generate_real_model_loader_config(
    detector: Optional[RealWorldModelDetector] = None,
    model_loader: Optional[Any] = None
) -> Dict[str, Any]:
    """ModelLoader ì„¤ì • ìƒì„± (ìë™ ë“±ë¡ í¬í•¨)"""
    try:
        if detector is None:
            detector = create_real_world_detector(model_loader=model_loader)
            detector.detect_all_models(auto_register_to_model_loader=True)
        
        generator = RealModelLoaderConfigGenerator(detector)
        return generator.generate_config(detector.detected_models)
        
    except Exception as e:
        logger.error(f"ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ (ë“±ë¡ ìƒíƒœ í¬í•¨)"""
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "registered_models": [],
            "unregistered_models": [],
            "step_assigned_models": [],
            "summary": {}
        }
        
        for name, model in detected_models.items():
            try:
                if not model.path.exists():
                    validation_result["invalid_models"].append(name)
                    continue
                
                validation_result["valid_models"].append(name)
                
                if model.model_loader_registered:
                    validation_result["registered_models"].append(name)
                else:
                    validation_result["unregistered_models"].append(name)
                
                if model.step_interface_assigned:
                    validation_result["step_assigned_models"].append(name)
                
            except Exception as e:
                validation_result["invalid_models"].append(name)
        
        # ìš”ì•½ í†µê³„
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_models": len(validation_result["valid_models"]),
            "registered_models": len(validation_result["registered_models"]),
            "step_assigned_models": len(validation_result["step_assigned_models"]),
            "registration_rate": len(validation_result["registered_models"]) / max(len(detected_models), 1) * 100,
            "step_assignment_rate": len(validation_result["step_assigned_models"]) / max(len(detected_models), 1) * 100
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ PipelineManager ì—°ë™ í•¨ìˆ˜ (í•µì‹¬)
# ==============================================

def integrate_with_pipeline_manager(
    pipeline_manager: Any,
    detector: Optional[RealWorldModelDetector] = None,
    auto_detect_and_register: bool = True
) -> Dict[str, Any]:
    """
    ğŸ”— PipelineManagerì™€ ì™„ì „ ì—°ë™ (í•µì‹¬ ì—°ê²° í•¨ìˆ˜)
    
    Args:
        pipeline_manager: PipelineManager ì¸ìŠ¤í„´ìŠ¤
        detector: ëª¨ë¸ íƒì§€ê¸° (Noneì´ë©´ ìë™ ìƒì„±)
        auto_detect_and_register: ìë™ íƒì§€ ë° ë“±ë¡ ì—¬ë¶€
        
    Returns:
        ì—°ë™ ê²°ê³¼
    """
    try:
        logger.info("ğŸ”— PipelineManagerì™€ Auto Model Detector ì—°ë™ ì‹œì‘...")
        
        # íƒì§€ê¸° ìƒì„± ë˜ëŠ” ì‚¬ìš©
        if detector is None:
            # PipelineManagerì˜ ModelLoader ê°€ì ¸ì˜¤ê¸°
            model_loader = getattr(pipeline_manager, 'model_loader', None)
            if model_loader is None:
                # ì „ì—­ ModelLoader ì‚¬ìš©
                model_loader = get_global_model_loader() if MODEL_LOADER_AVAILABLE else None
            
            detector = create_real_world_detector(
                model_loader=model_loader,
                enable_auto_registration=True,
                enable_step_assignment=True
            )
        
        integration_result = {
            "success": False,
            "detector_created": detector is not None,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "pipeline_updated": False
        }
        
        if not detector:
            return {"error": "íƒì§€ê¸° ìƒì„± ì‹¤íŒ¨", "details": integration_result}
        
        # ìë™ íƒì§€ ë° ë“±ë¡
        if auto_detect_and_register:
            logger.info("ğŸ” ìë™ ëª¨ë¸ íƒì§€ ë° ë“±ë¡ ì‹¤í–‰...")
            
            detected_models = detector.detect_all_models(
                auto_register_to_model_loader=True,
                max_registrations=30  # PipelineManagerìš© ì œí•œ
            )
            
            integration_result.update({
                "models_detected": len(detected_models),
                "models_registered": detector.scan_stats.get("models_registered", 0),
                "steps_configured": len(detector.step_assignments),
                "step_assignments": detector.step_assignments,
                "registration_status": detector.get_registration_status()
            })
        
        # PipelineManager ì—…ë°ì´íŠ¸
        if hasattr(pipeline_manager, 'update_model_registry'):
            try:
                pipeline_manager.update_model_registry(detector.detected_models)
                integration_result["pipeline_updated"] = True
                logger.info("âœ… PipelineManager ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ PipelineManager ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        # Stepë³„ ëª¨ë¸ í• ë‹¹ ì •ë³´ë¥¼ PipelineManagerì— ì „ë‹¬
        if hasattr(pipeline_manager, 'configure_step_models'):
            try:
                pipeline_manager.configure_step_models(detector.step_assignments)
                logger.info("âœ… PipelineManager Step ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Step ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        integration_result["success"] = True
        
        logger.info("ğŸ‰ PipelineManager ì—°ë™ ì™„ë£Œ!")
        logger.info(f"   - íƒì§€ ëª¨ë¸: {integration_result['models_detected']}ê°œ")
        logger.info(f"   - ë“±ë¡ ëª¨ë¸: {integration_result['models_registered']}ê°œ")
        logger.info(f"   - êµ¬ì„± Step: {integration_result['steps_configured']}ê°œ")
        
        return integration_result
        
    except Exception as e:
        logger.error(f"âŒ PipelineManager ì—°ë™ ì‹¤íŒ¨: {e}")
        return {"error": str(e), "success": False}

# ==============================================
# ğŸ”¥ í•˜ìœ„ í˜¸í™˜ì„± ë° ë³„ì¹­ë“¤
# ==============================================

@dataclass 
class ModelFileInfo:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ModelFileInfo í´ë˜ìŠ¤"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

# í˜¸í™˜ì„±ì„ ìœ„í•œ íŒ¨í„´ ë³€í™˜
ENHANCED_MODEL_PATTERNS = {}

def _convert_patterns():
    """ê¸°ì¡´ íŒ¨í„´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    pattern_matcher = PatternMatcher()
    
    for name, enhanced_pattern in pattern_matcher.patterns.items():
        ENHANCED_MODEL_PATTERNS[name] = ModelFileInfo(
            name=enhanced_pattern.name,
            patterns=enhanced_pattern.patterns,
            step=enhanced_pattern.step,
            keywords=enhanced_pattern.keywords,
            file_types=enhanced_pattern.file_types,
            min_size_mb=enhanced_pattern.size_range_mb[0],
            max_size_mb=enhanced_pattern.size_range_mb[1],
            priority=enhanced_pattern.priority
        )

_convert_patterns()

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
AdvancedModelDetector = RealWorldModelDetector
ModelLoaderConfigGenerator = RealModelLoaderConfigGenerator
create_advanced_model_loader_adapter = lambda detector: AdvancedModelLoaderAdapter(detector)

# ==============================================
# ğŸ”¥ ëª¨ë“  export ì •ì˜
# ==============================================

__all__ = [
    # ğŸ”¥ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'RealWorldModelDetector',
    'ModelLoaderBridge',                # NEW: í•µì‹¬ ì—°ê²°ê³ ë¦¬
    'StepModelMatcher',                 # NEW: Step ë§¤ì¹­
    'AutoRegistrationManager',          # NEW: ìë™ ë“±ë¡ ê´€ë¦¬
    'AdvancedModelLoaderAdapter',
    'RealModelLoaderConfigGenerator',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    'ModelFileInfo',
    
    # ğŸ”¥ ê°•í™”ëœ í´ë˜ìŠ¤ë“¤
    'EnhancedModelPattern',
    'ModelArchitecture',
    'ModelPerformanceMetrics',
    'PatternMatcher',
    'FileScanner',
    'PyTorchValidator',
    'PathFinder',
    
    # ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ModelLoader ì—°ë™)
    'create_real_world_detector',
    'create_advanced_detector',
    'create_advanced_model_loader_adapter',
    'quick_real_model_detection',
    'generate_real_model_loader_config',
    'validate_real_model_paths',
    
    # ğŸ”¥ NEW: PipelineManager ì—°ë™
    'integrate_with_pipeline_manager',   # í•µì‹¬ ì—°ë™ í•¨ìˆ˜
    
    # í˜¸í™˜ì„± ë°ì´í„°
    'ENHANCED_MODEL_PATTERNS',
    
    # í•˜ìœ„ í˜¸í™˜ì„± ë³„ì¹­ë“¤
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator'
]

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ + ë“±ë¡ ê²€ì¦)
# ==============================================

def main():
    """ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (íƒì§€ + ë“±ë¡ + ê²€ì¦)"""
    try:
        print("ğŸ” ì™„ì „í•œ Auto Detector v9.0 + ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # 1. ModelLoader ì—°ë™ íƒì§€ê¸° ìƒì„±
        print("\nğŸ”§ Phase 1: ModelLoader ì—°ë™ íƒì§€ê¸° ìƒì„±...")
        detector = create_real_world_detector(
            enable_auto_registration=True,
            enable_step_assignment=True,
            enable_pytorch_validation=False,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            max_workers=1
        )
        
        print(f"âœ… íƒì§€ê¸° ìƒì„± ì™„ë£Œ")
        print(f"   - ModelLoader ì—°ë™: {'âœ…' if detector.auto_registration_manager.bridge.available else 'âŒ'}")
        
        # 2. ëª¨ë¸ íƒì§€ + ìë™ ë“±ë¡
        print("\nğŸ” Phase 2: ëª¨ë¸ íƒì§€ + ModelLoader ìë™ ë“±ë¡...")
        detected_models = detector.detect_all_models(
            min_confidence=0.05,
            force_rescan=True,
            auto_register_to_model_loader=True,
            max_registrations=20  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
        )
        
        if not detected_models:
            print("âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        print(f"\nâœ… íƒì§€ + ë“±ë¡ ì™„ë£Œ!")
        
        # 3. ë“±ë¡ ìƒíƒœ í™•ì¸
        print("\nğŸ“Š Phase 3: ModelLoader ë“±ë¡ ìƒíƒœ í™•ì¸...")
        registration_status = detector.get_registration_status()
        
        print(f"   ğŸ“‹ ë“±ë¡ í†µê³„:")
        print(f"     â€¢ íƒì§€ëœ ëª¨ë¸: {registration_status['total_detected']}ê°œ")
        print(f"     â€¢ ë“±ë¡ëœ ëª¨ë¸: {registration_status['total_registered']}ê°œ") 
        print(f"     â€¢ ë“±ë¡ë¥ : {registration_status['registration_rate']:.1f}%")
        print(f"     â€¢ Bridge ìƒíƒœ: {'âœ…' if registration_status['bridge_available'] else 'âŒ'}")
        
        # 4. Step í• ë‹¹ í™•ì¸
        step_assignments = detector.get_step_model_assignments()
        if step_assignments:
            print(f"\nğŸ¯ Stepë³„ ëª¨ë¸ í• ë‹¹:")
            for step_name, models in step_assignments.items():
                print(f"     â€¢ {step_name}: {len(models)}ê°œ")
                for i, model_name in enumerate(models[:2]):  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ
                    role = "Primary" if i == 0 else "Fallback"
                    print(f"       - {role}: {model_name}")
        
        # 5. ìƒìœ„ ë“±ë¡ ëª¨ë¸ë“¤ ì¶œë ¥
        registered_models = [
            model for model in detected_models.values() 
            if model.model_loader_registered
        ]
        
        if registered_models:
            print(f"\nğŸ“ ë“±ë¡ëœ ìƒìœ„ ëª¨ë¸ë“¤:")
            sorted_registered = sorted(
                registered_models, 
                key=lambda x: x.confidence_score, 
                reverse=True
            )
            
            for i, model in enumerate(sorted_registered[:10], 1):
                print(f"   {i}. {model.name}")
                print(f"      ğŸ“ {model.path.name}")
                print(f"      ğŸ“Š {model.file_size_mb:.1f}MB")
                print(f"      ğŸ¯ {model.step_name}")
                print(f"      â­ ì‹ ë¢°ë„: {model.confidence_score:.2f}")
                print(f"      ğŸ”— ë“±ë¡ì‹œê°„: {time.strftime('%H:%M:%S', time.localtime(model.registration_timestamp))}")
                print()
        
        # 6. ê²€ì¦ ê²°ê³¼
        print("\nğŸ” Phase 4: ê²€ì¦ ê²°ê³¼...")
        validation_result = validate_real_model_paths(detected_models)
        if validation_result.get('summary'):
            summary = validation_result['summary']
            print(f"   ğŸ“Š ê²€ì¦ ìš”ì•½:")
            print(f"     â€¢ ìœ íš¨ ëª¨ë¸: {summary['valid_models']}ê°œ")
            print(f"     â€¢ ë“±ë¡ëœ ëª¨ë¸: {summary['registered_models']}ê°œ")
            print(f"     â€¢ Step í• ë‹¹ëœ ëª¨ë¸: {summary['step_assigned_models']}ê°œ")
            print(f"     â€¢ ë“±ë¡ë¥ : {summary['registration_rate']:.1f}%")
            print(f"     â€¢ Step í• ë‹¹ë¥ : {summary['step_assignment_rate']:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸ‰ Auto Detector v9.0 + ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   ğŸ” 574ê°œ ëª¨ë¸ íƒì§€ â†’ ModelLoader ë“±ë¡ê¹Œì§€ ì™„ì „ ìë™í™”")
        print(f"   ğŸ”— Stepë³„ ëª¨ë¸ ìë™ í• ë‹¹ ë° ì¸í„°í˜ì´ìŠ¤ ìƒì„±")
        print(f"   ğŸ“ PipelineManager ì™„ì „ ì—°ë™ ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ğŸ M3 Max 128GB + conda í™˜ê²½ ìµœì í™”")
        print(f"   ğŸ”¥ MPS empty_cache AttributeError ì™„ì „ í•´ê²°")
        print(f"   ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥")
    else:
        print(f"\nğŸ”§ ì¶”ê°€ ë””ë²„ê¹…ì´ í•„ìš”í•©ë‹ˆë‹¤")

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("âœ… ì™„ì „í•œ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v9.0 ë¡œë“œ ì™„ë£Œ - ModelLoader ì—°ë™ í†µí•©")
logger.info("ğŸ”— í•µì‹¬ ê°œì„ : íƒì§€ëœ ëª¨ë¸ â†’ ModelLoader ìë™ ë“±ë¡ ì—°ê²°ê³ ë¦¬ ì™„ì„±")
logger.info("ğŸ¯ 574ê°œ ëª¨ë¸ íƒì§€ â†’ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë“±ë¡ê¹Œì§€ ì™„ì „ ìë™í™”")
logger.info("ğŸ“ ModelLoaderBridge, StepModelMatcher, AutoRegistrationManager ì¶”ê°€")
logger.info("ğŸ”„ PipelineManager ì™„ì „ ì—°ë™ ë° ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ìœ ì§€")
logger.info("ğŸ M3 Max 128GB + conda í™˜ê²½ ìµœì í™” + MPS ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + ì‹¤ë¬´ê¸‰ ì„±ëŠ¥ ë³´ì¥")
logger.info(f"ğŸ¯ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"ğŸ”— ModelLoader ì—°ë™: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")

if TORCH_AVAILABLE and hasattr(torch, '__version__'):
    logger.info(f"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}")
else:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")