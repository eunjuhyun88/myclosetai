# app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ” ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ ìë™ ë°œê²¬
âœ… ì‹¤ì œ 72GB+ ëª¨ë¸ë“¤ê³¼ ì™„ë²½ ì—°ê²°
âœ… ë™ì  ê²½ë¡œ ë§¤í•‘ ë° ìë™ ë“±ë¡
âœ… ModelLoaderì™€ ì™„ë²½ í†µí•©
"""

import os
import re
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, field
from enum import Enum
import json
import hashlib

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ëª¨ë¸ íƒì§€ ì„¤ì • ë° ë§¤í•‘
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    AUXILIARY = "auxiliary"

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    requirements: List[str] = field(default_factory=list)

# ==============================================
# ğŸ” ëª¨ë¸ ì‹ë³„ íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤
# ==============================================

MODEL_IDENTIFICATION_PATTERNS = {
    # Step 01: Human Parsing Models
    "human_parsing": {
        "patterns": [
            r".*human.*parsing.*\.pth$",
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*\.pth$",
            r".*atr.*model.*\.pth$",
            r".*lip.*parsing.*\.pth$",
            r".*segformer.*\.pth$",
            r".*densepose.*\.pkl$",
            r".*pytorch_model\.bin$"  # ì¼ë°˜ì ì¸ HF ëª¨ë¸
        ],
        "keywords": ["human", "parsing", "segmentation", "atr", "lip", "schp", "graphonomy", "densepose"],
        "category": ModelCategory.HUMAN_PARSING,
        "priority": 1,
        "min_size_mb": 50  # ìµœì†Œ í¬ê¸° í•„í„°
    },
    
    # Step 02: Pose Estimation Models
    "pose_estimation": {
        "patterns": [
            r".*pose.*model.*\.pth$",
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*hand.*pose.*\.pth$",
            r".*yolo.*pose.*\.pt$",
            r".*mediapipe.*\.tflite$",
            r".*res101.*\.pth$",
            r".*clip_g.*\.pth$"
        ],
        "keywords": ["pose", "openpose", "yolo", "mediapipe", "body", "hand", "keypoint"],
        "category": ModelCategory.POSE_ESTIMATION,
        "priority": 2,
        "min_size_mb": 5
    },
    
    # Step 03: Cloth Segmentation Models
    "cloth_segmentation": {
        "patterns": [
            r".*u2net.*\.pth$",
            r".*cloth.*segmentation.*\.(pth|onnx)$",
            r".*sam.*\.pth$",
            r".*mobile.*sam.*\.pth$",
            r".*parsing.*lip.*\.onnx$",
            r".*segmentation.*\.pth$"
        ],
        "keywords": ["u2net", "segmentation", "sam", "cloth", "mask", "mobile"],
        "category": ModelCategory.CLOTH_SEGMENTATION,
        "priority": 3,
        "min_size_mb": 10
    },
    
    # Step 04: Geometric Matching Models
    "geometric_matching": {
        "patterns": [
            r".*geometric.*matching.*\.pth$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$",
            r".*transformation.*\.pth$",
            r".*lightweight.*gmm.*\.pth$"
        ],
        "keywords": ["geometric", "matching", "gmm", "tps", "transformation", "alignment"],
        "category": ModelCategory.GEOMETRIC_MATCHING,
        "priority": 4,
        "min_size_mb": 1
    },
    
    # Step 05 & 06: Virtual Fitting & Cloth Warping Models
    "virtual_fitting": {
        "patterns": [
            r".*diffusion.*pytorch.*model\.(bin|safetensors)$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.(pth|bin)$",
            r".*unet.*diffusion.*\.bin$",
            r".*hrviton.*\.pth$",
            r".*viton.*\.pth$",
            r".*inpaint.*\.bin$"
        ],
        "keywords": ["diffusion", "stable", "oot", "viton", "unet", "inpaint", "generation"],
        "category": ModelCategory.VIRTUAL_FITTING,
        "priority": 5,
        "min_size_mb": 100  # Diffusion ëª¨ë¸ì€ ëŒ€ìš©ëŸ‰
    },
    
    # Step 07: Post Processing Models
    "post_processing": {
        "patterns": [
            r".*realesrgan.*\.pth$",
            r".*esrgan.*\.pth$",
            r".*super.*resolution.*\.pth$",
            r".*upscale.*\.pth$",
            r".*enhance.*\.pth$"
        ],
        "keywords": ["esrgan", "realesrgan", "upscale", "enhance", "super", "resolution"],
        "category": ModelCategory.POST_PROCESSING,
        "priority": 6,
        "min_size_mb": 10
    },
    
    # Step 08: Quality Assessment & Auxiliary Models
    "quality_assessment": {
        "patterns": [
            r".*clip.*vit.*\.bin$",
            r".*clip.*base.*\.bin$",
            r".*clip.*large.*\.bin$",
            r".*quality.*assessment.*\.pth$",
            r".*feature.*\.pth$",
            r".*resnet.*features.*\.pth$"
        ],
        "keywords": ["clip", "vit", "quality", "assessment", "feature", "resnet"],
        "category": ModelCategory.QUALITY_ASSESSMENT,
        "priority": 7,
        "min_size_mb": 50
    },
    
    # Auxiliary Models
    "auxiliary": {
        "patterns": [
            r".*vae.*\.bin$",
            r".*text.*encoder.*\.bin$",
            r".*tokenizer.*\.json$",
            r".*scheduler.*\.bin$",
            r".*safety.*checker.*\.bin$"
        ],
        "keywords": ["vae", "encoder", "tokenizer", "scheduler", "safety", "checker"],
        "category": ModelCategory.AUXILIARY,
        "priority": 8,
        "min_size_mb": 10
    }
}

# ==============================================
# ğŸ” í•µì‹¬ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class AutoModelDetector:
    """
    ğŸ” ìë™ AI ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ
    âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤ ìë™ ë°œê²¬
    âœ… ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ í• ë‹¹
    âœ… ModelLoaderì™€ ì™„ë²½ í†µí•©
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_metadata_extraction: bool = True,
        cache_results: bool = True
    ):
        """ìë™ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.AutoModelDetector")
        
        # ê¸°ë³¸ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì •
        if search_paths is None:
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parents[3]  # app/ai_pipeline/utilsì—ì„œ backendë¡œ
            
            self.search_paths = [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "app" / "models",
                backend_dir / "checkpoints",
                backend_dir / "ai_models" / "checkpoints"
            ]
        else:
            self.search_paths = search_paths
        
        # ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_metadata_extraction = enable_metadata_extraction
        self.cache_results = cache_results
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0
        }
        
        # ìºì‹œ ê´€ë¦¬
        self.cache_file = Path("model_detection_cache.json")
        self.cache_ttl = 3600  # 1ì‹œê°„
        
        self.logger.info(f"ğŸ” ìë™ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")

    def detect_all_models(self, force_rescan: bool = False) -> Dict[str, DetectedModel]:
        """
        ëª¨ë“  AI ëª¨ë¸ ìë™ íƒì§€
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            
        Returns:
            Dict[str, DetectedModel]: íƒì§€ëœ ëª¨ë¸ë“¤
        """
        try:
            self.logger.info("ğŸ” AI ëª¨ë¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.cache_results:
                cached_results = self._load_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ ëª¨ë¸")
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self.detected_models.clear()
            self.scan_stats["total_files_scanned"] = 0
            
            for search_path in self.search_paths:
                if search_path.exists():
                    self.logger.info(f"ğŸ“ ìŠ¤ìº” ì¤‘: {search_path}")
                    self._scan_directory(search_path)
                else:
                    self.logger.debug(f"âš ï¸ ê²½ë¡œ ì—†ìŒ: {search_path}")
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            self.scan_stats["last_scan_time"] = time.time()
            
            # ê²°ê³¼ ì •ë¦¬ ë° ìš°ì„ ìˆœìœ„ ì¡°ì •
            self._post_process_results()
            
            # ìºì‹œ ì €ì¥
            if self.cache_results:
                self._save_cache()
            
            self.logger.info(f"âœ… ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            raise

    def _scan_directory(self, directory: Path, max_depth: int = 5, current_depth: int = 0):
        """ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”"""
        try:
            if current_depth > max_depth:
                return
                
            for item in directory.iterdir():
                try:
                    if item.is_file():
                        self.scan_stats["total_files_scanned"] += 1
                        self._analyze_file(item)
                    elif item.is_dir() and self.enable_deep_scan:
                        # ìˆ¨ê¹€ í´ë”ë‚˜ ì¼ë°˜ì ì¸ ì œì™¸ í´ë” ê±´ë„ˆë›°ê¸°
                        if not item.name.startswith('.') and item.name not in ['__pycache__', 'node_modules']:
                            self._scan_directory(item, max_depth, current_depth + 1)
                except PermissionError:
                    continue
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜ {item}: {e}")
                    continue
                    
        except Exception as e:
            self.logger.debug(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {directory}: {e}")

    def _analyze_file(self, file_path: Path):
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        try:
            # íŒŒì¼ ê¸°ë³¸ ì •ë³´
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            file_extension = file_path.suffix.lower()
            
            # AI ëª¨ë¸ íŒŒì¼ í™•ì¥ì í•„í„°
            ai_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', '.tflite', '.h5'}
            if file_extension not in ai_extensions:
                return
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸ (1MB ë¯¸ë§Œ)
            if file_size_mb < 1.0:
                return
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëª¨ë¸ ë¶„ë¥˜
            detected_category, confidence_score = self._classify_model(file_path)
            
            if detected_category:
                # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
                model_name = self._generate_model_name(file_path, detected_category)
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = {}
                if self.enable_metadata_extraction:
                    metadata = self._extract_metadata(file_path)
                
                # ìš°ì„ ìˆœìœ„ ê³„ì‚°
                priority = self._calculate_priority(file_path, detected_category, file_size_mb)
                
                # DetectedModel ê°ì²´ ìƒì„±
                detected_model = DetectedModel(
                    name=model_name,
                    path=file_path,
                    category=detected_category,
                    model_type=self._determine_model_type(file_path, detected_category),
                    file_size_mb=file_size_mb,
                    file_extension=file_extension,
                    confidence_score=confidence_score,
                    priority=priority,
                    metadata=metadata
                )
                
                # ì¤‘ë³µ í™•ì¸ ë° ì €ì¥
                self._register_detected_model(detected_model)
                
        except Exception as e:
            self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")

    def _classify_model(self, file_path: Path) -> Tuple[Optional[ModelCategory], float]:
        """íŒŒì¼ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        try:
            file_name = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            best_category = None
            best_score = 0.0
            
            for category_name, config in MODEL_IDENTIFICATION_PATTERNS.items():
                score = 0.0
                matches = 0
                
                # íŒ¨í„´ ë§¤ì¹­
                for pattern in config["patterns"]:
                    if re.search(pattern, file_path_str, re.IGNORECASE):
                        score += 10.0
                        matches += 1
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                for keyword in config["keywords"]:
                    if keyword in file_name or keyword in file_path_str:
                        score += 5.0
                        matches += 1
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb >= config.get("min_size_mb", 0):
                    score += 2.0
                
                # ë§¤ì¹˜ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if matches > 0:
                    score += matches * 2.0
                
                # ìµœê³  ì ìˆ˜ ê°±ì‹ 
                if score > best_score:
                    best_score = score
                    best_category = config["category"]
            
            # ìµœì†Œ ì„ê³„ê°’ í™•ì¸ (ì‹ ë¢°ë„ 15ì  ì´ìƒ)
            if best_score >= 15.0:
                confidence = min(best_score / 30.0, 1.0)  # ì •ê·œí™”
                return best_category, confidence
            
            return None, 0.0
            
        except Exception as e:
            self.logger.debug(f"ë¶„ë¥˜ ì˜¤ë¥˜ {file_path}: {e}")
            return None, 0.0

    def _generate_model_name(self, file_path: Path, category: ModelCategory) -> str:
        """ëª¨ë¸ ê³ ìœ  ì´ë¦„ ìƒì„±"""
        try:
            # ê¸°ë³¸ ì´ë¦„: ì¹´í…Œê³ ë¦¬_íŒŒì¼ëª…
            base_name = f"{category.value}_{file_path.stem}"
            
            # íŠ¹ë³„í•œ ëª¨ë¸ë“¤ ì²˜ë¦¬
            special_names = {
                "graphonomy": "human_parsing_graphonomy",
                "schp": "human_parsing_schp",
                "openpose": "pose_estimation_openpose",
                "yolo": "pose_estimation_yolo",
                "u2net": "cloth_segmentation_u2net",
                "sam": "cloth_segmentation_sam",
                "ootdiffusion": "virtual_fitting_ootdiffusion",
                "stable_diffusion": "virtual_fitting_stable_diffusion",
                "realesrgan": "post_processing_realesrgan",
                "clip": "quality_assessment_clip"
            }
            
            file_name_lower = file_path.name.lower()
            for keyword, special_name in special_names.items():
                if keyword in file_name_lower:
                    return special_name
            
            # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì¶”ê°€
            path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
            return f"{base_name}_{path_hash}"
            
        except Exception as e:
            # í´ë°± ì´ë¦„
            return f"detected_model_{int(time.time())}"

    def _determine_model_type(self, file_path: Path, category: ModelCategory) -> str:
        """ëª¨ë¸ íƒ€ì… ê²°ì • (ModelLoaderì˜ í´ë˜ìŠ¤ì™€ ë§¤í•‘)"""
        model_type_mapping = {
            ModelCategory.HUMAN_PARSING: "GraphonomyModel",
            ModelCategory.POSE_ESTIMATION: "OpenPoseModel",
            ModelCategory.CLOTH_SEGMENTATION: "U2NetModel",
            ModelCategory.GEOMETRIC_MATCHING: "GeometricMatchingModel",
            ModelCategory.CLOTH_WARPING: "HRVITONModel",
            ModelCategory.VIRTUAL_FITTING: "HRVITONModel",
            ModelCategory.POST_PROCESSING: "GraphonomyModel",  # ë²”ìš© ì‚¬ìš©
            ModelCategory.QUALITY_ASSESSMENT: "GraphonomyModel",  # ë²”ìš© ì‚¬ìš©
            ModelCategory.AUXILIARY: "HRVITONModel"  # ë²”ìš© ì‚¬ìš©
        }
        
        # íŠ¹ë³„í•œ ëª¨ë¸ íƒ€ì… ì²˜ë¦¬
        file_name = file_path.name.lower()
        if "diffusion" in file_name:
            return "StableDiffusionPipeline"
        elif "clip" in file_name:
            return "CLIPModel"
        elif "vae" in file_name:
            return "AutoencoderKL"
        
        return model_type_mapping.get(category, "GenericModel")

    def _calculate_priority(self, file_path: Path, category: ModelCategory, file_size_mb: float) -> int:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ (ì¹´í…Œê³ ë¦¬ë³„)
            base_priority = {
                ModelCategory.HUMAN_PARSING: 10,
                ModelCategory.POSE_ESTIMATION: 20,
                ModelCategory.CLOTH_SEGMENTATION: 30,
                ModelCategory.GEOMETRIC_MATCHING: 40,
                ModelCategory.CLOTH_WARPING: 50,
                ModelCategory.VIRTUAL_FITTING: 50,
                ModelCategory.POST_PROCESSING: 60,
                ModelCategory.QUALITY_ASSESSMENT: 70,
                ModelCategory.AUXILIARY: 80
            }.get(category, 90)
            
            # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ë³´ì • (í° ëª¨ë¸ì¼ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            if file_size_mb > 1000:  # 1GB ì´ìƒ
                base_priority -= 5
            elif file_size_mb > 100:  # 100MB ì´ìƒ
                base_priority -= 2
            
            # íŒŒì¼ëª… ê¸°ë°˜ ë³´ì •
            file_name = file_path.name.lower()
            priority_keywords = {
                "base": -3,    # base ëª¨ë¸ ìš°ì„ 
                "large": -2,   # large ëª¨ë¸ ì°¨ìˆœìœ„
                "mini": +5,    # mini ëª¨ë¸ í›„ìˆœìœ„
                "fp16": -1,    # fp16 ìµœì í™” ëª¨ë¸ ìš°ì„ 
                "safetensors": -1  # safetensors í¬ë§· ìš°ì„ 
            }
            
            for keyword, adjustment in priority_keywords.items():
                if keyword in file_name:
                    base_priority += adjustment
            
            return max(1, base_priority)  # ìµœì†Œê°’ 1
            
        except Exception:
            return 50  # ê¸°ë³¸ê°’

    def _extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            "file_modified": file_path.stat().st_mtime,
            "file_created": file_path.stat().st_ctime,
            "parent_directory": file_path.parent.name
        }
        
        try:
            # PyTorch ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if TORCH_AVAILABLE and file_path.suffix in ['.pth', '.pt']:
                try:
                    # í—¤ë”ë§Œ ì½ì–´ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                    
                    if isinstance(checkpoint, dict):
                        # ì¼ë°˜ì ì¸ ë©”íƒ€ë°ì´í„° í‚¤ë“¤
                        meta_keys = ['arch', 'epoch', 'version', 'model_name', 'config']
                        for key in meta_keys:
                            if key in checkpoint:
                                metadata[key] = str(checkpoint[key])[:100]  # ê¸¸ì´ ì œí•œ
                        
                        # ëª¨ë¸ êµ¬ì¡° ì •ë³´
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                            metadata['num_parameters'] = sum(v.numel() for v in state_dict.values() if torch.is_tensor(v))
                        elif hasattr(checkpoint, 'keys'):
                            metadata['num_parameters'] = sum(v.numel() for v in checkpoint.values() if torch.is_tensor(v))
                            
                except Exception as e:
                    metadata['torch_load_error'] = str(e)[:100]
            
            # íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            path_parts = file_path.parts
            if len(path_parts) >= 2:
                metadata['model_family'] = path_parts[-2]  # ë¶€ëª¨ ë””ë ‰í† ë¦¬
            
            # íŠ¹ë³„í•œ êµ¬ì¡° ì¸ì‹
            for part in path_parts:
                if 'checkpoint' in part.lower():
                    metadata['is_checkpoint'] = True
                    break
                    
        except Exception as e:
            metadata['metadata_extraction_error'] = str(e)[:100]
        
        return metadata

    def _register_detected_model(self, detected_model: DetectedModel):
        """íƒì§€ëœ ëª¨ë¸ ë“±ë¡ (ì¤‘ë³µ ì²˜ë¦¬)"""
        try:
            model_name = detected_model.name
            
            # ê¸°ì¡´ ëª¨ë¸ê³¼ ì¤‘ë³µ í™•ì¸
            if model_name in self.detected_models:
                existing_model = self.detected_models[model_name]
                
                # ë” ë‚˜ì€ ëª¨ë¸ë¡œ êµì²´í• ì§€ ê²°ì •
                if self._is_better_model(detected_model, existing_model):
                    # ê¸°ì¡´ ëª¨ë¸ì„ ëŒ€ì²´ ê²½ë¡œë¡œ ì¶”ê°€
                    detected_model.alternative_paths.append(existing_model.path)
                    detected_model.alternative_paths.extend(existing_model.alternative_paths)
                    self.detected_models[model_name] = detected_model
                    self.logger.debug(f"ğŸ”„ ëª¨ë¸ êµì²´: {model_name}")
                else:
                    # ìƒˆ ëª¨ë¸ì„ ëŒ€ì²´ ê²½ë¡œë¡œ ì¶”ê°€
                    existing_model.alternative_paths.append(detected_model.path)
                    self.logger.debug(f"ğŸ“ ëŒ€ì²´ ê²½ë¡œ ì¶”ê°€: {model_name}")
            else:
                # ìƒˆ ëª¨ë¸ ë“±ë¡
                self.detected_models[model_name] = detected_model
                self.logger.debug(f"âœ… ìƒˆ ëª¨ë¸ ë“±ë¡: {model_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _is_better_model(self, new_model: DetectedModel, existing_model: DetectedModel) -> bool:
        """ìƒˆ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨"""
        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²½ìš° (ìˆ«ìê°€ ì‘ì„ìˆ˜ë¡ ë†’ìŒ)
        if new_model.priority < existing_model.priority:
            return True
        elif new_model.priority > existing_model.priority:
            return False
        
        # ìš°ì„ ìˆœìœ„ê°€ ê°™ìœ¼ë©´ ì‹ ë¢°ë„ ë¹„êµ
        if new_model.confidence_score > existing_model.confidence_score:
            return True
        elif new_model.confidence_score < existing_model.confidence_score:
            return False
        
        # ì‹ ë¢°ë„ë„ ê°™ìœ¼ë©´ íŒŒì¼ í¬ê¸° ë¹„êµ (í° ê²ƒì´ ì¢‹ìŒ)
        return new_model.file_size_mb > existing_model.file_size_mb

    def _post_process_results(self):
        """íƒì§€ ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
            sorted_models = sorted(
                self.detected_models.items(),
                key=lambda x: (x[1].priority, -x[1].confidence_score)
            )
            
            # ì •ë ¬ëœ ìˆœì„œë¡œ ì¬ì •ë ¬
            self.detected_models = {name: model for name, model in sorted_models}
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for model in self.detected_models.values():
                category = model.category.value
                if category not in category_stats:
                    category_stats[category] = {"count": 0, "total_size_mb": 0}
                category_stats[category]["count"] += 1
                category_stats[category]["total_size_mb"] += model.file_size_mb
            
            self.scan_stats["category_stats"] = category_stats
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            self.logger.info("=" * 60)
            self.logger.info("ğŸ¯ ìë™ ëª¨ë¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 60)
            
            total_size_gb = sum(model.file_size_mb for model in self.detected_models.values()) / 1024
            self.logger.info(f"ğŸ“Š ì´ íƒì§€ëœ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ ëª¨ë¸ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº”ëœ íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"â±ï¸ ìŠ¤ìº” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
            if "category_stats" in self.scan_stats:
                self.logger.info("\nğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
                for category, stats in self.scan_stats["category_stats"].items():
                    size_gb = stats["total_size_mb"] / 1024
                    self.logger.info(f"  {category}: {stats['count']}ê°œ ({size_gb:.2f}GB)")
            
            # ìƒìœ„ 5ê°œ ëª¨ë¸
            self.logger.info("\nğŸ† ì£¼ìš” íƒì§€ëœ ëª¨ë¸ë“¤:")
            for i, (name, model) in enumerate(list(self.detected_models.items())[:5]):
                self.logger.info(f"  {i+1}. {name} ({model.file_size_mb:.1f}MB, {model.category.value})")
                
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    def _load_cache(self) -> Optional[Dict[str, DetectedModel]]:
        """ìºì‹œ ë¡œë“œ"""
        try:
            if not self.cache_file.exists():
                return None
            
            # TTL í™•ì¸
            cache_age = time.time() - self.cache_file.stat().st_mtime
            if cache_age > self.cache_ttl:
                self.logger.debug("ìºì‹œ ë§Œë£Œë¨")
                return None
            
            with open(self.cache_file, 'r') as f:
                cache_data = json.load(f)
            
            # DetectedModel ê°ì²´ë¡œ ë³µì›
            detected_models = {}
            for name, model_data in cache_data.get("detected_models", {}).items():
                try:
                    detected_model = DetectedModel(
                        name=model_data["name"],
                        path=Path(model_data["path"]),
                        category=ModelCategory(model_data["category"]),
                        model_type=model_data["model_type"],
                        file_size_mb=model_data["file_size_mb"],
                        file_extension=model_data["file_extension"],
                        confidence_score=model_data["confidence_score"],
                        priority=model_data["priority"],
                        metadata=model_data.get("metadata", {}),
                        alternative_paths=[Path(p) for p in model_data.get("alternative_paths", [])],
                        requirements=model_data.get("requirements", [])
                    )
                    
                    # íŒŒì¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    if detected_model.path.exists():
                        detected_models[name] = detected_model
                except Exception as e:
                    self.logger.debug(f"ìºì‹œ ëª¨ë¸ ë³µì› ì‹¤íŒ¨ {name}: {e}")
            
            if detected_models:
                self.detected_models = detected_models
                return detected_models
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            cache_data = {
                "detected_models": {},
                "scan_stats": self.scan_stats,
                "cache_version": "1.0",
                "created_at": time.time()
            }
            
            # DetectedModel ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            for name, model in self.detected_models.items():
                cache_data["detected_models"][name] = {
                    "name": model.name,
                    "path": str(model.path),
                    "category": model.category.value,
                    "model_type": model.model_type,
                    "file_size_mb": model.file_size_mb,
                    "file_extension": model.file_extension,
                    "confidence_score": model.confidence_score,
                    "priority": model.priority,
                    "metadata": model.metadata,
                    "alternative_paths": [str(p) for p in model.alternative_paths],
                    "requirements": model.requirements
                }
            
            with open(self.cache_file, 'w') as f:
                json.dump(cache_data, f, indent=2)
                
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.cache_file}")
            
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]

    def get_best_model_for_category(self, category: ModelCategory) -> Optional[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ (ìš°ì„ ìˆœìœ„ ê¸°ì¤€)"""
        category_models = self.get_models_by_category(category)
        if not category_models:
            return None
        
        return min(category_models, key=lambda m: (m.priority, -m.confidence_score))

    def get_model_by_name(self, name: str) -> Optional[DetectedModel]:
        """ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ"""
        return self.detected_models.get(name)

    def get_all_model_paths(self) -> Dict[str, Path]:
        """ëª¨ë“  ëª¨ë¸ì˜ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {name: model.path for name, model in self.detected_models.items()}

    def export_model_config(self, output_path: Optional[Path] = None) -> Path:
        """íƒì§€ëœ ëª¨ë¸ë“¤ì„ ì„¤ì • íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            if output_path is None:
                output_path = Path("detected_models_config.json")
            
            config_data = {
                "detection_info": {
                    "detected_at": time.time(),
                    "total_models": len(self.detected_models),
                    "scan_stats": self.scan_stats
                },
                "models": {}
            }
            
            for name, model in self.detected_models.items():
                config_data["models"][name] = {
                    "name": model.name,
                    "path": str(model.path),
                    "alternative_paths": [str(p) for p in model.alternative_paths],
                    "category": model.category.value,
                    "model_type": model.model_type,
                    "file_size_mb": model.file_size_mb,
                    "confidence_score": model.confidence_score,
                    "priority": model.priority,
                    "metadata": model.metadata
                }
            
            with open(output_path, 'w') as f:
                json.dump(config_data, f, indent=2)
            
            self.logger.info(f"âœ… ëª¨ë¸ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise

# ==============================================
# ğŸ”— ModelLoader í†µí•©ì„ ìœ„í•œ ì–´ëŒ‘í„°
# ==============================================

class ModelLoaderAdapter:
    """
    ìë™ íƒì§€ ì‹œìŠ¤í…œì„ ê¸°ì¡´ ModelLoaderì™€ ì—°ê²°í•˜ëŠ” ì–´ëŒ‘í„°
    """
    
    def __init__(self, detector: AutoModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderAdapter")
    
    def generate_actual_model_paths(self) -> Dict[str, Dict[str, Any]]:
        """ModelLoaderì˜ ACTUAL_MODEL_PATHS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        actual_paths = {}
        
        for name, model in self.detector.detected_models.items():
            actual_paths[name] = {
                "primary": str(model.path),
                "alternatives": [str(p) for p in model.alternative_paths],
                "category": model.category.value,
                "model_type": model.model_type,
                "confidence": model.confidence_score,
                "priority": model.priority,
                "size_mb": model.file_size_mb
            }
        
        return actual_paths
    
    def generate_model_configs(self) -> List[Dict[str, Any]]:
        """ModelConfig í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        configs = []
        
        for name, model in self.detector.detected_models.items():
            config = {
                "name": name,
                "model_type": model.category.value,
                "model_class": model.model_type,
                "checkpoint_path": str(model.path),
                "device": "auto",
                "precision": "fp16",
                "input_size": self._get_input_size_for_category(model.category),
                "metadata": {
                    **model.metadata,
                    "auto_detected": True,
                    "confidence_score": model.confidence_score,
                    "alternative_paths": [str(p) for p in model.alternative_paths]
                }
            }
            configs.append(config)
        
        return configs
    
    def _get_input_size_for_category(self, category: ModelCategory) -> Tuple[int, int]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸°"""
        size_mapping = {
            ModelCategory.HUMAN_PARSING: (512, 512),
            ModelCategory.POSE_ESTIMATION: (368, 368),
            ModelCategory.CLOTH_SEGMENTATION: (320, 320),
            ModelCategory.GEOMETRIC_MATCHING: (512, 384),
            ModelCategory.CLOTH_WARPING: (512, 384),
            ModelCategory.VIRTUAL_FITTING: (512, 384),
            ModelCategory.POST_PROCESSING: (512, 512),
            ModelCategory.QUALITY_ASSESSMENT: (224, 224),
            ModelCategory.AUXILIARY: (224, 224)
        }
        return size_mapping.get(category, (512, 512))

# ==============================================
# ğŸš€ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_auto_detector(
    search_paths: Optional[List[Path]] = None,
    **kwargs
) -> AutoModelDetector:
    """ìë™ ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return AutoModelDetector(search_paths=search_paths, **kwargs)

def detect_models_and_generate_config(
    output_config_path: Optional[Path] = None,
    force_rescan: bool = False
) -> Dict[str, Any]:
    """ëª¨ë¸ íƒì§€ ë° ì„¤ì • ìƒì„± ì›ìŠ¤í†± í•¨ìˆ˜"""
    try:
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_auto_detector()
        detected_models = detector.detect_all_models(force_rescan=force_rescan)
        
        # ì–´ëŒ‘í„°ë¥¼ í†µí•œ ì„¤ì • ìƒì„±
        adapter = ModelLoaderAdapter(detector)
        actual_paths = adapter.generate_actual_model_paths()
        model_configs = adapter.generate_model_configs()
        
        # í†µí•© ì„¤ì •
        integrated_config = {
            "detection_summary": {
                "total_models": len(detected_models),
                "categories": list(set(model.category.value for model in detected_models.values())),
                "total_size_gb": sum(model.file_size_mb for model in detected_models.values()) / 1024
            },
            "actual_model_paths": actual_paths,
            "model_configs": model_configs,
            "raw_detections": {name: {
                "path": str(model.path),
                "category": model.category.value,
                "confidence": model.confidence_score
            } for name, model in detected_models.items()}
        }
        
        # íŒŒì¼ ì €ì¥
        if output_config_path:
            with open(output_config_path, 'w') as f:
                json.dump(integrated_config, f, indent=2)
            logger.info(f"âœ… í†µí•© ì„¤ì • ì €ì¥: {output_config_path}")
        
        return integrated_config
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ íƒì§€ ë° ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'AutoModelDetector',
    'ModelLoaderAdapter', 
    'DetectedModel',
    'ModelCategory',
    'create_auto_detector',
    'detect_models_and_generate_config',
    'MODEL_IDENTIFICATION_PATTERNS'
]