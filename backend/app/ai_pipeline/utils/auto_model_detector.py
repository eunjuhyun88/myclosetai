# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - ìµœì†Œ ìˆ˜ì • ìë™ ëª¨ë¸ íƒì§€ê¸° v3.1 (Step êµ¬í˜„ì²´ ì—°ë™)
================================================================================
âœ… ê¸°ì¡´ 2ë²ˆ íŒŒì¼ êµ¬ì¡° ìµœëŒ€í•œ ìœ ì§€
âœ… Step êµ¬í˜„ì²´ì˜ ê¸°ì¡´ load_models() í•¨ìˆ˜ì™€ ì™„ë²½ ì—°ë™
âœ… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë§Œ ì •í™•íˆ ì°¾ì•„ì„œ Stepì—ê²Œ ì „ë‹¬
âœ… Stepì´ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±í•˜ëŠ” êµ¬ì¡° í™œìš©
âœ… conda í™˜ê²½ + M3 Max ìµœì í™” ìœ ì§€
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì •í™•í•œ ë§¤í•‘ í…Œì´ë¸” (ê°œì„ )
# ==============================================

class RealFileMapper:
    """ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì •í™•í•œ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.RealFileMapper")
        
        # ğŸ”¥ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ì •í™•í•œ ë§¤í•‘
        self.step_file_mappings = {
            # Step 01: Human Parsing
            "human_parsing_graphonomy": {
                "actual_files": ["exp-schp-201908301523-atr.pth"],
                "search_paths": ["step_01_human_parsing", "checkpoints/step_01_human_parsing"],
                "patterns": [r".*exp-schp.*atr.*\.pth$", r".*graphonomy.*\.pth$"],
                "size_range": (250, 260),
                "priority": 1,
                "step_class": "HumanParsingImplementation",  # ğŸ”¥ Step êµ¬í˜„ì²´ í´ë˜ìŠ¤ëª…
                "model_load_method": "load_models",  # ğŸ”¥ Stepì˜ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ëª…
            },
            "human_parsing_schp_atr": {
                "actual_files": ["exp-schp-201908301523-atr.pth"],
                "search_paths": ["step_01_human_parsing"],
                "patterns": [r".*exp-schp.*atr.*\.pth$"],
                "size_range": (250, 260),
                "priority": 1,
                "step_class": "HumanParsingImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 02: Pose Estimation  
            "pose_estimation_openpose": {
                "actual_files": ["openpose.pth", "body_pose_model.pth"],
                "search_paths": ["step_02_pose_estimation", "checkpoints/step_02_pose_estimation"],
                "patterns": [r".*openpose.*\.pth$", r".*body.*pose.*\.pth$"],
                "size_range": (190, 210),
                "priority": 1,
                "step_class": "PoseEstimationImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 03: Cloth Segmentation
            "cloth_segmentation_u2net": {
                "actual_files": ["u2net.pth"],
                "search_paths": ["step_03_cloth_segmentation", "checkpoints/step_03_cloth_segmentation"],
                "patterns": [r".*u2net.*\.pth$"],
                "size_range": (160, 180),
                "priority": 1,
                "step_class": "ClothSegmentationImplementation",
                "model_load_method": "load_models",
            },
            "cloth_segmentation_sam": {
                "actual_files": ["sam_vit_h_4b8939.pth"],
                "search_paths": ["step_03_cloth_segmentation"],
                "patterns": [r".*sam_vit_h.*\.pth$"],
                "size_range": (2400, 2500),
                "priority": 2,
                "step_class": "ClothSegmentationImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 04: Geometric Matching
            "geometric_matching_gmm": {
                "actual_files": ["gmm.pth", "tps_network.pth"],
                "search_paths": ["step_04_geometric_matching"],
                "patterns": [r".*gmm.*\.pth$", r".*tps.*\.pth$"],
                "size_range": (1, 50),
                "priority": 1,
                "step_class": "GeometricMatchingImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 05: Cloth Warping
            "cloth_warping_tom": {
                "actual_files": ["cloth_warping_net.pth", "hrviton_final.pth"],
                "search_paths": ["step_05_cloth_warping"],
                "patterns": [r".*cloth.*warping.*\.pth$", r".*hrviton.*\.pth$"],
                "size_range": (50, 500),
                "priority": 1,
                "step_class": "ClothWarpingImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 06: Virtual Fitting
            "virtual_fitting_diffusion": {
                "actual_files": ["diffusion_pytorch_model.bin", "pytorch_model.bin"],
                "search_paths": ["step_06_virtual_fitting", "checkpoints/ootdiffusion"],
                "patterns": [r".*diffusion.*pytorch.*model.*\.bin$", r".*pytorch_model\.bin$"],
                "size_range": (300, 600),
                "priority": 1,
                "step_class": "VirtualFittingImplementation",
                "model_load_method": "load_models",
            },
            "virtual_fitting_ootd": {
                "actual_files": ["diffusion_pytorch_model.safetensors"],
                "search_paths": ["checkpoints/ootdiffusion", "step_06_virtual_fitting/ootdiffusion"],
                "patterns": [r".*diffusion.*safetensors$", r".*ootd.*\.pth$"],
                "size_range": (1000, 8000),
                "priority": 2,
                "step_class": "VirtualFittingImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 07: Post Processing
            "post_processing_enhance": {
                "actual_files": ["enhance_model.pth", "ESRGAN_x4.pth"],
                "search_paths": ["step_07_post_processing"],
                "patterns": [r".*enhance.*\.pth$", r".*ESRGAN.*\.pth$"],
                "size_range": (10, 200),
                "priority": 1,
                "step_class": "PostProcessingImplementation",
                "model_load_method": "load_models",
            },
            
            # Step 08: Quality Assessment
            "quality_assessment_clip": {
                "actual_files": ["pytorch_model.bin"],
                "search_paths": ["step_08_quality_assessment"],
                "patterns": [r".*pytorch_model\.bin$"],
                "size_range": (500, 600),
                "priority": 1,
                "step_class": "QualityAssessmentImplementation",
                "model_load_method": "load_models",
            }
        }
    
    def find_actual_file(self, request_name: str, ai_models_root: Path) -> Optional[Path]:
        """ìš”ì²­ëª…ì— ëŒ€í•œ ì‹¤ì œ íŒŒì¼ ì°¾ê¸° (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        if request_name not in self.step_file_mappings:
            self.logger.debug(f"â“ ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ëª…: {request_name}")
            return None
        
        mapping = self.step_file_mappings[request_name]
        
        # 1. ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
        for filename in mapping["actual_files"]:
            for search_path in mapping["search_paths"]:
                full_path = ai_models_root / search_path / filename
                if full_path.exists() and full_path.is_file():
                    file_size_mb = full_path.stat().st_size / (1024 * 1024)
                    min_size, max_size = mapping["size_range"]
                    
                    if min_size <= file_size_mb <= max_size:
                        self.logger.info(f"âœ… {request_name} â†’ {full_path} ({file_size_mb:.1f}MB)")
                        return full_path
                    else:
                        self.logger.debug(f"âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜: {full_path} ({file_size_mb:.1f}MB)")
        
        # 2. íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
        for search_path in mapping["search_paths"]:
            search_dir = ai_models_root / search_path
            if not search_dir.exists():
                continue
                
            for pattern in mapping["patterns"]:
                try:
                    for file_path in search_dir.rglob("*"):
                        if file_path.is_file() and re.search(pattern, file_path.name, re.IGNORECASE):
                            file_size_mb = file_path.stat().st_size / (1024 * 1024)
                            min_size, max_size = mapping["size_range"]
                            
                            if min_size <= file_size_mb <= max_size:
                                self.logger.info(f"âœ… {request_name} â†’ {file_path} (íŒ¨í„´ ë§¤ì¹­, {file_size_mb:.1f}MB)")
                                return file_path
                except Exception as e:
                    self.logger.debug(f"íŒ¨í„´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
        
        self.logger.warning(f"âŒ {request_name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def get_step_info(self, request_name: str) -> Optional[Dict[str, Any]]:
        """ğŸ”¥ Step êµ¬í˜„ì²´ ì •ë³´ ë°˜í™˜ (ìƒˆë¡œ ì¶”ê°€)"""
        if request_name in self.step_file_mappings:
            mapping = self.step_file_mappings[request_name]
            return {
                "step_class": mapping.get("step_class"),
                "model_load_method": mapping.get("model_load_method"),
                "priority": mapping.get("priority"),
                "patterns": mapping.get("patterns", []),
            }
        return None

# ==============================================
# ğŸ”¥ 2. DetectedModel í´ë˜ìŠ¤ (Step ì—°ë™ ì •ë³´ ì¶”ê°€)
# ==============================================

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ + Step ì—°ë™ ì •ë³´"""
    name: str
    path: Path
    step_name: str
    model_type: str
    file_size_mb: float
    confidence_score: float
    
    # ğŸ”¥ Step êµ¬í˜„ì²´ ì—°ë™ ì •ë³´
    step_class_name: Optional[str] = None
    model_load_method: Optional[str] = None
    step_can_load: bool = False
    
    # ì¶”ê°€ ì •ë³´
    checkpoint_path: Optional[str] = None
    device_compatible: bool = True
    recommended_device: str = "cpu"
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "step_class": self.step_name,
            "model_type": self.model_type,
            "size_mb": self.file_size_mb,
            "confidence": self.confidence_score,
            "device_config": {
                "recommended_device": self.recommended_device,
                "device_compatible": self.device_compatible
            },
            
            # ğŸ”¥ Step ì—°ë™ ì •ë³´
            "step_implementation": {
                "step_class_name": self.step_class_name,
                "model_load_method": self.model_load_method,
                "step_can_load": self.step_can_load,
                "load_ready": self.step_can_load and self.checkpoint_path is not None
            },
            
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.path.suffix
            }
        }
    
    def can_be_loaded_by_step(self) -> bool:
        """Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return (self.step_can_load and 
                self.step_class_name is not None and 
                self.model_load_method is not None and
                self.checkpoint_path is not None)

# ==============================================
# ğŸ”¥ 3. ìˆ˜ì •ëœ ëª¨ë¸ íƒì§€ê¸° (Step ì—°ë™)
# ==============================================

class FixedModelDetector:
    """ìˆ˜ì •ëœ ëª¨ë¸ íƒì§€ê¸° (Step êµ¬í˜„ì²´ ì—°ë™)"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.FixedModelDetector")
        self.file_mapper = RealFileMapper()
        self.ai_models_root = self._find_ai_models_root()
        self.detected_models: Dict[str, DetectedModel] = {}
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.is_m3_max = self._detect_m3_max()
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        self.logger.info(f"ğŸ”§ Step ì—°ë™ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”")
        self.logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        current = Path(__file__).resolve()
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        for _ in range(10):
            if current.name == 'mycloset-ai' or (current / 'backend').exists():
                return current / 'backend' / 'ai_models'
            if current.name == 'backend':
                return current / 'ai_models'
            if current.parent == current:
                break
            current = current.parent
        
        # í´ë°±
        fallback_path = Path(__file__).resolve().parent.parent.parent.parent / 'ai_models'
        self.logger.warning(f"âš ï¸ í´ë°± ê²½ë¡œ ì‚¬ìš©: {fallback_path}")
        return fallback_path
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            return 'arm64' in platform.machine().lower()
        except:
            return False
    
    def detect_all_models(self) -> Dict[str, DetectedModel]:
        """ëª¨ë“  ëª¨ë¸ íƒì§€ (Step ì—°ë™ ì •ë³´ í¬í•¨)"""
        self.detected_models.clear()
        
        if not self.ai_models_root.exists():
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.ai_models_root}")
            return {}
        
        # ìš”ì²­ëª…ë³„ë¡œ ì‹¤ì œ íŒŒì¼ ì°¾ê¸° + Step ì •ë³´ ì¶”ê°€
        for request_name in self.file_mapper.step_file_mappings.keys():
            try:
                # 1. ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
                actual_file = self.file_mapper.find_actual_file(request_name, self.ai_models_root)
                
                if actual_file:
                    # 2. Step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    step_info = self.file_mapper.get_step_info(request_name)
                    
                    # 3. DetectedModel ìƒì„± (Step ì—°ë™ ì •ë³´ í¬í•¨)
                    model = self._create_detected_model_with_step_info(request_name, actual_file, step_info)
                    if model:
                        self.detected_models[model.name] = model
                        
            except Exception as e:
                self.logger.error(f"âŒ {request_name} íƒì§€ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¶”ê°€ íŒŒì¼ë“¤ ìë™ ìŠ¤ìº”
        self._scan_additional_files()
        
        self.logger.info(f"ğŸ‰ Step ì—°ë™ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ")
        return self.detected_models
    
    def _create_detected_model_with_step_info(self, request_name: str, file_path: Path, step_info: Optional[Dict]) -> Optional[DetectedModel]:
        """DetectedModel ìƒì„± (Step ì—°ë™ ì •ë³´ í¬í•¨)"""
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # Step ì´ë¦„ ì¶”ì¶œ
            step_name = self._extract_step_name(request_name)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            recommended_device = "mps" if self.is_m3_max else "cpu"
            
            # ğŸ”¥ Step ì—°ë™ ì •ë³´ ì„¤ì •
            step_class_name = None
            model_load_method = None
            step_can_load = False
            
            if step_info:
                step_class_name = step_info.get("step_class")
                model_load_method = step_info.get("model_load_method", "load_models")
                step_can_load = bool(step_class_name and model_load_method)
            
            model = DetectedModel(
                name=request_name,
                path=file_path,
                step_name=step_name,
                model_type=step_name.replace("Step", "").lower(),
                file_size_mb=file_size_mb,
                confidence_score=1.0,  # ì •í™•í•œ ë§¤í•‘ì´ë¯€ë¡œ ìµœëŒ€ ì‹ ë¢°ë„
                
                # ğŸ”¥ Step ì—°ë™ ì •ë³´
                step_class_name=step_class_name,
                model_load_method=model_load_method,
                step_can_load=step_can_load,
                
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device=recommended_device
            )
            
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_step_name(self, request_name: str) -> str:
        """ìš”ì²­ëª…ì—ì„œ Step ì´ë¦„ ì¶”ì¶œ"""
        step_mappings = {
            "human_parsing": "HumanParsingStep",
            "pose_estimation": "PoseEstimationStep", 
            "cloth_segmentation": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting": "VirtualFittingStep",
            "post_processing": "PostProcessingStep",
            "quality_assessment": "QualityAssessmentStep"
        }
        
        for key, step_name in step_mappings.items():
            if key in request_name:
                return step_name
        
        return "UnknownStep"
    
    def _scan_additional_files(self):
        """ì¶”ê°€ íŒŒì¼ë“¤ ìë™ ìŠ¤ìº” (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        try:
            # Ultra ëª¨ë¸ë“¤ ìŠ¤ìº”
            ultra_dir = self.ai_models_root / "ultra_models"
            if ultra_dir.exists():
                self._scan_ultra_models(ultra_dir)
            
            # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
            checkpoints_dir = self.ai_models_root / "checkpoints"
            if checkpoints_dir.exists():
                self._scan_checkpoints(checkpoints_dir)
                
        except Exception as e:
            self.logger.debug(f"ì¶”ê°€ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
    
    def _scan_ultra_models(self, ultra_dir: Path):
        """Ultra ëª¨ë¸ ìŠ¤ìº” (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        model_extensions = {'.pth', '.bin', '.safetensors', '.ckpt'}
        
        for file_path in ultra_dir.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix.lower() in model_extensions and
                file_path.stat().st_size > 50 * 1024 * 1024):  # 50MB ì´ìƒ
                
                try:
                    file_size_mb = file_path.stat().st_size / (1024 * 1024)
                    model_name = f"ultra_{file_path.parent.name}_{file_path.stem}"
                    
                    # ì¤‘ë³µ ë°©ì§€
                    if model_name in self.detected_models:
                        continue
                    
                    model = DetectedModel(
                        name=model_name,
                        path=file_path,
                        step_name="UltraModel",
                        model_type="ultra",
                        file_size_mb=file_size_mb,
                        confidence_score=0.8,
                        checkpoint_path=str(file_path),
                        device_compatible=True,
                        recommended_device="mps" if self.is_m3_max else "cpu"
                    )
                    
                    self.detected_models[model_name] = model
                    self.logger.debug(f"âœ… Ultra ëª¨ë¸: {model_name} ({file_size_mb:.1f}MB)")
                    
                except Exception as e:
                    self.logger.debug(f"Ultra ëª¨ë¸ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
    
    def _scan_checkpoints(self, checkpoints_dir: Path):
        """ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        for subdir in checkpoints_dir.iterdir():
            if subdir.is_dir():
                for file_path in subdir.rglob('*.pth'):
                    if file_path.name not in [m.path.name for m in self.detected_models.values()]:
                        try:
                            file_size_mb = file_path.stat().st_size / (1024 * 1024)
                            if file_size_mb > 10:  # 10MB ì´ìƒë§Œ
                                model_name = f"checkpoint_{subdir.name}_{file_path.stem}"
                                
                                model = DetectedModel(
                                    name=model_name,
                                    path=file_path,
                                    step_name="CheckpointModel",
                                    model_type="checkpoint",
                                    file_size_mb=file_size_mb,
                                    confidence_score=0.6,
                                    checkpoint_path=str(file_path),
                                    device_compatible=True,
                                    recommended_device="mps" if self.is_m3_max else "cpu"
                                )
                                
                                self.detected_models[model_name] = model
                                self.logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸: {model_name} ({file_size_mb:.1f}MB)")
                                
                        except Exception as e:
                            self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                            continue

# ==============================================
# ğŸ”¥ 4. ModelLoader í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ (Step ì—°ë™)
# ==============================================

def get_step_loadable_models() -> List[Dict[str, Any]]:
    """ğŸ”¥ Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    loadable_models = []
    for model in models.values():
        if model.can_be_loaded_by_step():
            model_dict = model.to_dict()
            model_dict["load_instruction"] = {
                "step_class": model.step_class_name,
                "method": model.model_load_method,
                "checkpoint_path": model.checkpoint_path
            }
            loadable_models.append(model_dict)
    
    return sorted(loadable_models, key=lambda x: x["confidence"], reverse=True)

def create_step_model_loader_config() -> Dict[str, Any]:
    """ğŸ”¥ Step êµ¬í˜„ì²´ ì—°ë™ìš© ModelLoader ì„¤ì • ìƒì„±"""
    detector = get_global_detector()
    detected_models = detector.detect_all_models()
    
    config = {
        "version": "step_integrated_detector_v3.1",
        "generated_at": time.time(),
        "device": "mps" if detector.is_m3_max else "cpu",
        "is_m3_max": detector.is_m3_max,
        "conda_env": detector.conda_env,
        "models": {},
        "step_mappings": {},
        "step_loadable_count": 0
    }
    
    # ëª¨ë¸ë³„ ì„¤ì •
    for model_name, model in detected_models.items():
        model_dict = model.to_dict()
        config["models"][model_name] = model_dict
        
        # Step ë¡œë“œ ê°€ëŠ¥ ì¹´ìš´íŠ¸
        if model.can_be_loaded_by_step():
            config["step_loadable_count"] += 1
        
        # Step ë§¤í•‘
        step_name = model.step_name
        if step_name not in config["step_mappings"]:
            config["step_mappings"][step_name] = []
        config["step_mappings"][step_name].append(model_name)
    
    # í†µê³„
    config["summary"] = {
        "total_models": len(detected_models),
        "step_loadable_models": config["step_loadable_count"],
        "total_size_gb": sum(m.file_size_mb for m in detected_models.values()) / 1024,
        "device_optimized": detector.is_m3_max,
        "step_integration_ready": config["step_loadable_count"] > 0
    }
    
    logger.info(f"âœ… Step ì—°ë™ ì„¤ì • ìƒì„±: {len(detected_models)}ê°œ ëª¨ë¸, {config['step_loadable_count']}ê°œ Step ë¡œë“œ ê°€ëŠ¥")
    return config

# ==============================================
# ğŸ”¥ 5. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

_global_detector: Optional[FixedModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector() -> FixedModelDetector:
    """ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤"""
    global _global_detector
    if _global_detector is None:
        with _detector_lock:
            if _global_detector is None:
                _global_detector = FixedModelDetector()
    return _global_detector

def quick_model_detection() -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€"""
    detector = get_global_detector()
    return detector.detect_all_models()

def list_available_models(step_class: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    result = []
    for model in models.values():
        model_dict = model.to_dict()
        
        if step_class and model_dict["step_class"] != step_class:
            continue
        
        result.append(model_dict)
    
    return sorted(result, key=lambda x: x["confidence"], reverse=True)

def get_models_for_step(step_name: str) -> List[Dict[str, Any]]:
    """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
    models = list_available_models(step_class=step_name)
    return models

def validate_model_exists(model_name: str) -> bool:
    """ëª¨ë¸ ì¡´ì¬ í™•ì¸"""
    detector = get_global_detector()
    return model_name in detector.detected_models

def generate_advanced_model_loader_config() -> Dict[str, Any]:
    """ğŸ”¥ ê³ ê¸‰ ModelLoader ì„¤ì • ìƒì„± (Step ì—°ë™ í¬í•¨)"""
    return create_step_model_loader_config()

def create_step_interface(step_name: str, config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    models = get_models_for_step(step_name)
    if not models:
        return None
    
    # Step ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ ìš°ì„  ì„ íƒ
    loadable_models = [m for m in models if m.get("step_implementation", {}).get("load_ready", False)]
    primary_model = loadable_models[0] if loadable_models else models[0]
    
    return {
        "step_name": step_name,
        "primary_model": primary_model,
        "config": config or {},
        "load_ready": len(loadable_models) > 0,
        "step_integration": primary_model.get("step_implementation", {}),
        "created_at": time.time()
    }

# ê¸°ì¡´ í˜¸í™˜ì„± ë³„ì¹­
RealWorldModelDetector = FixedModelDetector
create_real_world_detector = lambda **kwargs: FixedModelDetector()
comprehensive_model_detection = quick_model_detection

# ==============================================
# ğŸ”¥ 6. ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'FixedModelDetector',
    'DetectedModel', 
    'RealFileMapper',
    'get_global_detector',
    'quick_model_detection',
    'list_available_models',
    'get_models_for_step',
    'get_step_loadable_models',  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    'create_step_model_loader_config',  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    'generate_advanced_model_loader_config',
    'validate_model_exists',
    'create_step_interface',
    
    # í˜¸í™˜ì„±
    'RealWorldModelDetector',
    'create_real_world_detector',
    'comprehensive_model_detection'
]

# ==============================================
# ğŸ”¥ 7. ì´ˆê¸°í™”
# ==============================================

logger.info("âœ… Step ì—°ë™ ìë™ ëª¨ë¸ íƒì§€ê¸° v3.1 ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ¯ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ â†’ Step êµ¬í˜„ì²´ ì™„ë²½ ì—°ë™")
logger.info("ğŸ”§ ê¸°ì¡´ load_models() í•¨ìˆ˜ í™œìš©")
logger.info("âœ… Stepì´ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±í•˜ëŠ” êµ¬ì¡° ì§€ì›")

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_detector = get_global_detector()
    logger.info(f"ğŸš€ Step ì—°ë™ íƒì§€ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {_test_detector.ai_models_root}")
    logger.info(f"   M3 Max: {_test_detector.is_m3_max}")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸ” Step ì—°ë™ ìë™ ëª¨ë¸ íƒì§€ê¸° v3.1 í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    models = quick_model_detection()
    print(f"âœ… íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    # Step ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ í™•ì¸
    loadable_models = get_step_loadable_models()
    print(f"âœ… Step ë¡œë“œ ê°€ëŠ¥: {len(loadable_models)}ê°œ")
    
    if loadable_models:
        for model in loadable_models[:3]:
            step_info = model["step_implementation"]
            print(f"   - {model['name']}: {step_info['step_class_name']}.{step_info['method']}()")
    
    print("ğŸ‰ Step ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")