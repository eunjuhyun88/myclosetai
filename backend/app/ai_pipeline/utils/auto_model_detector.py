# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ìˆ˜ì •ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° v3.2 (ìš°ì„ ìˆœìœ„ ë¬¸ì œ í•´ê²°)
================================================================================
âœ… ê¸°ì¡´ 2ë²ˆ íŒŒì¼ êµ¬ì¡° ìµœëŒ€í•œ ìœ ì§€
âœ… Step êµ¬í˜„ì²´ì˜ ê¸°ì¡´ load_models() í•¨ìˆ˜ì™€ ì™„ë²½ ì—°ë™
âœ… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë§Œ ì •í™•íˆ ì°¾ì•„ì„œ Stepì—ê²Œ ì „ë‹¬
âœ… Stepì´ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±í•˜ëŠ” êµ¬ì¡° í™œìš©
âœ… conda í™˜ê²½ + M3 Max ìµœì í™” ìœ ì§€
âœ… ğŸ”¥ í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì™„ì „ ìˆ˜ì • (50MB ì´ìƒ ìš°ì„ )
âœ… ğŸ”¥ ëŒ€í˜• ëª¨ë¸ ìš°ì„  íƒì§€ ë° ì •ë ¬
âœ… ğŸ”¥ ì‘ì€ ë”ë¯¸ íŒŒì¼ ìë™ ì œê±°
âœ… ğŸ”¥ ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì •í™•í•œ ë§¤í•‘ í…Œì´ë¸” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì¶”ê°€)
# ==============================================

# ğŸ“ ê¸°ì¡´ RealFileMapper í´ë˜ìŠ¤ë¥¼ ì™„ì „íˆ êµì²´
class RealFileMapper:
    """ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ë™ì  ë§¤í•‘ ì‹œìŠ¤í…œ (5ë²ˆ íŒŒì¼ êµ¬ì¡° ë°˜ì˜)"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.RealFileMapper")
        
        # ğŸ”¥ 5ë²ˆ íŒŒì¼ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ë°˜ì˜
        self.step_file_mappings = {
            # Human Parsing (255MB íŒŒì¼ë“¤)
            "human_parsing_schp_atr": {
                "actual_files": [
                    "exp-schp-201908301523-atr.pth",
                    "exp-schp-201908261155-atr.pth", 
                    "exp-schp-201908261155-lip.pth"
                ],
                "search_paths": [
                    "step_01_human_parsing",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing",
                    "Self-Correction-Human-Parsing"
                ],
                "patterns": [r".*exp-schp.*atr.*\.pth$", r".*exp-schp.*lip.*\.pth$"],
                "size_range": (250, 260),
                "min_size_mb": 250,
                "priority": 1,
                "step_class": "HumanParsingImplementation",
                "model_load_method": "load_models"
            },
            
            # Cloth Segmentation (2.4GB SAM + 168MB U2Net)
            "cloth_segmentation_sam": {
                "actual_files": ["sam_vit_h_4b8939.pth"],
                "search_paths": [
                    "step_03_cloth_segmentation",
                    "step_03_cloth_segmentation/ultra_models",
                    "step_04_geometric_matching",
                    "step_04_geometric_matching/ultra_models"
                ],
                "patterns": [r".*sam_vit_h.*\.pth$"],
                "size_range": (2400, 2500),  # 2.4GB
                "min_size_mb": 2400,
                "priority": 1,
                "step_class": "ClothSegmentationImplementation",
                "model_load_method": "load_models"
            },
            
            # Virtual Fitting (3.2GB Diffusion)
            "virtual_fitting_diffusion": {
                "actual_files": [
                    "diffusion_pytorch_model.bin",
                    "diffusion_pytorch_model.safetensors"
                ],
                "search_paths": [
                    "step_06_virtual_fitting/ootdiffusion",
                    "checkpoints/step_06_virtual_fitting",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton"
                ],
                "patterns": [
                    r".*diffusion_pytorch_model\.bin$",
                    r".*diffusion_pytorch_model\.safetensors$"
                ],
                "size_range": (3100, 3300),  # 3.2GB
                "min_size_mb": 3100,
                "priority": 1,
                "step_class": "VirtualFittingImplementation",
                "model_load_method": "load_models"
            }
            # ... ë‚˜ë¨¸ì§€ ë§¤í•‘ë“¤ ì¶”ê°€
        }

        # í¬ê¸° ìš°ì„ ìˆœìœ„ ì„¤ì •
        self.size_priority_threshold = 50  # 50MB ì´ìƒë§Œ
        
        self.logger.info(f"âœ… ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ë§¤í•‘ ì´ˆê¸°í™”: {len(self.step_file_mappings)}ê°œ íŒ¨í„´")

    def find_actual_file(self, request_name: str, ai_models_root: Path) -> Optional[Path]:
        """ğŸ”¥ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ íŒŒì¼ ì°¾ê¸° (5ë²ˆ íŒŒì¼ êµ¬ì¡° ë°˜ì˜)"""
        try:
            # ì§ì ‘ ë§¤í•‘ í™•ì¸
            if request_name in self.step_file_mappings:
                mapping = self.step_file_mappings[request_name]
                found_candidates = []
                
                # ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
                for filename in mapping["actual_files"]:
                    for search_path in mapping["search_paths"]:
                        full_path = ai_models_root / search_path / filename
                        if full_path.exists() and full_path.is_file():
                            file_size_mb = full_path.stat().st_size / (1024 * 1024)
                            
                            # í¬ê¸° ê²€ì¦
                            min_size, max_size = mapping["size_range"]
                            if min_size <= file_size_mb <= max_size:
                                found_candidates.append((full_path, file_size_mb, "exact_match"))
                                self.logger.info(f"âœ… ì •í™•í•œ ë§¤ì¹­: {request_name} â†’ {full_path} ({file_size_mb:.1f}MB)")
                
                # í¬ê¸°ìˆœ ì •ë ¬ í›„ ìµœì  ì„ íƒ
                if found_candidates:
                    found_candidates.sort(key=lambda x: x[1], reverse=True)
                    best_match = found_candidates[0]
                    self.logger.info(f"ğŸ† ìµœì  ë§¤ì¹­: {request_name} â†’ {best_match[0]} ({best_match[1]:.1f}MB)")
                    return best_match[0]
            
            # í´ë°±: ì „ì²´ ê²€ìƒ‰
            return self._fallback_search(request_name, ai_models_root)
                
        except Exception as e:
            self.logger.error(f"âŒ {request_name} íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _fallback_search(self, request_name: str, ai_models_root: Path) -> Optional[Path]:
        """í´ë°± ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        try:
            keywords = request_name.lower().split('_')
            candidates = []
            
            extensions = ['.pth', '.bin', '.safetensors']
            
            for ext in extensions:
                for model_file in ai_models_root.rglob(f"*{ext}"):
                    if model_file.is_file():
                        file_size_mb = model_file.stat().st_size / (1024 * 1024)
                        if file_size_mb >= self.size_priority_threshold:
                            filename_lower = model_file.name.lower()
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                            score = sum(1 for keyword in keywords if keyword in filename_lower)
                            if score > 0:
                                candidates.append((model_file, file_size_mb, score))
            
            if candidates:
                # ì ìˆ˜ ìš°ì„ , í¬ê¸° ì°¨ì„ ìœ¼ë¡œ ì •ë ¬
                candidates.sort(key=lambda x: (x[2], x[1]), reverse=True)
                best_match = candidates[0]
                self.logger.info(f"ğŸ” í´ë°± ë§¤ì¹­: {request_name} â†’ {best_match[0]} ({best_match[1]:.1f}MB)")
                return best_match[0]
                
            return None
            
        except Exception as e:
            self.logger.debug(f"í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def get_step_info(self, request_name: str) -> Optional[Dict[str, Any]]:
        """Step êµ¬í˜„ì²´ ì •ë³´ ë°˜í™˜"""
        if request_name in self.step_file_mappings:
            mapping = self.step_file_mappings[request_name]
            return {
                "step_class": mapping.get("step_class"),
                "model_load_method": mapping.get("model_load_method"),
                "priority": mapping.get("priority"),
                "patterns": mapping.get("patterns", []),
                "min_size_mb": mapping.get("min_size_mb", self.size_priority_threshold)
            }
        return None

# ==============================================
# ğŸ”¥ 2. DetectedModel í´ë˜ìŠ¤ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì¶”ê°€)
# ==============================================

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ + Step ì—°ë™ ì •ë³´ + í¬ê¸° ìš°ì„ ìˆœìœ„"""
    name: str
    path: Path
    step_name: str
    model_type: str
    file_size_mb: float
    confidence_score: float
    
    # ğŸ”¥ Step êµ¬í˜„ì²´ ì—°ë™ ì •ë³´
    step_class_name: Optional[str] = None
    model_load_method: Optional[str] = None
    step_can_load: bool = False
    
    # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë³´
    priority_score: float = 0.0
    is_large_model: bool = False
    meets_size_requirement: bool = False
    
    # ì¶”ê°€ ì •ë³´
    checkpoint_path: Optional[str] = None
    device_compatible: bool = True
    recommended_device: str = "cpu"
    
    def __post_init__(self):
        """ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ìë™ ê³„ì‚°"""
        self.priority_score = self._calculate_priority_score()
        self.is_large_model = self.file_size_mb > 1000  # 1GB ì´ìƒ
        self.meets_size_requirement = self.file_size_mb >= 50  # 50MB ì´ìƒ
    
    def _calculate_priority_score(self) -> float:
        """ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        if self.file_size_mb > 0:
            import math
            score += math.log10(max(self.file_size_mb, 1)) * 100
        
        # ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
        score += self.confidence_score * 50
        
        # ëŒ€í˜• ëª¨ë¸ ë³´ë„ˆìŠ¤
        if self.file_size_mb > 2000:  # 2GB ì´ìƒ
            score += 200
        elif self.file_size_mb > 1000:  # 1GB ì´ìƒ
            score += 100
        elif self.file_size_mb > 500:  # 500MB ì´ìƒ
            score += 50
        elif self.file_size_mb > 200:  # 200MB ì´ìƒ
            score += 20
        elif self.file_size_mb >= 50:  # 50MB ì´ìƒ
            score += 10
        else:
            score -= 100  # 50MB ë¯¸ë§Œì€ ê°ì 
        
        # Step ë¡œë“œ ê°€ëŠ¥ ë³´ë„ˆìŠ¤
        if self.step_can_load:
            score += 30
        
        return score
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (í¬ê¸° ì •ë³´ ì¶”ê°€)"""
        return {
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "step_class": self.step_name,
            "model_type": self.model_type,
            "size_mb": self.file_size_mb,
            "confidence": self.confidence_score,
            "device_config": {
                "recommended_device": self.recommended_device,
                "device_compatible": self.device_compatible
            },
            
            # ğŸ”¥ Step ì—°ë™ ì •ë³´
            "step_implementation": {
                "step_class_name": self.step_class_name,
                "model_load_method": self.model_load_method,
                "step_can_load": self.step_can_load,
                "load_ready": self.step_can_load and self.checkpoint_path is not None
            },
            
            # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë³´
            "priority_info": {
                "priority_score": self.priority_score,
                "is_large_model": self.is_large_model,
                "meets_size_requirement": self.meets_size_requirement,
                "size_category": self._get_size_category()
            },
            
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.path.suffix
            }
        }
    
    def _get_size_category(self) -> str:
        """í¬ê¸° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if self.file_size_mb >= 2000:
            return "ultra_large"  # 2GB+
        elif self.file_size_mb >= 1000:
            return "large"  # 1GB+
        elif self.file_size_mb >= 500:
            return "medium_large"  # 500MB+
        elif self.file_size_mb >= 200:
            return "medium"  # 200MB+
        elif self.file_size_mb >= 50:
            return "small_valid"  # 50MB+
        else:
            return "too_small"  # 50MB ë¯¸ë§Œ
    
    def can_be_loaded_by_step(self) -> bool:
        """Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸ (í¬ê¸° ìš”êµ¬ì‚¬í•­ í¬í•¨)"""
        return (self.step_can_load and 
                self.step_class_name is not None and 
                self.model_load_method is not None and
                self.checkpoint_path is not None and
                self.meets_size_requirement)

# ==============================================
# ğŸ”¥ 3. ìˆ˜ì •ëœ ëª¨ë¸ íƒì§€ê¸° (í¬ê¸° ìš°ì„ ìˆœìœ„ ì™„ì „ ì ìš©)
# ==============================================

class FixedModelDetector:
    """ìˆ˜ì •ëœ ëª¨ë¸ íƒì§€ê¸° (í¬ê¸° ìš°ì„ ìˆœìœ„ ì™„ì „ ì ìš©)"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.FixedModelDetector")
        self.file_mapper = RealFileMapper()
        self.ai_models_root = self._find_ai_models_root()
        self.detected_models: Dict[str, DetectedModel] = {}
        
        # ğŸ”¥ í¬ê¸° ê¸°ë°˜ í•„í„°ë§ ì„¤ì •
        self.min_model_size_mb = 50  # 50MB ë¯¸ë§Œì€ ì œì™¸
        self.prioritize_large_models = True
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.is_m3_max = self._detect_m3_max()
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # í†µê³„ ì •ë³´
        self.detection_stats = {
            "total_files_scanned": 0,
            "models_found": 0,
            "large_models_found": 0,
            "small_models_filtered": 0,
            "step_loadable_models": 0,
            "scan_duration": 0.0
        }
        
        self.logger.info(f"ğŸ”§ í¬ê¸° ìš°ì„ ìˆœìœ„ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”")
        self.logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        self.logger.info(f"   ìµœì†Œ í¬ê¸°: {self.min_model_size_mb}MB")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸° (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        current = Path(__file__).resolve()
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        for _ in range(10):
            if current.name == 'mycloset-ai' or (current / 'backend').exists():
                return current / 'backend' / 'ai_models'
            if current.name == 'backend':
                return current / 'ai_models'
            if current.parent == current:
                break
            current = current.parent
        
        # í´ë°±
        fallback_path = Path(__file__).resolve().parent.parent.parent.parent / 'ai_models'
        self.logger.warning(f"âš ï¸ í´ë°± ê²½ë¡œ ì‚¬ìš©: {fallback_path}")
        return fallback_path
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        try:
            import platform
            return 'arm64' in platform.machine().lower()
        except:
            return False
    
    def detect_all_models(self) -> Dict[str, DetectedModel]:
        """ğŸ”¥ ëª¨ë“  ëª¨ë¸ íƒì§€ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì™„ì „ ì ìš©)"""
        start_time = time.time()
        self.detected_models.clear()
        self.detection_stats = {
            "total_files_scanned": 0,
            "models_found": 0,
            "large_models_found": 0,
            "small_models_filtered": 0,
            "step_loadable_models": 0,
            "scan_duration": 0.0
        }
        
        if not self.ai_models_root.exists():
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.ai_models_root}")
            return {}
        
        self.logger.info("ğŸ” í¬ê¸° ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëª¨ë¸ íƒì§€ ì‹œì‘...")
        
        # ìš”ì²­ëª…ë³„ë¡œ ì‹¤ì œ íŒŒì¼ ì°¾ê¸° + Step ì •ë³´ ì¶”ê°€ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)
        for request_name in self.file_mapper.step_file_mappings.keys():
            try:
                # 1. ì‹¤ì œ íŒŒì¼ ì°¾ê¸° (í¬ê¸° í•„í„° ì ìš©)
                actual_file = self.file_mapper.find_actual_file(request_name, self.ai_models_root)
                
                if actual_file:
                    # 2. Step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    step_info = self.file_mapper.get_step_info(request_name)
                    
                    # 3. DetectedModel ìƒì„± (í¬ê¸° ìš°ì„ ìˆœìœ„ í¬í•¨)
                    model = self._create_detected_model_with_step_info(request_name, actual_file, step_info)
                    if model and model.meets_size_requirement:
                        self.detected_models[model.name] = model
                        self.detection_stats["models_found"] += 1
                        
                        if model.is_large_model:
                            self.detection_stats["large_models_found"] += 1
                        
                        if model.can_be_loaded_by_step():
                            self.detection_stats["step_loadable_models"] += 1
                    elif model:
                        self.detection_stats["small_models_filtered"] += 1
                        self.logger.debug(f"ğŸ—‘ï¸ í¬ê¸° ë¶€ì¡±ìœ¼ë¡œ ì œì™¸: {request_name} ({model.file_size_mb:.1f}MB)")
                        
            except Exception as e:
                self.logger.error(f"âŒ {request_name} íƒì§€ ì‹¤íŒ¨: {e}")
                continue
        
        # ğŸ”¥ ì¶”ê°€ íŒŒì¼ë“¤ ìë™ ìŠ¤ìº” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)
        self._scan_additional_files()
        
        # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        if self.prioritize_large_models:
            self._sort_models_by_priority()
        
        self.detection_stats["scan_duration"] = time.time() - start_time
        
        self.logger.info(f"ğŸ‰ í¬ê¸° ìš°ì„ ìˆœìœ„ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {self.detection_stats['models_found']}ê°œ")
        self.logger.info(f"ğŸ“Š ëŒ€í˜• ëª¨ë¸: {self.detection_stats['large_models_found']}ê°œ")
        self.logger.info(f"ğŸ—‘ï¸ ì‘ì€ ëª¨ë¸ ì œì™¸: {self.detection_stats['small_models_filtered']}ê°œ")
        self.logger.info(f"âœ… Step ë¡œë“œ ê°€ëŠ¥: {self.detection_stats['step_loadable_models']}ê°œ")
        self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {self.detection_stats['scan_duration']:.2f}ì´ˆ")
        
        return self.detected_models
    
    def _create_detected_model_with_step_info(self, request_name: str, file_path: Path, step_info: Optional[Dict]) -> Optional[DetectedModel]:
        """DetectedModel ìƒì„± (í¬ê¸° ìš°ì„ ìˆœìœ„ í¬í•¨)"""
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # Step ì´ë¦„ ì¶”ì¶œ
            step_name = self._extract_step_name(request_name)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            recommended_device = "mps" if self.is_m3_max else "cpu"
            
            # ğŸ”¥ Step ì—°ë™ ì •ë³´ ì„¤ì •
            step_class_name = None
            model_load_method = None
            step_can_load = False
            
            if step_info:
                step_class_name = step_info.get("step_class")
                model_load_method = step_info.get("model_load_method", "load_models")
                step_can_load = bool(step_class_name and model_load_method)
            
            # ğŸ”¥ ì‹ ë¢°ë„ ê³„ì‚° (í¬ê¸° ê¸°ë°˜)
            confidence_score = self._calculate_size_based_confidence(file_size_mb, step_info)
            
            model = DetectedModel(
                name=request_name,
                path=file_path,
                step_name=step_name,
                model_type=step_name.replace("Step", "").lower(),
                file_size_mb=file_size_mb,
                confidence_score=confidence_score,
                
                # ğŸ”¥ Step ì—°ë™ ì •ë³´
                step_class_name=step_class_name,
                model_load_method=model_load_method,
                step_can_load=step_can_load,
                
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device=recommended_device
            )
            
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_size_based_confidence(self, file_size_mb: float, step_info: Optional[Dict]) -> float:
        """ğŸ”¥ í¬ê¸° ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # í¬ê¸° ê¸°ë°˜ ì‹ ë¢°ë„
        if file_size_mb >= 2000:  # 2GB+
            confidence = 1.0
        elif file_size_mb >= 1000:  # 1GB+
            confidence = 0.95
        elif file_size_mb >= 500:  # 500MB+
            confidence = 0.9
        elif file_size_mb >= 200:  # 200MB+
            confidence = 0.8
        elif file_size_mb >= 100:  # 100MB+
            confidence = 0.7
        elif file_size_mb >= 50:  # 50MB+
            confidence = 0.6
        else:  # 50MB ë¯¸ë§Œ
            confidence = 0.1
        
        # Step ì •ë³´ ë³´ë„ˆìŠ¤
        if step_info:
            min_expected_size = step_info.get("min_size_mb", 50)
            if file_size_mb >= min_expected_size:
                confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _extract_step_name(self, request_name: str) -> str:
        """ìš”ì²­ëª…ì—ì„œ Step ì´ë¦„ ì¶”ì¶œ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        step_mappings = {
            "human_parsing": "HumanParsingStep",
            "pose_estimation": "PoseEstimationStep", 
            "cloth_segmentation": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting": "VirtualFittingStep",
            "post_processing": "PostProcessingStep",
            "quality_assessment": "QualityAssessmentStep"
        }
        
        for key, step_name in step_mappings.items():
            if key in request_name:
                return step_name
        
        return "UnknownStep"
    
    def _scan_additional_files(self):
        """ğŸ”¥ ì¶”ê°€ íŒŒì¼ë“¤ ìë™ ìŠ¤ìº” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
        try:
            # Ultra ëª¨ë¸ë“¤ ìŠ¤ìº”
            ultra_dir = self.ai_models_root / "ultra_models"
            if ultra_dir.exists():
                self._scan_ultra_models(ultra_dir)
            
            # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº 
            checkpoints_dir = self.ai_models_root / "checkpoints"
            if checkpoints_dir.exists():
                self._scan_checkpoints(checkpoints_dir)
                
        except Exception as e:
            self.logger.debug(f"ì¶”ê°€ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
    
    def _scan_ultra_models(self, ultra_dir: Path):
        """ğŸ”¥ Ultra ëª¨ë¸ ìŠ¤ìº” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
        model_extensions = {'.pth', '.bin', '.safetensors', '.ckpt'}
        
        candidates = []
        
        for file_path in ultra_dir.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix.lower() in model_extensions):
                
                try:
                    file_size_mb = file_path.stat().st_size / (1024 * 1024)
                    
                    # ğŸ”¥ í¬ê¸° í•„í„° ì ìš©
                    if file_size_mb < self.min_model_size_mb:
                        self.detection_stats["small_models_filtered"] += 1
                        continue
                    
                    candidates.append((file_path, file_size_mb))
                    
                except Exception as e:
                    self.logger.debug(f"Ultra ëª¨ë¸ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
        
        # ğŸ”¥ í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        for file_path, file_size_mb in candidates:
            model_name = f"ultra_{file_path.parent.name}_{file_path.stem}"
            
            # ì¤‘ë³µ ë°©ì§€
            if model_name in self.detected_models:
                continue
            
            model = DetectedModel(
                name=model_name,
                path=file_path,
                step_name="UltraModel",
                model_type="ultra",
                file_size_mb=file_size_mb,
                confidence_score=self._calculate_size_based_confidence(file_size_mb, None),
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device="mps" if self.is_m3_max else "cpu"
            )
            
            if model.meets_size_requirement:
                self.detected_models[model_name] = model
                self.detection_stats["models_found"] += 1
                
                if model.is_large_model:
                    self.detection_stats["large_models_found"] += 1
                
                self.logger.debug(f"âœ… Ultra ëª¨ë¸: {model_name} ({file_size_mb:.1f}MB)")
    
    def _scan_checkpoints(self, checkpoints_dir: Path):
        """ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
        candidates = []
        
        for subdir in checkpoints_dir.iterdir():
            if subdir.is_dir():
                for file_path in subdir.rglob('*.pth'):
                    # ì¤‘ë³µ ë°©ì§€
                    if file_path.name not in [m.path.name for m in self.detected_models.values()]:
                        try:
                            file_size_mb = file_path.stat().st_size / (1024 * 1024)
                            
                            # ğŸ”¥ í¬ê¸° í•„í„° ì ìš©
                            if file_size_mb < self.min_model_size_mb:
                                self.detection_stats["small_models_filtered"] += 1
                                continue
                            
                            candidates.append((file_path, file_size_mb, subdir.name))
                            
                        except Exception as e:
                            self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                            continue
        
        # ğŸ”¥ í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        for file_path, file_size_mb, subdir_name in candidates:
            model_name = f"checkpoint_{subdir_name}_{file_path.stem}"
            
            model = DetectedModel(
                name=model_name,
                path=file_path,
                step_name="CheckpointModel",
                model_type="checkpoint",
                file_size_mb=file_size_mb,
                confidence_score=self._calculate_size_based_confidence(file_size_mb, None),
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device="mps" if self.is_m3_max else "cpu"
            )
            
            if model.meets_size_requirement:
                self.detected_models[model_name] = model
                self.detection_stats["models_found"] += 1
                
                if model.is_large_model:
                    self.detection_stats["large_models_found"] += 1
                
                self.logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸: {model_name} ({file_size_mb:.1f}MB)")
    
    def _sort_models_by_priority(self):
        """ğŸ”¥ ëª¨ë¸ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬"""
        try:
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
            sorted_items = sorted(
                self.detected_models.items(),
                key=lambda x: x[1].priority_score,
                reverse=True
            )
            
            # ì •ë ¬ëœ ìˆœì„œë¡œ ì¬ë°°ì¹˜
            self.detected_models = dict(sorted_items)
            
            self.logger.info("ğŸ¯ ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì •ë ¬ ì™„ë£Œ")
            
            # ìƒìœ„ 5ê°œ ëª¨ë¸ ë¡œê¹…
            for i, (name, model) in enumerate(list(self.detected_models.items())[:5]):
                self.logger.info(f"  {i+1}. {name}: {model.file_size_mb:.1f}MB (ì ìˆ˜: {model.priority_score:.1f})")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë ¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 4. ModelLoader í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)
# ==============================================

def get_step_loadable_models() -> List[Dict[str, Any]]:
    """ğŸ”¥ Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    loadable_models = []
    for model in models.values():
        if model.can_be_loaded_by_step():
            model_dict = model.to_dict()
            model_dict["load_instruction"] = {
                "step_class": model.step_class_name,
                "method": model.model_load_method,
                "checkpoint_path": model.checkpoint_path
            }
            loadable_models.append(model_dict)
    
    # ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
    return sorted(loadable_models, key=lambda x: x["priority_info"]["priority_score"], reverse=True)

def create_step_model_loader_config() -> Dict[str, Any]:
    """ğŸ”¥ Step êµ¬í˜„ì²´ ì—°ë™ìš© ModelLoader ì„¤ì • ìƒì„± (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
    detector = get_global_detector()
    detected_models = detector.detect_all_models()
    
    config = {
        "version": "step_integrated_detector_v3.2_priority_fixed",
        "generated_at": time.time(),
        "device": "mps" if detector.is_m3_max else "cpu",
        "is_m3_max": detector.is_m3_max,
        "conda_env": detector.conda_env,
        "min_model_size_mb": detector.min_model_size_mb,
        "prioritize_large_models": detector.prioritize_large_models,
        "models": {},
        "step_mappings": {},
        "step_loadable_count": 0,
        "detection_stats": detector.detection_stats
    }
    
    # ëª¨ë¸ë³„ ì„¤ì • (ìš°ì„ ìˆœìœ„ ìˆœ)
    for model_name, model in detected_models.items():
        model_dict = model.to_dict()
        config["models"][model_name] = model_dict
        
        # Step ë¡œë“œ ê°€ëŠ¥ ì¹´ìš´íŠ¸
        if model.can_be_loaded_by_step():
            config["step_loadable_count"] += 1
        
        # Step ë§¤í•‘
        step_name = model.step_name
        if step_name not in config["step_mappings"]:
            config["step_mappings"][step_name] = []
        config["step_mappings"][step_name].append(model_name)
    
    # í†µê³„ (í¬ê¸° ê¸°ë°˜)
    config["summary"] = {
        "total_models": len(detected_models),
        "large_models": sum(1 for m in detected_models.values() if m.is_large_model),
        "step_loadable_models": config["step_loadable_count"],
        "total_size_gb": sum(m.file_size_mb for m in detected_models.values()) / 1024,
        "average_size_mb": sum(m.file_size_mb for m in detected_models.values()) / len(detected_models) if detected_models else 0,
        "device_optimized": detector.is_m3_max,
        "step_integration_ready": config["step_loadable_count"] > 0,
        "min_size_threshold_mb": detector.min_model_size_mb,
        "priority_sorting_enabled": detector.prioritize_large_models
    }
    
    logger.info(f"âœ… í¬ê¸° ìš°ì„ ìˆœìœ„ ì„¤ì • ìƒì„±: {len(detected_models)}ê°œ ëª¨ë¸, {config['step_loadable_count']}ê°œ Step ë¡œë“œ ê°€ëŠ¥")
    logger.info(f"ğŸ“Š ëŒ€í˜• ëª¨ë¸: {config['summary']['large_models']}ê°œ, í‰ê·  í¬ê¸°: {config['summary']['average_size_mb']:.1f}MB")
    return config

# ==============================================
# ğŸ”¥ 5. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€ + í¬ê¸° ìš°ì„ ìˆœìœ„)
# ==============================================

_global_detector: Optional[FixedModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector() -> FixedModelDetector:
    """ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    global _global_detector
    if _global_detector is None:
        with _detector_lock:
            if _global_detector is None:
                _global_detector = FixedModelDetector()
    return _global_detector

def quick_model_detection() -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€, í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
    detector = get_global_detector()
    return detector.detect_all_models()

def list_available_models(step_class: Optional[str] = None) -> List[Dict[str, Any]]:
    """ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë ¬, ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    result = []
    for model in models.values():
        model_dict = model.to_dict()
        
        if step_class and model_dict["step_class"] != step_class:
            continue
        
        result.append(model_dict)
    
    # ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„°)
    return sorted(result, key=lambda x: x["priority_info"]["priority_score"], reverse=True)

def get_models_for_step(step_name: str) -> List[Dict[str, Any]]:
    """ğŸ”¥ Stepë³„ ëª¨ë¸ ì¡°íšŒ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©, ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    models = list_available_models(step_class=step_name)
    return models

def validate_model_exists(model_name: str) -> bool:
    """ëª¨ë¸ ì¡´ì¬ í™•ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    detector = get_global_detector()
    return model_name in detector.detected_models

def generate_advanced_model_loader_config() -> Dict[str, Any]:
    """ğŸ”¥ ê³ ê¸‰ ModelLoader ì„¤ì • ìƒì„± (í¬ê¸° ìš°ì„ ìˆœìœ„ í¬í•¨, ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return create_step_model_loader_config()

def create_step_interface(step_name: str, config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©, ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    models = get_models_for_step(step_name)
    if not models:
        return None
    
    # Step ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ ìš°ì„  ì„ íƒ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)
    loadable_models = [m for m in models if m.get("step_implementation", {}).get("load_ready", False)]
    primary_model = loadable_models[0] if loadable_models else models[0]
    
    return {
        "step_name": step_name,
        "primary_model": primary_model,
        "config": config or {},
        "load_ready": len(loadable_models) > 0,
        "step_integration": primary_model.get("step_implementation", {}),
        "priority_info": primary_model.get("priority_info", {}),
        "created_at": time.time()
    }

def get_large_models_only() -> List[Dict[str, Any]]:
    """ğŸ”¥ ëŒ€í˜• ëª¨ë¸ë§Œ ë°˜í™˜ (1GB ì´ìƒ)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    large_models = []
    for model in models.values():
        if model.is_large_model:
            large_models.append(model.to_dict())
    
    return sorted(large_models, key=lambda x: x["size_mb"], reverse=True)

def get_detection_statistics() -> Dict[str, Any]:
    """ğŸ”¥ íƒì§€ í†µê³„ ë°˜í™˜"""
    detector = get_global_detector()
    detector.detect_all_models()  # ìµœì‹  í†µê³„ í™•ë³´
    
    return {
        "detection_stats": detector.detection_stats,
        "system_info": {
            "ai_models_root": str(detector.ai_models_root),
            "min_model_size_mb": detector.min_model_size_mb,
            "prioritize_large_models": detector.prioritize_large_models,
            "is_m3_max": detector.is_m3_max,
            "conda_env": detector.conda_env
        },
        "model_summary": {
            "total_detected": len(detector.detected_models),
            "large_models": sum(1 for m in detector.detected_models.values() if m.is_large_model),
            "step_loadable": sum(1 for m in detector.detected_models.values() if m.can_be_loaded_by_step()),
            "average_size_mb": sum(m.file_size_mb for m in detector.detected_models.values()) / len(detector.detected_models) if detector.detected_models else 0
        }
    }

# ê¸°ì¡´ í˜¸í™˜ì„± ë³„ì¹­ (í•¨ìˆ˜ëª… ìœ ì§€)
RealWorldModelDetector = FixedModelDetector
create_real_world_detector = lambda **kwargs: FixedModelDetector()
comprehensive_model_detection = quick_model_detection

# ==============================================
# ğŸ”¥ 6. ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
# ==============================================

__all__ = [
    'FixedModelDetector',
    'DetectedModel', 
    'RealFileMapper',
    'get_global_detector',
    'quick_model_detection',
    'list_available_models',
    'get_models_for_step',
    'get_step_loadable_models',
    'create_step_model_loader_config',
    'generate_advanced_model_loader_config',
    'validate_model_exists',
    'create_step_interface',
    'get_large_models_only',  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    'get_detection_statistics',  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    
    # í˜¸í™˜ì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
    'RealWorldModelDetector',
    'create_real_world_detector',
    'comprehensive_model_detection'
]

# ==============================================
# ğŸ”¥ 7. ì´ˆê¸°í™” (í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë³´ ì¶”ê°€)
# ==============================================

logger.info("âœ… ì™„ì „ ìˆ˜ì •ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° v3.2 ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ¯ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ â†’ Step êµ¬í˜„ì²´ ì™„ë²½ ì—°ë™")
logger.info("ğŸ”§ ê¸°ì¡´ load_models() í•¨ìˆ˜ í™œìš©")
logger.info("âœ… Stepì´ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±í•˜ëŠ” êµ¬ì¡° ì§€ì›")
logger.info("ğŸ”¥ âœ… í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì™„ì „ ì ìš© (50MB ì´ìƒ)")
logger.info("ğŸ”¥ âœ… ëŒ€í˜• ëª¨ë¸ ìš°ì„  íƒì§€ ë° ì •ë ¬")
logger.info("ğŸ”¥ âœ… ì‘ì€ ë”ë¯¸ íŒŒì¼ ìë™ ì œê±°")
logger.info("âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€")

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_detector = get_global_detector()
    logger.info(f"ğŸš€ í¬ê¸° ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {_test_detector.ai_models_root}")
    logger.info(f"   ìµœì†Œ í¬ê¸°: {_test_detector.min_model_size_mb}MB")
    logger.info(f"   M3 Max: {_test_detector.is_m3_max}")
    logger.info(f"   ëŒ€í˜• ëª¨ë¸ ìš°ì„ : {_test_detector.prioritize_large_models}")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸ” ì™„ì „ ìˆ˜ì •ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° v3.2 í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    models = quick_model_detection()
    print(f"âœ… íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    # í¬ê¸°ë³„ ë¶„ë¥˜
    large_models = [m for m in models.values() if m.is_large_model]
    valid_models = [m for m in models.values() if m.meets_size_requirement]
    step_loadable = [m for m in models.values() if m.can_be_loaded_by_step()]
    
    print(f"ğŸ“Š ëŒ€í˜• ëª¨ë¸ (1GB+): {len(large_models)}ê°œ")
    print(f"âœ… ìœ íš¨ ëª¨ë¸ (50MB+): {len(valid_models)}ê°œ")
    print(f"ğŸ”— Step ë¡œë“œ ê°€ëŠ¥: {len(step_loadable)}ê°œ")
    
    if step_loadable:
        print("\nğŸ† ìƒìœ„ Step ë¡œë“œ ê°€ëŠ¥ ëª¨ë¸:")
        for i, model in enumerate(step_loadable[:5]):
            step_info = model.step_implementation if hasattr(model, 'step_implementation') else {}
            print(f"   {i+1}. {model.name}: {model.file_size_mb:.1f}MB (ì ìˆ˜: {model.priority_score:.1f})")
    

    
    class RealFileMapper:
        """ModelLoader í˜¸í™˜ì„±ì„ ìœ„í•œ RealWorldModelDetector ì–´ëŒ‘í„°"""
        def __init__(self):
            self.detector = get_global_detector()
        
        def find_actual_file(self, request_name, ai_models_root):
            # RealWorldModelDetector ë©”ì„œë“œ í˜¸ì¶œ
            return self.detector.find_model_by_name(request_name)
        
        def get_step_info(self, request_name):
            return self.detector.step_mapper.match_file_to_step(request_name)
        
        def discover_all_search_paths(self, ai_models_root):
            return self.detector.path_discovery.discover_all_paths()

    
    RealFileMapper = RealWorldModelDetector 


    # í†µê³„ ì¶œë ¥
    stats = get_detection_statistics()
    print(f"\nğŸ“ˆ íƒì§€ í†µê³„:")
    print(f"   ìŠ¤ìº” ì‹œê°„: {stats['detection_stats']['scan_duration']:.2f}ì´ˆ")
    print(f"   ì œì™¸ëœ ì‘ì€ íŒŒì¼: {stats['detection_stats']['small_models_filtered']}ê°œ")
    
    print("ğŸ‰ í¬ê¸° ìš°ì„ ìˆœìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")