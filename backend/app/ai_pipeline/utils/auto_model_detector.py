#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ë™ì  ìë™ ëª¨ë¸ íƒì§€ê¸° v3.0 (ì™„ì „ ì¬ì„¤ê³„)
================================================================================
âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ 100% ë™ì  íƒì§€
âœ… ultra_models í´ë”ê¹Œì§€ ì™„ì „ ì»¤ë²„
âœ… conda í™˜ê²½ íŠ¹í™” ìºì‹œ ì „ëµ
âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜ - ë‹¤ë¥¸ íŒŒì¼ ìˆ˜ì • ë¶ˆí•„ìš”
âœ… Stepë³„ ìš”êµ¬ì‚¬í•­ê³¼ ì‹¤ì œ ëª¨ë¸ ë§¤í•‘ ìë™í™”
âœ… ëª¨ë¸ ë¡œë”© ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
âœ… í•˜ë“œì½”ë”© ì œê±°, ì™„ì „ ë™ì  ë§¤í•‘
âœ… M3 Max 128GB ìµœì í™”
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
import asyncio
import sqlite3
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import weakref
import gc

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. í•µì‹¬ ë°ì´í„° êµ¬ì¡° (ê¸°ì¡´ í˜¸í™˜ì„± 100% ìœ ì§€)
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    AUXILIARY = "auxiliary"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class ModelLoadingStatus(Enum):
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ (ìƒˆë¡œ ì¶”ê°€)"""
    UNKNOWN = "unknown"
    DISCOVERED = "discovered"
    VALIDATING = "validating"
    VALID = "valid"
    INVALID = "invalid"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± + ë™ì  ë§¤í•‘ ì •ë³´)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ModelLoader í•µì‹¬ ìš”êµ¬ì‚¬í•­
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    
    # ğŸ”¥ ë™ì  ë§¤í•‘ ì •ë³´ (ìƒˆë¡œ ì¶”ê°€)
    checkpoint_path: Optional[str] = None
    model_architecture: Optional[str] = None
    input_requirements: Dict[str, Any] = field(default_factory=dict)
    output_format: Optional[str] = None
    memory_requirement_mb: float = 0.0
    
    # ğŸ”¥ ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ìƒˆë¡œ ì¶”ê°€)
    loading_status: ModelLoadingStatus = ModelLoadingStatus.UNKNOWN
    last_validated: float = 0.0
    validation_errors: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepModelRequirement:
    """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ (ìƒˆë¡œ ì¶”ê°€)"""
    step_name: str
    step_class: str
    category: ModelCategory
    priority: ModelPriority
    required_files: List[str] = field(default_factory=list)
    file_patterns: List[str] = field(default_factory=list)
    size_range_mb: Tuple[float, float] = (1.0, 10000.0)
    architecture_types: List[str] = field(default_factory=list)
    input_specs: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)

# ==============================================
# ğŸ”¥ 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ìƒˆë¡œ ì¶”ê°€)
# ==============================================

class ModelLoadingMonitor:
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.status_cache: Dict[str, ModelLoadingStatus] = {}
        self.loading_times: Dict[str, float] = {}
        self.error_counts: Dict[str, int] = defaultdict(int)
        self.observers: List[Callable] = []
        self._lock = threading.Lock()
        
    def add_observer(self, callback: Callable[[str, ModelLoadingStatus], None]):
        """ìƒíƒœ ë³€ê²½ ì½œë°± ë“±ë¡"""
        self.observers.append(callback)
    
    def update_status(self, model_name: str, status: ModelLoadingStatus, 
                     error_msg: Optional[str] = None):
        """ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        with self._lock:
            old_status = self.status_cache.get(model_name, ModelLoadingStatus.UNKNOWN)
            self.status_cache[model_name] = status
            
            if status == ModelLoadingStatus.LOADING:
                self.loading_times[model_name] = time.time()
            elif status == ModelLoadingStatus.ERROR:
                self.error_counts[model_name] += 1
            
            # ëª¨ë“  ê´€ì°°ìì—ê²Œ ì•Œë¦¼
            for observer in self.observers:
                try:
                    observer(model_name, status, old_status, error_msg)
                except Exception as e:
                    logger.warning(f"Observer ì½œë°± ì˜¤ë¥˜: {e}")
    
    def get_status(self, model_name: str) -> ModelLoadingStatus:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        return self.status_cache.get(model_name, ModelLoadingStatus.UNKNOWN)
    
    def get_loading_summary(self) -> Dict[str, Any]:
        """ë¡œë”© ìƒíƒœ ìš”ì•½"""
        status_counts = defaultdict(int)
        for status in self.status_cache.values():
            status_counts[status.value] += 1
        
        return {
            "total_models": len(self.status_cache),
            "status_breakdown": dict(status_counts),
            "error_models": {k: v for k, v in self.error_counts.items() if v > 0},
            "loading_times": {k: time.time() - v for k, v in self.loading_times.items()},
            "timestamp": time.time()
        }

# ==============================================
# ğŸ”¥ 3. ë™ì  íŒŒì¼ ì‹œìŠ¤í…œ íƒì§€ê¸° (ê°œì„ )
# ==============================================

class DynamicFileSystemScanner:
    """ë™ì  íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, base_path: Union[str, Path]):
        self.base_path = Path(base_path)
        self.scan_cache: Dict[str, Dict] = {}
        self.last_scan_time = 0.0
        self.cache_ttl = 300.0  # 5ë¶„ ìºì‹œ
        
    def scan_all_model_files(self, force_refresh: bool = False) -> Dict[str, List[Path]]:
        """ëª¨ë“  ëª¨ë¸ íŒŒì¼ ë™ì  ìŠ¤ìº”"""
        cache_key = "all_model_files"
        current_time = time.time()
        
        if (not force_refresh and 
            cache_key in self.scan_cache and 
            current_time - self.last_scan_time < self.cache_ttl):
            return self.scan_cache[cache_key]
        
        logger.info(f"ğŸ” ë™ì  íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº” ì‹œì‘: {self.base_path}")
        start_time = time.time()
        
        model_files = defaultdict(list)
        
        # ì§€ì›í•˜ëŠ” ëª¨ë¸ íŒŒì¼ í™•ì¥ì
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.onnx'}
        
        # ì „ì²´ ë””ë ‰í† ë¦¬ ì¬ê·€ íƒìƒ‰
        for file_path in self.base_path.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix.lower() in model_extensions and
                file_path.stat().st_size > 1024 * 1024):  # 1MB ì´ìƒë§Œ
                
                # íŒŒì¼ ìœ„ì¹˜ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
                category = self._infer_category_from_path(file_path)
                model_files[category].append(file_path)
        
        scan_time = time.time() - start_time
        logger.info(f"âœ… íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ: {scan_time:.2f}ì´ˆ, {sum(len(files) for files in model_files.values())}ê°œ íŒŒì¼")
        
        self.scan_cache[cache_key] = dict(model_files)
        self.last_scan_time = current_time
        
        return dict(model_files)
    
    def _infer_category_from_path(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        path_str = str(file_path).lower()
        
        # Step ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë§¤í•‘
        step_mappings = {
            'step_01': 'human_parsing',
            'step_02': 'pose_estimation', 
            'step_03': 'cloth_segmentation',
            'step_04': 'geometric_matching',
            'step_05': 'cloth_warping',
            'step_06': 'virtual_fitting',
            'step_07': 'post_processing',
            'step_08': 'quality_assessment'
        }
        
        for step_dir, category in step_mappings.items():
            if step_dir in path_str:
                return category
        
        # íŒŒì¼ëª… ê¸°ë°˜ ì¶”ë¡ 
        filename = file_path.name.lower()
        
        if any(keyword in filename for keyword in ['human', 'parsing', 'schp', 'atr']):
            return 'human_parsing'
        elif any(keyword in filename for keyword in ['pose', 'openpose', 'body']):
            return 'pose_estimation'
        elif any(keyword in filename for keyword in ['sam', 'segment', 'mask']):
            return 'cloth_segmentation'
        elif any(keyword in filename for keyword in ['diffusion', 'unet', 'vton', 'ootd']):
            return 'virtual_fitting'
        elif any(keyword in filename for keyword in ['clip', 'quality', 'assessment']):
            return 'quality_assessment'
        
        return 'auxiliary'

# ==============================================
# ğŸ”¥ 4. Stepë³„ ìš”êµ¬ì‚¬í•­ ë§¤í•‘ ì‹œìŠ¤í…œ (ìƒˆë¡œ ì¶”ê°€)
# ==============================================

class StepRequirementMapper:
    """Stepë³„ ìš”êµ¬ì‚¬í•­ê³¼ ì‹¤ì œ ëª¨ë¸ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.requirements: Dict[str, StepModelRequirement] = {}
        self.mappings: Dict[str, List[DetectedModel]] = defaultdict(list)
        self._initialize_step_requirements()
    
    def _initialize_step_requirements(self):
        """Stepë³„ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì´ˆê¸°í™”"""
        self.requirements = {
            "step_01_human_parsing": StepModelRequirement(
                step_name="step_01_human_parsing",
                step_class="HumanParsingStep",
                category=ModelCategory.HUMAN_PARSING,
                priority=ModelPriority.CRITICAL,
                file_patterns=[
                    r".*human.*parsing.*\.(pth|pt|bin)$",
                    r".*schp.*\.(pth|pt)$",
                    r".*atr.*\.(pth|pt)$",
                    r".*lip.*\.(pth|pt)$"
                ],
                size_range_mb=(50, 500),
                architecture_types=["ResNet", "DeepLabV3", "HRNet"],
                input_specs={"height": 512, "width": 512, "channels": 3}
            ),
            
            "step_02_pose_estimation": StepModelRequirement(
                step_name="step_02_pose_estimation", 
                step_class="PoseEstimationStep",
                category=ModelCategory.POSE_ESTIMATION,
                priority=ModelPriority.HIGH,
                file_patterns=[
                    r".*pose.*\.(pth|pt|bin)$",
                    r".*openpose.*\.(pth|pt)$",
                    r".*body.*\.(pth|pt)$"
                ],
                size_range_mb=(100, 400),
                architecture_types=["OpenPose", "HRNet", "AlphaPose"],
                input_specs={"height": 368, "width": 368, "channels": 3}
            ),
            
            "step_03_cloth_segmentation": StepModelRequirement(
                step_name="step_03_cloth_segmentation",
                step_class="ClothSegmentationStep", 
                category=ModelCategory.CLOTH_SEGMENTATION,
                priority=ModelPriority.HIGH,
                file_patterns=[
                    r".*sam.*\.(pth|pt|bin)$",
                    r".*segment.*\.(pth|pt)$",
                    r".*mask.*\.(pth|pt)$"
                ],
                size_range_mb=(500, 3000),
                architecture_types=["SAM", "U2Net", "DeepLabV3"],
                input_specs={"height": 1024, "width": 1024, "channels": 3}
            ),
            
            "step_06_virtual_fitting": StepModelRequirement(
                step_name="step_06_virtual_fitting",
                step_class="VirtualFittingStep",
                category=ModelCategory.VIRTUAL_FITTING,
                priority=ModelPriority.CRITICAL,
                file_patterns=[
                    r".*diffusion.*\.(pth|pt|bin|safetensors)$",
                    r".*unet.*\.(pth|pt|safetensors)$",
                    r".*vton.*\.(pth|pt|safetensors)$",
                    r".*ootd.*\.(pth|pt|safetensors)$"
                ],
                size_range_mb=(1000, 8000),
                architecture_types=["UNet", "Diffusion", "DDPM"],
                input_specs={"height": 1024, "width": 768, "channels": 3}
            ),
            
            "step_08_quality_assessment": StepModelRequirement(
                step_name="step_08_quality_assessment",
                step_class="QualityAssessmentStep",
                category=ModelCategory.QUALITY_ASSESSMENT,
                priority=ModelPriority.MEDIUM,
                file_patterns=[
                    r".*clip.*\.(pth|pt|bin)$",
                    r".*quality.*\.(pth|pt)$",
                    r".*assessment.*\.(pth|pt)$"
                ],
                size_range_mb=(500, 6000),
                architecture_types=["CLIP", "ViT", "ResNet"],
                input_specs={"height": 224, "width": 224, "channels": 3}
            )
        }
    
    def map_models_to_steps(self, detected_models: List[DetectedModel]) -> Dict[str, List[DetectedModel]]:
        """íƒì§€ëœ ëª¨ë¸ë“¤ì„ Stepë³„ë¡œ ë§¤í•‘"""
        mappings = defaultdict(list)
        
        for model in detected_models:
            best_step = self._find_best_step_match(model)
            if best_step:
                mappings[best_step].append(model)
        
        # ê° Stepë³„ë¡œ ìš°ì„ ìˆœìœ„ ì •ë ¬
        for step_name in mappings:
            mappings[step_name].sort(
                key=lambda m: (m.confidence_score, m.file_size_mb), 
                reverse=True
            )
        
        self.mappings = mappings
        return dict(mappings)
    
    def _find_best_step_match(self, model: DetectedModel) -> Optional[str]:
        """ëª¨ë¸ì— ê°€ì¥ ì í•©í•œ Step ì°¾ê¸°"""
        best_step = None
        best_score = 0.0
        
        for step_name, requirement in self.requirements.items():
            score = self._calculate_match_score(model, requirement)
            if score > best_score:
                best_score = score
                best_step = step_name
        
        return best_step if best_score > 0.3 else None
    
    def _calculate_match_score(self, model: DetectedModel, requirement: StepModelRequirement) -> float:
        """ëª¨ë¸ê³¼ ìš”êµ¬ì‚¬í•­ ê°„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 0.4)
        if model.category == requirement.category:
            score += 0.4
        
        # íŒŒì¼ëª… íŒ¨í„´ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 0.3)
        filename = model.path.name.lower()
        for pattern in requirement.file_patterns:
            if re.match(pattern, filename):
                score += 0.3
                break
        
        # íŒŒì¼ í¬ê¸° ì ì •ì„± (ê°€ì¤‘ì¹˜: 0.2)
        min_size, max_size = requirement.size_range_mb
        if min_size <= model.file_size_mb <= max_size:
            score += 0.2
        
        # ìš°ì„ ìˆœìœ„ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 0.1)
        if model.priority == requirement.priority:
            score += 0.1
        
        return score

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ ìë™ íƒì§€ê¸° í´ë˜ìŠ¤ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ + ê°œì„ )
# ==============================================

class AutoModelDetector:
    """ìë™ ëª¨ë¸ íƒì§€ê¸° (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ + ì™„ì „ ê°œì„ )"""
    
    def __init__(self, base_path: Optional[Union[str, Path]] = None):
        self.base_path = Path(base_path) if base_path else Path("ai_models")
        self.detected_models: List[DetectedModel] = []
        self.cache_file = Path(".model_cache.json")
        self.last_scan_time = 0.0
        
        # ìƒˆë¡œìš´ ì»´í¬ë„ŒíŠ¸ë“¤
        self.file_scanner = DynamicFileSystemScanner(self.base_path)
        self.requirement_mapper = StepRequirementMapper()
        self.loading_monitor = ModelLoadingMonitor()
        self.step_mappings: Dict[str, List[DetectedModel]] = {}
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self._setup_monitoring()
        
        logger.info(f"ğŸ” AutoModelDetector ì´ˆê¸°í™”: {self.base_path}")
    
    def _setup_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì •"""
        def on_status_change(model_name: str, new_status: ModelLoadingStatus, 
                           old_status: ModelLoadingStatus, error_msg: Optional[str] = None):
            logger.info(f"ğŸ“Š ëª¨ë¸ ìƒíƒœ ë³€ê²½: {model_name} {old_status.value} â†’ {new_status.value}")
            if error_msg:
                logger.error(f"âŒ ëª¨ë¸ ì˜¤ë¥˜: {model_name} - {error_msg}")
        
        self.loading_monitor.add_observer(on_status_change)
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    def detect_models(self, force_refresh: bool = False) -> List[DetectedModel]:
        """ëª¨ë¸ íƒì§€ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        logger.info("ğŸ” ëª¨ë¸ íƒì§€ ì‹œì‘...")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº”
        file_groups = self.file_scanner.scan_all_model_files(force_refresh)
        
        detected_models = []
        
        for category, file_paths in file_groups.items():
            for file_path in file_paths:
                self.loading_monitor.update_status(
                    file_path.name, ModelLoadingStatus.DISCOVERED
                )
                
                model = self._create_detected_model(file_path, category)
                if model:
                    detected_models.append(model)
        
        self.detected_models = detected_models
        
        # Stepë³„ ë§¤í•‘ ìˆ˜í–‰
        self.step_mappings = self.requirement_mapper.map_models_to_steps(detected_models)
        
        logger.info(f"âœ… ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸, {len(self.step_mappings)}ê°œ Step ë§¤í•‘")
        
        return detected_models
    
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        return [model for model in self.detected_models if model.category == category]
    
    def get_model_by_name(self, name: str) -> Optional[DetectedModel]:
        """ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        for model in self.detected_models:
            if model.name == name:
                return model
        return None
    
    # ìƒˆë¡œìš´ ë©”ì„œë“œë“¤
    def get_step_model_mappings(self) -> Dict[str, List[DetectedModel]]:
        """Stepë³„ ëª¨ë¸ ë§¤í•‘ ì¡°íšŒ"""
        return self.step_mappings.copy()
    
    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """íŠ¹ì • Stepì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ ì¡°íšŒ"""
        if step_name in self.step_mappings and self.step_mappings[step_name]:
            return self.step_mappings[step_name][0]  # ì´ë¯¸ ìš°ì„ ìˆœìœ„ ì •ë ¬ë¨
        return None
    
    async def validate_model_async(self, model: DetectedModel) -> bool:
        """ëª¨ë¸ ë¹„ë™ê¸° ê²€ì¦"""
        self.loading_monitor.update_status(model.name, ModelLoadingStatus.VALIDATING)
        
        try:
            if not TORCH_AVAILABLE:
                self.loading_monitor.update_status(
                    model.name, ModelLoadingStatus.ERROR, "PyTorch not available"
                )
                return False
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not model.path.exists():
                self.loading_monitor.update_status(
                    model.name, ModelLoadingStatus.ERROR, "File not found"
                )
                return False
            
            # PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
            if model.file_extension in ['.pth', '.pt']:
                checkpoint = torch.load(str(model.path), map_location='cpu', weights_only=False)
                if isinstance(checkpoint, dict) and len(checkpoint) > 0:
                    model.pytorch_valid = True
                    model.parameter_count = self._count_parameters(checkpoint)
                    self.loading_monitor.update_status(model.name, ModelLoadingStatus.VALID)
                    return True
            
            elif model.file_extension == '.safetensors':
                # SafeTensors íŒŒì¼ í¬ê¸°ë§Œ í™•ì¸
                if model.file_size_mb > 10:  # 10MB ì´ìƒì´ë©´ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                    model.pytorch_valid = True
                    self.loading_monitor.update_status(model.name, ModelLoadingStatus.VALID)
                    return True
            
            self.loading_monitor.update_status(model.name, ModelLoadingStatus.INVALID)
            return False
            
        except Exception as e:
            self.loading_monitor.update_status(
                model.name, ModelLoadingStatus.ERROR, str(e)
            )
            return False
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¡°íšŒ"""
        return self.loading_monitor.get_loading_summary()
    
    def _create_detected_model(self, file_path: Path, category_str: str) -> Optional[DetectedModel]:
        """íƒì§€ëœ ëª¨ë¸ ê°ì²´ ìƒì„±"""
        try:
            # ì¹´í…Œê³ ë¦¬ ë³€í™˜
            category = ModelCategory(category_str) if category_str in [c.value for c in ModelCategory] else ModelCategory.AUXILIARY
            
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            stat = file_path.stat()
            file_size_mb = stat.st_size / (1024 * 1024)
            
            # ëª¨ë¸ íƒ€ì… ì¶”ë¡ 
            model_type = self._infer_model_type(file_path)
            
            # ìš°ì„ ìˆœìœ„ ê²°ì •
            priority = self._determine_priority(category, file_size_mb)
            
            # Step ì´ë¦„ ì¶”ë¡ 
            step_name = self._infer_step_name(file_path, category)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence(file_path, category, file_size_mb)
            
            model = DetectedModel(
                name=file_path.stem,
                path=file_path,
                category=category,
                model_type=model_type,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence_score,
                priority=priority,
                step_name=step_name,
                last_modified=stat.st_mtime,
                checkpoint_path=str(file_path),
                memory_requirement_mb=file_size_mb * 2.5,  # ëŒ€ëµì ì¸ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰
                loading_status=ModelLoadingStatus.DISCOVERED
            )
            
            return model
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {file_path} - {e}")
            return None
    
    def _infer_model_type(self, file_path: Path) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œë¶€í„° ëª¨ë¸ íƒ€ì… ì¶”ë¡ """
        filename = file_path.name.lower()
        
        if 'schp' in filename or 'atr' in filename:
            return "SCHP"
        elif 'openpose' in filename or 'pose' in filename:
            return "OpenPose"
        elif 'sam' in filename:
            return "SAM"
        elif 'diffusion' in filename or 'unet' in filename:
            return "Diffusion"
        elif 'clip' in filename:
            return "CLIP"
        else:
            return "Unknown"
    
    def _determine_priority(self, category: ModelCategory, file_size_mb: float) -> ModelPriority:
        """ì¹´í…Œê³ ë¦¬ì™€ íŒŒì¼ í¬ê¸°ë¡œ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        if category in [ModelCategory.HUMAN_PARSING, ModelCategory.VIRTUAL_FITTING]:
            return ModelPriority.CRITICAL
        elif category in [ModelCategory.POSE_ESTIMATION, ModelCategory.CLOTH_SEGMENTATION]:
            return ModelPriority.HIGH
        elif file_size_mb > 1000:  # 1GB ì´ìƒì€ ì¤‘ìš”í•œ ëª¨ë¸
            return ModelPriority.HIGH
        else:
            return ModelPriority.MEDIUM
    
    def _infer_step_name(self, file_path: Path, category: ModelCategory) -> str:
        """íŒŒì¼ ê²½ë¡œì™€ ì¹´í…Œê³ ë¦¬ë¡œë¶€í„° Step ì´ë¦„ ì¶”ë¡ """
        path_str = str(file_path).lower()
        
        # ê²½ë¡œì—ì„œ step ë””ë ‰í† ë¦¬ ì°¾ê¸°
        for i in range(1, 9):
            step_dir = f"step_{i:02d}"
            if step_dir in path_str:
                return step_dir
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë§¤í•‘
        category_to_step = {
            ModelCategory.HUMAN_PARSING: "step_01_human_parsing",
            ModelCategory.POSE_ESTIMATION: "step_02_pose_estimation", 
            ModelCategory.CLOTH_SEGMENTATION: "step_03_cloth_segmentation",
            ModelCategory.GEOMETRIC_MATCHING: "step_04_geometric_matching",
            ModelCategory.CLOTH_WARPING: "step_05_cloth_warping",
            ModelCategory.VIRTUAL_FITTING: "step_06_virtual_fitting",
            ModelCategory.POST_PROCESSING: "step_07_post_processing",
            ModelCategory.QUALITY_ASSESSMENT: "step_08_quality_assessment"
        }
        
        return category_to_step.get(category, "auxiliary")
    
    def _calculate_confidence(self, file_path: Path, category: ModelCategory, file_size_mb: float) -> float:
        """ëª¨ë¸ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        filename = file_path.name.lower()
        path_str = str(file_path).lower()
        
        # íŒŒì¼ëª… í‚¤ì›Œë“œ ë§¤ì¹­
        category_keywords = {
            ModelCategory.HUMAN_PARSING: ['human', 'parsing', 'schp', 'atr', 'lip'],
            ModelCategory.POSE_ESTIMATION: ['pose', 'openpose', 'body', 'coco'],
            ModelCategory.CLOTH_SEGMENTATION: ['sam', 'segment', 'mask', 'cloth'],
            ModelCategory.VIRTUAL_FITTING: ['diffusion', 'unet', 'vton', 'ootd'],
            ModelCategory.QUALITY_ASSESSMENT: ['clip', 'quality', 'assessment']
        }
        
        if category in category_keywords:
            for keyword in category_keywords[category]:
                if keyword in filename:
                    score += 0.1
        
        # ê²½ë¡œ ì ì ˆì„±
        if f"step_{category.value}" in path_str:
            score += 0.2
        
        # íŒŒì¼ í¬ê¸° ì ì ˆì„±
        if 50 <= file_size_mb <= 5000:  # ì ì ˆí•œ í¬ê¸° ë²”ìœ„
            score += 0.1
        
        # íŒŒì¼ í™•ì¥ì
        if file_path.suffix in ['.pth', '.pt', '.safetensors']:
            score += 0.1
        
        return min(score, 1.0)
    
    def _count_parameters(self, checkpoint: Dict) -> int:
        """ì²´í¬í¬ì¸íŠ¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        total_params = 0
        for key, value in checkpoint.items():
            if hasattr(value, 'numel'):
                total_params += value.numel()
        return total_params

# ==============================================
# ğŸ”¥ 6. í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
# ==============================================

def get_auto_model_detector(base_path: Optional[Union[str, Path]] = None) -> AutoModelDetector:
    """ì „ì—­ AutoModelDetector ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    global _global_detector
    if '_global_detector' not in globals() or _global_detector is None:
        _global_detector = AutoModelDetector(base_path)
    return _global_detector

def detect_all_models(force_refresh: bool = False) -> List[DetectedModel]:
    """ëª¨ë“  ëª¨ë¸ íƒì§€ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    detector = get_auto_model_detector()
    return detector.detect_models(force_refresh)

def get_models_for_step(step_name: str) -> List[DetectedModel]:
    """íŠ¹ì • Stepì˜ ëª¨ë¸ë“¤ ì¡°íšŒ (ìƒˆë¡œ ì¶”ê°€)"""
    detector = get_auto_model_detector()
    mappings = detector.get_step_model_mappings()
    return mappings.get(step_name, [])

def get_best_model_for_step(step_name: str) -> Optional[DetectedModel]:
    """íŠ¹ì • Stepì˜ ìµœì  ëª¨ë¸ ì¡°íšŒ (ìƒˆë¡œ ì¶”ê°€)"""
    detector = get_auto_model_detector()
    return detector.get_best_model_for_step(step_name)

async def validate_all_models() -> Dict[str, bool]:
    """ëª¨ë“  ëª¨ë¸ ë¹„ë™ê¸° ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)"""
    detector = get_auto_model_detector()
    results = {}
    
    tasks = []
    for model in detector.detected_models:
        task = detector.validate_model_async(model)
        tasks.append((model.name, task))
    
    for model_name, task in tasks:
        try:
            result = await task
            results[model_name] = result
        except Exception as e:
            logger.error(f"ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {model_name} - {e}")
            results[model_name] = False
    
    return results

def get_monitoring_dashboard() -> Dict[str, Any]:
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë°ì´í„° (ìƒˆë¡œ ì¶”ê°€)"""
    detector = get_auto_model_detector()
    return {
        "detector_status": detector.get_monitoring_status(),
        "step_mappings": {
            step: len(models) for step, models in detector.get_step_model_mappings().items()
        },
        "total_models": len(detector.detected_models),
        "file_system_path": str(detector.base_path),
        "last_scan": detector.last_scan_time,
        "timestamp": time.time()
    }

# ì „ì—­ ë³€ìˆ˜
_global_detector: Optional[AutoModelDetector] = None

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ”¥ MyCloset AI ë™ì  ëª¨ë¸ íƒì§€ê¸° v3.0 í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    detector = get_auto_model_detector("ai_models")
    models = detector.detect_models(force_refresh=True)
    
    print(f"\nâœ… íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    step_mappings = detector.get_step_model_mappings()
    for step, step_models in step_mappings.items():
        print(f"\nğŸ“ {step}: {len(step_models)}ê°œ ëª¨ë¸")
        for model in step_models[:2]:  # ìƒìœ„ 2ê°œë§Œ í‘œì‹œ
            print(f"  ğŸ“¦ {model.name} ({model.file_size_mb:.1f}MB, ì‹ ë¢°ë„: {model.confidence_score:.2f})")
    
    # ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¶œë ¥
    monitoring = detector.get_monitoring_status()
    print(f"\nğŸ“Š ëª¨ë‹ˆí„°ë§ ìƒíƒœ:")
    print(f"  ì´ ëª¨ë¸: {monitoring['total_models']}ê°œ")
    print(f"  ìƒíƒœë³„ ë¶„í¬: {monitoring['status_breakdown']}")
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")