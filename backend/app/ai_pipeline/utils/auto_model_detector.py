# app/ai_pipeline/utils/auto_model_detector.py (ì™„ì „íˆ ìƒˆë¡œìš´ ë²„ì „)
"""
ğŸ” ì™„ì „íˆ ìƒˆë¡œìš´ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v3.0
âœ… Stepë³„ ìš”ì²­ ì •ë³´ (3ë²ˆ íŒŒì¼)ì— ì •í™•íˆ ë§ì¶¤
âœ… ModelLoaderì™€ ì™„ë²½í•œ ë°ì´í„° êµí™˜
âœ… ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì™„ì „ ì¶”ì¶œ ë° ì „ë‹¬
âœ… M3 Max 128GB ìµœì í™”
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# Stepë³„ ìš”ì²­ ì •ë³´ ì„í¬íŠ¸
from .step_model_requests import (
    STEP_MODEL_REQUESTS,
    StepModelRequestAnalyzer,
    create_model_config_from_step_request,
    get_all_step_model_requirements
)

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì™„ì „íˆ ìƒˆë¡œìš´ ëª¨ë¸ íƒì§€ ë°ì´í„° êµ¬ì¡°
# ==============================================

@dataclass
class ModelCheckpoint:
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ - Step ìš”ì²­ ì‚¬í•­ì— ë§ì¶¤"""
    primary_path: Path                              # ì£¼ ëª¨ë¸ íŒŒì¼
    config_files: List[Path] = field(default_factory=list)        # config.json ë“±
    required_files: List[Path] = field(default_factory=list)      # í•„ìˆ˜ íŒŒì¼ë“¤
    optional_files: List[Path] = field(default_factory=list)      # ì„ íƒì  íŒŒì¼ë“¤
    tokenizer_files: List[Path] = field(default_factory=list)     # tokenizer ê´€ë ¨
    scheduler_files: List[Path] = field(default_factory=list)     # scheduler ê´€ë ¨
    
    # Stepë³„ íŠ¹ìˆ˜ ì²´í¬í¬ì¸íŠ¸
    unet_model: Optional[Path] = None               # VirtualFittingStepìš©
    vae_model: Optional[Path] = None                # VirtualFittingStepìš©
    text_encoder: Optional[Path] = None             # VirtualFittingStepìš©
    body_model: Optional[Path] = None               # PoseEstimationStepìš©
    hand_model: Optional[Path] = None               # PoseEstimationStepìš©
    face_model: Optional[Path] = None               # PoseEstimationStepìš©
    
    # ë©”íƒ€ë°ì´í„°
    total_size_mb: float = 0.0
    validation_passed: bool = False
    step_compatible: bool = False

@dataclass 
class StepModelInfo:
    """Stepë³„ ëª¨ë¸ ì •ë³´ - ModelLoader ì „ë‹¬ìš©"""
    # Step ê¸°ë³¸ ì •ë³´
    step_name: str                                  # Step í´ë˜ìŠ¤ëª…
    model_name: str                                 # ëª¨ë¸ ì´ë¦„
    model_class: str                                # AI ëª¨ë¸ í´ë˜ìŠ¤
    model_type: str                                 # ëª¨ë¸ íƒ€ì…
    
    # ë””ë°”ì´ìŠ¤ ë° ìµœì í™”
    device: str                                     # 'auto', 'mps', 'cuda', 'cpu'
    precision: str                                  # 'fp16', 'fp32'
    input_size: Tuple[int, int]                     # ì…ë ¥ í¬ê¸°
    num_classes: Optional[int]                      # í´ë˜ìŠ¤ ìˆ˜
    
    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ (ğŸ”¥ í•µì‹¬!)
    checkpoint: ModelCheckpoint                     # ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
    
    # ìµœì í™” íŒŒë¼ë¯¸í„° (Stepì—ì„œ ìš”ì²­í•˜ëŠ” ì •ë³´)
    optimization_params: Dict[str, Any]             # ìµœì í™” ì„¤ì •
    special_params: Dict[str, Any]                  # Stepë³„ íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„°
    
    # ëŒ€ì²´ ë° í´ë°±
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    confidence_score: float = 0.0
    priority_level: int = 5
    auto_detected: bool = True
    validation_info: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ” Stepë³„ íŠ¹í™” ëª¨ë¸ íŒ¨í„´ (ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜)
# ==============================================

REAL_STEP_MODEL_MAPPING = {
    "HumanParsingStep": {
        "model_patterns": [
            "**/*human*parsing*.pth", "**/*schp*.pth", "**/*graphonomy*.pth",
            "**/atr*.pth", "**/lip*.pth", "**/cihp*.pth"
        ],
        "model_class": "GraphonomyModel",
        "priority": 1,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt"],
            "min_size_mb": 50,
            "max_size_mb": 500,
            "required_files": ["*.pth"],
            "optional_files": ["config.json", "vocab.txt"]
        }
    },
    
    "PoseEstimationStep": {
        "model_patterns": [
            "**/*pose*.pth", "**/*openpose*.pth", "**/body_pose*.pth",
            "**/hand_pose*.pth", "**/sk_model*.pth"
        ],
        "model_class": "OpenPoseModel", 
        "priority": 2,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt"],
            "min_size_mb": 10,
            "max_size_mb": 200,
            "required_files": ["*pose*.pth"],
            "optional_files": ["*config*.json", "*hand*.pth", "*face*.pth"]
        }
    },
    
    "ClothSegmentationStep": {
        "model_patterns": [
            "**/*u2net*.pth", "**/*cloth*segmentation*.pth", 
            "**/*mobile*sam*.pt", "**/*sam*vit*.pth"
        ],
        "model_class": "U2NetModel",
        "priority": 2,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt", ".onnx"],
            "min_size_mb": 20,
            "max_size_mb": 1000,
            "required_files": ["*u2net*.pth"],
            "optional_files": ["*backup*.pth", "*config*.json"]
        }
    },
    
    "GeometricMatchingStep": {
        "model_patterns": [
            "**/*geometric*matching*.pth", "**/*gmm*.pth", "**/*tps*.pth"
        ],
        "model_class": "GeometricMatchingModel",
        "priority": 3,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt"],
            "min_size_mb": 5,
            "max_size_mb": 100,
            "required_files": ["*gmm*.pth"],
            "optional_files": ["*tps*.pth", "*config*.json"]
        }
    },
    
    "ClothWarpingStep": {
        "model_patterns": [
            "**/*tom*.pth", "**/*cloth*warping*.pth", "**/*hrviton*.pth"
        ],
        "model_class": "HRVITONModel",
        "priority": 2,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt"],
            "min_size_mb": 100,
            "max_size_mb": 1000,
            "required_files": ["*tom*.pth"],
            "optional_files": ["*warping*.pth", "*config*.json"]
        }
    },
    
    "VirtualFittingStep": {
        "model_patterns": [
            "**/*diffusion*pytorch*model*.bin", "**/*stable*diffusion*.safetensors",
            "**/*ootdiffusion*.pth", "**/*unet*.bin", "**/*vae*.bin"
        ],
        "model_class": "StableDiffusionPipeline",
        "priority": 1,
        "checkpoint_requirements": {
            "primary_extensions": [".bin", ".safetensors", ".pth"],
            "min_size_mb": 500,
            "max_size_mb": 5000,
            "required_files": ["*diffusion*model*.bin"],
            "optional_files": ["*unet*.bin", "*vae*.bin", "*text_encoder*.bin", "model_index.json", "config.json"]
        }
    },
    
    "PostProcessingStep": {
        "model_patterns": [
            "**/*realesrgan*.pth", "**/*esrgan*.pth", "**/*enhance*.pth"
        ],
        "model_class": "EnhancementModel",
        "priority": 4,
        "checkpoint_requirements": {
            "primary_extensions": [".pth", ".pt"],
            "min_size_mb": 10,
            "max_size_mb": 200,
            "required_files": ["*esrgan*.pth"],
            "optional_files": ["*config*.json"]
        }
    },
    
    "QualityAssessmentStep": {
        "model_patterns": [
            "**/*clip*vit*.bin", "**/*clip*base*.bin", "**/*quality*assessment*.pth"
        ],
        "model_class": "CLIPModel",
        "priority": 4,
        "checkpoint_requirements": {
            "primary_extensions": [".bin", ".pth", ".pt"],
            "min_size_mb": 100,
            "max_size_mb": 2000,
            "required_files": ["*clip*.bin"],
            "optional_files": ["config.json", "tokenizer.json", "*feature*extractor*.bin"]
        }
    }
}

# ==============================================
# ğŸ” ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class SmartModelDetector:
    """
    ğŸ” Step ìš”ì²­ ì •ë³´ì— ì •í™•íˆ ë§ì¶˜ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ íƒì§€ê¸°
    âœ… 3ë²ˆ íŒŒì¼ì˜ Step ìš”ì²­ ì‚¬í•­ ì™„ë²½ ì¤€ìˆ˜
    âœ… ModelLoaderì— ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì „ë‹¬
    âœ… conda í™˜ê²½ ë° M3 Max ìµœì í™”
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        self.logger = logging.getLogger(f"{__name__}.SmartModelDetector")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ìë™ ê°ì§€
        if project_root is None:
            current_file = Path(__file__).resolve()
            project_root = current_file.parents[3]  # backend ë””ë ‰í† ë¦¬
        
        self.project_root = Path(project_root)
        
        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ê²€ìƒ‰ ê²½ë¡œë“¤
        self.search_paths = self._get_real_search_paths()
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, StepModelInfo] = {}
        self.step_mappings: Dict[str, List[str]] = {}
        
        # ì„±ëŠ¥ í†µê³„
        self.scan_stats = {
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_time": 0.0,
            "step_coverage": {},
            "checkpoint_validation": {}
        }
        
        self.logger.info(f"ğŸ” ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” - {len(self.search_paths)}ê°œ ê²½ë¡œ")
    
    def _get_real_search_paths(self) -> List[Path]:
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ ê²€ìƒ‰ ê²½ë¡œë“¤ ë°˜í™˜"""
        potential_paths = [
            # í”„ë¡œì íŠ¸ ë‚´ë¶€
            self.project_root / "ai_models",
            self.project_root / "app" / "ai_models", 
            self.project_root / "checkpoints",
            self.project_root / "models",
            
            # Stepë³„ íŠ¹í™” ê²½ë¡œ
            self.project_root / "ai_models" / "human_parsing",
            self.project_root / "ai_models" / "pose_estimation",
            self.project_root / "ai_models" / "cloth_segmentation",
            self.project_root / "ai_models" / "geometric_matching",
            self.project_root / "ai_models" / "cloth_warping",
            self.project_root / "ai_models" / "virtual_fitting",
            self.project_root / "ai_models" / "post_processing",
            self.project_root / "ai_models" / "quality_assessment",
            
            # ì™¸ë¶€ ìºì‹œ (conda í™˜ê²½ ê³ ë ¤)
            Path.home() / ".cache" / "huggingface",
            Path.home() / ".cache" / "torch",
            Path("/opt/ml/models") if Path("/opt/ml/models").exists() else None,
            
            # conda í™˜ê²½ ê²½ë¡œ
            Path(os.environ.get("CONDA_PREFIX", "")) / "share" / "models" 
            if os.environ.get("CONDA_PREFIX") else None
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ ë°˜í™˜
        real_paths = []
        for path in potential_paths:
            if path and path.exists() and path.is_dir():
                real_paths.append(path)
                self.logger.debug(f"âœ… ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {path}")
        
        return real_paths
    
    def detect_all_models(
        self, 
        step_filter: Optional[List[str]] = None,
        force_rescan: bool = False
    ) -> Dict[str, StepModelInfo]:
        """
        ëª¨ë“  AI ëª¨ë¸ íƒì§€ ë° Stepë³„ ë§¤í•‘
        
        Args:
            step_filter: íŠ¹ì • Stepë§Œ íƒì§€ (ì˜ˆ: ['HumanParsingStep'])
            force_rescan: ê°•ì œ ì¬ìŠ¤ìº”
            
        Returns:
            Dict[str, StepModelInfo]: Stepë³„ ëª¨ë¸ ì •ë³´ë“¤
        """
        try:
            self.logger.info("ğŸ” Stepë³„ íŠ¹í™” AI ëª¨ë¸ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ì´ì „ ê²°ê³¼ ì´ˆê¸°í™”
            if force_rescan:
                self.detected_models.clear()
                self.step_mappings.clear()
            
            # Step í•„í„° ì ìš©
            target_steps = step_filter or list(REAL_STEP_MODEL_MAPPING.keys())
            
            # ê° Stepë³„ë¡œ ëª¨ë¸ íƒì§€
            for step_name in target_steps:
                step_models = self._detect_models_for_step(step_name)
                if step_models:
                    self.detected_models.update(step_models)
                    
                    # Step ë§¤í•‘ ì—…ë°ì´íŠ¸
                    if step_name not in self.step_mappings:
                        self.step_mappings[step_name] = []
                    self.step_mappings[step_name].extend(step_models.keys())
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_time"] = time.time() - start_time
            self.scan_stats["step_coverage"] = {
                step: len(models) for step, models in self.step_mappings.items()
            }
            
            self.logger.info(f"âœ… ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            raise
    
    def _detect_models_for_step(self, step_name: str) -> Dict[str, StepModelInfo]:
        """íŠ¹ì • Stepì— ëŒ€í•œ ëª¨ë¸ íƒì§€"""
        try:
            step_config = REAL_STEP_MODEL_MAPPING.get(step_name)
            if not step_config:
                self.logger.warning(f"âš ï¸ {step_name}ì— ëŒ€í•œ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤")
                return {}
            
            step_models = {}
            patterns = step_config["model_patterns"]
            
            # ê° ê²€ìƒ‰ ê²½ë¡œì—ì„œ íŒ¨í„´ ë§¤ì¹­
            for search_path in self.search_paths:
                for pattern in patterns:
                    # glob íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰
                    matched_files = list(search_path.glob(pattern))
                    
                    for file_path in matched_files:
                        if not file_path.is_file():
                            continue
                        
                        self.scan_stats["total_files_scanned"] += 1
                        
                        # ëª¨ë¸ ì •ë³´ ìƒì„±
                        model_info = self._create_step_model_info(
                            step_name, file_path, step_config
                        )
                        
                        if model_info:
                            model_key = f"{step_name}_{model_info.model_name}"
                            step_models[model_key] = model_info
                            self.logger.debug(f"âœ… {step_name} ëª¨ë¸ ë°œê²¬: {file_path.name}")
            
            return step_models
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_step_model_info(
        self, 
        step_name: str, 
        primary_file: Path,
        step_config: Dict[str, Any]
    ) -> Optional[StepModelInfo]:
        """Stepë³„ ëª¨ë¸ ì •ë³´ ìƒì„±"""
        try:
            # Step ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (3ë²ˆ íŒŒì¼ì—ì„œ)
            step_request_info = StepModelRequestAnalyzer.get_step_request_info(step_name)
            if not step_request_info:
                return None
            
            default_request = step_request_info["default_request"]
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì™„ì „ ì¶”ì¶œ
            checkpoint = self._extract_complete_checkpoint_info(
                primary_file, step_name, default_request.checkpoint_requirements
            )
            
            if not checkpoint.validation_passed:
                return None
            
            # ëª¨ë¸ ì´ë¦„ ìƒì„±
            model_name = self._generate_model_name(step_name, primary_file)
            
            # StepModelInfo ìƒì„± (ModelLoader ì „ë‹¬ìš©)
            model_info = StepModelInfo(
                # Step ê¸°ë³¸ ì •ë³´
                step_name=step_name,
                model_name=model_name,
                model_class=default_request.model_class,
                model_type=default_request.model_type,
                
                # ë””ë°”ì´ìŠ¤ ë° ìµœì í™” (Step ìš”ì²­ ê·¸ëŒ€ë¡œ)
                device=default_request.device,
                precision=default_request.precision,
                input_size=default_request.input_size,
                num_classes=default_request.num_classes,
                
                # ğŸ”¥ ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
                checkpoint=checkpoint,
                
                # Stepë³„ íŒŒë¼ë¯¸í„° (Step ìš”ì²­ ê·¸ëŒ€ë¡œ)
                optimization_params=default_request.optimization_params,
                special_params=default_request.special_params,
                
                # ëŒ€ì²´ ë° í´ë°±
                alternative_models=step_request_info.get("alternative_models", []),
                fallback_config=step_request_info.get("fallback_config", {}),
                
                # ë©”íƒ€ë°ì´í„°
                confidence_score=self._calculate_confidence(primary_file, step_config),
                priority_level=step_config["priority"],
                auto_detected=True,
                validation_info={
                    "step_compatible": True,
                    "checkpoint_complete": checkpoint.validation_passed,
                    "size_mb": checkpoint.total_size_mb
                }
            )
            
            return model_info
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_complete_checkpoint_info(
        self, 
        primary_file: Path, 
        step_name: str,
        requirements: Dict[str, Any]
    ) -> ModelCheckpoint:
        """ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            checkpoint = ModelCheckpoint(primary_path=primary_file)
            base_dir = primary_file.parent
            
            # íŒŒì¼ í¬ê¸° ê²€ì¦
            file_size_mb = primary_file.stat().st_size / (1024 * 1024)
            min_size = requirements.get("min_file_size_mb", 0)
            max_size = requirements.get("max_file_size_mb", float('inf'))
            
            if not (min_size <= file_size_mb <= max_size):
                self.logger.debug(f"âŒ íŒŒì¼ í¬ê¸° ê²€ì¦ ì‹¤íŒ¨: {file_size_mb}MB")
                return checkpoint
            
            checkpoint.total_size_mb = file_size_mb
            
            # í•„ìˆ˜ íŒŒì¼ë“¤ ì°¾ê¸°
            required_patterns = requirements.get("required_files", [])
            for pattern in required_patterns:
                matched_files = list(base_dir.glob(pattern))
                checkpoint.required_files.extend(matched_files)
            
            # ì„ íƒì  íŒŒì¼ë“¤ ì°¾ê¸°
            optional_patterns = requirements.get("optional_files", [])
            for pattern in optional_patterns:
                matched_files = list(base_dir.glob(pattern))
                checkpoint.optional_files.extend(matched_files)
            
            # config íŒŒì¼ë“¤ ì°¾ê¸°
            config_patterns = ["*config*.json", "config.json", "model_config.json"]
            for pattern in config_patterns:
                matched_files = list(base_dir.glob(pattern))
                checkpoint.config_files.extend(matched_files)
            
            # Stepë³„ íŠ¹ìˆ˜ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            if step_name == "VirtualFittingStep":
                checkpoint.unet_model = self._find_file(base_dir, "*unet*.bin")
                checkpoint.vae_model = self._find_file(base_dir, "*vae*.bin")
                checkpoint.text_encoder = self._find_file(base_dir, "*text_encoder*.bin")
                
                # tokenizer íŒŒì¼ë“¤
                tokenizer_patterns = ["*tokenizer*.json", "vocab.txt"]
                for pattern in tokenizer_patterns:
                    matched_files = list(base_dir.glob(pattern))
                    checkpoint.tokenizer_files.extend(matched_files)
                
                # scheduler íŒŒì¼ë“¤
                scheduler_patterns = ["*scheduler*.json"]
                for pattern in scheduler_patterns:
                    matched_files = list(base_dir.glob(pattern))
                    checkpoint.scheduler_files.extend(matched_files)
            
            elif step_name == "PoseEstimationStep":
                checkpoint.body_model = self._find_file(base_dir, "*body*pose*.pth")
                checkpoint.hand_model = self._find_file(base_dir, "*hand*pose*.pth")
                checkpoint.face_model = self._find_file(base_dir, "*face*pose*.pth")
            
            # ê²€ì¦ ì™„ë£Œ
            checkpoint.validation_passed = True
            checkpoint.step_compatible = True
            
            # ì´ í¬ê¸° ê³„ì‚°
            all_files = (
                [checkpoint.primary_path] + 
                checkpoint.config_files + 
                checkpoint.required_files + 
                checkpoint.optional_files +
                checkpoint.tokenizer_files +
                checkpoint.scheduler_files
            )
            
            if checkpoint.unet_model:
                all_files.append(checkpoint.unet_model)
            if checkpoint.vae_model:
                all_files.append(checkpoint.vae_model)
            if checkpoint.text_encoder:
                all_files.append(checkpoint.text_encoder)
            if checkpoint.body_model:
                all_files.append(checkpoint.body_model)
            if checkpoint.hand_model:
                all_files.append(checkpoint.hand_model)
            if checkpoint.face_model:
                all_files.append(checkpoint.face_model)
            
            total_size = 0.0
            for file_path in all_files:
                if file_path and file_path.exists():
                    total_size += file_path.stat().st_size / (1024 * 1024)
            
            checkpoint.total_size_mb = total_size
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ModelCheckpoint(primary_path=primary_file)
    
    def _find_file(self, base_dir: Path, pattern: str) -> Optional[Path]:
        """íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ ì°¾ê¸°"""
        try:
            matches = list(base_dir.glob(pattern))
            return matches[0] if matches else None
        except:
            return None
    
    def _generate_model_name(self, step_name: str, file_path: Path) -> str:
        """Stepë³„ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # Stepë³„ ê¸°ë³¸ ì´ë¦„ ë§¤í•‘
            step_base_names = {
                "HumanParsingStep": "human_parsing",
                "PoseEstimationStep": "pose_estimation", 
                "ClothSegmentationStep": "cloth_segmentation",
                "GeometricMatchingStep": "geometric_matching",
                "ClothWarpingStep": "cloth_warping",
                "VirtualFittingStep": "virtual_fitting",
                "PostProcessingStep": "post_processing",
                "QualityAssessmentStep": "quality_assessment"
            }
            
            base_name = step_base_names.get(step_name, "unknown_model")
            file_stem = file_path.stem.lower()
            
            # íŠ¹ë³„í•œ ì‹ë³„ì ì¶”ê°€
            if "graphonomy" in file_stem or "schp" in file_stem:
                return f"{base_name}_graphonomy"
            elif "openpose" in file_stem:
                return f"{base_name}_openpose"
            elif "u2net" in file_stem:
                return f"{base_name}_u2net"
            elif "gmm" in file_stem:
                return f"{base_name}_gmm"
            elif "tom" in file_stem:
                return f"{base_name}_tom"
            elif "stable_diffusion" in file_stem or "diffusion" in file_stem:
                return f"{base_name}_stable_diffusion"
            elif "esrgan" in file_stem:
                return f"{base_name}_realesrgan"
            elif "clip" in file_stem:
                return f"{base_name}_clip"
            else:
                # í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ì´ë¦„
                hash_suffix = hashlib.md5(str(file_path).encode()).hexdigest()[:4]
                return f"{base_name}_{hash_suffix}"
                
        except Exception as e:
            return f"detected_model_{int(time.time())}"
    
    def _calculate_confidence(self, file_path: Path, step_config: Dict[str, Any]) -> float:
        """ëª¨ë¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            confidence = 0.5  # ê¸°ë³¸ ì ìˆ˜
            
            file_name = file_path.name.lower()
            
            # íŒŒì¼ëª… ë§¤ì¹­
            for pattern in step_config["model_patterns"]:
                pattern_clean = pattern.replace("**/", "").replace("**/*", "")
                if any(part in file_name for part in pattern_clean.split("*") if part):
                    confidence += 0.2
            
            # íŒŒì¼ í¬ê¸° ì ì ˆì„±
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            requirements = step_config["checkpoint_requirements"]
            min_size = requirements.get("min_size_mb", 0)
            max_size = requirements.get("max_size_mb", float('inf'))
            
            if min_size <= file_size_mb <= max_size:
                confidence += 0.2
            
            # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤
            if step_config["priority"] <= 2:
                confidence += 0.1
            
            return min(confidence, 1.0)
            
        except Exception as e:
            return 0.5
    
    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            self.logger.info("=" * 60)
            self.logger.info("ğŸ¯ Stepë³„ ëª¨ë¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 60)
            
            total_size_gb = sum(
                model.checkpoint.total_size_mb 
                for model in self.detected_models.values()
            ) / 1024
            
            self.logger.info(f"ğŸ“Š ì´ íƒì§€ëœ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ ëª¨ë¸ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº”ëœ íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"â±ï¸ ìŠ¤ìº” ì‹œê°„: {self.scan_stats['scan_time']:.2f}ì´ˆ")
            
            # Stepë³„ ë¶„í¬
            if self.step_mappings:
                self.logger.info("\nğŸ“ Stepë³„ ëª¨ë¸ ë¶„í¬:")
                for step_name, model_keys in self.step_mappings.items():
                    step_models = [self.detected_models[key] for key in model_keys]
                    step_size_gb = sum(m.checkpoint.total_size_mb for m in step_models) / 1024
                    self.logger.info(f"  {step_name}: {len(model_keys)}ê°œ ({step_size_gb:.2f}GB)")
                    
                    # ìƒìœ„ ëª¨ë¸ í‘œì‹œ
                    for model_key in model_keys[:2]:  # ìƒìœ„ 2ê°œë§Œ
                        model = self.detected_models[model_key]
                        self.logger.info(f"    - {model.model_name} ({model.confidence_score:.2f})")
            
            self.logger.info("=" * 60)
            
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ModelLoader ì—°ë™ì„ ìœ„í•œ ê³µê°œ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_models_for_step(self, step_name: str) -> List[StepModelInfo]:
        """íŠ¹ì • Stepì˜ ëª¨ë¸ë“¤ ë°˜í™˜"""
        model_keys = self.step_mappings.get(step_name, [])
        return [self.detected_models[key] for key in model_keys if key in self.detected_models]
    
    def get_best_model_for_step(self, step_name: str) -> Optional[StepModelInfo]:
        """íŠ¹ì • Stepì˜ ìµœê³  ëª¨ë¸ ë°˜í™˜"""
        models = self.get_models_for_step(step_name)
        if not models:
            return None
        
        # ìš°ì„ ìˆœìœ„ì™€ ì‹ ë¢°ë„ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ
        return max(models, key=lambda m: (10 - m.priority_level, m.confidence_score))
    
    def export_model_loader_configs(self) -> Dict[str, Any]:
        """ModelLoaderê°€ ì‚¬ìš©í•  ì„¤ì •ë“¤ ë‚´ë³´ë‚´ê¸°"""
        try:
            configs = {
                "step_model_configs": {},  # Stepë³„ ëª¨ë¸ ì„¤ì •ë“¤
                "model_checkpoints": {},   # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ë“¤
                "optimization_settings": {},  # ìµœì í™” ì„¤ì •ë“¤
                "special_parameters": {},  # Stepë³„ íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„°ë“¤
                "detection_metadata": {
                    "total_models": len(self.detected_models),
                    "scan_time": self.scan_stats["scan_time"],
                    "step_coverage": self.scan_stats["step_coverage"]
                }
            }
            
            for model_key, model_info in self.detected_models.items():
                step_name = model_info.step_name
                
                # Stepë³„ ëª¨ë¸ ì„¤ì •
                if step_name not in configs["step_model_configs"]:
                    configs["step_model_configs"][step_name] = []
                
                model_config = {
                    "name": model_info.model_name,
                    "model_class": model_info.model_class,
                    "model_type": model_info.model_type,
                    "checkpoint_path": str(model_info.checkpoint.primary_path),
                    "device": model_info.device,
                    "precision": model_info.precision,
                    "input_size": model_info.input_size,
                    "num_classes": model_info.num_classes,
                    "priority": model_info.priority_level,
                    "confidence": model_info.confidence_score
                }
                configs["step_model_configs"][step_name].append(model_config)
                
                # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
                configs["model_checkpoints"][model_info.model_name] = {
                    "primary_path": str(model_info.checkpoint.primary_path),
                    "config_files": [str(f) for f in model_info.checkpoint.config_files],
                    "required_files": [str(f) for f in model_info.checkpoint.required_files],
                    "optional_files": [str(f) for f in model_info.checkpoint.optional_files],
                    "tokenizer_files": [str(f) for f in model_info.checkpoint.tokenizer_files],
                    "scheduler_files": [str(f) for f in model_info.checkpoint.scheduler_files],
                    "unet_model": str(model_info.checkpoint.unet_model) if model_info.checkpoint.unet_model else None,
                    "vae_model": str(model_info.checkpoint.vae_model) if model_info.checkpoint.vae_model else None,
                    "text_encoder": str(model_info.checkpoint.text_encoder) if model_info.checkpoint.text_encoder else None,
                    "body_model": str(model_info.checkpoint.body_model) if model_info.checkpoint.body_model else None,
                    "hand_model": str(model_info.checkpoint.hand_model) if model_info.checkpoint.hand_model else None,
                    "face_model": str(model_info.checkpoint.face_model) if model_info.checkpoint.face_model else None,
                    "total_size_mb": model_info.checkpoint.total_size_mb,
                    "validation_passed": model_info.checkpoint.validation_passed
                }
                
                # ìµœì í™” ì„¤ì •
                configs["optimization_settings"][model_info.model_name] = model_info.optimization_params
                
                # íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„°
                configs["special_parameters"][model_info.model_name] = model_info.special_params
            
            return configs
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ ModelLoader í†µí•©ì„ ìœ„í•œ ì–´ëŒ‘í„° í´ë˜ìŠ¤
# ==============================================

class ModelLoaderIntegration:
    """ModelLoaderì™€ auto_model_detector í†µí•©"""
    
    def __init__(self, detector: SmartModelDetector):
        self.detector = detector
        self.model_loader_instance = None
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderIntegration")
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
        self.model_loader_instance = model_loader
        self.logger.info("ğŸ”— ModelLoader ì¸ìŠ¤í„´ìŠ¤ ì—°ë™ ì™„ë£Œ")
    
    def register_all_models(self) -> Dict[str, Any]:
        """íƒì§€ëœ ëª¨ë“  ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
        try:
            if not self.model_loader_instance:
                raise ValueError("ModelLoader ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            registered_count = 0
            failed_count = 0
            registration_details = {}
            
            for model_key, model_info in self.detector.detected_models.items():
                try:
                    # Step ìš”ì²­ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ModelConfig ìƒì„±
                    model_config = create_model_config_from_step_request(
                        model_info.step_name, 
                        str(model_info.checkpoint.primary_path)
                    )
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶”ê°€
                    model_config["checkpoints"] = {
                        "primary": str(model_info.checkpoint.primary_path),
                        "config": [str(f) for f in model_info.checkpoint.config_files],
                        "required": [str(f) for f in model_info.checkpoint.required_files],
                        "optional": [str(f) for f in model_info.checkpoint.optional_files],
                        "unet": str(model_info.checkpoint.unet_model) if model_info.checkpoint.unet_model else None,
                        "vae": str(model_info.checkpoint.vae_model) if model_info.checkpoint.vae_model else None,
                        "text_encoder": str(model_info.checkpoint.text_encoder) if model_info.checkpoint.text_encoder else None,
                        "total_size_mb": model_info.checkpoint.total_size_mb
                    }
                    
                    # ModelLoaderì— ë“±ë¡ (ì‹¤ì œ êµ¬í˜„ì— ë”°ë¼ ì¡°ì •)
                    success = self._register_model_to_loader(model_info.model_name, model_config)
                    
                    if success:
                        registered_count += 1
                        registration_details[model_info.model_name] = {
                            "status": "success",
                            "step": model_info.step_name,
                            "config": model_config
                        }
                    else:
                        failed_count += 1
                        registration_details[model_info.model_name] = {
                            "status": "failed", 
                            "reason": "Registration failed"
                        }
                        
                except Exception as e:
                    failed_count += 1
                    registration_details[model_info.model_name] = {
                        "status": "error",
                        "reason": str(e)
                    }
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {model_info.model_name}: {e}")
            
            result = {
                "total_models": len(self.detector.detected_models),
                "registered": registered_count,
                "failed": failed_count,
                "registration_details": registration_details
            }
            
            self.logger.info(f"ğŸ”— ModelLoader ë“±ë¡ ì™„ë£Œ: {registered_count}/{len(self.detector.detected_models)}ê°œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë“±ë¡ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _register_model_to_loader(self, model_name: str, model_config: Dict[str, Any]) -> bool:
        """ì‹¤ì œ ModelLoaderì— ëª¨ë¸ ë“±ë¡"""
        try:
            # ModelLoaderì˜ ì‹¤ì œ register_model ë©”ì„œë“œ í˜¸ì¶œ
            # ì‹¤ì œ êµ¬í˜„ì— ë”°ë¼ ì´ ë¶€ë¶„ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤
            if hasattr(self.model_loader_instance, 'register_model'):
                return self.model_loader_instance.register_model(model_name, model_config)
            else:
                # ì„ì‹œë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (ì‹¤ì œ êµ¬í˜„ ì‹œ ìˆ˜ì •)
                self.logger.debug(f"ğŸ“ ëª¨ë¸ ë“±ë¡ ì‹œë®¬ë ˆì´ì…˜: {model_name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    def get_best_model_for_step(self, step_name: str) -> Optional[Dict[str, Any]]:
        """Stepì˜ ìµœê³  ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë°˜í™˜"""
        best_model = self.detector.get_best_model_for_step(step_name)
        if not best_model:
            return None
        
        return {
            "name": best_model.model_name,
            "model_class": best_model.model_class,
            "model_type": best_model.model_type,
            "checkpoints": {
                "primary": str(best_model.checkpoint.primary_path),
                "config": [str(f) for f in best_model.checkpoint.config_files],
                "unet": str(best_model.checkpoint.unet_model) if best_model.checkpoint.unet_model else None,
                "vae": str(best_model.checkpoint.vae_model) if best_model.checkpoint.vae_model else None,
                "text_encoder": str(best_model.checkpoint.text_encoder) if best_model.checkpoint.text_encoder else None
            },
            "optimization_params": best_model.optimization_params,
            "special_params": best_model.special_params,
            "device": best_model.device,
            "precision": best_model.precision,
            "input_size": best_model.input_size,
            "num_classes": best_model.num_classes,
            "confidence": best_model.confidence_score
        }

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_smart_detector(project_root: Optional[Path] = None) -> SmartModelDetector:
    """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return SmartModelDetector(project_root)

def quick_detect_and_register(
    model_loader_instance=None,
    step_filter: Optional[List[str]] = None,
    project_root: Optional[Path] = None
) -> Dict[str, Any]:
    """ë¹ ë¥¸ íƒì§€ ë° ë“±ë¡"""
    try:
        logger.info("ğŸš€ ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ë° ë“±ë¡ ì‹œì‘...")
        
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_smart_detector(project_root)
        detected_models = detector.detect_all_models(step_filter=step_filter)
        
        if not detected_models:
            return {"success": False, "message": "íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        # ModelLoader í†µí•©
        integration = ModelLoaderIntegration(detector)
        if model_loader_instance:
            integration.set_model_loader(model_loader_instance)
            registration_result = integration.register_all_models()
        else:
            registration_result = {"message": "ModelLoader ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"}
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "success": True,
            "detection_summary": {
                "total_models": len(detected_models),
                "step_coverage": detector.scan_stats["step_coverage"],
                "scan_time": detector.scan_stats["scan_time"]
            },
            "model_loader_configs": detector.export_model_loader_configs(),
            "registration_summary": registration_result
        }
        
        logger.info(f"âœ… ë¹ ë¥¸ íƒì§€ ë° ë“±ë¡ ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë¹ ë¥¸ íƒì§€ ë° ë“±ë¡ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_model_checkpoints(model_path: str) -> Dict[str, Any]:
    """íŠ¹ì • ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¡°íšŒ"""
    try:
        file_path = Path(model_path)
        if not file_path.exists():
            return {"error": "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
        
        # ê¸°ë³¸ ì •ë³´
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        
        # ëª¨ë¸ í¬ë§· ì¶”ì •
        model_format = "unknown"
        if file_path.suffix == ".pth":
            model_format = "pytorch"
        elif file_path.suffix == ".bin":
            model_format = "huggingface"
        elif file_path.suffix == ".safetensors":
            model_format = "safetensors"
        elif file_path.suffix == ".onnx":
            model_format = "onnx"
        
        # ê´€ë ¨ íŒŒì¼ë“¤ ì°¾ê¸°
        base_dir = file_path.parent
        related_files = []
        
        related_patterns = [
            "*config*.json", "*tokenizer*.json", "vocab.txt",
            "*scheduler*.json", "*unet*.bin", "*vae*.bin"
        ]
        
        for pattern in related_patterns:
            matches = list(base_dir.glob(pattern))
            related_files.extend([str(f) for f in matches])
        
        return {
            "model_path": str(file_path),
            "model_format": model_format,
            "file_size_mb": file_size_mb,
            "related_files": related_files,
            "base_directory": str(base_dir),
            "validation": {
                "exists": True,
                "readable": os.access(file_path, os.R_OK),
                "size_valid": file_size_mb > 0.1
            }
        }
        
    except Exception as e:
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤ ì¶”ê°€
# ==============================================

def validate_model_paths(detected_models: Dict[str, StepModelInfo]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ ê²½ë¡œë“¤ ê²€ì¦"""
    try:
        validation_results = {
            "valid_models": [],
            "invalid_models": [],
            "missing_files": [],
            "corrupted_files": [],
            "total_size_gb": 0.0
        }
        
        for model_name, model_info in detected_models.items():
            try:
                # ì£¼ ëª¨ë¸ íŒŒì¼ ê²€ì¦
                primary_path = Path(model_info.checkpoint.primary_path)
                if not primary_path.exists():
                    validation_results["invalid_models"].append({
                        "name": model_name,
                        "reason": "Primary file missing",
                        "path": str(primary_path)
                    })
                    continue
                
                # íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
                file_size = primary_path.stat().st_size
                if file_size < 1024:  # 1KB ë¯¸ë§Œ
                    validation_results["corrupted_files"].append({
                        "name": model_name,
                        "path": str(primary_path),
                        "size": file_size
                    })
                    continue
                
                # ê´€ë ¨ íŒŒì¼ë“¤ ê²€ì¦
                missing_files = []
                for config_file in model_info.checkpoint.config_files:
                    if config_file and not Path(config_file).exists():
                        missing_files.append(config_file)
                
                if missing_files:
                    validation_results["missing_files"].append({
                        "model": model_name,
                        "missing": missing_files
                    })
                
                # ìœ íš¨í•œ ëª¨ë¸ë¡œ ë“±ë¡
                validation_results["valid_models"].append(model_name)
                validation_results["total_size_gb"] += model_info.checkpoint.total_size_mb / 1024
                
            except Exception as e:
                validation_results["invalid_models"].append({
                    "name": model_name,
                    "reason": str(e)
                })
        
        logger.info(f"ğŸ“Š ëª¨ë¸ ê²½ë¡œ ê²€ì¦: {len(validation_results['valid_models'])}/{len(detected_models)}ê°œ ìœ íš¨")
        return validation_results
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def benchmark_model_loading(detected_models: Dict[str, StepModelInfo], test_count: int = 3) -> Dict[str, Any]:
    """ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    try:
        benchmark_results = {
            "loading_times": {},
            "memory_usage": {},
            "success_rate": {},
            "average_times": {},
            "recommendations": [],
            "tested_models": [],
            "errors": []
        }
        
        for model_name, model_info in detected_models.items():
            if len(benchmark_results["tested_models"]) >= 5:  # ìµœëŒ€ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                break
            
            try:
                loading_times = []
                
                for i in range(test_count):
                    start_time = time.time()
                    
                    # ì‹¤ì œ íŒŒì¼ ì½ê¸° ì‹œë®¬ë ˆì´ì…˜
                    primary_path = Path(model_info.checkpoint.primary_path)
                    if primary_path.exists():
                        with open(primary_path, 'rb') as f:
                            # íŒŒì¼ í—¤ë”ë§Œ ì½ê¸° (ì‹¤ì œ ë¡œë”©ì€ í•˜ì§€ ì•ŠìŒ)
                            f.read(1024)
                    
                    loading_time = time.time() - start_time
                    loading_times.append(loading_time)
                
                avg_time = sum(loading_times) / len(loading_times)
                memory_usage_mb = model_info.checkpoint.total_size_mb * 1.2  # ì¶”ì •ê°’
                
                benchmark_results["tested_models"].append(model_name)
                benchmark_results["loading_times"][model_name] = loading_times
                benchmark_results["average_times"][model_name] = avg_time
                benchmark_results["memory_usage"][model_name] = memory_usage_mb
                benchmark_results["success_rate"][model_name] = 1.0
                
            except Exception as e:
                benchmark_results["errors"].append({
                    "model": model_name,
                    "error": str(e)
                })
                benchmark_results["success_rate"][model_name] = 0.0
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if benchmark_results["average_times"]:
            avg_loading_time = sum(benchmark_results["average_times"].values()) / len(benchmark_results["average_times"])
            total_memory = sum(benchmark_results["memory_usage"].values())
            
            if avg_loading_time > 5.0:
                benchmark_results["recommendations"].append("Consider using model caching for faster loading")
            
            if total_memory > 16000:  # 16GB
                benchmark_results["recommendations"].append("Consider selective model loading to manage memory usage")
            
            fast_models = [name for name, time in benchmark_results["average_times"].items() if time < 1.0]
            if fast_models:
                benchmark_results["recommendations"].append(f"Fast loading models for quick startup: {fast_models[:3]}")
        
        logger.info(f"ğŸš€ ëª¨ë¸ ë¡œë”© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {len(benchmark_results['tested_models'])}ê°œ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        return benchmark_results
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def export_model_registry_code(detected_models: Dict[str, StepModelInfo], output_path: Optional[Path] = None) -> str:
    """íƒì§€ëœ ëª¨ë¸ë“¤ì„ Python ì½”ë“œë¡œ ë‚´ë³´ë‚´ê¸°"""
    try:
        if output_path is None:
            output_path = Path("generated_model_registry.py")
        
        code_lines = [
            "# ìë™ ìƒì„±ëœ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬",
            f"# ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"# íƒì§€ëœ ëª¨ë¸ ìˆ˜: {len(detected_models)}",
            "",
            "from pathlib import Path",
            "from typing import Dict, Any, Tuple",
            "",
            "# íƒì§€ëœ ëª¨ë¸ ì •ë³´",
            "DETECTED_MODELS = {"
        ]
        
        for model_name, model_info in detected_models.items():
            code_lines.extend([
                f"    '{model_name}': {{",
                f"        'step_name': '{model_info.step_name}',",
                f"        'model_class': '{model_info.model_class}',",
                f"        'model_type': '{model_info.model_type}',",
                f"        'checkpoint_path': r'{model_info.checkpoint.primary_path}',",
                f"        'device': '{model_info.device}',",
                f"        'precision': '{model_info.precision}',",
                f"        'input_size': {model_info.input_size},",
                f"        'num_classes': {model_info.num_classes},",
                f"        'total_size_mb': {model_info.checkpoint.total_size_mb:.2f},",
                f"        'confidence': {model_info.confidence_score:.3f},",
                f"        'priority': {model_info.priority_level},",
                f"        'config_files': {[str(f) for f in model_info.checkpoint.config_files]},",
                f"        'optimization_params': {repr(model_info.optimization_params)},",
                f"        'special_params': {repr(model_info.special_params)}",
                "    },"
            ])
        
        code_lines.extend([
            "}",
            "",
            "# Stepë³„ ëª¨ë¸ ë§¤í•‘",
            "STEP_MODEL_MAPPING = {"
        ])
        
        # Stepë³„ ë§¤í•‘ ìƒì„±
        step_models = {}
        for model_name, model_info in detected_models.items():
            step_name = model_info.step_name
            if step_name not in step_models:
                step_models[step_name] = []
            step_models[step_name].append(model_name)
        
        for step_name, models in step_models.items():
            code_lines.append(f"    '{step_name}': {models},")
        
        code_lines.extend([
            "}",
            "",
            "def get_model_info(model_name: str) -> Dict[str, Any]:",
            "    '''íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ'''",
            "    return DETECTED_MODELS.get(model_name, {})",
            "",
            "def get_models_for_step(step_name: str) -> List[str]:",
            "    '''íŠ¹ì • Stepì˜ ëª¨ë¸ë“¤ ì¡°íšŒ'''",
            "    return STEP_MODEL_MAPPING.get(step_name, [])",
            "",
            "def get_best_model_for_step(step_name: str) -> str:",
            "    '''íŠ¹ì • Stepì˜ ìµœê³  ëª¨ë¸ ì¡°íšŒ'''",
            "    models = get_models_for_step(step_name)",
            "    if not models:",
            "        return None",
            "    # ìš°ì„ ìˆœìœ„ì™€ ì‹ ë¢°ë„ë¡œ ì •ë ¬",
            "    sorted_models = sorted(",
            "        models,",
            "        key=lambda m: (DETECTED_MODELS[m]['priority'], -DETECTED_MODELS[m]['confidence'])",
            "    )",
            "    return sorted_models[0]",
            "",
            f"# ì´ ëª¨ë¸ ìˆ˜: {len(detected_models)}",
            f"# ì´ ìš©ëŸ‰: {sum(m.checkpoint.total_size_mb for m in detected_models.values()) / 1024:.2f}GB"
        ])
        
        code_content = "\n".join(code_lines)
        
        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(code_content)
        
        logger.info(f"ğŸ“„ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì½”ë“œ ìƒì„±: {output_path}")
        return code_content
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

def calculate_model_checksum(file_path: Path, algorithm: str = "md5") -> Optional[str]:
    """ëª¨ë¸ íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
    try:
        import hashlib
        
        if algorithm == "md5":
            hasher = hashlib.md5()
        elif algorithm == "sha256":
            hasher = hashlib.sha256()
        else:
            hasher = hashlib.md5()
        
        with open(file_path, 'rb') as f:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ìœ„í•œ ì²­í¬ ë‹¨ìœ„ ì½ê¸°
            chunk_size = 8192
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        
        return hasher.hexdigest()
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
        return None

def get_best_model_for_category(detected_models: Dict[str, StepModelInfo], category: str) -> Optional[StepModelInfo]:
    """ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸ ì„ íƒ"""
    try:
        category_models = [
            model for model in detected_models.values()
            if category.lower() in model.step_name.lower()
        ]
        
        if not category_models:
            return None
        
        # ìš°ì„ ìˆœìœ„ì™€ ì‹ ë¢°ë„ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model = min(
            category_models,
            key=lambda m: (m.priority_level, -m.confidence_score)
        )
        
        return best_model
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸ ì„ íƒ ì‹¤íŒ¨: {e}")
        return None

# ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def clear_model_detection_cache(cache_path: Optional[Path] = None):
    """ëª¨ë¸ íƒì§€ ìºì‹œ ì •ë¦¬"""
    try:
        if cache_path is None:
            cache_path = Path("model_detection_cache.db")
        
        if cache_path.exists():
            cache_path.unlink()
            logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ íƒì§€ ìºì‹œ ì •ë¦¬: {cache_path}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def get_cache_stats(cache_path: Optional[Path] = None) -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ ì¡°íšŒ"""
    try:
        if cache_path is None:
            cache_path = Path("model_detection_cache.db")
        
        if not cache_path.exists():
            return {"cache_exists": False}
        
        import sqlite3
        
        with sqlite3.connect(cache_path) as conn:
            cursor = conn.cursor()
            
            # ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM model_cache_v2")
            entry_count = cursor.fetchone()[0]
            
            # ìºì‹œ íŒŒì¼ í¬ê¸°
            cache_size_mb = cache_path.stat().st_size / (1024 * 1024)
            
            # ì˜¤ë˜ëœ ì—”íŠ¸ë¦¬ ìˆ˜
            cutoff_time = time.time() - 86400  # 24ì‹œê°„
            cursor.execute("SELECT COUNT(*) FROM model_cache_v2 WHERE created_at < ?", (cutoff_time,))
            old_entries = cursor.fetchone()[0]
            
            return {
                "cache_exists": True,
                "entry_count": entry_count,
                "cache_size_mb": cache_size_mb,
                "old_entries": old_entries,
                "cache_path": str(cache_path)
            }
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'SmartModelDetector',
    'ModelLoaderIntegration', 
    'StepModelInfo',
    'ModelCheckpoint',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_smart_detector',
    'quick_detect_and_register',
    'get_model_checkpoints',
    
    # ìƒˆë¡œ ì¶”ê°€ëœ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_model_paths',
    'benchmark_model_loading',
    'export_model_registry_code',
    'calculate_model_checksum',
    'get_best_model_for_category',
    'clear_model_detection_cache',
    'get_cache_stats'
]