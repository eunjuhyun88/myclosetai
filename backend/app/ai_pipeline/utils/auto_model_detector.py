# app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ” MyCloset AI - ì™„ì „ í†µí•© ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v5.0
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (model_loader ì§ì ‘ import ì œê±°)
âœ… step_model_requests.py ê¸°ë°˜ ì •í™•í•œ ëª¨ë¸ íƒì§€
âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ íŒŒì¼ë“¤ ìë™ ë°œê²¬
âœ… ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì„¤ì • ì¶œë ¥ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ íŠ¹í™” ìŠ¤ìº”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥

ğŸ”¥ í•µì‹¬ ë³€ê²½ì‚¬í•­:
- ModelLoader ì§ì ‘ import ì œê±°
- ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì„¤ì • ì¶œë ¥
- ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ì—°ë™
- ëŸ°íƒ€ì„ ì—ëŸ¬ ë°©ì§€
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import asyncio
import sqlite3
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import weakref

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì•ˆì „í•œ import)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ëª¨ë¸ íƒì§€ ì„¤ì • ë° ë§¤í•‘
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ (step_model_requests.pyì™€ ì—°ë™)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•„ìˆ˜ ëª¨ë¸
    HIGH = 2          # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = 3        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    LOW = 4           # ë‚®ì€ ìš°ì„ ìˆœìœ„
    EXPERIMENTAL = 5  # ì‹¤í—˜ì  ëª¨ë¸

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì—°ë™ìš©)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str  # ì—°ê²°ëœ Step í´ë˜ìŠ¤ëª…
    metadata: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    requirements: List[str] = field(default_factory=list)
    performance_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)
    last_modified: float = 0.0
    checksum: Optional[str] = None

# ==============================================
# ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ Import ë° ì²˜ë¦¬
# ==============================================

try:
    from .step_model_requests import (
        STEP_MODEL_REQUESTS,
        StepModelRequestAnalyzer,
        get_all_step_requirements
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("âœ… step_model_requests ëª¨ë“ˆ ì—°ë™ ì„±ê³µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"âš ï¸ step_model_requests ëª¨ë“ˆ ì—°ë™ ì‹¤íŒ¨: {e}")
    
    # ë‚´ì¥ ê¸°ë³¸ ìš”ì²­ì‚¬í•­
    STEP_MODEL_REQUESTS = {
        "HumanParsingStep": {
            "model_name": "human_parsing_graphonomy",
            "model_type": "GraphonomyModel",
            "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"],
            "step_priority": 1
        },
        "PoseEstimationStep": {
            "model_name": "pose_estimation_openpose",
            "model_type": "OpenPoseModel", 
            "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"],
            "step_priority": 2
        },
        "ClothSegmentationStep": {
            "model_name": "cloth_segmentation_u2net",
            "model_type": "U2NetModel",
            "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"],
            "step_priority": 2
        },
        "VirtualFittingStep": {
            "model_name": "virtual_fitting_stable_diffusion",
            "model_type": "StableDiffusionPipeline",
            "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"],
            "step_priority": 1
        }
    }

# ==============================================
# ğŸ”¥ í™•ì¥ëœ ëª¨ë¸ ì‹ë³„ íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤
# ==============================================

ADVANCED_MODEL_PATTERNS = {
    # Step 01: Human Parsing Models
    "human_parsing": {
        "patterns": [
            r".*human.*parsing.*\.pth$",
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*\.pth$",
            r".*atr.*model.*\.pth$",
            r".*lip.*parsing.*\.pth$",
            r".*segformer.*human.*\.pth$",
            r".*densepose.*\.pkl$",
            r".*cihp.*\.pth$",
            r".*pascal.*person.*\.pth$",
            r".*human.*segmentation.*\.pth$"
        ],
        "keywords": [
            "human", "parsing", "segmentation", "atr", "lip", "schp", 
            "graphonomy", "densepose", "cihp", "pascal", "person"
        ],
        "category": ModelCategory.HUMAN_PARSING,
        "priority": ModelPriority.CRITICAL,
        "step_name": "HumanParsingStep",
        "min_size_mb": 50,
        "max_size_mb": 500,
        "expected_formats": [".pth", ".pt", ".pkl"],
        "model_class": "GraphonomyModel"
    },
    
    # Step 02: Pose Estimation Models
    "pose_estimation": {
        "patterns": [
            r".*pose.*model.*\.pth$",
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*hand.*pose.*\.pth$",
            r".*face.*pose.*\.pth$",
            r".*yolo.*pose.*\.pt$",
            r".*mediapipe.*\.tflite$",
            r".*alphapose.*\.pth$",
            r".*hrnet.*pose.*\.pth$",
            r".*simplebaseline.*\.pth$",
            r".*res101.*\.pth$",
            r".*clip_g.*\.pth$"
        ],
        "keywords": [
            "pose", "openpose", "yolo", "mediapipe", "body", "hand", "face",
            "keypoint", "alphapose", "hrnet", "simplebaseline", "coco"
        ],
        "category": ModelCategory.POSE_ESTIMATION,
        "priority": ModelPriority.HIGH,
        "step_name": "PoseEstimationStep",
        "min_size_mb": 5,
        "max_size_mb": 1000,
        "expected_formats": [".pth", ".pt", ".tflite", ".onnx"],
        "model_class": "OpenPoseModel"
    },
    
    # Step 03: Cloth Segmentation Models
    "cloth_segmentation": {
        "patterns": [
            r".*u2net.*\.pth$",
            r".*cloth.*segmentation.*\.(pth|onnx)$",
            r".*sam.*\.pth$",
            r".*mobile.*sam.*\.pth$",
            r".*parsing.*lip.*\.onnx$",
            r".*segmentation.*\.pth$",
            r".*deeplab.*cloth.*\.pth$",
            r".*mask.*rcnn.*cloth.*\.pth$",
            r".*bisenet.*\.pth$",
            r".*pspnet.*cloth.*\.pth$"
        ],
        "keywords": [
            "u2net", "segmentation", "sam", "cloth", "mask", "mobile",
            "deeplab", "bisenet", "pspnet", "rcnn", "parsing"
        ],
        "category": ModelCategory.CLOTH_SEGMENTATION,
        "priority": ModelPriority.HIGH,
        "step_name": "ClothSegmentationStep",
        "min_size_mb": 10,
        "max_size_mb": 3000,
        "expected_formats": [".pth", ".pt", ".onnx"],
        "model_class": "U2NetModel"
    },
    
    # Step 04: Geometric Matching Models
    "geometric_matching": {
        "patterns": [
            r".*geometric.*matching.*\.pth$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$",
            r".*transformation.*\.pth$",
            r".*lightweight.*gmm.*\.pth$",
            r".*cpvton.*gmm.*\.pth$",
            r".*viton.*geometric.*\.pth$",
            r".*warp.*\.pth$"
        ],
        "keywords": [
            "geometric", "matching", "gmm", "tps", "transformation", 
            "alignment", "cpvton", "viton", "warp"
        ],
        "category": ModelCategory.GEOMETRIC_MATCHING,
        "priority": ModelPriority.MEDIUM,
        "step_name": "GeometricMatchingStep",
        "min_size_mb": 1,
        "max_size_mb": 100,
        "expected_formats": [".pth", ".pt"],
        "model_class": "GeometricMatchingModel"
    },
    
    # Step 05 & 06: Virtual Fitting & Diffusion Models
    "diffusion_models": {
        "patterns": [
            r".*diffusion.*pytorch.*model\.(bin|safetensors)$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.(pth|bin)$",
            r".*unet.*diffusion.*\.bin$",
            r".*hrviton.*\.pth$",
            r".*viton.*hd.*\.pth$",
            r".*inpaint.*\.bin$",
            r".*controlnet.*\.safetensors$",
            r".*lora.*\.safetensors$",
            r".*dreambooth.*\.bin$",
            r".*v1-5-pruned.*\.safetensors$",
            r".*runway.*diffusion.*\.bin$"
        ],
        "keywords": [
            "diffusion", "stable", "oot", "viton", "unet", "inpaint", 
            "generation", "controlnet", "lora", "dreambooth", "runway"
        ],
        "category": ModelCategory.DIFFUSION_MODELS,
        "priority": ModelPriority.CRITICAL,
        "step_name": "VirtualFittingStep",
        "min_size_mb": 100,
        "max_size_mb": 10000,
        "expected_formats": [".bin", ".safetensors", ".pth"],
        "model_class": "StableDiffusionPipeline"
    },
    
    # Transformer Models
    "transformer_models": {
        "patterns": [
            r".*clip.*vit.*\.bin$",
            r".*clip.*base.*\.bin$",
            r".*clip.*large.*\.bin$",
            r".*bert.*\.bin$",
            r".*roberta.*\.bin$",
            r".*t5.*\.bin$",
            r".*gpt.*\.bin$",
            r".*transformer.*\.bin$"
        ],
        "keywords": [
            "clip", "vit", "bert", "roberta", "t5", "gpt", 
            "transformer", "attention", "encoder", "decoder"
        ],
        "category": ModelCategory.TRANSFORMER_MODELS,
        "priority": ModelPriority.HIGH,
        "step_name": "QualityAssessmentStep",
        "min_size_mb": 50,
        "max_size_mb": 5000,
        "expected_formats": [".bin", ".safetensors"],
        "model_class": "CLIPModel"
    },
    
    # Post Processing Models
    "post_processing": {
        "patterns": [
            r".*realesrgan.*\.pth$",
            r".*esrgan.*\.pth$",
            r".*super.*resolution.*\.pth$",
            r".*upscale.*\.pth$",
            r".*enhance.*\.pth$",
            r".*srcnn.*\.pth$",
            r".*edsr.*\.pth$",
            r".*rcan.*\.pth$"
        ],
        "keywords": [
            "esrgan", "realesrgan", "upscale", "enhance", "super", 
            "resolution", "srcnn", "edsr", "rcan"
        ],
        "category": ModelCategory.POST_PROCESSING,
        "priority": ModelPriority.MEDIUM,
        "step_name": "PostProcessingStep",
        "min_size_mb": 10,
        "max_size_mb": 200,
        "expected_formats": [".pth", ".pt"],
        "model_class": "EnhancementModel"
    }
}

# ==============================================
# ğŸ”¥ ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤ (ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°)
# ==============================================

class AdvancedModelDetector:
    """
    ğŸ” ì™„ì „ í†µí•© AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v5.0
    âœ… step_model_requests.py ê¸°ë°˜ ì •í™•í•œ íƒì§€
    âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤ ìë™ ë°œê²¬
    âœ… ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì¶œë ¥ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_metadata_extraction: bool = True,
        enable_caching: bool = True,
        cache_db_path: Optional[Path] = None,
        max_workers: int = 4,
        scan_timeout: int = 300
    ):
        """ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelDetector")
        
        # ê¸°ë³¸ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • (conda í™˜ê²½ íŠ¹í™”)
        if search_paths is None:
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parents[3]  # app/ai_pipeline/utilsì—ì„œ backendë¡œ
            
            # conda í™˜ê²½ë³„ ê²½ë¡œ ì¶”ê°€
            conda_paths = []
            try:
                conda_prefix = os.environ.get('CONDA_PREFIX')
                if conda_prefix:
                    conda_paths.extend([
                        Path(conda_prefix) / "share" / "models",
                        Path(conda_prefix) / "lib" / "python3.11" / "site-packages" / "models",
                        Path(conda_prefix) / "models"
                    ])
            except:
                pass
            
            self.search_paths = [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "app" / "models",
                backend_dir / "checkpoints",
                backend_dir / "models",
                backend_dir / "weights",
                Path.home() / ".cache" / "huggingface",
                Path.home() / ".cache" / "torch",
                Path.home() / ".cache" / "models",
                *conda_paths
            ]
        else:
            self.search_paths = search_paths
        
        # ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_metadata_extraction = enable_metadata_extraction
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        # ìºì‹œ ê´€ë¦¬
        self.cache_db_path = cache_db_path or Path("model_detection_cache.db")
        self.cache_ttl = 86400  # 24ì‹œê°„
        self._cache_lock = threading.RLock()
        
        # Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements = {}
        if STEP_REQUESTS_AVAILABLE:
            try:
                self.step_requirements = get_all_step_requirements()
            except:
                self.step_requirements = STEP_MODEL_REQUESTS
        else:
            self.step_requirements = STEP_MODEL_REQUESTS
        
        self.logger.info(f"ğŸ” ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
        # ìºì‹œ DB ì´ˆê¸°í™”
        if self.enable_caching:
            self._init_cache_db()

    def _init_cache_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS model_cache (
                        file_path TEXT PRIMARY KEY,
                        file_size INTEGER,
                        file_mtime REAL,
                        checksum TEXT,
                        detection_data TEXT,
                        created_at REAL,
                        accessed_at REAL
                    )
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_accessed_at ON model_cache(accessed_at)
                """)
                
                conn.commit()
                
            self.logger.debug("âœ… ìºì‹œ DB ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enable_caching = False

    def detect_all_models(
        self, 
        force_rescan: bool = False,
        categories_filter: Optional[List[ModelCategory]] = None,
        min_confidence: float = 0.3,
        step_filter: Optional[List[str]] = None
    ) -> Dict[str, DetectedModel]:
        """
        ëª¨ë“  AI ëª¨ë¸ ìë™ íƒì§€ (step_model_requests.py ê¸°ë°˜)
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            step_filter: íŠ¹ì • Stepë“¤ë§Œ íƒì§€
            
        Returns:
            Dict[str, DetectedModel]: íƒì§€ëœ ëª¨ë¸ë“¤
        """
        try:
            self.logger.info("ğŸ” Step ê¸°ë°˜ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.enable_caching:
                cached_results = self._load_from_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ ëª¨ë¸")
                    self.scan_stats["cache_hits"] += len(cached_results)
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self._reset_scan_stats()
            
            # Stepë³„ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ìŠ¤ìº”
            if step_filter:
                filtered_requirements = {k: v for k, v in self.step_requirements.items() 
                                       if k in step_filter}
            else:
                filtered_requirements = self.step_requirements
            
            # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
            if self.max_workers > 1:
                self._parallel_scan_by_steps(filtered_requirements, categories_filter, min_confidence)
            else:
                self._sequential_scan_by_steps(filtered_requirements, categories_filter, min_confidence)
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            self.scan_stats["last_scan_time"] = time.time()
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            self._post_process_results(min_confidence)
            
            # ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache()
            
            self.logger.info(f"âœ… Step ê¸°ë°˜ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.scan_stats["errors_encountered"] += 1
            raise

    def _parallel_scan_by_steps(self, step_requirements: Dict, categories_filter, min_confidence):
        """Stepë³„ ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰"""
        try:
            # Stepë³„ ìŠ¤ìº” íƒœìŠ¤í¬ ìƒì„±
            scan_tasks = []
            for step_name, requirements in step_requirements.items():
                for search_path in self.search_paths:
                    if search_path.exists():
                        scan_tasks.append((step_name, requirements, search_path))
            
            if not scan_tasks:
                self.logger.warning("âš ï¸ ìŠ¤ìº”í•  ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {
                    executor.submit(
                        self._scan_path_for_step, 
                        step_name, 
                        requirements, 
                        search_path, 
                        categories_filter, 
                        min_confidence
                    ): (step_name, search_path)
                    for step_name, requirements, search_path in scan_tasks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                completed_count = 0
                for future in as_completed(future_to_task, timeout=self.scan_timeout):
                    step_name, search_path = future_to_task[future]
                    try:
                        step_results = future.result()
                        if step_results:
                            # ê²°ê³¼ ë³‘í•© (ìŠ¤ë ˆë“œ ì•ˆì „)
                            with threading.Lock():
                                for name, model in step_results.items():
                                    self._register_detected_model_safe(model)
                        
                        completed_count += 1
                        self.logger.debug(f"âœ… {step_name} @ {search_path} ìŠ¤ìº” ì™„ë£Œ ({completed_count}/{len(scan_tasks)})")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ {step_name} @ {search_path} ìŠ¤ìº” ì‹¤íŒ¨: {e}")
                        self.scan_stats["errors_encountered"] += 1
                        
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆœì°¨ ìŠ¤ìº”
            self._sequential_scan_by_steps(step_requirements, categories_filter, min_confidence)

    def _sequential_scan_by_steps(self, step_requirements: Dict, categories_filter, min_confidence):
        """Stepë³„ ìˆœì°¨ ìŠ¤ìº” ì‹¤í–‰"""
        try:
            for step_name, requirements in step_requirements.items():
                self.logger.debug(f"ğŸ“ {step_name} ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ìŠ¤ìº” ì¤‘...")
                
                for search_path in self.search_paths:
                    if search_path.exists():
                        step_results = self._scan_path_for_step(
                            step_name, requirements, search_path, categories_filter, min_confidence
                        )
                        if step_results:
                            for name, model in step_results.items():
                                self._register_detected_model_safe(model)
                    else:
                        self.logger.debug(f"âš ï¸ ê²½ë¡œ ì—†ìŒ: {search_path}")
                        
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ìŠ¤ìº” ì‹¤íŒ¨: {e}")

    def _scan_path_for_step(
        self, 
        step_name: str, 
        requirements: Dict, 
        search_path: Path, 
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float,
        max_depth: int = 6,
        current_depth: int = 0
    ) -> Dict[str, DetectedModel]:
        """íŠ¹ì • Step ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ëª¨ë¸ ìŠ¤ìº”"""
        results = {}
        
        try:
            if current_depth > max_depth:
                return results
            
            # Stepë³„ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
            if isinstance(requirements, dict):
                checkpoint_patterns = requirements.get("checkpoint_patterns", [])
                if not checkpoint_patterns:
                    # step_model_requests.py ìŠ¤íƒ€ì¼ íŒ¨í„´
                    checkpoint_requirements = requirements.get("checkpoint_requirements", {})
                    checkpoint_patterns = checkpoint_requirements.get("primary_model_patterns", [])
            else:
                checkpoint_patterns = getattr(requirements, "checkpoint_patterns", [])
            
            if not checkpoint_patterns:
                self.logger.debug(f"âš ï¸ {step_name}ì— ëŒ€í•œ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤")
                return results
                
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ë‚˜ì—´
            try:
                items = list(search_path.iterdir())
            except PermissionError:
                self.logger.debug(f"ê¶Œí•œ ì—†ìŒ: {search_path}")
                return results
            
            # íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë¶„ë¦¬
            files = [item for item in items if item.is_file()]
            subdirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]
            
            # íŒŒì¼ ë¶„ì„ (Stepë³„ íŒ¨í„´ ë§¤ì¹­)
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    # Stepë³„ íŒ¨í„´ ë§¤ì¹­
                    if self._matches_step_patterns(file_path, checkpoint_patterns):
                        detected_model = self._analyze_file_for_step(
                            file_path, step_name, requirements, categories_filter, min_confidence
                        )
                        if detected_model:
                            results[detected_model.name] = detected_model
                            self.logger.debug(f"ğŸ“¦ {step_name} ëª¨ë¸ ë°œê²¬: {file_path.name}")
                        
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
            if self.enable_deep_scan and current_depth < max_depth:
                for subdir in subdirs:
                    # ì œì™¸í•  ë””ë ‰í† ë¦¬ íŒ¨í„´
                    if subdir.name in ['__pycache__', '.git', 'node_modules', '.vscode', '.idea']:
                        continue
                    
                    try:
                        subdir_results = self._scan_path_for_step(
                            step_name, requirements, subdir, categories_filter, 
                            min_confidence, max_depth, current_depth + 1
                        )
                        results.update(subdir_results)
                    except Exception as e:
                        self.logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                        continue
            
            return results
            
        except Exception as e:
            self.logger.debug(f"Step ìŠ¤ìº” ì˜¤ë¥˜ {search_path}: {e}")
            return results

    def _matches_step_patterns(self, file_path: Path, patterns: List[str]) -> bool:
        """íŒŒì¼ì´ Stepë³„ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸"""
        try:
            file_name_lower = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            for pattern in patterns:
                # ê°„ë‹¨í•œ ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ë§¤ì¹­
                pattern_regex = pattern.replace("*", ".*").lower()
                if re.search(pattern_regex, file_name_lower) or re.search(pattern_regex, file_path_str):
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return False

    def _analyze_file_for_step(
        self, 
        file_path: Path, 
        step_name: str,
        requirements: Dict,
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float
    ) -> Optional[DetectedModel]:
        """Step ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒŒì¼ ë¶„ì„"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            file_extension = file_path.suffix.lower()
            last_modified = file_stat.st_mtime
            
            # Stepë³„ í¬ê¸° ì œí•œ í™•ì¸
            if isinstance(requirements, dict):
                checkpoint_requirements = requirements.get("checkpoint_requirements", {})
                min_size = checkpoint_requirements.get("min_file_size_mb", 1)
                max_size = checkpoint_requirements.get("max_file_size_mb", 10000)
            else:
                min_size = 1
                max_size = 10000
            
            if not (min_size <= file_size_mb <= max_size):
                return None
            
            # AI ëª¨ë¸ íŒŒì¼ í™•ì¥ì í•„í„°
            ai_extensions = {
                '.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', 
                '.h5', '.pb', '.json', '.yaml', '.yml'
            }
            if file_extension not in ai_extensions:
                return None
            
            # Stepë³„ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
            if isinstance(requirements, dict):
                model_name = requirements.get("model_name", f"{step_name.lower()}_model")
                model_type = requirements.get("model_type", "BaseModel")
                model_class = requirements.get("model_class", model_type)
            else:
                model_name = f"{step_name.lower()}_model"
                model_type = "BaseModel"
                model_class = model_type
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "HumanParsingStep": ModelCategory.HUMAN_PARSING,
                "PoseEstimationStep": ModelCategory.POSE_ESTIMATION,
                "ClothSegmentationStep": ModelCategory.CLOTH_SEGMENTATION,
                "GeometricMatchingStep": ModelCategory.GEOMETRIC_MATCHING,
                "ClothWarpingStep": ModelCategory.CLOTH_WARPING,
                "VirtualFittingStep": ModelCategory.VIRTUAL_FITTING,
                "PostProcessingStep": ModelCategory.POST_PROCESSING,
                "QualityAssessmentStep": ModelCategory.QUALITY_ASSESSMENT
            }
            
            detected_category = category_mapping.get(step_name, ModelCategory.AUXILIARY)
            
            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
            if categories_filter and detected_category not in categories_filter:
                return None
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_step_confidence(file_path, step_name, requirements)
            
            if confidence_score < min_confidence:
                return None
            
            # ìš°ì„ ìˆœìœ„ ê²°ì •
            priority_mapping = {
                "HumanParsingStep": ModelPriority.CRITICAL,
                "VirtualFittingStep": ModelPriority.CRITICAL,
                "PoseEstimationStep": ModelPriority.HIGH,
                "ClothSegmentationStep": ModelPriority.HIGH,
                "ClothWarpingStep": ModelPriority.MEDIUM,
                "GeometricMatchingStep": ModelPriority.MEDIUM,
                "PostProcessingStep": ModelPriority.LOW,
                "QualityAssessmentStep": ModelPriority.LOW
            }
            
            priority = priority_mapping.get(step_name, ModelPriority.MEDIUM)
            
            # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
            unique_name = self._generate_unique_model_name(file_path, step_name, model_name)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_step_metadata(file_path, step_name, requirements)
            
            # DetectedModel ê°ì²´ ìƒì„±
            detected_model = DetectedModel(
                name=unique_name,
                path=file_path,
                category=detected_category,
                model_type=model_class,
                file_size_mb=file_size_mb,
                file_extension=file_extension,
                confidence_score=confidence_score,
                priority=priority,
                step_name=step_name,
                metadata=metadata,
                last_modified=last_modified
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"Step íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _calculate_step_confidence(self, file_path: Path, step_name: str, requirements: Dict) -> float:
        """Stepë³„ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            file_name = file_path.name.lower()
            
            # Stepë³„ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ ë§¤ì¹­
            if isinstance(requirements, dict):
                patterns = requirements.get("checkpoint_patterns", [])
                for pattern in patterns:
                    pattern_regex = pattern.replace("*", ".*").lower()
                    if re.search(pattern_regex, file_name):
                        score += 15.0
                        break
            
            # íŒŒì¼ëª…ì—ì„œ Step ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
            step_keywords = {
                "HumanParsingStep": ["human", "parsing", "schp", "atr", "graphonomy"],
                "PoseEstimationStep": ["pose", "openpose", "body", "keypoint"],
                "ClothSegmentationStep": ["u2net", "cloth", "segmentation", "sam"],
                "GeometricMatchingStep": ["geometric", "matching", "gmm", "tps"],
                "ClothWarpingStep": ["warping", "tom", "hrviton", "cloth"],
                "VirtualFittingStep": ["diffusion", "stable", "viton", "fitting"],
                "PostProcessingStep": ["esrgan", "realesrgan", "enhance", "super"],
                "QualityAssessmentStep": ["clip", "quality", "assessment"]
            }
            
            keywords = step_keywords.get(step_name, [])
            for keyword in keywords:
                if keyword in file_name:
                    score += 8.0
            
            # íŒŒì¼ í¬ê¸° ì ì •ì„± (Stepë³„)
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            if step_name in ["VirtualFittingStep"]:
                # ëŒ€í˜• ëª¨ë¸ (Diffusion ë“±)
                if 500 <= file_size_mb <= 5000:
                    score += 10.0
                elif file_size_mb > 100:
                    score += 5.0
            elif step_name in ["HumanParsingStep", "ClothSegmentationStep"]:
                # ì¤‘í˜• ëª¨ë¸
                if 50 <= file_size_mb <= 500:
                    score += 10.0
                elif file_size_mb > 20:
                    score += 5.0
            else:
                # ì†Œí˜• ëª¨ë¸
                if 5 <= file_size_mb <= 200:
                    score += 10.0
                elif file_size_mb > 1:
                    score += 5.0
            
            # íŒŒì¼ í™•ì¥ì ë³´ë„ˆìŠ¤
            if file_path.suffix in ['.pth', '.pt']:
                score += 5.0
            elif file_path.suffix in ['.bin', '.safetensors']:
                score += 3.0
            
            # ì •ê·œí™”
            confidence = min(score / 50.0, 1.0)
            return confidence
            
        except Exception as e:
            self.logger.debug(f"ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

    def _generate_unique_model_name(self, file_path: Path, step_name: str, base_name: str) -> str:
        """ê³ ìœ í•œ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # Stepë³„ í‘œì¤€ ì´ë¦„ ë§¤í•‘
            standard_names = {
                "HumanParsingStep": "human_parsing_graphonomy",
                "PoseEstimationStep": "pose_estimation_openpose",
                "ClothSegmentationStep": "cloth_segmentation_u2net",
                "GeometricMatchingStep": "geometric_matching_gmm",
                "ClothWarpingStep": "cloth_warping_tom",
                "VirtualFittingStep": "virtual_fitting_stable_diffusion",
                "PostProcessingStep": "post_processing_realesrgan",
                "QualityAssessmentStep": "quality_assessment_clip"
            }
            
            standard_name = standard_names.get(step_name)
            if standard_name:
                return standard_name
            
            # íŒŒì¼ëª… ê¸°ë°˜ ì´ë¦„ ìƒì„±
            file_stem = file_path.stem.lower()
            clean_name = re.sub(r'[^a-z0-9_]', '_', file_stem)
            
            # í•´ì‹œ ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
            path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:6]
            
            return f"{step_name.lower()}_{clean_name}_{path_hash}"
            
        except Exception as e:
            timestamp = int(time.time())
            return f"detected_model_{timestamp}"

    def _extract_step_metadata(self, file_path: Path, step_name: str, requirements: Dict) -> Dict[str, Any]:
        """Stepë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            "file_name": file_path.name,
            "file_size_mb": file_path.stat().st_size / (1024 * 1024),
            "step_name": step_name,
            "detected_at": time.time(),
            "auto_detected": True
        }
        
        try:
            # Step ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ê°€
            if isinstance(requirements, dict):
                metadata.update({
                    "step_model_name": requirements.get("model_name", "unknown"),
                    "step_model_type": requirements.get("model_type", "unknown"),
                    "step_priority": requirements.get("step_priority", 5)
                })
                
                # ì²´í¬í¬ì¸íŠ¸ ìš”êµ¬ì‚¬í•­
                checkpoint_requirements = requirements.get("checkpoint_requirements", {})
                if checkpoint_requirements:
                    metadata["checkpoint_requirements"] = checkpoint_requirements
            
            # PyTorch ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬
            if TORCH_AVAILABLE and file_path.suffix in ['.pth', '.pt']:
                try:
                    # ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
                    checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                    
                    if isinstance(checkpoint, dict):
                        # ëª¨ë¸ êµ¬ì¡° ì •ë³´
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                            if isinstance(state_dict, dict):
                                metadata['torch_layers_count'] = len(state_dict)
                                
                                # ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                                total_params = 0
                                for tensor in state_dict.values():
                                    if torch.is_tensor(tensor):
                                        total_params += tensor.numel()
                                metadata['torch_total_parameters'] = total_params
                        
                        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                        for key in ['epoch', 'version', 'arch', 'model_name']:
                            if key in checkpoint:
                                metadata[f'torch_{key}'] = str(checkpoint[key])[:100]
                        
                except Exception as e:
                    metadata['torch_load_error'] = str(e)[:100]
            
            return metadata
            
        except Exception as e:
            metadata['metadata_extraction_error'] = str(e)[:100]
            return metadata

    def _register_detected_model_safe(self, detected_model: DetectedModel):
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ëª¨ë¸ ë“±ë¡"""
        with threading.Lock():
            self._register_detected_model(detected_model)

    def _register_detected_model(self, detected_model: DetectedModel):
        """íƒì§€ëœ ëª¨ë¸ ë“±ë¡ (ì¤‘ë³µ ì²˜ë¦¬)"""
        try:
            model_name = detected_model.name
            
            if model_name in self.detected_models:
                existing_model = self.detected_models[model_name]
                
                # ë” ë‚˜ì€ ëª¨ë¸ë¡œ êµì²´í• ì§€ ê²°ì •
                if self._is_better_model(detected_model, existing_model):
                    detected_model.alternative_paths.append(existing_model.path)
                    detected_model.alternative_paths.extend(existing_model.alternative_paths)
                    self.detected_models[model_name] = detected_model
                    self.logger.debug(f"ğŸ”„ ëª¨ë¸ êµì²´: {model_name}")
                else:
                    existing_model.alternative_paths.append(detected_model.path)
                    self.logger.debug(f"ğŸ“ ëŒ€ì²´ ê²½ë¡œ ì¶”ê°€: {model_name}")
            else:
                self.detected_models[model_name] = detected_model
                self.logger.debug(f"âœ… ìƒˆ ëª¨ë¸ ë“±ë¡: {model_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _is_better_model(self, new_model: DetectedModel, existing_model: DetectedModel) -> bool:
        """ìƒˆ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨"""
        try:
            # 1. ìš°ì„ ìˆœìœ„ ë¹„êµ
            if new_model.priority.value < existing_model.priority.value:
                return True
            elif new_model.priority.value > existing_model.priority.value:
                return False
            
            # 2. ì‹ ë¢°ë„ ë¹„êµ
            if abs(new_model.confidence_score - existing_model.confidence_score) > 0.1:
                return new_model.confidence_score > existing_model.confidence_score
            
            # 3. ìµœì‹ ì„± ë¹„êµ
            if abs(new_model.last_modified - existing_model.last_modified) > 86400:  # 1ì¼ ì´ìƒ ì°¨ì´
                return new_model.last_modified > existing_model.last_modified
            
            # 4. íŒŒì¼ í¬ê¸° ë¹„êµ
            return new_model.file_size_mb > existing_model.file_size_mb
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ë¹„êµ ì˜¤ë¥˜: {e}")
            return new_model.file_size_mb > existing_model.file_size_mb

    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ë¦¬ì…‹"""
        self.scan_stats.update({
            "total_files_scanned": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0
        })

    def _post_process_results(self, min_confidence: float):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ì‹ ë¢°ë„ í•„í„°ë§
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            self.detected_models = filtered_models
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
            sorted_models = sorted(
                self.detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score, -x[1].file_size_mb)
            )
            
            self.detected_models = {name: model for name, model in sorted_models}
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            self.logger.info("=" * 60)
            self.logger.info("ğŸ” Step ê¸°ë°˜ ëª¨ë¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 60)
            
            total_size_gb = sum(model.file_size_mb for model in self.detected_models.values()) / 1024
            avg_confidence = sum(model.confidence_score for model in self.detected_models.values()) / len(self.detected_models) if self.detected_models else 0
            
            self.logger.info(f"ğŸ“Š íƒì§€ëœ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº” íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            self.logger.info(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            
            # Stepë³„ ë¶„í¬
            step_distribution = {}
            for model in self.detected_models.values():
                step = model.step_name
                if step not in step_distribution:
                    step_distribution[step] = 0
                step_distribution[step] += 1
            
            if step_distribution:
                self.logger.info("\nğŸ“ Stepë³„ ë¶„í¬:")
                for step, count in step_distribution.items():
                    self.logger.info(f"  {step}: {count}ê°œ")
            
            # ì£¼ìš” ëª¨ë¸ë“¤
            if self.detected_models:
                self.logger.info("\nğŸ† íƒì§€ëœ ì£¼ìš” ëª¨ë¸ë“¤:")
                for i, (name, model) in enumerate(list(self.detected_models.items())[:5]):
                    self.logger.info(f"  {i+1}. {name}")
                    self.logger.info(f"     Step: {model.step_name}, í¬ê¸°: {model.file_size_mb:.1f}MB")
                    self.logger.info(f"     ì‹ ë¢°ë„: {model.confidence_score:.3f}, ìš°ì„ ìˆœìœ„: {model.priority.name}")
            
            self.logger.info("=" * 60)
                
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ ìºì‹œ ê´€ë ¨ ë©”ì„œë“œë“¤
    # ==============================================

    def _load_from_cache(self) -> Optional[Dict[str, DetectedModel]]:
        """ìºì‹œì—ì„œ ë¡œë“œ"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    cutoff_time = time.time() - self.cache_ttl
                    cursor.execute("DELETE FROM model_cache WHERE created_at < ?", (cutoff_time,))
                    
                    # ìºì‹œ ì¡°íšŒ
                    cursor.execute("""
                        SELECT file_path, detection_data 
                        FROM model_cache 
                        WHERE created_at > ?
                    """, (cutoff_time,))
                    
                    cached_models = {}
                    for file_path, detection_data in cursor.fetchall():
                        try:
                            # íŒŒì¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                            if not Path(file_path).exists():
                                continue
                            
                            model_data = json.loads(detection_data)
                            model = self._deserialize_detected_model(model_data)
                            if model:
                                cached_models[model.name] = model
                        except Exception as e:
                            self.logger.debug(f"ìºì‹œ í•­ëª© ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                    
                    if cached_models:
                        # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                        cursor.execute("UPDATE model_cache SET accessed_at = ?", (time.time(),))
                        conn.commit()
                        
                        self.detected_models = cached_models
                        return cached_models
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_cache(self):
        """ìºì‹œì— ì €ì¥"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    current_time = time.time()
                    
                    for model in self.detected_models.values():
                        try:
                            detection_data = json.dumps(self._serialize_detected_model(model))
                            
                            cursor.execute("""
                                INSERT OR REPLACE INTO model_cache 
                                (file_path, file_size, file_mtime, checksum, detection_data, created_at, accessed_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                str(model.path),
                                int(model.file_size_mb * 1024 * 1024),
                                model.last_modified,
                                model.checksum,
                                detection_data,
                                current_time,
                                current_time
                            ))
                        except Exception as e:
                            self.logger.debug(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {model.name}: {e}")
                    
                    conn.commit()
                    
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _serialize_detected_model(self, model: DetectedModel) -> Dict[str, Any]:
        """DetectedModelì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™”"""
        return {
            "name": model.name,
            "path": str(model.path),
            "category": model.category.value,
            "model_type": model.model_type,
            "file_size_mb": model.file_size_mb,
            "file_extension": model.file_extension,
            "confidence_score": model.confidence_score,
            "priority": model.priority.value,
            "step_name": model.step_name,
            "metadata": model.metadata,
            "alternative_paths": [str(p) for p in model.alternative_paths],
            "requirements": model.requirements,
            "performance_info": model.performance_info,
            "compatibility_info": model.compatibility_info,
            "last_modified": model.last_modified,
            "checksum": model.checksum
        }

    def _deserialize_detected_model(self, data: Dict[str, Any]) -> Optional[DetectedModel]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ DetectedModelë¡œ ì—­ì§ë ¬í™”"""
        try:
            return DetectedModel(
                name=data["name"],
                path=Path(data["path"]),
                category=ModelCategory(data["category"]),
                model_type=data["model_type"],
                file_size_mb=data["file_size_mb"],
                file_extension=data["file_extension"],
                confidence_score=data["confidence_score"],
                priority=ModelPriority(data["priority"]),
                step_name=data["step_name"],
                metadata=data.get("metadata", {}),
                alternative_paths=[Path(p) for p in data.get("alternative_paths", [])],
                requirements=data.get("requirements", []),
                performance_info=data.get("performance_info", {}),
                compatibility_info=data.get("compatibility_info", {}),
                last_modified=data.get("last_modified", 0.0),
                checksum=data.get("checksum")
            )
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return None

    # ==============================================
    # ğŸ”¥ ê³µê°œ ì¡°íšŒ ë©”ì„œë“œë“¤
    # ==============================================

    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]

    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]

    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """Stepë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        return min(step_models, key=lambda m: (m.priority.value, -m.confidence_score))

    def get_model_by_name(self, name: str) -> Optional[DetectedModel]:
        """ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ"""
        return self.detected_models.get(name)

    def get_all_model_paths(self) -> Dict[str, Path]:
        """ëª¨ë“  ëª¨ë¸ì˜ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {name: model.path for name, model in self.detected_models.items()}

    def search_models(
        self, 
        keywords: List[str], 
        step_filter: Optional[List[str]] = None,
        min_confidence: float = 0.0
    ) -> List[DetectedModel]:
        """í‚¤ì›Œë“œë¡œ ëª¨ë¸ ê²€ìƒ‰"""
        try:
            results = []
            keywords_lower = [kw.lower() for kw in keywords]
            
            for model in self.detected_models.values():
                # ì‹ ë¢°ë„ í•„í„°
                if model.confidence_score < min_confidence:
                    continue
                
                # Step í•„í„°
                if step_filter and model.step_name not in step_filter:
                    continue
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                model_text = f"{model.name} {model.path.name} {model.model_type} {model.step_name}".lower()
                if any(keyword in model_text for keyword in keywords_lower):
                    results.append(model)
            
            # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda m: (m.priority.value, -m.confidence_score))
            return results
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

# ==============================================
# ğŸ”¥ ModelLoader ì—°ë™ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ì¶œë ¥ (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
# ==============================================

class ModelLoaderConfigGenerator:
    """
    ğŸ”— ModelLoader ì—°ë™ìš© ì„¤ì • ìƒì„±ê¸° (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
    ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ìœ¼ë¡œë§Œ ë™ì‘í•˜ì—¬ ModelLoader import ë¶ˆí•„ìš”
    """
    
    def __init__(self, detector: AdvancedModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderConfigGenerator")
    
    def generate_complete_config(self) -> Dict[str, Any]:
        """ModelLoaderìš© ì™„ì „í•œ ì„¤ì • ìƒì„±"""
        try:
            config = {
                "model_configs": [],
                "model_paths": {},
                "step_mappings": {},
                "priority_rankings": {},
                "performance_estimates": {},
                "metadata": {
                    "total_models": len(self.detector.detected_models),
                    "generation_time": time.time(),
                    "detector_version": "5.0",
                    "scan_stats": self.detector.scan_stats
                }
            }
            
            for name, detected_model in self.detector.detected_models.items():
                # ModelConfig ë”•ì…”ë„ˆë¦¬ ìƒì„±
                model_config = {
                    "name": name,
                    "model_type": detected_model.category.value,
                    "model_class": detected_model.model_type,
                    "checkpoint_path": str(detected_model.path),
                    "device": "auto",
                    "precision": "fp16",
                    "input_size": self._get_input_size_for_step(detected_model.step_name),
                    "step_name": detected_model.step_name,
                    "metadata": {
                        **detected_model.metadata,
                        "auto_detected": True,
                        "confidence_score": detected_model.confidence_score,
                        "priority": detected_model.priority.name,
                        "alternative_paths": [str(p) for p in detected_model.alternative_paths]
                    }
                }
                config["model_configs"].append(model_config)
                
                # ê²½ë¡œ ë§¤í•‘
                config["model_paths"][name] = {
                    "primary": str(detected_model.path),
                    "alternatives": [str(p) for p in detected_model.alternative_paths],
                    "size_mb": detected_model.file_size_mb,
                    "confidence": detected_model.confidence_score
                }
                
                # Step ë§¤í•‘
                step_name = detected_model.step_name
                if step_name not in config["step_mappings"]:
                    config["step_mappings"][step_name] = []
                config["step_mappings"][step_name].append(name)
                
                # ìš°ì„ ìˆœìœ„
                config["priority_rankings"][name] = {
                    "priority_level": detected_model.priority.value,
                    "priority_name": detected_model.priority.name,
                    "confidence_score": detected_model.confidence_score,
                    "step_rank": self._get_step_rank(detected_model.step_name)
                }
                
                # ì„±ëŠ¥ ì¶”ì •
                config["performance_estimates"][name] = {
                    "estimated_memory_gb": detected_model.file_size_mb / 1024 * 2,
                    "estimated_load_time_sec": self._estimate_load_time(detected_model),
                    "recommended_batch_size": self._get_recommended_batch_size(detected_model),
                    "gpu_memory_required_gb": max(2.0, detected_model.file_size_mb / 1024 * 1.5)
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _get_input_size_for_step(self, step_name: str) -> Tuple[int, int]:
        """Stepë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸°"""
        size_mapping = {
            "HumanParsingStep": (512, 512),
            "PoseEstimationStep": (368, 368),
            "ClothSegmentationStep": (320, 320),
            "GeometricMatchingStep": (512, 384),
            "ClothWarpingStep": (512, 384),
            "VirtualFittingStep": (512, 512),
            "PostProcessingStep": (512, 512),
            "QualityAssessmentStep": (224, 224)
        }
        return size_mapping.get(step_name, (512, 512))

    def _estimate_load_time(self, detected_model: DetectedModel) -> float:
        """ëª¨ë¸ ë¡œë“œ ì‹œê°„ ì¶”ì • (ì´ˆ)"""
        base_times = {
            ModelCategory.HUMAN_PARSING: 2.0,
            ModelCategory.POSE_ESTIMATION: 1.0,
            ModelCategory.CLOTH_SEGMENTATION: 1.5,
            ModelCategory.GEOMETRIC_MATCHING: 0.5,
            ModelCategory.CLOTH_WARPING: 3.0,
            ModelCategory.VIRTUAL_FITTING: 8.0,
            ModelCategory.DIFFUSION_MODELS: 10.0,
            ModelCategory.TRANSFORMER_MODELS: 3.0,
            ModelCategory.POST_PROCESSING: 2.0,
            ModelCategory.QUALITY_ASSESSMENT: 1.0
        }
        
        base_time = base_times.get(detected_model.category, 2.0)
        
        # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¡°ì •
        size_factor = min(detected_model.file_size_mb / 100, 5.0)
        
        return base_time * size_factor

    def _get_recommended_batch_size(self, detected_model: DetectedModel) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸°"""
        if detected_model.file_size_mb > 1000:  # ëŒ€í˜• ëª¨ë¸
            return 1
        elif detected_model.file_size_mb > 100:  # ì¤‘í˜• ëª¨ë¸
            return 2
        else:  # ì†Œí˜• ëª¨ë¸
            return 4

    def _get_step_rank(self, step_name: str) -> int:
        """Stepë³„ ìˆœìœ„ (ì¤‘ìš”ë„)"""
        rank_mapping = {
            "HumanParsingStep": 1,
            "VirtualFittingStep": 2,
            "PoseEstimationStep": 3,
            "ClothSegmentationStep": 3,
            "ClothWarpingStep": 4,
            "GeometricMatchingStep": 5,
            "PostProcessingStep": 6,
            "QualityAssessmentStep": 7
        }
        return rank_mapping.get(step_name, 9)

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ ë° íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def create_advanced_detector(
    search_paths: Optional[List[Path]] = None,
    enable_parallel: bool = True,
    max_workers: int = 4,
    **kwargs
) -> AdvancedModelDetector:
    """ê³ ê¸‰ ìë™ ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return AdvancedModelDetector(
        search_paths=search_paths,
        max_workers=max_workers if enable_parallel else 1,
        **kwargs
    )

def quick_model_detection(
    step_filter: Optional[List[str]] = None,
    min_confidence: float = 0.5,
    force_rescan: bool = False
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ë° ê²°ê³¼ ë°˜í™˜"""
    try:
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_advanced_detector()
        detected_models = detector.detect_all_models(
            force_rescan=force_rescan,
            step_filter=step_filter,
            min_confidence=min_confidence
        )
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "total_models": len(detected_models),
            "models_by_step": {},
            "models_by_priority": {},
            "top_models": {},
            "scan_stats": detector.scan_stats
        }
        
        # Stepë³„ ë¶„ë¥˜
        for model in detected_models.values():
            step = model.step_name
            if step not in summary["models_by_step"]:
                summary["models_by_step"][step] = []
            summary["models_by_step"][step].append({
                "name": model.name,
                "path": str(model.path),
                "confidence": model.confidence_score,
                "size_mb": model.file_size_mb
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        for model in detected_models.values():
            priority = model.priority.name
            if priority not in summary["models_by_priority"]:
                summary["models_by_priority"][priority] = []
            summary["models_by_priority"][priority].append(model.name)
        
        # Stepë³„ ìµœê³  ëª¨ë¸
        step_names = set(model.step_name for model in detected_models.values())
        for step_name in step_names:
            best_model = detector.get_best_model_for_step(step_name)
            if best_model:
                summary["top_models"][step_name] = {
                    "name": best_model.name,
                    "path": str(best_model.path),
                    "confidence": best_model.confidence_score,
                    "priority": best_model.priority.name
                }
        
        return summary
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def generate_model_loader_config(
    detector: Optional[AdvancedModelDetector] = None,
    **detection_kwargs
) -> Dict[str, Any]:
    """
    ModelLoaderìš© ì„¤ì • ìƒì„± (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
    ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ìœ¼ë¡œë§Œ ì¶œë ¥
    """
    try:
        logger.info("ğŸ” ModelLoader ì„¤ì • ìƒì„± ì‹œì‘...")
        
        # íƒì§€ê¸°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if detector is None:
            detector = create_advanced_detector(**detection_kwargs)
            detected_models = detector.detect_all_models()
        else:
            detected_models = detector.detected_models
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No models detected"}
        
        # ì„¤ì • ìƒì„±ê¸° ì‚¬ìš©
        config_generator = ModelLoaderConfigGenerator(detector)
        model_loader_config = config_generator.generate_complete_config()
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "success": True,
            "model_loader_config": model_loader_config,
            "detection_summary": {
                "total_models": len(detected_models),
                "scan_duration": detector.scan_stats["scan_duration"],
                "confidence_avg": sum(m.confidence_score for m in detected_models.values()) / len(detected_models)
            }
        }
        
        logger.info(f"âœ… ModelLoader ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def validate_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ ê²½ë¡œë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "missing_files": [],
            "permission_errors": [],
            "total_size_gb": 0.0
        }
        
        for name, model in detected_models.items():
            try:
                # ì£¼ ê²½ë¡œ í™•ì¸
                if model.path.exists() and model.path.is_file():
                    validation_result["valid_models"].append(name)
                    validation_result["total_size_gb"] += model.file_size_mb / 1024
                else:
                    validation_result["missing_files"].append({
                        "name": name,
                        "path": str(model.path)
                    })
                
                # ëŒ€ì²´ ê²½ë¡œë“¤ í™•ì¸
                valid_alternatives = []
                for alt_path in model.alternative_paths:
                    if alt_path.exists() and alt_path.is_file():
                        valid_alternatives.append(str(alt_path))
                
                if valid_alternatives and name in [m["name"] for m in validation_result["missing_files"]]:
                    # ì£¼ ê²½ë¡œëŠ” ì—†ì§€ë§Œ ëŒ€ì²´ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                    validation_result["missing_files"] = [
                        m for m in validation_result["missing_files"] 
                        if m["name"] != name
                    ]
                    validation_result["valid_models"].append(name)
                
            except PermissionError:
                validation_result["permission_errors"].append({
                    "name": name,
                    "path": str(model.path)
                })
            except Exception as e:
                validation_result["invalid_models"].append({
                    "name": name,
                    "path": str(model.path),
                    "error": str(e)
                })
        
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_count": len(validation_result["valid_models"]),
            "invalid_count": len(validation_result["invalid_models"]),
            "missing_count": len(validation_result["missing_files"]),
            "permission_error_count": len(validation_result["permission_errors"]),
            "validation_rate": len(validation_result["valid_models"]) / len(detected_models) if detected_models else 0
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_advanced_detector',
    'quick_model_detection',
    'generate_model_loader_config',  # ğŸ”¥ ìˆœí™˜ì°¸ì¡° ë°©ì§€
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_model_paths',
    
    # ì„¤ì • ë° íŒ¨í„´
    'ADVANCED_MODEL_PATTERNS'
]

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
AutoModelDetector = AdvancedModelDetector
create_auto_detector = create_advanced_detector

logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v5.0 ë¡œë“œ ì™„ë£Œ")