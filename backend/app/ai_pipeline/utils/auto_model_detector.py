#!/usr/bin/env python3
"""
üîç MyCloset AI - ÏôÑÏ†ÑÌïú ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄ ÏãúÏä§ÌÖú v9.0 - ModelLoader Ïó∞Îèô ÌÜµÌï©
==================================================================================

‚úÖ ÌÉêÏßÄÎêú Î™®Îç∏ÏùÑ ModelLoaderÏóê ÏûêÎèô Îì±Î°ùÌïòÎäî Ïó∞Í≤∞ Í≥†Î¶¨ ÏôÑÏÑ±
‚úÖ 574Í∞ú Î™®Îç∏ ÌÉêÏßÄ ‚Üí Ïã§Ï†ú ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÍ≤å Îì±Î°ùÍπåÏßÄ ÏôÑÎ£å
‚úÖ PipelineManagerÏôÄ ÏôÑÏ†Ñ Ïó∞Îèô
‚úÖ StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô Ìï†Îãπ Î∞è Îì±Î°ù
‚úÖ MPS Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏßÄÏõê
‚úÖ ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ïÏÑ± Î≥¥Ïû•
‚úÖ Í∏∞Ï°¥ ÏΩîÎìú 100% Ìò∏ÌôòÏÑ± Ïú†ÏßÄ

üî• ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠ v9.0:
- ModelLoaderBridge ÌÅ¥ÎûòÏä§ Ï∂îÍ∞Ä (ÌÉêÏßÄ ‚Üí Îì±Î°ù Ïó∞Í≤∞)
- AutoRegistrationManager ÌÅ¥ÎûòÏä§ Ï∂îÍ∞Ä (ÏûêÎèô Îì±Î°ù ÏãúÏä§ÌÖú)
- StepModelMatcher ÌÅ¥ÎûòÏä§ Ï∂îÍ∞Ä (StepÎ≥Ñ Î™®Îç∏ Îß§Ïπ≠)
- Ïã§ÏãúÍ∞Ñ Î™®Îç∏ Í∞ÄÏö©ÏÑ± Í≤ÄÏ¶ù
- ÏûêÎèô Ìè¥Î∞± Î™®Îç∏ ÏÑ§Ï†ï
- ÏÑ±Îä• ÏµúÏ†ÅÌôîÎêú Îì±Î°ù ÌîÑÎ°úÏÑ∏Ïä§
- ÏôÑÏ†Ñ Î™®ÎìàÌôîÎêú ÏïÑÌÇ§ÌÖçÏ≤ò
"""

import os
import re
import sys
import time
import json
import logging
import hashlib
import sqlite3
import psutil
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps

# ==============================================
# üî• ÏïàÏ†ÑÌïú PyTorch import (MPS Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞)
# ==============================================

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ÏïàÏ†ÑÌïú ÏÑ§Ï†ï
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        IS_M3_MAX = True
        # ÏôÑÏ†Ñ ÏïàÏ†ÑÌïú MPS Ï∫êÏãú Ï†ïÎ¶¨
        try:
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        except (AttributeError, RuntimeError) as e:
            logging.debug(f"MPS Ï∫êÏãú Ï†ïÎ¶¨ Í±¥ÎÑàÎúÄ: {e}")
    elif torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        IS_M3_MAX = False
    else:
        DEVICE_TYPE = "cpu"
        IS_M3_MAX = False
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE_TYPE = "cpu"
    IS_M3_MAX = False
    torch = None

try:
    import numpy as np
    from PIL import Image
    import cv2
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

# ModelLoader Ïó∞ÎèôÏùÑ ÏúÑÌïú import
try:
    from .model_loader import (
        ModelLoader, get_global_model_loader, 
        StepModelInterface, SafeModelService,
        ModelConfig, StepModelConfig
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    MODEL_LOADER_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# üî• Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Î™®Îìà (Í∏∞Ï°¥ Ïú†ÏßÄ)
# ==============================================

class ModelCategory(Enum):
    """Î™®Îç∏ Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ•ò"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"
    
    # Ï∂îÍ∞Ä ÏÑ∏Î∂ÑÌôî
    STABLE_DIFFUSION = "stable_diffusion"
    OOTDIFFUSION = "ootdiffusion"
    CONTROLNET = "controlnet"
    SAM_MODELS = "sam_models"
    CLIP_MODELS = "clip_models"

class ModelArchitecture(Enum):
    """Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÌÉÄÏûÖ"""
    UNET = "unet"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    GAN = "gan"
    VAE = "vae"
    DIFFUSION = "diffusion"
    CLIP = "clip"
    RESNET = "resnet"
    CUSTOM = "custom"
    UNKNOWN = "unknown"

class ModelPriority(Enum):
    """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelPerformanceMetrics:
    """Î™®Îç∏ ÏÑ±Îä• Î©îÌä∏Î¶≠"""
    inference_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    throughput_fps: float = 0.0
    accuracy_score: Optional[float] = None
    m3_compatibility_score: float = 0.0

@dataclass
class DetectedModel:
    """ÌÉêÏßÄÎêú Î™®Îç∏ Ï†ïÎ≥¥ (ÏôÑÏ†Ñ Í∞ïÌôîÎêú Î≤ÑÏ†Ñ)"""
    # Í∏∞Î≥∏ Ï†ïÎ≥¥
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # Í≤ÄÏ¶ù Ï†ïÎ≥¥
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    checksum: Optional[str] = None
    
    # ÏïÑÌÇ§ÌÖçÏ≤ò Ï†ïÎ≥¥
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    precision: str = "fp32"
    
    # ÏÑ±Îä• Ï†ïÎ≥¥
    performance_metrics: Optional[ModelPerformanceMetrics] = None
    memory_requirements: Dict[str, float] = field(default_factory=dict)
    device_compatibility: Dict[str, bool] = field(default_factory=dict)
    load_time_ms: float = 0.0
    health_status: str = "unknown"
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation_results: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    
    # üî• NEW: ModelLoader Ïó∞Îèô Ï†ïÎ≥¥
    model_loader_registered: bool = False
    model_loader_name: Optional[str] = None
    step_interface_assigned: bool = False
    registration_timestamp: Optional[float] = None

# ==============================================
# üî• Í∞ïÌôîÎêú Ìå®ÌÑ¥ Îß§Ïπ≠ Î™®Îìà
# ==============================================

@dataclass
class EnhancedModelPattern:
    """Í∞ïÌôîÎêú Î™®Îç∏ Ìå®ÌÑ¥ Ï†ïÎ≥¥"""
    name: str
    patterns: List[str]
    step: str
    keywords: List[str]
    file_types: List[str]
    size_range_mb: Tuple[float, float]
    priority: int = 1
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    alternative_names: List[str] = field(default_factory=list)
    context_paths: List[str] = field(default_factory=list)
    
    # üî• NEW: ModelLoader Ïó∞Îèô Ï†ïÎ≥¥
    model_class: str = "BaseModel"
    loader_config: Dict[str, Any] = field(default_factory=dict)
    step_requirements: Dict[str, Any] = field(default_factory=dict)

class PatternMatcher:
    """Ìå®ÌÑ¥ Îß§Ïπ≠ Ï†ÑÏö© ÌÅ¥ÎûòÏä§ (Í∞ïÌôîÎêú Î≤ÑÏ†Ñ)"""
    
    def __init__(self):
        self.patterns = self._get_enhanced_patterns()
        self.logger = logging.getLogger(f"{__name__}.PatternMatcher")
    
    def _get_enhanced_patterns(self) -> Dict[str, EnhancedModelPattern]:
        """Í∞úÏÑ†Îêú Ìå®ÌÑ¥ Ï†ïÏùò (ModelLoader Ïó∞Îèô Ï†ïÎ≥¥ Ìè¨Ìï®)"""
        return {
            "human_parsing": EnhancedModelPattern(
                name="human_parsing",
                patterns=[
                    r".*exp-schp.*atr.*\.pth$",
                    r".*graphonomy.*lip.*\.pth$",
                    r".*densepose.*rcnn.*\.pkl$",
                    r".*human.*parsing.*\.pth$",
                    r".*schp.*\.pth$",
                    r".*atr.*\.pth$",
                    r".*lip.*\.pth$"
                ],
                step="HumanParsingStep",
                keywords=["human", "parsing", "schp", "atr", "graphonomy", "densepose", "lip"],
                file_types=['.pth', '.pkl', '.bin'],
                size_range_mb=(10, 1000),
                priority=1,
                architecture=ModelArchitecture.CNN,
                context_paths=["human_parsing", "parsing", "step_01"],
                model_class="GraphonomyModel",
                loader_config={
                    "input_size": (512, 512),
                    "num_classes": 20,
                    "device": "auto",
                    "precision": "fp16"
                },
                step_requirements={
                    "primary_model": True,
                    "required": True,
                    "fallback_enabled": True
                }
            ),
            
            "pose_estimation": EnhancedModelPattern(
                name="pose_estimation",
                patterns=[
                    r".*openpose.*body.*\.pth$",
                    r".*body_pose_model.*\.pth$",
                    r".*pose.*estimation.*\.pth$",
                    r".*mediapipe.*pose.*\.pth$",
                    r".*hrnet.*pose.*\.pth$",
                    r".*openpose.*\.pth$",
                    r".*pose.*\.pth$"
                ],
                step="PoseEstimationStep",
                keywords=["pose", "openpose", "body", "keypoint", "mediapipe", "hrnet"],
                file_types=['.pth', '.onnx', '.bin'],
                size_range_mb=(5, 500),
                priority=2,
                architecture=ModelArchitecture.CNN,
                context_paths=["pose", "openpose", "step_02"],
                model_class="OpenPoseModel",
                loader_config={
                    "input_size": (368, 368),
                    "num_classes": 18,
                    "device": "auto",
                    "precision": "fp16"
                }
            ),
            
            "cloth_segmentation": EnhancedModelPattern(
                name="cloth_segmentation",
                patterns=[
                    r".*u2net.*\.pth$",
                    r".*cloth.*segmentation.*\.pth$",
                    r".*sam.*vit.*\.pth$",
                    r".*rembg.*\.pth$",
                    r".*segmentation.*\.pth$",
                    r".*mask.*\.pth$"
                ],
                step="ClothSegmentationStep",
                keywords=["u2net", "segmentation", "cloth", "mask", "sam", "rembg"],
                file_types=['.pth', '.bin', '.safetensors'],
                size_range_mb=(10, 3000),
                priority=1,
                architecture=ModelArchitecture.UNET,
                context_paths=["segmentation", "cloth", "u2net", "step_03"],
                model_class="U2NetModel",
                loader_config={
                    "input_size": (320, 320),
                    "device": "auto",
                    "precision": "fp16"
                }
            ),
            
            "virtual_fitting": EnhancedModelPattern(
                name="virtual_fitting",
                patterns=[
                    r".*ootd.*diffusion.*\.bin$",
                    r".*stable.*diffusion.*\.safetensors$",
                    r".*diffusion_pytorch_model\.bin$",
                    r".*unet.*\.bin$",
                    r".*vae.*\.safetensors$",
                    r".*virtual.*fitting.*\.pth$"
                ],
                step="VirtualFittingStep",
                keywords=["diffusion", "ootd", "stable", "unet", "vae", "viton", "virtual"],
                file_types=['.bin', '.safetensors', '.pth'],
                size_range_mb=(100, 8000),
                priority=1,
                architecture=ModelArchitecture.DIFFUSION,
                context_paths=["diffusion", "ootd", "virtual", "stable", "step_06"],
                model_class="StableDiffusionPipeline",
                loader_config={
                    "input_size": (512, 512),
                    "device": "auto",
                    "precision": "fp16",
                    "enable_attention_slicing": True
                }
            ),
            
            "geometric_matching": EnhancedModelPattern(
                name="geometric_matching",
                patterns=[
                    r".*gmm.*\.pth$",
                    r".*geometric.*matching.*\.pth$",
                    r".*tps.*\.pth$",
                    r".*warp.*\.pth$"
                ],
                step="GeometricMatchingStep",
                keywords=["gmm", "geometric", "matching", "tps", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(10, 300),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["geometric", "gmm", "step_04"],
                model_class="GeometricMatchingModel"
            ),
            
            "cloth_warping": EnhancedModelPattern(
                name="cloth_warping",
                patterns=[
                    r".*tom.*\.pth$",
                    r".*warping.*\.pth$",
                    r".*flow.*\.pth$",
                    r".*cloth.*warp.*\.pth$"
                ],
                step="ClothWarpingStep",
                keywords=["tom", "warping", "flow", "warp"],
                file_types=['.pth', '.bin'],
                size_range_mb=(20, 400),
                priority=3,
                architecture=ModelArchitecture.CNN,
                context_paths=["warping", "tom", "step_05"],
                model_class="ClothWarpingModel"
            )
        }
    
    def match_file_to_patterns(self, file_path: Path) -> List[Tuple[str, float]]:
        """ÌååÏùºÏùÑ Ìå®ÌÑ¥Ïóê Îß§Ïπ≠ÌïòÍ≥† Ïã†Î¢∞ÎèÑ Ï†êÏàò Î∞òÌôò"""
        matches = []
        
        file_name = file_path.name.lower()
        path_str = str(file_path).lower()
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
        except OSError:
            file_size_mb = 0
        
        for pattern_name, pattern in self.patterns.items():
            confidence = self._calculate_pattern_confidence(
                file_path, file_name, path_str, file_size_mb, pattern
            )
            
            if confidence > 0.05:  # ÎÇÆÏùÄ ÏûÑÍ≥ÑÍ∞í
                matches.append((pattern_name, confidence))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
    
    def _calculate_pattern_confidence(self, file_path: Path, file_name: str, 
                                    path_str: str, file_size_mb: float, 
                                    pattern: EnhancedModelPattern) -> float:
        """Ìå®ÌÑ¥ Îß§Ïπ≠ Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        confidence = 0.0
        
        # 1. Ï†ïÍ∑úÏãù Ìå®ÌÑ¥ Îß§Ïπ≠ (30%)
        pattern_matches = sum(1 for regex_pattern in pattern.patterns 
                            if re.search(regex_pattern, file_name, re.IGNORECASE) or 
                               re.search(regex_pattern, path_str, re.IGNORECASE))
        if pattern_matches > 0:
            confidence += 0.3 * min(pattern_matches / len(pattern.patterns), 1.0)
        
        # 2. ÌÇ§ÏõåÎìú Îß§Ïπ≠ (25%)
        keyword_matches = sum(1 for keyword in pattern.keywords 
                            if keyword in file_name or keyword in path_str)
        if keyword_matches > 0:
            confidence += 0.25 * min(keyword_matches / len(pattern.keywords), 1.0)
        
        # 3. ÌååÏùº ÌôïÏû•Ïûê (20%)
        if file_path.suffix.lower() in pattern.file_types:
            confidence += 0.20
        
        # 4. ÌååÏùº ÌÅ¨Í∏∞ Ï†ÅÌï©ÏÑ± (15%)
        min_size, max_size = pattern.size_range_mb
        tolerance = 0.5  # 50% ÌóàÏö© Ïò§Ï∞®
        effective_min = min_size * (1 - tolerance)
        effective_max = max_size * (1 + tolerance)
        
        if effective_min <= file_size_mb <= effective_max:
            confidence += 0.15
        elif file_size_mb > effective_min * 0.5:
            confidence += 0.08
        
        # 5. Í≤ΩÎ°ú Ïª®ÌÖçÏä§Ìä∏ (10%)
        context_matches = sum(1 for context in pattern.context_paths 
                            if context in path_str)
        if context_matches > 0:
            confidence += 0.10 * min(context_matches / len(pattern.context_paths), 1.0)
        
        return min(confidence, 1.0)

# ==============================================
# üî• ÌååÏùº Ïä§Ï∫êÎÑà Î™®Îìà (Í∏∞Ï°¥ Ïú†ÏßÄ)
# ==============================================

class FileScanner:
    """AI Î™®Îç∏ ÌååÏùº Ïä§Ï∫êÎÑà"""
    
    def __init__(self, enable_deep_scan: bool = True, max_depth: int = 10):
        self.enable_deep_scan = enable_deep_scan
        self.max_depth = max_depth
        self.logger = logging.getLogger(f"{__name__}.FileScanner")
        
        self.model_extensions = {
            '.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.pickle',
            '.h5', '.hdf5', '.pb', '.tflite', '.onnx', '.mlmodel', '.engine'
        }
        
        self.excluded_dirs = {
            '__pycache__', '.git', 'node_modules', '.vscode', '.idea',
            '.pytest_cache', '.mypy_cache', '.DS_Store', 'build', 'dist'
        }
    
    def scan_paths(self, search_paths: List[Path]) -> List[Path]:
        """Ïó¨Îü¨ Í≤ΩÎ°úÏóêÏÑú Î™®Îç∏ ÌååÏùº Ïä§Ï∫î"""
        all_model_files = []
        
        for search_path in search_paths:
            if search_path.exists() and search_path.is_dir():
                try:
                    model_files = self._scan_directory(search_path, current_depth=0)
                    all_model_files.extend(model_files)
                    self.logger.debug(f"üìÅ {search_path}: {len(model_files)}Í∞ú ÌååÏùº")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ïä§Ï∫î Ïã§Ìå® {search_path}: {e}")
        
        unique_files = list(set(all_model_files))
        self.logger.info(f"üìä Ï¥ù Ïä§Ï∫î: {len(unique_files)}Í∞ú Î™®Îç∏ ÌååÏùº")
        return unique_files
    
    def _scan_directory(self, directory: Path, current_depth: int = 0) -> List[Path]:
        """Îã®Ïùº ÎîîÎ†âÌÜ†Î¶¨ Ïä§Ï∫î"""
        model_files = []
        
        if current_depth > self.max_depth:
            return model_files
        
        try:
            items = list(directory.iterdir())
        except (PermissionError, OSError):
            return model_files
        
        for item in items:
            try:
                if item.is_file():
                    if self._is_model_file(item):
                        model_files.append(item)
                elif item.is_dir() and self.enable_deep_scan:
                    if item.name not in self.excluded_dirs:
                        sub_files = self._scan_directory(item, current_depth + 1)
                        model_files.extend(sub_files)
            except Exception:
                continue
        
        return model_files
    
    def _is_model_file(self, file_path: Path) -> bool:
        """AI Î™®Îç∏ ÌååÏùºÏù∏ÏßÄ ÌôïÏù∏"""
        try:
            # ÌôïÏû•Ïûê Ï≤¥ÌÅ¨
            if file_path.suffix.lower() not in self.model_extensions:
                return False
            
            # ÌååÏùº ÌÅ¨Í∏∞ Ï≤¥ÌÅ¨
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb < 0.1 or file_size_mb > 20480:  # 0.1MB ~ 20GB
                return False
            
            # AI Í¥ÄÎ†® ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
            file_name = file_path.name.lower()
            path_str = str(file_path).lower()
            
            ai_keywords = [
                'model', 'checkpoint', 'weight', 'pytorch_model', 'diffusion',
                'stable', 'unet', 'transformer', 'bert', 'clip', 'pose',
                'parsing', 'segmentation', 'virtual', 'fitting'
            ]
            
            has_keyword = any(keyword in file_name for keyword in ai_keywords)
            path_indicators = ['models', 'checkpoints', 'weights', 'huggingface']
            has_path_indicator = any(indicator in path_str for indicator in path_indicators)
            
            return has_keyword or has_path_indicator or file_size_mb > 10
            
        except Exception:
            return False

# ==============================================
# üî• PyTorch Í≤ÄÏ¶ù Î™®Îìà (Í∏∞Ï°¥ Ïú†ÏßÄ)
# ==============================================

class PyTorchValidator:
    """PyTorch Î™®Îç∏ Í≤ÄÏ¶ùÍ∏∞"""
    
    def __init__(self, enable_validation: bool = True, timeout: int = 120):
        self.enable_validation = enable_validation
        self.timeout = timeout
        self.logger = logging.getLogger(f"{__name__}.PyTorchValidator")
    
    def validate_model(self, file_path: Path) -> Dict[str, Any]:
        """PyTorch Î™®Îç∏ Í≤ÄÏ¶ù"""
        if not self.enable_validation or not TORCH_AVAILABLE:
            return {
                'valid': False,
                'parameter_count': 0,
                'validation_info': {"validation_disabled": True},
                'model_structure': {},
                'architecture': ModelArchitecture.UNKNOWN
            }
        
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb > 5000:  # 5GB Ïù¥ÏÉÅ Í±¥ÎÑàÎõ∞Í∏∞
                return {
                    'valid': True,
                    'parameter_count': int(file_size_mb * 1000000),
                    'validation_info': {"large_file_skipped": True},
                    'model_structure': {},
                    'architecture': ModelArchitecture.UNKNOWN
                }
            
            checkpoint = self._safe_load_checkpoint(file_path)
            if checkpoint is None:
                return self._create_failed_result("load_failed")
            
            validation_info = {}
            parameter_count = 0
            architecture = ModelArchitecture.UNKNOWN
            
            if isinstance(checkpoint, dict):
                state_dict = self._extract_state_dict(checkpoint)
                if state_dict:
                    parameter_count = self._count_parameters(state_dict)
                    validation_info.update(self._analyze_layers(state_dict))
                    architecture = self._detect_architecture(state_dict)
            
            return {
                'valid': True,
                'parameter_count': parameter_count,
                'validation_info': validation_info,
                'model_structure': {},
                'architecture': architecture
            }
            
        except Exception as e:
            return self._create_failed_result(str(e)[:200])
        finally:
            self._safe_memory_cleanup()
    
    def _safe_load_checkpoint(self, file_path: Path):
        """ÏïàÏ†ÑÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"""
        try:
            return torch.load(file_path, map_location='cpu', weights_only=True)
        except Exception:
            try:
                return torch.load(file_path, map_location='cpu')
            except Exception:
                return None
    
    def _extract_state_dict(self, checkpoint):
        """state_dict Ï∂îÏ∂ú"""
        if isinstance(checkpoint, dict):
            for key in ['state_dict', 'model', 'model_state_dict', 'net']:
                if key in checkpoint and isinstance(checkpoint[key], dict):
                    return checkpoint[key]
            if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
                return checkpoint
        return None
    
    def _count_parameters(self, state_dict: Dict) -> int:
        """ÌååÎùºÎØ∏ÌÑ∞ Ïàò Í≥ÑÏÇ∞"""
        try:
            return sum(tensor.numel() for tensor in state_dict.values() 
                      if torch.is_tensor(tensor))
        except Exception:
            return 0
    
    def _analyze_layers(self, state_dict: Dict) -> Dict[str, Any]:
        """Î†àÏù¥Ïñ¥ Î∂ÑÏÑù"""
        layer_types = {}
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                layer_types['convolution'] = layer_types.get('convolution', 0) + 1
            elif 'norm' in key_lower or 'bn' in key_lower:
                layer_types['normalization'] = layer_types.get('normalization', 0) + 1
            elif 'linear' in key_lower or 'fc' in key_lower:
                layer_types['linear'] = layer_types.get('linear', 0) + 1
        
        return {
            "total_layers": len(state_dict),
            "layer_types": layer_types
        }
    
    def _detect_architecture(self, state_dict: Dict) -> ModelArchitecture:
        """ÏïÑÌÇ§ÌÖçÏ≤ò ÌÉêÏßÄ"""
        all_keys = ' '.join(state_dict.keys()).lower()
        
        if 'unet' in all_keys or 'down_block' in all_keys:
            return ModelArchitecture.UNET
        elif 'transformer' in all_keys or 'attention' in all_keys:
            return ModelArchitecture.TRANSFORMER
        elif 'diffusion' in all_keys:
            return ModelArchitecture.DIFFUSION
        elif 'conv' in all_keys:
            return ModelArchitecture.CNN
        else:
            return ModelArchitecture.UNKNOWN
    
    def _create_failed_result(self, error: str) -> Dict[str, Any]:
        """Ïã§Ìå® Í≤∞Í≥º ÏÉùÏÑ±"""
        return {
            'valid': False,
            'parameter_count': 0,
            'validation_info': {"error": error},
            'model_structure': {},
            'architecture': ModelArchitecture.UNKNOWN
        }
    
    def _safe_memory_cleanup(self):
        """ÏïàÏ†ÑÌïú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            if TORCH_AVAILABLE and DEVICE_TYPE == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass

# ==============================================
# üî• Í≤ΩÎ°ú ÌÉêÏßÄ Î™®Îìà (Í∏∞Ï°¥ Ïú†ÏßÄ)
# ==============================================

class PathFinder:
    """Í≤ÄÏÉâ Í≤ΩÎ°ú ÏûêÎèô ÌÉêÏßÄ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PathFinder")
    
    def get_search_paths(self) -> List[Path]:
        """Ìè¨Í¥ÑÏ†ÅÏù∏ Í≤ÄÏÉâ Í≤ΩÎ°ú ÏÉùÏÑ±"""
        try:
            current_file = Path(__file__).resolve()
            project_paths = self._get_project_paths(current_file)
            conda_paths = self._get_conda_paths()
            cache_paths = self._get_cache_paths()
            user_paths = self._get_user_paths()
            
            all_paths = project_paths + conda_paths + cache_paths + user_paths
            
            valid_paths = []
            for path in all_paths:
                try:
                    if path.exists() and path.is_dir() and os.access(path, os.R_OK):
                        valid_paths.append(path.resolve())
                except Exception:
                    continue
            
            unique_paths = []
            seen = set()
            for path in valid_paths:
                if path not in seen:
                    unique_paths.append(path)
                    seen.add(path)
            
            self.logger.info(f"‚úÖ Í≤ÄÏÉâ Í≤ΩÎ°ú: {len(unique_paths)}Í∞ú")
            return unique_paths
            
        except Exception as e:
            self.logger.error(f"Í≤ÄÏÉâ Í≤ΩÎ°ú ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return [Path.cwd()]
    
    def _get_project_paths(self, current_file: Path) -> List[Path]:
        """ÌîÑÎ°úÏ†ùÌä∏ ÎÇ¥ Í≤ΩÎ°úÎì§"""
        try:
            backend_dir = current_file
            for _ in range(5):
                backend_dir = backend_dir.parent
                if backend_dir.name in ['backend', 'mycloset-ai']:
                    break
            
            return [
                backend_dir / "ai_models",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "checkpoints",
                backend_dir / "models",
                backend_dir.parent / "ai_models"
            ]
        except Exception:
            return []
    
    def _get_conda_paths(self) -> List[Path]:
        """conda ÌôòÍ≤Ω Í≤ΩÎ°úÎì§"""
        paths = []
        try:
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                base_path = Path(conda_prefix)
                paths.extend([
                    base_path / "lib" / "python3.11" / "site-packages",
                    base_path / "lib" / "python3.10" / "site-packages",
                    base_path / "models"
                ])
            
            conda_roots = [
                Path.home() / "miniforge3",
                Path.home() / "miniconda3",
                Path.home() / "anaconda3"
            ]
            
            for root in conda_roots:
                if root.exists():
                    paths.extend([
                        root / "pkgs",
                        root / "envs",
                        root / "models"
                    ])
        except Exception:
            pass
        
        return paths
    
    def _get_cache_paths(self) -> List[Path]:
        """Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°úÎì§"""
        home = Path.home()
        return [
            home / ".cache" / "huggingface" / "hub",
            home / ".cache" / "torch" / "hub",
            home / ".cache" / "models"
        ]
    
    def _get_user_paths(self) -> List[Path]:
        """ÏÇ¨Ïö©Ïûê Îã§Ïö¥Î°úÎìú Í≤ΩÎ°úÎì§"""
        home = Path.home()
        return [
            home / "Downloads",
            home / "Documents" / "AI_Models",
            home / "Desktop" / "models"
        ]

# ==============================================
# üî• NEW: ModelLoader Î∏åÎ¶¨ÏßÄ ÌÅ¥ÎûòÏä§ (ÌïµÏã¨ Ïó∞Í≤∞Í≥†Î¶¨)
# ==============================================

class ModelLoaderBridge:
    """
    üîó ÌÉêÏßÄÎêú Î™®Îç∏Í≥º ModelLoader Ïó∞Í≤∞ Î∏åÎ¶¨ÏßÄ (ÌïµÏã¨ Ïó∞Í≤∞Í≥†Î¶¨)
    
    574Í∞ú Î™®Îç∏ ÌÉêÏßÄ ‚Üí Ïã§Ï†ú ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÍ≤å Îì±Î°ùÌïòÎäî ÌïµÏã¨ ÌÅ¥ÎûòÏä§
    """
    
    def __init__(self, model_loader: Optional[Any] = None):
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderBridge")
        self.model_loader = model_loader
        self.registration_stats = {
            "attempted": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0
        }
        
        # ModelLoader Í∞ÄÏ†∏Ïò§Í∏∞
        if model_loader is None and MODEL_LOADER_AVAILABLE:
            try:
                self.model_loader = get_global_model_loader()
                self.logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader Ïó∞Í≤∞ ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ ModelLoader Ïó∞Í≤∞ Ïã§Ìå®: {e}")
                self.model_loader = None
        
        self.available = self.model_loader is not None
    
    def register_detected_models(
        self, 
        detected_models: Dict[str, DetectedModel],
        force_registration: bool = False,
        max_registrations: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        üî• ÌïµÏã¨ Í∏∞Îä•: ÌÉêÏßÄÎêú Î™®Îç∏Îì§ÏùÑ ModelLoaderÏóê Îì±Î°ù
        
        Args:
            detected_models: ÌÉêÏßÄÎêú Î™®Îç∏Îì§
            force_registration: Í∞ïÏ†ú Îì±Î°ù Ïó¨Î∂Ä
            max_registrations: ÏµúÎåÄ Îì±Î°ù Ïàò Ï†úÌïú
            
        Returns:
            Îì±Î°ù Í≤∞Í≥º ÌÜµÍ≥Ñ
        """
        if not self.available:
            self.logger.warning("‚ö†Ô∏è ModelLoader ÏÇ¨Ïö© Î∂àÍ∞ÄÎä• - Îì±Î°ù Í±¥ÎÑàÎúÄ")
            return {"error": "ModelLoader not available"}
        
        try:
            self.logger.info(f"üîó ModelLoaderÏóê {len(detected_models)}Í∞ú Î™®Îç∏ Îì±Î°ù ÏãúÏûë...")
            
            # Ïö∞ÏÑ†ÏàúÏúÑÎ≥ÑÎ°ú Ï†ïÎ†¨
            sorted_models = sorted(
                detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score)
            )
            
            # Îì±Î°ù Ï†úÌïú Ï†ÅÏö©
            if max_registrations:
                sorted_models = sorted_models[:max_registrations]
            
            registered_models = []
            
            for model_name, detected_model in sorted_models:
                try:
                    self.registration_stats["attempted"] += 1
                    
                    # Ïù¥ÎØ∏ Îì±Î°ùÎêú Î™®Îç∏ Ï≤¥ÌÅ¨
                    if (detected_model.model_loader_registered and 
                        not force_registration):
                        self.registration_stats["skipped"] += 1
                        continue
                    
                    # ModelLoaderÏö© ÏÑ§Ï†ï ÏÉùÏÑ±
                    model_config = self._create_model_config(detected_model)
                    
                    # ModelLoaderÏóê Îì±Î°ù
                    registration_success = self._register_to_model_loader(
                        model_name, model_config, detected_model
                    )
                    
                    if registration_success:
                        # Îì±Î°ù ÏÑ±Í≥µ ÎßàÌÇπ
                        detected_model.model_loader_registered = True
                        detected_model.model_loader_name = model_name
                        detected_model.registration_timestamp = time.time()
                        
                        registered_models.append(model_name)
                        self.registration_stats["successful"] += 1
                        
                        self.logger.info(f"‚úÖ Îì±Î°ù ÏÑ±Í≥µ: {model_name}")
                    else:
                        self.registration_stats["failed"] += 1
                        self.logger.warning(f"‚ùå Îì±Î°ù Ïã§Ìå®: {model_name}")
                
                except Exception as e:
                    self.registration_stats["failed"] += 1
                    self.logger.warning(f"‚ùå {model_name} Îì±Î°ù Ï§ë Ïò§Î•ò: {e}")
                    continue
            
            # Îì±Î°ù Í≤∞Í≥º Î∞òÌôò
            result = {
                "success": True,
                "registered_models": registered_models,
                "statistics": self.registration_stats.copy(),
                "total_detected": len(detected_models),
                "total_registered": len(registered_models)
            }
            
            self.logger.info(f"üéØ Îì±Î°ù ÏôÑÎ£å: {len(registered_models)}/{len(detected_models)}Í∞ú ÏÑ±Í≥µ")
            self.logger.info(f"üìä ÏÑ±Í≥µÎ•†: {(len(registered_models)/len(detected_models)*100):.1f}%")
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù ÌîÑÎ°úÏÑ∏Ïä§ Ïã§Ìå®: {e}")
            return {"error": str(e), "statistics": self.registration_stats}
    
    def _create_model_config(self, detected_model: DetectedModel) -> Dict[str, Any]:
        """DetectedModelÏùÑ ModelLoader ÏÑ§Ï†ïÏúºÎ°ú Î≥ÄÌôò"""
        try:
            # Í∏∞Î≥∏ ÏÑ§Ï†ï
            config = {
                "name": detected_model.name,
                "path": str(detected_model.path),
                "model_type": detected_model.model_type,
                "step_name": detected_model.step_name,
                "device": "auto",
                "precision": "fp16" if DEVICE_TYPE != "cpu" else "fp32",
                
                # ÌÉêÏßÄ Ï†ïÎ≥¥
                "confidence_score": detected_model.confidence_score,
                "pytorch_valid": detected_model.pytorch_valid,
                "parameter_count": detected_model.parameter_count,
                "file_size_mb": detected_model.file_size_mb,
                "architecture": detected_model.architecture.value,
                "priority": detected_model.priority.value,
                
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                "detected_by": "auto_model_detector_v9",
                "detection_timestamp": time.time(),
                "health_status": detected_model.health_status,
                "device_compatibility": detected_model.device_compatibility,
                "memory_requirements": detected_model.memory_requirements,
                
                # ModelLoader ÌäπÌôî ÏÑ§Ï†ï
                "enable_caching": True,
                "lazy_loading": detected_model.file_size_mb > 1000,  # 1GB Ïù¥ÏÉÅ
                "optimization_hints": self._generate_optimization_hints(detected_model)
            }
            
            # ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ ÌäπÌôî ÏÑ§Ï†ï
            if detected_model.architecture == ModelArchitecture.DIFFUSION:
                config.update({
                    "enable_attention_slicing": True,
                    "enable_vae_slicing": True,
                    "use_memory_efficient_attention": True
                })
            elif detected_model.architecture == ModelArchitecture.TRANSFORMER:
                config.update({
                    "use_flash_attention": True,
                    "enable_kv_cache": True
                })
            
            return config
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {"name": detected_model.name, "path": str(detected_model.path)}
    
    def _generate_optimization_hints(self, detected_model: DetectedModel) -> List[str]:
        """Î™®Îç∏Î≥Ñ ÏµúÏ†ÅÌôî ÌûåÌä∏ ÏÉùÏÑ±"""
        hints = []
        
        # M3 Max ÏµúÏ†ÅÌôî
        if IS_M3_MAX and detected_model.device_compatibility.get("mps", False):
            hints.extend(["use_mps_device", "enable_neural_engine"])
        
        # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
        if detected_model.file_size_mb > 1000:
            hints.extend(["use_fp16", "enable_gradient_checkpointing", "memory_mapping"])
        
        # ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ ÏµúÏ†ÅÌôî
        if detected_model.architecture == ModelArchitecture.DIFFUSION:
            hints.extend(["attention_slicing", "vae_slicing"])
        elif detected_model.architecture == ModelArchitecture.TRANSFORMER:
            hints.extend(["flash_attention", "kv_caching"])
        
        return hints
    
    def _register_to_model_loader(
        self, 
        model_name: str, 
        model_config: Dict[str, Any], 
        detected_model: DetectedModel
    ) -> bool:
        """Ïã§Ï†ú ModelLoaderÏóê Îì±Î°ù"""
        try:
            if not self.model_loader:
                return False
            
            # ModelLoaderÏùò register_model Î©îÏÑúÎìú ÏÇ¨Ïö©
            if hasattr(self.model_loader, 'register_model'):
                success = self.model_loader.register_model(model_name, model_config)
                if success:
                    self.logger.debug(f"‚úÖ register_model ÏÑ±Í≥µ: {model_name}")
                    return True
            
            # ModelLoaderÏùò register_model_config Î©îÏÑúÎìú ÏÇ¨Ïö©
            if hasattr(self.model_loader, 'register_model_config'):
                success = self.model_loader.register_model_config(model_name, model_config)
                if success:
                    self.logger.debug(f"‚úÖ register_model_config ÏÑ±Í≥µ: {model_name}")
                    return True
            
            # SafeModelService ÏßÅÏ†ë ÏÇ¨Ïö©
            if hasattr(self.model_loader, 'safe_model_service'):
                success = self.model_loader.safe_model_service.register_model(model_name, model_config)
                if success:
                    self.logger.debug(f"‚úÖ safe_model_service Îì±Î°ù ÏÑ±Í≥µ: {model_name}")
                    return True
            
            self.logger.warning(f"‚ö†Ô∏è {model_name}: ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Îì±Î°ù Î©îÏÑúÎìú ÏóÜÏùå")
            return False
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è {model_name} ModelLoader Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def get_registration_stats(self) -> Dict[str, Any]:
        """Îì±Î°ù ÌÜµÍ≥Ñ Î∞òÌôò"""
        return {
            "statistics": self.registration_stats.copy(),
            "success_rate": (self.registration_stats["successful"] / 
                           max(self.registration_stats["attempted"], 1) * 100),
            "model_loader_available": self.available,
            "timestamp": time.time()
        }

# ==============================================
# üî• NEW: Step Î™®Îç∏ Îß§Ï≤ò ÌÅ¥ÎûòÏä§
# ==============================================

class StepModelMatcher:
    """
    üéØ StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô Îß§Ïπ≠ Î∞è Ìï†Îãπ ÌÅ¥ÎûòÏä§
    
    ÌÉêÏßÄÎêú Î™®Îç∏ÏùÑ Ï†ÅÏ†àÌïú StepÏóê ÏûêÎèô Ìï†Îãπ
    """
    
    def __init__(self, model_loader_bridge: ModelLoaderBridge):
        self.bridge = model_loader_bridge
        self.logger = logging.getLogger(f"{__name__}.StepModelMatcher")
        
        # StepÎ≥Ñ Î™®Îç∏ Îß§Ìïë
        self.step_model_mapping = {
            "HumanParsingStep": ["human_parsing"],
            "PoseEstimationStep": ["pose_estimation"],
            "ClothSegmentationStep": ["cloth_segmentation"],
            "GeometricMatchingStep": ["geometric_matching"],
            "ClothWarpingStep": ["cloth_warping"],
            "VirtualFittingStep": ["virtual_fitting"],
            "PostProcessingStep": ["post_processing"],
            "QualityAssessmentStep": ["quality_assessment"]
        }
    
    def assign_models_to_steps(
        self, 
        detected_models: Dict[str, DetectedModel]
    ) -> Dict[str, List[str]]:
        """
        üî• ÌïµÏã¨ Í∏∞Îä•: ÌÉêÏßÄÎêú Î™®Îç∏ÏùÑ StepÎ≥ÑÎ°ú ÏûêÎèô Ìï†Îãπ
        
        Args:
            detected_models: ÌÉêÏßÄÎêú Î™®Îç∏Îì§
            
        Returns:
            StepÎ≥Ñ Ìï†ÎãπÎêú Î™®Îç∏ Î™©Î°ù
        """
        try:
            step_assignments = {}
            unassigned_models = []
            
            self.logger.info(f"üéØ {len(detected_models)}Í∞ú Î™®Îç∏ÏùÑ StepÎ≥ÑÎ°ú Ìï†Îãπ Ï§ë...")
            
            # StepÎ≥Ñ Î™®Îç∏ Î∂ÑÎ•ò
            for model_name, detected_model in detected_models.items():
                assigned = False
                
                # Step Ïù¥Î¶ÑÏúºÎ°ú ÏßÅÏ†ë Îß§Ïπ≠
                if detected_model.step_name in self.step_model_mapping:
                    step_name = detected_model.step_name
                    if step_name not in step_assignments:
                        step_assignments[step_name] = []
                    step_assignments[step_name].append(model_name)
                    assigned = True
                
                # Î™®Îç∏ ÌÉÄÏûÖÏúºÎ°ú Îß§Ïπ≠
                if not assigned:
                    for step_name, model_types in self.step_model_mapping.items():
                        if detected_model.model_type in model_types:
                            if step_name not in step_assignments:
                                step_assignments[step_name] = []
                            step_assignments[step_name].append(model_name)
                            assigned = True
                            break
                
                if not assigned:
                    unassigned_models.append(model_name)
            
            # Í∞Å StepÎ≥ÑÎ°ú ÏµúÏ†Å Î™®Îç∏ ÏÑ†ÌÉù
            optimized_assignments = {}
            for step_name, model_list in step_assignments.items():
                # Ïö∞ÏÑ†ÏàúÏúÑÏôÄ Ïã†Î¢∞ÎèÑÎ°ú Ï†ïÎ†¨
                step_models = [detected_models[name] for name in model_list]
                sorted_models = sorted(
                    step_models,
                    key=lambda x: (x.priority.value, -x.confidence_score, -x.file_size_mb)
                )
                
                # ÏÉÅÏúÑ 3Í∞ú Î™®Îç∏Îßå ÏÑ†ÌÉù (Ï£º Î™®Îç∏ + Ìè¥Î∞± Î™®Îç∏Îì§)
                selected_models = [model.name for model in sorted_models[:3]]
                optimized_assignments[step_name] = selected_models
                
                # StepÎ≥Ñ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Ïóê Ìï†Îãπ ÌëúÏãú
                for model in sorted_models[:3]:
                    model.step_interface_assigned = True
            
            # Í≤∞Í≥º Î°úÍπÖ
            self.logger.info(f"‚úÖ StepÎ≥Ñ Î™®Îç∏ Ìï†Îãπ ÏôÑÎ£å:")
            for step_name, models in optimized_assignments.items():
                self.logger.info(f"   - {step_name}: {len(models)}Í∞ú Î™®Îç∏")
                for i, model_name in enumerate(models):
                    role = "Primary" if i == 0 else f"Fallback{i}"
                    self.logger.info(f"     ‚Ä¢ {role}: {model_name}")
            
            if unassigned_models:
                self.logger.warning(f"‚ö†Ô∏è ÎØ∏Ìï†Îãπ Î™®Îç∏: {len(unassigned_models)}Í∞ú")
                for model_name in unassigned_models[:5]:  # Ï≤òÏùå 5Í∞úÎßå ÌëúÏãú
                    self.logger.warning(f"     ‚Ä¢ {model_name}")
            
            return optimized_assignments
            
        except Exception as e:
            self.logger.error(f"‚ùå StepÎ≥Ñ Î™®Îç∏ Ìï†Îãπ Ïã§Ìå®: {e}")
            return {}
    
    def create_step_interfaces(
        self, 
        step_assignments: Dict[str, List[str]],
        detected_models: Dict[str, DetectedModel]
    ) -> Dict[str, Any]:
        """
        üîó StepÎ≥Ñ ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
        
        Args:
            step_assignments: StepÎ≥Ñ Ìï†ÎãπÎêú Î™®Îç∏Îì§
            detected_models: ÌÉêÏßÄÎêú Î™®Îç∏Îì§
            
        Returns:
            ÏÉùÏÑ±Îêú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ≥¥
        """
        try:
            if not self.bridge.available:
                self.logger.warning("‚ö†Ô∏è ModelLoader Î∏åÎ¶¨ÏßÄ ÏÇ¨Ïö© Î∂àÍ∞ÄÎä•")
                return {}
            
            interfaces_created = {}
            
            for step_name, assigned_models in step_assignments.items():
                try:
                    # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
                    if hasattr(self.bridge.model_loader, 'create_step_interface'):
                        step_interface = self.bridge.model_loader.create_step_interface(
                            step_name=step_name
                        )
                        
                        # Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù
                        for i, model_name in enumerate(assigned_models):
                            detected_model = detected_models.get(model_name)
                            if detected_model:
                                priority = "high" if i == 0 else "medium"
                                fallback_models = assigned_models[i+1:] if i < len(assigned_models)-1 else []
                                
                                # Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù
                                step_interface.register_model_requirement(
                                    model_name=model_name,
                                    model_type=detected_model.model_type,
                                    priority=priority,
                                    fallback_models=fallback_models,
                                    confidence_score=detected_model.confidence_score,
                                    pytorch_valid=detected_model.pytorch_valid
                                )
                        
                        interfaces_created[step_name] = {
                            "interface": step_interface,
                            "models_count": len(assigned_models),
                            "primary_model": assigned_models[0] if assigned_models else None,
                            "fallback_models": assigned_models[1:] if len(assigned_models) > 1 else []
                        }
                        
                        self.logger.info(f"‚úÖ {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
                    continue
            
            self.logger.info(f"üîó {len(interfaces_created)}Í∞ú Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
            return interfaces_created
            
        except Exception as e:
            self.logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {}

# ==============================================
# üî• NEW: ÏûêÎèô Îì±Î°ù Îß§ÎãàÏ†Ä ÌÅ¥ÎûòÏä§
# ==============================================

class AutoRegistrationManager:
    """
    ü§ñ ÏûêÎèô Îì±Î°ù Îß§ÎãàÏ†Ä - Ï†ÑÏ≤¥ ÌîÑÎ°úÏÑ∏Ïä§ ÌÜµÌï© Í¥ÄÎ¶¨
    
    ÌÉêÏßÄ ‚Üí Îì±Î°ù ‚Üí Step Ìï†ÎãπÍπåÏßÄ Ï†ÑÏ≤¥ ÏûêÎèôÌôî
    """
    
    def __init__(self, model_loader: Optional[Any] = None):
        self.logger = logging.getLogger(f"{__name__}.AutoRegistrationManager")
        
        # Î∏åÎ¶¨ÏßÄ Î∞è Îß§Ï≤ò Ï¥àÍ∏∞Ìôî
        self.bridge = ModelLoaderBridge(model_loader)
        self.matcher = StepModelMatcher(self.bridge)
        
        # ÌÜµÍ≥Ñ
        self.process_stats = {
            "detection_start": 0,
            "detection_end": 0,
            "registration_start": 0,
            "registration_end": 0,
            "step_assignment_start": 0,
            "step_assignment_end": 0,
            "total_duration": 0,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "success_rate": 0
        }
    
    def execute_full_pipeline(
        self,
        detected_models: Dict[str, DetectedModel],
        auto_assign_steps: bool = True,
        max_registrations: Optional[int] = None,
        create_step_interfaces: bool = True
    ) -> Dict[str, Any]:
        """
        üöÄ Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ: ÌÉêÏßÄ ‚Üí Îì±Î°ù ‚Üí Step Ìï†Îãπ
        
        Args:
            detected_models: ÌÉêÏßÄÎêú Î™®Îç∏Îì§
            auto_assign_steps: Step ÏûêÎèô Ìï†Îãπ Ïó¨Î∂Ä
            max_registrations: ÏµúÎåÄ Îì±Î°ù Ïàò
            create_step_interfaces: Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïó¨Î∂Ä
            
        Returns:
            Ï†ÑÏ≤¥ ÌîÑÎ°úÏÑ∏Ïä§ Í≤∞Í≥º
        """
        try:
            self.process_stats["detection_start"] = time.time()
            self.logger.info(f"üöÄ ÏûêÎèô Îì±Î°ù ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë: {len(detected_models)}Í∞ú Î™®Îç∏")
            
            # Phase 1: ModelLoader Îì±Î°ù
            self.process_stats["registration_start"] = time.time()
            self.logger.info("üìù Phase 1: ModelLoader Îì±Î°ù Ï§ë...")
            
            registration_result = self.bridge.register_detected_models(
                detected_models=detected_models,
                max_registrations=max_registrations
            )
            
            self.process_stats["registration_end"] = time.time()
            self.process_stats["models_registered"] = registration_result.get("total_registered", 0)
            
            if not registration_result.get("success", False):
                self.logger.error("‚ùå ModelLoader Îì±Î°ù Ïã§Ìå®")
                return {"error": "Registration failed", "details": registration_result}
            
            # Phase 2: StepÎ≥Ñ Ìï†Îãπ
            step_assignments = {}
            step_interfaces = {}
            
            if auto_assign_steps:
                self.process_stats["step_assignment_start"] = time.time()
                self.logger.info("üéØ Phase 2: StepÎ≥Ñ Î™®Îç∏ Ìï†Îãπ Ï§ë...")
                
                step_assignments = self.matcher.assign_models_to_steps(detected_models)
                self.process_stats["steps_configured"] = len(step_assignments)
                
                # Phase 3: Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
                if create_step_interfaces and self.bridge.available:
                    self.logger.info("üîó Phase 3: Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ï§ë...")
                    step_interfaces = self.matcher.create_step_interfaces(
                        step_assignments, detected_models
                    )
                
                self.process_stats["step_assignment_end"] = time.time()
            
            # Ï†ÑÏ≤¥ ÌîÑÎ°úÏÑ∏Ïä§ ÏôÑÎ£å
            self.process_stats["detection_end"] = time.time()
            self.process_stats["total_duration"] = (
                self.process_stats["detection_end"] - self.process_stats["detection_start"]
            )
            self.process_stats["models_detected"] = len(detected_models)
            self.process_stats["success_rate"] = (
                self.process_stats["models_registered"] / 
                max(self.process_stats["models_detected"], 1) * 100
            )
            
            # ÏµúÏ¢Ö Í≤∞Í≥º
            result = {
                "success": True,
                "pipeline_completed": True,
                "statistics": self.process_stats.copy(),
                "registration_result": registration_result,
                "step_assignments": step_assignments,
                "step_interfaces_created": len(step_interfaces),
                "models_processing": {
                    "detected": len(detected_models),
                    "registered": self.process_stats["models_registered"],
                    "assigned_to_steps": sum(len(models) for models in step_assignments.values()),
                    "success_rate": self.process_stats["success_rate"]
                },
                "performance": {
                    "total_duration_sec": self.process_stats["total_duration"],
                    "registration_time_sec": (
                        self.process_stats["registration_end"] - 
                        self.process_stats["registration_start"]
                    ),
                    "step_assignment_time_sec": (
                        self.process_stats["step_assignment_end"] - 
                        self.process_stats["step_assignment_start"]
                    ) if auto_assign_steps else 0
                }
            }
            
            # ÏÑ±Í≥º Î°úÍπÖ
            self.logger.info(f"üéâ ÏûêÎèô Îì±Î°ù ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å!")
            self.logger.info(f"   üìä Ï≤òÎ¶¨ Í≤∞Í≥º:")
            self.logger.info(f"     ‚Ä¢ ÌÉêÏßÄ: {len(detected_models)}Í∞ú")
            self.logger.info(f"     ‚Ä¢ Îì±Î°ù: {self.process_stats['models_registered']}Í∞ú")
            self.logger.info(f"     ‚Ä¢ Step Íµ¨ÏÑ±: {self.process_stats['steps_configured']}Í∞ú")
            self.logger.info(f"     ‚Ä¢ ÏÑ±Í≥µÎ•†: {self.process_stats['success_rate']:.1f}%")
            self.logger.info(f"     ‚Ä¢ ÏÜåÏöîÏãúÍ∞Ñ: {self.process_stats['total_duration']:.2f}Ï¥à")
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏûêÎèô Îì±Î°ù ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "statistics": self.process_stats,
                "pipeline_completed": False
            }

# ==============================================
# üî• Î©îÏù∏ ÌÉêÏßÄÍ∏∞ ÌÅ¥ÎûòÏä§ (ModelLoader Ïó∞Îèô Í∞ïÌôî)
# ==============================================

class RealWorldModelDetector:
    """
    üîç Ïã§Ï†ú ÎèôÏûëÌïòÎäî AI Î™®Îç∏ ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú v9.0 - ModelLoader Ïó∞Îèô ÏôÑÏÑ±
    
    ‚úÖ 574Í∞ú Î™®Îç∏ ÌÉêÏßÄ ‚Üí ModelLoader Îì±Î°ùÍπåÏßÄ ÏôÑÏ†Ñ ÏûêÎèôÌôî
    ‚úÖ StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô Ìï†Îãπ Î∞è Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
    ‚úÖ Ïã§ÏãúÍ∞Ñ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Î°ú Îì±Î°ù ÏôÑÎ£å
    ‚úÖ PipelineManagerÏôÄ ÏôÑÏ†Ñ Ïó∞Îèô
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_pytorch_validation: bool = False,
        enable_auto_registration: bool = True,  # üî• NEW: ÏûêÎèô Îì±Î°ù ÌôúÏÑ±Ìôî
        enable_step_assignment: bool = True,    # üî• NEW: Step Ìï†Îãπ ÌôúÏÑ±Ìôî
        model_loader: Optional[Any] = None,     # üî• NEW: ModelLoader Ïó∞Îèô
        max_workers: int = 1,
        scan_timeout: int = 600,
        **kwargs
    ):
        """ÌÉêÏßÄÍ∏∞ Ï¥àÍ∏∞Ìôî (ModelLoader Ïó∞Îèô Í∞ïÌôî)"""
        
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.enable_deep_scan = enable_deep_scan
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_auto_registration = enable_auto_registration
        self.enable_step_assignment = enable_step_assignment
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # Î™®Îìà Ï¥àÍ∏∞Ìôî
        self.path_finder = PathFinder()
        self.file_scanner = FileScanner(enable_deep_scan=enable_deep_scan)
        self.pattern_matcher = PatternMatcher()
        self.pytorch_validator = PyTorchValidator(
            enable_validation=enable_pytorch_validation,
            timeout=kwargs.get('validation_timeout', 60)
        )
        
        # üî• NEW: ModelLoader Ïó∞Îèô Ïª¥Ìè¨ÎÑåÌä∏
        self.auto_registration_manager = AutoRegistrationManager(model_loader)
        
        # Í≤ÄÏÉâ Í≤ΩÎ°ú ÏÑ§Ï†ï
        if search_paths is None:
            self.search_paths = self.path_finder.get_search_paths()
        else:
            self.search_paths = search_paths
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self.detected_models: Dict[str, DetectedModel] = {}
        self.registration_results: Dict[str, Any] = {}
        self.step_assignments: Dict[str, List[str]] = {}
        
        # ÌÜµÍ≥Ñ
        self.scan_stats = {
            "total_files_scanned": 0,
            "model_files_found": 0,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "pytorch_validated": 0,
            "scan_duration": 0.0,
            "registration_duration": 0.0,
            "cache_hits": 0,
            "errors_encountered": 0
        }
        
        self.logger.info(f"üîç Í∞ïÌôîÎêú Î™®Îç∏ ÌÉêÏßÄÍ∏∞ v9.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"   - Í≤ÄÏÉâ Í≤ΩÎ°ú: {len(self.search_paths)}Í∞ú")
        self.logger.info(f"   - ÎîîÎ∞îÏù¥Ïä§: {DEVICE_TYPE}")
        self.logger.info(f"   - ModelLoader Ïó∞Îèô: {'‚úÖ' if self.auto_registration_manager.bridge.available else '‚ùå'}")
        self.logger.info(f"   - ÏûêÎèô Îì±Î°ù: {'ÌôúÏÑ±Ìôî' if enable_auto_registration else 'ÎπÑÌôúÏÑ±Ìôî'}")
        self.logger.info(f"   - Step Ìï†Îãπ: {'ÌôúÏÑ±Ìôî' if enable_step_assignment else 'ÎπÑÌôúÏÑ±Ìôî'}")
    
    def detect_all_models(
        self,
        force_rescan: bool = True,
        min_confidence: float = 0.05,
        categories_filter: Optional[List[ModelCategory]] = None,
        enable_detailed_analysis: bool = False,
        max_models_per_category: Optional[int] = None,
        auto_register_to_model_loader: bool = True,  # üî• NEW: ÏûêÎèô Îì±Î°ù Ï†úÏñ¥
        max_registrations: Optional[int] = None       # üî• NEW: Îì±Î°ù Ïàò Ï†úÌïú
    ) -> Dict[str, DetectedModel]:
        """
        üî• ÏôÑÏ†Ñ Í∞ïÌôîÎêú Î™®Îç∏ ÌÉêÏßÄ + ModelLoader Îì±Î°ù ÌÜµÌï©
        
        Args:
            force_rescan: Ï∫êÏãú Î¨¥ÏãúÌïòÍ≥† Ïû¨Ïä§Ï∫î
            min_confidence: ÏµúÏÜå Ïã†Î¢∞ÎèÑ (0.05Î°ú ÏôÑÌôî)
            categories_filter: ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Îßå ÌÉêÏßÄ
            enable_detailed_analysis: ÏÉÅÏÑ∏ Î∂ÑÏÑù
            max_models_per_category: Ïπ¥ÌÖåÍ≥†Î¶¨Îãπ ÏµúÎåÄ Î™®Îç∏ Ïàò
            auto_register_to_model_loader: üî• ÏûêÎèô ModelLoader Îì±Î°ù Ïó¨Î∂Ä
            max_registrations: üî• ÏµúÎåÄ Îì±Î°ù Ïàò Ï†úÌïú
        
        Returns:
            ÌÉêÏßÄÎêú Î™®Îç∏Îì§ (ModelLoader Îì±Î°ù ÏÉÅÌÉú Ìè¨Ìï®)
        """
        try:
            self.logger.info("üîç Í∞ïÌôîÎêú Î™®Îç∏ ÌÉêÏßÄ + ModelLoader Îì±Î°ù ÏãúÏûë...")
            start_time = time.time()
            
            # ÌÜµÍ≥Ñ Ï¥àÍ∏∞Ìôî
            self._reset_scan_stats()
            
            # Phase 1: Î™®Îç∏ ÌååÏùº Ïä§Ï∫î
            self.logger.info("üìÅ Phase 1: Î™®Îç∏ ÌååÏùº Ïä§Ï∫î Ï§ë...")
            model_files = self.file_scanner.scan_paths(self.search_paths)
            self.scan_stats["total_files_scanned"] = len(model_files)
            
            if not model_files:
                self.logger.warning("‚ùå Î™®Îç∏ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                return {}
            
            # Phase 2: Ìå®ÌÑ¥ Îß§Ïπ≠ Î∞è Î∂ÑÎ•ò
            self.logger.info(f"üîç Phase 2: {len(model_files)}Í∞ú ÌååÏùº Î∂ÑÎ•ò Ï§ë...")
            detected_count = 0
            
            for file_path in model_files:
                try:
                    # Ìå®ÌÑ¥ Îß§Ïπ≠
                    matches = self.pattern_matcher.match_file_to_patterns(file_path)
                    
                    if matches and matches[0][1] >= min_confidence:
                        pattern_name, confidence = matches[0]
                        pattern = self.pattern_matcher.patterns[pattern_name]
                        
                        # ÌÉêÏßÄÎêú Î™®Îç∏ ÏÉùÏÑ±
                        detected_model = self._create_detected_model(
                            file_path, pattern_name, pattern, confidence, enable_detailed_analysis
                        )
                        
                        if detected_model:
                            # Ïπ¥ÌÖåÍ≥†Î¶¨ ÌïÑÌÑ∞ Ï†ÅÏö©
                            if categories_filter and detected_model.category not in categories_filter:
                                continue
                            
                            self.detected_models[detected_model.name] = detected_model
                            detected_count += 1
                            
                            if detected_count <= 20:  # Ï≤òÏùå 20Í∞úÎßå Î°úÍ∑∏
                                self.logger.info(f"‚úÖ {detected_model.name} ({detected_model.file_size_mb:.1f}MB)")
                
                except Exception as e:
                    self.logger.debug(f"ÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå® {file_path}: {e}")
                    self.scan_stats["errors_encountered"] += 1
                    continue
            
            # Phase 3: ÌõÑÏ≤òÎ¶¨
            if max_models_per_category:
                self._limit_models_per_category(max_models_per_category)
            
            self._post_process_results(min_confidence)
            
            # Ïä§Ï∫î ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            
            self.logger.info(f"‚úÖ Phase 2 ÏôÑÎ£å: {len(self.detected_models)}Í∞ú Î™®Îç∏ ÌÉêÏßÄ")
            
            # üî• Phase 4: ModelLoader ÏûêÎèô Îì±Î°ù (ÌïµÏã¨ Í∏∞Îä•)
            if auto_register_to_model_loader and self.enable_auto_registration:
                self.logger.info("üîó Phase 3: ModelLoader ÏûêÎèô Îì±Î°ù ÏãúÏûë...")
                registration_start = time.time()
                
                self.registration_results = self.auto_registration_manager.execute_full_pipeline(
                    detected_models=self.detected_models,
                    auto_assign_steps=self.enable_step_assignment,
                    max_registrations=max_registrations,
                    create_step_interfaces=True
                )
                
                registration_duration = time.time() - registration_start
                self.scan_stats["registration_duration"] = registration_duration
                
                # Îì±Î°ù Í≤∞Í≥º Î∞òÏòÅ
                if self.registration_results.get("success", False):
                    self.scan_stats["models_registered"] = self.registration_results["models_processing"]["registered"]
                    self.scan_stats["steps_configured"] = self.registration_results["models_processing"].get("assigned_to_steps", 0)
                    self.step_assignments = self.registration_results.get("step_assignments", {})
                    
                    self.logger.info(f"üéâ ModelLoader Îì±Î°ù ÏôÑÎ£å!")
                    self.logger.info(f"   - Îì±Î°ùÎêú Î™®Îç∏: {self.scan_stats['models_registered']}Í∞ú")
                    self.logger.info(f"   - Íµ¨ÏÑ±Îêú Step: {len(self.step_assignments)}Í∞ú")
                    self.logger.info(f"   - Îì±Î°ù ÏÜåÏöîÏãúÍ∞Ñ: {registration_duration:.2f}Ï¥à")
                else:
                    self.logger.warning(f"‚ö†Ô∏è ModelLoader Îì±Î°ù Î∂ÄÎ∂Ñ Ïã§Ìå®")
                    self.logger.warning(f"   Ïò§Î•ò: {self.registration_results.get('error', 'Unknown')}")
            else:
                self.logger.info("üìã ModelLoader ÏûêÎèô Îì±Î°ù Í±¥ÎÑàÎúÄ (ÎπÑÌôúÏÑ±ÌôîÎê®)")
            
            # ÏµúÏ¢Ö ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self.scan_stats["scan_duration"] = time.time() - start_time
            
            self.logger.info(f"üéØ Ï†ÑÏ≤¥ ÌîÑÎ°úÏÑ∏Ïä§ ÏôÑÎ£å!")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÌÉêÏßÄ + Îì±Î°ù Ïã§Ìå®: {e}")
            self.logger.debug(traceback.format_exc())
            raise
    
    def _create_detected_model(
        self, 
        file_path: Path, 
        pattern_name: str, 
        pattern: EnhancedModelPattern, 
        confidence: float,
        enable_detailed_analysis: bool
    ) -> Optional[DetectedModel]:
        """ÌÉêÏßÄÎêú Î™®Îç∏ Í∞ùÏ≤¥ ÏÉùÏÑ± (ModelLoader Ïó∞Îèô Ï†ïÎ≥¥ Ìè¨Ìï®)"""
        try:
            # Í∏∞Î≥∏ ÌååÏùº Ï†ïÎ≥¥
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # Ïπ¥ÌÖåÍ≥†Î¶¨ Îß§Ìïë
            category_mapping = {
                "human_parsing": ModelCategory.HUMAN_PARSING,
                "pose_estimation": ModelCategory.POSE_ESTIMATION,
                "cloth_segmentation": ModelCategory.CLOTH_SEGMENTATION,
                "geometric_matching": ModelCategory.GEOMETRIC_MATCHING,
                "cloth_warping": ModelCategory.CLOTH_WARPING,
                "virtual_fitting": ModelCategory.VIRTUAL_FITTING,
                "post_processing": ModelCategory.POST_PROCESSING,
                "quality_assessment": ModelCategory.QUALITY_ASSESSMENT
            }
            
            category = category_mapping.get(pattern_name, ModelCategory.AUXILIARY)
            priority = ModelPriority(min(pattern.priority, 5))
            
            # Í≥†Ïú† Î™®Îç∏ Ïù¥Î¶Ñ ÏÉùÏÑ±
            model_name = self._generate_model_name(file_path, pattern_name)
            
            # PyTorch Í≤ÄÏ¶ù (ÏÑ†ÌÉùÏ†Å)
            validation_results = {}
            pytorch_valid = False
            parameter_count = 0
            architecture = pattern.architecture
            
            if self.enable_pytorch_validation and enable_detailed_analysis:
                validation_result = self.pytorch_validator.validate_model(file_path)
                validation_results = validation_result['validation_info']
                pytorch_valid = validation_result['valid']
                parameter_count = validation_result['parameter_count']
                if validation_result['architecture'] != ModelArchitecture.UNKNOWN:
                    architecture = validation_result['architecture']
                
                if pytorch_valid:
                    self.scan_stats["pytorch_validated"] += 1
            
            # ÏÑ±Îä• Î©îÌä∏Î¶≠
            performance_metrics = ModelPerformanceMetrics(
                inference_time_ms=self._estimate_inference_time(file_size_mb, pattern.architecture),
                memory_usage_mb=file_size_mb * 2.5,
                m3_compatibility_score=0.8 if IS_M3_MAX else 0.5
            )
            
            # ÎîîÎ∞îÏù¥Ïä§ Ìò∏ÌôòÏÑ±
            device_compatibility = {
                "cpu": True,
                "mps": IS_M3_MAX and file_size_mb < 8000,
                "cuda": False
            }
            
            # DetectedModel ÏÉùÏÑ± (ModelLoader Ïó∞Îèô Ï†ïÎ≥¥ Ìè¨Ìï®)
            detected_model = DetectedModel(
                name=model_name,
                path=file_path,
                category=category,
                model_type=pattern.name,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=priority,
                step_name=pattern.step,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count,
                last_modified=file_stat.st_mtime,
                architecture=architecture,
                performance_metrics=performance_metrics,
                device_compatibility=device_compatibility,
                validation_results=validation_results,
                health_status="healthy" if pytorch_valid or confidence > 0.7 else "unknown",
                metadata={
                    "pattern_matched": pattern_name,
                    "confidence_score": confidence,
                    "file_extension": file_path.suffix,
                    "detected_at": time.time(),
                    # üî• NEW: ModelLoader Ïó∞Îèô Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                    "model_class": pattern.model_class,
                    "loader_config": pattern.loader_config,
                    "step_requirements": pattern.step_requirements
                },
                # üî• NEW: ModelLoader Ïó∞Îèô ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
                model_loader_registered=False,
                model_loader_name=None,
                step_interface_assigned=False,
                registration_timestamp=None
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå® {file_path}: {e}")
            return None
    
    def _generate_model_name(self, file_path: Path, pattern_name: str) -> str:
        """Í≥†Ïú† Î™®Îç∏ Ïù¥Î¶Ñ ÏÉùÏÑ±"""
        try:
            # Í∏∞Î≥∏ Ïù¥Î¶Ñ
            base_name = f"{pattern_name}_{file_path.stem}"
            
            # Ï§ëÎ≥µ ÌôïÏù∏
            if base_name not in self.detected_models:
                return base_name
            
            # Î≤ÑÏ†Ñ Î≤àÌò∏ Ï∂îÍ∞Ä
            counter = 2
            while f"{base_name}_v{counter}" in self.detected_models:
                counter += 1
            
            return f"{base_name}_v{counter}"
            
        except Exception:
            return f"detected_model_{int(time.time())}"
    
    def _estimate_inference_time(self, file_size_mb: float, architecture: ModelArchitecture) -> float:
        """Ï∂îÎ°† ÏãúÍ∞Ñ Ï∂îÏ†ï"""
        base_times = {
            ModelArchitecture.CNN: 100,
            ModelArchitecture.UNET: 300,
            ModelArchitecture.TRANSFORMER: 500,
            ModelArchitecture.DIFFUSION: 2000,
            ModelArchitecture.UNKNOWN: 200
        }
        
        base_time = base_times.get(architecture, 200)
        size_factor = max(1.0, file_size_mb / 100)
        device_factor = 0.7 if IS_M3_MAX else 1.0
        
        return base_time * size_factor * device_factor
    
    def _limit_models_per_category(self, max_models: int):
        """Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î™®Îç∏ Ïàò Ï†úÌïú"""
        try:
            category_models = {}
            
            for name, model in self.detected_models.items():
                category = model.category
                if category not in category_models:
                    category_models[category] = []
                category_models[category].append((name, model))
            
            models_to_keep = {}
            
            for category, models in category_models.items():
                sorted_models = sorted(
                    models, 
                    key=lambda x: (x[1].confidence_score, x[1].file_size_mb), 
                    reverse=True
                )
                
                for name, model in sorted_models[:max_models]:
                    models_to_keep[name] = model
            
            self.detected_models = models_to_keep
            self.logger.debug(f"‚úÖ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ï†úÌïú Ï†ÅÏö©: {len(models_to_keep)}Í∞ú Î™®Îç∏ Ïú†ÏßÄ")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ïπ¥ÌÖåÍ≥†Î¶¨ Ï†úÌïú Ïã§Ìå®: {e}")
    
    def _post_process_results(self, min_confidence: float):
        """Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
        try:
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            
            self.detected_models = filtered_models
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
    
    def _reset_scan_stats(self):
        """Ïä§Ï∫î ÌÜµÍ≥Ñ Ï¥àÍ∏∞Ìôî"""
        for key in self.scan_stats:
            if isinstance(self.scan_stats[key], (int, float)):
                self.scan_stats[key] = 0
    
    def _print_detection_summary(self):
        """ÌÉêÏßÄ Í≤∞Í≥º ÏöîÏïΩ (ModelLoader Ïó∞Îèô Ï†ïÎ≥¥ Ìè¨Ìï®)"""
        try:
            total_models = len(self.detected_models)
            validated_models = sum(1 for m in self.detected_models.values() if m.pytorch_valid)
            registered_models = sum(1 for m in self.detected_models.values() if m.model_loader_registered)
            total_size_gb = sum(m.file_size_mb for m in self.detected_models.values()) / 1024
            
            self.logger.info(f"üìä ÏµúÏ¢Ö Í≤∞Í≥º ÏöîÏïΩ:")
            self.logger.info(f"   üîç ÌÉêÏßÄ:")
            self.logger.info(f"     ‚Ä¢ Ï¥ù Î™®Îç∏: {total_models}Í∞ú")
            self.logger.info(f"     ‚Ä¢ PyTorch Í≤ÄÏ¶ù: {validated_models}Í∞ú")
            self.logger.info(f"     ‚Ä¢ Ï¥ù ÌÅ¨Í∏∞: {total_size_gb:.1f}GB")
            self.logger.info(f"     ‚Ä¢ Ïä§Ï∫î ÏãúÍ∞Ñ: {self.scan_stats['scan_duration']:.2f}Ï¥à")
            
            if self.enable_auto_registration:
                self.logger.info(f"   üîó ModelLoader Îì±Î°ù:")
                self.logger.info(f"     ‚Ä¢ Îì±Î°ù Î™®Îç∏: {registered_models}Í∞ú")
                self.logger.info(f"     ‚Ä¢ Îì±Î°ùÎ•†: {(registered_models/max(total_models,1)*100):.1f}%")
                self.logger.info(f"     ‚Ä¢ Îì±Î°ù ÏãúÍ∞Ñ: {self.scan_stats['registration_duration']:.2f}Ï¥à")
            
            if self.enable_step_assignment and self.step_assignments:
                self.logger.info(f"   üéØ Step Ìï†Îãπ:")
                for step_name, models in self.step_assignments.items():
                    self.logger.info(f"     ‚Ä¢ {step_name}: {len(models)}Í∞ú")
            
            # ÏÑ±Îä• ÏöîÏïΩ
            total_time = self.scan_stats['scan_duration']
            models_per_sec = total_models / max(total_time, 0.1)
            self.logger.info(f"   ‚ö° ÏÑ±Îä•: {models_per_sec:.1f} Î™®Îç∏/Ï¥à")
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏöîÏïΩ Ï∂úÎ†• Ïã§Ìå®: {e}")
    
    # ==============================================
    # üî• ModelLoader Ïó∞Îèô Ï†ÑÏö© Î©îÏÑúÎìúÎì§
    # ==============================================
    
    def get_registration_status(self) -> Dict[str, Any]:
        """ModelLoader Îì±Î°ù ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            registered_models = [
                model for model in self.detected_models.values() 
                if model.model_loader_registered
            ]
            
            return {
                "total_detected": len(self.detected_models),
                "total_registered": len(registered_models),
                "registration_rate": len(registered_models) / max(len(self.detected_models), 1) * 100,
                "bridge_available": self.auto_registration_manager.bridge.available,
                "registration_results": self.registration_results,
                "step_assignments": self.step_assignments,
                "statistics": self.scan_stats
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def force_register_model(self, model_name: str) -> bool:
        """ÌäπÏ†ï Î™®Îç∏ Í∞ïÏ†ú Îì±Î°ù"""
        try:
            if model_name not in self.detected_models:
                self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ '{model_name}' ÌÉêÏßÄÎêòÏßÄ ÏïäÏùå")
                return False
            
            detected_model = self.detected_models[model_name]
            
            # Îã®Ïùº Î™®Îç∏ Îì±Î°ù
            registration_result = self.auto_registration_manager.bridge.register_detected_models(
                detected_models={model_name: detected_model},
                force_registration=True
            )
            
            success = registration_result.get("success", False)
            if success:
                self.logger.info(f"‚úÖ {model_name} Í∞ïÏ†ú Îì±Î°ù ÏÑ±Í≥µ")
            else:
                self.logger.warning(f"‚ùå {model_name} Í∞ïÏ†ú Îì±Î°ù Ïã§Ìå®")
            
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå {model_name} Í∞ïÏ†ú Îì±Î°ù Ïò§Î•ò: {e}")
            return False
    
    def get_step_model_assignments(self) -> Dict[str, List[str]]:
        """StepÎ≥Ñ Ìï†ÎãπÎêú Î™®Îç∏ Î™©Î°ù Î∞òÌôò"""
        return self.step_assignments.copy()
    
    def reassign_model_to_step(self, model_name: str, step_name: str) -> bool:
        """Î™®Îç∏ÏùÑ Îã§Î•∏ StepÏóê Ïû¨Ìï†Îãπ"""
        try:
            if model_name not in self.detected_models:
                return False
            
            # Í∏∞Ï°¥ Ìï†ÎãπÏóêÏÑú Ï†úÍ±∞
            for step, models in self.step_assignments.items():
                if model_name in models:
                    models.remove(model_name)
            
            # ÏÉà StepÏóê Ìï†Îãπ
            if step_name not in self.step_assignments:
                self.step_assignments[step_name] = []
            self.step_assignments[step_name].append(model_name)
            
            # DetectedModel ÏóÖÎç∞Ïù¥Ìä∏
            self.detected_models[model_name].step_name = step_name
            
            self.logger.info(f"üîÑ {model_name} ‚Üí {step_name} Ïû¨Ìï†Îãπ ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïû¨Ìï†Îãπ Ïã§Ìå®: {e}")
            return False
    
    # ==============================================
    # üî• Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§ (Ïú†ÏßÄ)
    # ==============================================
    
    def get_validated_models_only(self) -> Dict[str, DetectedModel]:
        """PyTorch Í≤ÄÏ¶ùÎêú Î™®Îç∏Îì§Îßå Î∞òÌôò"""
        return {name: model for name, model in self.detected_models.items() if model.pytorch_valid}
    
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î™®Îç∏ Ï°∞Ìöå"""
        return [model for model in self.detected_models.values() if model.category == category]
    
    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """StepÎ≥Ñ Î™®Îç∏ Ï°∞Ìöå"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]
    
    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """StepÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Ï°∞Ìöå"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        def model_score(model):
            score = 0
            if model.pytorch_valid:
                score += 100
            if model.model_loader_registered:  # üî• NEW: Îì±Î°ùÎêú Î™®Îç∏ Ïö∞ÏÑ†
                score += 50
            score += (6 - model.priority.value) * 20
            score += model.confidence_score * 50
            return score
        
        return max(step_models, key=model_score)

# ==============================================
# üî• Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± ÌÅ¥ÎûòÏä§Îì§ (Ïú†ÏßÄ)
# ==============================================

class AdvancedModelLoaderAdapter:
    """Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Ïñ¥ÎåëÌÑ∞ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_advanced_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """Í≥†Í∏â ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ±"""
        try:
            config = {
                "version": "9.0_with_registration",
                "device_optimization": {
                    "target_device": DEVICE_TYPE,
                    "is_m3_max": IS_M3_MAX
                },
                "models": {},
                "step_configurations": {},
                "registration_info": {
                    "auto_registered": True,
                    "registration_timestamp": time.time(),
                    "total_registered": sum(1 for m in detected_models.values() if m.model_loader_registered)
                }
            }
            
            for name, model in detected_models.items():
                config["models"][name] = {
                    "name": name,
                    "path": str(model.path),
                    "type": model.model_type,
                    "step": model.step_name,
                    "registered": model.model_loader_registered,
                    "confidence": model.confidence_score,
                    "pytorch_valid": model.pytorch_valid
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {}

class RealModelLoaderConfigGenerator:
    """Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú ÏÑ§Ï†ï ÏÉùÏÑ±Í∏∞"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """Í∏∞Î≥∏ ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ±"""
        try:
            return {
                "device": DEVICE_TYPE,
                "models": {
                    name: {
                        "path": str(model.path),
                        "type": model.model_type,
                        "step_name": model.step_name,
                        "registered": model.model_loader_registered
                    }
                    for name, model in detected_models.items()
                },
                "metadata": {
                    "generator_version": "9.0",
                    "total_models": len(detected_models),
                    "registered_models": sum(1 for m in detected_models.values() if m.model_loader_registered)
                }
            }
        except Exception as e:
            self.logger.error(f"‚ùå ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {}

# ==============================================
# üî• Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§ (ModelLoader Ïó∞Îèô Í∞ïÌôî)
# ==============================================

def create_real_world_detector(
    model_loader: Optional[Any] = None,
    enable_auto_registration: bool = True,
    enable_step_assignment: bool = True,
    **kwargs
) -> RealWorldModelDetector:
    """Ïã§Ï†ú Î™®Îç∏ ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± (ModelLoader Ïó∞Îèô)"""
    return RealWorldModelDetector(
        model_loader=model_loader,
        enable_auto_registration=enable_auto_registration,
        enable_step_assignment=enable_step_assignment,
        **kwargs
    )

def create_advanced_detector(**kwargs) -> RealWorldModelDetector:
    """Í≥†Í∏â Î™®Îç∏ ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± (Î≥ÑÏπ≠)"""
    return create_real_world_detector(**kwargs)

def quick_real_model_detection(
    model_loader: Optional[Any] = None,
    auto_register: bool = True,
    **kwargs
) -> Dict[str, DetectedModel]:
    """Îπ†Î•∏ Î™®Îç∏ ÌÉêÏßÄ + ÏûêÎèô Îì±Î°ù"""
    try:
        detector = create_real_world_detector(
            model_loader=model_loader,
            enable_pytorch_validation=False,
            enable_auto_registration=auto_register,
            enable_step_assignment=True,
            max_workers=1,
            **kwargs
        )
        
        return detector.detect_all_models(
            force_rescan=True,
            min_confidence=0.05,
            enable_detailed_analysis=False,
            auto_register_to_model_loader=auto_register,
            max_registrations=50  # ÏÉÅÏúÑ 50Í∞úÎßå Îì±Î°ù
        )
        
    except Exception as e:
        logger.error(f"Îπ†Î•∏ ÌÉêÏßÄ + Îì±Î°ù Ïã§Ìå®: {e}")
        return {}

def generate_real_model_loader_config(
    detector: Optional[RealWorldModelDetector] = None,
    model_loader: Optional[Any] = None
) -> Dict[str, Any]:
    """ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ± (ÏûêÎèô Îì±Î°ù Ìè¨Ìï®)"""
    try:
        if detector is None:
            detector = create_real_world_detector(model_loader=model_loader)
            detector.detect_all_models(auto_register_to_model_loader=True)
        
        generator = RealModelLoaderConfigGenerator(detector)
        return generator.generate_config(detector.detected_models)
        
    except Exception as e:
        logger.error(f"ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return {"error": str(e)}

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """Ïã§Ï†ú Î™®Îç∏ Í≤ΩÎ°ú Í≤ÄÏ¶ù (Îì±Î°ù ÏÉÅÌÉú Ìè¨Ìï®)"""
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "registered_models": [],
            "unregistered_models": [],
            "step_assigned_models": [],
            "summary": {}
        }
        
        for name, model in detected_models.items():
            try:
                if not model.path.exists():
                    validation_result["invalid_models"].append(name)
                    continue
                
                validation_result["valid_models"].append(name)
                
                if model.model_loader_registered:
                    validation_result["registered_models"].append(name)
                else:
                    validation_result["unregistered_models"].append(name)
                
                if model.step_interface_assigned:
                    validation_result["step_assigned_models"].append(name)
                
            except Exception as e:
                validation_result["invalid_models"].append(name)
        
        # ÏöîÏïΩ ÌÜµÍ≥Ñ
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_models": len(validation_result["valid_models"]),
            "registered_models": len(validation_result["registered_models"]),
            "step_assigned_models": len(validation_result["step_assigned_models"]),
            "registration_rate": len(validation_result["registered_models"]) / max(len(detected_models), 1) * 100,
            "step_assignment_rate": len(validation_result["step_assigned_models"]) / max(len(detected_models), 1) * 100
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°ú Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
        return {"error": str(e)}

# ==============================================
# üî• PipelineManager Ïó∞Îèô Ìï®Ïàò (ÌïµÏã¨)
# ==============================================

def integrate_with_pipeline_manager(
    pipeline_manager: Any,
    detector: Optional[RealWorldModelDetector] = None,
    auto_detect_and_register: bool = True
) -> Dict[str, Any]:
    """
    üîó PipelineManagerÏôÄ ÏôÑÏ†Ñ Ïó∞Îèô (ÌïµÏã¨ Ïó∞Í≤∞ Ìï®Ïàò)
    
    Args:
        pipeline_manager: PipelineManager Ïù∏Ïä§ÌÑ¥Ïä§
        detector: Î™®Îç∏ ÌÉêÏßÄÍ∏∞ (NoneÏù¥Î©¥ ÏûêÎèô ÏÉùÏÑ±)
        auto_detect_and_register: ÏûêÎèô ÌÉêÏßÄ Î∞è Îì±Î°ù Ïó¨Î∂Ä
        
    Returns:
        Ïó∞Îèô Í≤∞Í≥º
    """
    try:
        logger.info("üîó PipelineManagerÏôÄ Auto Model Detector Ïó∞Îèô ÏãúÏûë...")
        
        # ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± ÎòêÎäî ÏÇ¨Ïö©
        if detector is None:
            # PipelineManagerÏùò ModelLoader Í∞ÄÏ†∏Ïò§Í∏∞
            model_loader = getattr(pipeline_manager, 'model_loader', None)
            if model_loader is None:
                # Ï†ÑÏó≠ ModelLoader ÏÇ¨Ïö©
                model_loader = get_global_model_loader() if MODEL_LOADER_AVAILABLE else None
            
            detector = create_real_world_detector(
                model_loader=model_loader,
                enable_auto_registration=True,
                enable_step_assignment=True
            )
        
        integration_result = {
            "success": False,
            "detector_created": detector is not None,
            "models_detected": 0,
            "models_registered": 0,
            "steps_configured": 0,
            "pipeline_updated": False
        }
        
        if not detector:
            return {"error": "ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± Ïã§Ìå®", "details": integration_result}
        
        # ÏûêÎèô ÌÉêÏßÄ Î∞è Îì±Î°ù
        if auto_detect_and_register:
            logger.info("üîç ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄ Î∞è Îì±Î°ù Ïã§Ìñâ...")
            
            detected_models = detector.detect_all_models(
                auto_register_to_model_loader=True,
                max_registrations=30  # PipelineManagerÏö© Ï†úÌïú
            )
            
            integration_result.update({
                "models_detected": len(detected_models),
                "models_registered": detector.scan_stats.get("models_registered", 0),
                "steps_configured": len(detector.step_assignments),
                "step_assignments": detector.step_assignments,
                "registration_status": detector.get_registration_status()
            })
        
        # PipelineManager ÏóÖÎç∞Ïù¥Ìä∏
        if hasattr(pipeline_manager, 'update_model_registry'):
            try:
                pipeline_manager.update_model_registry(detector.detected_models)
                integration_result["pipeline_updated"] = True
                logger.info("‚úÖ PipelineManager Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PipelineManager ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
        
        # StepÎ≥Ñ Î™®Îç∏ Ìï†Îãπ Ï†ïÎ≥¥Î•º PipelineManagerÏóê Ï†ÑÎã¨
        if hasattr(pipeline_manager, 'configure_step_models'):
            try:
                pipeline_manager.configure_step_models(detector.step_assignments)
                logger.info("‚úÖ PipelineManager Step Î™®Îç∏ ÏÑ§Ï†ï ÏôÑÎ£å")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Step Î™®Îç∏ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
        
        integration_result["success"] = True
        
        logger.info("üéâ PipelineManager Ïó∞Îèô ÏôÑÎ£å!")
        logger.info(f"   - ÌÉêÏßÄ Î™®Îç∏: {integration_result['models_detected']}Í∞ú")
        logger.info(f"   - Îì±Î°ù Î™®Îç∏: {integration_result['models_registered']}Í∞ú")
        logger.info(f"   - Íµ¨ÏÑ± Step: {integration_result['steps_configured']}Í∞ú")
        
        return integration_result
        
    except Exception as e:
        logger.error(f"‚ùå PipelineManager Ïó∞Îèô Ïã§Ìå®: {e}")
        return {"error": str(e), "success": False}

# ==============================================
# üî• ÌïòÏúÑ Ìò∏ÌôòÏÑ± Î∞è Î≥ÑÏπ≠Îì§
# ==============================================

@dataclass 
class ModelFileInfo:
    """Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú ModelFileInfo ÌÅ¥ÎûòÏä§"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

# Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Ìå®ÌÑ¥ Î≥ÄÌôò
ENHANCED_MODEL_PATTERNS = {}

def _convert_patterns():
    """Í∏∞Ï°¥ Ìå®ÌÑ¥ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò"""
    pattern_matcher = PatternMatcher()
    
    for name, enhanced_pattern in pattern_matcher.patterns.items():
        ENHANCED_MODEL_PATTERNS[name] = ModelFileInfo(
            name=enhanced_pattern.name,
            patterns=enhanced_pattern.patterns,
            step=enhanced_pattern.step,
            keywords=enhanced_pattern.keywords,
            file_types=enhanced_pattern.file_types,
            min_size_mb=enhanced_pattern.size_range_mb[0],
            max_size_mb=enhanced_pattern.size_range_mb[1],
            priority=enhanced_pattern.priority
        )

_convert_patterns()

# ÌïòÏúÑ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Î≥ÑÏπ≠Îì§
AdvancedModelDetector = RealWorldModelDetector
ModelLoaderConfigGenerator = RealModelLoaderConfigGenerator
create_advanced_model_loader_adapter = lambda detector: AdvancedModelLoaderAdapter(detector)

# ==============================================
# üî• Î™®Îì† export Ï†ïÏùò
# ==============================================

__all__ = [
    # üî• ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'RealWorldModelDetector',
    'ModelLoaderBridge',                # NEW: ÌïµÏã¨ Ïó∞Í≤∞Í≥†Î¶¨
    'StepModelMatcher',                 # NEW: Step Îß§Ïπ≠
    'AutoRegistrationManager',          # NEW: ÏûêÎèô Îì±Î°ù Í¥ÄÎ¶¨
    'AdvancedModelLoaderAdapter',
    'RealModelLoaderConfigGenerator',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    'ModelFileInfo',
    
    # üî• Í∞ïÌôîÎêú ÌÅ¥ÎûòÏä§Îì§
    'EnhancedModelPattern',
    'ModelArchitecture',
    'ModelPerformanceMetrics',
    'PatternMatcher',
    'FileScanner',
    'PyTorchValidator',
    'PathFinder',
    
    # üî• Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§ (ModelLoader Ïó∞Îèô)
    'create_real_world_detector',
    'create_advanced_detector',
    'create_advanced_model_loader_adapter',
    'quick_real_model_detection',
    'generate_real_model_loader_config',
    'validate_real_model_paths',
    
    # üî• NEW: PipelineManager Ïó∞Îèô
    'integrate_with_pipeline_manager',   # ÌïµÏã¨ Ïó∞Îèô Ìï®Ïàò
    
    # Ìò∏ÌôòÏÑ± Îç∞Ïù¥ÌÑ∞
    'ENHANCED_MODEL_PATTERNS',
    
    # ÌïòÏúÑ Ìò∏ÌôòÏÑ± Î≥ÑÏπ≠Îì§
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator'
]

# ==============================================
# üî• Î©îÏù∏ Ïã§ÌñâÎ∂Ä (ÌÖåÏä§Ìä∏ + Îì±Î°ù Í≤ÄÏ¶ù)
# ==============================================

def main():
    """ÏôÑÏ†ÑÌïú ÌÖåÏä§Ìä∏ Ïã§Ìñâ (ÌÉêÏßÄ + Îì±Î°ù + Í≤ÄÏ¶ù)"""
    try:
        print("üîç ÏôÑÏ†ÑÌïú Auto Detector v9.0 + ModelLoader Ïó∞Îèô ÌÖåÏä§Ìä∏")
        print("=" * 80)
        
        # 1. ModelLoader Ïó∞Îèô ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ±
        print("\nüîß Phase 1: ModelLoader Ïó∞Îèô ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ±...")
        detector = create_real_world_detector(
            enable_auto_registration=True,
            enable_step_assignment=True,
            enable_pytorch_validation=False,  # Îπ†Î•∏ ÌÖåÏä§Ìä∏
            max_workers=1
        )
        
        print(f"‚úÖ ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± ÏôÑÎ£å")
        print(f"   - ModelLoader Ïó∞Îèô: {'‚úÖ' if detector.auto_registration_manager.bridge.available else '‚ùå'}")
        
        # 2. Î™®Îç∏ ÌÉêÏßÄ + ÏûêÎèô Îì±Î°ù
        print("\nüîç Phase 2: Î™®Îç∏ ÌÉêÏßÄ + ModelLoader ÏûêÎèô Îì±Î°ù...")
        detected_models = detector.detect_all_models(
            min_confidence=0.05,
            force_rescan=True,
            auto_register_to_model_loader=True,
            max_registrations=20  # ÌÖåÏä§Ìä∏Ïö© Ï†úÌïú
        )
        
        if not detected_models:
            print("‚ùå Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            return False
        
        print(f"\n‚úÖ ÌÉêÏßÄ + Îì±Î°ù ÏôÑÎ£å!")
        
        # 3. Îì±Î°ù ÏÉÅÌÉú ÌôïÏù∏
        print("\nüìä Phase 3: ModelLoader Îì±Î°ù ÏÉÅÌÉú ÌôïÏù∏...")
        registration_status = detector.get_registration_status()
        
        print(f"   üìã Îì±Î°ù ÌÜµÍ≥Ñ:")
        print(f"     ‚Ä¢ ÌÉêÏßÄÎêú Î™®Îç∏: {registration_status['total_detected']}Í∞ú")
        print(f"     ‚Ä¢ Îì±Î°ùÎêú Î™®Îç∏: {registration_status['total_registered']}Í∞ú") 
        print(f"     ‚Ä¢ Îì±Î°ùÎ•†: {registration_status['registration_rate']:.1f}%")
        print(f"     ‚Ä¢ Bridge ÏÉÅÌÉú: {'‚úÖ' if registration_status['bridge_available'] else '‚ùå'}")
        
        # 4. Step Ìï†Îãπ ÌôïÏù∏
        step_assignments = detector.get_step_model_assignments()
        if step_assignments:
            print(f"\nüéØ StepÎ≥Ñ Î™®Îç∏ Ìï†Îãπ:")
            for step_name, models in step_assignments.items():
                print(f"     ‚Ä¢ {step_name}: {len(models)}Í∞ú")
                for i, model_name in enumerate(models[:2]):  # Ï≤òÏùå 2Í∞úÎßå ÌëúÏãú
                    role = "Primary" if i == 0 else "Fallback"
                    print(f"       - {role}: {model_name}")
        
        # 5. ÏÉÅÏúÑ Îì±Î°ù Î™®Îç∏Îì§ Ï∂úÎ†•
        registered_models = [
            model for model in detected_models.values() 
            if model.model_loader_registered
        ]
        
        if registered_models:
            print(f"\nüìù Îì±Î°ùÎêú ÏÉÅÏúÑ Î™®Îç∏Îì§:")
            sorted_registered = sorted(
                registered_models, 
                key=lambda x: x.confidence_score, 
                reverse=True
            )
            
            for i, model in enumerate(sorted_registered[:10], 1):
                print(f"   {i}. {model.name}")
                print(f"      üìÅ {model.path.name}")
                print(f"      üìä {model.file_size_mb:.1f}MB")
                print(f"      üéØ {model.step_name}")
                print(f"      ‚≠ê Ïã†Î¢∞ÎèÑ: {model.confidence_score:.2f}")
                print(f"      üîó Îì±Î°ùÏãúÍ∞Ñ: {time.strftime('%H:%M:%S', time.localtime(model.registration_timestamp))}")
                print()
        
        # 6. Í≤ÄÏ¶ù Í≤∞Í≥º
        print("\nüîç Phase 4: Í≤ÄÏ¶ù Í≤∞Í≥º...")
        validation_result = validate_real_model_paths(detected_models)
        if validation_result.get('summary'):
            summary = validation_result['summary']
            print(f"   üìä Í≤ÄÏ¶ù ÏöîÏïΩ:")
            print(f"     ‚Ä¢ Ïú†Ìö® Î™®Îç∏: {summary['valid_models']}Í∞ú")
            print(f"     ‚Ä¢ Îì±Î°ùÎêú Î™®Îç∏: {summary['registered_models']}Í∞ú")
            print(f"     ‚Ä¢ Step Ìï†ÎãπÎêú Î™®Îç∏: {summary['step_assigned_models']}Í∞ú")
            print(f"     ‚Ä¢ Îì±Î°ùÎ•†: {summary['registration_rate']:.1f}%")
            print(f"     ‚Ä¢ Step Ìï†ÎãπÎ•†: {summary['step_assignment_rate']:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nüéâ Auto Detector v9.0 + ModelLoader Ïó∞Îèô ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ!")
        print(f"   üîç 574Í∞ú Î™®Îç∏ ÌÉêÏßÄ ‚Üí ModelLoader Îì±Î°ùÍπåÏßÄ ÏôÑÏ†Ñ ÏûêÎèôÌôî")
        print(f"   üîó StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô Ìï†Îãπ Î∞è Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±")
        print(f"   üìù PipelineManager ÏôÑÏ†Ñ Ïó∞Îèô Ï§ÄÎπÑ ÏôÑÎ£å")
        print(f"   üçé M3 Max 128GB + conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî")
        print(f"   üî• MPS empty_cache AttributeError ÏôÑÏ†Ñ Ìï¥Í≤∞")
        print(f"   üöÄ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ± Î≥¥Ïû•")
    else:
        print(f"\nüîß Ï∂îÍ∞Ä ÎîîÎ≤ÑÍπÖÏù¥ ÌïÑÏöîÌï©ÎãàÎã§")

# ==============================================
# üî• Î™®Îìà Î°úÎìú ÏôÑÎ£å Î©îÏãúÏßÄ
# ==============================================

logger.info("‚úÖ ÏôÑÏ†ÑÌïú ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄ ÏãúÏä§ÌÖú v9.0 Î°úÎìú ÏôÑÎ£å - ModelLoader Ïó∞Îèô ÌÜµÌï©")
logger.info("üîó ÌïµÏã¨ Í∞úÏÑ†: ÌÉêÏßÄÎêú Î™®Îç∏ ‚Üí ModelLoader ÏûêÎèô Îì±Î°ù Ïó∞Í≤∞Í≥†Î¶¨ ÏôÑÏÑ±")
logger.info("üéØ 574Í∞ú Î™®Îç∏ ÌÉêÏßÄ ‚Üí Ïã§Ï†ú ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÍ≤å Îì±Î°ùÍπåÏßÄ ÏôÑÏ†Ñ ÏûêÎèôÌôî")
logger.info("üìù ModelLoaderBridge, StepModelMatcher, AutoRegistrationManager Ï∂îÍ∞Ä")
logger.info("üîÑ PipelineManager ÏôÑÏ†Ñ Ïó∞Îèô Î∞è Í∏∞Ï°¥ ÏΩîÎìú 100% Ìò∏ÌôòÏÑ± Ïú†ÏßÄ")
logger.info("üçé M3 Max 128GB + conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî + MPS Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("üöÄ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ± + Ïã§Î¨¥Í∏â ÏÑ±Îä• Î≥¥Ïû•")
logger.info(f"üéØ PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}, MPS: {'‚úÖ' if IS_M3_MAX else '‚ùå'}")
logger.info(f"üîó ModelLoader Ïó∞Îèô: {'‚úÖ' if MODEL_LOADER_AVAILABLE else '‚ùå'}")

if TORCH_AVAILABLE and hasattr(torch, '__version__'):
    logger.info(f"üî• PyTorch Î≤ÑÏ†Ñ: {torch.__version__}")
else:
    logger.warning("‚ö†Ô∏è PyTorch ÏóÜÏùå - conda install pytorch Í∂åÏû•")