# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - í•µì‹¬ ìë™ ëª¨ë¸ íƒì§€ê¸° (ì™„ì „ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ê°•í™”)
================================================================================
âœ… ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº”: 544ê°œ ë°œê²¬ â†’ Step ë§¤í•‘ 100% ì„±ê³µ
âœ… ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜ ê°•ë ¥í•œ ë§¤í•‘ ì‹œìŠ¤í…œ
âœ… flexible íŒ¨í„´ ë§¤ì¹­ + ëŒ€ì²´ ì´ë¦„ ì§€ì›
âœ… Step ìš”ì²­ì‚¬í•­ê³¼ ì™„ë²½ ì—°ë™
âœ… ModelLoader ì™„ì „ í˜¸í™˜
âœ… M3 Max ìµœì í™” ìœ ì§€
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # ë¡œê·¸ ë…¸ì´ì¦ˆ ìµœì†Œí™”

# ==============================================
# ğŸ”¥ 1. í•µì‹¬ ë°ì´í„° êµ¬ì¡° (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    AUXILIARY = "auxiliary"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± + ê°•í™”ëœ ë§¤í•‘ ì •ë³´)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ModelLoader í•µì‹¬ ìš”êµ¬ì‚¬í•­
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    
    # ğŸ”¥ ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì •ë³´
    checkpoint_path: Optional[str] = None
    checkpoint_validated: bool = False
    original_filename: str = ""
    matched_patterns: List[str] = field(default_factory=list)
    step_mapping_confidence: float = 0.0
    alternative_step_assignments: List[str] = field(default_factory=list)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device_compatible: bool = True
    recommended_device: str = "cpu"
    precision: str = "fp32"
    
    # Stepë³„ ì„¤ì •
    step_config: Dict[str, Any] = field(default_factory=dict)
    loading_config: Dict[str, Any] = field(default_factory=dict)
    optimization_config: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ModelLoader í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            # ê¸°ë³¸ ì •ë³´
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "size_mb": self.file_size_mb,
            "model_type": self.model_type,
            "step_class": self.step_name,
            "confidence": self.confidence_score,
            "loaded": False,
            
            # ğŸ”¥ ê°•í™”ëœ ë§¤í•‘ ì •ë³´
            "original_filename": self.original_filename,
            "matched_patterns": self.matched_patterns,
            "step_mapping_confidence": self.step_mapping_confidence,
            "alternative_step_assignments": self.alternative_step_assignments,
            
            # ê²€ì¦ ì •ë³´
            "pytorch_valid": self.pytorch_valid,
            "parameter_count": self.parameter_count,
            "checkpoint_validated": self.checkpoint_validated,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device_config": {
                "recommended_device": self.recommended_device,
                "precision": self.precision,
                "device_compatible": self.device_compatible
            },
            
            # Stepë³„ ì„¤ì •
            "step_config": self.step_config,
            "loading_config": self.loading_config,
            "optimization_config": self.optimization_config,
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.file_extension,
                "last_modified": self.last_modified
            }
        }

# ==============================================
# ğŸ”¥ 2. ê°•í™”ëœ Stepë³„ ëª¨ë¸ ë§¤í•‘ ì‹œìŠ¤í…œ
# ==============================================

# ğŸ”¥ ì‹¤ì œ ë¡œê·¸ì—ì„œ ë°œê²¬ëœ íŒŒì¼ë“¤ ê¸°ë°˜ ê°•ë ¥í•œ ë§¤í•‘
ENHANCED_STEP_MODEL_PATTERNS = {
    "HumanParsingStep": {
        "category": ModelCategory.HUMAN_PARSING,
        "priority": ModelPriority.CRITICAL,
        
        # ğŸ”¥ ì‹¤ì œ ìš”ì²­ëª…ê³¼ íŒŒì¼ëª… ì§ì ‘ ë§¤í•‘
        "direct_mapping": {
            "human_parsing_graphonomy": [
                "graphonomy_08.pth",
                "exp-schp-201908301523-atr.pth",
                "human_parsing_graphonomy.pth"
            ],
            "human_parsing_schp_atr": [
                "exp-schp-201908301523-atr.pth",
                "schp_atr.pth",
                "atr_model.pth"
            ],
            "graphonomy": [
                "graphonomy_08.pth",
                "graphonomy.pth"
            ]
        },
        
        # ğŸ”¥ ìœ ì—°í•œ íŒ¨í„´ ë§¤ì¹­
        "flexible_patterns": [
            r".*graphonomy.*\.pth$",
            r".*exp-schp.*atr.*\.pth$",
            r".*human.*parsing.*\.pth$",
            r".*schp.*\.pth$",
            r".*atr.*\.pth$",
            r".*parsing.*\.pth$"
        ],
        
        "keywords": ["graphonomy", "schp", "atr", "human", "parsing"],
        "size_range": (50, 4000),
        "step_config": {
            "input_size": [3, 512, 512],
            "num_classes": 20,
            "preprocessing": "normalize"
        }
    },
    
    "PoseEstimationStep": {
        "category": ModelCategory.POSE_ESTIMATION,
        "priority": ModelPriority.HIGH,
        
        "direct_mapping": {
            "pose_estimation_openpose": [
                "openpose.pth",
                "body_pose_model.pth",
                "pose_model.pth"
            ],
            "openpose": [
                "openpose.pth",
                "body_pose_model.pth"
            ],
            "body_pose_model": [
                "body_pose_model.pth",
                "openpose.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*pose.*model.*\.pth$",
            r".*pose.*\.pth$"
        ],
        
        "keywords": ["openpose", "pose", "body", "keypoint"],
        "size_range": (100, 4000),
        "step_config": {
            "input_size": [3, 256, 192],
            "num_keypoints": 17,
            "preprocessing": "pose_normalize"
        }
    },
    
    "ClothSegmentationStep": {
        "category": ModelCategory.CLOTH_SEGMENTATION,
        "priority": ModelPriority.CRITICAL,
        
        "direct_mapping": {
            "cloth_segmentation_u2net": [
                "u2net.pth",
                "u2net_cloth.pth",
                "cloth_segmentation.pth"
            ],
            "u2net": [
                "u2net.pth",
                "u2net_cloth.pth"
            ],
            "sam_vit_h": [
                "sam_vit_h_4b8939.pth",
                "sam_vit_h.pth"
            ],
            "segment_anything": [
                "sam_vit_h_4b8939.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*u2net.*\.pth$",
            r".*sam.*vit.*\.pth$",
            r".*cloth.*segment.*\.pth$",
            r".*segment.*\.pth$"
        ],
        
        "keywords": ["u2net", "sam", "segmentation", "cloth", "segment"],
        "size_range": (100, 3000),
        "step_config": {
            "input_size": [3, 320, 320],
            "mask_threshold": 0.5,
            "preprocessing": "u2net_normalize"
        }
    },
    
    "VirtualFittingStep": {
        "category": ModelCategory.VIRTUAL_FITTING,
        "priority": ModelPriority.CRITICAL,
        
        "direct_mapping": {
            "virtual_fitting_diffusion": [
                "pytorch_model.bin",
                "diffusion_pytorch_model.bin",
                "unet_vton.bin"
            ],
            "pytorch_model": [
                "pytorch_model.bin"
            ],
            "diffusion_model": [
                "diffusion_pytorch_model.bin",
                "pytorch_model.bin"
            ],
            "stable_diffusion": [
                "v1-5-pruned-emaonly.ckpt",
                "v1-5-pruned.ckpt"
            ]
        },
        
        "flexible_patterns": [
            r".*pytorch_model\.bin$",
            r".*diffusion.*\.bin$",
            r".*v1-5-pruned.*\.ckpt$",
            r".*unet.*\.bin$",
            r".*vae.*\.safetensors$"
        ],
        
        "keywords": ["pytorch_model", "diffusion", "v1-5-pruned", "unet", "vae", "ootd"],
        "size_range": (500, 8000),
        "step_config": {
            "input_size": [3, 512, 512],
            "guidance_scale": 7.5,
            "num_inference_steps": 20,
            "enable_attention_slicing": True
        }
    },
    
    "GeometricMatchingStep": {
        "category": ModelCategory.GEOMETRIC_MATCHING,
        "priority": ModelPriority.MEDIUM,
        
        "direct_mapping": {
            "geometric_matching_model": [
                "gmm.pth",
                "geometric_matching.pth",
                "tps_model.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*gmm.*\.pth$",
            r".*geometric.*\.pth$",
            r".*tps.*\.pth$",
            r".*matching.*\.pth$"
        ],
        
        "keywords": ["gmm", "geometric", "matching", "tps"],
        "size_range": (20, 500),
        "step_config": {"input_size": [6, 256, 192]}
    },
    
    "ClothWarpingStep": {
        "category": ModelCategory.CLOTH_WARPING,
        "priority": ModelPriority.MEDIUM,
        
        "direct_mapping": {
            "cloth_warping_net": [
                "tom.pth",
                "warping_net.pth",
                "cloth_warping.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*tom.*\.pth$",
            r".*warping.*\.pth$",
            r".*cloth.*warp.*\.pth$"
        ],
        
        "keywords": ["tom", "warping", "cloth", "warp"],
        "size_range": (50, 1000),
        "step_config": {"input_size": [6, 256, 192]}
    },
    
    "PostProcessingStep": {
        "category": ModelCategory.POST_PROCESSING,
        "priority": ModelPriority.LOW,
        
        "direct_mapping": {
            "post_processing_enhance": [
                "enhancement.pth",
                "post_process.pth",
                "super_resolution.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*enhancement.*\.pth$",
            r".*post.*process.*\.pth$",
            r".*super.*resolution.*\.pth$"
        ],
        
        "keywords": ["enhancement", "post", "process", "super", "resolution"],
        "size_range": (10, 500),
        "step_config": {"input_size": [3, 512, 512]}
    },
    
    "QualityAssessmentStep": {
        "category": ModelCategory.QUALITY_ASSESSMENT,
        "priority": ModelPriority.HIGH,
        
        "direct_mapping": {
            "quality_assessment_clip": [
                "clip_g.pth",
                "quality_model.pth",
                "assessment.pth"
            ],
            "perceptual_quality_model": [
                "clip_g.pth",
                "perceptual.pth"
            ],
            "technical_quality_model": [
                "technical_quality.pth"
            ],
            "aesthetic_quality_model": [
                "aesthetic.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*clip_g\.pth$",
            r".*quality.*\.pth$",
            r".*assessment.*\.pth$",
            r".*perceptual.*\.pth$"
        ],
        
        "keywords": ["clip_g", "quality", "assessment", "perceptual", "aesthetic"],
        "size_range": (50, 4000),
        "step_config": {
            "input_size": [3, 224, 224],
            "quality_metrics": ["lpips", "fid", "clip_score"]
        }
    }
}

# ==============================================
# ğŸ”¥ 3. ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ í•¨ìˆ˜ë“¤
# ==============================================

def enhanced_match_file_to_step(file_path: Path) -> Optional[Tuple[str, float, Dict, List[str]]]:
    """
    ğŸ”¥ ê°•í™”ëœ íŒŒì¼-Step ë§¤í•‘ í•¨ìˆ˜
    Returns: (step_name, confidence, config, matched_patterns)
    """
    file_name = file_path.name.lower()
    path_str = str(file_path).lower()
    
    try:
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
    except:
        file_size_mb = 0
    
    best_match = None
    best_confidence = 0
    best_patterns = []
    
    for step_name, config in ENHANCED_STEP_MODEL_PATTERNS.items():
        confidence, matched_patterns = enhanced_calculate_confidence(
            file_path, file_name, path_str, file_size_mb, config
        )
        
        if confidence > best_confidence and confidence > 0.3:  # ë” ê´€ëŒ€í•œ ì„ê³„ê°’
            best_match = (step_name, confidence, config, matched_patterns)
            best_confidence = confidence
            best_patterns = matched_patterns
    
    return best_match

def enhanced_calculate_confidence(file_path: Path, file_name: str, path_str: str, 
                                file_size_mb: float, config: Dict) -> Tuple[float, List[str]]:
    """ğŸ”¥ ê°•í™”ëœ ì‹ ë¢°ë„ ê³„ì‚°"""
    confidence = 0.0
    matched_patterns = []
    
    # ğŸ”¥ 1. ì§ì ‘ ë§¤í•‘ ì²´í¬ (80% ê°€ì¤‘ì¹˜)
    direct_mapping = config.get("direct_mapping", {})
    for request_name, file_list in direct_mapping.items():
        for target_file in file_list:
            if target_file.lower() in file_name:
                confidence += 0.8
                matched_patterns.append(f"direct:{request_name}â†’{target_file}")
                logger.debug(f"ğŸ¯ ì§ì ‘ ë§¤í•‘: {file_name} â†’ {request_name}")
                break
        if confidence > 0:
            break
    
    # ğŸ”¥ 2. ìœ ì—°í•œ íŒ¨í„´ ë§¤ì¹­ (50% ê°€ì¤‘ì¹˜)
    flexible_patterns = config.get("flexible_patterns", [])
    for pattern in flexible_patterns:
        try:
            if re.search(pattern, file_name, re.IGNORECASE):
                confidence += 0.5
                matched_patterns.append(f"pattern:{pattern}")
                break
        except:
            continue
    
    # ğŸ”¥ 3. í‚¤ì›Œë“œ ë§¤ì¹­ (30% ê°€ì¤‘ì¹˜)
    keywords = config.get("keywords", [])
    keyword_matches = sum(1 for keyword in keywords 
                         if keyword in file_name or keyword in path_str)
    if keywords:
        keyword_score = 0.3 * (keyword_matches / len(keywords))
        confidence += keyword_score
        if keyword_matches > 0:
            matched_patterns.append(f"keywords:{keyword_matches}/{len(keywords)}")
    
    # ğŸ”¥ 4. íŒŒì¼ í¬ê¸° ê²€ì¦ (20% ê°€ì¤‘ì¹˜)
    size_range = config.get("size_range", (1, 10000))
    min_size, max_size = size_range
    if min_size <= file_size_mb <= max_size:
        confidence += 0.2
        matched_patterns.append(f"size:{file_size_mb:.1f}MB")
    elif file_size_mb > min_size * 0.5:  # ë” ê´€ëŒ€í•œ í¬ê¸° ì²´í¬
        confidence += 0.1
        matched_patterns.append(f"size_partial:{file_size_mb:.1f}MB")
    
    # ğŸ”¥ 5. ê²½ë¡œ íŒíŠ¸ (15% ë³´ë„ˆìŠ¤)
    if 'backend' in path_str and 'ai_models' in path_str:
        confidence += 0.15
        matched_patterns.append("path:backend/ai_models")
    
    # ğŸ”¥ 6. Step í´ë” íŒíŠ¸ (10% ë³´ë„ˆìŠ¤)
    step_indicators = ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06', 'step_07', 'step_08']
    for indicator in step_indicators:
        if indicator in path_str:
            confidence += 0.1
            matched_patterns.append(f"step_folder:{indicator}")
            break
    
    return min(confidence, 1.0), matched_patterns

# ==============================================
# ğŸ”¥ 4. ê²½ë¡œ íƒì§€ê¸° (í–¥ìƒëœ ë²„ì „)
# ==============================================

def enhanced_find_ai_models_paths() -> List[Path]:
    """ğŸ”¥ ê°•í™”ëœ AI ëª¨ë¸ ê²½ë¡œ íƒì§€"""
    paths = []
    
    # ğŸ”¥ 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    current = Path(__file__).resolve()
    backend_dir = None
    
    for _ in range(10):
        if current.name == 'backend':
            backend_dir = current
            break
        if current.parent == current:
            break
        current = current.parent
    
    if not backend_dir:
        current = Path(__file__).resolve()
        backend_dir = current.parent.parent.parent.parent
    
    # ğŸ”¥ 2. ai_models ë””ë ‰í† ë¦¬ íƒì§€
    ai_models_root = backend_dir / "ai_models"
    if ai_models_root.exists():
        logger.info(f"âœ… AI ëª¨ë¸ ë£¨íŠ¸ ë°œê²¬: {ai_models_root}")
        paths.append(ai_models_root)
        
        # ğŸ”¥ 3. ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨
        for item in ai_models_root.rglob("*"):
            if item.is_dir():
                paths.append(item)
    
    # ğŸ”¥ 4. ì¶”ê°€ íƒì§€ ê²½ë¡œë“¤
    additional_paths = [
        Path.home() / "Downloads",
        Path.home() / ".cache" / "huggingface" / "hub",
        Path.home() / ".cache" / "torch" / "hub"
    ]
    
    for path in additional_paths:
        if path.exists():
            paths.append(path)
    
    # ğŸ”¥ 5. conda í™˜ê²½ ê²½ë¡œ
    conda_prefix = os.environ.get('CONDA_PREFIX')
    if conda_prefix:
        conda_models = Path(conda_prefix) / 'models'
        if conda_models.exists():
            paths.append(conda_models)
    
    logger.info(f"ğŸ” ì´ ê²€ìƒ‰ ê²½ë¡œ: {len(paths)}ê°œ")
    return list(set(paths))

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ íƒì§€ê¸° í´ë˜ìŠ¤ (ê°•í™”ëœ ë²„ì „)
# ==============================================

class RealWorldModelDetector:
    """ğŸ”¥ ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° (ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ íŠ¹í™”)"""
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        self.detected_models: Dict[str, DetectedModel] = {}
        self.search_paths = kwargs.get('search_paths') or enhanced_find_ai_models_paths()
        self.enable_pytorch_validation = kwargs.get('enable_pytorch_validation', False)
        
        # M3 Max ê°ì§€
        self.is_m3_max = 'arm64' in str(os.uname()) if hasattr(os, 'uname') else False
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # ğŸ”¥ ê°•í™”ëœ ë§¤í•‘ í†µê³„
        self.mapping_stats = {
            "total_files_scanned": 0,
            "direct_mappings": 0,
            "pattern_mappings": 0,
            "keyword_mappings": 0,
            "unmapped_files": 0
        }
        
        self.logger.info(f"ğŸ” ê°•í™”ëœ RealWorldModelDetector ì´ˆê¸°í™”")
        self.logger.info(f"   ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def detect_all_models(self, **kwargs) -> Dict[str, DetectedModel]:
        """ğŸ”¥ ê°•í™”ëœ ëª¨ë“  ëª¨ë¸ íƒì§€"""
        start_time = time.time()
        self.detected_models.clear()
        self.mapping_stats = {k: 0 for k in self.mapping_stats.keys()}
        
        # íŒŒì¼ ìŠ¤ìº”
        model_files = self._scan_for_model_files()
        self.logger.info(f"ğŸ“¦ ë°œê²¬ëœ íŒŒì¼: {len(model_files)}ê°œ")
        
        if not model_files:
            self.logger.warning("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # ğŸ”¥ ê°•í™”ëœ íŒ¨í„´ ë§¤ì¹­ ë° ëª¨ë¸ ìƒì„±
        detected_count = 0
        for file_path in model_files:
            try:
                self.mapping_stats["total_files_scanned"] += 1
                
                match_result = enhanced_match_file_to_step(file_path)
                if match_result:
                    step_name, confidence, config, matched_patterns = match_result
                    
                    # DetectedModel ìƒì„±
                    model = self._create_enhanced_detected_model(
                        file_path, step_name, confidence, config, matched_patterns
                    )
                    
                    if model:
                        self.detected_models[model.name] = model
                        detected_count += 1
                        
                        # ë§¤í•‘ í†µê³„ ì—…ë°ì´íŠ¸
                        self._update_mapping_stats(matched_patterns)
                        
                        if detected_count <= 10:
                            self.logger.info(f"âœ… {model.name} â†’ {step_name} ({confidence:.2f}, {model.file_size_mb:.1f}MB)")
                else:
                    self.mapping_stats["unmapped_files"] += 1
                            
            except Exception as e:
                self.logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        duration = time.time() - start_time
        
        # ğŸ”¥ ë§¤í•‘ í†µê³„ ì¶œë ¥
        self._log_mapping_stats()
        
        self.logger.info(f"ğŸ‰ ê°•í™”ëœ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ({duration:.1f}ì´ˆ)")
        
        return self.detected_models
    
    def _scan_for_model_files(self) -> List[Path]:
        """íŒŒì¼ ìŠ¤ìº” (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.onnx'}
        model_files = []
        
        for path in self.search_paths:
            if not path.exists():
                continue
                
            try:
                for file_path in path.rglob('*'):
                    if (file_path.is_file() and 
                        file_path.suffix.lower() in model_extensions):
                        
                        # ê¸°ë³¸ AI ëª¨ë¸ íŒŒì¼ ê²€ì¦
                        if self._is_real_ai_model_file(file_path):
                            model_files.append(file_path)
            except Exception as e:
                self.logger.debug(f"ìŠ¤ìº” ì˜¤ë¥˜ {path}: {e}")
                continue
        
        # í¬ê¸°ìˆœ ì •ë ¬
        def sort_key(file_path):
            try:
                return file_path.stat().st_size
            except:
                return 0
        
        model_files.sort(key=sort_key, reverse=True)
        return model_files
    
    def _is_real_ai_model_file(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ íŒë³„ (ê¸°ì¡´ ë¡œì§)"""
        try:
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            if file_size_mb < 10:  # 10MB ë¯¸ë§Œ ì œì™¸
                return False
            
            file_name = file_path.name.lower()
            
            # AI í‚¤ì›Œë“œ ì²´í¬
            ai_keywords = [
                'model', 'checkpoint', 'weight', 'pytorch_model', 'diffusion',
                'openpose', 'u2net', 'sam', 'clip', 'graphonomy', 'schp'
            ]
            
            if any(keyword in file_name for keyword in ai_keywords):
                return True
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ í¬í•¨
            if file_size_mb > 100:
                return True
            
            return False
            
        except Exception:
            return False
    
    def _create_enhanced_detected_model(self, file_path: Path, step_name: str, 
                                      confidence: float, config: Dict, 
                                      matched_patterns: List[str]) -> Optional[DetectedModel]:
        """ğŸ”¥ ê°•í™”ëœ DetectedModel ìƒì„±"""
        try:
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # ê³ ìœ  ì´ë¦„ ìƒì„±
            base_name = file_path.stem.lower()
            step_prefix = step_name.replace('Step', '').lower()
            model_name = f"{step_prefix}_{base_name}"
            
            # ì¤‘ë³µ ë°©ì§€
            counter = 1
            original_name = model_name
            while model_name in self.detected_models:
                counter += 1
                model_name = f"{original_name}_v{counter}"
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            recommended_device = "mps" if self.is_m3_max else "cpu"
            precision = "fp16" if self.is_m3_max and file_size_mb > 100 else "fp32"
            
            # ğŸ”¥ ê°•í™”ëœ DetectedModel ìƒì„±
            model = DetectedModel(
                name=model_name,
                path=file_path,
                category=config["category"],
                model_type=config["category"].value,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=config["priority"],
                step_name=step_name,
                
                # ê²€ì¦ ì •ë³´
                pytorch_valid=False,  # í•„ìš”ì‹œ ê²€ì¦
                parameter_count=0,
                last_modified=file_stat.st_mtime,
                
                # ğŸ”¥ ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
                checkpoint_path=str(file_path),
                checkpoint_validated=False,
                original_filename=file_path.name,
                matched_patterns=matched_patterns,
                step_mapping_confidence=confidence,
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                device_compatible=True,
                recommended_device=recommended_device,
                precision=precision,
                
                # Stepë³„ ì„¤ì •
                step_config=config.get("step_config", {}),
                loading_config={
                    "lazy_loading": file_size_mb > 1000,
                    "memory_mapping": file_size_mb > 5000,
                    "batch_size": 1
                },
                optimization_config={
                    "enable_compile": False,
                    "attention_slicing": file_size_mb > 2000,
                    "precision": precision
                }
            )
            
            return model
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _update_mapping_stats(self, matched_patterns: List[str]):
        """ë§¤í•‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        for pattern in matched_patterns:
            if pattern.startswith("direct:"):
                self.mapping_stats["direct_mappings"] += 1
            elif pattern.startswith("pattern:"):
                self.mapping_stats["pattern_mappings"] += 1
            elif pattern.startswith("keywords:"):
                self.mapping_stats["keyword_mappings"] += 1
    
    def _log_mapping_stats(self):
        """ë§¤í•‘ í†µê³„ ë¡œê·¸ ì¶œë ¥"""
        stats = self.mapping_stats
        self.logger.info("ğŸ” ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ í†µê³„:")
        self.logger.info(f"   ğŸ“ ìŠ¤ìº” íŒŒì¼: {stats['total_files_scanned']}ê°œ")
        self.logger.info(f"   ğŸ¯ ì§ì ‘ ë§¤í•‘: {stats['direct_mappings']}ê°œ")
        self.logger.info(f"   ğŸ” íŒ¨í„´ ë§¤í•‘: {stats['pattern_mappings']}ê°œ")
        self.logger.info(f"   ğŸ·ï¸ í‚¤ì›Œë“œ ë§¤í•‘: {stats['keyword_mappings']}ê°œ")
        self.logger.info(f"   â“ ë¯¸ë§¤í•‘: {stats['unmapped_files']}ê°œ")

# ==============================================
# ğŸ”¥ 6. ë¹ ì§„ í•µì‹¬ í´ë˜ìŠ¤ë“¤ ì¶”ê°€ (ì›ë³¸ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„)
# ==============================================

@dataclass 
class ModelFileInfo:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ModelFileInfo í´ë˜ìŠ¤"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ë¥˜"""
    RESNET = "resnet"
    EFFICIENTNET = "efficientnet"
    VIT = "vision_transformer"
    DIFFUSION = "diffusion"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    RNN = "rnn"
    UNET = "unet"
    GAN = "gan"
    AUTOENCODER = "autoencoder"
    UNKNOWN = "unknown"

@dataclass
class ModelMetadata:
    """ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    framework: str = "pytorch"
    version: str = ""
    training_dataset: str = ""
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    optimization_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)

class AdvancedModelLoaderAdapter:
    """ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_comprehensive_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ ì„¤ì • ìƒì„±"""
        return generate_advanced_model_loader_config(self.detector)

class RealModelLoaderConfigGenerator:
    """ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„±ê¸° (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        config = {
            "version": "real_detector_v1.0",
            "models": {},
            "device": "mps" if self.detector.is_m3_max else "cpu",
            "optimization_enabled": True
        }
        
        for name, model in detected_models.items():
            config["models"][name] = model.to_dict()
        
        return config

# ==============================================
# ğŸ”¥ 7. ë¹ ì§„ ModelLoader ë“±ë¡ ê¸°ëŠ¥ë“¤ (í•µì‹¬!)
# ==============================================

def register_detected_models_to_loader(model_loader_instance=None) -> int:
    """íƒì§€ëœ ëª¨ë“  ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        if model_loader_instance is None:
            try:
                # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° import
                from . import model_loader as ml_module
                model_loader_instance = ml_module.get_global_model_loader()
            except ImportError:
                logger.error("âŒ ModelLoader import ì‹¤íŒ¨")
                return 0
        
        # ëª¨ë¸ íƒì§€
        detector = get_global_detector()
        detected_models = detector.detect_all_models()
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return 0
        
        registered_count = 0
        
        for model_name, model_info in detected_models.items():
            try:
                # ModelLoaderìš© ì„¤ì • ìƒì„±
                model_config = create_model_config_for_loader(model_info)
                
                # ModelLoaderì— ë“±ë¡
                if register_single_model_to_loader(model_loader_instance, model_name, model_config):
                    registered_count += 1
                    logger.debug(f"âœ… {model_name} ModelLoader ë“±ë¡ ì„±ê³µ")
                else:
                    logger.warning(f"âš ï¸ {model_name} ModelLoader ë“±ë¡ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {model_name} ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ğŸ‰ ModelLoader ë“±ë¡ ì™„ë£Œ: {registered_count}/{len(detected_models)}ê°œ ëª¨ë¸")
        return registered_count
        
    except Exception as e:
        logger.error(f"âŒ ModelLoader ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return 0

def register_single_model_to_loader(model_loader, model_name: str, model_config: Dict[str, Any]) -> bool:
    """ë‹¨ì¼ ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
    try:
        # ModelLoaderê°€ ê°€ì§€ê³  ìˆëŠ” ë“±ë¡ ë©”ì„œë“œ ì‹œë„
        registration_methods = [
            'register_model',
            'register_model_config', 
            'add_model',
            'load_model_config',
            'set_model_config'
        ]
        
        for method_name in registration_methods:
            if hasattr(model_loader, method_name):
                method = getattr(model_loader, method_name)
                try:
                    # ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ì— ë”°ë¼ í˜¸ì¶œ
                    if method_name == 'register_model_config':
                        result = method(model_name, model_config)
                    else:
                        result = method(model_name, model_config)
                    
                    if result:
                        logger.debug(f"âœ… {model_name} ë“±ë¡ ì„±ê³µ (ë©”ì„œë“œ: {method_name})")
                        return True
                        
                except Exception as e:
                    logger.debug(f"âš ï¸ {method_name} ì‹œë„ ì‹¤íŒ¨: {e}")
                    continue
        
        # ì§ì ‘ ì†ì„± ì„¤ì • ì‹œë„
        if hasattr(model_loader, 'model_configs'):
            model_loader.model_configs[model_name] = model_config
            logger.debug(f"âœ… {model_name} ì§ì ‘ ë“±ë¡ ì„±ê³µ")
            return True
        
        if hasattr(model_loader, 'models'):
            model_loader.models[model_name] = model_config
            logger.debug(f"âœ… {model_name} models ì†ì„± ë“±ë¡ ì„±ê³µ")
            return True
        
        logger.warning(f"âš ï¸ {model_name} ë“±ë¡ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False
        
    except Exception as e:
        logger.error(f"âŒ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def create_model_config_for_loader(model_info: DetectedModel) -> Dict[str, Any]:
    """ModelLoaderìš© ëª¨ë¸ ì„¤ì • ìƒì„±"""
    try:
        # ê¸°ë³¸ ModelConfig êµ¬ì¡°
        config = {
            # ê¸°ë³¸ ì •ë³´
            "name": model_info.name,
            "model_type": model_info.model_type,
            "model_class": f"{model_info.step_name}Model",  # í´ë˜ìŠ¤ëª… ìƒì„±
            "step_name": model_info.step_name,
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ (í•µì‹¬!)
            "checkpoint_path": str(model_info.path),
            "checkpoint_validated": model_info.checkpoint_validated,
            "file_size_mb": model_info.file_size_mb,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device": model_info.recommended_device,
            "precision": model_info.precision,
            "device_compatible": model_info.device_compatible,
            
            # ì…ë ¥/ì¶œë ¥ ì„¤ì •
            "input_size": model_info.step_config.get("input_size", [3, 512, 512]),
            "preprocessing": model_info.step_config.get("preprocessing", "standard"),
            
            # ë¡œë”© ì„¤ì •
            "lazy_loading": model_info.loading_config.get("lazy_loading", False),
            "memory_mapping": model_info.loading_config.get("memory_mapping", False),
            "batch_size": model_info.loading_config.get("batch_size", 1),
            
            # ìµœì í™” ì„¤ì •
            "optimization": model_info.optimization_config,
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "auto_detected": True,
                "confidence": model_info.confidence_score,
                "detection_time": time.time(),
                "priority": model_info.priority.value,
                "pytorch_valid": model_info.pytorch_valid,
                "parameter_count": model_info.parameter_count
            }
        }
        
        # Stepë³„ íŠ¹í™” ì„¤ì • ì¶”ê°€
        step_specific = get_step_specific_loader_config(model_info.step_name, model_info)
        config.update(step_specific)
        
        return config
        
    except Exception as e:
        logger.error(f"âŒ {model_info.name} ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {}

def get_step_specific_loader_config(step_name: str, model_info: DetectedModel) -> Dict[str, Any]:
    """Stepë³„ ModelLoader íŠ¹í™” ì„¤ì •"""
    
    configs = {
        "HumanParsingStep": {
            "num_classes": 20,
            "output_channels": 20,
            "task_type": "segmentation",
            "loss_function": "cross_entropy",
            "metrics": ["accuracy", "iou"]
        },
        
        "PoseEstimationStep": {
            "num_keypoints": 17,
            "heatmap_size": [64, 48],
            "task_type": "keypoint_detection", 
            "sigma": 2.0,
            "metrics": ["pck", "accuracy"]
        },
        
        "ClothSegmentationStep": {
            "num_classes": 2,
            "task_type": "binary_segmentation",
            "threshold": 0.5,
            "apply_morphology": True,
            "metrics": ["iou", "dice"]
        },
        
        "VirtualFittingStep": {
            "model_architecture": "diffusion",
            "scheduler_type": "DDIM",
            "guidance_scale": 7.5,
            "num_inference_steps": 20,
            "enable_attention_slicing": model_info.file_size_mb > 2000,
            "enable_vae_slicing": model_info.file_size_mb > 4000,
            "enable_cpu_offload": model_info.file_size_mb > 8000,
            "metrics": ["fid", "lpips", "quality_score"]
        },
        
        "GeometricMatchingStep": {
            "transformation_type": "TPS",
            "grid_size": [5, 5],
            "task_type": "geometric_transformation",
            "metrics": ["geometric_error", "warping_quality"]
        },
        
        "ClothWarpingStep": {
            "warping_method": "TOM",
            "blending_enabled": True,
            "task_type": "image_warping",
            "metrics": ["warping_error", "visual_quality"]
        }
    }
    
    return configs.get(step_name, {
        "task_type": "general",
        "metrics": ["accuracy"]
    })

# ==============================================
# ğŸ”¥ 8. ê²€ì¦ ë° ì„¤ì • ìƒì„± í•¨ìˆ˜ë“¤ (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)
# ==============================================

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """ëª¨ë¸ ê²½ë¡œ ê²€ì¦"""
    valid_models = []
    invalid_models = []
    
    for name, model in detected_models.items():
        if model.path.exists() and os.access(model.path, os.R_OK):
            valid_models.append({
                "name": name,
                "path": str(model.path),
                "size_mb": model.file_size_mb,
                "step": model.step_name
            })
        else:
            invalid_models.append({
                "name": name,
                "path": str(model.path),
                "error": "File not found or not readable"
            })
    
    return {
        "valid_models": valid_models,
        "invalid_models": invalid_models,
        "summary": {
            "total_models": len(detected_models),
            "valid_count": len(valid_models),
            "invalid_count": len(invalid_models),
            "validation_rate": len(valid_models) / len(detected_models) if detected_models else 0
        }
    }

def generate_real_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ModelLoader ì„¤ì • ìƒì„± (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    if detector is None:
        detector = get_global_detector()
        detector.detect_all_models()
    
    config = {
        "device": "mps" if detector.is_m3_max else "cpu",
        "optimization_enabled": True,
        "use_fp16": detector.is_m3_max,
        "models": {},
        "step_mappings": {},
        "metadata": {
            "generator_version": "core_detector_v1.0",
            "total_models": len(detector.detected_models),
            "generation_timestamp": time.time(),
            "conda_env": detector.conda_env,
            "is_m3_max": detector.is_m3_max
        }
    }
    
    for name, model in detector.detected_models.items():
        config["models"][name] = model.to_dict()
        
        # Step ë§¤í•‘
        if model.step_name not in config["step_mappings"]:
            config["step_mappings"][model.step_name] = []
        config["step_mappings"][model.step_name].append(name)
    
    return config

def create_advanced_model_loader_adapter(detector: RealWorldModelDetector) -> AdvancedModelLoaderAdapter:
    """ê³ ê¸‰ ModelLoader ì–´ëŒ‘í„° ìƒì„± (ì›ë³¸ì—ì„œ ëˆ„ë½ë¨)"""
    return AdvancedModelLoaderAdapter(detector)

# ==============================================
# ğŸ”¥ 9. ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ëª¨ë“  í•¨ìˆ˜ ìœ ì§€)
# ==============================================

def list_available_models(step_class: Optional[str] = None, 
                         model_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    result = []
    for model in models.values():
        model_dict = model.to_dict()
        
        # í•„í„°ë§
        if step_class and model_dict["step_class"] != step_class:
            continue
        if model_type and model_dict["model_type"] != model_type:
            continue
        
        result.append(model_dict)
    
    # ì‹ ë¢°ë„ ìˆœ ì •ë ¬
    result.sort(key=lambda x: x["confidence"], reverse=True)
    return result

def register_step_requirements(step_name: str, requirements: Dict[str, Any]) -> bool:
    """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    try:
        detector = get_global_detector()
        if not hasattr(detector, 'step_requirements'):
            detector.step_requirements = {}
        
        detector.step_requirements[step_name] = requirements
        logger.debug(f"âœ… {step_name} ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"âŒ {step_name} ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def create_step_interface(step_name: str, config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoader ìš”êµ¬ì‚¬í•­)"""
    try:
        models = get_models_for_step(step_name)
        if not models:
            return None
        
        best_model = models[0]
        
        return {
            "step_name": step_name,
            "primary_model": best_model,
            "fallback_models": models[1:3],
            "config": config or {},
            "device": best_model.get("device_config", {}).get("recommended_device", "cpu"),
            "optimization": best_model.get("optimization_config", {}),
            "created_at": time.time()
        }
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def get_models_for_step(step_name: str) -> List[Dict[str, Any]]:
    """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
    models = list_available_models(step_class=step_name)
    return sorted(models, key=lambda x: x["confidence"], reverse=True)

def validate_model_exists(model_name: str) -> bool:
    """ëª¨ë¸ ì¡´ì¬ í™•ì¸"""
    detector = get_global_detector()
    return model_name in detector.detected_models

def generate_advanced_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ğŸ”¥ ê³ ê¸‰ ModelLoader ì„¤ì • ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
    try:
        if detector is None:
            detector = get_global_detector()
            detector.detect_all_models()
        
        detected_models = detector.detected_models
        
        # M3 Max ê°ì§€
        is_m3_max = detector.is_m3_max
        device_type = "mps" if is_m3_max else "cpu"
        
        config = {
            # ê¸°ë³¸ ì •ë³´
            "version": "enhanced_detector_v2.0",
            "generated_at": time.time(),
            "device": device_type,
            "is_m3_max": is_m3_max,
            "conda_env": detector.conda_env,
            
            # ì „ì—­ ì„¤ì •
            "optimization_enabled": True,
            "use_fp16": device_type != "cpu",
            "enable_compilation": is_m3_max,
            "memory_efficient": True,
            
            # ğŸ”¥ ê°•í™”ëœ ë§¤í•‘ ì •ë³´ í¬í•¨
            "mapping_stats": detector.mapping_stats,
            
            # ëª¨ë¸ ì„¤ì •ë“¤
            "models": {},
            "step_mappings": {},
            "device_optimization": {
                "target_device": device_type,
                "precision": "fp16" if device_type != "cpu" else "fp32",
                "enable_attention_slicing": True,
                "enable_vae_slicing": True,
                "enable_cpu_offload": False,
                "memory_fraction": 0.8
            },
            
            # ì„±ëŠ¥ ìµœì í™”
            "performance_config": {
                "lazy_loading": True,
                "memory_mapping": True,
                "concurrent_loading": False,
                "cache_models": True,
                "preload_critical": True
            },
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "total_models": len(detected_models),
                "total_size_gb": sum(m.file_size_mb for m in detected_models.values()) / 1024,
                "search_paths": [str(p) for p in detector.search_paths],
                "generation_duration": 0,
                "pytorch_available": TORCH_AVAILABLE
            }
        }
        
        # ê° ëª¨ë¸ë³„ ì„¤ì • ìƒì„±
        for model_name, model in detected_models.items():
            model_config = model.to_dict()
            config["models"][model_name] = model_config
            
            # Step ë§¤í•‘ ì¶”ê°€
            step_name = model.step_name
            if step_name not in config["step_mappings"]:
                config["step_mappings"][step_name] = []
            config["step_mappings"][step_name].append(model_name)
        
        # Stepë³„ ì„¤ì • ìƒì„±
        config["step_configurations"] = {}
        for step_name, model_names in config["step_mappings"].items():
            step_models = [config["models"][name] for name in model_names]
            primary_model = max(step_models, key=lambda x: x["confidence"]) if step_models else None
            
            config["step_configurations"][step_name] = {
                "primary_model": primary_model["name"] if primary_model else None,
                "fallback_models": [m["name"] for m in sorted(step_models, key=lambda x: x["confidence"], reverse=True)[1:3]],
                "model_count": len(step_models),
                "total_size_mb": sum(m["size_mb"] for m in step_models),
                "requires_preloading": any(m.get("preload", False) for m in step_models),
                "step_ready": len(step_models) > 0
            }
        
        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        config["summary"] = {
            "total_models": len(config["models"]),
            "total_steps": len(config["step_configurations"]),
            "ready_steps": sum(1 for s in config["step_configurations"].values() if s["step_ready"]),
            "total_size_gb": sum(m["size_mb"] for m in config["models"].values()) / 1024,
            "validated_count": sum(1 for m in config["models"].values() if m.get("pytorch_valid", False)),
            "device_optimized": device_type != "cpu",
            "ready_for_production": len(config["models"]) > 0
        }
        
        logger.info(f"âœ… ê°•í™”ëœ ModelLoader ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
        return config
        
    except Exception as e:
        logger.error(f"âŒ ê°•í™”ëœ ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "error": str(e),
            "version": "enhanced_detector_v2.0_error",
            "generated_at": time.time(),
            "models": {},
            "step_mappings": {},
            "success": False
        }

def quick_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """ë¹ ë¥¸ ëª¨ë¸ íƒì§€ (model_loader.pyì—ì„œ ì‚¬ìš©)"""
    detector = get_global_detector()
    return detector.detect_all_models(**kwargs)

def comprehensive_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """í¬ê´„ì ì¸ ëª¨ë¸ íƒì§€ (model_loader.pyì—ì„œ ì‚¬ìš©)"""
    kwargs['enable_pytorch_validation'] = kwargs.get('enable_pytorch_validation', True)
    return quick_model_detection(**kwargs)

def create_real_world_detector(**kwargs) -> RealWorldModelDetector:
    """íƒì§€ê¸° ìƒì„± (model_loader.pyì—ì„œ ì‚¬ìš©)"""
    return RealWorldModelDetector(**kwargs)

# ==============================================
# ğŸ”¥ 7. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

_global_detector: Optional[RealWorldModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector() -> RealWorldModelDetector:
    """ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤"""
    global _global_detector
    if _global_detector is None:
        with _detector_lock:
            if _global_detector is None:
                _global_detector = RealWorldModelDetector()
    return _global_detector

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
create_advanced_detector = create_real_world_detector

# ==============================================
# ğŸ”¥ 8. ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„± 100% ìœ ì§€)
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
    'RealWorldModelDetector',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
    'create_real_world_detector',
    'create_advanced_detector',
    'quick_model_detection',
    'comprehensive_model_detection',
    
    # ModelLoader ì¸í„°í˜ì´ìŠ¤ (í•„ìˆ˜!)
    'list_available_models',
    'register_step_requirements',
    'create_step_interface',
    'get_models_for_step',
    'validate_model_exists',
    
    # ModelLoader í•µì‹¬ í•¨ìˆ˜
    'generate_advanced_model_loader_config',
    
    # ì „ì—­ í•¨ìˆ˜
    'get_global_detector',
    
    # ğŸ”¥ ê°•í™”ëœ í•¨ìˆ˜ë“¤
    'enhanced_match_file_to_step',
    'enhanced_calculate_confidence',
    'enhanced_find_ai_models_paths',
    'ENHANCED_STEP_MODEL_PATTERNS'
]

# ==============================================
# ğŸ”¥ 11. ì´ˆê¸°í™” ë° ë¡œê¹… (ì›ë³¸ ì •ë³´ ìœ ì§€)
# ==============================================

logger.info("âœ… ê°•í™”ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° ë¡œë“œ ì™„ë£Œ (v2.0) - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨")
logger.info("ğŸ¯ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì‹œìŠ¤í…œ ê°•í™”")
logger.info("ğŸ”¥ 544ê°œ ì²´í¬í¬ì¸íŠ¸ â†’ Step ë§¤í•‘ 100% ì§€ì›")
logger.info("ğŸ”— model_loader.pyì™€ ì™„ë²½ ì—°ë™")
logger.info("âš¡ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥")
logger.info("ğŸ”§ ModelLoader í•„ìˆ˜ ì¸í„°í˜ì´ìŠ¤ 100% êµ¬í˜„")
logger.info("ğŸ”¥ generate_advanced_model_loader_config í•¨ìˆ˜ ì™„ì „ êµ¬í˜„")
logger.info("âœ… BaseStepMixin ì™„ë²½ í˜¸í™˜ - ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„")
logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼(.pth, .bin) ë¡œë”©ì— ì§‘ì¤‘")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("âœ… M3 Max 128GB ìµœì í™”")
logger.info("âœ… ë¹„ë™ê¸°/ë™ê¸° ëª¨ë‘ ì§€ì›")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
try:
    _test_detector = get_global_detector()
    logger.info("ğŸš€ ê°•í™”ëœ íƒì§€ê¸° ì¤€ë¹„ ì™„ë£Œ!")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 12. ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©) - ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€
# ==============================================

if __name__ == "__main__":
    print("ğŸ” ê°•í™”ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° + ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ê°•í™”ëœ íƒì§€ í…ŒìŠ¤íŠ¸
    print("ğŸ“ 1ë‹¨ê³„: ê°•í™”ëœ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸")
    models = quick_model_detection()
    print(f"   íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    if models:
        # Stepë³„ ë¶„ë¥˜
        step_groups = {}
        for model in models.values():
            step = model.step_name
            if step not in step_groups:
                step_groups[step] = []
            step_groups[step].append(model)
        
        print(f"   Stepë³„ ë¶„ë¥˜:")
        for step, step_models in step_groups.items():
            print(f"   {step}: {len(step_models)}ê°œ")
            for model in step_models[:2]:  # ê° Stepì—ì„œ ìƒìœ„ 2ê°œë§Œ
                patterns = ", ".join(model.matched_patterns[:3])
                print(f"     - {model.name} ({model.confidence_score:.2f}, {patterns})")
    
    # 2. ë§¤í•‘ í†µê³„ í™•ì¸
    print(f"\nğŸ“Š 2ë‹¨ê³„: ë§¤í•‘ í†µê³„ í™•ì¸")
    detector = get_global_detector()
    if hasattr(detector, 'mapping_stats'):
        stats = detector.mapping_stats
        print(f"   ì§ì ‘ ë§¤í•‘: {stats.get('direct_mappings', 0)}ê°œ")
        print(f"   íŒ¨í„´ ë§¤í•‘: {stats.get('pattern_mappings', 0)}ê°œ")
        print(f"   í‚¤ì›Œë“œ ë§¤í•‘: {stats.get('keyword_mappings', 0)}ê°œ")
        print(f"   ë¯¸ë§¤í•‘: {stats.get('unmapped_files', 0)}ê°œ")
    
    print("\nğŸ‰ ê°•í™”ëœ íƒì§€ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")