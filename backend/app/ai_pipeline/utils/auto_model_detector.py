# app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ” MyCloset AI - ì™„ì „ í†µí•© ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v6.0
âœ… 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ ë™ì‘í•˜ëŠ” íƒì§€ ë¡œì§ ì™„ì „ ë°˜ì˜
âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ íŒŒì¼ë“¤ ì •í™•í•œ íƒì§€
âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ê²€ì¦
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì—°ë™)
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ íŠ¹í™” ìŠ¤ìº”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥

ğŸ”¥ í•µì‹¬ ë³€ê²½ì‚¬í•­:
- 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ íƒì§€ íŒ¨í„´ 100% ë°˜ì˜
- PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ì‹¤ì œ ê²€ì¦
- íŒŒì¼ í¬ê¸°ì™€ ë§¤ê°œë³€ìˆ˜ ìˆ˜ ì‹¤ì œ í™•ì¸
- ModelLoader ì§ì ‘ import ì œê±°
- ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì„¤ì • ì¶œë ¥
- ì‹¤ì œ ë™ì‘í•˜ëŠ” íƒì§€ ë¡œì§ë§Œ ì‚¬ìš©
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import weakref

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì•ˆì „í•œ import)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹¤ì œ 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ë™ì‘í•˜ëŠ” íŒ¨í„´ ë°˜ì˜
# ==============================================

class ModelCategory(Enum):
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•„ìˆ˜ ëª¨ë¸
    HIGH = 2          # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = 3        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    LOW = 4           # ë‚®ì€ ìš°ì„ ìˆœìœ„
    EXPERIMENTAL = 5  # ì‹¤í—˜ì  ëª¨ë¸

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ (ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì—°ë™ìš©)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str  # ì—°ê²°ëœ Step í´ë˜ìŠ¤ëª…
    metadata: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    requirements: List[str] = field(default_factory=list)
    performance_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)
    last_modified: float = 0.0
    checksum: Optional[str] = None
    pytorch_valid: bool = False
    parameter_count: int = 0

# ==============================================
# ğŸ”¥ 2ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ ëª¨ë¸ ì •ì˜ íŒ¨í„´ 100% ë°˜ì˜
# ==============================================

@dataclass
class ModelFileInfo:
    """2ë²ˆ íŒŒì¼ì˜ ModelFileInfoì™€ 100% í˜¸í™˜"""
    name: str
    patterns: List[str]
    step: str
    required: bool
    min_size_mb: float
    max_size_mb: float
    target_path: str
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

# ì‹¤ì œ ë°œê²¬ëœ íŒŒì¼ë“¤ ê¸°ë°˜ìœ¼ë¡œ íŒ¨í„´ ìˆ˜ì •
ACTUAL_MODEL_PATTERNS = {
    "human_parsing": ModelFileInfo(
        name="human_parsing_graphonomy",
        patterns=[
            r".*checkpoints/human_parsing/.*\.pth$",
            r".*schp.*atr.*\.pth$", 
            r".*atr_model.*\.pth$",
            r".*lip_model.*\.pth$"
        ],
        step="step_01_human_parsing",
        required=True,
        min_size_mb=1,
        max_size_mb=500,
        target_path="ai_models/checkpoints/human_parsing/atr_model.pth",
        alternative_names=["schp_atr.pth", "atr_model.pth", "lip_model.pth"],
        keywords=["human", "parsing", "atr", "schp", "lip"],
        expected_layers=["backbone", "classifier", "conv"]
    ),
    
    "cloth_segmentation": ModelFileInfo(
        name="cloth_segmentation_u2net", 
        patterns=[
            r".*checkpoints/step_03.*u2net.*\.pth$",
            r".*u2net_segmentation.*\.pth$",
            r".*sam.*vit.*\.pth$"
        ],
        step="step_03_cloth_segmentation",
        required=True, 
        min_size_mb=10,
        max_size_mb=3000,
        target_path="ai_models/checkpoints/step_03/u2net_segmentation/u2net.pth",
        alternative_names=["u2net.pth", "sam_vit_h_4b8939.pth", "sam_vit_b_01ec64.pth"],
        keywords=["u2net", "segmentation", "sam"],
        expected_layers=["encoder", "decoder", "outconv"]
    ),
    
    "virtual_fitting": ModelFileInfo(
        name="virtual_fitting_ootd", 
        patterns=[
            r".*step_06_virtual_fitting.*\.bin$",
            r".*ootd.*unet.*\.bin$",
            r".*OOTDiffusion.*"
        ],
        step="virtual_fitting",
        required=True,
        min_size_mb=100, 
        max_size_mb=8000,
        target_path="ai_models/step_06_virtual_fitting/ootd_hd_unet.bin",
        alternative_names=["ootd_hd_unet.bin", "ootd_dc_unet.bin"],
        keywords=["ootd", "unet", "diffusion", "virtual"],
        expected_layers=["unet", "vae"],
        file_types=['.bin', '.pth', '.pt', '.safetensors']
    )
}
# 3ë²ˆ íŒŒì¼ì˜ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ë„ ë°˜ì˜
CHECKPOINT_VERIFICATION_PATTERNS = {
    "human_parsing": {
        "keywords": ["human", "parsing", "atr", "schp", "graphonomy", "segmentation"],
        "expected_size_range": (50, 500),  # MB
        "required_layers": ["backbone", "classifier", "conv"],
        "typical_parameters": (25000000, 70000000)  # 25M ~ 70M íŒŒë¼ë¯¸í„°
    },
    "pose_estimation": {
        "keywords": ["pose", "openpose", "body", "keypoint", "coco"],
        "expected_size_range": (10, 1000),
        "required_layers": ["stage", "paf", "heatmap"],
        "typical_parameters": (10000000, 200000000)  # 10M ~ 200M íŒŒë¼ë¯¸í„°
    },
    "cloth_segmentation": {
        "keywords": ["u2net", "cloth", "segmentation", "mask", "sam"],
        "expected_size_range": (10, 3000),
        "required_layers": ["encoder", "decoder", "outconv"],
        "typical_parameters": (4000000, 650000000)  # 4M ~ 650M íŒŒë¼ë¯¸í„° (SAM í¬í•¨)
    },
    "geometric_matching": {
        "keywords": ["gmm", "geometric", "tps", "matching", "alignment"],
        "expected_size_range": (1, 100),
        "required_layers": ["correlation", "regression", "flow"],
        "typical_parameters": (500000, 50000000)  # 0.5M ~ 50M íŒŒë¼ë¯¸í„°
    },
    "cloth_warping": {
        "keywords": ["tom", "warping", "cloth", "viton", "try"],
        "expected_size_range": (10, 4000),
        "required_layers": ["generator", "discriminator", "warp"],
        "typical_parameters": (10000000, 1000000000)  # 10M ~ 1B íŒŒë¼ë¯¸í„°
    },
    "virtual_fitting": {
        "keywords": ["diffusion", "viton", "unet", "stable", "fitting"],
        "expected_size_range": (100, 8000),
        "required_layers": ["unet", "vae", "text_encoder"],
        "typical_parameters": (100000000, 2000000000)  # 100M ~ 2B íŒŒë¼ë¯¸í„°
    }
}

# ==============================================
# ğŸ”¥ ì‹¤ì œ ë™ì‘í•˜ëŠ” ê³ ê¸‰ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class RealWorldModelDetector:
    """
    ğŸ” ì‹¤ì œ ë™ì‘í•˜ëŠ” AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v6.0
    âœ… 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ íƒì§€ ë¡œì§ 100% ë°˜ì˜
    âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ì‹¤ì œ ê²€ì¦
    âœ… ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì¶œë ¥ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_pytorch_validation: bool = True,
        enable_caching: bool = True,
        cache_db_path: Optional[Path] = None,
        max_workers: int = 4,
        scan_timeout: int = 300
    ):
        """ì‹¤ì œ ë™ì‘í•˜ëŠ” ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        
        # ì‹¤ì œ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • (2ë²ˆ íŒŒì¼ ë°©ì‹ ë°˜ì˜)
        if search_paths is None:
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parents[3]  # app/ai_pipeline/utilsì—ì„œ backendë¡œ
            
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²½ë¡œë“¤ë§Œ ì¶”ê°€
            self.search_paths = self._get_real_search_paths(backend_dir)
        else:
            self.search_paths = search_paths
        
        # ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        
        # íƒì§€ ê²°ê³¼ ì €ì¥
        self.detected_models: Dict[str, DetectedModel] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "pytorch_files_found": 0,
            "valid_pytorch_models": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "pytorch_validation_errors": 0
        }
        
        # ìºì‹œ ê´€ë¦¬
        self.cache_db_path = cache_db_path or Path("real_model_detection_cache.db")
        self.cache_ttl = 86400  # 24ì‹œê°„
        self._cache_lock = threading.RLock()
        
        self.logger.info(f"ğŸ” ì‹¤ì œ ë™ì‘ ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
        # ìºì‹œ DB ì´ˆê¸°í™”
        if self.enable_caching:
            self._init_cache_db()

    def _get_real_search_paths(self, backend_dir: Path) -> List[Path]:
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²€ìƒ‰ ê²½ë¡œë“¤ë§Œ ë°˜í™˜ (2ë²ˆ íŒŒì¼ ë°©ì‹)"""
        potential_paths = [
            # í”„ë¡œì íŠ¸ ë‚´ë¶€ ê²½ë¡œë“¤
            backend_dir / "ai_models",
            backend_dir / "app" / "ai_pipeline" / "models",
            backend_dir / "app" / "models",
            backend_dir / "checkpoints",
            backend_dir / "models",
            backend_dir / "weights",
            
            # ìƒìœ„ ë””ë ‰í† ë¦¬
            backend_dir.parent / "ai_models",
            backend_dir.parent / "models",
            
            # ì‚¬ìš©ì ìºì‹œ ê²½ë¡œë“¤
            Path.home() / ".cache" / "huggingface",
            Path.home() / ".cache" / "torch",
            Path.home() / ".cache" / "models",
            Path.home() / "Downloads",
            
            # conda í™˜ê²½ ê²½ë¡œë“¤
            *self._get_conda_paths()
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ë§Œ í•„í„°ë§
        real_paths = []
        for path in potential_paths:
            try:
                if path.exists() and path.is_dir():
                    # ì½ê¸° ê¶Œí•œ í™•ì¸
                    if os.access(path, os.R_OK):
                        real_paths.append(path)
                        self.logger.debug(f"âœ… ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {path}")
                    else:
                        self.logger.debug(f"âŒ ê¶Œí•œ ì—†ìŒ: {path}")
                else:
                    self.logger.debug(f"âŒ ê²½ë¡œ ì—†ìŒ: {path}")
            except Exception as e:
                self.logger.debug(f"âŒ ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨ {path}: {e}")
                continue
        
        return real_paths

    def _get_conda_paths(self) -> List[Path]:
        """conda í™˜ê²½ ê²½ë¡œë“¤ íƒì§€"""
        conda_paths = []
        
        try:
            # í˜„ì¬ conda í™˜ê²½
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix and Path(conda_prefix).exists():
                conda_base = Path(conda_prefix)
                conda_paths.extend([
                    conda_base / "lib" / "python3.11" / "site-packages",
                    conda_base / "share" / "models",
                    conda_base / "models"
                ])
            
            # conda ë£¨íŠ¸
            conda_root = os.environ.get('CONDA_ROOT')
            if not conda_root:
                # ì¼ë°˜ì ì¸ conda ì„¤ì¹˜ ê²½ë¡œë“¤
                possible_roots = [
                    Path.home() / "miniforge3",
                    Path.home() / "miniconda3",
                    Path.home() / "anaconda3",
                    Path("/opt/conda"),
                    Path("/usr/local/conda")
                ]
                for root in possible_roots:
                    if root.exists():
                        conda_root = str(root)
                        break
            
            if conda_root and Path(conda_root).exists():
                conda_paths.append(Path(conda_root) / "pkgs")
                
        except Exception as e:
            self.logger.debug(f"conda ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return conda_paths

    def _init_cache_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS real_model_cache (
                        file_path TEXT PRIMARY KEY,
                        file_size INTEGER,
                        file_mtime REAL,
                        checksum TEXT,
                        pytorch_valid INTEGER,
                        parameter_count INTEGER,
                        detection_data TEXT,
                        created_at REAL,
                        accessed_at REAL
                    )
                """)
                
                conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_real_accessed_at ON real_model_cache(accessed_at)
                """)
                
                conn.commit()
                
            self.logger.debug("âœ… ì‹¤ì œ ëª¨ë¸ ìºì‹œ DB ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ìºì‹œ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enable_caching = False

    def detect_all_models(
        self, 
        force_rescan: bool = False,
        categories_filter: Optional[List[ModelCategory]] = None,
        min_confidence: float = 0.3,
        model_type_filter: Optional[List[str]] = None
    ) -> Dict[str, DetectedModel]:
        """
        ì‹¤ì œ AI ëª¨ë¸ ìë™ íƒì§€ (2ë²ˆ,3ë²ˆ íŒŒì¼ ë¡œì§ ë°˜ì˜)
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            model_type_filter: íŠ¹ì • ëª¨ë¸ íƒ€ì…ë§Œ íƒì§€
            
        Returns:
            Dict[str, DetectedModel]: íƒì§€ëœ ëª¨ë¸ë“¤
        """
        try:
            self.logger.info("ğŸ” ì‹¤ì œ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.enable_caching:
                cached_results = self._load_from_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ ëª¨ë¸")
                    self.scan_stats["cache_hits"] += len(cached_results)
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self._reset_scan_stats()
            
            # ëª¨ë¸ íƒ€ì… í•„í„°ë§
            if model_type_filter:
                filtered_patterns = {k: v for k, v in ACTUAL_MODEL_PATTERNS.items() 
                                   if k in model_type_filter}
            else:
                filtered_patterns = ACTUAL_MODEL_PATTERNS
            
            # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
            if self.max_workers > 1:
                self._parallel_scan_real_models(filtered_patterns, categories_filter, min_confidence)
            else:
                self._sequential_scan_real_models(filtered_patterns, categories_filter, min_confidence)
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self.scan_stats["models_detected"] = len(self.detected_models)
            self.scan_stats["scan_duration"] = time.time() - start_time
            self.scan_stats["last_scan_time"] = time.time()
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            self._post_process_results(min_confidence)
            
            # ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache()
            
            self.logger.info(f"âœ… ì‹¤ì œ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.scan_stats["errors_encountered"] += 1
            raise

    def _parallel_scan_real_models(self, model_patterns: Dict, categories_filter, min_confidence):
        """ì‹¤ì œ ëª¨ë¸ë“¤ ë³‘ë ¬ ìŠ¤ìº”"""
        try:
            # ê²€ìƒ‰ íƒœìŠ¤í¬ ìƒì„±
            scan_tasks = []
            for model_type, pattern_info in model_patterns.items():
                for search_path in self.search_paths:
                    if search_path.exists():
                        scan_tasks.append((model_type, pattern_info, search_path))
            
            if not scan_tasks:
                self.logger.warning("âš ï¸ ìŠ¤ìº”í•  ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {
                    executor.submit(
                        self._scan_path_for_real_models, 
                        model_type, 
                        pattern_info, 
                        search_path, 
                        categories_filter, 
                        min_confidence
                    ): (model_type, search_path)
                    for model_type, pattern_info, search_path in scan_tasks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                completed_count = 0
                for future in as_completed(future_to_task, timeout=self.scan_timeout):
                    model_type, search_path = future_to_task[future]
                    try:
                        path_results = future.result()
                        if path_results:
                            # ê²°ê³¼ ë³‘í•© (ìŠ¤ë ˆë“œ ì•ˆì „)
                            with threading.Lock():
                                for name, model in path_results.items():
                                    self._register_detected_model_safe(model)
                        
                        completed_count += 1
                        self.logger.debug(f"âœ… {model_type} @ {search_path} ìŠ¤ìº” ì™„ë£Œ ({completed_count}/{len(scan_tasks)})")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ {model_type} @ {search_path} ìŠ¤ìº” ì‹¤íŒ¨: {e}")
                        self.scan_stats["errors_encountered"] += 1
                        
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆœì°¨ ìŠ¤ìº”
            self._sequential_scan_real_models(model_patterns, categories_filter, min_confidence)

    def _sequential_scan_real_models(self, model_patterns: Dict, categories_filter, min_confidence):
        """ì‹¤ì œ ëª¨ë¸ë“¤ ìˆœì°¨ ìŠ¤ìº”"""
        try:
            for model_type, pattern_info in model_patterns.items():
                self.logger.debug(f"ğŸ“ {model_type} ëª¨ë¸ íŒ¨í„´ ìŠ¤ìº” ì¤‘...")
                
                for search_path in self.search_paths:
                    if search_path.exists():
                        path_results = self._scan_path_for_real_models(
                            model_type, pattern_info, search_path, categories_filter, min_confidence
                        )
                        if path_results:
                            for name, model in path_results.items():
                                self._register_detected_model_safe(model)
                    else:
                        self.logger.debug(f"âš ï¸ ê²½ë¡œ ì—†ìŒ: {search_path}")
                        
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ìŠ¤ìº” ì‹¤íŒ¨: {e}")

    def _scan_path_for_real_models(
        self, 
        model_type: str, 
        pattern_info: ModelFileInfo, 
        search_path: Path, 
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float,
        max_depth: int = 6,
        current_depth: int = 0
    ) -> Dict[str, DetectedModel]:
        """ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ ìŠ¤ìº” (2ë²ˆ,3ë²ˆ íŒŒì¼ ë¡œì§ ë°˜ì˜)"""
        results = {}
        
        try:
            if current_depth > max_depth:
                return results
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ë‚˜ì—´
            try:
                items = list(search_path.iterdir())
            except PermissionError:
                self.logger.debug(f"ê¶Œí•œ ì—†ìŒ: {search_path}")
                return results
            
            # íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë¶„ë¦¬
            files = [item for item in items if item.is_file()]
            subdirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]
            
            # íŒŒì¼ë“¤ ë¶„ì„ (ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™•ì¸)
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    # ê¸°ë³¸ AI ëª¨ë¸ íŒŒì¼ í•„í„°ë§
                    if not self._is_potential_ai_model_file(file_path):
                        continue
                    
                    self.scan_stats["pytorch_files_found"] += 1
                    
                    # íŒ¨í„´ ë§¤ì¹­
                    if self._matches_model_patterns(file_path, pattern_info):
                        detected_model = self._analyze_real_model_file(
                            file_path, model_type, pattern_info, categories_filter, min_confidence
                        )
                        if detected_model:
                            results[detected_model.name] = detected_model
                            self.logger.debug(f"ğŸ“¦ {model_type} ëª¨ë¸ ë°œê²¬: {file_path.name}")
                        
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
            if self.enable_deep_scan and current_depth < max_depth:
                for subdir in subdirs:
                    # ì œì™¸í•  ë””ë ‰í† ë¦¬ íŒ¨í„´
                    if subdir.name in ['__pycache__', '.git', 'node_modules', '.vscode', '.idea', '.pytest_cache']:
                        continue
                    
                    try:
                        subdir_results = self._scan_path_for_real_models(
                            model_type, pattern_info, subdir, categories_filter, 
                            min_confidence, max_depth, current_depth + 1
                        )
                        results.update(subdir_results)
                    except Exception as e:
                        self.logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                        continue
            
            return results
            
        except Exception as e:
            self.logger.debug(f"ê²½ë¡œ ìŠ¤ìº” ì˜¤ë¥˜ {search_path}: {e}")
            return results

    def _is_potential_ai_model_file(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ ê°€ëŠ¥ì„± í™•ì¸ (2ë²ˆ,3ë²ˆ íŒŒì¼ ë°©ì‹)"""
        # í™•ì¥ì ì²´í¬
        ai_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', '.h5', '.pb'}
        if file_path.suffix.lower() not in ai_extensions:
            return False
        
        # íŒŒì¼ í¬ê¸° ì²´í¬ (ë„ˆë¬´ ì‘ìœ¼ë©´ ëª¨ë¸ì´ ì•„ë‹˜)
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb < 0.5:  # 0.5MB ë¯¸ë§Œì€ ì œì™¸
                return False
        except:
            return False
        
        # íŒŒì¼ëª… íŒ¨í„´ ì²´í¬  
        file_name = file_path.name.lower()
        ai_keywords = [
            'model', 'checkpoint', 'weight', 'state_dict', 'pytorch_model',
            'diffusion', 'transformer', 'bert', 'clip', 'vit', 'resnet',
            'pose', 'parsing', 'segmentation', 'u2net', 'openpose',
            'viton', 'hrviton', 'stable', 'unet', 'vae', 'gmm', 'tom',
            'schp', 'atr', 'graphonomy', 'sam'
        ]
        
        return any(keyword in file_name for keyword in ai_keywords)

    def _matches_model_patterns(self, file_path: Path, pattern_info: ModelFileInfo) -> bool:
        """ëª¨ë¸ íŒ¨í„´ ë§¤ì¹­ í™•ì¸ (2ë²ˆ íŒŒì¼ ë°©ì‹)"""
        try:
            file_name_lower = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­
            for pattern in pattern_info.patterns:
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    return True
            
            # ëŒ€ì²´ ì´ë¦„ ë§¤ì¹­
            for alt_name in pattern_info.alternative_names:
                if alt_name.lower() in file_name_lower:
                    return True
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in pattern_info.keywords:
                if keyword in file_name_lower:
                    return True
                    
            return False
            
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return False

    def _analyze_real_model_file(
        self, 
        file_path: Path, 
        model_type: str,
        pattern_info: ModelFileInfo,
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float
    ) -> Optional[DetectedModel]:
        """ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ë¶„ì„ (3ë²ˆ íŒŒì¼ì˜ ê²€ì¦ ë¡œì§ ë°˜ì˜)"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            file_extension = file_path.suffix.lower()
            last_modified = file_stat.st_mtime
            
            # í¬ê¸° ì œí•œ í™•ì¸
            if not (pattern_info.min_size_mb <= file_size_mb <= pattern_info.max_size_mb):
                return None
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if file_extension not in pattern_info.file_types:
                return None
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_real_confidence(file_path, model_type, pattern_info, file_size_mb)
            
            if confidence_score < min_confidence:
                return None
            
            # PyTorch ëª¨ë¸ ì‹¤ì œ ê²€ì¦ (3ë²ˆ íŒŒì¼ ë°©ì‹)
            pytorch_valid = False
            parameter_count = 0
            validation_info = {}
            
            if self.enable_pytorch_validation and file_extension in ['.pth', '.pt']:
                pytorch_valid, parameter_count, validation_info = self._validate_pytorch_model(
                    file_path, model_type
                )
                
                if pytorch_valid:
                    self.scan_stats["valid_pytorch_models"] += 1
                    # PyTorch ê²€ì¦ ì„±ê³µí•˜ë©´ ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                    confidence_score = min(confidence_score + 0.2, 1.0)
                else:
                    self.scan_stats["pytorch_validation_errors"] += 1
                    # ê²€ì¦ ì‹¤íŒ¨í•˜ë©´ ì‹ ë¢°ë„ ê°ì†Œ
                    confidence_score = max(confidence_score - 0.3, 0.0)
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "human_parsing": ModelCategory.HUMAN_PARSING,
                "pose_estimation": ModelCategory.POSE_ESTIMATION,
                "cloth_segmentation": ModelCategory.CLOTH_SEGMENTATION,
                "geometric_matching": ModelCategory.GEOMETRIC_MATCHING,
                "cloth_warping": ModelCategory.CLOTH_WARPING,
                "virtual_fitting": ModelCategory.VIRTUAL_FITTING
            }
            
            detected_category = category_mapping.get(model_type, ModelCategory.AUXILIARY)
            
            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
            if categories_filter and detected_category not in categories_filter:
                return None
            
            # ìš°ì„ ìˆœìœ„ ê²°ì •
            priority = ModelPriority(pattern_info.priority) if pattern_info.priority <= 5 else ModelPriority.EXPERIMENTAL
            
            # Step ì´ë¦„ ìƒì„±
            step_name = self._get_step_name_for_type(model_type)
            
            # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
            unique_name = self._generate_unique_model_name(file_path, model_type, pattern_info.name)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„± (3ë²ˆ íŒŒì¼ ë°©ì‹ í¬í•¨)
            metadata = {
                "file_name": file_path.name,
                "file_size_mb": file_size_mb,
                "model_type": model_type,
                "detected_at": time.time(),
                "auto_detected": True,
                "pattern_matched": True,
                "pytorch_validated": pytorch_valid,
                "parameter_count": parameter_count,
                **validation_info
            }
            
            # DetectedModel ê°ì²´ ìƒì„±
            detected_model = DetectedModel(
                name=unique_name,
                path=file_path,
                category=detected_category,
                model_type=pattern_info.name,
                file_size_mb=file_size_mb,
                file_extension=file_extension,
                confidence_score=confidence_score,
                priority=priority,
                step_name=step_name,
                metadata=metadata,
                last_modified=last_modified,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _validate_pytorch_model(self, file_path: Path, model_type: str) -> Tuple[bool, int, Dict[str, Any]]:
        """PyTorch ëª¨ë¸ ì‹¤ì œ ê²€ì¦ (3ë²ˆ íŒŒì¼ CheckpointFinder ë°©ì‹)"""
        try:
            if not TORCH_AVAILABLE:
                return False, 0, {"error": "PyTorch not available"}
            
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            try:
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
            except Exception as e:
                # weights_only=True ì‹¤íŒ¨ì‹œ ì¼ë°˜ ë¡œë“œ ì‹œë„
                try:
                    checkpoint = torch.load(file_path, map_location='cpu')
                except Exception as e2:
                    return False, 0, {"load_error": str(e2)}
            
            validation_info = {}
            parameter_count = 0
            
            if isinstance(checkpoint, dict):
                # state_dict í™•ì¸
                state_dict = None
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    validation_info["contains_state_dict"] = True
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                    validation_info["contains_model"] = True
                else:
                    # ì²´í¬í¬ì¸íŠ¸ ìì²´ê°€ state_dictì¼ ìˆ˜ ìˆìŒ
                    state_dict = checkpoint
                    validation_info["is_direct_state_dict"] = True
                
                if state_dict and isinstance(state_dict, dict):
                    # ë ˆì´ì–´ ì •ë³´ ë¶„ì„
                    layers_info = self._analyze_model_layers(state_dict, model_type)
                    validation_info.update(layers_info)
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                    parameter_count = self._count_parameters(state_dict)
                    validation_info["parameter_count"] = parameter_count
                    
                    # ëª¨ë¸ íƒ€ì…ë³„ ê²€ì¦
                    type_validation = self._validate_model_type_specific(state_dict, model_type, parameter_count)
                    validation_info.update(type_validation)
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                for key in ['epoch', 'version', 'arch', 'model_name', 'optimizer']:
                    if key in checkpoint:
                        validation_info[f'checkpoint_{key}'] = str(checkpoint[key])[:100]
                
                return True, parameter_count, validation_info
            
            else:
                # ë‹¨ìˆœ í…ì„œë‚˜ ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                if hasattr(checkpoint, 'state_dict'):
                    state_dict = checkpoint.state_dict()
                    parameter_count = self._count_parameters(state_dict)
                    return True, parameter_count, {"model_object": True}
                elif torch.is_tensor(checkpoint):
                    return True, checkpoint.numel(), {"single_tensor": True}
                else:
                    return False, 0, {"unknown_format": type(checkpoint).__name__}
            
        except Exception as e:
            return False, 0, {"validation_error": str(e)[:200]}

    def _analyze_model_layers(self, state_dict: Dict, model_type: str) -> Dict[str, Any]:
        """ëª¨ë¸ ë ˆì´ì–´ ë¶„ì„ (3ë²ˆ íŒŒì¼ ë°©ì‹)"""
        try:
            layers_info = {
                "total_layers": len(state_dict),
                "layer_types": {},
                "layer_names": list(state_dict.keys())[:10]  # ì²˜ìŒ 10ê°œë§Œ
            }
            
            # ë ˆì´ì–´ íƒ€ì… ë¶„ì„
            layer_type_counts = {}
            for key in state_dict.keys():
                # ì¼ë°˜ì ì¸ ë ˆì´ì–´ íƒ€ì…ë“¤
                if 'conv' in key.lower():
                    layer_type_counts['conv'] = layer_type_counts.get('conv', 0) + 1
                elif 'bn' in key.lower() or 'batch' in key.lower():
                    layer_type_counts['batch_norm'] = layer_type_counts.get('batch_norm', 0) + 1
                elif 'linear' in key.lower() or 'fc' in key.lower():
                    layer_type_counts['linear'] = layer_type_counts.get('linear', 0) + 1
                elif 'attention' in key.lower() or 'attn' in key.lower():
                    layer_type_counts['attention'] = layer_type_counts.get('attention', 0) + 1
                elif 'embed' in key.lower():
                    layer_type_counts['embedding'] = layer_type_counts.get('embedding', 0) + 1
            
            layers_info["layer_types"] = layer_type_counts
            
            # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ë ˆì´ì–´ í™•ì¸
            verification_pattern = CHECKPOINT_VERIFICATION_PATTERNS.get(model_type, {})
            required_layers = verification_pattern.get("required_layers", [])
            
            found_required = 0
            for required_layer in required_layers:
                if any(required_layer in key.lower() for key in state_dict.keys()):
                    found_required += 1
            
            layers_info["required_layers_found"] = found_required
            layers_info["required_layers_total"] = len(required_layers)
            layers_info["required_layers_match_rate"] = found_required / len(required_layers) if required_layers else 1.0
            
            return layers_info
            
        except Exception as e:
            return {"layer_analysis_error": str(e)[:100]}

    def _count_parameters(self, state_dict: Dict) -> int:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        try:
            total_params = 0
            for tensor in state_dict.values():
                if torch.is_tensor(tensor):
                    total_params += tensor.numel()
            return total_params
        except Exception as e:
            return 0

    def _validate_model_type_specific(self, state_dict: Dict, model_type: str, parameter_count: int) -> Dict[str, Any]:
        """ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ê²€ì¦"""
        try:
            validation = {"type_specific_validation": True}
            
            # 3ë²ˆ íŒŒì¼ì˜ ê²€ì¦ íŒ¨í„´ ì‚¬ìš©
            verification_pattern = CHECKPOINT_VERIFICATION_PATTERNS.get(model_type, {})
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ë²”ìœ„ í™•ì¸
            if "typical_parameters" in verification_pattern:
                min_params, max_params = verification_pattern["typical_parameters"]
                if min_params <= parameter_count <= max_params:
                    validation["parameter_count_valid"] = True
                    validation["parameter_confidence"] = 1.0
                else:
                    validation["parameter_count_valid"] = False
                    # ë²”ìœ„ ë°–ì´ë©´ ì‹ ë¢°ë„ ì¡°ì •
                    if parameter_count < min_params:
                        validation["parameter_confidence"] = max(0.3, parameter_count / min_params)
                    else:
                        validation["parameter_confidence"] = max(0.3, min_params / parameter_count)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keywords = verification_pattern.get("keywords", [])
            keyword_matches = 0
            for keyword in keywords:
                if any(keyword in key.lower() for key in state_dict.keys()):
                    keyword_matches += 1
            
            validation["keyword_matches"] = keyword_matches
            validation["keyword_match_rate"] = keyword_matches / len(keywords) if keywords else 1.0
            
            return validation
            
        except Exception as e:
            return {"type_validation_error": str(e)[:100]}

    def _calculate_real_confidence(self, file_path: Path, model_type: str, pattern_info: ModelFileInfo, file_size_mb: float) -> float:
        """ì‹¤ì œ ì‹ ë¢°ë„ ê³„ì‚° (2ë²ˆ,3ë²ˆ íŒŒì¼ ë°©ì‹ ì¢…í•©)"""
        try:
            score = 0.0
            file_name = file_path.name.lower()
            file_path_str = str(file_path).lower()
            
            # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
            for pattern in pattern_info.patterns:
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    score += 25.0
                    break
            
            # ëŒ€ì²´ ì´ë¦„ ë§¤ì¹­
            for alt_name in pattern_info.alternative_names:
                if alt_name.lower() in file_name:
                    score += 20.0
                    break
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in pattern_info.keywords:
                if keyword in file_name:
                    score += 8.0
            
            # íŒŒì¼ í¬ê¸° ì ì •ì„±
            size_min, size_max = pattern_info.min_size_mb, pattern_info.max_size_mb
            size_mid = (size_min + size_max) / 2
            
            if size_min <= file_size_mb <= size_max:
                # í¬ê¸°ê°€ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´
                if abs(file_size_mb - size_mid) / size_mid < 0.5:  # ì¤‘ê°„ê°’ì˜ 50% ì´ë‚´
                    score += 15.0
                else:
                    score += 10.0
            elif file_size_mb < size_min:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ê°ì 
                score -= 10.0
            else:
                # ë„ˆë¬´ í¬ë©´ ì•½ê°„ ê°ì 
                score -= 5.0
            
            # íŒŒì¼ í™•ì¥ì ë³´ë„ˆìŠ¤
            if file_path.suffix in pattern_info.file_types:
                score += 5.0
            
            # ê²½ë¡œ ê¸°ë°˜ ì ìˆ˜
            path_parts = [part.lower() for part in file_path.parts]
            if any(pattern_info.step in part for part in path_parts):
                score += 10.0
            
            # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤
            if pattern_info.priority == 1:
                score += 5.0
            elif pattern_info.priority == 2:
                score += 3.0
            
            # ì •ê·œí™” (0.0 ~ 1.0)
            confidence = min(score / 80.0, 1.0)
            return max(confidence, 0.0)
            
        except Exception as e:
            self.logger.debug(f"ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

    def _get_step_name_for_type(self, model_type: str) -> str:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ Step ì´ë¦„ ë°˜í™˜"""
        step_mapping = {
            "human_parsing": "HumanParsingStep",
            "pose_estimation": "PoseEstimationStep",
            "cloth_segmentation": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting": "VirtualFittingStep"
        }
        return step_mapping.get(model_type, "UnknownStep")

    def _generate_unique_model_name(self, file_path: Path, model_type: str, base_name: str) -> str:
        """ê³ ìœ í•œ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        try:
            # 2ë²ˆ íŒŒì¼ì˜ í‘œì¤€ ì´ë¦„ ì‚¬ìš©
            standard_names = {
                "human_parsing": "human_parsing_graphonomy",
                "pose_estimation": "pose_estimation_openpose",
                "cloth_segmentation": "cloth_segmentation_u2net",
                "geometric_matching": "geometric_matching_gmm",
                "cloth_warping": "cloth_warping_tom",
                "virtual_fitting": "virtual_fitting_diffusion"
            }
            
            standard_name = standard_names.get(model_type)
            if standard_name:
                return standard_name
            
            # íŒŒì¼ëª… ê¸°ë°˜ ì´ë¦„ ìƒì„±
            file_stem = file_path.stem.lower()
            clean_name = re.sub(r'[^a-z0-9_]', '_', file_stem)
            
            # í•´ì‹œ ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
            path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:6]
            
            return f"{model_type}_{clean_name}_{path_hash}"
            
        except Exception as e:
            timestamp = int(time.time())
            return f"detected_model_{timestamp}"

    def _register_detected_model_safe(self, detected_model: DetectedModel):
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ëª¨ë¸ ë“±ë¡"""
        with threading.Lock():
            self._register_detected_model(detected_model)

    def _register_detected_model(self, detected_model: DetectedModel):
        """íƒì§€ëœ ëª¨ë¸ ë“±ë¡ (ì¤‘ë³µ ì²˜ë¦¬)"""
        try:
            model_name = detected_model.name
            
            if model_name in self.detected_models:
                existing_model = self.detected_models[model_name]
                
                # ë” ë‚˜ì€ ëª¨ë¸ë¡œ êµì²´í• ì§€ ê²°ì •
                if self._is_better_model(detected_model, existing_model):
                    detected_model.alternative_paths.append(existing_model.path)
                    detected_model.alternative_paths.extend(existing_model.alternative_paths)
                    self.detected_models[model_name] = detected_model
                    self.logger.debug(f"ğŸ”„ ëª¨ë¸ êµì²´: {model_name}")
                else:
                    existing_model.alternative_paths.append(detected_model.path)
                    self.logger.debug(f"ğŸ“ ëŒ€ì²´ ê²½ë¡œ ì¶”ê°€: {model_name}")
            else:
                self.detected_models[model_name] = detected_model
                self.logger.debug(f"âœ… ìƒˆ ëª¨ë¸ ë“±ë¡: {model_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _is_better_model(self, new_model: DetectedModel, existing_model: DetectedModel) -> bool:
        """ìƒˆ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨"""
        try:
            # 1. PyTorch ê²€ì¦ ìƒíƒœ ìš°ì„ 
            if new_model.pytorch_valid and not existing_model.pytorch_valid:
                return True
            elif not new_model.pytorch_valid and existing_model.pytorch_valid:
                return False
            
            # 2. ìš°ì„ ìˆœìœ„ ë¹„êµ
            if new_model.priority.value < existing_model.priority.value:
                return True
            elif new_model.priority.value > existing_model.priority.value:
                return False
            
            # 3. ì‹ ë¢°ë„ ë¹„êµ
            if abs(new_model.confidence_score - existing_model.confidence_score) > 0.1:
                return new_model.confidence_score > existing_model.confidence_score
            
            # 4. íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ (ë” ë§ìœ¼ë©´ ì¼ë°˜ì ìœ¼ë¡œ ë” ì¢‹ìŒ)
            if new_model.parameter_count > 0 and existing_model.parameter_count > 0:
                if abs(new_model.parameter_count - existing_model.parameter_count) / max(new_model.parameter_count, existing_model.parameter_count) > 0.2:
                    return new_model.parameter_count > existing_model.parameter_count
            
            # 5. ìµœì‹ ì„± ë¹„êµ
            if abs(new_model.last_modified - existing_model.last_modified) > 86400:  # 1ì¼ ì´ìƒ ì°¨ì´
                return new_model.last_modified > existing_model.last_modified
            
            # 6. íŒŒì¼ í¬ê¸° ë¹„êµ
            return new_model.file_size_mb > existing_model.file_size_mb
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ë¹„êµ ì˜¤ë¥˜: {e}")
            return new_model.file_size_mb > existing_model.file_size_mb

    def _reset_scan_stats(self):
        """ìŠ¤ìº” í†µê³„ ë¦¬ì…‹"""
        self.scan_stats.update({
            "total_files_scanned": 0,
            "pytorch_files_found": 0,
            "valid_pytorch_models": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "pytorch_validation_errors": 0
        })

    def _post_process_results(self, min_confidence: float):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # ì‹ ë¢°ë„ í•„í„°ë§
            filtered_models = {
                name: model for name, model in self.detected_models.items()
                if model.confidence_score >= min_confidence
            }
            self.detected_models = filtered_models
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
            sorted_models = sorted(
                self.detected_models.items(),
                key=lambda x: (x[1].priority.value, -x[1].confidence_score, -x[1].parameter_count, -x[1].file_size_mb)
            )
            
            self.detected_models = {name: model for name, model in sorted_models}
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _print_detection_summary(self):
        """íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        try:
            self.logger.info("=" * 70)
            self.logger.info("ğŸ” ì‹¤ì œ AI ëª¨ë¸ íƒì§€ ê²°ê³¼ ìš”ì•½")
            self.logger.info("=" * 70)
            
            total_size_gb = sum(model.file_size_mb for model in self.detected_models.values()) / 1024
            total_params = sum(model.parameter_count for model in self.detected_models.values())
            avg_confidence = sum(model.confidence_score for model in self.detected_models.values()) / len(self.detected_models) if self.detected_models else 0
            pytorch_valid_count = sum(1 for model in self.detected_models.values() if model.pytorch_valid)
            
            self.logger.info(f"ğŸ“Š íƒì§€ëœ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.2f}GB")
            self.logger.info(f"ğŸ” ìŠ¤ìº” íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
            self.logger.info(f"ğŸ PyTorch íŒŒì¼: {self.scan_stats['pytorch_files_found']}ê°œ")
            self.logger.info(f"âœ… ê²€ì¦ëœ ëª¨ë¸: {pytorch_valid_count}ê°œ")
            self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
            self.logger.info(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            self.logger.info(f"ğŸ§® ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
            
            # ëª¨ë¸ íƒ€ì…ë³„ ë¶„í¬
            type_distribution = {}
            for model in self.detected_models.values():
                model_type = model.category.value
                if model_type not in type_distribution:
                    type_distribution[model_type] = 0
                type_distribution[model_type] += 1
            
            if type_distribution:
                self.logger.info("\nğŸ“ ëª¨ë¸ íƒ€ì…ë³„ ë¶„í¬:")
                for model_type, count in type_distribution.items():
                    self.logger.info(f"  {model_type}: {count}ê°œ")
            
            # ì£¼ìš” ëª¨ë¸ë“¤ (ê²€ì¦ ì •ë³´ í¬í•¨)
            if self.detected_models:
                self.logger.info("\nğŸ† íƒì§€ëœ ì£¼ìš” ëª¨ë¸ë“¤:")
                for i, (name, model) in enumerate(list(self.detected_models.items())[:5]):
                    status = "âœ…ê²€ì¦ë¨" if model.pytorch_valid else "â“ë¯¸ê²€ì¦"
                    self.logger.info(f"  {i+1}. {name}")
                    self.logger.info(f"     íƒ€ì…: {model.category.value}, í¬ê¸°: {model.file_size_mb:.1f}MB")
                    self.logger.info(f"     ì‹ ë¢°ë„: {model.confidence_score:.3f}, ìƒíƒœ: {status}")
                    if model.parameter_count > 0:
                        self.logger.info(f"     íŒŒë¼ë¯¸í„°: {model.parameter_count:,}ê°œ")
            
            self.logger.info("=" * 70)
                
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ ìºì‹œ ê´€ë ¨ ë©”ì„œë“œë“¤ (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)
    # ==============================================

    def _load_from_cache(self) -> Optional[Dict[str, DetectedModel]]:
        """ìºì‹œì—ì„œ ë¡œë“œ (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    cutoff_time = time.time() - self.cache_ttl
                    cursor.execute("DELETE FROM real_model_cache WHERE created_at < ?", (cutoff_time,))
                    
                    # ìºì‹œ ì¡°íšŒ
                    cursor.execute("""
                        SELECT file_path, detection_data, pytorch_valid, parameter_count
                        FROM real_model_cache 
                        WHERE created_at > ?
                    """, (cutoff_time,))
                    
                    cached_models = {}
                    for file_path, detection_data, pytorch_valid, parameter_count in cursor.fetchall():
                        try:
                            # íŒŒì¼ì´ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                            if not Path(file_path).exists():
                                continue
                            
                            model_data = json.loads(detection_data)
                            model = self._deserialize_detected_model(model_data)
                            if model:
                                # ìºì‹œëœ ê²€ì¦ ì •ë³´ ë³µì›
                                model.pytorch_valid = bool(pytorch_valid)
                                model.parameter_count = parameter_count or 0
                                cached_models[model.name] = model
                        except Exception as e:
                            self.logger.debug(f"ìºì‹œ í•­ëª© ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                    
                    if cached_models:
                        # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                        cursor.execute("UPDATE real_model_cache SET accessed_at = ?", (time.time(),))
                        conn.commit()
                        
                        self.detected_models = cached_models
                        return cached_models
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_cache(self):
        """ìºì‹œì— ì €ì¥ (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        try:
            with self._cache_lock:
                with sqlite3.connect(self.cache_db_path) as conn:
                    cursor = conn.cursor()
                    current_time = time.time()
                    
                    for model in self.detected_models.values():
                        try:
                            detection_data = json.dumps(self._serialize_detected_model(model))
                            
                            cursor.execute("""
                                INSERT OR REPLACE INTO real_model_cache 
                                (file_path, file_size, file_mtime, checksum, pytorch_valid, parameter_count, detection_data, created_at, accessed_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                str(model.path),
                                int(model.file_size_mb * 1024 * 1024),
                                model.last_modified,
                                model.checksum,
                                int(model.pytorch_valid),
                                model.parameter_count,
                                detection_data,
                                current_time,
                                current_time
                            ))
                        except Exception as e:
                            self.logger.debug(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {model.name}: {e}")
                    
                    conn.commit()
                    
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _serialize_detected_model(self, model: DetectedModel) -> Dict[str, Any]:
        """DetectedModelì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™” (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        return {
            "name": model.name,
            "path": str(model.path),
            "category": model.category.value,
            "model_type": model.model_type,
            "file_size_mb": model.file_size_mb,
            "file_extension": model.file_extension,
            "confidence_score": model.confidence_score,
            "priority": model.priority.value,
            "step_name": model.step_name,
            "metadata": model.metadata,
            "alternative_paths": [str(p) for p in model.alternative_paths],
            "requirements": model.requirements,
            "performance_info": model.performance_info,
            "compatibility_info": model.compatibility_info,
            "last_modified": model.last_modified,
            "checksum": model.checksum,
            "pytorch_valid": model.pytorch_valid,
            "parameter_count": model.parameter_count
        }

    def _deserialize_detected_model(self, data: Dict[str, Any]) -> Optional[DetectedModel]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ DetectedModelë¡œ ì—­ì§ë ¬í™” (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        try:
            return DetectedModel(
                name=data["name"],
                path=Path(data["path"]),
                category=ModelCategory(data["category"]),
                model_type=data["model_type"],
                file_size_mb=data["file_size_mb"],
                file_extension=data["file_extension"],
                confidence_score=data["confidence_score"],
                priority=ModelPriority(data["priority"]),
                step_name=data["step_name"],
                metadata=data.get("metadata", {}),
                alternative_paths=[Path(p) for p in data.get("alternative_paths", [])],
                requirements=data.get("requirements", []),
                performance_info=data.get("performance_info", {}),
                compatibility_info=data.get("compatibility_info", {}),
                last_modified=data.get("last_modified", 0.0),
                checksum=data.get("checksum"),
                pytorch_valid=data.get("pytorch_valid", False),
                parameter_count=data.get("parameter_count", 0)
            )
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return None

    # ==============================================
    # ğŸ”¥ ê³µê°œ ì¡°íšŒ ë©”ì„œë“œë“¤
    # ==============================================

    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.category == category]

    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """Stepë³„ ëª¨ë¸ ì¡°íšŒ"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]

    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """Stepë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        return min(step_models, key=lambda m: (m.priority.value, -m.confidence_score, -m.parameter_count))

    def get_model_by_name(self, name: str) -> Optional[DetectedModel]:
        """ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ ì¡°íšŒ"""
        return self.detected_models.get(name)

    def get_all_model_paths(self) -> Dict[str, Path]:
        """ëª¨ë“  ëª¨ë¸ì˜ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {name: model.path for name, model in self.detected_models.items()}

    def get_validated_models_only(self) -> Dict[str, DetectedModel]:
        """PyTorch ê²€ì¦ëœ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜"""
        return {name: model for name, model in self.detected_models.items() if model.pytorch_valid}

    def search_models(
        self, 
        keywords: List[str], 
        step_filter: Optional[List[str]] = None,
        min_confidence: float = 0.0,
        validated_only: bool = False
    ) -> List[DetectedModel]:
        """í‚¤ì›Œë“œë¡œ ëª¨ë¸ ê²€ìƒ‰"""
        try:
            results = []
            keywords_lower = [kw.lower() for kw in keywords]
            
            for model in self.detected_models.values():
                # ì‹ ë¢°ë„ í•„í„°
                if model.confidence_score < min_confidence:
                    continue
                
                # ê²€ì¦ í•„í„°
                if validated_only and not model.pytorch_valid:
                    continue
                
                # Step í•„í„°
                if step_filter and model.step_name not in step_filter:
                    continue
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                model_text = f"{model.name} {model.path.name} {model.model_type} {model.step_name}".lower()
                if any(keyword in model_text for keyword in keywords_lower):
                    results.append(model)
            
            # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬ (ê²€ì¦ëœ ëª¨ë¸ ìš°ì„ )
            results.sort(key=lambda m: (not m.pytorch_valid, m.priority.value, -m.confidence_score, -m.parameter_count))
            return results
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

# ==============================================
# ğŸ”¥ ModelLoader ì—°ë™ìš© ì„¤ì • ìƒì„±ê¸° (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
# ==============================================

class RealModelLoaderConfigGenerator:
    """
    ğŸ”— ì‹¤ì œ ModelLoader ì—°ë™ìš© ì„¤ì • ìƒì„±ê¸° v6.0
    âœ… ì‹¤ì œ ê²€ì¦ëœ ëª¨ë¸ ì •ë³´ í¬í•¨
    âœ… ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ìœ¼ë¡œë§Œ ë™ì‘
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
    """
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_complete_config(self) -> Dict[str, Any]:
        """ModelLoaderìš© ì™„ì „í•œ ì„¤ì • ìƒì„± (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)"""
        try:
            config = {
                "model_configs": [],
                "model_paths": {},
                "step_mappings": {},
                "priority_rankings": {},
                "performance_estimates": {},
                "validation_results": {},
                "metadata": {
                    "total_models": len(self.detector.detected_models),
                    "validated_models": len(self.detector.get_validated_models_only()),
                    "generation_time": time.time(),
                    "detector_version": "6.0",
                    "scan_stats": self.detector.scan_stats
                }
            }
            
            for name, detected_model in self.detector.detected_models.items():
                # ModelConfig ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)
                model_config = {
                    "name": name,
                    "model_type": detected_model.category.value,
                    "model_class": detected_model.model_type,
                    "checkpoint_path": str(detected_model.path),
                    "device": "auto",
                    "precision": "fp16",
                    "input_size": self._get_input_size_for_step(detected_model.step_name),
                    "step_name": detected_model.step_name,
                    "pytorch_validated": detected_model.pytorch_valid,
                    "parameter_count": detected_model.parameter_count,
                    "metadata": {
                        **detected_model.metadata,
                        "auto_detected": True,
                        "confidence_score": detected_model.confidence_score,
                        "priority": detected_model.priority.name,
                        "alternative_paths": [str(p) for p in detected_model.alternative_paths],
                        "pytorch_validated": detected_model.pytorch_valid,
                        "parameter_count": detected_model.parameter_count
                    }
                }
                config["model_configs"].append(model_config)
                
                # ê²½ë¡œ ë§¤í•‘ (ì‹¤ì œ ê²€ì¦ ì •ë³´ í¬í•¨)
                config["model_paths"][name] = {
                    "primary": str(detected_model.path),
                    "alternatives": [str(p) for p in detected_model.alternative_paths],
                    "size_mb": detected_model.file_size_mb,
                    "confidence": detected_model.confidence_score,
                    "pytorch_valid": detected_model.pytorch_valid,
                    "parameter_count": detected_model.parameter_count
                }
                
                # Step ë§¤í•‘
                step_name = detected_model.step_name
                if step_name not in config["step_mappings"]:
                    config["step_mappings"][step_name] = []
                config["step_mappings"][step_name].append(name)
                
                # ìš°ì„ ìˆœìœ„ (ê²€ì¦ ìƒíƒœ í¬í•¨)
                config["priority_rankings"][name] = {
                    "priority_level": detected_model.priority.value,
                    "priority_name": detected_model.priority.name,
                    "confidence_score": detected_model.confidence_score,
                    "step_rank": self._get_step_rank(detected_model.step_name),
                    "pytorch_validated": detected_model.pytorch_valid,
                    "parameter_count": detected_model.parameter_count
                }
                
                # ì„±ëŠ¥ ì¶”ì • (ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜)
                config["performance_estimates"][name] = {
                    "estimated_memory_gb": max(1.0, detected_model.file_size_mb / 1024 * 2),
                    "estimated_load_time_sec": self._estimate_load_time(detected_model),
                    "recommended_batch_size": self._get_recommended_batch_size(detected_model),
                    "gpu_memory_required_gb": max(2.0, detected_model.file_size_mb / 1024 * 1.5),
                    "parameter_count": detected_model.parameter_count,
                    "pytorch_validated": detected_model.pytorch_valid
                }
                
                # ê²€ì¦ ê²°ê³¼
                config["validation_results"][name] = {
                    "pytorch_valid": detected_model.pytorch_valid,
                    "parameter_count": detected_model.parameter_count,
                    "validation_metadata": {k: v for k, v in detected_model.metadata.items() 
                                          if k.startswith(('pytorch_', 'checkpoint_', 'layer_', 'parameter_', 'type_', 'validation_'))}
                }
            
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _get_input_size_for_step(self, step_name: str) -> Tuple[int, int]:
        """Stepë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸°"""
        size_mapping = {
            "HumanParsingStep": (512, 512),
            "PoseEstimationStep": (368, 368),
            "ClothSegmentationStep": (320, 320),
            "GeometricMatchingStep": (512, 384),
            "ClothWarpingStep": (512, 384),
            "VirtualFittingStep": (512, 512),
            "PostProcessingStep": (512, 512),
            "QualityAssessmentStep": (224, 224)
        }
        return size_mapping.get(step_name, (512, 512))

    def _estimate_load_time(self, detected_model: DetectedModel) -> float:
        """ëª¨ë¸ ë¡œë“œ ì‹œê°„ ì¶”ì • (ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜)"""
        base_times = {
            ModelCategory.HUMAN_PARSING: 2.0,
            ModelCategory.POSE_ESTIMATION: 1.0,
            ModelCategory.CLOTH_SEGMENTATION: 1.5,
            ModelCategory.GEOMETRIC_MATCHING: 0.5,
            ModelCategory.CLOTH_WARPING: 3.0,
            ModelCategory.VIRTUAL_FITTING: 8.0,
            ModelCategory.DIFFUSION_MODELS: 10.0,
            ModelCategory.TRANSFORMER_MODELS: 3.0,
            ModelCategory.POST_PROCESSING: 2.0,
            ModelCategory.QUALITY_ASSESSMENT: 1.0
        }
        
        base_time = base_times.get(detected_model.category, 2.0)
        
        # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¡°ì •
        size_factor = min(detected_model.file_size_mb / 100, 5.0)
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ì¡°ì •
        if detected_model.parameter_count > 0:
            param_factor = min(detected_model.parameter_count / 50000000, 3.0)  # 50M íŒŒë¼ë¯¸í„° ê¸°ì¤€
            return base_time * max(size_factor, param_factor)
        
        return base_time * size_factor

    def _get_recommended_batch_size(self, detected_model: DetectedModel) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° (ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜)"""
        if detected_model.parameter_count > 500000000:  # 500M+ íŒŒë¼ë¯¸í„°
            return 1
        elif detected_model.parameter_count > 100000000:  # 100M+ íŒŒë¼ë¯¸í„°
            return 2
        elif detected_model.file_size_mb > 1000:  # 1GB+ íŒŒì¼
            return 1
        elif detected_model.file_size_mb > 100:  # 100MB+ íŒŒì¼
            return 2
        else:
            return 4

    def _get_step_rank(self, step_name: str) -> int:
        """Stepë³„ ìˆœìœ„ (ì¤‘ìš”ë„)"""
        rank_mapping = {
            "HumanParsingStep": 1,
            "VirtualFittingStep": 2,
            "PoseEstimationStep": 3,
            "ClothSegmentationStep": 3,
            "ClothWarpingStep": 4,
            "GeometricMatchingStep": 5,
            "PostProcessingStep": 6,
            "QualityAssessmentStep": 7
        }
        return rank_mapping.get(step_name, 9)

# 1. auto_model_detector.py ëì— ì¶”ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)

# ==============================================
# ğŸ”¥ í•˜ìœ„ í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

class AdvancedModelLoaderAdapter:
    """
    ê¸°ì¡´ ModelLoaderì™€ í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„°
    """
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def get_model_configs(self) -> List[Dict[str, Any]]:
        """ModelLoaderê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ì„¤ì • ë°˜í™˜"""
        try:
            config_generator = RealModelLoaderConfigGenerator(self.detector)
            full_config = config_generator.generate_complete_config()
            return full_config.get("model_configs", [])
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def register_models_to_loader(self, model_loader):
        """íƒì§€ëœ ëª¨ë¸ë“¤ì„ ModelLoaderì— ë“±ë¡"""
        try:
            detected_models = self.detector.detected_models
            registered_count = 0
            
            for name, model in detected_models.items():
                try:
                    # ê¸°ì¡´ ModelLoader ë“±ë¡ ë°©ì‹ì— ë§ì¶¤
                    model_config = {
                        "name": name,
                        "model_type": model.model_type,
                        "checkpoint_path": str(model.path),
                        "device": "auto",
                        "precision": "fp16",
                        "pytorch_validated": model.pytorch_valid,
                        "parameter_count": model.parameter_count,
                        "confidence_score": model.confidence_score
                    }
                    
                    # ModelLoaderì— ë“±ë¡ (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                    if hasattr(model_loader, 'register_model'):
                        model_loader.register_model(name, model_config)
                        registered_count += 1
                    elif hasattr(model_loader, '_register_model'):
                        model_loader._register_model(name, model_config)
                        registered_count += 1
                        
                except Exception as e:
                    self.logger.warning(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            
            self.logger.info(f"âœ… {registered_count}ê°œ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
            return registered_count
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë“±ë¡ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return 0

# ==============================================
# 2. model_loader.pyì˜ StepModelInterface í´ë˜ìŠ¤ì— ì¶”ê°€
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader, step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"steps.{step_name}")
        self.loaded_models = {}
    
    # ğŸ”¥ ê¸°ì¡´ ë©”ì„œë“œë“¤...
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """
        ğŸ”¥ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
        """
        try:
            # ë™ê¸° ë©”ì„œë“œ ë˜í•‘
            return await asyncio.get_event_loop().run_in_executor(
                None, self.load_model, model_name, **kwargs
            )
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """
        ğŸ”¥ ë™ê¸° ëª¨ë¸ ë¡œë“œ (ê°œì„ ëœ ë²„ì „)
        """
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                return self.loaded_models[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ë¡œë“œ
            if hasattr(self.model_loader, 'load_model'):
                model = self.model_loader.load_model(model_name, **kwargs)
                if model:
                    self.loaded_models[model_name] = model
                    return model
            
            # í´ë°±: ì§ì ‘ ë¡œë“œ ì‹œë„
            return self._fallback_load_model(model_name, **kwargs)
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _fallback_load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """í´ë°± ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì‹¤ì œ ëª¨ë¸ íƒì§€ê¸° ì‚¬ìš©
            detector = create_real_world_detector()
            detected_models = detector.detect_all_models()
            
            # ëª¨ë¸ ì´ë¦„ ë§¤í•‘
            name_mapping = {
                "ootdiffusion": "virtual_fitting_diffusion",
                "human_parsing": "human_parsing_graphonomy", 
                "openpose": "pose_estimation_openpose",
                "u2net": "cloth_segmentation_u2net",
                "clip": "clip_vit_base"
            }
            
            target_name = name_mapping.get(model_name, model_name)
            
            if target_name in detected_models:
                model_info = detected_models[target_name]
                self.logger.info(f"âœ… í´ë°± ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name} -> {target_name}")
                return model_info
            
            return None
            
        except Exception as e:
            self.logger.error(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None

# ==============================================
# 3. model_loader.pyì˜ ModelLoader í´ë˜ìŠ¤ì— ì¶”ê°€
# ==============================================

class ModelLoader:
    """AI ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, device: str = "auto", **kwargs):
        # ğŸ”¥ ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ...
        
        # ìë™ íƒì§€ê¸° ì—°ë™
        self.auto_detector = None
        self.auto_adapter = None
        self._initialize_auto_detection()
    
    def _initialize_auto_detection(self):
        """ìë™ íƒì§€ê¸° ì´ˆê¸°í™” ë° ì—°ë™"""
        try:
            # ì‹¤ì œ ëª¨ë¸ íƒì§€ê¸° ìƒì„±
            self.auto_detector = create_real_world_detector()
            
            # ì–´ëŒ‘í„° ìƒì„±
            self.auto_adapter = AdvancedModelLoaderAdapter(self.auto_detector)
            
            # ëª¨ë¸ íƒì§€ ë° ë“±ë¡
            detected_models = self.auto_detector.detect_all_models()
            
            if detected_models:
                registered_count = self.auto_adapter.register_models_to_loader(self)
                self.logger.info(f"ğŸ” ìë™ íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ë°œê²¬, {registered_count}ê°œ ë“±ë¡")
            else:
                self.logger.warning("âš ï¸ ìë™ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            self.logger.error(f"âŒ ìë™ íƒì§€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.auto_detector = None
            self.auto_adapter = None
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """
        ğŸ”¥ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
        """
        try:
            return await asyncio.get_event_loop().run_in_executor(
                None, self.load_model, model_name, **kwargs
            )
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def register_model(self, name: str, config: Dict[str, Any]):
        """
        ğŸ”¥ ëª¨ë¸ ë“±ë¡ (ì–´ëŒ‘í„°ì—ì„œ ì‚¬ìš©)
        """
        try:
            # ê¸°ì¡´ ë“±ë¡ ë°©ì‹ ì‚¬ìš©
            if hasattr(self, 'model_registry'):
                self.model_registry[name] = config
            else:
                # ìƒˆë¡œìš´ registry ìƒì„±
                if not hasattr(self, 'detected_model_registry'):
                    self.detected_model_registry = {}
                self.detected_model_registry[name] = config
            
            self.logger.debug(f"âœ… ëª¨ë¸ ë“±ë¡: {name}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")


# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ ë° íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì‹¤ì œ ê²€ì¦ ê¸°ë°˜)
# ==============================================

def create_real_world_detector(
    search_paths: Optional[List[Path]] = None,
    enable_parallel: bool = True,
    enable_pytorch_validation: bool = True,
    max_workers: int = 4,
    **kwargs
) -> RealWorldModelDetector:
    """ì‹¤ì œ ë™ì‘í•˜ëŠ” ëª¨ë¸ íƒì§€ê¸° ìƒì„±"""
    return RealWorldModelDetector(
        search_paths=search_paths,
        enable_pytorch_validation=enable_pytorch_validation,
        max_workers=max_workers if enable_parallel else 1,
        **kwargs
    )

def quick_real_model_detection(
    model_type_filter: Optional[List[str]] = None,
    min_confidence: float = 0.5,
    force_rescan: bool = False,
    validated_only: bool = False
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ì‹¤ì œ ëª¨ë¸ íƒì§€ ë° ê²°ê³¼ ë°˜í™˜"""
    try:
        # íƒì§€ê¸° ìƒì„± ë° ì‹¤í–‰
        detector = create_real_world_detector()
        detected_models = detector.detect_all_models(
            force_rescan=force_rescan,
            model_type_filter=model_type_filter,
            min_confidence=min_confidence
        )
        
        # ê²€ì¦ëœ ëª¨ë¸ë§Œ í•„í„°ë§ (ì˜µì…˜)
        if validated_only:
            detected_models = detector.get_validated_models_only()
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "total_models": len(detected_models),
            "validated_models": len([m for m in detected_models.values() if m.pytorch_valid]),
            "models_by_type": {},
            "models_by_priority": {},
            "top_models": {},
            "validation_summary": {},
            "scan_stats": detector.scan_stats
        }
        
        # íƒ€ì…ë³„ ë¶„ë¥˜
        for model in detected_models.values():
            model_type = model.category.value
            if model_type not in summary["models_by_type"]:
                summary["models_by_type"][model_type] = []
            summary["models_by_type"][model_type].append({
                "name": model.name,
                "path": str(model.path),
                "confidence": model.confidence_score,
                "size_mb": model.file_size_mb,
                "pytorch_valid": model.pytorch_valid,
                "parameter_count": model.parameter_count
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        for model in detected_models.values():
            priority = model.priority.name
            if priority not in summary["models_by_priority"]:
                summary["models_by_priority"][priority] = []
            summary["models_by_priority"][priority].append(model.name)
        
        # íƒ€ì…ë³„ ìµœê³  ëª¨ë¸ (ê²€ì¦ëœ ê²ƒ ìš°ì„ )
        model_types = set(model.category.value for model in detected_models.values())
        for model_type in model_types:
            type_models = [m for m in detected_models.values() if m.category.value == model_type]
            if type_models:
                best_model = min(type_models, key=lambda m: (not m.pytorch_valid, m.priority.value, -m.confidence_score, -m.parameter_count))
                summary["top_models"][model_type] = {
                    "name": best_model.name,
                    "path": str(best_model.path),
                    "confidence": best_model.confidence_score,
                    "priority": best_model.priority.name,
                    "pytorch_valid": best_model.pytorch_valid,
                    "parameter_count": best_model.parameter_count
                }
        
        # ê²€ì¦ ìš”ì•½
        total_params = sum(m.parameter_count for m in detected_models.values())
        summary["validation_summary"] = {
            "total_parameters": total_params,
            "avg_confidence": sum(m.confidence_score for m in detected_models.values()) / len(detected_models) if detected_models else 0,
            "validation_rate": len([m for m in detected_models.values() if m.pytorch_valid]) / len(detected_models) if detected_models else 0
        }
        
        return summary
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ì‹¤ì œ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def generate_real_model_loader_config(
    detector: Optional[RealWorldModelDetector] = None,
    **detection_kwargs
) -> Dict[str, Any]:
    """
    ì‹¤ì œ ModelLoaderìš© ì„¤ì • ìƒì„± (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
    ì‹¤ì œ ê²€ì¦ëœ ëª¨ë¸ ì •ë³´ ê¸°ë°˜ ë”•ì…”ë„ˆë¦¬ ì¶œë ¥
    """
    try:
        logger.info("ğŸ” ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„± ì‹œì‘...")
        
        # íƒì§€ê¸°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if detector is None:
            detector = create_real_world_detector(**detection_kwargs)
            detected_models = detector.detect_all_models()
        else:
            detected_models = detector.detected_models
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No models detected"}
        
        # ì„¤ì • ìƒì„±ê¸° ì‚¬ìš©
        config_generator = RealModelLoaderConfigGenerator(detector)
        model_loader_config = config_generator.generate_complete_config()
        
        # ìµœì¢… ê²°ê³¼
        validated_count = len(detector.get_validated_models_only())
        result = {
            "success": True,
            "model_loader_config": model_loader_config,
            "detection_summary": {
                "total_models": len(detected_models),
                "validated_models": validated_count,
                "validation_rate": validated_count / len(detected_models) if detected_models else 0,
                "scan_duration": detector.scan_stats["scan_duration"],
                "confidence_avg": sum(m.confidence_score for m in detected_models.values()) / len(detected_models),
                "total_parameters": sum(m.parameter_count for m in detected_models.values())
            }
        }
        
        logger.info(f"âœ… ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„± ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸ ({validated_count}ê°œ ê²€ì¦)")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ModelLoader ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}
# backend/app/ai_pipeline/utils/auto_model_detector.py ë ë¶€ë¶„ì— ì¶”ê°€

def detect_and_integrate_with_model_loader(
    model_loader_instance = None,
    auto_register: bool = True,
    **detection_kwargs
) -> Dict[str, Any]:
    """ëª¨ë¸ íƒì§€ ë° ModelLoader í†µí•© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        logger.info("ğŸ” ëª¨ë¸ íƒì§€ ë° ModelLoader í†µí•© ì‹œì‘...")
        
        # íƒì§€ ì‹¤í–‰
        detector = create_advanced_detector(**detection_kwargs)
        detected_models = detector.detect_all_models()
        
        if not detected_models:
            logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No models detected"}
        
        # ì–´ëŒ‘í„° ìƒì„±
        adapter = AdvancedModelLoaderAdapter(detector)
        
        # ModelLoader ì„¤ì • ìƒì„±
        model_loader_config = adapter.generate_model_loader_config()
        
        # ModelLoaderì™€ í†µí•© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        integration_result = {}
        if auto_register and model_loader_instance:
            try:
                # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
                detector.set_model_loader(model_loader_instance)
                
                # ìë™ ë“±ë¡
                registered_count = adapter.register_models_to_loader(model_loader_instance)
                integration_result["registered_models"] = registered_count
                
            except Exception as e:
                logger.warning(f"âš ï¸ ModelLoader í†µí•© ì‹¤íŒ¨: {e}")
                integration_result["integration_error"] = str(e)
        
        return {
            "success": True,
            "detected_count": len(detected_models),
            "model_names": list(detected_models.keys()),
            "integration": integration_result,
            "config": model_loader_config
        }
        
    except Exception as e:
        logger.error(f"âŒ íƒì§€ ë° í†µí•© ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}


def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """ì‹¤ì œ íƒì§€ëœ ëª¨ë¸ ê²½ë¡œë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
    try:
        validation_result = {
            "valid_models": [],
            "invalid_models": [],
            "missing_files": [],
            "permission_errors": [],
            "pytorch_validated": [],
            "pytorch_failed": [],
            "total_size_gb": 0.0,
            "total_parameters": 0
        }
        
        for name, model in detected_models.items():
            try:
                # ì£¼ ê²½ë¡œ í™•ì¸
                if model.path.exists() and model.path.is_file():
                    validation_result["valid_models"].append(name)
                    validation_result["total_size_gb"] += model.file_size_mb / 1024
                    validation_result["total_parameters"] += model.parameter_count
                    
                    # PyTorch ê²€ì¦ ìƒíƒœ í™•ì¸
                    if model.pytorch_valid:
                        validation_result["pytorch_validated"].append(name)
                    else:
                        validation_result["pytorch_failed"].append(name)
                else:
                    validation_result["missing_files"].append({
                        "name": name,
                        "path": str(model.path)
                    })
                
                # ëŒ€ì²´ ê²½ë¡œë“¤ í™•ì¸
                valid_alternatives = []
                for alt_path in model.alternative_paths:
                    if alt_path.exists() and alt_path.is_file():
                        valid_alternatives.append(str(alt_path))
                
                if valid_alternatives and name in [m["name"] for m in validation_result["missing_files"]]:
                    # ì£¼ ê²½ë¡œëŠ” ì—†ì§€ë§Œ ëŒ€ì²´ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                    validation_result["missing_files"] = [
                        m for m in validation_result["missing_files"] 
                        if m["name"] != name
                    ]
                    validation_result["valid_models"].append(name)
                
            except PermissionError:
                validation_result["permission_errors"].append({
                    "name": name,
                    "path": str(model.path)
                })
            except Exception as e:
                validation_result["invalid_models"].append({
                    "name": name,
                    "path": str(model.path),
                    "error": str(e)
                })
        
        validation_result["summary"] = {
            "total_models": len(detected_models),
            "valid_count": len(validation_result["valid_models"]),
            "invalid_count": len(validation_result["invalid_models"]),
            "missing_count": len(validation_result["missing_files"]),
            "permission_error_count": len(validation_result["permission_errors"]),
            "pytorch_validated_count": len(validation_result["pytorch_validated"]),
            "pytorch_failed_count": len(validation_result["pytorch_failed"]),
            "validation_rate": len(validation_result["valid_models"]) / len(detected_models) if detected_models else 0,
            "pytorch_validation_rate": len(validation_result["pytorch_validated"]) / len(detected_models) if detected_models else 0
        }
        
        return validation_result
        
    except Exception as e:
        logger.error(f"ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # ê¸°ì¡´ exports...
    'RealWorldModelDetector',
    'RealModelLoaderConfigGenerator',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    'ModelFileInfo',
    'detect_and_integrate_with_model_loader' #ìƒˆë¡œì¶”ê°€
    # ìƒˆë¡œìš´ í˜¸í™˜ì„± í´ë˜ìŠ¤
    'AdvancedModelLoaderAdapter',  # ğŸ”¥ ì´ê²ƒì´ ëˆ„ë½ë˜ì—ˆìŒ

    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_real_world_detector',
    'quick_real_model_detection',
    'generate_real_model_loader_config',

    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_real_model_paths',

    # ì„¤ì • ë° íŒ¨í„´
    'ACTUAL_MODEL_PATTERNS',
    'CHECKPOINT_VERIFICATION_PATTERNS',

    # í•˜ìœ„ í˜¸í™˜ì„± ë³„ì¹­
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator',
    'create_advanced_detector',
    'quick_model_detection',
    'generate_model_loader_config',
    'validate_model_paths'
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
AdvancedModelDetector = RealWorldModelDetector
ModelLoaderConfigGenerator = RealModelLoaderConfigGenerator
create_advanced_detector = create_real_world_detector
quick_model_detection = quick_real_model_detection
generate_model_loader_config = generate_real_model_loader_config
validate_model_paths = validate_real_model_paths

logger.info("âœ… ì‹¤ì œ ë™ì‘í•˜ëŠ” ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v6.0 ë¡œë“œ ì™„ë£Œ - PyTorch ê²€ì¦ í¬í•¨")