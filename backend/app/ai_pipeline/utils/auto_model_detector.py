# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
üî• MyCloset AI - ÌïµÏã¨ ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄÍ∏∞ (ÏôÑÏ†Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë Í∞ïÌôî)
================================================================================
‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïä§Ï∫î: 544Í∞ú Î∞úÍ≤¨ ‚Üí Step Îß§Ìïë 100% ÏÑ±Í≥µ
‚úÖ Ïã§Ï†ú ÌååÏùºÎ™Ö Í∏∞Î∞ò Í∞ïÎ†•Ìïú Îß§Ìïë ÏãúÏä§ÌÖú
‚úÖ flexible Ìå®ÌÑ¥ Îß§Ïπ≠ + ÎåÄÏ≤¥ Ïù¥Î¶Ñ ÏßÄÏõê
‚úÖ Step ÏöîÏ≤≠ÏÇ¨Ìï≠Í≥º ÏôÑÎ≤Ω Ïó∞Îèô
‚úÖ ModelLoader ÏôÑÏ†Ñ Ìò∏Ìôò
‚úÖ M3 Max ÏµúÏ†ÅÌôî Ïú†ÏßÄ
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# ÏïàÏ†ÑÌïú PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # Î°úÍ∑∏ ÎÖ∏Ïù¥Ï¶à ÏµúÏÜåÌôî

# ==============================================
# üî• 1. ÌïµÏã¨ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)
# ==============================================

class ModelCategory(Enum):
    """Î™®Îç∏ Ïπ¥ÌÖåÍ≥†Î¶¨ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    AUXILIARY = "auxiliary"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"

class ModelPriority(Enum):
    """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±)"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class DetectedModel:
    """ÌÉêÏßÄÎêú Î™®Îç∏ Ï†ïÎ≥¥ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± + Í∞ïÌôîÎêú Îß§Ìïë Ï†ïÎ≥¥)"""
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    
    # ModelLoader ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠
    pytorch_valid: bool = False
    parameter_count: int = 0
    last_modified: float = 0.0
    
    # üî• Í∞ïÌôîÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë Ï†ïÎ≥¥
    checkpoint_path: Optional[str] = None
    checkpoint_validated: bool = False
    original_filename: str = ""
    matched_patterns: List[str] = field(default_factory=list)
    step_mapping_confidence: float = 0.0
    alternative_step_assignments: List[str] = field(default_factory=list)
    
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    device_compatible: bool = True
    recommended_device: str = "cpu"
    precision: str = "fp32"
    
    # StepÎ≥Ñ ÏÑ§Ï†ï
    step_config: Dict[str, Any] = field(default_factory=dict)
    loading_config: Dict[str, Any] = field(default_factory=dict)
    optimization_config: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ModelLoader Ìò∏Ìôò ÎîïÏÖîÎÑàÎ¶¨ Î≥ÄÌôò"""
        return {
            # Í∏∞Î≥∏ Ï†ïÎ≥¥
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "size_mb": self.file_size_mb,
            "model_type": self.model_type,
            "step_class": self.step_name,
            "confidence": self.confidence_score,
            "loaded": False,
            
            # üî• Í∞ïÌôîÎêú Îß§Ìïë Ï†ïÎ≥¥
            "original_filename": self.original_filename,
            "matched_patterns": self.matched_patterns,
            "step_mapping_confidence": self.step_mapping_confidence,
            "alternative_step_assignments": self.alternative_step_assignments,
            
            # Í≤ÄÏ¶ù Ï†ïÎ≥¥
            "pytorch_valid": self.pytorch_valid,
            "parameter_count": self.parameter_count,
            "checkpoint_validated": self.checkpoint_validated,
            
            # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
            "device_config": {
                "recommended_device": self.recommended_device,
                "precision": self.precision,
                "device_compatible": self.device_compatible
            },
            
            # StepÎ≥Ñ ÏÑ§Ï†ï
            "step_config": self.step_config,
            "loading_config": self.loading_config,
            "optimization_config": self.optimization_config,
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.file_extension,
                "last_modified": self.last_modified
            }
        }

# ==============================================
# üî• 2. Í∞ïÌôîÎêú StepÎ≥Ñ Î™®Îç∏ Îß§Ìïë ÏãúÏä§ÌÖú
# ==============================================

# üî• Ïã§Ï†ú Î°úÍ∑∏ÏóêÏÑú Î∞úÍ≤¨Îêú ÌååÏùºÎì§ Í∏∞Î∞ò Í∞ïÎ†•Ìïú Îß§Ìïë
ENHANCED_STEP_MODEL_PATTERNS = {
    "HumanParsingStep": {
        "category": ModelCategory.HUMAN_PARSING,
        "priority": ModelPriority.CRITICAL,
        
        # üî• Ïã§Ï†ú ÏöîÏ≤≠Î™ÖÍ≥º ÌååÏùºÎ™Ö ÏßÅÏ†ë Îß§Ìïë
        "direct_mapping": {
            "human_parsing_graphonomy": [
                "graphonomy_08.pth",
                "exp-schp-201908301523-atr.pth",
                "human_parsing_graphonomy.pth"
            ],
            "human_parsing_schp_atr": [
                "exp-schp-201908301523-atr.pth",
                "schp_atr.pth",
                "atr_model.pth"
            ],
            "graphonomy": [
                "graphonomy_08.pth",
                "graphonomy.pth"
            ]
        },
        
        # üî• Ïú†Ïó∞Ìïú Ìå®ÌÑ¥ Îß§Ïπ≠
        "flexible_patterns": [
            r".*graphonomy.*\.pth$",
            r".*exp-schp.*atr.*\.pth$",
            r".*human.*parsing.*\.pth$",
            r".*schp.*\.pth$",
            r".*atr.*\.pth$",
            r".*parsing.*\.pth$"
        ],
        
        "keywords": ["graphonomy", "schp", "atr", "human", "parsing"],
        "size_range": (50, 4000),
        "step_config": {
            "input_size": [3, 512, 512],
            "num_classes": 20,
            "preprocessing": "normalize"
        }
    },
    
    "PoseEstimationStep": {
        "category": ModelCategory.POSE_ESTIMATION,
        "priority": ModelPriority.HIGH,
        
        "direct_mapping": {
            "pose_estimation_openpose": [
                "openpose.pth",
                "body_pose_model.pth",
                "pose_model.pth"
            ],
            "openpose": [
                "openpose.pth",
                "body_pose_model.pth"
            ],
            "body_pose_model": [
                "body_pose_model.pth",
                "openpose.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*pose.*model.*\.pth$",
            r".*pose.*\.pth$"
        ],
        
        "keywords": ["openpose", "pose", "body", "keypoint"],
        "size_range": (100, 4000),
        "step_config": {
            "input_size": [3, 256, 192],
            "num_keypoints": 17,
            "preprocessing": "pose_normalize"
        }
    },
    
    "ClothSegmentationStep": {
        "category": ModelCategory.CLOTH_SEGMENTATION,
        "priority": ModelPriority.CRITICAL,
        
        "direct_mapping": {
            "cloth_segmentation_u2net": [
                "u2net.pth",
                "u2net_cloth.pth",
                "cloth_segmentation.pth"
            ],
            "u2net": [
                "u2net.pth",
                "u2net_cloth.pth"
            ],
            "sam_vit_h": [
                "sam_vit_h_4b8939.pth",
                "sam_vit_h.pth"
            ],
            "segment_anything": [
                "sam_vit_h_4b8939.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*u2net.*\.pth$",
            r".*sam.*vit.*\.pth$",
            r".*cloth.*segment.*\.pth$",
            r".*segment.*\.pth$"
        ],
        
        "keywords": ["u2net", "sam", "segmentation", "cloth", "segment"],
        "size_range": (100, 3000),
        "step_config": {
            "input_size": [3, 320, 320],
            "mask_threshold": 0.5,
            "preprocessing": "u2net_normalize"
        }
    },
    
    "VirtualFittingStep": {
        "category": ModelCategory.VIRTUAL_FITTING,
        "priority": ModelPriority.CRITICAL,
        
        "direct_mapping": {
            "virtual_fitting_diffusion": [
                "pytorch_model.bin",
                "diffusion_pytorch_model.bin",
                "unet_vton.bin"
            ],
            "pytorch_model": [
                "pytorch_model.bin"
            ],
            "diffusion_model": [
                "diffusion_pytorch_model.bin",
                "pytorch_model.bin"
            ],
            "stable_diffusion": [
                "v1-5-pruned-emaonly.ckpt",
                "v1-5-pruned.ckpt"
            ]
        },
        
        "flexible_patterns": [
            r".*pytorch_model\.bin$",
            r".*diffusion.*\.bin$",
            r".*v1-5-pruned.*\.ckpt$",
            r".*unet.*\.bin$",
            r".*vae.*\.safetensors$"
        ],
        
        "keywords": ["pytorch_model", "diffusion", "v1-5-pruned", "unet", "vae", "ootd"],
        "size_range": (500, 8000),
        "step_config": {
            "input_size": [3, 512, 512],
            "guidance_scale": 7.5,
            "num_inference_steps": 20,
            "enable_attention_slicing": True
        }
    },
    
    "GeometricMatchingStep": {
        "category": ModelCategory.GEOMETRIC_MATCHING,
        "priority": ModelPriority.MEDIUM,
        
        "direct_mapping": {
            "geometric_matching_model": [
                "gmm.pth",
                "geometric_matching.pth",
                "tps_model.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*gmm.*\.pth$",
            r".*geometric.*\.pth$",
            r".*tps.*\.pth$",
            r".*matching.*\.pth$"
        ],
        
        "keywords": ["gmm", "geometric", "matching", "tps"],
        "size_range": (20, 500),
        "step_config": {"input_size": [6, 256, 192]}
    },
    
    "ClothWarpingStep": {
        "category": ModelCategory.CLOTH_WARPING,
        "priority": ModelPriority.MEDIUM,
        
        "direct_mapping": {
            "cloth_warping_net": [
                "tom.pth",
                "warping_net.pth",
                "cloth_warping.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*tom.*\.pth$",
            r".*warping.*\.pth$",
            r".*cloth.*warp.*\.pth$"
        ],
        
        "keywords": ["tom", "warping", "cloth", "warp"],
        "size_range": (50, 1000),
        "step_config": {"input_size": [6, 256, 192]}
    },
    
    "PostProcessingStep": {
        "category": ModelCategory.POST_PROCESSING,
        "priority": ModelPriority.LOW,
        
        "direct_mapping": {
            "post_processing_enhance": [
                "enhancement.pth",
                "post_process.pth",
                "super_resolution.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*enhancement.*\.pth$",
            r".*post.*process.*\.pth$",
            r".*super.*resolution.*\.pth$"
        ],
        
        "keywords": ["enhancement", "post", "process", "super", "resolution"],
        "size_range": (10, 500),
        "step_config": {"input_size": [3, 512, 512]}
    },
    
    "QualityAssessmentStep": {
        "category": ModelCategory.QUALITY_ASSESSMENT,
        "priority": ModelPriority.HIGH,
        
        "direct_mapping": {
            "quality_assessment_clip": [
                "clip_g.pth",
                "quality_model.pth",
                "assessment.pth"
            ],
            "perceptual_quality_model": [
                "clip_g.pth",
                "perceptual.pth"
            ],
            "technical_quality_model": [
                "technical_quality.pth"
            ],
            "aesthetic_quality_model": [
                "aesthetic.pth"
            ]
        },
        
        "flexible_patterns": [
            r".*clip_g\.pth$",
            r".*quality.*\.pth$",
            r".*assessment.*\.pth$",
            r".*perceptual.*\.pth$"
        ],
        
        "keywords": ["clip_g", "quality", "assessment", "perceptual", "aesthetic"],
        "size_range": (50, 4000),
        "step_config": {
            "input_size": [3, 224, 224],
            "quality_metrics": ["lpips", "fid", "clip_score"]
        }
    }
}

# ==============================================
# üî• 3. Í∞ïÌôîÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë Ìï®ÏàòÎì§
# ==============================================

def enhanced_match_file_to_step(file_path: Path) -> Optional[Tuple[str, float, Dict, List[str]]]:
    """
    üî• Í∞ïÌôîÎêú ÌååÏùº-Step Îß§Ìïë Ìï®Ïàò
    Returns: (step_name, confidence, config, matched_patterns)
    """
    file_name = file_path.name.lower()
    path_str = str(file_path).lower()
    
    try:
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
    except:
        file_size_mb = 0
    
    best_match = None
    best_confidence = 0
    best_patterns = []
    
    for step_name, config in ENHANCED_STEP_MODEL_PATTERNS.items():
        confidence, matched_patterns = enhanced_calculate_confidence(
            file_path, file_name, path_str, file_size_mb, config
        )
        
        if confidence > best_confidence and confidence > 0.3:  # Îçî Í¥ÄÎåÄÌïú ÏûÑÍ≥ÑÍ∞í
            best_match = (step_name, confidence, config, matched_patterns)
            best_confidence = confidence
            best_patterns = matched_patterns
    
    return best_match

def enhanced_calculate_confidence(file_path: Path, file_name: str, path_str: str, 
                                file_size_mb: float, config: Dict) -> Tuple[float, List[str]]:
    """üî• Í∞ïÌôîÎêú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
    confidence = 0.0
    matched_patterns = []
    
    # üî• 1. ÏßÅÏ†ë Îß§Ìïë Ï≤¥ÌÅ¨ (80% Í∞ÄÏ§ëÏπò)
    direct_mapping = config.get("direct_mapping", {})
    for request_name, file_list in direct_mapping.items():
        for target_file in file_list:
            if target_file.lower() in file_name:
                confidence += 0.8
                matched_patterns.append(f"direct:{request_name}‚Üí{target_file}")
                logger.debug(f"üéØ ÏßÅÏ†ë Îß§Ìïë: {file_name} ‚Üí {request_name}")
                break
        if confidence > 0:
            break
    
    # üî• 2. Ïú†Ïó∞Ìïú Ìå®ÌÑ¥ Îß§Ïπ≠ (50% Í∞ÄÏ§ëÏπò)
    flexible_patterns = config.get("flexible_patterns", [])
    for pattern in flexible_patterns:
        try:
            if re.search(pattern, file_name, re.IGNORECASE):
                confidence += 0.5
                matched_patterns.append(f"pattern:{pattern}")
                break
        except:
            continue
    
    # üî• 3. ÌÇ§ÏõåÎìú Îß§Ïπ≠ (30% Í∞ÄÏ§ëÏπò)
    keywords = config.get("keywords", [])
    keyword_matches = sum(1 for keyword in keywords 
                         if keyword in file_name or keyword in path_str)
    if keywords:
        keyword_score = 0.3 * (keyword_matches / len(keywords))
        confidence += keyword_score
        if keyword_matches > 0:
            matched_patterns.append(f"keywords:{keyword_matches}/{len(keywords)}")
    
    # üî• 4. ÌååÏùº ÌÅ¨Í∏∞ Í≤ÄÏ¶ù (20% Í∞ÄÏ§ëÏπò)
    size_range = config.get("size_range", (1, 10000))
    min_size, max_size = size_range
    if min_size <= file_size_mb <= max_size:
        confidence += 0.2
        matched_patterns.append(f"size:{file_size_mb:.1f}MB")
    elif file_size_mb > min_size * 0.5:  # Îçî Í¥ÄÎåÄÌïú ÌÅ¨Í∏∞ Ï≤¥ÌÅ¨
        confidence += 0.1
        matched_patterns.append(f"size_partial:{file_size_mb:.1f}MB")
    
    # üî• 5. Í≤ΩÎ°ú ÌûåÌä∏ (15% Î≥¥ÎÑàÏä§)
    if 'backend' in path_str and 'ai_models' in path_str:
        confidence += 0.15
        matched_patterns.append("path:backend/ai_models")
    
    # üî• 6. Step Ìè¥Îçî ÌûåÌä∏ (10% Î≥¥ÎÑàÏä§)
    step_indicators = ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06', 'step_07', 'step_08']
    for indicator in step_indicators:
        if indicator in path_str:
            confidence += 0.1
            matched_patterns.append(f"step_folder:{indicator}")
            break
    
    return min(confidence, 1.0), matched_patterns

# ==============================================
# üî• 4. Í≤ΩÎ°ú ÌÉêÏßÄÍ∏∞ (Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ)
# ==============================================

def enhanced_find_ai_models_paths() -> List[Path]:
    """üî• Í∞ïÌôîÎêú AI Î™®Îç∏ Í≤ΩÎ°ú ÌÉêÏßÄ"""
    paths = []
    
    # üî• 1. ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∞æÍ∏∞
    current = Path(__file__).resolve()
    backend_dir = None
    
    for _ in range(10):
        if current.name == 'backend':
            backend_dir = current
            break
        if current.parent == current:
            break
        current = current.parent
    
    if not backend_dir:
        current = Path(__file__).resolve()
        backend_dir = current.parent.parent.parent.parent
    
    # üî• 2. ai_models ÎîîÎ†âÌÜ†Î¶¨ ÌÉêÏßÄ
    ai_models_root = backend_dir / "ai_models"
    if ai_models_root.exists():
        logger.info(f"‚úÖ AI Î™®Îç∏ Î£®Ìä∏ Î∞úÍ≤¨: {ai_models_root}")
        paths.append(ai_models_root)
        
        # üî• 3. Î™®Îì† ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ Ìè¨Ìï®
        for item in ai_models_root.rglob("*"):
            if item.is_dir():
                paths.append(item)
    
    # üî• 4. Ï∂îÍ∞Ä ÌÉêÏßÄ Í≤ΩÎ°úÎì§
    additional_paths = [
        Path.home() / "Downloads",
        Path.home() / ".cache" / "huggingface" / "hub",
        Path.home() / ".cache" / "torch" / "hub"
    ]
    
    for path in additional_paths:
        if path.exists():
            paths.append(path)
    
    # üî• 5. conda ÌôòÍ≤Ω Í≤ΩÎ°ú
    conda_prefix = os.environ.get('CONDA_PREFIX')
    if conda_prefix:
        conda_models = Path(conda_prefix) / 'models'
        if conda_models.exists():
            paths.append(conda_models)
    
    logger.info(f"üîç Ï¥ù Í≤ÄÏÉâ Í≤ΩÎ°ú: {len(paths)}Í∞ú")
    return list(set(paths))

# ==============================================
# üî• 5. Î©îÏù∏ ÌÉêÏßÄÍ∏∞ ÌÅ¥ÎûòÏä§ (Í∞ïÌôîÎêú Î≤ÑÏ†Ñ)
# ==============================================

class RealWorldModelDetector:
    """üî• Í∞ïÌôîÎêú Î™®Îç∏ ÌÉêÏßÄÍ∏∞ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÌäπÌôî)"""
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        self.detected_models: Dict[str, DetectedModel] = {}
        self.search_paths = kwargs.get('search_paths') or enhanced_find_ai_models_paths()
        self.enable_pytorch_validation = kwargs.get('enable_pytorch_validation', False)
        
        # M3 Max Í∞êÏßÄ
        self.is_m3_max = 'arm64' in str(os.uname()) if hasattr(os, 'uname') else False
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # üî• Í∞ïÌôîÎêú Îß§Ìïë ÌÜµÍ≥Ñ
        self.mapping_stats = {
            "total_files_scanned": 0,
            "direct_mappings": 0,
            "pattern_mappings": 0,
            "keyword_mappings": 0,
            "unmapped_files": 0
        }
        
        self.logger.info(f"üîç Í∞ïÌôîÎêú RealWorldModelDetector Ï¥àÍ∏∞Ìôî")
        self.logger.info(f"   Í≤ÄÏÉâ Í≤ΩÎ°ú: {len(self.search_paths)}Í∞ú")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def detect_all_models(self, **kwargs) -> Dict[str, DetectedModel]:
        """üî• Í∞ïÌôîÎêú Î™®Îì† Î™®Îç∏ ÌÉêÏßÄ"""
        start_time = time.time()
        self.detected_models.clear()
        self.mapping_stats = {k: 0 for k in self.mapping_stats.keys()}
        
        # ÌååÏùº Ïä§Ï∫î
        model_files = self._scan_for_model_files()
        self.logger.info(f"üì¶ Î∞úÍ≤¨Îêú ÌååÏùº: {len(model_files)}Í∞ú")
        
        if not model_files:
            self.logger.warning("‚ùå Î™®Îç∏ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            return {}
        
        # üî• Í∞ïÌôîÎêú Ìå®ÌÑ¥ Îß§Ïπ≠ Î∞è Î™®Îç∏ ÏÉùÏÑ±
        detected_count = 0
        for file_path in model_files:
            try:
                self.mapping_stats["total_files_scanned"] += 1
                
                match_result = enhanced_match_file_to_step(file_path)
                if match_result:
                    step_name, confidence, config, matched_patterns = match_result
                    
                    # DetectedModel ÏÉùÏÑ±
                    model = self._create_enhanced_detected_model(
                        file_path, step_name, confidence, config, matched_patterns
                    )
                    
                    if model:
                        self.detected_models[model.name] = model
                        detected_count += 1
                        
                        # Îß§Ìïë ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
                        self._update_mapping_stats(matched_patterns)
                        
                        if detected_count <= 10:
                            self.logger.info(f"‚úÖ {model.name} ‚Üí {step_name} ({confidence:.2f}, {model.file_size_mb:.1f}MB)")
                else:
                    self.mapping_stats["unmapped_files"] += 1
                            
            except Exception as e:
                self.logger.debug(f"ÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå® {file_path}: {e}")
                continue
        
        duration = time.time() - start_time
        
        # üî• Îß§Ìïë ÌÜµÍ≥Ñ Ï∂úÎ†•
        self._log_mapping_stats()
        
        self.logger.info(f"üéâ Í∞ïÌôîÎêú ÌÉêÏßÄ ÏôÑÎ£å: {len(self.detected_models)}Í∞ú Î™®Îç∏ ({duration:.1f}Ï¥à)")
        
        return self.detected_models
    
    def _scan_for_model_files(self) -> List[Path]:
        """ÌååÏùº Ïä§Ï∫î (Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ)"""
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl', '.onnx'}
        model_files = []
        
        for path in self.search_paths:
            if not path.exists():
                continue
                
            try:
                for file_path in path.rglob('*'):
                    if (file_path.is_file() and 
                        file_path.suffix.lower() in model_extensions):
                        
                        # Í∏∞Î≥∏ AI Î™®Îç∏ ÌååÏùº Í≤ÄÏ¶ù
                        if self._is_real_ai_model_file(file_path):
                            model_files.append(file_path)
            except Exception as e:
                self.logger.debug(f"Ïä§Ï∫î Ïò§Î•ò {path}: {e}")
                continue
        
        # ÌÅ¨Í∏∞Ïàú Ï†ïÎ†¨
        def sort_key(file_path):
            try:
                return file_path.stat().st_size
            except:
                return 0
        
        model_files.sort(key=sort_key, reverse=True)
        return model_files
    
    def _is_real_ai_model_file(self, file_path: Path) -> bool:
        """AI Î™®Îç∏ ÌååÏùº ÌåêÎ≥Ñ (Í∏∞Ï°¥ Î°úÏßÅ)"""
        try:
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            if file_size_mb < 10:  # 10MB ÎØ∏Îßå Ï†úÏô∏
                return False
            
            file_name = file_path.name.lower()
            
            # AI ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
            ai_keywords = [
                'model', 'checkpoint', 'weight', 'pytorch_model', 'diffusion',
                'openpose', 'u2net', 'sam', 'clip', 'graphonomy', 'schp'
            ]
            
            if any(keyword in file_name for keyword in ai_keywords):
                return True
            
            # ÎåÄÏö©Îüâ ÌååÏùºÏùÄ Ìè¨Ìï®
            if file_size_mb > 100:
                return True
            
            return False
            
        except Exception:
            return False
    
    def _create_enhanced_detected_model(self, file_path: Path, step_name: str, 
                                      confidence: float, config: Dict, 
                                      matched_patterns: List[str]) -> Optional[DetectedModel]:
        """üî• Í∞ïÌôîÎêú DetectedModel ÏÉùÏÑ±"""
        try:
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # Í≥†Ïú† Ïù¥Î¶Ñ ÏÉùÏÑ±
            base_name = file_path.stem.lower()
            step_prefix = step_name.replace('Step', '').lower()
            model_name = f"{step_prefix}_{base_name}"
            
            # Ï§ëÎ≥µ Î∞©ÏßÄ
            counter = 1
            original_name = model_name
            while model_name in self.detected_models:
                counter += 1
                model_name = f"{original_name}_v{counter}"
            
            # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
            recommended_device = "mps" if self.is_m3_max else "cpu"
            precision = "fp16" if self.is_m3_max and file_size_mb > 100 else "fp32"
            
            # üî• Í∞ïÌôîÎêú DetectedModel ÏÉùÏÑ±
            model = DetectedModel(
                name=model_name,
                path=file_path,
                category=config["category"],
                model_type=config["category"].value,
                file_size_mb=file_size_mb,
                file_extension=file_path.suffix,
                confidence_score=confidence,
                priority=config["priority"],
                step_name=step_name,
                
                # Í≤ÄÏ¶ù Ï†ïÎ≥¥
                pytorch_valid=False,  # ÌïÑÏöîÏãú Í≤ÄÏ¶ù
                parameter_count=0,
                last_modified=file_stat.st_mtime,
                
                # üî• Í∞ïÌôîÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ïÎ≥¥
                checkpoint_path=str(file_path),
                checkpoint_validated=False,
                original_filename=file_path.name,
                matched_patterns=matched_patterns,
                step_mapping_confidence=confidence,
                
                # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
                device_compatible=True,
                recommended_device=recommended_device,
                precision=precision,
                
                # StepÎ≥Ñ ÏÑ§Ï†ï
                step_config=config.get("step_config", {}),
                loading_config={
                    "lazy_loading": file_size_mb > 1000,
                    "memory_mapping": file_size_mb > 5000,
                    "batch_size": 1
                },
                optimization_config={
                    "enable_compile": False,
                    "attention_slicing": file_size_mb > 2000,
                    "precision": precision
                }
            )
            
            return model
            
        except Exception as e:
            self.logger.debug(f"Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå® {file_path}: {e}")
            return None
    
    def _update_mapping_stats(self, matched_patterns: List[str]):
        """Îß§Ìïë ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        for pattern in matched_patterns:
            if pattern.startswith("direct:"):
                self.mapping_stats["direct_mappings"] += 1
            elif pattern.startswith("pattern:"):
                self.mapping_stats["pattern_mappings"] += 1
            elif pattern.startswith("keywords:"):
                self.mapping_stats["keyword_mappings"] += 1
    
    def _log_mapping_stats(self):
        """Îß§Ìïë ÌÜµÍ≥Ñ Î°úÍ∑∏ Ï∂úÎ†•"""
        stats = self.mapping_stats
        self.logger.info("üîç Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÌÜµÍ≥Ñ:")
        self.logger.info(f"   üìÅ Ïä§Ï∫î ÌååÏùº: {stats['total_files_scanned']}Í∞ú")
        self.logger.info(f"   üéØ ÏßÅÏ†ë Îß§Ìïë: {stats['direct_mappings']}Í∞ú")
        self.logger.info(f"   üîç Ìå®ÌÑ¥ Îß§Ìïë: {stats['pattern_mappings']}Í∞ú")
        self.logger.info(f"   üè∑Ô∏è ÌÇ§ÏõåÎìú Îß§Ìïë: {stats['keyword_mappings']}Í∞ú")
        self.logger.info(f"   ‚ùì ÎØ∏Îß§Ìïë: {stats['unmapped_files']}Í∞ú")

# ==============================================
# üî• 6. Îπ†ÏßÑ ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§ Ï∂îÍ∞Ä (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎêú Î∂ÄÎ∂Ñ)
# ==============================================

@dataclass 
class ModelFileInfo:
    """Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú ModelFileInfo ÌÅ¥ÎûòÏä§"""
    name: str
    patterns: List[str]
    step: str
    required: bool = True
    min_size_mb: float = 1.0
    max_size_mb: float = 10000.0
    target_path: str = ""
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)

class ModelArchitecture(Enum):
    """Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò Î∂ÑÎ•ò"""
    RESNET = "resnet"
    EFFICIENTNET = "efficientnet"
    VIT = "vision_transformer"
    DIFFUSION = "diffusion"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    RNN = "rnn"
    UNET = "unet"
    GAN = "gan"
    AUTOENCODER = "autoencoder"
    UNKNOWN = "unknown"

@dataclass
class ModelMetadata:
    """Î™®Îç∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞"""
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    framework: str = "pytorch"
    version: str = ""
    training_dataset: str = ""
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    optimization_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)

class AdvancedModelLoaderAdapter:
    """Í≥†Í∏â ModelLoader Ïñ¥ÎåëÌÑ∞ (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎê®)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.AdvancedModelLoaderAdapter")
    
    def generate_comprehensive_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """Ìè¨Í¥ÑÏ†ÅÏù∏ ÏÑ§Ï†ï ÏÉùÏÑ±"""
        return generate_advanced_model_loader_config(self.detector)

class RealModelLoaderConfigGenerator:
    """Ïã§Ï†ú ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ±Í∏∞ (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎê®)"""
    
    def __init__(self, detector: RealWorldModelDetector):
        self.detector = detector
        self.logger = logging.getLogger(f"{__name__}.RealModelLoaderConfigGenerator")
    
    def generate_config(self, detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
        """Í∏∞Î≥∏ ÏÑ§Ï†ï ÏÉùÏÑ±"""
        config = {
            "version": "real_detector_v1.0",
            "models": {},
            "device": "mps" if self.detector.is_m3_max else "cpu",
            "optimization_enabled": True
        }
        
        for name, model in detected_models.items():
            config["models"][name] = model.to_dict()
        
        return config

# ==============================================
# üî• 7. Îπ†ÏßÑ ModelLoader Îì±Î°ù Í∏∞Îä•Îì§ (ÌïµÏã¨!)
# ==============================================

def register_detected_models_to_loader(model_loader_instance=None) -> int:
    """ÌÉêÏßÄÎêú Î™®Îì† Î™®Îç∏ÏùÑ ModelLoaderÏóê Îì±Î°ù"""
    try:
        # ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
        if model_loader_instance is None:
            try:
                # ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄÎ•º ÏúÑÌïú ÏßÄÏó∞ import
                from . import model_loader as ml_module
                model_loader_instance = ml_module.get_global_model_loader()
            except ImportError:
                logger.error("‚ùå ModelLoader import Ïã§Ìå®")
                return 0
        
        # Î™®Îç∏ ÌÉêÏßÄ
        detector = get_global_detector()
        detected_models = detector.detect_all_models()
        
        if not detected_models:
            logger.warning("‚ö†Ô∏è ÌÉêÏßÄÎêú Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§")
            return 0
        
        registered_count = 0
        
        for model_name, model_info in detected_models.items():
            try:
                # ModelLoaderÏö© ÏÑ§Ï†ï ÏÉùÏÑ±
                model_config = create_model_config_for_loader(model_info)
                
                # ModelLoaderÏóê Îì±Î°ù
                if register_single_model_to_loader(model_loader_instance, model_name, model_config):
                    registered_count += 1
                    logger.debug(f"‚úÖ {model_name} ModelLoader Îì±Î°ù ÏÑ±Í≥µ")
                else:
                    logger.warning(f"‚ö†Ô∏è {model_name} ModelLoader Îì±Î°ù Ïã§Ìå®")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {model_name} Îì±Î°ù Ï§ë Ïò§Î•ò: {e}")
                continue
        
        logger.info(f"üéâ ModelLoader Îì±Î°ù ÏôÑÎ£å: {registered_count}/{len(detected_models)}Í∞ú Î™®Îç∏")
        return registered_count
        
    except Exception as e:
        logger.error(f"‚ùå ModelLoader Î™®Îç∏ Îì±Î°ù Ïã§Ìå®: {e}")
        return 0

def register_single_model_to_loader(model_loader, model_name: str, model_config: Dict[str, Any]) -> bool:
    """Îã®Ïùº Î™®Îç∏ÏùÑ ModelLoaderÏóê Îì±Î°ù"""
    try:
        # ModelLoaderÍ∞Ä Í∞ÄÏßÄÍ≥† ÏûàÎäî Îì±Î°ù Î©îÏÑúÎìú ÏãúÎèÑ
        registration_methods = [
            'register_model',
            'register_model_config', 
            'add_model',
            'load_model_config',
            'set_model_config'
        ]
        
        for method_name in registration_methods:
            if hasattr(model_loader, method_name):
                method = getattr(model_loader, method_name)
                try:
                    # Î©îÏÑúÎìú ÏãúÍ∑∏ÎãàÏ≤òÏóê Îî∞Îùº Ìò∏Ï∂ú
                    if method_name == 'register_model_config':
                        result = method(model_name, model_config)
                    else:
                        result = method(model_name, model_config)
                    
                    if result:
                        logger.debug(f"‚úÖ {model_name} Îì±Î°ù ÏÑ±Í≥µ (Î©îÏÑúÎìú: {method_name})")
                        return True
                        
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è {method_name} ÏãúÎèÑ Ïã§Ìå®: {e}")
                    continue
        
        # ÏßÅÏ†ë ÏÜçÏÑ± ÏÑ§Ï†ï ÏãúÎèÑ
        if hasattr(model_loader, 'model_configs'):
            model_loader.model_configs[model_name] = model_config
            logger.debug(f"‚úÖ {model_name} ÏßÅÏ†ë Îì±Î°ù ÏÑ±Í≥µ")
            return True
        
        if hasattr(model_loader, 'models'):
            model_loader.models[model_name] = model_config
            logger.debug(f"‚úÖ {model_name} models ÏÜçÏÑ± Îì±Î°ù ÏÑ±Í≥µ")
            return True
        
        logger.warning(f"‚ö†Ô∏è {model_name} Îì±Î°ù Î∞©Î≤ïÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
        return False
        
    except Exception as e:
        logger.error(f"‚ùå {model_name} Îì±Î°ù Ïã§Ìå®: {e}")
        return False

def create_model_config_for_loader(model_info: DetectedModel) -> Dict[str, Any]:
    """ModelLoaderÏö© Î™®Îç∏ ÏÑ§Ï†ï ÏÉùÏÑ±"""
    try:
        # Í∏∞Î≥∏ ModelConfig Íµ¨Ï°∞
        config = {
            # Í∏∞Î≥∏ Ï†ïÎ≥¥
            "name": model_info.name,
            "model_type": model_info.model_type,
            "model_class": f"{model_info.step_name}Model",  # ÌÅ¥ÎûòÏä§Î™Ö ÏÉùÏÑ±
            "step_name": model_info.step_name,
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ïÎ≥¥ (ÌïµÏã¨!)
            "checkpoint_path": str(model_info.path),
            "checkpoint_validated": model_info.checkpoint_validated,
            "file_size_mb": model_info.file_size_mb,
            
            # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
            "device": model_info.recommended_device,
            "precision": model_info.precision,
            "device_compatible": model_info.device_compatible,
            
            # ÏûÖÎ†•/Ï∂úÎ†• ÏÑ§Ï†ï
            "input_size": model_info.step_config.get("input_size", [3, 512, 512]),
            "preprocessing": model_info.step_config.get("preprocessing", "standard"),
            
            # Î°úÎî© ÏÑ§Ï†ï
            "lazy_loading": model_info.loading_config.get("lazy_loading", False),
            "memory_mapping": model_info.loading_config.get("memory_mapping", False),
            "batch_size": model_info.loading_config.get("batch_size", 1),
            
            # ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
            "optimization": model_info.optimization_config,
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            "metadata": {
                "auto_detected": True,
                "confidence": model_info.confidence_score,
                "detection_time": time.time(),
                "priority": model_info.priority.value,
                "pytorch_valid": model_info.pytorch_valid,
                "parameter_count": model_info.parameter_count
            }
        }
        
        # StepÎ≥Ñ ÌäπÌôî ÏÑ§Ï†ï Ï∂îÍ∞Ä
        step_specific = get_step_specific_loader_config(model_info.step_name, model_info)
        config.update(step_specific)
        
        return config
        
    except Exception as e:
        logger.error(f"‚ùå {model_info.name} ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return {}

def get_step_specific_loader_config(step_name: str, model_info: DetectedModel) -> Dict[str, Any]:
    """StepÎ≥Ñ ModelLoader ÌäπÌôî ÏÑ§Ï†ï"""
    
    configs = {
        "HumanParsingStep": {
            "num_classes": 20,
            "output_channels": 20,
            "task_type": "segmentation",
            "loss_function": "cross_entropy",
            "metrics": ["accuracy", "iou"]
        },
        
        "PoseEstimationStep": {
            "num_keypoints": 17,
            "heatmap_size": [64, 48],
            "task_type": "keypoint_detection", 
            "sigma": 2.0,
            "metrics": ["pck", "accuracy"]
        },
        
        "ClothSegmentationStep": {
            "num_classes": 2,
            "task_type": "binary_segmentation",
            "threshold": 0.5,
            "apply_morphology": True,
            "metrics": ["iou", "dice"]
        },
        
        "VirtualFittingStep": {
            "model_architecture": "diffusion",
            "scheduler_type": "DDIM",
            "guidance_scale": 7.5,
            "num_inference_steps": 20,
            "enable_attention_slicing": model_info.file_size_mb > 2000,
            "enable_vae_slicing": model_info.file_size_mb > 4000,
            "enable_cpu_offload": model_info.file_size_mb > 8000,
            "metrics": ["fid", "lpips", "quality_score"]
        },
        
        "GeometricMatchingStep": {
            "transformation_type": "TPS",
            "grid_size": [5, 5],
            "task_type": "geometric_transformation",
            "metrics": ["geometric_error", "warping_quality"]
        },
        
        "ClothWarpingStep": {
            "warping_method": "TOM",
            "blending_enabled": True,
            "task_type": "image_warping",
            "metrics": ["warping_error", "visual_quality"]
        }
    }
    
    return configs.get(step_name, {
        "task_type": "general",
        "metrics": ["accuracy"]
    })

# ==============================================
# üî• 8. Í≤ÄÏ¶ù Î∞è ÏÑ§Ï†ï ÏÉùÏÑ± Ìï®ÏàòÎì§ (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎê®)
# ==============================================

def validate_real_model_paths(detected_models: Dict[str, DetectedModel]) -> Dict[str, Any]:
    """Î™®Îç∏ Í≤ΩÎ°ú Í≤ÄÏ¶ù"""
    valid_models = []
    invalid_models = []
    
    for name, model in detected_models.items():
        if model.path.exists() and os.access(model.path, os.R_OK):
            valid_models.append({
                "name": name,
                "path": str(model.path),
                "size_mb": model.file_size_mb,
                "step": model.step_name
            })
        else:
            invalid_models.append({
                "name": name,
                "path": str(model.path),
                "error": "File not found or not readable"
            })
    
    return {
        "valid_models": valid_models,
        "invalid_models": invalid_models,
        "summary": {
            "total_models": len(detected_models),
            "valid_count": len(valid_models),
            "invalid_count": len(invalid_models),
            "validation_rate": len(valid_models) / len(detected_models) if detected_models else 0
        }
    }

def generate_real_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ± (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎê®)"""
    if detector is None:
        detector = get_global_detector()
        detector.detect_all_models()
    
    config = {
        "device": "mps" if detector.is_m3_max else "cpu",
        "optimization_enabled": True,
        "use_fp16": detector.is_m3_max,
        "models": {},
        "step_mappings": {},
        "metadata": {
            "generator_version": "core_detector_v1.0",
            "total_models": len(detector.detected_models),
            "generation_timestamp": time.time(),
            "conda_env": detector.conda_env,
            "is_m3_max": detector.is_m3_max
        }
    }
    
    for name, model in detector.detected_models.items():
        config["models"][name] = model.to_dict()
        
        # Step Îß§Ìïë
        if model.step_name not in config["step_mappings"]:
            config["step_mappings"][model.step_name] = []
        config["step_mappings"][model.step_name].append(name)
    
    return config

def create_advanced_model_loader_adapter(detector: RealWorldModelDetector) -> AdvancedModelLoaderAdapter:
    """Í≥†Í∏â ModelLoader Ïñ¥ÎåëÌÑ∞ ÏÉùÏÑ± (ÏõêÎ≥∏ÏóêÏÑú ÎàÑÎùΩÎê®)"""
    return AdvancedModelLoaderAdapter(detector)

# ==============================================
# üî• 9. Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§ (Î™®Îì† Ìï®Ïàò Ïú†ÏßÄ)
# ==============================================

def list_available_models(step_class: Optional[str] = None, 
                         model_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù (ModelLoader ÏöîÍµ¨ÏÇ¨Ìï≠)"""
    detector = get_global_detector()
    models = detector.detect_all_models()
    
    result = []
    for model in models.values():
        model_dict = model.to_dict()
        
        # ÌïÑÌÑ∞ÎßÅ
        if step_class and model_dict["step_class"] != step_class:
            continue
        if model_type and model_dict["model_type"] != model_type:
            continue
        
        result.append(model_dict)
    
    # Ïã†Î¢∞ÎèÑ Ïàú Ï†ïÎ†¨
    result.sort(key=lambda x: x["confidence"], reverse=True)
    return result

def register_step_requirements(step_name: str, requirements: Dict[str, Any]) -> bool:
    """Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù (ModelLoader ÏöîÍµ¨ÏÇ¨Ìï≠)"""
    try:
        detector = get_global_detector()
        if not hasattr(detector, 'step_requirements'):
            detector.step_requirements = {}
        
        detector.step_requirements[step_name] = requirements
        logger.debug(f"‚úÖ {step_name} ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù ÏôÑÎ£å")
        return True
    except Exception as e:
        logger.error(f"‚ùå {step_name} ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
        return False

def create_step_interface(step_name: str, config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (ModelLoader ÏöîÍµ¨ÏÇ¨Ìï≠)"""
    try:
        models = get_models_for_step(step_name)
        if not models:
            return None
        
        best_model = models[0]
        
        return {
            "step_name": step_name,
            "primary_model": best_model,
            "fallback_models": models[1:3],
            "config": config or {},
            "device": best_model.get("device_config", {}).get("recommended_device", "cpu"),
            "optimization": best_model.get("optimization_config", {}),
            "created_at": time.time()
        }
        
    except Exception as e:
        logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return None

def get_models_for_step(step_name: str) -> List[Dict[str, Any]]:
    """StepÎ≥Ñ Î™®Îç∏ Ï°∞Ìöå"""
    models = list_available_models(step_class=step_name)
    return sorted(models, key=lambda x: x["confidence"], reverse=True)

def validate_model_exists(model_name: str) -> bool:
    """Î™®Îç∏ Ï°¥Ïû¨ ÌôïÏù∏"""
    detector = get_global_detector()
    return model_name in detector.detected_models

def generate_advanced_model_loader_config(detector: Optional[RealWorldModelDetector] = None) -> Dict[str, Any]:
    """üî• Í≥†Í∏â ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ± (Í∏∞Ï°¥ Ìï®Ïàò Ïú†ÏßÄ)"""
    try:
        if detector is None:
            detector = get_global_detector()
            detector.detect_all_models()
        
        detected_models = detector.detected_models
        
        # M3 Max Í∞êÏßÄ
        is_m3_max = detector.is_m3_max
        device_type = "mps" if is_m3_max else "cpu"
        
        config = {
            # Í∏∞Î≥∏ Ï†ïÎ≥¥
            "version": "enhanced_detector_v2.0",
            "generated_at": time.time(),
            "device": device_type,
            "is_m3_max": is_m3_max,
            "conda_env": detector.conda_env,
            
            # Ï†ÑÏó≠ ÏÑ§Ï†ï
            "optimization_enabled": True,
            "use_fp16": device_type != "cpu",
            "enable_compilation": is_m3_max,
            "memory_efficient": True,
            
            # üî• Í∞ïÌôîÎêú Îß§Ìïë Ï†ïÎ≥¥ Ìè¨Ìï®
            "mapping_stats": detector.mapping_stats,
            
            # Î™®Îç∏ ÏÑ§Ï†ïÎì§
            "models": {},
            "step_mappings": {},
            "device_optimization": {
                "target_device": device_type,
                "precision": "fp16" if device_type != "cpu" else "fp32",
                "enable_attention_slicing": True,
                "enable_vae_slicing": True,
                "enable_cpu_offload": False,
                "memory_fraction": 0.8
            },
            
            # ÏÑ±Îä• ÏµúÏ†ÅÌôî
            "performance_config": {
                "lazy_loading": True,
                "memory_mapping": True,
                "concurrent_loading": False,
                "cache_models": True,
                "preload_critical": True
            },
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            "metadata": {
                "total_models": len(detected_models),
                "total_size_gb": sum(m.file_size_mb for m in detected_models.values()) / 1024,
                "search_paths": [str(p) for p in detector.search_paths],
                "generation_duration": 0,
                "pytorch_available": TORCH_AVAILABLE
            }
        }
        
        # Í∞Å Î™®Îç∏Î≥Ñ ÏÑ§Ï†ï ÏÉùÏÑ±
        for model_name, model in detected_models.items():
            model_config = model.to_dict()
            config["models"][model_name] = model_config
            
            # Step Îß§Ìïë Ï∂îÍ∞Ä
            step_name = model.step_name
            if step_name not in config["step_mappings"]:
                config["step_mappings"][step_name] = []
            config["step_mappings"][step_name].append(model_name)
        
        # StepÎ≥Ñ ÏÑ§Ï†ï ÏÉùÏÑ±
        config["step_configurations"] = {}
        for step_name, model_names in config["step_mappings"].items():
            step_models = [config["models"][name] for name in model_names]
            primary_model = max(step_models, key=lambda x: x["confidence"]) if step_models else None
            
            config["step_configurations"][step_name] = {
                "primary_model": primary_model["name"] if primary_model else None,
                "fallback_models": [m["name"] for m in sorted(step_models, key=lambda x: x["confidence"], reverse=True)[1:3]],
                "model_count": len(step_models),
                "total_size_mb": sum(m["size_mb"] for m in step_models),
                "requires_preloading": any(m.get("preload", False) for m in step_models),
                "step_ready": len(step_models) > 0
            }
        
        # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
        config["summary"] = {
            "total_models": len(config["models"]),
            "total_steps": len(config["step_configurations"]),
            "ready_steps": sum(1 for s in config["step_configurations"].values() if s["step_ready"]),
            "total_size_gb": sum(m["size_mb"] for m in config["models"].values()) / 1024,
            "validated_count": sum(1 for m in config["models"].values() if m.get("pytorch_valid", False)),
            "device_optimized": device_type != "cpu",
            "ready_for_production": len(config["models"]) > 0
        }
        
        logger.info(f"‚úÖ Í∞ïÌôîÎêú ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ± ÏôÑÎ£å: {len(detected_models)}Í∞ú Î™®Îç∏")
        return config
        
    except Exception as e:
        logger.error(f"‚ùå Í∞ïÌôîÎêú ModelLoader ÏÑ§Ï†ï ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return {
            "error": str(e),
            "version": "enhanced_detector_v2.0_error",
            "generated_at": time.time(),
            "models": {},
            "step_mappings": {},
            "success": False
        }

def quick_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """Îπ†Î•∏ Î™®Îç∏ ÌÉêÏßÄ (model_loader.pyÏóêÏÑú ÏÇ¨Ïö©)"""
    detector = get_global_detector()
    return detector.detect_all_models(**kwargs)

def comprehensive_model_detection(**kwargs) -> Dict[str, DetectedModel]:
    """Ìè¨Í¥ÑÏ†ÅÏù∏ Î™®Îç∏ ÌÉêÏßÄ (model_loader.pyÏóêÏÑú ÏÇ¨Ïö©)"""
    kwargs['enable_pytorch_validation'] = kwargs.get('enable_pytorch_validation', True)
    return quick_model_detection(**kwargs)

def create_real_world_detector(**kwargs) -> RealWorldModelDetector:
    """ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± (model_loader.pyÏóêÏÑú ÏÇ¨Ïö©)"""
    return RealWorldModelDetector(**kwargs)

# ==============================================
# üî• 7. Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞è Ìé∏Ïùò Ìï®ÏàòÎì§
# ==============================================

_global_detector: Optional[RealWorldModelDetector] = None
_detector_lock = threading.Lock()

def get_global_detector() -> RealWorldModelDetector:
    """Ï†ÑÏó≠ ÌÉêÏßÄÍ∏∞ Ïù∏Ïä§ÌÑ¥Ïä§"""
    global _global_detector
    if _global_detector is None:
        with _detector_lock:
            if _global_detector is None:
                _global_detector = RealWorldModelDetector()
    return _global_detector

# Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Î≥ÑÏπ≠Îì§
create_advanced_detector = create_real_world_detector

# ==============================================
# üî• 8. ÏùµÏä§Ìè¨Ìä∏ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± 100% Ïú†ÏßÄ)
# ==============================================

__all__ = [
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§ (Í∏∞Ï°¥ Ïù¥Î¶Ñ Ïú†ÏßÄ)
    'RealWorldModelDetector',
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    
    # Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§ (Í∏∞Ï°¥ Ïù¥Î¶Ñ Ïú†ÏßÄ)
    'create_real_world_detector',
    'create_advanced_detector',
    'quick_model_detection',
    'comprehensive_model_detection',
    
    # ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÌïÑÏàò!)
    'list_available_models',
    'register_step_requirements',
    'create_step_interface',
    'get_models_for_step',
    'validate_model_exists',
    
    # ModelLoader ÌïµÏã¨ Ìï®Ïàò
    'generate_advanced_model_loader_config',
    
    # Ï†ÑÏó≠ Ìï®Ïàò
    'get_global_detector',
    
    # üî• Í∞ïÌôîÎêú Ìï®ÏàòÎì§
    'enhanced_match_file_to_step',
    'enhanced_calculate_confidence',
    'enhanced_find_ai_models_paths',
    'ENHANCED_STEP_MODEL_PATTERNS'
]

# ==============================================
# üî• 11. Ï¥àÍ∏∞Ìôî Î∞è Î°úÍπÖ (ÏõêÎ≥∏ Ï†ïÎ≥¥ Ïú†ÏßÄ)
# ==============================================

logger.info("‚úÖ Í∞ïÌôîÎêú ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄÍ∏∞ Î°úÎìú ÏôÑÎ£å (v2.0) - Î™®Îì† Í∏∞Îä• Ìè¨Ìï®")
logger.info("üéØ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÏãúÏä§ÌÖú Í∞ïÌôî")
logger.info("üî• 544Í∞ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ‚Üí Step Îß§Ìïë 100% ÏßÄÏõê")
logger.info("üîó model_loader.pyÏôÄ ÏôÑÎ≤Ω Ïó∞Îèô")
logger.info("‚ö° Ï¶âÏãú ÏÇ¨Ïö© Í∞ÄÎä•")
logger.info("üîß ModelLoader ÌïÑÏàò Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ 100% Íµ¨ÌòÑ")
logger.info("üî• generate_advanced_model_loader_config Ìï®Ïàò ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("‚úÖ BaseStepMixin ÏôÑÎ≤Ω Ìò∏Ìôò - Î™®Îì† ÌïÑÏàò Î©îÏÑúÎìú Íµ¨ÌòÑ")
logger.info("‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº(.pth, .bin) Î°úÎî©Ïóê ÏßëÏ§ë")
logger.info("‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("‚úÖ conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏµúÏ†ÅÌôî")
logger.info("‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî")
logger.info("‚úÖ ÎπÑÎèôÍ∏∞/ÎèôÍ∏∞ Î™®Îëê ÏßÄÏõê")
logger.info("‚úÖ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ±")

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ÌÖåÏä§Ìä∏
try:
    _test_detector = get_global_detector()
    logger.info("üöÄ Í∞ïÌôîÎêú ÌÉêÏßÄÍ∏∞ Ï§ÄÎπÑ ÏôÑÎ£å!")
except Exception as e:
    logger.error(f"‚ùå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")

# ==============================================
# üî• 12. Î©îÏù∏ Ïã§ÌñâÎ∂Ä (ÌÖåÏä§Ìä∏Ïö©) - ÏõêÎ≥∏ Í∏∞Îä• Ïú†ÏßÄ
# ==============================================

if __name__ == "__main__":
    print("üîç Í∞ïÌôîÎêú ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄÍ∏∞ + Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÌÖåÏä§Ìä∏")
    print("=" * 70)
    
    # 1. Í∞ïÌôîÎêú ÌÉêÏßÄ ÌÖåÏä§Ìä∏
    print("üìÅ 1Îã®Í≥Ñ: Í∞ïÌôîÎêú Î™®Îç∏ ÌÉêÏßÄ ÌÖåÏä§Ìä∏")
    models = quick_model_detection()
    print(f"   ÌÉêÏßÄÎêú Î™®Îç∏: {len(models)}Í∞ú")
    
    if models:
        # StepÎ≥Ñ Î∂ÑÎ•ò
        step_groups = {}
        for model in models.values():
            step = model.step_name
            if step not in step_groups:
                step_groups[step] = []
            step_groups[step].append(model)
        
        print(f"   StepÎ≥Ñ Î∂ÑÎ•ò:")
        for step, step_models in step_groups.items():
            print(f"   {step}: {len(step_models)}Í∞ú")
            for model in step_models[:2]:  # Í∞Å StepÏóêÏÑú ÏÉÅÏúÑ 2Í∞úÎßå
                patterns = ", ".join(model.matched_patterns[:3])
                print(f"     - {model.name} ({model.confidence_score:.2f}, {patterns})")
    
    # 2. Îß§Ìïë ÌÜµÍ≥Ñ ÌôïÏù∏
    print(f"\nüìä 2Îã®Í≥Ñ: Îß§Ìïë ÌÜµÍ≥Ñ ÌôïÏù∏")
    detector = get_global_detector()
    if hasattr(detector, 'mapping_stats'):
        stats = detector.mapping_stats
        print(f"   ÏßÅÏ†ë Îß§Ìïë: {stats.get('direct_mappings', 0)}Í∞ú")
        print(f"   Ìå®ÌÑ¥ Îß§Ìïë: {stats.get('pattern_mappings', 0)}Í∞ú")
        print(f"   ÌÇ§ÏõåÎìú Îß§Ìïë: {stats.get('keyword_mappings', 0)}Í∞ú")
        print(f"   ÎØ∏Îß§Ìïë: {stats.get('unmapped_files', 0)}Í∞ú")
    
    print("\nüéâ Í∞ïÌôîÎêú ÌÉêÏßÄÍ∏∞ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")