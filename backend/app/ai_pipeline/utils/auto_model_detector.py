# backend/app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ê°œì„ ëœ ìë™ ëª¨ë¸ íƒì§€ê¸° v4.0 (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)
================================================================================
âœ… í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ì™„ì „ ë°˜ì˜
âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ ë§¤í•‘ (ê°€ì§œ ê²½ë¡œ ì œê±°)
âœ… í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì™„ì „ ì •í™•íˆ ì ìš© (50MB ì´ìƒ)
âœ… ModelLoader v5.1ê³¼ ì™„ì „ ì—°ë™ (AI í´ë˜ìŠ¤ í¬í•¨)
âœ… conda í™˜ê²½ + M3 Max ìµœì í™” ìœ ì§€
âœ… 'auto_detector' is not defined ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… ì‹¤ì œ íŒŒì¼ í¬ê¸° ì •í™•íˆ ë°˜ì˜
âœ… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì™„ì „ ê²€ì¦
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ 155ê°œ ì •í™• ë§¤í•‘

ì‹¤ì œ ë°œê²¬ëœ ì£¼ìš” ëª¨ë¸ë“¤:
- v1-5-pruned.safetensors (7.2GB) - Stable Diffusion
- RealVisXL_V4.0.safetensors (6.5GB) - ì‹¤ì œ íŒŒì¼ í™•ì¸ë¨
- open_clip_pytorch_model.bin (5.1GB) - CLIP ëª¨ë¸
- sam_vit_h_4b8939.pth (2.4GB) - Segment Anything Model
- diffusion_pytorch_model.safetensors (3.2GBÃ—4) - OOTD Diffusion
- hrviton_final.pth - HR-VITON ê°€ìƒ í”¼íŒ…
- exp-schp-201908301523-atr.pth - Human Parsing
- body_pose_model.pth - OpenPose
================================================================================
"""

import os
import re
import logging
import time
import json
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì •í™•í•œ ë§¤í•‘ í…Œì´ë¸” (í„°ë¯¸ë„ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
# ==============================================

class RealFileMapper:
    """ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ë™ì  ë§¤í•‘ ì‹œìŠ¤í…œ (í„°ë¯¸ë„ ì¶œë ¥ ê¸°ë°˜ + ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ì§€ì›)"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.RealFileMapper")
        
        # ğŸ”¥ í„°ë¯¸ë„ ì¶œë ¥ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ë°˜ì˜ + ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ì§€ì›
        self.step_file_mappings = {
            # Step 01: Human Parsing (ì‹¤ì œ í™•ì¸ë¨)
            "human_parsing_schp": {
                "actual_files": [
                    "exp-schp-201908301523-atr.pth",
                    "exp-schp-201908261155-atr.pth", 
                    "exp-schp-201908261155-lip.pth",
                    "atr_model.pth",
                    "lip_model.pth",
                    "graphonomy.pth",
                    "graphonomy_alternative.pth",
                    "graphonomy_fixed.pth",
                    "graphonomy_new.pth"
                ],
                "checkpoint_files": [
                    "checkpoint.pth",
                    "model_checkpoint.pth",
                    "human_parsing_checkpoint.pth",
                    "schp_checkpoint.pth",
                    "latest_checkpoint.pth",
                    "best_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_01_human_parsing",
                    "step_01_human_parsing",
                    "step_01_human_parsing/ultra_models",
                    "step_01_human_parsing/checkpoints",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing",
                    "Self-Correction-Human-Parsing",
                    "Graphonomy",
                    "human_parsing",
                    "human_parsing/graphonomy",
                    "human_parsing/schp"
                ],
                "patterns": [r".*exp-schp.*atr.*\.pth$", r".*exp-schp.*lip.*\.pth$", r".*graphonomy.*\.pth$", r".*checkpoint.*\.pth$"],
                "size_range": (50, 1200),
                "min_size_mb": 50,
                "priority": 1,
                "step_class": "HumanParsingStep",
                "ai_class": "RealGraphonomyModel",
                "model_load_method": "load_models"
            },
            
            # Step 02: Pose Estimation (ì‹¤ì œ í™•ì¸ë¨)
            "pose_estimation_openpose": {
                "actual_files": [
                    "body_pose_model.pth",
                    "openpose.pth",
                    "yolov8n-pose.pt",
                    "yolov8m-pose.pt",
                    "yolov8s-pose.pt",
                    "hrnet_w32_coco_256x192.pth",
                    "hrnet_w48_coco_256x192.pth",
                    "hrnet_w48_coco_384x288.pth"
                ],
                "checkpoint_files": [
                    "pose_checkpoint.pth",
                    "openpose_checkpoint.pth",
                    "body_pose_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth",
                    "latest_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_02_pose_estimation",
                    "step_02_pose_estimation",
                    "step_02_pose_estimation/ultra_models",
                    "step_02_pose_estimation/checkpoints",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/ckpts",
                    "openpose",
                    "pose_estimation",
                    "pose_estimation/openpose"
                ],
                "patterns": [r".*openpose.*\.pth$", r".*body_pose.*\.pth$", r".*yolov8.*pose.*\.pt$", r".*hrnet.*\.pth$", r".*checkpoint.*\.pth$"],
                "size_range": (6, 1400),
                "min_size_mb": 6,
                "priority": 1,
                "step_class": "PoseEstimationStep",
                "ai_class": "RealOpenPoseModel",
                "model_load_method": "load_models"
            },
            
            # Step 03: Cloth Segmentation (ì‹¤ì œ í™•ì¸ë¨ - ê°€ì¥ í° ëª¨ë¸)
            "cloth_segmentation_sam": {
                "actual_files": [
                    "sam_vit_h_4b8939.pth",
                    "sam_vit_l_0b3195.pth",
                    "u2net.pth",
                    "u2net_alternative.pth",
                    "u2net_fallback.pth",
                    "u2net_official.pth",
                    "u2net_fixed.pth",
                    "deeplabv3_resnet101_ultra.pth",
                    "mobile_sam.pt",
                    "mobile_sam_alternative.pt"
                ],
                "checkpoint_files": [
                    "cloth_seg_checkpoint.pth",
                    "sam_checkpoint.pth",
                    "u2net_checkpoint.pth",
                    "segmentation_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_03_cloth_segmentation",
                    "step_03_cloth_segmentation",
                    "step_03_cloth_segmentation/ultra_models",
                    "step_03_cloth_segmentation/checkpoints",
                    "step_04_geometric_matching",
                    "step_04_geometric_matching/ultra_models",
                    "cloth_segmentation",
                    "cloth_segmentation/u2net"
                ],
                "patterns": [r".*sam_vit.*\.pth$", r".*u2net.*\.pth$", r".*deeplabv3.*\.pth$", r".*mobile_sam.*\.pt$", r".*checkpoint.*\.pth$"],
                "size_range": (100, 2500),
                "min_size_mb": 100,
                "priority": 1,
                "step_class": "ClothSegmentationStep",
                "ai_class": "RealSAMModel",
                "model_load_method": "load_models"
            },
            
            # Step 04: Geometric Matching (ì‹¤ì œ í™•ì¸ë¨)
            "geometric_matching_model": {
                "actual_files": [
                    "gmm_final.pth",
                    "tps_network.pth",
                    "sam_vit_h_4b8939.pth",
                    "resnet101_geometric.pth",
                    "resnet50_geometric_ultra.pth",
                    "efficientnet_b0_ultra.pth",
                    "raft-things.pth"
                ],
                "checkpoint_files": [
                    "geometric_checkpoint.pth",
                    "gmm_checkpoint.pth",
                    "tps_checkpoint.pth",
                    "matching_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_04_geometric_matching",
                    "step_04_geometric_matching",
                    "step_04_geometric_matching/ultra_models",
                    "step_04_geometric_matching/checkpoints"
                ],
                "patterns": [r".*gmm.*\.pth$", r".*tps.*\.pth$", r".*geometric.*\.pth$", r".*raft.*\.pth$", r".*checkpoint.*\.pth$"],
                "size_range": (10, 2500),
                "min_size_mb": 10,
                "priority": 1,
                "step_class": "GeometricMatchingStep",
                "ai_class": "RealGMMModel",
                "model_load_method": "load_models"
            },
            
            # Step 05: Cloth Warping (ì‹¤ì œ í™•ì¸ë¨ - RealVisXL_V4.0 6.5GB!)
            "cloth_warping_realvisxl": {
                "actual_files": [
                    "RealVisXL_V4.0.safetensors",
                    "vgg19_warping.pth",
                    "vgg16_warping_ultra.pth",
                    "densenet121_ultra.pth",
                    "tom_final.pth"
                ],
                "checkpoint_files": [
                    "warping_checkpoint.pth",
                    "realvisxl_checkpoint.safetensors",
                    "cloth_warping_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth",
                    "vgg_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_05_cloth_warping",
                    "step_05_cloth_warping",
                    "step_05_cloth_warping/ultra_models",
                    "step_05_cloth_warping/ultra_models/unet",
                    "step_05_cloth_warping/checkpoints"
                ],
                "patterns": [
                    r".*realvis.*\.safetensors$", 
                    r".*RealVis.*\.safetensors$",
                    r".*vgg.*warp.*\.pth$",
                    r".*densenet.*\.pth$",
                    r".*tom.*\.pth$",
                    r".*checkpoint.*\.(pth|safetensors)$"
                ],
                "size_range": (30, 7000),  # RealVisXLì€ 6.5GB
                "min_size_mb": 30,
                "priority": 1,
                "step_class": "ClothWarpingStep",
                "ai_class": "RealVisXLModel",
                "model_load_method": "load_models"
            },

            # Step 06: Virtual Fitting (ì‹¤ì œ í™•ì¸ë¨ - OOTD Diffusion)
            "virtual_fitting_ootd": {
                "actual_files": [
                    "diffusion_pytorch_model.safetensors",
                    "diffusion_pytorch_model.bin",
                    "pytorch_model.bin",
                    "hrviton_final.pth"
                ],
                "checkpoint_files": [
                    "ootd_checkpoint.safetensors",
                    "virtual_fitting_checkpoint.pth",
                    "hrviton_checkpoint.pth",
                    "diffusion_checkpoint.safetensors",
                    "checkpoint.pth",
                    "model_checkpoint.pth",
                    "unet_checkpoint.safetensors"
                ],
                "search_paths": [
                    "checkpoints/step_06_virtual_fitting",
                    "step_06_virtual_fitting",
                    "step_06_virtual_fitting/ootdiffusion",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm",
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm",
                    "step_06_virtual_fitting/unet",
                    "step_06_virtual_fitting/vae",
                    "checkpoints/ootdiffusion/checkpoints",
                    "virtual_fitting",
                    "virtual_fitting/ootd"
                ],
                "patterns": [
                    r".*diffusion_pytorch_model\.safetensors$",
                    r".*diffusion_pytorch_model\.bin$",
                    r".*hrviton.*\.pth$",
                    r".*checkpoint.*\.(pth|safetensors|bin)$"
                ],
                "size_range": (100, 3300),
                "min_size_mb": 100,
                "priority": 1,
                "step_class": "VirtualFittingStep",
                "ai_class": "RealOOTDDiffusionModel",
                "model_load_method": "load_models"
            },
            
            # Step 07: Post Processing (ì‹¤ì œ í™•ì¸ë¨)
            "post_processing_gfpgan": {
                "actual_files": [
                    "GFPGAN.pth",
                    "GFPGANv1.4.pth",
                    "ESRGAN_x8.pth",
                    "RealESRGAN_x4plus.pth",
                    "RealESRGAN_x2plus.pth",
                    "densenet161_enhance.pth",
                    "resnet101_enhance_ultra.pth",
                    "mobilenet_v3_ultra.pth",
                    "001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth"
                ],
                "checkpoint_files": [
                    "gfpgan_checkpoint.pth",
                    "post_processing_checkpoint.pth",
                    "esrgan_checkpoint.pth",
                    "enhance_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth"
                ],
                "search_paths": [
                    "checkpoints/step_07_post_processing",
                    "step_07_post_processing",
                    "step_07_post_processing/ultra_models",
                    "step_07_post_processing/esrgan_x8_ultra",
                    "step_07_post_processing/checkpoints"
                ],
                "patterns": [
                    r".*GFPGAN.*\.pth$",
                    r".*ESRGAN.*\.pth$",
                    r".*RealESRGAN.*\.pth$",
                    r".*enhance.*\.pth$",
                    r".*SwinIR.*\.pth$",
                    r".*checkpoint.*\.pth$"
                ],
                "size_range": (30, 350),
                "min_size_mb": 30,
                "priority": 1,
                "step_class": "PostProcessingStep",
                "ai_class": "RealGFPGANModel",
                "model_load_method": "load_models"
            },
            
            # Step 08: Quality Assessment (ì‹¤ì œ í™•ì¸ë¨ - CLIP 5.1GB!)
            "quality_assessment_clip": {
                "actual_files": [
                    "open_clip_pytorch_model.bin",  # âœ… 5.1GB íŒŒì¼ - ê°€ì¥ ì¤‘ìš”!
                    "ViT-L-14.pt",  # âœ… 890MB
                    "ViT-B-32.pt",
                    "lpips_vgg.pth",
                    "lpips_alex.pth",
                    "alex.pth",
                    "clip_vit_b32.pth"
                ],
                "checkpoint_files": [
                    "clip_checkpoint.bin",
                    "quality_assessment_checkpoint.pth",
                    "lpips_checkpoint.pth",
                    "checkpoint.pth",
                    "model_checkpoint.pth",
                    "open_clip_checkpoint.bin"
                ],
                "search_paths": [
                    "checkpoints/step_08_quality_assessment",
                    "step_08_quality_assessment",
                    "step_08_quality_assessment/ultra_models",
                    "step_08_quality_assessment/clip_vit_g14",  # âœ… ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜
                    "step_08_quality_assessment/checkpoints"
                ],
                "patterns": [
                    r".*open_clip.*\.bin$",     # âœ… open_clip íŒŒì¼ íŒ¨í„´
                    r".*ViT-.*\.pt$",
                    r".*clip.*\.pth$",
                    r".*lpips.*\.pth$", 
                    r".*checkpoint.*\.(pth|bin)$"
                ],
                "size_range": (50, 5300),  # âœ… 5.1GB íŒŒì¼ í—ˆìš©
                "min_size_mb": 50,
                "priority": 1,
                "step_class": "QualityAssessmentStep",  # âœ… ì˜¬ë°”ë¥¸ Step í´ë˜ìŠ¤
                "ai_class": "RealCLIPModel",
                "model_load_method": "load_models"
            },

            # Stable Diffusion Models (ëŒ€ìš©ëŸ‰ ëª¨ë¸ë“¤)
            "stable_diffusion_models": {
                "actual_files": [
                    "v1-5-pruned.safetensors",  # 7.2GB
                    "v1-5-pruned-emaonly.safetensors",  # 4.0GB
                    "diffusion_pytorch_model.safetensors",
                    "diffusion_pytorch_model.bin",
                    "diffusion_pytorch_model.fp16.safetensors"
                ],
                "checkpoint_files": [
                    "stable_diffusion_checkpoint.safetensors",
                    "diffusion_checkpoint.safetensors",
                    "checkpoint.safetensors"
                ],
                "search_paths": [
                    "checkpoints/stable-diffusion-v1-5",
                    "checkpoints/stable-diffusion-v1-5/unet",
                    "experimental_models/sdxl_turbo_ultra/unet",
                    "step_02_pose_estimation/ultra_models",
                    "step_04_geometric_matching/ultra_models",
                    "step_05_cloth_warping/ultra_models/unet"
                ],
                "patterns": [
                    r".*v1-5-pruned.*\.safetensors$",
                    r".*diffusion_pytorch_model.*\.safetensors$",
                    r".*diffusion_pytorch_model.*\.bin$"
                ],
                "size_range": (1000, 8000),  # 1-8GB
                "min_size_mb": 1000,
                "priority": 2,
                "step_class": "StableDiffusionStep",
                "ai_class": "RealStableDiffusionModel",
                "model_load_method": "load_models"
            }
        }

        # í¬ê¸° ìš°ì„ ìˆœìœ„ ì„¤ì •
        self.size_priority_threshold = 50  # 50MB ì´ìƒë§Œ
        
        # ì²´í¬í¬ì¸íŠ¸ ì „ìš© ê²€ìƒ‰ íŒ¨í„´
        self.checkpoint_patterns = [
            r".*checkpoint.*\.pth$",
            r".*checkpoint.*\.safetensors$",
            r".*checkpoint.*\.bin$",
            r".*ckpt.*\.pth$",
            r".*model_checkpoint.*",
            r".*latest_checkpoint.*",
            r".*best_checkpoint.*"
        ]
        
        self.logger.info(f"âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ë§¤í•‘ ì´ˆê¸°í™”: {len(self.step_file_mappings)}ê°œ íŒ¨í„´ (ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ì§€ì›)")

    def find_actual_file(self, request_name: str, ai_models_root: Path) -> Optional[Path]:
        """ì‹¤ì œ íŒŒì¼ ì°¾ê¸° - ê²½ë¡œ ìš°ì„ ìˆœìœ„ ê°œì„ """
        try:
            # ì§ì ‘ ë§¤í•‘ í™•ì¸
            if request_name in self.step_file_mappings:
                mapping = self.step_file_mappings[request_name]
                found_candidates = []
                
                # ğŸ”¥ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ìš°ì„  ê²€ìƒ‰
                for search_path in mapping["search_paths"]:
                    search_dir = ai_models_root / search_path
                    if search_dir.exists():
                        for filename in mapping["actual_files"]:
                            full_path = search_dir / filename
                            if full_path.exists() and full_path.is_file():
                                file_size_mb = full_path.stat().st_size / (1024 * 1024)
                                
                                # í¬ê¸° ê²€ì¦
                                min_size, max_size = mapping["size_range"]
                                if min_size <= file_size_mb <= max_size:
                                    # ğŸ”¥ ê²½ë¡œ ê¸°ë°˜ Step ê²€ì¦
                                    inferred_step = self._infer_step_from_path(full_path)
                                    expected_step = mapping.get("step_class", "").replace("Step", "").lower()
                                    
                                    # Step ë§¤ì¹­ í™•ì¸
                                    if expected_step in inferred_step or inferred_step in expected_step:
                                        found_candidates.append((full_path, file_size_mb, "exact_match"))
                                        self.logger.info(f"âœ… ì •í™•í•œ ê²½ë¡œ ë§¤ì¹­: {request_name} â†’ {full_path}")
                
                if found_candidates:
                    # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
                    found_candidates.sort(key=lambda x: x[1], reverse=True)
                    return found_candidates[0][0]
            
            # í´ë°± ê²€ìƒ‰ ì‹œë„
            return self._fallback_search_with_checkpoints(request_name, ai_models_root)
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _fallback_search_with_checkpoints(self, request_name: str, ai_models_root: Path) -> Optional[Path]:
        """í´ë°± ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜ + ì²´í¬í¬ì¸íŠ¸ ìš°ì„ )"""
        try:
            keywords = request_name.lower().split('_')
            candidates = []
            
            extensions = ['.pth', '.bin', '.safetensors', '.pt', '.ckpt']
            
            for ext in extensions:
                for model_file in ai_models_root.rglob(f"*{ext}"):
                    if model_file.is_file():
                        file_size_mb = model_file.stat().st_size / (1024 * 1024)
                        if file_size_mb >= self.size_priority_threshold:
                            filename_lower = model_file.name.lower()
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                            score = sum(1 for keyword in keywords if keyword in filename_lower)
                            
                            # ì²´í¬í¬ì¸íŠ¸ ë³´ë„ˆìŠ¤ ì ìˆ˜
                            is_checkpoint = any(pattern.replace(r'.*', '').replace(r'\.', '.').replace('$', '') in filename_lower 
                                              for pattern in self.checkpoint_patterns)
                            checkpoint_bonus = 10 if is_checkpoint else 0
                            
                            if score > 0:
                                total_score = score + checkpoint_bonus
                                match_type = "checkpoint_fallback" if is_checkpoint else "keyword_fallback"
                                candidates.append((model_file, file_size_mb, total_score, match_type))
            
            if candidates:
                # ì´ì  ìš°ì„ , í¬ê¸° ì°¨ì„ ìœ¼ë¡œ ì •ë ¬
                candidates.sort(key=lambda x: (x[2], x[1]), reverse=True)
                best_match = candidates[0]
                self.logger.info(f"ğŸ” í´ë°± ë§¤ì¹­: {request_name} â†’ {best_match[0]} ({best_match[1]:.1f}MB, {best_match[3]})")
                return best_match[0]
                
            return None
            
        except Exception as e:
            self.logger.debug(f"í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def find_checkpoint_file(self, model_key: str) -> Optional[str]:
        """
        ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì „ìš© ê²€ìƒ‰ ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
        
        Args:
            model_key: ëª¨ë¸ í‚¤ (ì˜ˆ: "human_parsing_schp")
            
        Returns:
            ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” None
        """
        try:
            # ai_models ë””ë ‰í† ë¦¬ ìë™ ê°ì§€
            possible_roots = [
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models"),
                Path("./backend/ai_models"),
                Path("./ai_models"),
                Path("../ai_models"),
                Path.cwd() / "ai_models",
                Path.cwd() / "backend" / "ai_models"
            ]
            
            ai_models_root = None
            for root in possible_roots:
                if root.exists():
                    ai_models_root = root.resolve()
                    break
            
            if not ai_models_root:
                self.logger.error("âŒ ai_models ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # ê¸°ì¡´ find_actual_file ë©”ì„œë“œ í™œìš©
            found_file = self.find_actual_file(model_key, ai_models_root)
            
            if found_file:
                return str(found_file)
            else:
                self.logger.warning(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_key}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨ ({model_key}): {e}")
            return None

    def get_step_info(self, request_name: str) -> Optional[Dict[str, Any]]:
        """Step êµ¬í˜„ì²´ ì •ë³´ ë°˜í™˜ (ModelLoader v5.1 í˜¸í™˜)"""
        if request_name in self.step_file_mappings:
            mapping = self.step_file_mappings[request_name]
            return {
                "step_class": mapping.get("step_class"),
                "ai_class": mapping.get("ai_class"),
                "model_load_method": mapping.get("model_load_method"),
                "priority": mapping.get("priority"),
                "patterns": mapping.get("patterns", []),
                "min_size_mb": mapping.get("min_size_mb", self.size_priority_threshold),
                "has_checkpoints": len(mapping.get("checkpoint_files", [])) > 0,
                "checkpoint_count": len(mapping.get("checkpoint_files", [])),
                "actual_file_count": len(mapping.get("actual_files", []))
            }
        return None

    def get_models_by_step(self, step_id: int) -> List[str]:
        """Step IDë¡œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        step_mapping = {
            1: "HumanParsingStep",
            2: "PoseEstimationStep", 
            3: "ClothSegmentationStep",
            4: "GeometricMatchingStep",
            5: "ClothWarpingStep",
            6: "VirtualFittingStep",
            7: "PostProcessingStep",
            8: "QualityAssessmentStep"
        }
        
        target_step = step_mapping.get(step_id)
        if not target_step:
            return []
        
        matching_models = []
        for model_name, mapping in self.step_file_mappings.items():
            if mapping.get("step_class") == target_step:
                matching_models.append(model_name)
        
        return matching_models

    def _infer_step_from_path(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ì •í™•í•œ Step ì¶”ë¡ """
        path_str = str(file_path).lower()
        
        # ğŸ”¥ ê²½ë¡œ ê¸°ë°˜ ìš°ì„  ë§¤í•‘ (ê°€ì¥ ì •í™•í•¨)
        step_path_mappings = {
            "step_01_human_parsing": "step_01_human_parsing",
            "step_02_pose_estimation": "step_02_pose_estimation", 
            "step_03_cloth_segmentation": "step_03_cloth_segmentation",
            "step_04_geometric_matching": "step_04_geometric_matching",
            "step_05_cloth_warping": "step_05_cloth_warping",
            "step_06_virtual_fitting": "step_06_virtual_fitting",
            "step_07_post_processing": "step_07_post_processing",
            "step_08_quality_assessment": "step_08_quality_assessment"
        }
        
        # ê²½ë¡œì—ì„œ step í´ë” ì°¾ê¸°
        for step_folder, step_name in step_path_mappings.items():
            if step_folder in path_str:
                return step_name
        
        # ğŸ”¥ íŒŒì¼ëª… ê¸°ë°˜ ë³´ì¡° ë§¤í•‘ (ê²½ë¡œ ë§¤í•‘ ì‹¤íŒ¨ì‹œë§Œ)
        filename = file_path.name.lower()
        
        # CLIP ëª¨ë¸ë“¤ â†’ Quality Assessment (Step 08)
        if any(pattern in filename for pattern in ['open_clip', 'clip_vit', 'vit-b-32', 'vit-l-14']):
            return "step_08_quality_assessment"
        
        # Human Parsing ëª¨ë¸ë“¤ â†’ Step 01
        if any(pattern in filename for pattern in ['schp', 'atr', 'lip', 'graphonomy', 'human_parsing']):
            return "step_01_human_parsing"
        
        # Pose ëª¨ë¸ë“¤ â†’ Step 02  
        if any(pattern in filename for pattern in ['openpose', 'body_pose', 'pose', 'hrnet', 'yolov8']):
            return "step_02_pose_estimation"
        
        # Cloth Segmentation ëª¨ë¸ë“¤ â†’ Step 03
        if any(pattern in filename for pattern in ['sam', 'u2net', 'segmentation', 'cloth_seg']):
            return "step_03_cloth_segmentation"
        
        # Geometric Matching ëª¨ë¸ë“¤ â†’ Step 04
        if any(pattern in filename for pattern in ['gmm', 'geometric', 'matching', 'tps']):
            return "step_04_geometric_matching"
        
        # Cloth Warping ëª¨ë¸ë“¤ â†’ Step 05
        if any(pattern in filename for pattern in ['realvis', 'warping', 'xl', 'stable_diffusion']):
            return "step_05_cloth_warping"
        
        # Virtual Fitting ëª¨ë¸ë“¤ â†’ Step 06
        if any(pattern in filename for pattern in ['ootd', 'virtual', 'fitting', 'hrviton', 'diffusion']):
            return "step_06_virtual_fitting"
        
        # Post Processing ëª¨ë¸ë“¤ â†’ Step 07
        if any(pattern in filename for pattern in ['gfpgan', 'esrgan', 'enhance', 'post_process']):
            return "step_07_post_processing"
        
        # Quality Assessment ëª¨ë¸ë“¤ â†’ Step 08
        if any(pattern in filename for pattern in ['quality', 'assessment', 'lpips', 'clip']):
            return "step_08_quality_assessment"
        
        # ê¸°ë³¸ê°’
        return "UnknownStep"

    def validate_model_files(self, ai_models_root: Path = None) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒì¼ ìœ íš¨ì„± ê²€ì¦"""
        if not ai_models_root:
            ai_models_root = Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
        
        validation_results = {
            "total_models": len(self.step_file_mappings),
            "found_models": 0,
            "missing_models": [],
            "found_checkpoints": 0,
            "model_details": {}
        }
        
        for model_name, mapping in self.step_file_mappings.items():
            model_found = False
            checkpoint_found = False
            
            # ì‹¤ì œ íŒŒì¼ í™•ì¸
            for filename in mapping["actual_files"]:
                for search_path in mapping["search_paths"]:
                    full_path = ai_models_root / search_path / filename
                    if full_path.exists():
                        model_found = True
                        break
                if model_found:
                    break
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
            for filename in mapping.get("checkpoint_files", []):
                for search_path in mapping["search_paths"]:
                    full_path = ai_models_root / search_path / filename
                    if full_path.exists():
                        checkpoint_found = True
                        break
                if checkpoint_found:
                    break
            
            if model_found or checkpoint_found:
                validation_results["found_models"] += 1
                if checkpoint_found:
                    validation_results["found_checkpoints"] += 1
            else:
                validation_results["missing_models"].append(model_name)
            
            validation_results["model_details"][model_name] = {
                "model_found": model_found,
                "checkpoint_found": checkpoint_found,
                "step_class": mapping.get("step_class"),
                "priority": mapping.get("priority")
            }
        
        return validation_results
# ==============================================
# ğŸ”¥ 2. DetectedModel í´ë˜ìŠ¤ (ModelLoader v5.1 ì™„ì „ í˜¸í™˜)
# ==============================================

@dataclass
class DetectedModel:
    """íƒì§€ëœ ëª¨ë¸ ì •ë³´ + ModelLoader v5.1 ì™„ì „ í˜¸í™˜"""
    name: str
    path: Path
    step_name: str
    model_type: str
    file_size_mb: float
    confidence_score: float
    
    # ğŸ”¥ ModelLoader v5.1 ì—°ë™ ì •ë³´
    step_class_name: Optional[str] = None
    ai_class: Optional[str] = None
    model_load_method: Optional[str] = None
    step_can_load: bool = False
    
    # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë³´
    priority_score: float = 0.0
    is_large_model: bool = False
    meets_size_requirement: bool = False
    
    # ì¶”ê°€ ì •ë³´
    checkpoint_path: Optional[str] = None
    device_compatible: bool = True
    recommended_device: str = "cpu"
    
    def __post_init__(self):
        """ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ìë™ ê³„ì‚°"""
        self.priority_score = self._calculate_priority_score()
        self.is_large_model = self.file_size_mb > 1000  # 1GB ì´ìƒ
        self.meets_size_requirement = self.file_size_mb >= 50  # 50MB ì´ìƒ
    
    def _calculate_priority_score(self) -> float:
        """ğŸ”¥ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        if self.file_size_mb > 0:
            import math
            score += math.log10(max(self.file_size_mb, 1)) * 100
        
        # ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
        score += self.confidence_score * 50
        
        # ëŒ€í˜• ëª¨ë¸ ë³´ë„ˆìŠ¤
        if self.file_size_mb > 7000:  # 7GB+ (v1-5-pruned.safetensors)
            score += 700
        elif self.file_size_mb > 6000:  # 6GB+ (RealVisXL)
            score += 600
        elif self.file_size_mb > 5000:  # 5GB+ (CLIP)
            score += 500
        elif self.file_size_mb > 3000:  # 3GB+ (OOTD Diffusion)
            score += 300
        elif self.file_size_mb > 2000:  # 2GB+ (SAM)
            score += 200
        elif self.file_size_mb > 1000:  # 1GB+
            score += 100
        elif self.file_size_mb > 500:   # 500MB+
            score += 50
        elif self.file_size_mb > 200:   # 200MB+
            score += 20
        elif self.file_size_mb >= 50:   # 50MB+
            score += 10
        else:
            score -= 100  # 50MB ë¯¸ë§Œì€ ê°ì 
        
        # Step ë¡œë“œ ê°€ëŠ¥ ë³´ë„ˆìŠ¤
        if self.step_can_load:
            score += 30
        
        # AI í´ë˜ìŠ¤ ë³´ë„ˆìŠ¤
        if self.ai_class and self.ai_class != "BaseRealAIModel":
            score += 20
        
        return score
    
    def to_dict(self) -> Dict[str, Any]:
        """ğŸ”¥ ModelLoader v5.1 ì™„ì „ í˜¸í™˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "name": self.name,
            "path": str(self.path),
            "checkpoint_path": self.checkpoint_path or str(self.path),
            "step_class": self.step_name,
            "model_type": self.model_type,
            "size_mb": self.file_size_mb,
            "confidence": self.confidence_score,
            "device_config": {
                "recommended_device": self.recommended_device,
                "device_compatible": self.device_compatible
            },
            
            # ğŸ”¥ ModelLoader v5.1 í˜¸í™˜ AI ëª¨ë¸ ì •ë³´
            "ai_model_info": {
                "ai_class": self.ai_class or "BaseRealAIModel",
                "can_create_ai_model": bool(self.ai_class),
                "device_compatible": self.device_compatible,
                "recommended_device": self.recommended_device
            },
            
            # ğŸ”¥ Step ì—°ë™ ì •ë³´
            "step_implementation": {
                "step_class_name": self.step_class_name,
                "model_load_method": self.model_load_method,
                "step_can_load": self.step_can_load,
                "load_ready": self.step_can_load and self.checkpoint_path is not None
            },
            
            # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ ì •ë³´
            "priority_info": {
                "priority_score": self.priority_score,
                "is_large_model": self.is_large_model,
                "meets_size_requirement": self.meets_size_requirement,
                "size_category": self._get_size_category()
            },
            
            "metadata": {
                "detection_time": time.time(),
                "file_extension": self.path.suffix,
                "detector_version": "v4.0_real_structure_based"
            }
        }
    
    def _get_size_category(self) -> str:
        """í¬ê¸° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if self.file_size_mb >= 7000:
            return "ultra_massive"  # 7GB+ (v1-5-pruned)
        elif self.file_size_mb >= 6000:
            return "ultra_large"    # 6GB+ (RealVisXL)
        elif self.file_size_mb >= 5000:
            return "very_large"     # 5GB+ (CLIP)
        elif self.file_size_mb >= 3000:
            return "large"          # 3GB+ (OOTD)
        elif self.file_size_mb >= 2000:
            return "medium_large"   # 2GB+ (SAM)
        elif self.file_size_mb >= 1000:
            return "medium"         # 1GB+
        elif self.file_size_mb >= 500:
            return "small_large"    # 500MB+
        elif self.file_size_mb >= 200:
            return "small_medium"   # 200MB+
        elif self.file_size_mb >= 50:
            return "small_valid"    # 50MB+
        else:
            return "too_small"      # 50MB ë¯¸ë§Œ
    
    def can_be_loaded_by_step(self) -> bool:
        """Step êµ¬í˜„ì²´ë¡œ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return (self.step_can_load and 
                self.step_class_name is not None and 
                self.model_load_method is not None and
                self.checkpoint_path is not None and
                self.meets_size_requirement and
                self.ai_class is not None)

# ==============================================
# ğŸ”¥ 3. ìˆ˜ì •ëœ ëª¨ë¸ íƒì§€ê¸° (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)
# ==============================================

class FixedModelDetector:
    """ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ íƒì§€ê¸° v4.0"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.FixedModelDetector")
        self.file_mapper = RealFileMapper()
        self.ai_models_root = self._find_ai_models_root()
        self.detected_models: Dict[str, DetectedModel] = {}
        
        # ğŸ”¥ í¬ê¸° ê¸°ë°˜ í•„í„°ë§ ì„¤ì •
        self.min_model_size_mb = 50  # 50MB ë¯¸ë§Œì€ ì œì™¸
        self.prioritize_large_models = True
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.is_m3_max = self._detect_m3_max()
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        
        # í†µê³„ ì •ë³´
        self.detection_stats = {
            "total_files_scanned": 0,
            "models_found": 0,
            "large_models_found": 0,
            "small_models_filtered": 0,
            "step_loadable_models": 0,
            "ai_class_assigned": 0,
            "scan_duration": 0.0
        }
        
        self.logger.info(f"ğŸ”§ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ íƒì§€ê¸° v4.0 ì´ˆê¸°í™”")
        self.logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        self.logger.info(f"   ìµœì†Œ í¬ê¸°: {self.min_model_size_mb}MB")
        self.logger.info(f"   M3 Max: {self.is_m3_max}, conda: {bool(self.conda_env)}")
    
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        backend_root = None
        current = Path(__file__).parent.absolute()
        temp_current = current
        
        for _ in range(10):
            if temp_current.name == 'backend':
                backend_root = temp_current
                break
            if temp_current.name == 'mycloset-ai':
                backend_root = temp_current / 'backend'
                break
            if temp_current.parent == temp_current:
                break
            temp_current = temp_current.parent
        
        if backend_root:
            ai_models_path = backend_root / 'ai_models'
            self.logger.info(f"âœ… AI ëª¨ë¸ ê²½ë¡œ ê³„ì‚°: {ai_models_path}")
            return ai_models_path
        
        fallback_backend = current.parent.parent.parent.parent
        if fallback_backend.name == 'backend':
            ai_models_path = fallback_backend / 'ai_models'
            self.logger.info(f"âœ… í´ë°± AI ëª¨ë¸ ê²½ë¡œ: {ai_models_path}")
            return ai_models_path
        
        # ìµœì¢… í´ë°±: í•˜ë“œì½”ë”©ëœ ê²½ë¡œ
        final_fallback = Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
        self.logger.warning(f"âš ï¸ ìµœì¢… í´ë°± ê²½ë¡œ ì‚¬ìš©: {final_fallback}")
        return final_fallback

    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            return 'arm64' in platform.machine().lower()
        except:
            return False
    
    def detect_all_models(self) -> Dict[str, DetectedModel]:
        """ğŸ”¥ ëª¨ë“  ëª¨ë¸ íƒì§€ (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)"""
        start_time = time.time()
        self.detected_models.clear()
        self.detection_stats = {
            "total_files_scanned": 0,
            "models_found": 0,
            "large_models_found": 0,
            "small_models_filtered": 0,
            "step_loadable_models": 0,
            "ai_class_assigned": 0,
            "scan_duration": 0.0
        }
        
        if not self.ai_models_root.exists():
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.ai_models_root}")
            return {}
        
        self.logger.info("ğŸ” ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ íƒì§€ ì‹œì‘...")
        
        # ìš”ì²­ëª…ë³„ë¡œ ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
        for request_name in self.file_mapper.step_file_mappings.keys():
            try:
                # 1. ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
                actual_file = self.file_mapper.find_actual_file(request_name, self.ai_models_root)
                
                if actual_file:
                    # 2. Step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    step_info = self.file_mapper.get_step_info(request_name)
                    
                    # 3. DetectedModel ìƒì„±
                    model = self._create_detected_model_with_step_info(request_name, actual_file, step_info)
                    if model and model.meets_size_requirement:
                        self.detected_models[model.name] = model
                        self.detection_stats["models_found"] += 1
                        
                        if model.is_large_model:
                            self.detection_stats["large_models_found"] += 1
                        
                        if model.can_be_loaded_by_step():
                            self.detection_stats["step_loadable_models"] += 1
                        
                        if model.ai_class and model.ai_class != "BaseRealAIModel":
                            self.detection_stats["ai_class_assigned"] += 1
                            
                    elif model:
                        self.detection_stats["small_models_filtered"] += 1
                        self.logger.debug(f"ğŸ—‘ï¸ í¬ê¸° ë¶€ì¡±ìœ¼ë¡œ ì œì™¸: {request_name} ({model.file_size_mb:.1f}MB)")
                        
            except Exception as e:
                self.logger.error(f"âŒ {request_name} íƒì§€ ì‹¤íŒ¨: {e}")
                continue
        
        # ğŸ”¥ ì¶”ê°€ íŒŒì¼ë“¤ ìë™ ìŠ¤ìº”
        self._scan_additional_large_files()
        
        # ğŸ”¥ í¬ê¸° ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        if self.prioritize_large_models:
            self._sort_models_by_priority()
        
        self.detection_stats["scan_duration"] = time.time() - start_time
        
        self.logger.info(f"ğŸ‰ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {self.detection_stats['models_found']}ê°œ")
        self.logger.info(f"ğŸ“Š ëŒ€í˜• ëª¨ë¸: {self.detection_stats['large_models_found']}ê°œ")
        self.logger.info(f"ğŸ§  AI í´ë˜ìŠ¤ í• ë‹¹: {self.detection_stats['ai_class_assigned']}ê°œ")
        self.logger.info(f"ğŸ—‘ï¸ ì‘ì€ ëª¨ë¸ ì œì™¸: {self.detection_stats['small_models_filtered']}ê°œ")
        self.logger.info(f"âœ… Step ë¡œë“œ ê°€ëŠ¥: {self.detection_stats['step_loadable_models']}ê°œ")
        self.logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {self.detection_stats['scan_duration']:.2f}ì´ˆ")
        
        return self.detected_models
    
    def _create_detected_model_with_step_info(self, request_name: str, file_path: Path, step_info: Optional[Dict]) -> Optional[DetectedModel]:
        """DetectedModel ìƒì„±"""
        try:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # Step ì´ë¦„ ì¶”ì¶œ
            step_name = self._extract_step_name(request_name)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            recommended_device = "mps" if self.is_m3_max else "cpu"
            
            # Step ì—°ë™ ì •ë³´ ì„¤ì •
            step_class_name = None
            ai_class = None
            model_load_method = None
            step_can_load = False
            
            if step_info:
                step_class_name = step_info.get("step_class")
                ai_class = step_info.get("ai_class")
                model_load_method = step_info.get("model_load_method", "load_models")
                step_can_load = bool(step_class_name and model_load_method and ai_class)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_size_based_confidence(file_size_mb, step_info, ai_class)
            
            model = DetectedModel(
                name=request_name,
                path=file_path,
                step_name=step_name,
                model_type=step_name.replace("Step", "").lower(),
                file_size_mb=file_size_mb,
                confidence_score=confidence_score,
                
                # ModelLoader v5.1 ì—°ë™ ì •ë³´
                step_class_name=step_class_name,
                ai_class=ai_class,
                model_load_method=model_load_method,
                step_can_load=step_can_load,
                
                checkpoint_path=str(file_path),
                device_compatible=True,
                recommended_device=recommended_device
            )
            
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ {request_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_size_based_confidence(self, file_size_mb: float, step_info: Optional[Dict], ai_class: Optional[str]) -> float:
        """í¬ê¸° ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # í¬ê¸° ê¸°ë°˜ ì‹ ë¢°ë„
        if file_size_mb >= 7000:    # 7GB+ (v1-5-pruned)
            confidence = 1.0
        elif file_size_mb >= 6000:  # 6GB+ (RealVisXL)
            confidence = 0.99
        elif file_size_mb >= 5000:  # 5GB+ (CLIP)
            confidence = 0.98
        elif file_size_mb >= 3000:  # 3GB+ (OOTD)
            confidence = 0.95
        elif file_size_mb >= 2000:  # 2GB+ (SAM)
            confidence = 0.92
        elif file_size_mb >= 1000:  # 1GB+
            confidence = 0.9
        elif file_size_mb >= 500:   # 500MB+
            confidence = 0.8
        elif file_size_mb >= 200:   # 200MB+
            confidence = 0.7
        elif file_size_mb >= 100:   # 100MB+
            confidence = 0.6
        elif file_size_mb >= 50:    # 50MB+
            confidence = 0.5
        else:  # 50MB ë¯¸ë§Œ
            confidence = 0.1
        
        # Step ì •ë³´ ë³´ë„ˆìŠ¤
        if step_info:
            min_expected_size = step_info.get("min_size_mb", 50)
            if file_size_mb >= min_expected_size:
                confidence += 0.1
        
        # AI í´ë˜ìŠ¤ ë³´ë„ˆìŠ¤
        if ai_class and ai_class != "BaseRealAIModel":
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _extract_step_name(self, request_name: str) -> str:
        """ìš”ì²­ëª…ì—ì„œ Step ì´ë¦„ ì¶”ì¶œ"""
        step_mappings = {
            "human_parsing": "HumanParsingStep",
            "pose_estimation": "PoseEstimationStep", 
            "cloth_segmentation": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting": "VirtualFittingStep",
            "post_processing": "PostProcessingStep",
            "quality_assessment": "QualityAssessmentStep",
            "stable_diffusion": "StableDiffusionStep"
        }
        
        for key, step_name in step_mappings.items():
            if key in request_name:
                return step_name
        
        return "UnknownStep"
    
    def _scan_additional_large_files(self):
        """ğŸ”¥ ì¶”ê°€ ëŒ€í˜• íŒŒì¼ë“¤ ìë™ ìŠ¤ìº”"""
        try:
            # 1GB ì´ìƒ íŒŒì¼ë“¤ ìŠ¤ìº”
            large_file_threshold_mb = 1000
            model_extensions = {'.pth', '.bin', '.safetensors', '.ckpt'}
            
            candidates = []
            
            for file_path in self.ai_models_root.rglob('*'):
                if (file_path.is_file() and 
                    file_path.suffix.lower() in model_extensions):
                    
                    try:
                        file_size_mb = file_path.stat().st_size / (1024 * 1024)
                        
                        # 1GB ì´ìƒë§Œ ìŠ¤ìº”
                        if file_size_mb >= large_file_threshold_mb:
                            candidates.append((file_path, file_size_mb))
                            
                    except Exception as e:
                        self.logger.debug(f"ëŒ€í˜• íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                        continue
            
            # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
            candidates.sort(key=lambda x: x[1], reverse=True)
            
            for file_path, file_size_mb in candidates:
                model_name = f"large_{file_path.parent.name}_{file_path.stem}"
                
                # ì¤‘ë³µ ë°©ì§€
                if any(m.path == file_path for m in self.detected_models.values()):
                    continue
                
                # AI í´ë˜ìŠ¤ ìë™ ì¶”ë¡ 
                ai_class = self._infer_ai_class_from_filename(file_path.name)
                
                model = DetectedModel(
                    name=model_name,
                    path=file_path,
                    step_name="LargeModel",
                    model_type="large",
                    file_size_mb=file_size_mb,
                    confidence_score=self._calculate_size_based_confidence(file_size_mb, None, ai_class),
                    ai_class=ai_class,
                    checkpoint_path=str(file_path),
                    device_compatible=True,
                    recommended_device="mps" if self.is_m3_max else "cpu"
                )
                
                if model.meets_size_requirement:
                    self.detected_models[model_name] = model
                    self.detection_stats["models_found"] += 1
                    
                    if model.is_large_model:
                        self.detection_stats["large_models_found"] += 1
                    
                    if ai_class and ai_class != "BaseRealAIModel":
                        self.detection_stats["ai_class_assigned"] += 1
                    
                    self.logger.debug(f"âœ… ëŒ€í˜• ëª¨ë¸ ì¶”ê°€: {model_name} ({file_size_mb:.1f}MB) â†’ {ai_class}")
                
        except Exception as e:
            self.logger.debug(f"ëŒ€í˜• íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
    
    def _infer_ai_class_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œë¶€í„° AI í´ë˜ìŠ¤ ì¶”ë¡ """
        filename_lower = filename.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ AI í´ë˜ìŠ¤ ë§¤í•‘
        ai_class_patterns = {
            "RealGraphonomyModel": ["graphonomy", "schp", "atr", "lip", "human_parsing"],
            "RealSAMModel": ["sam", "segment", "u2net", "cloth_segmentation"],
            "RealVisXLModel": ["realvis", "visxl", "xl", "warping"],
            "RealOOTDDiffusionModel": ["diffusion", "ootd", "unet", "virtual", "fitting"],
            "RealCLIPModel": ["clip", "vit", "open_clip", "quality"],
            "RealGFPGANModel": ["gfpgan", "gfp", "post_processing"],
            "RealESRGANModel": ["esrgan", "esr", "enhance"],
            "RealOpenPoseModel": ["openpose", "pose", "body", "keypoint"],
            "RealYOLOModel": ["yolo", "detection"],
            "RealHRVITONModel": ["hrviton", "hr_viton"],
            "RealStableDiffusionModel": ["v1-5-pruned", "stable_diffusion", "diffusion_pytorch_model"]
        }