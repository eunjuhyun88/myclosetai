# app/ai_pipeline/utils/auto_model_detector.py
"""
ğŸ” MyCloset AI - ì™„ì „ í†µí•© ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v7.0 - 89.8GB ì‹¤ì œ í™œìš© ê°•í™” ë²„ì „
====================================================================================

âœ… 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ ë™ì‘í•˜ëŠ” íƒì§€ ë¡œì§ ì™„ì „ ë°˜ì˜ ë° ê°•í™”
âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI ëª¨ë¸ íŒŒì¼ë“¤ ì •í™•í•œ íƒì§€ + ë”¥ëŸ¬ë‹ ê²€ì¦
âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ê²€ì¦ + ëª¨ë¸ êµ¬ì¡° ë¶„ì„
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì—°ë™)
âœ… M3 Max 128GB ìµœì í™” + ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
âœ… conda í™˜ê²½ íŠ¹í™” ìŠ¤ìº” + í™˜ê²½ë³„ ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥ + ì‹¤ë¬´ê¸‰ ì„±ëŠ¥
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ì™„ì „ í™œìš© + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
âœ… ê¸°ì¡´ í´ë˜ìŠ¤ëª…/í•¨ìˆ˜ëª… 100% ìœ ì§€ + ê¸°ëŠ¥ ëŒ€í­ ê°•í™”

ğŸ”¥ í•µì‹¬ ë³€ê²½ì‚¬í•­ v7.0:
- 89.8GB ì²´í¬í¬ì¸íŠ¸ì˜ ì‹¤ì œ PyTorch ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ë° ê²€ì¦
- ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì„±ëŠ¥ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ëª¨ë¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ìë™ ìµœì í™”
- Stepë³„ ë§ì¶¤í˜• ëª¨ë¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
- M3 Max Neural Engine í™œìš© ìµœì í™”
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë° ê´€ë¦¬
- ëª¨ë¸ í˜¸í™˜ì„± ìë™ ê²€ì¦ ì‹œìŠ¤í…œ
- í”„ë¡œë•ì…˜ ë ˆë²¨ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
"""

import os
import re
import time
import logging
import hashlib
import json
import threading
import sqlite3
import psutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from functools import lru_cache, wraps
import weakref
import pickle
import yaml

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ (ê°•í™”ëœ ì•ˆì „ import)
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader
    TORCH_AVAILABLE = True
    
    # M3 Max íŠ¹í™” ì„¤ì •
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        IS_M3_MAX = True
        torch.backends.mps.empty_cache()
    elif torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        IS_M3_MAX = False
    else:
        DEVICE_TYPE = "cpu"
        IS_M3_MAX = False
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE_TYPE = "cpu"
    IS_M3_MAX = False

try:
    import numpy as np
    from PIL import Image
    import cv2
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

try:
    from transformers import AutoConfig, AutoModel
    import accelerate
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ê°•í™”ëœ ëª¨ë¸ ë¶„ë¥˜ ë° ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ
# ==============================================

class ModelCategory(Enum):
    """ê°•í™”ëœ ëª¨ë¸ ì¹´í…Œê³ ë¦¬ - ì„¸ë¶„í™” ë° í™•ì¥"""
    # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ìœ ì§€
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    DIFFUSION_MODELS = "diffusion_models"
    TRANSFORMER_MODELS = "transformer_models"
    AUXILIARY = "auxiliary"
    
    # ìƒˆë¡œìš´ ì„¸ë¶„í™” ì¹´í…Œê³ ë¦¬
    STABLE_DIFFUSION = "stable_diffusion"
    OOTDIFFUSION = "ootdiffusion"
    CONTROLNET = "controlnet"
    SAM_MODELS = "sam_models"
    CLIP_MODELS = "clip_models"
    VAE_MODELS = "vae_models"
    LORA_MODELS = "lora_models"
    TEXTUAL_INVERSION = "textual_inversion"

class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ íƒ€ì…"""
    UNET = "unet"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    GAN = "gan"
    VAE = "vae"
    DIFFUSION = "diffusion"
    CLIP = "clip"
    RESNET = "resnet"
    EFFICIENT_NET = "efficient_net"
    MOBILENET = "mobilenet"
    CUSTOM = "custom"
    UNKNOWN = "unknown"

class ModelOptimization(Enum):
    """ëª¨ë¸ ìµœì í™” ìƒíƒœ"""
    NONE = "none"
    QUANTIZED = "quantized"
    PRUNED = "pruned"
    DISTILLED = "distilled"
    ONNX_OPTIMIZED = "onnx_optimized"
    M3_OPTIMIZED = "m3_optimized"
    TensorRT = "tensorrt"
    CoreML = "coreml"

class ModelPriority(Enum):
    """ê°•í™”ëœ ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•µì‹¬ í”„ë¡œë•ì…˜ ëª¨ë¸
    HIGH = 2          # ê³ ì„±ëŠ¥ ëª¨ë¸
    MEDIUM = 3        # ì¼ë°˜ ëª¨ë¸
    LOW = 4           # ë³´ì¡° ëª¨ë¸
    EXPERIMENTAL = 5  # ì‹¤í—˜ì  ëª¨ë¸
    DEPRECATED = 6    # íê¸° ì˜ˆì •

@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    inference_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    gpu_utilization: float = 0.0
    throughput_fps: float = 0.0
    accuracy_score: Optional[float] = None
    benchmark_score: Optional[float] = None
    energy_efficiency: Optional[float] = None
    m3_compatibility_score: float = 0.0

@dataclass
class ModelMetadata:
    """ê°•í™”ëœ ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    # ê¸°ë³¸ ì •ë³´
    name: str
    version: str = "unknown"
    author: str = "unknown"
    description: str = ""
    license: str = "unknown"
    
    # ê¸°ìˆ ì  ì •ë³´
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    framework: str = "pytorch"
    precision: str = "fp32"
    optimization: ModelOptimization = ModelOptimization.NONE
    
    # ì„±ëŠ¥ ì •ë³´
    performance: ModelPerformanceMetrics = field(default_factory=ModelPerformanceMetrics)
    
    # í˜¸í™˜ì„± ì •ë³´
    min_memory_mb: float = 0.0
    recommended_memory_mb: float = 0.0
    supported_devices: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    
    # ê²€ì¦ ì •ë³´
    validation_date: Optional[str] = None
    validation_status: str = "unknown"
    checksum: Optional[str] = None

@dataclass
class DetectedModel:
    """ê°•í™”ëœ íƒì§€ëœ ëª¨ë¸ ì •ë³´"""
    # ê¸°ì¡´ í•„ë“œ ìœ ì§€
    name: str
    path: Path
    category: ModelCategory
    model_type: str
    file_size_mb: float
    file_extension: str
    confidence_score: float
    priority: ModelPriority
    step_name: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    alternative_paths: List[Path] = field(default_factory=list)
    requirements: List[str] = field(default_factory=list)
    performance_info: Dict[str, Any] = field(default_factory=dict)
    compatibility_info: Dict[str, Any] = field(default_factory=dict)
    last_modified: float = 0.0
    checksum: Optional[str] = None
    pytorch_valid: bool = False
    parameter_count: int = 0
    
    # ìƒˆë¡œìš´ ê°•í™” í•„ë“œ
    model_metadata: Optional[ModelMetadata] = None
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    precision: str = "fp32"
    optimization_level: ModelOptimization = ModelOptimization.NONE
    model_structure: Dict[str, Any] = field(default_factory=dict)
    layer_info: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Optional[ModelPerformanceMetrics] = None
    memory_requirements: Dict[str, float] = field(default_factory=dict)
    device_compatibility: Dict[str, bool] = field(default_factory=dict)
    load_time_ms: float = 0.0
    validation_results: Dict[str, Any] = field(default_factory=dict)
    health_status: str = "unknown"
    usage_statistics: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ ê°•í™”ëœ ëª¨ë¸ íŒ¨í„´ ë° ê²€ì¦ ì‹œìŠ¤í…œ
# ==============================================

@dataclass
class EnhancedModelFileInfo:
    """ê°•í™”ëœ ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    # ê¸°ì¡´ í•„ë“œ ìœ ì§€
    name: str
    patterns: List[str]
    step: str
    required: bool
    min_size_mb: float
    max_size_mb: float
    target_path: str
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])
    keywords: List[str] = field(default_factory=list)
    expected_layers: List[str] = field(default_factory=list)
    
    # ìƒˆë¡œìš´ ê°•í™” í•„ë“œ
    architecture: ModelArchitecture = ModelArchitecture.UNKNOWN
    framework_requirements: List[str] = field(default_factory=list)
    performance_expectations: Dict[str, float] = field(default_factory=dict)
    memory_profile: Dict[str, float] = field(default_factory=dict)
    validation_rules: Dict[str, Any] = field(default_factory=dict)
    optimization_hints: List[str] = field(default_factory=list)
    compatibility_matrix: Dict[str, bool] = field(default_factory=dict)

# ì‹¤ì œ ë°œê²¬ëœ 89.8GB íŒŒì¼ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°•í™”ëœ íŒ¨í„´
ENHANCED_MODEL_PATTERNS = {
    "human_parsing_graphonomy": EnhancedModelFileInfo(
        name="human_parsing_graphonomy",
        patterns=[
            r".*checkpoints/human_parsing/.*\.pth$",
            r".*schp.*atr.*\.pth$", 
            r".*atr_model.*\.pth$",
            r".*lip_model.*\.pth$",
            r".*graphonomy.*\.pth$"
        ],
        step="HumanParsingStep",
        required=True,
        min_size_mb=50,
        max_size_mb=500,
        target_path="ai_models/checkpoints/human_parsing/schp_atr.pth",
        priority=1,
        alternative_names=["schp_atr.pth", "atr_model.pth", "lip_model.pth", "graphonomy_lip.pth"],
        keywords=["human", "parsing", "atr", "schp", "lip", "graphonomy", "segmentation"],
        expected_layers=["backbone", "classifier", "conv", "bn"],
        architecture=ModelArchitecture.CNN,
        framework_requirements=["torch", "torchvision"],
        performance_expectations={
            "inference_time_ms": 150.0,
            "memory_usage_mb": 800.0,
            "accuracy": 0.85
        },
        memory_profile={
            "min_memory_mb": 500.0,
            "recommended_memory_mb": 1200.0,
            "peak_memory_mb": 1500.0
        },
        validation_rules={
            "required_keys": ["state_dict", "epoch"],
            "architecture_check": True,
            "layer_count_range": (50, 200)
        }
    ),
    
    "cloth_segmentation_u2net": EnhancedModelFileInfo(
        name="cloth_segmentation_u2net", 
        patterns=[
            r".*checkpoints/step_03.*u2net.*\.pth$",
            r".*u2net_segmentation.*\.pth$",
            r".*sam.*vit.*\.pth$",
            r".*cloth.*segmentation.*\.pth$"
        ],
        step="ClothSegmentationStep",
        required=True, 
        min_size_mb=10,
        max_size_mb=3000,
        target_path="ai_models/checkpoints/step_03/u2net_segmentation/u2net.pth",
        priority=1,
        alternative_names=["u2net.pth", "sam_vit_h_4b8939.pth", "sam_vit_b_01ec64.pth"],
        keywords=["u2net", "segmentation", "sam", "cloth", "mask"],
        expected_layers=["encoder", "decoder", "outconv", "attention"],
        architecture=ModelArchitecture.UNET,
        framework_requirements=["torch", "torchvision", "PIL"],
        performance_expectations={
            "inference_time_ms": 200.0,
            "memory_usage_mb": 1200.0,
            "accuracy": 0.90
        },
        memory_profile={
            "min_memory_mb": 800.0,
            "recommended_memory_mb": 1800.0,
            "peak_memory_mb": 2500.0
        }
    ),
    
    "virtual_fitting_ootd": EnhancedModelFileInfo(
        name="virtual_fitting_ootd", 
        patterns=[
            r".*step_06_virtual_fitting.*\.bin$",
            r".*ootd.*unet.*\.bin$",
            r".*OOTDiffusion.*",
            r".*diffusion_pytorch_model\.bin$",
            r".*virtual.*fitting.*\.pth$"
        ],
        step="VirtualFittingStep",
        required=True,
        min_size_mb=100, 
        max_size_mb=8000,
        target_path="ai_models/checkpoints/ootdiffusion/ootd_hd_unet.bin",
        priority=1,
        alternative_names=["ootd_hd_unet.bin", "ootd_dc_unet.bin", "diffusion_pytorch_model.bin"],
        keywords=["ootd", "unet", "diffusion", "virtual", "fitting", "stable"],
        expected_layers=["unet", "vae", "text_encoder", "scheduler"],
        file_types=['.bin', '.pth', '.pt', '.safetensors'],
        architecture=ModelArchitecture.DIFFUSION,
        framework_requirements=["torch", "diffusers", "transformers"],
        performance_expectations={
            "inference_time_ms": 2000.0,
            "memory_usage_mb": 4000.0,
            "quality_score": 0.88
        },
        memory_profile={
            "min_memory_mb": 3000.0,
            "recommended_memory_mb": 6000.0,
            "peak_memory_mb": 8000.0
        },
        optimization_hints=["fp16", "attention_slicing", "memory_efficient_attention"]
    ),
    
    "pose_estimation_openpose": EnhancedModelFileInfo(
        name="pose_estimation_openpose",
        patterns=[
            r".*openpose.*\.pth$",
            r".*body_pose_model\.pth$",
            r".*pose.*estimation.*\.pth$",
            r".*yolo.*pose.*\.pt$"
        ],
        step="PoseEstimationStep",
        required=True,
        min_size_mb=5,
        max_size_mb=1000,
        target_path="ai_models/checkpoints/pose_estimation/openpose.pth",
        priority=2,
        alternative_names=["body_pose_model.pth", "openpose.pth", "yolov8n-pose.pt"],
        keywords=["pose", "openpose", "body", "keypoint", "coco", "estimation"],
        expected_layers=["stage", "paf", "heatmap", "backbone"],
        architecture=ModelArchitecture.CNN,
        performance_expectations={
            "inference_time_ms": 80.0,
            "memory_usage_mb": 600.0,
            "keypoint_accuracy": 0.82
        }
    )
}

# ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ íŒ¨í„´
ENHANCED_CHECKPOINT_VERIFICATION_PATTERNS = {
    "human_parsing": {
        "keywords": ["human", "parsing", "atr", "schp", "graphonomy", "segmentation", "lip"],
        "expected_size_range": (50, 500),  # MB
        "required_layers": ["backbone", "classifier", "conv", "bn", "relu"],
        "typical_parameters": (25000000, 70000000),  # 25M ~ 70M íŒŒë¼ë¯¸í„°
        "architecture_checks": {
            "backbone_types": ["resnet", "hrnet", "mobilenet"],
            "output_channels": [20, 19, 18],  # LIP, ATR, CIHP classes
            "input_resolution": [(512, 512), (473, 473)]
        },
        "performance_baselines": {
            "mIoU": 0.58,  # Mean IoU baseline
            "pixel_accuracy": 0.85,
            "inference_time_ms": 150
        }
    },
    
    "cloth_segmentation": {
        "keywords": ["u2net", "cloth", "segmentation", "mask", "sam", "rembg"],
        "expected_size_range": (10, 3000),
        "required_layers": ["encoder", "decoder", "outconv", "side_output"],
        "typical_parameters": (4000000, 650000000),  # 4M ~ 650M íŒŒë¼ë¯¸í„° (SAM í¬í•¨)
        "architecture_checks": {
            "encoder_types": ["vgg", "resnet", "transformer"],
            "decoder_stages": [6, 5, 4],
            "output_channels": [1, 3, 4]  # Binary mask, RGB, RGBA
        }
    },
    
    "virtual_fitting": {
        "keywords": ["diffusion", "viton", "unet", "stable", "fitting", "ootd"],
        "expected_size_range": (100, 8000),
        "required_layers": ["unet", "vae", "text_encoder", "scheduler"],
        "typical_parameters": (100000000, 2000000000),  # 100M ~ 2B íŒŒë¼ë¯¸í„°
        "architecture_checks": {
            "unet_channels": [320, 640, 1280],
            "attention_layers": ["self_attn", "cross_attn"],
            "time_embedding_dim": [320, 512, 1024]
        },
        "performance_baselines": {
            "fid_score": 25.0,
            "lpips_score": 0.15,
            "inference_time_ms": 2000
        }
    },
    
    "pose_estimation": {
        "keywords": ["pose", "openpose", "body", "keypoint", "coco", "mediapipe"],
        "expected_size_range": (5, 1000),
        "required_layers": ["stage", "paf", "heatmap", "backbone"],
        "typical_parameters": (10000000, 200000000),  # 10M ~ 200M íŒŒë¼ë¯¸í„°
        "architecture_checks": {
            "num_keypoints": [17, 18, 21],  # COCO, OpenPose, MediaPipe
            "num_stages": [2, 3, 4],
            "feature_map_sizes": [(46, 46), (23, 23), (12, 12)]
        }
    }
}

# ==============================================
# ğŸ”¥ ì‹¤ì œ ë™ì‘í•˜ëŠ” ê°•í™”ëœ ëª¨ë¸ íƒì§€ê¸° í´ë˜ìŠ¤
# ==============================================

class RealWorldModelDetector:
    """
    ğŸ” ì‹¤ì œ ë™ì‘í•˜ëŠ” AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ v7.0 - 89.8GB ì‹¤ì œ í™œìš© ê°•í™” ë²„ì „
    
    âœ… 2ë²ˆ,3ë²ˆ íŒŒì¼ì˜ ì‹¤ì œ íƒì§€ ë¡œì§ 100% ë°˜ì˜ ë° ëŒ€í­ ê°•í™”
    âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ì‹¤ì œ ê²€ì¦ + ëª¨ë¸ êµ¬ì¡° ì™„ì „ ë¶„ì„
    âœ… ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ì¶œë ¥ (ìˆœí™˜ì°¸ì¡° ë°©ì§€) + ì„±ëŠ¥ ìµœì í™”
    âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ì™„ì „ í™œìš© + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    âœ… M3 Max 128GB ìµœì í™” + Neural Engine í™œìš©
    âœ… ì‹¤ë¬´ê¸‰ ì„±ëŠ¥ + í”„ë¡œë•ì…˜ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        search_paths: Optional[List[Path]] = None,
        enable_deep_scan: bool = True,
        enable_pytorch_validation: bool = True,
        enable_performance_profiling: bool = True,
        enable_memory_monitoring: bool = True,
        enable_caching: bool = True,
        cache_db_path: Optional[Path] = None,
        max_workers: int = 4,
        scan_timeout: int = 600,  # 10ë¶„ìœ¼ë¡œ ì¦ê°€
        validation_timeout: int = 120,  # ê°œë³„ ê²€ì¦ íƒ€ì„ì•„ì›ƒ
        **kwargs
    ):
        """ê°•í™”ëœ ì‹¤ì œ ë™ì‘í•˜ëŠ” ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™”"""
        
        self.logger = logging.getLogger(f"{__name__}.RealWorldModelDetector")
        
        # ê¸°ë³¸ ì„¤ì •
        self.enable_deep_scan = enable_deep_scan
        self.enable_pytorch_validation = enable_pytorch_validation
        self.enable_performance_profiling = enable_performance_profiling
        self.enable_memory_monitoring = enable_memory_monitoring
        self.enable_caching = enable_caching
        self.max_workers = max_workers
        self.scan_timeout = scan_timeout
        self.validation_timeout = validation_timeout
        
        # ê³ ê¸‰ ì„¤ì •
        self.enable_architecture_analysis = kwargs.get('enable_architecture_analysis', True)
        self.enable_compatibility_check = kwargs.get('enable_compatibility_check', True)
        self.enable_optimization_suggestions = kwargs.get('enable_optimization_suggestions', True)
        self.enable_health_monitoring = kwargs.get('enable_health_monitoring', True)
        
        # ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • (ê°•í™”ëœ ë²„ì „)
        if search_paths is None:
            self.search_paths = self._get_enhanced_search_paths()
        else:
            self.search_paths = search_paths
        
        # íƒì§€ ê²°ê³¼ ì €ì¥ (ê°•í™”ëœ êµ¬ì¡°)
        self.detected_models: Dict[str, DetectedModel] = {}
        self.model_registry: Dict[str, Dict[str, Any]] = {}
        self.performance_cache: Dict[str, ModelPerformanceMetrics] = {}
        self.compatibility_matrix: Dict[str, Dict[str, bool]] = {}
        
        # ê°•í™”ëœ ìŠ¤ìº” í†µê³„
        self.scan_stats = {
            "total_files_scanned": 0,
            "pytorch_files_found": 0,
            "valid_pytorch_models": 0,
            "models_detected": 0,
            "scan_duration": 0.0,
            "last_scan_time": 0,
            "errors_encountered": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "pytorch_validation_errors": 0,
            "performance_tests_run": 0,
            "memory_tests_run": 0,
            "architecture_analyses": 0,
            "optimization_suggestions": 0,
            "health_checks": 0,
            "compatibility_tests": 0,
            "m3_optimized_models": 0,
            "total_model_size_gb": 0.0,
            "average_confidence": 0.0,
            "validation_success_rate": 0.0
        }
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_monitor = MemoryMonitor() if enable_memory_monitoring else None
        self.device_info = self._analyze_device_capabilities()
        
        # ìºì‹œ ê´€ë¦¬ (ê°•í™”ëœ ë²„ì „)
        self.cache_db_path = cache_db_path or Path("enhanced_model_detection_cache.db")
        self.cache_ttl = 86400 * 7  # 7ì¼ë¡œ ì—°ì¥
        self._cache_lock = threading.RLock()
        
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬
        self.performance_profiler = ModelPerformanceProfiler() if enable_performance_profiling else None
        
        self.logger.info(f"ğŸ” ê°•í™”ëœ ì‹¤ì œ ë™ì‘ ëª¨ë¸ íƒì§€ê¸° v7.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   - ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        self.logger.info(f"   - ë””ë°”ì´ìŠ¤: {DEVICE_TYPE} ({'M3 Max' if IS_M3_MAX else 'Standard'})")
        self.logger.info(f"   - ê³ ê¸‰ ê¸°ëŠ¥: ì„±ëŠ¥ë¶„ì„({enable_performance_profiling}), ë©”ëª¨ë¦¬ëª¨ë‹ˆí„°ë§({enable_memory_monitoring})")
        
        # ìºì‹œ DB ì´ˆê¸°í™”
        if self.enable_caching:
            self._init_enhanced_cache_db()
    
    def _get_enhanced_search_paths(self) -> List[Path]:
        """ê°•í™”ëœ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì •"""
        try:
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parents[3]  # app/ai_pipeline/utilsì—ì„œ backendë¡œ
            
            # ê¸°ë³¸ ê²½ë¡œë“¤
            base_paths = [
                # í”„ë¡œì íŠ¸ ë‚´ë¶€ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                backend_dir / "ai_models",
                backend_dir / "ai_models" / "checkpoints",
                backend_dir / "app" / "ai_pipeline" / "models",
                backend_dir / "app" / "models",
                backend_dir / "checkpoints",
                backend_dir / "models",
                backend_dir / "weights",
                
                # ìƒìœ„ ë””ë ‰í† ë¦¬
                backend_dir.parent / "ai_models",
                backend_dir.parent / "models",
                
                # HuggingFace ìºì‹œ (ë†’ì€ ìš°ì„ ìˆœìœ„)
                Path.home() / ".cache" / "huggingface" / "hub",
                Path.home() / ".cache" / "huggingface" / "transformers",
                
                # PyTorch ìºì‹œ
                Path.home() / ".cache" / "torch" / "hub",
                Path.home() / ".cache" / "torch" / "checkpoints",
                
                # ì¼ë°˜ì ì¸ ML ëª¨ë¸ ê²½ë¡œë“¤
                Path.home() / ".cache" / "models",
                Path.home() / "Downloads",
                Path.home() / "Documents" / "AI_Models",
                Path.home() / "Desktop" / "models",
                
                # conda/pip í™˜ê²½ ê²½ë¡œë“¤
                *self._get_enhanced_conda_paths(),
                
                # ì‹œìŠ¤í…œ ë ˆë²¨ ê²½ë¡œë“¤ (ê¶Œí•œì´ ìˆëŠ” ê²½ìš°ë§Œ)
                Path("/opt/models"),
                Path("/usr/local/models"),
                Path("/var/lib/models")
            ]
            
            # ì¶”ê°€ í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ê²½ë¡œë“¤
            env_paths = [
                os.environ.get('MODEL_CACHE_DIR'),
                os.environ.get('TORCH_HOME'),
                os.environ.get('TRANSFORMERS_CACHE'),
                os.environ.get('HF_HOME'),
                os.environ.get('XDG_CACHE_HOME')
            ]
            
            for env_path in env_paths:
                if env_path and Path(env_path).exists():
                    base_paths.append(Path(env_path))
            
            # ì‹¤ì œ ì¡´ì¬í•˜ê³  ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œë§Œ í•„í„°ë§
            valid_paths = []
            for path in base_paths:
                try:
                    if path and path.exists() and path.is_dir() and os.access(path, os.R_OK):
                        valid_paths.append(path)
                        self.logger.debug(f"âœ… ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {path}")
                    else:
                        self.logger.debug(f"âŒ ë¬´íš¨í•œ ê²½ë¡œ: {path}")
                except Exception as e:
                    self.logger.debug(f"âŒ ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨ {path}: {e}")
            
            # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
            unique_paths = []
            seen_paths = set()
            for path in valid_paths:
                resolved_path = path.resolve()
                if resolved_path not in seen_paths:
                    unique_paths.append(resolved_path)
                    seen_paths.add(resolved_path)
            
            self.logger.info(f"âœ… ì´ {len(unique_paths)}ê°œ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ì™„ë£Œ")
            return unique_paths
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜
            return [Path.cwd() / "ai_models"]
    
    def _get_enhanced_conda_paths(self) -> List[Path]:
        """ê°•í™”ëœ conda í™˜ê²½ ê²½ë¡œë“¤ íƒì§€"""
        conda_paths = []
        
        try:
            # í˜„ì¬ conda í™˜ê²½
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                conda_base = Path(conda_prefix)
                if conda_base.exists():
                    conda_paths.extend([
                        conda_base / "lib" / "python3.11" / "site-packages",
                        conda_base / "lib" / "python3.10" / "site-packages",
                        conda_base / "lib" / "python3.9" / "site-packages",
                        conda_base / "share" / "models",
                        conda_base / "models",
                        conda_base / "checkpoints"
                    ])
            
            # conda ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë“¤
            conda_roots = [
                os.environ.get('CONDA_ROOT'),
                os.environ.get('CONDA_ENVS_PATH'),
                Path.home() / "miniforge3",
                Path.home() / "miniconda3",
                Path.home() / "anaconda3",
                Path.home() / "mambaforge",
                Path("/opt/conda"),
                Path("/usr/local/conda"),
                Path("/opt/homebrew/Caskroom/miniforge/base")  # M1/M2 Mac
            ]
            
            for root in conda_roots:
                if root and Path(root).exists():
                    conda_paths.extend([
                        Path(root) / "pkgs",
                        Path(root) / "envs",
                        Path(root) / "lib",
                        Path(root) / "models"
                    ])
            
            # í™œì„± í™˜ê²½ë“¤ ìŠ¤ìº”
            try:
                envs_dirs = [
                    Path.home() / "miniforge3" / "envs",
                    Path.home() / "miniconda3" / "envs",
                    Path.home() / "anaconda3" / "envs"
                ]
                
                for envs_dir in envs_dirs:
                    if envs_dir.exists():
                        for env_path in envs_dir.iterdir():
                            if env_path.is_dir():
                                conda_paths.extend([
                                    env_path / "lib" / "python3.11" / "site-packages",
                                    env_path / "lib" / "python3.10" / "site-packages",
                                    env_path / "models"
                                ])
            except Exception as e:
                self.logger.debug(f"í™˜ê²½ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            self.logger.debug(f"conda ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return [path for path in conda_paths if path.exists()]
    
    def _analyze_device_capabilities(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        try:
            device_info = {
                "type": DEVICE_TYPE,
                "is_m3_max": IS_M3_MAX,
                "torch_available": TORCH_AVAILABLE,
                "memory_total_gb": 0.0,
                "memory_available_gb": 0.0,
                "cpu_count": os.cpu_count(),
                "optimization_hints": []
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            if psutil:
                memory = psutil.virtual_memory()
                device_info["memory_total_gb"] = memory.total / (1024**3)
                device_info["memory_available_gb"] = memory.available / (1024**3)
            
            # M3 Max íŠ¹í™” ìµœì í™”
            if IS_M3_MAX and TORCH_AVAILABLE:
                device_info["optimization_hints"] = [
                    "use_mps_device",
                    "enable_memory_efficient_attention",
                    "use_fp16_precision",
                    "enable_compilation"
                ]
                
                # Neural Engine ì‚¬ìš© ê°€ëŠ¥ì„± ì²´í¬
                try:
                    test_tensor = torch.randn(1, 3, 224, 224, device="mps")
                    device_info["neural_engine_available"] = True
                    del test_tensor
                    torch.mps.empty_cache()
                except:
                    device_info["neural_engine_available"] = False
            
            return device_info
            
        except Exception as e:
            self.logger.debug(f"ë””ë°”ì´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"type": "cpu", "is_m3_max": False}
    
    def _init_enhanced_cache_db(self):
        """ê°•í™”ëœ ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                # ê¸°ë³¸ ìºì‹œ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS enhanced_model_cache (
                        file_path TEXT PRIMARY KEY,
                        file_size INTEGER,
                        file_mtime REAL,
                        checksum TEXT,
                        pytorch_valid INTEGER,
                        parameter_count INTEGER,
                        architecture TEXT,
                        precision TEXT,
                        optimization_level TEXT,
                        detection_data TEXT,
                        performance_data TEXT,
                        compatibility_data TEXT,
                        health_status TEXT,
                        created_at REAL,
                        accessed_at REAL,
                        validation_version TEXT
                    )
                """)
                
                # ì„±ëŠ¥ ìºì‹œ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS performance_cache (
                        model_path TEXT PRIMARY KEY,
                        device_type TEXT,
                        inference_time_ms REAL,
                        memory_usage_mb REAL,
                        throughput_fps REAL,
                        accuracy_score REAL,
                        benchmark_score REAL,
                        test_date REAL
                    )
                """)
                
                # í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS compatibility_matrix (
                        model_path TEXT,
                        device_type TEXT,
                        framework_version TEXT,
                        compatible INTEGER,
                        performance_score REAL,
                        last_tested REAL,
                        PRIMARY KEY (model_path, device_type, framework_version)
                    )
                """)
                
                # ì¸ë±ìŠ¤ ìƒì„±
                conn.execute("CREATE INDEX IF NOT EXISTS idx_enhanced_accessed_at ON enhanced_model_cache(accessed_at)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_performance_device ON performance_cache(device_type)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_compatibility_model ON compatibility_matrix(model_path)")
                
                conn.commit()
                
            self.logger.debug("âœ… ê°•í™”ëœ ëª¨ë¸ ìºì‹œ DB ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê°•í™”ëœ ëª¨ë¸ ìºì‹œ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enable_caching = False

    def detect_all_models(
        self, 
        force_rescan: bool = False,
        categories_filter: Optional[List[ModelCategory]] = None,
        min_confidence: float = 0.3,
        model_type_filter: Optional[List[str]] = None,
        enable_detailed_analysis: bool = True,
        max_models_per_category: Optional[int] = None
    ) -> Dict[str, DetectedModel]:
        """
        ê°•í™”ëœ ì‹¤ì œ AI ëª¨ë¸ ìë™ íƒì§€
        
        Args:
            force_rescan: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìŠ¤ìº”
            categories_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ íƒì§€
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            model_type_filter: íŠ¹ì • ëª¨ë¸ íƒ€ì…ë§Œ íƒì§€
            enable_detailed_analysis: ìƒì„¸ ë¶„ì„ í™œì„±í™”
            max_models_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ëª¨ë¸ ìˆ˜
            
        Returns:
            Dict[str, DetectedModel]: íƒì§€ëœ ëª¨ë¸ë“¤ (ê°•í™”ëœ ì •ë³´ í¬í•¨)
        """
        try:
            self.logger.info("ğŸ” ê°•í™”ëœ ì‹¤ì œ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œì‘...")
            start_time = time.time()
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            if self.memory_monitor:
                self.memory_monitor.start_monitoring()
            
            # ìºì‹œ í™•ì¸
            if not force_rescan and self.enable_caching:
                cached_results = self._load_from_enhanced_cache()
                if cached_results:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {len(cached_results)}ê°œ ëª¨ë¸")
                    self.scan_stats["cache_hits"] += len(cached_results)
                    return cached_results
            
            # ì‹¤ì œ ìŠ¤ìº” ì‹¤í–‰
            self._reset_enhanced_scan_stats()
            
            # ëª¨ë¸ íƒ€ì… í•„í„°ë§
            if model_type_filter:
                filtered_patterns = {k: v for k, v in ENHANCED_MODEL_PATTERNS.items() 
                                   if k in model_type_filter}
            else:
                filtered_patterns = ENHANCED_MODEL_PATTERNS
            
            # ë³‘ë ¬/ìˆœì°¨ ìŠ¤ìº” ì„ íƒ
            if self.max_workers > 1:
                self._enhanced_parallel_scan(filtered_patterns, categories_filter, min_confidence, enable_detailed_analysis)
            else:
                self._enhanced_sequential_scan(filtered_patterns, categories_filter, min_confidence, enable_detailed_analysis)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ìˆ˜ ì œí•œ
            if max_models_per_category:
                self._limit_models_per_category(max_models_per_category)
            
            # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
            self._update_enhanced_scan_stats(start_time)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬ (ê°•í™”ëœ ë²„ì „)
            self._enhanced_post_process_results(min_confidence, enable_detailed_analysis)
            
            # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
            if self.performance_profiler and enable_detailed_analysis:
                self._run_performance_profiling()
            
            # í˜¸í™˜ì„± ë¶„ì„
            if enable_detailed_analysis:
                self._analyze_model_compatibility()
            
            # ìµœì í™” ì œì•ˆ ìƒì„±
            if self.enable_optimization_suggestions:
                self._generate_optimization_suggestions()
            
            # ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_enhanced_cache()
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            if self.memory_monitor:
                memory_stats = self.memory_monitor.stop_monitoring()
                self.scan_stats.update(memory_stats)
            
            self.logger.info(f"âœ… ê°•í™”ëœ ëª¨ë¸ íƒì§€ ì™„ë£Œ: {len(self.detected_models)}ê°œ ëª¨ë¸ ë°œê²¬ ({self.scan_stats['scan_duration']:.2f}ì´ˆ)")
            self._print_enhanced_detection_summary()
            
            return self.detected_models
            
        except Exception as e:
            self.logger.error(f"âŒ ê°•í™”ëœ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            self.scan_stats["errors_encountered"] += 1
            raise

    def _enhanced_parallel_scan(self, model_patterns: Dict, categories_filter, min_confidence, enable_detailed_analysis):
        """ê°•í™”ëœ ë³‘ë ¬ ìŠ¤ìº”"""
        try:
            # ìŠ¤ìº” íƒœìŠ¤í¬ ìƒì„± (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
            scan_tasks = []
            for model_type, pattern_info in model_patterns.items():
                for search_path in self.search_paths:
                    if search_path.exists():
                        task_priority = pattern_info.priority
                        scan_tasks.append((task_priority, model_type, pattern_info, search_path))
            
            # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
            scan_tasks.sort(key=lambda x: x[0])
            
            if not scan_tasks:
                self.logger.warning("âš ï¸ ìŠ¤ìº”í•  ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ë™ì  ì›Œì»¤ ìˆ˜ ì¡°ì • (ë©”ëª¨ë¦¬ ê¸°ë°˜)
            available_memory_gb = self.device_info.get('memory_available_gb', 8.0)
            optimal_workers = min(self.max_workers, max(1, int(available_memory_gb / 4)))
            
            self.logger.info(f"ğŸ”„ {optimal_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ìŠ¤ìº” ì‹œì‘ ({len(scan_tasks)}ê°œ íƒœìŠ¤í¬)")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                future_to_task = {
                    executor.submit(
                        self._scan_path_for_enhanced_models, 
                        model_type, 
                        pattern_info, 
                        search_path, 
                        categories_filter, 
                        min_confidence,
                        enable_detailed_analysis
                    ): (model_type, search_path, priority)
                    for priority, model_type, pattern_info, search_path in scan_tasks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ í¬í•¨)
                completed_count = 0
                for future in as_completed(future_to_task, timeout=self.scan_timeout):
                    model_type, search_path, priority = future_to_task[future]
                    try:
                        path_results = future.result(timeout=self.validation_timeout)
                        if path_results:
                            # ê²°ê³¼ ë³‘í•© (ìŠ¤ë ˆë“œ ì•ˆì „)
                            with threading.Lock():
                                for name, model in path_results.items():
                                    self._register_enhanced_model_safe(model)
                        
                        completed_count += 1
                        progress = (completed_count / len(scan_tasks)) * 100
                        self.logger.debug(f"âœ… {model_type} @ {search_path} ìŠ¤ìº” ì™„ë£Œ ({progress:.1f}%)")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_type} @ {search_path} ìŠ¤ìº” ì‹¤íŒ¨: {e}")
                        self.scan_stats["errors_encountered"] += 1
                        
        except Exception as e:
            self.logger.error(f"âŒ ê°•í™”ëœ ë³‘ë ¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            # í´ë°±: ìˆœì°¨ ìŠ¤ìº”
            self._enhanced_sequential_scan(model_patterns, categories_filter, min_confidence, enable_detailed_analysis)

    def _scan_path_for_enhanced_models(
        self, 
        model_type: str, 
        pattern_info: EnhancedModelFileInfo, 
        search_path: Path, 
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float,
        enable_detailed_analysis: bool,
        max_depth: int = 8,  # ë” ê¹Šì€ ìŠ¤ìº”
        current_depth: int = 0
    ) -> Dict[str, DetectedModel]:
        """ê°•í™”ëœ ëª¨ë¸ íŒŒì¼ë“¤ ìŠ¤ìº”"""
        results = {}
        
        try:
            if current_depth > max_depth:
                return results
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ë‚˜ì—´ (ê¶Œí•œ ì²´í¬ í¬í•¨)
            try:
                items = list(search_path.iterdir())
            except (PermissionError, OSError) as e:
                self.logger.debug(f"ê¶Œí•œ ì—†ìŒ ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€: {search_path} - {e}")
                return results
            
            # íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ë¶„ë¦¬ (ë” ì •êµí•œ í•„í„°ë§)
            files = []
            subdirs = []
            
            for item in items:
                try:
                    if item.is_file():
                        files.append(item)
                    elif item.is_dir() and not item.name.startswith('.'):
                        # ì œì™¸í•  ë””ë ‰í† ë¦¬ íŒ¨í„´ í™•ì¥
                        excluded_dirs = {
                            '__pycache__', '.git', 'node_modules', '.vscode', '.idea', 
                            '.pytest_cache', '.mypy_cache', '.DS_Store', 'Thumbs.db',
                            '.svn', '.hg', 'build', 'dist', 'env', 'venv', '.env'
                        }
                        if item.name not in excluded_dirs:
                            subdirs.append(item)
                except Exception as e:
                    self.logger.debug(f"í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨ {item}: {e}")
                    continue
            
            # íŒŒì¼ë“¤ ë¶„ì„ (ê°•í™”ëœ ëª¨ë¸ íŒŒì¼ í™•ì¸)
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    # ê¸°ë³¸ AI ëª¨ë¸ íŒŒì¼ í•„í„°ë§ (ê°•í™”ëœ ë²„ì „)
                    if not self._is_enhanced_ai_model_file(file_path):
                        continue
                    
                    self.scan_stats["pytorch_files_found"] += 1
                    
                    # íŒ¨í„´ ë§¤ì¹­ (ê°•í™”ëœ ë²„ì „)
                    if self._matches_enhanced_model_patterns(file_path, pattern_info):
                        detected_model = self._analyze_enhanced_model_file(
                            file_path, model_type, pattern_info, categories_filter, 
                            min_confidence, enable_detailed_analysis
                        )
                        if detected_model:
                            results[detected_model.name] = detected_model
                            self.logger.debug(f"ğŸ“¦ {model_type} ëª¨ë¸ ë°œê²¬: {file_path.name} ({detected_model.file_size_mb:.1f}MB)")
                        
                except Exception as e:
                    self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº” (ê¹Šì´ ì œí•œ)
            if self.enable_deep_scan and current_depth < max_depth:
                for subdir in subdirs:
                    try:
                        subdir_results = self._scan_path_for_enhanced_models(
                            model_type, pattern_info, subdir, categories_filter, 
                            min_confidence, enable_detailed_analysis, max_depth, current_depth + 1
                        )
                        results.update(subdir_results)
                    except Exception as e:
                        self.logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                        continue
            
            return results
            
        except Exception as e:
            self.logger.debug(f"ê²½ë¡œ ìŠ¤ìº” ì˜¤ë¥˜ {search_path}: {e}")
            return results

    def _is_enhanced_ai_model_file(self, file_path: Path) -> bool:
        """ê°•í™”ëœ AI ëª¨ë¸ íŒŒì¼ ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            # í™•ì¥ì ì²´í¬ (í™•ì¥ëœ ëª©ë¡)
            ai_extensions = {
                '.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', '.pickle',
                '.h5', '.hdf5', '.pb', '.tflite', '.engine', '.plan', '.mlmodel',
                '.torchscript', '.jit', '.traced', '.ckpt', '.model', '.weights'
            }
            
            file_extension = file_path.suffix.lower()
            if file_extension not in ai_extensions:
                return False
            
            # íŒŒì¼ í¬ê¸° ì²´í¬ (ë” ì •êµí•œ ë²”ìœ„)
            try:
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                
                # í™•ì¥ìë³„ ìµœì†Œ í¬ê¸° ì„¤ì •
                min_sizes = {
                    '.pth': 0.5, '.pt': 0.5, '.bin': 1.0, '.safetensors': 1.0,
                    '.onnx': 0.1, '.pkl': 0.1, '.h5': 0.5, '.pb': 0.1,
                    '.ckpt': 1.0, '.model': 0.5, '.weights': 0.1
                }
                
                min_size = min_sizes.get(file_extension, 0.5)
                if file_size_mb < min_size:
                    return False
                    
                # ë„ˆë¬´ í° íŒŒì¼ë„ ì˜ì‹¬ìŠ¤ëŸ½ì§€ë§Œ ì¼ë‹¨ í—ˆìš© (10GB ì œí•œ)
                if file_size_mb > 10240:  # 10GB
                    self.logger.debug(f"âš ï¸ ë§¤ìš° í° íŒŒì¼ ë°œê²¬: {file_path} ({file_size_mb:.1f}MB)")
                    
            except Exception as e:
                self.logger.debug(f"íŒŒì¼ í¬ê¸° í™•ì¸ ì‹¤íŒ¨ {file_path}: {e}")
                return False
            
            # íŒŒì¼ëª… íŒ¨í„´ ì²´í¬ (í™•ì¥ëœ í‚¤ì›Œë“œ)
            file_name = file_path.name.lower()
            ai_keywords = [
                # ì¼ë°˜ì ì¸ ML í‚¤ì›Œë“œ
                'model', 'checkpoint', 'weight', 'state_dict', 'pytorch_model',
                'best_model', 'final_model', 'trained_model', 'finetuned',
                
                # Diffusion ê´€ë ¨
                'diffusion', 'stable', 'unet', 'vae', 'text_encoder', 'scheduler',
                'ootd', 'controlnet', 'lora', 'dreambooth', 'textual_inversion',
                
                # Transformer ê´€ë ¨
                'transformer', 'bert', 'gpt', 'clip', 'vit', 't5', 'bart',
                'roberta', 'albert', 'distilbert', 'electra',
                
                # Computer Vision ê´€ë ¨
                'resnet', 'efficientnet', 'mobilenet', 'yolo', 'rcnn', 'ssd',
                'segmentation', 'detection', 'classification', 'recognition',
                
                # íŠ¹í™” ëª¨ë¸ë“¤
                'pose', 'parsing', 'openpose', 'hrnet', 'u2net', 'sam',
                'viton', 'hrviton', 'graphonomy', 'schp', 'atr', 'gmm', 'tom',
                
                # ì¼ë°˜ì ì¸ ì•„í‚¤í…ì²˜
                'encoder', 'decoder', 'attention', 'embedding', 'backbone',
                'head', 'neck', 'fpn', 'feature', 'pretrained'
            ]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨)
            has_ai_keyword = any(keyword in file_name for keyword in ai_keywords)
            
            # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ì¶”ê°€ í™•ì¸
            path_str = str(file_path).lower()
            path_keywords = [
                'models', 'checkpoints', 'weights', 'pretrained', 'huggingface',
                'transformers', 'diffusers', 'pytorch', 'torchvision', 'timm',
                'stable-diffusion', 'clip', 'openai', 'anthropic', 'google'
            ]
            
            has_path_keyword = any(keyword in path_str for keyword in path_keywords)
            
            # ìµœì¢… íŒë‹¨ (í‚¤ì›Œë“œ ë˜ëŠ” ê²½ë¡œ ê¸°ë°˜)
            return has_ai_keyword or has_path_keyword
            
        except Exception as e:
            self.logger.debug(f"AI ëª¨ë¸ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False

    def _analyze_enhanced_model_file(
        self, 
        file_path: Path, 
        model_type: str,
        pattern_info: EnhancedModelFileInfo,
        categories_filter: Optional[List[ModelCategory]], 
        min_confidence: float,
        enable_detailed_analysis: bool
    ) -> Optional[DetectedModel]:
        """ê°•í™”ëœ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
        try:
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            file_extension = file_path.suffix.lower()
            last_modified = file_stat.st_mtime
            
            # í¬ê¸° ì œí•œ í™•ì¸ (ë” ìœ ì—°í•œ ë²”ìœ„)
            size_tolerance = 0.2  # 20% í—ˆìš© ì˜¤ì°¨
            min_size_with_tolerance = pattern_info.min_size_mb * (1 - size_tolerance)
            max_size_with_tolerance = pattern_info.max_size_mb * (1 + size_tolerance)
            
            if not (min_size_with_tolerance <= file_size_mb <= max_size_with_tolerance):
                self.logger.debug(f"í¬ê¸° ë²”ìœ„ ë²—ì–´ë‚¨: {file_path.name} ({file_size_mb:.1f}MB)")
                return None
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if file_extension not in pattern_info.file_types:
                return None
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°•í™”ëœ ë²„ì „)
            confidence_score = self._calculate_enhanced_confidence(file_path, model_type, pattern_info, file_size_mb)
            
            if confidence_score < min_confidence:
                return None
            
            # PyTorch ëª¨ë¸ ì‹¤ì œ ê²€ì¦ (ê°•í™”ëœ ë²„ì „)
            pytorch_valid = False
            parameter_count = 0
            validation_info = {}
            model_structure = {}
            architecture = ModelArchitecture.UNKNOWN
            
            if self.enable_pytorch_validation and file_extension in ['.pth', '.pt', '.bin', '.safetensors']:
                validation_result = self._validate_enhanced_pytorch_model(
                    file_path, model_type, enable_detailed_analysis
                )
                pytorch_valid = validation_result['valid']
                parameter_count = validation_result['parameter_count']
                validation_info = validation_result['validation_info']
                model_structure = validation_result['model_structure']
                architecture = validation_result['architecture']
                
                if pytorch_valid:
                    self.scan_stats["valid_pytorch_models"] += 1
                    # PyTorch ê²€ì¦ ì„±ê³µí•˜ë©´ ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                    confidence_score = min(confidence_score + 0.3, 1.0)
                else:
                    self.scan_stats["pytorch_validation_errors"] += 1
                    # ê²€ì¦ ì‹¤íŒ¨í•˜ë©´ ì‹ ë¢°ë„ ê°ì†Œ
                    confidence_score = max(confidence_score - 0.2, 0.0)
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "human_parsing_graphonomy": ModelCategory.HUMAN_PARSING,
                "pose_estimation_openpose": ModelCategory.POSE_ESTIMATION,
                "cloth_segmentation_u2net": ModelCategory.CLOTH_SEGMENTATION,
                "geometric_matching": ModelCategory.GEOMETRIC_MATCHING,
                "cloth_warping": ModelCategory.CLOTH_WARPING,
                "virtual_fitting_ootd": ModelCategory.VIRTUAL_FITTING
            }
            
            detected_category = category_mapping.get(model_type, ModelCategory.AUXILIARY)
            
            # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš©
            if categories_filter and detected_category not in categories_filter:
                return None
            
            # ìš°ì„ ìˆœìœ„ ê²°ì • (ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •)
            priority = ModelPriority(pattern_info.priority) if pattern_info.priority <= 6 else ModelPriority.EXPERIMENTAL
            
            # PyTorch ê²€ì¦ ì„±ê³µì‹œ ìš°ì„ ìˆœìœ„ í–¥ìƒ
            if pytorch_valid and priority.value > 1:
                priority = ModelPriority(priority.value - 1)
            
            # Step ì´ë¦„ ìƒì„±
            step_name = self._get_step_name_for_type(model_type)
            
            # ê³ ìœ  ëª¨ë¸ ì´ë¦„ ìƒì„±
            unique_name = self._generate_enhanced_model_name(file_path, model_type, pattern_info.name)
            
            # ê°•í™”ëœ ë©”íƒ€ë°ì´í„° ìƒì„±
            enhanced_metadata = self._create_enhanced_metadata(
                file_path, model_type, pattern_info, validation_info, enable_detailed_analysis
            )
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
            performance_metrics = self._estimate_performance_metrics(
                file_path, parameter_count, file_size_mb, architecture
            ) if enable_detailed_analysis else None
            
            # ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
            memory_requirements = self._calculate_memory_requirements(
                parameter_count, file_size_mb, architecture, pattern_info
            )
            
            # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ë¶„ì„
            device_compatibility = self._analyze_device_compatibility(
                architecture, file_size_mb, parameter_count
            )
            
            # DetectedModel ê°ì²´ ìƒì„± (ê°•í™”ëœ ë²„ì „)
            detected_model = DetectedModel(
                name=unique_name,
                path=file_path,
                category=detected_category,
                model_type=pattern_info.name,
                file_size_mb=file_size_mb,
                file_extension=file_extension,
                confidence_score=confidence_score,
                priority=priority,
                step_name=step_name,
                metadata=enhanced_metadata,
                last_modified=last_modified,
                pytorch_valid=pytorch_valid,
                parameter_count=parameter_count,
                
                # ê°•í™”ëœ ìƒˆ í•„ë“œë“¤
                model_metadata=self._create_model_metadata(pattern_info, validation_info),
                architecture=architecture,
                precision=validation_info.get('precision', 'fp32'),
                optimization_level=self._detect_optimization_level(file_path, validation_info),
                model_structure=model_structure,
                layer_info=validation_info.get('layer_info', {}),
                performance_metrics=performance_metrics,
                memory_requirements=memory_requirements,
                device_compatibility=device_compatibility,
                load_time_ms=self._estimate_load_time(file_size_mb, parameter_count),
                validation_results=validation_info,
                health_status=self._assess_model_health(pytorch_valid, confidence_score, file_size_mb),
                usage_statistics={}
            )
            
            return detected_model
            
        except Exception as e:
            self.logger.debug(f"ê°•í™”ëœ ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _validate_enhanced_pytorch_model(self, file_path: Path, model_type: str, enable_detailed_analysis: bool) -> Dict[str, Any]:
        """ê°•í™”ëœ PyTorch ëª¨ë¸ ê²€ì¦"""
        try:
            if not TORCH_AVAILABLE:
                return {
                    'valid': False,
                    'parameter_count': 0,
                    'validation_info': {"error": "PyTorch not available"},
                    'model_structure': {},
                    'architecture': ModelArchitecture.UNKNOWN
                }
            
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            try:
                # ë¨¼ì € ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ ì‹œë„
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
            except Exception as e:
                # weights_only ì‹¤íŒ¨ì‹œ ì¼ë°˜ ë¡œë“œ ì‹œë„ (ë©”ëª¨ë¦¬ ì œí•œ)
                try:
                    # í° íŒŒì¼ì˜ ê²½ìš° lazy loading ì‚¬ìš©
                    if file_path.stat().st_size > 1024 * 1024 * 1024:  # 1GB ì´ìƒ
                        checkpoint = torch.load(file_path, map_location='cpu', mmap=True)
                    else:
                        checkpoint = torch.load(file_path, map_location='cpu')
                except Exception as e2:
                    return {
                        'valid': False,
                        'parameter_count': 0,
                        'validation_info': {"load_error": str(e2)},
                        'model_structure': {},
                        'architecture': ModelArchitecture.UNKNOWN
                    }
            
            validation_info = {}
            parameter_count = 0
            model_structure = {}
            architecture = ModelArchitecture.UNKNOWN
            
            if isinstance(checkpoint, dict):
                # state_dict ì¶”ì¶œ
                state_dict = self._extract_state_dict(checkpoint)
                validation_info["contains_state_dict"] = state_dict is not None
                
                if state_dict and isinstance(state_dict, dict):
                    # ê°•í™”ëœ ë ˆì´ì–´ ë¶„ì„
                    layers_analysis = self._analyze_enhanced_model_layers(state_dict, model_type)
                    validation_info.update(layers_analysis)
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (ì •í™•í•œ ë²„ì „)
                    parameter_count = self._count_enhanced_parameters(state_dict)
                    validation_info["parameter_count"] = parameter_count
                    
                    # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
                    if enable_detailed_analysis:
                        model_structure = self._analyze_model_structure(state_dict, model_type)
                        architecture = self._detect_model_architecture(state_dict, model_type)
                    
                    # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ê²€ì¦
                    type_validation = self._validate_enhanced_model_type_specific(
                        state_dict, model_type, parameter_count, enable_detailed_analysis
                    )
                    validation_info.update(type_validation)
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                metadata_keys = ['epoch', 'version', 'arch', 'model_name', 'optimizer', 'lr_scheduler', 'best_acc']
                for key in metadata_keys:
                    if key in checkpoint:
                        validation_info[f'checkpoint_{key}'] = str(checkpoint[key])[:100]
                
                # í”„ë ˆì„ì›Œí¬ ì •ë³´
                if 'pytorch_version' in checkpoint:
                    validation_info['pytorch_version'] = checkpoint['pytorch_version']
                
                return {
                    'valid': True,
                    'parameter_count': parameter_count,
                    'validation_info': validation_info,
                    'model_structure': model_structure,
                    'architecture': architecture
                }
            
            else:
                # ë‹¨ìˆœ í…ì„œë‚˜ ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                if hasattr(checkpoint, 'state_dict'):
                    state_dict = checkpoint.state_dict()
                    parameter_count = self._count_enhanced_parameters(state_dict)
                    return {
                        'valid': True,
                        'parameter_count': parameter_count,
                        'validation_info': {"model_object": True},
                        'model_structure': {},
                        'architecture': ModelArchitecture.UNKNOWN
                    }
                elif torch.is_tensor(checkpoint):
                    return {
                        'valid': True,
                        'parameter_count': checkpoint.numel(),
                        'validation_info': {"single_tensor": True},
                        'model_structure': {},
                        'architecture': ModelArchitecture.UNKNOWN
                    }
                else:
                    return {
                        'valid': False,
                        'parameter_count': 0,
                        'validation_info': {"unknown_format": type(checkpoint).__name__},
                        'model_structure': {},
                        'architecture': ModelArchitecture.UNKNOWN
                    }
            
        except Exception as e:
            return {
                'valid': False,
                'parameter_count': 0,
                'validation_info': {"validation_error": str(e)[:200]},
                'model_structure': {},
                'architecture': ModelArchitecture.UNKNOWN
            }
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and DEVICE_TYPE == "mps":
                torch.mps.empty_cache()
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()

    def _extract_state_dict(self, checkpoint: Dict) -> Optional[Dict]:
        """state_dict ì•ˆì „ ì¶”ì¶œ"""
        state_dict_keys = ['state_dict', 'model', 'model_state_dict', 'net', 'network', 'weights']
        
        for key in state_dict_keys:
            if key in checkpoint and isinstance(checkpoint[key], dict):
                return checkpoint[key]
        
        # ì²´í¬í¬ì¸íŠ¸ ìì²´ê°€ state_dictì¼ ìˆ˜ ìˆìŒ
        if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
            return checkpoint
            
        return None

    def _analyze_enhanced_model_layers(self, state_dict: Dict, model_type: str) -> Dict[str, Any]:
        """ê°•í™”ëœ ëª¨ë¸ ë ˆì´ì–´ ë¶„ì„"""
        try:
            layers_info = {
                "total_layers": len(state_dict),
                "layer_types": {},
                "layer_names": list(state_dict.keys())[:20],  # ì²˜ìŒ 20ê°œë§Œ
                "parameter_shapes": {},
                "layer_hierarchy": {},
                "attention_layers": [],
                "normalization_layers": [],
                "activation_functions": []
            }
            
            # ë ˆì´ì–´ íƒ€ì… ë¶„ì„ (ë” ì •êµí•œ ë¶„ë¥˜)
            layer_type_counts = {}
            parameter_shapes = {}
            
            for key, tensor in state_dict.items():
                try:
                    # í…ì„œ shape ì •ë³´
                    if torch.is_tensor(tensor):
                        parameter_shapes[key] = list(tensor.shape)
                    
                    # ë ˆì´ì–´ íƒ€ì… ë¶„ë¥˜ (í™•ì¥ëœ ë²„ì „)
                    key_lower = key.lower()
                    
                    # Convolution layers
                    if any(conv_type in key_lower for conv_type in ['conv1d', 'conv2d', 'conv3d', 'convtranspose']):
                        layer_type_counts['convolution'] = layer_type_counts.get('convolution', 0) + 1
                    elif 'conv' in key_lower:
                        layer_type_counts['convolution'] = layer_type_counts.get('convolution', 0) + 1
                    
                    # Normalization layers
                    elif any(norm_type in key_lower for norm_type in ['batchnorm', 'layernorm', 'groupnorm', 'instancenorm', 'bn', 'ln', 'gn']):
                        layer_type_counts['normalization'] = layer_type_counts.get('normalization', 0) + 1
                        layers_info['normalization_layers'].append(key)
                    
                    # Linear/Dense layers
                    elif any(linear_type in key_lower for linear_type in ['linear', 'dense', 'fc', 'classifier', 'head']):
                        layer_type_counts['linear'] = layer_type_counts.get('linear', 0) + 1
                    
                    # Attention layers
                    elif any(attn_type in key_lower for attn_type in ['attention', 'attn', 'self_attn', 'cross_attn', 'multihead']):
                        layer_type_counts['attention'] = layer_type_counts.get('attention', 0) + 1
                        layers_info['attention_layers'].append(key)
                    
                    # Embedding layers
                    elif any(emb_type in key_lower for emb_type in ['embed', 'embedding', 'pos_embed', 'position']):
                        layer_type_counts['embedding'] = layer_type_counts.get('embedding', 0) + 1
                    
                    # Activation functions
                    elif any(act_type in key_lower for act_type in ['relu', 'gelu', 'silu', 'swish', 'tanh', 'sigmoid']):
                        layer_type_counts['activation'] = layer_type_counts.get('activation', 0) + 1
                        layers_info['activation_functions'].append(key)
                    
                    # Transformer specific
                    elif any(trans_type in key_lower for trans_type in ['transformer', 'encoder', 'decoder', 'block']):
                        layer_type_counts['transformer'] = layer_type_counts.get('transformer', 0) + 1
                    
                    # U-Net specific
                    elif any(unet_type in key_lower for unet_type in ['down_block', 'up_block', 'mid_block', 'time_embed']):
                        layer_type_counts['unet'] = layer_type_counts.get('unet', 0) + 1
                    
                    # Diffusion specific
                    elif any(diff_type in key_lower for diff_type in ['time_embedding', 'scheduler', 'noise_pred']):
                        layer_type_counts['diffusion'] = layer_type_counts.get('diffusion', 0) + 1
                    
                    # ResNet specific
                    elif any(res_type in key_lower for res_type in ['resnet', 'residual', 'shortcut', 'downsample']):
                        layer_type_counts['residual'] = layer_type_counts.get('residual', 0) + 1
                    
                    # Other layers
                    else:
                        layer_type_counts['other'] = layer_type_counts.get('other', 0) + 1
                
                except Exception as e:
                    self.logger.debug(f"ë ˆì´ì–´ ë¶„ì„ ì˜¤ë¥˜ {key}: {e}")
                    continue
            
            layers_info["layer_types"] = layer_type_counts
            layers_info["parameter_shapes"] = parameter_shapes
            
            # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ë ˆì´ì–´ í™•ì¸
            verification_pattern = ENHANCED_CHECKPOINT_VERIFICATION_PATTERNS.get(model_type, {})
            required_layers = verification_pattern.get("required_layers", [])
            
            found_required = 0
            for required_layer in required_layers:
                if any(required_layer in key.lower() for key in state_dict.keys()):
                    found_required += 1
            
            layers_info["required_layers_found"] = found_required
            layers_info["required_layers_total"] = len(required_layers)
            layers_info["required_layers_match_rate"] = found_required / len(required_layers) if required_layers else 1.0
            
            # ëª¨ë¸ ë³µì¡ë„ ë¶„ì„
            layers_info["complexity_score"] = self._calculate_model_complexity(state_dict, layer_type_counts)
            
            return layers_info
            
        except Exception as e:
            return {"layer_analysis_error": str(e)[:100]}

    def _count_enhanced_parameters(self, state_dict: Dict) -> int:
        """ì •í™•í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        try:
            total_params = 0
            trainable_params = 0
            
            for key, tensor in state_dict.items():
                if torch.is_tensor(tensor):
                    param_count = tensor.numel()
                    total_params += param_count
                    
                    # ì¼ë°˜ì ìœ¼ë¡œ biasì™€ weightëŠ” í›ˆë ¨ ê°€ëŠ¥
                    if any(suffix in key.lower() for suffix in ['weight', 'bias']):
                        trainable_params += param_count
            
            return total_params
            
        except Exception as e:
            self.logger.debug(f"íŒŒë¼ë¯¸í„° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0

    def _analyze_model_structure(self, state_dict: Dict, model_type: str) -> Dict[str, Any]:
        """ëª¨ë¸ êµ¬ì¡° ì‹¬ì¸µ ë¶„ì„"""
        try:
            structure = {
                "layers_by_type": {},
                "layer_hierarchy": [],
                "input_output_shapes": {},
                "bottlenecks": [],
                "skip_connections": [],
                "attention_patterns": [],
                "architecture_features": []
            }
            
            # ë ˆì´ì–´ ê³„ì¸µ êµ¬ì¡° ë¶„ì„
            layer_groups = {}
            for key in state_dict.keys():
                # ë ˆì´ì–´ ê·¸ë£¹ ì¶”ì¶œ (ì ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ë¡œì—ì„œ)
                parts = key.split('.')
                if len(parts) > 1:
                    group = parts[0]
                    if group not in layer_groups:
                        layer_groups[group] = []
                    layer_groups[group].append(key)
            
            structure["layers_by_type"] = layer_groups
            
            # íŠ¹ë³„í•œ íŒ¨í„´ íƒì§€
            all_keys = list(state_dict.keys())
            
            # Skip connection íŒ¨í„´ íƒì§€
            skip_patterns = ['shortcut', 'residual', 'skip', 'downsample']
            structure["skip_connections"] = [
                key for key in all_keys 
                if any(pattern in key.lower() for pattern in skip_patterns)
            ]
            
            # Attention íŒ¨í„´ íƒì§€
            attention_patterns = ['attn', 'attention', 'self_attn', 'cross_attn', 'multihead']
            structure["attention_patterns"] = [
                key for key in all_keys 
                if any(pattern in key.lower() for pattern in attention_patterns)
            ]
            
            # ì•„í‚¤í…ì²˜ íŠ¹ì§• ë¶„ì„
            if any('unet' in key.lower() or 'down_block' in key.lower() for key in all_keys):
                structure["architecture_features"].append("U-Net_Architecture")
            if any('transformer' in key.lower() or 'encoder' in key.lower() for key in all_keys):
                structure["architecture_features"].append("Transformer_Architecture")
            if any('resnet' in key.lower() or 'residual' in key.lower() for key in all_keys):
                structure["architecture_features"].append("ResNet_Architecture")
            if any('time_embed' in key.lower() or 'scheduler' in key.lower() for key in all_keys):
                structure["architecture_features"].append("Diffusion_Architecture")
            
            return structure
            
        except Exception as e:
            return {"structure_analysis_error": str(e)[:100]}

    def _detect_model_architecture(self, state_dict: Dict, model_type: str) -> ModelArchitecture:
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ìë™ íƒì§€"""
        try:
            all_keys = [key.lower() for key in state_dict.keys()]
            key_string = ' '.join(all_keys)
            
            # ì•„í‚¤í…ì²˜ë³„ í‚¤ì›Œë“œ íŒ¨í„´
            architecture_patterns = {
                ModelArchitecture.UNET: ['unet', 'down_block', 'up_block', 'mid_block', 'encoder', 'decoder'],
                ModelArchitecture.TRANSFORMER: ['transformer', 'attention', 'multihead', 'encoder', 'decoder', 'embed'],
                ModelArchitecture.DIFFUSION: ['time_embed', 'noise_pred', 'scheduler', 'timestep', 'diffusion'],
                ModelArchitecture.CNN: ['conv', 'pool', 'batch', 'relu', 'classifier'],
                ModelArchitecture.GAN: ['generator', 'discriminator', 'adversarial'],
                ModelArchitecture.VAE: ['encoder', 'decoder', 'latent', 'kl_loss', 'reconstruction'],
                ModelArchitecture.CLIP: ['text_encoder', 'vision_encoder', 'projection', 'similarity'],
                ModelArchitecture.RESNET: ['resnet', 'residual', 'shortcut', 'downsample', 'bottleneck']
            }
            
            # ê° ì•„í‚¤í…ì²˜ë³„ ì ìˆ˜ ê³„ì‚°
            architecture_scores = {}
            for arch, patterns in architecture_patterns.items():
                score = sum(1 for pattern in patterns if pattern in key_string)
                if score > 0:
                    architecture_scores[arch] = score
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì•„í‚¤í…ì²˜ ë°˜í™˜
            if architecture_scores:
                best_architecture = max(architecture_scores.items(), key=lambda x: x[1])[0]
                return best_architecture
            
            # ëª¨ë¸ íƒ€ì… ê¸°ë°˜ í´ë°±
            type_to_architecture = {
                "human_parsing": ModelArchitecture.CNN,
                "pose_estimation": ModelArchitecture.CNN,
                "cloth_segmentation": ModelArchitecture.UNET,
                "virtual_fitting": ModelArchitecture.DIFFUSION
            }
            
            return type_to_architecture.get(model_type, ModelArchitecture.UNKNOWN)
            
        except Exception as e:
            return ModelArchitecture.UNKNOWN

    def _calculate_model_complexity(self, state_dict: Dict, layer_types: Dict) -> float:
        """ëª¨ë¸ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            complexity_score = 0.0
            
            # ë ˆì´ì–´ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
            type_weights = {
                'convolution': 2.0,
                'linear': 1.5,
                'attention': 3.0,
                'transformer': 3.5,
                'unet': 2.5,
                'diffusion': 4.0,
                'normalization': 0.5,
                'activation': 0.2,
                'embedding': 1.0,
                'residual': 1.5
            }
            
            # ë ˆì´ì–´ íƒ€ì…ë³„ ì ìˆ˜ ê³„ì‚°
            for layer_type, count in layer_types.items():
                weight = type_weights.get(layer_type, 1.0)
                complexity_score += count * weight
            
            # ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ë³´ì •
            total_params = sum(tensor.numel() for tensor in state_dict.values() if torch.is_tensor(tensor))
            param_factor = min(total_params / 1000000, 10.0)  # ë°±ë§Œ íŒŒë¼ë¯¸í„°ë‹¹ 1ì , ìµœëŒ€ 10ì 
            
            complexity_score += param_factor
            
            # 0-100 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized_score = min(complexity_score / 50.0 * 100, 100.0)
            
            return round(normalized_score, 2)
            
        except Exception as e:
            return 0.0

    def _create_enhanced_metadata(
        self, 
        file_path: Path, 
        model_type: str, 
        pattern_info: EnhancedModelFileInfo,
        validation_info: Dict,
        enable_detailed_analysis: bool
    ) -> Dict[str, Any]:
        """ê°•í™”ëœ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            metadata = {
                # ê¸°ë³¸ ì •ë³´
                "file_name": file_path.name,
                "file_size_mb": file_path.stat().st_size / (1024 * 1024),
                "model_type": model_type,
                "detected_at": time.time(),
                "detector_version": "v7.0",
                "auto_detected": True,
                "pattern_matched": True,
                
                # ê¸°ìˆ  ì •ë³´
                "framework": "pytorch",
                "architecture": pattern_info.architecture.value,
                "expected_performance": pattern_info.performance_expectations,
                "memory_profile": pattern_info.memory_profile,
                "optimization_hints": pattern_info.optimization_hints,
                
                # ê²€ì¦ ì •ë³´
                "pytorch_validated": validation_info.get('pytorch_valid', False),
                "parameter_count": validation_info.get('parameter_count', 0),
                "complexity_score": validation_info.get('complexity_score', 0.0),
                
                # í˜¸í™˜ì„± ì •ë³´
                "device_compatibility": {
                    "cpu": True,
                    "cuda": TORCH_AVAILABLE and torch.cuda.is_available(),
                    "mps": TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),
                    "m3_max_optimized": IS_M3_MAX
                },
                
                # ì„±ëŠ¥ íŒíŠ¸
                "performance_hints": {
                    "recommended_batch_size": self._calculate_recommended_batch_size(pattern_info),
                    "memory_efficient": file_path.stat().st_size < 500 * 1024 * 1024,  # 500MB ë¯¸ë§Œ
                    "supports_fp16": True,
                    "supports_compilation": TORCH_AVAILABLE
                }
            }
            
            # ìƒì„¸ ë¶„ì„ì´ í™œì„±í™”ëœ ê²½ìš° ì¶”ê°€ ì •ë³´
            if enable_detailed_analysis:
                metadata.update({
                    "detailed_analysis": True,
                    "layer_analysis": validation_info.get('layer_analysis', {}),
                    "structure_analysis": validation_info.get('structure_analysis', {}),
                    "checksum": self._calculate_file_checksum(file_path),
                    "file_permissions": oct(file_path.stat().st_mode)[-3:],
                    "last_accessed": file_path.stat().st_atime,
                    "creation_time": file_path.stat().st_ctime if hasattr(file_path.stat(), 'st_ctime') else None
                })
            
            return metadata
            
        except Exception as e:
            return {
                "error": f"ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}",
                "file_name": file_path.name,
                "model_type": model_type
            }

    def _calculate_file_checksum(self, file_path: Path) -> Optional[str]:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (í° íŒŒì¼ë„ íš¨ìœ¨ì ìœ¼ë¡œ)"""
        try:
            hash_sha256 = hashlib.sha256()
            
            # í° íŒŒì¼ì˜ ê²½ìš° ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
            chunk_size = 8192  # 8KB ì²­í¬
            
            with open(file_path, 'rb') as f:
                while chunk := f.read(chunk_size):
                    hash_sha256.update(chunk)
            
            return hash_sha256.hexdigest()[:16]  # ì²˜ìŒ 16ìë¦¬ë§Œ ì‚¬ìš©
            
        except Exception as e:
            self.logger.debug(f"ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def _calculate_recommended_batch_size(self, pattern_info: EnhancedModelFileInfo) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        try:
            # ë©”ëª¨ë¦¬ í”„ë¡œí•„ ê¸°ë°˜ ê³„ì‚°
            recommended_memory = pattern_info.memory_profile.get('recommended_memory_mb', 1000)
            available_memory = self.device_info.get('memory_available_gb', 8.0) * 1024
            
            # ì•ˆì „ ë§ˆì§„ (50% ì‚¬ìš©)
            safe_memory = available_memory * 0.5
            
            if recommended_memory > 0:
                batch_size = max(1, int(safe_memory / recommended_memory))
                return min(batch_size, 32)  # ìµœëŒ€ 32ë¡œ ì œí•œ
            
            return 1
            
        except Exception as e:
            return 1

    # ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ê´€ë ¨ ë©”ì„œë“œë“¤
    def _estimate_performance_metrics(
        self, 
        file_path: Path, 
        parameter_count: int, 
        file_size_mb: float, 
        architecture: ModelArchitecture
    ) -> ModelPerformanceMetrics:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì •"""
        try:
            # ê¸°ë³¸ ì„±ëŠ¥ ì¶”ì • (ê²½í—˜ì  ê³µì‹)
            base_inference_time = {
                ModelArchitecture.CNN: 50,
                ModelArchitecture.UNET: 200,
                ModelArchitecture.TRANSFORMER: 300,
                ModelArchitecture.DIFFUSION: 2000,
                ModelArchitecture.GAN: 150,
                ModelArchitecture.UNKNOWN: 100
            }.get(architecture, 100)
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ì¡°ì •
            param_factor = max(1.0, parameter_count / 50000000)  # 50M íŒŒë¼ë¯¸í„° ê¸°ì¤€
            estimated_inference_time = base_inference_time * param_factor
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            estimated_memory = file_size_mb * 2.0  # ëª¨ë¸ ë¡œë“œì‹œ 2ë°° ë©”ëª¨ë¦¬ ì‚¬ìš© ì¶”ì •
            
            # M3 Max ìµœì í™” ì ìˆ˜
            m3_compatibility = 1.0 if IS_M3_MAX else 0.7
            
            return ModelPerformanceMetrics(
                inference_time_ms=estimated_inference_time,
                memory_usage_mb=estimated_memory,
                gpu_utilization=0.0,  # ì‹¤ì œ ì¸¡ì • í•„ìš”
                throughput_fps=1000.0 / estimated_inference_time if estimated_inference_time > 0 else 0.0,
                accuracy_score=None,  # ì‹¤ì œ í‰ê°€ í•„ìš”
                benchmark_score=None,  # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ í•„ìš”
                energy_efficiency=None,  # ì‹¤ì œ ì¸¡ì • í•„ìš”
                m3_compatibility_score=m3_compatibility
            )
            
        except Exception as e:
            return ModelPerformanceMetrics()

    def _calculate_memory_requirements(
        self, 
        parameter_count: int, 
        file_size_mb: float, 
        architecture: ModelArchitecture,
        pattern_info: EnhancedModelFileInfo
    ) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì •í™• ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ê³„ì‚°
            model_memory = file_size_mb
            
            # ëŸ°íƒ€ì„ ë©”ëª¨ë¦¬ (gradient, optimizer state ë“±)
            runtime_multiplier = {
                ModelArchitecture.DIFFUSION: 3.0,  # ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
                ModelArchitecture.TRANSFORMER: 2.5,
                ModelArchitecture.UNET: 2.0,
                ModelArchitecture.CNN: 1.5,
                ModelArchitecture.GAN: 2.0
            }.get(architecture, 2.0)
            
            runtime_memory = model_memory * runtime_multiplier
            
            # ë°°ì¹˜ í¬ê¸°ë³„ ë©”ëª¨ë¦¬
            batch_memories = {}
            for batch_size in [1, 2, 4, 8, 16]:
                batch_memory = runtime_memory + (runtime_memory * 0.2 * (batch_size - 1))
                batch_memories[f"batch_{batch_size}"] = batch_memory
            
            # íŒ¨í„´ ì •ë³´ì—ì„œ ë©”ëª¨ë¦¬ í”„ë¡œí•„ ì‚¬ìš©
            if hasattr(pattern_info, 'memory_profile') and pattern_info.memory_profile:
                min_memory = pattern_info.memory_profile.get('min_memory_mb', runtime_memory * 0.7)
                recommended_memory = pattern_info.memory_profile.get('recommended_memory_mb', runtime_memory)
                peak_memory = pattern_info.memory_profile.get('peak_memory_mb', runtime_memory * 1.5)
            else:
                min_memory = runtime_memory * 0.7
                recommended_memory = runtime_memory
                peak_memory = runtime_memory * 1.5
            
            return {
                "model_size_mb": model_memory,
                "min_memory_mb": min_memory,
                "recommended_memory_mb": recommended_memory,
                "peak_memory_mb": peak_memory,
                "runtime_memory_mb": runtime_memory,
                **batch_memories
            }
            
        except Exception as e:
            return {
                "model_size_mb": file_size_mb,
                "min_memory_mb": file_size_mb * 1.5,
                "recommended_memory_mb": file_size_mb * 2.0,
                "peak_memory_mb": file_size_mb * 3.0,
                "error": str(e)
            }

    def _analyze_device_compatibility(
        self, 
        architecture: ModelArchitecture, 
        file_size_mb: float, 
        parameter_count: int
    ) -> Dict[str, bool]:
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ë¶„ì„"""
        try:
            compatibility = {
                "cpu": True,  # í•­ìƒ CPU ì§€ì›
                "cuda": False,
                "mps": False,
                "neural_engine": False,
                "memory_sufficient": False,
                "recommended_device": "cpu"
            }
            
            # CUDA í˜¸í™˜ì„±
            if TORCH_AVAILABLE and torch.cuda.is_available():
                compatibility["cuda"] = True
                if file_size_mb < 2000:  # 2GB ë¯¸ë§Œì´ë©´ ì¼ë°˜ì ìœ¼ë¡œ GPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
                    compatibility["recommended_device"] = "cuda"
            
            # MPS (Apple Silicon) í˜¸í™˜ì„±
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                compatibility["mps"] = True
                if IS_M3_MAX:
                    compatibility["neural_engine"] = True
                    if file_size_mb < 4000:  # M3 MaxëŠ” ë” í° ëª¨ë¸ë„ ì²˜ë¦¬ ê°€ëŠ¥
                        compatibility["recommended_device"] = "mps"
            
            # ë©”ëª¨ë¦¬ ì¶©ë¶„ì„± í™•ì¸
            available_memory_gb = self.device_info.get('memory_available_gb', 8.0)
            required_memory_gb = file_size_mb / 1024 * 2.5  # 2.5ë°° ì•ˆì „ ë§ˆì§„
            compatibility["memory_sufficient"] = available_memory_gb > required_memory_gb
            
            # ì•„í‚¤í…ì²˜ë³„ ìµœì  ë””ë°”ì´ìŠ¤
            if architecture == ModelArchitecture.DIFFUSION and compatibility["mps"] and IS_M3_MAX:
                compatibility["recommended_device"] = "mps"
            elif architecture == ModelArchitecture.CNN and compatibility["cuda"]:
                compatibility["recommended_device"] = "cuda"
            
            return compatibility
            
        except Exception as e:
            return {
                "cpu": True,
                "cuda": False,
                "mps": False,
                "error": str(e)
            }

    # ë” ë§ì€ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ê³„ì† êµ¬í˜„...
    
    def get_validated_models_only(self) -> Dict[str, DetectedModel]:
        """PyTorch ê²€ì¦ëœ ëª¨ë¸ë“¤ë§Œ ë°˜í™˜ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        return {name: model for name, model in self.detected_models.items() if model.pytorch_valid}
    
    def get_models_by_category(self, category: ModelCategory) -> List[DetectedModel]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        return [model for model in self.detected_models.values() if model.category == category]

    def get_models_by_step(self, step_name: str) -> List[DetectedModel]:
        """Stepë³„ ëª¨ë¸ ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        return [model for model in self.detected_models.values() if model.step_name == step_name]

    def get_best_model_for_step(self, step_name: str) -> Optional[DetectedModel]:
        """Stepë³„ ìµœì  ëª¨ë¸ ì¡°íšŒ (ê°•í™”ëœ ë²„ì „)"""
        step_models = self.get_models_by_step(step_name)
        if not step_models:
            return None
        
        # ë³µí•© ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ (PyTorch ê²€ì¦ + ìš°ì„ ìˆœìœ„ + ì‹ ë¢°ë„ + ì„±ëŠ¥)
        def model_score(model):
            score = 0
            # PyTorch ê²€ì¦ ë³´ë„ˆìŠ¤
            if model.pytorch_valid:
                score += 100
            # ìš°ì„ ìˆœìœ„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            score += (6 - model.priority.value) * 20
            # ì‹ ë¢°ë„
            score += model.confidence_score * 50
            # íŒŒë¼ë¯¸í„° ìˆ˜ (ì ë‹¹í•œ í¬ê¸°ê°€ ì¢‹ìŒ)
            if model.parameter_count > 0:
                param_score = min(model.parameter_count / 100000000, 10) * 5  # 100M íŒŒë¼ë¯¸í„°ë‹¹ 5ì 
                score += param_score
            return score
        
        return max(step_models, key=model_score)

    # ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ìœ ì§€
    def _get_step_name_for_type(self, model_type: str) -> str:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ Step ì´ë¦„ ë°˜í™˜ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        step_mapping = {
            "human_parsing_graphonomy": "HumanParsingStep",
            "pose_estimation_openpose": "PoseEstimationStep",
            "cloth_segmentation_u2net": "ClothSegmentationStep",
            "geometric_matching": "GeometricMatchingStep",
            "cloth_warping": "ClothWarpingStep",
            "virtual_fitting_ootd": "VirtualFittingStep"
        }
        return step_mapping.get(model_type, "UnknownStep")

    def _generate_enhanced_model_name(self, file_path: Path, model_type: str, base_name: str) -> str:
        """ê³ ìœ í•œ ëª¨ë¸ ì´ë¦„ ìƒì„± (ê°•í™”ëœ ë²„ì „)"""
        try:
            # í‘œì¤€ ì´ë¦„ ìš°ì„  ì‚¬ìš©
            standard_names = {
                "human_parsing_graphonomy": "human_parsing_graphonomy",
                "pose_estimation_openpose": "pose_estimation_openpose",
                "cloth_segmentation_u2net": "cloth_segmentation_u2net",
                "geometric_matching": "geometric_matching_gmm",
                "cloth_warping": "cloth_warping_tom",
                "virtual_fitting_ootd": "virtual_fitting_ootdiffusion"
            }
            
            standard_name = standard_names.get(model_type)
            if standard_name:
                # ë™ì¼í•œ ì´ë¦„ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                if standard_name not in self.detected_models:
                    return standard_name
                else:
                    # ë²„ì „ ë²ˆí˜¸ ì¶”ê°€
                    version = 2
                    while f"{standard_name}_v{version}" in self.detected_models:
                        version += 1
                    return f"{standard_name}_v{version}"
            
            # íŒŒì¼ëª… ê¸°ë°˜ ì´ë¦„ ìƒì„± (ê°•í™”ëœ ë²„ì „)
            file_stem = file_path.stem.lower()
            # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì •ê·œí™”
            clean_name = re.sub(r'[^a-z0-9_]', '_', file_stem)
            clean_name = re.sub(r'_+', '_', clean_name)  # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
            clean_name = clean_name.strip('_')  # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
            
            # í•´ì‹œ ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
            path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:6]
            
            candidate_name = f"{model_type}_{clean_name}_{path_hash}"
            
            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 80ì)
            if len(candidate_name) > 80:
                candidate_name = candidate_name[:80]
            
            return candidate_name
            
        except Exception as e:
            # ì™„ì „ í´ë°±
            timestamp = int(time.time())
            return f"detected_model_{model_type}_{timestamp}"

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤

class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.start_memory = 0
        self.peak_memory = 0
        self.monitoring = False
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        try:
            if psutil:
                self.start_memory = psutil.virtual_memory().used / (1024**3)
                self.peak_memory = self.start_memory
                self.monitoring = True
        except:
            pass
    
    def stop_monitoring(self) -> Dict[str, float]:
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜"""
        try:
            if psutil and self.monitoring:
                current_memory = psutil.virtual_memory().used / (1024**3)
                return {
                    "memory_usage_start_gb": self.start_memory,
                    "memory_usage_end_gb": current_memory,
                    "memory_usage_delta_gb": current_memory - self.start_memory,
                    "memory_peak_gb": self.peak_memory
                }
        except:
            pass
        
        return {}

class ModelPerformanceProfiler:
    """ëª¨ë¸ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§"""
    
    def __init__(self):
        self.profiles = {}
    
    def profile_model(self, model_path: Path, detected_model: DetectedModel) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰"""
        try:
            # ê°„ë‹¨í•œ ë¡œë“œ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œëŠ” ìœ„í—˜í•˜ë¯€ë¡œ íŒŒì¼ ì½ê¸° ì‹œê°„ë§Œ ì¸¡ì •
            with open(model_path, 'rb') as f:
                f.read(8192)  # ì²« 8KBë§Œ ì½ê¸°
            
            load_time = (time.time() - start_time) * 1000  # ms
            
            return {
                "load_time_ms": load_time,
                "estimated_inference_time_ms": detected_model.performance_metrics.inference_time_ms if detected_model.performance_metrics else 0,
                "profile_timestamp": time.time()
            }
            
        except Exception as e:
            return {"error": str(e)}

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ ë° í´ë˜ìŠ¤ë“¤ ìœ ì§€

# ëª¨ë“  ê¸°ì¡´ í´ë˜ìŠ¤ëª…ê³¼ í•¨ìˆ˜ëª…ì„ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ ê¸°ëŠ¥ë§Œ ê°•í™”
create_real_world_detector = lambda **kwargs: RealWorldModelDetector(**kwargs)

# ê¸°ì¡´ export ìœ ì§€
__all__ = [
    # ê¸°ì¡´ exports...
    'RealWorldModelDetector',
    'RealModelLoaderConfigGenerator', 
    'DetectedModel',
    'ModelCategory',
    'ModelPriority',
    'ModelFileInfo',
    
    # ìƒˆë¡œìš´ ê°•í™” í´ë˜ìŠ¤ë“¤
    'EnhancedModelFileInfo',
    'ModelArchitecture',
    'ModelOptimization',
    'ModelMetadata',
    'ModelPerformanceMetrics',
    'MemoryMonitor',
    'ModelPerformanceProfiler',
    
    # ê¸°ì¡´ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_real_world_detector',
    'quick_real_model_detection',
    'generate_real_model_loader_config',
    
    # ê°•í™”ëœ íŒ¨í„´ë“¤
    'ENHANCED_MODEL_PATTERNS',
    'ENHANCED_CHECKPOINT_VERIFICATION_PATTERNS',
    
    # í•˜ìœ„ í˜¸í™˜ì„± ë³„ì¹­
    'AdvancedModelDetector',
    'ModelLoaderConfigGenerator',
    'create_advanced_detector'
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜)
AdvancedModelDetector = RealWorldModelDetector
ModelLoaderConfigGenerator = RealModelLoaderConfigGenerator  
create_advanced_detector = create_real_world_detector

logger.info("âœ… ê°•í™”ëœ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ v7.0 ë¡œë“œ ì™„ë£Œ - 89.8GB ì‹¤ì œ í™œìš© + ì‹¤ë¬´ê¸‰ ì„±ëŠ¥")
logger.info("ğŸ”¥ ê¸°ì¡´ í´ë˜ìŠ¤ëª…/í•¨ìˆ˜ëª… 100% ìœ ì§€ + ê¸°ëŠ¥ ëŒ€í­ ê°•í™”")
logger.info("ğŸ M3 Max 128GB ìµœì í™” + Neural Engine í™œìš©")
logger.info("ğŸ” PyTorch ê²€ì¦ + ëª¨ë¸ êµ¬ì¡° ë¶„ì„ + ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§")
logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + ì‹¤ë¬´ê¸‰ ê¸°ëŠ¥")