# app/ai_pipeline/utils/step_model_requests.py
"""
ğŸ”¥ Stepë³„ AI ëª¨ë¸ ìš”ì²­ ì •ì˜ ì‹œìŠ¤í…œ v4.0
âœ… ì‹¤ì œ Step í´ë˜ìŠ¤ ìš”êµ¬ì‚¬í•­ ì™„ë²½ ë°˜ì˜
âœ… ModelLoaderì™€ 100% í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
âœ… auto_model_detector ì™„ë²½ ì—°ë™
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
"""

import time
import logging
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ ë°ì´í„° êµ¬ì¡°
# ==============================================

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•„ìˆ˜ (Human Parsing, Virtual Fitting)
    HIGH = 2          # ì¤‘ìš” (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3        # ì¼ë°˜ (Cloth Warping, Geometric Matching)
    LOW = 4           # ë³´ì¡° (Post Processing, Quality Assessment)

@dataclass
class ModelRequest:
    """Stepì´ ModelLoaderì— ìš”ì²­í•˜ëŠ” ì™„ì „í•œ ì •ë³´"""
    # ê¸°ë³¸ ì •ë³´
    model_name: str
    step_class: str
    step_priority: StepPriority
    model_class: str
    
    # ì…ì¶œë ¥ ìŠ¤í™
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    output_format: str = "tensor"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "fp16"
    
    # ì²´í¬í¬ì¸íŠ¸ íƒì§€ ì •ë³´ (auto_model_detectorìš©)
    checkpoint_patterns: List[str] = field(default_factory=list)
    file_extensions: List[str] = field(default_factory=list)
    size_range_mb: Tuple[float, float] = (1.0, 10000.0)
    
    # ìµœì í™” ì„¤ì •
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    
    # ëŒ€ì²´ ëª¨ë¸
    alternative_models: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ Stepë³„ ì‹¤ì œ ëª¨ë¸ ìš”ì²­ ì •ì˜
# ==============================================

STEP_MODEL_REQUESTS = {
    
    # Step 01: Human Parsing
    "HumanParsingStep": ModelRequest(
        model_name="human_parsing_graphonomy",
        step_class="HumanParsingStep",
        step_priority=StepPriority.CRITICAL,
        model_class="GraphonomyModel",
        input_size=(512, 512),
        num_classes=20,
        output_format="segmentation_mask",
        
        # ìë™ íƒì§€ íŒ¨í„´
        checkpoint_patterns=[
            r".*human.*parsing.*\.pth$",
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*\.pth$",
            r".*inference.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".pkl"],
        size_range_mb=(50.0, 500.0),
        
        # ìµœì í™” ì„¤ì •
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.3,
            "enable_amp": True,
            "cache_model": True,
            "warmup_iterations": 3
        },
        
        # ëŒ€ì²´ ëª¨ë¸
        alternative_models=[
            "human_parsing_atr",
            "human_parsing_lip",
            "human_parsing_u2net"
        ],
        
        # ë©”íƒ€ë°ì´í„°
        metadata={
            "description": "20ê°œ ë¶€ìœ„ ì¸ì²´ íŒŒì‹±",
            "body_parts": ["head", "torso", "arms", "legs", "accessories"],
            "supports_refinement": True,
            "postprocess_enabled": True
        }
    ),
    
    # Step 02: Pose Estimation
    "PoseEstimationStep": ModelRequest(
        model_name="pose_estimation_openpose",
        step_class="PoseEstimationStep",
        step_priority=StepPriority.HIGH,
        model_class="OpenPoseModel",
        input_size=(368, 368),
        num_classes=18,
        output_format="keypoints_heatmap",
        
        checkpoint_patterns=[
            r".*pose.*model.*\.pth$",
            r".*openpose.*\.pth$",
            r".*body.*pose.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".tflite"],
        size_range_mb=(10.0, 200.0),
        
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.25,
            "inference_threads": 4,
            "enable_tensorrt": True
        },
        
        alternative_models=[
            "pose_estimation_sk",
            "pose_estimation_lightweight"
        ],
        
        metadata={
            "description": "18ê°œ í‚¤í¬ì¸íŠ¸ í¬ì¦ˆ ì¶”ì •",
            "keypoints_format": "coco",
            "supports_hands": True,
            "num_stages": 6
        }
    ),
    
    # Step 03: Cloth Segmentation
    "ClothSegmentationStep": ModelRequest(
        model_name="cloth_segmentation_u2net",
        step_class="ClothSegmentationStep",
        step_priority=StepPriority.HIGH,
        model_class="U2NetModel",
        input_size=(320, 320),
        num_classes=1,
        output_format="binary_mask",
        
        checkpoint_patterns=[
            r".*u2net.*\.pth$",
            r".*cloth.*segmentation.*\.pth$",
            r".*mobile.*sam.*\.pt$"
        ],
        file_extensions=[".pth", ".pt", ".onnx"],
        size_range_mb=(20.0, 1000.0),
        
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.4,
            "enable_half_precision": True,
            "tile_processing": True
        },
        
        alternative_models=[
            "cloth_segmentation_sam",
            "cloth_segmentation_deeplabv3"
        ],
        
        metadata={
            "description": "ì˜ë¥˜ ì´ì§„ ì„¸ê·¸ë©˜í…Œì´ì…˜",
            "supports_multiple_items": True,
            "background_removal": True,
            "edge_refinement": True
        }
    ),
    
    # Step 04: Geometric Matching
    "GeometricMatchingStep": ModelRequest(
        model_name="geometric_matching_gmm",
        step_class="GeometricMatchingStep",
        step_priority=StepPriority.MEDIUM,
        model_class="GeometricMatchingModel",
        input_size=(512, 384),
        output_format="transformation_matrix",
        
        checkpoint_patterns=[
            r".*geometric.*matching.*\.pth$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(5.0, 100.0),
        
        optimization_params={
            "batch_size": 2,
            "memory_fraction": 0.2,
            "enable_jit_compile": True
        },
        
        alternative_models=[
            "geometric_matching_lightweight"
        ],
        
        metadata={
            "description": "TPS ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­",
            "num_control_points": 25,
            "max_iterations": 1000
        }
    ),
    
    # Step 05: Cloth Warping
    "ClothWarpingStep": ModelRequest(
        model_name="cloth_warping_tom",
        step_class="ClothWarpingStep",
        step_priority=StepPriority.MEDIUM,
        model_class="HRVITONModel",
        input_size=(512, 384),
        output_format="warped_cloth",
        
        checkpoint_patterns=[
            r".*tom.*final.*\.pth$",
            r".*cloth.*warping.*\.pth$",
            r".*hrviton.*\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(100.0, 1000.0),
        
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.5,
            "enable_amp": True,
            "gradient_accumulation": 2
        },
        
        alternative_models=[
            "cloth_warping_hrviton_v2"
        ],
        
        metadata={
            "description": "ë¬¼ë¦¬ ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘",
            "enable_physics": True,
            "cloth_stiffness": 0.3,
            "supports_wrinkles": True
        }
    ),
    
    # Step 06: Virtual Fitting
    "VirtualFittingStep": ModelRequest(
        model_name="virtual_fitting_stable_diffusion",
        step_class="VirtualFittingStep",
        step_priority=StepPriority.CRITICAL,
        model_class="StableDiffusionPipeline",
        input_size=(512, 512),
        output_format="rgb_image",
        
        checkpoint_patterns=[
            r".*diffusion.*pytorch.*model.*\.bin$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.pth$",
            r".*unet.*\.bin$"
        ],
        file_extensions=[".bin", ".safetensors", ".pth"],
        size_range_mb=(500.0, 5000.0),
        
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.7,
            "enable_attention_slicing": True,
            "enable_cpu_offload": True
        },
        
        alternative_models=[
            "virtual_fitting_oot",
            "virtual_fitting_hrviton"
        ],
        
        metadata={
            "description": "Diffusion ê¸°ë°˜ ê°€ìƒ í”¼íŒ…",
            "num_inference_steps": 50,
            "guidance_scale": 7.5,
            "scheduler_type": "ddim"
        }
    ),
    # Step 07: Post Processing (ì¶”ê°€í•  ë‚´ìš©)
# ê¸°ì¡´ STEP_MODEL_REQUESTS ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€

"PostProcessingStep": ModelRequest(
    model_name="post_processing_enhancement",
    step_class="PostProcessingStep", 
    step_priority=StepPriority.LOW,
    model_class="EnhancementModel",
    input_size=(512, 512),
    num_classes=None,
    output_format="enhanced_image",
    
    # ìë™ íƒì§€ íŒ¨í„´
    checkpoint_patterns=[
        r".*super.*resolution.*\.pth$",
        r".*sr.*resnet.*\.pth$", 
        r".*esrgan.*\.pth$",
        r".*denoise.*net.*\.pth$",
        r".*enhancement.*\.pth$",
        r".*post.*process.*\.pth$"
    ],
    file_extensions=[".pth", ".pt", ".ckpt", ".bin"],
    size_range_mb=(5.0, 500.0),
    
    # ìµœì í™” ì„¤ì •
    optimization_params={
        "batch_size": 4,
        "precision": "fp16" if torch.cuda.is_available() else "fp32",
        "memory_efficient": True,
        "cache_models": True,
        "use_amp": True,
        "compile_model": False,  # PyTorch 2.x ì»´íŒŒì¼ì€ í›„ì²˜ë¦¬ì—ì„œëŠ” ë¶ˆí•„ìš”
        "gradient_checkpointing": False,  # ì¶”ë¡ ë§Œ í•˜ë¯€ë¡œ ë¶ˆí•„ìš”
        "model_parallel": False
    },
    
    # ëŒ€ì²´ ëª¨ë¸ë“¤
    alternative_models=[
        "basic_sr_model", 
        "lightweight_enhancement",
        "traditional_enhancement"
    ],
    
    # ë©”íƒ€ë°ì´í„°
    metadata={
        "description": "ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ",
        "capabilities": [
            "super_resolution",
            "denoising", 
            "sharpening",
            "color_correction",
            "contrast_enhancement"
        ],
        "input_formats": ["RGB", "RGBA"],
        "output_formats": ["RGB"],
        "processing_modes": ["real_time", "balanced", "quality"],
        "supported_resolutions": [(256, 256), (512, 512), (1024, 1024), (2048, 2048)],
        "enhancement_methods": [
            "ai_super_resolution",
            "ai_denoising", 
            "traditional_sharpening",
            "adaptive_contrast",
            "color_balance"
        ],
        "quality_levels": ["low", "medium", "high", "ultra"],
        "m3_max_optimized": True,
        "memory_requirements": {
            "minimum_gb": 4.0,
            "recommended_gb": 8.0,
            "optimal_gb": 16.0
        },
        "performance_benchmarks": {
            "512x512_processing_time_ms": 150,
            "1024x1024_processing_time_ms": 600,
            "max_concurrent_requests": 8
        }
    }
),

# Step 07 ë³´ì¡° ëª¨ë¸ë“¤ (ì¶”ê°€ ìš”ì²­ì‚¬í•­)
    "SuperResolutionModel": ModelRequest(
    model_name="super_resolution_model",
    step_class="PostProcessingStep",
    step_priority=StepPriority.LOW,
    model_class="SRResNet",
    input_size=(512, 512),
    output_format="upscaled_image",
    
    checkpoint_patterns=[
        r".*srresnet.*\.pth$",
        r".*super.*resolution.*\.pth$",
        r".*sr.*x[2-4].*\.pth$"
    ],
    file_extensions=[".pth", ".pt"],
    size_range_mb=(20.0, 200.0),
    
    optimization_params={
        "scale_factor": 2,
        "num_features": 64,
        "num_blocks": 16,
        "precision": "fp16"
    },
    
    metadata={
        "description": "Super Resolution ì „ìš© ëª¨ë¸",
        "scale_factors": [2, 4],
        "architecture": "ResNet-based"
    }
),

    "DenoisingModel": ModelRequest(
    model_name="denoising_model", 
    step_class="PostProcessingStep",
    step_priority=StepPriority.LOW,
    model_class="DenoiseNet",
    input_size=(512, 512),
    output_format="denoised_image",
    
    checkpoint_patterns=[
        r".*denoise.*net.*\.pth$",
        r".*noise.*reduction.*\.pth$",
        r".*clean.*model.*\.pth$"
    ],
    file_extensions=[".pth", ".pt"], 
    size_range_mb=(10.0, 100.0),
    
    optimization_params={
        "num_features": 64,
        "noise_levels": [0.1, 0.3, 0.5, 0.7],
        "precision": "fp16"
    },
    
    metadata={
        "description": "ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸",
        "noise_types": ["gaussian", "poisson", "speckle"],
        "strength_levels": ["light", "medium", "strong"]
    }
)
    
    # Step 08: Quality Assessment
    "QualityAssessmentStep": ModelRequest(
        model_name="quality_assessment_clip",
        step_class="QualityAssessmentStep",
        step_priority=StepPriority.LOW,
        model_class="CLIPModel",
        input_size=(224, 224),
        output_format="quality_scores",
        
        checkpoint_patterns=[
            r".*clip.*vit.*\.bin$",
            r".*clip.*base.*\.bin$",
            r".*quality.*assessment.*\.pth$"
        ],
        file_extensions=[".bin", ".pth", ".pt"],
        size_range_mb=(100.0, 2000.0),
        
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.25,
            "enable_flash_attention": True
        },
        
        alternative_models=[
            "quality_assessment_combined"
        ],
        
        metadata={
            "description": "CLIP ê¸°ë°˜ í’ˆì§ˆ í‰ê°€",
            "assessment_metrics": ["quality", "realism", "consistency"],
            "quality_threshold": 0.7
        }
    )
}

# ==============================================
# ğŸ”¥ ìš”ì²­ ë¶„ì„ í•¨ìˆ˜ë“¤
# ==============================================

def get_step_request(step_name: str) -> Optional[ModelRequest]:
    """Stepë³„ ëª¨ë¸ ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.get(step_name)

def get_all_step_requests() -> Dict[str, ModelRequest]:
    """ëª¨ë“  Step ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.copy()

def get_checkpoint_patterns(step_name: str) -> List[str]:
    """Stepë³„ ì²´í¬í¬ì¸íŠ¸ íƒì§€ íŒ¨í„´ ë°˜í™˜"""
    request = get_step_request(step_name)
    return request.checkpoint_patterns if request else []

def get_model_config_for_step(step_name: str, detected_path: Path) -> Dict[str, Any]:
    """Step ìš”ì²­ì„ ModelLoader ì„¤ì •ìœ¼ë¡œ ë³€í™˜"""
    request = get_step_request(step_name)
    if not request:
        return {}
    
    return {
        "name": request.model_name,
        "model_type": request.model_class,
        "model_class": request.model_class,
        "checkpoint_path": str(detected_path),
        "device": request.device,
        "precision": request.precision,
        "input_size": request.input_size,
        "num_classes": request.num_classes,
        "optimization_params": request.optimization_params,
        "metadata": {
            **request.metadata,
            "step_name": step_name,
            "step_priority": request.step_priority.name,
            "auto_detected": True,
            "detection_time": time.time()
        }
    }

def validate_model_for_step(step_name: str, model_path: Path, size_mb: float) -> Dict[str, Any]:
    """Step ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ëª¨ë¸ ê²€ì¦"""
    request = get_step_request(step_name)
    if not request:
        return {"valid": False, "reason": f"Unknown step: {step_name}"}
    
    # í¬ê¸° ê²€ì¦
    min_size, max_size = request.size_range_mb
    if not (min_size <= size_mb <= max_size):
        return {
            "valid": False,
            "reason": f"Size {size_mb}MB not in range [{min_size}, {max_size}]"
        }
    
    # í™•ì¥ì ê²€ì¦
    if model_path.suffix.lower() not in request.file_extensions:
        return {
            "valid": False,
            "reason": f"Extension {model_path.suffix} not in {request.file_extensions}"
        }
    
    # íŒ¨í„´ ë§¤ì¹­
    import re
    model_name = model_path.name.lower()
    pattern_matched = False
    
    for pattern in request.checkpoint_patterns:
        if re.search(pattern, model_name):
            pattern_matched = True
            break
    
    if not pattern_matched:
        return {
            "valid": False,
            "reason": f"Name doesn't match patterns: {request.checkpoint_patterns}"
        }
    
    return {
        "valid": True,
        "confidence": 0.9,
        "step_priority": request.step_priority.name,
        "model_class": request.model_class
    }

def get_step_priorities() -> Dict[str, int]:
    """Stepë³„ ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
    return {
        step_name: request.step_priority.value
        for step_name, request in STEP_MODEL_REQUESTS.items()
    }

def get_steps_by_priority(priority: StepPriority) -> List[str]:
    """ìš°ì„ ìˆœìœ„ë³„ Step ëª©ë¡ ë°˜í™˜"""
    return [
        step_name for step_name, request in STEP_MODEL_REQUESTS.items()
        if request.step_priority == priority
    ]
# ===============================================
# ğŸ”¥ ëˆ„ë½ëœ í´ë˜ìŠ¤ë“¤ ì¶”ê°€ (step_model_requests.py ë§¨ ëì— ì¶”ê°€)
# ===============================================

class StepModelRequestAnalyzer:
    """Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë¶„ì„ê¸° - ModelLoader ì—°ë™ìš©"""
    
    @staticmethod
    def get_step_request_info(step_name: str) -> Optional[Dict[str, Any]]:
        """Stepë³„ ìš”ì²­ ì •ë³´ ë°˜í™˜ (ModelLoader í˜¸í™˜)"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        if not request:
            return None
        
        return {
            "model_name": request.model_name,
            "model_type": request.model_class,
            "input_size": request.input_size,
            "num_classes": request.num_classes,
            "device": request.device,
            "precision": request.precision,
            "checkpoint_patterns": request.checkpoint_patterns,
            "optimization_params": request.optimization_params,
            "step_priority": request.step_priority.value,
            "alternative_models": request.alternative_models,
            "metadata": request.metadata
        }
    
    @staticmethod
    def get_all_step_requirements() -> Dict[str, Any]:
        """ëª¨ë“  Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        return {
            step_name: StepModelRequestAnalyzer.get_step_request_info(step_name)
            for step_name in STEP_MODEL_REQUESTS.keys()
        }
    
    @staticmethod
    def get_critical_steps() -> List[str]:
        """ì¤‘ìš”í•œ Stepë“¤ ë°˜í™˜"""
        return [
            step_name for step_name, request in STEP_MODEL_REQUESTS.items()
            if request.step_priority == StepPriority.CRITICAL
        ]
    
    @staticmethod
    def get_model_for_step(step_name: str) -> Optional[str]:
        """Stepì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ëª… ë°˜í™˜"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        return request.model_name if request else None

# ModelLoader í˜¸í™˜ í•¨ìˆ˜ë“¤ ì¶”ê°€
def get_all_step_requirements() -> Dict[str, Any]:
    """ì „ì²´ Step ìš”êµ¬ì‚¬í•­ (ModelLoader í˜¸í™˜)"""
    return StepModelRequestAnalyzer.get_all_step_requirements()

def create_model_loader_config_from_detection(step_name: str, detected_models: List[Path]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ë¡œë¶€í„° ModelLoader ì„¤ì • ìƒì„±"""
    request = get_step_request(step_name)
    if not request or not detected_models:
        return {}
    
    # ê°€ì¥ í° ëª¨ë¸ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ ë©”ì¸ ëª¨ë¸)
    best_model = max(detected_models, key=lambda p: p.stat().st_size)
    
    return get_model_config_for_step(step_name, best_model)

logger.info(f"âœ… Step Model Requests v4.1 ë¡œë“œ ì™„ë£Œ - {len(STEP_MODEL_REQUESTS)}ê°œ Step ì •ì˜")
logger.info("ğŸ”§ StepModelRequestAnalyzer í´ë˜ìŠ¤ ì¶”ê°€ - ModelLoader í˜¸í™˜ì„± ì™„ë£Œ")
# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤
    'StepPriority',
    'ModelRequest',
    'StepModelRequestAnalyzer',  # ğŸ”¥ ì¶”ê°€

    # ë°ì´í„°
    'STEP_MODEL_REQUESTS',

    # í•¨ìˆ˜ë“¤
    'get_step_request',
    'get_all_step_requests',
    'get_checkpoint_patterns',
    'get_model_config_for_step',
    'validate_model_for_step',
    'get_step_priorities',
    'get_steps_by_priority',
    'get_all_step_requirements',  # ğŸ”¥ ì¶”ê°€
    'create_model_loader_config_from_detection'  # ğŸ”¥ ì¶”ê°€
]

logger.info(f"âœ… Step Model Requests ë¡œë“œ ì™„ë£Œ - {len(STEP_MODEL_REQUESTS)}ê°œ Step ì •ì˜")