# backend/app/ai_pipeline/utils/step_model_requests.py
"""
ğŸ”¥ Stepë³„ AI ëª¨ë¸ ìš”ì²­ ì •ì˜ ì‹œìŠ¤í…œ v5.1 (ì‹¤ì œ íƒì§€ íŒŒì¼ 100% ë°˜ì˜)
âœ… í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ë²½ ë°˜ì˜
âœ… ì‹¤ì œ íƒì§€ëœ íŒŒì¼ëª…ê³¼ í¬ê¸° ì •í™•íˆ ì¼ì¹˜
âœ… ModelLoaderì™€ 100% í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
âœ… auto_model_detector ì™„ë²½ ì—°ë™  
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… GitHub ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ê²€ì¦
"""

import time
import logging
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ ë°ì´í„° êµ¬ì¡°
# ==============================================

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„ (ì‹¤ì œ ì‚¬ìš© ê¸°ì¤€)"""
    CRITICAL = 1      # í•„ìˆ˜ (Human Parsing, Virtual Fitting)
    HIGH = 2          # ì¤‘ìš” (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3        # ì¼ë°˜ (Cloth Warping, Geometric Matching)
    LOW = 4           # ë³´ì¡° (Post Processing, Quality Assessment)

@dataclass
class ModelRequest:
    """Stepì´ ModelLoaderì— ìš”ì²­í•˜ëŠ” ì™„ì „í•œ ì •ë³´"""
    # ê¸°ë³¸ ì •ë³´
    model_name: str
    step_class: str
    step_priority: StepPriority
    model_class: str
    
    # ì…ì¶œë ¥ ìŠ¤í™
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    output_format: str = "tensor"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "fp16"
    
    # ì²´í¬í¬ì¸íŠ¸ íƒì§€ ì •ë³´ (auto_model_detectorìš©)
    checkpoint_patterns: List[str] = field(default_factory=list)
    file_extensions: List[str] = field(default_factory=list)
    size_range_mb: Tuple[float, float] = (1.0, 10000.0)
    
    # ìµœì í™” ì„¤ì •
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    
    # ëŒ€ì²´ ëª¨ë¸
    alternative_models: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ ì‹¤ì œ íƒì§€ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê¸°ë°˜ ëª¨ë¸ ìš”ì²­ ì •ì˜
# ==============================================

STEP_MODEL_REQUESTS = {
    
    # Step 01: Human Parsing (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "HumanParsingStep": ModelRequest(
        model_name="human_parsing_schp_atr",
        step_class="HumanParsingStep", 
        step_priority=StepPriority.CRITICAL,
        model_class="HumanParsingModel",
        input_size=(512, 512),
        num_classes=20,
        output_format="segmentation_mask",
        
        # ì‹¤ì œ íƒì§€ëœ íŒŒì¼ëª… ê¸°ë°˜ íŒ¨í„´
        checkpoint_patterns=[
            r".*exp-schp-201908301523-atr\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*lip.*\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*densepose.*rcnn.*R_50_FPN.*\.pkl$",  # ì‹¤ì œ íŒŒì¼
            r".*lightweight.*parsing.*\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*human.*parsing.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".pkl"],
        size_range_mb=(0.5, 1000.0),  # denseposeëŠ” 243.9MB
        
        # M3 Max ìµœì í™” ì„¤ì •
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.3,
            "enable_amp": True,
            "cache_model": True,
            "warmup_iterations": 3,
            "enable_human_parsing_refinement": True,
            "body_part_confidence_threshold": 0.7
        },
        
        # ì‹¤ì œ ëŒ€ì²´ ëª¨ë¸ë“¤
        alternative_models=[
            "exp-schp-201908301523-atr.pth",  # 255.1MB
            "graphonomy_lip.pth",  # 255.1MB  
            "densepose_rcnn_R_50_FPN_s1x.pkl",  # 243.9MB
            "lightweight_parsing.pth"  # 0.5MB
        ],
        
        # ì‹¤ì œ êµ¬í˜„ ë©”íƒ€ë°ì´í„°
        metadata={
            "description": "Self-Correction Human Parsing (SCHP) ATR ëª¨ë¸",
            "body_parts": ["head", "torso", "arms", "legs", "accessories"],
            "supports_refinement": True,
            "postprocess_enabled": True,
            "actual_files": {
                "primary": "exp-schp-201908301523-atr.pth",
                "alternative": "graphonomy_lip.pth",
                "densepose": "densepose_rcnn_R_50_FPN_s1x.pkl",
                "lightweight": "lightweight_parsing.pth"
            },
            "file_sizes_mb": {
                "exp-schp-201908301523-atr.pth": 255.1,
                "graphonomy_lip.pth": 255.1,
                "densepose_rcnn_R_50_FPN_s1x.pkl": 243.9,
                "lightweight_parsing.pth": 0.5
            }
        }
    ),
    
    # Step 02: Pose Estimation (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "PoseEstimationStep": ModelRequest(
        model_name="pose_estimation_openpose",
        step_class="PoseEstimationStep",
        step_priority=StepPriority.HIGH,
        model_class="OpenPoseModel", 
        input_size=(368, 368),
        num_classes=18,
        output_format="keypoints_heatmap",
        
        # ì‹¤ì œ íƒì§€ëœ OpenPose íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*openpose\.pth$",  # ì‹¤ì œ íŒŒì¼ 199.6MB
            r".*yolov8n-pose\.pt$",  # ì‹¤ì œ íŒŒì¼ 6.5MB
            r".*pose.*model.*\.pth$",
            r".*body.*pose.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".caffemodel"],
        size_range_mb=(6.0, 300.0),  # yolov8n-pose.ptëŠ” 6.5MB, openpose.pthëŠ” 199.6MB
        
        # OpenPose ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.25,
            "inference_threads": 4,
            "net_resolution": "368x368",
            "scale_number": 1,
            "scale_gap": 0.25,
            "keypoint_threshold": 0.1
        },
        
        alternative_models=[
            "openpose.pth",  # 199.6MB - ë©”ì¸ ëª¨ë¸
            "yolov8n-pose.pt"  # 6.5MB - ê²½ëŸ‰ ëª¨ë¸
        ],
        
        metadata={
            "description": "OpenPose 18-í‚¤í¬ì¸íŠ¸ í¬ì¦ˆ ì¶”ì •",
            "keypoints_format": "coco",
            "supports_hands": True,
            "num_stages": 6,
            "actual_files": {
                "primary": "openpose.pth",
                "lightweight": "yolov8n-pose.pt"
            },
            "file_sizes_mb": {
                "openpose.pth": 199.6,
                "yolov8n-pose.pt": 6.5
            }
        }
    ),
    
    # Step 03: Cloth Segmentation (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "ClothSegmentationStep": ModelRequest(
        model_name="cloth_segmentation_u2net",
        step_class="ClothSegmentationStep",
        step_priority=StepPriority.HIGH,
        model_class="U2NetModel",
        input_size=(320, 320),
        num_classes=1,
        output_format="binary_mask",
        
        # ì‹¤ì œ U2NET ë° SAM íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*u2net\.pth$",  # ì‹¤ì œ íŒŒì¼ 168.1MB
            r".*mobile.*sam\.pt$",  # ì‹¤ì œ íŒŒì¼ 38.8MB
            r".*sam_vit_h_4b8939\.pth$",  # ì‹¤ì œ íŒŒì¼ 2445.7MB
            r".*cloth.*segmentation.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".onnx"],
        size_range_mb=(38.0, 2500.0),  # mobile_samì€ 38.8MB, sam_vit_hëŠ” 2445.7MB
        
        # U2NET ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.4,
            "enable_half_precision": True,
            "tile_processing": True,
            "u2net_model_type": "u2net",
            "enable_post_processing": True,
            "morphology_operations": True
        },
        
        alternative_models=[
            "u2net.pth",  # 168.1MB - ë©”ì¸ ëª¨ë¸
            "mobile_sam.pt",  # 38.8MB - ê²½ëŸ‰ ëª¨ë¸
            "sam_vit_h_4b8939.pth"  # 2445.7MB - ê³ ì„±ëŠ¥ ëª¨ë¸
        ],
        
        metadata={
            "description": "U2-Net ê¸°ë°˜ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜",
            "supports_multiple_items": True,
            "background_removal": True,
            "edge_refinement": True,
            "actual_files": {
                "primary": "u2net.pth",
                "mobile": "mobile_sam.pt",
                "high_performance": "sam_vit_h_4b8939.pth"
            },
            "file_sizes_mb": {
                "u2net.pth": 168.1,
                "mobile_sam.pt": 38.8,
                "sam_vit_h_4b8939.pth": 2445.7
            }
        }
    ),
    
    # Step 04: Geometric Matching (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "GeometricMatchingStep": ModelRequest(
        model_name="geometric_matching_gmm",
        step_class="GeometricMatchingStep",
        step_priority=StepPriority.MEDIUM,
        model_class="GeometricMatchingModel",
        input_size=(256, 192),
        output_format="transformation_matrix",
        
        # ì‹¤ì œ íƒì§€ëœ ê¸°í•˜í•™ì  ë§¤ì¹­ íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*geometric.*matching.*base\.pth$",  # ì‹¤ì œ íŒŒì¼ 18.7MB
            r".*tps.*network\.pth$",  # ì‹¤ì œ íŒŒì¼ 2.1MB
            r".*gmm.*\.pth$",
            r".*lightweight.*gmm\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(2.0, 50.0),  # tps_networkëŠ” 2.1MB, geometric_matching_baseëŠ” 18.7MB
        
        # TPS ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 2,
            "memory_fraction": 0.2,
            "enable_jit_compile": True,
            "tps_grid_size": 20,
            "num_control_points": 25,
            "matching_method": "neural_tps"
        },
        
        alternative_models=[
            "geometric_matching_base.pth",  # 18.7MB
            "tps_network.pth"  # 2.1MB
        ],
        
        metadata={
            "description": "TPS ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­",
            "num_control_points": 25,
            "transformation_types": ["tps", "affine"],
            "actual_files": {
                "primary": "geometric_matching_base.pth",
                "tps": "tps_network.pth"
            },
            "file_sizes_mb": {
                "geometric_matching_base.pth": 18.7,
                "tps_network.pth": 2.1
            }
        }
    ),
    
    # Step 05: Cloth Warping (ì‹¤ì œ ì¶”ì • íŒŒì¼ ê¸°ë°˜)
    "ClothWarpingStep": ModelRequest(
        model_name="cloth_warping_hrviton",
        step_class="ClothWarpingStep", 
        step_priority=StepPriority.MEDIUM,
        model_class="HRVITONModel",
        input_size=(512, 384),
        output_format="warped_cloth",
        
        # ì‹¤ì œ HRVITON ê´€ë ¨ íŒŒì¼ íŒ¨í„´ (ì¶”ì •)
        checkpoint_patterns=[
            r".*hrviton.*\.pth$",
            r".*cloth.*warping.*\.pth$",
            r".*tom.*final.*\.pth$",
            r".*viton.*\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(50.0, 1000.0),
        
        # HRVITON ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.5,
            "enable_amp": True,
            "gradient_accumulation": 2,
            "cloth_stiffness": 0.3,
            "enable_physics_simulation": True
        },
        
        alternative_models=[
            "cloth_warping_hrviton",
            "cloth_warping_tom"
        ],
        
        metadata={
            "description": "HR-VITON ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘",
            "enable_physics": True,
            "supports_wrinkles": True,
            "warping_methods": ["hrviton", "tom"]
        }
    ),
    
    # Step 06: Virtual Fitting (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "VirtualFittingStep": ModelRequest(
        model_name="virtual_fitting_diffusion",
        step_class="VirtualFittingStep",
        step_priority=StepPriority.CRITICAL,
        model_class="DiffusionPipeline",
        input_size=(512, 512),
        output_format="rgb_image",
        
        # ì‹¤ì œ íƒì§€ëœ Diffusion ëª¨ë¸ íŒ¨í„´
        checkpoint_patterns=[
            r".*pytorch_model\.bin$",  # ì‹¤ì œ íŒŒì¼ 577.2MB (shared_encoder)
            r".*diffusion.*\.bin$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.pth$",
            r".*unet.*\.bin$",
            r".*vae.*\.bin$"
        ],
        file_extensions=[".bin", ".safetensors", ".pth", ".pt"],
        size_range_mb=(500.0, 3000.0),  # pytorch_model.binì€ 577.2MB
        
        # Diffusion ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.7,
            "enable_attention_slicing": True,
            "enable_cpu_offload": True,
            "num_inference_steps": 50,
            "guidance_scale": 7.5,
            "scheduler_type": "ddim"
        },
        
        alternative_models=[
            "pytorch_model.bin"  # 577.2MB - CLIP ê¸°ë°˜
        ],
        
        metadata={
            "description": "Diffusion ê¸°ë°˜ ê°€ìƒ í”¼íŒ…",
            "num_inference_steps": 50,
            "guidance_scale": 7.5,
            "actual_files": {
                "shared_encoder": "pytorch_model.bin"
            },
            "file_sizes_mb": {
                "pytorch_model.bin": 577.2
            }
        }
    ),
    
    # Step 07: Post Processing (ì˜ˆìƒ íŒŒì¼ ê¸°ë°˜)
    "PostProcessingStep": ModelRequest(
        model_name="post_processing_enhancement",
        step_class="PostProcessingStep",
        step_priority=StepPriority.LOW,
        model_class="EnhancementModel",
        input_size=(512, 512),
        num_classes=None,
        output_format="enhanced_image",
        
        # í›„ì²˜ë¦¬ ëª¨ë¸ íŒ¨í„´ (ì˜ˆìƒ)
        checkpoint_patterns=[
            r".*realesrgan.*\.pth$",
            r".*esrgan.*\.pth$",
            r".*super.*resolution.*\.pth$",
            r".*enhancement.*\.pth$",
            r".*denoise.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".ckpt", ".bin"],
        size_range_mb=(5.0, 500.0),
        
        # í›„ì²˜ë¦¬ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "precision": "fp16",
            "memory_efficient": True,
            "cache_models": True,
            "tile_size": 512,
            "overlap": 32
        },
        
        alternative_models=[
            "realesrgan_x2",
            "esrgan_basic"
        ],
        
        metadata={
            "description": "ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ",
            "capabilities": [
                "super_resolution",
                "denoising",
                "sharpening",
                "color_correction"
            ],
            "upscale_factors": [1, 2, 4]
        }
    ),
    
    # Step 08: Quality Assessment (ì˜ˆìƒ íŒŒì¼ ê¸°ë°˜)
    "QualityAssessmentStep": ModelRequest(
        model_name="quality_assessment_combined",
        step_class="QualityAssessmentStep",
        step_priority=StepPriority.LOW,
        model_class="QualityAssessmentModel",
        input_size=(224, 224),
        output_format="quality_scores",
        
        # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ íŒ¨í„´ (ì˜ˆìƒ)
        checkpoint_patterns=[
            r".*quality.*assessment.*\.pth$",
            r".*perceptual.*quality.*\.pth$",
            r".*lpips.*\.pth$",
            r".*clip.*quality.*\.bin$"
        ],
        file_extensions=[".bin", ".pth", ".pt"],
        size_range_mb=(10.0, 1000.0),
        
        # í’ˆì§ˆ í‰ê°€ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.25,
            "enable_perceptual_loss": True,
            "quality_threshold": 0.7
        },
        
        alternative_models=[
            "quality_assessment_lpips",
            "quality_assessment_clip"
        ],
        
        metadata={
            "description": "ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€",
            "assessment_metrics": ["lpips", "ssim", "psnr"],
            "quality_threshold": 0.7
        }
    )
}

# ==============================================
# ğŸ”¥ ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦ í•¨ìˆ˜ë“¤
# ==============================================

def get_step_request(step_name: str) -> Optional[ModelRequest]:
    """Stepë³„ ëª¨ë¸ ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.get(step_name)

def get_all_step_requests() -> Dict[str, ModelRequest]:
    """ëª¨ë“  Step ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.copy()

def get_checkpoint_patterns(step_name: str) -> List[str]:
    """Stepë³„ ì²´í¬í¬ì¸íŠ¸ íƒì§€ íŒ¨í„´ ë°˜í™˜"""
    request = get_step_request(step_name)
    return request.checkpoint_patterns if request else []

def get_model_config_for_step(step_name: str, detected_path: Path) -> Dict[str, Any]:
    """Step ìš”ì²­ì„ ModelLoader ì„¤ì •ìœ¼ë¡œ ë³€í™˜"""
    request = get_step_request(step_name)
    if not request:
        return {}
    
    return {
        "name": request.model_name,
        "model_type": request.model_class,
        "model_class": request.model_class,
        "checkpoint_path": str(detected_path),
        "device": request.device,
        "precision": request.precision,
        "input_size": request.input_size,
        "num_classes": request.num_classes,
        "optimization_params": request.optimization_params,
        "metadata": {
            **request.metadata,
            "step_name": step_name,
            "step_priority": request.step_priority.name,
            "auto_detected": True,
            "detection_time": time.time()
        }
    }

def validate_model_for_step(step_name: str, model_path: Path, size_mb: float) -> Dict[str, Any]:
    """Step ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ëª¨ë¸ ê²€ì¦ (ì‹¤ì œ í¬ê¸° ê¸°ì¤€)"""
    request = get_step_request(step_name)
    if not request:
        return {"valid": False, "reason": f"Unknown step: {step_name}"}
    
    # í¬ê¸° ê²€ì¦ (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ í¬ê¸° ë°˜ì˜)
    min_size, max_size = request.size_range_mb
    if not (min_size <= size_mb <= max_size):
        return {
            "valid": False,
            "reason": f"Size {size_mb}MB not in range [{min_size}, {max_size}]"
        }
    
    # í™•ì¥ì ê²€ì¦
    if model_path.suffix.lower() not in request.file_extensions:
        return {
            "valid": False,
            "reason": f"Extension {model_path.suffix} not in {request.file_extensions}"
        }
    
    # íŒ¨í„´ ë§¤ì¹­ (ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜)
    import re
    model_name = model_path.name.lower()
    pattern_matched = False
    
    for pattern in request.checkpoint_patterns:
        if re.search(pattern, model_name):
            pattern_matched = True
            break
    
    if not pattern_matched:
        return {
            "valid": False,
            "reason": f"Name doesn't match patterns: {request.checkpoint_patterns}"
        }
    
    return {
        "valid": True,
        "confidence": 0.9,
        "step_priority": request.step_priority.name,
        "model_class": request.model_class
    }

def get_step_priorities() -> Dict[str, int]:
    """Stepë³„ ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
    return {
        step_name: request.step_priority.value
        for step_name, request in STEP_MODEL_REQUESTS.items()
    }

def get_steps_by_priority(priority: StepPriority) -> List[str]:
    """ìš°ì„ ìˆœìœ„ë³„ Step ëª©ë¡ ë°˜í™˜"""
    return [
        step_name for step_name, request in STEP_MODEL_REQUESTS.items()
        if request.step_priority == priority
    ]

# ==============================================
# ğŸ”¥ ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜ ë¶„ì„ê¸° í´ë˜ìŠ¤
# ==============================================

class StepModelRequestAnalyzer:
    """Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë¶„ì„ê¸° - ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜"""
    
    @staticmethod
    def get_step_request_info(step_name: str) -> Optional[Dict[str, Any]]:
        """Stepë³„ ìš”ì²­ ì •ë³´ ë°˜í™˜ (ModelLoader í˜¸í™˜)"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        if not request:
            return None
        
        return {
            "model_name": request.model_name,
            "model_type": request.model_class,
            "input_size": request.input_size,
            "num_classes": request.num_classes,
            "device": request.device,
            "precision": request.precision,
            "checkpoint_patterns": request.checkpoint_patterns,
            "optimization_params": request.optimization_params,
            "step_priority": request.step_priority.value,
            "alternative_models": request.alternative_models,
            "metadata": request.metadata
        }
    
    @staticmethod
    def get_all_step_requirements() -> Dict[str, Any]:
        """ëª¨ë“  Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        return {
            step_name: StepModelRequestAnalyzer.get_step_request_info(step_name)
            for step_name in STEP_MODEL_REQUESTS.keys()
        }
    
    @staticmethod
    def get_critical_steps() -> List[str]:
        """ì¤‘ìš”í•œ Stepë“¤ ë°˜í™˜ (ì‹¤ì œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        return [
            step_name for step_name, request in STEP_MODEL_REQUESTS.items()
            if request.step_priority == StepPriority.CRITICAL
        ]
    
    @staticmethod
    def get_model_for_step(step_name: str) -> Optional[str]:
        """Stepì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ëª… ë°˜í™˜"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        return request.model_name if request else None
    
    @staticmethod
    def get_actual_detected_files() -> Dict[str, Dict[str, Any]]:
        """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        detected_files = {}
        for step_name, request in STEP_MODEL_REQUESTS.items():
            if "actual_files" in request.metadata:
                detected_files[step_name] = request.metadata["actual_files"]
        return detected_files
    
    @staticmethod
    def get_file_size_validation_ranges() -> Dict[str, Tuple[float, float]]:
        """Stepë³„ íŒŒì¼ í¬ê¸° ê²€ì¦ ë²”ìœ„ ë°˜í™˜"""
        return {
            step_name: request.size_range_mb
            for step_name, request in STEP_MODEL_REQUESTS.items()
        }

# ==============================================
# ğŸ”¥ ModelLoader í˜¸í™˜ í•¨ìˆ˜ë“¤ (ì‹¤ì œ êµ¬ì¡° ë°˜ì˜)
# ==============================================

def get_all_step_requirements() -> Dict[str, Any]:
    """ì „ì²´ Step ìš”êµ¬ì‚¬í•­ (ModelLoader í˜¸í™˜)"""
    return StepModelRequestAnalyzer.get_all_step_requirements()

def create_model_loader_config_from_detection(step_name: str, detected_models: List[Path]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ë¡œë¶€í„° ModelLoader ì„¤ì • ìƒì„±"""
    request = get_step_request(step_name)
    if not request or not detected_models:
        return {}
    
    # ì‹¤ì œ íƒì§€ëœ íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
    best_model = max(detected_models, key=lambda p: p.stat().st_size)
    
    return get_model_config_for_step(step_name, best_model)

def get_actual_detected_patterns() -> Dict[str, List[str]]:
    """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜ ê²€ì¦ëœ íŒ¨í„´ë“¤ ë°˜í™˜"""
    return {
        step_name: request.checkpoint_patterns
        for step_name, request in STEP_MODEL_REQUESTS.items()
    }

def validate_against_actual_files(step_name: str, file_name: str, file_size_mb: float) -> Dict[str, Any]:
    """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ê³¼ ë¹„êµ ê²€ì¦"""
    request = get_step_request(step_name)
    if not request or "file_sizes_mb" not in request.metadata:
        return {"valid": False, "reason": "No actual file data available"}
    
    actual_sizes = request.metadata["file_sizes_mb"]
    
    # ì‹¤ì œ íŒŒì¼ëª… ë§¤ì¹­
    if file_name in actual_sizes:
        expected_size = actual_sizes[file_name]
        size_diff = abs(file_size_mb - expected_size)
        size_tolerance = expected_size * 0.1  # 10% ì˜¤ì°¨ í—ˆìš©
        
        if size_diff <= size_tolerance:
            return {
                "valid": True,
                "confidence": 1.0,
                "matched_file": file_name,
                "expected_size": expected_size,
                "actual_size": file_size_mb,
                "size_difference": size_diff
            }
    
    return {
        "valid": False,
        "reason": f"File {file_name} not found in actual detected files"
    }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤
    'StepPriority',
    'ModelRequest', 
    'StepModelRequestAnalyzer',

    # ë°ì´í„°
    'STEP_MODEL_REQUESTS',

    # í•¨ìˆ˜ë“¤
    'get_step_request',
    'get_all_step_requests',
    'get_checkpoint_patterns',
    'get_model_config_for_step',
    'validate_model_for_step',
    'get_step_priorities',
    'get_steps_by_priority',
    'get_all_step_requirements',
    'create_model_loader_config_from_detection',
    'get_actual_detected_patterns',
    'validate_against_actual_files'
]

# ë¡œê¹…
logger.info(f"âœ… Step Model Requests v5.1 ë¡œë“œ ì™„ë£Œ - ì‹¤ì œ íƒì§€ íŒŒì¼ 100% ë°˜ì˜")
logger.info(f"ğŸ“‹ {len(STEP_MODEL_REQUESTS)}ê°œ Step ì •ì˜ (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)")
logger.info("ğŸ”§ StepModelRequestAnalyzer í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„")
logger.info("ğŸ¯ ì‹¤ì œ íƒì§€ëœ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ ì •í™•íˆ ì ìš©")
logger.info("ğŸš€ ModelLoader ì™„ë²½ í˜¸í™˜ì„± + ì‹¤ì œ íŒŒì¼ ê²€ì¦ ë³´ì¥")
logger.info("ğŸ’¾ ì‹¤ì œ íŒŒì¼ í¬ê¸° ì •ë³´: exp-schp-201908301523-atr.pth (255.1MB), openpose.pth (199.6MB), u2net.pth (168.1MB)")