# backend/app/ai_pipeline/utils/step_model_requests.py
"""
ğŸ”¥ Stepë³„ AI ëª¨ë¸ ìš”ì²­ ì •ì˜ ì‹œìŠ¤í…œ v6.0 (ì™„ì „í•œ êµ¬í˜„)
âœ… í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ë²½ ë°˜ì˜
âœ… ì‹¤ì œ íƒì§€ëœ íŒŒì¼ëª…ê³¼ í¬ê¸° ì •í™•íˆ ì¼ì¹˜
âœ… ModelLoaderì™€ 100% í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
âœ… auto_model_detector ì™„ë²½ ì—°ë™  
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… GitHub ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ê²€ì¦
âœ… StepModelRequestAnalyzer ì™„ì „ êµ¬í˜„
âœ… register_step_requirements ì§€ì›
âœ… ëª¨ë“  ëˆ„ë½ëœ í•¨ìˆ˜/í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„
"""

import os
import sys
import time
import logging
import asyncio
import threading
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import weakref
import gc

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ ë°ì´í„° êµ¬ì¡°
# ==============================================

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„ (ì‹¤ì œ ì‚¬ìš© ê¸°ì¤€)"""
    CRITICAL = 1      # í•„ìˆ˜ (Human Parsing, Virtual Fitting)
    HIGH = 2          # ì¤‘ìš” (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3        # ì¼ë°˜ (Cloth Warping, Geometric Matching)
    LOW = 4           # ë³´ì¡° (Post Processing, Quality Assessment)

@dataclass
class ModelRequest:
    """Stepì´ ModelLoaderì— ìš”ì²­í•˜ëŠ” ì™„ì „í•œ ì •ë³´"""
    # ê¸°ë³¸ ì •ë³´
    model_name: str
    step_class: str
    step_priority: StepPriority
    model_class: str
    
    # ì…ì¶œë ¥ ìŠ¤í™
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    output_format: str = "tensor"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    precision: str = "fp16"
    
    # ì²´í¬í¬ì¸íŠ¸ íƒì§€ ì •ë³´ (auto_model_detectorìš©)
    checkpoint_patterns: List[str] = field(default_factory=list)
    file_extensions: List[str] = field(default_factory=list)
    size_range_mb: Tuple[float, float] = (1.0, 10000.0)
    
    # ìµœì í™” ì„¤ì •
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    
    # ëŒ€ì²´ ëª¨ë¸
    alternative_models: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "model_name": self.model_name,
            "step_class": self.step_class,
            "step_priority": self.step_priority.value,
            "model_class": self.model_class,
            "input_size": self.input_size,
            "num_classes": self.num_classes,
            "output_format": self.output_format,
            "device": self.device,
            "precision": self.precision,
            "checkpoint_patterns": self.checkpoint_patterns,
            "file_extensions": self.file_extensions,
            "size_range_mb": self.size_range_mb,
            "optimization_params": self.optimization_params,
            "alternative_models": self.alternative_models,
            "metadata": self.metadata
        }

# ==============================================
# ğŸ”¥ ì‹¤ì œ íƒì§€ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê¸°ë°˜ ëª¨ë¸ ìš”ì²­ ì •ì˜
# ==============================================

STEP_MODEL_REQUESTS = {
    
    # Step 01: Human Parsing (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "HumanParsingStep": ModelRequest(
        model_name="human_parsing_schp_atr",
        step_class="HumanParsingStep", 
        step_priority=StepPriority.CRITICAL,
        model_class="HumanParsingModel",
        input_size=(512, 512),
        num_classes=20,
        output_format="segmentation_mask",
        
        # ì‹¤ì œ íƒì§€ëœ íŒŒì¼ëª… ê¸°ë°˜ íŒ¨í„´
        checkpoint_patterns=[
            r".*exp-schp-201908301523-atr\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*schp.*atr.*\.pth$",
            r".*graphonomy.*lip.*\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*densepose.*rcnn.*R_50_FPN.*\.pkl$",  # ì‹¤ì œ íŒŒì¼
            r".*lightweight.*parsing.*\.pth$",  # ì‹¤ì œ íŒŒì¼
            r".*human.*parsing.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".pkl"],
        size_range_mb=(0.5, 1000.0),  # denseposeëŠ” 243.9MB
        
        # M3 Max ìµœì í™” ì„¤ì •
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.3,
            "enable_amp": True,
            "cache_model": True,
            "warmup_iterations": 3,
            "enable_human_parsing_refinement": True,
            "body_part_confidence_threshold": 0.7
        },
        
        # ì‹¤ì œ ëŒ€ì²´ ëª¨ë¸ë“¤
        alternative_models=[
            "exp-schp-201908301523-atr.pth",  # 255.1MB
            "graphonomy_lip.pth",  # 255.1MB  
            "densepose_rcnn_R_50_FPN_s1x.pkl",  # 243.9MB
            "lightweight_parsing.pth"  # 0.5MB
        ],
        
        # ì‹¤ì œ êµ¬í˜„ ë©”íƒ€ë°ì´í„°
        metadata={
            "description": "Self-Correction Human Parsing (SCHP) ATR ëª¨ë¸",
            "body_parts": ["head", "torso", "arms", "legs", "accessories"],
            "supports_refinement": True,
            "postprocess_enabled": True,
            "actual_files": {
                "primary": "exp-schp-201908301523-atr.pth",
                "alternative": "graphonomy_lip.pth",
                "densepose": "densepose_rcnn_R_50_FPN_s1x.pkl",
                "lightweight": "lightweight_parsing.pth"
            },
            "file_sizes_mb": {
                "exp-schp-201908301523-atr.pth": 255.1,
                "graphonomy_lip.pth": 255.1,
                "densepose_rcnn_R_50_FPN_s1x.pkl": 243.9,
                "lightweight_parsing.pth": 0.5
            }
        }
    ),
    
    # Step 02: Pose Estimation (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "PoseEstimationStep": ModelRequest(
        model_name="pose_estimation_openpose",
        step_class="PoseEstimationStep",
        step_priority=StepPriority.HIGH,
        model_class="OpenPoseModel", 
        input_size=(368, 368),
        num_classes=18,
        output_format="keypoints_heatmap",
        
        # ì‹¤ì œ íƒì§€ëœ OpenPose íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*openpose\.pth$",  # ì‹¤ì œ íŒŒì¼ 199.6MB
            r".*yolov8n-pose\.pt$",  # ì‹¤ì œ íŒŒì¼ 6.5MB
            r".*pose.*model.*\.pth$",
            r".*body.*pose.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".caffemodel"],
        size_range_mb=(6.0, 300.0),  # yolov8n-pose.ptëŠ” 6.5MB, openpose.pthëŠ” 199.6MB
        
        # OpenPose ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.25,
            "inference_threads": 4,
            "net_resolution": "368x368",
            "scale_number": 1,
            "scale_gap": 0.25,
            "keypoint_threshold": 0.1
        },
        
        alternative_models=[
            "openpose.pth",  # 199.6MB - ë©”ì¸ ëª¨ë¸
            "yolov8n-pose.pt"  # 6.5MB - ê²½ëŸ‰ ëª¨ë¸
        ],
        
        metadata={
            "description": "OpenPose 18-í‚¤í¬ì¸íŠ¸ í¬ì¦ˆ ì¶”ì •",
            "keypoints_format": "coco",
            "supports_hands": True,
            "num_stages": 6,
            "actual_files": {
                "primary": "openpose.pth",
                "lightweight": "yolov8n-pose.pt"
            },
            "file_sizes_mb": {
                "openpose.pth": 199.6,
                "yolov8n-pose.pt": 6.5
            }
        }
    ),
    
    # Step 03: Cloth Segmentation (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "ClothSegmentationStep": ModelRequest(
        model_name="cloth_segmentation_u2net",
        step_class="ClothSegmentationStep",
        step_priority=StepPriority.HIGH,
        model_class="U2NetModel",
        input_size=(320, 320),
        num_classes=1,
        output_format="binary_mask",
        
        # ì‹¤ì œ U2NET ë° SAM íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*u2net\.pth$",  # ì‹¤ì œ íŒŒì¼ 168.1MB
            r".*mobile.*sam\.pt$",  # ì‹¤ì œ íŒŒì¼ 38.8MB
            r".*sam_vit_h_4b8939\.pth$",  # ì‹¤ì œ íŒŒì¼ 2445.7MB
            r".*cloth.*segmentation.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".onnx"],
        size_range_mb=(38.0, 2500.0),  # mobile_samì€ 38.8MB, sam_vit_hëŠ” 2445.7MB
        
        # U2NET ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.4,
            "enable_half_precision": True,
            "tile_processing": True,
            "u2net_model_type": "u2net",
            "enable_post_processing": True,
            "morphology_operations": True
        },
        
        alternative_models=[
            "u2net.pth",  # 168.1MB - ë©”ì¸ ëª¨ë¸
            "mobile_sam.pt",  # 38.8MB - ê²½ëŸ‰ ëª¨ë¸
            "sam_vit_h_4b8939.pth"  # 2445.7MB - ê³ ì„±ëŠ¥ ëª¨ë¸
        ],
        
        metadata={
            "description": "U2-Net ê¸°ë°˜ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜",
            "supports_multiple_items": True,
            "background_removal": True,
            "edge_refinement": True,
            "actual_files": {
                "primary": "u2net.pth",
                "mobile": "mobile_sam.pt",
                "high_performance": "sam_vit_h_4b8939.pth"
            },
            "file_sizes_mb": {
                "u2net.pth": 168.1,
                "mobile_sam.pt": 38.8,
                "sam_vit_h_4b8939.pth": 2445.7
            }
        }
    ),
    
    # Step 04: Geometric Matching (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "GeometricMatchingStep": ModelRequest(
        model_name="geometric_matching_gmm",
        step_class="GeometricMatchingStep",
        step_priority=StepPriority.MEDIUM,
        model_class="GeometricMatchingModel",
        input_size=(256, 192),
        output_format="transformation_matrix",
        
        # ì‹¤ì œ íƒì§€ëœ ê¸°í•˜í•™ì  ë§¤ì¹­ íŒŒì¼ íŒ¨í„´
        checkpoint_patterns=[
            r".*geometric.*matching.*base\.pth$",  # ì‹¤ì œ íŒŒì¼ 18.7MB
            r".*tps.*network\.pth$",  # ì‹¤ì œ íŒŒì¼ 2.1MB
            r".*gmm.*\.pth$",
            r".*lightweight.*gmm\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(2.0, 50.0),  # tps_networkëŠ” 2.1MB, geometric_matching_baseëŠ” 18.7MB
        
        # TPS ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 2,
            "memory_fraction": 0.2,
            "enable_jit_compile": True,
            "tps_grid_size": 20,
            "num_control_points": 25,
            "matching_method": "neural_tps"
        },
        
        alternative_models=[
            "geometric_matching_base.pth",  # 18.7MB
            "tps_network.pth"  # 2.1MB
        ],
        
        metadata={
            "description": "TPS ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­",
            "num_control_points": 25,
            "transformation_types": ["tps", "affine"],
            "actual_files": {
                "primary": "geometric_matching_base.pth",
                "tps": "tps_network.pth"
            },
            "file_sizes_mb": {
                "geometric_matching_base.pth": 18.7,
                "tps_network.pth": 2.1
            }
        }
    ),
    
    # Step 05: Cloth Warping (ì‹¤ì œ ì¶”ì • íŒŒì¼ ê¸°ë°˜)
    "ClothWarpingStep": ModelRequest(
        model_name="cloth_warping_hrviton",
        step_class="ClothWarpingStep", 
        step_priority=StepPriority.MEDIUM,
        model_class="HRVITONModel",
        input_size=(512, 384),
        output_format="warped_cloth",
        
        # ì‹¤ì œ HRVITON ê´€ë ¨ íŒŒì¼ íŒ¨í„´ (ì¶”ì •)
        checkpoint_patterns=[
            r".*hrviton.*\.pth$",
            r".*cloth.*warping.*\.pth$",
            r".*tom.*final.*\.pth$",
            r".*viton.*\.pth$"
        ],
        file_extensions=[".pth", ".pt"],
        size_range_mb=(50.0, 1000.0),
        
        # HRVITON ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.5,
            "enable_amp": True,
            "gradient_accumulation": 2,
            "cloth_stiffness": 0.3,
            "enable_physics_simulation": True
        },
        
        alternative_models=[
            "cloth_warping_hrviton",
            "cloth_warping_tom"
        ],
        
        metadata={
            "description": "HR-VITON ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘",
            "enable_physics": True,
            "supports_wrinkles": True,
            "warping_methods": ["hrviton", "tom"]
        }
    ),
    
    # Step 06: Virtual Fitting (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜)
    "VirtualFittingStep": ModelRequest(
        model_name="virtual_fitting_diffusion",
        step_class="VirtualFittingStep",
        step_priority=StepPriority.CRITICAL,
        model_class="DiffusionPipeline",
        input_size=(512, 512),
        output_format="rgb_image",
        
        # ì‹¤ì œ íƒì§€ëœ Diffusion ëª¨ë¸ íŒ¨í„´
        checkpoint_patterns=[
            r".*pytorch_model\.bin$",  # ì‹¤ì œ íŒŒì¼ 577.2MB (shared_encoder)
            r".*diffusion.*\.bin$",
            r".*stable.*diffusion.*\.safetensors$",
            r".*ootdiffusion.*\.pth$",
            r".*unet.*\.bin$",
            r".*vae.*\.bin$"
        ],
        file_extensions=[".bin", ".safetensors", ".pth", ".pt"],
        size_range_mb=(500.0, 3000.0),  # pytorch_model.binì€ 577.2MB
        
        # Diffusion ì‹¤ì œ ìµœì í™”
        optimization_params={
            "batch_size": 1,
            "memory_fraction": 0.7,
            "enable_attention_slicing": True,
            "enable_cpu_offload": True,
            "num_inference_steps": 50,
            "guidance_scale": 7.5,
            "scheduler_type": "ddim"
        },
        
        alternative_models=[
            "pytorch_model.bin"  # 577.2MB - CLIP ê¸°ë°˜
        ],
        
        metadata={
            "description": "Diffusion ê¸°ë°˜ ê°€ìƒ í”¼íŒ…",
            "num_inference_steps": 50,
            "guidance_scale": 7.5,
            "actual_files": {
                "shared_encoder": "pytorch_model.bin"
            },
            "file_sizes_mb": {
                "pytorch_model.bin": 577.2
            }
        }
    ),
    
    # Step 07: Post Processing (ì˜ˆìƒ íŒŒì¼ ê¸°ë°˜)
    "PostProcessingStep": ModelRequest(
        model_name="post_processing_enhancement",
        step_class="PostProcessingStep",
        step_priority=StepPriority.LOW,
        model_class="EnhancementModel",
        input_size=(512, 512),
        num_classes=None,
        output_format="enhanced_image",
        
        # í›„ì²˜ë¦¬ ëª¨ë¸ íŒ¨í„´ (ì˜ˆìƒ)
        checkpoint_patterns=[
            r".*realesrgan.*\.pth$",
            r".*esrgan.*\.pth$",
            r".*super.*resolution.*\.pth$",
            r".*enhancement.*\.pth$",
            r".*denoise.*\.pth$"
        ],
        file_extensions=[".pth", ".pt", ".ckpt", ".bin"],
        size_range_mb=(5.0, 500.0),
        
        # í›„ì²˜ë¦¬ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "precision": "fp16",
            "memory_efficient": True,
            "cache_models": True,
            "tile_size": 512,
            "overlap": 32
        },
        
        alternative_models=[
            "realesrgan_x2",
            "esrgan_basic"
        ],
        
        metadata={
            "description": "ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ",
            "capabilities": [
                "super_resolution",
                "denoising",
                "sharpening",
                "color_correction"
            ],
            "upscale_factors": [1, 2, 4]
        }
    ),
    
    # Step 08: Quality Assessment (ì˜ˆìƒ íŒŒì¼ ê¸°ë°˜)
    "QualityAssessmentStep": ModelRequest(
        model_name="quality_assessment_combined",
        step_class="QualityAssessmentStep",
        step_priority=StepPriority.LOW,
        model_class="QualityAssessmentModel",
        input_size=(224, 224),
        output_format="quality_scores",
        
        # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ íŒ¨í„´ (ì˜ˆìƒ)
        checkpoint_patterns=[
            r".*quality.*assessment.*\.pth$",
            r".*perceptual.*quality.*\.pth$",
            r".*lpips.*\.pth$",
            r".*clip.*quality.*\.bin$"
        ],
        file_extensions=[".bin", ".pth", ".pt"],
        size_range_mb=(10.0, 1000.0),
        
        # í’ˆì§ˆ í‰ê°€ ìµœì í™”
        optimization_params={
            "batch_size": 4,
            "memory_fraction": 0.25,
            "enable_perceptual_loss": True,
            "quality_threshold": 0.7
        },
        
        alternative_models=[
            "quality_assessment_lpips",
            "quality_assessment_clip"
        ],
        
        metadata={
            "description": "ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€",
            "assessment_metrics": ["lpips", "ssim", "psnr"],
            "quality_threshold": 0.7
        }
    )
}

# ==============================================
# ğŸ”¥ ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦ í•¨ìˆ˜ë“¤
# ==============================================

def get_step_request(step_name: str) -> Optional[ModelRequest]:
    """Stepë³„ ëª¨ë¸ ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.get(step_name)

def get_all_step_requests() -> Dict[str, ModelRequest]:
    """ëª¨ë“  Step ìš”ì²­ ì •ë³´ ë°˜í™˜"""
    return STEP_MODEL_REQUESTS.copy()

def get_checkpoint_patterns(step_name: str) -> List[str]:
    """Stepë³„ ì²´í¬í¬ì¸íŠ¸ íƒì§€ íŒ¨í„´ ë°˜í™˜"""
    request = get_step_request(step_name)
    return request.checkpoint_patterns if request else []

def get_model_config_for_step(step_name: str, detected_path: Path) -> Dict[str, Any]:
    """Step ìš”ì²­ì„ ModelLoader ì„¤ì •ìœ¼ë¡œ ë³€í™˜"""
    request = get_step_request(step_name)
    if not request:
        return {}
    
    return {
        "name": request.model_name,
        "model_type": request.model_class,
        "model_class": request.model_class,
        "checkpoint_path": str(detected_path),
        "device": request.device,
        "precision": request.precision,
        "input_size": request.input_size,
        "num_classes": request.num_classes,
        "optimization_params": request.optimization_params,
        "metadata": {
            **request.metadata,
            "step_name": step_name,
            "step_priority": request.step_priority.name,
            "auto_detected": True,
            "detection_time": time.time()
        }
    }

def validate_model_for_step(step_name: str, model_path: Path, size_mb: float) -> Dict[str, Any]:
    """Step ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ëª¨ë¸ ê²€ì¦ (ì‹¤ì œ í¬ê¸° ê¸°ì¤€)"""
    request = get_step_request(step_name)
    if not request:
        return {"valid": False, "reason": f"Unknown step: {step_name}"}
    
    # í¬ê¸° ê²€ì¦ (ì‹¤ì œ íƒì§€ëœ íŒŒì¼ í¬ê¸° ë°˜ì˜)
    min_size, max_size = request.size_range_mb
    if not (min_size <= size_mb <= max_size):
        return {
            "valid": False,
            "reason": f"Size {size_mb}MB not in range [{min_size}, {max_size}]"
        }
    
    # í™•ì¥ì ê²€ì¦
    if model_path.suffix.lower() not in request.file_extensions:
        return {
            "valid": False,
            "reason": f"Extension {model_path.suffix} not in {request.file_extensions}"
        }
    
    # íŒ¨í„´ ë§¤ì¹­ (ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜)
    import re
    model_name = model_path.name.lower()
    pattern_matched = False
    
    for pattern in request.checkpoint_patterns:
        if re.search(pattern, model_name):
            pattern_matched = True
            break
    
    if not pattern_matched:
        return {
            "valid": False,
            "reason": f"Name doesn't match patterns: {request.checkpoint_patterns}"
        }
    
    return {
        "valid": True,
        "confidence": 0.9,
        "step_priority": request.step_priority.name,
        "model_class": request.model_class
    }

def get_step_priorities() -> Dict[str, int]:
    """Stepë³„ ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
    return {
        step_name: request.step_priority.value
        for step_name, request in STEP_MODEL_REQUESTS.items()
    }

def get_steps_by_priority(priority: StepPriority) -> List[str]:
    """ìš°ì„ ìˆœìœ„ë³„ Step ëª©ë¡ ë°˜í™˜"""
    return [
        step_name for step_name, request in STEP_MODEL_REQUESTS.items()
        if request.step_priority == priority
    ]

def validate_against_actual_files(step_name: str, file_name: str, file_size_mb: float) -> Dict[str, Any]:
    """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ê³¼ ë¹„êµ ê²€ì¦"""
    request = get_step_request(step_name)
    if not request or "file_sizes_mb" not in request.metadata:
        return {"valid": False, "reason": "No actual file data available"}
    
    actual_sizes = request.metadata["file_sizes_mb"]
    
    # ì‹¤ì œ íŒŒì¼ëª… ë§¤ì¹­
    if file_name in actual_sizes:
        expected_size = actual_sizes[file_name]
        size_diff = abs(file_size_mb - expected_size)
        size_tolerance = expected_size * 0.1  # 10% ì˜¤ì°¨ í—ˆìš©
        
        if size_diff <= size_tolerance:
            return {
                "valid": True,
                "confidence": 1.0,
                "matched_file": file_name,
                "expected_size": expected_size,
                "actual_size": file_size_mb,
                "size_difference": size_diff
            }
    
    return {
        "valid": False,
        "reason": f"File {file_name} not found in actual detected files"
    }

# ==============================================
# ğŸ”¥ ì™„ì „í•œ StepModelRequestAnalyzer í´ë˜ìŠ¤ êµ¬í˜„
# ==============================================

class StepModelRequestAnalyzer:
    """Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë¶„ì„ê¸° - ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜ (ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self._cache = {}
        self._registered_requirements = {}
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="StepAnalyzer")
        self._lock = threading.Lock()
        logger.info("âœ… StepModelRequestAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=False)
    
    # ==============================================
    # ğŸ”¥ ê¸°ë³¸ ë¶„ì„ ë©”ì„œë“œë“¤
    # ==============================================
    
    def analyze_requirements(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìš”êµ¬ì‚¬í•­ ë¶„ì„ (í•µì‹¬ ë©”ì„œë“œ)"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        if not request:
            return {
                "error": f"Unknown step: {step_name}",
                "available_steps": list(STEP_MODEL_REQUESTS.keys())
            }
        
        # ìºì‹œ í™•ì¸
        with self._lock:
            cache_key = f"analyze_{step_name}"
            if cache_key in self._cache:
                return self._cache[cache_key]
        
        # ë¶„ì„ ì‹¤í–‰
        analysis = {
            "step_name": step_name,
            "model_name": request.model_name,
            "model_type": request.model_class,
            "step_class": request.step_class,
            "step_priority": request.step_priority.value,
            "priority_name": request.step_priority.name,
            
            # ëª¨ë¸ ìŠ¤í™
            "input_size": request.input_size,
            "num_classes": request.num_classes,
            "output_format": request.output_format,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device": request.device,
            "precision": request.precision,
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
            "checkpoint_patterns": request.checkpoint_patterns,
            "file_extensions": request.file_extensions,
            "size_range_mb": request.size_range_mb,
            
            # ìµœì í™” íŒŒë¼ë¯¸í„°
            "optimization_params": request.optimization_params,
            
            # ëŒ€ì²´ ëª¨ë¸
            "alternative_models": request.alternative_models,
            
            # ì‹¤ì œ íŒŒì¼ ì •ë³´
            "actual_files": request.metadata.get("actual_files", {}),
            "file_sizes_mb": request.metadata.get("file_sizes_mb", {}),
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": request.metadata,
            "description": request.metadata.get("description", ""),
            
            # ModelLoader í˜¸í™˜ ìš”êµ¬ì‚¬í•­
            "requirements": {
                "models": [request.model_name],
                "device": request.device,
                "precision": request.precision,
                "memory_fraction": request.optimization_params.get("memory_fraction", 0.8),
                "batch_size": request.optimization_params.get("batch_size", 1)
            },
            
            # ë¶„ì„ ë©”íƒ€ë°ì´í„°
            "analysis_timestamp": time.time(),
            "is_critical": request.step_priority == StepPriority.CRITICAL,
            "supports_alternatives": len(request.alternative_models) > 0
        }
        
        # ìºì‹œ ì €ì¥
        with self._lock:
            self._cache[cache_key] = analysis
        
        return analysis
    
    def get_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (analyze_requirementsì™€ ë™ì¼)"""
        return self.analyze_requirements(step_name)
    
    @staticmethod
    def get_step_request_info(step_name: str) -> Optional[Dict[str, Any]]:
        """Stepë³„ ìš”ì²­ ì •ë³´ ë°˜í™˜ (ModelLoader í˜¸í™˜)"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        if not request:
            return None
        
        return {
            "model_name": request.model_name,
            "model_type": request.model_class,
            "input_size": request.input_size,
            "num_classes": request.num_classes,
            "device": request.device,
            "precision": request.precision,
            "checkpoint_patterns": request.checkpoint_patterns,
            "optimization_params": request.optimization_params,
            "step_priority": request.step_priority.value,
            "alternative_models": request.alternative_models,
            "metadata": request.metadata
        }
    
    @staticmethod
    def get_all_step_requirements() -> Dict[str, Any]:
        """ëª¨ë“  Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        return {
            step_name: StepModelRequestAnalyzer.get_step_request_info(step_name)
            for step_name in STEP_MODEL_REQUESTS.keys()
        }
    
    # ==============================================
    # ğŸ”¥ ìš°ì„ ìˆœìœ„ ë° ë¶„ë¥˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    @staticmethod
    def get_critical_steps() -> List[str]:
        """ì¤‘ìš”í•œ Stepë“¤ ë°˜í™˜ (ì‹¤ì œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        return [
            step_name for step_name, request in STEP_MODEL_REQUESTS.items()
            if request.step_priority == StepPriority.CRITICAL
        ]
    
    @staticmethod
    def get_high_priority_steps() -> List[str]:
        """ë†’ì€ ìš°ì„ ìˆœìœ„ Stepë“¤ ë°˜í™˜"""
        return [
            step_name for step_name, request in STEP_MODEL_REQUESTS.items()
            if request.step_priority == StepPriority.HIGH
        ]
    
    @staticmethod
    def get_steps_by_priority_level(priority: StepPriority) -> List[str]:
        """ìš°ì„ ìˆœìœ„ë³„ Step ëª©ë¡ ë°˜í™˜"""
        return [
            step_name for step_name, request in STEP_MODEL_REQUESTS.items()
            if request.step_priority == priority
        ]
    
    def get_priority_analysis(self) -> Dict[str, Any]:
        """ìš°ì„ ìˆœìœ„ ë¶„ì„ ê²°ê³¼"""
        analysis = {
            "total_steps": len(STEP_MODEL_REQUESTS),
            "by_priority": {},
            "priority_distribution": {},
            "critical_steps": self.get_critical_steps(),
            "recommended_order": []
        }
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        for priority in StepPriority:
            steps = self.get_steps_by_priority_level(priority)
            analysis["by_priority"][priority.name] = steps
            analysis["priority_distribution"][priority.name] = len(steps)
        
        # ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
        for priority in [StepPriority.CRITICAL, StepPriority.HIGH, StepPriority.MEDIUM, StepPriority.LOW]:
            analysis["recommended_order"].extend(self.get_steps_by_priority_level(priority))
        
        return analysis
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ê´€ë ¨ ë©”ì„œë“œë“¤
    # ==============================================
    
    @staticmethod
    def get_model_for_step(step_name: str) -> Optional[str]:
        """Stepì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ëª… ë°˜í™˜"""
        request = STEP_MODEL_REQUESTS.get(step_name)
        return request.model_name if request else None
    
    @staticmethod
    def get_all_models() -> Dict[str, str]:
        """ëª¨ë“  Stepì˜ ëª¨ë¸ëª… ë°˜í™˜"""
        return {
            step_name: request.model_name
            for step_name, request in STEP_MODEL_REQUESTS.items()
        }
    
    def get_model_requirements_summary(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìš”ì•½"""
        summary = {
            "total_models": len(STEP_MODEL_REQUESTS),
            "device_requirements": {},
            "precision_requirements": {},
            "memory_requirements": {},
            "size_distribution": {
                "small": [],    # < 100MB
                "medium": [],   # 100MB - 1GB
                "large": [],    # > 1GB
            }
        }
        
        for step_name, request in STEP_MODEL_REQUESTS.items():
            # ë””ë°”ì´ìŠ¤ ìš”êµ¬ì‚¬í•­
            device = request.device
            if device not in summary["device_requirements"]:
                summary["device_requirements"][device] = []
            summary["device_requirements"][device].append(step_name)
            
            # ì •ë°€ë„ ìš”êµ¬ì‚¬í•­
            precision = request.precision
            if precision not in summary["precision_requirements"]:
                summary["precision_requirements"][precision] = []
            summary["precision_requirements"][precision].append(step_name)
            
            # ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            memory_fraction = request.optimization_params.get("memory_fraction", 0.5)
            if memory_fraction not in summary["memory_requirements"]:
                summary["memory_requirements"][memory_fraction] = []
            summary["memory_requirements"][memory_fraction].append(step_name)
            
            # í¬ê¸° ë¶„í¬
            min_size, max_size = request.size_range_mb
            avg_size = (min_size + max_size) / 2
            
            if avg_size < 100:
                summary["size_distribution"]["small"].append(step_name)
            elif avg_size < 1000:
                summary["size_distribution"]["medium"].append(step_name)
            else:
                summary["size_distribution"]["large"].append(step_name)
        
        return summary
    
    # ==============================================
    # ğŸ”¥ íŒŒì¼ íƒì§€ ë° ê²€ì¦ ë©”ì„œë“œë“¤
    # ==============================================
    
    @staticmethod
    def get_actual_detected_files() -> Dict[str, Dict[str, Any]]:
        """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        detected_files = {}
        for step_name, request in STEP_MODEL_REQUESTS.items():
            if "actual_files" in request.metadata:
                detected_files[step_name] = request.metadata["actual_files"]
        return detected_files
    
    @staticmethod
    def get_file_size_validation_ranges() -> Dict[str, Tuple[float, float]]:
        """Stepë³„ íŒŒì¼ í¬ê¸° ê²€ì¦ ë²”ìœ„ ë°˜í™˜"""
        return {
            step_name: request.size_range_mb
            for step_name, request in STEP_MODEL_REQUESTS.items()
        }
    
    def validate_file_for_step(self, step_name: str, file_path: Union[str, Path], 
                              file_size_mb: Optional[float] = None) -> Dict[str, Any]:
        """íŒŒì¼ì´ Step ìš”êµ¬ì‚¬í•­ì— ë§ëŠ”ì§€ ê²€ì¦"""
        if isinstance(file_path, str):
            file_path = Path(file_path)
        
        # íŒŒì¼ í¬ê¸° ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if file_size_mb is None:
            try:
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
            except OSError:
                return {"valid": False, "reason": f"Cannot access file: {file_path}"}
        
        return validate_model_for_step(step_name, file_path, file_size_mb)
    
    def find_best_model_for_step(self, step_name: str, available_files: List[Path]) -> Optional[Dict[str, Any]]:
        """Stepì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ ì°¾ê¸°"""
        request = get_step_request(step_name)
        if not request or not available_files:
            return None
        
        candidates = []
        
        for file_path in available_files:
            try:
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                validation = self.validate_file_for_step(step_name, file_path, file_size_mb)
                
                if validation.get("valid", False):
                    candidates.append({
                        "path": file_path,
                        "size_mb": file_size_mb,
                        "confidence": validation.get("confidence", 0.0),
                        "validation": validation
                    })
            except OSError:
                continue
        
        if not candidates:
            return None
        
        # ê°€ì¥ ë†’ì€ confidenceë¥¼ ê°€ì§„ ëª¨ë¸ ì„ íƒ
        best_candidate = max(candidates, key=lambda x: x["confidence"])
        
        return {
            "step_name": step_name,
            "best_model": best_candidate,
            "all_candidates": candidates,
            "recommendation_reason": "Highest confidence score",
            "model_config": get_model_config_for_step(step_name, best_candidate["path"])
        }
    
    # ==============================================
    # ğŸ”¥ register_step_requirements ë©”ì„œë“œ (í•µì‹¬)
    # ==============================================
    
    def register_step_requirements(self, step_name: str, **requirements) -> bool:
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (ModelLoader ì—°ë™ìš©)"""
        try:
            with self._lock:
                self._registered_requirements[step_name] = {
                    "timestamp": time.time(),
                    "requirements": requirements,
                    "source": "external_registration"
                }
            
            logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {step_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
    
    def get_registered_requirements(self, step_name: str) -> Optional[Dict[str, Any]]:
        """ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        with self._lock:
            return self._registered_requirements.get(step_name)
    
    def get_all_registered_requirements(self) -> Dict[str, Any]:
        """ëª¨ë“  ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        with self._lock:
            return self._registered_requirements.copy()
    
    def clear_registered_requirements(self, step_name: Optional[str] = None):
        """ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ ì •ë¦¬"""
        with self._lock:
            if step_name:
                self._registered_requirements.pop(step_name, None)
            else:
                self._registered_requirements.clear()
    
    # ==============================================
    # ğŸ”¥ ìºì‹œ ë° ì„±ëŠ¥ ê´€ë¦¬
    # ==============================================
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        with self._lock:
            self._cache.clear()
        logger.info("âœ… StepModelRequestAnalyzer ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        with self._lock:
            return {
                "cache_size": len(self._cache),
                "registered_requirements": len(self._registered_requirements),
                "cache_keys": list(self._cache.keys())
            }
    
    # ==============================================
    # ğŸ”¥ ë¹„ë™ê¸° ë©”ì„œë“œë“¤
    # ==============================================
    
    async def analyze_requirements_async(self, step_name: str) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self._executor, self.analyze_requirements, step_name)
    
    async def validate_file_for_step_async(self, step_name: str, file_path: Union[str, Path], 
                                          file_size_mb: Optional[float] = None) -> Dict[str, Any]:
        """ë¹„ë™ê¸° íŒŒì¼ ê²€ì¦"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor, 
            self.validate_file_for_step, 
            step_name, file_path, file_size_mb
        )
    
    # ==============================================
    # ğŸ”¥ ì§„ë‹¨ ë° ì‹œìŠ¤í…œ ì •ë³´
    # ==============================================
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "step_model_requests_version": "6.0",
            "total_steps": len(STEP_MODEL_REQUESTS),
            "step_names": list(STEP_MODEL_REQUESTS.keys()),
            "priority_levels": [p.name for p in StepPriority],
            "cache_enabled": True,
            "thread_pool_workers": self._executor._max_workers if hasattr(self._executor, '_max_workers') else 4,
            "registered_requirements_count": len(self._registered_requirements),
            "cache_size": len(self._cache)
        }
    
    def diagnose_step(self, step_name: str) -> Dict[str, Any]:
        """Step ì§„ë‹¨"""
        request = get_step_request(step_name)
        if not request:
            return {"error": f"Step {step_name} not found"}
        
        diagnosis = {
            "step_name": step_name,
            "exists": True,
            "priority": request.step_priority.name,
            "model_defined": bool(request.model_name),
            "patterns_defined": len(request.checkpoint_patterns) > 0,
            "alternatives_available": len(request.alternative_models) > 0,
            "optimization_configured": len(request.optimization_params) > 0,
            "metadata_complete": len(request.metadata) > 0,
            "actual_files_mapped": "actual_files" in request.metadata,
            "file_sizes_known": "file_sizes_mb" in request.metadata,
            "health_score": 0.0
        }
        
        # ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        health_factors = [
            diagnosis["model_defined"],
            diagnosis["patterns_defined"],
            diagnosis["alternatives_available"],
            diagnosis["optimization_configured"],
            diagnosis["metadata_complete"],
            diagnosis["actual_files_mapped"],
            diagnosis["file_sizes_known"]
        ]
        
        diagnosis["health_score"] = sum(health_factors) / len(health_factors)
        diagnosis["health_status"] = "excellent" if diagnosis["health_score"] >= 0.9 else \
                                   "good" if diagnosis["health_score"] >= 0.7 else \
                                   "fair" if diagnosis["health_score"] >= 0.5 else "poor"
        
        return diagnosis
    
    def get_full_diagnostic_report(self) -> Dict[str, Any]:
        """ì „ì²´ ì§„ë‹¨ ë³´ê³ ì„œ"""
        report = {
            "system_info": self.get_system_info(),
            "priority_analysis": self.get_priority_analysis(),
            "model_requirements_summary": self.get_model_requirements_summary(),
            "step_diagnostics": {},
            "overall_health": {
                "total_steps": len(STEP_MODEL_REQUESTS),
                "healthy_steps": 0,
                "average_health_score": 0.0
            }
        }
        
        # ê° Step ì§„ë‹¨
        total_health = 0.0
        for step_name in STEP_MODEL_REQUESTS.keys():
            diagnosis = self.diagnose_step(step_name)
            report["step_diagnostics"][step_name] = diagnosis
            total_health += diagnosis.get("health_score", 0.0)
            if diagnosis.get("health_status") in ["excellent", "good"]:
                report["overall_health"]["healthy_steps"] += 1
        
        report["overall_health"]["average_health_score"] = total_health / len(STEP_MODEL_REQUESTS)
        
        return report

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
_global_analyzer: Optional[StepModelRequestAnalyzer] = None
_analyzer_lock = threading.Lock()

def get_global_analyzer() -> StepModelRequestAnalyzer:
    """ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _global_analyzer
    if _global_analyzer is None:
        with _analyzer_lock:
            if _global_analyzer is None:
                _global_analyzer = StepModelRequestAnalyzer()
    return _global_analyzer

def analyze_step_requirements(step_name: str) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: Step ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
    analyzer = get_global_analyzer()
    return analyzer.analyze_requirements(step_name)

def register_step_requirements(step_name: str, **requirements) -> bool:
    """í¸ì˜ í•¨ìˆ˜: Step ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
    analyzer = get_global_analyzer()
    return analyzer.register_step_requirements(step_name, **requirements)

def validate_step_file(step_name: str, file_path: Union[str, Path], 
                      file_size_mb: Optional[float] = None) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: Step íŒŒì¼ ê²€ì¦"""
    analyzer = get_global_analyzer()
    return analyzer.validate_file_for_step(step_name, file_path, file_size_mb)

# ==============================================
# ğŸ”¥ ModelLoader í˜¸í™˜ í•¨ìˆ˜ë“¤ (ì‹¤ì œ êµ¬ì¡° ë°˜ì˜)
# ==============================================

def get_all_step_requirements() -> Dict[str, Any]:
    """ì „ì²´ Step ìš”êµ¬ì‚¬í•­ (ModelLoader í˜¸í™˜)"""
    return StepModelRequestAnalyzer.get_all_step_requirements()

def create_model_loader_config_from_detection(step_name: str, detected_models: List[Path]) -> Dict[str, Any]:
    """íƒì§€ëœ ëª¨ë¸ë¡œë¶€í„° ModelLoader ì„¤ì • ìƒì„±"""
    request = get_step_request(step_name)
    if not request or not detected_models:
        return {}
    
    # ì‹¤ì œ íƒì§€ëœ íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
    best_model = max(detected_models, key=lambda p: p.stat().st_size)
    
    return get_model_config_for_step(step_name, best_model)

def get_actual_detected_patterns() -> Dict[str, List[str]]:
    """ì‹¤ì œ íƒì§€ëœ íŒŒì¼ ê¸°ë°˜ ê²€ì¦ëœ íŒ¨í„´ë“¤ ë°˜í™˜"""
    return {
        step_name: request.checkpoint_patterns
        for step_name, request in STEP_MODEL_REQUESTS.items()
    }

# ==============================================
# ğŸ”¥ ì •ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def cleanup_analyzer():
    """ë¶„ì„ê¸° ì •ë¦¬"""
    global _global_analyzer
    if _global_analyzer:
        _global_analyzer.clear_cache()
        _global_analyzer.clear_registered_requirements()
        _global_analyzer = None

import atexit
atexit.register(cleanup_analyzer)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤
    'StepPriority',
    'ModelRequest', 
    'StepModelRequestAnalyzer',

    # ë°ì´í„°
    'STEP_MODEL_REQUESTS',

    # ê¸°ë³¸ í•¨ìˆ˜ë“¤
    'get_step_request',
    'get_all_step_requests',
    'get_checkpoint_patterns',
    'get_model_config_for_step',
    'validate_model_for_step',
    'get_step_priorities',
    'get_steps_by_priority',
    'get_all_step_requirements',
    'create_model_loader_config_from_detection',
    'get_actual_detected_patterns',
    'validate_against_actual_files',
    
    # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
    'get_global_analyzer',
    'analyze_step_requirements',
    'register_step_requirements',
    'validate_step_file',
    'cleanup_analyzer'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger.info("=" * 80)
logger.info("ğŸ”¥ Step Model Requests v6.0 ë¡œë“œ ì™„ë£Œ - ì™„ì „í•œ êµ¬í˜„")
logger.info("=" * 80)
logger.info(f"ğŸ“‹ {len(STEP_MODEL_REQUESTS)}ê°œ Step ì •ì˜ (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)")
logger.info("ğŸ”§ StepModelRequestAnalyzer í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„")
logger.info("ğŸ¯ ì‹¤ì œ íƒì§€ëœ ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´ ì •í™•íˆ ì ìš©")
logger.info("ğŸš€ ModelLoader ì™„ë²½ í˜¸í™˜ì„± + ì‹¤ì œ íŒŒì¼ ê²€ì¦ ë³´ì¥")
logger.info("ğŸ’¾ ì‹¤ì œ íŒŒì¼ í¬ê¸° ì •ë³´:")
logger.info("   - exp-schp-201908301523-atr.pth (255.1MB)")
logger.info("   - openpose.pth (199.6MB)")
logger.info("   - u2net.pth (168.1MB)")
logger.info("   - sam_vit_h_4b8939.pth (2445.7MB)")
logger.info("   - pytorch_model.bin (577.2MB)")
logger.info("âœ… register_step_requirements ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›")
logger.info("âœ… ìºì‹œ ì‹œìŠ¤í…œ êµ¬í˜„")
logger.info("âœ… ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì œê³µ")
logger.info("âœ… ì§„ë‹¨ ë° ì‹œìŠ¤í…œ ì •ë³´ ì œê³µ")
logger.info("âœ… ëª¨ë“  ModelLoader ì—°ë™ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±")
logger.info("=" * 80)

# ì´ˆê¸°í™” ì‹œ ì „ì—­ ë¶„ì„ê¸° ìƒì„±
try:
    _initial_analyzer = get_global_analyzer()
    logger.info("âœ… ì „ì—­ StepModelRequestAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ ì „ì—­ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")