#!/usr/bin/env python3
"""
ğŸ”¥ AI ì¶”ë¡  ê²€ì¦ ì‹œìŠ¤í…œ
=====================

ì‹¤ì œ AI ì¶”ë¡ ì— í•„ìš”í•œ ëª¨ë“  ìš”ì†Œë“¤ì„ ê²€ì¦í•˜ê³  ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì‹œìŠ¤í…œ

Author: MyCloset AI Team
Date: 2025-08-07
Version: 1.0
"""

import os
import sys
import time
import logging
import traceback
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# NumPy ê´€ë ¨
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# OpenCV ê´€ë ¨
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class AIInferenceRequirements:
    """AI ì¶”ë¡ ì— í•„ìš”í•œ ìš”êµ¬ì‚¬í•­ë“¤"""
    
    # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ìš”êµ¬ì‚¬í•­
    model_files: Dict[str, bool] = field(default_factory=dict)
    model_sizes: Dict[str, float] = field(default_factory=dict)  # MB ë‹¨ìœ„
    model_formats: Dict[str, str] = field(default_factory=dict)  # .pth, .safetensors ë“±
    
    # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
    gpu_memory_required: float = 0.0  # GB
    system_memory_required: float = 0.0  # GB
    model_memory_usage: Dict[str, float] = field(default_factory=dict)  # MB ë‹¨ìœ„
    
    # ğŸ”¥ 3. ë””ë°”ì´ìŠ¤ ìš”êµ¬ì‚¬í•­
    device_available: bool = False
    device_type: str = "unknown"  # cpu, cuda, mps
    device_memory: float = 0.0  # GB
    
    # ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš”êµ¬ì‚¬í•­
    required_libraries: Dict[str, bool] = field(default_factory=dict)
    library_versions: Dict[str, str] = field(default_factory=dict)
    
    # ğŸ”¥ 5. ì²´í¬í¬ì¸íŠ¸ ìš”êµ¬ì‚¬í•­
    checkpoint_loaded: bool = False
    checkpoint_keys: List[str] = field(default_factory=list)
    checkpoint_size: float = 0.0  # MB
    
    # ğŸ”¥ 6. ì…ë ¥ ë°ì´í„° ìš”êµ¬ì‚¬í•­
    input_shape: Tuple[int, ...] = field(default_factory=tuple)
    input_dtype: str = "unknown"
    input_normalization: bool = False
    
    # ğŸ”¥ 7. ì¶œë ¥ ë°ì´í„° ìš”êµ¬ì‚¬í•­
    output_shape: Tuple[int, ...] = field(default_factory=tuple)
    output_dtype: str = "unknown"
    output_postprocessing: bool = False

class AIInferenceValidator:
    """AI ì¶”ë¡  ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.requirements = AIInferenceRequirements()
        self.validation_results = {}
        
    def validate_step_01_human_parsing(self) -> Dict[str, Any]:
        """Step 1 (Human Parsing) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Human Parsing',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'graphonomy': 'ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth',
                'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',
                'deeplabv3plus': 'ai_models/step_01_human_parsing/deeplabv3plus.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
            total_model_size = sum(self.requirements.model_sizes.values())
            self.requirements.gpu_memory_required = total_model_size / 1024 * 2  # 2ë°° ë²„í¼
            self.requirements.system_memory_required = total_model_size / 1024 * 3  # 3ë°° ë²„í¼
            
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = self.requirements.gpu_memory_required
            result['system_memory_required_gb'] = self.requirements.system_memory_required
            
            # ğŸ”¥ 3. ë””ë°”ì´ìŠ¤ ê²€ì¦
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "cuda"
                    self.requirements.device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "mps"
                    # MPS ë©”ëª¨ë¦¬ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì™€ ê³µìœ 
                    self.requirements.device_memory = 0.0
                else:
                    self.requirements.device_available = True
                    self.requirements.device_type = "cpu"
                    self.requirements.device_memory = 0.0
                
                result['device_available'] = self.requirements.device_available
                result['device_type'] = self.requirements.device_type
                result['device_memory_gb'] = self.requirements.device_memory
            
            # ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
            required_libs = {
                'torch': TORCH_AVAILABLE,
                'numpy': NUMPY_AVAILABLE,
                'opencv': CV2_AVAILABLE
            }
            
            for lib_name, available in required_libs.items():
                self.requirements.required_libraries[lib_name] = available
                if available:
                    result[f'{lib_name}_available'] = True
                else:
                    result[f'{lib_name}_available'] = False
                    result['issues'].append(f"{lib_name} ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            
            # ğŸ”¥ 5. ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            if self.requirements.model_files.get('graphonomy', False):
                try:
                    checkpoint_path = model_files['graphonomy']
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    
                    if isinstance(checkpoint, dict):
                        self.requirements.checkpoint_loaded = True
                        self.requirements.checkpoint_keys = list(checkpoint.keys())
                        self.requirements.checkpoint_size = Path(checkpoint_path).stat().st_size / (1024 * 1024)
                        
                        result['checkpoint_loaded'] = True
                        result['checkpoint_keys_count'] = len(self.requirements.checkpoint_keys)
                        result['checkpoint_size_mb'] = self.requirements.checkpoint_size
                        
                        # state_dict í™•ì¸
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                            result['state_dict_keys_count'] = len(state_dict)
                            result['state_dict_sample_keys'] = list(state_dict.keys())[:5]
                    else:
                        result['issues'].append("ì²´í¬í¬ì¸íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹˜")
                        
                except Exception as e:
                    result['issues'].append(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 6. ì…ë ¥ ë°ì´í„° ìš”êµ¬ì‚¬í•­
            self.requirements.input_shape = (3, 512, 512)  # RGB, 512x512
            self.requirements.input_dtype = "float32"
            self.requirements.input_normalization = True
            
            result['input_shape'] = self.requirements.input_shape
            result['input_dtype'] = self.requirements.input_dtype
            result['input_normalization'] = self.requirements.input_normalization
            
            # ğŸ”¥ 7. ì¶œë ¥ ë°ì´í„° ìš”êµ¬ì‚¬í•­
            self.requirements.output_shape = (20, 512, 512)  # 20ê°œ í´ë˜ìŠ¤, 512x512
            self.requirements.output_dtype = "float32"
            self.requirements.output_postprocessing = True
            
            result['output_shape'] = self.requirements.output_shape
            result['output_dtype'] = self.requirements.output_dtype
            result['output_postprocessing'] = self.requirements.output_postprocessing
            
            # ğŸ”¥ 8. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = (
                all(self.requirements.model_files.values()) and
                self.requirements.device_available and
                all(self.requirements.required_libraries.values()) and
                self.requirements.checkpoint_loaded
            )
            
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 9. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
                
                if not self.requirements.device_available:
                    result['recommendations'].append("GPU ë˜ëŠ” MPS ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                
                if not all(self.requirements.required_libraries.values()):
                    result['recommendations'].append("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•˜ì„¸ìš”")
                
                if not self.requirements.checkpoint_loaded:
                    result['recommendations'].append("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”")
            
            # ë©”ëª¨ë¦¬ ê¶Œì¥ì‚¬í•­
            if self.requirements.gpu_memory_required > self.requirements.device_memory:
                result['recommendations'].append(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {self.requirements.gpu_memory_required:.1f}GB í•„ìš”, {self.requirements.device_memory:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
            
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_01'] = result
        return result
    
    def validate_step_02_pose_estimation(self) -> Dict[str, Any]:
        """Step 2 (Pose Estimation) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Pose Estimation',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'body_pose_model': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/ckpts/body_pose_model.pth',
                'yolov8m_pose': 'ai_models/step_02_pose_estimation/yolov8m-pose.pt',
                'openpose': 'ai_models/openpose.pth',
                'hrnet_w48': 'ai_models/step_02_pose_estimation/hrnet_w48_coco_384x288.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
            total_model_size = sum(self.requirements.model_sizes.values())
            self.requirements.gpu_memory_required = total_model_size / 1024 * 1.5  # 1.5ë°° ë²„í¼
            self.requirements.system_memory_required = total_model_size / 1024 * 2  # 2ë°° ë²„í¼
            
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = self.requirements.gpu_memory_required
            result['system_memory_required_gb'] = self.requirements.system_memory_required
            
            # ğŸ”¥ 3. ë””ë°”ì´ìŠ¤ ê²€ì¦
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "cuda"
                    self.requirements.device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "mps"
                    self.requirements.device_memory = 0.0
                else:
                    self.requirements.device_available = True
                    self.requirements.device_type = "cpu"
                    self.requirements.device_memory = 0.0
                
                result['device_available'] = self.requirements.device_available
                result['device_type'] = self.requirements.device_type
                result['device_memory_gb'] = self.requirements.device_memory
            
            # ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
            required_libs = {
                'torch': TORCH_AVAILABLE,
                'numpy': NUMPY_AVAILABLE,
                'opencv': CV2_AVAILABLE,
                'mediapipe': self._check_library_availability('mediapipe')
            }
            
            for lib_name, available in required_libs.items():
                self.requirements.required_libraries[lib_name] = available
                if available:
                    result[f'{lib_name}_available'] = True
                else:
                    result[f'{lib_name}_available'] = False
                    result['issues'].append(f"{lib_name} ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            
            # ğŸ”¥ 5. ì¢…í•© íŒì •
            all_models_exist = all(self.requirements.model_files.values())
            all_libs_available = all(self.requirements.required_libraries.values())
            
            result['requirements_met'] = all([all_models_exist, all_libs_available, self.requirements.device_available])
            
            if result['requirements_met']:
                result['recommendations'].append("ğŸ‰ Step 2 ëª¨ë“  ìš”êµ¬ì‚¬í•­ ì¶©ì¡±!")
            else:
                result['issues'].append("âŒ Step 2 ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±")
                
        except Exception as e:
            result['issues'].append(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return result
    
    def validate_step_03_cloth_segmentation(self) -> Dict[str, Any]:
        """Step 3 (Cloth Segmentation) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Cloth Segmentation',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'sam_vit_h': 'ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',
                'deeplabv3_resnet101': 'ai_models/step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth',
                'mobile_sam': 'ai_models/step_03_cloth_segmentation/mobile_sam_alternative.pt'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚° (SAM ëª¨ë¸ì´ í¼)
            total_model_size = sum(self.requirements.model_sizes.values())
            self.requirements.gpu_memory_required = total_model_size / 1024 * 3  # 3ë°° ë²„í¼
            self.requirements.system_memory_required = total_model_size / 1024 * 4  # 4ë°° ë²„í¼
            
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = self.requirements.gpu_memory_required
            result['system_memory_required_gb'] = self.requirements.system_memory_required
            
            # ğŸ”¥ 3. ë””ë°”ì´ìŠ¤ ê²€ì¦
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "cuda"
                    self.requirements.device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.requirements.device_available = True
                    self.requirements.device_type = "mps"
                    self.requirements.device_memory = 0.0
                else:
                    self.requirements.device_available = True
                    self.requirements.device_type = "cpu"
                    self.requirements.device_memory = 0.0
                
                result['device_available'] = self.requirements.device_available
                result['device_type'] = self.requirements.device_type
                result['device_memory_gb'] = self.requirements.device_memory
            
            # ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
            required_libs = {
                'torch': TORCH_AVAILABLE,
                'numpy': NUMPY_AVAILABLE,
                'opencv': CV2_AVAILABLE,
                'PIL': PIL_AVAILABLE,
                'segment_anything': self._check_library_availability('segment_anything')
            }
            
            for lib_name, available in required_libs.items():
                self.requirements.required_libraries[lib_name] = available
                if available:
                    result[f'{lib_name}_available'] = True
                else:
                    result[f'{lib_name}_available'] = False
                    result['issues'].append(f"{lib_name} ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            
            # ğŸ”¥ 5. ì¢…í•© íŒì •
            all_models_exist = all(self.requirements.model_files.values())
            all_libs_available = all(self.requirements.required_libraries.values())
            
            result['requirements_met'] = all([all_models_exist, all_libs_available, self.requirements.device_available])
            
            if result['requirements_met']:
                result['recommendations'].append("ğŸ‰ Step 3 ëª¨ë“  ìš”êµ¬ì‚¬í•­ ì¶©ì¡±!")
            else:
                result['issues'].append("âŒ Step 3 ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±")
                
        except Exception as e:
            result['issues'].append(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return result
    
    def validate_step_04_geometric_matching(self) -> Dict[str, Any]:
        """Step 4 (Geometric Matching) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Geometric Matching',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'gmm_final': 'ai_models/step_04_geometric_matching/gmm_final.pth',
                'tps_network': 'ai_models/step_04_geometric_matching/tps_network.pth',
                'raft_things': 'ai_models/step_04_geometric_matching/raft-things.pth',
                'sam_vit_h': 'ai_models/step_04_geometric_matching/sam_vit_h_4b8939.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. MPS íƒ€ì… í˜¸í™˜ì„± ê²€ì¦
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                result['mps_available'] = True
                result['mps_type_compatibility'] = True
                result['recommendations'].append("MPS ë””ë°”ì´ìŠ¤ì—ì„œ torch.float32 íƒ€ì… í†µì¼ í•„ìš”")
            else:
                result['mps_available'] = False
            
            # ğŸ”¥ 3. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            total_model_size = sum(self.requirements.model_sizes.values())
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = total_model_size / 1024 * 2
            
            # ğŸ”¥ 4. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = all(self.requirements.model_files.values())
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 5. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            else:
                result['recommendations'].append("ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_04'] = result
        return result
    
    def validate_step_05_cloth_warping(self) -> Dict[str, Any]:
        """Step 5 (Cloth Warping) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Cloth Warping',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'tom_final': 'ai_models/step_05_cloth_warping/tom_final.pth',
                'viton_hd_warping': 'ai_models/step_05_cloth_warping/viton_hd_warping.pth',
                'tps_transformation': 'ai_models/step_05_cloth_warping/tps_transformation.pth',
                'dpt_hybrid_midas': 'ai_models/step_05_cloth_warping/dpt_hybrid_midas.pth',
                'vgg19_warping': 'ai_models/step_05_cloth_warping/vgg19_warping.pth',
                'u2net_warping': 'ai_models/step_05_cloth_warping/u2net_warping.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            total_model_size = sum(self.requirements.model_sizes.values())
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = total_model_size / 1024 * 2
            
            # ğŸ”¥ 3. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = all(self.requirements.model_files.values())
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            else:
                result['recommendations'].append("ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_05'] = result
        return result
    
    def validate_step_06_virtual_fitting(self) -> Dict[str, Any]:
        """Step 6 (Virtual Fitting) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Virtual Fitting',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'stable_diffusion': 'ai_models/step_06_virtual_fitting/stable_diffusion_4.8gb.pth',
                'viton_hd': 'ai_models/step_06_virtual_fitting/viton_hd_2.1gb.pth',
                'ootd': 'ai_models/step_06_virtual_fitting/ootd_3.2gb.pth',
                'hrviton': 'ai_models/step_06_virtual_fitting/hrviton_final.pth',
                'ootd_checkpoint': 'ai_models/step_06_virtual_fitting/validated_checkpoints/ootd_checkpoint.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (ê°€ì¥ í° ë‹¨ê³„)
            total_model_size = sum(self.requirements.model_sizes.values())
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = total_model_size / 1024 * 3  # 3ë°° ë²„í¼ (ê°€ì¥ í° ë‹¨ê³„)
            
            # ğŸ”¥ 3. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = all(self.requirements.model_files.values())
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            else:
                result['recommendations'].append("ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                result['recommendations'].append("ê°€ì¥ í° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë‹¨ê³„ - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í•„ìš”")
                
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_06'] = result
        return result
    
    def validate_step_07_post_processing(self) -> Dict[str, Any]:
        """Step 7 (Post Processing) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Post Processing',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'densenet161_enhance': 'ai_models/step_07_post_processing/densenet161_enhance.pth',
                'mobilenet_v3_ultra': 'ai_models/step_07_post_processing/mobilenet_v3_ultra.pth',
                'GFPGAN': 'ai_models/step_07_post_processing/GFPGAN.pth',
                'resnet101_enhance_ultra': 'ai_models/step_07_post_processing/resnet101_enhance_ultra.pth',
                'RealESRGAN_x2plus': 'ai_models/step_07_post_processing/RealESRGAN_x2plus.pth',
                'ESRGAN_x8': 'ai_models/step_07_post_processing/ESRGAN_x8.pth',
                'swinir_real_sr': 'ai_models/step_07_post_processing/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth',
                'swinir_large': 'ai_models/step_07_post_processing/swinir_real_sr_x4_large.pth',
                'RealESRGAN_x4plus': 'ai_models/step_07_post_processing/RealESRGAN_x4plus.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            total_model_size = sum(self.requirements.model_sizes.values())
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = total_model_size / 1024 * 2
            
            # ğŸ”¥ 3. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = all(self.requirements.model_files.values())
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            else:
                result['recommendations'].append("ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_07'] = result
        return result
    
    def validate_step_08_quality_assessment(self) -> Dict[str, Any]:
        """Step 8 (Quality Assessment) ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        result = {
            'step_name': 'Quality Assessment',
            'validation_time': datetime.now().isoformat(),
            'requirements_met': False,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # ğŸ”¥ 1. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (ì‹¤ì œ ê²½ë¡œ)
            model_files = {
                'clip_vit_b32': 'ai_models/step_08_quality_assessment/clip_vit_b32.pth',
                'alex': 'ai_models/step_08_quality_assessment/alex.pth',
                'ViT_B_32': 'ai_models/step_08_quality_assessment/ViT-B-32.pt',
                'ViT_L_14': 'ai_models/step_08_quality_assessment/ViT-L-14.pt',
                'lpips_alex': 'ai_models/step_08_quality_assessment/lpips_alex.pth'
            }
            
            for model_name, file_path in model_files.items():
                if Path(file_path).exists():
                    size_mb = Path(file_path).stat().st_size / (1024 * 1024)
                    self.requirements.model_files[model_name] = True
                    self.requirements.model_sizes[model_name] = size_mb
                    result[f'{model_name}_loaded'] = True
                    result[f'{model_name}_size_mb'] = size_mb
                else:
                    self.requirements.model_files[model_name] = False
                    result[f'{model_name}_loaded'] = False
                    result['issues'].append(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            
            # ğŸ”¥ 2. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            total_model_size = sum(self.requirements.model_sizes.values())
            result['total_model_size_mb'] = total_model_size
            result['gpu_memory_required_gb'] = total_model_size / 1024 * 2
            
            # ğŸ”¥ 3. ìµœì¢… ê²€ì¦ ê²°ê³¼
            all_requirements_met = all(self.requirements.model_files.values())
            result['requirements_met'] = all_requirements_met
            
            # ğŸ”¥ 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not all_requirements_met:
                if not all(self.requirements.model_files.values()):
                    result['recommendations'].append("ëª¨ë¸ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            else:
                result['recommendations'].append("ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            result['issues'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            result['traceback'] = traceback.format_exc()
        
        self.validation_results['step_08'] = result
        return result
    
    def validate_checkpoint_content(self, checkpoint_path: str) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ê²€ì¦ (ë‹¤ì–‘í•œ êµ¬ì¡° íƒ€ì… ì§€ì›)"""
        result = {
            'checkpoint_path': checkpoint_path,
            'exists': False,
            'valid': False,
            'size_mb': 0.0,
            'structure': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            if not Path(checkpoint_path).exists():
                result['issues'].append("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                return result
            
            result['exists'] = True
            result['size_mb'] = Path(checkpoint_path).stat().st_size / (1024 * 1024)
            
            # ğŸ”¥ ë‹¤ì–‘í•œ ë¡œë”© ë°©ë²• ì‹œë„
            checkpoint = None
            loading_method = None
            
            # ë°©ë²• 1: weights_only=True (ì•ˆì „í•œ ë°©ë²•)
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                loading_method = 'weights_only_true'
                result['recommendations'].append("ì•ˆì „í•œ weights_only=Trueë¡œ ë¡œë”©ë¨")
            except Exception as e1:
                # ë°©ë²• 2: weights_only=False (ì „í†µì ì¸ ë°©ë²•)
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                    loading_method = 'weights_only_false'
                    result['recommendations'].append("weights_only=Falseë¡œ ë¡œë”©ë¨ (ë³´ì•ˆ ì£¼ì˜)")
                except Exception as e2:
                    # ë°©ë²• 3: TorchScript ëª¨ë¸
                    try:
                        checkpoint = torch.jit.load(checkpoint_path, map_location='cpu')
                        loading_method = 'torchscript'
                        result['recommendations'].append("TorchScript ëª¨ë¸ë¡œ ë¡œë”©ë¨")
                    except Exception as e3:
                        # ë°©ë²• 4: SafeTensors (ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)
                        try:
                            from safetensors import safe_open
                            with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
                                checkpoint = {key: f.get_tensor(key) for key in f.keys()}
                            loading_method = 'safetensors'
                            result['recommendations'].append("SafeTensorsë¡œ ë¡œë”©ë¨")
                        except Exception as e4:
                            result['issues'].append(f"ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e4}")
                            return result
            
            result['structure']['loading_method'] = loading_method
            
            # ğŸ”¥ êµ¬ì¡° íƒ€ì… ë¶„ë¥˜ ë° ê²€ì¦
            if isinstance(checkpoint, dict):
                result['structure']['type'] = 'dict'
                result['structure']['keys'] = list(checkpoint.keys())
                result['structure']['key_count'] = len(checkpoint.keys())
                
                # ë‹¤ì–‘í•œ êµ¬ì¡° íƒ€ì… ì²˜ë¦¬
                if 'state_dict' in checkpoint:
                    # í‘œì¤€ PyTorch ëª¨ë¸
                    result['structure']['subtype'] = 'state_dict'
                    state_dict = checkpoint['state_dict']
                    result['structure']['state_dict_keys'] = list(state_dict.keys())[:10]
                    result['structure']['state_dict_count'] = len(state_dict.keys())
                    
                    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                    total_params = 0
                    for key, tensor in state_dict.items():
                        if hasattr(tensor, 'numel'):
                            total_params += tensor.numel()
                    result['structure']['total_parameters'] = total_params
                    
                    # ì•„í‚¤í…ì²˜ ê°ì§€
                    architecture_hints = self._detect_architecture_from_keys(state_dict.keys())
                    result['structure']['architecture_hints'] = architecture_hints
                    
                    result['valid'] = True
                    result['recommendations'].append("í‘œì¤€ state_dict êµ¬ì¡°")
                    
                elif 'model' in checkpoint:
                    # ëª¨ë¸ ë˜í¼ êµ¬ì¡°
                    result['structure']['subtype'] = 'model_wrapper'
                    result['valid'] = True
                    result['recommendations'].append("ëª¨ë¸ ë˜í¼ êµ¬ì¡°")
                    
                elif 'weights' in checkpoint:
                    # ê°€ì¤‘ì¹˜ë§Œ ìˆëŠ” êµ¬ì¡°
                    result['structure']['subtype'] = 'weights_only'
                    result['valid'] = True
                    result['recommendations'].append("ê°€ì¤‘ì¹˜ ì „ìš© êµ¬ì¡°")
                    
                elif 'parameters' in checkpoint:
                    # íŒŒë¼ë¯¸í„°ë§Œ ìˆëŠ” êµ¬ì¡°
                    result['structure']['subtype'] = 'parameters_only'
                    result['valid'] = True
                    result['recommendations'].append("íŒŒë¼ë¯¸í„° ì „ìš© êµ¬ì¡°")
                    
                else:
                    # ì»¤ìŠ¤í…€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°
                    result['structure']['subtype'] = 'custom_dict'
                    
                    # ì»¤ìŠ¤í…€ êµ¬ì¡°ì—ì„œë„ íŒŒë¼ë¯¸í„° ì°¾ê¸° ì‹œë„
                    total_params = 0
                    param_keys = []
                    
                    # ğŸ”¥ ì¤‘ì²©ëœ êµ¬ì¡° ì²˜ë¦¬ (RealESRGAN ë“±)
                    def extract_tensors(obj, prefix=""):
                        nonlocal total_params, param_keys
                        if isinstance(obj, torch.Tensor):
                            total_params += obj.numel()
                            param_keys.append(prefix)
                        elif isinstance(obj, dict):
                            for key, value in obj.items():
                                new_prefix = f"{prefix}.{key}" if prefix else key
                                extract_tensors(value, new_prefix)
                    
                    extract_tensors(checkpoint)
                    
                    if total_params > 0:
                        result['structure']['total_parameters'] = total_params
                        result['structure']['param_keys'] = param_keys[:10]  # ì²˜ìŒ 10ê°œë§Œ
                        result['valid'] = True
                        result['recommendations'].append("ì»¤ìŠ¤í…€ êµ¬ì¡°ì—ì„œ íŒŒë¼ë¯¸í„° ë°œê²¬")
                        
                        # ì•„í‚¤í…ì²˜ íŒíŠ¸ ê°ì§€
                        architecture_hints = self._detect_architecture_from_keys(param_keys)
                        result['structure']['architecture_hints'] = architecture_hints
                    else:
                        result['issues'].append("íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        result['recommendations'].append("ì»¤ìŠ¤í…€ êµ¬ì¡° ê²€ì¦ í•„ìš”")
                        
            elif isinstance(checkpoint, torch.Tensor):
                # ì§ì ‘ í…ì„œ í˜•íƒœ
                result['structure']['type'] = 'tensor'
                result['structure']['shape'] = list(checkpoint.shape)
                result['structure']['dtype'] = str(checkpoint.dtype)
                result['structure']['total_parameters'] = checkpoint.numel()
                result['valid'] = True
                result['recommendations'].append("ì§ì ‘ í…ì„œ í˜•íƒœ")
                
            elif hasattr(checkpoint, 'state_dict'):
                # TorchScript ëª¨ë¸
                result['structure']['type'] = 'torchscript'
                try:
                    state_dict = checkpoint.state_dict()
                    result['structure']['state_dict_keys'] = list(state_dict.keys())[:10]
                    result['structure']['state_dict_count'] = len(state_dict.keys())
                    
                    total_params = 0
                    for key, tensor in state_dict.items():
                        if hasattr(tensor, 'numel'):
                            total_params += tensor.numel()
                    result['structure']['total_parameters'] = total_params
                    
                    result['valid'] = True
                    result['recommendations'].append("TorchScript ëª¨ë¸")
                except Exception as e:
                    result['issues'].append(f"TorchScript state_dict ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                    
            else:
                result['structure']['type'] = str(type(checkpoint))
                result['issues'].append(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(checkpoint)}")
                
        except Exception as e:
            result['issues'].append(f"ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return result
    
    def _detect_architecture_from_keys(self, keys: List[str]) -> List[str]:
        """í‚¤ ëª©ë¡ì—ì„œ ì•„í‚¤í…ì²˜ ê°ì§€"""
        hints = []
        
        # í™•ì¥ëœ ì•„í‚¤í…ì²˜ í‚¤ì›Œë“œ
        architecture_keywords = {
            'graphonomy': ['backbone', 'decoder', 'classifier', 'schp', 'hrnet'],
            'u2net': ['stage1', 'stage2', 'stage3', 'stage4', 'side', 'u2net'],
            'deeplabv3plus': ['backbone', 'decoder', 'classifier', 'aspp', 'deeplab'],
            'gmm': ['feature_extraction', 'regression', 'gmm', 'geometric'],
            'tps': ['localization_net', 'grid_generator', 'tps', 'transformation'],
            'raft': ['feature_encoder', 'context_encoder', 'flow_head', 'raft'],
            'sam': ['image_encoder', 'prompt_encoder', 'mask_decoder', 'sam'],
            'stable_diffusion': ['unet', 'vae', 'text_encoder', 'diffusion', 'model'],
            'ootd': ['unet_vton', 'unet_garm', 'vae', 'ootd'],
            'real_esrgan': ['body', 'upsampling', 'esrgan', 'real_esrgan'],
            'swinir': ['layers', 'patch_embed', 'norm', 'swin', 'swinir'],
            'clip': ['visual', 'transformer', 'text_projection', 'clip'],
            'hrnet': ['hrnet', 'stage', 'transition', 'hrnet_w'],
            'openpose': ['pose', 'body', 'hand', 'face', 'openpose'],
            'yolo': ['yolo', 'detect', 'anchor', 'yolov'],
            'mediapipe': ['mediapipe', 'landmark', 'pose'],
            'viton': ['viton', 'vton', 'warping', 'tom'],
            'dpt': ['dpt', 'depth', 'midas'],
            'efficientnet': ['efficientnet', 'efficient'],
            'resnet': ['resnet', 'residual'],
            'mobilenet': ['mobilenet', 'mobile'],
            'densenet': ['densenet', 'dense']
        }
        
        for arch_name, keywords in architecture_keywords.items():
            matches = sum(1 for keyword in keywords if any(keyword.lower() in key.lower() for key in keys))
            if matches > 0:
                hints.append(f"{arch_name} (ë§¤ì¹­: {matches}ê°œ)")
        
        return hints
    
    def validate_model_architecture(self, checkpoint_path: str, expected_architecture: str = None) -> Dict[str, Any]:
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ê²€ì¦"""
        result = {
            'checkpoint_path': checkpoint_path,
            'architecture_valid': False,
            'expected_architecture': expected_architecture,
            'detected_architecture': None,
            'layer_analysis': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            
            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                
                # ì•„í‚¤í…ì²˜ ê°ì§€
                architecture_indicators = {
                    'graphonomy': ['backbone', 'decoder', 'classifier'],
                    'u2net': ['stage1', 'stage2', 'stage3', 'stage4'],
                    'deeplabv3plus': ['backbone', 'decoder', 'classifier'],
                    'gmm': ['feature_extraction', 'regression'],
                    'tps': ['localization_net', 'grid_generator'],
                    'raft': ['feature_encoder', 'context_encoder', 'flow_head'],
                    'sam': ['image_encoder', 'prompt_encoder', 'mask_decoder'],
                    'stable_diffusion': ['unet', 'vae', 'text_encoder'],
                    'ootd': ['unet_vton', 'unet_garm', 'vae'],
                    'real_esrgan': ['body', 'upsampling'],
                    'swinir': ['layers', 'patch_embed', 'norm'],
                    'clip': ['visual', 'transformer', 'text_projection']
                }
                
                detected_arch = None
                max_matches = 0
                
                for arch_name, indicators in architecture_indicators.items():
                    matches = sum(1 for indicator in indicators if any(indicator in key for key in state_dict.keys()))
                    if matches > max_matches:
                        max_matches = matches
                        detected_arch = arch_name
                
                result['detected_architecture'] = detected_arch
                result['architecture_valid'] = detected_arch is not None
                
                if expected_architecture and detected_arch != expected_architecture:
                    result['issues'].append(f"ì˜ˆìƒ ì•„í‚¤í…ì²˜: {expected_architecture}, ê°ì§€ëœ ì•„í‚¤í…ì²˜: {detected_arch}")
                elif detected_arch:
                    result['recommendations'].append(f"ê°ì§€ëœ ì•„í‚¤í…ì²˜: {detected_arch}")
                
                # ë ˆì´ì–´ ë¶„ì„
                layer_groups = {}
                for key in state_dict.keys():
                    if '.' in key:
                        layer_group = key.split('.')[0]
                        layer_groups[layer_group] = layer_groups.get(layer_group, 0) + 1
                
                result['layer_analysis'] = layer_groups
                
        except Exception as e:
            result['issues'].append(f"ì•„í‚¤í…ì²˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return result
    
    def comprehensive_model_validation(self) -> Dict[str, Any]:
        """ì¢…í•© ëª¨ë¸ ê²€ì¦ (íŒŒì¼ + ì²´í¬í¬ì¸íŠ¸ + ì•„í‚¤í…ì²˜)"""
        comprehensive_result = {
            'validation_time': datetime.now().isoformat(),
            'total_models': 0,
            'valid_models': 0,
            'invalid_models': 0,
            'detailed_results': {},
            'summary': {
                'critical_issues': [],
                'warnings': [],
                'recommendations': []
            }
        }
        
        # ê° ë‹¨ê³„ë³„ ëª¨ë¸ ê²€ì¦
        step_validations = [
            ('step_01', self.validate_step_01_human_parsing),
            ('step_02', self.validate_step_02_pose_estimation),
            ('step_03', self.validate_step_03_cloth_segmentation),
            ('step_04', self.validate_step_04_geometric_matching),
            ('step_05', self.validate_step_05_cloth_warping),
            ('step_06', self.validate_step_06_virtual_fitting),
            ('step_07', self.validate_step_07_post_processing),
            ('step_08', self.validate_step_08_quality_assessment)
        ]
        
        for step_name, validation_func in step_validations:
            step_result = validation_func()
            comprehensive_result['detailed_results'][step_name] = step_result
            
            # ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ê²€ì¦
            if step_result.get('requirements_met', False):
                model_files = {
                    'step_01': {
                        'graphonomy': 'ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth',
                        'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',
                        'deeplabv3plus': 'ai_models/step_01_human_parsing/deeplabv3plus.pth'
                    },
                    'step_02': {
                        'body_pose_model': 'ai_models/step_02_pose_estimation/body_pose_model.pth',
                        'yolov8n_pose': 'ai_models/step_02_pose_estimation/yolov8n-pose.pt',
                        'openpose': 'ai_models/step_02_pose_estimation/openpose.pth',
                        'hrnet_w48': 'ai_models/step_02_pose_estimation/hrnet_w48_coco_384x288.pth'
                    },
                    'step_03': {
                        'sam_vit_h': 'ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',
                        'deeplabv3_resnet101': 'ai_models/step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth',
                        'mobile_sam': 'ai_models/step_03_cloth_segmentation/mobile_sam.pt'
                    },
                    'step_04': {
                        'gmm_final': 'ai_models/step_04_geometric_matching/gmm_final.pth',
                        'tps_network': 'ai_models/step_04_geometric_matching/tps_network.pth',
                        'raft_things': 'ai_models/step_04_geometric_matching/raft-things.pth',
                        'sam_vit_h': 'ai_models/step_04_geometric_matching/sam_vit_h_4b8939.pth'
                    },
                    'step_05': {
                        'tom_final': 'ai_models/step_05_cloth_warping/tom_final.pth',
                        'viton_hd_warping': 'ai_models/step_05_cloth_warping/viton_hd_warping.pth',
                        'tps_transformation': 'ai_models/step_05_cloth_warping/tps_transformation.pth'
                    },
                    'step_06': {
                        'stable_diffusion': 'ai_models/step_06_virtual_fitting/stable_diffusion_4.8gb.pth',
                        'ootd': 'ai_models/step_06_virtual_fitting/ootd_3.2gb.pth',
                        'ootd_checkpoint': 'ai_models/step_06_virtual_fitting/validated_checkpoints/ootd_checkpoint.pth'
                    },
                    'step_07': {
                        'RealESRGAN_x4plus': 'ai_models/step_07_post_processing/RealESRGAN_x4plus.pth',
                        'swinir_large': 'ai_models/step_07_post_processing/swinir_real_sr_x4_large.pth',
                        'GFPGAN': 'ai_models/step_07_post_processing/GFPGAN.pth'
                    },
                    'step_08': {
                        'clip_vit_b32': 'ai_models/step_08_quality_assessment/clip_vit_b32.pth',
                        'ViT_L_14': 'ai_models/step_08_quality_assessment/ViT-L-14.pt',
                        'lpips_alex': 'ai_models/step_08_quality_assessment/lpips_alex.pth'
                    }
                }
                
                if step_name in model_files:
                    step_checkpoint_results = {}
                    for model_name, file_path in model_files[step_name].items():
                        # ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ê²€ì¦
                        checkpoint_result = self.validate_checkpoint_content(file_path)
                        step_checkpoint_results[model_name] = checkpoint_result
                        
                        # ì•„í‚¤í…ì²˜ ê²€ì¦
                        architecture_result = self.validate_model_architecture(file_path)
                        step_checkpoint_results[f"{model_name}_architecture"] = architecture_result
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        comprehensive_result['total_models'] += 1
                        if checkpoint_result['valid']:
                            comprehensive_result['valid_models'] += 1
                        else:
                            comprehensive_result['invalid_models'] += 1
                            comprehensive_result['summary']['critical_issues'].append(f"{step_name}/{model_name}: ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨")
                    
                    comprehensive_result['detailed_results'][f"{step_name}_checkpoints"] = step_checkpoint_results
        
        # ì¢…í•© ê¶Œì¥ì‚¬í•­
        if comprehensive_result['invalid_models'] > 0:
            comprehensive_result['summary']['recommendations'].append(f"{comprehensive_result['invalid_models']}ê°œ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        else:
            comprehensive_result['summary']['recommendations'].append("ëª¨ë“  ëª¨ë¸ì´ ìœ íš¨í•©ë‹ˆë‹¤!")
        
        return comprehensive_result
    
    def get_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'validation_time': datetime.now().isoformat(),
            'total_steps_validated': len(self.validation_results),
            'overall_status': 'unknown',
            'steps': self.validation_results,
            'summary': {
                'total_issues': 0,
                'total_recommendations': 0,
                'critical_issues': [],
                'memory_requirements': {},
                'device_requirements': {}
            }
        }
        
        # ì¢…í•© ë¶„ì„
        all_requirements_met = True
        total_issues = 0
        total_recommendations = 0
        critical_issues = []
        
        for step_name, step_result in self.validation_results.items():
            if not step_result.get('requirements_met', False):
                all_requirements_met = False
            
            total_issues += len(step_result.get('issues', []))
            total_recommendations += len(step_result.get('recommendations', []))
            
            # ì¹˜ëª…ì  ë¬¸ì œë“¤ ìˆ˜ì§‘
            for issue in step_result.get('issues', []):
                if any(keyword in issue.lower() for keyword in ['ì—†ìŒ', 'ì‹¤íŒ¨', 'ì˜¤ë¥˜', 'ë¶€ì¡±']):
                    critical_issues.append(f"{step_name}: {issue}")
        
        report['overall_status'] = 'ready' if all_requirements_met else 'issues_detected'
        report['summary']['total_issues'] = total_issues
        report['summary']['total_recommendations'] = total_recommendations
        report['summary']['critical_issues'] = critical_issues
        
        return report

def get_ai_inference_validator() -> AIInferenceValidator:
    """AI ì¶”ë¡  ê²€ì¦ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if not hasattr(get_ai_inference_validator, '_instance'):
        get_ai_inference_validator._instance = AIInferenceValidator()
    return get_ai_inference_validator._instance

# ğŸ”¥ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    validator = get_ai_inference_validator()
    
    print("ğŸ” AI ì¶”ë¡  ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì‹œì‘...")
    print("=" * 60)
    
    # ê¸°ë³¸ ê²€ì¦
    print("ğŸ“‹ 1ë‹¨ê³„: ê¸°ë³¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦")
    step1_result = validator.validate_step_01_human_parsing()
    print(f"ğŸ“Š Step 1 (Human Parsing): {'âœ…' if step1_result['requirements_met'] else 'âŒ'}")
    
    step2_result = validator.validate_step_02_pose_estimation()
    print(f"ğŸ“Š Step 2 (Pose Estimation): {'âœ…' if step2_result['requirements_met'] else 'âŒ'}")
    
    step3_result = validator.validate_step_03_cloth_segmentation()
    print(f"ğŸ“Š Step 3 (Cloth Segmentation): {'âœ…' if step3_result['requirements_met'] else 'âŒ'}")
    
    step4_result = validator.validate_step_04_geometric_matching()
    print(f"ğŸ“Š Step 4 (Geometric Matching): {'âœ…' if step4_result['requirements_met'] else 'âŒ'}")
    
    step5_result = validator.validate_step_05_cloth_warping()
    print(f"ğŸ“Š Step 5 (Cloth Warping): {'âœ…' if step5_result['requirements_met'] else 'âŒ'}")

    step6_result = validator.validate_step_06_virtual_fitting()
    print(f"ğŸ“Š Step 6 (Virtual Fitting): {'âœ…' if step6_result['requirements_met'] else 'âŒ'}")

    step7_result = validator.validate_step_07_post_processing()
    print(f"ğŸ“Š Step 7 (Post Processing): {'âœ…' if step7_result['requirements_met'] else 'âŒ'}")

    step8_result = validator.validate_step_08_quality_assessment()
    print(f"ğŸ“Š Step 8 (Quality Assessment): {'âœ…' if step8_result['requirements_met'] else 'âŒ'}")
    
    print("\nğŸ“‹ 2ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë° ì•„í‚¤í…ì²˜ ê²€ì¦")
    print("=" * 60)
    
    # ì¢…í•© ê²€ì¦ (ì²´í¬í¬ì¸íŠ¸ + ì•„í‚¤í…ì²˜)
    comprehensive_result = validator.comprehensive_model_validation()
    
    print(f"ğŸ“Š ì´ ëª¨ë¸ ìˆ˜: {comprehensive_result['total_models']}ê°œ")
    print(f"âœ… ìœ íš¨í•œ ëª¨ë¸: {comprehensive_result['valid_models']}ê°œ")
    print(f"âŒ ë¬´íš¨í•œ ëª¨ë¸: {comprehensive_result['invalid_models']}ê°œ")
    
    # ì£¼ìš” ëª¨ë¸ë“¤ì˜ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê²°ê³¼ ì¶œë ¥
    key_models = [
        ('Step 1 - Graphonomy', 'ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth'),
        ('Step 2 - HRNet', 'ai_models/step_02_pose_estimation/hrnet_w48_coco_384x288.pth'),
        ('Step 3 - SAM ViT-H', 'ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth'),
        ('Step 4 - GMM', 'ai_models/step_04_geometric_matching/gmm_final.pth'),
        ('Step 6 - Stable Diffusion', 'ai_models/step_06_virtual_fitting/stable_diffusion_4.8gb.pth'),
        ('Step 6 - OOTD', 'ai_models/step_06_virtual_fitting/ootd_3.2gb.pth')
    ]
    
    for model_name, checkpoint_path in key_models:
        checkpoint_result = validator.validate_checkpoint_content(checkpoint_path)
        architecture_result = validator.validate_model_architecture(checkpoint_path)
        
        status = "âœ…" if checkpoint_result['valid'] else "âŒ"
        arch_status = "âœ…" if architecture_result['architecture_valid'] else "âŒ"
        
        print(f"{status} {model_name}:")
        print(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {checkpoint_result['size_mb']:.1f}MB")
        print(f"   ğŸ”§ ì²´í¬í¬ì¸íŠ¸: {'ìœ íš¨' if checkpoint_result['valid'] else 'ë¬´íš¨'}")
        if checkpoint_result['valid'] and 'structure' in checkpoint_result:
            print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {checkpoint_result['structure'].get('total_parameters', 0):,}")
            print(f"   ğŸ—ï¸ ë ˆì´ì–´ ìˆ˜: {checkpoint_result['structure'].get('state_dict_count', 0)}")
        print(f"   ğŸ›ï¸ ì•„í‚¤í…ì²˜: {arch_status} {architecture_result.get('detected_architecture', 'Unknown')}")
        
        if checkpoint_result['issues']:
            print(f"   âš ï¸ ë¬¸ì œì : {checkpoint_result['issues']}")
    
    # ì¢…í•© ë¦¬í¬íŠ¸
    report = validator.get_comprehensive_report()
    print(f"\nğŸ“‹ ì¢…í•© ìƒíƒœ: {report['overall_status']}")
    print(f"ğŸ”§ ì´ ë¬¸ì œì : {report['summary']['total_issues']}ê°œ")
    print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {report['summary']['total_recommendations']}ê°œ")
    
    if report['summary']['critical_issues']:
        print("\nğŸš¨ ì¹˜ëª…ì  ë¬¸ì œì ë“¤:")
        for issue in report['summary']['critical_issues']:
            print(f"   - {issue}")
    
    if comprehensive_result['summary']['critical_issues']:
        print("\nğŸš¨ ì²´í¬í¬ì¸íŠ¸ ë¬¸ì œì ë“¤:")
        for issue in comprehensive_result['summary']['critical_issues']:
            print(f"   - {issue}")
