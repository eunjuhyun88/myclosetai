#!/usr/bin/env python3
"""
ğŸ”¥ Enhanced Model Loader v7.0 - ê³ ê¸‰ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ
================================================================================
âœ… ì˜ˆì¸¡ ë¡œë”© ë° ìŠ¤ë§ˆíŠ¸ ìºì‹±
âœ… ëª¨ë¸ ê³µìœ  ë©”ì»¤ë‹ˆì¦˜
âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
âœ… ë³‘ë ¬ ë¡œë”© ì‹œìŠ¤í…œ
âœ… ë©”ëª¨ë¦¬ ìµœì í™” ê³ ë„í™”
âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
================================================================================
"""

import os
import sys
import time
import logging
import threading
import asyncio
import gc
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum
import queue
import weakref

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
current_file = Path(__file__).resolve()
backend_root = current_file.parents[3]
sys.path.insert(0, str(backend_root))

# PyTorch ì•ˆì „ import
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ì œí•œëœ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥")

# ì‹œìŠ¤í…œ ì •ë³´
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ê¸°ì¡´ model_loader.pyì˜ ì•„í‚¤í…ì²˜ í´ë˜ìŠ¤ë“¤ import
try:
    from .model_loader import (
        HumanParsingArchitecture,
        PoseEstimationArchitecture,
        ClothSegmentationArchitecture,
        GeometricMatchingArchitecture,
        VirtualFittingArchitecture,
        ClothWarpingArchitecture,
        StepSpecificArchitecture
    )
    ARCHITECTURE_AVAILABLE = True
except ImportError:
    ARCHITECTURE_AVAILABLE = False
    print("âš ï¸ ê¸°ì¡´ ì•„í‚¤í…ì²˜ í´ë˜ìŠ¤ import ì‹¤íŒ¨ - ê¸°ë³¸ ì•„í‚¤í…ì²˜ ì‚¬ìš©")

# ==============================================
# ğŸ”¥ 1. ê³ ê¸‰ ìºì‹± ì „ëµ
# ==============================================

class CacheStrategy(Enum):
    """ìºì‹± ì „ëµ"""
    LRU = "lru"
    LFU = "lfu"
    ADAPTIVE = "adaptive"
    PREDICTIVE = "predictive"

@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    data: Any
    size_mb: float
    access_count: int = 0
    last_access: float = field(default_factory=time.time)
    creation_time: float = field(default_factory=time.time)
    priority: float = 1.0
    predicted_next_access: Optional[float] = None

class PredictiveCache:
    """ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ìºì‹œ"""
    
    def __init__(self, max_size_mb: float = 1024, strategy: CacheStrategy = CacheStrategy.ADAPTIVE):
        self.max_size_mb = max_size_mb
        self.strategy = strategy
        self.cache: Dict[str, CacheEntry] = {}
        self.access_patterns: Dict[str, List[float]] = {}
        self.prediction_model = self._create_prediction_model()
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.PredictiveCache")
        
        # ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        self._cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)
        self._cleanup_thread.start()
    
    def _create_prediction_model(self) -> Dict[str, float]:
        """ê°„ë‹¨í•œ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±"""
        return {}
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        with self._lock:
            if key in self.cache:
                entry = self.cache[key]
                entry.access_count += 1
                entry.last_access = time.time()
                
                # ì ‘ê·¼ íŒ¨í„´ ê¸°ë¡
                if key not in self.access_patterns:
                    self.access_patterns[key] = []
                self.access_patterns[key].append(time.time())
                
                # ì˜ˆì¸¡ ëª¨ë¸ ì—…ë°ì´íŠ¸
                self._update_prediction_model(key)
                
                return entry.data
            return None
    
    def set(self, key: str, data: Any, size_mb: float, ttl: Optional[float] = None):
        """ìºì‹œì— ì €ì¥"""
        with self._lock:
            # ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬
            if self._get_total_size() + size_mb > self.max_size_mb:
                self._evict_entries(size_mb)
            
            # ìƒˆ ì—”íŠ¸ë¦¬ ìƒì„±
            entry = CacheEntry(
                data=data,
                size_mb=size_mb,
                priority=self._calculate_priority(key)
            )
            
            self.cache[key] = entry
            
            # ì˜ˆì¸¡ ëª¨ë¸ì— ì¶”ê°€
            if key not in self.prediction_model:
                self.prediction_model[key] = time.time() + 3600  # 1ì‹œê°„ í›„ ì˜ˆì¸¡
    
    def _calculate_priority(self, key: str) -> float:
        """ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        base_priority = 1.0
        
        # ì ‘ê·¼ ë¹ˆë„ ê¸°ë°˜
        if key in self.access_patterns:
            recent_accesses = [t for t in self.access_patterns[key] if time.time() - t < 3600]
            base_priority += len(recent_accesses) * 0.1
        
        # ì˜ˆì¸¡ ê¸°ë°˜
        if key in self.prediction_model:
            predicted_time = self.prediction_model[key]
            time_until_predicted = predicted_time - time.time()
            if time_until_predicted < 300:  # 5ë¶„ ë‚´ ì˜ˆì¸¡
                base_priority += 2.0
        
        return base_priority
    
    def _evict_entries(self, required_size_mb: float):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if self.strategy == CacheStrategy.LRU:
            self._evict_lru(required_size_mb)
        elif self.strategy == CacheStrategy.LFU:
            self._evict_lfu(required_size_mb)
        elif self.strategy == CacheStrategy.ADAPTIVE:
            self._evict_adaptive(required_size_mb)
        else:
            self._evict_predictive(required_size_mb)
    
    def _evict_lru(self, required_size_mb: float):
        """LRU ê¸°ë°˜ ì œê±°"""
        sorted_entries = sorted(
            self.cache.items(),
            key=lambda x: x[1].last_access
        )
        
        freed_size = 0.0
        for key, entry in sorted_entries:
            if freed_size >= required_size_mb:
                break
            del self.cache[key]
            freed_size += entry.size_mb
    
    def _evict_lfu(self, required_size_mb: float):
        """LFU ê¸°ë°˜ ì œê±°"""
        sorted_entries = sorted(
            self.cache.items(),
            key=lambda x: x[1].access_count
        )
        
        freed_size = 0.0
        for key, entry in sorted_entries:
            if freed_size >= required_size_mb:
                break
            del self.cache[key]
            freed_size += entry.size_mb
    
    def _evict_adaptive(self, required_size_mb: float):
        """ì ì‘í˜• ì œê±° (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        sorted_entries = sorted(
            self.cache.items(),
            key=lambda x: x[1].priority,
            reverse=True
        )
        
        # ë‚®ì€ ìš°ì„ ìˆœìœ„ë¶€í„° ì œê±°
        entries_to_remove = sorted_entries[:-len(sorted_entries)//2]  # í•˜ìœ„ 50% ì œê±°
        
        freed_size = 0.0
        for key, entry in entries_to_remove:
            if freed_size >= required_size_mb:
                break
            del self.cache[key]
            freed_size += entry.size_mb
    
    def _evict_predictive(self, required_size_mb: float):
        """ì˜ˆì¸¡ ê¸°ë°˜ ì œê±°"""
        current_time = time.time()
        
        # ì˜ˆì¸¡ ì‹œê°„ì´ ê°€ê¹Œìš´ ê²ƒë“¤ì€ ë³´ì¡´
        entries_with_prediction = []
        for key, entry in self.cache.items():
            if key in self.prediction_model:
                predicted_time = self.prediction_model[key]
                time_until_predicted = predicted_time - current_time
                entries_with_prediction.append((key, entry, time_until_predicted))
        
        # ì˜ˆì¸¡ ì‹œê°„ì´ ë¨¼ ê²ƒë¶€í„° ì œê±°
        entries_with_prediction.sort(key=lambda x: x[2], reverse=True)
        
        freed_size = 0.0
        for key, entry, _ in entries_with_prediction:
            if freed_size >= required_size_mb:
                break
            del self.cache[key]
            freed_size += entry.size_mb
    
    def _get_total_size(self) -> float:
        """ìºì‹œ ì´ í¬ê¸°"""
        return sum(entry.size_mb for entry in self.cache.values())
    
    def _update_prediction_model(self, key: str):
        """ì˜ˆì¸¡ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if key in self.access_patterns:
            pattern = self.access_patterns[key]
            if len(pattern) >= 3:
                # ê°„ë‹¨í•œ ì„ í˜• ì˜ˆì¸¡
                intervals = [pattern[i] - pattern[i-1] for i in range(1, len(pattern))]
                avg_interval = sum(intervals) / len(intervals)
                self.prediction_model[key] = time.time() + avg_interval
    
    def _cleanup_worker(self):
        """ìºì‹œ ì •ë¦¬ ì›Œì»¤"""
        while True:
            try:
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì •ë¦¬
                with self._lock:
                    current_time = time.time()
                    
                    # ì˜¤ë˜ëœ ì ‘ê·¼ íŒ¨í„´ ì •ë¦¬
                    for key in list(self.access_patterns.keys()):
                        self.access_patterns[key] = [
                            t for t in self.access_patterns[key]
                            if current_time - t < 3600  # 1ì‹œê°„ ì´ë‚´ë§Œ ìœ ì§€
                        ]
                        if not self.access_patterns[key]:
                            del self.access_patterns[key]
                    
                    # ìºì‹œ í¬ê¸° ì¡°ì •
                    if self._get_total_size() > self.max_size_mb * 0.9:
                        self._evict_entries(self.max_size_mb * 0.1)
                        
            except Exception as e:
                self.logger.warning(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 2. ëª¨ë¸ ê³µìœ  ë©”ì»¤ë‹ˆì¦˜
# ==============================================

class SharedModelRegistry:
    """ëª¨ë¸ ê³µìœ  ë ˆì§€ìŠ¤íŠ¸ë¦¬"""
    
    def __init__(self):
        self.shared_models: Dict[str, nn.Module] = {}
        self.model_references: Dict[str, List[str]] = {}
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.SharedModelRegistry")
    
    def register_shared_model(self, model_name: str, model: nn.Module, step_types: List[str]):
        """ê³µìœ  ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            self.shared_models[model_name] = model
            self.model_references[model_name] = step_types
            self.logger.info(f"âœ… ê³µìœ  ëª¨ë¸ ë“±ë¡: {model_name} -> {step_types}")
    
    def get_shared_model(self, model_name: str) -> Optional[nn.Module]:
        """ê³µìœ  ëª¨ë¸ ì¡°íšŒ"""
        with self._lock:
            return self.shared_models.get(model_name)
    
    def is_shared_model(self, model_name: str) -> bool:
        """ê³µìœ  ëª¨ë¸ ì—¬ë¶€ í™•ì¸"""
        return model_name in self.shared_models
    
    def get_shared_components(self, step_type: str) -> Dict[str, nn.Module]:
        """Stepë³„ ê³µìœ  ì»´í¬ë„ŒíŠ¸ ì¡°íšŒ"""
        with self._lock:
            shared_components = {}
            for model_name, step_types in self.model_references.items():
                if step_type in step_types:
                    shared_components[model_name] = self.shared_models[model_name]
            return shared_components

# ==============================================
# ğŸ”¥ 3. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
# ==============================================

class GradientCheckpointingManager:
    """ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ê´€ë¦¬ì"""
    
    def __init__(self):
        self.checkpointed_models: Dict[str, nn.Module] = {}
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.GradientCheckpointingManager")
    
    def enable_checkpointing(self, model: nn.Module, model_name: str) -> nn.Module:
        """ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”"""
        if not TORCH_AVAILABLE:
            return model
        
        try:
            with self._lock:
                # ì´ë¯¸ ì²´í¬í¬ì¸íŒ…ëœ ëª¨ë¸ì¸ì§€ í™•ì¸
                if model_name in self.checkpointed_models:
                    return self.checkpointed_models[model_name]
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì ìš©
                if hasattr(model, 'gradient_checkpointing_enable'):
                    model.gradient_checkpointing_enable()
                    self.logger.info(f"âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”: {model_name}")
                else:
                    # ìˆ˜ë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŒ… ì ìš©
                    model = self._apply_manual_checkpointing(model)
                    self.logger.info(f"âœ… ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì ìš©: {model_name}")
                
                self.checkpointed_models[model_name] = model
                return model
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‹¤íŒ¨: {e}")
            return model
    
    def _apply_manual_checkpointing(self, model: nn.Module) -> nn.Module:
        """ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì ìš©"""
        # ê°„ë‹¨í•œ ë˜í¼ í´ë˜ìŠ¤ë¡œ ì²´í¬í¬ì¸íŒ… êµ¬í˜„
        class CheckpointedModel(nn.Module):
            def __init__(self, base_model):
                super().__init__()
                self.base_model = base_model
            
            def forward(self, *args, **kwargs):
                return torch.utils.checkpoint.checkpoint(
                    self.base_model, *args, **kwargs
                )
        
        return CheckpointedModel(model)

# ==============================================
# ğŸ”¥ 4. ë³‘ë ¬ ë¡œë”© ì‹œìŠ¤í…œ
# ==============================================

class ParallelModelLoader:
    """ë³‘ë ¬ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.loading_queue = queue.Queue()
        self.loading_results: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.ParallelModelLoader")
        
        # ë¡œë”© ì›Œì»¤ ì‹œì‘
        self._start_loading_workers()
    
    def _start_loading_workers(self):
        """ë¡œë”© ì›Œì»¤ ì‹œì‘"""
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._loading_worker, daemon=True)
            worker.start()
    
    def _loading_worker(self):
        """ë¡œë”© ì›Œì»¤"""
        while True:
            try:
                task = self.loading_queue.get(timeout=1)
                if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                model_id, load_func, args, kwargs = task
                
                try:
                    result = load_func(*args, **kwargs)
                    with self._lock:
                        self.loading_results[model_id] = {
                            'success': True,
                            'result': result,
                            'timestamp': time.time()
                        }
                except Exception as e:
                    with self._lock:
                        self.loading_results[model_id] = {
                            'success': False,
                            'error': str(e),
                            'timestamp': time.time()
                        }
                
                self.loading_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"ë¡œë”© ì›Œì»¤ ì˜¤ë¥˜: {e}")
    
    def submit_loading_task(self, model_id: str, load_func: Callable, *args, **kwargs):
        """ë¡œë”© ì‘ì—… ì œì¶œ"""
        task = (model_id, load_func, args, kwargs)
        self.loading_queue.put(task)
        self.logger.debug(f"ë¡œë”© ì‘ì—… ì œì¶œ: {model_id}")
    
    def get_loading_result(self, model_id: str, timeout: float = 30.0) -> Optional[Any]:
        """ë¡œë”© ê²°ê³¼ ì¡°íšŒ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            with self._lock:
                if model_id in self.loading_results:
                    result = self.loading_results[model_id]
                    if result['success']:
                        return result['result']
                    else:
                        raise Exception(f"ë¡œë”© ì‹¤íŒ¨: {result['error']}")
            
            time.sleep(0.1)
        
        raise TimeoutError(f"ë¡œë”© íƒ€ì„ì•„ì›ƒ: {model_id}")
    
    def shutdown(self):
        """ì¢…ë£Œ"""
        for _ in range(self.max_workers):
            self.loading_queue.put(None)
        self.executor.shutdown(wait=True)

# ==============================================
# ğŸ”¥ 5. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
# ==============================================

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    load_time: float
    memory_usage_mb: float
    cache_hit_rate: float
    throughput: float
    error_rate: float
    timestamp: float

class PerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.max_history_size = 1000
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.PerformanceMonitor")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self._start_monitoring()
    
    def record_metric(self, metric: PerformanceMetrics):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        with self._lock:
            self.metrics_history.append(metric)
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
            if len(self.metrics_history) > self.max_history_size:
                self.metrics_history = self.metrics_history[-self.max_history_size:]
    
    def get_recent_metrics(self, minutes: int = 5) -> List[PerformanceMetrics]:
        """ìµœê·¼ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        with self._lock:
            cutoff_time = time.time() - (minutes * 60)
            return [
                m for m in self.metrics_history
                if m.timestamp >= cutoff_time
            ]
    
    def get_average_metrics(self, minutes: int = 5) -> Dict[str, float]:
        """í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        recent_metrics = self.get_recent_metrics(minutes)
        
        if not recent_metrics:
            return {}
        
        return {
            'avg_load_time': sum(m.load_time for m in recent_metrics) / len(recent_metrics),
            'avg_memory_usage': sum(m.memory_usage_mb for m in recent_metrics) / len(recent_metrics),
            'avg_cache_hit_rate': sum(m.cache_hit_rate for m in recent_metrics) / len(recent_metrics),
            'avg_throughput': sum(m.throughput for m in recent_metrics) / len(recent_metrics),
            'avg_error_rate': sum(m.error_rate for m in recent_metrics) / len(recent_metrics)
        }
    
    def _start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        def monitor_worker():
            while True:
                try:
                    time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                    
                    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    if PSUTIL_AVAILABLE:
                        memory = psutil.virtual_memory()
                        memory_usage_mb = memory.used / (1024 * 1024)
                        
                        metric = PerformanceMetrics(
                            load_time=0.0,
                            memory_usage_mb=memory_usage_mb,
                            cache_hit_rate=0.0,
                            throughput=0.0,
                            error_rate=0.0,
                            timestamp=time.time()
                        )
                        
                        self.record_metric(metric)
                        
                except Exception as e:
                    self.logger.warning(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
        
        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()

# ==============================================
# ğŸ”¥ 6. Enhanced Model Loader (ë©”ì¸ í´ë˜ìŠ¤)
# ==============================================

class EnhancedModelLoader:
    """ê³ ê¸‰ ëª¨ë¸ ë¡œë” v7.0"""
    
    def __init__(self, 
                 device: str = "auto",
                 cache_size_mb: float = 2048,
                 max_workers: int = 4,
                 enable_checkpointing: bool = True):
        
        self.device = self._detect_device(device)
        self.logger = logging.getLogger(f"{__name__}.EnhancedModelLoader")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cache = PredictiveCache(max_size_mb=cache_size_mb)
        self.shared_registry = SharedModelRegistry()
        self.checkpointing_manager = GradientCheckpointingManager()
        self.parallel_loader = ParallelModelLoader(max_workers=max_workers)
        self.performance_monitor = PerformanceMonitor()
        
        # ì„¤ì •
        self.enable_checkpointing = enable_checkpointing
        
        # ë¡œë”©ëœ ëª¨ë¸ë“¤
        self.loaded_models: Dict[str, nn.Module] = {}
        self.model_metadata: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()
        
        self.logger.info(f"ğŸš€ Enhanced Model Loader v7.0 ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _detect_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ê°ì§€"""
        if device == "auto":
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
                else:
                    return "cpu"
            else:
                return "cpu"
        return device
    
    def load_model(self, 
                  model_path: str, 
                  model_name: str,
                  step_type: str,
                  enable_parallel: bool = True) -> Optional[nn.Module]:
        """ëª¨ë¸ ë¡œë”© (ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨)"""
        
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"{model_name}_{step_type}"
            cached_model = self.cache.get(cache_key)
            if cached_model:
                self._record_performance_metric(start_time, cache_hit=True)
                return cached_model
            
            # ê³µìœ  ëª¨ë¸ í™•ì¸
            if self.shared_registry.is_shared_model(model_name):
                shared_model = self.shared_registry.get_shared_model(model_name)
                if shared_model:
                    self._record_performance_metric(start_time, cache_hit=True)
                    return shared_model
            
            # ì‹¤ì œ ë¡œë”©
            if enable_parallel:
                return self._load_model_parallel(model_path, model_name, step_type, start_time)
            else:
                return self._load_model_sequential(model_path, model_name, step_type, start_time)
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self._record_performance_metric(start_time, error=True)
            return None
    
    def _load_model_parallel(self, model_path: str, model_name: str, step_type: str, start_time: float) -> Optional[nn.Module]:
        """ë³‘ë ¬ ëª¨ë¸ ë¡œë”©"""
        model_id = f"{model_name}_{step_type}"
        
        # ë¡œë”© ì‘ì—… ì œì¶œ
        self.parallel_loader.submit_loading_task(
            model_id, 
            self._load_model_worker, 
            model_path, 
            model_name, 
            step_type
        )
        
        # ê²°ê³¼ ëŒ€ê¸°
        try:
            model = self.parallel_loader.get_loading_result(model_id)
            
            # ìºì‹œì— ì €ì¥
            model_size = self._estimate_model_size(model)
            self.cache.set(model_id, model, model_size)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with self._lock:
                self.loaded_models[model_id] = model
                self.model_metadata[model_id] = {
                    'path': model_path,
                    'name': model_name,
                    'step_type': step_type,
                    'load_time': time.time() - start_time,
                    'size_mb': model_size
                }
            
            self._record_performance_metric(start_time, cache_hit=False)
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_model_sequential(self, model_path: str, model_name: str, step_type: str, start_time: float) -> Optional[nn.Module]:
        """ìˆœì°¨ ëª¨ë¸ ë¡œë”©"""
        try:
            model = self._load_model_worker(model_path, model_name, step_type)
            
            # ìºì‹œì— ì €ì¥
            model_id = f"{model_name}_{step_type}"
            model_size = self._estimate_model_size(model)
            self.cache.set(model_id, model, model_size)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with self._lock:
                self.loaded_models[model_id] = model
                self.model_metadata[model_id] = {
                    'path': model_path,
                    'name': model_name,
                    'step_type': step_type,
                    'load_time': time.time() - start_time,
                    'size_mb': model_size
                }
            
            self._record_performance_metric(start_time, cache_hit=False)
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_model_worker(self, model_path: str, model_name: str, step_type: str) -> nn.Module:
        """ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ì›Œì»¤"""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘: {model_path}")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
        checkpoint_analysis = self._analyze_checkpoint(model_path)
        
        # Stepë³„ íŠ¹í™” ì•„í‚¤í…ì²˜ ìƒì„±
        architecture = self._create_step_architecture(step_type)
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±
        model = architecture.create_model(checkpoint_analysis)
        
        # ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë”©
        success = self._load_checkpoint_weights(model, model_path, checkpoint_analysis, architecture)
        
        if not success:
            raise RuntimeError(f"ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {model_path}")
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        model = model.to(self.device)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì ìš©
        if self.enable_checkpointing:
            model = self.checkpointing_manager.enable_checkpointing(model, model_name)
        
        self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model_path}")
        return model
    
    def _analyze_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„"""
        try:
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = self._load_checkpoint_safe(checkpoint_path)
            
            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif isinstance(checkpoint, dict):
                state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
            analysis = {
                'state_dict': state_dict,
                'total_params': sum(p.numel() for p in state_dict.values() if hasattr(p, 'numel')),
                'layer_count': len(state_dict),
                'key_patterns': self._analyze_key_patterns(state_dict),
                'layer_types': self._analyze_layer_types(state_dict),
                'has_batch_norm': self._has_batch_normalization(state_dict),
                'has_attention': self._has_attention_layers(state_dict),
                'model_depth': self._estimate_model_depth(state_dict),
                'parameter_counts': self._count_parameters_by_type(state_dict)
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def _load_checkpoint_safe(self, checkpoint_path: str) -> Any:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (3ë‹¨ê³„)"""
        # 1ë‹¨ê³„: weights_only=True (ìµœê³  ë³´ì•ˆ)
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            return checkpoint
        except Exception as e1:
            self.logger.debug(f"1ë‹¨ê³„ ë¡œë”© ì‹¤íŒ¨: {e1}")
        
        # 2ë‹¨ê³„: weights_only=False (í˜¸í™˜ì„±)
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            return checkpoint
        except Exception as e2:
            self.logger.debug(f"2ë‹¨ê³„ ë¡œë”© ì‹¤íŒ¨: {e2}")
        
        # 3ë‹¨ê³„: Legacy ëª¨ë“œ
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
            return checkpoint
        except Exception as e3:
            raise RuntimeError(f"ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e3}")
    
    def _create_step_architecture(self, step_type: str):
        """Stepë³„ íŠ¹í™” ì•„í‚¤í…ì²˜ ìƒì„±"""
        # ê¸°ì¡´ model_loader.pyì˜ ì•„í‚¤í…ì²˜ í´ë˜ìŠ¤ë“¤ì„ í™œìš©
        if step_type == 'human_parsing':
            return HumanParsingArchitecture(step_type, self.device)
        elif step_type == 'pose_estimation':
            return PoseEstimationArchitecture(step_type, self.device)
        elif step_type == 'cloth_segmentation':
            return ClothSegmentationArchitecture(step_type, self.device)
        elif step_type == 'geometric_matching':
            return GeometricMatchingArchitecture(step_type, self.device)
        elif step_type == 'virtual_fitting':
            return VirtualFittingArchitecture(step_type, self.device)
        elif step_type == 'cloth_warping':
            return ClothWarpingArchitecture(step_type, self.device)
        else:
            # ê¸°ë³¸ ì•„í‚¤í…ì²˜
            return self._create_generic_architecture(step_type)
    
    def _create_generic_architecture(self, step_type: str):
        """ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class GenericArchitecture:
            def __init__(self, step_name: str, device: str):
                self.step_name = step_name
                self.device = device
            
            def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
                # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë™ì  ëª¨ë¸ ìƒì„±
                state_dict = checkpoint_analysis['state_dict']
                
                # ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„±
                class GenericModel(nn.Module):
                    def __init__(self, input_channels=3, num_classes=1):
                        super().__init__()
                        self.features = nn.Sequential(
                            nn.Conv2d(input_channels, 64, 3, padding=1),
                            nn.ReLU(inplace=True),
                            nn.Conv2d(64, 128, 3, padding=1),
                            nn.ReLU(inplace=True),
                            nn.Conv2d(128, 256, 3, padding=1),
                            nn.ReLU(inplace=True)
                        )
                        self.classifier = nn.Conv2d(256, num_classes, 1)
                    
                    def forward(self, x):
                        x = self.features(x)
                        x = self.classifier(x)
                        return x
                
                return GenericModel()
            
            def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
                return checkpoint
            
            def validate_model(self, model) -> bool:
                return True
        
        return GenericArchitecture(step_type, self.device)
    
    def _load_checkpoint_weights(self, model: nn.Module, checkpoint_path: str, 
                                analysis: Dict[str, Any], architecture) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ë¡œë”©"""
        try:
            state_dict = analysis['state_dict']
            
            # í‚¤ ë§¤í•‘ ì ìš©
            mapped_state_dict = architecture.map_checkpoint_keys(state_dict)
            
            # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë”©
            model.load_state_dict(mapped_state_dict, strict=False)
            
            self.logger.info(f"âœ… ê°€ì¤‘ì¹˜ ë¡œë”© ì„±ê³µ: {checkpoint_path}")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨ (strict=False ì‹œë„): {e}")
            
            # strict=Falseë¡œ ì¬ì‹œë„
            try:
                model.load_state_dict(mapped_state_dict, strict=False)
                self.logger.info(f"âœ… ê°€ì¤‘ì¹˜ ë¡œë”© ì„±ê³µ (strict=False): {checkpoint_path}")
                return True
            except Exception as e2:
                self.logger.error(f"âŒ ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {e2}")
                return False
    
    def _analyze_key_patterns(self, state_dict: Dict[str, Any]) -> Dict[str, List[str]]:
        """í‚¤ íŒ¨í„´ ë¶„ì„"""
        patterns = {
            'conv': [],
            'bn': [],
            'linear': [],
            'attention': [],
            'embedding': []
        }
        
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                patterns['conv'].append(key)
            elif any(kw in key_lower for kw in ['bn', 'batch_norm']):
                patterns['bn'].append(key)
            elif any(kw in key_lower for kw in ['linear', 'fc', 'classifier']):
                patterns['linear'].append(key)
            elif any(kw in key_lower for kw in ['attn', 'attention']):
                patterns['attention'].append(key)
            elif 'embed' in key_lower:
                patterns['embedding'].append(key)
        
        return patterns
    
    def _analyze_layer_types(self, state_dict: Dict[str, Any]) -> Dict[str, int]:
        """ë ˆì´ì–´ íƒ€ì… ë¶„ì„"""
        layer_counts = {
            'conv': 0,
            'bn': 0,
            'linear': 0,
            'attention': 0,
            'embedding': 0
        }
        
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                layer_counts['conv'] += 1
            elif any(kw in key_lower for kw in ['bn', 'batch_norm']):
                layer_counts['bn'] += 1
            elif any(kw in key_lower for kw in ['linear', 'fc', 'classifier']):
                layer_counts['linear'] += 1
            elif any(kw in key_lower for kw in ['attn', 'attention']):
                layer_counts['attention'] += 1
            elif 'embed' in key_lower:
                layer_counts['embedding'] += 1
        
        return layer_counts
    
    def _has_batch_normalization(self, state_dict: Dict[str, Any]) -> bool:
        """BatchNorm ë ˆì´ì–´ ì¡´ì¬ ì—¬ë¶€"""
        return any('bn' in key.lower() or 'batch_norm' in key.lower() for key in state_dict.keys())
    
    def _has_attention_layers(self, state_dict: Dict[str, Any]) -> bool:
        """Attention ë ˆì´ì–´ ì¡´ì¬ ì—¬ë¶€"""
        return any(keyword in key.lower() for key in state_dict.keys() 
                  for keyword in ['attn', 'attention', 'self_attn', 'cross_attn'])
    
    def _extract_metadata(self, checkpoint: Any) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        if isinstance(checkpoint, dict):
            # state_dict í˜•íƒœ
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                metadata['total_parameters'] = sum(tensor.numel() for tensor in state_dict.values() if hasattr(tensor, 'numel'))
                metadata['total_keys'] = len(state_dict)
            else:
                metadata['total_parameters'] = sum(tensor.numel() for tensor in checkpoint.values() if hasattr(tensor, 'numel'))
                metadata['total_keys'] = len(checkpoint)
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            for key in ['epoch', 'step', 'optimizer', 'scheduler', 'config', 'args']:
                if key in checkpoint:
                    metadata[key] = checkpoint[key]
        
        return metadata
    
    def _estimate_model_depth(self, state_dict: Dict[str, Any]) -> int:
        """ëª¨ë¸ ê¹Šì´ ì¶”ì •"""
        # ë ˆì´ì–´ ë²ˆí˜¸ë¡œ ê¹Šì´ ì¶”ì •
        layer_numbers = []
        for key in state_dict.keys():
            # ìˆ«ì ì¶”ì¶œ (ì˜ˆ: layer1.0.conv1.weight -> 1)
            import re
            numbers = re.findall(r'\d+', key)
            if numbers:
                layer_numbers.extend([int(n) for n in numbers])
        
        return max(layer_numbers) if layer_numbers else 10
    
    def _count_parameters_by_type(self, state_dict: Dict[str, Any]) -> Dict[str, int]:
        """íƒ€ì…ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        param_counts = {
            'conv_params': 0,
            'linear_params': 0,
            'norm_params': 0,
            'embedding_params': 0,
            'total_params': 0
        }
        
        for key, tensor in state_dict.items():
            if hasattr(tensor, 'numel'):
                param_count = tensor.numel()
                param_counts['total_params'] += param_count
                
                key_lower = key.lower()
                if 'conv' in key_lower:
                    param_counts['conv_params'] += param_count
                elif any(kw in key_lower for kw in ['linear', 'fc', 'classifier']):
                    param_counts['linear_params'] += param_count
                elif any(kw in key_lower for kw in ['bn', 'norm']):
                    param_counts['norm_params'] += param_count
                elif 'embed' in key_lower:
                    param_counts['embedding_params'] += param_count
        
        return param_counts
    
    def _estimate_model_size(self, model: nn.Module) -> float:
        """ëª¨ë¸ í¬ê¸° ì¶”ì • (MB)"""
        if not TORCH_AVAILABLE:
            return 100.0  # ê¸°ë³¸ê°’
        
        try:
            param_size = 0
            buffer_size = 0
            
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
            
            size_mb = (param_size + buffer_size) / (1024 * 1024)
            return size_mb
        except:
            return 100.0  # ê¸°ë³¸ê°’
    
    def _record_performance_metric(self, start_time: float, cache_hit: bool = False, error: bool = False):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        load_time = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_usage_mb = 0.0
        if PSUTIL_AVAILABLE:
            memory = psutil.virtual_memory()
            memory_usage_mb = memory.used / (1024 * 1024)
        
        # ìºì‹œ íˆíŠ¸ìœ¨ (ê°„ë‹¨í•œ ê³„ì‚°)
        cache_hit_rate = 1.0 if cache_hit else 0.0
        
        # ì²˜ë¦¬ëŸ‰ (ê°„ë‹¨í•œ ê³„ì‚°)
        throughput = 1.0 / max(load_time, 0.001)
        
        # ì—ëŸ¬ìœ¨
        error_rate = 1.0 if error else 0.0
        
        metric = PerformanceMetrics(
            load_time=load_time,
            memory_usage_mb=memory_usage_mb,
            cache_hit_rate=cache_hit_rate,
            throughput=throughput,
            error_rate=error_rate,
            timestamp=time.time()
        )
        
        self.performance_monitor.record_metric(metric)
    
    def get_performance_stats(self, minutes: int = 5) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        avg_metrics = self.performance_monitor.get_average_metrics(minutes)
        
        return {
            'average_metrics': avg_metrics,
            'cache_stats': {
                'total_size_mb': self.cache._get_total_size(),
                'entry_count': len(self.cache.cache)
            },
            'loaded_models_count': len(self.loaded_models),
            'shared_models_count': len(self.shared_registry.shared_models)
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ë³‘ë ¬ ë¡œë” ì¢…ë£Œ
            self.parallel_loader.shutdown()
            
            # ìºì‹œ ì •ë¦¬
            self.cache.cache.clear()
            
            # ëª¨ë¸ ì–¸ë¡œë“œ
            with self._lock:
                self.loaded_models.clear()
                self.model_metadata.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            self.logger.info("âœ… Enhanced Model Loader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 7. íŒ©í† ë¦¬ í•¨ìˆ˜
# ==============================================

def create_enhanced_model_loader(device: str = "auto", 
                                cache_size_mb: float = 2048,
                                max_workers: int = 4,
                                enable_checkpointing: bool = True) -> EnhancedModelLoader:
    """ê³ ê¸‰ ëª¨ë¸ ë¡œë” ìƒì„±"""
    return EnhancedModelLoader(
        device=device,
        cache_size_mb=cache_size_mb,
        max_workers=max_workers,
        enable_checkpointing=enable_checkpointing
    )

# ==============================================
# ğŸ”¥ 8. ì‚¬ìš© ì˜ˆì‹œ
# ==============================================

if __name__ == "__main__":
    # ë¡œê±° ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ê³ ê¸‰ ëª¨ë¸ ë¡œë” ìƒì„±
    loader = create_enhanced_model_loader(
        device="auto",
        cache_size_mb=1024,
        max_workers=2,
        enable_checkpointing=True
    )
    
    try:
        # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        model = loader.load_model(
            model_path="test_model.pth",
            model_name="test_model",
            step_type="human_parsing",
            enable_parallel=True
        )
        
        if model:
            print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            
            # ì„±ëŠ¥ í†µê³„ ì¡°íšŒ
            stats = loader.get_performance_stats()
            print(f"ğŸ“Š ì„±ëŠ¥ í†µê³„: {stats}")
        else:
            print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
    
    finally:
        # ì •ë¦¬
        loader.cleanup()
