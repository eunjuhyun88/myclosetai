# backend/app/ai_pipeline/utils/performance_optimizer.py
"""
âš¡ MyCloset AI - ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ v1.0
==========================================
âœ… M3 Max 128GB íŠ¹í™” ìµœì í™”
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìºì‹± ì‹œìŠ¤í…œ
âœ… GPU/MPS ìµœì í™”
âœ… ëª¨ë¸ ë¡œë”© ìµœì í™”
âœ… ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€ - ë…ë¦½ì  ëª¨ë“ˆ
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›

Author: MyCloset AI Team
Date: 2025-07-21
Version: 1.0 (ë¶„ë¦¬ëœ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ)
"""

import gc
import os
import time
import logging
import threading
import asyncio
import psutil
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from functools import wraps, lru_cache
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ì‹œìŠ¤í…œ í˜¸í™˜ì„± ë° í™˜ê²½ ì²´í¬
# ==============================================

class SystemCompatibility:
    """ì‹œìŠ¤í…œ í˜¸í™˜ì„± ê´€ë¦¬"""
    
    def __init__(self):
        self.torch_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        self.memory_gb = 16.0
        self.cpu_count = 1
        self.gpu_available = False
        
        self._check_system()
    
    def _check_system(self):
        """ì‹œìŠ¤í…œ í™˜ê²½ ì²´í¬"""
        # PyTorch ì²´í¬
        try:
            import torch
            self.torch_available = True
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device_type = "mps"
                self.is_m3_max = True
                self.gpu_available = True
                
                # MPS ì•ˆì „ ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                logger.info("âœ… M3 Max MPS ìµœì í™” í™œì„±í™”")
            elif torch.cuda.is_available():
                self.device_type = "cuda"
                self.gpu_available = True
                logger.info("âœ… CUDA GPU ê°ì§€")
            else:
                self.device_type = "cpu"
            
            globals()['torch'] = torch
        except ImportError:
            self.torch_available = False
            logger.warning("âš ï¸ PyTorch ì—†ìŒ")
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì²´í¬
        try:
            memory = psutil.virtual_memory()
            self.memory_gb = memory.total / (1024**3)
            
            # M3 Max ê°ì§€ (macOS + ë†’ì€ ë©”ëª¨ë¦¬)
            if self.memory_gb > 64 and self.device_type == "mps":
                self.is_m3_max = True
                logger.info(f"ğŸ M3 Max ê°ì§€: {self.memory_gb:.1f}GB ë©”ëª¨ë¦¬")
            
        except ImportError:
            self.memory_gb = 16.0
        
        # CPU ì½”ì–´ ìˆ˜
        self.cpu_count = psutil.cpu_count() or 1

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì
_sys_compat = SystemCompatibility()

TORCH_AVAILABLE = _sys_compat.torch_available
DEFAULT_DEVICE = _sys_compat.device_type
IS_M3_MAX = _sys_compat.is_m3_max
MEMORY_GB = _sys_compat.memory_gb
CPU_COUNT = _sys_compat.cpu_count
GPU_AVAILABLE = _sys_compat.gpu_available

# ==============================================
# ğŸ”¥ 2. ì„±ëŠ¥ ìµœì í™” ì„¤ì •
# ==============================================

class OptimizationLevel(Enum):
    """ìµœì í™” ë ˆë²¨"""
    CONSERVATIVE = "conservative"  # ì•ˆì „ ìš°ì„ 
    BALANCED = "balanced"         # ê· í˜•
    AGGRESSIVE = "aggressive"     # ì„±ëŠ¥ ìš°ì„ 
    MAXIMUM = "maximum"          # ìµœëŒ€ ì„±ëŠ¥

class CacheStrategy(Enum):
    """ìºì‹± ì „ëµ"""
    NO_CACHE = "no_cache"
    MEMORY_ONLY = "memory_only"
    DISK_ONLY = "disk_only"
    HYBRID = "hybrid"
    AUTO = "auto"

@dataclass
class PerformanceConfig:
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    optimization_level: OptimizationLevel = OptimizationLevel.BALANCED
    cache_strategy: CacheStrategy = CacheStrategy.AUTO
    device: str = DEFAULT_DEVICE
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    memory_limit_ratio: float = 0.8  # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ 80%
    enable_memory_mapping: bool = True
    auto_cleanup: bool = True
    
    # ëª¨ë¸ ìµœì í™”
    use_fp16: bool = True if GPU_AVAILABLE else False
    enable_model_compilation: bool = False
    batch_size_optimization: bool = True
    
    # ìºì‹± ì„¤ì •
    cache_size_mb: int = int(MEMORY_GB * 1024 * 0.1)  # ë©”ëª¨ë¦¬ì˜ 10%
    disk_cache_path: Optional[str] = "./cache"
    cache_ttl_seconds: int = 3600  # 1ì‹œê°„
    
    # ë³‘ë ¬ ì²˜ë¦¬
    max_workers: int = min(CPU_COUNT, 4)
    enable_async: bool = True
    
    # M3 Max íŠ¹í™” ì„¤ì •
    mps_optimization: bool = IS_M3_MAX
    unified_memory_optimization: bool = IS_M3_MAX

# ==============================================
# ğŸ”¥ 3. ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë¦¬ì
# ==============================================

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.MemoryOptimizer")
        self._lock = threading.RLock()
        
        # ë©”ëª¨ë¦¬ ì¶”ì 
        self.allocated_memory = 0
        self.peak_memory = 0
        self.cleanup_threshold = self.config.memory_limit_ratio
        
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            memory = psutil.virtual_memory()
            
            info = {
                "total_gb": memory.total / (1024**3),
                "available_gb": memory.available / (1024**3),
                "used_gb": memory.used / (1024**3),
                "percent": memory.percent,
                "device": self.config.device,
                "is_m3_max": IS_M3_MAX
            }
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            if TORCH_AVAILABLE:
                if self.config.device == "cuda":
                    try:
                        info["gpu_allocated_gb"] = torch.cuda.memory_allocated() / (1024**3)
                        info["gpu_reserved_gb"] = torch.cuda.memory_reserved() / (1024**3)
                    except:
                        pass
                elif self.config.device == "mps":
                    try:
                        # MPS ë©”ëª¨ë¦¬ ì •ë³´ëŠ” ì œí•œì 
                        info["mps_available"] = True
                        info["unified_memory"] = IS_M3_MAX
                    except:
                        pass
            
            return info
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            with self._lock:
                self.logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘")
                
                results = []
                before_memory = self.get_memory_info()
                
                # 1. Python GC
                gc.collect()
                results.append("Python GC ì‹¤í–‰")
                
                # 2. PyTorch ìºì‹œ ì •ë¦¬
                if TORCH_AVAILABLE:
                    if self.config.device == "cuda":
                        try:
                            torch.cuda.empty_cache()
                            results.append("CUDA ìºì‹œ ì •ë¦¬")
                        except:
                            pass
                    elif self.config.device == "mps" and IS_M3_MAX:
                        try:
                            # M3 Max MPS ìºì‹œ ì •ë¦¬
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                            elif hasattr(torch.backends.mps, 'empty_cache'):
                                torch.backends.mps.empty_cache()
                            results.append("MPS ìºì‹œ ì •ë¦¬")
                        except Exception as e:
                            self.logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ê±´ë„ˆëœ€: {e}")
                
                # 3. ê°•ì œ ì •ë¦¬ (aggressive ëª¨ë“œ)
                if aggressive:
                    import ctypes
                    try:
                        ctypes.CDLL("libc.so.6").malloc_trim(0)
                        results.append("libc malloc_trim ì‹¤í–‰")
                    except:
                        pass
                
                after_memory = self.get_memory_info()
                
                # ê²°ê³¼ ê³„ì‚°
                freed_memory = before_memory.get("used_gb", 0) - after_memory.get("used_gb", 0)
                
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                
                return {
                    "success": True,
                    "freed_memory_gb": max(0, freed_memory),
                    "before_memory": before_memory,
                    "after_memory": after_memory,
                    "results": results,
                    "aggressive": aggressive
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.optimize_memory, aggressive)
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            memory_info = self.get_memory_info()
            usage_ratio = memory_info.get("percent", 0) / 100.0
            
            return usage_ratio > self.cleanup_threshold
            
        except Exception:
            return False
    
    @contextmanager
    def memory_context(self, auto_cleanup: bool = True):
        """ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì"""
        try:
            before = self.get_memory_info()
            yield
        finally:
            if auto_cleanup and self.check_memory_pressure():
                self.optimize_memory()
                after = self.get_memory_info()
                self.logger.debug(f"ğŸ§¹ ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬: {before.get('percent', 0):.1f}% -> {after.get('percent', 0):.1f}%")

# ==============================================
# ğŸ”¥ 4. ëª¨ë¸ ë¡œë”© ìµœì í™”ê¸°
# ==============================================

class ModelLoadingOptimizer:
    """ëª¨ë¸ ë¡œë”© ìµœì í™”ê¸°"""
    
    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.ModelLoadingOptimizer")
        self.memory_optimizer = MemoryOptimizer(config)
        
    def optimize_model_loading(self, model_path: Union[str, Path]) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë”© ìµœì í™”"""
        try:
            model_path = Path(model_path)
            if not model_path.exists():
                return {"success": False, "error": "ëª¨ë¸ íŒŒì¼ ì—†ìŒ"}
            
            start_time = time.time()
            
            optimization_steps = []
            
            # 1. ë©”ëª¨ë¦¬ ì‚¬ì „ ìµœì í™”
            if self.config.auto_cleanup:
                self.memory_optimizer.optimize_memory()
                optimization_steps.append("ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬")
            
            # 2. ë¡œë”© ì „ëµ ê²°ì •
            file_size_mb = model_path.stat().st_size / (1024 * 1024)
            
            loading_strategy = self._determine_loading_strategy(file_size_mb)
            optimization_steps.append(f"ë¡œë”© ì „ëµ: {loading_strategy}")
            
            # 3. ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
            device_optimization = self._apply_device_optimization()
            optimization_steps.extend(device_optimization)
            
            load_time = time.time() - start_time
            
            return {
                "success": True,
                "load_time_ms": load_time * 1000,
                "file_size_mb": file_size_mb,
                "loading_strategy": loading_strategy,
                "optimization_steps": optimization_steps,
                "device": self.config.device
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _determine_loading_strategy(self, file_size_mb: float) -> str:
        """ë¡œë”© ì „ëµ ê²°ì •"""
        available_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš© ê°€ëŠ¥
        
        if file_size_mb > available_memory_gb * 1024 * 0.5:  # 50% ì´ìƒ
            return "memory_mapped"
        elif file_size_mb > 1000:  # 1GB ì´ìƒ
            return "chunked_loading"
        else:
            return "direct_loading"
    
    def _apply_device_optimization(self) -> List[str]:
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©"""
        optimizations = []
        
        if self.config.device == "mps" and IS_M3_MAX:
            # M3 Max MPS ìµœì í™”
            optimizations.extend([
                "MPS í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”",
                "MPS í´ë°± í™œì„±í™”",
                "MPS ìºì‹œ ì›Œë°ì—…"
            ])
            
            if TORCH_AVAILABLE:
                try:
                    # M3 Max íŠ¹í™” ì„¤ì •
                    torch.mps.set_per_process_memory_fraction(0.8)
                    optimizations.append("MPS ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”")
                except:
                    pass
        
        elif self.config.device == "cuda":
            # CUDA ìµœì í™”
            optimizations.extend([
                "CUDA ìºì‹œ ìµœì í™”",
                "CUDA ë©”ëª¨ë¦¬ í’€ ì„¤ì •"
            ])
            
            if TORCH_AVAILABLE:
                try:
                    torch.cuda.empty_cache()
                    optimizations.append("CUDA ìºì‹œ ì‚¬ì „ ì •ë¦¬")
                except:
                    pass
        
        else:
            # CPU ìµœì í™”
            optimizations.extend([
                f"CPU ë©€í‹°ìŠ¤ë ˆë”© ({self.config.max_workers})",
                "ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”"
            ])
        
        return optimizations
    
    def get_optimal_batch_size(self, model_memory_mb: float, input_size: Tuple[int, ...]) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        try:
            available_memory_gb = MEMORY_GB * 0.6  # 60% ì‚¬ìš©
            available_memory_mb = available_memory_gb * 1024
            
            # ì…ë ¥ í¬ê¸° ê¸°ë°˜ ë©”ëª¨ë¦¬ ì¶”ì •
            if len(input_size) >= 2:
                input_memory_mb = (input_size[0] * input_size[1] * 3 * 4) / (1024 * 1024)  # RGB, float32
            else:
                input_memory_mb = 10  # ê¸°ë³¸ê°’
            
            # ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            memory_per_batch = model_memory_mb + input_memory_mb * 2  # ëª¨ë¸ + ì…ë ¥ + ì¶œë ¥
            max_batch_size = int(available_memory_mb / memory_per_batch)
            
            # ìµœì†Œ/ìµœëŒ€ ì œí•œ
            batch_size = max(1, min(max_batch_size, 32))
            
            # ë””ë°”ì´ìŠ¤ë³„ ì¡°ì •
            if self.config.device == "mps" and IS_M3_MAX:
                batch_size = min(batch_size, 8)  # MPSëŠ” ì‘ì€ ë°°ì¹˜ê°€ ì•ˆì •ì 
            elif self.config.device == "cpu":
                batch_size = min(batch_size, 4)  # CPUëŠ” ì‘ì€ ë°°ì¹˜
            
            return batch_size
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°°ì¹˜ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 1

# ==============================================
# ğŸ”¥ 5. ìºì‹± ì‹œìŠ¤í…œ
# ==============================================

class PerformanceCache:
    """ì„±ëŠ¥ ìµœì í™” ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.PerformanceCache")
        
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_access_times: Dict[str, float] = {}
        self.cache_access_counts: Dict[str, int] = {}
        
        # ë””ìŠ¤í¬ ìºì‹œ
        self.disk_cache_enabled = False
        if self.config.disk_cache_path:
            self.disk_cache_path = Path(self.config.disk_cache_path)
            self.disk_cache_path.mkdir(parents=True, exist_ok=True)
            self.disk_cache_enabled = True
        
        self._lock = threading.RLock()
        
        # ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        self._start_cleanup_thread()
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
                if key in self.memory_cache:
                    self.cache_access_times[key] = time.time()
                    self.cache_access_counts[key] = self.cache_access_counts.get(key, 0) + 1
                    
                    cache_entry = self.memory_cache[key]
                    
                    # TTL ì²´í¬
                    if time.time() - cache_entry['timestamp'] < self.config.cache_ttl_seconds:
                        return cache_entry['data']
                    else:
                        # ë§Œë£Œëœ ìºì‹œ ì œê±°
                        self._remove_key(key)
                
                # ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
                if self.disk_cache_enabled:
                    return self._get_from_disk(key)
                
                return None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        try:
            with self._lock:
                ttl = ttl or self.config.cache_ttl_seconds
                
                cache_entry = {
                    'data': value,
                    'timestamp': time.time(),
                    'ttl': ttl,
                    'size_mb': self._estimate_size(value)
                }
                
                # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
                self.memory_cache[key] = cache_entry
                self.cache_access_times[key] = time.time()
                self.cache_access_counts[key] = self.cache_access_counts.get(key, 0) + 1
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                if self._get_cache_size_mb() > self.config.cache_size_mb:
                    self._evict_lru_entries()
                
                # ë””ìŠ¤í¬ ìºì‹œ ì €ì¥
                if self.disk_cache_enabled and cache_entry['size_mb'] > 10:  # 10MB ì´ìƒë§Œ
                    self._save_to_disk(key, value, ttl)
                
                return True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
            return False
    
    def _estimate_size(self, obj: Any) -> float:
        """ê°ì²´ í¬ê¸° ì¶”ì • (MB)"""
        try:
            import sys
            
            if hasattr(obj, 'nbytes'):  # numpy array
                return obj.nbytes / (1024 * 1024)
            elif TORCH_AVAILABLE and hasattr(obj, 'element_size'):  # torch tensor
                return obj.numel() * obj.element_size() / (1024 * 1024)
            else:
                return sys.getsizeof(obj) / (1024 * 1024)
        except:
            return 1.0  # ê¸°ë³¸ê°’
    
    def _get_cache_size_mb(self) -> float:
        """ì „ì²´ ìºì‹œ í¬ê¸° ê³„ì‚°"""
        return sum(entry['size_mb'] for entry in self.memory_cache.values())
    
    def _evict_lru_entries(self):
        """LRU ê¸°ë°˜ ìºì‹œ ì •ë¦¬"""
        try:
            # ì ‘ê·¼ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
            sorted_keys = sorted(
                self.cache_access_times.keys(),
                key=lambda k: self.cache_access_times[k]
            )
            
            # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
            target_size = self.config.cache_size_mb * 0.8  # 80%ê¹Œì§€ ì¤„ì„
            
            for key in sorted_keys:
                if self._get_cache_size_mb() <= target_size:
                    break
                self._remove_key(key)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ LRU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _remove_key(self, key: str):
        """í‚¤ ì œê±°"""
        try:
            if key in self.memory_cache:
                del self.memory_cache[key]
            if key in self.cache_access_times:
                del self.cache_access_times[key]
            if key in self.cache_access_counts:
                del self.cache_access_counts[key]
        except:
            pass
    
    def _get_from_disk(self, key: str) -> Optional[Any]:
        """ë””ìŠ¤í¬ ìºì‹œì—ì„œ ì¡°íšŒ"""
        try:
            cache_file = self.disk_cache_path / f"{key}.cache"
            if cache_file.exists():
                import pickle
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                # TTL ì²´í¬
                if time.time() - cache_data['timestamp'] < cache_data['ttl']:
                    return cache_data['data']
                else:
                    cache_file.unlink()  # ë§Œë£Œëœ íŒŒì¼ ì œê±°
            
            return None
        except:
            return None
    
    def _save_to_disk(self, key: str, value: Any, ttl: int):
        """ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥"""
        try:
            cache_file = self.disk_cache_path / f"{key}.cache"
            cache_data = {
                'data': value,
                'timestamp': time.time(),
                'ttl': ttl
            }
            
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
                
        except Exception as e:
            self.logger.debug(f"ë””ìŠ¤í¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
    
    def _start_cleanup_thread(self):
        """ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def cleanup_worker():
            while True:
                try:
                    time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì •ë¦¬
                    self._cleanup_expired_entries()
                except Exception as e:
                    self.logger.debug(f"ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
    
    def _cleanup_expired_entries(self):
        """ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        try:
            with self._lock:
                current_time = time.time()
                expired_keys = []
                
                for key, entry in self.memory_cache.items():
                    if current_time - entry['timestamp'] > entry['ttl']:
                        expired_keys.append(key)
                
                for key in expired_keys:
                    self._remove_key(key)
                
                if expired_keys:
                    self.logger.debug(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ ì •ë¦¬")
                    
        except Exception as e:
            self.logger.debug(f"ë§Œë£Œ ì—”íŠ¸ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        try:
            with self._lock:
                return {
                    "memory_entries": len(self.memory_cache),
                    "cache_size_mb": self._get_cache_size_mb(),
                    "cache_limit_mb": self.config.cache_size_mb,
                    "disk_cache_enabled": self.disk_cache_enabled,
                    "total_accesses": sum(self.cache_access_counts.values()),
                    "avg_access_per_key": sum(self.cache_access_counts.values()) / max(1, len(self.cache_access_counts))
                }
        except Exception as e:
            return {"error": str(e)}

# ==============================================
# ğŸ”¥ 6. í†µí•© ì„±ëŠ¥ ìµœì í™”ê¸°
# ==============================================

class PerformanceOptimizer:
    """í†µí•© ì„±ëŠ¥ ìµœì í™”ê¸°"""
    
    def __init__(self, config: Optional[PerformanceConfig] = None):
        self.config = config or PerformanceConfig()
        self.logger = logging.getLogger(f"{__name__}.PerformanceOptimizer")
        
        # ì„œë¸Œ ì‹œìŠ¤í…œë“¤
        self.memory_optimizer = MemoryOptimizer(self.config)
        self.model_optimizer = ModelLoadingOptimizer(self.config)
        self.cache = PerformanceCache(self.config)
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "optimizations_count": 0,
            "total_time_saved_ms": 0,
            "memory_cleanups": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        self._lock = threading.RLock()
        
        self.logger.info(f"âš¡ ì„±ëŠ¥ ìµœì í™”ê¸° ì´ˆê¸°í™”: {self.config.optimization_level.value}")
    
    def optimize_system(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì „ì²´ ìµœì í™”"""
        try:
            with self._lock:
                start_time = time.time()
                
                optimization_results = {
                    "success": True,
                    "optimizations": [],
                    "before_stats": self.get_system_stats(),
                    "warnings": []
                }
                
                # 1. ë©”ëª¨ë¦¬ ìµœì í™”
                memory_result = self.memory_optimizer.optimize_memory(
                    aggressive=(self.config.optimization_level == OptimizationLevel.AGGRESSIVE)
                )
                optimization_results["optimizations"].append({
                    "type": "memory",
                    "result": memory_result
                })
                
                # 2. ìºì‹œ ìµœì í™”
                self.cache._cleanup_expired_entries()
                optimization_results["optimizations"].append({
                    "type": "cache",
                    "result": {"cleaned_expired": True}
                })
                
                # 3. ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
                device_optimizations = self._optimize_device_specific()
                optimization_results["optimizations"].append({
                    "type": "device",
                    "result": device_optimizations
                })
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["optimizations_count"] += 1
                optimization_time = (time.time() - start_time) * 1000
                self.stats["total_time_saved_ms"] += optimization_time
                
                optimization_results["after_stats"] = self.get_system_stats()
                optimization_results["optimization_time_ms"] = optimization_time
                
                self.logger.info(f"âœ… ì‹œìŠ¤í…œ ìµœì í™” ì™„ë£Œ: {optimization_time:.1f}ms")
                
                return optimization_results
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_system_async(self) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì‹œìŠ¤í…œ ìµœì í™”"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.optimize_system)
    
    def _optimize_device_specific(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ë³„ íŠ¹í™” ìµœì í™”"""
        optimizations = []
        
        try:
            if self.config.device == "mps" and IS_M3_MAX:
                # M3 Max MPS ìµœì í™”
                if TORCH_AVAILABLE:
                    try:
                        # MPS ìºì‹œ ì •ë¦¬
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        optimizations.append("MPS ìºì‹œ ì •ë¦¬")
                        
                        # MPS ë©”ëª¨ë¦¬ ìµœì í™”
                        torch.mps.set_per_process_memory_fraction(0.8)
                        optimizations.append("MPS ë©”ëª¨ë¦¬ ë¹„ìœ¨ ìµœì í™”")
                        
                    except Exception as e:
                        optimizations.append(f"MPS ìµœì í™” ë¶€ë¶„ ì‹¤íŒ¨: {e}")
            
            elif self.config.device == "cuda":
                # CUDA ìµœì í™”
                if TORCH_AVAILABLE:
                    try:
                        torch.cuda.empty_cache()
                        optimizations.append("CUDA ìºì‹œ ì •ë¦¬")
                        
                        # CUDA ë©”ëª¨ë¦¬ í’€ ìµœì í™”
                        torch.cuda.memory.set_per_process_memory_fraction(0.9)
                        optimizations.append("CUDA ë©”ëª¨ë¦¬ í’€ ìµœì í™”")
                        
                    except Exception as e:
                        optimizations.append(f"CUDA ìµœì í™” ë¶€ë¶„ ì‹¤íŒ¨: {e}")
            
            else:
                # CPU ìµœì í™”
                optimizations.append(f"CPU ë©€í‹°í”„ë¡œì„¸ì‹± ({self.config.max_workers} workers)")
            
            return {"optimizations": optimizations, "device": self.config.device}
            
        except Exception as e:
            return {"error": str(e), "device": self.config.device}
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
        try:
            memory_info = self.memory_optimizer.get_memory_info()
            cache_stats = self.cache.get_cache_stats()
            
            return {
                "memory": memory_info,
                "cache": cache_stats,
                "performance": self.stats.copy(),
                "config": {
                    "optimization_level": self.config.optimization_level.value,
                    "device": self.config.device,
                    "is_m3_max": IS_M3_MAX,
                    "gpu_available": GPU_AVAILABLE
                }
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def create_performance_context(self, auto_optimize: bool = True):
        """ì„±ëŠ¥ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        return PerformanceContext(self, auto_optimize)

# ==============================================
# ğŸ”¥ 7. ì„±ëŠ¥ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# ==============================================

class PerformanceContext:
    """ì„±ëŠ¥ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, optimizer: PerformanceOptimizer, auto_optimize: bool = True):
        self.optimizer = optimizer
        self.auto_optimize = auto_optimize
        self.start_time = None
        self.start_stats = None
        
    def __enter__(self):
        self.start_time = time.time()
        self.start_stats = self.optimizer.get_system_stats()
        
        if self.auto_optimize:
            self.optimizer.memory_optimizer.optimize_memory()
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        end_time = time.time()
        execution_time = (end_time - self.start_time) * 1000
        
        if self.auto_optimize:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬ í›„ ì •ë¦¬
            if self.optimizer.memory_optimizer.check_memory_pressure():
                self.optimizer.memory_optimizer.optimize_memory()
        
        end_stats = self.optimizer.get_system_stats()
        
        self.optimizer.logger.debug(
            f"âš¡ ì„±ëŠ¥ ì»¨í…ìŠ¤íŠ¸ ì™„ë£Œ: {execution_time:.1f}ms, "
            f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.start_stats.get('memory', {}).get('percent', 0):.1f}% -> "
            f"{end_stats.get('memory', {}).get('percent', 0):.1f}%"
        )

# ==============================================
# ğŸ”¥ 8. ì„±ëŠ¥ ë°ì½”ë ˆì´í„°ë“¤
# ==============================================

def performance_optimized(
    cache_key: Optional[str] = None,
    memory_optimize: bool = True,
    cache_ttl: int = 3600
):
    """ì„±ëŠ¥ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ì „ì—­ ì˜µí‹°ë§ˆì´ì € ê°€ì ¸ì˜¤ê¸°
            optimizer = get_global_optimizer()
            
            # ìºì‹œ í‚¤ ìƒì„±
            if cache_key:
                key = f"{cache_key}_{hash(str(args) + str(sorted(kwargs.items())))}"
            else:
                key = f"{func.__name__}_{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # ìºì‹œ í™•ì¸
            cached_result = optimizer.cache.get(key)
            if cached_result is not None:
                optimizer.stats["cache_hits"] += 1
                return cached_result
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if memory_optimize:
                with optimizer.memory_optimizer.memory_context():
                    result = func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            optimizer.cache.set(key, result, ttl=cache_ttl)
            optimizer.stats["cache_misses"] += 1
            
            return result
        
        return wrapper
    return decorator

def async_performance_optimized(
    cache_key: Optional[str] = None,
    memory_optimize: bool = True,
    cache_ttl: int = 3600
):
    """ë¹„ë™ê¸° ì„±ëŠ¥ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            optimizer = get_global_optimizer()
            
            # ìºì‹œ í‚¤ ìƒì„±
            if cache_key:
                key = f"{cache_key}_{hash(str(args) + str(sorted(kwargs.items())))}"
            else:
                key = f"{func.__name__}_{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # ìºì‹œ í™•ì¸
            cached_result = optimizer.cache.get(key)
            if cached_result is not None:
                optimizer.stats["cache_hits"] += 1
                return cached_result
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if memory_optimize:
                await optimizer.memory_optimizer.optimize_memory_async()
            
            result = await func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            optimizer.cache.set(key, result, ttl=cache_ttl)
            optimizer.stats["cache_misses"] += 1
            
            return result
        
        return wrapper
    return decorator

# ==============================================
# ğŸ”¥ 9. ì „ì—­ ìµœì í™”ê¸° ê´€ë¦¬
# ==============================================

_global_optimizer: Optional[PerformanceOptimizer] = None
_optimizer_lock = threading.Lock()

def get_global_optimizer(config: Optional[PerformanceConfig] = None) -> PerformanceOptimizer:
    """ì „ì—­ ì„±ëŠ¥ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_optimizer
    
    with _optimizer_lock:
        if _global_optimizer is None:
            _global_optimizer = PerformanceOptimizer(config)
            logger.info("ğŸŒ ì „ì—­ ì„±ëŠ¥ ìµœì í™”ê¸° ìƒì„±")
        
        return _global_optimizer

def optimize_system() -> Dict[str, Any]:
    """ì „ì—­ ì‹œìŠ¤í…œ ìµœì í™”"""
    optimizer = get_global_optimizer()
    return optimizer.optimize_system()

async def optimize_system_async() -> Dict[str, Any]:
    """ì „ì—­ ë¹„ë™ê¸° ì‹œìŠ¤í…œ ìµœì í™”"""
    optimizer = get_global_optimizer()
    return await optimizer.optimize_system_async()

def get_system_performance_stats() -> Dict[str, Any]:
    """ì „ì—­ ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µê³„"""
    optimizer = get_global_optimizer()
    return optimizer.get_system_stats()

def cleanup_global_optimizer():
    """ì „ì—­ ìµœì í™”ê¸° ì •ë¦¬"""
    global _global_optimizer
    
    with _optimizer_lock:
        if _global_optimizer:
            _global_optimizer.memory_optimizer.optimize_memory(aggressive=True)
            _global_optimizer = None
        logger.info("ğŸŒ ì „ì—­ ì„±ëŠ¥ ìµœì í™”ê¸° ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ 10. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'PerformanceOptimizer',
    'MemoryOptimizer',
    'ModelLoadingOptimizer',
    'PerformanceCache',
    'PerformanceContext',
    
    # ì„¤ì • í´ë˜ìŠ¤ë“¤
    'PerformanceConfig',
    'OptimizationLevel',
    'CacheStrategy',
    
    # ë°ì½”ë ˆì´í„°ë“¤
    'performance_optimized',
    'async_performance_optimized',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_optimizer',
    'optimize_system',
    'optimize_system_async',
    'get_system_performance_stats',
    'cleanup_global_optimizer',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'MEMORY_GB',
    'CPU_COUNT',
    'GPU_AVAILABLE'
]

# ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_optimizer)

logger.info("âœ… ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ v1.0 ë¡œë“œ ì™„ë£Œ")
logger.info(f"âš¡ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - CPU ì½”ì–´: {CPU_COUNT}")
logger.info(f"   - ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - GPU: {'âœ…' if GPU_AVAILABLE else 'âŒ'}")
logger.info("ğŸ M3 Max 128GB íŠ¹í™” ìµœì í™”")
logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìºì‹± ì‹œìŠ¤í…œ")
logger.info("ğŸ”§ GPU/MPS ìµœì í™”")
logger.info("ğŸ“¦ ëª¨ë¸ ë¡œë”© ìµœì í™”")
logger.info("ğŸ”— ìˆœí™˜ì°¸ì¡° ë°©ì§€ - ë…ë¦½ì  ëª¨ë“ˆ")