# backend/app/ai_pipeline/utils/model_loader_integration_patch.py
"""
ğŸ”¥ ModelLoader í†µí•© íŒ¨ì¹˜ - ì›Œë‹ ì™„ì „ ì œê±° v1.0
================================================================================
âœ… SmartModelPathMapper ì™„ì „ ì—°ë™
âœ… ëˆ„ë½ëœ ëª¨ë¸ ì›Œë‹ í•´ê²° 
âœ… BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜
âœ… ê¸°ì¡´ model_loader.py ì½”ë“œ ìµœì†Œ ìˆ˜ì •
================================================================================
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union
from .smart_model_mapper import get_global_smart_mapper, resolve_model_path

logger = logging.getLogger(__name__)

class ModelLoaderIntegrationPatch:
    """ModelLoader í†µí•© íŒ¨ì¹˜ í´ë˜ìŠ¤"""
    
    def __init__(self, original_model_loader):
        self.original_loader = original_model_loader
        self.smart_mapper = get_global_smart_mapper()
        self.logger = logging.getLogger(f"{__name__}.IntegrationPatch")
        
        # ì›Œë‹ í•´ê²° ì¹´ìš´í„°
        self.resolved_warnings = {
            "missing_models": 0,
            "path_corrections": 0,
            "successful_mappings": 0
        }
        
        self.logger.info("ğŸ”§ ModelLoader í†µí•© íŒ¨ì¹˜ í™œì„±í™”")
    
    def resolve_missing_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """ğŸ”¥ ëˆ„ë½ëœ ëª¨ë¸ ê²½ë¡œ í•´ê²°"""
        try:
            # SmartMapperë¡œ ê²½ë¡œ í•´ê²°
            mapping_info = self.smart_mapper.get_model_path(model_name)
            
            if mapping_info and mapping_info.actual_path:
                self.resolved_warnings["missing_models"] += 1
                self.logger.info(f"âœ… ëˆ„ë½ ëª¨ë¸ í•´ê²°: {model_name} â†’ {mapping_info.actual_path}")
                return str(mapping_info.actual_path)
            
            # í´ë°±: ì›ë³¸ ModelLoaderì˜ available_models í™•ì¸
            if hasattr(self.original_loader, 'available_models'):
                available_dict = self.original_loader.available_models
                if model_name in available_dict:
                    model_info = available_dict[model_name]
                    path = model_info.get("checkpoint_path") or model_info.get("path")
                    if path and Path(path).exists():
                        self.resolved_warnings["path_corrections"] += 1
                        return str(path)
            
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ í•´ê²° ì‹¤íŒ¨: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ í•´ê²° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def patch_load_model_method(self):
        """load_model ë©”ì„œë“œ íŒ¨ì¹˜"""
        original_load_model = self.original_loader.load_model
        
        def patched_load_model(model_name: str, **kwargs):
            try:
                # ì›ë³¸ ë©”ì„œë“œ ë¨¼ì € ì‹œë„
                result = original_load_model(model_name, **kwargs)
                if result:
                    self.resolved_warnings["successful_mappings"] += 1
                    return result
                
                # ì‹¤íŒ¨ ì‹œ SmartMapperë¡œ ê²½ë¡œ í•´ê²° í›„ ì¬ì‹œë„
                resolved_path = self.resolve_missing_model_path(model_name)
                if resolved_path:
                    self.logger.info(f"ğŸ”„ íŒ¨ì¹˜ëœ ê²½ë¡œë¡œ ì¬ì‹œë„: {model_name}")
                    
                    # available_modelsì— ì„ì‹œ ì¶”ê°€
                    if hasattr(self.original_loader, '_available_models_cache'):
                        self.original_loader._available_models_cache[model_name] = {
                            "name": model_name,
                            "path": resolved_path,
                            "checkpoint_path": resolved_path,
                            "ai_model_info": {"ai_class": "BaseRealAIModel"},
                            "size_mb": Path(resolved_path).stat().st_size / (1024 * 1024)
                        }
                    
                    return original_load_model(model_name, **kwargs)
                
                return None
                
            except Exception as e:
                self.logger.error(f"âŒ íŒ¨ì¹˜ëœ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return None
        
        # ë©”ì„œë“œ êµì²´
        self.original_loader.load_model = patched_load_model
        self.logger.info("âœ… load_model ë©”ì„œë“œ íŒ¨ì¹˜ ì™„ë£Œ")
    
    def patch_load_model_with_fallback_method(self):
        """load_model_with_fallback ë©”ì„œë“œ ì¶”ê°€"""
        def load_model_with_fallback(model_name: str, **kwargs):
            """ëˆ„ë½ëœ ëª¨ë¸ì— ëŒ€í•œ í´ë°± ì²˜ë¦¬"""
            try:
                # ê¸°ë³¸ ë¡œë”© ì‹œë„
                result = self.original_loader.load_model(model_name, **kwargs)
                if result:
                    return result
                
                self.logger.warning(f"âš ï¸ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
                
                # SmartMapperë¡œ ê²½ë¡œ í•´ê²°
                resolved_path = self.resolve_missing_model_path(model_name)
                
                if resolved_path:
                    self.logger.info(f"ğŸ”„ í´ë°± ê²½ë¡œë¡œ ì¬ì‹œë„: {model_name}")
                    
                    # ì§ì ‘ ê²½ë¡œ ì§€ì •í•´ì„œ ë¡œë”©
                    if hasattr(self.original_loader, 'load_model_from_path'):
                        return self.original_loader.load_model_from_path(resolved_path, **kwargs)
                    else:
                        # ëŒ€ì•ˆ: available_models ì—…ë°ì´íŠ¸ í›„ ì¬ì‹œë„
                        self._update_available_models_with_path(model_name, resolved_path)
                        return self.original_loader.load_model(model_name, **kwargs)
                else:
                    self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {model_name}")
                    return None
                    
            except Exception as e:
                self.logger.error(f"âŒ í´ë°± ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                return None
        
        # ë©”ì„œë“œ ì¶”ê°€
        self.original_loader.load_model_with_fallback = load_model_with_fallback
        self.logger.info("âœ… load_model_with_fallback ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ")
    
    def _update_available_models_with_path(self, model_name: str, resolved_path: str):
        """available_modelsì— í•´ê²°ëœ ê²½ë¡œ ì—…ë°ì´íŠ¸"""
        try:
            if hasattr(self.original_loader, '_available_models_cache'):
                mapping_info = self.smart_mapper.get_model_path(model_name)
                
                model_info = {
                    "name": model_name,
                    "path": resolved_path,
                    "checkpoint_path": resolved_path,
                    "size_mb": Path(resolved_path).stat().st_size / (1024 * 1024),
                    "loaded": False,
                    "device": getattr(self.original_loader, 'device', 'cpu'),
                    "ai_model_info": {
                        "ai_class": mapping_info.ai_class if mapping_info else "BaseRealAIModel",
                        "can_create_ai_model": True,
                        "device_compatible": True
                    },
                    "metadata": {
                        "resolution_source": "smart_mapper",
                        "original_missing": True
                    }
                }
                
                self.original_loader._available_models_cache[model_name] = model_info
                self.logger.info(f"âœ… available_models ì—…ë°ì´íŠ¸: {model_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ available_models ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def patch_available_models_property(self):
        """available_models ì†ì„± íŒ¨ì¹˜"""
        original_available_models = self.original_loader.available_models
        
        def patched_available_models():
            try:
                # ì›ë³¸ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ê¸°
                available_dict = original_available_models
                if not isinstance(available_dict, dict):
                    available_dict = {}
                
                # SmartMapperì—ì„œ ì¶”ê°€ ëª¨ë¸ë“¤ ì°¾ê¸°
                for model_name in self.smart_mapper.unified_model_mappings.keys():
                    if model_name not in available_dict:
                        mapping_info = self.smart_mapper.get_model_path(model_name)
                        if mapping_info and mapping_info.actual_path:
                            available_dict[model_name] = {
                                "name": model_name,
                                "path": str(mapping_info.actual_path),
                                "checkpoint_path": str(mapping_info.actual_path),
                                "size_mb": mapping_info.size_mb,
                                "step_class": mapping_info.step_class or "UnknownStep",
                                "ai_model_info": {
                                    "ai_class": mapping_info.ai_class or "BaseRealAIModel"
                                },
                                "metadata": {
                                    "source": "smart_mapper_discovery"
                                }
                            }
                
                return available_dict
                
            except Exception as e:
                self.logger.error(f"âŒ available_models íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")
                return original_available_models
        
        # ì†ì„±ì„ propertyë¡œ êµì²´
        if hasattr(self.original_loader.__class__, 'available_models'):
            self.original_loader.__class__.available_models = property(lambda self: patched_available_models())
        else:
            self.original_loader.available_models = patched_available_models()
        
        self.logger.info("âœ… available_models ì†ì„± íŒ¨ì¹˜ ì™„ë£Œ")
    
    def apply_all_patches(self):
        """ëª¨ë“  íŒ¨ì¹˜ ì ìš©"""
        try:
            self.patch_load_model_method()
            self.patch_load_model_with_fallback_method()
            self.patch_available_models_property()
            
            self.logger.info("ğŸ‰ ëª¨ë“  ModelLoader íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
            self.logger.info(f"ğŸ“Š í•´ê²°ëœ ì›Œë‹ë“¤: {self.resolved_warnings}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
            return False
    
    def get_patch_status(self) -> Dict[str, Any]:
        """íŒ¨ì¹˜ ìƒíƒœ ì¡°íšŒ"""
        return {
            "patches_applied": True,
            "resolved_warnings": self.resolved_warnings.copy(),
            "smart_mapper_stats": self.smart_mapper.get_mapping_statistics(),
            "original_loader_type": type(self.original_loader).__name__
        }

# ==============================================
# ğŸ”¥ ê°„í¸ ì ìš© í•¨ìˆ˜ë“¤
# ==============================================

def apply_model_loader_patches(model_loader_instance) -> bool:
    """ModelLoaderì— íŒ¨ì¹˜ ì ìš©"""
    try:
        patch_system = ModelLoaderIntegrationPatch(model_loader_instance)
        return patch_system.apply_all_patches()
    except Exception as e:
        logger.error(f"âŒ ModelLoader íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False

def resolve_missing_models_globally() -> Dict[str, Any]:
    """ì „ì—­ì ìœ¼ë¡œ ëˆ„ë½ëœ ëª¨ë¸ë“¤ í•´ê²°"""
    try:
        smart_mapper = get_global_smart_mapper()
        
        # ê³µí†µ ëˆ„ë½ ëª¨ë¸ë“¤
        missing_models = [
            "realvis_xl", "vgg16_warping", "vgg19_warping", "densenet121",
            "post_processing_model", "super_resolution", 
            "clip_vit_large", "quality_assessment"
        ]
        
        resolved_models = {}
        
        for model_name in missing_models:
            mapping_info = smart_mapper.get_model_path(model_name)
            if mapping_info and mapping_info.actual_path:
                resolved_models[model_name] = {
                    "path": str(mapping_info.actual_path),
                    "size_mb": mapping_info.size_mb,
                    "ai_class": mapping_info.ai_class
                }
        
        logger.info(f"âœ… ì „ì—­ ëˆ„ë½ ëª¨ë¸ í•´ê²°: {len(resolved_models)}ê°œ")
        return {
            "resolved_count": len(resolved_models),
            "resolved_models": resolved_models,
            "success": True
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ëª¨ë¸ í•´ê²° ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def create_missing_model_mapping() -> Dict[str, str]:
    """ëˆ„ë½ëœ ëª¨ë¸ë“¤ì˜ ë§¤í•‘ í…Œì´ë¸” ìƒì„±"""
    try:
        smart_mapper = get_global_smart_mapper()
        mapping_table = {}
        
        for model_name in smart_mapper.unified_model_mappings.keys():
            mapping_info = smart_mapper.get_model_path(model_name)
            if mapping_info and mapping_info.actual_path:
                mapping_table[model_name] = str(mapping_info.actual_path)
        
        return mapping_table
        
    except Exception as e:
        logger.error(f"âŒ ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
        return {}

# Export
__all__ = [
    'ModelLoaderIntegrationPatch',
    'apply_model_loader_patches', 
    'resolve_missing_models_globally',
    'create_missing_model_mapping'
]

logger.info("ğŸ”§ ModelLoader í†µí•© íŒ¨ì¹˜ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ¯ ì›Œë‹ ì œê±° ë° ëˆ„ë½ ëª¨ë¸ í•´ê²° ì¤€ë¹„ ì™„ë£Œ")