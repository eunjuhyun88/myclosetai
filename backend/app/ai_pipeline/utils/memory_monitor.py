#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
========================================

ê° ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

Author: MyCloset AI Team
Date: 2025-08-07
Version: 1.0
"""

import os
import psutil
import gc
import time
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from datetime import datetime

# PyTorch ê´€ë ¨
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class MemorySnapshot:
    """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: float = field(default_factory=time.time)
    step_name: str = ""
    step_id: str = ""
    
    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
    system_total_gb: float = 0.0
    system_used_gb: float = 0.0
    system_available_gb: float = 0.0
    system_percent: float = 0.0
    
    # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
    process_rss_gb: float = 0.0
    process_vms_gb: float = 0.0
    process_percent: float = 0.0
    
    # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
    gpu_total_gb: float = 0.0
    gpu_used_gb: float = 0.0
    gpu_available_gb: float = 0.0
    gpu_percent: float = 0.0
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì •ë³´
    gc_objects: int = 0
    gc_collections: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'timestamp': self.timestamp,
            'step_name': self.step_name,
            'step_id': self.step_id,
            'system': {
                'total_gb': self.system_total_gb,
                'used_gb': self.system_used_gb,
                'available_gb': self.system_available_gb,
                'percent': self.system_percent
            },
            'process': {
                'rss_gb': self.process_rss_gb,
                'vms_gb': self.process_vms_gb,
                'percent': self.process_percent
            },
            'gpu': {
                'total_gb': self.gpu_total_gb,
                'used_gb': self.gpu_used_gb,
                'available_gb': self.gpu_available_gb,
                'percent': self.gpu_percent
            },
            'gc': {
                'objects': self.gc_objects,
                'collections': self.gc_collections
            }
        }

class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.snapshots: List[MemorySnapshot] = []
        self.warning_threshold = 0.7  # 70% ê²½ê³ 
        self.critical_threshold = 0.85  # 85% ìœ„í—˜
        self.optimal_range = (0.2, 0.4)  # 20-40% ì ì • ë²”ìœ„
        
    def take_snapshot(self, step_name: str = "", step_id: str = "") -> MemorySnapshot:
        """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        snapshot = MemorySnapshot(step_name=step_name, step_id=step_id)
        
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            snapshot.system_total_gb = memory.total / (1024**3)
            snapshot.system_used_gb = memory.used / (1024**3)
            snapshot.system_available_gb = memory.available / (1024**3)
            snapshot.system_percent = memory.percent / 100.0
            
            # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì •ë³´
            process = psutil.Process()
            memory_info = process.memory_info()
            snapshot.process_rss_gb = memory_info.rss / (1024**3)
            snapshot.process_vms_gb = memory_info.vms / (1024**3)
            snapshot.process_percent = process.memory_percent() / 100.0
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE:
                try:
                    if torch.backends.mps.is_available():
                        # M3 Max MPS ë©”ëª¨ë¦¬ ì •ë³´
                        snapshot.gpu_total_gb = 16.0  # M3 Max í†µí•© ë©”ëª¨ë¦¬
                        snapshot.gpu_used_gb = snapshot.system_used_gb * 0.3  # ì¶”ì •ì¹˜
                        snapshot.gpu_available_gb = snapshot.gpu_total_gb - snapshot.gpu_used_gb
                        snapshot.gpu_percent = snapshot.gpu_used_gb / snapshot.gpu_total_gb
                    elif torch.cuda.is_available():
                        # CUDA ë©”ëª¨ë¦¬ ì •ë³´
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory
                        snapshot.gpu_total_gb = gpu_memory / (1024**3)
                        snapshot.gpu_used_gb = torch.cuda.memory_allocated() / (1024**3)
                        snapshot.gpu_available_gb = snapshot.gpu_total_gb - snapshot.gpu_used_gb
                        snapshot.gpu_percent = snapshot.gpu_used_gb / snapshot.gpu_total_gb
                except Exception as e:
                    logger.debug(f"GPU ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì •ë³´
            snapshot.gc_objects = len(gc.get_objects())
            snapshot.gc_collections = gc.get_count()[0]
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨: {e}")
        
        self.snapshots.append(snapshot)
        return snapshot
    
    def log_memory_status(self, step_name: str = "", step_id: str = "") -> None:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…"""
        snapshot = self.take_snapshot(step_name, step_id)
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í‰ê°€
        status = self._evaluate_memory_status(snapshot)
        
        # ë¡œê¹…
        logger.info(f"ğŸ”¥ [ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°] {step_name} ({step_id}) - {status}")
        logger.info(f"   ğŸ’¾ ì‹œìŠ¤í…œ: {snapshot.system_used_gb:.1f}GB/{snapshot.system_total_gb:.1f}GB ({snapshot.system_percent*100:.1f}%)")
        logger.info(f"   ğŸ–¥ï¸  í”„ë¡œì„¸ìŠ¤: {snapshot.process_rss_gb:.1f}GB ({snapshot.process_percent*100:.1f}%)")
        
        if snapshot.gpu_total_gb > 0:
            logger.info(f"   ğŸ® GPU: {snapshot.gpu_used_gb:.1f}GB/{snapshot.gpu_total_gb:.1f}GB ({snapshot.gpu_percent*100:.1f}%)")
        
        logger.info(f"   ğŸ—‘ï¸  GC ê°ì²´: {snapshot.gc_objects:,}ê°œ")
        
        # ê²½ê³ /ìœ„í—˜ ìƒíƒœ ì•Œë¦¼
        if snapshot.system_percent > self.critical_threshold:
            logger.warning(f"âš ï¸ [ë©”ëª¨ë¦¬ ìœ„í—˜] ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {snapshot.system_percent*100:.1f}%ë¡œ ìœ„í—˜ ìˆ˜ì¤€ì…ë‹ˆë‹¤!")
        elif snapshot.system_percent > self.warning_threshold:
            logger.warning(f"âš ï¸ [ë©”ëª¨ë¦¬ ê²½ê³ ] ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {snapshot.system_percent*100:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤.")
        
        # ì ì • ë²”ìœ„ í™•ì¸
        if self.optimal_range[0] <= snapshot.system_percent <= self.optimal_range[1]:
            logger.info(f"âœ… [ë©”ëª¨ë¦¬ ì ì •] ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì • ë²”ìœ„({self.optimal_range[0]*100:.0f}-{self.optimal_range[1]*100:.0f}%) ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
    
    def _evaluate_memory_status(self, snapshot: MemorySnapshot) -> str:
        """ë©”ëª¨ë¦¬ ìƒíƒœ í‰ê°€"""
        if snapshot.system_percent > self.critical_threshold:
            return "ìœ„í—˜"
        elif snapshot.system_percent > self.warning_threshold:
            return "ê²½ê³ "
        elif self.optimal_range[0] <= snapshot.system_percent <= self.optimal_range[1]:
            return "ì ì •"
        else:
            return "ì–‘í˜¸"
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        start_time = time.time()
        
        # ì •ë¦¬ ì „ ìŠ¤ëƒ…ìƒ·
        before_snapshot = self.take_snapshot("before_cleanup", "cleanup")
        
        try:
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = gc.collect()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    if torch.backends.mps.is_available():
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    elif torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        if aggressive:
                            torch.cuda.synchronize()
                except Exception as e:
                    logger.warning(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì •ë¦¬ í›„ ìŠ¤ëƒ…ìƒ·
            after_snapshot = self.take_snapshot("after_cleanup", "cleanup")
            
            # ì •ë¦¬ íš¨ê³¼ ê³„ì‚°
            memory_freed = before_snapshot.system_used_gb - after_snapshot.system_used_gb
            process_freed = before_snapshot.process_rss_gb - after_snapshot.process_rss_gb
            
            cleanup_time = time.time() - start_time
            
            result = {
                'success': True,
                'cleanup_time': cleanup_time,
                'memory_freed_gb': memory_freed,
                'process_freed_gb': process_freed,
                'objects_collected': collected_objects,
                'aggressive': aggressive
            }
            
            logger.info(f"âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {memory_freed:.2f}GB í•´ì œ, {cleanup_time:.2f}ì´ˆ ì†Œìš”")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'cleanup_time': time.time() - start_time
            }
    
    def get_memory_trend(self, steps: int = 10) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.snapshots) < 2:
            return {'error': 'ì¶©ë¶„í•œ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤'}
        
        recent_snapshots = self.snapshots[-steps:]
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™”
        memory_changes = []
        for i in range(1, len(recent_snapshots)):
            change = recent_snapshots[i].system_used_gb - recent_snapshots[i-1].system_used_gb
            memory_changes.append(change)
        
        # í†µê³„ ê³„ì‚°
        avg_change = sum(memory_changes) / len(memory_changes) if memory_changes else 0
        max_change = max(memory_changes) if memory_changes else 0
        min_change = min(memory_changes) if memory_changes else 0
        
        return {
            'total_snapshots': len(self.snapshots),
            'analyzed_snapshots': len(recent_snapshots),
            'average_memory_change_gb': avg_change,
            'max_memory_increase_gb': max_change,
            'max_memory_decrease_gb': min_change,
            'trend': 'increasing' if avg_change > 0.1 else 'decreasing' if avg_change < -0.1 else 'stable'
        }
    
    def print_memory_report(self) -> None:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        if not self.snapshots:
            logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸: ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        latest = self.snapshots[-1]
        trend = self.get_memory_trend()
        
        logger.info("ğŸ“Š === ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ===")
        logger.info(f"   ğŸ“ˆ ì´ ìŠ¤ëƒ…ìƒ·: {len(self.snapshots)}ê°œ")
        logger.info(f"   ğŸ’¾ í˜„ì¬ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {latest.system_used_gb:.1f}GB/{latest.system_total_gb:.1f}GB ({latest.system_percent*100:.1f}%)")
        logger.info(f"   ğŸ–¥ï¸  í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {latest.process_rss_gb:.1f}GB ({latest.process_percent*100:.1f}%)")
        logger.info(f"   ğŸ—‘ï¸  í˜„ì¬ GC ê°ì²´: {latest.gc_objects:,}ê°œ")
        
        if trend.get('trend'):
            logger.info(f"   ğŸ“Š ë©”ëª¨ë¦¬ íŠ¸ë Œë“œ: {trend['trend']} (í‰ê·  ë³€í™”: {trend['average_memory_change_gb']:.2f}GB)")
        
        # ê¶Œì¥ì‚¬í•­
        if latest.system_percent > self.critical_threshold:
            logger.warning("   âš ï¸ ê¶Œì¥ì‚¬í•­: ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”!")
        elif latest.system_percent > self.warning_threshold:
            logger.warning("   âš ï¸ ê¶Œì¥ì‚¬í•­: ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        elif latest.system_percent < self.optimal_range[0]:
            logger.info("   âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.")
        else:
            logger.info("   âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì • ë²”ìœ„ì…ë‹ˆë‹¤.")

# ì „ì—­ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
memory_monitor = MemoryMonitor()

def get_memory_monitor() -> MemoryMonitor:
    """ì „ì—­ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° ë°˜í™˜"""
    return memory_monitor

def log_step_memory(step_name: str, step_id: str = "") -> None:
    """ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ë¡œê¹… (í¸ì˜ í•¨ìˆ˜)"""
    memory_monitor.log_memory_status(step_name, step_id)

def cleanup_step_memory(aggressive: bool = False) -> Dict[str, Any]:
    """ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬ (í¸ì˜ í•¨ìˆ˜)"""
    return memory_monitor.cleanup_memory(aggressive)
