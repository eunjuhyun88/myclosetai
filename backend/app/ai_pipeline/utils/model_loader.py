# app/ai_pipeline/utils/model_loader.py
"""
ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ AI ëª¨ë¸ ë¡œë”
âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™
âœ… í´ë°± ì œê±°, ì‹¤ì œ ëª¨ë¸ë§Œ ì‚¬ìš©
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê´€ë¦¬
"""

import os
import gc
import time
import threading
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

# PyTorch ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    raise ImportError("PyTorch is required for production ModelLoader")

try:
    import cv2
    import numpy as np
    from PIL import Image
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    raise ImportError("OpenCV and PIL are required for production ModelLoader")

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ ëª¨ë¸ ì •ì˜ í´ë˜ìŠ¤ë“¤
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§· ì •ì˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    COREML = "coreml"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ - í”„ë¡œë•ì…˜ ë²„ì „
# ==============================================

class GraphonomyModel(nn.Module):
    """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - Step 01"""
    
    def __init__(self, num_classes=20, backbone='resnet101', pretrained=True):
        super().__init__()
        self.num_classes = num_classes
        self.backbone_name = backbone
        
        # ResNet ë°±ë³¸ êµ¬ì„±
        self.backbone = self._build_backbone(pretrained)
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = self._build_aspp()
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)
        
        # ë³´ì¡° ë¶„ë¥˜ê¸°
        self.aux_classifier = nn.Conv2d(1024, num_classes, kernel_size=1)
        
    def _build_backbone(self, pretrained=True):
        """ResNet ë°±ë³¸ êµ¬ì„±"""
        try:
            import torchvision.models as models
            if self.backbone_name == 'resnet101':
                backbone = models.resnet101(pretrained=pretrained)
            else:
                backbone = models.resnet50(pretrained=pretrained)
                
            # Atrous convolutionì„ ìœ„í•œ ì„¤ì •
            backbone.layer3[0].conv2.stride = (1, 1)
            backbone.layer3[0].downsample[0].stride = (1, 1)
            backbone.layer4[0].conv2.stride = (1, 1)
            backbone.layer4[0].downsample[0].stride = (1, 1)
            
            # Dilation ì ìš©
            for module in backbone.layer3[1:]:
                module.conv2.dilation = (2, 2)
                module.conv2.padding = (2, 2)
            for module in backbone.layer4:
                module.conv2.dilation = (4, 4)
                module.conv2.padding = (4, 4)
                
            return nn.Sequential(*list(backbone.children())[:-2])
        except ImportError:
            # ê¸°ë³¸ CNN ë°±ë³¸
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                *[self._make_layer(64, 128, 2, stride=2) for _ in range(3)],
                *[self._make_layer(128, 256, 2, stride=2) for _ in range(4)],
                *[self._make_layer(256, 512, 2, stride=2) for _ in range(6)],
                *[self._make_layer(512, 1024, 2, stride=2) for _ in range(3)],
                nn.AdaptiveAvgPool2d((1, 1))
            )
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """ResNet ë ˆì´ì–´ êµ¬ì„±"""
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_aspp(self):
        """ASPP ëª¨ë“ˆ êµ¬ì„±"""
        return nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(1024, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=6, dilation=6, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=12, dilation=12, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=18, dilation=18, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Conv2d(1024, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            )
        ])
    
    def forward(self, x):
        input_size = x.size()[2:]
        
        # ë°±ë³¸ í†µê³¼
        features = self.backbone(x)
        
        # ASPP ì ìš©
        aspp_outputs = []
        for i, aspp_layer in enumerate(self.aspp[:-1]):
            aspp_outputs.append(aspp_layer(features))
        
        # Global average pooling
        global_feat = self.aspp[-1](features)
        global_feat = F.interpolate(global_feat, size=features.size()[2:], 
                                   mode='bilinear', align_corners=False)
        aspp_outputs.append(global_feat)
        
        # íŠ¹ì§• ìœµí•©
        fused = torch.cat(aspp_outputs, dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.classifier(fused)
        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
        
        return output

class OpenPoseModel(nn.Module):
    """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - Step 02"""
    
    def __init__(self, num_keypoints=18, num_pafs=38):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.num_pafs = num_pafs
        
        # VGG ë°±ë³¸
        self.backbone = self._build_vgg_backbone()
        
        # ì´ˆê¸° ìŠ¤í…Œì´ì§€
        self.stage1_paf = self._build_initial_stage(num_pafs)
        self.stage1_heatmap = self._build_initial_stage(num_keypoints + 1)
        
        # ê°œì„  ìŠ¤í…Œì´ì§€ë“¤
        self.refinement_stages = nn.ModuleList()
        for i in range(5):
            self.refinement_stages.append(nn.ModuleDict({
                'paf': self._build_refinement_stage(num_pafs),
                'heatmap': self._build_refinement_stage(num_keypoints + 1)
            }))
    
    def _build_vgg_backbone(self):
        """VGG ë°±ë³¸ êµ¬ì„±"""
        return nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 4
            nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
        )
    
    def _build_initial_stage(self, output_channels):
        """ì´ˆê¸° ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(512, output_channels, 1, 1, 0)
        )
    
    def _build_refinement_stage(self, output_channels):
        """ê°œì„  ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        input_channels = 512 + self.num_pafs + self.num_keypoints + 1
        return nn.Sequential(
            nn.Conv2d(input_channels, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(128, output_channels, 1, 1, 0)
        )
    
    def forward(self, x):
        # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # ì´ˆê¸° ìŠ¤í…Œì´ì§€
        paf = self.stage1_paf(features)
        heatmap = self.stage1_heatmap(features)
        
        stage_outputs = [(paf, heatmap)]
        
        # ê°œì„  ìŠ¤í…Œì´ì§€ë“¤
        for stage in self.refinement_stages:
            combined = torch.cat([features, paf, heatmap], dim=1)
            paf = stage['paf'](combined)
            heatmap = stage['heatmap'](combined)
            stage_outputs.append((paf, heatmap))
        
        return stage_outputs

class U2NetModel(nn.Module):
    """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - Step 03"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super().__init__()
        
        # ì¸ì½”ë”
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU6(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU5(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU4(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU4F(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU4F(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU4F(1024, 256, 512)
        self.stage4d = RSU4(1024, 128, 256)
        self.stage3d = RSU5(512, 64, 128)
        self.stage2d = RSU6(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # ì¶œë ¥ ë ˆì´ì–´ë“¤
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6 * out_ch, out_ch, 1)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx6up = F.interpolate(hx6, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # ì¶œë ¥
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=x.shape[2:], mode='bilinear', align_corners=False)
        
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0)

# RSU ë¸”ë¡ë“¤ êµ¬í˜„
class RSU7(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = F.interpolate(hx6d, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RSU6(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx6 = self.rebnconv6(hx5)
        
        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RSU5(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx5 = self.rebnconv5(hx4)
        
        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RSU4(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx4 = self.rebnconv4(hx3)
        
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

class RSU4F(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=4)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=8)
        
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=4)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=2)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx2 = self.rebnconv2(hx1)
        hx3 = self.rebnconv3(hx2)
        hx4 = self.rebnconv4(hx3)
        
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx2d = self.rebnconv2d(torch.cat((hx3d, hx2), 1))
        hx1d = self.rebnconv1d(torch.cat((hx2d, hx1), 1))
        
        return hx1d + hxin

class REBNCONV(nn.Module):
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super().__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - Step 04"""
    
    def __init__(self, feature_size=256, num_control_points=18):
        super().__init__()
        self.feature_size = feature_size
        self.num_control_points = num_control_points
        
        # íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬
        self.feature_extractor = self._build_feature_extractor()
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        self.correlation = self._build_correlation_layer()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        self.tps_regression = self._build_tps_regression()
        
    def _build_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, self.feature_size, 3, 1, 1), nn.BatchNorm2d(self.feature_size), nn.ReLU(inplace=True)
        )
    
    def _build_correlation_layer(self):
        """ìƒê´€ê´€ê³„ ê³„ì‚° ë ˆì´ì–´"""
        return nn.Sequential(
            nn.Conv2d(self.feature_size * 2, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_tps_regression(self):
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(64, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(128, self.num_control_points * 2)  # x, y ì¢Œí‘œ
        )
    
    def forward(self, source_img, target_img):
        # íŠ¹ì§• ì¶”ì¶œ
        source_feat = self.feature_extractor(source_img)
        target_feat = self.feature_extractor(target_img)
        
        # íŠ¹ì§• ê²°í•©
        combined_feat = torch.cat([source_feat, target_feat], dim=1)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_map = self.correlation(combined_feat)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.tps_regression(correlation_map)
        tps_params = tps_params.view(-1, self.num_control_points, 2)
        
        return {
            'correlation_map': correlation_map,
            'tps_params': tps_params,
            'source_features': source_feat,
            'target_features': target_feat
        }

class HRVITONModel(nn.Module):
    """HR-VITON ê°€ìƒ í”¼íŒ… ëª¨ë¸ - Step 06"""
    
    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_downsampling=2, n_blocks=9):
        super().__init__()
        
        # ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬
        self.generator = self._build_generator(input_nc, output_nc, ngf, n_downsampling, n_blocks)
        
        # ì–´í…ì…˜ ëª¨ë“ˆ
        self.attention = self._build_attention_module()
        
        # ìœµí•© ëª¨ë“ˆ
        self.fusion = self._build_fusion_module()
    
    def _build_generator(self, input_nc, output_nc, ngf, n_downsampling, n_blocks):
        """ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±"""
        model = [
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=False),
            nn.InstanceNorm2d(ngf),
            nn.ReLU(True)
        ]
        
        # ë‹¤ìš´ìƒ˜í”Œë§
        for i in range(n_downsampling):
            mult = 2 ** i
            model += [
                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=False),
                nn.InstanceNorm2d(ngf * mult * 2),
                nn.ReLU(True)
            ]
        
        # ResNet ë¸”ë¡ë“¤
        mult = 2 ** n_downsampling
        for i in range(n_blocks):
            model += [ResnetBlock(ngf * mult, use_dropout=False)]
        
        # ì—…ìƒ˜í”Œë§
        for i in range(n_downsampling):
            mult = 2 ** (n_downsampling - i)
            model += [
                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),
                nn.InstanceNorm2d(int(ngf * mult / 2)),
                nn.ReLU(True)
            ]
        
        model += [nn.ReflectionPad2d(3)]
        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]
        model += [nn.Tanh()]
        
        return nn.Sequential(*model)
    
    def _build_attention_module(self):
        """ì–´í…ì…˜ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_fusion_module(self):
        """ìœµí•© ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 3, 3, 1, 1), nn.Tanh()
        )
    
    def forward(self, person_img, cloth_img, person_parse=None):
        # ê¸°ë³¸ ìƒì„±
        input_concat = torch.cat([person_img, cloth_img], dim=1)
        generated = self.generator(input_concat)
        
        # ì–´í…ì…˜ ë§µ ê³„ì‚°
        attention_map = self.attention(input_concat)
        
        # ì–´í…ì…˜ ì ìš© ìœµí•©
        attended_result = generated * attention_map + person_img * (1 - attention_map)
        
        # ì¶”ê°€ ìœµí•©
        final_result = self.fusion(torch.cat([attended_result, cloth_img], dim=1))
        
        return {
            'generated_image': final_result,
            'attention_map': attention_map,
            'intermediate': generated,
            'warped_cloth': cloth_img
        }

class ResnetBlock(nn.Module):
    """ResNet ë¸”ë¡"""
    def __init__(self, dim, use_dropout=False):
        super().__init__()
        self.conv_block = self._build_conv_block(dim, use_dropout)

    def _build_conv_block(self, dim, use_dropout):
        layers = []
        layers += [nn.ReflectionPad2d(1)]
        layers += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=False), nn.InstanceNorm2d(dim), nn.ReLU(True)]
        if use_dropout:
            layers += [nn.Dropout(0.5)]
        layers += [nn.ReflectionPad2d(1)]
        layers += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=False), nn.InstanceNorm2d(dim)]
        return nn.Sequential(*layers)

    def forward(self, x):
        return x + self.conv_block(x)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - í”„ë¡œë•ì…˜ ë²„ì „
# ==============================================

class ModelMemoryManager:
    """í”„ë¡œë•ì…˜ ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps", memory_limit_gb: float = 128.0):
        self.device = device
        self.memory_limit_gb = memory_limit_gb
        self.memory_threshold = 0.8
        
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return self.memory_limit_gb * 0.8
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.memory_limit_gb * 0.5
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif self.device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            
            logger.debug("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            if available_memory < self.memory_limit_gb * 0.2:  # 20% ë¯¸ë§Œ
                return True
            return False
        except Exception:
            return False

# ==============================================
# ğŸ”¥ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - í”„ë¡œë•ì…˜ ë²„ì „
# ==============================================

class ModelRegistry:
    """í”„ë¡œë•ì…˜ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not getattr(self, '_initialized', False):
            self.registered_models: Dict[str, Dict[str, Any]] = {}
            self._lock = threading.RLock()
            self._initialized = True
            logger.info("ModelRegistry ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model(self, 
                      name: str, 
                      model_class: Type, 
                      default_config: Dict[str, Any] = None,
                      loader_func: Optional[Callable] = None):
        """ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            try:
                self.registered_models[name] = {
                    'class': model_class,
                    'config': default_config or {},
                    'loader': loader_func,
                    'registered_at': time.time()
                }
                logger.info(f"ëª¨ë¸ ë“±ë¡: {name}")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            return self.registered_models.get(name)
    
    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            return list(self.registered_models.keys())
    
    def unregister_model(self, name: str) -> bool:
        """ëª¨ë¸ ë“±ë¡ í•´ì œ"""
        with self._lock:
            try:
                if name in self.registered_models:
                    del self.registered_models[name]
                    logger.info(f"ëª¨ë¸ ë“±ë¡ í•´ì œ: {name}")
                    return True
                return False
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ í•´ì œ ì‹¤íŒ¨ {name}: {e}")
                return False

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ - í”„ë¡œë•ì…˜ ë²„ì „
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ì™€ ModelLoader ê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.loaded_models: Dict[str, Any] = {}
        self._lock = threading.RLock()
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Stepì—ì„œ í•„ìš”í•œ ëª¨ë¸ ìš”ì²­"""
        try:
            with self._lock:
                cache_key = f"{self.step_name}_{model_name}"
                
                # ìºì‹œ í™•ì¸
                if cache_key in self.loaded_models:
                    return self.loaded_models[cache_key]
                
                # ëª¨ë¸ ë¡œë“œ
                model = await self.model_loader.load_model(model_name, **kwargs)
                
                if model:
                    self.loaded_models[cache_key] = model
                    logger.info(f"ğŸ“¦ {self.step_name}ì— {model_name} ëª¨ë¸ ì „ë‹¬ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™•ì¸ í•„ìš”")
                
                return model
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_recommended_model(self) -> Optional[Any]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ì„ íƒ"""
        model_recommendations = {
            'HumanParsingStep': 'human_parsing_graphonomy',
            'PoseEstimationStep': 'pose_estimation_openpose', 
            'ClothSegmentationStep': 'cloth_segmentation_u2net',
            'GeometricMatchingStep': 'geometric_matching_gmm',
            'ClothWarpingStep': 'cloth_warping_tom',
            'VirtualFittingStep': 'virtual_fitting_hrviton',
            'PostProcessingStep': 'post_processing_enhancer',
            'QualityAssessmentStep': 'quality_assessment_combined'
        }
        
        recommended_model = model_recommendations.get(self.step_name)
        if recommended_model:
            return await self.get_model(recommended_model)
        
        logger.error(f"âŒ {self.step_name}ì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def unload_models(self):
        """Stepì˜ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model_name, model in self.loaded_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                
                self.loaded_models.clear()
                logger.info(f"ğŸ—‘ï¸ {self.step_name} ëª¨ë¸ë“¤ ì–¸ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤ - í”„ë¡œë•ì…˜ ë²„ì „
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ AI ëª¨ë¸ ë¡œë”
    âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™
    âœ… í´ë°± ì œê±°, ì‹¤ì œ ëª¨ë¸ë§Œ ì‚¬ìš©
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """Step í´ë˜ìŠ¤ì™€ ì™„ë²½ í˜¸í™˜ë˜ëŠ” ìƒì„±ì"""
        
        # ğŸ”¥ Step í´ë˜ìŠ¤ ìƒì„±ì íŒ¨í„´ ì™„ì „ í˜¸í™˜
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # ModelLoader íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', 'backend/app/ai_pipeline/models/ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        
        # Step íŠ¹í™” ì„¤ì • ë³‘í•©
        self._merge_step_specific_config(kwargs)
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ í”„ë¡œë•ì…˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")

        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """Step íŠ¹í™” ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'model_cache_dir', 'use_fp16', 'max_cached_models',
            'lazy_loading'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _initialize_step_specific(self):
        """ModelLoader íŠ¹í™” ì´ˆê¸°í™”"""
        # í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤
        self.registry = ModelRegistry()
        self.memory_manager = ModelMemoryManager(device=self.device, memory_limit_gb=self.memory_gb)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        self._interface_lock = threading.RLock()
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self.use_fp16 = True
            if COREML_AVAILABLE:
                self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
        
        # ì‹¤ì œ AI ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
        self._initialize_model_registry()
        
        self.logger.info(f"ğŸ“¦ í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” - {self.device} (FP16: {self.use_fp16})")

    def _initialize_model_registry(self):
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ ë“±ë¡ - í”„ë¡œë•ì…˜ ê²½ë¡œ"""
        base_models_dir = self.model_cache_dir
        
        model_configs = {
            # Step 01: Human Parsing - Graphonomy
            "human_parsing_graphonomy": ModelConfig(
                name="human_parsing_graphonomy",
                model_type=ModelType.HUMAN_PARSING,
                model_class="GraphonomyModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "step_01_human_parsing" / "graphonomy.pth"),
                input_size=(512, 512),
                num_classes=20,
                metadata={"backbone": "resnet101", "pretrained": True}
            ),
            
            # Step 02: Pose Estimation - OpenPose
            "pose_estimation_openpose": ModelConfig(
                name="pose_estimation_openpose", 
                model_type=ModelType.POSE_ESTIMATION,
                model_class="OpenPoseModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "step_02_pose_estimation" / "openpose.pth"),
                input_size=(368, 368),
                num_classes=18,
                metadata={"num_pafs": 38, "stages": 6}
            ),
            
            # Step 03: Cloth Segmentation - U2Net
            "cloth_segmentation_u2net": ModelConfig(
                name="cloth_segmentation_u2net",
                model_type=ModelType.CLOTH_SEGMENTATION, 
                model_class="U2NetModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "step_03_cloth_segmentation" / "u2net.pth"),
                input_size=(320, 320),
                metadata={"architecture": "u2net", "output_channels": 1}
            ),
            
            # Step 04: Geometric Matching - GMM
            "geometric_matching_gmm": ModelConfig(
                name="geometric_matching_gmm",
                model_type=ModelType.GEOMETRIC_MATCHING,
                model_class="GeometricMatchingModel", 
                checkpoint_path=str(base_models_dir / "checkpoints" / "gmm_final.pth"),
                input_size=(512, 384),
                metadata={"control_points": 18, "feature_size": 256}
            ),
            
            # Step 05: Cloth Warping - TOM
            "cloth_warping_tom": ModelConfig(
                name="cloth_warping_tom",
                model_type=ModelType.CLOTH_WARPING,
                model_class="HRVITONModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "tom_final.pth"),
                input_size=(512, 384),
                metadata={"generator_type": "unet", "blocks": 9}
            ),
            
            # Step 06: Virtual Fitting - HR-VITON
            "virtual_fitting_hrviton": ModelConfig(
                name="virtual_fitting_hrviton",
                model_type=ModelType.VIRTUAL_FITTING,
                model_class="HRVITONModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "hrviton_final.pth"),
                input_size=(512, 384),
                metadata={"has_attention": True, "has_fusion": True}
            )
        }
        
        # ëª¨ë¸ ë“±ë¡
        for name, config in model_configs.items():
            self.register_model(name, config)

    def register_model(
        self,
        name: str,
        model_config: Union[ModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self._lock:
                # ModelConfig ê°ì²´ë¡œ ë³€í™˜
                if isinstance(model_config, dict):
                    model_config = ModelConfig(name=name, **model_config)
                elif not isinstance(model_config, ModelConfig):
                    raise ValueError(f"Invalid model_config type: {type(model_config)}")
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ìë™ ê°ì§€
                if model_config.device == "auto":
                    model_config.device = self.device
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                self.registry.register_model(
                    name=name,
                    model_class=self._get_model_class(model_config.model_class),
                    default_config=model_config.__dict__,
                    loader_func=loader_func
                )
                
                # ë‚´ë¶€ ì„¤ì • ì €ì¥
                self.model_configs[name] = model_config
                
                self.logger.info(f"ğŸ“ í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë“±ë¡: {name} ({model_config.model_type.value})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False

    def _get_model_class(self, model_class_name: str) -> Type:
        """ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ ë°˜í™˜"""
        model_classes = {
            'GraphonomyModel': GraphonomyModel,
            'OpenPoseModel': OpenPoseModel,
            'U2NetModel': U2NetModel,
            'GeometricMatchingModel': GeometricMatchingModel,
            'HRVITONModel': HRVITONModel,
            'StableDiffusionPipeline': None  # íŠ¹ë³„ ì²˜ë¦¬
        }
        return model_classes.get(model_class_name, None)

    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)

    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë¡œë“œ - í´ë°± ì—†ìŒ"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.error(f"âŒ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    raise ValueError(f"Model {name} not registered")
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë¡œë”© ì‹œì‘: {name} ({model_config.model_type.value})")
                
                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.error(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    raise RuntimeError(f"Failed to create model {name}")
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                await self._load_checkpoint(model, model_config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"âœ… í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡œë•ì…˜ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            raise

    async def _create_model_instance(
        self,
        model_config: ModelConfig,
        **kwargs
    ) -> Optional[Any]:
        """í”„ë¡œë•ì…˜ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class = model_config.model_class
            
            if model_class == "GraphonomyModel":
                return GraphonomyModel(
                    num_classes=model_config.num_classes or 20,
                    backbone=model_config.metadata.get('backbone', 'resnet101'),
                    pretrained=model_config.metadata.get('pretrained', True)
                )
            
            elif model_class == "OpenPoseModel":
                return OpenPoseModel(
                    num_keypoints=model_config.num_classes or 18,
                    num_pafs=model_config.metadata.get('num_pafs', 38)
                )
            
            elif model_class == "U2NetModel":
                return U2NetModel(
                    in_ch=3, 
                    out_ch=model_config.metadata.get('output_channels', 1)
                )
            
            elif model_class == "GeometricMatchingModel":
                return GeometricMatchingModel(
                    feature_size=model_config.metadata.get('feature_size', 256),
                    num_control_points=model_config.metadata.get('control_points', 18)
                )
            
            elif model_class == "HRVITONModel":
                return HRVITONModel(
                    input_nc=3, 
                    output_nc=3, 
                    ngf=64,
                    n_blocks=model_config.metadata.get('blocks', 9)
                )
            
            elif model_class == "StableDiffusionPipeline":
                return await self._create_diffusion_model(model_config)
            
            else:
                self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class}")
                raise ValueError(f"Unsupported model class: {model_class}")
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _create_diffusion_model(self, model_config: ModelConfig):
        """Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                if model_config.checkpoint_path and Path(model_config.checkpoint_path).exists():
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        model_config.checkpoint_path,
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                else:
                    self.logger.error(f"âŒ Diffusion ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_config.checkpoint_path}")
                    raise FileNotFoundError(f"Diffusion checkpoint not found: {model_config.checkpoint_path}")
                
                return pipeline
            else:
                self.logger.error("âŒ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                raise ImportError("diffusers library is required")
                
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _load_checkpoint(self, model: Any, model_config: ModelConfig):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ - í”„ë¡œë•ì…˜ ë²„ì „"""
        if not model_config.checkpoint_path:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {model_config.name}")
            return
            
        checkpoint_path = Path(model_config.checkpoint_path)
        
        if not checkpoint_path.exists():
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {checkpoint_path}")
            raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")
        
        try:
            # PyTorch ëª¨ë¸ì¸ ê²½ìš°
            if hasattr(model, 'load_state_dict'):
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict ì •ë¦¬
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif isinstance(state_dict, dict) and 'model' in state_dict:
                    state_dict = state_dict['model']
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                model.load_state_dict(cleaned_state_dict, strict=False)
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
            
            else:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê±´ë„ˆëœ€ (íŒŒì´í”„ë¼ì¸): {model_config.name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    async def _apply_m3_max_optimization(self, model: Any, model_config: ModelConfig) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™”"""
        try:
            optimizations_applied = []
            
            # 1. MPS ë””ë°”ì´ìŠ¤ ìµœì í™”
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # 3. CoreML ì»´íŒŒì¼ ì¤€ë¹„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if (COREML_AVAILABLE and 
                hasattr(model, 'eval') and 
                model_config.model_type in [ModelType.HUMAN_PARSING, ModelType.CLOTH_SEGMENTATION]):
                optimizations_applied.append("CoreML compilation ready")
            
            # 4. Metal Performance Shaders ìµœì í™”
            if self.device == 'mps':
                try:
                    # PyTorch MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
            if self.memory_manager.check_memory_pressure():
                await self._cleanup_least_used_models()
            
            # ìºì‹œëœ ëª¨ë¸ ìˆ˜ í™•ì¸
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                # ì‚¬ìš© ë¹ˆë„ì™€ ìµœê·¼ ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    # ëª¨ë¸ í•´ì œ
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                    
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def unload_model(self, name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                # ìºì‹œì—ì„œ ì œê±°
                keys_to_remove = [k for k in self.model_cache.keys() 
                                 if k.startswith(f"{name}_")]
                
                removed_count = 0
                for key in keys_to_remove:
                    if key in self.model_cache:
                        model = self.model_cache[key]
                        
                        # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                        del self.model_cache[key]
                        removed_count += 1
                    
                    self.access_counts.pop(key, None)
                    self.load_times.pop(key, None)
                    self.last_access.pop(key, None)
                
                if removed_count > 0:
                    self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {name} ({removed_count}ê°œ ì¸ìŠ¤í„´ìŠ¤)")
                    self.memory_manager.cleanup_memory()
                    return True
                else:
                    self.logger.warning(f"ì–¸ë¡œë“œí•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {name}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
            return False

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            if name not in self.model_configs:
                return None
                
            config = self.model_configs[name]
            cache_keys = [k for k in self.model_cache.keys() if k.startswith(f"{name}_")]
            
            return {
                "name": name,
                "model_type": config.model_type.value,
                "model_class": config.model_class,
                "device": config.device,
                "loaded": len(cache_keys) > 0,
                "cache_instances": len(cache_keys),
                "total_access_count": sum(self.access_counts.get(k, 0) for k in cache_keys),
                "average_load_time": sum(self.load_times.get(k, 0) for k in cache_keys) / max(1, len(cache_keys)),
                "checkpoint_path": config.checkpoint_path,
                "input_size": config.input_size,
                "last_access": max((self.last_access.get(k, 0) for k in cache_keys), default=0),
                "metadata": config.metadata
            }

    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            result = {}
            for name in self.model_configs.keys():
                info = self.get_model_info(name)
                if info:
                    result[name] = info
            return result

    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            usage = {
                "loaded_models": len(self.model_cache),
                "device": self.device,
                "available_memory_gb": self.memory_manager.get_available_memory(),
                "memory_pressure": self.memory_manager.check_memory_pressure(),
                "memory_limit_gb": self.memory_gb
            }
            
            if self.device == "cuda" and torch.cuda.is_available():
                usage.update({
                    "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                    "cached_gb": torch.cuda.memory_reserved() / 1024**3
                })
            elif self.device == "mps":
                try:
                    import psutil
                    process = psutil.Process()
                    usage.update({
                        "process_memory_gb": process.memory_info().rss / 1024**3,
                        "system_memory_percent": psutil.virtual_memory().percent
                    })
                except ImportError:
                    usage["memory_info"] = "psutil not available"
            
            return usage
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    interface = self.step_interfaces[step_name]
                    interface.unload_models()
                    del self.step_interfaces[step_name]
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.warning(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def initialize(self) -> bool:
        """ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
            missing_checkpoints = []
            for name, config in self.model_configs.items():
                if config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if not checkpoint_path.exists():
                        missing_checkpoints.append(name)
            
            if missing_checkpoints:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤: {missing_checkpoints}")
                self.logger.error("í”„ë¡œë•ì…˜ ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤")
                return False
            
            # M3 Max ìµœì í™” ì„¤ì •
            if COREML_AVAILABLE and self.is_m3_max:
                self.logger.info("ğŸ CoreML ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
            self.logger.info(f"âœ… í”„ë¡œë•ì…˜ AI ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ - {len(self.model_configs)}ê°œ ëª¨ë¸ ë“±ë¡ë¨")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                # ì „ì—­ ëª¨ë¸ ë¡œë” ì‚¬ìš©
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                return await self.model_interface.get_model(model_name)
            else:
                # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                return await self.model_interface.get_recommended_model()
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ëª¨ë¸ ë¡œë” ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None

@lru_cache(maxsize=1)
def get_global_model_loader() -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    try:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader()
        return _global_model_loader
    except Exception as e:
        logger.error(f"ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"Failed to create global ModelLoader: {e}")

def cleanup_global_loader():
    """ì „ì—­ ë¡œë” ì •ë¦¬"""
    global _global_model_loader
    
    try:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("âœ… ì „ì—­ ModelLoader ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def preprocess_image(image: Union[np.ndarray, Image.Image], target_size: tuple, normalize: bool = True) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV and PIL are required")
            
        if isinstance(image, np.ndarray):
            if image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            elif len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image).astype(np.float32)
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1) / 255.0
        
        # ì •ê·œí™”
        if normalize:
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            image_tensor = (image_tensor - mean) / std
        
        return image_tensor.unsqueeze(0)
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_segmentation(output: torch.Tensor, original_size: tuple, threshold: float = 0.5) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV is required")
            
        if output.dim() == 4:
            output = output.squeeze(0)
        
        # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
        if output.shape[0] > 1:
            output = torch.argmax(output, dim=0)
        else:
            output = (output > threshold).float()
        
        # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
        output = output.cpu().numpy().astype(np.uint8)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if output.shape != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        return output
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_pose(output: torch.Tensor, original_size: tuple, confidence_threshold: float = 0.3) -> Dict[str, Any]:
    """í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬"""
    try:
        if isinstance(output, (list, tuple)):
            # OpenPose ìŠ¤íƒ€ì¼ ì¶œë ¥ (PAF, heatmaps)
            pafs, heatmaps = output[-1]  # ë§ˆì§€ë§‰ ìŠ¤í…Œì´ì§€ ê²°ê³¼ ì‚¬ìš©
        else:
            heatmaps = output
            pafs = None
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = []
        if heatmaps.dim() == 4:
            heatmaps = heatmaps.squeeze(0)
        
        for i in range(heatmaps.shape[0] - 1):  # ë°°ê²½ ì œì™¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = heatmap[y, x]
            
            if confidence > confidence_threshold:
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                x_scaled = int(x * original_size[0] / heatmap.shape[1])
                y_scaled = int(y * original_size[1] / heatmap.shape[0])
                keypoints.append([x_scaled, y_scaled, confidence])
            else:
                keypoints.append([0, 0, 0])
        
        return {
            'keypoints': keypoints,
            'pafs': pafs.cpu().numpy() if pafs is not None else None,
            'heatmaps': heatmaps.cpu().numpy()
        }
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_model_loader(device: str = "mps", use_fp16: bool = True, **kwargs) -> ModelLoader:
    """ëª¨ë¸ ë¡œë” ìƒì„±"""
    return ModelLoader(device=device, use_fp16=use_fp16, **kwargs)

async def load_model_async(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        return await loader.load_model(model_name, config)
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def load_model_sync(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(loader.load_model(model_name, config))
    except Exception as e:
        logger.error(f"ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

# ğŸ”¥ ì´ˆê¸°í™” í•¨ìˆ˜ - í”„ë¡œë•ì…˜ ë²„ì „
def initialize_global_model_loader(
    device: str = "mps",
    memory_gb: float = 128.0,
    optimization_enabled: bool = True,
    **kwargs
) -> Dict[str, Any]:
    """
    ì „ì—­ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” - í”„ë¡œë•ì…˜ ë²„ì „
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (mps, cuda, cpu)
        memory_gb: ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
        optimization_enabled: ìµœì í™” í™œì„±í™” ì—¬ë¶€
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        Dict[str, Any]: ì´ˆê¸°í™”ëœ ë¡œë” ì„¤ì •
    """
    try:
        logger.info(f"ğŸš€ í”„ë¡œë•ì…˜ ModelLoader ì´ˆê¸°í™”: {device}, {memory_gb}GB")
        
        # ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë” ì„¤ì •
        loader_config = {
            "device": device,
            "memory_gb": memory_gb,
            "optimization_enabled": optimization_enabled,
            "cache_enabled": True,
            "lazy_loading": True,
            "memory_efficient": True,
            "production_mode": True
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if device == "mps":
            is_m3_max = memory_gb >= 64
            loader_config.update({
                "use_neural_engine": True,
                "use_unified_memory": True,
                "optimization_level": "maximum" if is_m3_max else "balanced",
                "coreml_enabled": COREML_AVAILABLE,
                "batch_size": 4 if is_m3_max else 2,
                "precision": "float16",
                "memory_pooling": True
            })
            
            if is_m3_max:
                loader_config.update({
                    "m3_max_optimizations": {
                        "neural_engine": True,
                        "metal_shaders": True,
                        "unified_memory": True,
                        "pipeline_parallel": True,
                        "memory_bandwidth": "400GB/s"
                    }
                })
        
        elif device == "cuda":
            loader_config.update({
                "mixed_precision": optimization_enabled,
                "tensorrt_enabled": False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì•ˆì •ì„± ìš°ì„ 
                "batch_size": 8,
                "memory_growth": True
            })
        
        else:  # CPU
            loader_config.update({
                "num_threads": os.cpu_count() or 4,
                "batch_size": 1,
                "memory_mapping": True
            })
        
        # ê²½ë¡œ ì„¤ì •
        model_paths = {
            "base_dir": Path("backend/app/ai_pipeline/models/ai_models"),
            "cache_dir": Path("backend/app/ai_pipeline/cache"),
            "checkpoints_dir": Path("backend/app/ai_pipeline/models/ai_models/checkpoints")
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for path in model_paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        loader_config["paths"] = {str(k): str(v) for k, v in model_paths.items()}
        
        logger.info("âœ… í”„ë¡œë•ì…˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
        return loader_config
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œë•ì…˜ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelFormat',
    'ModelConfig', 
    'ModelType',
    'ModelMemoryManager',
    'ModelRegistry',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    'RSU7', 'RSU6', 'RSU5', 'RSU4', 'RSU4F', 'REBNCONV',
    'ResnetBlock',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'load_model_async',
    'load_model_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'postprocess_pose',
    'cleanup_global_loader',
    'initialize_global_model_loader'
]

# ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

logger.info("âœ… í”„ë¡œë•ì…˜ ModelLoader ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - Step í´ë˜ìŠ¤ ì™„ë²½ ì—°ë™")