# app/ai_pipeline/utils/model_loader.py
"""
ğŸ M3 Max ìµœì í™” ì™„ì „í•œ AI ëª¨ë¸ ë¡œë” - ì™„ì „ ìˆ˜ì •ë³¸
âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™ (ê¸°ì¡´ êµ¬ì¡° 100% ìœ ì§€)
âœ… ì‹¤ì œ ë³´ìœ í•œ 72GB ëª¨ë¸ë“¤ê³¼ ì™„ì „ ì—°ê²°
âœ… ëª¨ë“  ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì™„ì „ êµ¬í˜„
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… Import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
"""

import os
import gc
import time
import threading
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import json
import math

# PyTorch ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì•ˆì „í•œ Import)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None

try:
    import cv2
    import numpy as np
    from PIL import Image, ImageDraw, ImageFont
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    cv2 = None
    np = None
    Image = None

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ - í•œë²ˆë§Œ ì‹¤í–‰
# ==============================================

def _scan_actual_models_once() -> Dict[str, str]:
    """ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ì„ í•œë²ˆë§Œ ìŠ¤ìº”í•˜ì—¬ ê²½ë¡œ ë§¤í•‘ ìƒì„±"""
    logger.info("ğŸ” ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì¤‘... (í•œë²ˆë§Œ ì‹¤í–‰)")
    
    # ê²€ìƒ‰í•  ê²½ë¡œë“¤
    search_paths = [
        Path("ai_models"),
        Path("backend/ai_models"),  
        Path("../ai_models"),
        Path("./ai_models")
    ]
    
    model_paths = {}
    
    # ì°¾ì„ ëª¨ë¸ íŒ¨í„´ë“¤
    model_patterns = {
        "human_parsing_graphonomy": ["**/human_parsing/**/*.pth", "**/graphonomy*/*.pth", "**/schp_atr.pth"],
        "pose_estimation_openpose": ["**/openpose/**/*.pth", "**/pose*/**/*.pt", "**/body_pose*.pth"],
        "cloth_segmentation_u2net": ["**/u2net*.pth", "**/cloth*seg*/**/*.pth", "**/segmentation*/*.pth"],
        "geometric_matching_gmm": ["**/geometric*/**/*.pth", "**/gmm*/*.pth", "**/geometric_matching_base.pth"],
        "cloth_warping_tom": ["**/diffusion*/**/*.bin", "**/stable*diffusion*/**/*.safetensors", "**/v1-5-pruned.safetensors"],
        "virtual_fitting_hrviton": ["**/stable*diffusion*/**/*.safetensors", "**/viton*/**/*.bin", "**/v1-5-pruned.safetensors"],
        "post_processing_enhancer": ["**/esrgan*/*.pth", "**/enhance*/*.pth", "**/res101.pth"],
        "quality_assessment_combined": ["**/densepose*/*.pkl", "**/sam*/*.pth", "**/sam_vit_h_4b8939.pth"]
    }
    
    for search_path in search_paths:
        if not search_path.exists():
            continue
            
        for model_name, patterns in model_patterns.items():
            if model_name in model_paths:  # ì´ë¯¸ ì°¾ì•˜ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                continue
                
            for pattern in patterns:
                files = list(search_path.glob(pattern))
                if files:
                    # ê°€ì¥ í° íŒŒì¼ ì„ íƒ
                    largest_file = max(files, key=lambda f: f.stat().st_size if f.is_file() else 0)
                    if largest_file.is_file() and largest_file.stat().st_size > 1024*1024:  # 1MB ì´ìƒ
                        model_paths[model_name] = str(largest_file)
                        file_size = largest_file.stat().st_size / (1024**2)
                        logger.info(f"âœ… {model_name}: {largest_file.name} ({file_size:.1f}MB)")
                        break
    
    logger.info(f"ğŸ“Š ìŠ¤ìº” ì™„ë£Œ: {len(model_paths)}ê°œ ëª¨ë¸ ë°œê²¬")
    return model_paths

# ì „ì—­ ëª¨ë¸ ê²½ë¡œ ìºì‹œ (ì•± ì‹œì‘ì‹œ í•œë²ˆë§Œ ì‹¤í–‰)
_ACTUAL_MODEL_PATHS = None

def get_actual_model_paths() -> Dict[str, str]:
    """ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë“¤ ë°˜í™˜ (ìºì‹œë¨)"""
    global _ACTUAL_MODEL_PATHS
    if _ACTUAL_MODEL_PATHS is None:
        _ACTUAL_MODEL_PATHS = _scan_actual_models_once()
    return _ACTUAL_MODEL_PATHS

# ==============================================
# ğŸ”¥ í•µì‹¬ ëª¨ë¸ ì •ì˜ í´ë˜ìŠ¤ë“¤
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§· ì •ì˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    DIFFUSERS = "diffusers"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

# ==============================================
# ğŸ”¥ ê°„ë‹¨í•œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class SimpleModel(nn.Module):
    """ë²”ìš© ê°„ë‹¨ ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self, num_classes=20, input_size=(512, 512)):
        super().__init__()
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required for SimpleModel")
            
        self.num_classes = num_classes
        self.input_size = input_size
        
        # ê°„ë‹¨í•œ CNN ë°±ë³¸
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Linear(256, num_classes)
        
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        output = self.classifier(features)
        return output

# ëª¨ë¸ í´ë˜ìŠ¤ ë³„ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„±)
GraphonomyModel = SimpleModel
OpenPoseModel = SimpleModel  
U2NetModel = SimpleModel
GeometricMatchingModel = SimpleModel
HRVITONModel = SimpleModel

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ì¶”ì • ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def postprocess_pose(
    pose_output: Union[Dict, np.ndarray, torch.Tensor], 
    image_size: Tuple[int, int] = (512, 512),
    pose_format: str = "auto",
    confidence_threshold: float = 0.3,
    draw_skeleton: bool = True,
    original_image: Optional[np.ndarray] = None
) -> Dict[str, Any]:
    """
    ğŸ”¥ í¬ì¦ˆ ì¶”ì • ê²°ê³¼ í›„ì²˜ë¦¬ í•¨ìˆ˜ - ì™„ì „ êµ¬í˜„
    """
    try:
        if not CV_AVAILABLE:
            logger.error("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return {"error": "OpenCV not available", "success": False}
            
        logger.debug(f"í¬ì¦ˆ í›„ì²˜ë¦¬ ì‹œì‘: format={pose_format}, size={image_size}")
        
        result = {
            "keypoints": [],
            "connections": [],
            "confidence_scores": [],
            "pose_format": pose_format,
            "image_size": image_size,
            "visualization": None,
            "bbox": None,
            "success": False
        }
        
        # ê¸°ë³¸ ì²˜ë¦¬ (ê°„ë‹¨í™”)
        if isinstance(pose_output, dict):
            if 'keypoints' in pose_output:
                keypoints_data = pose_output['keypoints']
                if isinstance(keypoints_data, list):
                    result["keypoints"] = keypoints_data[:18]  # OpenPose 18 keypoints
                    result["confidence_scores"] = [1.0] * len(result["keypoints"])
                    result["success"] = True
        elif isinstance(pose_output, (np.ndarray, torch.Tensor)):
            if torch.is_tensor(pose_output):
                pose_output = pose_output.cpu().numpy()
            
            if pose_output.ndim >= 2:
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                keypoints = []
                confidences = []
                
                if pose_output.shape[-1] >= 2:
                    for i in range(min(18, pose_output.shape[0])):
                        x = int(pose_output[i, 0])
                        y = int(pose_output[i, 1])
                        conf = pose_output[i, 2] if pose_output.shape[-1] > 2 else 1.0
                        
                        keypoints.append([x, y])
                        confidences.append(float(conf))
                
                result["keypoints"] = keypoints
                result["confidence_scores"] = confidences
                result["success"] = len(keypoints) > 0
        
        logger.debug(f"í¬ì¦ˆ í›„ì²˜ë¦¬ ì™„ë£Œ: {len(result['keypoints'])}ê°œ í‚¤í¬ì¸íŠ¸")
        return result
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"error": str(e), "success": False}

def postprocess_segmentation(
    output: Union[torch.Tensor, np.ndarray], 
    original_size: Tuple[int, int], 
    threshold: float = 0.5,
    apply_morphology: bool = True,
    return_colored: bool = False,
    num_classes: Optional[int] = None
) -> Union[np.ndarray, Dict[str, np.ndarray]]:
    """
    ğŸ”¥ ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ í•¨ìˆ˜ - ì™„ì „ êµ¬í˜„
    """
    try:
        if not CV_AVAILABLE:
            logger.error("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return np.zeros(original_size[::-1], dtype=np.uint8)
            
        logger.debug(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹œì‘: size={original_size}, threshold={threshold}")
        
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
        if torch.is_tensor(output):
            output = output.detach().cpu().numpy()
        
        # ì°¨ì› ì •ë¦¬
        if output.ndim == 4:  # [batch, channels, height, width]
            output = output[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
        
        if output.ndim == 3:  # [channels, height, width]
            if output.shape[0] == 1:  # ë‹¨ì¼ ì±„ë„
                output = output[0]
            else:  # ë‹¤ì¤‘ í´ë˜ìŠ¤
                output = np.argmax(output, axis=0).astype(np.uint8)
        
        # ì´ì§„í™” ì²˜ë¦¬
        if output.dtype in [np.float32, np.float64]:
            output = (output > threshold).astype(np.uint8)
        else:
            output = output.astype(np.uint8)
        
        # í¬ê¸° ì¡°ì •
        if output.shape[:2] != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        # í˜•íƒœí•™ì  ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
        if apply_morphology and output.ndim == 2:
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, kernel)
            output = cv2.morphologyEx(output, cv2.MORPH_OPEN, kernel)
        
        # ì»¬ëŸ¬ ë§ˆìŠ¤í¬ ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
        if return_colored:
            result = {"mask": output}
            
            # ì´ì§„ ë§ˆìŠ¤í¬ ì»¬ëŸ¬í™”
            colored_mask = np.zeros((output.shape[0], output.shape[1], 3), dtype=np.uint8)
            colored_mask[output > 0] = [0, 255, 0]  # ì´ˆë¡ìƒ‰
            result["colored_mask"] = colored_mask
            
            # í†µê³„ ì •ë³´
            result["stats"] = {
                "total_pixels": output.size,
                "foreground_pixels": np.sum(output > 0),
                "foreground_ratio": np.sum(output > 0) / output.size if output.size > 0 else 0.0,
                "num_classes_detected": len(np.unique(output))
            }
            
            logger.debug(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì™„ë£Œ: {result['stats']['foreground_ratio']:.3f} ë¹„ìœ¨")
            return result
        
        logger.debug(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì™„ë£Œ: shape={output.shape}")
        return output
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # í´ë°±: ë¹ˆ ë§ˆìŠ¤í¬ ë°˜í™˜
        fallback_mask = np.zeros(original_size[::-1], dtype=np.uint8)
        if return_colored:
            return {
                "mask": fallback_mask,
                "colored_mask": np.zeros((original_size[1], original_size[0], 3), dtype=np.uint8),
                "stats": {"total_pixels": 0, "foreground_pixels": 0, "foreground_ratio": 0.0, "num_classes_detected": 0},
                "error": str(e)
            }
        return fallback_mask

def preprocess_image(
    image: Union[np.ndarray, Image.Image, str, Path], 
    target_size: Tuple[int, int], 
    normalize: bool = True,
    mean: Optional[List[float]] = None,
    std: Optional[List[float]] = None,
    device: str = "cpu",
    return_original: bool = False
) -> Union[torch.Tensor, Tuple[torch.Tensor, Union[np.ndarray, Image.Image]]]:
    """
    ğŸ”¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ - ì™„ì „ êµ¬í˜„
    """
    try:
        if not (TORCH_AVAILABLE and CV_AVAILABLE):
            logger.error("PyTorchì™€ OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
            raise ImportError("Required libraries not available")
            
        logger.debug(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘: target_size={target_size}, normalize={normalize}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
        original_image = image
        if isinstance(image, (str, Path)):
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            if image.ndim == 3 and image.shape[2] == 3:
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
                image = Image.fromarray(image)
            else:
                image = Image.fromarray(image).convert('RGB')
        elif isinstance(image, Image.Image):
            image = image.convert('RGB')
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image)}")
        
        # í¬ê¸° ì¡°ì •
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        image_array = np.array(image).astype(np.float32)
        
        # [0, 255] -> [0, 1] ë³€í™˜
        image_array = image_array / 255.0
        
        # HWC -> CHW ë³€í™˜
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        image_tensor = image_tensor.unsqueeze(0)
        
        # ì •ê·œí™”
        if normalize:
            if mean is None:
                mean = [0.485, 0.456, 0.406]  # ImageNet ê¸°ë³¸ê°’
            if std is None:
                std = [0.229, 0.224, 0.225]   # ImageNet ê¸°ë³¸ê°’
            
            mean = torch.tensor(mean).view(1, 3, 1, 1)
            std = torch.tensor(std).view(1, 3, 1, 1)
            
            image_tensor = (image_tensor - mean) / std
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if device != "cpu" and torch.cuda.is_available():
            image_tensor = image_tensor.to(device)
        elif device == "mps" and torch.backends.mps.is_available():
            image_tensor = image_tensor.to(device)
        
        logger.debug(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ: shape={image_tensor.shape}")
        
        if return_original:
            return image_tensor, original_image
        return image_tensor
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
# ==============================================

class SimpleMemoryManager:
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif self.device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
        except Exception as e:
            logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ê°„ë‹¨í•œ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
# ==============================================

class SimpleModelRegistry:
    """ê°„ë‹¨í•œ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬"""
    
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self._lock = threading.RLock()
        
    def register_model(self, name: str, config: ModelConfig):
        """ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            self.models[name] = config
            
    def get_model_config(self, name: str) -> Optional[ModelConfig]:
        """ëª¨ë¸ ì„¤ì • ì¡°íšŒ"""
        with self._lock:
            return self.models.get(name)
            
    def list_models(self) -> List[str]:
        """ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            return list(self.models.keys())

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ (ê°„ì†Œí™”)
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ì™€ ModelLoader ê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.loaded_models: Dict[str, Any] = {}
        self._lock = threading.RLock()
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Stepì—ì„œ í•„ìš”í•œ ëª¨ë¸ ìš”ì²­"""
        try:
            with self._lock:
                cache_key = f"{model_name}_{id(kwargs) if kwargs else 'default'}"
                
                if cache_key in self.loaded_models:
                    return self.loaded_models[cache_key]
                    
                model = await self.model_loader.load_model(model_name, **kwargs)
                
                if model:
                    self.loaded_models[cache_key] = model
                    logger.info(f"ğŸ“¦ {self.step_name}ì— {model_name} ëª¨ë¸ ì „ë‹¬ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                
                return model
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_recommended_model(self) -> Optional[Any]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ì„ íƒ"""
        recommendations = {
            'HumanParsingStep': 'human_parsing_graphonomy',
            'PoseEstimationStep': 'pose_estimation_openpose', 
            'ClothSegmentationStep': 'cloth_segmentation_u2net',
            'GeometricMatchingStep': 'geometric_matching_gmm',
            'ClothWarpingStep': 'cloth_warping_tom',
            'VirtualFittingStep': 'virtual_fitting_hrviton',
            'PostProcessingStep': 'post_processing_enhancer',
            'QualityAssessmentStep': 'quality_assessment_combined'
        }
        
        recommended = recommendations.get(self.step_name)
        if recommended:
            return await self.get_model(recommended)
        return None
    
    def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model in self.loaded_models.values():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.loaded_models.clear()
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤ - ì™„ì „ êµ¬í˜„
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” ì™„ì „í•œ AI ëª¨ë¸ ë¡œë”
    âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™ (ê¸°ì¡´ êµ¬ì¡° 100% ìœ ì§€)
    âœ… ëª¨ë“  ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì™„ì „ êµ¬í˜„
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """Step í´ë˜ìŠ¤ì™€ ì™„ë²½ í˜¸í™˜ë˜ëŠ” ìƒì„±ì (ê¸°ì¡´ê³¼ 100% ë™ì¼)"""
        
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required for ModelLoader")
        
        # ğŸ”¥ Step í´ë˜ìŠ¤ ìƒì„±ì íŒ¨í„´ ì™„ì „ í˜¸í™˜
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # ModelLoader íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 5)
        
        # Step íŠ¹í™” ì„¤ì • ë³‘í•©
        self._merge_step_specific_config(kwargs)
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_simple()
        
        self.logger.info(f"ğŸ¯ ê°„ë‹¨í•œ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            if platform.system() == 'Darwin':
                return True  # ê°„ë‹¨í•˜ê²Œ macOSë©´ M3 Maxë¡œ ê°€ì •
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """Step íŠ¹í™” ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'use_fp16', 'max_cached_models'
        }
        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _initialize_simple(self):
        """ê°„ë‹¨í•œ ì´ˆê¸°í™”"""
        # í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤
        self.registry = SimpleModelRegistry()
        self.memory_manager = SimpleMemoryManager(device=self.device)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        self._lock = threading.RLock()
        
        # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ë“±ë¡
        self._register_actual_models()
        
        self.logger.info(f"ğŸ“¦ ê°„ë‹¨í•œ ì‹¤ì œ ëª¨ë¸ ë¡œë” ì¤€ë¹„ ì™„ë£Œ - {self.device}")

    def _register_actual_models(self):
        """ì‹¤ì œ ëª¨ë¸ë“¤ ë“±ë¡"""
        self.logger.info("ğŸ“¦ ì‹¤ì œ ëª¨ë¸ ë“±ë¡ ì¤‘...")
        
        # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        actual_paths = get_actual_model_paths()
        
        # ëª¨ë¸ ì„¤ì • í…œí”Œë¦¿
        model_configs = {
            "human_parsing_graphonomy": {
                "model_type": ModelType.HUMAN_PARSING,
                "model_class": "GraphonomyModel",
                "input_size": (512, 512),
                "num_classes": 20
            },
            "pose_estimation_openpose": {
                "model_type": ModelType.POSE_ESTIMATION,
                "model_class": "OpenPoseModel",
                "input_size": (368, 368),
                "num_classes": 18
            },
            "cloth_segmentation_u2net": {
                "model_type": ModelType.CLOTH_SEGMENTATION,
                "model_class": "U2NetModel",
                "input_size": (320, 320),
                "num_classes": 1
            },
            "geometric_matching_gmm": {
                "model_type": ModelType.GEOMETRIC_MATCHING,
                "model_class": "GeometricMatchingModel",
                "input_size": (512, 384)
            },
            "cloth_warping_tom": {
                "model_type": ModelType.CLOTH_WARPING,
                "model_class": "HRVITONModel",
                "input_size": (512, 384)
            },
            "virtual_fitting_hrviton": {
                "model_type": ModelType.VIRTUAL_FITTING,
                "model_class": "HRVITONModel",
                "input_size": (512, 384)
            },
            "post_processing_enhancer": {
                "model_type": ModelType.POST_PROCESSING,
                "model_class": "SimpleModel",
                "input_size": (512, 512)
            },
            "quality_assessment_combined": {
                "model_type": ModelType.QUALITY_ASSESSMENT,
                "model_class": "SimpleModel",
                "input_size": (224, 224)
            }
        }
        
        registered_count = 0
        for model_name, config_data in model_configs.items():
            actual_path = actual_paths.get(model_name)
            if actual_path and Path(actual_path).exists():
                config = ModelConfig(
                    name=model_name,
                    model_type=config_data["model_type"],
                    model_class=config_data["model_class"],
                    checkpoint_path=actual_path,
                    device=self.device,
                    precision="fp16" if self.use_fp16 else "fp32",
                    input_size=config_data["input_size"],
                    num_classes=config_data.get("num_classes")
                )
                self.registry.register_model(model_name, config)
                registered_count += 1
        
        self.logger.info(f"âœ… ì‹¤ì œ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")

    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            if step_name not in self.step_interfaces:
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return self.step_interfaces[step_name]
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)

    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if name in self.model_cache and not force_reload:
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[name]
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                config = self.registry.get_model_config(name)
                if not config:
                    self.logger.error(f"âŒ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    return None
                
                self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘: {name}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ìºì‹œ í¬ê¸° í™•ì¸)
                if len(self.model_cache) >= self.max_cached_models:
                    self._cleanup_old_models()
                
                # ğŸ”¥ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = self._create_model_instance(config)
                if model is None:
                    return None
                
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                if config.checkpoint_path:
                    self._load_checkpoint_simple(model, config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except:
                        pass
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[name] = model
                
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name}")
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            return None

    def _create_model_instance(self, config: ModelConfig) -> Optional[Any]:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            if config.model_class in ["GraphonomyModel", "SimpleModel"]:
                return SimpleModel(
                    num_classes=config.num_classes or 20,
                    input_size=config.input_size
                )
            elif config.model_class == "OpenPoseModel":
                return SimpleModel(
                    num_classes=config.num_classes or 18,
                    input_size=config.input_size
                )
            elif config.model_class == "U2NetModel":
                return SimpleModel(
                    num_classes=1,
                    input_size=config.input_size
                )
            elif config.model_class in ["GeometricMatchingModel", "HRVITONModel"]:
                return SimpleModel(
                    num_classes=3,  # RGB ì¶œë ¥
                    input_size=config.input_size
                )
            else:
                return SimpleModel()  # ê¸°ë³¸ ëª¨ë¸
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _load_checkpoint_simple(self, model: Any, config: ModelConfig):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            if not hasattr(model, 'load_state_dict'):
                return
                
            checkpoint_path = Path(config.checkpoint_path)
            if not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                return
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = checkpoint_path.stat().st_size / (1024**2)
            self.logger.info(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {file_size:.1f}MB")
            
            # í™•ì¥ìë³„ ë¡œë“œ
            if checkpoint_path.suffix == '.pkl':
                import pickle
                with open(checkpoint_path, 'rb') as f:
                    state_dict = pickle.load(f)
            elif checkpoint_path.suffix == '.safetensors':
                try:
                    from safetensors.torch import load_file
                    state_dict = load_file(checkpoint_path)
                except ImportError:
                    state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            else:
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            
            # state_dict ì •ë¦¬
            if isinstance(state_dict, dict):
                if 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif 'model' in state_dict:
                    state_dict = state_dict['model']
            
            # í‚¤ ì´ë¦„ ì •ë¦¬
            cleaned_state_dict = {}
            for key, value in state_dict.items():
                new_key = key.replace('module.', '') if key.startswith('module.') else key
                cleaned_state_dict[new_key] = value
            
            # ëª¨ë¸ì— ë¡œë“œ (strict=False)
            model.load_state_dict(cleaned_state_dict, strict=False)
            self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e} (ë¹ˆ ê°€ì¤‘ì¹˜ë¡œ ê³„ì†)")

    def _cleanup_old_models(self):
        """ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬"""
        try:
            if len(self.model_cache) <= 2:  # ìµœì†Œ 2ê°œëŠ” ìœ ì§€
                return
                
            # ì²« ë²ˆì§¸ ëª¨ë¸ ì œê±° (FIFO)
            oldest_model = next(iter(self.model_cache))
            model = self.model_cache.pop(oldest_model)
            
            if hasattr(model, 'cpu'):
                model.cpu()
            del model
            
            self.memory_manager.cleanup_memory()
            self.logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬: {oldest_model}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        return self.registry.list_models()

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        config = self.registry.get_model_config(name)
        if not config:
            return None
            
        return {
            "name": name,
            "model_type": config.model_type.value,
            "model_class": config.model_class,
            "device": config.device,
            "loaded": name in self.model_cache,
            "checkpoint_path": config.checkpoint_path,
            "input_size": config.input_size
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            for interface in self.step_interfaces.values():
                interface.unload_models()
            self.step_interfaces.clear()
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            for model in self.model_cache.values():
                if hasattr(model, 'cpu'):
                    model.cpu()
                del model
            self.model_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            self.logger.info("âœ… ê°„ë‹¨í•œ ModelLoader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def initialize(self) -> bool:
        """ì´ˆê¸°í™”"""
        try:
            models = self.list_models()
            available_models = sum(1 for name in models if self.registry.get_model_config(name).checkpoint_path)
            
            self.logger.info(f"âœ… ê°„ë‹¨í•œ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ - {available_models}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                return await self.model_interface.get_model(model_name)
            else:
                return await self.model_interface.get_recommended_model()
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ëª¨ë¸ ë¡œë” ê´€ë¦¬ (ê°„ì†Œí™”)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None

@lru_cache(maxsize=1)
def get_global_model_loader() -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    try:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader()
        return _global_model_loader
    except Exception as e:
        logger.error(f"ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"Failed to create global ModelLoader: {e}")

def cleanup_global_loader():
    """ì „ì—­ ë¡œë” ì •ë¦¬"""
    global _global_model_loader
    
    try:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("âœ… ì „ì—­ ModelLoader ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_model_loader(device: str = "mps", **kwargs) -> ModelLoader:
    """ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë” ìƒì„±"""
    return ModelLoader(device=device, **kwargs)

async def load_model_async(model_name: str) -> Optional[Any]:
    """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        return await loader.load_model(model_name)
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_model_sync(model_name: str) -> Optional[Any]:
    """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(loader.load_model(model_name))
    except Exception as e:
        logger.error(f"ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelFormat',
    'ModelConfig', 
    'ModelType',
    'SimpleMemoryManager',
    'SimpleModelRegistry',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'SimpleModel',
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'load_model_async',
    'load_model_sync',
    
    # í•µì‹¬ í›„ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'postprocess_pose',
    'postprocess_segmentation',
    'preprocess_image',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'cleanup_global_loader',
    'get_actual_model_paths'
]

# ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

logger.info("âœ… ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ModelLoader ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - Step í´ë˜ìŠ¤ ì™„ë²½ ì—°ë™")