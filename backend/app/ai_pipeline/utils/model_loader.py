# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ MyCloset AI - ì•ˆì •ì ì¸ ModelLoader v3.0 (AI ì¶”ë¡  ì œê±°, í•µì‹¬ ê¸°ëŠ¥ ìœ ì§€)
================================================================================
âœ… AI ì¶”ë¡  ë¡œì§ ì™„ì „ ì œê±° - ì•ˆì •ì„± ìš°ì„ 
âœ… í•µì‹¬ ëª¨ë¸ ë¡œë” ê¸°ëŠ¥ë§Œ ìœ ì§€
âœ… BaseStepMixin 100% í˜¸í™˜ì„± ë³´ì¥
âœ… StepModelInterface ì •ì˜ ë¬¸ì œ í•´ê²°
âœ… auto_model_detector ì—°ë™ ìœ ì§€
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€
âœ… ì‹¤í–‰ ë©ˆì¶¤ í˜„ìƒ ì™„ì „ í•´ê²°
================================================================================

Author: MyCloset AI Team
Date: 2025-07-28
Version: 3.0 (ì•ˆì •ì ì¸ í•µì‹¬ ê¸°ëŠ¥ë§Œ)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (í•„ìˆ˜ë§Œ)
# ==============================================

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# ë””ë°”ì´ìŠ¤ ë° ì‹œìŠ¤í…œ ì •ë³´
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')

try:
    import platform
    if platform.system() == 'Darwin':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            if 'M3' in result.stdout:
                IS_M3_MAX = True
                DEFAULT_DEVICE = "mps"  # M3ì—ì„œëŠ” MPS ìš°ì„ 
        except:
            pass
except:
    pass

# auto_model_detector import (ì•ˆì „ ì²˜ë¦¬)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 2. ê¸°ë³¸ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì…"""
    SEGMENTATION = "segmentation"
    DETECTION = "detection"
    POSE_ESTIMATION = "pose_estimation"
    DIFFUSION = "diffusion"
    CLASSIFICATION = "classification"
    MATCHING = "matching"
    ENHANCEMENT = "enhancement"
    QUALITY = "quality"

class ModelStatus(Enum):
    """ëª¨ë¸ ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    path: str
    model_type: ModelType
    device: str
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    access_count: int = 0
    last_access: float = 0.0
    error: Optional[str] = None

@dataclass 
class StepModelRequirement:
    """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­"""
    step_name: str
    required_models: List[str]
    optional_models: List[str] = field(default_factory=list)
    model_configs: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ 3. ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (AI ì¶”ë¡  ì œê±°)
# ==============================================

class BaseModel:
    """ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (AI ì¶”ë¡  ì œê±°)"""
    
    def __init__(self, model_name: str, model_path: str, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.logger = logging.getLogger(f"BaseModel.{model_name}")
        
    def load(self) -> bool:
        """ëª¨ë¸ ë¡œë”© (ë©”íƒ€ë°ì´í„°ë§Œ)"""
        try:
            start_time = time.time()
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.model_path.exists():
                self.logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
                return False
            
            # ë©”íƒ€ë°ì´í„° ë¡œë”©
            self.memory_usage_mb = self.model_path.stat().st_size / (1024 * 1024)
            self.load_time = time.time() - start_time
            self.loaded = True
            
            self.logger.info(f"âœ… ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë”© ì™„ë£Œ: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        self.loaded = False
        gc.collect()
    
    def get_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0
        }

# ==============================================
# ğŸ”¥ 4. StepModelInterface ì •ì˜ (ì˜¤ë¥˜ í•´ê²°)
# ==============================================

class StepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - BaseStepMixin í˜¸í™˜"""
    
    def __init__(self, model_loader, step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # Stepë³„ ëª¨ë¸ë“¤ (ë©”íƒ€ë°ì´í„°ë§Œ)
        self.step_models: Dict[str, BaseModel] = {}
        self.primary_model: Optional[BaseModel] = None
        
        # ìš”êµ¬ì‚¬í•­
        self.requirements: Optional[StepModelRequirement] = None
        
        # ìƒì„± ì‹œê°„ ë° í†µê³„
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            self.requirements = StepModelRequirement(
                step_name=self.step_name,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                model_configs=requirements.get('model_configs', {})
            )
            self.logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {len(self.requirements.required_models)}ê°œ í•„ìˆ˜ ëª¨ë¸")
        except Exception as e:
            self.logger.error(f"âŒ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë©”íƒ€ë°ì´í„°ë§Œ)"""
        try:
            if not model_name or model_name == "default":
                if self.primary_model:
                    return self.primary_model
                elif self.step_models:
                    return next(iter(self.step_models.values()))
                else:
                    return self._load_default_model()
            
            # íŠ¹ì • ëª¨ë¸ ìš”ì²­
            if model_name in self.step_models:
                return self.step_models[model_name]
            
            # ìƒˆ ëª¨ë¸ ë¡œë”©
            model = self.model_loader.load_model(model_name, step_name=self.step_name)
            
            if model:
                self.step_models[model_name] = model
                if not self.primary_model:
                    self.primary_model = model
                    
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - BaseStepMixin í˜¸í™˜"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _load_default_model(self) -> Optional[BaseModel]:
        """ê¸°ë³¸ ëª¨ë¸ ë¡œë”©"""
        try:
            if self.step_name in self.model_loader.default_mappings:
                mapping = self.model_loader.default_mappings[self.step_name]
                
                # ë¡œì»¬ ëª¨ë¸ ìš°ì„  ì‹œë„
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_loader.model_cache_dir / local_path
                    if full_path.exists():
                        model = BaseModel(
                            model_name=local_path,
                            model_path=str(full_path),
                            device=self.model_loader.device
                        )
                        if model.load():
                            self.primary_model = model
                            self.step_models['default'] = model
                            return model
            
            self.logger.warning(f"âš ï¸ {self.step_name}ì— ëŒ€í•œ ê¸°ë³¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin í˜¸í™˜"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'required': kwargs.get('required', True),
                'device': kwargs.get('device', self.model_loader.device),
                **kwargs
            }
            
            self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_step_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ"""
        return {
            "step_name": self.step_name,
            "creation_time": self.creation_time,
            "models_loaded": len(self.step_models),
            "primary_model": self.primary_model.model_name if self.primary_model else None,
            "access_count": self.access_count,
            "error_count": self.error_count,
            "available_models": list(self.step_models.keys()),
            "requirements": {
                "required_models": self.requirements.required_models if self.requirements else [],
                "optional_models": self.requirements.optional_models if self.requirements else []
            }
        }

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ ModelLoader í´ë˜ìŠ¤ v3.0
# ==============================================

class ModelLoader:
    """
    ğŸ”¥ ModelLoader v3.0 - ì•ˆì •ì ì¸ í•µì‹¬ ê¸°ëŠ¥ë§Œ (AI ì¶”ë¡  ì œê±°)
    
    íŠ¹ì§•:
    - AI ì¶”ë¡  ë¡œì§ ì™„ì „ ì œê±°
    - ëª¨ë¸ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ë§Œ ìˆ˜í–‰
    - BaseStepMixin 100% í˜¸í™˜
    - StepModelInterface ì •ìƒ ì œê³µ
    - auto_model_detector ì—°ë™ ìœ ì§€
    """
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):
        """ModelLoader ì´ˆê¸°í™”"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # ìë™ ê°ì§€: backend/ai_models
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ê´€ë¦¬ (ë©”íƒ€ë°ì´í„°ë§Œ)
        self.loaded_models: Dict[str, BaseModel] = {}
        self.model_info: Dict[str, ModelInfo] = {}
        self.model_status: Dict[str, ModelStatus] = {}
        
        # Step ìš”êµ¬ì‚¬í•­
        self.step_requirements: Dict[str, StepModelRequirement] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # auto_model_detector ì—°ë™
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0
        }
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
        self.logger.info(f"ğŸš€ ModelLoader v3.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“± Device: {self.device} (M3 Max: {IS_M3_MAX})")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ: {self.model_cache_dir}")
        
        # ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ ë¡œë”©
        self._load_model_mappings()
    
    def _initialize_auto_detector(self):
        """auto_model_detector ì´ˆê¸°í™”"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("âœ… auto_model_detector ì—°ë™ ì™„ë£Œ")
                    # ìë™ í†µí•© ì‹œë„
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("âš ï¸ auto_detector ì¸ìŠ¤í„´ìŠ¤ê°€ None")
            else:
                self.logger.warning("âš ï¸ AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"âŒ auto_model_detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector í†µí•©"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            # ê°„ë‹¨í•œ ëª¨ë¸ íƒì§€ ë° í†µí•©
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            # ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "auto_detected": True
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"âœ… AutoDetector í†µí•© ì™„ë£Œ: {integrated_count}ê°œ ëª¨ë¸")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ AutoDetector í†µí•© ì‹¤íŒ¨: {e}")
            return False
    
    @property
    def available_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
        return self._available_models_cache
    
    def _load_model_mappings(self):
        """ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ ë¡œë”©"""
        try:
            # Stepë³„ ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘
            self.default_mappings = {
                'HumanParsingStep': {
                    'model_type': 'segmentation',
                    'local_paths': [
                        'step_01_human_parsing/graphonomy.pth',
                        'step_01_human_parsing/atr_model.pth'
                    ]
                },
                'PoseEstimationStep': {
                    'model_type': 'pose',
                    'local_paths': [
                        'step_02_pose_estimation/yolov8n-pose.pt',
                        'step_02_pose_estimation/openpose_pose_coco.pth'
                    ]
                },
                'ClothSegmentationStep': {
                    'model_type': 'segmentation',
                    'local_paths': [
                        'step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'step_03_cloth_segmentation/u2net.pth'
                    ]
                },
                'GeometricMatchingStep': {
                    'model_type': 'matching',
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth',
                        'step_04_geometric_matching/tps_model.pth'
                    ]
                },
                'ClothWarpingStep': {
                    'model_type': 'warping',
                    'local_paths': [
                        'step_05_cloth_warping/RealVisXL_V4.0.safetensors',
                        'step_05_cloth_warping/vgg19_warping.pth'
                    ]
                },
                'VirtualFittingStep': {
                    'model_type': 'diffusion',
                    'local_paths': [
                        'step_06_virtual_fitting/diffusion_pytorch_model.safetensors',
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.bin'
                    ]
                },
                'PostProcessingStep': {
                    'model_type': 'enhancement',
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth',
                        'step_07_post_processing/sr_model.pth'
                    ]
                },
                'QualityAssessmentStep': {
                    'model_type': 'quality',
                    'local_paths': [
                        'step_08_quality_assessment/ViT-L-14.pt',
                        'step_08_quality_assessment/open_clip_pytorch_model.bin'
                    ]
                }
            }
            
            self.logger.info(f"âœ… ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ ë¡œë”© ì™„ë£Œ: {len(self.default_mappings)}ê°œ Step")
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ë§¤í•‘ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.default_mappings = {}
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """ëª¨ë¸ ë¡œë”© (ë©”íƒ€ë°ì´í„°ë§Œ)"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return model
                
                # ìƒˆ ëª¨ë¸ ë¡œë”©
                self.model_status[model_name] = ModelStatus.LOADING
                
                # ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                    self.model_status[model_name] = ModelStatus.ERROR
                    return None
                
                # BaseModel ìƒì„± ë° ë¡œë”©
                model = BaseModel(
                    model_name=model_name,
                    model_path=model_path,
                    device=self.device
                )
                
                if model.load():
                    # ìºì‹œì— ì €ì¥
                    self.loaded_models[model_name] = model
                    self.model_info[model_name] = ModelInfo(
                        name=model_name,
                        path=model_path,
                        model_type=ModelType(kwargs.get('model_type', 'classification')),
                        device=self.device,
                        loaded=True,
                        load_time=model.load_time,
                        memory_mb=model.memory_usage_mb,
                        access_count=1,
                        last_access=time.time()
                    )
                    self.model_status[model_name] = ModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({model.memory_usage_mb:.1f}MB)")
                    
                    # ìºì‹œ í¬ê¸° ê´€ë¦¬
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = ModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.model_status[model_name] = ModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°"""
        try:
            # ì§ì ‘ ê²½ë¡œ ì§€ì •
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # available_modelsì—ì„œ ì°¾ê¸°
            if model_name in self.available_models:
                model_info = self.available_models[model_name]
                path = Path(model_info.get('path', ''))
                if path.exists():
                    return str(path)
            
            # ë¡œì»¬ ìºì‹œì—ì„œ ì°¾ê¸°
            possible_paths = [
                self.model_cache_dir / f"{model_name}",
                self.model_cache_dir / f"{model_name}.pth",
                self.model_cache_dir / f"{model_name}.pt",
                self.model_cache_dir / f"{model_name}.safetensors"
            ]
            
            # Step ê¸°ë°˜ ë§¤í•‘ì—ì„œ ì°¾ê¸°
            step_name = kwargs.get('step_name')
            if step_name and step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        possible_paths.insert(0, full_path)
            
            # ë””ë ‰í† ë¦¬ ê²€ìƒ‰
            for pattern in [f"**/{model_name}.*", f"**/*{model_name}*"]:
                for found_path in self.model_cache_dir.glob(pattern):
                    if found_path.is_file():
                        possible_paths.append(found_path)
            
            # ì²« ë²ˆì§¸ ì¡´ì¬í•˜ëŠ” ê²½ë¡œ ë°˜í™˜
            for path in possible_paths:
                if Path(path).exists():
                    return str(path)
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _manage_cache(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ ëª¨ë¸ ì œê±°
            models_by_access = sorted(
                self.model_info.items(),
                key=lambda x: x[1].last_access
            )
            
            models_to_remove = models_by_access[:len(self.loaded_models) - self.max_cached_models]
            
            for model_name, _ in models_to_remove:
                self.unload_model(model_name)
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = ModelStatus.NOT_LOADED
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ì§€ì›
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            interface = StepModelInterface(self, step_name)
            
            if step_requirements:
                interface.register_requirements(step_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name}")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            return StepModelInterface(self, step_name)
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            self.step_requirements[step_name] = StepModelRequirement(
                step_name=step_name,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                model_configs=requirements.get('model_configs', {})
            )
            
            self.logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("âœ… ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        return self.initialize(**kwargs)
    
    # ==============================================
    # ğŸ”¥ ëˆ„ë½ëœ í•µì‹¬ ë©”ì„œë“œë“¤ ì¶”ê°€ (Step íŒŒì¼ì—ì„œ ìš”ì²­)
    # ==============================================
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'required': kwargs.get('required', True),
                    'device': kwargs.get('device', self.device),
                    'priority': kwargs.get('priority', 1.0),
                    **kwargs
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦"""
        try:
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            if model_name not in self.model_info and model_name not in self.available_models:
                return False
            
            # Step ìš”êµ¬ì‚¬í•­ í™•ì¸
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # ê¸°ë³¸ ë§¤í•‘ í™•ì¸
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path:
                        return True
            
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í˜¸í™˜ ê°€ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return (model_name in self.loaded_models or 
                model_name in self.available_models or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        return self.create_step_interface(step_name)
    
    def register_step_model_dependencies(self, step_name: str, dependencies: Dict[str, Any]) -> bool:
        """Step ëª¨ë¸ ì˜ì¡´ì„± ë“±ë¡"""
        try:
            for model_name, model_config in dependencies.items():
                self.register_model_requirement(
                    model_name=model_name,
                    **model_config
                )
            
            self.logger.info(f"âœ… Step ì˜ì¡´ì„± ë“±ë¡: {step_name} ({len(dependencies)}ê°œ ëª¨ë¸)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì˜ì¡´ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """Step ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        try:
            validation_result = {
                'step_name': step_name,
                'valid': True,
                'missing_models': [],
                'incompatible_models': [],
                'available_models': [],
                'errors': []
            }
            
            if step_name not in self.step_requirements:
                validation_result['valid'] = False
                validation_result['errors'].append(f"Step ìš”êµ¬ì‚¬í•­ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {step_name}")
                return validation_result
            
            step_req = self.step_requirements[step_name]
            
            # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
            for model_name in step_req.required_models:
                if not self.has_model(model_name):
                    validation_result['missing_models'].append(model_name)
                    validation_result['valid'] = False
                elif not self.validate_model_compatibility(model_name, step_name):
                    validation_result['incompatible_models'].append(model_name)
                    validation_result['valid'] = False
                else:
                    validation_result['available_models'].append(model_name)
            
            return validation_result
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'step_name': step_name, 'valid': False, 'error': str(e)}
    
    def get_step_model_status(self, step_name: str) -> Dict[str, Any]:
        """Step ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if step_name in self.step_interfaces:
                interface = self.step_interfaces[step_name]
                return interface.get_step_status()
            else:
                return {
                    'step_name': step_name,
                    'interface_exists': False,
                    'requirements': self.step_requirements.get(step_name)
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'step_name': step_name, 'error': str(e)}
    
    def list_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        return list(self.loaded_models.keys())
    
    def get_models_by_step(self, step_name: str) -> List[str]:
        """Stepë³„ ëª¨ë¸ ëª©ë¡"""
        try:
            models = []
            
            # Step ìš”êµ¬ì‚¬í•­ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                models.extend(step_req.required_models)
                models.extend(step_req.optional_models)
            
            # ê¸°ë³¸ ë§¤í•‘ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    model_name = Path(local_path).stem
                    if model_name not in models:
                        models.append(model_name)
            
            return list(set(models))
            
        except Exception as e:
            self.logger.error(f"âŒ Stepë³„ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_models_by_type(self, model_type: str) -> List[str]:
        """ëª¨ë¸ íƒ€ì…ë³„ ëª©ë¡"""
        try:
            models = []
            
            for model_name, model_info in self.model_info.items():
                if model_info.model_type.value == model_type:
                    models.append(model_name)
            
            # available_modelsì—ì„œë„ í™•ì¸
            for model_name, model_info in self.available_models.items():
                if model_info.get('model_type') == model_type and model_name not in models:
                    models.append(model_name)
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒ€ì…ë³„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def load_model_sync(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        return self.load_model(model_name, **kwargs)
    
    async def unload_model_async(self, model_name: str) -> bool:
        """ë¹„ë™ê¸° ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self._executor, self.unload_model, model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def preload_models(self, model_names: List[str]) -> Dict[str, bool]:
        """ëª¨ë¸ ì¼ê´„ ì‚¬ì „ ë¡œë”©"""
        try:
            results = {}
            
            for model_name in model_names:
                try:
                    model = self.load_model(model_name)
                    results[model_name] = model is not None and model.loaded
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                    results[model_name] = False
            
            success_count = sum(results.values())
            self.logger.info(f"âœ… ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ: {success_count}/{len(model_names)}ê°œ ì„±ê³µ")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¼ê´„ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {name: False for name in model_names}
    
    def verify_model_integrity(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦"""
        try:
            if model_name not in self.model_info and model_name not in self.available_models:
                return False
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if model_name in self.model_info:
                model_path = Path(self.model_info[model_name].path)
            else:
                model_path = Path(self.available_models[model_name].get('path', ''))
            
            if not model_path.exists():
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸ (0ë°”ì´íŠ¸ê°€ ì•„ë‹˜)
            if model_path.stat().st_size == 0:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def check_model_dependencies(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì˜ì¡´ì„± í™•ì¸"""
        try:
            dependencies = {
                'model_name': model_name,
                'dependencies_met': True,
                'missing_dependencies': [],
                'hardware_requirements': []
            }
            
            # ê¸°ë³¸ ì˜ì¡´ì„± í™•ì¸
            if not self.has_model(model_name):
                dependencies['missing_dependencies'].append('model_file')
                dependencies['dependencies_met'] = False
            
            # í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
            if self.device == 'mps' and not IS_M3_MAX:
                dependencies['hardware_requirements'].append('Apple Silicon required for MPS')
            
            return dependencies
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'model_name': model_name, 'dependencies_met': False, 'error': str(e)}
    
    def validate_hardware_compatibility(self, model_name: str) -> bool:
        """í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± ê²€ì¦"""
        try:
            # ê¸°ë³¸ì ìœ¼ë¡œ í˜¸í™˜ ê°€ëŠ¥
            if self.device == 'cpu':
                return True
            
            # MPS í˜¸í™˜ì„±
            if self.device == 'mps':
                return IS_M3_MAX
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_performance_stats(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í†µê³„"""
        try:
            if model_name not in self.model_info:
                return {'model_name': model_name, 'available': False}
            
            model_info = self.model_info[model_name]
            
            return {
                'model_name': model_name,
                'available': True,
                'load_time': model_info.load_time,
                'memory_usage_mb': model_info.memory_mb,
                'access_count': model_info.access_count,
                'last_access': model_info.last_access,
                'efficiency_score': model_info.access_count / max(1, model_info.memory_mb / 100)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'model_name': model_name, 'error': str(e)}
    
    def estimate_model_memory_usage(self, model_name: str) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            if model_name in self.model_info:
                return self.model_info[model_name].memory_mb
            
            if model_name in self.available_models:
                return self.available_models[model_name].get('size_mb', 0)
            
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •
            for step_name, mapping in self.default_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path:
                        full_path = self.model_cache_dir / local_path
                        if full_path.exists():
                            file_size_mb = full_path.stat().st_size / (1024 * 1024)
                            return file_size_mb * 1.2  # ë¡œë”© ì‹œ ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œ
            
            return 0.0
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_inference_history(self, model_name: str) -> List[Dict[str, Any]]:
        """ì¶”ë¡  ì´ë ¥ ì¡°íšŒ"""
        try:
            # ê°„ë‹¨í•œ í†µê³„ë§Œ ë°˜í™˜
            if model_name in self.model_info:
                model_info = self.model_info[model_name]
                return [{
                    'model_name': model_name,
                    'total_accesses': model_info.access_count,
                    'last_access': model_info.last_access,
                    'load_time': model_info.load_time
                }]
            
            return []
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶”ë¡  ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def inject_dependencies(self, step_instance) -> bool:
        """Step ì¸ìŠ¤í„´ìŠ¤ì— ì˜ì¡´ì„± ì£¼ì…"""
        try:
            # ModelLoader ì£¼ì…
            if hasattr(step_instance, 'set_model_loader'):
                step_instance.set_model_loader(self)
            
            # Step ì¸í„°í˜ì´ìŠ¤ ì£¼ì…
            step_name = getattr(step_instance, 'step_name', step_instance.__class__.__name__)
            if hasattr(step_instance, 'set_model_interface'):
                interface = self.create_step_interface(step_name)
                step_instance.set_model_interface(interface)
            
            self.logger.info(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def setup_step_environment(self, step_name: str) -> Dict[str, Any]:
        """Step í™˜ê²½ ì„¤ì •"""
        try:
            environment = {
                'step_name': step_name,
                'device': self.device,
                'model_cache_dir': str(self.model_cache_dir),
                'available_models': self.get_models_by_step(step_name),
                'hardware_info': {
                    'is_m3_max': IS_M3_MAX,
                    'default_device': DEFAULT_DEVICE,
                    'conda_env': CONDA_ENV
                }
            }
            
            # Stepë³„ ê¸°ë³¸ ì„¤ì • ì ìš©
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                environment['model_type'] = mapping.get('model_type', 'unknown')
                environment['local_models'] = mapping.get('local_paths', [])
            
            return environment
            
        except Exception as e:
            self.logger.error(f"âŒ Step í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            return {'step_name': step_name, 'error': str(e)}
    
    def configure_step_models(self, step_name: str, config: Dict[str, Any]) -> bool:
        """Step ëª¨ë¸ ì„¤ì •"""
        try:
            # Step ìš”êµ¬ì‚¬í•­ ì—…ë°ì´íŠ¸
            if 'required_models' in config:
                for model_name in config['required_models']:
                    self.register_model_requirement(
                        model_name=model_name,
                        model_type=config.get('model_type', 'BaseModel'),
                        required=True
                    )
            
            if 'optional_models' in config:
                for model_name in config['optional_models']:
                    self.register_model_requirement(
                        model_name=model_name,
                        model_type=config.get('model_type', 'BaseModel'),
                        required=False
                    )
            
            self.logger.info(f"âœ… Step ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    "name": info.name,
                    "status": "loaded" if info.loaded else "not_loaded",
                    "device": info.device,
                    "memory_mb": info.memory_mb,
                    "load_time": info.load_time,
                    "access_count": info.access_count,
                    "last_access": info.last_access
                }
            else:
                status = self.model_status.get(model_name, ModelStatus.NOT_LOADED)
                return {"name": model_name, "status": status.value}
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"name": model_name, "status": "error", "error": str(e)}
    
    # ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def clear_cache(self, force: bool = False) -> bool:
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        try:
            if force:
                # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
                for model_name in list(self.loaded_models.keys()):
                    self.unload_model(model_name)
            else:
                # ì˜¤ë˜ëœ ëª¨ë¸ë“¤ë§Œ ì •ë¦¬
                self._manage_cache()
            
            gc.collect()
            self.logger.info("âœ… ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        try:
            initial_memory = sum(info.memory_mb for info in self.model_info.values())
            
            # ì˜¤ë˜ëœ ëª¨ë¸ë“¤ ì–¸ë¡œë“œ (1ì‹œê°„ ì´ìƒ ë¯¸ì‚¬ìš©)
            current_time = time.time()
            models_to_unload = []
            
            for model_name, info in self.model_info.items():
                if current_time - info.last_access > 3600:  # 1ì‹œê°„
                    models_to_unload.append(model_name)
            
            unloaded_count = 0
            for model_name in models_to_unload:
                if self.unload_model(model_name):
                    unloaded_count += 1
            
            gc.collect()
            
            final_memory = sum(info.memory_mb for info in self.model_info.values())
            freed_memory = initial_memory - final_memory
            
            result = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "freed_memory_mb": freed_memory,
                "unloaded_models": unloaded_count,
                "optimization_successful": freed_memory > 0
            }
            
            self.logger.info(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {freed_memory:.1f}MB í•´ì œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e), "optimization_successful": False}
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            total_memory = sum(info.memory_mb for info in self.model_info.values())
            loaded_count = len(self.loaded_models)
            
            return {
                "total_memory_mb": total_memory,
                "loaded_models_count": loaded_count,
                "average_per_model_mb": total_memory / loaded_count if loaded_count > 0 else 0,
                "device": self.device,
                "cache_size": len(self.model_info)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ModelLoader ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "initialized": self.is_initialized,
                "device": self.device,
                "loaded_models_count": len(self.loaded_models),
                "total_memory_mb": sum(info.memory_mb for info in self.model_info.values()),
                "auto_detector_integration": self._integration_successful,
                "available_models_count": len(self.available_models),
                "step_interfaces_count": len(self.step_interfaces)
            }
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì§„ë‹¨"""
        try:
            health_status = {
                "status": "healthy",
                "timestamp": time.time(),
                "system_info": {
                    "device": self.device,
                    "is_m3_max": IS_M3_MAX,
                    "conda_env": CONDA_ENV
                },
                "models": {
                    "loaded_count": len(self.loaded_models),
                    "available_count": len(self.available_models),
                    "total_memory_mb": sum(info.memory_mb for info in self.model_info.values())
                },
                "issues": []
            }
            
            # ë¬¸ì œ í™•ì¸
            if len(self.available_models) == 0:
                health_status["issues"].append("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŒ")
                health_status["status"] = "warning"
            
            if not self._integration_successful and AUTO_DETECTOR_AVAILABLE:
                health_status["issues"].append("AutoDetector í†µí•© ì‹¤íŒ¨")
                health_status["status"] = "warning"
            
            total_memory = health_status["models"]["total_memory_mb"]
            if total_memory > 10000:  # 10GB ì´ìƒ
                health_status["issues"].append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {total_memory:.1f}MB")
                health_status["status"] = "warning"
            
            if health_status["issues"]:
                self.logger.warning(f"âš ï¸ ModelLoader ê±´ê°•ìƒíƒœ ê²½ê³ : {len(health_status['issues'])}ê°œ ë¬¸ì œ")
            else:
                self.logger.info("âœ… ModelLoader ê±´ê°•ìƒíƒœ ì–‘í˜¸")
            
            return health_status
            
        except Exception as e:
            self.logger.error(f"âŒ ê±´ê°•ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
    
    def detect_available_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìë™ ê°ì§€"""
        try:
            detected = {}
            
            # AutoDetector ì‚¬ìš©
            if self.auto_detector and self._integration_successful:
                detected.update(self.available_models)
            
            # ê¸°ë³¸ ë§¤í•‘ì—ì„œ ê°ì§€
            for step_name, mapping in self.default_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        model_name = full_path.stem
                        detected[model_name] = {
                            'name': model_name,
                            'path': str(full_path),
                            'size_mb': full_path.stat().st_size / (1024 * 1024),
                            'step_class': step_name,
                            'model_type': mapping.get('model_type', 'unknown'),
                            'detected_by': 'default_mapping'
                        }
            
            return detected
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {e}")
            return {}
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            models = []
            
            # available_modelsì—ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            for model_name, model_info in self.available_models.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # ë¡œë”© ìƒíƒœ ì¶”ê°€
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                models.append(model_info_copy)
            
            # ê¸°ë³¸ ë§¤í•‘ì—ì„œ ì¶”ê°€
            for step_name, mapping in self.default_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        model_name = full_path.stem
                        if model_name not in [m['name'] for m in models]:
                            models.append({
                                'name': model_name,
                                'path': str(full_path),
                                'type': mapping.get('model_type', 'unknown'),
                                'loaded': model_name in self.loaded_models,
                                'step_class': step_name,
                                'size_mb': full_path.stat().st_size / (1024 * 1024)
                            })
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'model_type': info.model_type.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'error': info.error
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self.available_models)
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # ìºì‹œ ì •ë¦¬
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            self.logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 6. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            try:
                # ì„¤ì • ì ìš©
                loader_config = config or {}
                
                _global_model_loader = ModelLoader(
                    device=loader_config.get('device', 'auto'),
                    max_cached_models=loader_config.get('max_cached_models', 10),
                    enable_optimization=loader_config.get('enable_optimization', True),
                    **loader_config
                )
                
                logger.info("âœ… ì „ì—­ ModelLoader v3.0 ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°±
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(get_global_model_loader(), step_name)

def get_model(model_name: str) -> Optional[BaseModel]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[BaseModel]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# ==============================================
# ğŸ”¥ 7. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> Dict[str, Any]:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦"""
    try:
        path = Path(checkpoint_path)
        
        validation = {
            "path": str(path),
            "exists": path.exists(),
            "is_file": path.is_file() if path.exists() else False,
            "size_mb": 0,
            "readable": False,
            "valid_extension": False,
            "is_valid": False,
            "errors": []
        }
        
        if not path.exists():
            validation["errors"].append("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            return validation
        
        if not path.is_file():
            validation["errors"].append("íŒŒì¼ì´ ì•„ë‹˜")
            return validation
        
        # í¬ê¸° í™•ì¸
        try:
            size_bytes = path.stat().st_size
            validation["size_mb"] = size_bytes / (1024 * 1024)
        except Exception as e:
            validation["errors"].append(f"í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ì½ê¸° ê¶Œí•œ í™•ì¸
        try:
            validation["readable"] = os.access(path, os.R_OK)
            if not validation["readable"]:
                validation["errors"].append("ì½ê¸° ê¶Œí•œ ì—†ìŒ")
        except Exception as e:
            validation["errors"].append(f"ê¶Œí•œ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # í™•ì¥ì í™•ì¸
        valid_extensions = ['.pth', '.pt', '.ckpt', '.safetensors', '.bin']
        validation["valid_extension"] = path.suffix.lower() in valid_extensions
        if not validation["valid_extension"]:
            validation["errors"].append(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {path.suffix}")
        
        # ì „ì²´ ìœ íš¨ì„± íŒë‹¨
        validation["is_valid"] = (
            validation["exists"] and 
            validation["is_file"] and 
            validation["readable"] and 
            validation["valid_extension"] and
            validation["size_mb"] > 0 and
            len(validation["errors"]) == 0
        )
        
        return validation
        
    except Exception as e:
        return {
            "path": str(checkpoint_path),
            "exists": False,
            "is_valid": False,
            "errors": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}"]
        }

def get_system_capabilities() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ëŠ¥ë ¥ ì¡°íšŒ"""
    return {
        "numpy_available": NUMPY_AVAILABLE,
        "pil_available": PIL_AVAILABLE,
        "auto_detector_available": AUTO_DETECTOR_AVAILABLE,
        "default_device": DEFAULT_DEVICE,
        "is_m3_max": IS_M3_MAX,
        "conda_env": CONDA_ENV,
        "python_version": sys.version
    }

def emergency_cleanup() -> bool:
    """ë¹„ìƒ ì •ë¦¬ í•¨ìˆ˜"""
    try:
        logger.warning("ğŸš¨ ë¹„ìƒ ì •ë¦¬ ì‹œì‘...")
        
        # ì „ì—­ ModelLoader ì •ë¦¬
        global _global_model_loader
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        logger.info("âœ… ë¹„ìƒ ì •ë¦¬ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë¹„ìƒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ 8. Export ë° ì´ˆê¸°í™”
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'BaseModel',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelType',
    'ModelStatus',
    'ModelInfo',
    'StepModelRequirement',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_checkpoint_file',
    'get_system_capabilities',
    'emergency_cleanup',
    
    # ìƒìˆ˜ë“¤
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ==============================================
# ğŸ”¥ 9. ëª¨ë“ˆ ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸
# ==============================================

logger.info("=" * 80)
logger.info("ğŸš€ ì•ˆì •ì ì¸ ModelLoader v3.0 ë¡œë“œ ì™„ë£Œ (AI ì¶”ë¡  ì œê±°)")
logger.info("=" * 80)
logger.info("âœ… AI ì¶”ë¡  ë¡œì§ ì™„ì „ ì œê±° - ì•ˆì •ì„± ìš°ì„ ")
logger.info("âœ… í•µì‹¬ ëª¨ë¸ ë¡œë” ê¸°ëŠ¥ë§Œ ìœ ì§€")
logger.info("âœ… BaseStepMixin 100% í˜¸í™˜ì„± ë³´ì¥")
logger.info("âœ… StepModelInterface ì •ì˜ ë¬¸ì œ í•´ê²°")
logger.info("âœ… auto_model_detector ì—°ë™ ìœ ì§€")
logger.info("âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€")
logger.info("âœ… ì‹¤í–‰ ë©ˆì¶¤ í˜„ìƒ ì™„ì „ í•´ê²°")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX})")
logger.info(f"   NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info("=" * 80)

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_loader = get_global_model_loader()
    logger.info(f"ğŸ‰ ì•ˆì •ì ì¸ ModelLoader v3.0 ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   ë””ë°”ì´ìŠ¤: {_test_loader.device}")
    logger.info(f"   ëª¨ë¸ ìºì‹œ: {_test_loader.model_cache_dir}")
    logger.info(f"   ê¸°ë³¸ ë§¤í•‘: {len(_test_loader.default_mappings)}ê°œ Step")
    logger.info(f"   AutoDetector í†µí•©: {_test_loader._integration_successful}")
    logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(_test_loader.available_models)}ê°œ")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸš€ ì•ˆì •ì ì¸ ModelLoader v3.0 í…ŒìŠ¤íŠ¸ (AI ì¶”ë¡  ì œê±°)")
    print("=" * 60)
    
    async def test_model_loader():
        # ModelLoader ìƒì„±
        loader = get_global_model_loader()
        print(f"âœ… ModelLoader ìƒì„±: {type(loader).__name__}")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {loader.device}")
        print(f"ğŸ“ ëª¨ë¸ ìºì‹œ: {loader.model_cache_dir}")
        
        # ì‹œìŠ¤í…œ ëŠ¥ë ¥ í™•ì¸
        capabilities = get_system_capabilities()
        print(f"\nğŸ“Š ì‹œìŠ¤í…œ ëŠ¥ë ¥:")
        print(f"   NumPy: {'âœ…' if capabilities['numpy_available'] else 'âŒ'}")
        print(f"   PIL: {'âœ…' if capabilities['pil_available'] else 'âŒ'}")
        print(f"   AutoDetector: {'âœ…' if capabilities['auto_detector_available'] else 'âŒ'}")
        print(f"   M3 Max: {'âœ…' if capabilities['is_m3_max'] else 'âŒ'}")
        
        # Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        step_interface = create_step_interface("HumanParsingStep")
        print(f"\nğŸ”— Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {type(step_interface).__name__}")
        
        step_status = step_interface.get_step_status()
        print(f"ğŸ“Š Step ìƒíƒœ:")
        print(f"   Step ì´ë¦„: {step_status['step_name']}")
        print(f"   ë¡œë”©ëœ ëª¨ë¸: {step_status['models_loaded']}ê°œ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        models = loader.list_available_models()
        print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models)}ê°œ")
        if models:
            for i, model in enumerate(models[:3]):
                print(f"   {i+1}. {model['name']}: {model.get('size_mb', 0):.1f}MB")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics = loader.get_performance_metrics()
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   ë¡œë”©ëœ ëª¨ë¸: {metrics['loaded_models_count']}ê°œ")
        print(f"   ìºì‹œ íˆíŠ¸: {metrics['cache_hits']}íšŒ")
        print(f"   ì´ ë©”ëª¨ë¦¬: {metrics['total_memory_mb']:.1f}MB")
        print(f"   ì˜¤ë¥˜ íšŸìˆ˜: {metrics['error_count']}íšŒ")
        print(f"   AutoDetector í†µí•©: {metrics['auto_detector_integration']}")
    
    try:
        asyncio.run(test_model_loader())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ ì•ˆì •ì ì¸ ModelLoader v3.0 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("âœ… AI ì¶”ë¡  ë¡œì§ ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´")
    print("âœ… StepModelInterface ì •ì˜ ë¬¸ì œ í•´ê²°")
    print("âœ… í•µì‹¬ ëª¨ë¸ ë¡œë” ê¸°ëŠ¥ ìœ ì§€")
    print("âœ… BaseStepMixin 100% í˜¸í™˜ì„± í™•ë³´")
    print("âœ… ì‹¤í–‰ ë©ˆì¶¤ í˜„ìƒ ì™„ì „ í•´ê²°")