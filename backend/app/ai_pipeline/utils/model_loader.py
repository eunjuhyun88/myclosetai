# backend/app/ai_pipeline/utils/model_loader.py
"""
üî• MyCloset AI - ÏïàÏ†ïÏ†ÅÏù∏ ModelLoader v3.0 (AI Ï∂îÎ°† Ï†úÍ±∞, ÌïµÏã¨ Í∏∞Îä• Ïú†ÏßÄ)
================================================================================
‚úÖ AI Ï∂îÎ°† Î°úÏßÅ ÏôÑÏ†Ñ Ï†úÍ±∞ - ÏïàÏ†ïÏÑ± Ïö∞ÏÑ†
‚úÖ ÌïµÏã¨ Î™®Îç∏ Î°úÎçî Í∏∞Îä•Îßå Ïú†ÏßÄ
‚úÖ BaseStepMixin 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•
‚úÖ StepModelInterface Ï†ïÏùò Î¨∏Ï†ú Ìï¥Í≤∞
‚úÖ auto_model_detector Ïó∞Îèô Ïú†ÏßÄ
‚úÖ Í∏∞Ï°¥ Ìï®ÏàòÎ™Ö/Î©îÏÑúÎìúÎ™Ö 100% Ïú†ÏßÄ
‚úÖ Ïã§Ìñâ Î©àÏ∂§ ÌòÑÏÉÅ ÏôÑÏ†Ñ Ìï¥Í≤∞
================================================================================

Author: MyCloset AI Team
Date: 2025-07-28
Version: 3.0 (ÏïàÏ†ïÏ†ÅÏù∏ ÌïµÏã¨ Í∏∞Îä•Îßå)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod

# ==============================================
# üî• 1. ÏïàÏ†ÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Import (ÌïÑÏàòÎßå)
# ==============================================

# Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# ÎîîÎ∞îÏù¥Ïä§ Î∞è ÏãúÏä§ÌÖú Ï†ïÎ≥¥
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')

try:
    import platform
    if platform.system() == 'Darwin':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            if 'M3' in result.stdout:
                IS_M3_MAX = True
                DEFAULT_DEVICE = "mps"  # M3ÏóêÏÑúÎäî MPS Ïö∞ÏÑ†
        except:
            pass
except:
    pass

# auto_model_detector import (ÏïàÏ†Ñ Ï≤òÎ¶¨)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# TYPE_CHECKING Ìå®ÌÑ¥ÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# Î°úÍπÖ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)

# ==============================================
# üî• 2. Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Ï†ïÏùò
# ==============================================

class ModelType(Enum):
    """Î™®Îç∏ ÌÉÄÏûÖ"""
    SEGMENTATION = "segmentation"
    DETECTION = "detection"
    POSE_ESTIMATION = "pose_estimation"
    DIFFUSION = "diffusion"
    CLASSIFICATION = "classification"
    MATCHING = "matching"
    ENHANCEMENT = "enhancement"
    QUALITY = "quality"

class ModelStatus(Enum):
    """Î™®Îç∏ ÏÉÅÌÉú"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"

@dataclass
class ModelInfo:
    """Î™®Îç∏ Ï†ïÎ≥¥"""
    name: str
    path: str
    model_type: ModelType
    device: str
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    access_count: int = 0
    last_access: float = 0.0
    error: Optional[str] = None

@dataclass 
class StepModelRequirement:
    """StepÎ≥Ñ Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠"""
    step_name: str
    required_models: List[str]
    optional_models: List[str] = field(default_factory=list)
    model_configs: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# üî• 3. Í∏∞Î≥∏ Î™®Îç∏ ÌÅ¥ÎûòÏä§ (AI Ï∂îÎ°† Ï†úÍ±∞)
# ==============================================

class BaseModel:
    """Í∏∞Î≥∏ Î™®Îç∏ ÌÅ¥ÎûòÏä§ (AI Ï∂îÎ°† Ï†úÍ±∞)"""
    
    def __init__(self, model_name: str, model_path: str, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.logger = logging.getLogger(f"BaseModel.{model_name}")
        
    def load(self) -> bool:
        """Î™®Îç∏ Î°úÎî© (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå)"""
        try:
            start_time = time.time()
            
            # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
            if not self.model_path.exists():
                self.logger.error(f"‚ùå Î™®Îç∏ ÌååÏùº ÏóÜÏùå: {self.model_path}")
                return False
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎî©
            self.memory_usage_mb = self.model_path.stat().st_size / (1024 * 1024)
            self.load_time = time.time() - start_time
            self.loaded = True
            
            self.logger.info(f"‚úÖ Î™®Îç∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎî© ÏôÑÎ£å: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def unload(self):
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        self.loaded = False
        gc.collect()
    
    def get_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0
        }

# ==============================================
# üî• 4. StepModelInterface Ï†ïÏùò (Ïò§Î•ò Ìï¥Í≤∞)
# ==============================================

class StepModelInterface:
    """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ - BaseStepMixin Ìò∏Ìôò"""
    
    def __init__(self, model_loader, step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # StepÎ≥Ñ Î™®Îç∏Îì§ (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå)
        self.step_models: Dict[str, BaseModel] = {}
        self.primary_model: Optional[BaseModel] = None
        
        # ÏöîÍµ¨ÏÇ¨Ìï≠
        self.requirements: Optional[StepModelRequirement] = None
        
        # ÏÉùÏÑ± ÏãúÍ∞Ñ Î∞è ÌÜµÍ≥Ñ
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            self.requirements = StepModelRequirement(
                step_name=self.step_name,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                model_configs=requirements.get('model_configs', {})
            )
            self.logger.info(f"‚úÖ Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {len(self.requirements.required_models)}Í∞ú ÌïÑÏàò Î™®Îç∏")
        except Exception as e:
            self.logger.error(f"‚ùå ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå)"""
        try:
            if not model_name or model_name == "default":
                if self.primary_model:
                    return self.primary_model
                elif self.step_models:
                    return next(iter(self.step_models.values()))
                else:
                    return self._load_default_model()
            
            # ÌäπÏ†ï Î™®Îç∏ ÏöîÏ≤≠
            if model_name in self.step_models:
                return self.step_models[model_name]
            
            # ÏÉà Î™®Îç∏ Î°úÎî©
            model = self.model_loader.load_model(model_name, step_name=self.step_name)
            
            if model:
                self.step_models[model_name] = model
                if not self.primary_model:
                    self.primary_model = model
                    
            return model
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """ÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ - BaseStepMixin Ìò∏Ìôò"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[BaseModel]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            return None
    
    def _load_default_model(self) -> Optional[BaseModel]:
        """Í∏∞Î≥∏ Î™®Îç∏ Î°úÎî©"""
        try:
            if self.step_name in self.model_loader.default_mappings:
                mapping = self.model_loader.default_mappings[self.step_name]
                
                # Î°úÏª¨ Î™®Îç∏ Ïö∞ÏÑ† ÏãúÎèÑ
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_loader.model_cache_dir / local_path
                    if full_path.exists():
                        model = BaseModel(
                            model_name=local_path,
                            model_path=str(full_path),
                            device=self.model_loader.device
                        )
                        if model.load():
                            self.primary_model = model
                            self.step_models['default'] = model
                            return model
            
            self.logger.warning(f"‚ö†Ô∏è {self.step_name}Ïóê ÎåÄÌïú Í∏∞Î≥∏ Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Í∏∞Î≥∏ Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù - BaseStepMixin Ìò∏Ìôò"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'required': kwargs.get('required', True),
                'device': kwargs.get('device', self.model_loader.device),
                **kwargs
            }
            
            self.logger.info(f"‚úÖ Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def get_step_status(self) -> Dict[str, Any]:
        """Step ÏÉÅÌÉú Ï°∞Ìöå"""
        return {
            "step_name": self.step_name,
            "creation_time": self.creation_time,
            "models_loaded": len(self.step_models),
            "primary_model": self.primary_model.model_name if self.primary_model else None,
            "access_count": self.access_count,
            "error_count": self.error_count,
            "available_models": list(self.step_models.keys()),
            "requirements": {
                "required_models": self.requirements.required_models if self.requirements else [],
                "optional_models": self.requirements.optional_models if self.requirements else []
            }
        }

# ==============================================
# üî• 5. Î©îÏù∏ ModelLoader ÌÅ¥ÎûòÏä§ v3.0
# ==============================================

class ModelLoader:
    """
    üî• ModelLoader v3.0 - ÏïàÏ†ïÏ†ÅÏù∏ ÌïµÏã¨ Í∏∞Îä•Îßå (AI Ï∂îÎ°† Ï†úÍ±∞)
    
    ÌäπÏßï:
    - AI Ï∂îÎ°† Î°úÏßÅ ÏôÑÏ†Ñ Ï†úÍ±∞
    - Î™®Îç∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨Îßå ÏàòÌñâ
    - BaseStepMixin 100% Ìò∏Ìôò
    - StepModelInterface Ï†ïÏÉÅ Ï†úÍ≥µ
    - auto_model_detector Ïó∞Îèô Ïú†ÏßÄ
    """
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):
        """ModelLoader Ï¥àÍ∏∞Ìôî"""
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # Î™®Îç∏ Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # ÏûêÎèô Í∞êÏßÄ: backend/ai_models
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Î™®Îç∏ Í¥ÄÎ¶¨ (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå)
        self.loaded_models: Dict[str, BaseModel] = {}
        self.model_info: Dict[str, ModelInfo] = {}
        self.model_status: Dict[str, ModelStatus] = {}
        
        # Step ÏöîÍµ¨ÏÇ¨Ìï≠
        self.step_requirements: Dict[str, StepModelRequirement] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # auto_model_detector Ïó∞Îèô
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0
        }
        
        # ÎèôÍ∏∞Ìôî
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î°úÍπÖ
        self.logger.info(f"üöÄ ModelLoader v3.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"üì± Device: {self.device} (M3 Max: {IS_M3_MAX})")
        self.logger.info(f"üìÅ Î™®Îç∏ Ï∫êÏãú: {self.model_cache_dir}")
        
        # Í∏∞Î≥∏ Î™®Îç∏ Îß§Ìïë Î°úÎî©
        self._load_model_mappings()
    
    def _initialize_auto_detector(self):
        """auto_model_detector Ï¥àÍ∏∞Ìôî"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("‚úÖ auto_model_detector Ïó∞Îèô ÏôÑÎ£å")
                    # ÏûêÎèô ÌÜµÌï© ÏãúÎèÑ
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("‚ö†Ô∏è auto_detector Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä None")
            else:
                self.logger.warning("‚ö†Ô∏è AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"‚ùå auto_model_detector Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector ÌÜµÌï©"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            # Í∞ÑÎã®Ìïú Î™®Îç∏ ÌÉêÏßÄ Î∞è ÌÜµÌï©
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            # Í∏∞Î≥∏ Ï†ïÎ≥¥Îßå Ï∂îÏ∂ú
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "auto_detected": True
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"‚úÖ AutoDetector ÌÜµÌï© ÏôÑÎ£å: {integrated_count}Í∞ú Î™®Îç∏")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå AutoDetector ÌÜµÌï© Ïã§Ìå®: {e}")
            return False
    
    @property
    def available_models(self) -> Dict[str, Any]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§"""
        return self._available_models_cache
    
    def _load_model_mappings(self):
        """Í∏∞Î≥∏ Î™®Îç∏ Îß§Ìïë Î°úÎî©"""
        try:
            # StepÎ≥Ñ Í∏∞Î≥∏ Î™®Îç∏ Îß§Ìïë
            self.default_mappings = {
                'HumanParsingStep': {
                    'model_type': 'segmentation',
                    'local_paths': [
                        'step_01_human_parsing/graphonomy.pth',
                        'step_01_human_parsing/atr_model.pth'
                    ]
                },
                'PoseEstimationStep': {
                    'model_type': 'pose',
                    'local_paths': [
                        'step_02_pose_estimation/yolov8n-pose.pt',
                        'step_02_pose_estimation/openpose_pose_coco.pth'
                    ]
                },
                'ClothSegmentationStep': {
                    'model_type': 'segmentation',
                    'local_paths': [
                        'step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'step_03_cloth_segmentation/u2net.pth'
                    ]
                },
                'GeometricMatchingStep': {
                    'model_type': 'matching',
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth',
                        'step_04_geometric_matching/tps_model.pth'
                    ]
                },
                'ClothWarpingStep': {
                    'model_type': 'warping',
                    'local_paths': [
                        'step_05_cloth_warping/RealVisXL_V4.0.safetensors',
                        'step_05_cloth_warping/vgg19_warping.pth'
                    ]
                },
                'VirtualFittingStep': {
                    'model_type': 'diffusion',
                    'local_paths': [
                        'step_06_virtual_fitting/diffusion_pytorch_model.safetensors',
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.bin'
                    ]
                },
                'PostProcessingStep': {
                    'model_type': 'enhancement',
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth',
                        'step_07_post_processing/sr_model.pth'
                    ]
                },
                'QualityAssessmentStep': {
                    'model_type': 'quality',
                    'local_paths': [
                        'step_08_quality_assessment/ViT-L-14.pt',
                        'step_08_quality_assessment/open_clip_pytorch_model.bin'
                    ]
                }
            }
            
            self.logger.info(f"‚úÖ Í∏∞Î≥∏ Î™®Îç∏ Îß§Ìïë Î°úÎî© ÏôÑÎ£å: {len(self.default_mappings)}Í∞ú Step")
            
        except Exception as e:
            self.logger.error(f"‚ùå Í∏∞Î≥∏ Î™®Îç∏ Îß§Ìïë Î°úÎî© Ïã§Ìå®: {e}")
            self.default_mappings = {}
    
    # ==============================================
    # üî• ÌïµÏã¨ Î™®Îç∏ Î°úÎî© Î©îÏÑúÎìúÎì§
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """Î™®Îç∏ Î°úÎî© (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå)"""
        try:
            with self._lock:
                # Ï∫êÏãú ÌôïÏù∏
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        self.logger.debug(f"‚ôªÔ∏è Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_name}")
                        return model
                
                # ÏÉà Î™®Îç∏ Î°úÎî©
                self.model_status[model_name] = ModelStatus.LOADING
                
                # Î™®Îç∏ Í≤ΩÎ°ú Ï∞æÍ∏∞
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°úÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {model_name}")
                    self.model_status[model_name] = ModelStatus.ERROR
                    return None
                
                # BaseModel ÏÉùÏÑ± Î∞è Î°úÎî©
                model = BaseModel(
                    model_name=model_name,
                    model_path=model_path,
                    device=self.device
                )
                
                if model.load():
                    # Ï∫êÏãúÏóê Ï†ÄÏû•
                    self.loaded_models[model_name] = model
                    self.model_info[model_name] = ModelInfo(
                        name=model_name,
                        path=model_path,
                        model_type=ModelType(kwargs.get('model_type', 'classification')),
                        device=self.device,
                        loaded=True,
                        load_time=model.load_time,
                        memory_mb=model.memory_usage_mb,
                        access_count=1,
                        last_access=time.time()
                    )
                    self.model_status[model_name] = ModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ: {model_name} ({model.memory_usage_mb:.1f}MB)")
                    
                    # Ï∫êÏãú ÌÅ¨Í∏∞ Í¥ÄÎ¶¨
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = ModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            self.model_status[model_name] = ModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎî©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name
            )
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """Î™®Îç∏ Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        try:
            # ÏßÅÏ†ë Í≤ΩÎ°ú ÏßÄÏ†ï
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # available_modelsÏóêÏÑú Ï∞æÍ∏∞
            if model_name in self.available_models:
                model_info = self.available_models[model_name]
                path = Path(model_info.get('path', ''))
                if path.exists():
                    return str(path)
            
            # Î°úÏª¨ Ï∫êÏãúÏóêÏÑú Ï∞æÍ∏∞
            possible_paths = [
                self.model_cache_dir / f"{model_name}",
                self.model_cache_dir / f"{model_name}.pth",
                self.model_cache_dir / f"{model_name}.pt",
                self.model_cache_dir / f"{model_name}.safetensors"
            ]
            
            # Step Í∏∞Î∞ò Îß§ÌïëÏóêÏÑú Ï∞æÍ∏∞
            step_name = kwargs.get('step_name')
            if step_name and step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        possible_paths.insert(0, full_path)
            
            # ÎîîÎ†âÌÜ†Î¶¨ Í≤ÄÏÉâ
            for pattern in [f"**/{model_name}.*", f"**/*{model_name}*"]:
                for found_path in self.model_cache_dir.glob(pattern):
                    if found_path.is_file():
                        possible_paths.append(found_path)
            
            # Ï≤´ Î≤àÏß∏ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÎ°ú Î∞òÌôò
            for path in possible_paths:
                if Path(path).exists():
                    return str(path)
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°ú Ï∞æÍ∏∞ Ïã§Ìå® {model_name}: {e}")
            return None
    
    def _manage_cache(self):
        """Ï∫êÏãú ÌÅ¨Í∏∞ Í¥ÄÎ¶¨"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # Í∞ÄÏû• Ïò§Îûò ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÏùÄ Î™®Îç∏ Ï†úÍ±∞
            models_by_access = sorted(
                self.model_info.items(),
                key=lambda x: x[1].last_access
            )
            
            models_to_remove = models_by_access[:len(self.loaded_models) - self.max_cached_models]
            
            for model_name, _ in models_to_remove:
                self.unload_model(model_name)
                
        except Exception as e:
            self.logger.error(f"‚ùå Ï∫êÏãú Í¥ÄÎ¶¨ Ïã§Ìå®: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = ModelStatus.NOT_LOADED
                    
                    self.logger.info(f"‚úÖ Î™®Îç∏ Ïñ∏Î°úÎìú ÏôÑÎ£å: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return False
    
    # ==============================================
    # üî• Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏßÄÏõê
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            interface = StepModelInterface(self, step_name)
            
            if step_requirements:
                interface.register_requirements(step_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"‚úÖ Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±: {step_name}")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
            return StepModelInterface(self, step_name)
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            self.step_requirements[step_name] = StepModelRequirement(
                step_name=step_name,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                model_configs=requirements.get('model_configs', {})
            )
            
            self.logger.info(f"‚úÖ Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå® {step_name}: {e}")
            return False
    
    # ==============================================
    # üî• BaseStepMixin Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """Ï¥àÍ∏∞Ìôî ÏÉÅÌÉú ÌôïÏù∏"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """Ï¥àÍ∏∞Ìôî"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("‚úÖ ModelLoader Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî"""
        return self.initialize(**kwargs)
    
    # ==============================================
    # üî• ÎàÑÎùΩÎêú ÌïµÏã¨ Î©îÏÑúÎìúÎì§ Ï∂îÍ∞Ä (Step ÌååÏùºÏóêÏÑú ÏöîÏ≤≠)
    # ==============================================
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù - BaseStepMixin Ìò∏Ìôò"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'required': kwargs.get('required', True),
                    'device': kwargs.get('device', self.device),
                    'priority': kwargs.get('priority', 1.0),
                    **kwargs
                }
                
                self.logger.info(f"‚úÖ Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """Î™®Îç∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"""
        try:
            # Î™®Îç∏ Ï†ïÎ≥¥ ÌôïÏù∏
            if model_name not in self.model_info and model_name not in self.available_models:
                return False
            
            # Step ÏöîÍµ¨ÏÇ¨Ìï≠ ÌôïÏù∏
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # Í∏∞Î≥∏ Îß§Ìïë ÌôïÏù∏
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path:
                        return True
            
            return True  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ìò∏Ìôò Í∞ÄÎä•ÏúºÎ°ú Ï≤òÎ¶¨
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """Î™®Îç∏ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏"""
        return (model_name in self.loaded_models or 
                model_name in self.available_models or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """Î™®Îç∏ Î°úÎî© ÏÉÅÌÉú ÌôïÏù∏"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
        return self.create_step_interface(step_name)
    
    def register_step_model_dependencies(self, step_name: str, dependencies: Dict[str, Any]) -> bool:
        """Step Î™®Îç∏ ÏùòÏ°¥ÏÑ± Îì±Î°ù"""
        try:
            for model_name, model_config in dependencies.items():
                self.register_model_requirement(
                    model_name=model_name,
                    **model_config
                )
            
            self.logger.info(f"‚úÖ Step ÏùòÏ°¥ÏÑ± Îì±Î°ù: {step_name} ({len(dependencies)}Í∞ú Î™®Îç∏)")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏùòÏ°¥ÏÑ± Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def validate_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """Step ÏöîÍµ¨ÏÇ¨Ìï≠ Í≤ÄÏ¶ù"""
        try:
            validation_result = {
                'step_name': step_name,
                'valid': True,
                'missing_models': [],
                'incompatible_models': [],
                'available_models': [],
                'errors': []
            }
            
            if step_name not in self.step_requirements:
                validation_result['valid'] = False
                validation_result['errors'].append(f"Step ÏöîÍµ¨ÏÇ¨Ìï≠Ïù¥ Îì±Î°ùÎêòÏßÄ ÏïäÏùå: {step_name}")
                return validation_result
            
            step_req = self.step_requirements[step_name]
            
            # ÌïÑÏàò Î™®Îç∏ ÌôïÏù∏
            for model_name in step_req.required_models:
                if not self.has_model(model_name):
                    validation_result['missing_models'].append(model_name)
                    validation_result['valid'] = False
                elif not self.validate_model_compatibility(model_name, step_name):
                    validation_result['incompatible_models'].append(model_name)
                    validation_result['valid'] = False
                else:
                    validation_result['available_models'].append(model_name)
            
            return validation_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏöîÍµ¨ÏÇ¨Ìï≠ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return {'step_name': step_name, 'valid': False, 'error': str(e)}
    
    def get_step_model_status(self, step_name: str) -> Dict[str, Any]:
        """Step Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            if step_name in self.step_interfaces:
                interface = self.step_interfaces[step_name]
                return interface.get_step_status()
            else:
                return {
                    'step_name': step_name,
                    'interface_exists': False,
                    'requirements': self.step_requirements.get(step_name)
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Step Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'step_name': step_name, 'error': str(e)}
    
    def list_loaded_models(self) -> List[str]:
        """Î°úÎìúÎêú Î™®Îç∏ Î™©Î°ù"""
        return list(self.loaded_models.keys())
    
    def get_models_by_step(self, step_name: str) -> List[str]:
        """StepÎ≥Ñ Î™®Îç∏ Î™©Î°ù"""
        try:
            models = []
            
            # Step ÏöîÍµ¨ÏÇ¨Ìï≠ÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                models.extend(step_req.required_models)
                models.extend(step_req.optional_models)
            
            # Í∏∞Î≥∏ Îß§ÌïëÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    model_name = Path(local_path).stem
                    if model_name not in models:
                        models.append(model_name)
            
            return list(set(models))
            
        except Exception as e:
            self.logger.error(f"‚ùå StepÎ≥Ñ Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def get_models_by_type(self, model_type: str) -> List[str]:
        """Î™®Îç∏ ÌÉÄÏûÖÎ≥Ñ Î™©Î°ù"""
        try:
            models = []
            
            for model_name, model_info in self.model_info.items():
                if model_info.model_type.value == model_type:
                    models.append(model_name)
            
            # available_modelsÏóêÏÑúÎèÑ ÌôïÏù∏
            for model_name, model_info in self.available_models.items():
                if model_info.get('model_type') == model_type and model_name not in models:
                    models.append(model_name)
            
            return models
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÌÉÄÏûÖÎ≥Ñ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def load_model_sync(self, model_name: str, **kwargs) -> Optional[BaseModel]:
        """ÎèôÍ∏∞ Î™®Îç∏ Î°úÎî©"""
        return self.load_model(model_name, **kwargs)
    
    async def unload_model_async(self, model_name: str) -> bool:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self._executor, self.unload_model, model_name)
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå®: {e}")
            return False
    
    def preload_models(self, model_names: List[str]) -> Dict[str, bool]:
        """Î™®Îç∏ ÏùºÍ¥Ñ ÏÇ¨Ï†Ñ Î°úÎî©"""
        try:
            results = {}
            
            for model_name in model_names:
                try:
                    model = self.load_model(model_name)
                    results[model_name] = model is not None and model.loaded
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏÇ¨Ï†Ñ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
                    results[model_name] = False
            
            success_count = sum(results.values())
            self.logger.info(f"‚úÖ Î™®Îç∏ ÏÇ¨Ï†Ñ Î°úÎî© ÏôÑÎ£å: {success_count}/{len(model_names)}Í∞ú ÏÑ±Í≥µ")
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏùºÍ¥Ñ ÏÇ¨Ï†Ñ Î°úÎî© Ïã§Ìå®: {e}")
            return {name: False for name in model_names}
    
    def verify_model_integrity(self, model_name: str) -> bool:
        """Î™®Îç∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù"""
        try:
            if model_name not in self.model_info and model_name not in self.available_models:
                return False
            
            # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
            if model_name in self.model_info:
                model_path = Path(self.model_info[model_name].path)
            else:
                model_path = Path(self.available_models[model_name].get('path', ''))
            
            if not model_path.exists():
                return False
            
            # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏ (0Î∞îÏù¥Ìä∏Í∞Ä ÏïÑÎãò)
            if model_path.stat().st_size == 0:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False
    
    def check_model_dependencies(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ ÏùòÏ°¥ÏÑ± ÌôïÏù∏"""
        try:
            dependencies = {
                'model_name': model_name,
                'dependencies_met': True,
                'missing_dependencies': [],
                'hardware_requirements': []
            }
            
            # Í∏∞Î≥∏ ÏùòÏ°¥ÏÑ± ÌôïÏù∏
            if not self.has_model(model_name):
                dependencies['missing_dependencies'].append('model_file')
                dependencies['dependencies_met'] = False
            
            # ÌïòÎìúÏõ®Ïñ¥ ÏöîÍµ¨ÏÇ¨Ìï≠
            if self.device == 'mps' and not IS_M3_MAX:
                dependencies['hardware_requirements'].append('Apple Silicon required for MPS')
            
            return dependencies
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏùòÏ°¥ÏÑ± ÌôïÏù∏ Ïã§Ìå®: {e}")
            return {'model_name': model_name, 'dependencies_met': False, 'error': str(e)}
    
    def validate_hardware_compatibility(self, model_name: str) -> bool:
        """ÌïòÎìúÏõ®Ïñ¥ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"""
        try:
            # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ìò∏Ìôò Í∞ÄÎä•
            if self.device == 'cpu':
                return True
            
            # MPS Ìò∏ÌôòÏÑ±
            if self.device == 'mps':
                return IS_M3_MAX
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ÌïòÎìúÏõ®Ïñ¥ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False
    
    def get_model_performance_stats(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÑ±Îä• ÌÜµÍ≥Ñ"""
        try:
            if model_name not in self.model_info:
                return {'model_name': model_name, 'available': False}
            
            model_info = self.model_info[model_name]
            
            return {
                'model_name': model_name,
                'available': True,
                'load_time': model_info.load_time,
                'memory_usage_mb': model_info.memory_mb,
                'access_count': model_info.access_count,
                'last_access': model_info.last_access,
                'efficiency_score': model_info.access_count / max(1, model_info.memory_mb / 100)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏÑ±Îä• ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'model_name': model_name, 'error': str(e)}
    
    def estimate_model_memory_usage(self, model_name: str) -> float:
        """Î™®Îç∏ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï∂îÏ†ï"""
        try:
            if model_name in self.model_info:
                return self.model_info[model_name].memory_mb
            
            if model_name in self.available_models:
                return self.available_models[model_name].get('size_mb', 0)
            
            # ÌååÏùº ÌÅ¨Í∏∞ Í∏∞Î∞ò Ï∂îÏ†ï
            for step_name, mapping in self.default_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path:
                        full_path = self.model_cache_dir / local_path
                        if full_path.exists():
                            file_size_mb = full_path.stat().st_size / (1024 * 1024)
                            return file_size_mb * 1.2  # Î°úÎî© Ïãú ÏïΩÍ∞ÑÏùò Ïò§Î≤ÑÌó§Îìú
            
            return 0.0
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï∂îÏ†ï Ïã§Ìå®: {e}")
            return 0.0
    
    def get_inference_history(self, model_name: str) -> List[Dict[str, Any]]:
        """Ï∂îÎ°† Ïù¥Î†• Ï°∞Ìöå"""
        try:
            # Í∞ÑÎã®Ìïú ÌÜµÍ≥ÑÎßå Î∞òÌôò
            if model_name in self.model_info:
                model_info = self.model_info[model_name]
                return [{
                    'model_name': model_name,
                    'total_accesses': model_info.access_count,
                    'last_access': model_info.last_access,
                    'load_time': model_info.load_time
                }]
            
            return []
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï∂îÎ°† Ïù¥Î†• Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def inject_dependencies(self, step_instance) -> bool:
        """Step Ïù∏Ïä§ÌÑ¥Ïä§Ïóê ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        try:
            # ModelLoader Ï£ºÏûÖ
            if hasattr(step_instance, 'set_model_loader'):
                step_instance.set_model_loader(self)
            
            # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï£ºÏûÖ
            step_name = getattr(step_instance, 'step_name', step_instance.__class__.__name__)
            if hasattr(step_instance, 'set_model_interface'):
                interface = self.create_step_interface(step_name)
                step_instance.set_model_interface(interface)
            
            self.logger.info(f"‚úÖ ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
            return False
    
    def setup_step_environment(self, step_name: str) -> Dict[str, Any]:
        """Step ÌôòÍ≤Ω ÏÑ§Ï†ï"""
        try:
            environment = {
                'step_name': step_name,
                'device': self.device,
                'model_cache_dir': str(self.model_cache_dir),
                'available_models': self.get_models_by_step(step_name),
                'hardware_info': {
                    'is_m3_max': IS_M3_MAX,
                    'default_device': DEFAULT_DEVICE,
                    'conda_env': CONDA_ENV
                }
            }
            
            # StepÎ≥Ñ Í∏∞Î≥∏ ÏÑ§Ï†ï Ï†ÅÏö©
            if step_name in self.default_mappings:
                mapping = self.default_mappings[step_name]
                environment['model_type'] = mapping.get('model_type', 'unknown')
                environment['local_models'] = mapping.get('local_paths', [])
            
            return environment
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÌôòÍ≤Ω ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            return {'step_name': step_name, 'error': str(e)}
    
    def configure_step_models(self, step_name: str, config: Dict[str, Any]) -> bool:
        """Step Î™®Îç∏ ÏÑ§Ï†ï"""
        try:
            # Step ÏöîÍµ¨ÏÇ¨Ìï≠ ÏóÖÎç∞Ïù¥Ìä∏
            if 'required_models' in config:
                for model_name in config['required_models']:
                    self.register_model_requirement(
                        model_name=model_name,
                        model_type=config.get('model_type', 'BaseModel'),
                        required=True
                    )
            
            if 'optional_models' in config:
                for model_name in config['optional_models']:
                    self.register_model_requirement(
                        model_name=model_name,
                        model_type=config.get('model_type', 'BaseModel'),
                        required=False
                    )
            
            self.logger.info(f"‚úÖ Step Î™®Îç∏ ÏÑ§Ï†ï ÏôÑÎ£å: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Step Î™®Îç∏ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            return False
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    "name": info.name,
                    "status": "loaded" if info.loaded else "not_loaded",
                    "device": info.device,
                    "memory_mb": info.memory_mb,
                    "load_time": info.load_time,
                    "access_count": info.access_count,
                    "last_access": info.last_access
                }
            else:
                status = self.model_status.get(model_name, ModelStatus.NOT_LOADED)
                return {"name": model_name, "status": status.value}
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå® {model_name}: {e}")
            return {"name": model_name, "status": "error", "error": str(e)}
    
    # Ï∂îÍ∞Ä Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìúÎì§
    def clear_cache(self, force: bool = False) -> bool:
        """Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨"""
        try:
            if force:
                # Î™®Îì† Î™®Îç∏ Ïñ∏Î°úÎìú
                for model_name in list(self.loaded_models.keys()):
                    self.unload_model(model_name)
            else:
                # Ïò§ÎûòÎêú Î™®Îç∏Îì§Îßå Ï†ïÎ¶¨
                self._manage_cache()
            
            gc.collect()
            self.logger.info("‚úÖ Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï∫êÏãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return False
    
    def optimize_memory(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÏµúÏ†ÅÌôî"""
        try:
            initial_memory = sum(info.memory_mb for info in self.model_info.values())
            
            # Ïò§ÎûòÎêú Î™®Îç∏Îì§ Ïñ∏Î°úÎìú (1ÏãúÍ∞Ñ Ïù¥ÏÉÅ ÎØ∏ÏÇ¨Ïö©)
            current_time = time.time()
            models_to_unload = []
            
            for model_name, info in self.model_info.items():
                if current_time - info.last_access > 3600:  # 1ÏãúÍ∞Ñ
                    models_to_unload.append(model_name)
            
            unloaded_count = 0
            for model_name in models_to_unload:
                if self.unload_model(model_name):
                    unloaded_count += 1
            
            gc.collect()
            
            final_memory = sum(info.memory_mb for info in self.model_info.values())
            freed_memory = initial_memory - final_memory
            
            result = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "freed_memory_mb": freed_memory,
                "unloaded_models": unloaded_count,
                "optimization_successful": freed_memory > 0
            }
            
            self.logger.info(f"‚úÖ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏôÑÎ£å: {freed_memory:.1f}MB Ìï¥Ï†ú")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return {"error": str(e), "optimization_successful": False}
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """ÌòÑÏû¨ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï°∞Ìöå"""
        try:
            total_memory = sum(info.memory_mb for info in self.model_info.values())
            loaded_count = len(self.loaded_models)
            
            return {
                "total_memory_mb": total_memory,
                "loaded_models_count": loaded_count,
                "average_per_model_mb": total_memory / loaded_count if loaded_count > 0 else 0,
                "device": self.device,
                "cache_size": len(self.model_info)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {"error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ModelLoader ÏÉÅÌÉú Ï†ïÎ≥¥ Î∞òÌôò"""
        try:
            return {
                "initialized": self.is_initialized,
                "device": self.device,
                "loaded_models_count": len(self.loaded_models),
                "total_memory_mb": sum(info.memory_mb for info in self.model_info.values()),
                "auto_detector_integration": self._integration_successful,
                "available_models_count": len(self.available_models),
                "step_interfaces_count": len(self.step_interfaces)
            }
        except Exception as e:
            self.logger.error(f"‚ùå ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {"error": str(e)}
    
    def health_check(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú ÏÉÅÌÉú ÏßÑÎã®"""
        try:
            health_status = {
                "status": "healthy",
                "timestamp": time.time(),
                "system_info": {
                    "device": self.device,
                    "is_m3_max": IS_M3_MAX,
                    "conda_env": CONDA_ENV
                },
                "models": {
                    "loaded_count": len(self.loaded_models),
                    "available_count": len(self.available_models),
                    "total_memory_mb": sum(info.memory_mb for info in self.model_info.values())
                },
                "issues": []
            }
            
            # Î¨∏Ï†ú ÌôïÏù∏
            if len(self.available_models) == 0:
                health_status["issues"].append("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Ïù¥ ÏóÜÏùå")
                health_status["status"] = "warning"
            
            if not self._integration_successful and AUTO_DETECTOR_AVAILABLE:
                health_status["issues"].append("AutoDetector ÌÜµÌï© Ïã§Ìå®")
                health_status["status"] = "warning"
            
            total_memory = health_status["models"]["total_memory_mb"]
            if total_memory > 10000:  # 10GB Ïù¥ÏÉÅ
                health_status["issues"].append(f"ÎÜíÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {total_memory:.1f}MB")
                health_status["status"] = "warning"
            
            if health_status["issues"]:
                self.logger.warning(f"‚ö†Ô∏è ModelLoader Í±¥Í∞ïÏÉÅÌÉú Í≤ΩÍ≥†: {len(health_status['issues'])}Í∞ú Î¨∏Ï†ú")
            else:
                self.logger.info("‚úÖ ModelLoader Í±¥Í∞ïÏÉÅÌÉú ÏñëÌò∏")
            
            return health_status
            
        except Exception as e:
            self.logger.error(f"‚ùå Í±¥Í∞ïÏÉÅÌÉú Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
            return {"status": "error", "error": str(e)}
    
    def detect_available_models(self) -> Dict[str, Any]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÏûêÎèô Í∞êÏßÄ"""
        try:
            detected = {}
            
            # AutoDetector ÏÇ¨Ïö©
            if self.auto_detector and self._integration_successful:
                detected.update(self.available_models)
            
            # Í∏∞Î≥∏ Îß§ÌïëÏóêÏÑú Í∞êÏßÄ
            for step_name, mapping in self.default_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        model_name = full_path.stem
                        detected[model_name] = {
                            'name': model_name,
                            'path': str(full_path),
                            'size_mb': full_path.stat().st_size / (1024 * 1024),
                            'step_class': step_name,
                            'model_type': mapping.get('model_type', 'unknown'),
                            'detected_by': 'default_mapping'
                        }
            
            return detected
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í∞êÏßÄ Ïã§Ìå®: {e}")
            return {}
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
        try:
            models = []
            
            # available_modelsÏóêÏÑú Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
            for model_name, model_info in self.available_models.items():
                # ÌïÑÌÑ∞ÎßÅ
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # Î°úÎî© ÏÉÅÌÉú Ï∂îÍ∞Ä
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                models.append(model_info_copy)
            
            # Í∏∞Î≥∏ Îß§ÌïëÏóêÏÑú Ï∂îÍ∞Ä
            for step_name, mapping in self.default_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        model_name = full_path.stem
                        if model_name not in [m['name'] for m in models]:
                            models.append({
                                'name': model_name,
                                'path': str(full_path),
                                'type': mapping.get('model_type', 'unknown'),
                                'loaded': model_name in self.loaded_models,
                                'step_class': step_name,
                                'size_mb': full_path.stat().st_size / (1024 * 1024)
                            })
            
            return models
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'model_type': info.model_type.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'error': info.error
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self.available_models)
        }
    
    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            self.logger.info("üßπ ModelLoader Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ï§ë...")
            
            # Î™®Îì† Î™®Îç∏ Ïñ∏Î°úÎìú
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            
            # Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å
            self._executor.shutdown(wait=True)
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            gc.collect()
            
            self.logger.info("‚úÖ ModelLoader Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• 6. Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞è Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§
# ==============================================

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            try:
                # ÏÑ§Ï†ï Ï†ÅÏö©
                loader_config = config or {}
                
                _global_model_loader = ModelLoader(
                    device=loader_config.get('device', 'auto'),
                    max_cached_models=loader_config.get('max_cached_models', 10),
                    enable_optimization=loader_config.get('enable_optimization', True),
                    **loader_config
                )
                
                logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader v3.0 ÏÉùÏÑ± ÏÑ±Í≥µ")
                
            except Exception as e:
                logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÏÉùÏÑ± Ïã§Ìå®: {e}")
                # Í∏∞Î≥∏ ÏÑ§Ï†ïÏúºÎ°ú Ìè¥Î∞±
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        else:
            logger.warning("‚ö†Ô∏è Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî ÏùºÎ∂Ä Ïã§Ìå®")
            
        return loader
        
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
        return StepModelInterface(get_global_model_loader(), step_name)

def get_model(model_name: str) -> Optional[BaseModel]:
    """Ï†ÑÏó≠ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[BaseModel]:
    """Ï†ÑÏó≠ ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# ==============================================
# üî• 7. Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
# ==============================================

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> Dict[str, Any]:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº Í≤ÄÏ¶ù"""
    try:
        path = Path(checkpoint_path)
        
        validation = {
            "path": str(path),
            "exists": path.exists(),
            "is_file": path.is_file() if path.exists() else False,
            "size_mb": 0,
            "readable": False,
            "valid_extension": False,
            "is_valid": False,
            "errors": []
        }
        
        if not path.exists():
            validation["errors"].append("ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
            return validation
        
        if not path.is_file():
            validation["errors"].append("ÌååÏùºÏù¥ ÏïÑÎãò")
            return validation
        
        # ÌÅ¨Í∏∞ ÌôïÏù∏
        try:
            size_bytes = path.stat().st_size
            validation["size_mb"] = size_bytes / (1024 * 1024)
        except Exception as e:
            validation["errors"].append(f"ÌÅ¨Í∏∞ ÌôïÏù∏ Ïã§Ìå®: {e}")
        
        # ÏùΩÍ∏∞ Í∂åÌïú ÌôïÏù∏
        try:
            validation["readable"] = os.access(path, os.R_OK)
            if not validation["readable"]:
                validation["errors"].append("ÏùΩÍ∏∞ Í∂åÌïú ÏóÜÏùå")
        except Exception as e:
            validation["errors"].append(f"Í∂åÌïú ÌôïÏù∏ Ïã§Ìå®: {e}")
        
        # ÌôïÏû•Ïûê ÌôïÏù∏
        valid_extensions = ['.pth', '.pt', '.ckpt', '.safetensors', '.bin']
        validation["valid_extension"] = path.suffix.lower() in valid_extensions
        if not validation["valid_extension"]:
            validation["errors"].append(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌôïÏû•Ïûê: {path.suffix}")
        
        # Ï†ÑÏ≤¥ Ïú†Ìö®ÏÑ± ÌåêÎã®
        validation["is_valid"] = (
            validation["exists"] and 
            validation["is_file"] and 
            validation["readable"] and 
            validation["valid_extension"] and
            validation["size_mb"] > 0 and
            len(validation["errors"]) == 0
        )
        
        return validation
        
    except Exception as e:
        return {
            "path": str(checkpoint_path),
            "exists": False,
            "is_valid": False,
            "errors": [f"Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {e}"]
        }

def get_system_capabilities() -> Dict[str, Any]:
    """ÏãúÏä§ÌÖú Îä•Î†• Ï°∞Ìöå"""
    return {
        "numpy_available": NUMPY_AVAILABLE,
        "pil_available": PIL_AVAILABLE,
        "auto_detector_available": AUTO_DETECTOR_AVAILABLE,
        "default_device": DEFAULT_DEVICE,
        "is_m3_max": IS_M3_MAX,
        "conda_env": CONDA_ENV,
        "python_version": sys.version
    }

def emergency_cleanup() -> bool:
    """ÎπÑÏÉÅ Ï†ïÎ¶¨ Ìï®Ïàò"""
    try:
        logger.warning("üö® ÎπÑÏÉÅ Ï†ïÎ¶¨ ÏãúÏûë...")
        
        # Ï†ÑÏó≠ ModelLoader Ï†ïÎ¶¨
        global _global_model_loader
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        
        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        gc.collect()
        
        logger.info("‚úÖ ÎπÑÏÉÅ Ï†ïÎ¶¨ ÏôÑÎ£å")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå ÎπÑÏÉÅ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        return False

# ==============================================
# üî• 8. Export Î∞è Ï¥àÍ∏∞Ìôî
# ==============================================

__all__ = [
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'ModelLoader',
    'StepModelInterface',
    'BaseModel',
    
    # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îì§
    'ModelType',
    'ModelStatus',
    'ModelInfo',
    'StepModelRequirement',
    
    # Ï†ÑÏó≠ Ìï®ÏàòÎì§
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'validate_checkpoint_file',
    'get_system_capabilities',
    'emergency_cleanup',
    
    # ÏÉÅÏàòÎì§
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ==============================================
# üî• 9. Î™®Îìà Ï¥àÍ∏∞Ìôî Î∞è ÌÖåÏä§Ìä∏
# ==============================================

logger.info("=" * 80)
logger.info("üöÄ ÏïàÏ†ïÏ†ÅÏù∏ ModelLoader v3.0 Î°úÎìú ÏôÑÎ£å (AI Ï∂îÎ°† Ï†úÍ±∞)")
logger.info("=" * 80)
logger.info("‚úÖ AI Ï∂îÎ°† Î°úÏßÅ ÏôÑÏ†Ñ Ï†úÍ±∞ - ÏïàÏ†ïÏÑ± Ïö∞ÏÑ†")
logger.info("‚úÖ ÌïµÏã¨ Î™®Îç∏ Î°úÎçî Í∏∞Îä•Îßå Ïú†ÏßÄ")
logger.info("‚úÖ BaseStepMixin 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•")
logger.info("‚úÖ StepModelInterface Ï†ïÏùò Î¨∏Ï†ú Ìï¥Í≤∞")
logger.info("‚úÖ auto_model_detector Ïó∞Îèô Ïú†ÏßÄ")
logger.info("‚úÖ Í∏∞Ï°¥ Ìï®ÏàòÎ™Ö/Î©îÏÑúÎìúÎ™Ö 100% Ïú†ÏßÄ")
logger.info("‚úÖ Ïã§Ìñâ Î©àÏ∂§ ÌòÑÏÉÅ ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info(f"üîß ÏãúÏä§ÌÖú Ï†ïÎ≥¥:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX})")
logger.info(f"   NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info("=" * 80)

# Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏
try:
    _test_loader = get_global_model_loader()
    logger.info(f"üéâ ÏïàÏ†ïÏ†ÅÏù∏ ModelLoader v3.0 Ï§ÄÎπÑ ÏôÑÎ£å!")
    logger.info(f"   ÎîîÎ∞îÏù¥Ïä§: {_test_loader.device}")
    logger.info(f"   Î™®Îç∏ Ï∫êÏãú: {_test_loader.model_cache_dir}")
    logger.info(f"   Í∏∞Î≥∏ Îß§Ìïë: {len(_test_loader.default_mappings)}Í∞ú Step")
    logger.info(f"   AutoDetector ÌÜµÌï©: {_test_loader._integration_successful}")
    logger.info(f"   ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏: {len(_test_loader.available_models)}Í∞ú")
except Exception as e:
    logger.error(f"‚ùå Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

if __name__ == "__main__":
    print("üöÄ ÏïàÏ†ïÏ†ÅÏù∏ ModelLoader v3.0 ÌÖåÏä§Ìä∏ (AI Ï∂îÎ°† Ï†úÍ±∞)")
    print("=" * 60)
    
    async def test_model_loader():
        # ModelLoader ÏÉùÏÑ±
        loader = get_global_model_loader()
        print(f"‚úÖ ModelLoader ÏÉùÏÑ±: {type(loader).__name__}")
        print(f"üîß ÎîîÎ∞îÏù¥Ïä§: {loader.device}")
        print(f"üìÅ Î™®Îç∏ Ï∫êÏãú: {loader.model_cache_dir}")
        
        # ÏãúÏä§ÌÖú Îä•Î†• ÌôïÏù∏
        capabilities = get_system_capabilities()
        print(f"\nüìä ÏãúÏä§ÌÖú Îä•Î†•:")
        print(f"   NumPy: {'‚úÖ' if capabilities['numpy_available'] else '‚ùå'}")
        print(f"   PIL: {'‚úÖ' if capabilities['pil_available'] else '‚ùå'}")
        print(f"   AutoDetector: {'‚úÖ' if capabilities['auto_detector_available'] else '‚ùå'}")
        print(f"   M3 Max: {'‚úÖ' if capabilities['is_m3_max'] else '‚ùå'}")
        
        # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏
        step_interface = create_step_interface("HumanParsingStep")
        print(f"\nüîó Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±: {type(step_interface).__name__}")
        
        step_status = step_interface.get_step_status()
        print(f"üìä Step ÏÉÅÌÉú:")
        print(f"   Step Ïù¥Î¶Ñ: {step_status['step_name']}")
        print(f"   Î°úÎî©Îêú Î™®Îç∏: {step_status['models_loaded']}Í∞ú")
        
        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù
        models = loader.list_available_models()
        print(f"\nüìã ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏: {len(models)}Í∞ú")
        if models:
            for i, model in enumerate(models[:3]):
                print(f"   {i+1}. {model['name']}: {model.get('size_mb', 0):.1f}MB")
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        metrics = loader.get_performance_metrics()
        print(f"\nüìà ÏÑ±Îä• Î©îÌä∏Î¶≠:")
        print(f"   Î°úÎî©Îêú Î™®Îç∏: {metrics['loaded_models_count']}Í∞ú")
        print(f"   Ï∫êÏãú ÌûàÌä∏: {metrics['cache_hits']}Ìöå")
        print(f"   Ï¥ù Î©îÎ™®Î¶¨: {metrics['total_memory_mb']:.1f}MB")
        print(f"   Ïò§Î•ò ÌöüÏàò: {metrics['error_count']}Ìöå")
        print(f"   AutoDetector ÌÜµÌï©: {metrics['auto_detector_integration']}")
    
    try:
        asyncio.run(test_model_loader())
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ïã§Ìå®: {e}")
    
    print("\nüéâ ÏïàÏ†ïÏ†ÅÏù∏ ModelLoader v3.0 ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")
    print("‚úÖ AI Ï∂îÎ°† Î°úÏßÅ Ï†úÍ±∞Î°ú ÏïàÏ†ïÏÑ± ÌôïÎ≥¥")
    print("‚úÖ StepModelInterface Ï†ïÏùò Î¨∏Ï†ú Ìï¥Í≤∞")
    print("‚úÖ ÌïµÏã¨ Î™®Îç∏ Î°úÎçî Í∏∞Îä• Ïú†ÏßÄ")
    print("‚úÖ BaseStepMixin 100% Ìò∏ÌôòÏÑ± ÌôïÎ≥¥")
    print("‚úÖ Ïã§Ìñâ Î©àÏ∂§ ÌòÑÏÉÅ ÏôÑÏ†Ñ Ìï¥Í≤∞")