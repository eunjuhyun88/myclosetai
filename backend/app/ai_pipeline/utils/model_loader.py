"""
ğŸ”¥ MyCloset AI - ì™„ì „í•œ ModelLoader v19.0 (í”„ë¡œë•ì…˜ ë ˆë²¨ ì™„ì„±íŒ)
===============================================================================
âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„ - ë¹ ì§ì—†ëŠ” ì™„ì „ì²´
âœ… auto_model_detector ì™„ì „ ì—°ë™ - ìš”ì²­ëª…â†’íŒŒì¼ëª… ë§¤í•‘ ì™„ë²½
âœ… Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ì™„ì „ ê´€ë¦¬
âœ… BaseStepMixin 100% í˜¸í™˜ - ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„
âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ íƒì§€ ë° ë¡œë”©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì˜ì¡´ì„± ì£¼ì…
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… M3 Max 128GB ìµœì í™”
âœ… ë¹„ë™ê¸°/ë™ê¸° ì™„ì „ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ
âœ… ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜

ğŸ¯ í•µì‹¬ ì—­í• :
- ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ íƒì§€ ë° ë¡œë”©
- Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ê´€ë¦¬  
- ëª¨ë¸ ìºì‹± ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
- Step íŒŒì¼ë“¤ì—ê²Œ ê¹”ë”í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- auto_model_detector ë§¤í•‘ í™œìš©

Author: MyCloset AI Team
Date: 2025-07-22
Version: 19.0 (Complete Production Ready)
===============================================================================
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë¡œê¹… ì„¤ì •
# ==============================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ==============================================
# ğŸ”¥ 2ë‹¨ê³„: TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° í•´ê²°
# ==============================================

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì (conda í™˜ê²½ ìš°ì„ )
# ==============================================

class LibraryCompatibility:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì - conda í™˜ê²½ ìš°ì„ """
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        self.conda_env = self._detect_conda_env()
        self._check_libraries()
    
    def _detect_conda_env(self) -> str:
        """conda í™˜ê²½ íƒì§€"""
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        if conda_env:
            return conda_env
        
        conda_prefix = os.environ.get('CONDA_PREFIX', '')
        if conda_prefix:
            return os.path.basename(conda_prefix)
        
        return ""

    def _check_libraries(self):
        """conda í™˜ê²½ ìš°ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬"""
        # NumPy ì²´í¬
        try:
            import numpy as np
            self.numpy_available = True
            globals()['np'] = np
        except ImportError:
            self.numpy_available = False
        
        # PyTorch ì²´í¬ (conda í™˜ê²½ ìµœì í™”)
        try:
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.device_type = "cpu"
            
            # M3 Max MPS ì„¤ì •
            if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if torch.backends.mps.is_available():
                    self.mps_available = True
                    self.device_type = "mps"
                    self.is_m3_max = True
                    self._safe_mps_empty_cache()
                    
            elif torch.cuda.is_available():
                self.device_type = "cuda"
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError:
            self.torch_available = False
            self.mps_available = False

    def _safe_mps_empty_cache(self):
        """ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬"""
        try:
            if not self.torch_available:
                return False
            
            import torch as local_torch
            
            if hasattr(local_torch, 'mps') and hasattr(local_torch.mps, 'empty_cache'):
                local_torch.mps.empty_cache()
                return True
            elif hasattr(local_torch, 'backends') and hasattr(local_torch.backends, 'mps'):
                if hasattr(local_torch.backends.mps, 'empty_cache'):
                    local_torch.backends.mps.empty_cache()
                    return True
            
            return False
        except (AttributeError, RuntimeError, ImportError):
            return False

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™”
_compat = LibraryCompatibility()

# ì „ì—­ ìƒìˆ˜ ì •ì˜
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
NUMPY_AVAILABLE = _compat.numpy_available
DEFAULT_DEVICE = _compat.device_type
IS_M3_MAX = _compat.is_m3_max
CONDA_ENV = _compat.conda_env

# ==============================================
# ğŸ”¥ 4ë‹¨ê³„: ì•ˆì „í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
                return True
            elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    return True
            return False
    except (AttributeError, RuntimeError) as e:
        logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ì •ìƒ): {e}")
        return False

def safe_torch_cleanup():
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        gc.collect()
        
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            safe_mps_empty_cache()
        
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def get_memory_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        return {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "percent": memory.percent,
            "is_m3_max": IS_M3_MAX
        }
    except ImportError:
        return {
            "total_gb": 128.0 if IS_M3_MAX else 16.0,
            "available_gb": 100.0 if IS_M3_MAX else 12.0,
            "used_gb": 28.0 if IS_M3_MAX else 4.0,
            "percent": 22.0 if IS_M3_MAX else 25.0,
            "is_m3_max": IS_M3_MAX
        }

# ==============================================
# ğŸ”¥ 5ë‹¨ê³„: ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class StepPriority(IntEnum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    TENSORFLOW = "bin"
    ONNX = "onnx"
    PICKLE = "pkl"
    CHECKPOINT = "ckpt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    file_size_mb: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    file_size_mb: float = 0.0
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

@dataclass
class ModelCacheEntry:
    """ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    load_time: float
    last_access: float
    access_count: int
    memory_usage_mb: float
    device: str
    step_name: Optional[str] = None

# ==============================================
# ğŸ”¥ 6ë‹¨ê³„: ì™¸ë¶€ ëª¨ë“ˆ ì—°ë™ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# step_model_requirements ì—°ë™
try:
    from .step_model_requirements import (
        STEP_MODEL_REQUESTS,
        StepModelRequestAnalyzer,
        get_step_request,
        get_global_analyzer,
        get_all_step_requirements
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("âœ… step_model_requirements ì—°ë™ ì„±ê³µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"âš ï¸ step_model_requirements ì—°ë™ ì‹¤íŒ¨: {e}")
    
    # í´ë°± ë°ì´í„° (ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜)
    STEP_MODEL_REQUESTS = {
        "HumanParsingStep": {
            "model_name": "human_parsing_graphonomy",
            "model_type": "GraphonomyModel",
            "input_size": (512, 512),
            "num_classes": 20,
            "checkpoint_patterns": ["*atr*.pth", "*schp*.pth", "*graphonomy*.pth"],
            "priority": 1
        },
        "PoseEstimationStep": {
            "model_name": "pose_estimation_openpose",
            "model_type": "OpenPoseModel",
            "input_size": (368, 368),
            "num_classes": 18,
            "checkpoint_patterns": ["*openpose*.pth", "*pose*.pth"],
            "priority": 2
        },
        "ClothSegmentationStep": {
            "model_name": "cloth_segmentation_u2net",
            "model_type": "U2NetModel",
            "input_size": (320, 320),
            "num_classes": 1,
            "checkpoint_patterns": ["*u2net*.pth", "*cloth*.pth", "*seg*.pth"],
            "priority": 3
        },
        "GeometricMatchingStep": {
            "model_name": "geometric_matching_model",
            "model_type": "GeometricMatchingModel",
            "input_size": (256, 192),
            "checkpoint_patterns": ["*gmm*.pth", "*geometric*.pth", "*matching*.pth"],
            "priority": 4
        },
        "ClothWarpingStep": {
            "model_name": "cloth_warping_net",
            "model_type": "ClothWarpingModel",
            "input_size": (256, 192),
            "checkpoint_patterns": ["*warp*.pth", "*tps*.pth"],
            "priority": 5
        },
        "VirtualFittingStep": {
            "model_name": "virtual_fitting_diffusion",
            "model_type": "StableDiffusionPipeline",
            "input_size": (512, 512),
            "checkpoint_patterns": ["*diffusion*.bin", "*stable*.bin", "*viton*.pth"],
            "priority": 6
        },
        "PostProcessingStep": {
            "model_name": "post_processing_enhance",
            "model_type": "EnhancementModel",
            "input_size": (512, 512),
            "checkpoint_patterns": ["*enhance*.pth", "*sr*.pth", "*upscale*.pth"],
            "priority": 7
        },
        "QualityAssessmentStep": {
            "model_name": "quality_assessment_clip",
            "model_type": "CLIPModel",
            "input_size": (224, 224),
            "checkpoint_patterns": ["*clip*.bin", "*quality*.pth"],
            "priority": 8
        }
    }
    
    class StepModelRequestAnalyzer:
        @staticmethod
        def get_all_step_requirements():
            return STEP_MODEL_REQUESTS
    
    def get_step_request(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)
    
    def get_global_analyzer():
        return StepModelRequestAnalyzer()
    
    def get_all_step_requirements():
        return STEP_MODEL_REQUESTS

# auto_model_detector ì—°ë™
try:
    from .auto_model_detector import (
        create_real_world_detector,
        quick_model_detection,
        comprehensive_model_detection,
        get_global_detector,
        ENHANCED_STEP_MODEL_PATTERNS
    )
    AUTO_MODEL_DETECTOR_AVAILABLE = True
    logger.info("âœ… auto_model_detector ì—°ë™ ì„±ê³µ")
except ImportError as e:
    AUTO_MODEL_DETECTOR_AVAILABLE = False
    logger.warning(f"âš ï¸ auto_model_detector ì—°ë™ ì‹¤íŒ¨: {e}")
    
    def quick_model_detection(**kwargs):
        return {}
    
    def comprehensive_model_detection(**kwargs):
        return {}
    
    def get_global_detector():
        return None
    
    ENHANCED_STEP_MODEL_PATTERNS = {}

# ==============================================
# ğŸ”¥ 7ë‹¨ê³„: Step ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤ (BaseStepMixin ì™„ë²½ í˜¸í™˜)
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - BaseStepMixinì—ì„œ ì§ì ‘ ì‚¬ìš©"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, ModelCacheEntry] = {}
        self.model_status: Dict[str, str] = {}
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = self._get_step_request()
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.creation_time = time.time()
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_step_request(self):
        """Stepë³„ ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if STEP_REQUESTS_AVAILABLE:
            try:
                request = get_step_request(self.step_name)
                if request and hasattr(request, '__dict__'):
                    return request.__dict__
                return request
            except:
                pass
        
        fallback_request = STEP_MODEL_REQUESTS.get(self.step_name)
        if fallback_request and hasattr(fallback_request, '__dict__'):
            return fallback_request.__dict__
        return fallback_request
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        if self.step_request:
            if isinstance(self.step_request, dict):
                model_name = self.step_request.get("model_name", "default_model")
            else:
                model_name = getattr(self.step_request, "model_name", "default_model")
            return [model_name]
        
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_schp_atr"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["cloth_segmentation_u2net", "u2net_cloth_seg"],
            "GeometricMatchingStep": ["geometric_matching_model", "geometric_matching_gmm"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["virtual_fitting_diffusion", "stable_diffusion"],
            "PostProcessingStep": ["post_processing_enhance", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    # ==============================================
    # ğŸ”¥ BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixinì—ì„œ await interface.get_model() í˜¸ì¶œ"""
        try:
            async with self._async_lock:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # ìºì‹œ í™•ì¸
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return cache_entry.model
                
                # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                checkpoint = None
                if hasattr(self.model_loader, 'load_model_async'):
                    checkpoint = await self.model_loader.load_model_async(model_name)
                elif hasattr(self.model_loader, 'load_model'):
                    checkpoint = self.model_loader.load_model(model_name)
                
                if checkpoint:
                    # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜)
                    cache_entry = ModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', 'cpu') if hasattr(checkpoint, 'device') else 'cpu',
                        step_name=self.step_name
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = "loaded"
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return checkpoint
                
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixinì—ì„œ interface.get_model_sync() í˜¸ì¶œ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    return cache_entry.model
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(model_name)
            
            if checkpoint:
                with self._lock:
                    cache_entry = ModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', 'cpu') if hasattr(checkpoint, 'device') else 'cpu',
                        step_name=self.step_name
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = "loaded"
                return checkpoint
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixinì—ì„œ interface.get_model_status() í˜¸ì¶œ"""
        try:
            if not model_name:
                # ì „ì²´ ëª¨ë¸ ìƒíƒœ ë°˜í™˜
                models_status = {}
                with self._lock:
                    for name, cache_entry in self.model_cache.items():
                        models_status[name] = {
                            "status": self.model_status.get(name, "loaded"),
                            "device": cache_entry.device,
                            "memory_usage_mb": cache_entry.memory_usage_mb,
                            "last_access": cache_entry.last_access,
                            "access_count": cache_entry.access_count,
                            "load_time": cache_entry.load_time
                        }
                
                return {
                    "step_name": self.step_name,
                    "models": models_status,
                    "loaded_count": len(self.loaded_models),
                    "total_memory_mb": sum(entry.memory_usage_mb for entry in self.model_cache.values()),
                    "recommended_models": self.recommended_models
                }
            
            # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    return {
                        "status": self.model_status.get(model_name, "loaded"),
                        "device": cache_entry.device,
                        "memory_usage_mb": cache_entry.memory_usage_mb,
                        "last_access": cache_entry.last_access,
                        "access_count": cache_entry.access_count,
                        "load_time": cache_entry.load_time,
                        "model_type": type(cache_entry.model).__name__,
                        "loaded": True
                    }
                else:
                    return {
                        "status": "not_loaded",
                        "device": None,
                        "memory_usage_mb": 0.0,
                        "last_access": 0,
                        "access_count": 0,
                        "load_time": 0,
                        "model_type": None,
                        "loaded": False
                    }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
    
    def list_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ - BaseStepMixinì—ì„œ interface.list_available_models() í˜¸ì¶œ"""
        models = []
        
        # ê¶Œì¥ ëª¨ë¸ë“¤ ì¶”ê°€
        for model_name in self.recommended_models:
            is_loaded = model_name in self.loaded_models
            cache_entry = self.model_cache.get(model_name)
            
            models.append({
                "name": model_name,
                "path": f"recommended/{model_name}",
                "size_mb": cache_entry.memory_usage_mb if cache_entry else 100.0,
                "model_type": self.step_name.lower(),
                "step_class": self.step_name,
                "loaded": is_loaded,
                "device": cache_entry.device if cache_entry else "auto",
                "metadata": {
                    "recommended": True,
                    "step_name": self.step_name,
                    "access_count": cache_entry.access_count if cache_entry else 0
                }
            })
        
        return models
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ - BaseStepMixinì—ì„œ interface.register_model_requirement() í˜¸ì¶œ"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ í¬ê¸° ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE:
                if isinstance(checkpoint, dict):
                    # state_dictì¸ ê²½ìš°
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
                elif hasattr(checkpoint, 'parameters'):
                    # ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0

# ==============================================
# ğŸ”¥ 8ë‹¨ê³„: ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ìˆœìˆ˜ ë¡œë”©/ê´€ë¦¬)
# ==============================================

class ModelLoader:
    """ìˆœìˆ˜ ëª¨ë¸ ë¡œë”©/ê´€ë¦¬ ì „ìš© ModelLoader v19.0"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ìˆœìˆ˜ ëª¨ë¸ ë¡œë” ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._resolve_device(device or "auto")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        memory_info = get_memory_info()
        self.memory_gb = memory_info["total_gb"]
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ModelLoader íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 30 if self.is_m3_max else 15)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ğŸ”¥ BaseStepMixinì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ì†ì„±ë“¤
        self.loaded_models: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.model_cache: Dict[str, ModelCacheEntry] = {}
        self.available_models: Dict[str, Any] = {}
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_model_requests: Dict[str, Any] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ì„±ëŠ¥ ì¶”ì 
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'auto_detections': 0,
            'checkpoint_loads': 0,
            'total_models_found': 0
        }
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="model_loader")
        
        # ì´ë²¤íŠ¸ ì½œë°± ì‹œìŠ¤í…œ
        self._event_callbacks: Dict[str, List[Callable]] = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        self.logger.info(f"ğŸ¯ ìˆœìˆ˜ ModelLoader v19.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
        self.logger.info(f"ğŸ’¾ Memory: {self.memory_gb:.1f}GB")
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device == "auto":
            return DEFAULT_DEVICE
        return device
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” - ğŸ”¥ auto_model_detector ì™„ì „ ì—°ë™"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ğŸ”¥ auto_model_detectorë¥¼ í†µí•œ ëª¨ë¸ ìë™ íƒì§€ ë° ë“±ë¡
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                try:
                    self.logger.info("ğŸ” auto_model_detector ìë™ íƒì§€ ì‹œì‘...")
                    detected_count = self.scan_and_register_all_models()
                    self.logger.info(f"âœ… ìë™ íƒì§€ ì™„ë£Œ: {detected_count}ê°œ ëª¨ë¸ ë“±ë¡")
                    
                    # ğŸ¯ í•µì‹¬ ëª¨ë¸ë“¤ì´ ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    critical_models = [
                        'cloth_segmentation_u2net',
                        'human_parsing_schp_atr', 
                        'pose_estimation_openpose',
                        'virtual_fitting_diffusion'
                    ]
                    
                    found_critical = 0
                    for model_name in critical_models:
                        if model_name in self.model_configs:
                            found_critical += 1
                            self.logger.info(f"âœ… í•µì‹¬ ëª¨ë¸ ë°œê²¬: {model_name}")
                        elif any(alias in self.model_configs for alias in self._get_model_aliases(model_name)):
                            found_critical += 1
                            self.logger.info(f"âœ… í•µì‹¬ ëª¨ë¸ ë³„ì¹­ ë°œê²¬: {model_name}")
                    
                    self.logger.info(f"ğŸ¯ í•µì‹¬ ëª¨ë¸ ë“±ë¡ë¥ : {found_critical}/{len(critical_models)} ({found_critical/len(critical_models)*100:.1f}%)")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ auto_model_detector ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
            else:
                self.logger.warning("âš ï¸ auto_model_detector ì‚¬ìš© ë¶ˆê°€ - ìˆ˜ë™ ë“±ë¡ë§Œ ì‚¬ìš©")
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” (í´ë°±)
            self._initialize_model_registry()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº” (ì¶”ê°€ ìŠ¤ìº”)
            self._scan_available_models()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _get_model_aliases(self, model_name: str) -> List[str]:
        """ëª¨ë¸ ë³„ì¹­ë“¤ ë°˜í™˜"""
        aliases_map = {
            'cloth_segmentation_u2net': ['u2net', 'clothsegmentation_u2net'],
            'human_parsing_schp_atr': ['schp_atr', 'humanparsing_schp_atr', 'human_parsing_graphonomy'],
            'pose_estimation_openpose': ['openpose', 'poseestimation_openpose'],
            'virtual_fitting_diffusion': ['pytorch_model', 'virtualfitting_diffusion']
        }
        return aliases_map.get(model_name, [])
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            if STEP_REQUESTS_AVAILABLE:
                self.step_requirements = get_all_step_requirements()
                self.logger.info(f"âœ… Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë¡œë“œ: {len(self.step_requirements)}ê°œ")
            else:
                self.step_requirements = STEP_MODEL_REQUESTS
                self.logger.warning("âš ï¸ í´ë°± Step ìš”ì²­ì‚¬í•­ ì‚¬ìš©")
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if hasattr(request_info, '__dict__'):
                        request_dict = request_info.__dict__
                    else:
                        request_dict = request_info
                    
                    if isinstance(request_dict, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_dict.get("model_name", step_name.lower()),
                            model_class=request_dict.get("model_type", "BaseModel"),
                            model_type=request_dict.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_dict.get("input_size", (512, 512)),
                            num_classes=request_dict.get("num_classes", None),
                            file_size_mb=request_dict.get("file_size_mb", 0.0)
                        )
                        
                        self.model_configs[request_dict.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            # ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ ì„¤ì •ë“¤
            model_configs = {
                # ì¸ì²´ íŒŒì‹± ëª¨ë¸ë“¤
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20,
                    file_size_mb=255.1
                ),
                "human_parsing_schp_atr": ModelConfig(
                    name="human_parsing_schp_atr",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="SCHPModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "exp-schp-201908301523-atr.pth"),
                    input_size=(512, 512),
                    num_classes=20,
                    file_size_mb=255.1
                ),
                
                # í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë“¤
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "openpose.pth"),
                    input_size=(368, 368),
                    num_classes=18,
                    file_size_mb=199.6
                ),
                
                # ì˜ë¥˜ ë¶„í•  ëª¨ë¸ë“¤
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320),
                    file_size_mb=168.1
                ),
                
                # ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ë“¤
                "geometric_matching_model": ModelConfig(
                    name="geometric_matching_model",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "gmm.pth"),
                    input_size=(256, 192),
                    file_size_mb=200.0
                ),
                
                # ì˜ë¥˜ ë³€í˜• ëª¨ë¸ë“¤
                "cloth_warping_net": ModelConfig(
                    name="cloth_warping_net",
                    model_type=ModelType.CLOTH_WARPING,
                    model_class="ClothWarpingNet",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "warping_net.pth"),
                    input_size=(256, 192),
                    file_size_mb=180.0
                ),
                
                # ê°€ìƒ í”¼íŒ… ëª¨ë¸ë“¤
                "virtual_fitting_diffusion": ModelConfig(
                    name="virtual_fitting_diffusion",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="StableDiffusionPipeline", 
                    checkpoint_path=str(base_models_dir / "stable-diffusion" / "pytorch_model.bin"),
                    input_size=(512, 512),
                    file_size_mb=577.2
                ),
                
                # í›„ì²˜ë¦¬ ëª¨ë¸ë“¤
                "post_processing_enhance": ModelConfig(
                    name="post_processing_enhance",
                    model_type=ModelType.POST_PROCESSING,
                    model_class="EnhancementModel",
                    checkpoint_path=str(base_models_dir / "enhancement" / "enhance.pth"),
                    input_size=(512, 512),
                    file_size_mb=120.0
                ),
                
                # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ë“¤
                "quality_assessment_clip": ModelConfig(
                    name="quality_assessment_clip",
                    model_type=ModelType.QUALITY_ASSESSMENT,
                    model_class="CLIPModel",
                    checkpoint_path=str(base_models_dir / "clip" / "clip.bin"),
                    input_size=(224, 224),
                    file_size_mb=440.0
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _scan_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ìŠ¤ìº”"""
        try:
            self.logger.info("ğŸ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            
            if not self.model_cache_dir.exists():
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_cache_dir}")
                return
                
            scanned_count = 0
            large_models_count = 0
            total_size_gb = 0.0
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¥ì ì§€ì›
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt", ".pkl", ".pickle"]
            
            for ext in extensions:
                for model_file in self.model_cache_dir.rglob(f"*{ext}"):
                    if any(exclude in str(model_file) for exclude in ["cleanup_backup", "__pycache__", ".git"]):
                        continue
                        
                    try:
                        size_mb = model_file.stat().st_size / (1024 * 1024)
                        total_size_gb += size_mb / 1024
                        
                        if size_mb > 1000:  # 1GB ì´ìƒ
                            large_models_count += 1
                        
                        relative_path = model_file.relative_to(self.model_cache_dir)
                        
                        model_info = {
                            "name": model_file.stem,
                            "path": str(relative_path),
                            "size_mb": round(size_mb, 2),
                            "model_type": self._detect_model_type(model_file),
                            "step_class": self._detect_step_class(model_file),
                            "loaded": False,
                            "device": self.device,
                            "metadata": {
                                "extension": ext,
                                "parent_dir": model_file.parent.name,
                                "full_path": str(model_file),
                                "is_large": size_mb > 1000,
                                "last_modified": model_file.stat().st_mtime
                            }
                        }
                        
                        self.available_models[model_info["name"]] = model_info
                        scanned_count += 1
                        
                        # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¡œê¹…
                        if scanned_count <= 10:
                            self.logger.info(f"ğŸ“¦ ë°œê²¬: {model_info['name']} ({size_mb:.1f}MB)")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨ {model_file}: {e}")
                        
            self.performance_stats['total_models_found'] = scanned_count
            self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì™„ë£Œ: {scanned_count}ê°œ ë°œê²¬")
            self.logger.info(f"ğŸ“Š ëŒ€ìš©ëŸ‰ ëª¨ë¸(1GB+): {large_models_count}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ ëª¨ë¸ í¬ê¸°: {total_size_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
    
    def _detect_model_type(self, model_file: Path) -> str:
        """ëª¨ë¸ íƒ€ì… ê°ì§€ - ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜"""
        filename = model_file.name.lower()
        
        type_keywords = {
            "human_parsing": ["schp", "atr", "lip", "graphonomy", "parsing", "exp-schp"],
            "pose_estimation": ["pose", "openpose", "body_pose", "hand_pose"],
            "cloth_segmentation": ["u2net", "sam", "segment", "cloth"],
            "geometric_matching": ["gmm", "geometric", "matching", "tps"],
            "cloth_warping": ["warp", "tps", "deformation"],
            "virtual_fitting": ["viton", "hrviton", "ootd", "diffusion", "vae", "pytorch_model"],
            "post_processing": ["esrgan", "enhancement", "super_resolution"],
            "quality_assessment": ["lpips", "quality", "metric", "clip"]
        }
        
        for model_type, keywords in type_keywords.items():
            if any(keyword in filename for keyword in keywords):
                return model_type
                
        return "unknown"
        
    def _detect_step_class(self, model_file: Path) -> str:
        """Step í´ë˜ìŠ¤ ê°ì§€"""
        parent_dir = model_file.parent.name.lower()
        filename = model_file.name.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë§¤í•‘
        if "schp" in filename or "graphonomy" in filename or "parsing" in filename:
            return "HumanParsingStep"
        elif "openpose" in filename or "pose" in filename:
            return "PoseEstimationStep"
        elif "u2net" in filename or ("cloth" in filename and "segment" in filename):
            return "ClothSegmentationStep"
        elif "sam" in filename and "vit" in filename:
            return "ClothSegmentationStep"
        elif "pytorch_model" in filename or "diffusion" in filename:
            return "VirtualFittingStep"
        elif "gmm" in filename or "geometric" in filename:
            return "GeometricMatchingStep"
        elif "warp" in filename or "tps" in filename:
            return "ClothWarpingStep"
        elif "enhance" in filename or "sr" in filename:
            return "PostProcessingStep"
        elif "clip" in filename or "quality" in filename:
            return "QualityAssessmentStep"
        
        # ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë§¤í•‘
        if parent_dir.startswith("step_"):
            step_mapping = {
                "step_01": "HumanParsingStep",
                "step_02": "PoseEstimationStep", 
                "step_03": "ClothSegmentationStep",
                "step_04": "GeometricMatchingStep",
                "step_05": "ClothWarpingStep",
                "step_06": "VirtualFittingStep",
                "step_07": "PostProcessingStep",
                "step_08": "QualityAssessmentStep"
            }
            
            for prefix, step_class in step_mapping.items():
                if parent_dir.startswith(prefix):
                    return step_class
                    
        return "UnknownStep"
    
    # ==============================================
    # ğŸ”¥ BaseStepMixinì´ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Union[Dict[str, Any], List[Dict[str, Any]]]
    ) -> bool:
        """ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                # ê¸°ì¡´ ìš”ì²­ì‚¬í•­ê³¼ ë³‘í•©
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # requirementsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(requirements, list):
                    processed_requirements = {}
                    for i, req in enumerate(requirements):
                        if isinstance(req, dict):
                            model_name = req.get("model_name", f"{step_name}_model_{i}")
                            processed_requirements[model_name] = req
                    requirements = processed_requirements
                
                # ìš”ì²­ì‚¬í•­ ì—…ë°ì´íŠ¸
                if isinstance(requirements, dict):
                    self.step_requirements[step_name].update(requirements)
                else:
                    # ë‹¨ì¼ ìš”ì²­ì‚¬í•­ì¸ ê²½ìš°
                    self.step_requirements[step_name]["default_model"] = requirements
                
                # StepModelConfig ìƒì„±
                registered_models = 0
                for model_name, model_req in self.step_requirements[step_name].items():
                    try:
                        if isinstance(model_req, dict):
                            step_config = StepModelConfig(
                                step_name=step_name,
                                model_name=model_name,
                                model_class=model_req.get("model_class", "BaseModel"),
                                model_type=model_req.get("model_type", "unknown"),
                                device=model_req.get("device", "auto"),
                                precision=model_req.get("precision", "fp16"),
                                input_size=tuple(model_req.get("input_size", (512, 512))),
                                num_classes=model_req.get("num_classes"),
                                file_size_mb=model_req.get("file_size_mb", 0.0),
                                priority=model_req.get("priority", 5),
                                confidence_score=model_req.get("confidence_score", 0.0),
                                registration_time=time.time()
                            )
                            
                            self.model_configs[model_name] = step_config
                            registered_models += 1
                            
                            self.logger.debug(f"   âœ… {model_name} ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                            
                    except Exception as model_error:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {model_error}")
                        continue
                
                self.logger.info(f"âœ… {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {registered_models}ê°œ ëª¨ë¸")
                self._trigger_model_event("step_requirements_registered", step_name, count=registered_models)
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ"""
        try:
            with self._interface_lock:
                # Step ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ ë“±ë¡
                if step_requirements:
                    self.register_step_requirements(step_name, step_requirements)
                
                # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if step_name in self.step_interfaces:
                    return self.step_interfaces[step_name]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                
                self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(self, step_name)
    
    def register_model_config(self, name: str, config: Union[ModelConfig, Dict[str, Any]]) -> bool:
        """ğŸ”¥ ëª¨ë¸ ì„¤ì • ë“±ë¡ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ"""
        try:
            with self._lock:
                if isinstance(config, dict):
                    # ë”•ì…”ë„ˆë¦¬ì—ì„œ ModelConfig ìƒì„±
                    model_config = ModelConfig(
                        name=name,
                        model_type=config.get("model_type", "unknown"),
                        model_class=config.get("model_class", "BaseModel"),
                        checkpoint_path=config.get("checkpoint_path"),
                        config_path=config.get("config_path"),
                        device=config.get("device", "auto"),
                        precision=config.get("precision", "fp16"),
                        input_size=tuple(config.get("input_size", (512, 512))),
                        num_classes=config.get("num_classes"),
                        file_size_mb=config.get("file_size_mb", 0.0),
                        metadata=config.get("metadata", {})
                    )
                else:
                    model_config = config
                
                self.model_configs[name] = model_config
                
                # available_modelsì—ë„ ì¶”ê°€
                self.available_models[name] = {
                    "name": name,
                    "path": model_config.checkpoint_path or f"config/{name}",
                    "size_mb": model_config.file_size_mb,
                    "model_type": str(model_config.model_type),
                    "step_class": model_config.model_class,
                    "loaded": False,
                    "device": model_config.device,
                    "metadata": model_config.metadata
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ì„¤ì • ë“±ë¡ ì™„ë£Œ: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„¤ì • ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ"""
        try:
            models = []
            
            for model_name, model_info in self.available_models.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                    
                models.append({
                    "name": model_info["name"],
                    "path": model_info["path"],
                    "size_mb": model_info["size_mb"],
                    "model_type": model_info["model_type"],
                    "step_class": model_info["step_class"],
                    "loaded": model_info["loaded"],
                    "device": model_info["device"],
                    "metadata": model_info["metadata"]
                })
            
            # í¬ê¸°ìˆœ ì •ë ¬
            models.sort(key=lambda x: x["size_mb"], reverse=True)
            
            self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ìš”ì²­: {len(models)}ê°œ ë°˜í™˜ (step={step_class}, type={model_type})")
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    # ==============================================
    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë©”ì„œë“œë“¤ (í•µì‹¬ ê¸°ëŠ¥)
    # ==============================================
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                cache_entry.last_access = time.time()
                cache_entry.access_count += 1
                self.performance_stats['cache_hits'] += 1
                self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                return cache_entry.model
                
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
                
            # ë¹„ë™ê¸°ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤í–‰
            loop = asyncio.get_event_loop()
            checkpoint = await loop.run_in_executor(
                self._executor, 
                self._load_checkpoint_sync,
                model_name,
                kwargs
            )
            
            if checkpoint is not None:
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                cache_entry = ModelCacheEntry(
                    model=checkpoint,
                    load_time=time.time(),
                    last_access=time.time(),
                    access_count=1,
                    memory_usage_mb=self._get_checkpoint_memory_usage(checkpoint),
                    device=getattr(checkpoint, 'device', self.device) if hasattr(checkpoint, 'device') else self.device
                )
                
                self.model_cache[model_name] = cache_entry
                self.loaded_models[model_name] = checkpoint
                
                if model_name in self.available_models:
                    self.available_models[model_name]["loaded"] = True
                
                self.performance_stats['models_loaded'] += 1
                self.performance_stats['checkpoint_loads'] += 1
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                self._trigger_model_event("model_loaded", model_name)
                
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self._trigger_model_event("model_error", model_name, error=str(e))
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                cache_entry.last_access = time.time()
                cache_entry.access_count += 1
                self.performance_stats['cache_hits'] += 1
                self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                return cache_entry.model
                
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
            
            return self._load_checkpoint_sync(model_name, kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self._trigger_model_event("model_error", model_name, error=str(e))
            return None
    
    def _load_checkpoint_sync(self, model_name: str, kwargs: Dict[str, Any]) -> Optional[Any]:
        """ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ì‹¤ì œ êµ¬í˜„)"""
        try:
            start_time = time.time()
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            checkpoint_path = self._find_checkpoint_file(model_name)
            if not checkpoint_path or not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
                return None
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if TORCH_AVAILABLE:
                try:
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    if self.device in ["mps", "cuda"]:
                        safe_mps_empty_cache()
                    
                    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”©: {checkpoint_path}")
                    
                    # ì•ˆì „í•œ ë¡œë”© (weights_only=Falseë¡œ ì„¤ì •í•˜ë˜, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ë§Œ)
                    checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                    
                    # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    load_time = time.time() - start_time
                    cache_entry = ModelCacheEntry(
                        model=checkpoint,
                        load_time=load_time,
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._get_checkpoint_memory_usage(checkpoint),
                        device=str(self.device)
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.load_times[model_name] = load_time
                    self.last_access[model_name] = time.time()
                    self.access_counts[model_name] = self.access_counts.get(model_name, 0) + 1
                    
                    if model_name in self.available_models:
                        self.available_models[model_name]["loaded"] = True
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_name} ({load_time:.2f}ì´ˆ, {cache_entry.memory_usage_mb:.1f}MB)")
                    return checkpoint
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            
            # PyTorch ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë¶ˆê°€: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_checkpoint_file(self, model_name: str) -> Optional[Path]:
        """ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° - auto_model_detector ë§¤í•‘ í™œìš© (í•µì‹¬ ìˆ˜ì •!)"""
        try:
            # ğŸ”¥ 1ë‹¨ê³„: auto_model_detector ë§¤í•‘ ìš°ì„  ì²´í¬
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                mapped_path = self._find_via_auto_detector(model_name)
                if mapped_path:
                    self.logger.info(f"ğŸ¯ auto_detector ë§¤í•‘ ì„±ê³µ: {model_name} â†’ {mapped_path}")
                    return mapped_path
            
            # ğŸ”¥ 2ë‹¨ê³„: ê¸°ì¡´ ëª¨ë¸ ì„¤ì •ì—ì„œ ì§ì ‘ ê²½ë¡œ í™•ì¸
            if model_name in self.model_configs:
                config = self.model_configs[model_name]
                if hasattr(config, 'checkpoint_path') and config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if checkpoint_path.exists():
                        self.logger.debug(f"ğŸ“ ëª¨ë¸ ì„¤ì • ê²½ë¡œ ì‚¬ìš©: {model_name}")
                        return checkpoint_path
            
            # ğŸ”¥ 3ë‹¨ê³„: ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            for ext in extensions:
                direct_path = self.model_cache_dir / f"{model_name}{ext}"
                if direct_path.exists():
                    self.logger.debug(f"ğŸ“ ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­: {model_name}")
                    return direct_path
            
            # ğŸ”¥ 4ë‹¨ê³„: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
            pattern_result = self._find_via_pattern_matching(model_name, extensions)
            if pattern_result:
                return pattern_result
            
            # ğŸ”¥ 5ë‹¨ê³„: Step ìš”ì²­ì‚¬í•­ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
            step_result = self._find_via_step_patterns(model_name)
            if step_result:
                return step_result
            
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None

    def _find_via_auto_detector(self, model_name: str) -> Optional[Path]:
        """ğŸ”¥ auto_model_detectorë¥¼ í†µí•œ íŒŒì¼ ì°¾ê¸° (í•µì‹¬!)"""
        try:
            if not AUTO_MODEL_DETECTOR_AVAILABLE:
                return None
                
            detector = get_global_detector()
            if not detector:
                return None
            
            # íƒì§€ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìŠ¤ìº” ì‹¤í–‰
            if not hasattr(detector, 'detected_models') or not detector.detected_models:
                self.logger.info("ğŸ” auto_detector ìŠ¤ìº” ì‹¤í–‰ ì¤‘...")
                if hasattr(detector, 'detect_all_models'):
                    detector.detect_all_models()
            
            # ğŸ¯ ë°©ë²• 1: Stepë³„ ì§ì ‘ ë§¤í•‘ ì²´í¬
            if ENHANCED_STEP_MODEL_PATTERNS:
                for step_name, config in ENHANCED_STEP_MODEL_PATTERNS.items():
                    direct_mapping = config.get("direct_mapping", {})
                    
                    # ìš”ì²­ëª…ì´ ì§ì ‘ ë§¤í•‘ì— ìˆëŠ”ì§€ í™•ì¸
                    if model_name in direct_mapping:
                        target_files = direct_mapping[model_name]
                        
                        # ê° ëŒ€ìƒ íŒŒì¼ëª…ì„ íƒì§€ëœ ëª¨ë¸ì—ì„œ ì°¾ê¸°
                        for target_file in target_files:
                            if hasattr(detector, 'detected_models'):
                                for detected_model in detector.detected_models.values():
                                    original_filename = getattr(detected_model, 'original_filename', detected_model.path.name)
                                    
                                    if target_file.lower() == original_filename.lower():
                                        self.logger.info(f"ğŸ¯ ì§ì ‘ ë§¤í•‘: {model_name} â†’ {target_file} â†’ {detected_model.path}")
                                        return detected_model.path
                                    
                                    # ë¶€ë¶„ ë§¤ì¹­ë„ ì‹œë„
                                    if target_file.lower() in original_filename.lower():
                                        self.logger.info(f"ğŸ¯ ë¶€ë¶„ ë§¤í•‘: {model_name} â†’ {target_file} â†’ {detected_model.path}")
                                        return detected_model.path
            
            # ğŸ” ë°©ë²• 2: íƒì§€ëœ ëª¨ë¸ì—ì„œ ì´ë¦„ ë§¤ì¹­
            if hasattr(detector, 'detected_models'):
                for detected_model in detector.detected_models.values():
                    # ëª¨ë¸ëª… ë¶€ë¶„ ì¼ì¹˜
                    if model_name.lower() in detected_model.name.lower():
                        self.logger.info(f"ğŸ” ëª¨ë¸ëª… ë§¤ì¹­: {model_name} â†’ {detected_model.name}")
                        return detected_model.path
                    
                    # ì›ë³¸ íŒŒì¼ëª… ë¶€ë¶„ ì¼ì¹˜  
                    original_filename = getattr(detected_model, 'original_filename', detected_model.path.name)
                    if model_name.lower() in original_filename.lower():
                        self.logger.info(f"ğŸ” íŒŒì¼ëª… ë§¤ì¹­: {model_name} â†’ {original_filename}")
                        return detected_model.path
            
            # ğŸ”§ ë°©ë²• 3: ìŠ¤ë§ˆíŠ¸ ë§¤í•‘ (íŠ¹ìˆ˜í•œ ê²½ìš°ë“¤)
            smart_mapping = self._get_smart_model_mapping()
            if model_name in smart_mapping:
                target_pattern = smart_mapping[model_name]
                if hasattr(detector, 'detected_models'):
                    for detected_model in detector.detected_models.values():
                        original_filename = getattr(detected_model, 'original_filename', detected_model.path.name)
                        if target_pattern.lower() in original_filename.lower():
                            self.logger.info(f"ğŸ”§ ìŠ¤ë§ˆíŠ¸ ë§¤í•‘: {model_name} â†’ {target_pattern} â†’ {detected_model.path}")
                            return detected_model.path
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ auto_detector ë§¤í•‘ ì‹¤íŒ¨ {model_name}: {e}")
            return None

    def _get_smart_model_mapping(self) -> Dict[str, str]:
        """ğŸ”§ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë§¤í•‘ (íŠ¹ìˆ˜í•œ ê²½ìš°ë“¤)"""
        return {
            # ì¸ì²´ íŒŒì‹± ëª¨ë¸ë“¤
            "human_parsing_graphonomy": "exp-schp-201908301523-atr.pth",
            "human_parsing_schp_atr": "exp-schp-201908301523-atr.pth", 
            "graphonomy": "exp-schp-201908301523-atr.pth",
            "schp_atr": "exp-schp-201908301523-atr.pth",
            
            # ì˜ë¥˜ ë¶„í•  ëª¨ë¸ë“¤
            "cloth_segmentation_u2net": "u2net.pth",
            "u2net": "u2net.pth",
            "cloth_segmentation_sam": "sam_vit_h_4b8939.pth",
            "sam_vit_h": "sam_vit_h_4b8939.pth",
            
            # í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë“¤
            "pose_estimation_openpose": "openpose.pth",
            "openpose": "openpose.pth",
            "body_pose_model": "openpose.pth",
            
            # ê°€ìƒ í”¼íŒ… ëª¨ë¸ë“¤
            "virtual_fitting_diffusion": "pytorch_model.bin",
            "pytorch_model": "pytorch_model.bin",
            "diffusion_model": "pytorch_model.bin",
            
            # ê¸°íƒ€ ëª¨ë¸ë“¤
            "geometric_matching_model": "gmm.pth",
            "cloth_warping_net": "tom.pth",
            "post_processing_enhance": "enhancement.pth",
            "quality_assessment_clip": "clip_g.pth"
        }

    def _find_via_pattern_matching(self, model_name: str, extensions: List[str]) -> Optional[Path]:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°"""
        try:
            for model_file in self.model_cache_dir.rglob("*"):
                if model_file.is_file() and model_file.suffix.lower() in extensions:
                    if model_name.lower() in model_file.name.lower():
                        self.logger.debug(f"ğŸ” íŒ¨í„´ ë§¤ì¹­: {model_name} â†’ {model_file}")
                        return model_file
            return None
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return None

    def _find_via_step_patterns(self, model_name: str) -> Optional[Path]:
        """Step ìš”ì²­ì‚¬í•­ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­"""
        try:
            if not STEP_REQUESTS_AVAILABLE:
                return None
                
            for step_name, step_req in self.step_requirements.items():
                if isinstance(step_req, dict):
                    step_model_name = step_req.get("model_name")
                    if step_model_name == model_name:
                        patterns = step_req.get("checkpoint_patterns", [])
                        for pattern in patterns:
                            import re
                            # ê°„ë‹¨í•œ glob íŒ¨í„´ì„ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜
                            regex_pattern = pattern.replace('*', '.*').replace('?', '.')
                            for model_file in self.model_cache_dir.rglob("*"):
                                if model_file.is_file() and re.search(regex_pattern, model_file.name):
                                    self.logger.debug(f"ğŸ¯ Step íŒ¨í„´ ë§¤ì¹­: {model_name} â†’ {model_file}")
                                    return model_file
            return None
        except Exception as e:
            self.logger.debug(f"Step íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_checkpoint_memory_usage(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE:
                if isinstance(checkpoint, dict):
                    # state_dictì¸ ê²½ìš°
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    # float32 ê¸°ì¤€ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê³„ì‚°
                    memory_mb = total_params * 4 / (1024 * 1024)
                    return memory_mb
                elif hasattr(checkpoint, 'parameters'):
                    # ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    memory_mb = total_params * 4 / (1024 * 1024)
                    return memory_mb
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ ê³ ê¸‰ ëª¨ë¸ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixinì—ì„œ self.model_loader.get_model_status() í˜¸ì¶œ"""
        try:
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                return {
                    "status": "loaded",
                    "device": cache_entry.device,
                    "memory_usage_mb": cache_entry.memory_usage_mb,
                    "last_used": cache_entry.last_access,
                    "load_time": cache_entry.load_time,
                    "access_count": cache_entry.access_count,
                    "model_type": type(cache_entry.model).__name__,
                    "loaded": True
                }
            elif model_name in self.model_configs:
                return {
                    "status": "registered",
                    "device": self.device,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": "Not Loaded",
                    "loaded": False
                }
            else:
                return {
                    "status": "not_found",
                    "device": None,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": None,
                    "loaded": False
                }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"status": "error", "error": str(e)}

    def get_step_model_status(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ëª¨ë¸ ìƒíƒœ ì¼ê´„ ì¡°íšŒ - BaseStepMixinì—ì„œ í˜¸ì¶œ"""
        try:
            step_models = {}
            if step_name in self.step_requirements:
                for model_name in self.step_requirements[step_name]:
                    step_models[model_name] = self.get_model_status(model_name)
            
            total_memory = sum(status.get("memory_usage_mb", 0) for status in step_models.values())
            loaded_count = sum(1 for status in step_models.values() if status.get("status") == "loaded")
            
            return {
                "step_name": step_name,
                "models": step_models,
                "total_models": len(step_models),
                "loaded_models": loaded_count,
                "total_memory_usage_mb": total_memory,
                "readiness_score": loaded_count / max(1, len(step_models))
            }
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {step_name}: {e}")
            return {"step_name": step_name, "error": str(e)}

    def preload_models_for_step(self, step_name: str, priority_models: Optional[List[str]] = None) -> bool:
        """Stepìš© ëª¨ë¸ë“¤ ì‚¬ì „ ë¡œë”© - BaseStepMixinì—ì„œ ì‹¤í–‰ ì „ ë¯¸ë¦¬ ì¤€ë¹„"""
        try:
            if step_name not in self.step_requirements:
                self.logger.warning(f"âš ï¸ Step ìš”êµ¬ì‚¬í•­ ì—†ìŒ: {step_name}")
                return False
            
            models_to_load = priority_models or list(self.step_requirements[step_name].keys())
            loaded_count = 0
            
            for model_name in models_to_load:
                try:
                    if model_name not in self.model_cache:
                        checkpoint = self.load_model(model_name)
                        if checkpoint:
                            loaded_count += 1
                            self.logger.info(f"âœ… ì‚¬ì „ ë¡œë”© ì™„ë£Œ: {model_name}")
                    else:
                        loaded_count += 1
                        self.logger.debug(f"ğŸ“¦ ì´ë¯¸ ë¡œë”©ë¨: {model_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            
            success_rate = loaded_count / len(models_to_load) if models_to_load else 0
            self.logger.info(f"ğŸ“Š {step_name} ì‚¬ì „ ë¡œë”© ì™„ë£Œ: {loaded_count}/{len(models_to_load)} ({success_rate:.1%})")
            return success_rate > 0.5  # 50% ì´ìƒ ì„±ê³µ ì‹œ True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨ {step_name}: {e}")
            return False

    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            if model_name in self.model_cache:
                del self.model_cache[model_name]
                
            if model_name in self.loaded_models:
                del self.loaded_models[model_name]
                
            if model_name in self.available_models:
                self.available_models[model_name]["loaded"] = False
                
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device in ["mps", "cuda"]:
                safe_mps_empty_cache()
                    
            gc.collect()
            
            self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
            self._trigger_model_event("model_unloaded", model_name)
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False

    # ==============================================
    # ğŸ”¥ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì§„ë‹¨ ë©”ì„œë“œë“¤
    # ==============================================

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë” ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ - BaseStepMixinì—ì„œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_memory = sum(cache_entry.memory_usage_mb for cache_entry in self.model_cache.values())
            
            # ë¡œë”© ì‹œê°„ í†µê³„
            load_times = list(self.load_times.values())
            avg_load_time = sum(load_times) / len(load_times) if load_times else 0
            
            return {
                "model_counts": {
                    "loaded": len(self.model_cache),
                    "registered": len(self.model_configs),
                    "available": len(self.available_models),
                    "total_found": self.performance_stats.get('total_models_found', 0)
                },
                "memory_usage": {
                    "total_mb": total_memory,
                    "average_per_model_mb": total_memory / len(self.model_cache) if self.model_cache else 0,
                    "device": self.device,
                    "available_memory_gb": self.memory_gb
                },
                "performance_stats": {
                    "cache_hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['models_loaded']),
                    "average_load_time_sec": avg_load_time,
                    "total_models_loaded": self.performance_stats['models_loaded'],
                    "checkpoint_loads": self.performance_stats.get('checkpoint_loads', 0),
                    "auto_detections": self.performance_stats.get('auto_detections', 0)
                },
                "step_interfaces": len(self.step_interfaces),
                "system_info": {
                    "conda_env": self.conda_env,
                    "is_m3_max": self.is_m3_max,
                    "torch_available": TORCH_AVAILABLE,
                    "mps_available": MPS_AVAILABLE
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    # ==============================================
    # ğŸ”¥ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ë° ì½œë°±
    # ==============================================

    def register_model_event_callback(self, event_type: str, callback: Callable) -> bool:
        """ëª¨ë¸ ì´ë²¤íŠ¸ ì½œë°± ë“±ë¡ - BaseStepMixinì—ì„œ ì´ë²¤íŠ¸ êµ¬ë…"""
        try:
            if event_type not in self._event_callbacks:
                self._event_callbacks[event_type] = []
            
            self._event_callbacks[event_type].append(callback)
            self.logger.info(f"âœ… ì´ë²¤íŠ¸ ì½œë°± ë“±ë¡: {event_type}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë²¤íŠ¸ ì½œë°± ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False

    def _trigger_model_event(self, event_type: str, model_name: str, **kwargs):
        """ëª¨ë¸ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°"""
        try:
            if event_type in self._event_callbacks:
                for callback in self._event_callbacks[event_type]:
                    try:
                        callback(model_name, **kwargs)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì´ë²¤íŠ¸ ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        except:
            pass

    # ==============================================
    # ğŸ”¥ auto_model_detector ì—°ë™ ë©”ì„œë“œë“¤
    # ==============================================

    def register_detected_models(self, detected_models: Dict[str, Any]) -> int:
        """ğŸ”¥ íƒì§€ëœ ëª¨ë¸ë“¤ ë“±ë¡ (auto_model_detector ì—°ë™)"""
        registered_count = 0
        try:
            for model_name, model_info in detected_models.items():
                try:
                    # íƒì§€ëœ ëª¨ë¸ ì •ë³´ë¥¼ ModelConfigë¡œ ë³€í™˜
                    if hasattr(model_info, 'path'):
                        config = ModelConfig(
                            name=model_name,
                            model_type=getattr(model_info, 'model_type', 'unknown'),
                            model_class=getattr(model_info, 'category', 'BaseModel'),
                            checkpoint_path=str(model_info.path),
                            file_size_mb=getattr(model_info, 'file_size_mb', 0.0),
                            metadata={
                                'auto_detected': True,
                                'confidence': getattr(model_info, 'confidence_score', 0.0),
                                'detection_time': time.time(),
                                'step_assignment': getattr(model_info, 'step_assignment', 'unknown')
                            }
                        )
                        
                        if self.register_model_config(model_name, config):
                            registered_count += 1
                            self.performance_stats['auto_detections'] += 1
                            
                            # ğŸ”¥ Step ìš”ì²­ëª… ë³„ì¹­ ë“±ë¡ (ì¤‘ìš”!)
                            self._register_model_aliases(model_name, model_info)
                            
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
        
        except Exception as e:
            self.logger.error(f"âŒ íƒì§€ëœ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        return registered_count
    
    def _register_model_aliases(self, model_name: str, model_info) -> None:
        """ğŸ”¥ Step ìš”ì²­ëª… ë³„ì¹­ ë“±ë¡ (ì¤‘ìš”!)"""
        try:
            # Stepë³„ ìš”ì²­ëª… ë§¤í•‘
            step_name = getattr(model_info, 'step_name', '')
            original_filename = getattr(model_info, 'original_filename', '')
            
            # ğŸ¯ ì‹¤ì œ ìš”ì²­ëª…ë“¤ ë“±ë¡
            aliases = []
            
            if 'exp-schp-201908301523-atr.pth' in original_filename:
                aliases.extend([
                    'human_parsing_schp_atr',
                    'human_parsing_graphonomy',
                    'schp_atr'
                ])
            elif 'u2net.pth' in original_filename:
                aliases.extend([
                    'cloth_segmentation_u2net',
                    'u2net'
                ])
            elif 'openpose.pth' in original_filename:
                aliases.extend([
                    'pose_estimation_openpose',
                    'openpose'
                ])
            elif 'pytorch_model.bin' in original_filename:
                aliases.extend([
                    'virtual_fitting_diffusion',
                    'pytorch_model'
                ])
            elif 'sam_vit_h_4b8939.pth' in original_filename:
                aliases.extend([
                    'cloth_segmentation_sam',
                    'sam_vit_h'
                ])
            
            # ë³„ì¹­ë“¤ ë“±ë¡
            for alias in aliases:
                if alias not in self.model_configs:
                    # ì›ë³¸ ì„¤ì • ë³µì‚¬í•´ì„œ ë³„ì¹­ ë“±ë¡
                    if model_name in self.model_configs:
                        original_config = self.model_configs[model_name]
                        alias_config = ModelConfig(
                            name=alias,
                            model_type=original_config.model_type,
                            model_class=original_config.model_class,
                            checkpoint_path=original_config.checkpoint_path,
                            file_size_mb=original_config.file_size_mb,
                            metadata={
                                **original_config.metadata,
                                'is_alias': True,
                                'alias_for': model_name
                            }
                        )
                        self.model_configs[alias] = alias_config
                        self.logger.debug(f"âœ… ë³„ì¹­ ë“±ë¡: {alias} â†’ {model_name}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³„ì¹­ ë“±ë¡ ì‹¤íŒ¨: {e}")

    def scan_and_register_all_models(self) -> int:
        """ğŸ”¥ ëª¨ë“  ëª¨ë¸ ìŠ¤ìº” ë° ìë™ ë“±ë¡"""
        try:
            registered_count = 0
            
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                # auto_model_detectorë¥¼ ì‚¬ìš©í•œ í¬ê´„ì  íƒì§€
                detected = comprehensive_model_detection(
                    enable_pytorch_validation=True,
                    enable_detailed_analysis=True,
                    prioritize_backend_models=True,
                    min_confidence=0.3
                )
                
                if detected:
                    registered_count += self.register_detected_models(detected)
                    self.logger.info(f"ğŸ” auto_model_detector íƒì§€: {len(detected)}ê°œ ëª¨ë¸")
            
            # ì¶”ê°€ë¡œ ì§ì ‘ ìŠ¤ìº” (ë†“ì¹œ ëª¨ë¸ë“¤ì„ ìœ„í•´)
            self._scan_available_models()
            
            self.logger.info(f"âœ… ì „ì²´ ëª¨ë¸ ìŠ¤ìº” ë° ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            return registered_count
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²´ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            return 0

    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë° í—¬í¼ ë©”ì„œë“œë“¤
    # ==============================================

    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            system_memory = get_memory_info()
            
            model_memory = sum(cache_entry.memory_usage_mb for cache_entry in self.model_cache.values())
            
            memory_info = {
                "system": system_memory,
                "models": {
                    "loaded_count": len(self.model_cache),
                    "total_memory_mb": model_memory,
                    "average_per_model_mb": model_memory / len(self.model_cache) if self.model_cache else 0,
                    "largest_model_mb": max((entry.memory_usage_mb for entry in self.model_cache.values()), default=0)
                },
                "device": self.device,
                "conda_env": self.conda_env,
                "is_m3_max": self.is_m3_max
            }
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    memory_info["gpu"] = {
                        "allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                        "reserved_mb": torch.cuda.memory_reserved() / (1024**2),
                        "max_allocated_mb": torch.cuda.max_memory_allocated() / (1024**2)
                    }
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'current_allocated_memory'):
                            memory_info["mps"] = {
                                "allocated_mb": torch.mps.current_allocated_memory() / (1024**2)
                            }
                    except:
                        pass
                
            return memory_info
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        memory_info = get_memory_info()
        
        return {
            "device": self.device,
            "conda_env": self.conda_env,
            "is_m3_max": self.is_m3_max,
            "memory": memory_info,
            "torch_available": TORCH_AVAILABLE,
            "mps_available": MPS_AVAILABLE,
            "numpy_available": NUMPY_AVAILABLE,
            "step_requirements_available": STEP_REQUESTS_AVAILABLE,
            "auto_detector_available": AUTO_MODEL_DETECTOR_AVAILABLE,
            "model_cache_dir": str(self.model_cache_dir),
            "loaded_models": len(self.model_cache),
            "available_models": len(self.available_models),
            "step_interfaces": len(self.step_interfaces),
            "performance_stats": self.performance_stats,
            "version": "19.0",
            "features": [
                "ìˆœìˆ˜ ëª¨ë¸ ë¡œë”©/ê´€ë¦¬ ì „ìš©",
                "Step íŒŒì¼ê³¼ì˜ ê¹”ë”í•œ ì¸í„°í˜ì´ìŠ¤",
                "ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜",
                "BaseStepMixin ì™„ë²½ í˜¸í™˜",
                "ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”© ì§‘ì¤‘",
                "ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°",
                "conda í™˜ê²½ ìš°ì„  ìµœì í™”",
                "M3 Max 128GB ìµœì í™”",
                "ë¹„ë™ê¸°/ë™ê¸° ì™„ì „ ì§€ì›",
                "í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±",
                "auto_model_detector ì™„ì „ ì—°ë™",
                "ìš”ì²­ëª…â†’íŒŒì¼ëª… ë§¤í•‘ ì™„ë²½"
            ]
        }

    def initialize(self) -> bool:
        """ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ - ğŸ”¥ auto_model_detector ì—°ë™ ê°•í™”"""
        try:
            self.logger.info("ğŸš€ ModelLoader v19.0 ì´ˆê¸°í™” ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_torch_cleanup()
            
            # ğŸ”¥ ì´ë¯¸ _initialize_components()ì—ì„œ ìë™ íƒì§€ê°€ ì‹¤í–‰ë˜ì—ˆì§€ë§Œ
            # initialize() í˜¸ì¶œ ì‹œ ì¶”ê°€ ê²€ì¦ ë° ë³´ì™„
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                try:
                    # í•µì‹¬ ëª¨ë¸ë“¤ ì¬í™•ì¸
                    critical_models = [
                        'cloth_segmentation_u2net',
                        'human_parsing_schp_atr', 
                        'pose_estimation_openpose',
                        'virtual_fitting_diffusion'
                    ]
                    
                    missing_models = []
                    for model_name in critical_models:
                        if model_name not in self.model_configs:
                            # ë³„ì¹­ìœ¼ë¡œë„ ì°¾ê¸° ì‹œë„
                            aliases = self._get_model_aliases(model_name)
                            if not any(alias in self.model_configs for alias in aliases):
                                missing_models.append(model_name)
                    
                    if missing_models:
                        self.logger.warning(f"âš ï¸ í•µì‹¬ ëª¨ë¸ ëˆ„ë½: {missing_models}")
                        
                        # ì¶”ê°€ íƒì§€ ì‹œë„
                        detected = quick_model_detection(
                            enable_pytorch_validation=True,
                            min_confidence=0.2,  # ë” ê´€ëŒ€í•œ ì„ê³„ê°’
                            prioritize_backend_models=True
                        )
                        
                        if detected:
                            additional_registered = self.register_detected_models(detected)
                            self.logger.info(f"ğŸ” ì¶”ê°€ íƒì§€ ì™„ë£Œ: {additional_registered}ê°œ ëª¨ë¸ ë“±ë¡")
                    else:
                        self.logger.info("âœ… ëª¨ë“  í•µì‹¬ ëª¨ë¸ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¶”ê°€ ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader v19.0 ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self._executor, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.model_cache.keys()):
                self.unload_model(model_name)
                
            # ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_torch_cleanup()
            
            self.logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì „ì—­ ì™„ì „í•œ ModelLoader v19.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader async initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        success = loader.initialize()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ì™„ì „í•œ ModelLoader v19.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜)
# ==============================================

def get_model_service() -> ModelLoader:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return get_global_model_loader()

def auto_detect_and_register_models() -> int:
    """ëª¨ë“  ëª¨ë¸ ìë™ íƒì§€ ë° ë“±ë¡"""
    try:
        loader = get_global_model_loader()
        return loader.scan_and_register_all_models()
    except Exception as e:
        logger.error(f"âŒ ìë™ íƒì§€ ë° ë“±ë¡ ì‹¤íŒ¨: {e}")
        return 0

def validate_all_checkpoints() -> Dict[str, bool]:
    """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        loader = get_global_model_loader()
        results = {}
        
        for model_name, config in loader.model_configs.items():
            if hasattr(config, 'checkpoint_path') and config.checkpoint_path:
                checkpoint_path = Path(config.checkpoint_path)
                results[model_name] = checkpoint_path.exists()
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {}

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        
        # Step ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ ë“±ë¡
        if step_requirements:
            loader.register_step_requirements(step_name, step_requirements)
        
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        # í´ë°±ìœ¼ë¡œ ì§ì ‘ ìƒì„±
        return StepModelInterface(get_global_model_loader(), step_name)

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        loader = get_global_model_loader()
        return loader.get_system_info()
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def register_model_config(name: str, config: Union[ModelConfig, Dict[str, Any]]) -> bool:
    """ì „ì—­ ëª¨ë¸ ì„¤ì • ë“±ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.register_model_config(name, config)

def list_all_models() -> List[Dict[str, Any]]:
    """ì „ì—­ ëª¨ë¸ ëª©ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.list_available_models()

# BaseStepMixin í˜¸í™˜ í•¨ìˆ˜ë“¤
def get_model_for_step(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        interface = loader.create_step_interface(step_name)
        return interface.get_model_sync(model_name)
    except Exception as e:
        logger.error(f"âŒ Step ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {step_name}: {e}")
        return None

async def get_model_for_step_async(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ë¹„ë™ê¸° ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        interface = loader.create_step_interface(step_name)
        return await interface.get_model(model_name)
    except Exception as e:
        logger.error(f"âŒ Step ëª¨ë¸ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨ {step_name}: {e}")
        return None

# ==============================================
# ğŸ”¥ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (Step íŒŒì¼ í˜¸í™˜ì„±)
# ==============================================

def preprocess_image(image, target_size=(512, 512), **kwargs):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if isinstance(image, str):
            from PIL import Image
            image = Image.open(image)
        
        if hasattr(image, 'resize'):
            image = image.resize(target_size)
        
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            if hasattr(image, 'convert'):
                image = image.convert('RGB')
            return transform(image)
        
        return image
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return image

def postprocess_segmentation(output, threshold=0.5):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if hasattr(output, 'squeeze'):
            output = output.squeeze()
        
        if threshold is not None:
            output = (output > threshold).astype(float)
        
        return output
    except Exception as e:
        logger.error(f"âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return output

def tensor_to_pil(tensor):
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        if hasattr(tensor, 'numpy'):
            arr = tensor.numpy()
        else:
            arr = tensor
        
        if len(arr.shape) == 3 and arr.shape[0] in [1, 3]:
            arr = arr.transpose(1, 2, 0)
        
        if arr.max() <= 1.0:
            arr = (arr * 255).astype('uint8')
        
        from PIL import Image
        return Image.fromarray(arr)
    except Exception as e:
        logger.error(f"âŒ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return tensor

def pil_to_tensor(image, device="cpu"):
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.ToTensor()
            tensor = transform(image)
            if device != "cpu":
                tensor = tensor.to(device)
            return tensor
        return image
    except Exception as e:
        logger.error(f"âŒ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return image

# Stepë³„ íŠ¹í™” ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def preprocess_pose_input(image, **kwargs):
    """í¬ì¦ˆ ì¶”ì •ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(368, 368), **kwargs)

def preprocess_human_parsing_input(image, **kwargs):
    """ì¸ì²´ íŒŒì‹±ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

def preprocess_cloth_segmentation_input(image, **kwargs):
    """ì˜ë¥˜ ë¶„í• ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(320, 320), **kwargs)

def preprocess_virtual_fitting_input(image, **kwargs):
    """ê°€ìƒ í”¼íŒ…ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ì •ì˜
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType',
    'ModelConfig',
    'StepModelConfig',
    'ModelCacheEntry',
    'StepPriority',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'cleanup_global_loader',
    
    # auto_model_detector ì—°ë™ í•¨ìˆ˜ë“¤
    'auto_detect_and_register_models',
    'validate_all_checkpoints',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_model_service',
    'create_step_interface',
    'get_device_info',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_model',
    'get_model_async',
    'register_model_config',
    'list_all_models',
    'get_model_for_step',
    'get_model_for_step_async',
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'tensor_to_pil',
    'pil_to_tensor',
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'preprocess_virtual_fitting_input',
    
    # ì•ˆì „í•œ í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_torch_cleanup',
    'get_memory_info',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'AUTO_MODEL_DETECTOR_AVAILABLE',
    'STEP_REQUESTS_AVAILABLE'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ í™•ì¸ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("âœ… ì™„ì „í•œ ModelLoader v19.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ”¥ í”„ë¡œë•ì…˜ ë ˆë²¨ ì™„ì„±íŒ")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„ - ë¹ ì§ì—†ëŠ” ì™„ì „ì²´")
logger.info("âœ… auto_model_detector ì™„ì „ ì—°ë™ - ìš”ì²­ëª…â†’íŒŒì¼ëª… ë§¤í•‘ ì™„ë²½")
logger.info("âœ… Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ì™„ì „ ê´€ë¦¬")
logger.info("âœ… BaseStepMixin 100% í˜¸í™˜ - ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„")
logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìë™ íƒì§€ ë° ë¡œë”©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("âœ… M3 Max 128GB ìµœì í™”")
logger.info("âœ… ë¹„ë™ê¸°/ë™ê¸° ì™„ì „ ì§€ì›")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("âœ… ëª¨ë“  ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ")
logger.info("âœ… ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜")
logger.info("=" * 80)

logger.info(f"ğŸ¯ í•µì‹¬ ì—­í• :")
logger.info(f"   - ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ íƒì§€ ë° ë¡œë”©")
logger.info(f"   - Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ê´€ë¦¬")
logger.info(f"   - ëª¨ë¸ ìºì‹± ë° ë©”ëª¨ë¦¬ ê´€ë¦¬")
logger.info(f"   - Step íŒŒì¼ë“¤ì—ê²Œ ê¹”ë”í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ")
logger.info(f"   - auto_model_detector ë§¤í•‘ í™œìš©")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
logger.info(f"   - auto_model_detector: {'âœ…' if AUTO_MODEL_DETECTOR_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step ìš”ì²­ì‚¬í•­: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
logger.info(f"   - Device: {DEFAULT_DEVICE}")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - conda í™˜ê²½: {'âœ…' if CONDA_ENV else 'âŒ'}")

memory_info = get_memory_info()
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë³´:")
logger.info(f"   - ì´ ë©”ëª¨ë¦¬: {memory_info['total_gb']:.1f}GB")
logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥: {memory_info['available_gb']:.1f}GB")
logger.info(f"   - ì‚¬ìš©ë¥ : {memory_info['percent']:.1f}%")

logger.info("=" * 80)
logger.info("ğŸš€ ì™„ì „í•œ ModelLoader v19.0 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… BaseStepMixinì—ì„œ model_loader ì†ì„±ìœ¼ë¡œ ì£¼ì…ë°›ì•„ ì‚¬ìš©")
logger.info("   âœ… Step íŒŒì¼ë“¤ì´ self.model_loader.load_model() ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥")
logger.info("   âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë§Œ ë¡œë”©í•˜ì—¬ Stepì—ê²Œ ì „ë‹¬")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ êµ¬í˜„ì€ ê° Step íŒŒì¼ì—ì„œ ë‹´ë‹¹")
logger.info("   âœ… ì™„ì „í•œ ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬ ë‹¬ì„±")
logger.info("   âœ… auto_model_detectorì™€ ì™„ë²½ ì—°ë™")
logger.info("   âœ… ìš”ì²­ëª…â†’íŒŒì¼ëª… ë§¤í•‘ ì™„ë²½ ì‘ë™")
logger.info("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° ì„±ëŠ¥")
logger.info("=" * 80)