# app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI - ì™„ì „ í†µí•© ModelLoader ì‹œìŠ¤í…œ v5.0 - ğŸ”¥ Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
========================================================================================

âœ… 'dict' object is not callable ê·¼ë³¸ ì›ì¸ í•´ê²°
âœ… SafeModelService í†µí•©ìœ¼ë¡œ ì•ˆì „í•œ í˜¸ì¶œ ë³´ì¥
âœ… NumPy 2.x ì™„ì „ í˜¸í™˜ì„± í•´ê²°
âœ… BaseStepMixin v3.3 ì™„ë²½ ì—°ë™
âœ… M3 Max 128GB ìµœì í™” ì™„ì„±
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… StepModelInterface ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥ ì™„ì „ í†µí•©
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ + ê°œì„ 

Author: MyCloset AI Team
Date: 2025-07-19
Version: 5.0 (Dict Callable Error Complete Fix)
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
import json
import pickle
import sqlite3
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import weakref

# ==============================================
# ğŸ”¥ NumPy 2.x í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
# ==============================================

# NumPy ë²„ì „ í™•ì¸ ë° ê°•ì œ ë‹¤ìš´ê·¸ë ˆì´ë“œ ì²´í¬
try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        logging.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€ë¨. NumPy 1.x ê¶Œì¥")
        logging.warning("ğŸ”§ í•´ê²°ë°©ë²•: conda install numpy=1.24.3 -y --force-reinstall")
        # NumPy 2.xì—ì„œë„ ë™ì‘í•˜ë„ë¡ í˜¸í™˜ì„± ì„¤ì •
        try:
            np.set_printoptions(legacy='1.25')
            logging.info("âœ… NumPy 2.x í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”")
        except:
            pass
    
    NUMPY_AVAILABLE = True
    
except ImportError as e:
    NUMPY_AVAILABLE = False
    logging.error(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    np = None

# ì•ˆì „í•œ PyTorch import (NumPy ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°)
try:
    # PyTorch import ì „ì— í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ì§€ì› í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
        logging.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    else:
        MPS_AVAILABLE = False
        DEFAULT_DEVICE = "cpu"
        logging.info("â„¹ï¸ CPU ëª¨ë“œ ì‚¬ìš©")
        
except ImportError as e:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    torch = None
    nn = None
    logging.warning(f"âš ï¸ PyTorch ì—†ìŒ: {e}")

# ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import cv2
    from PIL import Image, ImageEnhance
    CV_AVAILABLE = True
    PIL_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    PIL_AVAILABLE = False

# ì™¸ë¶€ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer, AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ Enum ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

class ModelFormat(Enum):
    """ğŸ”¥ ëª¨ë¸ í¬ë§· ì •ì˜ - main.pyì—ì„œ í•„ìˆ˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²° - SafeFunctionValidator í´ë˜ìŠ¤
# ==============================================

class SafeFunctionValidator:
    """
    ğŸ”¥ í•¨ìˆ˜/ë©”ì„œë“œ/ê°ì²´ í˜¸ì¶œ ì•ˆì „ì„± ê²€ì¦ í´ë˜ìŠ¤
    - Dict Callable ì˜¤ë¥˜ ê·¼ë³¸ ì›ì¸ í•´ê²°
    - ëª¨ë“  í˜¸ì¶œ ì „ì— íƒ€ì…ê³¼ callable ì—¬ë¶€ ì—„ê²© ê²€ì¦
    """
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """
        ê°ì²´ê°€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ ê²€ì¦
        
        Returns:
            (is_callable, reason, safe_callable)
        """
        try:
            # 1. None ì²´í¬
            if obj is None:
                return False, "Object is None", None
            
            # 2. ë”•ì…”ë„ˆë¦¬ ì²´í¬ (ê°€ì¥ ì¤‘ìš”!)
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            # 3. ê¸°ë³¸ ë°ì´í„° íƒ€ì… ì²´í¬
            if isinstance(obj, (str, int, float, bool, list, tuple, set)):
                return False, f"Object is basic data type {type(obj)}, not callable", None
            
            # 4. callable ì²´í¬
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            # 5. í•¨ìˆ˜/ë©”ì„œë“œ íƒ€ì…ë³„ ê²€ì¦
            import types
            if isinstance(obj, (types.FunctionType, types.MethodType, types.BuiltinFunctionType, types.BuiltinMethodType)):
                return True, "Valid function/method", obj
            
            # 6. í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ __call__ ë©”ì„œë“œ ì²´í¬
            if hasattr(obj, '__call__'):
                call_method = getattr(obj, '__call__')
                if callable(call_method):
                    return True, "Valid callable object with __call__", obj
            
            # 7. ê¸°íƒ€ callable ê°ì²´
            if callable(obj):
                return True, "Generic callable object", obj
            
            return False, f"Unknown callable validation failure for {type(obj)}", None
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """
        ì•ˆì „í•œ í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ
        
        Returns:
            (success, result, message)
        """
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            # ì‹¤ì œ í˜¸ì¶œ
            result = safe_obj(*args, **kwargs)
            return True, result, "Success"
            
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """
        ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ
        
        Returns:
            (success, result, message)
        """
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_async_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            # ë¹„ë™ê¸° í˜¸ì¶œ í™•ì¸
            if asyncio.iscoroutinefunction(safe_obj):
                result = await safe_obj(*args, **kwargs)
                return True, result, "Async success"
            else:
                # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                result = await asyncio.get_event_loop().run_in_executor(
                    None, safe_obj, *args
                )
                return True, result, "Sync-to-async success"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"
    
    @staticmethod
    def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
        """
        ì•ˆì „í•œ ì†ì„± ì ‘ê·¼ ë° í˜¸ì¶œ
        
        Returns:
            (success, result, message)
        """
        try:
            # 1. ê°ì²´ ìì²´ ê²€ì¦
            if obj is None:
                return False, None, "Object is None"
            
            # 2. ì†ì„± ì¡´ì¬ í™•ì¸
            if not hasattr(obj, attr_name):
                return False, None, f"Object has no attribute '{attr_name}'"
            
            # 3. ì†ì„± ê°€ì ¸ì˜¤ê¸°
            attr = getattr(obj, attr_name)
            
            # 4. ì†ì„±ì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if isinstance(attr, dict):
                if args or kwargs:
                    return False, None, f"Attribute '{attr_name}' is dict, cannot call with arguments"
                else:
                    return True, attr, f"Returned dict attribute '{attr_name}'"
            
            # 5. ì†ì„±ì´ callableì¸ ê²½ìš°
            if callable(attr):
                return SafeFunctionValidator.safe_call(attr, *args, **kwargs)
            
            # 6. ì†ì„±ì´ callableí•˜ì§€ ì•Šì€ ê²½ìš°
            if args or kwargs:
                return False, None, f"Attribute '{attr_name}' is not callable, cannot call with arguments"
            else:
                return True, attr, f"Returned non-callable attribute '{attr_name}'"
                
        except Exception as e:
            return False, None, f"Getattr call failed: {e}"

# ==============================================
# ğŸ”¥ ì™„ì „ ê°œì„ ëœ SafeConfig í´ë˜ìŠ¤
# ==============================================

class SafeConfig:
    """
    ğŸ”§ ì•ˆì „í•œ ì„¤ì • í´ë˜ìŠ¤ v5.0 - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    
    âœ… ë”•ì…”ë„ˆë¦¬ì™€ ê°ì²´ ì™„ì „ ë¶„ë¦¬
    âœ… callable ê°ì²´ ì•ˆì „ ì²˜ë¦¬
    âœ… VirtualFittingConfig ì™„ë²½ í˜¸í™˜ì„±
    âœ… get() ë©”ì„œë“œ ì•ˆì „ì„± ê°•í™”
    """
    
    def __init__(self, data: Any = None):
        self._data = {}
        self._original_data = data
        self._is_dict_source = False
        self._callable_methods = {}
        
        try:
            if data is None:
                self._data = {}
                
            elif isinstance(data, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° - ì™„ì „ ë³µì‚¬
                self._data = data.copy()
                self._is_dict_source = True
                
            elif hasattr(data, '__dict__'):
                # ì„¤ì • ê°ì²´ì¸ ê²½ìš° (VirtualFittingConfig ë“±)
                self._data = {}
                
                # ê³µê°œ ì†ì„±ë“¤ë§Œ ì•ˆì „í•˜ê²Œ ë³µì‚¬
                for attr_name in dir(data):
                    if not attr_name.startswith('_'):
                        try:
                            attr_value = getattr(data, attr_name)
                            
                            # callable ë©”ì„œë“œëŠ” ë³„ë„ ì €ì¥
                            if callable(attr_value):
                                self._callable_methods[attr_name] = attr_value
                            else:
                                # ì¼ë°˜ ì†ì„±ë§Œ _dataì— ì €ì¥
                                self._data[attr_name] = attr_value
                                
                        except Exception:
                            pass
                            
            elif callable(data):
                # callable ê°ì²´ì¸ ê²½ìš° - í˜¸ì¶œí•˜ì§€ ì•Šê³  ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©
                logger.warning("âš ï¸ callable ì„¤ì • ê°ì²´ ê°ì§€ë¨, ë¹ˆ ì„¤ì •ìœ¼ë¡œ ì²˜ë¦¬")
                self._data = {}
                self._callable_methods = {'original_callable': data}
                
            else:
                # ê¸°íƒ€ ê²½ìš°
                self._data = {}
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¤ì • ê°ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}, ë¹ˆ ì„¤ì • ì‚¬ìš©")
            self._data = {}
            self._callable_methods = {}
        
        # ì†ì„±ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì„¤ì •
        self._setup_attributes()
    
    def _setup_attributes(self):
        """ì•ˆì „í•œ ì†ì„± ì„¤ì •"""
        for key, value in self._data.items():
            try:
                if isinstance(key, str) and key.isidentifier() and not hasattr(self, key):
                    setattr(self, key, value)
            except Exception:
                pass
    
    def get(self, key: str, default=None):
        """ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ get ë©”ì„œë“œ ì§€ì› - ì•ˆì „ì„± ê°•í™”"""
        try:
            # 1. ì¼ë°˜ ë°ì´í„°ì—ì„œ ì°¾ê¸°
            if key in self._data:
                return self._data[key]
            
            # 2. callable ë©”ì„œë“œì—ì„œ ì°¾ê¸° (í˜¸ì¶œí•˜ì§€ ì•Šê³  ë°˜í™˜)
            if key in self._callable_methods:
                logger.debug(f"âš ï¸ get() í˜¸ì¶œì—ì„œ callable ë©”ì„œë“œ ë°œê²¬: {key}")
                return default  # callableì€ ê¸°ë³¸ê°’ ë°˜í™˜
            
            # 3. ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
            if hasattr(self, key):
                attr = getattr(self, key)
                if not callable(attr):  # callable ì†ì„±ì€ ì œì™¸
                    return attr
            
            return default
            
        except Exception as e:
            logger.warning(f"âš ï¸ SafeConfig.get() ì˜¤ë¥˜: {e}")
            return default
    
    def safe_call_method(self, method_name: str, *args, **kwargs):
        """ì €ì¥ëœ callable ë©”ì„œë“œ ì•ˆì „ í˜¸ì¶œ"""
        if method_name in self._callable_methods:
            method = self._callable_methods[method_name]
            success, result, message = SafeFunctionValidator.safe_call(method, *args, **kwargs)
            if success:
                return result
            else:
                logger.warning(f"âš ï¸ ë©”ì„œë“œ í˜¸ì¶œ ì‹¤íŒ¨ {method_name}: {message}")
                return None
        else:
            logger.warning(f"âš ï¸ ë©”ì„œë“œ ì—†ìŒ: {method_name}")
            return None
    
    def __getitem__(self, key):
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        return self.get(key, None)
    
    def __setitem__(self, key, value):
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì„¤ì •"""
        if callable(value):
            self._callable_methods[key] = value
        else:
            self._data[key] = value
            self._setup_attributes()
    
    def __contains__(self, key):
        """in ì—°ì‚°ì ì§€ì›"""
        return key in self._data or key in self._callable_methods
    
    def keys(self):
        """í‚¤ ëª©ë¡"""
        return list(self._data.keys()) + list(self._callable_methods.keys())
    
    def values(self):
        """ê°’ ëª©ë¡ (callable ì œì™¸)"""
        return self._data.values()
    
    def items(self):
        """ì•„ì´í…œ ëª©ë¡ (callable ì œì™¸)"""
        return self._data.items()
    
    def update(self, other):
        """ì—…ë°ì´íŠ¸"""
        if isinstance(other, dict):
            for key, value in other.items():
                self[key] = value
        elif isinstance(other, SafeConfig):
            self._data.update(other._data)
            self._callable_methods.update(other._callable_methods)
            self._setup_attributes()
    
    def get_callable_methods(self):
        """ì €ì¥ëœ callable ë©”ì„œë“œ ëª©ë¡"""
        return list(self._callable_methods.keys())
    
    def __str__(self):
        return f"SafeConfig(data={self._data}, callables={list(self._callable_methods.keys())})"
    
    def __repr__(self):
        return self.__str__()
    
    def __bool__(self):
        return bool(self._data) or bool(self._callable_methods)

# ==============================================
# ğŸ”¥ SafeModelService í†µí•© í´ë˜ìŠ¤
# ==============================================

class SafeModelService:
    """
    ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²° í†µí•© ë²„ì „
    ëª¨ë“  ëª¨ë¸ ê´€ë ¨ ì‘ì—…ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    """
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.warmup_status = {}
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡ - ëª¨ë“  íƒ€ì… ì•ˆì „ ì²˜ë¦¬"""
        try:
            with self.lock:
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° callable wrapperë¡œ ê°ì‹¸ê¸°
                if isinstance(model, dict):
                    self.models[name] = self._create_dict_wrapper(model)
                    self.logger.info(f"ğŸ“ ë”•ì…”ë„ˆë¦¬ ëª¨ë¸ì„ callable wrapperë¡œ ë“±ë¡: {name}")
                    
                elif callable(model):
                    # ì´ë¯¸ callableí•œ ê²½ìš° ê·¸ëŒ€ë¡œ ë“±ë¡
                    self.models[name] = model
                    self.logger.info(f"ğŸ“ callable ëª¨ë¸ ë“±ë¡: {name}")
                    
                else:
                    # ê¸°íƒ€ ê°ì²´ëŠ” wrapperë¡œ ê°ì‹¸ê¸°
                    self.models[name] = self._create_object_wrapper(model)
                    self.logger.info(f"ğŸ“ ê°ì²´ ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def _create_dict_wrapper(self, model_dict: Dict[str, Any]) -> Callable:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ callable wrapperë¡œ ë³€í™˜"""
        
        class DictModelWrapper:
            """ë”•ì…”ë„ˆë¦¬ ëª¨ë¸ì„ callableë¡œ ë§Œë“œëŠ” ë˜í¼"""
            
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.type = data.get('type', 'dict_model')
                self.logger = logging.getLogger(f"DictModelWrapper.{self.name}")
            
            def __call__(self, *args, **kwargs):
                """ëª¨ë¸ì„ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œ"""
                try:
                    self.logger.debug(f"ğŸ”„ DictModelWrapper í˜¸ì¶œ: {self.name}")
                    return {
                        'status': 'success',
                        'model_name': self.name,
                        'model_type': self.type,
                        'result': f'mock_result_for_{self.name}',
                        'data': self.data,
                        'input_args': len(args),
                        'input_kwargs': list(kwargs.keys()) if kwargs else []
                    }
                except Exception as e:
                    self.logger.error(f"âŒ DictModelWrapper í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    return {
                        'status': 'error', 
                        'error': str(e),
                        'model_name': self.name
                    }
            
            def get_info(self):
                """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
                return self.data.copy()
            
            def is_loaded(self):
                """ë¡œë“œ ìƒíƒœ í™•ì¸"""
                return True
            
            def warmup(self):
                """ì›Œë°ì—…"""
                self.logger.info(f"ğŸ”¥ {self.name} ì›Œë°ì—… ì™„ë£Œ")
                return True
        
        return DictModelWrapper(model_dict)
    
    def _create_object_wrapper(self, obj: Any) -> Callable:
        """ì¼ë°˜ ê°ì²´ë¥¼ callable wrapperë¡œ ë³€í™˜"""
        
        class ObjectWrapper:
            """ì¼ë°˜ ê°ì²´ë¥¼ callableë¡œ ë§Œë“œëŠ” ë˜í¼"""
            
            def __init__(self, wrapped_obj: Any):
                self.wrapped_obj = wrapped_obj
                self.name = getattr(wrapped_obj, 'name', str(type(wrapped_obj).__name__))
                self.type = type(wrapped_obj).__name__
                self.logger = logging.getLogger(f"ObjectWrapper.{self.name}")
            
            def __call__(self, *args, **kwargs):
                """ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œ"""
                try:
                    # ì›ë³¸ ê°ì²´ê°€ callableì¸ ê²½ìš°
                    if callable(self.wrapped_obj):
                        return self.wrapped_obj(*args, **kwargs)
                    
                    # callableí•˜ì§€ ì•Šì€ ê²½ìš° mock ì‘ë‹µ
                    return {
                        'status': 'success',
                        'model_name': self.name,
                        'model_type': self.type,
                        'result': f'mock_result_for_{self.name}',
                        'wrapped_type': self.type
                    }
                    
                except Exception as e:
                    self.logger.error(f"âŒ ObjectWrapper í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    return {
                        'status': 'error',
                        'error': str(e),
                        'model_name': self.name
                    }
            
            def __getattr__(self, name):
                """ì†ì„± ì ‘ê·¼ì„ ì›ë³¸ ê°ì²´ë¡œ ìœ„ì„"""
                return getattr(self.wrapped_obj, name)
        
        return ObjectWrapper(obj)
    
    async def warmup_model(self, name: str) -> bool:
        """ì•ˆì „í•œ ëª¨ë¸ ì›Œë°ì—…"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    self.warmup_status[name] = False
                    return False
                
                model = self.models[name]
                
                # ì›Œë°ì—… ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°
                if hasattr(model, 'warmup'):
                    success, result, message = self.validator.safe_getattr_call(model, 'warmup')
                    if success:
                        self.logger.info(f"âœ… ëª¨ë¸ ì›Œë°ì—… ì„±ê³µ: {name}")
                        self.warmup_status[name] = True
                        return True
                
                # warmup ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
                success, result, message = self.validator.safe_call(model)
                if success:
                    self.logger.info(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    self.warmup_status[name] = True
                    return True
                
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {name} - {message}")
                self.warmup_status[name] = False
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì˜¤ë¥˜ {name}: {e}")
            self.warmup_status[name] = False
            return False
    
    async def call_model(self, name: str, *args, **kwargs) -> Any:
        """ì•ˆì „í•œ ëª¨ë¸ í˜¸ì¶œ"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                success, result, message = await self.validator.safe_async_call(model, *args, **kwargs)
                
                if success:
                    self.logger.debug(f"âœ… ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            return None
    
    def get_model_status(self, name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            with self.lock:
                if name not in self.models:
                    return {'status': 'not_registered', 'warmup': False}
                
                model = self.models[name]
                return {
                    'status': 'registered',
                    'warmup': self.warmup_status.get(name, False),
                    'type': type(model).__name__,
                    'callable': callable(model),
                    'has_warmup': hasattr(model, 'warmup')
                }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {name}: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = self.get_model_status(name)
                return result
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ Step ìš”ì²­ì‚¬í•­ ì—°ë™ (ë‚´ì¥ ê¸°ë³¸ ìš”ì²­ì‚¬í•­)
# ==============================================

# ğŸ”¥ ë‚´ì¥ ê¸°ë³¸ ìš”ì²­ì‚¬í•­ (step_model_requests.py ë‚´ìš© ì¼ë¶€)
STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

class StepModelRequestAnalyzer:
    @staticmethod
    def get_step_request_info(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class BaseModel(nn.Module if TORCH_AVAILABLE else object):
    """ê¸°ë³¸ AI ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self):
        if TORCH_AVAILABLE:
            super().__init__()
        self.model_name = "BaseModel"
        self.device = "cpu"
    
    def forward(self, x):
        return x

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - Step 01"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            
            # ê°„ë‹¨í•œ ë°±ë³¸ êµ¬ì„±
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            # ë¶„ë¥˜ í—¤ë“œ
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - Step 02"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            
            # VGG ìŠ¤íƒ€ì¼ ë°±ë³¸
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            # PAF ë° íˆíŠ¸ë§µ í—¤ë“œ
            self.paf_head = nn.Conv2d(512, 38, 1)  # 19 limbs * 2
            self.heatmap_head = nn.Conv2d(512, 19, 1)  # 18 keypoints + 1 background
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - Step 03"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            
            # ì¸ì½”ë”
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            # ë””ì½”ë”
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - Step 04"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)  # 6ê°œ ì œì–´ì  * 3
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]  # ì²« 3ì±„ë„ë§Œ
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

    class HRVITONModel(nn.Module):
        """HR-VITON ê°€ìƒ í”¼íŒ… ëª¨ë¸ - Step 06"""
        
        def __init__(self, input_nc=3, output_nc=3, ngf=64):
            super().__init__()
            
            # U-Net ìŠ¤íƒ€ì¼ ìƒì„±ê¸°
            self.encoder = nn.Sequential(
                nn.Conv2d(input_nc * 2, ngf, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, ngf * 2, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 2, ngf * 4, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 4, ngf * 8, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, output_nc, 3, 1, 1), nn.Tanh()
            )
            
            # ì–´í…ì…˜ ëª¨ë“ˆ
            self.attention = nn.Sequential(
                nn.Conv2d(input_nc * 2, 32, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
            )
        
        def forward(self, person_img, cloth_img, **kwargs):
            combined_input = torch.cat([person_img, cloth_img], dim=1)
            
            features = self.encoder(combined_input)
            generated = self.decoder(features)
            
            attention_map = self.attention(combined_input)
            result = generated * attention_map + person_img * (1 - attention_map)
            
            return {
                'generated_image': result,
                'attention_map': attention_map,
                'warped_cloth': cloth_img,
                'intermediate': generated
            }
else:
    # PyTorch ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤ë“¤
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel
    HRVITONModel = BaseModel

# ==============================================
# ğŸ”¥ ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì - M3 Max íŠ¹í™” (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class DeviceManager:
    """M3 Max íŠ¹í™” ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                devices.append("mps")
                self.logger.info("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            if torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - M3 Max 128GB íŠ¹í™” (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - M3 Max íŠ¹í™”"""
    
    def __init__(self, device: str = "mps", memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)  # 128GB ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ - M3 Max ìµœì í™”"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            threshold = 4.0 if self.is_m3_max else 2.0
            return available_memory < threshold
        except Exception:
            return False

# ==============================================
# ğŸ”¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

def preprocess_image(
    image: Union[Image.Image, np.ndarray, torch.Tensor],
    target_size: Tuple[int, int] = (512, 512),
    device: str = "mps",
    normalize: bool = True,
    to_tensor: bool = True
) -> torch.Tensor:
    """
    ğŸ”¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ - Step í´ë˜ìŠ¤ë“¤ì—ì„œ ì‚¬ìš©
    
    Args:
        image: ì…ë ¥ ì´ë¯¸ì§€ (PIL.Image, numpy array, tensor)
        target_size: ëª©í‘œ í¬ê¸° (height, width)
        device: ë””ë°”ì´ìŠ¤ ("mps", "cuda", "cpu")
        normalize: ì •ê·œí™” ì—¬ë¶€ (0-1 ë²”ìœ„ë¡œ)
        to_tensor: í…ì„œë¡œ ë³€í™˜ ì—¬ë¶€
    
    Returns:
        torch.Tensor: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ
    """
    try:
        # 1. PIL Imageë¡œ ë³€í™˜
        if isinstance(image, torch.Tensor):
            if image.dim() == 4:
                image = image.squeeze(0)
            if image.dim() == 3 and image.shape[0] == 3:
                image = image.permute(1, 2, 0)
            image = image.cpu().numpy()
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8)
            image = Image.fromarray(image)
        elif isinstance(image, np.ndarray):
            if image.ndim == 3 and image.shape[2] == 3:
                image = Image.fromarray(image.astype(np.uint8))
            else:
                image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # 2. RGB ë³€í™˜
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # 3. í¬ê¸° ì¡°ì •
        if target_size != image.size:
            image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # 4. numpy ë°°ì—´ë¡œ ë³€í™˜
        img_array = np.array(image).astype(np.float32)
        
        # 5. ì •ê·œí™”
        if normalize:
            img_array = img_array / 255.0
        
        # 6. í…ì„œ ë³€í™˜
        if to_tensor and TORCH_AVAILABLE:
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
            img_tensor = img_tensor.to(device)
            return img_tensor
        else:
            return img_array
            
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ í¬ê¸° í…ì„œ ë°˜í™˜
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)

def postprocess_segmentation(output: torch.Tensor, threshold: float = 0.5) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í›„ì²˜ë¦¬"""
    try:
        if isinstance(output, torch.Tensor):
            output = output.cpu().numpy()
        
        if output.ndim == 4:
            output = output.squeeze(0)
        if output.ndim == 3:
            output = output.squeeze(0)
            
        # ì„ê³„ê°’ ì ìš©
        binary_mask = (output > threshold).astype(np.uint8) * 255
        return binary_mask
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return np.zeros((512, 512), dtype=np.uint8)

def preprocess_pose_input(image: np.ndarray, target_size: Tuple[int, int] = (368, 368)) -> torch.Tensor:
    """í¬ì¦ˆ ì¶”ì •ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_human_parsing_input(image: np.ndarray, target_size: Tuple[int, int] = (512, 512)) -> torch.Tensor:
    """ì¸ê°„ íŒŒì‹±ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_cloth_segmentation_input(image: np.ndarray, target_size: Tuple[int, int] = (320, 320)) -> torch.Tensor:
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        if tensor.dim() == 3:
            tensor = tensor.permute(1, 2, 0)
        
        tensor = tensor.cpu().numpy()
        if tensor.dtype != np.uint8:
            tensor = (tensor * 255).astype(np.uint8)
        
        return Image.fromarray(tensor)
    except Exception as e:
        logger.error(f"í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return Image.new('RGB', (512, 512), color='black')

def pil_to_tensor(image: Image.Image, device: str = "mps") -> torch.Tensor:
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
    try:
        img_array = np.array(image).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
        return tensor.to(device)
    except Exception as e:
        logger.error(f"PIL->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return torch.zeros(1, 3, 512, 512, device=device)

# ==============================================
# ğŸ”¥ ì™„ì „ ê°œì„ ëœ StepModelInterface - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

class StepModelInterface:
    """
    ğŸ”¥ Step í´ë˜ìŠ¤ë“¤ì„ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    SafeModelService í†µí•©ìœ¼ë¡œ ëª¨ë“  í˜¸ì¶œ ì•ˆì „ì„± ë³´ì¥
    """
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        """ğŸ”¥ ì™„ì „ ì•ˆì „í•œ ìƒì„±ì"""
        
        # ğŸ”¥ ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.model_loader = model_loader
        self.step_name = step_name
        
        # ğŸ”¥ logger ì†ì„± ì•ˆì „í•˜ê²Œ ì„¤ì •
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ğŸ”¥ ì•ˆì „í•œ ì†ì„± ì¶”ì¶œ (callable ê²€ì¦ í¬í•¨)
        self.device = getattr(model_loader, 'device', 'mps')
        self.model_cache_dir = Path(getattr(model_loader, 'model_cache_dir', './ai_models'))
        
        # ğŸ”¥ SafeModelService í†µí•©
        self.model_service = SafeModelService()
        
        # ğŸ”¥ ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        
        # ğŸ”¥ Stepë³„ ëª¨ë¸ ì„¤ì •
        self.recommended_models = self._get_recommended_models()
        self.access_count = 0
        self.last_used = time.time()
        
        # ğŸ”¥ ModelLoader ë©”ì„œë“œ ê°€ìš©ì„± ì²´í¬ - Dict Callable ë°©ì§€
        self.has_async_loader = self._safe_check_method(model_loader, 'load_model_async')
        self.has_sync_wrapper = self._safe_check_method(model_loader, '_load_model_sync_wrapper')
        
        # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        try:
            self.model_paths = self._setup_model_paths()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_paths = self._get_fallback_model_paths()
        
        # ğŸ”¥ Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘ ì„¤ì •
        self.step_model_mapping = self._get_step_model_mapping()
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (SafeModelService í†µí•©)")
        self.logger.info(f"ğŸ”§ Device: {self.device}, Cache Dir: {self.model_cache_dir}")
        self.logger.info(f"ğŸ“¦ ì¶”ì²œ ëª¨ë¸: {self.recommended_models}")
    
    def _safe_check_method(self, obj: Any, method_name: str) -> bool:
        """ë©”ì„œë“œ ì¡´ì¬ ë° callable ì—¬ë¶€ ì•ˆì „ í™•ì¸"""
        try:
            if obj is None:
                return False
            
            if not hasattr(obj, method_name):
                return False
            
            method = getattr(obj, method_name)
            is_callable, reason, safe_method = SafeFunctionValidator.validate_callable(method, f"check_{method_name}")
            
            if is_callable:
                self.logger.debug(f"âœ… {method_name} ë©”ì„œë“œ ì‚¬ìš© ê°€ëŠ¥")
                return True
            else:
                self.logger.debug(f"âš ï¸ {method_name} ë©”ì„œë“œ ì‚¬ìš© ë¶ˆê°€: {reason}")
                return False
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {method_name} ë©”ì„œë“œ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_u2net", "graphonomy"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose", "mediapipe_pose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "u2net", "cloth_segmentation"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network", "geometric_matching"],
            "ClothWarpingStep": ["cloth_warping_net", "tom_final", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "diffusion_pipeline"],
            "PostProcessingStep": ["srresnet_x4", "denoise_net", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip", "image_quality"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    def _setup_model_paths(self) -> Dict[str, str]:
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì‹¤ì œ ë°œê²¬ëœ íŒŒì¼ë“¤ ê¸°ë°˜"""
        base_path = self.model_cache_dir
        
        return {
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Human Parsing Models
            'graphonomy': str(base_path / "checkpoints" / "human_parsing" / "schp_atr.pth"),
            'human_parsing_graphonomy': str(base_path / "checkpoints" / "human_parsing" / "schp_atr.pth"),
            'human_parsing_u2net': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Pose Estimation Models  
            'openpose': str(base_path / "openpose"),
            'pose_estimation_openpose': str(base_path / "openpose"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Cloth Segmentation Models
            'u2net': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'u2net_cloth_seg': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'cloth_segmentation_u2net': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Virtual Fitting Models
            'ootdiffusion': str(base_path / "OOTDiffusion"),
            'stable_diffusion': str(base_path / "OOTDiffusion"),
            'diffusion_pipeline': str(base_path / "OOTDiffusion"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Geometric Matching
            'geometric_matching_gmm': str(base_path / "checkpoints" / "step_04" / "step_04_geometric_matching_base" / "geometric_matching_base.pth"),
            'tps_network': str(base_path / "checkpoints" / "step_04" / "step_04_tps_network" / "tps_network.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ ê¸°íƒ€ ëª¨ë¸ë“¤
            'clip': str(base_path / "clip-vit-base-patch32"),
            'quality_assessment_clip': str(base_path / "clip-vit-base-patch32"),
            'srresnet_x4': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
        }
    
    def _get_fallback_model_paths(self) -> Dict[str, str]:
        """í´ë°± ëª¨ë¸ ê²½ë¡œ"""
        return {
            'default_model': str(self.model_cache_dir / "default_model.pth"),
            'fallback_model': str(self.model_cache_dir / "fallback_model.pth")
        }
    
    def _get_step_model_mapping(self) -> Dict[str, Dict[str, Any]]:
        """Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘"""
        return {
            'HumanParsingStep': {
                'primary': 'human_parsing_graphonomy',
                'models': ['graphonomy', 'human_parsing_u2net']
            },
            'PoseEstimationStep': {
                'primary': 'pose_estimation_openpose',
                'models': ['openpose']
            },
            'ClothSegmentationStep': {
                'primary': 'u2net_cloth_seg',
                'models': ['u2net', 'cloth_segmentation_u2net']
            },
            'GeometricMatchingStep': {
                'primary': 'geometric_matching_gmm',
                'models': ['tps_network']
            },
            'VirtualFittingStep': {
                'primary': 'ootdiffusion',
                'models': ['stable_diffusion', 'diffusion_pipeline']
            },
            'PostProcessingStep': {
                'primary': 'srresnet_x4',
                'models': ['enhancement', 'denoise_net']
            },
            'QualityAssessmentStep': {
                'primary': 'quality_assessment_clip',
                'models': ['clip']
            }
        }
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ğŸ”¥ ëª¨ë¸ ë¡œë“œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ëª¨ë¸ëª… ê²°ì •
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                self.access_count += 1
                self.last_used = time.time()
                self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self.loaded_models[model_name]
            
            # ğŸ”¥ SafeModelServiceë¥¼ í†µí•œ ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ
            model = await self._safe_load_model_via_service(model_name)
            
            if model:
                self.loaded_models[model_name] = model
                self.access_count += 1
                self.last_used = time.time()
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return model
            else:
                # í´ë°± ëª¨ë¸ ìƒì„±
                fallback = self._create_smart_fallback_model(model_name)
                self.loaded_models[model_name] = fallback
                self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            # ìµœì¢… í´ë°±
            fallback = self._create_smart_fallback_model(model_name)
            self.loaded_models[model_name] = fallback
            return fallback
    
    async def _safe_load_model_via_service(self, model_name: str) -> Optional[Any]:
        """ğŸ”¥ SafeModelServiceë¥¼ í†µí•œ ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            # 1. SafeModelServiceì— ëª¨ë¸ ë“±ë¡ ì‹œë„
            model_dict = {
                'name': model_name,
                'type': self.step_name,
                'device': self.device,
                'path': self.model_paths.get(model_name, 'unknown'),
                'step_mapping': self.step_model_mapping.get(self.step_name, {})
            }
            
            # SafeModelServiceì— ë“±ë¡
            registration_success = self.model_service.register_model(model_name, model_dict)
            
            if registration_success:
                # ë“±ë¡ëœ ëª¨ë¸ í˜¸ì¶œ
                model = await self.model_service.call_model(model_name)
                if model:
                    return model
            
            # 2. ê¸°ì¡´ ModelLoader ë°©ì‹ ì‹œë„ (ì•ˆì „í•˜ê²Œ)
            return await self._legacy_safe_load_model(model_name)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ SafeModelService ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _legacy_safe_load_model(self, model_name: str) -> Optional[Any]:
        """ğŸ”¥ ê¸°ì¡´ ModelLoader ë°©ì‹ ì•ˆì „í•œ ë¡œë“œ"""
        try:
            # ë¹„ë™ê¸° ë¡œë” ì‚¬ìš© ì‹œë„
            if self.has_async_loader:
                load_async_method = getattr(self.model_loader, 'load_model_async', None)
                success, result, message = await SafeFunctionValidator.safe_async_call(
                    load_async_method, model_name
                )
                if success:
                    return result
                else:
                    self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨: {message}")
            
            # ë™ê¸° ë˜í¼ ì‚¬ìš© ì‹œë„
            if self.has_sync_wrapper:
                sync_wrapper_method = getattr(self.model_loader, '_load_model_sync_wrapper', None)
                success, result, message = SafeFunctionValidator.safe_call(
                    sync_wrapper_method, model_name, {}
                )
                if success:
                    return result
                else:
                    self.logger.warning(f"âš ï¸ ë™ê¸° ë˜í¼ ì‹¤íŒ¨: {message}")
            
            # ê¸°ë³¸ load_model ë©”ì„œë“œ ì‹œë„
            if hasattr(self.model_loader, 'load_model'):
                load_model_method = getattr(self.model_loader, 'load_model', None)
                success, result, message = await SafeFunctionValidator.safe_async_call(
                    load_model_method, model_name
                )
                if success:
                    return result
                else:
                    self.logger.warning(f"âš ï¸ ê¸°ë³¸ ë¡œë“œ ì‹¤íŒ¨: {message}")
            
            # ì§ì ‘ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
            return self._direct_model_load(model_name)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ Legacy ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _direct_model_load(self, model_name: str) -> Optional[Any]:
        """ì§ì ‘ ëª¨ë¸ íŒŒì¼ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ê²½ë¡œì—ì„œ ì°¾ê¸°
            if model_name in self.model_paths:
                model_path = Path(self.model_paths[model_name])
                if model_path.exists() and model_path.stat().st_size > 1024:
                    self.logger.info(f"ğŸ“‚ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                    try:
                        # ì•ˆì „í•œ ì„í¬íŠ¸
                        if TORCH_AVAILABLE:
                            model = torch.load(model_path, map_location=self.device)
                            return model
                        else:
                            self.logger.warning("âš ï¸ PyTorchê°€ ì—†ì–´ì„œ ëª¨ë¸ ë¡œë“œ ë¶ˆê°€")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_smart_fallback_model(self, model_name: str) -> Any:
        """ğŸ”¥ ìŠ¤ë§ˆíŠ¸ í´ë°± ëª¨ë¸ ìƒì„± - Stepë³„ íŠ¹í™”"""
        
        class SmartMockModel:
            """ìŠ¤ë§ˆíŠ¸ Mock AI ëª¨ë¸ - Stepë³„ íŠ¹í™” ì¶œë ¥"""
            
            def __init__(self, name: str, device: str, step_name: str):
                self.name = name
                self.device = device
                self.step_name = step_name
                self.model_type = self._detect_model_type(name, step_name)
                self.is_loaded = True
                self.eval_mode = True
                
            def _detect_model_type(self, name: str, step_name: str) -> str:
                """ëª¨ë¸ íƒ€ì… ê°ì§€"""
                if 'human_parsing' in name or 'HumanParsing' in step_name:
                    return 'human_parsing'
                elif 'pose' in name or 'Pose' in step_name:
                    return 'pose_estimation'
                elif 'segmentation' in name or 'u2net' in name or 'Segmentation' in step_name:
                    return 'segmentation'
                elif 'geometric' in name or 'Geometric' in step_name:
                    return 'geometric_matching'
                elif 'diffusion' in name or 'ootd' in name or 'Fitting' in step_name:
                    return 'diffusion'
                else:
                    return 'general'
            
            def __call__(self, *args, **kwargs):
                """ëª¨ë¸ì„ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œ"""
                return self.forward(*args, **kwargs)
            
            def forward(self, *args, **kwargs):
                """Stepë³„ íŠ¹í™” Mock ì¶œë ¥"""
                try:
                    # ê¸°ë³¸ í¬ê¸° ì„¤ì •
                    height, width = 512, 512
                    batch_size = 1
                    
                    # Stepë³„ íŠ¹í™” ì¶œë ¥
                    if self.model_type == 'human_parsing':
                        # 20ê°œ í´ë˜ìŠ¤ ì¸ê°„ íŒŒì‹±
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 20, height, width), device='cpu')
                        else:
                            return np.zeros((batch_size, 20, height, width), dtype=np.float32)
                    elif self.model_type == 'pose_estimation':
                        # 18ê°œ í‚¤í¬ì¸íŠ¸
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 18, height//4, width//4), device='cpu')
                        else:
                            return np.zeros((batch_size, 18, height//4, width//4), dtype=np.float32)
                    elif self.model_type == 'segmentation':
                        # Binary mask
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 1, height, width), device='cpu')
                        else:
                            return np.zeros((batch_size, 1, height, width), dtype=np.float32)
                    elif self.model_type == 'geometric_matching':
                        # Transformation parameters
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 25, 2), device='cpu')
                        else:
                            return np.zeros((batch_size, 25, 2), dtype=np.float32)
                    elif self.model_type == 'diffusion':
                        # Generated image
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 3, height, width), device='cpu')
                        else:
                            return np.zeros((batch_size, 3, height, width), dtype=np.float32)
                    else:
                        # Default output
                        if TORCH_AVAILABLE:
                            return torch.zeros((batch_size, 3, height, width), device='cpu')
                        else:
                            return np.zeros((batch_size, 3, height, width), dtype=np.float32)
                        
                except Exception:
                    # ìµœì¢… í´ë°±: numpy ì‚¬ìš©
                    return np.zeros((batch_size, 3, height, width), dtype=np.float32)
            
            def to(self, device):
                """ë””ë°”ì´ìŠ¤ ì´ë™"""
                self.device = str(device)
                return self
            
            def eval(self):
                """í‰ê°€ ëª¨ë“œ"""
                self.eval_mode = True
                return self
            
            def cuda(self):
                return self.to('cuda')
            
            def cpu(self):
                return self.to('cpu')
            
            def warmup(self):
                """ì›Œë°ì—…"""
                return True
        
        mock = SmartMockModel(model_name, self.device, self.step_name)
        self.logger.info(f"ğŸ­ Smart Mock ëª¨ë¸ ìƒì„±: {model_name} ({mock.model_type})")
        return mock
    
    async def get_recommended_model(self) -> Optional[Any]:
        """ê¶Œì¥ ëª¨ë¸ ë¡œë“œ"""
        if self.recommended_models:
            return await self.get_model(self.recommended_models[0])
        return await self.get_model("default_model")
    
    def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬ - ì•ˆì „í•œ í˜¸ì¶œ"""
        try:
            unloaded_count = 0
            for model_name, model in list(self.loaded_models.items()):
                try:
                    # CPUë¡œ ì´ë™ ì‹œë„ (ì•ˆì „í•œ í˜¸ì¶œ)
                    if hasattr(model, 'cpu'):
                        cpu_method = getattr(model, 'cpu', None)
                        success, result, message = SafeFunctionValidator.safe_call(cpu_method)
                        if not success:
                            self.logger.warning(f"âš ï¸ CPU ì´ë™ ì‹¤íŒ¨ {model_name}: {message}")
                    
                    del model
                    unloaded_count += 1
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            
            self.loaded_models.clear()
            self.logger.info(f"ğŸ§¹ {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {self.step_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_info(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "device": self.device,
            "model_cache_dir": str(self.model_cache_dir),
            "recommended_models": self.recommended_models,
            "loaded_models": list(self.loaded_models.keys()),
            "available_model_paths": len(self.model_paths),
            "access_count": self.access_count,
            "last_used": self.last_used,
            "has_async_loader": self.has_async_loader,
            "has_sync_wrapper": self.has_sync_wrapper,
            "step_model_mapping": self.step_model_mapping.get(self.step_name, {}),
            "safe_model_service_status": True
        }

# ==============================================
# ğŸ”¥ ì™„ì „ í†µí•© ModelLoader í´ë˜ìŠ¤ v5.0 - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” ì™„ì „ í†µí•© ModelLoader v5.0
    âœ… 'dict' object is not callable ê·¼ë³¸ ì›ì¸ í•´ê²°
    âœ… SafeModelService + SafeFunctionValidator í†µí•©
    âœ… NumPy 2.x ì™„ì „ í˜¸í™˜ì„±
    âœ… BaseStepMixin v3.3 ì™„ë²½ ì—°ë™
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€
    âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ëª…/í´ë˜ìŠ¤ëª… ìœ ì§€
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        **kwargs
    ):
        """ì™„ì „ í†µí•© ìƒì„±ì - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        
        # ğŸ”¥ NumPy í˜¸í™˜ì„± ì²´í¬
        self._check_numpy_compatibility()
        
        # ğŸ”¥ ê¸°ë³¸ ì„¤ì • - SafeConfig ì‚¬ìš©
        self.config = SafeConfig(config or {})
        self.step_name = self.__class__.__name__
        
        # ğŸ”¥ logger ì†ì„± ì„¤ì •
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ğŸ”¥ SafeModelService í†µí•©
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        
        # ğŸ”¥ ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ğŸ”¥ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ğŸ”¥ ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ğŸ”¥ ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ğŸ”¥ ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ğŸ”¥ Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        
        # ğŸ”¥ ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        self.logger.info(f"ğŸ¯ ModelLoader v5.0 ì´ˆê¸°í™” ì™„ë£Œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
        self.logger.info(f"ğŸ”§ Device: {self.device}, SafeModelService: âœ…")
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x í˜¸í™˜ì„± ì²´í¬ ë° ê²½ê³ """
        try:
            if NUMPY_AVAILABLE:
                numpy_version = np.__version__
                major_version = int(numpy_version.split('.')[0])
                
                if major_version >= 2:
                    temp_logger = logging.getLogger(f"ModelLoader.{self.__class__.__name__}")
                    temp_logger.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€ë¨ (2.x)")
                    temp_logger.warning("ğŸ”§ conda install numpy=1.24.3 -y --force-reinstall ê¶Œì¥")
                    
                    # NumPy 2.xìš© í˜¸í™˜ì„± ì„¤ì •
                    try:
                        np.set_printoptions(legacy='1.25')
                        temp_logger.info("âœ… NumPy 2.x í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”")
                    except:
                        pass
        except Exception as e:
            temp_logger = logging.getLogger(f"ModelLoader.{self.__class__.__name__}")
            temp_logger.warning(f"âš ï¸ NumPy ë²„ì „ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.use_fp16 = True
                if COREML_AVAILABLE:
                    self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ë‚´ì¥ ìš”ì²­ì‚¬í•­ ì‚¬ìš©
            self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì²˜ë¦¬ - SafeConfig ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ì²˜ë¦¬
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                # Step 01: Human Parsing
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                
                # Step 02: Pose Estimation
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                
                # Step 03: Cloth Segmentation
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                
                # Step 04: Geometric Matching
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                ),
                
                # Step 06: Virtual Fitting
                "virtual_fitting_hrviton": ModelConfig(
                    name="virtual_fitting_hrviton",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="HRVITONModel",
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡ - ëª¨ë“  íƒ€ì… ì§€ì› - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            with self._lock:
                # ì„¤ì • íƒ€ì…ë³„ ì²˜ë¦¬
                if isinstance(model_config, dict):
                    # Dictë¥¼ ModelConfigë¡œ ë³€í™˜
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ìë™ ê°ì§€
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                # ë‚´ë¶€ ì„¤ì • ì €ì¥
                self.model_configs[name] = config
                
                # ğŸ”¥ SafeModelServiceì—ë„ ë“±ë¡
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device
                }
                self.safe_model_service.register_model(name, model_dict)
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} ({model_type}) - SafeModelService í¬í•¨")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def register_model(self, name: str, config: Dict[str, Any]):
        """ëª¨ë¸ ë“±ë¡ (ì–´ëŒ‘í„°ì—ì„œ ì‚¬ìš©) - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ğŸ”¥ dict íƒ€ì… í™•ì¸ í›„ ì•ˆì „í•œ ì²˜ë¦¬
            if not isinstance(config, dict):
                self.logger.error(f"âŒ configëŠ” dict íƒ€ì…ì´ì–´ì•¼ í•¨: {type(config)}")
                return False
            
            if not hasattr(self, 'detected_model_registry'):
                self.detected_model_registry = {}
            
            # ğŸ”¥ ë”•ì…”ë„ˆë¦¬ ë³µì‚¬ë¡œ ì•ˆì „í•œ ì €ì¥
            self.detected_model_registry[name] = config.copy()
            
            # ğŸ”¥ SafeModelServiceì—ë„ ë“±ë¡
            self.safe_model_service.register_model(name, config)
            
            self.logger.debug(f"âœ… ëª¨ë¸ ë“±ë¡: {name} - SafeModelService í¬í•¨")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def _get_model_class(self, model_class_name: str) -> Type:
        """ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ ë°˜í™˜"""
        model_classes = {
            'GraphonomyModel': GraphonomyModel,
            'OpenPoseModel': OpenPoseModel,
            'U2NetModel': U2NetModel,
            'GeometricMatchingModel': GeometricMatchingModel,
            'HRVITONModel': HRVITONModel,
            'BaseModel': BaseModel
        }
        return model_classes.get(model_class_name, BaseModel)
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ğŸ”¥ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ğŸ”¥ SafeModelService ìš°ì„  ì‚¬ìš©
            model = await self.safe_model_service.call_model(model_name)
            if model:
                self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return model
            
            # ğŸ”¥ ê¸°ì¡´ ë°©ì‹ ì•ˆì „í•œ í˜¸ì¶œ
            load_func = getattr(self, '_load_model_sync_wrapper', None)
            success, result, message = await self.function_validator.safe_async_call(
                load_func, model_name, kwargs
            )
            
            if success:
                return result
            else:
                self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨: {message}")
                # í´ë°±: ì§ì ‘ ë¡œë“œ ì‹œë„
                return await self._direct_async_load(model_name, **kwargs)
                
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def _direct_async_load(self, model_name: str, **kwargs) -> Optional[Any]:
        """ì§ì ‘ ë¹„ë™ê¸° ë¡œë“œ"""
        try:
            # load_model ë©”ì„œë“œ ì•ˆì „í•œ í˜¸ì¶œ
            load_method = getattr(self, 'load_model', None)
            success, result, message = await self.function_validator.safe_async_call(
                load_method, model_name, **kwargs
            )
            
            if success:
                return result
            else:
                self.logger.warning(f"âš ï¸ ì§ì ‘ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨: {message}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_model_sync_wrapper(self, model_name: str, kwargs: Dict) -> Optional[Any]:
        """ë™ê¸° ë¡œë“œ ë˜í¼ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ğŸ”¥ SafeModelServiceë¥¼ í†µí•œ ì•ˆì „í•œ ëª¨ë¸ ìƒì„±
            model_dict = {
                'name': model_name,
                'status': 'loaded',
                'type': 'sync_wrapper_model',
                'device': self.device,
                'kwargs': kwargs
            }
            
            # SafeModelServiceì— ë“±ë¡ í›„ ë°˜í™˜
            if self.safe_model_service.register_model(model_name, model_dict):
                # ë“±ë¡ëœ ëª¨ë¸ì˜ wrapper ë°˜í™˜
                return self.safe_model_service.models.get(model_name)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"ë™ê¸° ë˜í¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ì™„ì „ í†µí•© ëª¨ë¸ ë¡œë“œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # ğŸ”¥ SafeModelService ìš°ì„  ì‚¬ìš©
                model = await self.safe_model_service.call_model(name)
                if model:
                    # ìºì‹œì— ì €ì¥
                    self.model_cache[cache_key] = model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {name}")
                    return model
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    # ğŸ”¥ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì‹œë„
                    default_config = {
                        'name': name,
                        'type': 'unknown',
                        'device': self.device
                    }
                    self.safe_model_service.register_model(name, default_config)
                    model = await self.safe_model_service.call_model(name)
                    if model:
                        self.model_cache[cache_key] = model
                        return model
                    else:
                        return None
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘: {name}")
                
                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    return None
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                await self._load_checkpoint(model, model_config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ - ì•ˆì „í•œ í˜¸ì¶œ
                if hasattr(model, 'to'):
                    to_method = getattr(model, 'to', None)
                    success, result, message = self.function_validator.safe_call(to_method, self.device)
                    if success:
                        model = result
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™” - ì•ˆì „í•œ í˜¸ì¶œ
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        half_method = getattr(model, 'half', None)
                        success, result, message = self.function_validator.safe_call(half_method)
                        if success:
                            model = result
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ - ì•ˆì „í•œ í˜¸ì¶œ
                if hasattr(model, 'eval'):
                    eval_method = getattr(model, 'eval', None)
                    self.function_validator.safe_call(eval_method)
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            return None
    
    async def initialize(self) -> bool:
        """ğŸ”¥ ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ ModelLoader v5.0 ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ë³¸ ì´ˆê¸°í™” ì‘ì—…ë“¤
            await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
            
            # ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ ì²´í¬
            if not hasattr(self, 'device_manager'):
                self.logger.warning("âš ï¸ ë””ë°”ì´ìŠ¤ ë§¤ë‹ˆì €ê°€ ì—†ìŒ")
                return False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ - ì•ˆì „í•œ í˜¸ì¶œ
            if hasattr(self, 'memory_manager'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
                
            self.logger.info("âœ… ModelLoader v5.0 ì´ˆê¸°í™” ì™„ë£Œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_model_instance(
        self,
        model_config: Union[ModelConfig, StepModelConfig],
        **kwargs
    ) -> Optional[Any]:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class_name = getattr(model_config, 'model_class', 'BaseModel')
            
            if model_class_name == "GraphonomyModel":
                num_classes = getattr(model_config, 'num_classes', 20)
                return GraphonomyModel(num_classes=num_classes, backbone='resnet101')
            
            elif model_class_name == "OpenPoseModel":
                num_keypoints = getattr(model_config, 'num_classes', 18)
                return OpenPoseModel(num_keypoints=num_keypoints)
            
            elif model_class_name == "U2NetModel":
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class_name == "GeometricMatchingModel":
                return GeometricMatchingModel(feature_size=256)
            
            elif model_class_name == "HRVITONModel":
                return HRVITONModel(input_nc=3, output_nc=3, ngf=64)
            
            elif model_class_name == "StableDiffusionPipeline":
                return await self._create_diffusion_model(model_config)
            
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class_name}")
                return BaseModel()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _create_diffusion_model(self, model_config):
        """Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                checkpoint_path = getattr(model_config, 'checkpoint_path', None)
                if checkpoint_path and Path(checkpoint_path).exists():
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        checkpoint_path,
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                else:
                    # ê¸°ë³¸ Stable Diffusion ë¡œë“œ
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        "runwayml/stable-diffusion-v1-5",
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                
                return pipeline
            else:
                self.logger.warning("âš ï¸ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_checkpoint(self, model: Any, model_config: Union[ModelConfig, StepModelConfig]):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            # checkpoint_path ë˜ëŠ” checkpointsì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            checkpoint_path = None
            
            if hasattr(model_config, 'checkpoint_path'):
                checkpoint_path = model_config.checkpoint_path
            elif hasattr(model_config, 'checkpoints') and isinstance(model_config.checkpoints, dict):
                checkpoints = getattr(model_config, 'checkpoints', {})
                if isinstance(checkpoints, dict):
                    checkpoint_path = checkpoints.get('primary_path')
            
            if not checkpoint_path:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {getattr(model_config, 'name', 'unknown')}")
                return
                
            checkpoint_path = Path(checkpoint_path)
            
            if not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {checkpoint_path}")
                return
            
            # PyTorch ëª¨ë¸ì¸ ê²½ìš° - ì•ˆì „í•œ í˜¸ì¶œ
            if hasattr(model, 'load_state_dict') and TORCH_AVAILABLE:
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict ì •ë¦¬
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif isinstance(state_dict, dict) and 'model' in state_dict:
                    state_dict = state_dict['model']
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                # ì•ˆì „í•œ í˜¸ì¶œ
                load_state_dict_method = getattr(model, 'load_state_dict', None)
                success, result, message = self.function_validator.safe_call(
                    load_state_dict_method, cleaned_state_dict, strict=False
                )
                
                if success:
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
                else:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {message}")
            
            else:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê±´ë„ˆëœ€ (íŒŒì´í”„ë¼ì¸): {getattr(model_config, 'name', 'unknown')}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _apply_m3_max_optimization(self, model: Any, model_config) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™”"""
        try:
            optimizations_applied = []
            
            # 1. MPS ë””ë°”ì´ìŠ¤ ìµœì í™”
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # 3. CoreML ì»´íŒŒì¼ ì¤€ë¹„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if COREML_AVAILABLE and hasattr(model, 'eval'):
                optimizations_applied.append("CoreML compilation ready")
            
            # 4. Metal Performance Shaders ìµœì í™”
            if self.device == 'mps':
                try:
                    # PyTorch MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬ - ì•ˆì „í•œ í˜¸ì¶œ
            if hasattr(self.memory_manager, 'check_memory_pressure'):
                check_method = getattr(self.memory_manager, 'check_memory_pressure', None)
                success, is_pressure, message = self.function_validator.safe_call(check_method)
                
                if success and is_pressure:
                    await self._cleanup_least_used_models()
            
            # ìºì‹œëœ ëª¨ë¸ ìˆ˜ í™•ì¸
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ - ì•ˆì „í•œ í˜¸ì¶œ
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                # ì‚¬ìš© ë¹ˆë„ì™€ ìµœê·¼ ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    # ëª¨ë¸ í•´ì œ
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±° - ì•ˆì „í•œ í˜¸ì¶œ
                    if hasattr(model, 'cpu'):
                        cpu_method = getattr(model, 'cpu', None)
                        success, result, message = self.function_validator.safe_call(cpu_method)
                        if not success:
                            self.logger.warning(f"âš ï¸ CPU ì´ë™ ì‹¤íŒ¨: {message}")
                    
                    del model
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ (SafeModelService í†µí•©)")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """ê¸°ì¡´ Step ì¸í„°í˜ì´ìŠ¤ ì¡°íšŒ"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    # ì•ˆì „í•œ í˜¸ì¶œ
                    if hasattr(interface, 'unload_models'):
                        unload_method = getattr(interface, 'unload_models', None)
                        success, result, message = self.function_validator.safe_call(unload_method)
                        if not success:
                            self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì–¸ë¡œë“œ ì‹¤íŒ¨: {message}")
                    
                    del self.step_interfaces[step_name]
                    self.logger.info(f"ğŸ—‘ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ğŸ”¥ ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    self.cleanup_step_interface(step_name)
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            cpu_method = getattr(model, 'cpu', None)
                            success, result, message = self.function_validator.safe_call(cpu_method)
                            if not success:
                                self.logger.warning(f"âš ï¸ ëª¨ë¸ CPU ì´ë™ ì‹¤íŒ¨: {message}")
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ - ì•ˆì „í•œ í˜¸ì¶œ
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ - ì•ˆì „í•œ í˜¸ì¶œ
            try:
                if hasattr(self, '_executor'):
                    shutdown_method = getattr(self._executor, 'shutdown', None)
                    success, result, message = self.function_validator.safe_call(shutdown_method, wait=True)
                    if not success:
                        self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {message}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader v5.0 ì •ë¦¬ ì™„ë£Œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”¥ ì™„ì „ ê°œì„ ëœ BaseStepMixin - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    SafeModelService + SafeFunctionValidator í†µí•©ìœ¼ë¡œ ëª¨ë“  í˜¸ì¶œ ì•ˆì „ì„± ë³´ì¥
    """
    
    def __init__(self, *args, **kwargs):
        """ğŸ”¥ ì™„ì „ ì•ˆì „í•œ ì´ˆê¸°í™” - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        # NumPy í˜¸í™˜ì„± ì²´í¬
        self._check_numpy_compatibility()
        
        # ì•ˆì „í•œ super() í˜¸ì¶œ
        try:
            mro = type(self).__mro__
            if len(mro) > 2:
                super().__init__()
        except TypeError:
            pass
        
        # logger ì†ì„± ì„¤ì •
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ SafeFunctionValidator í†µí•©
        self.function_validator = SafeFunctionValidator()
        
        # ê¸°ë³¸ ì†ì„±ë“¤ ì„¤ì •
        self.device = kwargs.get('device', 'auto')
        self.model_interface = None
        self.config = SafeConfig(kwargs.get('config', {}))
        
        # ğŸ”¥ ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì •
        self._setup_warmup_functions()
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x í˜¸í™˜ì„± ì²´í¬"""
        try:
            if NUMPY_AVAILABLE:
                numpy_version = np.__version__
                major_version = int(numpy_version.split('.')[0])
                
                if major_version >= 2:
                    temp_logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
                    temp_logger.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€ë¨ (2.x)")
                    temp_logger.warning("ğŸ”§ conda install numpy=1.24.3 -y --force-reinstall ê¶Œì¥")
        except Exception as e:
            temp_logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            temp_logger.warning(f"âš ï¸ NumPy ë²„ì „ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _safe_model_warmup(self, *args, **kwargs):
        """ì•ˆì „í•œ ëª¨ë¸ ì›Œë°ì—… - Dict Callable ì˜¤ë¥˜ ë°©ì§€"""
        try:
            if hasattr(self, 'model_loader') and self.model_loader:
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì›Œë°ì—…
                if hasattr(self.model_loader, 'warmup_model'):
                    warmup_method = getattr(self.model_loader, 'warmup_model', None)
                    success, result, message = self.function_validator.safe_call(
                        warmup_method, *args, **kwargs
                    )
                    if success:
                        return result
                    else:
                        self.logger.warning(f"âš ï¸ warmup_model í˜¸ì¶œ ì‹¤íŒ¨: {message}")
            
            # ê¸°ë³¸ ì›Œë°ì—… (ì•ˆì „í•œ ì²˜ë¦¬)
            self.logger.debug("âœ… ê¸°ë³¸ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            return {"success": True, "method": "default_warmup"}
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _safe_device_warmup(self, *args, **kwargs):
        """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì›Œë°ì—… - Dict Callable ì˜¤ë¥˜ ë°©ì§€"""
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìµœì í™”
            if hasattr(self, 'gpu_config') and self.gpu_config:
                if hasattr(self.gpu_config, 'cleanup_memory'):
                    cleanup_method = getattr(self.gpu_config, 'cleanup_memory', None)
                    success, result, message = self.function_validator.safe_call(cleanup_method)
                    if not success:
                        self.logger.warning(f"âš ï¸ GPU ì •ë¦¬ ì‹¤íŒ¨: {message}")
            
            # Torch ìºì‹œ ì •ë¦¬ (ì•ˆì „í•œ ì²˜ë¦¬)
            try:
                if TORCH_AVAILABLE:
                    if hasattr(torch, 'cuda') and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        # PyTorch ë²„ì „ë³„ ì•ˆì „í•œ ì²˜ë¦¬
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except AttributeError:
                            pass  # ì˜¤ë˜ëœ PyTorch ë²„ì „ì—ì„œëŠ” ë¬´ì‹œ
            except Exception:
                pass  # Torch ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
            
            self.logger.debug("âœ… ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì™„ë£Œ")
            return {"success": True, "method": "device_warmup"}
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _safe_memory_warmup(self, *args, **kwargs):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì›Œë°ì—… - Dict Callable ì˜¤ë¥˜ ë°©ì§€"""
        try:
            import gc
            gc.collect()
            
            self.logger.debug("âœ… ë©”ëª¨ë¦¬ ì›Œë°ì—… ì™„ë£Œ")
            return {"success": True, "method": "memory_cleanup"}
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _safe_pipeline_warmup(self, *args, **kwargs):
        """ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… - Dict Callable ì˜¤ë¥˜ ë°©ì§€"""
        try:
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
            if hasattr(self, 'pipeline_manager') and self.pipeline_manager:
                if hasattr(self.pipeline_manager, 'is_ready'):
                    is_ready_method = getattr(self.pipeline_manager, 'is_ready', None)
                    success, ready, message = self.function_validator.safe_call(is_ready_method)
                    if success and ready:
                        self.logger.debug("âœ… íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤")
                    else:
                        self.logger.warning("âš ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            self.logger.debug("âœ… íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì™„ë£Œ")
            return {"success": True, "method": "pipeline_check"}
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _setup_warmup_functions(self):
        """ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì • - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            # ì‹¤ì œ ë©”ì„œë“œ ê°ì²´ë¡œ ì„¤ì • (ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ)
            self.warmup_functions = {
                'model_warmup': self._safe_model_warmup,
                'device_warmup': self._safe_device_warmup,
                'memory_warmup': self._safe_memory_warmup,
                'pipeline_warmup': self._safe_pipeline_warmup
            }
            
            # ëª¨ë“  ì›Œë°ì—… í•¨ìˆ˜ê°€ callableì¸ì§€ í™•ì¸
            for name, func in self.warmup_functions.items():
                is_callable, reason, safe_func = SafeFunctionValidator.validate_callable(func, f"warmup_{name}")
                if not is_callable:
                    self.logger.error(f"âŒ {name}ì´ callableí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {reason}")
                    # ì•ˆì „í•œ ë”ë¯¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´
                    self.warmup_functions[name] = lambda *args, **kwargs: {"success": True, "method": "dummy"}
            
            if hasattr(self, 'logger'):
                self.logger.debug("âœ… ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì„¤ì • ì™„ë£Œ (SafeFunctionValidator ê²€ì¦)")
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… í•¨ìˆ˜ ì„¤ì • ì‹¤íŒ¨: {e}")
            # ì™„ì „í•œ í´ë°±
            self.warmup_functions = {}
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            if model_loader is None:
                # ì „ì—­ ëª¨ë¸ ë¡œë” ì‚¬ìš©
                model_loader = get_global_model_loader()
            
            # ğŸ”¥ ì•ˆì „í•œ í˜¸ì¶œ
            create_method = getattr(model_loader, 'create_step_interface', None)
            success, interface, message = self.function_validator.safe_call(
                create_method, self.__class__.__name__
            )
            
            if success:
                self.model_interface = interface
                logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ (SafeFunctionValidator)")
            else:
                self.logger.warning(f"âš ï¸ create_step_interface í˜¸ì¶œ ì‹¤íŒ¨: {message}")
                self.model_interface = None
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©) - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.warning(f"âš ï¸ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                # ğŸ”¥ ì•ˆì „í•œ í˜¸ì¶œ
                get_method = getattr(self.model_interface, 'get_model', None)
                success, result, message = await self.function_validator.safe_async_call(
                    get_method, model_name
                )
                
                if success:
                    return result
                else:
                    logger.warning(f"âš ï¸ get_model í˜¸ì¶œ ì‹¤íŒ¨: {message}")
                    return None
            else:
                # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                rec_method = getattr(self.model_interface, 'get_recommended_model', None)
                success, result, message = await self.function_validator.safe_async_call(rec_method)
                
                if success:
                    return result
                else:
                    logger.warning(f"âš ï¸ get_recommended_model í˜¸ì¶œ ì‹¤íŒ¨: {message}")
                    return None
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                # ğŸ”¥ ì•ˆì „í•œ í˜¸ì¶œ
                cleanup_method = getattr(self.model_interface, 'unload_models', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                
                if not success:
                    logger.warning(f"âš ï¸ unload_models í˜¸ì¶œ ì‹¤íŒ¨: {message}")
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì „ì—­ ModelLoader v5.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°)")
        
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> Dict[str, Any]:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
    try:
        loader = get_global_model_loader()
        validator = SafeFunctionValidator()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰ - ì•ˆì „í•œ í˜¸ì¶œ
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            init_method = getattr(loader, 'initialize', None)
            is_callable, reason, safe_method = SafeFunctionValidator.validate_callable(
                init_method, "initialize_global"
            )
            
            if is_callable:
                future = asyncio.create_task(safe_method())
                return {"success": True, "message": "Initialization started", "future": future}
            else:
                logger.warning(f"âš ï¸ initializeê°€ callableí•˜ì§€ ì•ŠìŒ: {reason}")
                return {"success": False, "error": f"initialize method not callable: {reason}"}
        else:
            init_method = getattr(loader, 'initialize', None)
            success, result, message = asyncio.run(
                validator.safe_async_call(init_method)
            )
            
            if success:
                return {"success": result, "message": "Initialization completed"}
            else:
                return {"success": False, "error": f"Initialization failed: {message}"}
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            validator = SafeFunctionValidator()
            cleanup_method = getattr(_global_model_loader, 'cleanup', None)
            success, result, message = validator.safe_call(cleanup_method)
            
            if not success:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {message}")
            
            _global_model_loader = None
        # ìºì‹œ í´ë¦¬ì–´
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ModelLoader v5.0 ì •ë¦¬ ì™„ë£Œ (Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°)")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

# ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_global_model_service = None
_service_lock = threading.Lock()

def get_model_service() -> SafeModelService:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_service
    
    if _global_model_service is None:
        with _service_lock:
            if _global_model_service is None:
                _global_model_service = SafeModelService()
                logger.info("âœ… ì „ì—­ SafeModelService ìƒì„±")
    
    return _global_model_service

async def safe_warmup_models(model_names: list) -> Dict[str, bool]:
    """ì—¬ëŸ¬ ëª¨ë¸ ì•ˆì „ ì›Œë°ì—… - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
    service = get_model_service()
    results = {}
    
    for name in model_names:
        try:
            results[name] = await service.warmup_model(name)
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ {name}: {e}")
            results[name] = False
    
    return results

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ë¡œ ì•ˆì „í•˜ê²Œ ë“±ë¡"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock ëª¨ë¸ ìƒì„± - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': 'mps',
        'loaded_at': '2025-01-19T12:00:00Z'
    }
    
    service = get_model_service()
    return service._create_dict_wrapper(mock_dict)

# ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤ - ì „ì—­ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ"""
    return await SafeFunctionValidator.safe_async_call(obj, *args, **kwargs)

def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ ì†ì„± ì ‘ê·¼ ë° í˜¸ì¶œ"""
    return SafeFunctionValidator.safe_getattr_call(obj, attr_name, *args, **kwargs)

def is_safely_callable(obj: Any) -> bool:
    """ì „ì—­ callable ì•ˆì „ì„± ê²€ì¦"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ - ì™„ì „ í†µí•© + Dict Callable ì˜¤ë¥˜ í•´ê²°
# ==============================================

__all__ = [
    # ğŸ”¥ Dict Callable ì˜¤ë¥˜ í•´ê²° í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'SafeFunctionValidator',
    'SafeModelService',
    
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
    'ModelLoader',
    'ModelFormat',
    'ModelConfig', 
    'StepModelConfig',
    'ModelType',
    'ModelPriority',
    'DeviceManager',
    'ModelMemoryManager',
    'StepModelInterface',
    'BaseStepMixin',
    'SafeConfig',
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
    'preprocess_image',
    'postprocess_segmentation', 
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'tensor_to_pil',
    'pil_to_tensor',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    
    # íŒ©í† ë¦¬ ë° ê´€ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    
    # ğŸ”¥ ìƒˆë¡œìš´ ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤
    'get_model_service',
    'safe_warmup_models',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_async_call',
    'safe_getattr_call',
    'is_safely_callable',
    
    # ìƒìˆ˜ (ê¸°ì¡´ ìœ ì§€)
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE'
]

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì•ˆì „í•œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… ModelLoader v5.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - Dict Callable ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("ğŸ”— SafeModelService + SafeFunctionValidator í†µí•©")
logger.info("ğŸ”§ NumPy 2.x + BaseStepMixin ì™„ë²½ í˜¸í™˜")
logger.info("ğŸ M3 Max 128GB ìµœì í™”")
logger.info("ğŸ›¡ï¸ ëª¨ë“  í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ ì•ˆì „ì„± ë³´ì¥")
logger.info(f"ğŸ¯ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ”¢ NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'} v{np.__version__ if NUMPY_AVAILABLE else 'N/A'}")

if NUMPY_AVAILABLE and int(np.__version__.split('.')[0]) >= 2:
    logger.warning("âš ï¸ NumPy 2.x ê°ì§€ë¨ - conda install numpy=1.24.3 ê¶Œì¥")
else:
    logger.info("âœ… NumPy í˜¸í™˜ì„± í™•ì¸ë¨")