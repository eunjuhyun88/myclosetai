#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì™„ì „í•œ ModelLoader v20.0 (Human Parsing ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
===============================================================================
âœ… Human Parsing ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì™„ì „ í•´ê²°
âœ… __aenter__ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì •
âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œìŠ¤í…œ êµ¬í˜„
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” + M3 Max 128GB ì™„ì „ í™œìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING + ì˜ì¡´ì„± ì£¼ì…)
âœ… BaseStepMixin 100% í˜¸í™˜ ìœ ì§€
âœ… ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê°•í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… ì‹¤ì‹œê°„ ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ

í•µì‹¬ ê°œì„ ì‚¬í•­:
- ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € êµ¬í˜„
- ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê°•í™” (íŒŒì¼ ë¬´ê²°ì„± ì²´í¬)
- M3 Max MPS ë””ë°”ì´ìŠ¤ ì•ˆì •í™”
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ì‹œìŠ¤í…œ
- ì‹¤íŒ¨ ì‹œ ìë™ í´ë°± ë©”ì»¤ë‹ˆì¦˜

Author: MyCloset AI Team
Date: 2025-07-23
Version: 20.0 (Human Parsing Error Fix)
===============================================================================
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager, contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë¡œê¹… ì„¤ì • (ê°œì„ )
# ==============================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ë³„ë„ í•¸ë“¤ëŸ¬ ì„¤ì • (ì¤‘ë³µ ë¡œê·¸ ë°©ì§€)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = False

# ==============================================
# ğŸ”¥ 2ë‹¨ê³„: TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° í•´ê²° (ê°•í™”)
# ==============================================

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..steps.base_step_mixin import BaseStepMixin
    from PIL import Image

# ëŸ°íƒ€ì„ PIL ì²´í¬
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None
    PIL_AVAILABLE = False

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì (ì™„ì „ ê°œì„ )
# ==============================================

class LibraryCompatibilityManager:
    """conda í™˜ê²½ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        self.conda_env = self._detect_conda_env()
        self.torch_version = "unknown"
        self.numpy_version = "unknown"
        self._check_libraries()
        self._optimize_environment()
    
    def _detect_conda_env(self) -> str:
        """conda í™˜ê²½ íƒì§€ ê°œì„ """
        # 1ìˆœìœ„: CONDA_DEFAULT_ENV
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        if conda_env and conda_env != 'base':
            return conda_env
        
        # 2ìˆœìœ„: CONDA_PREFIX
        conda_prefix = os.environ.get('CONDA_PREFIX', '')
        if conda_prefix:
            env_name = os.path.basename(conda_prefix)
            if env_name and env_name != 'conda':
                return env_name
        
        # 3ìˆœìœ„: ê°€ìƒí™˜ê²½ ê²½ë¡œ ì§ì ‘ ì²´í¬
        if 'envs' in conda_prefix:
            parts = conda_prefix.split('envs')
            if len(parts) > 1:
                env_name = parts[-1].strip('/\\').split('/')[0].split('\\')[0]
                if env_name:
                    return env_name
        
        return ""

    def _check_libraries(self):
        """conda í™˜ê²½ ìš°ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬ (ê°œì„ )"""
        # NumPy ì²´í¬ (conda ìµœì í™”)
        try:
            import numpy as np
            self.numpy_available = True
            self.numpy_version = np.__version__
            globals()['np'] = np
            logger.debug(f"âœ… NumPy {self.numpy_version} ë¡œë“œ ì™„ë£Œ (conda í™˜ê²½)")
        except ImportError:
            self.numpy_available = False
            logger.warning("âš ï¸ NumPy ì‚¬ìš© ë¶ˆê°€")
        
        # PyTorch ì²´í¬ (conda í™˜ê²½ + M3 Max ìµœì í™”)
        try:
            # ğŸ”¥ M3 Max MPS í™˜ê²½ ìµœì í™”
            os.environ.update({
                'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
                'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1'
            })
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.torch_version = torch.__version__
            self.device_type = "cpu"
            
            # M3 Max MPS ì„¤ì • (ê°œì„ )
            if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if torch.backends.mps.is_available():
                    self.mps_available = True
                    self.device_type = "mps"
                    self.is_m3_max = True
                    self._safe_mps_empty_cache()
                    logger.info("ğŸ M3 Max MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨ - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ")
                    
            elif torch.cuda.is_available():
                self.device_type = "cuda"
                logger.info("ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤ ê°ì§€ë¨")
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
            logger.info(f"âœ… PyTorch {self.torch_version} ë¡œë“œ ì™„ë£Œ (Device: {self.device_type})")
            
        except ImportError as e:
            self.torch_available = False
            self.mps_available = False
            logger.warning(f"âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€: {e}")

    def _safe_mps_empty_cache(self):
        """ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬ (M3 Max ìµœì í™”)"""
        try:
            if not self.torch_available:
                return False
            
            import torch as local_torch
            
            # ğŸ”¥ M3 Max ì „ìš© ìµœì í™”
            if hasattr(local_torch, 'mps'):
                if hasattr(local_torch.mps, 'empty_cache'):
                    local_torch.mps.empty_cache()
                    return True
                elif hasattr(local_torch.mps, 'synchronize'):
                    local_torch.mps.synchronize()
            
            if hasattr(local_torch, 'backends') and hasattr(local_torch.backends, 'mps'):
                if hasattr(local_torch.backends.mps, 'empty_cache'):
                    local_torch.backends.mps.empty_cache()
                    return True
            
            return False
        except (AttributeError, RuntimeError, ImportError):
            return False

    def _optimize_environment(self):
        """í™˜ê²½ ìµœì í™” ì„¤ì •"""
        if self.is_m3_max:
            # M3 Max ì „ìš© ìµœì í™”
            os.environ.update({
                'OMP_NUM_THREADS': '8',
                'MKL_NUM_THREADS': '8',
                'NUMEXPR_NUM_THREADS': '8',
                'OPENBLAS_NUM_THREADS': '8'
            })
        
        if self.conda_env:
            logger.info(f"ğŸ conda í™˜ê²½ ê°ì§€: {self.conda_env}")

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™”
_compat = LibraryCompatibilityManager()

# ì „ì—­ ìƒìˆ˜ ì •ì˜
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available  
NUMPY_AVAILABLE = _compat.numpy_available
DEFAULT_DEVICE = _compat.device_type
IS_M3_MAX = _compat.is_m3_max
CONDA_ENV = _compat.conda_env

# ==============================================
# ğŸ”¥ 4ë‹¨ê³„: ì•ˆì „í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (ê°•í™”)
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max ìµœì í™”)"""
    try:
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            import torch
            
            # ğŸ”¥ M3 Max ì „ìš© ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œí€€ìŠ¤
            if hasattr(torch, 'mps'):
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                if hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                return True
            elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                return True
            return False
    except (AttributeError, RuntimeError) as e:
        logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ì •ìƒ): {e}")
        return False

def safe_torch_cleanup():
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°•í™”)"""
    try:
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        
        if TORCH_AVAILABLE:
            import torch
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE:
                safe_mps_empty_cache()
                
            # í…ì„œ ìºì‹œ ì •ë¦¬
            if hasattr(torch, '_C') and hasattr(torch._C, '_cuda_clearCublasWorkspaces'):
                try:
                    torch._C._cuda_clearCublasWorkspaces()
                except:
                    pass
        
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def get_enhanced_memory_info() -> Dict[str, Any]:
    """í–¥ìƒëœ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        
        memory_info = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "percent": memory.percent,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if TORCH_AVAILABLE:
            import torch
            if torch.cuda.is_available():
                memory_info["gpu"] = {
                    "allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                    "reserved_mb": torch.cuda.memory_reserved() / (1024**2),
                    "max_allocated_mb": torch.cuda.max_memory_allocated() / (1024**2)
                }
            elif MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'current_allocated_memory'):
                        memory_info["mps"] = {
                            "allocated_mb": torch.mps.current_allocated_memory() / (1024**2)
                        }
                except:
                    memory_info["mps"] = {"status": "available"}
        
        return memory_info
        
    except ImportError:
        return {
            "total_gb": 128.0 if IS_M3_MAX else 16.0,
            "available_gb": 100.0 if IS_M3_MAX else 12.0,
            "used_gb": 28.0 if IS_M3_MAX else 4.0,
            "percent": 22.0 if IS_M3_MAX else 25.0,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV
        }

# ==============================================
# ğŸ”¥ 5ë‹¨ê³„: ë°ì´í„° êµ¬ì¡° ì •ì˜ (ê°•í™”)
# ==============================================

class StepPriority(IntEnum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    TENSORFLOW = "bin"
    ONNX = "onnx"
    PICKLE = "pkl"
    CHECKPOINT = "ckpt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class LoadingStatus(Enum):
    """ë¡œë”© ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

@dataclass
class CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    file_exists: bool
    size_mb: float
    checksum: Optional[str] = None
    error_message: Optional[str] = None
    validation_time: float = 0.0

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´ (ê°•í™”)"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    file_size_mb: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation: Optional[CheckpointValidation] = None
    loading_status: LoadingStatus = LoadingStatus.NOT_LOADED
    last_validated: float = 0.0

@dataclass
class SafeModelCacheEntry:
    """ì•ˆì „í•œ ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    load_time: float
    last_access: float
    access_count: int
    memory_usage_mb: float
    device: str
    step_name: Optional[str] = None
    validation: Optional[CheckpointValidation] = None
    is_healthy: bool = True
    error_count: int = 0

# ==============================================
# ğŸ”¥ 6ë‹¨ê³„: ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸°
# ==============================================

class CheckpointValidator:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ê¸° (Human Parsing ì˜¤ë¥˜ í•´ê²°)"""
    
    @staticmethod
    def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ (ì™„ì „í•œ êµ¬í˜„)"""
        start_time = time.time()
        checkpoint_path = Path(checkpoint_path)
        
        try:
            # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not checkpoint_path.exists():
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=False,
                    size_mb=0.0,
                    error_message=f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {checkpoint_path}",
                    validation_time=time.time() - start_time
                )
            
            # 2. íŒŒì¼ í¬ê¸° í™•ì¸
            size_bytes = checkpoint_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            if size_bytes == 0:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=0.0,
                    error_message="íŒŒì¼ í¬ê¸°ê°€ 0ë°”ì´íŠ¸",
                    validation_time=time.time() - start_time
                )
            
            # 3. íŒŒì¼ í™•ì¥ì í™•ì¸
            valid_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.pkl'}
            if checkpoint_path.suffix.lower() not in valid_extensions:
                logger.warning(f"âš ï¸ ë¹„í‘œì¤€ í™•ì¥ì: {checkpoint_path.suffix}")
            
            # 4. PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (í•µì‹¬!)
            if TORCH_AVAILABLE:
                validation_result = CheckpointValidator._validate_pytorch_checkpoint(checkpoint_path)
                if not validation_result.is_valid:
                    return validation_result
            
            # 5. ì²´í¬ì„¬ ê³„ì‚° (ì„ íƒì )
            checksum = None
            if size_mb < 1000:  # 1GB ë¯¸ë§Œì¸ ê²½ìš°ë§Œ ì²´í¬ì„¬ ê³„ì‚°
                try:
                    checksum = CheckpointValidator._calculate_checksum(checkpoint_path)
                except:
                    pass
            
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=size_mb,
                checksum=checksum,
                validation_time=time.time() - start_time
            )
            
        except Exception as e:
            return CheckpointValidation(
                is_valid=False,
                file_exists=checkpoint_path.exists(),
                size_mb=0.0,
                error_message=f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                validation_time=time.time() - start_time
            )
    
    @staticmethod
    def _validate_pytorch_checkpoint(checkpoint_path: Path) -> CheckpointValidation:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (Human Parsing ì˜¤ë¥˜ í•µì‹¬ í•´ê²°)"""
        start_time = time.time()
        
        try:
            import torch
            
            # ğŸ”¥ ì•ˆì „í•œ ë¡œë”© ì‹œë„ (weights_only=Trueë¡œ ìš°ì„  ì‹œë„)
            try:
                # weights_only=Trueë¡œ ì•ˆì „í•˜ê²Œ ì‹œë„
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {checkpoint_path.name}")
                
            except Exception as weights_only_error:
                # weights_only=Falseë¡œ ì‹œë„ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼)
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                    logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ (weights_only=False): {checkpoint_path.name}")
                    
                except Exception as load_error:
                    return CheckpointValidation(
                        is_valid=False,
                        file_exists=True,
                        size_mb=checkpoint_path.stat().st_size / (1024**2),
                        error_message=f"PyTorch ë¡œë”© ì‹¤íŒ¨: {str(load_error)}",
                        validation_time=time.time() - start_time
                    )
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ê²€ì¦
            if checkpoint is None:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=checkpoint_path.stat().st_size / (1024**2),
                    error_message="ì²´í¬í¬ì¸íŠ¸ê°€ None",
                    validation_time=time.time() - start_time
                )
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸
            if isinstance(checkpoint, dict):
                # state_dict í˜•íƒœ í™•ì¸
                if len(checkpoint) == 0:
                    return CheckpointValidation(
                        is_valid=False,
                        file_exists=True,
                        size_mb=checkpoint_path.stat().st_size / (1024**2),
                        error_message="ë¹ˆ ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬",
                        validation_time=time.time() - start_time
                    )
                
                # Human Parsing ëª¨ë¸ íŠ¹í™” ê²€ì¦
                if 'exp-schp' in checkpoint_path.name.lower():
                    if not CheckpointValidator._validate_human_parsing_checkpoint(checkpoint):
                        return CheckpointValidation(
                            is_valid=False,
                            file_exists=True,
                            size_mb=checkpoint_path.stat().st_size / (1024**2),
                            error_message="Human Parsing ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶ˆì¼ì¹˜",
                            validation_time=time.time() - start_time
                        )
            
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                validation_time=time.time() - start_time
            )
            
        except ImportError:
            # PyTorch ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ê²€ì¦ë§Œ
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                error_message="PyTorch ê²€ì¦ ë¶ˆê°€ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ)",
                validation_time=time.time() - start_time
            )
        except Exception as e:
            return CheckpointValidation(
                is_valid=False,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                error_message=f"PyTorch ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                validation_time=time.time() - start_time
            )
    
    @staticmethod
    def _validate_human_parsing_checkpoint(checkpoint: Dict[str, Any]) -> bool:
        """Human Parsing ì²´í¬í¬ì¸íŠ¸ íŠ¹í™” ê²€ì¦"""
        try:
            # ì¼ë°˜ì ì¸ Human Parsing ëª¨ë¸ í‚¤ í™•ì¸
            expected_keys = ['model', 'state_dict', 'net']
            has_model_key = any(key in checkpoint for key in expected_keys)
            
            if has_model_key:
                return True
            
            # ì§ì ‘ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            param_count = 0
            for key, value in checkpoint.items():
                if hasattr(value, 'shape') or hasattr(value, 'size'):
                    param_count += 1
                    if param_count > 10:  # ì¶©ë¶„í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆìŒ
                        return True
            
            return param_count > 0
            
        except Exception:
            return True  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ í†µê³¼ë¡œ ì²˜ë¦¬
    
    @staticmethod
    def _calculate_checksum(file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

# ==============================================
# ğŸ”¥ 7ë‹¨ê³„: ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (í•µì‹¬ ìˆ˜ì •!)
# ==============================================

class SafeAsyncContextManager:
    """ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (__aenter__ ì˜¤ë¥˜ í•´ê²°)"""
    
    def __init__(self, resource_name: str = "ModelLoader"):
        self.resource_name = resource_name
        self.is_entered = False
        self.logger = logging.getLogger(f"SafeAsyncCM.{resource_name}")
    
    async def __aenter__(self):
        """ì•ˆì „í•œ ë¹„ë™ê¸° ì§„ì…"""
        try:
            self.logger.debug(f"ğŸ”„ {self.resource_name} ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ì§„ì…")
            self.is_entered = True
            return self
        except Exception as e:
            self.logger.error(f"âŒ {self.resource_name} ë¹„ë™ê¸° ì§„ì… ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Async context enter failed for {self.resource_name}: {e}")
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì•ˆì „í•œ ë¹„ë™ê¸° ì¢…ë£Œ"""
        try:
            if self.is_entered:
                self.logger.debug(f"ğŸ”„ {self.resource_name} ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ")
                self.is_entered = False
                
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹…
                if exc_type is not None:
                    self.logger.warning(f"âš ï¸ {self.resource_name} ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {exc_type.__name__}: {exc_val}")
                    
            return False  # ì˜ˆì™¸ ì „íŒŒ
        except Exception as e:
            self.logger.error(f"âŒ {self.resource_name} ë¹„ë™ê¸° ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 8ë‹¨ê³„: ê°œì„ ëœ Step ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - BaseStepMixinì—ì„œ ì§ì ‘ ì‚¬ìš© (ê°œì„ )"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ (ì•ˆì „í•œ êµ¬ì¡°)
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, SafeModelCacheEntry] = {}
        self.model_status: Dict[str, LoadingStatus] = {}
        self._lock = threading.RLock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = self._get_step_request()
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.creation_time = time.time()
        self.error_count = 0
        self.last_error = None
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_step_request(self):
        """Stepë³„ ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ê°œì„ )"""
        try:
            # model_loaderì—ì„œ ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.model_loader, 'step_requirements'):
                step_req = self.model_loader.step_requirements.get(self.step_name)
                if step_req:
                    return step_req
            
            # ê¸°ë³¸ ìš”ì²­ ì •ë³´
            return {
                "model_name": f"{self.step_name.lower()}_model",
                "model_type": "BaseModel",
                "input_size": (512, 512),
                "priority": 5
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ìš”ì²­ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡ (ê°œì„ )"""
        try:
            if self.step_request:
                if isinstance(self.step_request, dict):
                    model_name = self.step_request.get("model_name", "default_model")
                else:
                    model_name = getattr(self.step_request, "model_name", "default_model")
                return [model_name]
            
            # ê¸°ë³¸ ë§¤í•‘
            model_mapping = {
                "HumanParsingStep": ["human_parsing_schp_atr", "human_parsing_graphonomy"],
                "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
                "ClothSegmentationStep": ["cloth_segmentation_u2net", "u2net"],
                "GeometricMatchingStep": ["geometric_matching_model"],
                "ClothWarpingStep": ["cloth_warping_net"],
                "VirtualFittingStep": ["virtual_fitting_diffusion", "pytorch_model"],
                "PostProcessingStep": ["post_processing_enhance"],
                "QualityAssessmentStep": ["quality_assessment_clip"]
            }
            return model_mapping.get(self.step_name, ["default_model"])
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê¶Œì¥ ëª¨ë¸ ëª©ë¡ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["default_model"]
    
    # ==============================================
    # ğŸ”¥ BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤ (ê°œì„ )
    # ==============================================
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixinì—ì„œ await interface.get_model() í˜¸ì¶œ"""
        async with SafeAsyncContextManager(f"GetModel.{self.step_name}"):
            try:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # ìºì‹œ í™•ì¸
                with self._lock:
                    if model_name in self.model_cache:
                        cache_entry = self.model_cache[model_name]
                        if cache_entry.is_healthy:
                            cache_entry.last_access = time.time()
                            cache_entry.access_count += 1
                            self.logger.debug(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                            return cache_entry.model
                        else:
                            self.logger.warning(f"âš ï¸ ë¹„ì •ìƒ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°: {model_name}")
                            del self.model_cache[model_name]
                
                # ë¡œë”© ìƒíƒœ ì„¤ì •
                self.model_status[model_name] = LoadingStatus.LOADING
                
                # ModelLoaderë¥¼ í†µí•œ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                checkpoint = await self._safe_load_checkpoint(model_name)
                
                if checkpoint:
                    # ì•ˆì „í•œ ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                        step_name=self.step_name,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    with self._lock:
                        self.model_cache[model_name] = cache_entry
                        self.loaded_models[model_name] = checkpoint
                        self.model_status[model_name] = LoadingStatus.LOADED
                    
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return checkpoint
                
                self.model_status[model_name] = LoadingStatus.ERROR
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
            except Exception as e:
                self.error_count += 1
                self.last_error = str(e)
                self.model_status[model_name] = LoadingStatus.ERROR
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return None
    
    async def _safe_load_checkpoint(self, model_name: str) -> Optional[Any]:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # ModelLoaderì—ì„œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if hasattr(self.model_loader, 'load_model_async'):
                return await self.model_loader.load_model_async(model_name)
            elif hasattr(self.model_loader, 'load_model'):
                # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None, 
                    self.model_loader.load_model, 
                    model_name
                )
            else:
                self.logger.error(f"âŒ ModelLoaderì— ë¡œë”© ë©”ì„œë“œ ì—†ìŒ")
                return None
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixinì—ì„œ interface.get_model_sync() í˜¸ì¶œ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.is_healthy:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        return cache_entry.model
                    else:
                        del self.model_cache[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(model_name)
            
            if checkpoint:
                with self._lock:
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                        step_name=self.step_name,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = LoadingStatus.LOADED
                return checkpoint
            
            self.model_status[model_name] = LoadingStatus.ERROR
            return None
            
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ í¬ê¸° ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    # state_dictì¸ ê²½ìš°
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
                elif hasattr(checkpoint, 'parameters'):
                    # ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixinì—ì„œ interface.get_model_status() í˜¸ì¶œ"""
        try:
            if not model_name:
                # ì „ì²´ ëª¨ë¸ ìƒíƒœ ë°˜í™˜
                models_status = {}
                with self._lock:
                    for name, cache_entry in self.model_cache.items():
                        models_status[name] = {
                            "status": self.model_status.get(name, LoadingStatus.NOT_LOADED).value,
                            "device": cache_entry.device,
                            "memory_usage_mb": cache_entry.memory_usage_mb,
                            "last_access": cache_entry.last_access,
                            "access_count": cache_entry.access_count,
                            "load_time": cache_entry.load_time,
                            "is_healthy": cache_entry.is_healthy,
                            "error_count": cache_entry.error_count
                        }
                
                return {
                    "step_name": self.step_name,
                    "models": models_status,
                    "loaded_count": len(self.loaded_models),
                    "total_memory_mb": sum(entry.memory_usage_mb for entry in self.model_cache.values()),
                    "recommended_models": self.recommended_models,
                    "interface_error_count": self.error_count,
                    "last_error": self.last_error
                }
            
            # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    return {
                        "status": self.model_status.get(model_name, LoadingStatus.NOT_LOADED).value,
                        "device": cache_entry.device,
                        "memory_usage_mb": cache_entry.memory_usage_mb,
                        "last_access": cache_entry.last_access,
                        "access_count": cache_entry.access_count,
                        "load_time": cache_entry.load_time,
                        "model_type": type(cache_entry.model).__name__,
                        "loaded": True,
                        "is_healthy": cache_entry.is_healthy,
                        "error_count": cache_entry.error_count
                    }
                else:
                    return {
                        "status": LoadingStatus.NOT_LOADED.value,
                        "device": None,
                        "memory_usage_mb": 0.0,
                        "last_access": 0,
                        "access_count": 0,
                        "load_time": 0,
                        "model_type": None,
                        "loaded": False,
                        "is_healthy": False,
                        "error_count": 0
                    }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}

# ==============================================
# ğŸ”¥ 9ë‹¨ê³„: ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ì™„ì „ ê°œì„ )
# ==============================================

class ModelLoader:
    """ì™„ì „ ê°œì„ ëœ ModelLoader v20.0 (Human Parsing ì˜¤ë¥˜ í•´ê²°)"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ê°œì„ ëœ ModelLoader ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._resolve_device(device or "auto")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        memory_info = get_enhanced_memory_info()
        self.memory_gb = memory_info["total_gb"]
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ModelLoader íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 30 if self.is_m3_max else 15)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ğŸ”¥ BaseStepMixinì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ì†ì„±ë“¤
        self.loaded_models: Dict[str, Any] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        self.model_cache: Dict[str, SafeModelCacheEntry] = {}
        self.available_models: Dict[str, Any] = {}
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ì„±ëŠ¥ ì¶”ì 
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'validation_count': 0,
            'validation_success': 0,
            'checkpoint_loads': 0,
            'total_models_found': 0
        }
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="model_loader_v20")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸°
        self.validator = CheckpointValidator()
        
        # ğŸ”¥ ì•ˆì „í•œ ì´ˆê¸°í™” ì‹¤í–‰
        self._safe_initialize_components()
        
        self.logger.info(f"ğŸ¯ ì™„ì „ ê°œì„ ëœ ModelLoader v20.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
        self.logger.info(f"ğŸ’¾ Memory: {self.memory_gb:.1f}GB")
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device == "auto":
            return DEFAULT_DEVICE
        return device
    
    def _safe_initialize_components(self):
        """ì•ˆì „í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº”
            self._scan_available_models()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.optimization_enabled:
                safe_torch_cleanup()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ë™ì‘í•˜ë„ë¡ í•¨
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ (ê°œì„ )"""
        try:
            # ê¸°ë³¸ Step ìš”ì²­ì‚¬í•­ ì •ì˜ (ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜)
            default_requirements = {
                "HumanParsingStep": {
                    "model_name": "human_parsing_schp_atr",
                    "model_type": "SCHPModel",
                    "input_size": (512, 512),
                    "num_classes": 20,
                    "checkpoint_patterns": ["*schp*.pth", "*atr*.pth", "*exp-schp*.pth"],
                    "priority": 1
                },
                "PoseEstimationStep": {
                    "model_name": "pose_estimation_openpose",
                    "model_type": "OpenPoseModel",
                    "input_size": (368, 368),
                    "num_classes": 18,
                    "checkpoint_patterns": ["*openpose*.pth", "*pose*.pth"],
                    "priority": 2
                },
                "ClothSegmentationStep": {
                    "model_name": "cloth_segmentation_u2net",
                    "model_type": "U2NetModel",
                    "input_size": (320, 320),
                    "num_classes": 1,
                    "checkpoint_patterns": ["*u2net*.pth", "*cloth*.pth"],
                    "priority": 3
                },
                "VirtualFittingStep": {
                    "model_name": "virtual_fitting_diffusion",
                    "model_type": "StableDiffusionPipeline",
                    "input_size": (512, 512),
                    "checkpoint_patterns": ["*pytorch_model*.bin", "*diffusion*.bin"],
                    "priority": 6
                }
            }
            
            self.step_requirements = default_requirements
            
            loaded_steps = len(self.step_requirements)
            for step_name, request_info in self.step_requirements.items():
                try:
                    step_config = ModelConfig(
                        name=request_info.get("model_name", step_name.lower()),
                        model_type=request_info.get("model_type", "BaseModel"),
                        model_class=request_info.get("model_type", "BaseModel"),
                        device="auto",
                        precision="fp16",
                        input_size=tuple(request_info.get("input_size", (512, 512))),
                        num_classes=request_info.get("num_classes", None),
                        file_size_mb=request_info.get("file_size_mb", 0.0)
                    )
                    
                    self.model_configs[request_info.get("model_name", step_name)] = step_config
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” (ê°œì„ )"""
        try:
            base_models_dir = self.model_cache_dir
            
            # ì‹¤ì œ GitHub êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ ì„¤ì •ë“¤
            model_configs = {
                # ì¸ì²´ íŒŒì‹± ëª¨ë¸ë“¤ (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)
                "human_parsing_schp_atr": ModelConfig(
                    name="human_parsing_schp_atr",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="SCHPModel",
                    checkpoint_path=str(base_models_dir / "exp-schp-201908301523-atr.pth"),
                    input_size=(512, 512),
                    num_classes=20,
                    file_size_mb=255.1
                ),
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20,
                    file_size_mb=255.1
                ),
                
                # ì˜ë¥˜ ë¶„í•  ëª¨ë¸ë“¤
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "u2net.pth"),
                    input_size=(320, 320),
                    file_size_mb=168.1
                ),
                
                # í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë“¤
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose.pth"),
                    input_size=(368, 368),
                    num_classes=18,
                    file_size_mb=199.6
                ),
                
                # ê°€ìƒ í”¼íŒ… ëª¨ë¸ë“¤
                "virtual_fitting_diffusion": ModelConfig(
                    name="virtual_fitting_diffusion",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="StableDiffusionPipeline", 
                    checkpoint_path=str(base_models_dir / "pytorch_model.bin"),
                    input_size=(512, 512),
                    file_size_mb=577.2
                )
            }
            
            # ëª¨ë¸ ë“±ë¡ ë° ê²€ì¦
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _scan_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ìŠ¤ìº” (ê°œì„ )"""
        try:
            self.logger.info("ğŸ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            
            if not self.model_cache_dir.exists():
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_cache_dir}")
                return
                
            scanned_count = 0
            validated_count = 0
            large_models_count = 0
            total_size_gb = 0.0
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¥ì ì§€ì›
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt", ".pkl", ".pickle"]
            
            for ext in extensions:
                for model_file in self.model_cache_dir.rglob(f"*{ext}"):
                    if any(exclude in str(model_file) for exclude in ["cleanup_backup", "__pycache__", ".git"]):
                        continue
                        
                    try:
                        size_mb = model_file.stat().st_size / (1024 * 1024)
                        total_size_gb += size_mb / 1024
                        
                        if size_mb > 1000:  # 1GB ì´ìƒ
                            large_models_count += 1
                        
                        # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (Human Parsing ì˜¤ë¥˜ í•´ê²°)
                        validation = self.validator.validate_checkpoint_file(model_file)
                        self.performance_stats['validation_count'] += 1
                        
                        if validation.is_valid:
                            self.performance_stats['validation_success'] += 1
                            validated_count += 1
                        
                        relative_path = model_file.relative_to(self.model_cache_dir)
                        
                        model_info = {
                            "name": model_file.stem,
                            "path": str(relative_path),
                            "size_mb": round(size_mb, 2),
                            "model_type": self._detect_model_type(model_file),
                            "step_class": self._detect_step_class(model_file),
                            "loaded": False,
                            "device": self.device,
                            "validation": validation,
                            "is_valid": validation.is_valid,
                            "metadata": {
                                "extension": ext,
                                "parent_dir": model_file.parent.name,
                                "full_path": str(model_file),
                                "is_large": size_mb > 1000,
                                "last_modified": model_file.stat().st_mtime,
                                "validation_time": validation.validation_time
                            }
                        }
                        
                        self.available_models[model_info["name"]] = model_info
                        scanned_count += 1
                        
                        # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¡œê¹…
                        if scanned_count <= 10:
                            status = "âœ…" if validation.is_valid else "âš ï¸"
                            self.logger.info(f"ğŸ“¦ {status} ë°œê²¬: {model_info['name']} ({size_mb:.1f}MB)")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨ {model_file}: {e}")
                        
            self.performance_stats['total_models_found'] = scanned_count
            validation_rate = validated_count / scanned_count if scanned_count > 0 else 0
            
            self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì™„ë£Œ: {scanned_count}ê°œ ë°œê²¬")
            self.logger.info(f"ğŸ” ê²€ì¦ ì„±ê³µ: {validated_count}ê°œ ({validation_rate:.1%})")
            self.logger.info(f"ğŸ“Š ëŒ€ìš©ëŸ‰ ëª¨ë¸(1GB+): {large_models_count}ê°œ")
            self.logger.info(f"ğŸ’¾ ì´ ëª¨ë¸ í¬ê¸°: {total_size_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
    
    def _detect_model_type(self, model_file: Path) -> str:
        """ëª¨ë¸ íƒ€ì… ê°ì§€ - ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜ (ê°œì„ )"""
        filename = model_file.name.lower()
        
        type_keywords = {
            "human_parsing": ["schp", "atr", "lip", "graphonomy", "parsing", "exp-schp"],
            "pose_estimation": ["pose", "openpose", "body_pose", "hand_pose"],
            "cloth_segmentation": ["u2net", "sam", "segment", "cloth"],
            "geometric_matching": ["gmm", "geometric", "matching", "tps"],
            "cloth_warping": ["warp", "tps", "deformation"],
            "virtual_fitting": ["viton", "hrviton", "ootd", "diffusion", "vae", "pytorch_model"],
            "post_processing": ["esrgan", "enhancement", "super_resolution"],
            "quality_assessment": ["lpips", "quality", "metric", "clip"]
        }
        
        for model_type, keywords in type_keywords.items():
            if any(keyword in filename for keyword in keywords):
                return model_type
                
        return "unknown"
        
    def _detect_step_class(self, model_file: Path) -> str:
        """Step í´ë˜ìŠ¤ ê°ì§€ (ê°œì„ )"""
        parent_dir = model_file.parent.name.lower()
        filename = model_file.name.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë§¤í•‘ (Human Parsing ìš°ì„ )
        if "exp-schp" in filename or "schp" in filename or "atr" in filename:
            return "HumanParsingStep"
        elif "graphonomy" in filename or "parsing" in filename:
            return "HumanParsingStep"
        elif "openpose" in filename or "pose" in filename:
            return "PoseEstimationStep"
        elif "u2net" in filename or ("cloth" in filename and "segment" in filename):
            return "ClothSegmentationStep"
        elif "sam" in filename and "vit" in filename:
            return "ClothSegmentationStep"
        elif "pytorch_model" in filename or "diffusion" in filename:
            return "VirtualFittingStep"
        elif "gmm" in filename or "geometric" in filename:
            return "GeometricMatchingStep"
        elif "warp" in filename or "tps" in filename:
            return "ClothWarpingStep"
        elif "enhance" in filename or "sr" in filename:
            return "PostProcessingStep"
        elif "clip" in filename or "quality" in filename:
            return "QualityAssessmentStep"
        
        return "UnknownStep"
    
    # ==============================================
    # ğŸ”¥ BaseStepMixinì´ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤ (ê°œì„ )
    # ==============================================
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Union[Dict[str, Any], List[Dict[str, Any]]]
    ) -> bool:
        """ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (ê°œì„ )"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                # ê¸°ì¡´ ìš”ì²­ì‚¬í•­ê³¼ ë³‘í•©
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # requirementsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(requirements, list):
                    processed_requirements = {}
                    for i, req in enumerate(requirements):
                        if isinstance(req, dict):
                            model_name = req.get("model_name", f"{step_name}_model_{i}")
                            processed_requirements[model_name] = req
                    requirements = processed_requirements
                
                # ìš”ì²­ì‚¬í•­ ì—…ë°ì´íŠ¸
                if isinstance(requirements, dict):
                    self.step_requirements[step_name].update(requirements)
                else:
                    # ë‹¨ì¼ ìš”ì²­ì‚¬í•­ì¸ ê²½ìš°
                    self.step_requirements[step_name]["default_model"] = requirements
                
                # ModelConfig ìƒì„± ë° ê²€ì¦
                registered_models = 0
                for model_name, model_req in self.step_requirements[step_name].items():
                    try:
                        if isinstance(model_req, dict):
                            model_config = ModelConfig(
                                name=model_name,
                                model_type=model_req.get("model_type", "unknown"),
                                model_class=model_req.get("model_class", "BaseModel"),
                                device=model_req.get("device", "auto"),
                                precision=model_req.get("precision", "fp16"),
                                input_size=tuple(model_req.get("input_size", (512, 512))),
                                num_classes=model_req.get("num_classes"),
                                file_size_mb=model_req.get("file_size_mb", 0.0)
                            )
                            
                            self.model_configs[model_name] = model_config
                            registered_models += 1
                            
                            self.logger.debug(f"   âœ… {model_name} ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                            
                    except Exception as model_error:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {model_error}")
                        continue
                
                self.logger.info(f"âœ… {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {registered_models}ê°œ ëª¨ë¸")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (ê°œì„ )"""
        try:
            with self._interface_lock:
                # Step ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ ë“±ë¡
                if step_requirements:
                    self.register_step_requirements(step_name, step_requirements)
                
                # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if step_name in self.step_interfaces:
                    return self.step_interfaces[step_name]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                
                self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(self, step_name)
    
    def register_model_config(self, name: str, config: Union[ModelConfig, Dict[str, Any]]) -> bool:
        """ğŸ”¥ ëª¨ë¸ ì„¤ì • ë“±ë¡ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (ê°œì„ )"""
        try:
            with self._lock:
                if isinstance(config, dict):
                    # ë”•ì…”ë„ˆë¦¬ì—ì„œ ModelConfig ìƒì„±
                    model_config = ModelConfig(
                        name=name,
                        model_type=config.get("model_type", "unknown"),
                        model_class=config.get("model_class", "BaseModel"),
                        checkpoint_path=config.get("checkpoint_path"),
                        config_path=config.get("config_path"),
                        device=config.get("device", "auto"),
                        precision=config.get("precision", "fp16"),
                        input_size=tuple(config.get("input_size", (512, 512))),
                        num_classes=config.get("num_classes"),
                        file_size_mb=config.get("file_size_mb", 0.0),
                        metadata=config.get("metadata", {})
                    )
                else:
                    model_config = config
                
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)
                if model_config.checkpoint_path:
                    validation = self.validator.validate_checkpoint_file(model_config.checkpoint_path)
                    model_config.validation = validation
                    model_config.last_validated = time.time()
                    
                    if not validation.is_valid:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {name} - {validation.error_message}")
                
                self.model_configs[name] = model_config
                
                # available_modelsì—ë„ ì¶”ê°€
                self.available_models[name] = {
                    "name": name,
                    "path": model_config.checkpoint_path or f"config/{name}",
                    "size_mb": model_config.file_size_mb,
                    "model_type": str(model_config.model_type),
                    "step_class": model_config.model_class,
                    "loaded": False,
                    "device": model_config.device,
                    "validation": model_config.validation,
                    "is_valid": model_config.validation.is_valid if model_config.validation else True,
                    "metadata": model_config.metadata
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ì„¤ì • ë“±ë¡ ì™„ë£Œ: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„¤ì • ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (ê°œì„ )"""
        try:
            models = []
            
            for model_name, model_info in self.available_models.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                    
                models.append({
                    "name": model_info["name"],
                    "path": model_info["path"],
                    "size_mb": model_info["size_mb"],
                    "model_type": model_info["model_type"],
                    "step_class": model_info["step_class"],
                    "loaded": model_info["loaded"],
                    "device": model_info["device"],
                    "is_valid": model_info.get("is_valid", True),
                    "validation": model_info.get("validation"),
                    "metadata": model_info["metadata"]
                })
            
            # í¬ê¸°ìˆœ ì •ë ¬
            models.sort(key=lambda x: x["size_mb"], reverse=True)
            
            self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ìš”ì²­: {len(models)}ê°œ ë°˜í™˜ (step={step_class}, type={model_type})")
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    # ==============================================
    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë©”ì„œë“œë“¤ (ì™„ì „ ê°œì„ )
    # ==============================================
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)"""
        async with SafeAsyncContextManager(f"LoadModel.{model_name}"):
            try:
                # ìºì‹œ í™•ì¸
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.is_healthy:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        self.performance_stats['cache_hits'] += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                        return cache_entry.model
                    else:
                        # ë¹„ì •ìƒ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°
                        del self.model_cache[model_name]
                        self.logger.warning(f"âš ï¸ ë¹„ì •ìƒ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°: {model_name}")
                        
                if model_name not in self.available_models and model_name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                    return None
                    
                # ë¹„ë™ê¸°ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤í–‰
                loop = asyncio.get_event_loop()
                checkpoint = await loop.run_in_executor(
                    self._executor, 
                    self._safe_load_checkpoint_sync,
                    model_name,
                    kwargs
                )
                
                if checkpoint is not None:
                    # ì•ˆì „í•œ ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._get_checkpoint_memory_usage(checkpoint),
                        device=getattr(checkpoint, 'device', self.device) if hasattr(checkpoint, 'device') else self.device,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    
                    if model_name in self.available_models:
                        self.available_models[model_name]["loaded"] = True
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                    
                return checkpoint
                
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                if cache_entry.is_healthy:
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                    return cache_entry.model
                else:
                    del self.model_cache[model_name]
                    self.logger.warning(f"âš ï¸ ë¹„ì •ìƒ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°: {model_name}")
                    
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
            
            return self._safe_load_checkpoint_sync(model_name, kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _safe_load_checkpoint_sync(self, model_name: str, kwargs: Dict[str, Any]) -> Optional[Any]:
        """ì•ˆì „í•œ ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (Human Parsing ì˜¤ë¥˜ í•µì‹¬ í•´ê²°)"""
        try:
            start_time = time.time()
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            checkpoint_path = self._find_checkpoint_file(model_name)
            if not checkpoint_path or not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
                return None
            
            # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)
            validation = self.validator.validate_checkpoint_file(checkpoint_path)
            if not validation.is_valid:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {model_name} - {validation.error_message}")
                return None
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if TORCH_AVAILABLE:
                try:
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    if self.device in ["mps", "cuda"]:
                        safe_mps_empty_cache()
                    
                    # ğŸ”¥ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (Human Parsing ì˜¤ë¥˜ í•µì‹¬ í•´ê²°)
                    self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”©: {checkpoint_path}")
                    
                    # ë‹¨ê³„ë³„ ì•ˆì „í•œ ë¡œë”© ì‹œë„
                    checkpoint = None
                    
                    # 1ë‹¨ê³„: weights_only=Trueë¡œ ì•ˆì „í•˜ê²Œ ì‹œë„
                    try:
                        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                        self.logger.debug(f"âœ… ì•ˆì „í•œ ë¡œë”© ì„±ê³µ (weights_only=True): {model_name}")
                    except Exception as weights_only_error:
                        self.logger.debug(f"âš ï¸ weights_only=True ì‹¤íŒ¨, ì¼ë°˜ ë¡œë”© ì‹œë„: {weights_only_error}")
                        
                        # 2ë‹¨ê³„: weights_only=Falseë¡œ ì‹œë„ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼)
                        try:
                            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                            self.logger.debug(f"âœ… ì¼ë°˜ ë¡œë”© ì„±ê³µ (weights_only=False): {model_name}")
                        except Exception as general_error:
                            self.logger.error(f"âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {general_error}")
                            return None
                    
                    if checkpoint is None:
                        self.logger.error(f"âŒ ë¡œë”©ëœ ì²´í¬í¬ì¸íŠ¸ê°€ None: {model_name}")
                        return None
                    
                    # ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬
                    processed_checkpoint = self._post_process_checkpoint(checkpoint, model_name)
                    
                    # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    load_time = time.time() - start_time
                    cache_entry = SafeModelCacheEntry(
                        model=processed_checkpoint,
                        load_time=load_time,
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._get_checkpoint_memory_usage(processed_checkpoint),
                        device=str(self.device),
                        validation=validation,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = processed_checkpoint
                    self.load_times[model_name] = load_time
                    self.last_access[model_name] = time.time()
                    self.access_counts[model_name] = self.access_counts.get(model_name, 0) + 1
                    
                    if model_name in self.available_models:
                        self.available_models[model_name]["loaded"] = True
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_name} ({load_time:.2f}ì´ˆ, {cache_entry.memory_usage_mb:.1f}MB)")
                    return processed_checkpoint
                    
                except Exception as e:
                    self.logger.error(f"âŒ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                    return None
            
            # PyTorch ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë¶ˆê°€: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _post_process_checkpoint(self, checkpoint: Any, model_name: str) -> Any:
        """ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬ (Human Parsing íŠ¹í™” ì²˜ë¦¬)"""
        try:
            # Human Parsing ëª¨ë¸ íŠ¹í™” ì²˜ë¦¬
            if "human_parsing" in model_name.lower() or "schp" in model_name.lower():
                if isinstance(checkpoint, dict):
                    # ì¼ë°˜ì ì¸ í‚¤ í™•ì¸
                    if 'model' in checkpoint:
                        return checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        return checkpoint['state_dict']
                    elif 'net' in checkpoint:
                        return checkpoint['net']
                    else:
                        # ì§ì ‘ state_dictì¸ ê²½ìš°
                        return checkpoint
            
            # ê¸°íƒ€ ëª¨ë¸ ì²˜ë¦¬
            if isinstance(checkpoint, dict):
                if 'model' in checkpoint:
                    return checkpoint['model']
                elif 'state_dict' in checkpoint:
                    return checkpoint['state_dict']
                else:
                    return checkpoint
            
            return checkpoint
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return checkpoint
    
    def _find_checkpoint_file(self, model_name: str) -> Optional[Path]:
        """ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° (Human Parsing ì˜¤ë¥˜ í•´ê²° í•µì‹¬)"""
        try:
            # ğŸ”¥ 1ë‹¨ê³„: ëª¨ë¸ ì„¤ì •ì—ì„œ ì§ì ‘ ê²½ë¡œ í™•ì¸
            if model_name in self.model_configs:
                config = self.model_configs[model_name]
                if config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if checkpoint_path.exists():
                        self.logger.debug(f"ğŸ“ ëª¨ë¸ ì„¤ì • ê²½ë¡œ ì‚¬ìš©: {model_name}")
                        return checkpoint_path
            
            # ğŸ”¥ 2ë‹¨ê³„: Human Parsing ëª¨ë¸ íŠ¹í™” ê²€ìƒ‰
            if "human_parsing" in model_name.lower() or "schp" in model_name.lower():
                human_parsing_patterns = [
                    "exp-schp-201908301523-atr.pth",
                    "schp_atr.pth",
                    "human_parsing.pth",
                    "graphonomy.pth"
                ]
                
                for pattern in human_parsing_patterns:
                    for candidate in self.model_cache_dir.rglob(pattern):
                        if candidate.exists():
                            self.logger.info(f"ğŸ¯ Human Parsing ëª¨ë¸ ë°œê²¬: {model_name} â†’ {candidate}")
                            return candidate
            
            # ğŸ”¥ 3ë‹¨ê³„: ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            for ext in extensions:
                direct_path = self.model_cache_dir / f"{model_name}{ext}"
                if direct_path.exists():
                    self.logger.debug(f"ğŸ“ ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­: {model_name}")
                    return direct_path
            
            # ğŸ”¥ 4ë‹¨ê³„: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
            pattern_result = self._find_via_pattern_matching(model_name, extensions)
            if pattern_result:
                return pattern_result
            
            # ğŸ”¥ 5ë‹¨ê³„: available_modelsì—ì„œ ì°¾ê¸°
            if model_name in self.available_models:
                model_info = self.available_models[model_name]
                if "full_path" in model_info["metadata"]:
                    full_path = Path(model_info["metadata"]["full_path"])
                    if full_path.exists():
                        return full_path
            
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None

    def _find_via_pattern_matching(self, model_name: str, extensions: List[str]) -> Optional[Path]:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸° (ê°œì„ )"""
        try:
            # ìŠ¤ë§ˆíŠ¸ ë§¤í•‘ ì ìš©
            smart_mapping = {
                "human_parsing_schp_atr": "exp-schp-201908301523-atr.pth",
                "human_parsing_graphonomy": "graphonomy.pth", 
                "cloth_segmentation_u2net": "u2net.pth",
                "pose_estimation_openpose": "openpose.pth",
                "virtual_fitting_diffusion": "pytorch_model.bin"
            }
            
            if model_name in smart_mapping:
                target_file = smart_mapping[model_name]
                for candidate in self.model_cache_dir.rglob(target_file):
                    if candidate.exists():
                        self.logger.info(f"ğŸ”§ ìŠ¤ë§ˆíŠ¸ ë§¤í•‘: {model_name} â†’ {target_file}")
                        return candidate
            
            # ì¼ë°˜ íŒ¨í„´ ë§¤ì¹­
            for model_file in self.model_cache_dir.rglob("*"):
                if model_file.is_file() and model_file.suffix.lower() in extensions:
                    if model_name.lower() in model_file.name.lower():
                        self.logger.debug(f"ğŸ” íŒ¨í„´ ë§¤ì¹­: {model_name} â†’ {model_file}")
                        return model_file
            return None
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_checkpoint_memory_usage(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB) (ê°œì„ )"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    # state_dictì¸ ê²½ìš°
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
                elif hasattr(checkpoint, 'parameters'):
                    # ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ ê³ ê¸‰ ëª¨ë¸ ê´€ë¦¬ ë©”ì„œë“œë“¤ (ê°œì„ )
    # ==============================================
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixinì—ì„œ self.model_loader.get_model_status() í˜¸ì¶œ (ê°œì„ )"""
        try:
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                return {
                    "status": "loaded",
                    "device": cache_entry.device,
                    "memory_usage_mb": cache_entry.memory_usage_mb,
                    "last_used": cache_entry.last_access,
                    "load_time": cache_entry.load_time,
                    "access_count": cache_entry.access_count,
                    "model_type": type(cache_entry.model).__name__,
                    "loaded": True,
                    "is_healthy": cache_entry.is_healthy,
                    "error_count": cache_entry.error_count,
                    "validation": cache_entry.validation.__dict__ if cache_entry.validation else None
                }
            elif model_name in self.model_configs:
                config = self.model_configs[model_name]
                return {
                    "status": "registered",
                    "device": self.device,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": "Not Loaded",
                    "loaded": False,
                    "is_healthy": True,
                    "error_count": 0,
                    "validation": config.validation.__dict__ if config.validation else None,
                    "last_validated": config.last_validated
                }
            else:
                return {
                    "status": "not_found",
                    "device": None,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": None,
                    "loaded": False,
                    "is_healthy": False,
                    "error_count": 0,
                    "validation": None
                }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"status": "error", "error": str(e)}

    def get_step_model_status(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ëª¨ë¸ ìƒíƒœ ì¼ê´„ ì¡°íšŒ - BaseStepMixinì—ì„œ í˜¸ì¶œ (ê°œì„ )"""
        try:
            step_models = {}
            if step_name in self.step_requirements:
                for model_name in self.step_requirements[step_name]:
                    step_models[model_name] = self.get_model_status(model_name)
            
            total_memory = sum(status.get("memory_usage_mb", 0) for status in step_models.values())
            loaded_count = sum(1 for status in step_models.values() if status.get("status") == "loaded")
            healthy_count = sum(1 for status in step_models.values() if status.get("is_healthy", False))
            
            return {
                "step_name": step_name,
                "models": step_models,
                "total_models": len(step_models),
                "loaded_models": loaded_count,
                "healthy_models": healthy_count,
                "total_memory_usage_mb": total_memory,
                "readiness_score": loaded_count / max(1, len(step_models)),
                "health_score": healthy_count / max(1, len(step_models))
            }
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {step_name}: {e}")
            return {"step_name": step_name, "error": str(e)}

    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ê°œì„ )"""
        try:
            if model_name in self.model_cache:
                del self.model_cache[model_name]
                
            if model_name in self.loaded_models:
                del self.loaded_models[model_name]
                
            if model_name in self.available_models:
                self.available_models[model_name]["loaded"] = False
                
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device in ["mps", "cuda"]:
                safe_mps_empty_cache()
                    
            gc.collect()
            
            self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False

    # ==============================================
    # ğŸ”¥ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì§„ë‹¨ ë©”ì„œë“œë“¤ (ê°œì„ )
    # ==============================================

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë” ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ - BaseStepMixinì—ì„œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ê°œì„ )"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_memory = sum(cache_entry.memory_usage_mb for cache_entry in self.model_cache.values())
            
            # ë¡œë”© ì‹œê°„ í†µê³„
            load_times = list(self.load_times.values())
            avg_load_time = sum(load_times) / len(load_times) if load_times else 0
            
            # ê²€ì¦ í†µê³„
            validation_rate = (self.performance_stats['validation_success'] / 
                             max(1, self.performance_stats['validation_count']))
            
            return {
                "model_counts": {
                    "loaded": len(self.model_cache),
                    "registered": len(self.model_configs),
                    "available": len(self.available_models),
                    "total_found": self.performance_stats.get('total_models_found', 0)
                },
                "memory_usage": {
                    "total_mb": total_memory,
                    "average_per_model_mb": total_memory / len(self.model_cache) if self.model_cache else 0,
                    "device": self.device,
                    "available_memory_gb": self.memory_gb
                },
                "performance_stats": {
                    "cache_hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['models_loaded']),
                    "average_load_time_sec": avg_load_time,
                    "total_models_loaded": self.performance_stats['models_loaded'],
                    "checkpoint_loads": self.performance_stats.get('checkpoint_loads', 0),
                    "validation_rate": validation_rate,
                    "validation_count": self.performance_stats['validation_count'],
                    "validation_success": self.performance_stats['validation_success']
                },
                "step_interfaces": len(self.step_interfaces),
                "system_info": {
                    "conda_env": self.conda_env,
                    "is_m3_max": self.is_m3_max,
                    "torch_available": TORCH_AVAILABLE,
                    "mps_available": MPS_AVAILABLE
                },
                "health_status": {
                    "healthy_models": sum(1 for entry in self.model_cache.values() if entry.is_healthy),
                    "total_errors": sum(entry.error_count for entry in self.model_cache.values()),
                    "version": "20.0"
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def validate_all_models(self) -> Dict[str, CheckpointValidation]:
        """ëª¨ë“  ëª¨ë¸ ê²€ì¦ ì‹¤í–‰"""
        validation_results = {}
        
        try:
            for model_name, config in self.model_configs.items():
                if config.checkpoint_path:
                    validation = self.validator.validate_checkpoint_file(config.checkpoint_path)
                    validation_results[model_name] = validation
                    
                    # ì„¤ì • ì—…ë°ì´íŠ¸
                    config.validation = validation
                    config.last_validated = time.time()
                    
                    self.performance_stats['validation_count'] += 1
                    if validation.is_valid:
                        self.performance_stats['validation_success'] += 1
            
            valid_count = sum(1 for v in validation_results.values() if v.is_valid)
            total_count = len(validation_results)
            
            self.logger.info(f"âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ: {valid_count}/{total_count} ì„±ê³µ")
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return validation_results

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ê°œì„ )"""
        self.logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.model_cache.keys()):
                self.unload_model(model_name)
                
            # ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_torch_cleanup()
            
            self.logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€, ê°œì„ )
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê°œì„ )"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì™„ì „ ê°œì„ ëœ ModelLoader v20.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” (ê°œì„ )"""
    try:
        loader = get_global_model_loader()
        
        # ë¹„ë™ê¸° ê²€ì¦ ì‹¤í–‰
        validation_results = await asyncio.get_event_loop().run_in_executor(
            None, loader.validate_all_models
        )
        
        valid_count = sum(1 for v in validation_results.values() if v.is_valid)
        total_count = len(validation_results)
        
        if total_count > 0:
            logger.info(f"âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ê²€ì¦: {valid_count}/{total_count}")
        else:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        return loader
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜, ê°œì„ )
# ==============================================

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „ (ê°œì„ )"""
    try:
        loader = get_global_model_loader()
        
        # Step ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ ë“±ë¡
        if step_requirements:
            loader.register_step_requirements(step_name, step_requirements)
        
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        # í´ë°±ìœ¼ë¡œ ì§ì ‘ ìƒì„±
        return StepModelInterface(get_global_model_loader(), step_name)

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ í•¨ìˆ˜"""
    return CheckpointValidator.validate_checkpoint_file(checkpoint_path)

def safe_load_checkpoint(checkpoint_path: Union[str, Path], device: str = "cpu") -> Optional[Any]:
    """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        # ê²€ì¦ ë¨¼ì € ì‹¤í–‰
        validation = validate_checkpoint_file(checkpoint_path)
        if not validation.is_valid:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {validation.error_message}")
            return None
        
        if TORCH_AVAILABLE:
            import torch
            
            # ì•ˆì „í•œ ë¡œë”© ì‹œë„
            try:
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
                    logger.debug(f"âœ… ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    return checkpoint
                except Exception as e:
                    logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    return None
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ (ê°œì„ )
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜ (ê°œì„ )"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜ (ê°œì„ )"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ì •ì˜ (ê°œì„ )
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'CheckpointValidator',
    'SafeAsyncContextManager',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType', 
    'ModelConfig',
    'SafeModelCacheEntry',
    'CheckpointValidation',
    'LoadingStatus',
    'StepPriority',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader_async',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'create_step_interface',
    'validate_checkpoint_file',
    'safe_load_checkpoint',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_model',
    'get_model_async',
    
    # ì•ˆì „í•œ í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_torch_cleanup',
    'get_enhanced_memory_info',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ í™•ì¸ ë©”ì‹œì§€ (ê°œì„ )
# ==============================================

logger.info("=" * 80)
logger.info("âœ… ì™„ì „ ê°œì„ ëœ ModelLoader v20.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ”¥ Human Parsing ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì™„ì „ í•´ê²°")
logger.info("âœ… __aenter__ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì •")
logger.info("âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œìŠ¤í…œ êµ¬í˜„")
logger.info("âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” + M3 Max 128GB ì™„ì „ í™œìš©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING + ì˜ì¡´ì„± ì£¼ì…)")
logger.info("âœ… BaseStepMixin 100% í˜¸í™˜ ìœ ì§€")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê°•í™”")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜")
logger.info("âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”")
logger.info("âœ… ì‹¤ì‹œê°„ ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ")
logger.info("=" * 80)

memory_info = get_enhanced_memory_info()
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë³´:")
logger.info(f"   - ì´ ë©”ëª¨ë¦¬: {memory_info['total_gb']:.1f}GB")
logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥: {memory_info['available_gb']:.1f}GB")
logger.info(f"   - conda í™˜ê²½: {memory_info['conda_env']}")
logger.info(f"   - M3 Max: {'âœ…' if memory_info['is_m3_max'] else 'âŒ'}")

logger.info("=" * 80)
logger.info("ğŸš€ ì™„ì „ ê°œì„ ëœ ModelLoader v20.0 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… Human Parsing ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("   âœ… ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € êµ¬í˜„")
logger.info("   âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê°•í™”ë¡œ ì•ˆì •ì„± ë³´ì¥")
logger.info("   âœ… BaseStepMixin ì™„ë²½ í˜¸í™˜ ìœ ì§€")
logger.info("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° ì„±ëŠ¥")
logger.info("=" * 80)