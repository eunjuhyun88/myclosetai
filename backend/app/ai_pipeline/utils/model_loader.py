# app/ai_pipeline/utils/model_loader.py
"""
üçé MyCloset AI - ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî ModelLoader ÏãúÏä§ÌÖú v8.0 FINAL - üî• Î™®Îì† Í∏∞Îä• ÏôÑÏ†Ñ ÌÜµÌï©
========================================================================================

‚úÖ initialize_global_model_loader Dict Î∞òÌôò Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞
‚úÖ ModelLoader Í∞ùÏ≤¥ ÏßÅÏ†ë Î∞òÌôòÏúºÎ°ú Î≥ÄÍ≤Ω
‚úÖ ÎπÑÎèôÍ∏∞/ÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ï≤¥Ïù∏ ÏôÑÏ†Ñ Ï†ïÎ¶¨
‚úÖ Coroutine Î¨∏Ï†ú ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ FallbackModel await Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ Dict callable Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ
‚úÖ Ï¥àÍ∏∞Ìôî ÏàúÏÑú Î™ÖÌôïÌôî: DeviceManager ‚Üí ModelLoader ‚Üí StepInterface
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî ÏôÑÏÑ±
‚úÖ conda ÌôòÍ≤Ω ÏôÑÎ≤Ω ÏßÄÏõê
‚úÖ ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ïÏÑ± ÏµúÍ≥† ÏàòÏ§Ä
‚úÖ Î™®Îì† Í∏∞Ï°¥ Í∏∞Îä•Î™Ö/ÌÅ¥ÎûòÏä§Î™Ö 100% Ïú†ÏßÄ
‚úÖ ÎπÑÎèôÍ∏∞ Ìò∏ÌôòÏÑ± ÏôÑÎ≤Ω ÏßÄÏõê
‚úÖ Step ÌååÏùºÎì§Í≥º 100% Ìò∏Ìôò

Author: MyCloset AI Team
Date: 2025-07-20
Version: 8.0 FINAL (Complete Integration + Async Compatibility)
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
import json
import pickle
import sqlite3
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache, wraps
from concurrent.futures import ThreadPoolExecutor
import weakref

# ==============================================
# üî• ÎùºÏù¥Î∏åÎü¨Î¶¨ Ìò∏ÌôòÏÑ± Î∞è ÏïàÏ†ÑÌïú ÏûÑÌè¨Ìä∏
# ==============================================

class LibraryCompatibility:
    """ÎùºÏù¥Î∏åÎü¨Î¶¨ Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨ Î∞è Í¥ÄÎ¶¨"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.cv_available = False
        self.transformers_available = False
        self.diffusers_available = False
        self.coreml_available = False
        
        self._check_numpy_compatibility()
        self._check_torch_compatibility()
        self._check_optional_libraries()
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨"""
        try:
            import numpy as np
            self.numpy_available = True
            self.numpy_version = np.__version__
            
            major_version = int(self.numpy_version.split('.')[0])
            if major_version >= 2:
                logging.warning(f"‚ö†Ô∏è NumPy {self.numpy_version} Í∞êÏßÄÎê®. NumPy 1.x Í∂åÏû•")
                logging.warning("üîß Ìï¥Í≤∞Î∞©Î≤ï: conda install numpy=1.24.3 -y --force-reinstall")
                try:
                    np.set_printoptions(legacy='1.25')
                    logging.info("‚úÖ NumPy 2.x Ìò∏ÌôòÏÑ± Î™®Îìú ÌôúÏÑ±Ìôî")
                except:
                    pass
            
            globals()['np'] = np
            
        except ImportError as e:
            self.numpy_available = False
            logging.error(f"‚ùå NumPy import Ïã§Ìå®: {e}")
    
    def _check_torch_compatibility(self):
        """PyTorch Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨"""
        try:
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.default_device = "cpu"
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.mps_available = True
                self.default_device = "mps"
                logging.info("‚úÖ M3 Max MPS ÏÇ¨Ïö© Í∞ÄÎä•")
            else:
                self.mps_available = False
                logging.info("‚ÑπÔ∏è CPU Î™®Îìú ÏÇ¨Ïö©")
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError as e:
            self.torch_available = False
            self.mps_available = False
            self.default_device = "cpu"
            logging.warning(f"‚ö†Ô∏è PyTorch ÏóÜÏùå: {e}")
    
    def _check_optional_libraries(self):
        """ÏÑ†ÌÉùÏ†Å ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§ Ï≤¥ÌÅ¨"""
        try:
            import cv2
            from PIL import Image, ImageEnhance
            self.cv_available = True
            globals()['cv2'] = cv2
            globals()['Image'] = Image
            globals()['ImageEnhance'] = ImageEnhance
        except ImportError:
            self.cv_available = False
        
        try:
            from transformers import AutoModel, AutoTokenizer, AutoConfig
            self.transformers_available = True
        except ImportError:
            self.transformers_available = False
        
        try:
            from diffusers import StableDiffusionPipeline, UNet2DConditionModel
            self.diffusers_available = True
        except ImportError:
            self.diffusers_available = False
        
        try:
            import coremltools as ct
            self.coreml_available = True
        except ImportError:
            self.coreml_available = False

# Ï†ÑÏó≠ Ìò∏ÌôòÏÑ± Í¥ÄÎ¶¨Ïûê
_compat = LibraryCompatibility()

# ÏÉÅÏàò ÏÑ§Ï†ï
NUMPY_AVAILABLE = _compat.numpy_available
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
CV_AVAILABLE = _compat.cv_available
DEFAULT_DEVICE = _compat.default_device

logger = logging.getLogger(__name__)

# ==============================================
# üî• ÌïµÏã¨ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞
# ==============================================

class ModelFormat(Enum):
    """Î™®Îç∏ Ìè¨Îß∑ Ï†ïÏùò"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelType(Enum):
    """AI Î™®Îç∏ ÌÉÄÏûÖ"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelConfig:
    """Î™®Îç∏ ÏÑ§Ï†ï Ï†ïÎ≥¥"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """StepÎ≥Ñ ÌäπÌôî Î™®Îç∏ ÏÑ§Ï†ï"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# üî• Step ÏöîÏ≤≠ÏÇ¨Ìï≠ ÌÜµÌï©
# ==============================================

STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "ClothWarpingStep": {
        "model_name": "cloth_warping_net",
        "model_type": "ClothWarpingModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*warping*.pth", "*flow*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

# ==============================================
# üî• SafeFunctionValidator v8.0 - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞
# ==============================================

class SafeFunctionValidator:
    """
    üî• Ìï®Ïàò/Î©îÏÑúÎìú/Í∞ùÏ≤¥ Ìò∏Ï∂ú ÏïàÏ†ÑÏÑ± Í≤ÄÏ¶ù ÌÅ¥ÎûòÏä§ v8.0 - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞
    """
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """Í∞ùÏ≤¥Í∞Ä ÏïàÏ†ÑÌïòÍ≤å Ìò∏Ï∂ú Í∞ÄÎä•ÌïúÏßÄ Í≤ÄÏ¶ù"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            # DictÎäî Î¨¥Ï°∞Í±¥ callableÌïòÏßÄ ÏïäÏùå
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            # Coroutine Í∞ùÏ≤¥ Ï≤¥ÌÅ¨
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            # Async function Ï≤¥ÌÅ¨  
            if asyncio.iscoroutinefunction(obj):
                return False, f"Object is async function, need await in context: {context}", None
            
            # Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ï≤¥ÌÅ¨
            basic_types = (str, int, float, bool, list, tuple, set, bytes, bytearray)
            if isinstance(obj, basic_types):
                return False, f"Object is basic data type {type(obj)}, not callable", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            # Ìï®Ïàò/Î©îÏÑúÎìú ÌÉÄÏûÖÎ≥Ñ Í≤ÄÏ¶ù
            import types
            if isinstance(obj, (types.FunctionType, types.MethodType, types.BuiltinFunctionType, types.BuiltinMethodType)):
                return True, "Valid function/method", obj
            
            # ÌÅ¥ÎûòÏä§ Ïù∏Ïä§ÌÑ¥Ïä§Ïùò __call__ Î©îÏÑúÎìú Ï≤¥ÌÅ¨
            if hasattr(obj, '__call__'):
                call_method = getattr(obj, '__call__')
                if callable(call_method) and not isinstance(call_method, dict):
                    return True, "Valid callable object with __call__", obj
                else:
                    return False, "__call__ method is dict, not callable", None
            
            if callable(obj):
                return True, "Generic callable object", obj
            
            return False, f"Unknown callable validation failure for {type(obj)}", None
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except TypeError as e:
                error_msg = str(e)
                if "not callable" in error_msg.lower():
                    return False, None, f"Runtime callable error: {error_msg}"
                else:
                    return False, None, f"Type error in call: {error_msg}"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"

# ==============================================
# üî• Device & Memory Management
# ==============================================

class DeviceManager:
    """ÎîîÎ∞îÏù¥Ïä§ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        
    def _detect_available_devices(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§ ÌÉêÏßÄ"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                devices.append("mps")
                self.logger.info("‚úÖ M3 Max MPS ÏÇ¨Ïö© Í∞ÄÎä•")
            
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"üî• CUDA ÎîîÎ∞îÏù¥Ïä§: {cuda_devices}")
        
        self.logger.info(f"üîç ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max Ïπ© Í∞êÏßÄ"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ÏöîÏ≤≠Îêú ÎîîÎ∞îÏù¥Ïä§Î•º Ïã§Ï†ú ÎîîÎ∞îÏù¥Ïä§Î°ú Î≥ÄÌôò"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"‚ö†Ô∏è ÏöîÏ≤≠Îêú ÎîîÎ∞îÏù¥Ïä§ {requested_device} ÏÇ¨Ïö© Î∂àÍ∞Ä, {self.optimal_device} ÏÇ¨Ïö©")
            return self.optimal_device

class ModelMemoryManager:
    """Î™®Îç∏ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, device: str = "mps", memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max Ïπ© Í∞êÏßÄ"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ (GB) Î∞òÌôò"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)  # 128GB Ï§ë ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∂ÄÎ∂Ñ
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return 8.0
    
    def cleanup_memory(self):
        """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            logger.debug("üßπ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• AI Model Classes
# ==============================================

class BaseModel:
    """Í∏∞Î≥∏ AI Î™®Îç∏ ÌÅ¥ÎûòÏä§"""
    def __init__(self):
        self.model_name = "BaseModel"
        self.device = "cpu"
    
    def forward(self, x):
        return x
    
    def __call__(self, x):
        return self.forward(x)

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy Ïù∏Ï≤¥ ÌååÏã± Î™®Îç∏"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            self.paf_head = nn.Conv2d(512, 38, 1)
            self.heatmap_head = nn.Conv2d(512, 19, 1)
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """U¬≤-Net ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """Í∏∞ÌïòÌïôÏ†Å Îß§Ïπ≠ Î™®Îç∏"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

else:
    # PyTorch ÏóÜÎäî Í≤ΩÏö∞ ÎçîÎØ∏ ÌÅ¥ÎûòÏä§Îì§
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel

# ==============================================
# üî• SafeModelService v8.0 - Dict Callable ÏôÑÏ†Ñ Ìï¥Í≤∞
# ==============================================

class SafeModelService:
    """ÏïàÏ†ÑÌïú Î™®Îç∏ ÏÑúÎπÑÏä§ v8.0"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        
    def register_model(self, name: str, model: Any) -> bool:
        """Î™®Îç∏ Îì±Î°ù - DictÎ•º CallableÎ°ú Î≥ÄÌôò"""
        try:
            with self.lock:
                if isinstance(model, dict):
                    # DictÎ•º callable wrapperÎ°ú Î≥ÄÌôò
                    wrapper = self._create_callable_dict_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"üìù ÎîïÏÖîÎÑàÎ¶¨ Î™®Îç∏ÏùÑ callable wrapperÎ°ú Îì±Î°ù: {name}")
                elif callable(model):
                    is_callable, reason, safe_model = self.validator.validate_callable(model, f"register_{name}")
                    if is_callable:
                        self.models[name] = safe_model
                        self.logger.info(f"üìù Í≤ÄÏ¶ùÎêú callable Î™®Îç∏ Îì±Î°ù: {name}")
                    else:
                        wrapper = self._create_object_wrapper(model)
                        self.models[name] = wrapper
                        self.logger.warning(f"‚ö†Ô∏è ÏïàÏ†ÑÌïòÏßÄ ÏïäÏùÄ callable Î™®Îç∏ÏùÑ wrapperÎ°ú Îì±Î°ù: {name}")
                else:
                    wrapper = self._create_object_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"üìù Í∞ùÏ≤¥ Î™®Îç∏ÏùÑ wrapperÎ°ú Îì±Î°ù: {name}")
                
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def _create_callable_dict_wrapper(self, model_dict: Dict[str, Any]) -> Callable:
        """üî• ÎπÑÎèôÍ∏∞ Ìò∏Ìôò ÎîïÏÖîÎÑàÎ¶¨Î•º callable wrapperÎ°ú Î≥ÄÌôò"""
        
        class AsyncCompatibleDictWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.type = data.get('type', 'dict_model')
                self.call_count = 0
                self.last_call_time = None
            
            def __call__(self, *args, **kwargs):
                """ÎèôÍ∏∞ Ìò∏Ï∂ú"""
                self.call_count += 1
                self.last_call_time = time.time()
                
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'async_compatible_dict'
                    }
                }
            
            def get_info(self):
                return {
                    **self.data,
                    'wrapper_info': {
                        'type': 'async_compatible_dict_wrapper',
                        'call_count': self.call_count,
                        'last_call_time': self.last_call_time
                    }
                }
            
            def warmup(self):
                try:
                    test_result = self()
                    return test_result.get('status') == 'success'
                except Exception:
                    return False
            
            # üî• ÌïµÏã¨ Ï∂îÍ∞Ä: __await__ Î©îÏÑúÎìú
            def __await__(self):
                """await ÏßÄÏõê"""
                async def _async_wrapper():
                    return self()  # ÎèôÍ∏∞ __call__ Í≤∞Í≥º Î∞òÌôò
                return _async_wrapper().__await__()
        
        return AsyncCompatibleDictWrapper(model_dict)
    
    def _create_object_wrapper(self, obj: Any) -> Callable:
        """ÏùºÎ∞ò Í∞ùÏ≤¥Î•º callable wrapperÎ°ú Î≥ÄÌôò"""
        
        class ObjectWrapper:
            def __init__(self, wrapped_obj: Any):
                self.wrapped_obj = wrapped_obj
                self.name = getattr(wrapped_obj, 'name', str(type(wrapped_obj).__name__))
                self.type = type(wrapped_obj).__name__
                self.call_count = 0
                self.last_call_time = None
                self.original_callable = callable(wrapped_obj)
            
            def __call__(self, *args, **kwargs):
                self.call_count += 1
                self.last_call_time = time.time()
                
                if self.original_callable:
                    validator = SafeFunctionValidator()
                    success, result, message = validator.safe_call(self.wrapped_obj, *args, **kwargs)
                    
                    if success:
                        return result
                    else:
                        return self._create_mock_response("call_failed", message)
                
                return self._create_mock_response("not_callable")
            
            def _create_mock_response(self, reason: str, details: str = ""):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'wrapped_type': self.type,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'object',
                        'reason': reason,
                        'details': details
                    }
                }
            
            def __getattr__(self, name):
                if hasattr(self.wrapped_obj, name):
                    attr = getattr(self.wrapped_obj, name)
                    if callable(attr):
                        validator = SafeFunctionValidator()
                        return lambda *args, **kwargs: validator.safe_call(attr, *args, **kwargs)[1]
                    else:
                        return attr
                else:
                    raise AttributeError(f"'{self.type}' object has no attribute '{name}'")
            
            # üî• Ï∂îÍ∞Ä: await ÏßÄÏõê
            def __await__(self):
                async def _async_result():
                    return self()
                return _async_result().__await__()
        
        return ObjectWrapper(obj)
    
    def call_model(self, name: str, *args, **kwargs) -> Any:
        """Î™®Îç∏ Ìò∏Ï∂ú - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏Ïù¥ Îì±Î°ùÎêòÏßÄ ÏïäÏùå: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                # DictÍ∞Ä ÏïÑÎãå callable ÌôïÏù∏
                if isinstance(model, dict):
                    self.logger.error(f"‚ùå Îì±Î°ùÎêú Î™®Îç∏Ïù¥ dictÏûÖÎãàÎã§: {name}")
                    return None
                
                success, result, message = self.validator.safe_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"‚úÖ Î™®Îç∏ Ìò∏Ï∂ú ÏÑ±Í≥µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ìò∏Ï∂ú Ïã§Ìå®: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ìò∏Ï∂ú Ïò§Î•ò {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    async def call_model_async(self, name: str, *args, **kwargs) -> Any:
        """üî• Î™®Îç∏ Ìò∏Ï∂ú - ÎπÑÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏Ïù¥ Îì±Î°ùÎêòÏßÄ ÏïäÏùå: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                # ÎπÑÎèôÍ∏∞ Ìò∏Ï∂ú ÏãúÎèÑ
                try:
                    if hasattr(model, '__await__'):
                        result = await model
                    elif hasattr(model, '__call__'):
                        result = model(*args, **kwargs)
                    else:
                        result = model
                    
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"‚úÖ ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ìò∏Ï∂ú ÏÑ±Í≥µ: {name}")
                    return result
                    
                except Exception as e:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ìò∏Ï∂ú Ïã§Ìå®: {name} - {e}")
                    # Ìè¥Î∞±: ÎèôÍ∏∞ Î∞©Ïãù ÏãúÎèÑ
                    return self.call_model(name, *args, **kwargs)
                
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ìò∏Ï∂ú Ïò§Î•ò {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îç∏ Î™©Î°ù"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {}),
                        'async_compatible': True
                    }
                return result
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}

# ==============================================
# üî• StepModelInterface v8.0 - ÏôÑÏ†Ñ Íµ¨ÌòÑ
# ==============================================

class StepModelInterface:
    """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ v8.0 - ÏôÑÏ†Ñ Íµ¨ÌòÑ"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # Î™®Îç∏ Ï∫êÏãú
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        
        # Step ÏöîÏ≤≠ Ï†ïÎ≥¥ Î°úÎìú
        self.step_request = STEP_MODEL_REQUESTS.get(step_name)
        self.recommended_models = self._get_recommended_models()
        
        # Ï∂îÍ∞Ä ÏÜçÏÑ±Îì§
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        
        self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _get_recommended_models(self) -> List[str]:
        """StepÎ≥Ñ Í∂åÏû• Î™®Îç∏ Î™©Î°ù"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_u2net"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """üî• Î™®Îç∏ Î°úÎìú - ÎπÑÎèôÍ∏∞ Ìò∏Ìôò Î≤ÑÏ†Ñ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # Ï∫êÏãú ÌôïÏù∏
            if model_name in self.loaded_models:
                self.logger.info(f"‚úÖ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_name}")
                return self.loaded_models[model_name]
            
            # SafeModelServiceÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú (ÎèôÍ∏∞ Î∞©Ïãù)
            model = self.model_loader.safe_model_service.call_model(model_name)
            
            if model:
                with self._lock:
                    self.loaded_models[model_name] = model
                    self.model_status[model_name] = "loaded"
                self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                return model
            else:
                # Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±
                fallback = self._create_fallback_model(model_name)
                with self._lock:
                    self.loaded_models[model_name] = fallback
                    self.model_status[model_name] = "fallback"
                self.logger.warning(f"‚ö†Ô∏è Ìè¥Î∞± Î™®Îç∏ ÏÇ¨Ïö©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            fallback = self._create_fallback_model(model_name)
            with self._lock:
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "error_fallback"
            return fallback
    
    def _create_fallback_model(self, model_name: str) -> Any:
        """üî• ÎπÑÎèôÍ∏∞ Ìò∏Ìôò Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±"""
        
        class AsyncCompatibleFallbackModel:
            def __init__(self, name: str, step_name: str):
                self.name = name
                self.step_name = step_name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                """ÎèôÍ∏∞ Ìò∏Ï∂ú"""
                return self.forward(*args, **kwargs)
            
            def forward(self, *args, **kwargs):
                """Ïã§Ï†ú Ï∂îÎ°† Î°úÏßÅ"""
                # StepÎ≥Ñ Ï†ÅÏ†àÌïú Ï∂úÎ†• ÌÅ¨Í∏∞ Î∞òÌôò
                if "human_parsing" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 20, 512, 512)
                    else:
                        return [[[[0.0 for _ in range(512)] for _ in range(512)] for _ in range(20)]]
                elif "pose" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return [(torch.zeros(1, 38, 46, 46), torch.zeros(1, 19, 46, 46))]
                    else:
                        return [([[0.0 for _ in range(46)] for _ in range(46)] for _ in range(38)),
                               ([[0.0 for _ in range(46)] for _ in range(46)] for _ in range(19))]
                elif "segmentation" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 1, 320, 320)
                    else:
                        return [[[0.0 for _ in range(320)] for _ in range(320)]]
                else:
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 3, 512, 512)
                    else:
                        return [[[[0.0 for _ in range(512)] for _ in range(512)] for _ in range(3)]]
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
            
            def cpu(self):
                self.device = "cpu"
                return self
            
            # üî• ÌïµÏã¨ Ï∂îÍ∞Ä: __await__ Î©îÏÑúÎìú
            def __await__(self):
                """ÎπÑÎèôÍ∏∞ Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú await Í∞ÄÎä•ÌïòÎèÑÎ°ù"""
                async def _async_wrapper():
                    return self  # ÏûêÍ∏∞ ÏûêÏã†ÏùÑ Î∞òÌôò
                return _async_wrapper().__await__()
        
        return AsyncCompatibleFallbackModel(model_name, self.step_name)
    
    # ÎèôÍ∏∞ Î≤ÑÏ†ÑÎèÑ Ï†úÍ≥µ (Ìò∏ÌôòÏÑ±)
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """Î™®Îç∏ Î°úÎìú - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        import asyncio
        
        try:
            # Ïù¥Î≤§Ìä∏ Î£®ÌîÑÍ∞Ä ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ëÏù∏ Î£®ÌîÑÏóêÏÑúÎäî ÌÉúÏä§ÌÅ¨Î°ú Ïã§Ìñâ
                    future = asyncio.ensure_future(self.get_model(model_name))
                    # Í∞ÑÎã®Ìïú Ìè¥ÎßÅÏúºÎ°ú Í≤∞Í≥º ÎåÄÍ∏∞
                    import time
                    while not future.done():
                        time.sleep(0.001)
                    return future.result()
                else:
                    return loop.run_until_complete(self.get_model(model_name))
            except RuntimeError:
                # ÏÉà Ïù¥Î≤§Ìä∏ Î£®ÌîÑ ÏÉùÏÑ±
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    return loop.run_until_complete(self.get_model(model_name))
                finally:
                    loop.close()
                    
        except Exception as e:
            self.logger.error(f"‚ùå ÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            return self._create_fallback_model(model_name or "sync_error")
    
    async def list_available_models(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù Î∞òÌôò"""
        try:
            with self._lock:
                # Îì±Î°ùÎêú Î™®Îç∏Îì§
                registered_models = list(self.loaded_models.keys())
                
                # Ï∂îÏ≤ú Î™®Îç∏Îì§
                recommended = self.recommended_models.copy()
                
                # SafeModelServiceÏóê Îì±Î°ùÎêú Î™®Îç∏Îì§
                safe_models = list(self.model_loader.safe_model_service.models.keys())
                
                # Ï§ëÎ≥µ Ï†úÍ±∞ÌïòÏó¨ Î∞òÌôò
                all_models = list(set(registered_models + recommended + safe_models))
                
                self.available_models = all_models
                return all_models
                
        except Exception as e:
            self.logger.error(f"‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return self.recommended_models
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"üìù Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå® {model_name}: {e}")
            return False

# ==============================================
# üî• Main ModelLoader Class v8.0 - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞
# ==============================================

class ModelLoader:
    """ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî ModelLoader v8.0 - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        **kwargs
    ):
        """ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî ÏÉùÏÑ±Ïûê - ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ"""
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # SafeModelService ÌÜµÌï©
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        
        # ÎîîÎ∞îÏù¥Ïä§ Î∞è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # Î™®Îç∏ Î°úÎçî ÌäπÌôî ÌååÎùºÎØ∏ÌÑ∞
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # Î™®Îç∏ Ï∫êÏãú Î∞è ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í¥ÄÎ¶¨
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ÎèôÍ∏∞Ìôî Î∞è Ïä§Î†àÎìú Í¥ÄÎ¶¨
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        
        # ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú
        self.enable_auto_detection = enable_auto_detection
        self.detected_model_registry = {}
        
        # Ï¥àÍ∏∞Ìôî Ïã§Ìñâ
        self._initialize_components()
        
        # ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú ÏÑ§Ï†ï
        if self.enable_auto_detection:
            self._setup_auto_detection()
        
        self.logger.info(f"üéØ ModelLoader v8.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"üîß Device: {self.device}, SafeModelService: ‚úÖ")
    
    def _initialize_components(self):
        """Î™®Îì† Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî"""
        try:
            # Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max ÌäπÌôî ÏÑ§Ï†ï
            if self.is_m3_max:
                self.use_fp16 = True
                if _compat.coreml_available:
                    self.logger.info("üçé CoreML ÏµúÏ†ÅÌôî ÌôúÏÑ±ÌôîÎê®")
            
            # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú
            self._load_step_requirements()
            
            # Í∏∞Î≥∏ Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî
            self._initialize_model_registry()
            
            self.logger.info(f"üì¶ ModelLoader Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
        except Exception as e:
            self.logger.error(f"‚ùå Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _load_step_requirements(self):
        """Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú"""
        try:
            self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {step_name} ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú Ïã§Ìå®: {e}")
                    continue
            
            self.logger.info(f"üìù {loaded_steps}Í∞ú Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú Ïã§Ìå®: {e}")
    
    def _initialize_model_registry(self):
        """Í∏∞Î≥∏ Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # Î™®Îç∏ Îì±Î°ù
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"üìù Í∏∞Î≥∏ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {registered_count}Í∞ú")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _setup_auto_detection(self):
        """ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        try:
            self.logger.info("üîç ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ï§ë...")
            self._detect_available_models()
            self.logger.info("‚úÖ ÏûêÎèô ÌÉêÏßÄ ÏôÑÎ£å")
                
        except Exception as e:
            self.logger.error(f"‚ùå ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _detect_available_models(self):
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÌÉêÏßÄ"""
        try:
            detected_count = 0
            search_paths = [
                self.model_cache_dir,
                Path.cwd() / "models",
                Path.cwd() / "checkpoints"
            ]
            
            for search_path in search_paths:
                if search_path.exists():
                    for file_path in search_path.rglob("*.pth"):
                        if file_path.is_file():
                            model_name = file_path.stem
                            model_info = {
                                'path': str(file_path),
                                'size_mb': file_path.stat().st_size / (1024 * 1024),
                                'auto_detected': True
                            }
                            self.detected_model_registry[model_name] = model_info
                            detected_count += 1
            
            self.logger.info(f"üîç {detected_count}Í∞ú Î™®Îç∏ ÌååÏùº ÌÉêÏßÄÎê®")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÌÉêÏßÄ Ïã§Ìå®: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """Î™®Îç∏ Îì±Î°ù"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                # SafeModelServiceÏóêÎèÑ Îì±Î°ù
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device
                }
                self.safe_model_service.register_model(name, model_dict)
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"üìù Î™®Îç∏ Îì±Î°ù: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def initialize(self) -> bool:
        """üî• ModelLoader Ï¥àÍ∏∞Ìôî Î©îÏÑúÎìú - ÏàúÏàò ÎèôÍ∏∞ Î≤ÑÏ†Ñ (Í∑ºÎ≥∏ Ìï¥Í≤∞)"""
        try:
            self.logger.info("üöÄ ModelLoader v8.0 Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            # Í∏∞Î≥∏ Í≤ÄÏ¶ù
            if not hasattr(self, 'device_manager'):
                self.logger.warning("‚ö†Ô∏è ÎîîÎ∞îÏù¥Ïä§ Îß§ÎãàÏ†ÄÍ∞Ä ÏóÜÏùå")
                return False
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (ÎèôÍ∏∞)
            if hasattr(self, 'memory_manager'):
                try:
                    self.memory_manager.cleanup_memory()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
            self.logger.info("‚úÖ ModelLoader v8.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def create_step_interface(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± - ÏôÑÏ†Ñ Íµ¨ÌòÑ"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements Ï≤òÎ¶¨
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"‚ö†Ô∏è {req_name} ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return StepModelInterface(self, step_name)
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îì† Î™®Îç∏ Î™©Î°ù"""
        try:
            with self._lock:
                models_info = {}
                
                for model_name in self.model_configs.keys():
                    models_info[model_name] = {
                        'name': model_name,
                        'registered': True,
                        'device': self.device,
                        'config': self.model_configs[model_name]
                    }
                
                if hasattr(self, 'detected_model_registry'):
                    for model_name in self.detected_model_registry.keys():
                        if model_name not in models_info:
                            models_info[model_name] = {
                                'name': model_name,
                                'auto_detected': True,
                                'info': self.detected_model_registry[model_name]
                            }
                
                safe_models = self.safe_model_service.list_models()
                for model_name, status in safe_models.items():
                    if model_name not in models_info:
                        models_info[model_name] = {
                            'name': model_name,
                            'source': 'SafeModelService',
                            'status': status
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}
    
    def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú - ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú ÎûòÌïë"""
        try:
            return self.load_model_sync(model_name, **kwargs)
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return None
    
    def load_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú"""
        try:
            cache_key = f"{model_name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # Ï∫êÏãúÎêú Î™®Îç∏ ÌôïÏù∏
                if cache_key in self.model_cache:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"üì¶ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_name}")
                    return self.model_cache[cache_key]
                
                # SafeModelService Ïö∞ÏÑ† ÏÇ¨Ïö©
                model = self.safe_model_service.call_model(model_name)
                if model:
                    self.model_cache[cache_key] = model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"‚úÖ SafeModelServiceÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                    return model
                
                # Î™®Îç∏ ÏÑ§Ï†ï ÌôïÏù∏
                if model_name not in self.model_configs:
                    self.logger.warning(f"‚ö†Ô∏è Îì±Î°ùÎêòÏßÄ ÏïäÏùÄ Î™®Îç∏: {model_name}")
                    # Í∏∞Î≥∏ Î™®Îç∏ Îì±Î°ù ÏãúÎèÑ
                    default_config = {
                        'name': model_name,
                        'type': 'unknown',
                        'device': self.device
                    }
                    self.safe_model_service.register_model(model_name, default_config)
                    model = self.safe_model_service.call_model(model_name)
                    if model:
                        self.model_cache[cache_key] = model
                        return model
                    else:
                        return None
                
                start_time = time.time()
                model_config = self.model_configs[model_name]
                
                self.logger.info(f"üì¶ Î™®Îç∏ Î°úÎî© ÏãúÏûë: {model_name}")
                
                # Î©îÎ™®Î¶¨ ÏïïÎ∞ï ÌôïÏù∏ Î∞è Ï†ïÎ¶¨
                self._check_memory_and_cleanup_sync()
                
                # Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
                model = self._create_model_instance_sync(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {model_name}")
                    return None
                
                # ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô
                if hasattr(model, 'to'):
                    to_method = getattr(model, 'to', None)
                    success, result, message = self.function_validator.safe_call(to_method, self.device)
                    if success:
                        model = result
                
                # M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©
                if self.is_m3_max and self.optimization_enabled:
                    model = self._apply_m3_max_optimization_sync(model, model_config)
                
                # FP16 ÏµúÏ†ÅÌôî
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        half_method = getattr(model, 'half', None)
                        success, result, message = self.function_validator.safe_call(half_method)
                        if success:
                            model = result
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è FP16 Î≥ÄÌôò Ïã§Ìå®: {e}")
                
                # ÌèâÍ∞Ä Î™®Îìú
                if hasattr(model, 'eval'):
                    eval_method = getattr(model, 'eval', None)
                    self.function_validator.safe_call(eval_method)
                
                # Ï∫êÏãúÏóê Ï†ÄÏû•
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {model_name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            return None
    
    def _create_model_instance_sync(
        self,
        model_config: Union[ModelConfig, StepModelConfig],
        **kwargs
    ) -> Optional[Any]:
        """Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            model_class_name = getattr(model_config, 'model_class', 'BaseModel')
            
            if model_class_name == "GraphonomyModel" and TORCH_AVAILABLE:
                num_classes = getattr(model_config, 'num_classes', 20)
                return GraphonomyModel(num_classes=num_classes, backbone='resnet101')
            
            elif model_class_name == "OpenPoseModel" and TORCH_AVAILABLE:
                num_keypoints = getattr(model_config, 'num_classes', 18)
                return OpenPoseModel(num_keypoints=num_keypoints)
            
            elif model_class_name == "U2NetModel" and TORCH_AVAILABLE:
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class_name == "GeometricMatchingModel" and TORCH_AVAILABLE:
                return GeometricMatchingModel(feature_size=256)
            
            else:
                self.logger.warning(f"‚ö†Ô∏è ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î™®Îç∏ ÌÅ¥ÎûòÏä§: {model_class_name}")
                return BaseModel()
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _apply_m3_max_optimization_sync(self, model: Any, model_config) -> Any:
        """M3 Max ÌäπÌôî Î™®Îç∏ ÏµúÏ†ÅÌôî - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            optimizations_applied = []
            
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            if _compat.coreml_available and hasattr(model, 'eval'):
                optimizations_applied.append("CoreML compilation ready")
            
            if self.device == 'mps':
                try:
                    if TORCH_AVAILABLE and hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"üçé M3 Max Î™®Îç∏ ÏµúÏ†ÅÌôî Ï†ÅÏö©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max Î™®Îç∏ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return model
    
    def _check_memory_and_cleanup_sync(self):
        """Î©îÎ™®Î¶¨ ÌôïÏù∏ Î∞è Ï†ïÎ¶¨ - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            if hasattr(self.memory_manager, 'check_memory_pressure'):
                check_method = getattr(self.memory_manager, 'check_memory_pressure', None)
                success, is_pressure, message = self.function_validator.safe_call(check_method)
                
                if success and is_pressure:
                    self._cleanup_least_used_models_sync()
            
            if len(self.model_cache) >= self.max_cached_models:
                self._cleanup_least_used_models_sync()
            
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {message}")
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def _cleanup_least_used_models_sync(self, keep_count: int = 5):
        """ÏÇ¨Ïö©ÎüâÏù¥ Ï†ÅÏùÄ Î™®Îç∏ Ï†ïÎ¶¨ - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    if hasattr(model, 'cpu'):
                        cpu_method = getattr(model, 'cpu', None)
                        success, result, message = self.function_validator.safe_call(cpu_method)
                        if not success:
                            self.logger.warning(f"‚ö†Ô∏è CPU Ïù¥Îèô Ïã§Ìå®: {message}")
                    
                    del model
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"üßπ Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨: {len(cleaned_models)}Í∞ú Î™®Îç∏ Ìï¥Ï†ú")
                    
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            with self._lock:
                info = {
                    'name': model_name,
                    'registered': model_name in self.model_configs,
                    'cached': any(model_name in key for key in self.model_cache.keys()),
                    'device': self.device,
                    'config': None,
                    'load_time': None,
                    'last_access': None,
                    'access_count': 0,
                    'auto_detected': False
                }
                
                if model_name in self.model_configs:
                    config = self.model_configs[model_name]
                    info['config'] = {
                        'model_type': str(getattr(config, 'model_type', 'unknown')),
                        'model_class': getattr(config, 'model_class', 'unknown'),
                        'input_size': getattr(config, 'input_size', (512, 512)),
                        'num_classes': getattr(config, 'num_classes', None)
                    }
                
                if hasattr(self, 'detected_model_registry') and model_name in self.detected_model_registry:
                    detected_info = self.detected_model_registry[model_name]
                    info['auto_detected'] = True
                    info['detection_info'] = detected_info
                
                for cache_key in self.model_cache.keys():
                    if model_name in cache_key:
                        info['cached'] = True
                        info['load_time'] = self.load_times.get(cache_key)
                        info['last_access'] = self.last_access.get(cache_key)
                        info['access_count'] = self.access_counts.get(cache_key, 0)
                        break
                
                return info
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå® {model_name}: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def register_model(self, name: str, model: Any) -> bool:
        """Î™®Îç∏ Îì±Î°ù - SafeModelServiceÏóê ÏúÑÏûÑ"""
        try:
            return self.safe_model_service.register_model(name, model)
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """Í∏∞Ï°¥ Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï°∞Ìöå"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    if hasattr(interface, 'unload_models'):
                        unload_method = getattr(interface, 'unload_models', None)
                        success, result, message = self.function_validator.safe_call(unload_method)
                        if not success:
                            self.logger.warning(f"‚ö†Ô∏è Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïñ∏Î°úÎìú Ïã§Ìå®: {message}")
                    
                    del self.step_interfaces[step_name]
                    self.logger.info(f"üóëÔ∏è {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
                    
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def warmup_models(self, model_names: List[str]) -> Dict[str, bool]:
        """Ïó¨Îü¨ Î™®Îç∏ ÏõåÎ∞çÏóÖ - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        warmup_results = {}
        
        for model_name in model_names:
            try:
                model = self.load_model_sync(model_name)
                if model:
                    # ÏõåÎ∞çÏóÖ ÌÖåÏä§Ìä∏ Ìò∏Ï∂ú
                    success, result, message = self.function_validator.safe_call(model)
                    warmup_results[model_name] = success
                    if success:
                        self.logger.info(f"üî• Î™®Îç∏ ÏõåÎ∞çÏóÖ ÏÑ±Í≥µ: {model_name}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {model_name} - {message}")
                else:
                    warmup_results[model_name] = False
                    
            except Exception as e:
                self.logger.error(f"‚ùå Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïò§Î•ò {model_name}: {e}")
                warmup_results[model_name] = False
        
        success_count = sum(1 for success in warmup_results.values() if success)
        total_count = len(warmup_results)
        
        self.logger.info(f"üî• Î™®Îç∏ ÏõåÎ∞çÏóÖ ÏôÑÎ£å: {success_count}/{total_count} ÏÑ±Í≥µ")
        
        return warmup_results
    
    def cleanup(self):
        """ÏôÑÏ†ÑÌïú Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Îì§ Ï†ïÎ¶¨
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    try:
                        if step_name in self.step_interfaces:
                            del self.step_interfaces[step_name]
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            try:
                                model.cpu()
                            except:
                                pass
                        del model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if hasattr(self.memory_manager, 'cleanup_memory'):
                try:
                    self.memory_manager.cleanup_memory()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å Ïã§Ìå®: {e}")
            
            self.logger.info("‚úÖ ModelLoader v8.0 Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

# ==============================================
# üî• Ï†ÑÏó≠ ModelLoader Í¥ÄÎ¶¨ - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("üåê Ï†ÑÏó≠ ModelLoader v8.0 Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±")
        
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """üî• Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî - Í∑ºÎ≥∏ Î¨∏Ï†ú Ìï¥Í≤∞ (Dict ÎåÄÏã† Í∞ùÏ≤¥ Î∞òÌôò)"""
    try:
        loader = get_global_model_loader()
        
        # ÎèôÍ∏∞ Ï¥àÍ∏∞ÌôîÎßå ÏàòÌñâ
        success = loader.initialize()
        
        if success:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return loader  # üî• ÌïµÏã¨: Í∞ùÏ≤¥ ÏßÅÏ†ë Î∞òÌôò
        else:
            logger.error("‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        raise

def cleanup_global_loader():
    """Ï†ÑÏó≠ ModelLoader Ï†ïÎ¶¨"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ Î°úÎçî Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("üåê Ï†ÑÏó≠ ModelLoader v8.0 Ï†ïÎ¶¨ ÏôÑÎ£å")

# ==============================================
# üî• Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§ - ÏôÑÏ†Ñ Î≥¥Ï°¥
# ==============================================

def preprocess_image(
    image: Union[Any, Any, Any],
    target_size: Tuple[int, int] = (512, 512),
    device: str = "mps",
    normalize: bool = True,
    to_tensor: bool = True
) -> Any:
    """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò"""
    try:
        if not CV_AVAILABLE:
            logger.warning("‚ö†Ô∏è OpenCV/PIL ÏóÜÏùå, Í∏∞Î≥∏ Ï≤òÎ¶¨")
            if TORCH_AVAILABLE and to_tensor:
                return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
            else:
                if NUMPY_AVAILABLE:
                    return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
                else:
                    return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
        
        # PIL/OpenCVÎ•º ÏÇ¨Ïö©Ìïú Ïã§Ï†ú Ï†ÑÏ≤òÎ¶¨
        if hasattr(image, 'resize'):  # PIL Image
            image = image.resize(target_size)
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32)
                if normalize:
                    img_array = img_array / 255.0
                
                if to_tensor and TORCH_AVAILABLE:
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return img_tensor.to(device)
                else:
                    return img_array
        
        # Ìè¥Î∞± Ï≤òÎ¶¨
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
                
    except Exception as e:
        logger.error(f"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]

def postprocess_segmentation(output: Any, threshold: float = 0.5) -> Any:
    """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if NUMPY_AVAILABLE and hasattr(output, 'squeeze'):
            if output.ndim == 4:
                output = output.squeeze(0)
            if output.ndim == 3:
                output = output.squeeze(0)
                
            binary_mask = (output > threshold).astype(np.uint8) * 255
            return binary_mask
        else:
            # NumPy ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏ Ï≤òÎ¶¨
            return [[255 if x > threshold else 0 for x in row] for row in output] if hasattr(output, '__iter__') else output
            
    except Exception as e:
        logger.error(f"ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        if NUMPY_AVAILABLE:
            return np.zeros((512, 512), dtype=np.uint8)
        else:
            return [[0 for _ in range(512)] for _ in range(512)]

# Ï∂îÍ∞Ä Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§
def preprocess_pose_input(image: Any, target_size: Tuple[int, int] = (368, 368)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_human_parsing_input(image: Any, target_size: Tuple[int, int] = (512, 512)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_cloth_segmentation_input(image: Any, target_size: Tuple[int, int] = (320, 320)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def tensor_to_pil(tensor: Any) -> Any:
    """ÌÖêÏÑúÎ•º PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'dim'):
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.dim() == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = tensor.cpu().numpy()
            
        if NUMPY_AVAILABLE and hasattr(tensor, 'dtype'):
            if tensor.dtype != np.uint8:
                tensor = (tensor * 255).astype(np.uint8)
        
        if CV_AVAILABLE:
            return Image.fromarray(tensor)
        else:
            # PIL ÏóÜÎäî Í≤ΩÏö∞ ÎçîÎØ∏ Î∞òÌôò
            return tensor
    except Exception as e:
        logger.error(f"ÌÖêÏÑú->PIL Î≥ÄÌôò Ïã§Ìå®: {e}")
        return None

def pil_to_tensor(image: Any, device: str = "mps") -> Any:
    """PIL Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò"""
    try:
        if CV_AVAILABLE and hasattr(image, 'size'):
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32) / 255.0
                if TORCH_AVAILABLE:
                    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return tensor.to(device)
                else:
                    return img_array
        
        # Ìè¥Î∞±
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return np.zeros((1, 3, 512, 512), dtype=np.float32) if NUMPY_AVAILABLE else None
            
    except Exception as e:
        logger.error(f"PIL->ÌÖêÏÑú Î≥ÄÌôò Ïã§Ìå®: {e}")
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return None

# ==============================================
# üî• Utility Functions
# ==============================================

def get_model_service() -> SafeModelService:
    """Ï†ÑÏó≠ Î™®Îç∏ ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    loader = get_global_model_loader()
    return loader.safe_model_service

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ÎîïÏÖîÎÑàÎ¶¨Î•º Î™®Îç∏Î°ú ÏïàÏ†ÑÌïòÍ≤å Îì±Î°ù"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock Î™®Îç∏ ÏÉùÏÑ±"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': 'mps',
        'loaded_at': '2025-07-20T12:00:00Z'
    }
    
    service = get_model_service()
    return service._create_callable_dict_wrapper(mock_dict)

# ÏïàÏ†ÑÌïú Ìò∏Ï∂ú Ìï®ÏàòÎì§
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """Ï†ÑÏó≠ ÏïàÏ†ÑÌïú Ìï®Ïàò Ìò∏Ï∂ú"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
    """Ï†ÑÏó≠ ÏïàÏ†ÑÌïú ÏÜçÏÑ± Ï†ëÍ∑º Î∞è Ìò∏Ï∂ú"""
    try:
        if obj is None:
            return False, None, "Object is None"
        
        if not isinstance(attr_name, str) or not attr_name:
            return False, None, f"Invalid attribute name: {attr_name}"
        
        if not hasattr(obj, attr_name):
            return False, None, f"Object has no attribute '{attr_name}'"
        
        try:
            attr = getattr(obj, attr_name)
        except Exception as e:
            return False, None, f"Error getting attribute '{attr_name}': {e}"
        
        if isinstance(attr, dict):
            if args or kwargs:
                return False, None, f"Attribute '{attr_name}' is dict, cannot call with arguments"
            else:
                return True, attr, f"Returned dict attribute '{attr_name}'"
        
        if callable(attr):
            is_callable, reason, safe_attr = SafeFunctionValidator.validate_callable(attr, f"getattr_{attr_name}")
            if is_callable:
                return SafeFunctionValidator.safe_call(safe_attr, *args, **kwargs)
            else:
                return False, None, f"Attribute '{attr_name}' validation failed: {reason}"
        
        if args or kwargs:
            return False, None, f"Attribute '{attr_name}' is not callable, cannot call with arguments"
        else:
            return True, attr, f"Returned non-callable attribute '{attr_name}'"
            
    except Exception as e:
        return False, None, f"Getattr call failed: {e}"

def is_safely_callable(obj: Any) -> bool:
    """Ï†ÑÏó≠ callable ÏïàÏ†ÑÏÑ± Í≤ÄÏ¶ù"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

# ==============================================
# üî• Ï∂îÍ∞Ä Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
# ==============================================

def safe_warmup_models(model_names: List[str]) -> Dict[str, bool]:
    """Ïó¨Îü¨ Î™®Îç∏ ÏïàÏ†Ñ ÏõåÎ∞çÏóÖ"""
    try:
        loader = get_global_model_loader()
        return loader.warmup_models(model_names)
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
        return {name: False for name in model_names}

def list_available_models() -> Dict[str, Dict[str, Any]]:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îì† Î™®Îç∏ Î™©Î°ù"""
    try:
        loader = get_global_model_loader()
        return loader.list_models()
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {}

def get_model_info(model_name: str) -> Dict[str, Any]:
    """ÌäπÏ†ï Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        loader = get_global_model_loader()
        return loader.get_model_info(model_name)
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå® {model_name}: {e}")
        return {'name': model_name, 'error': str(e)}

def register_model_config(name: str, config: Union[ModelConfig, StepModelConfig, Dict[str, Any]]) -> bool:
    """Î™®Îç∏ ÏÑ§Ï†ï Îì±Î°ù"""
    try:
        loader = get_global_model_loader()
        return loader.register_model_config(name, config)
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ ÏÑ§Ï†ï Îì±Î°ù Ïã§Ìå® {name}: {e}")
        return False

def load_model_sync(model_name: str, **kwargs) -> Optional[Any]:
    """ÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú"""
    try:
        loader = get_global_model_loader()
        return loader.load_model_sync(model_name, **kwargs)
    except Exception as e:
        logger.error(f"‚ùå ÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
        return None

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
        return StepModelInterface(loader, step_name)

def cleanup_model_cache():
    """Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨"""
    try:
        loader = get_global_model_loader()
        loader._cleanup_least_used_models_sync()
        logger.info("‚úÖ Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

def check_memory_usage() -> Dict[str, float]:
    """Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌôïÏù∏"""
    try:
        loader = get_global_model_loader()
        return loader.memory_manager.get_available_memory()
    except Exception as e:
        logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌôïÏù∏ Ïã§Ìå®: {e}")
        return {'error': str(e)}

def get_device_info() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        loader = get_global_model_loader()
        return {
            'device': loader.device,
            'is_m3_max': loader.is_m3_max,
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'memory_gb': loader.memory_gb,
            'optimization_enabled': loader.optimization_enabled,
            'use_fp16': loader.use_fp16
        }
    except Exception as e:
        logger.error(f"‚ùå ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {'error': str(e)}

def validate_model_config(config: Union[ModelConfig, StepModelConfig, Dict[str, Any]]) -> bool:
    """Î™®Îç∏ ÏÑ§Ï†ï Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
    try:
        if isinstance(config, dict):
            required_fields = ['name', 'model_class']
            for field in required_fields:
                if field not in config:
                    logger.warning(f"‚ö†Ô∏è ÌïÑÏàò ÌïÑÎìú ÎàÑÎùΩ: {field}")
                    return False
            return True
        elif isinstance(config, (ModelConfig, StepModelConfig)):
            return hasattr(config, 'name') and hasattr(config, 'model_class')
        else:
            return False
    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ ÏÑ§Ï†ï Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
        return False

def create_async_compatible_fallback(model_name: str, step_name: str = "unknown") -> Any:
    """üî• ÎπÑÎèôÍ∏∞ Ìò∏Ìôò Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ± (Ï†ÑÏó≠ Ìï®Ïàò)"""
    
    class GlobalAsyncCompatibleFallback:
        def __init__(self, name: str, step: str):
            self.name = name
            self.step_name = step
            self.device = "cpu"
            
        def __call__(self, *args, **kwargs):
            return self.forward(*args, **kwargs)
            
        def forward(self, *args, **kwargs):
            return {
                'success': True,
                'result': f'fallback_result_for_{self.name}',
                'model_name': self.name,
                'step_name': self.step_name,
                'type': 'async_compatible_fallback'
            }
        
        def __await__(self):
            async def _async_result():
                return self
            return _async_result().__await__()
    
    return GlobalAsyncCompatibleFallback(model_name, step_name)

def create_fallback_model(model_name: str, model_type: str = "fallback") -> Any:
    """Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±"""
    
    class FallbackModel:
        def __init__(self, name: str, model_type: str):
            self.name = name
            self.model_type = model_type
            self.device = "cpu"
            
        def __call__(self, *args, **kwargs):
            return self.forward(*args, **kwargs)
        
        def forward(self, *args, **kwargs):
            logger.warning(f"‚ö†Ô∏è Ìè¥Î∞± Î™®Îç∏ Ïã§Ìñâ: {self.name}")
            if TORCH_AVAILABLE:
                return torch.zeros(1, 3, 512, 512)
            else:
                return None
        
        def to(self, device):
            self.device = str(device)
            return self
        
        def eval(self):
            return self
        
        def cpu(self):
            self.device = "cpu"
            return self
        
        def __await__(self):
            async def _async_result():
                return self
            return _async_result().__await__()
    
    return FallbackModel(model_name, model_type)

def register_multiple_models(model_configs: Dict[str, Union[ModelConfig, StepModelConfig, Dict[str, Any]]]) -> Dict[str, bool]:
    """Ïó¨Îü¨ Î™®Îç∏ ÏùºÍ¥Ñ Îì±Î°ù"""
    results = {}
    
    for name, config in model_configs.items():
        try:
            results[name] = register_model_config(name, config)
        except Exception as e:
            logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            results[name] = False
    
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    
    logger.info(f"üìù Î™®Îç∏ ÏùºÍ¥Ñ Îì±Î°ù ÏôÑÎ£å: {success_count}/{total_count} ÏÑ±Í≥µ")
    
    return results

def get_pipeline_summary() -> Dict[str, Any]:
    """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏ≤¥ ÏöîÏïΩ"""
    try:
        loader = get_global_model_loader()
        models = loader.list_models()
        
        return {
            'model_loader_status': 'initialized' if loader else 'not_initialized',
            'total_models': len(models),
            'device_info': get_device_info(),
            'memory_info': check_memory_usage(),
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE,
            'cv_available': CV_AVAILABLE,
            'step_interfaces': len(loader.step_interfaces) if loader else 0,
            'detected_models': len(loader.detected_model_registry) if hasattr(loader, 'detected_model_registry') else 0
        }
    except Exception as e:
        logger.error(f"‚ùå ÌååÏù¥ÌîÑÎùºÏù∏ ÏöîÏïΩ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {'error': str(e)}

def benchmark_model_loading(model_names: List[str]) -> Dict[str, Dict[str, float]]:
    """Î™®Îç∏ Î°úÎî© ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨"""
    results = {}
    
    for model_name in model_names:
        try:
            start_time = time.time()
            model = load_model_sync(model_name)
            load_time = time.time() - start_time
            
            if model:
                # Ï∂îÎ°† ÏÑ±Îä• ÌÖåÏä§Ìä∏
                inference_start = time.time()
                try:
                    # ÎçîÎØ∏ ÏûÖÎ†•ÏúºÎ°ú Ï∂îÎ°† ÌÖåÏä§Ìä∏
                    if TORCH_AVAILABLE:
                        dummy_input = torch.zeros(1, 3, 512, 512)
                        if hasattr(model, 'forward'):
                            _ = model.forward(dummy_input)
                        elif callable(model):
                            _ = model(dummy_input)
                    inference_time = time.time() - inference_start
                except:
                    inference_time = -1
                
                results[model_name] = {
                    'load_time': load_time,
                    'inference_time': inference_time,
                    'total_time': load_time + max(inference_time, 0),
                    'success': True
                }
            else:
                results[model_name] = {
                    'load_time': load_time,
                    'inference_time': -1,
                    'total_time': load_time,
                    'success': False
                }
                
        except Exception as e:
            results[model_name] = {
                'load_time': -1,
                'inference_time': -1,
                'total_time': -1,
                'success': False,
                'error': str(e)
            }
    
    return results

# ==============================================
# üî• Module Exports
# ==============================================

__all__ = [
    # üî• ÏóêÎü¨ Ìï¥Í≤∞ ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'SafeFunctionValidator',
    'SafeModelService',
    
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'ModelLoader',
    'StepModelInterface',
    'DeviceManager',
    'ModelMemoryManager',
    
    # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îì§
    'ModelFormat',
    'ModelType',
    'ModelPriority',
    'ModelConfig',
    'StepModelConfig',
    
    # AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel',
    'U2NetModel',
    'GeometricMatchingModel',
    
    # Ìå©ÌÜ†Î¶¨ Î∞è Í¥ÄÎ¶¨ Ìï®ÏàòÎì§
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    
    # ÏïàÏ†ÑÌïú Ìò∏Ï∂ú Ìï®ÏàòÎì§
    'get_model_service',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_getattr_call',
    'is_safely_callable',
    
    # üî• Ï∂îÍ∞ÄÎêú Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'safe_warmup_models',
    'list_available_models',
    'get_model_info',
    'register_model_config',
    'load_model_sync',
    'create_step_interface',
    'cleanup_model_cache',
    'check_memory_usage',
    'get_device_info',
    'validate_model_config',
    'create_fallback_model',
    'create_async_compatible_fallback',
    'register_multiple_models',
    'get_pipeline_summary',
    'benchmark_model_loading',
    
    # Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ìï®ÏàòÎì§
    'preprocess_image',
    'postprocess_segmentation',
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'tensor_to_pil',
    'pil_to_tensor',
    
    # ÏÉÅÏàòÎì§
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    
    # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô
    'STEP_MODEL_REQUESTS'
]

# Î™®Îìà Î†àÎ≤®ÏóêÏÑú ÏïàÏ†ÑÌïú Ï†ïÎ¶¨ Ìï®Ïàò Îì±Î°ù
import atexit
atexit.register(cleanup_global_loader)

# Î™®Îìà Î°úÎìú ÌôïÏù∏
logger.info("‚úÖ ModelLoader v8.0 FINAL Î™®Îìà Î°úÎìú ÏôÑÎ£å - Î™®Îì† Í∏∞Îä• ÏôÑÏ†Ñ ÌÜµÌï©")
logger.info("üî• initialize_global_model_loader Í∞ùÏ≤¥ ÏßÅÏ†ë Î∞òÌôòÏúºÎ°ú ÏàòÏ†ï")
logger.info("üîß Dict Î∞òÌôò Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞")
logger.info("‚ö° ÎπÑÎèôÍ∏∞/ÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ï≤¥Ïù∏ ÏôÑÏ†Ñ Ï†ïÎ¶¨")
logger.info("üõ°Ô∏è Coroutine Î¨∏Ï†ú ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("üîó ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ")
logger.info("üéØ FallbackModel await Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("üçé M3 Max 128GB ÏµúÏ†ÅÌôî")
logger.info(f"üîß PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}, MPS: {'‚úÖ' if MPS_AVAILABLE else '‚ùå'}")
logger.info(f"üî¢ NumPy: {'‚úÖ' if NUMPY_AVAILABLE else '‚ùå'}")

if NUMPY_AVAILABLE and hasattr(_compat, 'numpy_version'):
    numpy_major = int(_compat.numpy_version.split('.')[0])
    if numpy_major >= 2:
        logger.warning("‚ö†Ô∏è NumPy 2.x Í∞êÏßÄÎê® - conda install numpy=1.24.3 Í∂åÏû•")
    else:
        logger.info("‚úÖ NumPy Ìò∏ÌôòÏÑ± ÌôïÏù∏Îê®")

logger.info("üöÄ ModelLoader v8.0 FINAL ÏôÑÏ†Ñ ÌÜµÌï© ÏôÑÎ£å!")
logger.info("   ‚úÖ initialize_global_model_loader Dict Î∞òÌôò ‚Üí Í∞ùÏ≤¥ Î∞òÌôò ÏàòÏ†ï")
logger.info("   ‚úÖ ÎπÑÎèôÍ∏∞/ÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ï≤¥Ïù∏ ÏôÑÏ†Ñ Ï†ïÎ¶¨")
logger.info("   ‚úÖ Coroutine Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞")
logger.info("   ‚úÖ FallbackModel await Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("   ‚úÖ Dict callable Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞")
logger.info("   ‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ")
logger.info("   ‚úÖ Î™®Îì† Í∏∞Ï°¥ Í∏∞Îä•Î™Ö/ÌÅ¥ÎûòÏä§Î™Ö 100% Ïú†ÏßÄ")
logger.info("   ‚úÖ Step ÌååÏùºÎì§Í≥º 100% Ìò∏Ìôò")
logger.info("   ‚úÖ M3 Max 128GB Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî")
logger.info("   ‚úÖ ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ïÏÑ± ÏµúÍ≥† ÏàòÏ§Ä")