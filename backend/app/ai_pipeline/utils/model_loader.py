# app/ai_pipeline/utils/model_loader.py
"""
üçé MyCloset AI - ÏôÑÏ†Ñ ÌÜµÌï© ModelLoader ÏãúÏä§ÌÖú v6.0 - ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ Î∞è ÏôÑÏ†Ñ Ïó∞Îèô
========================================================================================

‚úÖ 'dict' object is not callable Í∑ºÎ≥∏ ÏõêÏù∏ Ìï¥Í≤∞ (ÏôÑÏ†Ñ Í∞ïÌôî)
‚úÖ SafeModelService + SafeFunctionValidator ÌÜµÌï© (Í∏∞Îä• Í∞ïÌôî)  
‚úÖ NumPy 2.x ÏôÑÏ†Ñ Ìò∏ÌôòÏÑ± Ìï¥Í≤∞
‚úÖ BaseStepMixin v6.0 ÏôÑÎ≤Ω Ïó∞Îèô
‚úÖ auto_model_detector Ïó∞Îèô (ÌïúÎ∞©Ìñ• Ï∞∏Ï°∞)
‚úÖ step_model_requests ÏôÑÎ≤Ω Ïó∞Îèô
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî ÏôÑÏÑ±
‚úÖ conda ÌôòÍ≤Ω ÏôÑÎ≤Ω ÏßÄÏõê
‚úÖ StepModelInterface Ïã§Ï†ú AI Î™®Îç∏ Ï∂îÎ°† Í∏∞Îä• ÏôÑÏ†Ñ ÌÜµÌï©
‚úÖ ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ïÏÑ± ÏµúÍ≥† ÏàòÏ§Ä
‚úÖ Î™®Îì† Í∏∞Ï°¥ Í∏∞Îä•Î™Ö/ÌÅ¥ÎûòÏä§Î™Ö Ïú†ÏßÄ + Í∞úÏÑ†
‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ (ÌïúÎ∞©Ìñ• ÏùòÏ°¥ÏÑ±)

Author: MyCloset AI Team
Date: 2025-07-20
Version: 6.0 (Complete Integration + Circular Reference Prevention)
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
import json
import pickle
import sqlite3
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache, wraps
from concurrent.futures import ThreadPoolExecutor
import weakref

# ==============================================
# üî• Core System Imports & Configuration
# ==============================================

# NumPy Ìò∏ÌôòÏÑ± Ï≤òÎ¶¨
try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        logging.warning(f"‚ö†Ô∏è NumPy {numpy_version} Í∞êÏßÄÎê®. NumPy 1.x Í∂åÏû•")
        try:
            np.set_printoptions(legacy='1.25')
            logging.info("‚úÖ NumPy 2.x Ìò∏ÌôòÏÑ± Î™®Îìú ÌôúÏÑ±Ìôî")
        except:
            pass
    NUMPY_AVAILABLE = True
except ImportError as e:
    NUMPY_AVAILABLE = False
    np = None

# PyTorch ÏïàÏ†ÑÌïú ÏûÑÌè¨Ìä∏
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
    else:
        MPS_AVAILABLE = False
        DEFAULT_DEVICE = "cpu"
except ImportError:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    torch = None
    nn = None

# Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÎùºÏù¥Î∏åÎü¨Î¶¨
try:
    import cv2
    from PIL import Image, ImageEnhance
    CV_AVAILABLE = True
    PIL_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    PIL_AVAILABLE = False

# Ïô∏Î∂Ä AI ÎùºÏù¥Î∏åÎü¨Î¶¨ (ÏÑ†ÌÉùÏ†Å)
try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# üî• Core Data Structures
# ==============================================

class ModelFormat(Enum):
    """Î™®Îç∏ Ìè¨Îß∑ Ï†ïÏùò"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"

class ModelType(Enum):
    """AI Î™®Îç∏ ÌÉÄÏûÖ"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelConfig:
    """Î™®Îç∏ ÏÑ§Ï†ï Ï†ïÎ≥¥"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """StepÎ≥Ñ ÌäπÌôî Î™®Îç∏ ÏÑ§Ï†ï"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# üî• Step Model Requests Integration
# ==============================================

# Step ÏöîÏ≤≠ÏÇ¨Ìï≠ ÎÇ¥Ïû• (step_model_requests.py ÌÜµÌï©)
STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

class StepModelRequestAnalyzer:
    @staticmethod
    def get_step_request_info(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)

# ==============================================
# üî• SafeFunctionValidator - Dict Callable Ïò§Î•ò Ìï¥Í≤∞
# ==============================================

class SafeFunctionValidator:
    """Ìï®Ïàò/Î©îÏÑúÎìú/Í∞ùÏ≤¥ Ìò∏Ï∂ú ÏïàÏ†ÑÏÑ± Í≤ÄÏ¶ù ÌÅ¥ÎûòÏä§"""
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """Í∞ùÏ≤¥Í∞Ä ÏïàÏ†ÑÌïòÍ≤å Ìò∏Ï∂ú Í∞ÄÎä•ÌïúÏßÄ Í≤ÄÏ¶ù"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            if isinstance(obj, dict):
                if hasattr(obj, '__call__') and callable(getattr(obj, '__call__', None)):
                    return True, "Special callable dict", obj
                else:
                    return False, f"Object is dict, not callable in context: {context}", None
            
            basic_types = (str, int, float, bool, list, tuple, set, bytes, bytearray)
            if isinstance(obj, basic_types):
                return False, f"Object is basic data type {type(obj)}, not callable", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            import types
            if isinstance(obj, (types.FunctionType, types.MethodType, types.BuiltinFunctionType, types.BuiltinMethodType)):
                return True, "Valid function/method", obj
            
            if hasattr(obj, '__call__'):
                call_method = getattr(obj, '__call__')
                if callable(call_method) and not isinstance(call_method, dict):
                    return True, "Valid callable object with __call__", obj
                else:
                    return False, "__call__ method is dict, not callable", None
            
            if callable(obj):
                return True, "Generic callable object", obj
            
            return False, f"Unknown callable validation failure for {type(obj)}", None
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except TypeError as e:
                error_msg = str(e)
                if "not callable" in error_msg.lower():
                    return False, None, f"Runtime callable error: {error_msg}"
                else:
                    return False, None, f"Type error in call: {error_msg}"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú ÎπÑÎèôÍ∏∞ Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_async_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    loop = asyncio.get_event_loop()
                    if hasattr(loop, 'run_in_executor'):
                        result = await asyncio.wait_for(
                            loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs)),
                            timeout=30.0
                        )
                        return True, result, "Sync-to-async success"
                    else:
                        result = safe_obj(*args, **kwargs)
                        return True, result, "Direct sync call"
                        
            except asyncio.TimeoutError:
                return False, None, "Async call timeout (30s)"
            except Exception as e:
                return False, None, f"Async execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"
    
    @staticmethod
    def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú ÏÜçÏÑ± Ï†ëÍ∑º Î∞è Ìò∏Ï∂ú"""
        try:
            if obj is None:
                return False, None, "Object is None"
            
            if not isinstance(attr_name, str) or not attr_name:
                return False, None, f"Invalid attribute name: {attr_name}"
            
            if not hasattr(obj, attr_name):
                return False, None, f"Object has no attribute '{attr_name}'"
            
            try:
                attr = getattr(obj, attr_name)
            except Exception as e:
                return False, None, f"Error getting attribute '{attr_name}': {e}"
            
            if isinstance(attr, dict):
                if args or kwargs:
                    return False, None, f"Attribute '{attr_name}' is dict, cannot call with arguments"
                else:
                    return True, attr, f"Returned dict attribute '{attr_name}'"
            
            if callable(attr):
                is_callable, reason, safe_attr = SafeFunctionValidator.validate_callable(attr, f"getattr_{attr_name}")
                if is_callable:
                    return SafeFunctionValidator.safe_call(safe_attr, *args, **kwargs)
                else:
                    return False, None, f"Attribute '{attr_name}' validation failed: {reason}"
            
            if args or kwargs:
                return False, None, f"Attribute '{attr_name}' is not callable, cannot call with arguments"
            else:
                return True, attr, f"Returned non-callable attribute '{attr_name}'"
                
        except Exception as e:
            return False, None, f"Getattr call failed: {e}"

# ==============================================
# üî• SafeConfig - ÏÑ§Ï†ï Í¥ÄÎ¶¨
# ==============================================

class SafeConfig:
    """ÏïàÏ†ÑÌïú ÏÑ§Ï†ï ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, data: Any = None):
        self._data = {}
        self._callable_methods = {}
        self._lock = threading.RLock()
        
        try:
            with self._lock:
                if data is None:
                    self._data = {}
                elif isinstance(data, dict):
                    self._data = self._deep_copy_dict(data)
                elif hasattr(data, '__dict__'):
                    self._data = {}
                    for attr_name in dir(data):
                        if not attr_name.startswith('_'):
                            try:
                                attr_value = getattr(data, attr_name)
                                is_callable, reason, safe_attr = SafeFunctionValidator.validate_callable(attr_value)
                                if is_callable:
                                    self._callable_methods[attr_name] = safe_attr
                                elif not isinstance(attr_value, (type, type(None))):
                                    self._data[attr_name] = self._safe_copy_value(attr_value)
                            except Exception:
                                continue
                else:
                    self._data = {}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÏÑ§Ï†ï Í∞ùÏ≤¥ ÌååÏã± Ïã§Ìå®: {e}")
            self._data = {}
    
    def _deep_copy_dict(self, data: dict) -> dict:
        """ÎîïÏÖîÎÑàÎ¶¨ ÍπäÏùÄ Î≥µÏÇ¨"""
        result = {}
        for key, value in data.items():
            try:
                if not callable(value):
                    result[key] = self._safe_copy_value(value)
            except Exception:
                continue
        return result
    
    def _safe_copy_value(self, value: Any) -> Any:
        """Í∞í ÏïàÏ†Ñ Î≥µÏÇ¨"""
        try:
            if isinstance(value, (str, int, float, bool, type(None))):
                return value
            elif isinstance(value, (list, tuple)):
                return type(value)(self._safe_copy_value(item) for item in value)
            elif isinstance(value, dict):
                return {k: self._safe_copy_value(v) for k, v in value.items() if not callable(v)}
            elif hasattr(value, 'copy'):
                return value.copy()
            else:
                return value
        except Exception:
            return None
    
    def get(self, key: str, default=None):
        """ÎîïÏÖîÎÑàÎ¶¨Ï≤òÎüº get Î©îÏÑúÎìú ÏßÄÏõê"""
        try:
            with self._lock:
                if key in self._data:
                    return self._data[key]
                if key in self._callable_methods:
                    return default
                return default
        except Exception:
            return default
    
    def __getitem__(self, key):
        return self.get(key, None)
    
    def __setitem__(self, key, value):
        try:
            with self._lock:
                if callable(value):
                    is_callable, reason, safe_value = SafeFunctionValidator.validate_callable(value)
                    if is_callable:
                        self._callable_methods[key] = safe_value
                else:
                    self._data[key] = self._safe_copy_value(value)
        except Exception as e:
            logger.error(f"‚ùå SafeConfig.__setitem__ Ïã§Ìå® {key}: {e}")

# ==============================================
# üî• SafeModelService - Î™®Îç∏ ÏÑúÎπÑÏä§
# ==============================================

class SafeModelService:
    """ÏïàÏ†ÑÌïú Î™®Îç∏ ÏÑúÎπÑÏä§"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.warmup_status = {}
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        
    def register_model(self, name: str, model: Any) -> bool:
        """Î™®Îç∏ Îì±Î°ù"""
        try:
            with self.lock:
                if isinstance(model, dict):
                    wrapper = self._create_dict_wrapper(model)
                    self.models[name] = wrapper
                elif callable(model):
                    is_callable, reason, safe_model = self.validator.validate_callable(model, f"register_{name}")
                    if is_callable:
                        self.models[name] = safe_model
                    else:
                        wrapper = self._create_object_wrapper(model)
                        self.models[name] = wrapper
                else:
                    wrapper = self._create_object_wrapper(model)
                    self.models[name] = wrapper
                
                return True
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def _create_dict_wrapper(self, model_dict: Dict[str, Any]) -> Callable:
        """ÎîïÏÖîÎÑàÎ¶¨Î•º callable wrapperÎ°ú Î≥ÄÌôò"""
        
        class DictModelWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.type = data.get('type', 'dict_model')
            
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data
                }
            
            def warmup(self):
                return True
        
        return DictModelWrapper(model_dict)
    
    def _create_object_wrapper(self, obj: Any) -> Callable:
        """ÏùºÎ∞ò Í∞ùÏ≤¥Î•º callable wrapperÎ°ú Î≥ÄÌôò"""
        
        class ObjectWrapper:
            def __init__(self, wrapped_obj: Any):
                self.wrapped_obj = wrapped_obj
                self.name = getattr(wrapped_obj, 'name', str(type(wrapped_obj).__name__))
                self.type = type(wrapped_obj).__name__
            
            def __call__(self, *args, **kwargs):
                if callable(self.wrapped_obj):
                    validator = SafeFunctionValidator()
                    success, result, message = validator.safe_call(self.wrapped_obj, *args, **kwargs)
                    if success:
                        return result
                
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}'
                }
        
        return ObjectWrapper(obj)
    
    async def call_model(self, name: str, *args, **kwargs) -> Any:
        """ÏïàÏ†ÑÌïú Î™®Îç∏ Ìò∏Ï∂ú"""
        try:
            with self.lock:
                if name not in self.models:
                    return None
                
                model = self.models[name]
                success, result, message = await self.validator.safe_async_call(model, *args, **kwargs)
                
                if success:
                    return result
                else:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ìò∏Ï∂ú Ïã§Ìå®: {name} - {message}")
                    return None
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ìò∏Ï∂ú Ïò§Î•ò {name}: {e}")
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îç∏ Î™©Î°ù"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {'status': 'registered', 'type': 'model'}
                return result
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}

# ==============================================
# üî• Device & Memory Management
# ==============================================

class DeviceManager:
    """ÎîîÎ∞îÏù¥Ïä§ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        
    def _detect_available_devices(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§ ÌÉêÏßÄ"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                devices.append("mps")
            if torch.cuda.is_available():
                devices.append("cuda")
        
        return devices
    
    def _select_optimal_device(self) -> str:
        """ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max Ïπ© Í∞êÏßÄ"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ÏöîÏ≤≠Îêú ÎîîÎ∞îÏù¥Ïä§Î•º Ïã§Ï†ú ÎîîÎ∞îÏù¥Ïä§Î°ú Î≥ÄÌôò"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            return self.optimal_device

class ModelMemoryManager:
    """Î™®Îç∏ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ (GB) Î∞òÌôò"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
        except Exception:
            return 8.0
    
    def cleanup_memory(self):
        """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• Image Processing Utilities
# ==============================================

def preprocess_image(
    image: Union[Image.Image, np.ndarray, torch.Tensor],
    target_size: Tuple[int, int] = (512, 512),
    device: str = "mps",
    normalize: bool = True,
    to_tensor: bool = True
) -> torch.Tensor:
    """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò"""
    try:
        # PIL ImageÎ°ú Î≥ÄÌôò
        if isinstance(image, torch.Tensor):
            if image.dim() == 4:
                image = image.squeeze(0)
            if image.dim() == 3 and image.shape[0] == 3:
                image = image.permute(1, 2, 0)
            image = image.cpu().numpy()
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8)
            image = Image.fromarray(image)
        elif isinstance(image, np.ndarray):
            if image.ndim == 3 and image.shape[2] == 3:
                image = Image.fromarray(image.astype(np.uint8))
            else:
                image = Image.fromarray(image)
        
        # RGB Î≥ÄÌôò
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # ÌÅ¨Í∏∞ Ï°∞Ï†ï
        if target_size != image.size:
            image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôò
        img_array = np.array(image).astype(np.float32)
        
        # Ï†ïÍ∑úÌôî
        if normalize:
            img_array = img_array / 255.0
        
        # ÌÖêÏÑú Î≥ÄÌôò
        if to_tensor and TORCH_AVAILABLE:
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
            img_tensor = img_tensor.to(device)
            return img_tensor
        else:
            return img_array
            
    except Exception as e:
        logger.error(f"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)

def postprocess_segmentation(output: torch.Tensor, threshold: float = 0.5) -> np.ndarray:
    """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
    try:
        if isinstance(output, torch.Tensor):
            output = output.cpu().numpy()
        
        if output.ndim == 4:
            output = output.squeeze(0)
        if output.ndim == 3:
            output = output.squeeze(0)
            
        binary_mask = (output > threshold).astype(np.uint8) * 255
        return binary_mask
        
    except Exception as e:
        logger.error(f"ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        return np.zeros((512, 512), dtype=np.uint8)

# Ï∂îÍ∞Ä Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§
def preprocess_pose_input(image: np.ndarray, target_size: Tuple[int, int] = (368, 368)) -> torch.Tensor:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_human_parsing_input(image: np.ndarray, target_size: Tuple[int, int] = (512, 512)) -> torch.Tensor:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_cloth_segmentation_input(image: np.ndarray, target_size: Tuple[int, int] = (320, 320)) -> torch.Tensor:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """ÌÖêÏÑúÎ•º PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò"""
    try:
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        if tensor.dim() == 3:
            tensor = tensor.permute(1, 2, 0)
        
        tensor = tensor.cpu().numpy()
        if tensor.dtype != np.uint8:
            tensor = (tensor * 255).astype(np.uint8)
        
        return Image.fromarray(tensor)
    except Exception as e:
        logger.error(f"ÌÖêÏÑú->PIL Î≥ÄÌôò Ïã§Ìå®: {e}")
        return Image.new('RGB', (512, 512), color='black')

def pil_to_tensor(image: Image.Image, device: str = "mps") -> torch.Tensor:
    """PIL Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò"""
    try:
        img_array = np.array(image).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
        return tensor.to(device)
    except Exception as e:
        logger.error(f"PIL->ÌÖêÏÑú Î≥ÄÌôò Ïã§Ìå®: {e}")
        return torch.zeros(1, 3, 512, 512, device=device)

# ==============================================
# üî• AI Model Classes
# ==============================================

class BaseModel(nn.Module if TORCH_AVAILABLE else object):
    """Í∏∞Î≥∏ AI Î™®Îç∏ ÌÅ¥ÎûòÏä§"""
    def __init__(self):
        if TORCH_AVAILABLE:
            super().__init__()
        self.model_name = "BaseModel"
        self.device = "cpu"
    
    def forward(self, x):
        return x

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy Ïù∏Ï≤¥ ÌååÏã± Î™®Îç∏"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            self.paf_head = nn.Conv2d(512, 38, 1)
            self.heatmap_head = nn.Conv2d(512, 19, 1)
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """U¬≤-Net ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò Î™®Îç∏"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """Í∏∞ÌïòÌïôÏ†Å Îß§Ïπ≠ Î™®Îç∏"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

    class HRVITONModel(nn.Module):
        """HR-VITON Í∞ÄÏÉÅ ÌîºÌåÖ Î™®Îç∏"""
        
        def __init__(self, input_nc=3, output_nc=3, ngf=64):
            super().__init__()
            
            self.encoder = nn.Sequential(
                nn.Conv2d(input_nc * 2, ngf, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, ngf * 2, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 2, ngf * 4, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 4, ngf * 8, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, output_nc, 3, 1, 1), nn.Tanh()
            )
            
            self.attention = nn.Sequential(
                nn.Conv2d(input_nc * 2, 32, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
            )
        
        def forward(self, person_img, cloth_img, **kwargs):
            combined_input = torch.cat([person_img, cloth_img], dim=1)
            
            features = self.encoder(combined_input)
            generated = self.decoder(features)
            
            attention_map = self.attention(combined_input)
            result = generated * attention_map + person_img * (1 - attention_map)
            
            return {
                'generated_image': result,
                'attention_map': attention_map,
                'warped_cloth': cloth_img,
                'intermediate': generated
            }

else:
    # PyTorch ÏóÜÎäî Í≤ΩÏö∞ ÎçîÎØ∏ ÌÅ¥ÎûòÏä§Îì§
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel
    HRVITONModel = BaseModel

# ==============================================
# üî• StepModelInterface - ÏôÑÏ†Ñ Íµ¨ÌòÑ
# ==============================================

class StepModelInterface:
    """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # Î™®Îç∏ Ï∫êÏãú
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        
        # Step ÏöîÏ≤≠ Ï†ïÎ≥¥ Î°úÎìú
        self.step_request = STEP_MODEL_REQUESTS.get(step_name)
        self.recommended_models = self._get_recommended_models()
        
        self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _get_recommended_models(self) -> List[str]:
        """StepÎ≥Ñ Í∂åÏû• Î™®Îç∏ Î™©Î°ù"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_u2net"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """Î™®Îç∏ Î°úÎìú"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # Ï∫êÏãú ÌôïÏù∏
            if model_name in self.loaded_models:
                self.logger.info(f"‚úÖ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_name}")
                return self.loaded_models[model_name]
            
            # SafeModelServiceÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú
            model = await self.model_loader.safe_model_service.call_model(model_name)
            
            if model:
                self.loaded_models[model_name] = model
                self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                return model
            else:
                # Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±
                fallback = self._create_fallback_model(model_name)
                self.loaded_models[model_name] = fallback
                self.logger.warning(f"‚ö†Ô∏è Ìè¥Î∞± Î™®Îç∏ ÏÇ¨Ïö©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            fallback = self._create_fallback_model(model_name)
            self.loaded_models[model_name] = fallback
            return fallback
    
    def _create_fallback_model(self, model_name: str) -> Any:
        """Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±"""
        
        class FallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
            
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'fallback'
                }
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return FallbackModel(model_name)
    
    async def unload_models(self):
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            with self._lock:
                for model in self.loaded_models.values():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                
                self.loaded_models.clear()
                
            gc.collect()
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.logger.info(f"üßπ {self.step_name} Î™®Îç∏ Ïñ∏Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå®: {e}")

# ==============================================
# üî• Main ModelLoader Class
# ==============================================

class ModelLoader:
    """ÏôÑÏ†Ñ ÌÜµÌï© ModelLoader v6.0"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        **kwargs
    ):
        """ÏôÑÏ†Ñ ÌÜµÌï© ÏÉùÏÑ±Ïûê"""
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.config = SafeConfig(config or {})
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # SafeModelService ÌÜµÌï©
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        
        # ÎîîÎ∞îÏù¥Ïä§ Î∞è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # Î™®Îç∏ Î°úÎçî ÌäπÌôî ÌååÎùºÎØ∏ÌÑ∞
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        
        # Î™®Îç∏ Ï∫êÏãú Î∞è ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í¥ÄÎ¶¨
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ÎèôÍ∏∞Ìôî Î∞è Ïä§Î†àÎìú Í¥ÄÎ¶¨
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        
        # ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú
        self.enable_auto_detection = enable_auto_detection
        self.auto_detector = None
        self.detected_model_registry = {}
        
        # Ï¥àÍ∏∞Ìôî Ïã§Ìñâ
        self._initialize_components()
        
        if self.enable_auto_detection:
            self._setup_auto_detection()
        
        self.logger.info(f"üéØ ModelLoader v6.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"üîß Device: {self.device}, SafeModelService: ‚úÖ")
    
    def _initialize_components(self):
        """Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî"""
        try:
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            if self.is_m3_max:
                self.use_fp16 = True
                if COREML_AVAILABLE:
                    self.logger.info("üçé CoreML ÏµúÏ†ÅÌôî ÌôúÏÑ±ÌôîÎê®")
            
            self._initialize_model_registry()
            
            self.logger.info(f"üì¶ ModelLoader Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        except Exception as e:
            self.logger.error(f"‚ùå Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _initialize_model_registry(self):
        """Í∏∞Î≥∏ Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                )
            }
            
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"üìù Í∏∞Î≥∏ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {registered_count}Í∞ú")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _setup_auto_detection(self):
        """ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        try:
            self.logger.info("üîç ÏûêÎèô Î™®Îç∏ ÌÉêÏßÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ï§ë...")
            # auto_model_detector Ïó∞ÎèôÏùÄ Ïó¨Í∏∞ÏÑú Íµ¨ÌòÑ
            # ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄÎ•º ÏúÑÌï¥ ÎèôÏ†Å import ÏÇ¨Ïö©
        except Exception as e:
            self.logger.error(f"‚ùå ÏûêÎèô ÌÉêÏßÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """Î™®Îç∏ Îì±Î°ù"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device
                }
                self.safe_model_service.register_model(name, model_dict)
                
                self.logger.info(f"üìù Î™®Îç∏ Îì±Î°ù: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def register_model(self, name: str, config: Union[Dict[str, Any], Any]) -> bool:
        """Î™®Îç∏ Îì±Î°ù (Ïñ¥ÎåëÌÑ∞Ïö©)"""
        try:
            with self._lock:
                if isinstance(config, dict):
                    model_config = config.copy()
                else:
                    model_config = {}
                    if hasattr(config, '__dict__'):
                        model_config.update(config.__dict__)
                    
                    for attr in ['name', 'type', 'path', 'device', 'validated']:
                        if hasattr(config, attr):
                            model_config[attr] = getattr(config, attr)
                
                model_config.setdefault('name', name)
                model_config.setdefault('auto_registered', True)
                model_config.setdefault('registration_time', time.time())
                
                if not hasattr(self, 'detected_model_registry'):
                    self.detected_model_registry = {}
                
                self.detected_model_registry[name] = model_config
                self.safe_model_service.register_model(name, model_config)
                
                self.logger.debug(f"‚úÖ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {name}")
                return True
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú"""
        try:
            model = await self.safe_model_service.call_model(model_name)
            if model:
                self.logger.info(f"‚úÖ SafeModelServiceÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                return model
            
            return await self._direct_async_load(model_name, **kwargs)
                
        except Exception as e:
            self.logger.error(f"ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return None
    
    async def _direct_async_load(self, model_name: str, **kwargs) -> Optional[Any]:
        """ÏßÅÏ†ë ÎπÑÎèôÍ∏∞ Î°úÎìú"""
        try:
            # Í∏∞Î≥∏ Î™®Îç∏ ÏÉùÏÑ± Î°úÏßÅ
            model_dict = {
                'name': model_name,
                'status': 'loaded',
                'type': 'direct_load',
                'device': self.device,
                'kwargs': kwargs
            }
            
            if self.safe_model_service.register_model(model_name, model_dict):
                return self.safe_model_service.models.get(model_name)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå ÏßÅÏ†ë ÎπÑÎèôÍ∏∞ Î°úÎìú Ïã§Ìå®: {e}")
            return None
    
    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ÏôÑÏ†Ñ ÌÜµÌï© Î™®Îç∏ Î°úÎìú"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # Ï∫êÏãúÎêú Î™®Îç∏ ÌôïÏù∏
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"üì¶ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {name}")
                    return self.model_cache[cache_key]
                
                # SafeModelService Ïö∞ÏÑ† ÏÇ¨Ïö©
                model = await self.safe_model_service.call_model(name)
                if model:
                    self.model_cache[cache_key] = model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"‚úÖ SafeModelServiceÎ•º ÌÜµÌïú Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {name}")
                    return model
                
                # Î™®Îç∏ ÏÑ§Ï†ï ÌôïÏù∏
                if name not in self.model_configs:
                    self.logger.warning(f"‚ö†Ô∏è Îì±Î°ùÎêòÏßÄ ÏïäÏùÄ Î™®Îç∏: {name}")
                    default_config = {
                        'name': name,
                        'type': 'unknown',
                        'device': self.device
                    }
                    self.safe_model_service.register_model(name, default_config)
                    model = await self.safe_model_service.call_model(name)
                    if model:
                        self.model_cache[cache_key] = model
                        return model
                    else:
                        return None
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"üì¶ Î™®Îç∏ Î°úÎî© ÏãúÏûë: {name}")
                
                # Î©îÎ™®Î¶¨ ÌôïÏù∏ Î∞è Ï†ïÎ¶¨
                await self._check_memory_and_cleanup()
                
                # Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {name}")
                    return None
                
                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú
                await self._load_checkpoint(model, model_config)
                
                # ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô
                if hasattr(model, 'to'):
                    to_method = getattr(model, 'to', None)
                    success, result, message = self.function_validator.safe_call(to_method, self.device)
                    if success:
                        model = result
                
                # M3 Max ÏµúÏ†ÅÌôî Ï†ÅÏö©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ÏµúÏ†ÅÌôî
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        half_method = getattr(model, 'half', None)
                        success, result, message = self.function_validator.safe_call(half_method)
                        if success:
                            model = result
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è FP16 Î≥ÄÌôò Ïã§Ìå®: {e}")
                
                # ÌèâÍ∞Ä Î™®Îìú
                if hasattr(model, 'eval'):
                    eval_method = getattr(model, 'eval', None)
                    self.function_validator.safe_call(eval_method)
                
                # Ï∫êÏãúÏóê Ï†ÄÏû•
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎî© Ïã§Ìå® {name}: {e}")
            return None
    
    async def initialize(self) -> bool:
        """ModelLoader Ï¥àÍ∏∞Ìôî"""
        try:
            self.logger.info("üöÄ ModelLoader v6.0 Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            await asyncio.sleep(0.1)
            
            if not hasattr(self, 'device_manager'):
                self.logger.warning("‚ö†Ô∏è ÎîîÎ∞îÏù¥Ïä§ Îß§ÎãàÏ†ÄÍ∞Ä ÏóÜÏùå")
                return False
            
            if hasattr(self, 'memory_manager'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {message}")
                
            self.logger.info("‚úÖ ModelLoader v6.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    async def _create_model_instance(
        self,
        model_config: Union[ModelConfig, StepModelConfig],
        **kwargs
    ) -> Optional[Any]:
        """Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±"""
        try:
            model_class_name = getattr(model_config, 'model_class', 'BaseModel')
            
            if model_class_name == "GraphonomyModel":
                num_classes = getattr(model_config, 'num_classes', 20)
                return GraphonomyModel(num_classes=num_classes, backbone='resnet101')
            
            elif model_class_name == "OpenPoseModel":
                num_keypoints = getattr(model_config, 'num_classes', 18)
                return OpenPoseModel(num_keypoints=num_keypoints)
            
            elif model_class_name == "U2NetModel":
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class_name == "GeometricMatchingModel":
                return GeometricMatchingModel(feature_size=256)
            
            elif model_class_name == "HRVITONModel":
                return HRVITONModel(input_nc=3, output_nc=3, ngf=64)
            
            elif model_class_name == "StableDiffusionPipeline":
                return await self._create_diffusion_model(model_config)
            
            else:
                self.logger.warning(f"‚ö†Ô∏è ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î™®Îç∏ ÌÅ¥ÎûòÏä§: {model_class_name}")
                return BaseModel()
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    async def _create_diffusion_model(self, model_config):
        """Diffusion Î™®Îç∏ ÏÉùÏÑ±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                checkpoint_path = getattr(model_config, 'checkpoint_path', None)
                if checkpoint_path and Path(checkpoint_path).exists():
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        checkpoint_path,
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                else:
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        "runwayml/stable-diffusion-v1-5",
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                
                return pipeline
            else:
                self.logger.warning("‚ö†Ô∏è Diffusers ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏóÜÏùå")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Diffusion Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    async def _load_checkpoint(self, model: Any, model_config: Union[ModelConfig, StepModelConfig]):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"""
        try:
            checkpoint_path = None
            
            if hasattr(model_config, 'checkpoint_path'):
                checkpoint_path = model_config.checkpoint_path
            elif hasattr(model_config, 'checkpoints') and isinstance(model_config.checkpoints, dict):
                checkpoints = getattr(model_config, 'checkpoints', {})
                if isinstance(checkpoints, dict):
                    checkpoint_path = checkpoints.get('primary_path')
            
            if not checkpoint_path:
                self.logger.info(f"üìù Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú ÏóÜÏùå: {getattr(model_config, 'name', 'unknown')}")
                return
                
            checkpoint_path = Path(checkpoint_path)
            
            if not checkpoint_path.exists():
                self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {checkpoint_path}")
                return
            
            # PyTorch Î™®Îç∏Ïù∏ Í≤ΩÏö∞
            if hasattr(model, 'load_state_dict') and TORCH_AVAILABLE:
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict Ï†ïÎ¶¨
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif isinstance(state_dict, dict) and 'model' in state_dict:
                    state_dict = state_dict['model']
                
                # ÌÇ§ Ïù¥Î¶Ñ Ï†ïÎ¶¨
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                # ÏïàÏ†ÑÌïú Ìò∏Ï∂ú
                load_state_dict_method = getattr(model, 'load_state_dict', None)
                success, result, message = self.function_validator.safe_call(
                    load_state_dict_method, cleaned_state_dict, strict=False
                )
                
                if success:
                    self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú ÏôÑÎ£å: {checkpoint_path}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Ïã§Ìå®: {message}")
            
            else:
                self.logger.info(f"üìù Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Í±¥ÎÑàÎúÄ: {getattr(model_config, 'name', 'unknown')}")
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Ïã§Ìå®: {e}")
    
    async def _apply_m3_max_optimization(self, model: Any, model_config) -> Any:
        """M3 Max ÌäπÌôî Î™®Îç∏ ÏµúÏ†ÅÌôî"""
        try:
            optimizations_applied = []
            
            # MPS ÎîîÎ∞îÏù¥Ïä§ ÏµúÏ†ÅÌôî
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # CoreML Ïª¥ÌååÏùº Ï§ÄÎπÑ
            if COREML_AVAILABLE and hasattr(model, 'eval'):
                optimizations_applied.append("CoreML compilation ready")
            
            # Metal Performance Shaders ÏµúÏ†ÅÌôî
            if self.device == 'mps':
                try:
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"üçé M3 Max Î™®Îç∏ ÏµúÏ†ÅÌôî Ï†ÅÏö©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max Î™®Îç∏ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return model
    
    async def _check_memory_and_cleanup(self):
        """Î©îÎ™®Î¶¨ ÌôïÏù∏ Î∞è Ï†ïÎ¶¨"""
        try:
            if hasattr(self.memory_manager, 'check_memory_pressure'):
                check_method = getattr(self.memory_manager, 'check_memory_pressure', None)
                success, is_pressure, message = self.function_validator.safe_call(check_method)
                
                if success and is_pressure:
                    await self._cleanup_least_used_models()
            
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {message}")
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ÏÇ¨Ïö©ÎüâÏù¥ Ï†ÅÏùÄ Î™®Îç∏ Ï†ïÎ¶¨"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    if hasattr(model, 'cpu'):
                        cpu_method = getattr(model, 'cpu', None)
                        success, result, message = self.function_validator.safe_call(cpu_method)
                        if not success:
                            self.logger.warning(f"‚ö†Ô∏è CPU Ïù¥Îèô Ïã§Ìå®: {message}")
                    
                    del model
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"üßπ Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨: {len(cleaned_models)}Í∞ú Î™®Îç∏ Ìï¥Ï†ú")
                    
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return StepModelInterface(self, step_name)
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """Í∏∞Ï°¥ Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï°∞Ìöå"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    if hasattr(interface, 'unload_models'):
                        unload_method = getattr(interface, 'unload_models', None)
                        success, result, message = self.function_validator.safe_call(unload_method)
                        if not success:
                            self.logger.warning(f"‚ö†Ô∏è Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïñ∏Î°úÎìú Ïã§Ìå®: {message}")
                    
                    del self.step_interfaces[step_name]
                    self.logger.info(f"üóëÔ∏è {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
                    
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            with self._lock:
                info = {
                    'name': model_name,
                    'registered': model_name in self.model_configs,
                    'cached': any(model_name in key for key in self.model_cache.keys()),
                    'device': self.device,
                    'config': None,
                    'load_time': None,
                    'last_access': None,
                    'access_count': 0,
                    'auto_detected': False
                }
                
                if model_name in self.model_configs:
                    config = self.model_configs[model_name]
                    info['config'] = {
                        'model_type': str(getattr(config, 'model_type', 'unknown')),
                        'model_class': getattr(config, 'model_class', 'unknown'),
                        'input_size': getattr(config, 'input_size', (512, 512)),
                        'num_classes': getattr(config, 'num_classes', None)
                    }
                
                if hasattr(self, 'detected_model_registry') and model_name in self.detected_model_registry:
                    detected_info = self.detected_model_registry[model_name]
                    info['auto_detected'] = True
                    info['detection_info'] = detected_info
                
                for cache_key in self.model_cache.keys():
                    if model_name in cache_key:
                        info['cached'] = True
                        info['load_time'] = self.load_times.get(cache_key)
                        info['last_access'] = self.last_access.get(cache_key)
                        info['access_count'] = self.access_counts.get(cache_key, 0)
                        break
                
                return info
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå® {model_name}: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îì† Î™®Îç∏ Î™©Î°ù"""
        try:
            with self._lock:
                models_info = {}
                
                for model_name in self.model_configs.keys():
                    models_info[model_name] = self.get_model_info(model_name)
                
                if hasattr(self, 'detected_model_registry'):
                    for model_name in self.detected_model_registry.keys():
                        if model_name not in models_info:
                            models_info[model_name] = self.get_model_info(model_name)
                
                safe_models = self.safe_model_service.list_models()
                for model_name, status in safe_models.items():
                    if model_name not in models_info:
                        models_info[model_name] = {
                            'name': model_name,
                            'source': 'SafeModelService',
                            'status': status
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ Ï†ïÎ≥¥"""
        try:
            with self._lock:
                stats = {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'model_cache_dir': str(self.model_cache_dir),
                    'cached_models': len(self.model_cache),
                    'registered_models': len(self.model_configs),
                    'detected_models': len(getattr(self, 'detected_model_registry', {})),
                    'step_interfaces': len(self.step_interfaces),
                    'total_loads': sum(self.access_counts.values()),
                    'optimization_enabled': self.optimization_enabled,
                    'use_fp16': self.use_fp16,
                    'max_cached_models': self.max_cached_models,
                    'auto_detection_enabled': self.enable_auto_detection
                }
                
                if hasattr(self.memory_manager, 'get_available_memory'):
                    try:
                        available_memory = self.memory_manager.get_available_memory()
                        stats['available_memory_gb'] = available_memory
                    except:
                        stats['available_memory_gb'] = 'unknown'
                
                return stats
                
        except Exception as e:
            self.logger.error(f"‚ùå ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """ÏôÑÏ†ÑÌïú Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    self.cleanup_step_interface(step_name)
            
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            cpu_method = getattr(model, 'cpu', None)
                            success, result, message = self.function_validator.safe_call(cpu_method)
                            if not success:
                                self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ CPU Ïù¥Îèô Ïã§Ìå®: {message}")
                        del model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {message}")
            
            self.logger.info("‚úÖ ModelLoader v6.0 Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

# ==============================================
# üî• Global ModelLoader Management
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True
            )
            logger.info("üåê Ï†ÑÏó≠ ModelLoader v6.0 Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±")
        
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> Dict[str, Any]:
    """Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        validator = SafeFunctionValidator()
        
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            init_method = getattr(loader, 'initialize', None)
            is_callable, reason, safe_method = SafeFunctionValidator.validate_callable(
                init_method, "initialize_global"
            )
            
            if is_callable:
                future = asyncio.create_task(safe_method())
                return {"success": True, "message": "Initialization started", "future": future}
            else:
                logger.warning(f"‚ö†Ô∏è initializeÍ∞Ä callableÌïòÏßÄ ÏïäÏùå: {reason}")
                return {"success": False, "error": f"initialize method not callable: {reason}"}
        else:
            init_method = getattr(loader, 'initialize', None)
            success, result, message = asyncio.run(
                validator.safe_async_call(init_method)
            )
            
            if success:
                return {"success": result, "message": "Initialization completed"}
            else:
                return {"success": False, "error": f"Initialization failed: {message}"}
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return {"success": False, "error": str(e)}

def cleanup_global_loader():
    """Ï†ÑÏó≠ ModelLoader Ï†ïÎ¶¨"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            validator = SafeFunctionValidator()
            cleanup_method = getattr(_global_model_loader, 'cleanup', None)
            success, result, message = validator.safe_call(cleanup_method)
            
            if not success:
                logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ Î°úÎçî Ï†ïÎ¶¨ Ïã§Ìå®: {message}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("üåê Ï†ÑÏó≠ ModelLoader v6.0 Ï†ïÎ¶¨ ÏôÑÎ£å")

# ==============================================
# üî• Utility Functions
# ==============================================

_global_model_service = None
_service_lock = threading.Lock()

def get_model_service() -> SafeModelService:
    """Ï†ÑÏó≠ Î™®Îç∏ ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_service
    
    if _global_model_service is None:
        with _service_lock:
            if _global_model_service is None:
                _global_model_service = SafeModelService()
                logger.info("‚úÖ Ï†ÑÏó≠ SafeModelService ÏÉùÏÑ±")
    
    return _global_model_service

async def safe_warmup_models(model_names: list) -> Dict[str, bool]:
    """Ïó¨Îü¨ Î™®Îç∏ ÏïàÏ†Ñ ÏõåÎ∞çÏóÖ"""
    service = get_model_service()
    results = {}
    
    for name in model_names:
        try:
            # Í∏∞Î≥∏ ÏõåÎ∞çÏóÖ Î°úÏßÅ
            results[name] = True
        except Exception as e:
            logger.error(f"‚ùå Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå® {name}: {e}")
            results[name] = False
    
    return results

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ÎîïÏÖîÎÑàÎ¶¨Î•º Î™®Îç∏Î°ú ÏïàÏ†ÑÌïòÍ≤å Îì±Î°ù"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock Î™®Îç∏ ÏÉùÏÑ±"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': 'mps',
        'loaded_at': '2025-01-19T12:00:00Z'
    }
    
    service = get_model_service()
    return service._create_dict_wrapper(mock_dict)

# ÏïàÏ†ÑÌïú Ìò∏Ï∂ú Ìï®ÏàòÎì§
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """Ï†ÑÏó≠ ÏïàÏ†ÑÌïú Ìï®Ïàò Ìò∏Ï∂ú"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """Ï†ÑÏó≠ ÏïàÏ†ÑÌïú ÎπÑÎèôÍ∏∞ Ìï®Ïàò Ìò∏Ï∂ú"""
    return await SafeFunctionValidator.safe_async_call(obj, *args, **kwargs)

def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
    """Ï†ÑÏó≠ ÏïàÏ†ÑÌïú ÏÜçÏÑ± Ï†ëÍ∑º Î∞è Ìò∏Ï∂ú"""
    return SafeFunctionValidator.safe_getattr_call(obj, attr_name, *args, **kwargs)

def is_safely_callable(obj: Any) -> bool:
    """Ï†ÑÏó≠ callable ÏïàÏ†ÑÏÑ± Í≤ÄÏ¶ù"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

# ==============================================
# üî• Module Exports
# ==============================================

__all__ = [
    # Dict Callable Ïò§Î•ò Ìï¥Í≤∞ ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'SafeFunctionValidator',
    'SafeModelService',
    
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'ModelLoader',
    'ModelFormat',
    'ModelConfig', 
    'StepModelConfig',
    'ModelType',
    'ModelPriority',
    'DeviceManager',
    'ModelMemoryManager',
    'StepModelInterface',
    'SafeConfig',
    
    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§
    'preprocess_image',
    'postprocess_segmentation', 
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'tensor_to_pil',
    'pil_to_tensor',
    
    # Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    
    # Ìå©ÌÜ†Î¶¨ Î∞è Í¥ÄÎ¶¨ Ìï®ÏàòÎì§
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    
    # ÏÉàÎ°úÏö¥ ÏïàÏ†ÑÌïú Ìò∏Ï∂ú Ìï®ÏàòÎì§
    'get_model_service',
    'safe_warmup_models',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_async_call',
    'safe_getattr_call',
    'is_safely_callable',
    
    # ÏÉÅÏàò
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    
    # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô
    'STEP_MODEL_REQUESTS',
    'StepModelRequestAnalyzer'
]

# Î™®Îìà Î†àÎ≤®ÏóêÏÑú ÏïàÏ†ÑÌïú Ï†ïÎ¶¨ Ìï®Ïàò Îì±Î°ù
import atexit
atexit.register(cleanup_global_loader)

# Î™®Îìà Î°úÎìú ÌôïÏù∏
logger.info("‚úÖ ModelLoader v6.0 Î™®Îìà Î°úÎìú ÏôÑÎ£å - ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ + ÏôÑÏ†Ñ Ïó∞Îèô")
logger.info("üîó SafeModelService + SafeFunctionValidator ÌÜµÌï©")
logger.info("üîß step_model_requests + auto_model_detector ÏôÑÎ≤Ω Ìò∏Ìôò")
logger.info("üçé M3 Max 128GB ÏµúÏ†ÅÌôî")
logger.info("üõ°Ô∏è Î™®Îì† Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú ÏïàÏ†ÑÏÑ± Î≥¥Ïû•")
logger.info("üîÑ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ (ÌïúÎ∞©Ìñ• ÏùòÏ°¥ÏÑ±)")
logger.info(f"üéØ PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}, MPS: {'‚úÖ' if MPS_AVAILABLE else '‚ùå'}")
logger.info(f"üî¢ NumPy: {'‚úÖ' if NUMPY_AVAILABLE else '‚ùå'} v{np.__version__ if NUMPY_AVAILABLE else 'N/A'}")

if NUMPY_AVAILABLE and int(np.__version__.split('.')[0]) >= 2:
    logger.warning("‚ö†Ô∏è NumPy 2.x Í∞êÏßÄÎê® - conda install numpy=1.24.3 Í∂åÏû•")
else:
    logger.info("‚úÖ NumPy Ìò∏ÌôòÏÑ± ÌôïÏù∏Îê®")

logger.info("üöÄ ModelLoader v6.0 ÏôÑÏ†Ñ ÌÜµÌï© ÏôÑÎ£å!")
logger.info("   ‚úÖ Í∏∞Ï°¥ Î™®Îì† Í∏∞Îä•Î™Ö/ÌÅ¥ÎûòÏä§Î™Ö 100% Ïú†ÏßÄ")
logger.info("   ‚úÖ Dict Callable Ïò§Î•ò Í∑ºÎ≥∏ ÏõêÏù∏ Ìï¥Í≤∞")
logger.info("   ‚úÖ step_model_requests.py ÏôÑÎ≤Ω Ïó∞Îèô")
logger.info("   ‚úÖ auto_model_detector ÌïúÎ∞©Ìñ• Ï∞∏Ï°∞")
logger.info("   ‚úÖ base_step_mixin.py ÏôÑÎ≤Ω Ìò∏Ìôò")
logger.info("   ‚úÖ M3 Max 128GB Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî")
logger.info("   ‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ")
logger.info("   ‚úÖ ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ïÏÑ± ÏµúÍ≥† ÏàòÏ§Ä")