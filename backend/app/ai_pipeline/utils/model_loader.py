# backend/app/ai_pipeline/utils/model_loader_v6.py
"""
üî• ModelLoader v6.0 - Complete Refactoring for Step-Specific Neural Networks
================================================================================

‚úÖ ÏôÑÏ†Ñ Î¶¨Ìå©ÌÜ†ÎßÅ - StepÎ≥Ñ ÌäπÌôî Ïã†Í≤ΩÎßù Íµ¨Ï°∞ Îß§Ìïë
‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Íµ¨Ï°∞ ÏûêÎèô Î∂ÑÏÑù Î∞è Îß§Ìïë ÏãúÏä§ÌÖú
‚úÖ StepÎ≥Ñ Ïã§Ï†ú AI Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò Ï†ïÏùò
‚úÖ Í∞ÄÏ§ëÏπò ÌÇ§ Îß§Ìïë Î∞è Ìò∏ÌôòÏÑ± Î≥¥Ïû•
‚úÖ ÎèôÏ†Å Ïã†Í≤ΩÎßù ÏÉùÏÑ± ÏãúÏä§ÌÖú
‚úÖ Central Hub DI Container ÏôÑÏ†Ñ Ïó∞Îèô
‚úÖ BaseStepMixin ÏôÑÏ†Ñ Ìò∏Ìôò

ÌïµÏã¨ ÏÑ§Í≥Ñ ÏõêÏπô:
1. Step-Specific Architecture: Í∞Å StepÎßàÎã§ ÌäπÌôîÎêú Ïã†Í≤ΩÎßù Íµ¨Ï°∞
2. Checkpoint Mapping: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Íµ¨Ï°∞Î•º Ïã†Í≤ΩÎßù Íµ¨Ï°∞Ïóê ÏûêÎèô Îß§Ìïë
3. Dynamic Model Creation: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í∏∞Î∞ò ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ±
4. Weight Compatibility: Í∏∞Ï°¥ Í∞ÄÏ§ëÏπòÏôÄ 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•
5. Step Integration: Step ÌÅ¥ÎûòÏä§ÏôÄ ÏôÑÏ†Ñ ÌÜµÌï©

Author: MyCloset AI Team
Date: 2025-08-09
Version: 6.0 (Complete Refactoring for Step-Specific Networks)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import warnings
import hashlib
import pickle
import mmap
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from abc import ABC, abstractmethod
from io import BytesIO
import copy

# PyTorch ÏïàÏ†Ñ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.nn import Module
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None
    Module = object

# Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò import
try:
    from .model_architectures import (
        SAMModel, U2NetModel, OpenPoseModel, GMMModel, 
        TOMModel, OOTDModel, TPSModel, RAFTModel,
        RealESRGANModel, CLIPModel, LPIPSModel,
        DeepLabV3PlusModel, MobileSAMModel, VITONHDModel, GFPGANModel,
        HRNetPoseModel, GraphonomyModel
    )
    MODEL_ARCHITECTURES_AVAILABLE = True
except ImportError as e:
    MODEL_ARCHITECTURES_AVAILABLE = False
    print(f"‚ö†Ô∏è model_architectures import Ïã§Ìå®: {e}")
    # Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤òÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÎäî Í≤ΩÏö∞Î•º ÏúÑÌïú ÎåÄÏ≤¥ ÌÅ¥ÎûòÏä§Îì§
    SAMModel = None
    U2NetModel = None
    OpenPoseModel = None
    GMMModel = None
    TOMModel = None
    OOTDModel = None
    TPSModel = None
    RAFTModel = None
    RealESRGANModel = None
    CLIPModel = None
    LPIPSModel = None
    DeepLabV3PlusModel = None
    MobileSAMModel = None
    VITONHDModel = None
    GFPGANModel = None
    HRNetPoseModel = None
    GraphonomyModel = None

# Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# ÏãúÏä§ÌÖú Ï†ïÎ≥¥
import platform
IS_M3_MAX = False
MEMORY_GB = 16.0
MPS_AVAILABLE = False

if platform.system() == 'Darwin':
    import subprocess
    try:
        result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                              capture_output=True, text=True, timeout=5)
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(['sysctl', '-n', 'hw.memsize'],
                                     capture_output=True, text=True, timeout=5)
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
    except:
        pass

if TORCH_AVAILABLE:
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True

# Ï†ÑÏó≠ Î≥ÄÏàòÎì§
_global_model_loader_v6 = None
_global_step_factory = None
_loader_lock = threading.Lock()

# Í∏∞Î≥∏ ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
if TORCH_AVAILABLE:
    if MPS_AVAILABLE:
        DEFAULT_DEVICE = 'mps'
    elif torch.cuda.is_available():
        DEFAULT_DEVICE = 'cuda'
    else:
        DEFAULT_DEVICE = 'cpu'
else:
    DEFAULT_DEVICE = 'cpu'

# Î°úÍ±∞ ÏÑ§Ï†ï
if IS_M3_MAX and MPS_AVAILABLE:
    DEFAULT_DEVICE = "mps"
elif TORCH_AVAILABLE and torch.cuda.is_available():
    DEFAULT_DEVICE = "cuda"

logger = logging.getLogger(__name__)

# ==============================================
# üî• 1. StepÎ≥Ñ ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò Ï†ïÏùò
# ==============================================

class StepSpecificArchitecture(ABC):
    """StepÎ≥Ñ ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò Í∏∞Î∞ò ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, step_name: str, device: str = DEFAULT_DEVICE):
        self.step_name = step_name
        self.device = device
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
    @abstractmethod
    def create_model(self, checkpoint_analysis: Dict[str, Any]):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í∏∞Î∞ò Î™®Îç∏ ÏÉùÏÑ±"""
        pass
    
    @abstractmethod
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§Î•º Î™®Îç∏ Íµ¨Ï°∞Ïóê Îß§Ìïë"""
        pass
    
    @abstractmethod
    def validate_model(self, model) -> bool:
        """Î™®Îç∏ Í≤ÄÏ¶ù"""
        pass

class HumanParsingArchitecture(StepSpecificArchitecture):
    """Human Parsing ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Human Parsing Î™®Îç∏ ÏÉùÏÑ±"""
        num_classes = checkpoint_analysis.get('num_classes', 20)
        input_channels = checkpoint_analysis.get('input_channels', 3)
        architecture_type = checkpoint_analysis.get('architecture_type', 'graphonomy')
        
        if architecture_type == 'graphonomy':
            return self._create_graphonomy_model(num_classes, input_channels)
        elif architecture_type == 'u2net':
            return self._create_u2net_model(num_classes, input_channels)
        elif architecture_type == 'hrnet':
            return self._create_hrnet_model(num_classes)
        else:
            return self._create_generic_parsing_model(num_classes, input_channels)
    
    def _create_graphonomy_model(self, num_classes: int, input_channels: int) -> nn.Module:
        """Graphonomy Î™®Îç∏ ÏÉùÏÑ±"""
        class GraphonomyModel(nn.Module):
            def __init__(self, num_classes=20, input_channels=3):
                super().__init__()
                self.num_classes = num_classes
                
                # ResNet-101 based backbone
                self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                
                # ResNet blocks
                self.layer1 = self._make_layer(64, 64, 3)
                self.layer2 = self._make_layer(64, 128, 4, stride=2)
                self.layer3 = self._make_layer(128, 256, 23, stride=2)
                self.layer4 = self._make_layer(256, 512, 3, stride=2)
                
                # ASPP (Atrous Spatial Pyramid Pooling)
                self.aspp = self._create_aspp(512)
                
                # Decoder
                self.decoder = self._create_decoder(256, num_classes)
                
                # Edge detection branch (Graphonomy specific)
                self.edge_branch = nn.Sequential(
                    nn.Conv2d(256, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 1, 1)
                )
                
            def _make_layer(self, inplanes, planes, blocks, stride=1):
                layers = []
                layers.append(self._make_block(inplanes, planes, stride))
                for _ in range(1, blocks):
                    layers.append(self._make_block(planes, planes))
                return nn.Sequential(*layers)
            
            def _make_block(self, inplanes, planes, stride=1):
                return nn.Sequential(
                    nn.Conv2d(inplanes, planes, 3, stride=stride, padding=1, bias=False),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(planes, planes, 3, padding=1, bias=False),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True)
                )
            
            def _create_aspp(self, inplanes):
                return nn.Sequential(
                    nn.Conv2d(inplanes, 256, 1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=6, dilation=6, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=12, dilation=12, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=18, dilation=18, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True)
                )
            
            def _create_decoder(self, inplanes, num_classes):
                return nn.Sequential(
                    nn.Conv2d(inplanes, 256, 3, padding=1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Conv2d(256, 256, 3, padding=1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.1),
                    nn.Conv2d(256, num_classes, 1)
                )
            
            def forward(self, x):
                # Backbone
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.maxpool(x)
                
                x = self.layer1(x)
                x = self.layer2(x)
                x = self.layer3(x)
                x = self.layer4(x)
                
                # ASPP
                x = self.aspp(x)
                
                # Decoder
                parsing = self.decoder(x)
                edge = self.edge_branch(x)
                
                # Upsample to original size
                parsing = F.interpolate(parsing, scale_factor=8, mode='bilinear', align_corners=True)
                edge = F.interpolate(edge, scale_factor=8, mode='bilinear', align_corners=True)
                
                return {
                    'parsing': parsing,
                    'edge': edge
                }
        
        return GraphonomyModel(num_classes, input_channels)
    
    def _create_u2net_model(self, num_classes: int, input_channels: int) -> nn.Module:
        """U2Net Î™®Îç∏ ÏÉùÏÑ±"""
        class U2NetModel(nn.Module):
            def __init__(self, num_classes=20, input_channels=3):
                super().__init__()
                self.num_classes = num_classes
                
                # Encoder
                self.stage1 = self._make_stage(input_channels, 32, 7)
                self.stage2 = self._make_stage(32, 32, 6)
                self.stage3 = self._make_stage(32, 64, 5)
                self.stage4 = self._make_stage(64, 128, 4)
                self.stage5 = self._make_stage(128, 256, 4)
                self.stage6 = self._make_stage(256, 512, 4)
                
                # Bridge
                self.bridge = self._make_stage(512, 512, 4)
                
                # Decoder
                self.stage5d = self._make_stage(1024, 256, 4)
                self.stage4d = self._make_stage(512, 128, 4)
                self.stage3d = self._make_stage(256, 64, 5)
                self.stage2d = self._make_stage(128, 32, 6)
                self.stage1d = self._make_stage(64, 16, 7)
                
                # Output
                self.outconv = nn.Conv2d(16, num_classes, 3, padding=1)
                
            def _make_stage(self, inplanes, planes, depth):
                layers = []
                layers.append(nn.Conv2d(inplanes, planes, 3, padding=1))
                layers.append(nn.BatchNorm2d(planes))
                layers.append(nn.ReLU(inplace=True))
                
                for _ in range(depth - 1):
                    layers.append(nn.Conv2d(planes, planes, 3, padding=1))
                    layers.append(nn.BatchNorm2d(planes))
                    layers.append(nn.ReLU(inplace=True))
                
                return nn.Sequential(*layers)
            
            def forward(self, x):
                # Encoder
                e1 = self.stage1(x)
                e2 = self.stage2(F.max_pool2d(e1, 2))
                e3 = self.stage3(F.max_pool2d(e2, 2))
                e4 = self.stage4(F.max_pool2d(e3, 2))
                e5 = self.stage5(F.max_pool2d(e4, 2))
                e6 = self.stage6(F.max_pool2d(e5, 2))
                
                # Bridge
                bridge = self.bridge(F.max_pool2d(e6, 2))
                
                # Decoder
                d5 = self.stage5d(torch.cat([F.interpolate(bridge, scale_factor=2), e6], dim=1))
                d4 = self.stage4d(torch.cat([F.interpolate(d5, scale_factor=2), e5], dim=1))
                d3 = self.stage3d(torch.cat([F.interpolate(d4, scale_factor=2), e4], dim=1))
                d2 = self.stage2d(torch.cat([F.interpolate(d3, scale_factor=2), e3], dim=1))
                d1 = self.stage1d(torch.cat([F.interpolate(d2, scale_factor=2), e2], dim=1))
                
                # Output
                out = self.outconv(F.interpolate(d1, scale_factor=2))
                
                return {'parsing': out}
        
        return U2NetModel(num_classes, input_channels)
    
    def _create_hrnet_model(self, num_classes: int) -> nn.Module:
        """HRNet Î™®Îç∏ ÏÉùÏÑ±"""
        class HRNetModel(nn.Module):
            def __init__(self, num_classes=20):
                super().__init__()
                self.num_classes = num_classes
                
                # Stem
                self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                
                # Stage 1
                self.layer1 = self._make_layer(64, 64, 4)
                
                # Multi-resolution stages
                self.stage2 = self._make_stage(64, [32, 64], 2)
                self.stage3 = self._make_stage(64, [32, 64, 128], 3)
                self.stage4 = self._make_stage(128, [32, 64, 128, 256], 4)
                
                # Final layer
                self.final_layer = nn.Conv2d(32, num_classes, 1)
                
            def _make_layer(self, inplanes, planes, blocks):
                layers = []
                for _ in range(blocks):
                    layers.append(nn.Conv2d(inplanes, planes, 3, padding=1, bias=False))
                    layers.append(nn.BatchNorm2d(planes))
                    layers.append(nn.ReLU(inplace=True))
                    inplanes = planes
                return nn.Sequential(*layers)
            
            def _make_stage(self, inplanes, channels, num_branches):
                # Simplified HRNet stage
                branches = nn.ModuleList()
                for i in range(num_branches):
                    branch_layers = []
                    in_ch = inplanes if i == 0 else channels[i-1]
                    out_ch = channels[i]
                    
                    branch_layers.append(nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False))
                    branch_layers.append(nn.BatchNorm2d(out_ch))
                    branch_layers.append(nn.ReLU(inplace=True))
                    
                    branches.append(nn.Sequential(*branch_layers))
                
                return branches
            
            def forward(self, x):
                # Stem
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.conv2(x)
                x = self.bn2(x)
                x = self.relu(x)
                
                # Stage 1
                x = self.layer1(x)
                
                # Multi-resolution processing (simplified)
                x = self.stage2[0](x)  # Use first branch for simplicity
                
                # Final prediction
                out = self.final_layer(x)
                out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)
                
                return {'parsing': out}
        
        return HRNetModel(num_classes)
    
    def _create_generic_parsing_model(self, num_classes: int, input_channels: int) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Human Parsing Î™®Îç∏ ÏÉùÏÑ±"""
        class GenericParsingModel(nn.Module):
            def __init__(self, num_classes=20, input_channels=3):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(input_channels, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 512, 3, padding=1),
                    nn.BatchNorm2d(512),
                    nn.ReLU(inplace=True),
                )
                
                self.classifier = nn.Sequential(
                    nn.Conv2d(512, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Conv2d(256, num_classes, 1)
                )
            
            def forward(self, x):
                features = self.backbone(x)
                parsing = self.classifier(features)
                return {'parsing': parsing}
        
        return GenericParsingModel(num_classes, input_channels)
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Human Parsing Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # Graphonomy specific mappings
            'backbone.': '',
            'decoder.': 'decoder.',
            'edge_decoder.': 'edge_branch.',
            'aspp.': 'aspp.',
            'ppm.': 'aspp.',
            
            # U2Net specific mappings
            'stage1.': 'stage1.',
            'stage2.': 'stage2.',
            'stage3.': 'stage3.',
            'stage4.': 'stage4.',
            'stage5.': 'stage5.',
            'stage6.': 'stage6.',
            'stage1d.': 'stage1d.',
            'stage2d.': 'stage2d.',
            'stage3d.': 'stage3d.',
            'stage4d.': 'stage4d.',
            'stage5d.': 'stage5d.',
            'outconv.': 'outconv.',
            
            # HRNet specific mappings
            'stem.': '',
            'stages.': 'stage',
            'final_layers.': 'final_layer.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Human Parsing ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Human Parsing Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            # ÏûÖÎ†• ÌÖêÏÑúÎ°ú ÌÖåÏä§Ìä∏
            test_input = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_input)
            
            # Ï∂úÎ†• Í≤ÄÏ¶ù
            if isinstance(output, dict):
                if 'parsing' in output:
                    parsing_output = output['parsing']
                    if parsing_output.dim() == 4 and parsing_output.size(1) > 0:
                        self.logger.info(f"‚úÖ Human Parsing Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ: {parsing_output.shape}")
                        return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Human Parsing Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

class PoseEstimationArchitecture(StepSpecificArchitecture):
    """Pose Estimation ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Pose Estimation Î™®Îç∏ ÏÉùÏÑ±"""
        num_keypoints = checkpoint_analysis.get('num_keypoints', 17)
        architecture_type = checkpoint_analysis.get('architecture_type', 'hrnet')
        
        if architecture_type == 'hrnet':
            return self._create_hrnet_pose_model(num_keypoints)
        elif architecture_type == 'openpose':
            return self._create_openpose_model()
        elif architecture_type == 'yolo':
            return self._create_yolo_pose_model()
        else:
            return self._create_generic_pose_model(num_keypoints)
    
    def _create_hrnet_pose_model(self, num_keypoints: int) -> nn.Module:
        """HRNet Pose Î™®Îç∏ ÏÉùÏÑ±"""
        class HRNetPoseModel(nn.Module):
            def __init__(self, num_keypoints=17):
                super().__init__()
                self.num_keypoints = num_keypoints
                
                # Stem
                self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                
                # Stage 1 (ResNet-like)
                self.layer1 = self._make_layer(64, 64, 4)
                
                # Transition to multi-resolution
                self.transition1 = self._make_transition([64], [32, 64])
                self.stage2 = self._make_stage([32, 64], 2)
                
                self.transition2 = self._make_transition([32, 64], [32, 64, 128])
                self.stage3 = self._make_stage([32, 64, 128], 3)
                
                self.transition3 = self._make_transition([32, 64, 128], [32, 64, 128, 256])
                self.stage4 = self._make_stage([32, 64, 128, 256], 4)
                
                # Final layers
                self.final_layer = nn.Conv2d(32, num_keypoints, 1)
                
            def _make_layer(self, inplanes, planes, blocks):
                layers = []
                for _ in range(blocks):
                    layers.extend([
                        nn.Conv2d(inplanes, planes, 3, padding=1, bias=False),
                        nn.BatchNorm2d(planes),
                        nn.ReLU(inplace=True)
                    ])
                    inplanes = planes
                return nn.Sequential(*layers)
            
            def _make_transition(self, in_channels, out_channels):
                transitions = nn.ModuleList()
                for i, out_ch in enumerate(out_channels):
                    if i < len(in_channels):
                        in_ch = in_channels[i]
                    else:
                        in_ch = in_channels[-1]
                    
                    transitions.append(nn.Sequential(
                        nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
                        nn.BatchNorm2d(out_ch),
                        nn.ReLU(inplace=True)
                    ))
                
                return transitions
            
            def _make_stage(self, channels, num_branches):
                branches = nn.ModuleList()
                for i in range(num_branches):
                    layers = []
                    for _ in range(4):  # 4 blocks per branch
                        layers.extend([
                            nn.Conv2d(channels[i], channels[i], 3, padding=1, bias=False),
                            nn.BatchNorm2d(channels[i]),
                            nn.ReLU(inplace=True)
                        ])
                    branches.append(nn.Sequential(*layers))
                
                return branches
            
            def forward(self, x):
                # Stem
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.conv2(x)
                x = self.bn2(x)
                x = self.relu(x)
                
                # Stage 1
                x = self.layer1(x)
                
                # Multi-resolution processing
                x_list = [trans(x) for trans in self.transition1]
                
                # Stage 2
                for i, branch in enumerate(self.stage2):
                    x_list[i] = branch(x_list[i])
                
                # Use highest resolution output
                heatmaps = self.final_layer(x_list[0])
                
                # Step ÌååÏùºÍ≥º Ìò∏ÌôòÎêòÎèÑÎ°ù tensor Î∞òÌôò
                return heatmaps
        
        return HRNetPoseModel(num_keypoints)
    
    def _create_openpose_model(self) -> nn.Module:
        """OpenPose Î™®Îç∏ ÏÉùÏÑ±"""
        class OpenPoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # VGG19-based backbone
                self.backbone = nn.Sequential(
                    # Block 1
                    nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    
                    # Block 2
                    nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    
                    # Block 3
                    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    
                    # Block 4
                    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
                )
                
                # CPM Stages (Convolutional Pose Machines)
                self.stage1_paf = self._make_stage(512, 38)  # PAF branches
                self.stage1_conf = self._make_stage(512, 19)  # Confidence maps
                
                # Refinement stages
                self.stage2_paf = self._make_stage(512 + 38 + 19, 38)
                self.stage2_conf = self._make_stage(512 + 38 + 19, 19)
                
                self.stage3_paf = self._make_stage(512 + 38 + 19, 38)
                self.stage3_conf = self._make_stage(512 + 38 + 19, 19)
                
            def _make_stage(self, inplanes, outplanes):
                return nn.Sequential(
                    nn.Conv2d(inplanes, 128, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 512, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(512, outplanes, 1)
                )
            
            def forward(self, x):
                # Backbone features
                features = self.backbone(x)
                
                # Stage 1
                paf1 = self.stage1_paf(features)
                conf1 = self.stage1_conf(features)
                
                # Stage 2
                concat1 = torch.cat([features, paf1, conf1], dim=1)
                paf2 = self.stage2_paf(concat1)
                conf2 = self.stage2_conf(concat1)
                
                # Stage 3
                concat2 = torch.cat([features, paf2, conf2], dim=1)
                paf3 = self.stage3_paf(concat2)
                conf3 = self.stage3_conf(concat2)
                
                return conf3
        
        return OpenPoseModel()
    
    def _create_yolo_pose_model(self) -> nn.Module:
        """YOLO Pose Î™®Îç∏ ÏÉùÏÑ±"""
        class YOLOPoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # YOLO backbone (simplified CSPDarknet)
                self.backbone = nn.Sequential(
                    # Stem
                    nn.Conv2d(3, 32, 6, stride=2, padding=2), nn.BatchNorm2d(32), nn.SiLU(inplace=True),
                    nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.SiLU(inplace=True),
                    
                    # CSP Blocks
                    self._make_csp_block(64, 64, 1),
                    nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.SiLU(inplace=True),
                    self._make_csp_block(128, 128, 2),
                    nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.SiLU(inplace=True),
                    self._make_csp_block(256, 256, 8),
                    nn.Conv2d(256, 512, 3, stride=2, padding=1), nn.BatchNorm2d(512), nn.SiLU(inplace=True),
                    self._make_csp_block(512, 512, 4),
                )
                
                # Detection head
                self.detection_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, padding=1), nn.BatchNorm2d(256), nn.SiLU(inplace=True),
                    nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.SiLU(inplace=True),
                    nn.Conv2d(256, 85 + 51, 1)  # 85 for detection + 51 for pose (17*3)
                )
                
            def _make_csp_block(self, inplanes, planes, num_blocks):
                return nn.Sequential(
                    nn.Conv2d(inplanes, planes // 2, 1), nn.BatchNorm2d(planes // 2), nn.SiLU(inplace=True),
                    *[nn.Sequential(
                        nn.Conv2d(planes // 2, planes // 2, 3, padding=1), nn.BatchNorm2d(planes // 2), nn.SiLU(inplace=True),
                        nn.Conv2d(planes // 2, planes // 2, 3, padding=1), nn.BatchNorm2d(planes // 2), nn.SiLU(inplace=True)
                    ) for _ in range(num_blocks)],
                    nn.Conv2d(planes // 2, planes, 1), nn.BatchNorm2d(planes), nn.SiLU(inplace=True)
                )
            
            def forward(self, x):
                features = self.backbone(x)
                output = self.detection_head(features)
                
                # Split detection and pose outputs
                detection = output[:, :85, :, :]
                pose = output[:, 85:, :, :].view(output.size(0), 17, 3, output.size(2), output.size(3))
                
                return {
                    'detection': detection,
                    'keypoints': pose,
                    'heatmaps': pose[:, :, 2:3, :, :]  # Confidence channel as heatmap
                }
        
        return YOLOPoseModel()
    
    def _create_generic_pose_model(self, num_keypoints: int) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Pose Î™®Îç∏ ÏÉùÏÑ±"""
        class GenericPoseModel(nn.Module):
            def __init__(self, num_keypoints=17):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                )
                
                self.keypoint_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
                    nn.Conv2d(128, num_keypoints, 1)
                )
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.keypoint_head(features)
                return {'heatmaps': heatmaps, 'keypoints': heatmaps}
        
        return GenericPoseModel(num_keypoints)
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Pose Estimation Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # HRNet specific mappings
            'backbone.stage1.': 'layer1.',
            'backbone.stage2.': 'stage2.',
            'backbone.stage3.': 'stage3.',
            'backbone.stage4.': 'stage4.',
            'backbone.conv1.': 'conv1.',
            'backbone.conv2.': 'conv2.',
            'backbone.bn1.': 'bn1.',
            'backbone.bn2.': 'bn2.',
            'keypoint_head.final_layer.': 'final_layer.',
            
            # OpenPose specific mappings
            'features.': 'backbone.',
            'stage1_paf.': 'stage1_paf.',
            'stage1_conf.': 'stage1_conf.',
            'stage2_paf.': 'stage2_paf.',
            'stage2_conf.': 'stage2_conf.',
            'stage3_paf.': 'stage3_paf.',
            'stage3_conf.': 'stage3_conf.',
            
            # YOLO specific mappings
            'backbone.': 'backbone.',
            'head.': 'detection_head.',
            'neck.': 'neck.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Pose Estimation ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Pose Estimation Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            test_input = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_input)
            
            if isinstance(output, dict):
                if 'heatmaps' in output or 'keypoints' in output:
                    self.logger.info(f"‚úÖ Pose Estimation Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ")
                    return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Pose Estimation Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

class ClothSegmentationArchitecture(StepSpecificArchitecture):
    """Cloth Segmentation ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Cloth Segmentation Î™®Îç∏ ÏÉùÏÑ±"""
        architecture_type = checkpoint_analysis.get('architecture_type', 'sam')
        
        if architecture_type == 'sam':
            return self._create_sam_model()
        elif architecture_type == 'u2net':
            return self._create_u2net_segmentation_model()
        elif architecture_type == 'deeplabv3':
            return self._create_deeplabv3_model()
        else:
            return self._create_generic_segmentation_model()
    
    def _create_sam_model(self) -> nn.Module:
        """SAM (Segment Anything Model) ÏÉùÏÑ±"""
        class SAMModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Vision Transformer encoder
                self.patch_embed = nn.Conv2d(3, 768, kernel_size=16, stride=16)
                self.pos_embed = nn.Parameter(torch.zeros(1, 1024, 768))
                
                # Transformer blocks
                self.transformer_blocks = nn.ModuleList([
                    self._make_transformer_block(768, 12, 3072) for _ in range(12)
                ])
                
                # Prompt encoder
                self.prompt_embed = nn.Embedding(1000, 256)
                
                # Mask decoder
                self.mask_decoder = nn.Sequential(
                    nn.ConvTranspose2d(768, 256, 4, stride=2, padding=1),
                    nn.BatchNorm2d(256), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.BatchNorm2d(128), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
                    nn.BatchNorm2d(64), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
                    nn.BatchNorm2d(32), nn.ReLU(inplace=True),
                    nn.Conv2d(32, 1, 1)
                )
                
            def _make_transformer_block(self, d_model, nhead, dim_feedforward):
                return nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=nhead,
                    dim_feedforward=dim_feedforward,
                    batch_first=True
                )
            
            def forward(self, x, prompts=None):
                B, C, H, W = x.shape
                
                # Patch embedding
                x = self.patch_embed(x)  # [B, 768, H/16, W/16]
                x = x.flatten(2).transpose(1, 2)  # [B, N, 768]
                
                # Add positional embedding
                x = x + self.pos_embed[:, :x.size(1), :]
                
                # Transformer encoding
                for block in self.transformer_blocks:
                    x = block(x)
                
                # Reshape for mask decoder
                patch_h, patch_w = H // 16, W // 16
                x = x.transpose(1, 2).view(B, 768, patch_h, patch_w)
                
                # Mask decoding
                mask = self.mask_decoder(x)
                
                return {
                    'masks': mask,
                    'segmentation': mask
                }
        
        return SAMModel()
    
    def _create_u2net_segmentation_model(self) -> nn.Module:
        """U2Net Segmentation Î™®Îç∏ ÏÉùÏÑ±"""
        class U2NetSegmentationModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # U2-Net structure
                # Encoder
                self.stage1 = self._make_u2_block(3, 32, 7)
                self.stage2 = self._make_u2_block(32, 32, 6)
                self.stage3 = self._make_u2_block(32, 64, 5)
                self.stage4 = self._make_u2_block(64, 128, 4)
                self.stage5 = self._make_u2_block(128, 256, 4)
                self.stage6 = self._make_u2_block(256, 512, 4)
                
                # Bridge
                self.bridge = self._make_u2_block(512, 512, 4)
                
                # Decoder
                self.stage5d = self._make_u2_block(1024, 256, 4)
                self.stage4d = self._make_u2_block(512, 128, 4)
                self.stage3d = self._make_u2_block(256, 64, 5)
                self.stage2d = self._make_u2_block(128, 32, 6)
                self.stage1d = self._make_u2_block(64, 16, 7)
                
                # Side outputs
                self.side1 = nn.Conv2d(16, 1, 3, padding=1)
                self.side2 = nn.Conv2d(32, 1, 3, padding=1)
                self.side3 = nn.Conv2d(64, 1, 3, padding=1)
                self.side4 = nn.Conv2d(128, 1, 3, padding=1)
                self.side5 = nn.Conv2d(256, 1, 3, padding=1)
                self.side6 = nn.Conv2d(512, 1, 3, padding=1)
                
                # Output fusion
                self.outconv = nn.Conv2d(6, 1, 1)
                
            def _make_u2_block(self, inplanes, planes, depth):
                layers = []
                layers.append(nn.Conv2d(inplanes, planes, 3, padding=1))
                layers.append(nn.BatchNorm2d(planes))
                layers.append(nn.ReLU(inplace=True))
                
                # U2-Net specific nested U-structure
                for i in range(depth - 1):
                    if i < depth // 2:
                        layers.append(nn.Conv2d(planes, planes, 3, padding=1))
                        layers.append(nn.BatchNorm2d(planes))
                        layers.append(nn.ReLU(inplace=True))
                        if i % 2 == 0:
                            layers.append(nn.MaxPool2d(2, 2))
                    else:
                        if i % 2 == 1:
                            layers.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))
                        layers.append(nn.Conv2d(planes, planes, 3, padding=1))
                        layers.append(nn.BatchNorm2d(planes))
                        layers.append(nn.ReLU(inplace=True))
                
                return nn.Sequential(*layers)
            
            def forward(self, x):
                # Encoder
                e1 = self.stage1(x)
                e2 = self.stage2(F.max_pool2d(e1, 2))
                e3 = self.stage3(F.max_pool2d(e2, 2))
                e4 = self.stage4(F.max_pool2d(e3, 2))
                e5 = self.stage5(F.max_pool2d(e4, 2))
                e6 = self.stage6(F.max_pool2d(e5, 2))
                
                # Bridge
                bridge = self.bridge(F.max_pool2d(e6, 2))
                
                # Decoder
                d5 = self.stage5d(torch.cat([F.interpolate(bridge, scale_factor=2, mode='bilinear'), e6], dim=1))
                d4 = self.stage4d(torch.cat([F.interpolate(d5, scale_factor=2, mode='bilinear'), e5], dim=1))
                d3 = self.stage3d(torch.cat([F.interpolate(d4, scale_factor=2, mode='bilinear'), e4], dim=1))
                d2 = self.stage2d(torch.cat([F.interpolate(d3, scale_factor=2, mode='bilinear'), e3], dim=1))
                d1 = self.stage1d(torch.cat([F.interpolate(d2, scale_factor=2, mode='bilinear'), e2], dim=1))
                
                # Side outputs
                s1 = self.side1(F.interpolate(d1, scale_factor=2, mode='bilinear'))
                s2 = self.side2(F.interpolate(d2, scale_factor=4, mode='bilinear'))
                s3 = self.side3(F.interpolate(d3, scale_factor=8, mode='bilinear'))
                s4 = self.side4(F.interpolate(d4, scale_factor=16, mode='bilinear'))
                s5 = self.side5(F.interpolate(d5, scale_factor=32, mode='bilinear'))
                s6 = self.side6(F.interpolate(bridge, scale_factor=64, mode='bilinear'))
                
                # Fusion
                fused = self.outconv(torch.cat([s1, s2, s3, s4, s5, s6], dim=1))
                
                # Step ÌååÏùºÍ≥º Ìò∏ÌôòÎêòÎèÑÎ°ù tensor Î∞òÌôò
                return fused
        
        return U2NetSegmentationModel()
    
    def _create_deeplabv3_model(self) -> nn.Module:
        """DeepLabV3+ Î™®Îç∏ ÏÉùÏÑ±"""
        class DeepLabV3PlusModel(nn.Module):
            def __init__(self, num_classes=21):
                super().__init__()
                self.num_classes = num_classes
                
                # ResNet101 backbone
                self.backbone = self._make_resnet_backbone()
                
                # ASPP
                self.aspp = self._make_aspp(2048)
                
                # Decoder
                self.decoder = self._make_decoder(256, num_classes)
                
            def _make_resnet_backbone(self):
                layers = []
                
                # Stem
                layers.extend([
                    nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(3, stride=2, padding=1)
                ])
                
                # ResNet blocks
                layers.extend(self._make_resnet_layer(64, 64, 3))
                layers.extend(self._make_resnet_layer(64, 128, 4, stride=2))
                layers.extend(self._make_resnet_layer(128, 256, 23, stride=2))
                layers.extend(self._make_resnet_layer(256, 512, 3, stride=2))
                
                return nn.Sequential(*layers)
            
            def _make_resnet_layer(self, inplanes, planes, blocks, stride=1):
                layers = []
                for i in range(blocks):
                    s = stride if i == 0 else 1
                    layers.extend([
                        nn.Conv2d(inplanes, planes, 3, stride=s, padding=1, bias=False),
                        nn.BatchNorm2d(planes),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(planes, planes, 3, padding=1, bias=False),
                        nn.BatchNorm2d(planes),
                        nn.ReLU(inplace=True)
                    ])
                    inplanes = planes
                return layers
            
            def _make_aspp(self, inplanes):
                return nn.ModuleList([
                    nn.Sequential(
                        nn.Conv2d(inplanes, 256, 1, bias=False),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    ),
                    nn.Sequential(
                        nn.Conv2d(inplanes, 256, 3, padding=6, dilation=6, bias=False),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    ),
                    nn.Sequential(
                        nn.Conv2d(inplanes, 256, 3, padding=12, dilation=12, bias=False),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    ),
                    nn.Sequential(
                        nn.Conv2d(inplanes, 256, 3, padding=18, dilation=18, bias=False),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    ),
                    nn.Sequential(
                        nn.AdaptiveAvgPool2d(1),
                        nn.Conv2d(inplanes, 256, 1, bias=False),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    )
                ])
            
            def _make_decoder(self, inplanes, num_classes):
                return nn.Sequential(
                    nn.Conv2d(inplanes * 5, 256, 3, padding=1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Conv2d(256, 256, 3, padding=1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.1),
                    nn.Conv2d(256, num_classes, 1)
                )
            
            def forward(self, x):
                # Backbone
                features = self.backbone(x)
                
                # ASPP
                aspp_results = []
                for aspp_layer in self.aspp:
                    if isinstance(aspp_layer[0], nn.AdaptiveAvgPool2d):
                        # Global average pooling branch
                        result = aspp_layer(features)
                        result = F.interpolate(result, size=features.shape[2:], mode='bilinear', align_corners=True)
                    else:
                        result = aspp_layer(features)
                    aspp_results.append(result)
                
                # Concatenate ASPP results
                aspp_concat = torch.cat(aspp_results, dim=1)
                
                # Decoder
                output = self.decoder(aspp_concat)
                output = F.interpolate(output, scale_factor=8, mode='bilinear', align_corners=True)
                
                return {'segmentation': output}
        
        return DeepLabV3PlusModel()
    
    def _create_generic_segmentation_model(self) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Segmentation Î™®Îç∏ ÏÉùÏÑ±"""
        class GenericSegmentationModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # U-Net like structure
                # Encoder
                self.enc1 = self._make_conv_block(3, 64)
                self.enc2 = self._make_conv_block(64, 128)
                self.enc3 = self._make_conv_block(128, 256)
                self.enc4 = self._make_conv_block(256, 512)
                
                # Bridge
                self.bridge = self._make_conv_block(512, 1024)
                
                # Decoder
                self.dec4 = self._make_conv_block(1024 + 512, 512)
                self.dec3 = self._make_conv_block(512 + 256, 256)
                self.dec2 = self._make_conv_block(256 + 128, 128)
                self.dec1 = self._make_conv_block(128 + 64, 64)
                
                # Output
                self.final = nn.Conv2d(64, 1, 1)
                
            def _make_conv_block(self, inplanes, planes):
                return nn.Sequential(
                    nn.Conv2d(inplanes, planes, 3, padding=1),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(planes, planes, 3, padding=1),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True)
                )
            
            def forward(self, x):
                # Encoder
                e1 = self.enc1(x)
                e2 = self.enc2(F.max_pool2d(e1, 2))
                e3 = self.enc3(F.max_pool2d(e2, 2))
                e4 = self.enc4(F.max_pool2d(e3, 2))
                
                # Bridge
                bridge = self.bridge(F.max_pool2d(e4, 2))
                
                # Decoder
                d4 = self.dec4(torch.cat([F.interpolate(bridge, scale_factor=2, mode='bilinear'), e4], dim=1))
                d3 = self.dec3(torch.cat([F.interpolate(d4, scale_factor=2, mode='bilinear'), e3], dim=1))
                d2 = self.dec2(torch.cat([F.interpolate(d3, scale_factor=2, mode='bilinear'), e2], dim=1))
                d1 = self.dec1(torch.cat([F.interpolate(d2, scale_factor=2, mode='bilinear'), e1], dim=1))
                
                # Output
                output = self.final(d1)
                
                return {'segmentation': output}
        
        return GenericSegmentationModel()
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Cloth Segmentation Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # SAM specific mappings
            'image_encoder.': '',
            'prompt_encoder.': 'prompt_embed.',
            'mask_decoder.': 'mask_decoder.',
            'patch_embed.proj.': 'patch_embed.',
            'blocks.': 'transformer_blocks.',
            
            # U2Net specific mappings
            'stage1.': 'stage1.',
            'stage2.': 'stage2.',
            'stage3.': 'stage3.',
            'stage4.': 'stage4.',
            'stage5.': 'stage5.',
            'stage6.': 'stage6.',
            'stage1d.': 'stage1d.',
            'stage2d.': 'stage2d.',
            'stage3d.': 'stage3d.',
            'stage4d.': 'stage4d.',
            'stage5d.': 'stage5d.',
            'outconv.': 'outconv.',
            'side1.': 'side1.',
            'side2.': 'side2.',
            'side3.': 'side3.',
            'side4.': 'side4.',
            'side5.': 'side5.',
            'side6.': 'side6.',
            
            # DeepLabV3+ specific mappings
            'backbone.': 'backbone.',
            'aspp.': 'aspp.',
            'decoder.': 'decoder.',
            'classifier.': 'decoder.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Cloth Segmentation ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Cloth Segmentation Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            test_input = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_input)
            
            if isinstance(output, dict):
                if 'segmentation' in output or 'masks' in output:
                    self.logger.info(f"‚úÖ Cloth Segmentation Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ")
                    return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Cloth Segmentation Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False

class GeometricMatchingArchitecture(StepSpecificArchitecture):
    """Geometric Matching ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Geometric Matching Î™®Îç∏ ÏÉùÏÑ±"""
        architecture_type = checkpoint_analysis.get('architecture_type', 'gmm')
        num_control_points = checkpoint_analysis.get('num_control_points', 20)
        
        if architecture_type == 'gmm':
            return self._create_gmm_model(num_control_points)
        elif architecture_type == 'tps':
            return self._create_tps_model(num_control_points)
        elif architecture_type == 'raft':
            return self._create_raft_model()
        else:
            return self._create_generic_geometric_model(num_control_points)
    
    def _create_gmm_model(self, num_control_points: int) -> nn.Module:
        """GMM (Geometric Matching Module) Î™®Îç∏ ÏÉùÏÑ±"""
        class GMMModel(nn.Module):
            def __init__(self, num_control_points=20):
                super().__init__()
                self.num_control_points = num_control_points
                
                # Feature extractor (ResNet-like)
                self.feature_extractor = nn.Sequential(
                    # Initial conv
                    nn.Conv2d(6, 64, 7, stride=2, padding=3, bias=False),  # Person + Cloth = 6 channels
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(3, stride=2, padding=1),
                    
                    # ResNet blocks
                    self._make_resnet_block(64, 64, 3),
                    self._make_resnet_block(64, 128, 4, stride=2),
                    self._make_resnet_block(128, 256, 6, stride=2),
                    self._make_resnet_block(256, 512, 3, stride=2),
                )
                
                # Global average pooling
                self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
                
                # Regression head for control points
                self.regression_head = nn.Sequential(
                    nn.Linear(512, 512),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(512, 256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(256, num_control_points * 2)  # x, y coordinates
                )
                
                # Grid generator
                self.grid_generator = self._create_grid_generator()
                
            def _make_resnet_block(self, inplanes, planes, blocks, stride=1):
                layers = []
                # First block with potential stride
                layers.extend([
                    nn.Conv2d(inplanes, planes, 3, stride=stride, padding=1, bias=False),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(planes, planes, 3, padding=1, bias=False),
                    nn.BatchNorm2d(planes),
                    nn.ReLU(inplace=True)
                ])
                
                # Remaining blocks
                for _ in range(1, blocks):
                    layers.extend([
                        nn.Conv2d(planes, planes, 3, padding=1, bias=False),
                        nn.BatchNorm2d(planes),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(planes, planes, 3, padding=1, bias=False),
                        nn.BatchNorm2d(planes),
                        nn.ReLU(inplace=True)
                    ])
                
                return nn.Sequential(*layers)
            
            def _create_grid_generator(self):
                return nn.Sequential(
                    nn.Conv2d(512, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 2, 3, padding=1)  # Flow field
                )
            
            def forward(self, person_image, cloth_image):
                batch_size = person_image.size(0)
                
                # Concatenate person and cloth images
                combined_input = torch.cat([person_image, cloth_image], dim=1)
                
                # Extract features
                features = self.feature_extractor(combined_input)
                
                # Global features for control points
                global_features = self.global_pool(features).view(batch_size, -1)
                control_points = self.regression_head(global_features)
                control_points = control_points.view(batch_size, self.num_control_points, 2)
                
                # Generate transformation grid
                transformation_grid = self.grid_generator(features)
                
                # Apply transformation to cloth image
                warped_cloth = F.grid_sample(
                    cloth_image, 
                    transformation_grid.permute(0, 2, 3, 1), 
                    align_corners=True
                )
                
                return {
                    'control_points': control_points,
                    'transformation_grid': transformation_grid,
                    'warped_cloth': warped_cloth,
                    'theta': control_points  # For compatibility
                }
        
        return GMMModel(num_control_points)
    
    def _create_tps_model(self, num_control_points: int) -> nn.Module:
        """TPS (Thin Plate Spline) Î™®Îç∏ ÏÉùÏÑ±"""
        class TPSModel(nn.Module):
            def __init__(self, num_control_points=20):
                super().__init__()
                self.num_control_points = num_control_points
                
                # Feature network
                self.features = nn.Sequential(
                    nn.Conv2d(6, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 512, 3, padding=1),
                    nn.BatchNorm2d(512),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    
                    nn.AdaptiveAvgPool2d((4, 4)),
                    nn.Flatten(),
                    nn.Linear(512 * 16, 1024),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(1024, num_control_points * 2)
                )
                
            def forward(self, person_image, cloth_image):
                batch_size = person_image.size(0)
                
                # Combine inputs
                combined = torch.cat([person_image, cloth_image], dim=1)
                
                # Predict control points
                control_points = self.features(combined)
                control_points = control_points.view(batch_size, self.num_control_points, 2)
                
                # Normalize to [-1, 1]
                control_points = torch.tanh(control_points)
                
                return {
                    'control_points': control_points,
                    'theta': control_points
                }
        
        return TPSModel(num_control_points)
    
    def _create_raft_model(self) -> nn.Module:
        """RAFT (Recurrent All-Pairs Field Transforms) Î™®Îç∏ ÏÉùÏÑ±"""
        class RAFTModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Feature encoder
                self.fnet = nn.Sequential(
                    nn.Conv2d(3, 64, 7, stride=2, padding=3),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, stride=2, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, stride=2, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True)
                )
                
                # Context encoder
                self.cnet = nn.Sequential(
                    nn.Conv2d(3, 64, 7, stride=2, padding=3),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, stride=2, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 128, 3, stride=2, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True)
                )
                
                # Update block
                self.update_block = nn.Sequential(
                    nn.Conv2d(128 + 256 + 2, 128, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 128, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 2, 3, padding=1)
                )
                
            def forward(self, person_image, cloth_image):
                # Extract features
                fmap1 = self.fnet(person_image)
                fmap2 = self.fnet(cloth_image)
                
                # Context features
                cnet_features = self.cnet(person_image)
                
                # Initialize flow
                batch_size, _, h, w = fmap1.shape
                flow = torch.zeros(batch_size, 2, h, w, device=person_image.device)
                
                # Iterative refinement (simplified)
                for _ in range(3):  # Reduced iterations for efficiency
                    # Correlation (simplified)
                    correlation = torch.sum(fmap1 * fmap2, dim=1, keepdim=True)
                    correlation = correlation.expand(-1, 2, -1, -1)
                    
                    # Update flow
                    flow_input = torch.cat([cnet_features, correlation, flow], dim=1)
                    flow_delta = self.update_block(flow_input)
                    flow = flow + flow_delta
                
                return {
                    'flow': flow,
                    'transformation_grid': flow,
                    'optical_flow': flow
                }
        
        return RAFTModel()
    
    def _create_generic_geometric_model(self, num_control_points: int) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Geometric Matching Î™®Îç∏ ÏÉùÏÑ±"""
        class GenericGeometricModel(nn.Module):
            def __init__(self, num_control_points=20):
                super().__init__()
                self.num_control_points = num_control_points
                
                self.features = nn.Sequential(
                    nn.Conv2d(6, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    nn.AdaptiveAvgPool2d((8, 8)),
                    nn.Flatten(),
                    nn.Linear(256 * 64, 512),
                    nn.ReLU(inplace=True),
                    nn.Linear(512, num_control_points * 2)
                )
            
            def forward(self, person_image, cloth_image):
                batch_size = person_image.size(0)
                combined = torch.cat([person_image, cloth_image], dim=1)
                
                output = self.features(combined)
                control_points = output.view(batch_size, self.num_control_points, 2)
                
                return {
                    'control_points': control_points,
                    'theta': control_points
                }
        
        return GenericGeometricModel(num_control_points)
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Geometric Matching Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # GMM specific mappings
            'extractionA.': 'feature_extractor.',
            'extractionB.': 'feature_extractor.',
            'regression.': 'regression_head.',
            'gridGen.': 'grid_generator.',
            'localization.': 'regression_head.',
            
            # TPS specific mappings
            'features.': 'features.',
            'loc_net.': 'features.',
            
            # RAFT specific mappings
            'fnet.': 'fnet.',
            'cnet.': 'cnet.',
            'update_block.': 'update_block.',
            'corr_fn.': '',  # Correlation function - not needed in our simplified version
            'update.': 'update_block.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Geometric Matching ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Geometric Matching Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            test_person = torch.randn(1, 3, 256, 256).to(self.device)
            test_cloth = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_person, test_cloth)
            
            if isinstance(output, dict):
                if 'control_points' in output or 'theta' in output or 'flow' in output:
                    self.logger.info(f"‚úÖ Geometric Matching Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ")
                    return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Geometric Matching Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False


class VirtualFittingArchitecture(StepSpecificArchitecture):
    """Virtual Fitting ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Virtual Fitting Î™®Îç∏ ÏÉùÏÑ±"""
        architecture_type = checkpoint_analysis.get('architecture_type', 'ootd')
        
        if architecture_type == 'ootd':
            return self._create_ootd_model()
        elif architecture_type == 'diffusion':
            return self._create_diffusion_model()
        else:
            return self._create_generic_virtual_fitting_model()
    
    def _create_ootd_model(self) -> nn.Module:
        """OOTD (Outfit of the Day) Î™®Îç∏ ÏÉùÏÑ±"""
        class OOTDModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # UNet-based Diffusion Model
                self.time_embedding = nn.Sequential(
                    nn.Linear(320, 1280),
                    nn.SiLU(),
                    nn.Linear(1280, 1280)
                )
                
                # Encoder (Simplified)
                self.encoder = nn.Sequential(
                    nn.Conv2d(4, 320, 3, padding=1),  # Latent space input
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 320, 3, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                )
                
                # Middle Block
                self.middle_block = nn.Sequential(
                    nn.Conv2d(320, 1280, 3, padding=1),
                    nn.GroupNorm(32, 1280),
                    nn.SiLU(),
                    nn.Conv2d(1280, 1280, 3, padding=1),
                    nn.GroupNorm(32, 1280),
                    nn.SiLU(),
                )
                
                # Decoder (Simplified)
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(1280, 320, 4, stride=2, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 4, 3, padding=1)  # Latent space output
                )
                
                # ControlNet branch
                self.controlnet = nn.Sequential(
                    nn.Conv2d(3, 320, 3, padding=1),  # Person image
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 320, 3, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None, timestep=None):
                batch_size = person_image.size(0)
                
                # Time embedding
                if timestep is None:
                    timestep = torch.randint(0, 1000, (batch_size,), device=person_image.device)
                
                time_emb = self.time_embedding(self._get_timestep_embedding(timestep))
                
                # ControlNet features
                control_features = self.controlnet(person_image)
                
                # Encoder
                x = self.encoder(torch.randn(batch_size, 4, 64, 64, device=person_image.device))
                
                # Add control features
                x = x + control_features
                
                # Middle block
                x = self.middle_block(x)
                
                # Decoder
                output = self.decoder(x)
                
                return {
                    'fitted_image': torch.tanh(output),
                    'latent_output': output
                }
            
            def _get_timestep_embedding(self, timesteps, embedding_dim=320):
                """ÏãúÍ∞Ñ ÏûÑÎ≤†Îî© ÏÉùÏÑ±"""
                half_dim = embedding_dim // 2
                emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
                emb = torch.exp(torch.arange(half_dim) * -emb).to(timesteps.device)
                emb = timesteps[:, None] * emb[None, :]
                emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
                return emb
        
        return OOTDModel()
    
    def _create_diffusion_model(self) -> nn.Module:
        """Diffusion Î™®Îç∏ ÏÉùÏÑ±"""
        class DiffusionVirtualFittingModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # VAE Encoder
                self.vae_encoder = nn.Sequential(
                    nn.Conv2d(3, 128, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 512, 3, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(512, 8, 3, padding=1)  # To latent space
                )
                
                # VAE Decoder
                self.vae_decoder = nn.Sequential(
                    nn.Conv2d(4, 512, 3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 3, 3, padding=1),
                    nn.Tanh()
                )
                
                # UNet for denoising
                self.unet = self._create_unet()
            
            def _create_unet(self):
                """UNet ÏÉùÏÑ±"""
                return nn.Sequential(
                    nn.Conv2d(4, 320, 3, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 640, 3, stride=2, padding=1),
                    nn.GroupNorm(32, 640),
                    nn.SiLU(),
                    nn.ConvTranspose2d(640, 320, 4, stride=2, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 4, 3, padding=1)
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None):
                # Encode to latent space
                person_latent = self.vae_encoder(person_image)
                cloth_latent = self.vae_encoder(cloth_image)
                
                # Sample noise
                noise = torch.randn_like(person_latent[:, :4])
                
                # Denoising
                denoised = self.unet(noise)
                
                # Decode
                fitted_image = self.vae_decoder(denoised)
                
                return {
                    'fitted_image': fitted_image,
                    'person_latent': person_latent,
                    'cloth_latent': cloth_latent
                }
        
        return DiffusionVirtualFittingModel()
    
    def _create_generic_virtual_fitting_model(self) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Virtual Fitting Î™®Îç∏"""
        class GenericVirtualFittingModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Feature extractor
                self.feature_extractor = nn.Sequential(
                    nn.Conv2d(6, 64, 3, padding=1),  # Person + Cloth
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                )
                
                # Fitting generator
                self.generator = nn.Sequential(
                    nn.Conv2d(256, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 3, 3, padding=1),
                    nn.Tanh()
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None):
                combined = torch.cat([person_image, cloth_image], dim=1)
                features = self.feature_extractor(combined)
                fitted_image = self.generator(features)
                
                return {'fitted_image': fitted_image}
        
        return GenericVirtualFittingModel()
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Virtual Fitting Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # OOTD specific mappings
            'unet.': '',
            'vae.encoder.': 'vae_encoder.',
            'vae.decoder.': 'vae_decoder.',
            'time_embed.': 'time_embedding.',
            'controlnet.': 'controlnet.',
            
            # Diffusion specific mappings
            'denoise_fn.': 'unet.',
            'encoder.': 'vae_encoder.',
            'decoder.': 'vae_decoder.',
            
            # Generic mappings
            'feature_net.': 'feature_extractor.',
            'gen.': 'generator.',
            'generator.': 'generator.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Virtual Fitting ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Virtual Fitting Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            test_person = torch.randn(1, 3, 256, 256).to(self.device)
            test_cloth = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_person, test_cloth)
            
            if isinstance(output, dict) and 'fitted_image' in output:
                self.logger.info(f"‚úÖ Virtual Fitting Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ")
                return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Virtual Fitting Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False


class ClothWarpingArchitecture(StepSpecificArchitecture):
    """Cloth Warping ÌäπÌôî Ïã†Í≤ΩÎßù ÏïÑÌÇ§ÌÖçÏ≤ò"""
    
    def create_model(self, checkpoint_analysis: Dict[str, Any]) -> nn.Module:
        """Cloth Warping Î™®Îç∏ ÏÉùÏÑ±"""
        architecture_type = checkpoint_analysis.get('architecture_type', 'realvis')
        
        if architecture_type == 'realvis':
            return self._create_realvis_model()
        elif architecture_type == 'stable_diffusion':
            return self._create_stable_diffusion_model()
        else:
            return self._create_generic_warping_model()
    
    def _create_realvis_model(self) -> nn.Module:
        """RealVisXL Î™®Îç∏ ÏÉùÏÑ±"""
        class RealVisXLModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Text encoder
                self.text_encoder = nn.Sequential(
                    nn.Embedding(49408, 768),  # CLIP tokenizer vocab size
                    nn.TransformerEncoder(
                        nn.TransformerEncoderLayer(d_model=768, nhead=12), 
                        num_layers=12
                    ),
                    nn.LayerNorm(768)
                )
                
                # UNet backbone
                self.unet = self._create_unet_xl()
                
                # VAE for latent space
                self.vae_encoder = self._create_vae_encoder()
                self.vae_decoder = self._create_vae_decoder()
                
            def _create_unet_xl(self):
                """UNet XL ÏïÑÌÇ§ÌÖçÏ≤ò"""
                return nn.Sequential(
                    # Down blocks
                    nn.Conv2d(4, 320, 3, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 640, 3, stride=2, padding=1),
                    nn.GroupNorm(32, 640),
                    nn.SiLU(),
                    nn.Conv2d(640, 1280, 3, stride=2, padding=1),
                    nn.GroupNorm(32, 1280),
                    nn.SiLU(),
                    
                    # Middle block with attention
                    nn.Conv2d(1280, 1280, 3, padding=1),
                    nn.GroupNorm(32, 1280),
                    nn.SiLU(),
                    
                    # Up blocks
                    nn.ConvTranspose2d(1280, 640, 4, stride=2, padding=1),
                    nn.GroupNorm(32, 640),
                    nn.SiLU(),
                    nn.ConvTranspose2d(640, 320, 4, stride=2, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 4, 3, padding=1)
                )
            
            def _create_vae_encoder(self):
                """VAE Ïù∏ÏΩîÎçî"""
                return nn.Sequential(
                    nn.Conv2d(3, 128, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 256, 3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(256, 512, 3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(512, 8, 3, padding=1)
                )
            
            def _create_vae_decoder(self):
                """VAE ÎîîÏΩîÎçî"""
                return nn.Sequential(
                    nn.Conv2d(4, 512, 3, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 3, 3, padding=1),
                    nn.Tanh()
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None):
                batch_size = person_image.size(0)
                
                # Encode inputs to latent space
                person_latent = self.vae_encoder(person_image)
                cloth_latent = self.vae_encoder(cloth_image)
                
                # Sample from latent distribution
                person_latent = person_latent[:, :4]  # Take first 4 channels
                
                # Add noise for diffusion
                noise = torch.randn_like(person_latent)
                noisy_latent = person_latent + 0.1 * noise
                
                # Denoising with UNet
                denoised_latent = self.unet(noisy_latent)
                
                # Decode to image
                warped_cloth = self.vae_decoder(denoised_latent)
                
                return {
                    'warped_cloth': warped_cloth,
                    'latent_representation': denoised_latent
                }
        
        return RealVisXLModel()
    
    def _create_stable_diffusion_model(self) -> nn.Module:
        """Stable Diffusion Î™®Îç∏ ÏÉùÏÑ±"""
        class StableDiffusionWarpingModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Simplified Stable Diffusion components
                self.unet = nn.Sequential(
                    nn.Conv2d(4, 320, 3, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 640, 3, stride=2, padding=1),
                    nn.GroupNorm(32, 640),
                    nn.SiLU(),
                    nn.ConvTranspose2d(640, 320, 4, stride=2, padding=1),
                    nn.GroupNorm(32, 320),
                    nn.SiLU(),
                    nn.Conv2d(320, 4, 3, padding=1)
                )
                
                self.vae_decoder = nn.Sequential(
                    nn.Conv2d(4, 512, 3, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 3, 3, padding=1),
                    nn.Tanh()
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None):
                batch_size = person_image.size(0)
                
                # Create latent noise
                latent = torch.randn(batch_size, 4, 64, 64, device=person_image.device)
                
                # Denoising
                denoised = self.unet(latent)
                
                # Decode
                warped_cloth = self.vae_decoder(denoised)
                
                return {'warped_cloth': warped_cloth}
        
        return StableDiffusionWarpingModel()
    
    def _create_generic_warping_model(self) -> nn.Module:
        """ÏùºÎ∞òÏ†ÅÏù∏ Warping Î™®Îç∏"""
        class GenericWarpingModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                self.warping_net = nn.Sequential(
                    nn.Conv2d(6, 64, 3, padding=1),  # Person + Cloth
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, padding=1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, padding=1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(256, 3, 3, padding=1),
                    nn.Tanh()
                )
            
            def forward(self, person_image, cloth_image, text_prompt=None):
                combined = torch.cat([person_image, cloth_image], dim=1)
                warped_cloth = self.warping_net(combined)
                
                return {'warped_cloth': warped_cloth}
        
        return GenericWarpingModel()
    
    def map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """Cloth Warping Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë"""
        if not isinstance(checkpoint, dict):
            return checkpoint
        
        # State dict Ï∂îÏ∂ú
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        mapped_state_dict = {}
        
        # ÌÇ§ Îß§Ìïë Í∑úÏπô
        key_mappings = {
            # Module prefix Ï†úÍ±∞
            'module.': '',
            'model.': '',
            'net.': '',
            
            # RealVisXL specific mappings
            'unet.': 'unet.',
            'vae.encoder.': 'vae_encoder.',
            'vae.decoder.': 'vae_decoder.',
            'text_encoder.': 'text_encoder.',
            
            # Stable Diffusion mappings
            'model.diffusion_model.': 'unet.',
            'first_stage_model.encoder.': 'vae_encoder.',
            'first_stage_model.decoder.': 'vae_decoder.',
            
            # Generic mappings
            'warp_net.': 'warping_net.',
            'warping.': 'warping_net.',
        }
        
        for key, value in state_dict.items():
            new_key = key
            
            # Apply key mappings
            for old_prefix, new_prefix in key_mappings.items():
                if new_key.startswith(old_prefix):
                    new_key = new_key.replace(old_prefix, new_prefix, 1)
                    break
            
            mapped_state_dict[new_key] = value
        
        self.logger.info(f"‚úÖ Cloth Warping ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(state_dict)} ‚Üí {len(mapped_state_dict)}")
        return mapped_state_dict
    
    def validate_model(self, model: nn.Module) -> bool:
        """Cloth Warping Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            test_person = torch.randn(1, 3, 256, 256).to(self.device)
            test_cloth = torch.randn(1, 3, 256, 256).to(self.device)
            model.eval()
            
            with torch.no_grad():
                output = model(test_person, test_cloth)
            
            if isinstance(output, dict) and 'warped_cloth' in output:
                self.logger.info(f"‚úÖ Cloth Warping Î™®Îç∏ Í≤ÄÏ¶ù ÏÑ±Í≥µ")
                return True
            
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Cloth Warping Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False


# ==============================================
# üî• 2. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Î∞è Îß§Ìïë ÏãúÏä§ÌÖú
# ==============================================
class CheckpointAnalyzer:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Íµ¨Ï°∞ ÏûêÎèô Î∂ÑÏÑù"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    def analyze_checkpoint(self, checkpoint_path: Union[str, Path]) -> Dict[str, Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù - Î©îÏù∏ Î©îÏÑúÎìú"""
        try:
            checkpoint_path = Path(checkpoint_path)
            
            if not checkpoint_path.exists():
                return {'error': f'Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {checkpoint_path}'}
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
            checkpoint = self._load_checkpoint_safe(checkpoint_path)
            if checkpoint is None:
                return {'error': f'Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {checkpoint_path}'}
            
            # State dict Ï∂îÏ∂ú
            state_dict = self._extract_state_dict(checkpoint)
            if not state_dict:
                return {'error': 'Ïú†Ìö®Ìïú state_dictÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'}
            
            # enhanced_model_loaderÏóêÏÑú Î∂ÑÏÑù Î©îÏÑúÎìú import
            try:
                from .enhanced_model_loader import EnhancedModelLoader
                enhanced_analyzer = EnhancedModelLoader(device="cpu")
                
                # Ï¢ÖÌï© Î∂ÑÏÑù
                analysis = {
                    'file_path': str(checkpoint_path),
                    'file_size_mb': checkpoint_path.stat().st_size / (1024 * 1024),
                    'file_type': checkpoint_path.suffix,
                    'architecture_type': self._infer_architecture_type(state_dict, checkpoint_path),
                    'total_keys': len(state_dict),
                    'total_parameters': sum(tensor.numel() for tensor in state_dict.values() if hasattr(tensor, 'numel')),
                    'key_patterns': enhanced_analyzer._analyze_key_patterns(state_dict),
                    'layer_types': enhanced_analyzer._analyze_layer_types(state_dict),
                    'model_depth': enhanced_analyzer._estimate_model_depth(state_dict),
                    'parameter_counts': enhanced_analyzer._count_parameters_by_type(state_dict),
                    'has_batch_norm': enhanced_analyzer._has_batch_normalization(state_dict),
                    'has_attention': enhanced_analyzer._has_attention_layers(state_dict),
                    'metadata': enhanced_analyzer._extract_metadata(checkpoint),
                    'num_control_points': self._infer_num_control_points(state_dict),
                    'num_classes': self._infer_num_classes(state_dict),
                    'input_channels': self._infer_input_channels(state_dict),
                    'num_keypoints': self._infer_num_keypoints(state_dict),
                }
            except ImportError:
                # enhanced_model_loaderÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏ Î∂ÑÏÑù
                analysis = {
                    'file_path': str(checkpoint_path),
                    'file_size_mb': checkpoint_path.stat().st_size / (1024 * 1024),
                    'file_type': checkpoint_path.suffix,
                    'architecture_type': self._infer_architecture_type(state_dict, checkpoint_path),
                    'total_keys': len(state_dict),
                    'total_parameters': sum(tensor.numel() for tensor in state_dict.values() if hasattr(tensor, 'numel')),
                    'key_patterns': {},
                    'layer_types': {},
                    'model_depth': 0,
                    'parameter_counts': {},
                    'has_batch_norm': False,
                    'has_attention': False,
                    'metadata': {},
                    'num_control_points': self._infer_num_control_points(state_dict),
                    'num_classes': self._infer_num_classes(state_dict),
                    'input_channels': self._infer_input_channels(state_dict),
                    'num_keypoints': self._infer_num_keypoints(state_dict),
                }
            
            self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù ÏôÑÎ£å: {checkpoint_path.name}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'error': str(e)}
    
    def _load_checkpoint_safe(self, checkpoint_path: Path) -> Optional[Any]:
        """ÏïàÏ†ÑÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© - PyTorch 2.7 Ìò∏ÌôòÏÑ± Í∞ïÌôî"""
        try:
            if checkpoint_path.suffix == '.safetensors':
                try:
                    from safetensors.torch import load_file
                    return load_file(str(checkpoint_path))
                except ImportError:
                    self.logger.warning("safetensors ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏóÜÏùå, torch Î°úÎî© ÏãúÎèÑ")
                    return torch.load(str(checkpoint_path), map_location='cpu', weights_only=False)
            else:
                # üî• PyTorch 2.7 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î≤ÑÏ†Ñ Ïò§Î•ò Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Îã§ÏñëÌïú Î°úÎî© Î∞©Ïãù ÏãúÎèÑ
                loading_methods = [
                    # 1. Í∏∞Î≥∏ Î°úÎî© (weights_only=False)
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', weights_only=False),
                    
                    # 2. weights_only=True (PyTorch 2.7 Í∂åÏû•)
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', weights_only=True),
                    
                    # 3. pickle_module ÏÇ¨Ïö©
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', pickle_module=torch.serialization._get_safe_import_globals()),
                    
                    # 4. _use_new_zipfile_serialization=False
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', _use_new_zipfile_serialization=False),
                    
                    # 5. ÎßàÏßÄÎßâ ÏãúÎèÑ: Î™®Îì† ÏòµÏÖò Ï°∞Ìï©
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', weights_only=True, pickle_module=torch.serialization._get_safe_import_globals()),
                    
                    # 6. Ï∂îÍ∞Ä ÏãúÎèÑ: mmap ÏÇ¨Ïö©
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', weights_only=True, mmap=True),
                    
                    # 7. Ï∂îÍ∞Ä ÏãúÎèÑ: pickle_protocol=2
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', pickle_protocol=2),
                    
                    # 8. Ï∂îÍ∞Ä ÏãúÎèÑ: _use_new_zipfile_serialization=False + weights_only=True
                    lambda: torch.load(str(checkpoint_path), map_location='cpu', _use_new_zipfile_serialization=False, weights_only=True),
                    
                    # 9. Ï∂îÍ∞Ä ÏãúÎèÑ: Í∞ïÏ†ú Î°úÎî© (Ïò§Î•ò Î¨¥Ïãú)
                    lambda: self._force_load_checkpoint(checkpoint_path),
                    
                    # 10. Ï∂îÍ∞Ä ÏãúÎèÑ: Î∞îÏù¥ÎÑàÎ¶¨ Î™®ÎìúÎ°ú ÏßÅÏ†ë Î°úÎî©
                    lambda: self._load_checkpoint_binary(checkpoint_path)
                ]
                
                for i, loading_method in enumerate(loading_methods, 1):
                    try:
                        self.logger.info(f"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î∞©Ïãù {i} ÏãúÎèÑ Ï§ë...")
                        result = loading_method()
                        self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏÑ±Í≥µ (Î∞©Ïãù {i})")
                        return result
                    except Exception as e:
                        error_msg = str(e)
                        if "hasRecord" in error_msg and "version" in error_msg:
                            self.logger.warning(f"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î≤ÑÏ†Ñ Ïò§Î•ò (Î∞©Ïãù {i}): {error_msg}")
                        else:
                            self.logger.warning(f"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå® (Î∞©Ïãù {i}): {error_msg}")
                        continue
                
                # Î™®Îì† Î∞©Ïãù Ïã§Ìå®
                self.logger.error(f"‚ùå Î™®Îì† Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î∞©Ïãù Ïã§Ìå®: {checkpoint_path}")
                return None
                
        except Exception as e:
            self.logger.error(f"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _force_load_checkpoint(self, checkpoint_path: Path) -> Optional[Any]:
        """Í∞ïÏ†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© (Ïò§Î•ò Î¨¥Ïãú)"""
        try:
            import warnings
            warnings.filterwarnings("ignore")
            
            # ÏûÑÏãúÎ°ú torch.serialization Î™®Îìà ÏàòÏ†ï
            original_has_record = torch.serialization._has_record
            
            def patched_has_record(reader, name):
                try:
                    return original_has_record(reader, name)
                except:
                    return True  # Ìï≠ÏÉÅ True Î∞òÌôò
            
            torch.serialization._has_record = patched_has_record
            
            try:
                result = torch.load(str(checkpoint_path), map_location='cpu', weights_only=True)
                return result
            finally:
                # ÏõêÎûò Ìï®Ïàò Î≥µÏõê
                torch.serialization._has_record = original_has_record
                
        except Exception as e:
            self.logger.warning(f"Í∞ïÏ†ú Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_checkpoint_binary(self, checkpoint_path: Path) -> Optional[Any]:
        """Î∞îÏù¥ÎÑàÎ¶¨ Î™®ÎìúÎ°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            import pickle
            import gzip
            
            with open(checkpoint_path, 'rb') as f:
                # gzip ÏïïÏ∂ï ÌôïÏù∏
                magic = f.read(2)
                f.seek(0)
                
                if magic.startswith(b'\x1f\x8b'):
                    # gzip ÏïïÏ∂ï ÌååÏùº
                    with gzip.open(checkpoint_path, 'rb') as gz:
                        return pickle.load(gz)
                else:
                    # ÏùºÎ∞ò pickle ÌååÏùº
                    return pickle.load(f)
                    
        except Exception as e:
            self.logger.warning(f"Î∞îÏù¥ÎÑàÎ¶¨ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _extract_state_dict(self, checkpoint: Any) -> Optional[Dict[str, Any]]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú state_dict Ï∂îÏ∂ú"""
        if isinstance(checkpoint, dict):
            # Îã§ÏñëÌïú ÌÇ§ Ìå®ÌÑ¥ ÏãúÎèÑ
            for key in ['state_dict', 'model', 'model_state_dict', 'network', 'net']:
                if key in checkpoint:
                    return checkpoint[key]
            return checkpoint  # ÏßÅÏ†ë state_dict
        else:
            if hasattr(checkpoint, 'state_dict'):
                return checkpoint.state_dict()
            return checkpoint
    
    def _infer_architecture_type(self, state_dict: Dict[str, Any], checkpoint_path: Path) -> str:
        """ÏïÑÌÇ§ÌÖçÏ≤ò ÌÉÄÏûÖ Ï∂îÎ°†"""
        file_name = checkpoint_path.name.lower()
        
        # ÌååÏùºÎ™Ö Í∏∞Î∞ò Ï∂îÎ°†
        if 'graphonomy' in file_name or 'schp' in file_name:
            return 'graphonomy'
        elif 'hrnet' in file_name:
            return 'hrnet'
        elif 'openpose' in file_name:
            return 'openpose'
        elif 'sam' in file_name:
            return 'sam'
        elif 'u2net' in file_name:
            return 'u2net'
        elif 'gmm' in file_name:
            return 'gmm'
        elif 'tps' in file_name:
            return 'tps'
        elif 'raft' in file_name:
            return 'raft'
        elif 'realvis' in file_name or 'stable' in file_name:
            return 'stable_diffusion'
        elif 'ootd' in file_name:
            return 'ootd'
        
        # ÌÇ§ Ìå®ÌÑ¥ Í∏∞Î∞ò Ï∂îÎ°†
        key_patterns = {
            'graphonomy': ['aspp', 'edge_branch', 'decoder'],
            'hrnet': ['stage1', 'stage2', 'stage3', 'stage4', 'transition'],
            'openpose': ['features', 'stage1_paf', 'stage1_conf'],
            'sam': ['patch_embed', 'blocks', 'mask_decoder'],
            'u2net': ['stage1', 'stage1d', 'side1', 'outconv'],
            'gmm': ['extractionA', 'extractionB', 'regression'],
            'raft': ['fnet', 'cnet', 'update_block'],
            'ootd': ['time_embedding', 'down_blocks', 'up_blocks'],
            'stable_diffusion': ['unet', 'vae_encoder', 'vae_decoder']
        }
        
        for arch_type, patterns in key_patterns.items():
            if any(any(pattern in key for key in state_dict.keys()) for pattern in patterns):
                return arch_type
        
        return 'generic'
    
    def _infer_num_classes(self, state_dict: Dict[str, Any]) -> int:
        """ÌÅ¥ÎûòÏä§ Ïàò Ï∂îÎ°†"""
        for key, tensor in state_dict.items():
            if any(keyword in key.lower() for keyword in ['classifier', 'final', 'output', 'head']):
                if 'weight' in key and hasattr(tensor, 'shape') and len(tensor.shape) >= 2:
                    return tensor.shape[0]
        return 20  # Í∏∞Î≥∏Í∞í
    
    def _infer_input_channels(self, state_dict: Dict[str, Any]) -> int:
        """ÏûÖÎ†• Ï±ÑÎÑê Ïàò Ï∂îÎ°†"""
        for key, tensor in state_dict.items():
            if 'conv1' in key or 'input' in key or key.endswith('conv.weight'):
                if hasattr(tensor, 'shape') and len(tensor.shape) == 4:
                    return tensor.shape[1]
        return 3  # Í∏∞Î≥∏Í∞í
    
    def _infer_num_keypoints(self, state_dict: Dict[str, Any]) -> int:
        """ÌÇ§Ìè¨Ïù∏Ìä∏ Ïàò Ï∂îÎ°†"""
        for key, tensor in state_dict.items():
            if any(keyword in key.lower() for keyword in ['keypoint', 'joint', 'pose']):
                if 'weight' in key and hasattr(tensor, 'shape') and len(tensor.shape) >= 2:
                    return tensor.shape[0]
        return 17  # Í∏∞Î≥∏Í∞í (COCO format)
    
    def _infer_num_control_points(self, state_dict: Dict[str, Any]) -> int:
        """Ïª®Ìä∏Î°§ Ìè¨Ïù∏Ìä∏ Ïàò Ï∂îÎ°†"""
        for key, tensor in state_dict.items():
            if any(keyword in key.lower() for keyword in ['control', 'points', 'grid']):
                if 'weight' in key and hasattr(tensor, 'shape') and len(tensor.shape) >= 2:
                    return tensor.shape[0]
        return 10  # Í∏∞Î≥∏Í∞í
    
    def _analyze_key_patterns(self, state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ÌÇ§ Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        patterns = {
            'conv_layers': 0,
            'linear_layers': 0,
            'batch_norm': 0,
            'attention': 0,
            'residual': 0
        }
        
        for key in state_dict.keys():
            key_lower = key.lower()
            if 'conv' in key_lower:
                patterns['conv_layers'] += 1
            elif 'linear' in key_lower or 'fc' in key_lower:
                patterns['linear_layers'] += 1
            elif 'bn' in key_lower or 'batch' in key_lower:
                patterns['batch_norm'] += 1
            elif 'attention' in key_lower or 'attn' in key_lower:
                patterns['attention'] += 1
            elif 'residual' in key_lower or 'skip' in key_lower:
                patterns['residual'] += 1
        
        return patterns
    
    def _analyze_layer_types(self, state_dict: Dict[str, Any]) -> Dict[str, int]:
        """Î†àÏù¥Ïñ¥ ÌÉÄÏûÖ Î∂ÑÏÑù"""
        layer_types = {}
        
        for key in state_dict.keys():
            if hasattr(state_dict[key], 'shape'):
                shape = state_dict[key].shape
                if len(shape) == 4:  # Conv2d
                    layer_types['conv2d'] = layer_types.get('conv2d', 0) + 1
                elif len(shape) == 2:  # Linear
                    layer_types['linear'] = layer_types.get('linear', 0) + 1
                elif len(shape) == 1:  # BatchNorm, Bias
                    layer_types['bias_or_bn'] = layer_types.get('bias_or_bn', 0) + 1
        
        return layer_types
    
    def _estimate_model_depth(self, state_dict: Dict[str, Any]) -> int:
        """Î™®Îç∏ ÍπäÏù¥ Ï∂îÏ†ï"""
        # Î†àÏù¥Ïñ¥ Í∞úÏàòÎ°ú ÍπäÏù¥ Ï∂îÏ†ï
        conv_layers = sum(1 for key in state_dict.keys() if 'conv' in key.lower())
        linear_layers = sum(1 for key in state_dict.keys() if 'linear' in key.lower() or 'fc' in key.lower())
        
        return conv_layers + linear_layers
    
    def _count_parameters_by_type(self, state_dict: Dict[str, Any]) -> Dict[str, int]:
        """ÌÉÄÏûÖÎ≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Í∞úÏàò"""
        param_counts = {
            'conv': 0,
            'linear': 0,
            'bias': 0,
            'bn': 0,
            'other': 0
        }
        
        for key, tensor in state_dict.items():
            if hasattr(tensor, 'numel'):
                num_params = tensor.numel()
                key_lower = key.lower()
                
                if 'conv' in key_lower and 'bias' not in key_lower:
                    param_counts['conv'] += num_params
                elif 'linear' in key_lower or 'fc' in key_lower:
                    if 'bias' in key_lower:
                        param_counts['bias'] += num_params
                    else:
                        param_counts['linear'] += num_params
                elif 'bn' in key_lower or 'batch' in key_lower:
                    param_counts['bn'] += num_params
                else:
                    param_counts['other'] += num_params
        
        return param_counts
    
    def _has_batch_normalization(self, state_dict: Dict[str, Any]) -> bool:
        """Î∞∞Ïπò Ï†ïÍ∑úÌôî Î†àÏù¥Ïñ¥ Ï°¥Ïû¨ Ïó¨Î∂Ä"""
        return any('bn' in key.lower() or 'batch' in key.lower() for key in state_dict.keys())
    
    def _has_attention_layers(self, state_dict: Dict[str, Any]) -> bool:
        """Ïñ¥ÌÖêÏÖò Î†àÏù¥Ïñ¥ Ï°¥Ïû¨ Ïó¨Î∂Ä"""
        return any('attention' in key.lower() or 'attn' in key.lower() for key in state_dict.keys())
    
    def _extract_metadata(self, checkpoint: Any) -> Dict[str, Any]:
        """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        metadata = {}
        
        if isinstance(checkpoint, dict):
            # ÏùºÎ∞òÏ†ÅÏù∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌÇ§Îì§
            for key in ['epoch', 'iteration', 'optimizer', 'scheduler', 'config', 'args']:
                if key in checkpoint:
                    metadata[key] = checkpoint[key]
        
        return metadata


class KeyMapper:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§Î•º Î™®Îç∏ Íµ¨Ï°∞Ïóê Îß§Ìïë"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    def map_keys(self, checkpoint: Dict[str, Any], target_architecture: str, model_state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ÌÇ§ Îß§Ìïë ÏàòÌñâ"""
        try:
            # State dict Ï∂îÏ∂ú
            if 'state_dict' in checkpoint:
                source_state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                source_state_dict = checkpoint['model']
            else:
                source_state_dict = checkpoint
            
            # ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ Îß§Ìïë ÏàòÌñâ
            if target_architecture == 'graphonomy':
                return self._map_graphonomy_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'hrnet':
                return self._map_hrnet_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'openpose':
                return self._map_openpose_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'sam':
                return self._map_sam_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'u2net':
                return self._map_u2net_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'gmm':
                return self._map_gmm_keys(source_state_dict, model_state_dict)
            elif target_architecture == 'raft':
                return self._map_raft_keys(source_state_dict, model_state_dict)
            else:
                return self._map_generic_keys(source_state_dict, model_state_dict)
                
        except Exception as e:
            self.logger.error(f"‚ùå ÌÇ§ Îß§Ìïë Ïã§Ìå®: {e}")
            return {}
    
    def _map_graphonomy_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Graphonomy ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'model.': '',
            'backbone.': '',
            'layer1.': 'layer1.',
            'layer2.': 'layer2.',
            'layer3.': 'layer3.',
            'layer4.': 'layer4.',
            'aspp.': 'aspp.',
            'decoder.': 'decoder.',
            'edge_branch.': 'edge_branch.',
            'edge_decoder.': 'edge_branch.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_hrnet_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """HRNet ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'backbone.': '',
            'stem.': '',
            'stage1.': 'layer1.',
            'stage2.': 'stage2.',
            'stage3.': 'stage3.',
            'stage4.': 'stage4.',
            'transition1.': 'transition1.',
            'transition2.': 'transition2.',
            'transition3.': 'transition3.',
            'final_layer.': 'final_layer.',
            'keypoint_head.': 'final_layer.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_openpose_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """OpenPose ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'model.': '',
            'features.': 'backbone.',
            'stage1_L1.': 'stage1_paf.',
            'stage1_L2.': 'stage1_conf.',
            'stage2_L1.': 'stage2_paf.',
            'stage2_L2.': 'stage2_conf.',
            'stage3_L1.': 'stage3_paf.',
            'stage3_L2.': 'stage3_conf.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_sam_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """SAM ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'image_encoder.': '',
            'patch_embed.proj.': 'patch_embed.',
            'blocks.': 'transformer_blocks.',
            'norm.': 'norm.',
            'prompt_encoder.': 'prompt_embed.',
            'mask_decoder.': 'mask_decoder.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_u2net_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """U2Net ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'stage1.': 'stage1.',
            'stage2.': 'stage2.',
            'stage3.': 'stage3.',
            'stage4.': 'stage4.',
            'stage5.': 'stage5.',
            'stage6.': 'stage6.',
            'stage5d.': 'stage5d.',
            'stage4d.': 'stage4d.',
            'stage3d.': 'stage3d.',
            'stage2d.': 'stage2d.',
            'stage1d.': 'stage1d.',
            'side1.': 'side1.',
            'side2.': 'side2.',
            'side3.': 'side3.',
            'side4.': 'side4.',
            'side5.': 'side5.',
            'side6.': 'side6.',
            'outconv.': 'outconv.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_gmm_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """GMM ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'model.': '',
            'extractionA.': 'feature_extractor.',
            'extractionB.': 'feature_extractor.',
            'regression.': 'regression_head.',
            'gridGen.': 'grid_generator.',
            'localization.': 'regression_head.',
            'theta.': 'regression_head.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_raft_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """RAFT ÌÇ§ Îß§Ìïë"""
        mapped_dict = {}
        
        mapping_rules = {
            'module.': '',
            'fnet.': 'fnet.',
            'cnet.': 'cnet.',
            'update_block.': 'update_block.',
            'encoder.': 'fnet.',
            'context_encoder.': 'cnet.',
            'update.': 'update_block.',
        }
        
        return self._apply_mapping_rules(source_dict, target_dict, mapping_rules)
    
    def _map_generic_keys(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ÏùºÎ∞òÏ†ÅÏù∏ ÌÇ§ Îß§Ìïë - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        mapped_dict = {}
        success_count = 0
        
        # ÌÇ§ Ï†ïÍ∑úÌôîÎ•º ÏúÑÌïú Ìï®Ïàò
        def normalize_key(key):
            return key.lower().replace('_', '').replace('.', '').replace('-', '')
        
        # ÏÜåÏä§ ÌÇ§ Ï†ïÍ∑úÌôî
        normalized_source = {}
        for key, value in source_dict.items():
            norm_key = normalize_key(key)
            normalized_source[norm_key] = (key, value)
        
        # ÌÉÄÍ≤ü ÌÇ§ Ï†ïÍ∑úÌôî
        normalized_target = {}
        for key in target_dict.keys():
            norm_key = normalize_key(key)
            normalized_target[norm_key] = key
        
        # Îß§Ìïë ÏàòÌñâ
        for norm_source_key, (orig_source_key, source_value) in normalized_source.items():
            # Ï†ïÌôï Îß§Ïπ≠
            if norm_source_key in normalized_target:
                target_key = normalized_target[norm_source_key]
                if hasattr(source_value, 'shape') and hasattr(target_dict[target_key], 'shape'):
                    if source_value.shape == target_dict[target_key].shape:
                        mapped_dict[target_key] = source_value
                        success_count += 1
                        continue
            
            # Î∂ÄÎ∂Ñ Îß§Ïπ≠
            for norm_target_key, target_key in normalized_target.items():
                if (norm_source_key in norm_target_key or norm_target_key in norm_source_key):
                    if hasattr(source_value, 'shape') and hasattr(target_dict[target_key], 'shape'):
                        if source_value.shape == target_dict[target_key].shape:
                            mapped_dict[target_key] = source_value
                            success_count += 1
                            break
        
        self.logger.info(f"‚úÖ Í∞úÏÑ†Îêú ÏùºÎ∞ò ÌÇ§ Îß§Ìïë: {success_count}/{len(target_dict)} ÏÑ±Í≥µ")
        return mapped_dict
    
    def _apply_mapping_rules(self, source_dict: Dict[str, Any], target_dict: Dict[str, Any], 
                           mapping_rules: Dict[str, str]) -> Dict[str, Any]:
        """Îß§Ìïë Í∑úÏπô Ï†ÅÏö©"""
        mapped_dict = {}
        unmapped_keys = []
        
        for source_key, tensor in source_dict.items():
            # Í∑úÏπô Í∏∞Î∞ò Îß§Ìïë
            mapped_key = source_key
            for old_prefix, new_prefix in mapping_rules.items():
                if mapped_key.startswith(old_prefix):
                    mapped_key = mapped_key.replace(old_prefix, new_prefix, 1)
                    break
            
            # ÌÉÄÍ≤ü ÎîïÏÖîÎÑàÎ¶¨Ïóê Ìï¥Îãπ ÌÇ§Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            if mapped_key in target_dict:
                # ÌÖêÏÑú ÌÅ¨Í∏∞ Ìò∏ÌôòÏÑ± ÌôïÏù∏
                if hasattr(tensor, 'shape') and hasattr(target_dict[mapped_key], 'shape'):
                    if tensor.shape == target_dict[mapped_key].shape:
                        mapped_dict[mapped_key] = tensor
                    else:
                        self.logger.debug(f"ÌÅ¨Í∏∞ Î∂àÏùºÏπò: {mapped_key} {tensor.shape} vs {target_dict[mapped_key].shape}")
                        unmapped_keys.append(source_key)
                else:
                    mapped_dict[mapped_key] = tensor
            else:
                # Î∂ÄÎ∂Ñ Îß§Ïπ≠ ÏãúÎèÑ
                partial_match = self._find_partial_match(mapped_key, target_dict, tensor)
                if partial_match:
                    mapped_dict[partial_match] = tensor
                else:
                    unmapped_keys.append(source_key)
        
        self.logger.info(f"‚úÖ ÌÇ§ Îß§Ìïë ÏôÑÎ£å: {len(mapped_dict)}/{len(source_dict)} ÏÑ±Í≥µ")
        if unmapped_keys:
            self.logger.debug(f"Îß§ÌïëÎêòÏßÄ ÏïäÏùÄ ÌÇ§: {len(unmapped_keys)}Í∞ú")
        
        return mapped_dict
    
    def _find_partial_match(self, source_key: str, target_dict: Dict[str, Any], tensor: Any) -> Optional[str]:
        """Î∂ÄÎ∂Ñ Îß§Ïπ≠ÏúºÎ°ú ÌÇ§ Ï∞æÍ∏∞"""
        source_parts = source_key.split('.')
        
        for target_key in target_dict.keys():
            target_parts = target_key.split('.')
            
            # ÎßàÏßÄÎßâ Îëê Î∂ÄÎ∂ÑÏù¥ ÏùºÏπòÌïòÎäî Í≤ΩÏö∞
            if len(source_parts) >= 2 and len(target_parts) >= 2:
                if source_parts[-2:] == target_parts[-2:]:
                    # ÌÅ¨Í∏∞ Ìò∏ÌôòÏÑ± ÌôïÏù∏
                    if hasattr(tensor, 'shape') and hasattr(target_dict[target_key], 'shape'):
                        if tensor.shape == target_dict[target_key].shape:
                            return target_key
        
        return None

# ==============================================
# üî• 3. ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ± ÏãúÏä§ÌÖú
# ==============================================

class DynamicModelCreator:
    """ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ± ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # StepÎ≥Ñ ÏïÑÌÇ§ÌÖçÏ≤ò Îß§Ìïë - ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÌôïÏû•
        self.architectures = {
            'human_parsing': HumanParsingArchitecture,
            'pose_estimation': PoseEstimationArchitecture,
            'cloth_segmentation': ClothSegmentationArchitecture,
            'geometric_matching': GeometricMatchingArchitecture,
            'cloth_warping': ClothWarpingArchitecture,          # ÏÉàÎ°ú Ï∂îÍ∞Ä
            'virtual_fitting': VirtualFittingArchitecture,      # ÏÉàÎ°ú Ï∂îÍ∞Ä
            'post_processing': ClothSegmentationArchitecture,   # ESRGAN Îì± - Ïû¨ÏÇ¨Ïö©
            'quality_assessment': PoseEstimationArchitecture,   # CLIP Îì± - Ïû¨ÏÇ¨Ïö©
        }
    
    def create_model_from_checkpoint(self, checkpoint_path: Union[str, Path], 
                                   step_type: str, device: str = DEFAULT_DEVICE) -> Optional[nn.Module]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Î™®Îç∏ ÎèôÏ†Å ÏÉùÏÑ± - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        try:
            # Step ÌÉÄÏûÖ Ï†ïÍ∑úÌôî
            step_type = self._normalize_step_type(step_type)
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù
            analyzer = CheckpointAnalyzer()
            analysis = analyzer.analyze_checkpoint(checkpoint_path)
            
            if 'error' in analysis:
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Ïã§Ìå®: {analysis['error']}")
                return None
            
            # Í∞Å Î™®Îç∏Î≥Ñ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±
            model = self._create_model_from_analysis(analysis, step_type)
            
            if model is None:
                self.logger.error(f"‚ùå Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ± Ïã§Ìå®: {step_type}")
                return None
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
            success = self._load_checkpoint_to_model(
                model, checkpoint_path, analysis, None
            )
            
            if success:
                model.to(device)
                model.eval()
                
                # Î™®Îç∏ Í≤ÄÏ¶ù (Í∞ÑÎã®Ìïú Í≤ÄÏ¶ù)
                try:
                    # Í∞ÑÎã®Ìïú forward pass ÌÖåÏä§Ìä∏
                    test_input = torch.randn(1, 3, 512, 512).to(device)
                    with torch.no_grad():
                        _ = model(test_input)
                    self.logger.info(f"‚úÖ ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ: {step_type}")
                    return model
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®ÌïòÏßÄÎßå Î∞òÌôò: {step_type} - {e}")
                    return model  # Í≤ÄÏ¶ù Ïã§Ìå®Ìï¥ÎèÑ Î™®Îç∏ Î∞òÌôò
            else:
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {step_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _create_model_from_analysis(self, analysis: Dict[str, Any], step_type: str) -> Optional[nn.Module]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Î™®Îç∏ ÏÉùÏÑ± - ModelArchitectureFactory ÏÇ¨Ïö©"""
        try:
            # ModelArchitectureFactory import Î∞è ÏÇ¨Ïö©
            from app.ai_pipeline.utils.model_architectures import ModelArchitectureFactory
            
            architecture_type = analysis.get('architecture_type', 'unknown')
            model_name = analysis.get('model_name', 'unknown')
            
            print(f"üèóÔ∏è {step_type} - {architecture_type} Î™®Îç∏ ÏÉùÏÑ± ÏãúÏûë")
            
            # ModelArchitectureFactoryÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ ÏÉùÏÑ±
            factory = ModelArchitectureFactory()
            model = factory.create_model_from_analysis(analysis)
            
            if model:
                print(f"‚úÖ {step_type} - {architecture_type} Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ")
                return model
            else:
                print(f"‚ùå {step_type} - {architecture_type} Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®")
                return None
                
        except Exception as e:
            print(f"‚ùå Î™®Îç∏ ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {e}")
            return None
    
    def _create_hrnet_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """HRNet Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è HRNet Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # HRNet ÏÑ§Ï†ï Ï∂îÏ∂ú
        num_joints = analysis.get('num_joints', 17)  # COCO Ìè¨Ï¶à ÌÇ§Ìè¨Ïù∏Ìä∏
        
        # model_architectures.pyÏùò HRNetPoseModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and 'HRNetPoseModel' in globals():
            model = HRNetPoseModel(num_joints=num_joints)
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ HRNet Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, num_joints, 1)  # ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞úÏàòÎßåÌÅº Ï∂úÎ†•
            )
        
        return model
    
    def _create_graphonomy_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """Graphonomy Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è Graphonomy Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # Graphonomy ÏÑ§Ï†ï Ï∂îÏ∂ú
        num_classes = analysis.get('num_classes', 20)  # Í∏∞Î≥∏ ATR Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ Ïàò
        
        # model_architectures.pyÏùò GraphonomyModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and 'GraphonomyModel' in globals():
            model = GraphonomyModel(num_classes=num_classes)
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ Graphonomy Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, num_classes, 1)  # ÌÅ¥ÎûòÏä§ ÏàòÎßåÌÅº Ï∂úÎ†•
            )
        
        return model
    
    def _create_u2net_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """U2Net Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è U2Net Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò U2NetModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and U2NetModel is not None:
            model = U2NetModel(out_channels=1)  # Î∞îÏù¥ÎÑàÎ¶¨ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ U2Net Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 1, 1)
            )
        
        return model
    
    def _create_openpose_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """OpenPose Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è OpenPose Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò OpenPoseModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and OpenPoseModel is not None:
            model = OpenPoseModel()
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ OpenPose Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 18, 1)  # 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏
            )
        
        return model
    
    def _create_gmm_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """GMM (Geometric Matching Module) Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è GMM Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò GMMModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and GMMModel is not None:
            num_control_points = analysis.get('num_control_points', 10)
            model = GMMModel(num_control_points=num_control_points)
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ GMM Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(6, 64, 3, padding=1),  # person + cloth
                nn.ReLU(),
                nn.Conv2d(64, num_control_points * 2, 1)  # x, y Ï¢åÌëú
            )
        
        return model
    
    def _create_tom_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """TOM (Try-On Module) Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è TOM Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # TOM ÏÑ§Ï†ï
        model = TOMModel()
        
        return model
    
    def _create_sam_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """SAM (Segment Anything Model) Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è SAM Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò SAMModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and SAMModel is not None:
            model = SAMModel()
        else:
            # Ìè¥Î∞±: ClothSegmentationArchitectureÏùò _create_sam_model ÏÇ¨Ïö©
            architecture = ClothSegmentationArchitecture("cloth_segmentation", self.device)
            model = architecture._create_sam_model()
        
        return model
    
    def _create_real_esrgan_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """Real-ESRGAN Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è Real-ESRGAN Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò RealESRGANModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and RealESRGANModel is not None:
            scale = analysis.get('scale', 4)  # 4x ÏóÖÏä§ÏºÄÏùº
            model = RealESRGANModel(scale=scale)
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ Real-ESRGAN Î™®Îç∏ ÏÉùÏÑ±
            scale = analysis.get('scale', 4)
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 3 * scale * scale, 3, padding=1),
                nn.PixelShuffle(scale)
            )
        
        return model
    
    def _create_tom_model(self, analysis: Dict[str, Any]) -> nn.Module:
        """TOM (Try-On Module) Î™®Îç∏ Ï†ïÌôïÌïú ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±"""
        self.logger.info("üèóÔ∏è TOM Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±")
        
        # model_architectures.pyÏùò TOMModel ÏÇ¨Ïö©
        if MODEL_ARCHITECTURES_AVAILABLE and TOMModel is not None:
            model = TOMModel()
        elif MODEL_ARCHITECTURES_AVAILABLE and OOTDModel is not None:
            # Ìè¥Î∞±: OOTDModel ÏÇ¨Ïö© (Ïú†ÏÇ¨Ìïú Í∏∞Îä•)
            model = OOTDModel()
        else:
            # Ìè¥Î∞±: Í∏∞Î≥∏ TOM Î™®Îç∏ ÏÉùÏÑ±
            model = nn.Sequential(
                nn.Conv2d(6, 64, 3, padding=1),  # person + cloth
                nn.ReLU(),
                nn.Conv2d(64, 3, 1)  # RGB Ï∂úÎ†•
            )
        
        return model
    
    def _normalize_step_type(self, step_type: str) -> str:
        """Step ÌÉÄÏûÖ Ï†ïÍ∑úÌôî"""
        step_mappings = {
            'HumanParsingStep': 'human_parsing',
            'PoseEstimationStep': 'pose_estimation', 
            'ClothSegmentationStep': 'cloth_segmentation',
            'GeometricMatchingStep': 'geometric_matching',
            'ClothWarpingStep': 'cloth_warping',
            'VirtualFittingStep': 'virtual_fitting',
            'PostProcessingStep': 'post_processing',
            'QualityAssessmentStep': 'quality_assessment',
            # ÏÜåÎ¨∏Ïûê Î≤ÑÏ†ÑÎèÑ ÏßÄÏõê
            'human_parsing': 'human_parsing',
            'pose_estimation': 'pose_estimation',
            'cloth_segmentation': 'cloth_segmentation',
            'geometric_matching': 'geometric_matching',
            'cloth_warping': 'cloth_warping',
            'virtual_fitting': 'virtual_fitting',
            'post_processing': 'post_processing',
            'quality_assessment': 'quality_assessment',
        }
        
        return step_mappings.get(step_type, step_type.lower())
    
    def _load_checkpoint_to_model(self, model: nn.Module, checkpoint_path: Union[str, Path],
                                analysis: Dict[str, Any], architecture: StepSpecificArchitecture) -> bool:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Î™®Îç∏Ïóê Î°úÎî© - Í∞úÏÑ†Îêú ÏóêÎü¨ Ï≤òÎ¶¨"""
        try:
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
            analyzer = CheckpointAnalyzer()
            checkpoint = analyzer._load_checkpoint_safe(Path(checkpoint_path))
            
            if checkpoint is None:
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {checkpoint_path}")
                return False
            
            # ÌÇ§ Îß§Ìïë
            mapper = KeyMapper()
            model_state_dict = model.state_dict()
            
            # 1Ï∞®: ÏùºÎ∞ò Îß§Ìïë ÏãúÎèÑ
            mapped_checkpoint = mapper.map_keys(
                checkpoint, analysis['architecture_type'], model_state_dict
            )
            
            # 2Ï∞®: ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ ÌäπÌôî Îß§Ìïë ÏãúÎèÑ
            if (not mapped_checkpoint or len(mapped_checkpoint) < len(model_state_dict) * 0.3) and architecture is not None:
                self.logger.warning("‚ö†Ô∏è ÏùºÎ∞ò ÌÇ§ Îß§Ìïë Í≤∞Í≥º Î∂ÄÏ°±, ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ Îß§Ìïë ÏãúÎèÑ")
                try:
                    mapped_checkpoint = architecture.map_checkpoint_keys(checkpoint)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÏïÑÌÇ§ÌÖçÏ≤òÎ≥Ñ Îß§Ìïë Ïã§Ìå®: {e}")
                    # Îß§Ìïë Ïã§Ìå® Ïãú ÏõêÎ≥∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÇ¨Ïö©
                    if isinstance(checkpoint, dict):
                        mapped_checkpoint = checkpoint
            
            # Î™®Îç∏Ïóê Î°úÎî©
            if mapped_checkpoint:
                try:
                    missing_keys, unexpected_keys = model.load_state_dict(mapped_checkpoint, strict=False)
                    
                    loaded_count = len(mapped_checkpoint) - len(unexpected_keys)
                    total_count = len(model_state_dict)
                    loaded_ratio = loaded_count / total_count if total_count > 0 else 0
                    
                    self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {loaded_ratio:.1%} Í∞ÄÏ§ëÏπò Î°úÎî©Îê®")
                    
                    if missing_keys:
                        self.logger.debug(f"ÎàÑÎùΩÎêú ÌÇ§: {len(missing_keys)}Í∞ú")
                    if unexpected_keys:
                        self.logger.debug(f"ÏòàÏÉÅÏπò Î™ªÌïú ÌÇ§: {len(unexpected_keys)}Í∞ú")
                    
                    return loaded_ratio > 0.1  # 10% Ïù¥ÏÉÅ Î°úÎî©ÎêòÎ©¥ ÏÑ±Í≥µ
                    
                except Exception as load_error:
                    self.logger.error(f"‚ùå state_dict Î°úÎî© Ï§ë Ïò§Î•ò: {load_error}")
                    return False
            else:
                self.logger.error("‚ùå Îß§ÌïëÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏùå")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False

# ==============================================
# üî• 4. ÏÉàÎ°úÏö¥ ModelLoader v6.0 Î©îÏù∏ ÌÅ¥ÎûòÏä§
# ==============================================

class ModelLoader:
    """ModelLoader v6.0 - StepÎ≥Ñ ÌäπÌôî Ïã†Í≤ΩÎßù Íµ¨Ï°∞ ÏßÄÏõê"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, model_cache_dir: Optional[str] = None):
        self.device = device
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # Î°úÍ±∞ ÏÑ§Ï†ï
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(levelname)s: %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # Î™®Îç∏ Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
        
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ÎèôÏ†Å Î™®Îç∏ ÏÉùÏÑ±Í∏∞
        self.model_creator = DynamicModelCreator()
        
        # Î°úÎî©Îêú Î™®Îç∏Îì§
        self.loaded_models: Dict[str, Dict[str, Any]] = {}
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Îß§Ìïë (ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù Í∏∞Î∞òÏúºÎ°ú ÏàòÏ†ï)
        self.checkpoint_mappings = {
            # Human Parsing
            'human_parsing': {
                'graphonomy.pth': 'checkpoints/step_01_human_parsing/graphonomy.pth',
                'exp-schp-201908301523-atr.pth': 'checkpoints/step_01_human_parsing/exp-schp-201908301523-atr.pth',
                'parsing_atr.pth': 'checkpoints/step_01_human_parsing/parsing_atr.pth',
            },
            
            # Pose Estimation
            'pose_estimation': {
                'body_pose_model.pth': 'checkpoints/step_02_pose_estimation/body_pose_model.pth',
                'pose_iter_440000.caffemodel': 'checkpoints/step_02_pose_estimation/pose_iter_440000.caffemodel',
                'pose_deploy.prototxt': 'checkpoints/step_02_pose_estimation/pose_deploy.prototxt',
            },
            
            # Cloth Segmentation
            'cloth_segmentation': {
                'sam_vit_h_4b8939.pth': 'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                'u2net.pth': 'checkpoints/step_03_cloth_segmentation/u2net.pth',
                'cloth_segm.pth': 'checkpoints/step_03_cloth_segmentation/cloth_segm.pth',
            },
            
            # Geometric Matching
            'geometric_matching': {
                'gmm_final.pth': 'checkpoints/step_04_geometric_matching/gmm_final.pth',
                'gmm_train_new.pth': 'checkpoints/step_04_geometric_matching/gmm_train_new.pth',
                'tps_network.pth': 'checkpoints/step_04_geometric_matching/tps_network.pth',
            },
            
            # Cloth Warping (ÏÉàÎ°ú Ï∂îÍ∞Ä)
            'cloth_warping': {
                'RealVisXL_V4.0.safetensors': 'checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors',
                'warping_model.pth': 'checkpoints/step_05_cloth_warping/warping_model.pth',
            },
            
            # Virtual Fitting (ÏÉàÎ°ú Ï∂îÍ∞Ä)
            'virtual_fitting': {
                'diffusion_pytorch_model.safetensors': 'checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.safetensors',
                'ootd_hdm_vitonhd.ckpt': 'checkpoints/step_06_virtual_fitting/ootd_hdm_vitonhd.ckpt',
                'vton_model.pth': 'checkpoints/step_06_virtual_fitting/vton_model.pth',
            },
        }
        
        self.logger.info(f"üöÄ ModelLoader v6.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (ÎîîÎ∞îÏù¥Ïä§: {device})")
    
    def _normalize_step_type(self, step_type: str) -> str:
        """Step ÌÉÄÏûÖ Ï†ïÍ∑úÌôî"""
        step_mappings = {
            'HumanParsingStep': 'human_parsing',
            'PoseEstimationStep': 'pose_estimation', 
            'ClothSegmentationStep': 'cloth_segmentation',
            'GeometricMatchingStep': 'geometric_matching',
            'ClothWarpingStep': 'cloth_warping',
            'VirtualFittingStep': 'virtual_fitting',
            'PostProcessingStep': 'post_processing',
            'QualityAssessmentStep': 'quality_assessment',
        }
        
        return step_mappings.get(step_type, step_type.lower())
    
    def create_step_interface(self, step_type: str) -> 'StepModelInterface':
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± - BaseStepMixin Ìò∏ÌôòÏÑ±"""
        step_type = self._normalize_step_type(step_type)
        
        # StepModelInterface ÏÉùÏÑ± (Í∏∞Ï°¥ Ìå®ÌÑ¥ Ïú†ÏßÄ)
        interface = StepModelInterface(self, step_type)
        
        # ÏûêÎèôÏúºÎ°ú Í∏∞Î≥∏ Î™®Îç∏ Î°úÎî© ÏãúÎèÑ
        try:
            interface.load_primary_model()
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è {step_type} Í∏∞Î≥∏ Î™®Îç∏ ÏûêÎèô Î°úÎî© Ïã§Ìå®: {e}")
        
        return interface
    
    def validate_di_container_integration(self) -> Dict[str, Any]:
        """DI Container ÌÜµÌï© Í≤ÄÏ¶ù - BaseStepMixin Ìò∏ÌôòÏÑ±"""
        return {
            'di_container_available': True,
            'checkpoint_loading_capable': True,
            'step_interface_creation': True,
            'model_cache_available': True,
            'architecture_support': list(self.checkpoint_mappings.keys())
        }
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú load_model Î©îÏÑúÎìú"""
        try:
            # step_nameÏù¥ kwargsÏóê ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ model_nameÏóêÏÑú Ï∂îÎ°†
            step_type = kwargs.get('step_name')
            if not step_type:
                # model_nameÏóêÏÑú step_type Ï∂îÎ°†
                step_type = self._infer_step_type_from_model_name(model_name)
            
            checkpoint_path = kwargs.get('checkpoint_path')
            
            # load_model_for_step Ìò∏Ï∂ú
            model = self.load_model_for_step(step_type, model_name, checkpoint_path)
            
            if model:
                # Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ ÎûòÌïë
                wrapped_model = ModelV6Wrapper(model, model_name, step_type)
                return wrapped_model
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå load_model Ïã§Ìå®: {e}")
            return None
    
    def _infer_step_type_from_model_name(self, model_name: str) -> str:
        """Î™®Îç∏Î™ÖÏóêÏÑú Step ÌÉÄÏûÖ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        # Í∞Å StepÎ≥Ñ ÌÇ§ÏõåÎìú Ìå®ÌÑ¥
        step_patterns = {
            'human_parsing': ['graphonomy', 'parsing', 'schp', 'atr', 'human'],
            'pose_estimation': ['pose', 'hrnet', 'openpose', 'cmu', 'body', 'yolo'],
            'cloth_segmentation': ['sam', 'segment', 'u2net', 'cloth', 'segm'],
            'geometric_matching': ['gmm', 'geometric', 'tps', 'matching', 'warp'],
            'cloth_warping': ['realvis', 'xl', 'stable', 'diffusion', 'warping'],
            'virtual_fitting': ['ootd', 'vton', 'fitting', 'virtual', 'hdm'],
            'post_processing': ['esrgan', 'super', 'resolution', 'enhance'],
            'quality_assessment': ['clip', 'quality', 'assessment', 'score']
        }
        
        # Ìå®ÌÑ¥ Îß§Ïπ≠
        for step_type, patterns in step_patterns.items():
            if any(pattern in model_name_lower for pattern in patterns):
                return step_type
        
        return 'human_parsing'  # Í∏∞Î≥∏Í∞í
    
    def load_model_for_step(self, step_type: str, model_name: Optional[str] = None,
                           checkpoint_path: Optional[str] = None) -> Optional[nn.Module]:
        """StepÎ≥Ñ ÌäπÌôî Î™®Îç∏ Î°úÎî© - ÏïôÏÉÅÎ∏î ÏãúÏä§ÌÖú Î∞©ÏãùÏúºÎ°ú Í∞úÏÑ†"""
        try:
            # Step ÌÉÄÏûÖ Ï†ïÍ∑úÌôî
            step_type = self._normalize_step_type(step_type)
            
            # Î™®Îç∏ ÏãùÎ≥ÑÏûê ÏÉùÏÑ±
            model_id = f"{step_type}_{model_name or 'default'}"
            
            # Ï∫êÏãú ÌôïÏù∏
            if model_id in self.loaded_models:
                cached_model = self.loaded_models[model_id]
                self.logger.info(f"‚ôªÔ∏è Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_id}")
                return cached_model['model']
            
            # 1. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Í≤∞Ï†ï
            if not checkpoint_path:
                checkpoint_path = self._find_checkpoint_path(step_type, model_name)
            
            # 2. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏûàÏúºÎ©¥ Î°úÎî© ÏãúÎèÑ
            if checkpoint_path and os.path.exists(checkpoint_path):
                try:
                    self.logger.info(f"üî• Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏãúÎèÑ: {checkpoint_path}")
                    model = self.model_creator.create_model_from_checkpoint(
                        checkpoint_path, step_type, self.device
                    )
                    
                    if model:
                        # Ï∫êÏãúÏóê Ï†ÄÏû•
                        self.loaded_models[model_id] = {
                            'model': model,
                            'step_type': step_type,
                            'model_name': model_name,
                            'checkpoint_path': str(checkpoint_path),
                            'device': self.device,
                            'loaded_time': time.time()
                        }
                        
                        self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í∏∞Î∞ò Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ: {model_id}")
                        return model
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®, ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± ÏãúÎèÑ: {model_id}")
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå® ({checkpoint_path}): {e}")
            
            # 3. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå® Ïãú ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± (ÏïôÏÉÅÎ∏î Î∞©Ïãù)
            self.logger.info(f"üî• ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± ÏãúÎèÑ: {model_id}")
            model = self._create_model_directly(step_type, model_name)
            
            if model:
                # Ï∫êÏãúÏóê Ï†ÄÏû•
                self.loaded_models[model_id] = {
                    'model': model,
                    'step_type': step_type,
                    'model_name': model_name,
                    'checkpoint_path': 'direct_creation',
                    'device': self.device,
                    'loaded_time': time.time()
                }
                
                self.logger.info(f"‚úÖ ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ: {model_id}")
                return model
            else:
                self.logger.error(f"‚ùå ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ±ÎèÑ Ïã§Ìå®: {model_id}")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå StepÎ≥Ñ Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _create_model_directly(self, step_type: str, model_name: Optional[str]) -> Optional[nn.Module]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏóÜÏù¥ ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± (ÏïôÏÉÅÎ∏î ÏãúÏä§ÌÖú Î∞©Ïãù)"""
        try:
            self.logger.info(f"üî• ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± ÏãúÏûë: {step_type}, {model_name}")
            
            if step_type == 'human_parsing':
                if model_name and 'graphonomy' in model_name.lower():
                    # Graphonomy Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ±
                    try:
                        from app.ai_pipeline.steps.human_parsing.models.graphonomy_models import AdvancedGraphonomyResNetASPP
                        model = AdvancedGraphonomyResNetASPP(num_classes=20, pretrained=False)
                        model.checkpoint_path = f"model_loader_{model_name}_direct"
                        model.checkpoint_data = {"graphonomy": True, "model_type": "AdvancedGraphonomyResNetASPP", "source": "model_loader_direct"}
                        self.logger.info(f"‚úÖ Graphonomy Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ: {model_name}")
                        return model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Graphonomy Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå®: {e}")
                
                elif model_name and 'u2net' in model_name.lower():
                    # U2Net Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ±
                    try:
                        from app.ai_pipeline.utils.model_architectures import U2NetModel
                        model = U2NetModel(out_channels=1)
                        model.checkpoint_path = f"model_loader_{model_name}_direct"
                        model.checkpoint_data = {"u2net": True, "model_type": "U2NetModel", "source": "model_loader_direct"}
                        self.logger.info(f"‚úÖ U2Net Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ: {model_name}")
                        return model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è U2Net Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå®: {e}")
                
                else:
                    # Í∏∞Î≥∏ Graphonomy Î™®Îç∏ ÏÉùÏÑ±
                    try:
                        from app.ai_pipeline.steps.human_parsing.models.graphonomy_models import AdvancedGraphonomyResNetASPP
                        model = AdvancedGraphonomyResNetASPP(num_classes=20, pretrained=False)
                        model.checkpoint_path = f"model_loader_human_parsing_default_direct"
                        model.checkpoint_data = {"graphonomy": True, "model_type": "AdvancedGraphonomyResNetASPP", "source": "model_loader_direct"}
                        self.logger.info(f"‚úÖ Í∏∞Î≥∏ Graphonomy Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ")
                        return model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Í∏∞Î≥∏ Graphonomy Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå®: {e}")
            
            elif step_type == 'cloth_segmentation':
                if model_name and 'sam' in model_name.lower():
                    # SAM Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ±
                    try:
                        from app.ai_pipeline.utils.model_architectures import SAMModel
                        model = SAMModel()
                        model.checkpoint_path = f"model_loader_{model_name}_direct"
                        model.checkpoint_data = {"sam": True, "model_type": "SAMModel", "source": "model_loader_direct"}
                        self.logger.info(f"‚úÖ SAM Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ: {model_name}")
                        return model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è SAM Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå®: {e}")
                
                else:
                    # Í∏∞Î≥∏ U2Net Î™®Îç∏ ÏÉùÏÑ±
                    try:
                        from app.ai_pipeline.utils.model_architectures import U2NetModel
                        model = U2NetModel(out_channels=1)
                        model.checkpoint_path = f"model_loader_cloth_segmentation_default_direct"
                        model.checkpoint_data = {"u2net": True, "model_type": "U2NetModel", "source": "model_loader_direct"}
                        self.logger.info(f"‚úÖ Í∏∞Î≥∏ U2Net Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ")
                        return model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Í∏∞Î≥∏ U2Net Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå®: {e}")
            
            # Îã§Î•∏ Step ÌÉÄÏûÖÎì§Ïóê ÎåÄÌïú Í∏∞Î≥∏ Î™®Îç∏ ÏÉùÏÑ±
            else:
                try:
                    # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Graphonomy Î™®Îç∏ ÏÉùÏÑ±
                    from app.ai_pipeline.steps.human_parsing.models.graphonomy_models import AdvancedGraphonomyResNetASPP
                    model = AdvancedGraphonomyResNetASPP(num_classes=20, pretrained=False)
                    model.checkpoint_path = f"model_loader_{step_type}_default_direct"
                    model.checkpoint_data = {"default": True, "model_type": "AdvancedGraphonomyResNetASPP", "source": "model_loader_direct"}
                    self.logger.info(f"‚úÖ Í∏∞Î≥∏ Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± ÏÑ±Í≥µ: {step_type}")
                    return model
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Í∏∞Î≥∏ Î™®Îç∏ ÏßÅÏ†ë ÏÉùÏÑ± Ïã§Ìå® ({step_type}): {e}")
            
            self.logger.warning(f"‚ö†Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± Î∞©Î≤ïÏù¥ ÏóÜÏùå: {step_type}, {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏßÅÏ†ë Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _find_checkpoint_path(self, step_type: str, model_name: Optional[str]) -> Optional[str]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        try:
            # 1. Îß§Ìïë ÌÖåÏù¥Î∏îÏóêÏÑú Ï∞æÍ∏∞
            if step_type in self.checkpoint_mappings:
                mappings = self.checkpoint_mappings[step_type]
                
                if model_name and model_name in mappings:
                    relative_path = mappings[model_name]
                    full_path = self.model_cache_dir / relative_path
                    
                    if full_path.exists():
                        return str(full_path)
                
                # Ï≤´ Î≤àÏß∏ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ï∞æÍ∏∞
                for model_file, relative_path in mappings.items():
                    full_path = self.model_cache_dir / relative_path
                    if full_path.exists():
                        self.logger.info(f"üîç Í∏∞Î≥∏ Î™®Îç∏ ÏÇ¨Ïö©: {model_file}")
                        return str(full_path)
            
            # 2. Ìå®ÌÑ¥ Í∏∞Î∞ò Í≤ÄÏÉâ
            step_patterns = {
                'human_parsing': ['*graphonomy*.pth', '*parsing*.pth', '*schp*.pth'],
                'pose_estimation': ['*pose*.pth', '*hrnet*.pth', '*yolo*pose*.pt'],
                'cloth_segmentation': ['*sam*.pth', '*u2net*.pth', '*segment*.pth'],
                'geometric_matching': ['*gmm*.pth', '*tps*.pth', '*raft*.pth'],
            }
            
            if step_type in step_patterns:
                for pattern in step_patterns[step_type]:
                    matches = list(self.model_cache_dir.rglob(pattern))
                    if matches:
                        # ÌååÏùº ÌÅ¨Í∏∞Î°ú Ï†ïÎ†¨ (ÌÅ∞ Í≤ÉÎ∂ÄÌÑ∞)
                        matches.sort(key=lambda x: x.stat().st_size, reverse=True)
                        self.logger.info(f"üîç Ìå®ÌÑ¥ Îß§Ïπ≠ Î™®Îç∏ Î∞úÍ≤¨: {matches[0].name}")
                        return str(matches[0])
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Ï∞æÍ∏∞ Ïã§Ìå®: {e}")
            return None
    
    def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        if model_id in self.loaded_models:
            return self.loaded_models[model_id].copy()
        return None
    
    def unload_model(self, model_id: str) -> bool:
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            if model_id in self.loaded_models:
                del self.loaded_models[model_id]
                gc.collect()
                
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                    except:
                        pass
                
                self.logger.info(f"‚úÖ Î™®Îç∏ Ïñ∏Î°úÎìú ÏôÑÎ£å: {model_id}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå®: {e}")
            return False
    
    def list_loaded_models(self) -> List[Dict[str, Any]]:
        """Î°úÎî©Îêú Î™®Îç∏ Î™©Î°ù"""
        return [
            {
                'model_id': model_id,
                'step_type': info['step_type'],
                'model_name': info['model_name'],
                'checkpoint_path': info['checkpoint_path'],
                'device': info['device'],
                'loaded_time': info['loaded_time']
            }
            for model_id, info in self.loaded_models.items()
        ]
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù Î∞òÌôò"""
        try:
            available_models = []
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§ÌïëÏóêÏÑú Î™®Îç∏ Ï†ïÎ≥¥ ÏàòÏßë
            for step_type, model_mappings in self.checkpoint_mappings.items():
                # ÌïÑÌÑ∞ÎßÅ
                if step_class and step_type != step_class:
                    continue
                
                for model_name, relative_path in model_mappings.items():
                    full_path = self.model_cache_dir / relative_path
                    
                    if full_path.exists():
                        # ÌååÏùº ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
                        size_mb = full_path.stat().st_size / (1024 * 1024)
                        
                        # Î™®Îç∏ Ï†ïÎ≥¥ ÏÉùÏÑ±
                        model_info = {
                            'name': model_name,
                            'path': str(full_path),
                            'checkpoint_path': str(full_path),
                            'size_mb': size_mb,
                            'ai_model_info': {
                                'ai_class': self._infer_ai_class_from_name(model_name)
                            },
                            'step_class': self._map_step_type_to_class(step_type),
                            'model_type': self._infer_model_type(model_name),
                            'loaded': model_name in [info['model_name'] for info in self.loaded_models.values()],
                            'device': self.device
                        }
                        
                        # Î™®Îç∏ ÌÉÄÏûÖ ÌïÑÌÑ∞ÎßÅ
                        if model_type and model_info['model_type'] != model_type:
                            continue
                        
                        available_models.append(model_info)
            
            # ÌååÏùº ÌÅ¨Í∏∞Î°ú Ï†ïÎ†¨ (ÌÅ∞ Í≤ÉÎ∂ÄÌÑ∞)
            available_models.sort(key=lambda x: x['size_mb'], reverse=True)
            
            self.logger.info(f"‚úÖ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ {len(available_models)}Í∞ú Î∞úÍ≤¨")
            return available_models
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def _infer_ai_class_from_name(self, model_name: str) -> str:
        """Î™®Îç∏Î™ÖÏóêÏÑú AI ÌÅ¥ÎûòÏä§ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        if 'graphonomy' in model_name_lower:
            return 'GraphonomyModel'
        elif 'hrnet' in model_name_lower or 'pose' in model_name_lower:
            return 'HRNetPoseModel'
        elif 'sam' in model_name_lower:
            return 'SAMModel'
        elif 'u2net' in model_name_lower:
            return 'U2NetModel'
        elif 'gmm' in model_name_lower:
            return 'GMMModel'
        elif 'realvis' in model_name_lower or 'xl' in model_name_lower:
            return 'RealVisXLModel'
        elif 'ootd' in model_name_lower:
            return 'OOTDModel'
        elif 'esrgan' in model_name_lower:
            return 'RealESRGANModel'
        else:
            return 'GenericModel'
    
    def _map_step_type_to_class(self, step_type: str) -> str:
        """Step ÌÉÄÏûÖÏùÑ ÌÅ¥ÎûòÏä§Î™ÖÏúºÎ°ú Îß§Ìïë"""
        mappings = {
            'human_parsing': 'HumanParsingStep',
            'pose_estimation': 'PoseEstimationStep',
            'cloth_segmentation': 'ClothSegmentationStep',
            'geometric_matching': 'GeometricMatchingStep',
            'cloth_warping': 'ClothWarpingStep',
            'virtual_fitting': 'VirtualFittingStep',
            'post_processing': 'PostProcessingStep',
            'quality_assessment': 'QualityAssessmentStep',
        }
        
        return mappings.get(step_type, 'UnknownStep')
    
    def get_model_path(self, model_name: str, step_name: Optional[str] = None) -> Optional[Path]:
        """Î™®Îç∏ Í≤ΩÎ°ú Ï°∞Ìöå"""
        try:
            # step_nameÏù¥ ÏûàÏúºÎ©¥ Ìï¥Îãπ stepÏóêÏÑú Í≤ÄÏÉâ
            if step_name:
                step_type = self._normalize_step_type(step_name)
                if step_type in self.checkpoint_mappings:
                    mappings = self.checkpoint_mappings[step_type]
                    if model_name in mappings:
                        relative_path = mappings[model_name]
                        full_path = self.model_cache_dir / relative_path
                        if full_path.exists():
                            return full_path
            
            # Î™®Îì† stepÏóêÏÑú Í≤ÄÏÉâ
            for step_type, model_mappings in self.checkpoint_mappings.items():
                if model_name in model_mappings:
                    relative_path = model_mappings[model_name]
                    full_path = self.model_cache_dir / relative_path
                    if full_path.exists():
                        return full_path
            
            # Ìå®ÌÑ¥ Í∏∞Î∞ò Í≤ÄÏÉâ
            for step_type, model_mappings in self.checkpoint_mappings.items():
                for pattern_name, relative_path in model_mappings.items():
                    if model_name.lower() in pattern_name.lower() or pattern_name.lower() in model_name.lower():
                        full_path = self.model_cache_dir / relative_path
                        if full_path.exists():
                            return full_path
            
            # ÌååÏùº ÏãúÏä§ÌÖú ÏßÅÏ†ë Í≤ÄÏÉâ (Ìè¥Î∞±)
            search_dirs = [
                self.model_cache_dir,
                self.model_cache_dir / "step_02_pose_estimation",
                self.model_cache_dir / "step_01_human_parsing",
                self.model_cache_dir / "step_03_cloth_segmentation",
                self.model_cache_dir / "step_04_geometric_matching",
                self.model_cache_dir / "step_05_cloth_warping",
                self.model_cache_dir / "step_06_virtual_fitting",
            ]
            
            for search_dir in search_dirs:
                if search_dir.exists():
                    # Ï†ïÌôïÌïú ÌååÏùºÎ™Ö Í≤ÄÏÉâ
                    exact_path = search_dir / model_name
                    if exact_path.exists():
                        return exact_path
                    
                    # Î∂ÄÎ∂Ñ Îß§Ïπ≠ Í≤ÄÏÉâ
                    for file_path in search_dir.glob("*"):
                        if file_path.is_file() and model_name.lower() in file_path.name.lower():
                            return file_path
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°ú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return None

    def _infer_model_type(self, model_name: str) -> str:
        """Î™®Îç∏Î™ÖÏóêÏÑú Î™®Îç∏ ÌÉÄÏûÖ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ['realvis', 'xl', 'stable', 'diffusion']):
            return 'processing'
        elif any(keyword in model_name_lower for keyword in ['warping', 'warp']):
            return 'warping'
        elif any(keyword in model_name_lower for keyword in ['pose', 'hrnet']):
            return 'pose_estimation'
        elif any(keyword in model_name_lower for keyword in ['parsing', 'graphonomy']):
            return 'parsing'
        elif any(keyword in model_name_lower for keyword in ['sam', 'segment', 'u2net']):
            return 'segmentation'
        elif any(keyword in model_name_lower for keyword in ['gmm', 'geometric']):
            return 'matching'
        else:
            return 'generic'

    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            model_ids = list(self.loaded_models.keys())
            for model_id in model_ids:
                self.unload_model(model_id)
            
            self.logger.info("‚úÖ ModelLoader v6.0 Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• 5. StepÎ≥Ñ ÌäπÌôî Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
# ==============================================

class StepModelInterface:
    """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ - BaseStepMixin ÏôÑÏ†Ñ Ìò∏Ìôò"""
    
    def __init__(self, model_loader: ModelLoader, step_type: str):
        self.model_loader = model_loader
        self.step_type = step_type
        self.logger = logging.getLogger(f"StepInterface.{step_type}")
        
        # Î°úÎî©Îêú Î™®Îç∏
        self.primary_model: Optional[nn.Module] = None
        self.fallback_models: List[nn.Module] = []
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.last_inference_time = 0.0
        
        # BaseStepMixin Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú ÏÜçÏÑ±Îì§
        self.loaded = False
        self.model_instance = None
        self.checkpoint_data = None
    
    def load_primary_model(self, model_name: Optional[str] = None, 
                          checkpoint_path: Optional[str] = None) -> bool:
        """Ï£ºÏöî Î™®Îç∏ Î°úÎî©"""
        try:
            model = self.model_loader.load_model_for_step(
                self.step_type, model_name, checkpoint_path
            )
            
            if model:
                self.primary_model = model
                self.model_instance = model  # BaseStepMixin Ìò∏ÌôòÏÑ±
                self.checkpoint_data = model.state_dict()  # BaseStepMixin Ìò∏ÌôòÏÑ±
                self.loaded = True  # BaseStepMixin Ìò∏ÌôòÏÑ±
                
                self.logger.info(f"‚úÖ {self.step_type} Ï£ºÏöî Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
                return True
            else:
                self.logger.error(f"‚ùå {self.step_type} Ï£ºÏöî Î™®Îç∏ Î°úÎî© Ïã§Ìå®")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_type} Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def get_model(self) -> Optional[nn.Module]:
        """Î™®Îç∏ Ï°∞Ìöå - BaseStepMixin Ìò∏ÌôòÏÑ±"""
        if self.primary_model:
            return self.primary_model
        elif self.fallback_models:
            return self.fallback_models[0]
        else:
            return None
    
    def get_model_instance(self) -> Optional[nn.Module]:
        """Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò - BaseStepMixin Ìò∏ÌôòÏÑ±"""
        return self.get_model()
    
    def get_checkpoint_data(self) -> Optional[Dict[str, Any]]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò - BaseStepMixin Ìò∏ÌôòÏÑ±"""
        return self.checkpoint_data
    
    def run_inference(self, *args, **kwargs) -> Optional[Dict[str, Any]]:
        """Ï∂îÎ°† Ïã§Ìñâ"""
        model = self.get_model()
        if not model:
            self.logger.error(f"‚ùå {self.step_type} Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
            return None
        
        try:
            start_time = time.time()
            
            model.eval()
            with torch.no_grad():
                # StepÎ≥Ñ ÌäπÌôîÎêú ÏûÖÎ†• Ï≤òÎ¶¨
                result = self._run_step_specific_inference(model, *args, **kwargs)
            
            inference_time = time.time() - start_time
            
            # ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
            self.inference_count += 1
            self.total_inference_time += inference_time
            self.last_inference_time = inference_time
            
            self.logger.debug(f"‚úÖ {self.step_type} Ï∂îÎ°† ÏôÑÎ£å ({inference_time:.3f}Ï¥à)")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_type} Ï∂îÎ°† Ïã§Ìå®: {e}")
            return None
    
    def _run_step_specific_inference(self, model: nn.Module, *args, **kwargs) -> Dict[str, Any]:
        """StepÎ≥Ñ ÌäπÌôî Ï∂îÎ°† Î°úÏßÅ"""
        # Step ÌÉÄÏûÖÏóê Îî∞Î•∏ ÌäπÌôîÎêú Ï≤òÎ¶¨
        if self.step_type == 'geometric_matching' and len(args) >= 2:
            # Geometric MatchingÏùÄ 2Í∞ú Ïù∏Ïûê ÌïÑÏöî (person_image, cloth_image)
            result = model(args[0], args[1])
        elif self.step_type == 'virtual_fitting' and len(args) >= 3:
            # Virtual FittingÏùÄ 3Í∞ú Ïù∏Ïûê ÌïÑÏöî (person_image, cloth_image, text_prompt)
            result = model(args[0], args[1], args[2], **kwargs)
        else:
            # Îã§Î•∏ StepÎì§ÏùÄ 1Í∞ú Ïù∏Ïûê
            result = model(args[0])
        
        # Í≤∞Í≥º Ï†ïÍ∑úÌôî
        if not isinstance(result, dict):
            result = {'output': result}
        
        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
        result['step_type'] = self.step_type
        result['inference_time'] = self.last_inference_time
        result['model_loaded'] = True
        
        return result
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå"""
        avg_time = self.total_inference_time / max(1, self.inference_count)
        
        return {
            'step_type': self.step_type,
            'inference_count': self.inference_count,
            'total_inference_time': self.total_inference_time,
            'average_inference_time': avg_time,
            'last_inference_time': self.last_inference_time,
            'model_loaded': self.primary_model is not None,
            'fallback_models_count': len(self.fallback_models)
        }

# ==============================================
# üî• 6. StepÎ≥Ñ ÌäπÌôî Ìå©ÌÜ†Î¶¨
# ==============================================

class StepModelFactory:
    """StepÎ≥Ñ Î™®Îç∏ Ìå©ÌÜ†Î¶¨"""
    
    def __init__(self, model_loader: ModelLoader):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # StepÎ≥Ñ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï∫êÏãú
        self.step_interfaces: Dict[str, StepModelInterface] = {}
    
    def create_step_interface(self, step_type: str) -> StepModelInterface:
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
        if step_type not in self.step_interfaces:
            interface = StepModelInterface(self.model_loader, step_type)
            self.step_interfaces[step_type] = interface
            self.logger.info(f"‚úÖ {step_type} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
        
        return self.step_interfaces[step_type]
    
    def get_step_interface(self, step_type: str) -> Optional[StepModelInterface]:
        """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï°∞Ìöå"""
        return self.step_interfaces.get(step_type)
    
    def initialize_all_steps(self) -> Dict[str, bool]:
        """Î™®Îì† Step Ï¥àÍ∏∞Ìôî"""
        results = {}
        
        step_types = ['human_parsing', 'pose_estimation', 'cloth_segmentation', 'geometric_matching']
        
        for step_type in step_types:
            try:
                interface = self.create_step_interface(step_type)
                success = interface.load_primary_model()
                results[step_type] = success
                
                if success:
                    self.logger.info(f"‚úÖ {step_type} Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ")
                else:
                    self.logger.warning(f"‚ö†Ô∏è {step_type} Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
                    
            except Exception as e:
                self.logger.error(f"‚ùå {step_type} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                results[step_type] = False
        
        success_count = sum(results.values())
        self.logger.info(f"üéØ Step Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {success_count}/{len(step_types)} ÏÑ±Í≥µ")
        
        return results
    
    def get_all_performance_metrics(self) -> Dict[str, Dict[str, Any]]:
        """Î™®Îì† Step ÏÑ±Îä• Î©îÌä∏Î¶≠"""
        metrics = {}
        
        for step_type, interface in self.step_interfaces.items():
            metrics[step_type] = interface.get_performance_metrics()
        
        return metrics

# ==============================================
# üî• 7. ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Î∞è Ìé∏Ïùò Ìï®Ïàò
# ==============================================

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
_global_model_loader_v6: Optional[ModelLoader] = None
_global_step_factory: Optional[StepModelFactory] = None
_loader_lock = threading.Lock()

# üî• Ï†ÑÏó≠ ModelLoader v5.1 Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
_global_model_loader: Optional[ModelLoader] = None

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (v5.1 Ìò∏ÌôòÏÑ±)"""
    global _global_model_loader
    
    if _global_model_loader is None:
        try:
            # v6.0 Î°úÎçîÎ•º v5.1 Ìò∏ÌôòÏÑ±ÏúºÎ°ú ÎûòÌïë
            v6_loader = get_model_loader_v6()
            if v6_loader:
                _global_model_loader = v6_loader
                logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader v5.1 Ìò∏ÌôòÏÑ± Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            else:
                logger.error("‚ùå ModelLoader v6.0 Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
                return None
        except Exception as e:
            logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return None
    
    return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî (v5.1 Ìò∏ÌôòÏÑ±)"""
    try:
        global _global_model_loader
        
        if _global_model_loader is None:
            # v6.0 Î°úÎçî Ï¥àÍ∏∞Ìôî
            v6_loader = get_model_loader_v6()
            if v6_loader:
                _global_model_loader = v6_loader
                logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ")
                return True
            else:
                logger.error("‚ùå ModelLoader v6.0 Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
                return False
        else:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader Ïù¥ÎØ∏ Ï¥àÍ∏∞ÌôîÎê®")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî (v5.1 Ìò∏ÌôòÏÑ±)"""
    try:
        # ÎèôÍ∏∞ Ìï®ÏàòÎ•º ÎπÑÎèôÍ∏∞Î°ú ÎûòÌïë
        success = initialize_global_model_loader(**kwargs)
        if success:
            return get_global_model_loader()
        else:
            return None
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return None

def get_model_loader_v6(device: str = DEFAULT_DEVICE) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader v6.0 Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_loader_v6
    
    # ÎùΩ ÏóÜÏù¥ Í∞ÑÎã®ÌïòÍ≤å Ï≤òÎ¶¨
    if _global_model_loader_v6 is None:
        _global_model_loader_v6 = ModelLoader(device=device)
        logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader v6.0 ÏÉùÏÑ± ÏôÑÎ£å")
    
    return _global_model_loader_v6

def get_step_factory() -> StepModelFactory:
    """Ï†ÑÏó≠ Step Ìå©ÌÜ†Î¶¨ Î∞òÌôò"""
    global _global_step_factory
    
    # ÎùΩ ÏóÜÏù¥ Í∞ÑÎã®ÌïòÍ≤å Ï≤òÎ¶¨
    if _global_step_factory is None:
        model_loader = get_model_loader_v6()
        _global_step_factory = StepModelFactory(model_loader)
        logger.info("‚úÖ Ï†ÑÏó≠ Step Ìå©ÌÜ†Î¶¨ ÏÉùÏÑ± ÏôÑÎ£å")
    
    return _global_step_factory

def load_model_for_step(step_type: str, model_name: Optional[str] = None, 
                       checkpoint_path: Optional[str] = None) -> Optional[nn.Module]:
    """StepÎ≥Ñ Î™®Îç∏ Î°úÎî© (Ï†ÑÏó≠ Ìï®Ïàò)"""
    model_loader = get_model_loader_v6()
    return model_loader.load_model_for_step(step_type, model_name, checkpoint_path)

def create_step_interface(step_type: str) -> StepModelInterface:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (Ï†ÑÏó≠ Ìï®Ïàò)"""
    factory = get_step_factory()
    return factory.create_step_interface(step_type)

def initialize_all_steps() -> Dict[str, bool]:
    """Î™®Îì† Step Ï¥àÍ∏∞Ìôî (Ï†ÑÏó≠ Ìï®Ïàò)"""
    factory = get_step_factory()
    return factory.initialize_all_steps()

def analyze_checkpoint(checkpoint_path: Union[str, Path]) -> Dict[str, Any]:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù (Ï†ÑÏó≠ Ìï®Ïàò)"""
    analyzer = CheckpointAnalyzer()
    return analyzer.analyze_checkpoint(checkpoint_path)

def cleanup_model_loader():
    """Ï†ÑÏó≠ Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
    global _global_model_loader_v6, _global_step_factory, _global_model_loader
    
    with _loader_lock:
        if _global_model_loader_v6:
            _global_model_loader_v6.cleanup()
            _global_model_loader_v6 = None
        
        _global_step_factory = None
        _global_model_loader = None
        
        logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader v6.0 Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")

# ==============================================
# üî• 8. Ìò∏ÌôòÏÑ± Î†àÏù¥Ïñ¥ (Í∏∞Ï°¥ ModelLoaderÏôÄ Ìò∏Ìôò)
# ==============================================

class ModelLoaderCompatibilityAdapter:
    """Í∏∞Ï°¥ ModelLoaderÏôÄÏùò Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Ïñ¥ÎåëÌÑ∞"""
    
    def __init__(self):
        self.v6_loader = get_model_loader_v6()
        self.step_factory = get_step_factory()
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    def load_model(self, model_name: str, step_name: Optional[str] = None, 
                   step_type: Optional[str] = None, **kwargs) -> Optional[Any]:
        """Í∏∞Ï°¥ ModelLoader.load_model() Ìò∏Ìôò"""
        try:
            # Step ÌÉÄÏûÖ Í≤∞Ï†ï
            if step_type:
                target_step_type = step_type
            elif step_name:
                target_step_type = self._map_step_name_to_type(step_name)
            else:
                target_step_type = self._infer_step_type_from_model_name(model_name)
            
            # v6.0 Î∞©ÏãùÏúºÎ°ú Î°úÎî©
            model = self.v6_loader.load_model_for_step(
                target_step_type, model_name, kwargs.get('checkpoint_path')
            )
            
            if model:
                # Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ ÎûòÌïë
                wrapped_model = ModelV6Wrapper(model, model_name, target_step_type)
                return wrapped_model
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Ìò∏ÌôòÏÑ± Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _map_step_name_to_type(self, step_name: str) -> str:
        """Step Ïù¥Î¶ÑÏùÑ ÌÉÄÏûÖÏúºÎ°ú Îß§Ìïë"""
        mappings = {
            'HumanParsingStep': 'human_parsing',
            'PoseEstimationStep': 'pose_estimation',
            'ClothSegmentationStep': 'cloth_segmentation',
            'GeometricMatchingStep': 'geometric_matching',
            'ClothWarpingStep': 'cloth_warping',
            'VirtualFittingStep': 'virtual_fitting',
            'PostProcessingStep': 'post_processing',
            'QualityAssessmentStep': 'quality_assessment',
        }
        
        return mappings.get(step_name, 'human_parsing')
    
    def _infer_step_type_from_model_name(self, model_name: str) -> str:
        """Î™®Îç∏Î™ÖÏóêÏÑú Step ÌÉÄÏûÖ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ['graphonomy', 'parsing', 'schp']):
            return 'human_parsing'
        elif any(keyword in model_name_lower for keyword in ['pose', 'hrnet', 'openpose']):
            return 'pose_estimation'
        elif any(keyword in model_name_lower for keyword in ['sam', 'segment', 'u2net']):
            return 'cloth_segmentation'
        elif any(keyword in model_name_lower for keyword in ['gmm', 'geometric', 'tps', 'raft']):
            return 'geometric_matching'
        else:
            return 'human_parsing'

class ModelV6Wrapper:
    """v6.0 Î™®Îç∏ÏùÑ Í∏∞Ï°¥ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÎûòÌïë"""
    
    def __init__(self, model: nn.Module, model_name: str, step_type: str):
        self.model = model
        self.model_name = model_name
        self.step_type = step_type
        self.loaded = True
        self.checkpoint_data = model.state_dict()
        self.model_instance = model
        
    def get_model_instance(self) -> nn.Module:
        """Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
        return self.model
    
    def get_checkpoint_data(self) -> Dict[str, Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        return self.checkpoint_data
    
    def unload(self):
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        self.loaded = False
        self.model = None
        self.checkpoint_data = None
        self.model_instance = None

# ==============================================
# üî• 9. Ï¥àÍ∏∞Ìôî Î∞è ÌÖåÏä§Ìä∏
# ==============================================

def test_model_loader_v6():
    """ModelLoader v6.0 ÌÖåÏä§Ìä∏"""
    try:
        logger.info("üß™ ModelLoader v6.0 ÌÖåÏä§Ìä∏ ÏãúÏûë")
        
        # 1. Í∏∞Î≥∏ Î°úÎçî ÌÖåÏä§Ìä∏
        model_loader = get_model_loader_v6()
        logger.info(f"‚úÖ ModelLoader v6.0 Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ÏÑ±Í≥µ")
        
        # 2. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù ÌÖåÏä§Ìä∏
        test_checkpoints = [
            'checkpoints/step_01_human_parsing/graphonomy.pth',
            'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
        ]
        
        for checkpoint_rel_path in test_checkpoints:
            checkpoint_path = model_loader.model_cache_dir / checkpoint_rel_path
            if checkpoint_path.exists():
                analysis = analyze_checkpoint(checkpoint_path)
                if 'error' not in analysis:
                    logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù ÏÑ±Í≥µ: {checkpoint_path.name}")
                    logger.info(f"   ÏïÑÌÇ§ÌÖçÏ≤ò: {analysis.get('architecture_type', 'unknown')}")
                    logger.info(f"   ÌååÎùºÎØ∏ÌÑ∞: {analysis.get('total_parameters', 0)}Í∞ú")
                else:
                    logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Ïã§Ìå®: {checkpoint_path.name}")
        
        # 3. Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÖåÏä§Ìä∏
        factory = get_step_factory()
        
        test_steps = ['human_parsing', 'pose_estimation', 'cloth_segmentation']
        for step_type in test_steps:
            try:
                interface = factory.create_step_interface(step_type)
                logger.info(f"‚úÖ {step_type} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏÑ±Í≥µ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {step_type} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        
        # 4. Ìò∏ÌôòÏÑ± Ïñ¥ÎåëÌÑ∞ ÌÖåÏä§Ìä∏
        adapter = ModelLoaderCompatibilityAdapter()
        logger.info(f"‚úÖ Ìò∏ÌôòÏÑ± Ïñ¥ÎåëÌÑ∞ ÏÉùÏÑ± ÏÑ±Í≥µ")
        
        logger.info("üéâ ModelLoader v6.0 Í∏∞Î≥∏ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå ModelLoader v6.0 ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        return False


def validate_project_integration() -> Dict[str, Any]:
    """ÌîÑÎ°úÏ†ùÌä∏ ÌÜµÌï© Í≤ÄÏ¶ù"""
    try:
        validation_result = {
            'model_loader_v6': False,
            'step_factory': False,
            'compatibility_adapter': False,
            'di_container_integration': False,
            'step_interfaces': {},
            'checkpoint_analysis': {},
            'overall_status': 'failed'
        }
        
        # ModelLoader v6.0 Í≤ÄÏ¶ù
        model_loader = get_model_loader_v6()
        if model_loader:
            validation_result['model_loader_v6'] = True
        
        # Step Factory Í≤ÄÏ¶ù
        factory = get_step_factory()
        if factory:
            validation_result['step_factory'] = True
        
        # Ìò∏ÌôòÏÑ± Ïñ¥ÎåëÌÑ∞ Í≤ÄÏ¶ù
        try:
            adapter = ModelLoaderV6CompatibilityAdapter()
            if adapter.v6_loader:
                validation_result['compatibility_adapter'] = True
                
                # DI Container ÌÜµÌï© Í≤ÄÏ¶ù
                di_validation = adapter.validate_di_container_integration()
                validation_result['di_container_integration'] = di_validation.get('di_container_available', False)
        except Exception:
            pass
        
        # StepÎ≥Ñ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í≤ÄÏ¶ù
        step_types = ['human_parsing', 'pose_estimation', 'cloth_segmentation', 
                     'geometric_matching', 'cloth_warping', 'virtual_fitting']
        
        for step_type in step_types:
            try:
                if factory:
                    interface = factory.create_step_interface(step_type)
                    validation_result['step_interfaces'][step_type] = interface is not None
                else:
                    validation_result['step_interfaces'][step_type] = False
            except Exception:
                validation_result['step_interfaces'][step_type] = False
        
        # Ï¢ÖÌï© ÏÉÅÌÉú Í≤∞Ï†ï
        success_count = sum([
            validation_result['model_loader_v6'],
            validation_result['step_factory'],
            validation_result['compatibility_adapter'],
            validation_result['di_container_integration']
        ])
        
        interface_success_count = sum(validation_result['step_interfaces'].values())
        
        if success_count >= 3 and interface_success_count >= 4:
            validation_result['overall_status'] = 'success'
        elif success_count >= 2 and interface_success_count >= 2:
            validation_result['overall_status'] = 'partial'
        else:
            validation_result['overall_status'] = 'failed'
        
        return validation_result
        
    except Exception as e:
        return {
            'overall_status': 'error',
            'error': str(e)
        }


# ==============================================
# üî• 7. Î™®Îìà ÏôÑÎ£å Î∞è Export
# ==============================================

# Ï∂îÍ∞ÄÎêú Export Ìï≠Î™©Îì§
__all__ = [
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'ModelLoader',
    'StepModelInterface',
    'StepModelFactory',
    'DynamicModelCreator',
    'CheckpointAnalyzer',
    'KeyMapper',
    
    # ÏïÑÌÇ§ÌÖçÏ≤ò ÌÅ¥ÎûòÏä§Îì§
    'HumanParsingArchitecture',
    'PoseEstimationArchitecture', 
    'ClothSegmentationArchitecture',
    'GeometricMatchingArchitecture',
    'VirtualFittingArchitecture',
    'ClothWarpingArchitecture',
    
    # Ï†ÑÏó≠ Ìï®ÏàòÎì§
    'get_model_loader_v6',
    'get_step_factory',
    'load_model_for_step',
    'create_step_interface',
    'initialize_all_steps',
    'analyze_checkpoint',
    'cleanup_model_loader',
    
    # Ìò∏ÌôòÏÑ±
    'ModelLoaderCompatibilityAdapter',
    'ModelV6Wrapper',
    'ModelLoaderV6CompatibilityAdapter',
    
    # ÌÖåÏä§Ìä∏
    'test_model_loader_v6',
    'validate_project_integration',
    
    # ÏÉÅÏàòÎì§
    'DEFAULT_DEVICE',
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'IS_M3_MAX',
]

# Î™®Îìà Ï¥àÍ∏∞Ìôî Ïãú ÏûêÎèô Í≤ÄÏ¶ù (ÏàúÌôò Ï∞∏Ï°∞ Î∞©ÏßÄ)
if __name__ == "__main__":
    # ÏßÅÏ†ë Ïã§Ìñâ Ïãú Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏
    logger.info("üöÄ ModelLoader v6.0 ÏßÅÏ†ë Ïã§Ìñâ Î™®Îìú")
    test_result = test_model_loader_v6()
    validation_result = validate_project_integration()
    
    logger.info("=" * 80)
    logger.info("üìã ÏµúÏ¢Ö Í≤ÄÏ¶ù Í≤∞Í≥º:")
    logger.info(f"   ÌÖåÏä§Ìä∏ Í≤∞Í≥º: {'ÏÑ±Í≥µ' if test_result else 'Ïã§Ìå®'}")
    logger.info(f"   ÌÜµÌï© ÏÉÅÌÉú: {validation_result.get('overall_status', 'unknown')}")
    logger.info("=" * 80)
    
else:
    # Î™®ÎìàÎ°ú importÎê† ÎïåÎäî Í∏∞Î≥∏ Î°úÎìúÎßå ÏàòÌñâ (Í≤ÄÏ¶ù Ï†úÍ±∞)
    logger.info("üéâ ModelLoader v6.0 Î™®Îìà Î°úÎìú ÏôÑÎ£å!")
    logger.info("=" * 80)
