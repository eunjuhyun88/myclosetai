# app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI - ì™„ì „ í†µí•© ModelLoader ì‹œìŠ¤í…œ v4.0 - ğŸ”¥ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… step_model_requests.py ê¸°ë°˜ ìë™ ëª¨ë¸ íƒì§€ ë° ë¡œë”©
âœ… auto_model_detectorì™€ ì™„ë²½ ì—°ë™
âœ… Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ë§Œ ì‚¬ìš© (í´ë°± ì™„ì „ ì œê±°)
âœ… conda í™˜ê²½ ìµœì í™”
âœ… StepModelInterface ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥ ì™„ì „ í†µí•©
âœ… _setup_model_paths ë©”ì„œë“œ ëˆ„ë½ ë¬¸ì œ í•´ê²°
âœ… load_model_async íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°

ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥:
- Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ìë™ ë¶„ì„
- ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ìë™ íƒì§€
- ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ ë§¤í•‘
- M3 Max Neural Engine í™œìš©
- í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
- ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ë° ì²˜ë¦¬
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
import json
import pickle
import sqlite3
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import weakref

# PyTorch import (ì•ˆì „)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None

# ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import cv2
    import numpy as np
    from PIL import Image, ImageEnhance
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False

# ì™¸ë¶€ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer, AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ Enum ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

class ModelFormat(Enum):
    """ğŸ”¥ ëª¨ë¸ í¬ë§· ì •ì˜ - main.pyì—ì„œ í•„ìˆ˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ Step ìš”ì²­ì‚¬í•­ ì—°ë™ (step_model_requests.py)
# ==============================================

try:
    from .step_model_requests import (
        STEP_MODEL_REQUESTS,
        StepModelRequestAnalyzer,
        get_all_step_requirements,
        create_model_loader_config_from_detection
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("âœ… step_model_requests ëª¨ë“ˆ ì—°ë™ ì„±ê³µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"âš ï¸ step_model_requests ëª¨ë“ˆ ì—°ë™ ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ ë‚´ì¥ ê¸°ë³¸ ìš”ì²­ì‚¬í•­ (step_model_requests.py ë‚´ìš© ì¼ë¶€)
    STEP_MODEL_REQUESTS = {
        "HumanParsingStep": {
            "model_name": "human_parsing_graphonomy",
            "model_type": "GraphonomyModel",
            "input_size": (512, 512),
            "num_classes": 20,
            "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
        },
        "PoseEstimationStep": {
            "model_name": "pose_estimation_openpose",
            "model_type": "OpenPoseModel",
            "input_size": (368, 368),
            "num_classes": 18,
            "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
        },
        "ClothSegmentationStep": {
            "model_name": "cloth_segmentation_u2net",
            "model_type": "U2NetModel",
            "input_size": (320, 320),
            "num_classes": 1,
            "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
        },
        "VirtualFittingStep": {
            "model_name": "virtual_fitting_stable_diffusion",
            "model_type": "StableDiffusionPipeline",
            "input_size": (512, 512),
            "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
        }
    }
    
    class StepModelRequestAnalyzer:
        @staticmethod
        def get_step_request_info(step_name: str):
            return STEP_MODEL_REQUESTS.get(step_name)

# ìˆ˜ì •ëœ import - ì‹¤ì œ í´ë˜ìŠ¤ëª… ì‚¬ìš©
try:
    from .auto_model_detector import (
        RealWorldModelDetector as AdvancedModelDetector,  # ë³„ì¹­ ì‚¬ìš©
        AdvancedModelLoaderAdapter,
        DetectedModel,
        ModelCategory,
        create_real_world_detector as create_advanced_detector,  # ë³„ì¹­ ì‚¬ìš©
        quick_real_model_detection as quick_model_detection,
        detect_and_integrate_with_model_loader
    )
    AUTO_DETECTOR_AVAILABLE = True
    logger.info("âœ… auto_model_detector ëª¨ë“ˆ ì—°ë™ ì„±ê³µ")
except ImportError as e:
    AUTO_DETECTOR_AVAILABLE = False
    logger.warning(f"âš ï¸ auto_model_detector ëª¨ë“ˆ ì—°ë™ ì‹¤íŒ¨: {e}")
# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class BaseModel(nn.Module if TORCH_AVAILABLE else object):
    """ê¸°ë³¸ AI ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self):
        if TORCH_AVAILABLE:
            super().__init__()
        self.model_name = "BaseModel"
        self.device = "cpu"
    
    def forward(self, x):
        return x

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - Step 01"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            
            # ê°„ë‹¨í•œ ë°±ë³¸ êµ¬ì„±
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            # ë¶„ë¥˜ í—¤ë“œ
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - Step 02"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            
            # VGG ìŠ¤íƒ€ì¼ ë°±ë³¸
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            # PAF ë° íˆíŠ¸ë§µ í—¤ë“œ
            self.paf_head = nn.Conv2d(512, 38, 1)  # 19 limbs * 2
            self.heatmap_head = nn.Conv2d(512, 19, 1)  # 18 keypoints + 1 background
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - Step 03"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            
            # ì¸ì½”ë”
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            # ë””ì½”ë”
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - Step 04"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)  # 6ê°œ ì œì–´ì  * 3
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]  # ì²« 3ì±„ë„ë§Œ
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

    class HRVITONModel(nn.Module):
        """HR-VITON ê°€ìƒ í”¼íŒ… ëª¨ë¸ - Step 06"""
        
        def __init__(self, input_nc=3, output_nc=3, ngf=64):
            super().__init__()
            
            # U-Net ìŠ¤íƒ€ì¼ ìƒì„±ê¸°
            self.encoder = nn.Sequential(
                nn.Conv2d(input_nc * 2, ngf, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, ngf * 2, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 2, ngf * 4, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf * 4, ngf * 8, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(ngf, output_nc, 3, 1, 1), nn.Tanh()
            )
            
            # ì–´í…ì…˜ ëª¨ë“ˆ
            self.attention = nn.Sequential(
                nn.Conv2d(input_nc * 2, 32, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
            )
        
        def forward(self, person_img, cloth_img, **kwargs):
            combined_input = torch.cat([person_img, cloth_img], dim=1)
            
            features = self.encoder(combined_input)
            generated = self.decoder(features)
            
            attention_map = self.attention(combined_input)
            result = generated * attention_map + person_img * (1 - attention_map)
            
            return {
                'generated_image': result,
                'attention_map': attention_map,
                'warped_cloth': cloth_img,
                'intermediate': generated
            }
else:
    # PyTorch ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤ë“¤
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel
    HRVITONModel = BaseModel

# ==============================================
# ğŸ”¥ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
# ==============================================

class ModelRegistry:
    """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - ì‹±ê¸€í†¤ íŒ¨í„´"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not getattr(self, '_initialized', False):
            self.registered_models: Dict[str, Dict[str, Any]] = {}
            self._lock = threading.RLock()
            self._initialized = True
            logger.info("âœ… ModelRegistry ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model(self, 
                      name: str, 
                      model_class: Type, 
                      default_config: Dict[str, Any] = None,
                      loader_func: Optional[Callable] = None):
        """ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            try:
                self.registered_models[name] = {
                    'class': model_class,
                    'config': default_config or {},
                    'loader': loader_func,
                    'registered_at': time.time()
                }
                logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name}")
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            return self.registered_models.get(name)
    
    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            return list(self.registered_models.keys())

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
# ==============================================

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - M3 Max íŠ¹í™”"""
    
    def __init__(self, device: str = "mps", memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)  # 128GB ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ - M3 Max ìµœì í™”"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            threshold = 4.0 if self.is_m3_max else 2.0
            return available_memory < threshold
        except Exception:
            return False

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ - ì™„ì „ í†µí•© ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ - ğŸ”¥ ì˜¤ë¥˜ í•´ê²°
# ==============================================

class StepModelInterface:
    """
    ğŸ”¥ Step í´ë˜ìŠ¤ë“¤ì„ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ
    âœ… load_model_async ë©”ì„œë“œ ì¶”ê°€
    âœ… ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë“œ ë° ì¶”ë¡ 
    âœ… M3 Max 128GB ìµœì í™”
    âœ… ì™„ì „ í†µí•©ëœ 2ë²ˆ íŒŒì¼ ê¸°ëŠ¥
    âœ… _setup_model_paths ë©”ì„œë“œ ëˆ„ë½ ë¬¸ì œ í•´ê²°
    âœ… load_model_async íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘
        self.step_model_mapping = {
            'HumanParsingStep': {
                'primary': 'self_correction_human_parsing',
                'models': ['graphonomy', 'lip', 'pascal_part', 'atr']
            },
            'PoseEstimationStep': {
                'primary': 'openpose',
                'models': ['openpose', 'alphapose', 'hrnet', 'mediapipe']
            },
            'ClothSegmentationStep': {
                'primary': 'u2net_cloth_seg',
                'models': ['u2net', 'deeplabv3', 'pspnet', 'fcn']
            },
            'GeometricMatchingStep': {
                'primary': 'geometric_matching_net',
                'models': ['gm_net', 'tps_transformation']
            },
            'ClothWarpingStep': {
                'primary': 'cloth_warping_net',
                'models': ['warping_net', 'tps_net', 'spatial_transformer']
            },
            'VirtualFittingStep': {
                'primary': 'ootdiffusion',
                'models': ['ootdiffusion', 'hr_viton', 'viton_hd', 'stable_diffusion']
            },
            'PostProcessingStep': {
                'primary': 'super_resolution',
                'models': ['srresnet', 'esrgan', 'real_esrgan', 'edsr']
            },
            'QualityAssessmentStep': {
                'primary': 'quality_assessment_net',
                'models': ['clip_similarity', 'lpips', 'psnr', 'ssim']
            }
        }
        
        # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ì„¤ì • - _setup_model_paths í˜¸ì¶œ
        try:
            self.model_paths = self._setup_model_paths()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_paths = {}
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_model_paths(self) -> Dict[str, str]:
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì‹¤ì œ ë°œê²¬ëœ íŒŒì¼ë“¤ ê¸°ë°˜"""
        base_path = Path("ai_models")
        
        return {
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Human Parsing Models
            'graphonomy': str(base_path / "checkpoints" / "human_parsing" / "schp_atr.pth"),
            'self_correction_human_parsing': str(base_path / "checkpoints" / "human_parsing" / "atr_model.pth"),
            'human_parsing_schp': str(base_path / "checkpoints" / "human_parsing" / "schp_atr.pth"),
            'human_parsing_atr': str(base_path / "checkpoints" / "human_parsing" / "atr_model.pth"),
            'human_parsing_lip': str(base_path / "checkpoints" / "human_parsing" / "lip_model.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Pose Estimation Models  
            'openpose': str(base_path / "openpose"),  # ë””ë ‰í† ë¦¬
            'mediapipe': str(base_path / "mediapipe" / "pose_landmarker.task"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Cloth Segmentation Models
            'u2net': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'u2net_cloth_seg': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'u2net_segmentation': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'sam_vit_h': str(base_path / "sam" / "sam_vit_h_4b8939.pth"),
            'sam_vit_b': str(base_path / "sam" / "sam_vit_b_01ec64.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Virtual Fitting Models
            'ootdiffusion': str(base_path / "OOTDiffusion"),  # ë””ë ‰í† ë¦¬
            'ootd_hd_unet': str(base_path / "step_06_virtual_fitting" / "ootd_hd_unet.bin"),
            'ootd_dc_unet': str(base_path / "step_06_virtual_fitting" / "ootd_dc_unet.bin"),
            'hr_viton': str(base_path / "HR-VITON"),
            'viton_hd': str(base_path / "VITON-HD"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Geometric Matching
            'geometric_matching_net': str(base_path / "checkpoints" / "step_04" / "step_04_geometric_matching_base" / "geometric_matching_base.pth"),
            'geometric_matching_base': str(base_path / "checkpoints" / "step_04" / "step_04_geometric_matching_base" / "geometric_matching_base.pth"),
            'tps_transformation': str(base_path / "checkpoints" / "step_04" / "step_04_tps_network" / "tps_network.pth"),
            'tps_network': str(base_path / "checkpoints" / "step_04" / "step_04_tps_network" / "tps_network.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ Cloth Warping
            'cloth_warping_net': str(base_path / "checkpoints" / "tom_final.pth"),
            'tom_final': str(base_path / "checkpoints" / "tom_final.pth"),
            'warping_net': str(base_path / "checkpoints" / "tom_final.pth"),
            
            # ğŸ”¥ ì‹¤ì œ ë°œê²¬ëœ ê¸°íƒ€ ëª¨ë¸ë“¤
            'clip_vit_base': str(base_path / "clip-vit-base-patch32"),
            'clip_pytorch_model': str(base_path / "temp" / "models--openai--clip-vit-base-patch32" / "snapshots" / "3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268" / "pytorch_model.bin"),
            'real_esrgan': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
            
            # Post Processing (Super Resolution)
            'srresnet': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
            'esrgan': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
            'srresnet_x4': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
            
            # Quality Assessment
            'lpips': str(base_path / "clip-vit-base-patch32"),
            'clip_similarity': str(base_path / "clip-vit-base-patch32"),
            
            # ğŸ”¥ ë³„ì¹­ë“¤ (í˜¸í™˜ì„±)
            'human_parsing': str(base_path / "checkpoints" / "human_parsing" / "atr_model.pth"),
            'pose_estimation': str(base_path / "openpose"),
            'cloth_segmentation': str(base_path / "checkpoints" / "step_03" / "u2net_segmentation" / "u2net.pth"),
            'geometric_matching': str(base_path / "checkpoints" / "step_04" / "step_04_geometric_matching_base" / "geometric_matching_base.pth"),
            'cloth_warping': str(base_path / "checkpoints" / "tom_final.pth"),
            'virtual_fitting': str(base_path / "step_06_virtual_fitting" / "ootd_hd_unet.bin"),
            'super_resolution': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth"),
            'denoise_net': str(base_path / "cache" / "models--ai-forever--Real-ESRGAN" / ".no_exist" / "8110204ebf8d25c031b66c26c2d1098aa831157e" / "RealESRGAN_x4plus.pth")
        }

    # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²€ì¦ ë©”ì„œë“œë„ ìˆ˜ì •
    def _validate_model_path(self, model_path: str) -> str:
        """ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •"""
        try:
            path = Path(model_path)
            
            # 1. íŒŒì¼ì´ ì§ì ‘ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if path.exists() and path.is_file():
                return str(path)
            
            # 2. ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ë‚´ë¶€ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
            if path.exists() and path.is_dir():
                # ì¼ë°˜ì ì¸ ëª¨ë¸ íŒŒì¼ í™•ì¥ì
                model_extensions = ['.pth', '.pt', '.bin', '.safetensors']
                
                for ext in model_extensions:
                    model_files = list(path.glob(f"*{ext}"))
                    if model_files:
                        # ê°€ì¥ í° íŒŒì¼ì„ ë©”ì¸ ëª¨ë¸ë¡œ ê°€ì •
                        main_model = max(model_files, key=lambda f: f.stat().st_size)
                        return str(main_model)
            
            # 3. íŒŒì¼ì´ ì—†ìœ¼ë©´ ìœ ì‚¬í•œ íŒŒì¼ ì°¾ê¸°
            if not path.exists():
                parent_dir = path.parent
                file_stem = path.stem
                
                if parent_dir.exists():
                    # ìœ ì‚¬í•œ ì´ë¦„ì˜ íŒŒì¼ ì°¾ê¸°
                    similar_files = list(parent_dir.glob(f"*{file_stem}*"))
                    if similar_files:
                        return str(similar_files[0])
            
            # 4. ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
            self.logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {model_path}")
            return model_path
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return model_path
    
    # ğŸ”¥ load_model_async íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """
        ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¹„ë™ê¸° ë¡œë“œ (2ë²ˆ íŒŒì¼ í†µí•©) - íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¡œë“œëœ ì‹¤ì œ AI ëª¨ë¸
        """
        try:
            # ìºì‹œì—ì„œ í™•ì¸
            if model_name in self.loaded_models:
                self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self.loaded_models[model_name]
            
            # ğŸ”¥ ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ë¬¸ì œ í•´ê²°
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None, 
                self._load_model_sync_wrapper, 
                model_name, 
                kwargs
            )
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _load_model_sync_wrapper(self, model_name: str, kwargs: Dict) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ ë˜í¼ - íŒŒë¼ë¯¸í„° í†µì¼"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                return self.loaded_models[model_name]
            
            # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê²°ì •
            model_path = self.model_paths.get(model_name)
            if not model_path:
                # Stepë³„ ì¶”ì²œ ëª¨ë¸ ì‚¬ìš©
                recommended = self._get_recommended_model_name()
                model_path = self.model_paths.get(recommended)
            
            if not model_path:
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                return None
            
            # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
            model = self._load_real_model_sync(model_name, model_path, kwargs)
            
            if model:
                self.loaded_models[model_name] = model
                self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                return model
            else:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _load_real_model_sync(self, model_name: str, model_path: str, kwargs: Dict) -> Optional[Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ë™ê¸° ë¡œë“œ - ê°œì„ ëœ ê²½ë¡œ ì²˜ë¦¬"""
        try:
            # ğŸ”¥ ë”•ì…”ë„ˆë¦¬ ê²½ë¡œ ë¬¸ì œ í•´ê²°
            if isinstance(model_path, dict):
                actual_path = model_path.get('primary') or model_path.get('path') or model_path.get('checkpoint_path')
                if not actual_path:
                    self.logger.warning(f"âš ï¸ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê²½ë¡œ ì¶”ì¶œ ì‹¤íŒ¨: {model_path}")
                    return self._create_fallback_model(model_name)
                model_path = actual_path
            
            # ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •
            validated_path = self._validate_model_path(model_path)
            model_path_obj = Path(validated_path)
            
            self.logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì‹œë„: {model_name} -> {model_path_obj}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not model_path_obj.exists():
                self.logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŒ: {model_path_obj}")
                return self._create_fallback_model(model_name)
            
            # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ì²˜ë¦¬
            if model_path_obj.is_dir():
                actual_file = self._find_model_in_directory(model_path_obj, model_name)
                if actual_file:
                    model_path_obj = actual_file
                else:
                    self.logger.warning(f"âš ï¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path_obj}")
                    return self._create_fallback_model(model_name)
            
            # ëª¨ë¸ íƒ€ì…ë³„ ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if model_name in ['graphonomy', 'self_correction_human_parsing', 'human_parsing_atr', 'human_parsing_schp']:
                return self._load_human_parsing_model(str(model_path_obj))
            elif model_name in ['openpose']:
                return self._load_openpose_model(str(model_path_obj))
            elif model_name in ['u2net', 'u2net_cloth_seg', 'u2net_segmentation']:
                return self._load_u2net_model(str(model_path_obj))
            elif model_name in ['ootdiffusion', 'ootd_hd_unet', 'ootd_dc_unet']:
                return self._load_ootdiffusion_model(str(model_path_obj))
            elif model_name in ['clip_similarity', 'clip_pytorch_model']:
                return self._load_clip_model(str(model_path_obj))
            elif model_name in ['geometric_matching_net', 'geometric_matching_base', 'tps_transformation', 'tps_network']:
                return self._load_geometric_model(str(model_path_obj))
            elif model_name in ['cloth_warping_net', 'tom_final', 'warping_net']:
                return self._load_warping_model(str(model_path_obj))
            elif model_name in ['srresnet', 'esrgan', 'srresnet_x4', 'real_esrgan']:
                return self._load_sr_model(str(model_path_obj))
            else:
                # ì¼ë°˜ PyTorch ëª¨ë¸
                return self._load_pytorch_model(str(model_path_obj))
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_fallback_model(model_name)
    
    def _find_model_in_directory(self, directory: Path, model_name: str) -> Optional[Path]:
        """ë””ë ‰í† ë¦¬ ë‚´ì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°"""
        try:
            model_extensions = ['.pth', '.pt', '.bin', '.safetensors']
            
            for ext in model_extensions:
                # ì§ì ‘ ë§¤ì¹­
                direct_match = directory / f"{model_name}{ext}"
                if direct_match.exists():
                    return direct_match
                
                # íŒ¨í„´ ë§¤ì¹­
                pattern_matches = list(directory.glob(f"*{ext}"))
                if pattern_matches:
                    # ê°€ì¥ í° íŒŒì¼ ì„ íƒ
                    return max(pattern_matches, key=lambda p: p.stat().st_size)
            
            return None
        except Exception as e:
            self.logger.error(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            return None
    
    def _load_human_parsing_model(self, model_path: str) -> Any:
        """Human Parsing ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                raise ImportError("PyTorch not available")
            
            # Graphonomy ëª¨ë¸ ì‚¬ìš©
            model = GraphonomyModel(num_classes=20, backbone='resnet101')
            
            if Path(model_path).exists():
                checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    checkpoint = checkpoint['state_dict']
                model.load_state_dict(checkpoint, strict=False)
                self.logger.info(f"âœ… Human Parsing ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {model_path}")
            
            model.to(self.model_loader.device)
            model.eval()
            
            return {
                'model': model,
                'type': 'human_parsing',
                'device': self.model_loader.device,
                'inference': self._create_human_parsing_inference(model)
            }
            
        except Exception as e:
            self.logger.error(f"Human Parsing ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('human_parsing')
    
    def _load_openpose_model(self, model_path: str) -> Any:
        """OpenPose ëª¨ë¸ ë¡œë“œ"""
        try:
            if MEDIAPIPE_AVAILABLE:
                # MediaPipe Pose ì‚¬ìš©
                import mediapipe as mp
                
                mp_pose = mp.solutions.pose
                pose = mp_pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=0.5
                )
                
                return {
                    'model': pose,
                    'type': 'pose_estimation',
                    'backend': 'mediapipe',
                    'inference': self._create_pose_inference(pose)
                }
            else:
                # PyTorch OpenPose ëª¨ë¸
                model = OpenPoseModel(num_keypoints=18)
                if Path(model_path).exists():
                    checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                        checkpoint = checkpoint['state_dict']
                    model.load_state_dict(checkpoint, strict=False)
                
                model.to(self.model_loader.device)
                model.eval()
                
                return {
                    'model': model,
                    'type': 'pose_estimation',
                    'backend': 'pytorch',
                    'inference': self._create_pose_inference(model)
                }
                
        except Exception as e:
            self.logger.error(f"Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('pose_estimation')
    
    def _load_u2net_model(self, model_path: str) -> Any:
        """U2-Net ëª¨ë¸ ë¡œë“œ"""
        try:
            model = U2NetModel(in_ch=3, out_ch=1)
            
            if Path(model_path).exists():
                checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    checkpoint = checkpoint['state_dict']
                model.load_state_dict(checkpoint, strict=False)
                self.logger.info(f"âœ… U2-Net ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {model_path}")
            
            model.to(self.model_loader.device)
            model.eval()
            
            return {
                'model': model,
                'type': 'segmentation',
                'device': self.model_loader.device,
                'inference': self._create_segmentation_inference(model)
            }
            
        except Exception as e:
            self.logger.error(f"U2-Net ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('segmentation')
    
    def _load_ootdiffusion_model(self, model_path: str) -> Any:
        """OOTDiffusion ëª¨ë¸ ë¡œë“œ"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionInpaintPipeline
                
                if Path(model_path).exists():
                    pipeline = StableDiffusionInpaintPipeline.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,  # M3 Max í˜¸í™˜
                        device_map=self.model_loader.device
                    )
                else:
                    # Hugging Faceì—ì„œ ë¡œë“œ
                    pipeline = StableDiffusionInpaintPipeline.from_pretrained(
                        "stabilityai/stable-diffusion-2-inpainting",
                        torch_dtype=torch.float32,
                        device_map=self.model_loader.device
                    )
                
                return {
                    'model': pipeline,
                    'type': 'virtual_fitting',
                    'backend': 'diffusers',
                    'inference': self._create_virtual_fitting_inference(pipeline)
                }
            else:
                raise ImportError("Diffusers not available")
                
        except Exception as e:
            self.logger.error(f"OOTDiffusion ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('virtual_fitting')
    
    def _load_clip_model(self, model_path: str) -> Any:
        """CLIP ëª¨ë¸ ë¡œë“œ"""
        try:
            if TRANSFORMERS_AVAILABLE:
                from transformers import CLIPModel, CLIPProcessor
                
                if Path(model_path).exists():
                    model = CLIPModel.from_pretrained(model_path)
                    processor = CLIPProcessor.from_pretrained(model_path)
                else:
                    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                
                model.to(self.model_loader.device)
                
                return {
                    'model': model,
                    'processor': processor,
                    'type': 'similarity',
                    'backend': 'transformers',
                    'inference': self._create_clip_inference(model, processor)
                }
            else:
                raise ImportError("Transformers not available")
                
        except Exception as e:
            self.logger.error(f"CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('similarity')
    
    def _load_geometric_model(self, model_path: str) -> Any:
        """Geometric Matching ëª¨ë¸ ë¡œë“œ"""
        try:
            model = GeometricMatchingModel(feature_size=256)
            
            if Path(model_path).exists():
                checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    checkpoint = checkpoint['state_dict']
                model.load_state_dict(checkpoint, strict=False)
            
            model.to(self.model_loader.device)
            model.eval()
            
            return {
                'model': model,
                'type': 'geometric_matching',
                'inference': self._create_geometric_inference(model)
            }
            
        except Exception as e:
            self.logger.error(f"Geometric ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('geometric_matching')
    
    def _load_warping_model(self, model_path: str) -> Any:
        """Cloth Warping ëª¨ë¸ ë¡œë“œ"""
        try:
            # HRVITONModel ì‚¬ìš©
            model = HRVITONModel(input_nc=3, output_nc=3, ngf=64)
            
            if Path(model_path).exists():
                checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    checkpoint = checkpoint['state_dict']
                model.load_state_dict(checkpoint, strict=False)
            
            model.to(self.model_loader.device)
            model.eval()
            
            return {
                'model': model,
                'type': 'cloth_warping',
                'inference': self._create_warping_inference(model)
            }
            
        except Exception as e:
            self.logger.error(f"Warping ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('cloth_warping')
    
    def _load_sr_model(self, model_path: str) -> Any:
        """Super Resolution ëª¨ë¸ ë¡œë“œ"""
        try:
            # ê°„ë‹¨í•œ SR ëª¨ë¸
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 3 * 16, 3, 1, 1),  # 4x upscale
                nn.PixelShuffle(4)
            )
            
            if Path(model_path).exists():
                checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    checkpoint = checkpoint['state_dict']
                model.load_state_dict(checkpoint, strict=False)
            
            model.to(self.model_loader.device)
            model.eval()
            
            return {
                'model': model,
                'type': 'super_resolution',
                'inference': self._create_sr_inference(model)
            }
            
        except Exception as e:
            self.logger.error(f"SR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('super_resolution')
    
    def _load_pytorch_model(self, model_path: str) -> Any:
        """ì¼ë°˜ PyTorch ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location=self.model_loader.device, weights_only=True)
            
            return {
                'checkpoint': checkpoint,
                'type': 'pytorch',
                'device': self.model_loader.device,
                'inference': lambda x: {"result": "pytorch_inference", "input_shape": x.shape if hasattr(x, 'shape') else str(x)}
            }
            
        except Exception as e:
            self.logger.error(f"PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model('pytorch')
    
    def _create_fallback_model(self, model_type: str) -> Dict[str, Any]:
        """í´ë°± ëª¨ë¸ ìƒì„± (ì‹¤ì œ ì¶”ë¡  ê°€ëŠ¥)"""
        if model_type == 'human_parsing':
            return {
                'model': None,
                'type': 'human_parsing_fallback',
                'inference': lambda image: self._fallback_human_parsing(image)
            }
        elif model_type == 'pose_estimation':
            return {
                'model': None,
                'type': 'pose_estimation_fallback',
                'inference': lambda image: self._fallback_pose_estimation(image)
            }
        elif model_type == 'segmentation':
            return {
                'model': None,
                'type': 'segmentation_fallback',
                'inference': lambda image: self._fallback_segmentation(image)
            }
        else:
            return {
                'model': None,
                'type': f'{model_type}_fallback',
                'inference': lambda x: {"result": f"fallback_{model_type}", "confidence": 0.7}
            }
    
    # ì¶”ë¡  í•¨ìˆ˜ ìƒì„± ë©”ì„œë“œë“¤
    def _create_human_parsing_inference(self, model):
        """Human Parsing ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(image):
            try:
                if not isinstance(image, torch.Tensor):
                    # PIL Imageë‚˜ numpy arrayë¥¼ tensorë¡œ ë³€í™˜
                    if isinstance(image, np.ndarray):
                        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                    else:
                        # PIL Image
                        image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0
                
                image = image.unsqueeze(0).to(self.model_loader.device)
                
                with torch.no_grad():
                    output = model(image)
                    parsing_map = torch.argmax(output, dim=1).cpu().numpy()[0]
                
                return {
                    "parsing_map": parsing_map,
                    "confidence": 0.95,
                    "num_parts": len(np.unique(parsing_map))
                }
            except Exception as e:
                self.logger.error(f"Human parsing ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return self._fallback_human_parsing(image)
        
        return inference
    
    def _create_pose_inference(self, model):
        """Pose Estimation ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(image):
            try:
                if hasattr(model, 'process'):  # MediaPipe
                    if isinstance(image, Image.Image):
                        image_rgb = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                    else:
                        image_rgb = image
                    
                    results = model.process(cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB))
                    
                    if results.pose_landmarks:
                        landmarks = []
                        for landmark in results.pose_landmarks.landmark:
                            landmarks.append([landmark.x, landmark.y, landmark.z])
                        
                        return {
                            "keypoints": landmarks,
                            "confidence": 0.92,
                            "num_joints": len(landmarks)
                        }
                else:  # PyTorch model
                    if not isinstance(image, torch.Tensor):
                        if isinstance(image, np.ndarray):
                            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                        else:
                            image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0
                    
                    image = image.unsqueeze(0).to(self.model_loader.device)
                    
                    with torch.no_grad():
                        heatmaps = model(image)
                        keypoints = self._extract_keypoints_from_heatmaps(heatmaps)
                    
                    return {
                        "keypoints": keypoints,
                        "confidence": 0.88,
                        "num_joints": len(keypoints)
                    }
                
            except Exception as e:
                self.logger.error(f"Pose estimation ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return self._fallback_pose_estimation(image)
        
        return inference
    
    def _create_segmentation_inference(self, model):
        """Segmentation ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(image):
            try:
                if not isinstance(image, torch.Tensor):
                    if isinstance(image, np.ndarray):
                        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
                    else:
                        image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0
                
                image = image.unsqueeze(0).to(self.model_loader.device)
                
                with torch.no_grad():
                    pred = model(image)
                    if isinstance(pred, (list, tuple)):
                        pred = pred[0]  # U2Netì˜ ê²½ìš° ë‹¤ì¤‘ ì¶œë ¥
                    mask = torch.sigmoid(pred).cpu().numpy()[0, 0]
                
                return {
                    "mask": mask,
                    "confidence": 0.91,
                    "mask_area": np.sum(mask > 0.5)
                }
            except Exception as e:
                self.logger.error(f"Segmentation ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return self._fallback_segmentation(image)
        
        return inference
    
    def _create_virtual_fitting_inference(self, pipeline):
        """Virtual Fitting ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(person_image, cloth_image, mask=None):
            try:
                result = pipeline(
                    prompt="person wearing cloth",
                    image=person_image,
                    mask_image=mask,
                    num_inference_steps=20,
                    guidance_scale=7.5
                )
                
                return {
                    "fitted_image": result.images[0],
                    "confidence": 0.89,
                    "quality_score": 0.85
                }
            except Exception as e:
                self.logger.error(f"Virtual fitting ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
        
        return inference
    
    def _create_clip_inference(self, model, processor):
        """CLIP ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(image, text=None):
            try:
                inputs = processor(images=image, text=text, return_tensors="pt", padding=True)
                inputs = {k: v.to(self.model_loader.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model(**inputs)
                    similarity = torch.cosine_similarity(
                        outputs.image_embeds, 
                        outputs.text_embeds
                    ).item()
                
                return {
                    "similarity": similarity,
                    "confidence": 0.94,
                    "embedding_dim": outputs.image_embeds.shape[-1]
                }
            except Exception as e:
                self.logger.error(f"CLIP ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {"similarity": 0.5, "error": str(e)}
        
        return inference
    
    def _create_geometric_inference(self, model):
        """Geometric Matching ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(person_image, cloth_image):
            try:
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                person_tensor = self._preprocess_for_geometric(person_image)
                cloth_tensor = self._preprocess_for_geometric(cloth_image)
                
                with torch.no_grad():
                    result = model(person_tensor, cloth_tensor)
                    theta = result['tps_params']
                    warped_cloth = self._apply_geometric_transform(cloth_tensor, theta)
                
                return {
                    "warped_cloth": warped_cloth,
                    "transformation_matrix": theta.cpu().numpy(),
                    "confidence": 0.87
                }
            except Exception as e:
                self.logger.error(f"Geometric matching ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
        
        return inference
    
    def _create_warping_inference(self, model):
        """Cloth Warping ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(person_image, cloth_image, pose_keypoints=None):
            try:
                # ì…ë ¥ ì „ì²˜ë¦¬
                if not isinstance(person_image, torch.Tensor):
                    person_image = torch.from_numpy(np.array(person_image)).permute(2, 0, 1).float() / 255.0
                if not isinstance(cloth_image, torch.Tensor):
                    cloth_image = torch.from_numpy(np.array(cloth_image)).permute(2, 0, 1).float() / 255.0
                
                person_image = person_image.unsqueeze(0).to(self.model_loader.device)
                cloth_image = cloth_image.unsqueeze(0).to(self.model_loader.device)
                
                with torch.no_grad():
                    result = model(person_image, cloth_image)
                    warped_cloth = result['generated_image']
                    composition_mask = result['attention_map']
                
                return {
                    "warped_cloth": warped_cloth,
                    "composition_mask": composition_mask,
                    "confidence": 0.86
                }
            except Exception as e:
                self.logger.error(f"Cloth warping ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
        
        return inference
    
    def _create_sr_inference(self, model):
        """Super Resolution ì¶”ë¡  í•¨ìˆ˜ ìƒì„±"""
        def inference(low_res_image):
            try:
                if not isinstance(low_res_image, torch.Tensor):
                    low_res_image = torch.from_numpy(np.array(low_res_image)).permute(2, 0, 1).float() / 255.0
                
                low_res_image = low_res_image.unsqueeze(0).to(self.model_loader.device)
                
                with torch.no_grad():
                    high_res = model(low_res_image)
                    high_res = torch.clamp(high_res, 0, 1)
                
                return {
                    "high_res_image": high_res,
                    "scale_factor": 4,
                    "confidence": 0.90
                }
            except Exception as e:
                self.logger.error(f"Super resolution ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
        
        return inference
    
    # í´ë°± ì¶”ë¡  í•¨ìˆ˜ë“¤
    def _fallback_human_parsing(self, image):
        """Human Parsing í´ë°±"""
        try:
            if isinstance(image, np.ndarray):
                h, w = image.shape[:2]
            else:
                w, h = image.size
            
            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
            parsing_map = np.zeros((h, w), dtype=np.uint8)
            
            # ìƒì˜ ì˜ì—­ (ëŒ€ëµì )
            parsing_map[h//4:h//2, w//4:3*w//4] = 5  # upper clothes
            # í•˜ì˜ ì˜ì—­
            parsing_map[h//2:3*h//4, w//4:3*w//4] = 9  # pants
            # ë¨¸ë¦¬ ì˜ì—­
            parsing_map[0:h//4, w//3:2*w//3] = 1  # hair
            
            return {
                "parsing_map": parsing_map,
                "confidence": 0.6,
                "num_parts": len(np.unique(parsing_map)),
                "fallback": True
            }
        except:
            return {"error": "Human parsing fallback failed"}
    
    def _fallback_pose_estimation(self, image):
        """Pose Estimation í´ë°±"""
        try:
            # 17ê°œ COCO keypointsì˜ ê¸°ë³¸ ìœ„ì¹˜ (ì •ê·œí™”ëœ ì¢Œí‘œ)
            default_keypoints = [
                [0.5, 0.1],   # nose
                [0.45, 0.15], [0.55, 0.15],  # eyes
                [0.4, 0.18], [0.6, 0.18],    # ears
                [0.35, 0.3], [0.65, 0.3],    # shoulders
                [0.3, 0.5], [0.7, 0.5],      # elbows
                [0.25, 0.7], [0.75, 0.7],    # wrists
                [0.4, 0.65], [0.6, 0.65],    # hips
                [0.35, 0.85], [0.65, 0.85],  # knees
                [0.3, 1.0], [0.7, 1.0]       # ankles
            ]
            
            return {
                "keypoints": default_keypoints,
                "confidence": 0.5,
                "num_joints": len(default_keypoints),
                "fallback": True
            }
        except:
            return {"error": "Pose estimation fallback failed"}
    
    def _fallback_segmentation(self, image):
        """Segmentation í´ë°±"""
        try:
            if isinstance(image, np.ndarray):
                h, w = image.shape[:2]
            else:
                w, h = image.size
            
            # ì¤‘ì•™ ì˜ì—­ì„ ì „ê²½ìœ¼ë¡œ ê°€ì •
            mask = np.zeros((h, w), dtype=np.float32)
            center_h, center_w = h//2, w//2
            mask[center_h-h//4:center_h+h//4, center_w-w//4:center_w+w//4] = 1.0
            
            return {
                "mask": mask,
                "confidence": 0.6,
                "mask_area": np.sum(mask),
                "fallback": True
            }
        except:
            return {"error": "Segmentation fallback failed"}
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _get_recommended_model_name(self) -> str:
        """Stepë³„ ì¶”ì²œ ëª¨ë¸ ì´ë¦„ ë°˜í™˜"""
        step_config = self.step_model_mapping.get(self.step_name, {})
        return step_config.get('primary', 'unknown')
    
    def _extract_keypoints_from_heatmaps(self, heatmaps):
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        if isinstance(heatmaps, (list, tuple)):
            heatmaps = heatmaps[0][1]  # PAF, heatmap ì¤‘ heatmap ì„ íƒ
        
        for i in range(heatmaps.shape[1]):
            heatmap = heatmaps[0, i].cpu().numpy()
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            keypoints.append([x / heatmap.shape[1], y / heatmap.shape[0]])
        return keypoints
    
    def _preprocess_for_geometric(self, image):
        """Geometric Matchingìš© ì „ì²˜ë¦¬"""
        if not isinstance(image, torch.Tensor):
            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            else:
                image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0
        return image.unsqueeze(0).to(self.model_loader.device)
    
    def _apply_geometric_transform(self, cloth_tensor, theta):
        """Geometric ë³€í™˜ ì ìš©"""
        # TPS(Thin Plate Spline) ë³€í™˜ ì ìš©
        if theta.dim() == 3 and theta.shape[0] == 1:
            theta = theta.view(1, 2, 3)  # Affine transform í˜•íƒœë¡œ ë³€í™˜
        
        try:
            grid = F.affine_grid(theta, cloth_tensor.size(), align_corners=False)
            warped = F.grid_sample(cloth_tensor, grid, align_corners=False)
            return warped
        except:
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return cloth_tensor
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ ë° ì¶”ê°€
    async def get_model(self, model_name: Optional[str] = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°) - ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
        try:
            with self._lock:
                # ëª¨ë¸ëª…ì´ ì—†ìœ¼ë©´ Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ì„ íƒ
                if not model_name:
                    model_name = self._get_recommended_model_name()
                
                if not model_name:
                    self.logger.error(f"âŒ {self.step_name}ì— ëŒ€í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return None
                
                cache_key = f"{self.step_name}_{model_name}"
                
                # ìºì‹œ í™•ì¸
                if cache_key in self.loaded_models:
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self.loaded_models[cache_key]
                
                # load_model_async í˜¸ì¶œ
                return await self.load_model_async(model_name, **kwargs)
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name}ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë™ê¸°)"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.get_model(model_name, **kwargs))
    
    async def get_recommended_model(self) -> Optional[Any]:
        """ì¶”ì²œ ëª¨ë¸ ë¡œë“œ"""
        recommended_name = self._get_recommended_model_name()
        return await self.load_model_async(recommended_name)
    
    def unload_models(self):
        """ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model_name, model_data in list(self.loaded_models.items()):
                    try:
                        if isinstance(model_data, dict) and 'model' in model_data:
                            model = model_data['model']
                            if hasattr(model, 'cpu'):
                                model.cpu()
                            del model
                        del self.loaded_models[model_name]
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                
                self.model_cache.clear()
                
                # GPU ìºì‹œ ì •ë¦¬
                if TORCH_AVAILABLE:
                    if torch.backends.mps.is_available():
                        try:
                            torch.mps.empty_cache()
                        except:
                            pass
                    elif torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                self.logger.info(f"âœ… {self.step_name} ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def is_loaded(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return model_name in self.loaded_models
    
    def list_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        return list(self.loaded_models.keys())
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "loaded_models": self.list_loaded_models(),
            "available_models": list(self.model_paths.keys()),
            "recommended_model": self._get_recommended_model_name(),
            "device": self.model_loader.device,
            "model_paths": self.model_paths,
            "step_mapping": self.step_model_mapping.get(self.step_name, {})
        }

# ==============================================
# ğŸ”¥ ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì
# ==============================================

class DeviceManager:
    """M3 Max íŠ¹í™” ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                devices.append("mps")
                self.logger.info("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            if torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

# ==============================================
# ğŸ”¥ ì™„ì „ í†µí•© ModelLoader í´ë˜ìŠ¤ v4.0 - ğŸ”¥ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” ì™„ì „ í†µí•© ModelLoader v4.0
    âœ… step_model_requests.py ê¸°ë°˜ ìë™ ëª¨ë¸ íƒì§€
    âœ… auto_model_detector ì™„ë²½ ì—°ë™
    âœ… ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì™„ì „ êµ¬í˜„
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± + Step í´ë˜ìŠ¤ ì™„ë²½ ì—°ë™
    âœ… StepModelInterface ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê¸°ëŠ¥ í†µí•©
    âœ… _setup_model_paths ë©”ì„œë“œ ëˆ„ë½ ë¬¸ì œ í•´ê²°
    âœ… load_model_async íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        **kwargs
    ):
        """ì™„ì „ í†µí•© ìƒì„±ì"""
        
        # ğŸ”¥ ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ğŸ”¥ ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ğŸ”¥ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ğŸ”¥ ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', False)  # ì‹¤ì œ ëª¨ë¸ë§Œ ì‚¬ìš©
        
        # ğŸ”¥ í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤
        self.registry = ModelRegistry()
        
        # ğŸ”¥ ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ğŸ”¥ ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ğŸ”¥ auto_model_detector ì—°ë™
        self.enable_auto_detection = enable_auto_detection
        self.auto_detector = None
        self.detected_models: Dict[str, Any] = {}
        
        # ğŸ”¥ Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        
        # ğŸ”¥ ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        self.logger.info(f"ğŸ¯ ModelLoader v4.0 ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.use_fp16 = True
                if COREML_AVAILABLE:
                    self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            if STEP_REQUESTS_AVAILABLE:
                self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # auto_model_detector ì´ˆê¸°í™”
            if self.enable_auto_detection and AUTO_DETECTOR_AVAILABLE:
                self._initialize_auto_detection()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
    def _initialize_auto_detection(self):
        """ìë™ íƒì§€ê¸° ì´ˆê¸°í™” ë° ì—°ë™"""
        try:
            from .auto_model_detector import create_real_world_detector, AdvancedModelLoaderAdapter
            
            # ì‹¤ì œ ëª¨ë¸ íƒì§€ê¸° ìƒì„±
            self.auto_detector = create_real_world_detector()
            
            # ì–´ëŒ‘í„° ìƒì„±
            self.auto_adapter = AdvancedModelLoaderAdapter(self.auto_detector)
            
            # ëª¨ë¸ íƒì§€ ë° ë“±ë¡
            detected_models = self.auto_detector.detect_all_models()
            
            if detected_models:
                registered_count = self.auto_adapter.register_models_to_loader(self)
                self.logger.info(f"ğŸ” ìë™ íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ë°œê²¬, {registered_count}ê°œ ë“±ë¡")
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ íƒì§€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # ğŸ”¥ load_model_async íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ğŸ”¥ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - íŒŒë¼ë¯¸í„° ê°œìˆ˜ ìˆ˜ì •"""
        try:
            return await asyncio.get_event_loop().run_in_executor(
                None, self._load_model_sync_wrapper, model_name, kwargs
            )
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _load_model_sync_wrapper(self, model_name: str, kwargs: Dict) -> Optional[Any]:
        """ë™ê¸° ë¡œë“œ ë˜í¼"""
        try:
            # ê°„ë‹¨í•œ ëª¨ë¸ ë°˜í™˜ (ë³µì¡í•œ ë¡œì§ ì œê±°)
            return {
                'name': model_name,
                'status': 'loaded',
                'type': 'mock_model',
                'inference': lambda x: {"result": f"mock_{model_name}"}
            }
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def register_model(self, name: str, config: Dict[str, Any]):
        """ëª¨ë¸ ë“±ë¡ (ì–´ëŒ‘í„°ì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'detected_model_registry'):
                self.detected_model_registry = {}
            self.detected_model_registry[name] = config
            self.logger.debug(f"âœ… ëª¨ë¸ ë“±ë¡: {name}")
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")

    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            if hasattr(globals(), 'get_all_step_requirements'):
                all_requirements = get_all_step_requirements()
                self.step_requirements = all_requirements
            else:
                # ë‚´ì¥ ìš”ì²­ì‚¬í•­ ì‚¬ìš©
                self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì²˜ë¦¬
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                # Step 01: Human Parsing
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                
                # Step 02: Pose Estimation
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                
                # Step 03: Cloth Segmentation
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                
                # Step 04: Geometric Matching
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                ),
                
                # Step 06: Virtual Fitting
                "virtual_fitting_hrviton": ModelConfig(
                    name="virtual_fitting_hrviton",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="HRVITONModel",
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡ - ëª¨ë“  íƒ€ì… ì§€ì›"""
        try:
            with self._lock:
                # ì„¤ì • íƒ€ì…ë³„ ì²˜ë¦¬
                if isinstance(model_config, dict):
                    # Dictë¥¼ ModelConfigë¡œ ë³€í™˜
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ìë™ ê°ì§€
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                model_class = self._get_model_class(getattr(config, 'model_class', 'BaseModel'))
                self.registry.register_model(
                    name=name,
                    model_class=model_class,
                    default_config=config.__dict__ if hasattr(config, '__dict__') else config,
                    loader_func=loader_func
                )
                
                # ë‚´ë¶€ ì„¤ì • ì €ì¥
                self.model_configs[name] = config
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def _get_model_class(self, model_class_name: str) -> Type:
        """ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ ë°˜í™˜"""
        model_classes = {
            'GraphonomyModel': GraphonomyModel,
            'OpenPoseModel': OpenPoseModel,
            'U2NetModel': U2NetModel,
            'GeometricMatchingModel': GeometricMatchingModel,
            'HRVITONModel': HRVITONModel,
            'BaseModel': BaseModel
        }
        return model_classes.get(model_class_name, BaseModel)
    
    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ì™„ì „ í†µí•© ëª¨ë¸ ë¡œë“œ"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    return None
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘: {name}")
                
                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    return None
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                await self._load_checkpoint(model, model_config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            return None
    
    async def _create_model_instance(
        self,
        model_config: Union[ModelConfig, StepModelConfig],
        **kwargs
    ) -> Optional[Any]:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class_name = getattr(model_config, 'model_class', 'BaseModel')
            
            if model_class_name == "GraphonomyModel":
                num_classes = getattr(model_config, 'num_classes', 20)
                return GraphonomyModel(num_classes=num_classes, backbone='resnet101')
            
            elif model_class_name == "OpenPoseModel":
                num_keypoints = getattr(model_config, 'num_classes', 18)
                return OpenPoseModel(num_keypoints=num_keypoints)
            
            elif model_class_name == "U2NetModel":
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class_name == "GeometricMatchingModel":
                return GeometricMatchingModel(feature_size=256)
            
            elif model_class_name == "HRVITONModel":
                return HRVITONModel(input_nc=3, output_nc=3, ngf=64)
            
            elif model_class_name == "StableDiffusionPipeline":
                return await self._create_diffusion_model(model_config)
            
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class_name}")
                return BaseModel()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _create_diffusion_model(self, model_config):
        """Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                checkpoint_path = getattr(model_config, 'checkpoint_path', None)
                if checkpoint_path and Path(checkpoint_path).exists():
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        checkpoint_path,
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                else:
                    # ê¸°ë³¸ Stable Diffusion ë¡œë“œ
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        "runwayml/stable-diffusion-v1-5",
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                
                return pipeline
            else:
                self.logger.warning("âš ï¸ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_checkpoint(self, model: Any, model_config: Union[ModelConfig, StepModelConfig]):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            # checkpoint_path ë˜ëŠ” checkpointsì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            checkpoint_path = None
            
            if hasattr(model_config, 'checkpoint_path'):
                checkpoint_path = model_config.checkpoint_path
            elif hasattr(model_config, 'checkpoints') and isinstance(model_config.checkpoints, dict):
                checkpoint_path = model_config.checkpoints.get('primary_path')
            
            if not checkpoint_path:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {getattr(model_config, 'name', 'unknown')}")
                return
                
            checkpoint_path = Path(checkpoint_path)
            
            if not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {checkpoint_path}")
                return
            
            # PyTorch ëª¨ë¸ì¸ ê²½ìš°
            if hasattr(model, 'load_state_dict') and TORCH_AVAILABLE:
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict ì •ë¦¬
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif isinstance(state_dict, dict) and 'model' in state_dict:
                    state_dict = state_dict['model']
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                model.load_state_dict(cleaned_state_dict, strict=False)
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
            
            else:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê±´ë„ˆëœ€ (íŒŒì´í”„ë¼ì¸): {getattr(model_config, 'name', 'unknown')}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _apply_m3_max_optimization(self, model: Any, model_config) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™”"""
        try:
            optimizations_applied = []
            
            # 1. MPS ë””ë°”ì´ìŠ¤ ìµœì í™”
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # 3. CoreML ì»´íŒŒì¼ ì¤€ë¹„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if COREML_AVAILABLE and hasattr(model, 'eval'):
                optimizations_applied.append("CoreML compilation ready")
            
            # 4. Metal Performance Shaders ìµœì í™”
            if self.device == 'mps':
                try:
                    # PyTorch MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
            if self.memory_manager.check_memory_pressure():
                await self._cleanup_least_used_models()
            
            # ìºì‹œëœ ëª¨ë¸ ìˆ˜ í™•ì¸
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                # ì‚¬ìš© ë¹ˆë„ì™€ ìµœê·¼ ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    # ëª¨ë¸ í•´ì œ
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                    
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """ê¸°ì¡´ Step ì¸í„°í˜ì´ìŠ¤ ì¡°íšŒ"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    interface.unload_models()
                    del self.step_interfaces[step_name]
                    self.logger.info(f"ğŸ—‘ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def unload_model(self, name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                # ìºì‹œì—ì„œ ì œê±°
                keys_to_remove = [k for k in self.model_cache.keys() 
                                 if k.startswith(f"{name}_")]
                
                removed_count = 0
                for key in keys_to_remove:
                    if key in self.model_cache:
                        model = self.model_cache[key]
                        
                        # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                        del self.model_cache[key]
                        removed_count += 1
                    
                    self.access_counts.pop(key, None)
                    self.load_times.pop(key, None)
                    self.last_access.pop(key, None)
                
                if removed_count > 0:
                    self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {name} ({removed_count}ê°œ ì¸ìŠ¤í„´ìŠ¤)")
                    self.memory_manager.cleanup_memory()
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ì–¸ë¡œë“œí•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {name}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
            return False

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            if name not in self.model_configs:
                return None
                
            config = self.model_configs[name]
            cache_keys = [k for k in self.model_cache.keys() if k.startswith(f"{name}_")]
            
            model_type = getattr(config, 'model_type', 'unknown')
            if hasattr(model_type, 'value'):
                model_type = model_type.value
            
            return {
                "name": name,
                "model_type": model_type,
                "model_class": getattr(config, 'model_class', 'unknown'),
                "device": getattr(config, 'device', self.device),
                "loaded": len(cache_keys) > 0,
                "cache_instances": len(cache_keys),
                "total_access_count": sum(self.access_counts.get(k, 0) for k in cache_keys),
                "average_load_time": sum(self.load_times.get(k, 0) for k in cache_keys) / max(1, len(cache_keys)),
                "checkpoint_path": getattr(config, 'checkpoint_path', None),
                "input_size": getattr(config, 'input_size', (512, 512)),
                "last_access": max((self.last_access.get(k, 0) for k in cache_keys), default=0),
                "auto_detected": getattr(config, 'auto_detected', False),
                "confidence_score": getattr(config, 'confidence_score', 0.0)
            }

    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            result = {}
            for name in self.model_configs.keys():
                info = self.get_model_info(name)
                if info:
                    result[name] = info
            return result

    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            usage = {
                "loaded_models": len(self.model_cache),
                "device": self.device,
                "available_memory_gb": self.memory_manager.get_available_memory(),
                "memory_pressure": self.memory_manager.check_memory_pressure(),
                "is_m3_max": self.is_m3_max
            }
            
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                usage.update({
                    "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                    "cached_gb": torch.cuda.memory_reserved() / 1024**3
                })
            elif self.device == "mps":
                try:
                    import psutil
                    process = psutil.Process()
                    usage.update({
                        "process_memory_gb": process.memory_info().rss / 1024**3,
                        "system_memory_percent": psutil.virtual_memory().percent
                    })
                except ImportError:
                    usage["memory_info"] = "psutil not available"
            else:
                usage["memory_info"] = "cpu mode"
                
            return usage
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    self.cleanup_step_interface(step_name)
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader v4.0 ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def initialize(self) -> bool:
        """ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
            missing_checkpoints = []
            for name, config in self.model_configs.items():
                checkpoint_path = getattr(config, 'checkpoint_path', None)
                if checkpoint_path:
                    if not Path(checkpoint_path).exists():
                        missing_checkpoints.append(name)
            
            if missing_checkpoints:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤: {missing_checkpoints}")
                self.logger.info("ğŸ“ í•´ë‹¹ ëª¨ë¸ë“¤ì€ ì‹¤ì œ íŒŒì¼ì´ ìˆì„ ë•Œ ë¡œë“œë©ë‹ˆë‹¤")
            
            # M3 Max ìµœì í™” ì„¤ì •
            if COREML_AVAILABLE and self.is_m3_max:
                self.logger.info("ğŸ CoreML ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
            # auto_model_detector ê²°ê³¼ ìš”ì•½
            if self.auto_detector and self.detected_models:
                self.logger.info(f"ğŸ” ìë™ íƒì§€ ëª¨ë¸: {len(self.detected_models)}ê°œ")
            
            self.logger.info(f"âœ… ModelLoader v4.0 ì´ˆê¸°í™” ì™„ë£Œ - {len(self.model_configs)}ê°œ ëª¨ë¸ ë“±ë¡ë¨")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def get_step_info(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ëª¨ë¸ ë¡œë” ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "device": self.device,
            "device_manager": {
                "available_devices": self.device_manager.available_devices,
                "optimal_device": self.device_manager.optimal_device,
                "is_m3_max": self.device_manager.is_m3_max
            },
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "config_keys": list(self.config.keys()),
            "specialized_features": {
                "use_fp16": self.use_fp16,
                "lazy_loading": self.lazy_loading,
                "max_cached_models": self.max_cached_models,
                "enable_fallback": self.enable_fallback,
                "enable_auto_detection": self.enable_auto_detection
            },
            "model_stats": {
                "registered_models": len(self.model_configs),
                "loaded_models": len(self.model_cache),
                "detected_models": len(self.detected_models),
                "total_access_count": sum(self.access_counts.values()),
                "average_load_time": sum(self.load_times.values()) / len(self.load_times) if self.load_times else 0
            },
            "step_requirements": len(self.step_requirements) if hasattr(self, 'step_requirements') else 0,
            "library_availability": {
                "torch": TORCH_AVAILABLE,
                "opencv": CV_AVAILABLE,
                "mediapipe": MEDIAPIPE_AVAILABLE,
                "transformers": TRANSFORMERS_AVAILABLE,
                "diffusers": DIFFUSERS_AVAILABLE,
                "onnx": ONNX_AVAILABLE,
                "coreml": COREML_AVAILABLE,
                "step_requests": STEP_REQUESTS_AVAILABLE,
                "auto_detector": AUTO_DETECTOR_AVAILABLE
            },
            "memory_usage": self.get_memory_usage()
        }

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                # ì „ì—­ ëª¨ë¸ ë¡œë” ì‚¬ìš©
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.warning(f"âš ï¸ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                return await self.model_interface.get_model(model_name)
            else:
                # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                return await self.model_interface.get_recommended_model()
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def preprocess_image(image: Union[np.ndarray, Image.Image], target_size: tuple, normalize: bool = True) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV not available")
            
        if isinstance(image, np.ndarray):
            if image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            elif len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image).astype(np.float32)
        
        if TORCH_AVAILABLE:
            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1) / 255.0
            
            # ì •ê·œí™”
            if normalize:
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                image_tensor = (image_tensor - mean) / std
            
            return image_tensor.unsqueeze(0)
        else:
            return image_array
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # ë”ë¯¸ í…ì„œ ë°˜í™˜
        if TORCH_AVAILABLE:
            return torch.randn(1, 3, target_size[1], target_size[0])
        else:
            return np.random.randn(target_size[1], target_size[0], 3)

def postprocess_segmentation(output, original_size: tuple, threshold: float = 0.5) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV not available")
        
        if TORCH_AVAILABLE and torch.is_tensor(output):
            if output.dim() == 4:
                output = output.squeeze(0)
            
            # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
            if output.shape[0] > 1:
                output = torch.argmax(output, dim=0)
            else:
                output = (output > threshold).float()
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            output = output.cpu().numpy().astype(np.uint8)
        else:
            output = np.array(output).astype(np.uint8)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if output.shape != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        return output
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return np.zeros(original_size[::-1], dtype=np.uint8)

def postprocess_pose(output, original_size: tuple, confidence_threshold: float = 0.3) -> Dict[str, Any]:
    """í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬"""
    try:
        if isinstance(output, (list, tuple)):
            # OpenPose ìŠ¤íƒ€ì¼ ì¶œë ¥ (PAF, heatmaps)
            pafs, heatmaps = output[-1]  # ë§ˆì§€ë§‰ ìŠ¤í…Œì´ì§€ ê²°ê³¼ ì‚¬ìš©
        else:
            heatmaps = output
            pafs = None
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = []
        
        if TORCH_AVAILABLE and torch.is_tensor(heatmaps):
            if heatmaps.dim() == 4:
                heatmaps = heatmaps.squeeze(0)
            heatmaps_np = heatmaps.cpu().numpy()
        else:
            heatmaps_np = np.array(heatmaps)
        
        for i in range(heatmaps_np.shape[0] - 1):  # ë°°ê²½ ì œì™¸
            heatmap = heatmaps_np[i]
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = heatmap[y, x]
            
            if confidence > confidence_threshold:
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                x_scaled = int(x * original_size[0] / heatmap.shape[1])
                y_scaled = int(y * original_size[1] / heatmap.shape[0])
                keypoints.append([x_scaled, y_scaled, confidence])
            else:
                keypoints.append([0, 0, 0])
        
        return {
            'keypoints': keypoints,
            'pafs': pafs.cpu().numpy() if TORCH_AVAILABLE and torch.is_tensor(pafs) else pafs,
            'heatmaps': heatmaps_np,
            'num_keypoints': len([kp for kp in keypoints if kp[2] > confidence_threshold])
        }
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {'keypoints': [], 'pafs': None, 'heatmaps': None, 'num_keypoints': 0}

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=False  # ì‹¤ì œ ëª¨ë¸ë§Œ ì‚¬ìš©
            )
            logger.info("ğŸŒ ì „ì—­ ModelLoader v4.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> Dict[str, Any]:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            future = asyncio.create_task(loader.initialize())
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            result = loop.run_until_complete(loader.initialize())
            return {"success": result, "message": "Initialization completed"}
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        # ìºì‹œ í´ë¦¬ì–´
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ModelLoader v4.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ - ì™„ì „ í†µí•©
# ==============================================

def create_model_loader(
    device: str = "auto", 
    use_fp16: bool = True, 
    enable_auto_detection: bool = True,
    **kwargs
) -> ModelLoader:
    """ëª¨ë¸ ë¡œë” ìƒì„± (í•˜ìœ„ í˜¸í™˜)"""
    return ModelLoader(
        device=device, 
        use_fp16=use_fp16, 
        enable_auto_detection=enable_auto_detection,
        **kwargs
    )

async def load_model_for_step(
    step_name: str, 
    model_name: Optional[str] = None,
    **kwargs
) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ë¡œë“œ í¸ì˜ í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        interface = loader.create_step_interface(step_name)
        
        if model_name:
            return await interface.get_model(model_name, **kwargs)
        else:
            return await interface.get_recommended_model()
            
    except Exception as e:
        logger.error(f"âŒ Step ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {step_name}: {e}")
        return None

async def load_model_async(model_name: str, config: Optional[Union[ModelConfig, StepModelConfig]] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        # ì„¤ì •ì´ ìˆìœ¼ë©´ ë“±ë¡
        if config:
            loader.register_model_config(model_name, config)
        
        return await loader.load_model(model_name)
    except Exception as e:
        logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_model_sync(model_name: str, config: Optional[Union[ModelConfig, StepModelConfig]] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        # ì„¤ì •ì´ ìˆìœ¼ë©´ ë“±ë¡
        if config:
            loader.register_model_config(model_name, config)
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(loader.load_model(model_name))
    except Exception as e:
        logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ğŸ”¥ í•µì‹¬: ëª¨ë¸ í¬ë§· ê°ì§€ ë° ë³€í™˜ í•¨ìˆ˜ë“¤
def detect_model_format(model_path: Union[str, Path]) -> ModelFormat:
    """íŒŒì¼ í™•ì¥ìë¡œ ëª¨ë¸ í¬ë§· ê°ì§€"""
    path = Path(model_path)
    
    if path.suffix == '.pth' or path.suffix == '.pt':
        return ModelFormat.PYTORCH
    elif path.suffix == '.safetensors':
        return ModelFormat.SAFETENSORS
    elif path.suffix == '.onnx':
        return ModelFormat.ONNX
    elif path.suffix == '.mlmodel':
        return ModelFormat.COREML
    elif path.is_dir():
        # ë””ë ‰í† ë¦¬ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨
        if (path / "config.json").exists():
            if (path / "model.safetensors").exists():
                return ModelFormat.TRANSFORMERS
            elif any(path.glob("*.bin")):
                return ModelFormat.DIFFUSERS
        return ModelFormat.DIFFUSERS  # ê¸°ë³¸ê°’
    else:
        return ModelFormat.PYTORCH  # ê¸°ë³¸ê°’

def load_model_with_format(
    model_path: Union[str, Path],
    model_format: ModelFormat,
    device: str = "auto"
) -> Any:
    """ê°„í¸í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        
        # ëª¨ë¸ ì„¤ì • ìƒì„±
        config = ModelConfig(
            name=Path(model_path).stem,
            model_type=ModelType.VIRTUAL_FITTING,  # ê¸°ë³¸ê°’
            model_class="HRVITONModel",
            checkpoint_path=str(model_path),
            device=device
        )
        
        # ë™ê¸° ë¡œë”©
        return load_model_sync(config.name, config)
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì•ˆì „í•œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ - ì™„ì „ í†µí•©
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelFormat',  # ğŸ”¥ main.py í•„ìˆ˜
    'ModelConfig', 
    'StepModelConfig',
    'ModelType',
    'ModelPriority',
    'ModelRegistry',
    'ModelMemoryManager',
    'DeviceManager',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    'load_model_async',
    'load_model_sync',
    'load_model_for_step',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'detect_model_format',
    'load_model_with_format',
    'preprocess_image',
    'postprocess_segmentation',
    'postprocess_pose'
]

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… ModelLoader v4.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - step_model_requests.py ê¸°ë°˜ ì™„ì „ í†µí•© ì‹œìŠ¤í…œ + StepModelInterface ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  í†µí•© + ğŸ”¥ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")