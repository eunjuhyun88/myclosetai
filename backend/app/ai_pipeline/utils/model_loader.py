# app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI - ì™„ì „ DI ê¸°ë°˜ ModelLoader ì‹œìŠ¤í…œ v10.0 - ğŸ”¥ base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©
======================================================================================================

âœ… base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©
âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
âœ… TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”
âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥/í´ë˜ìŠ¤ëª…/í•¨ìˆ˜ëª… 100% ìœ ì§€
âœ… MemoryManagerAdapter optimize_memory ì™„ì „ êµ¬í˜„
âœ… ë¹„ë™ê¸°(async/await) ì™„ì „ ì§€ì› ê°•í™”
âœ… StepModelInterface ë¹„ë™ê¸° í˜¸í™˜ ê°•í™”
âœ… SafeModelService ë¹„ë™ê¸° í™•ì¥ ê°•í™”
âœ… Coroutine 'not callable' ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… Dict callable ë¬¸ì œ ê·¼ë³¸ í•´ê²°
âœ… AttributeError ì™„ì „ í•´ê²°
âœ… M3 Max 128GB ìµœì í™” ìœ ì§€
âœ… íŒŒì´ì¬ ìµœì í™”ëœ ìˆœì„œë¡œ ì™„ì „ ì •ë¦¬

Author: MyCloset AI Team
Date: 2025-07-20
Version: 10.0 (Complete DI Integration + base_step_mixin.py Pattern)
"""

# ==============================================
# ğŸ”¥ 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì•ŒíŒŒë²³ ìˆœ)
# ==============================================
import asyncio
import gc
import hashlib
import json
import logging
import os
import pickle
import sqlite3
import threading
import time
import traceback
import weakref
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache, wraps
from pathlib import Path
from typing import Any, Awaitable, Callable, Dict, List, Optional, Tuple, Type, Union, TYPE_CHECKING

# ==============================================
# ğŸ”¥ 2. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..interfaces.model_interface import IModelLoader, IStepInterface, IMemoryManager, IDataConverter
    from ..steps.base_step_mixin import BaseStepMixin
    from ...core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 3. ë¡œê¹… ì„¤ì •
# ==============================================
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 4. ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ë° ì•ˆì „í•œ ì„í¬íŠ¸ (base_step_mixin.py íŒ¨í„´)
# ==============================================

class LibraryCompatibility:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬ ë° ê´€ë¦¬ - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.cv_available = False
        self.transformers_available = False
        self.diffusers_available = False
        self.coreml_available = False
        
        self._check_numpy_compatibility()
        self._check_torch_compatibility()
        self._check_optional_libraries()
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x í˜¸í™˜ì„± ì²´í¬"""
        try:
            import numpy as np
            self.numpy_available = True
            self.numpy_version = np.__version__
            
            major_version = int(self.numpy_version.split('.')[0])
            if major_version >= 2:
                logging.warning(f"âš ï¸ NumPy {self.numpy_version} ê°ì§€ë¨. NumPy 1.x ê¶Œì¥")
                logging.warning("ğŸ”§ í•´ê²°ë°©ë²•: conda install numpy=1.24.3 -y --force-reinstall")
                try:
                    np.set_printoptions(legacy='1.25')
                    logging.info("âœ… NumPy 2.x í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”")
                except:
                    pass
            
            globals()['np'] = np
            
        except ImportError as e:
            self.numpy_available = False
            logging.error(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    
    def _check_torch_compatibility(self):
        """PyTorch í˜¸í™˜ì„± ì²´í¬"""
        try:
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.default_device = "cpu"
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.mps_available = True
                self.default_device = "mps"
                logging.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            else:
                self.mps_available = False
                logging.info("â„¹ï¸ CPU ëª¨ë“œ ì‚¬ìš©")
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError as e:
            self.torch_available = False
            self.mps_available = False
            self.default_device = "cpu"
            logging.warning(f"âš ï¸ PyTorch ì—†ìŒ: {e}")
    
    def _check_optional_libraries(self):
        """ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì²´í¬"""
        try:
            import cv2
            from PIL import Image, ImageEnhance
            self.cv_available = True
            globals()['cv2'] = cv2
            globals()['Image'] = Image
            globals()['ImageEnhance'] = ImageEnhance
        except ImportError:
            self.cv_available = False
        
        try:
            from transformers import AutoModel, AutoTokenizer, AutoConfig
            self.transformers_available = True
        except ImportError:
            self.transformers_available = False
        
        try:
            from diffusers import StableDiffusionPipeline, UNet2DConditionModel
            self.diffusers_available = True
        except ImportError:
            self.diffusers_available = False
        
        try:
            import coremltools as ct
            self.coreml_available = True
        except ImportError:
            self.coreml_available = False

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™”
_compat = LibraryCompatibility()

# ==============================================
# ğŸ”¥ 5. ìƒìˆ˜ ì •ì˜
# ==============================================
NUMPY_AVAILABLE = _compat.numpy_available
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
CV_AVAILABLE = _compat.cv_available
DEFAULT_DEVICE = _compat.default_device

# ==============================================
# ğŸ”¥ 6. DI Container ë° ì¸í„°í˜ì´ìŠ¤ ì•ˆì „í•œ import
# ==============================================

# DI Container (ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)
DI_CONTAINER_AVAILABLE = False
try:
    from ...core.di_container import (
        get_di_container, create_step_with_di, inject_dependencies_to_step,
        initialize_di_system
    )
    DI_CONTAINER_AVAILABLE = True
    logging.info("âœ… DI Container ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    DI_CONTAINER_AVAILABLE = False
    logging.warning(f"âš ï¸ DI Container ì‚¬ìš© ë¶ˆê°€: {e}")

# ==============================================
# ğŸ”¥ 7. ì—´ê±°í˜• ì •ì˜ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§· ì •ì˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨ ì •ì˜"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    ULTRA = "ultra"
    MAXIMUM = "ultra"  # í•˜ìœ„ í˜¸í™˜ì„±

# ==============================================
# ğŸ”¥ 8. ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ 9. Step ìš”ì²­ì‚¬í•­ ì •ì˜ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "ClothWarpingStep": {
        "model_name": "cloth_warping_net",
        "model_type": "ClothWarpingModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*warping*.pth", "*flow*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

# ==============================================
# ğŸ”¥ 10. DI ë„ìš°ë¯¸ í´ë˜ìŠ¤ (base_step_mixin.py íŒ¨í„´)
# ==============================================

class DIHelper:
    """ì˜ì¡´ì„± ì£¼ì… ë„ìš°ë¯¸ - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    @staticmethod
    def get_di_container() -> Optional['DIContainer']:
        """DI Container ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if DI_CONTAINER_AVAILABLE:
                return get_di_container()
            return None
        except ImportError:
            return None
        except Exception as e:
            logging.warning(f"âš ï¸ DI Container ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def inject_model_loader(instance) -> bool:
        """ModelLoader ì£¼ì…"""
        try:
            container = DIHelper.get_di_container()
            if container:
                model_loader = container.get('IModelLoader')
                if model_loader:
                    instance.model_loader = model_loader
                    return True
            
            # í´ë°±: ì§ì ‘ import
            try:
                from ..adapters.model_adapter import ModelLoaderAdapter
                instance.model_loader = ModelLoaderAdapter()
                return True
            except ImportError:
                pass
            
            return False
        except Exception as e:
            logging.warning(f"âš ï¸ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 11. ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤ (base_step_mixin.py íŒ¨í„´)
# ==============================================

class SafeConfig:
    """ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ì - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    def __init__(self, config_data: Optional[Dict[str, Any]] = None):
        self._data = config_data or {}
        self._lock = threading.RLock()
        
        # ì„¤ì • ê²€ì¦ ë° ì†ì„± ìë™ ì„¤ì •
        with self._lock:
            for key, value in self._data.items():
                if isinstance(key, str) and key.isidentifier() and not callable(value):
                    try:
                        setattr(self, key, value)
                    except Exception:
                        pass
    
    def get(self, key: str, default: Any = None) -> Any:
        """ì•ˆì „í•œ ê°’ ì¡°íšŒ"""
        try:
            with self._lock:
                return self._data.get(key, default)
        except Exception:
            return default
    
    def set(self, key: str, value: Any):
        """ì•ˆì „í•œ ê°’ ì„¤ì •"""
        try:
            with self._lock:
                if not callable(value):
                    self._data[key] = value
                    if isinstance(key, str) and key.isidentifier():
                        setattr(self, key, value)
        except Exception:
            pass
    
    def __getitem__(self, key):
        try:
            return self._data[key]
        except KeyError:
            raise KeyError(f"ì„¤ì • í‚¤ '{key}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logging.debug(f"SafeConfig.__getitem__ ì˜¤ë¥˜: {e}")
            raise
    
    def __setitem__(self, key, value):
        try:
            self.set(key, value)
        except Exception as e:
            logging.debug(f"SafeConfig.__setitem__ ì˜¤ë¥˜: {e}")
    
    def __contains__(self, key):
        try:
            return key in self._data
        except:
            return False
    
    def keys(self):
        try:
            return self._data.keys()
        except:
            return []
    
    def values(self):
        try:
            return self._data.values()
        except:
            return []
    
    def items(self):
        try:
            return self._data.items()
        except:
            return []
    
    def update(self, other):
        try:
            with self._lock:
                if isinstance(other, dict):
                    for key, value in other.items():
                        if not callable(value):
                            self._data[key] = value
                            if isinstance(key, str) and key.isidentifier():
                                setattr(self, key, value)
        except Exception as e:
            logging.debug(f"SafeConfig.update ì˜¤ë¥˜: {e}")
    
    def to_dict(self):
        try:
            with self._lock:
                return self._data.copy()
        except:
            return {}

# ==============================================
# ğŸ”¥ 12. ì•ˆì „ í•¨ìˆ˜ ê²€ì¦ì (base_step_mixin.py íŒ¨í„´)
# ==============================================

class SafeFunctionValidator:
    """í•¨ìˆ˜/ë©”ì„œë“œ/ê°ì²´ í˜¸ì¶œ ì•ˆì „ì„± ê²€ì¦ í´ë˜ìŠ¤ - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """ê°ì²´ê°€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ ê²€ì¦"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            # DictëŠ” ë¬´ì¡°ê±´ callableí•˜ì§€ ì•ŠìŒ
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            # Coroutine ê°ì²´ ì²´í¬
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            # Async function ì²´í¬  
            if asyncio.iscoroutinefunction(obj):
                return True, f"Object is async function in context: {context}", obj
            
            # ê¸°ë³¸ ë°ì´í„° íƒ€ì… ì²´í¬
            basic_types = (str, int, float, bool, list, tuple, set, bytes, bytearray)
            if isinstance(obj, basic_types):
                return False, f"Object is basic data type {type(obj)}, not callable", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            # í•¨ìˆ˜/ë©”ì„œë“œ íƒ€ì…ë³„ ê²€ì¦
            import types
            if isinstance(obj, (types.FunctionType, types.MethodType, types.BuiltinFunctionType, types.BuiltinMethodType)):
                return True, "Valid function/method", obj
            
            # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ __call__ ë©”ì„œë“œ ì²´í¬
            if hasattr(obj, '__call__'):
                call_method = getattr(obj, '__call__')
                if callable(call_method) and not isinstance(call_method, dict):
                    return True, "Valid callable object with __call__", obj
                else:
                    return False, "__call__ method is dict, not callable", None
            
            if callable(obj):
                return True, "Generic callable object", obj
            
            return False, f"Unknown callable validation failure for {type(obj)}", None
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except TypeError as e:
                error_msg = str(e)
                if "not callable" in error_msg.lower():
                    return False, None, f"Runtime callable error: {error_msg}"
                else:
                    return False, None, f"Type error in call: {error_msg}"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ"""
        try:
            # Coroutine ê°ì²´ ì§ì ‘ ì²´í¬
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, None, f"Cannot call coroutine object directly - need await"
            
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call_async")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    # ë™ê¸° í•¨ìˆ˜ëŠ” ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs))
                    return True, result, "Sync in executor success"
                    
            except TypeError as e:
                error_msg = str(e)
                if "not callable" in error_msg.lower():
                    return False, None, f"Runtime callable error: {error_msg}"
                else:
                    return False, None, f"Type error in async call: {error_msg}"
            except Exception as e:
                return False, None, f"Async call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"

# ==============================================
# ğŸ”¥ 13. ë¹„ë™ê¸° í˜¸í™˜ì„± ê´€ë¦¬ì (base_step_mixin.py íŒ¨í„´ ê°•í™”)
# ==============================================

class AsyncCompatibilityManager:
    """ë¹„ë™ê¸° í˜¸í™˜ì„± ê´€ë¦¬ì - base_step_mixin.py íŒ¨í„´ ê°•í™”"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AsyncCompatibilityManager")
        self._lock = threading.Lock()
        
    def make_callable_safe(self, obj: Any) -> Any:
        """ê°ì²´ë¥¼ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜ - DI íŒ¨í„´ ì ìš©"""
        try:
            if obj is None:
                return self._create_none_wrapper()
            
            # Coroutine ê°ì²´ ìš°ì„  ì²˜ë¦¬
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                self.logger.warning("âš ï¸ Coroutine ê°ì²´ ê°ì§€, ì•ˆì „í•œ ë˜í¼ ìƒì„±")
                return self._create_coroutine_wrapper(obj)
            
            # Dict íƒ€ì… ì²˜ë¦¬
            if isinstance(obj, dict):
                return self._create_dict_wrapper(obj)
            
            # ì´ë¯¸ callableí•œ ê°ì²´
            if callable(obj):
                return self._create_callable_wrapper(obj)
            
            # ê¸°ë³¸ ë°ì´í„° íƒ€ì…ë“¤
            if isinstance(obj, (str, int, float, bool, list, tuple)):
                return self._create_data_wrapper(obj)
            
            # ê¸°ë³¸ ê°ì²´ - callableì´ ì•„ë‹Œ ê²½ìš°
            return self._create_object_wrapper(obj)
            
        except Exception as e:
            self.logger.error(f"âŒ make_callable_safe ì˜¤ë¥˜: {e}")
            return self._create_emergency_wrapper(obj, str(e))
    
    def _create_none_wrapper(self) -> Any:
        """None ê°ì²´ìš© ë˜í¼ - DI í˜¸í™˜"""
        class SafeNoneWrapper:
            def __init__(self):
                self.name = "none_wrapper"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': None,
                    'call_type': 'none_wrapper',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
        
        return SafeNoneWrapper()
    
    def _create_data_wrapper(self, data: Any) -> Any:
        """ê¸°ë³¸ ë°ì´í„° íƒ€ì…ìš© ë˜í¼ - DI í˜¸í™˜"""
        class SafeDataWrapper:
            def __init__(self, data: Any):
                self.data = data
                self.name = f"data_wrapper_{type(data).__name__}"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': self.data,
                    'call_type': 'data_wrapper',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
        
        return SafeDataWrapper(data)
    
    def _create_object_wrapper(self, obj: Any) -> Any:
        """ì¼ë°˜ ê°ì²´ìš© ë˜í¼ - DI í˜¸í™˜"""
        class SafeObjectWrapper:
            def __init__(self, obj: Any):
                self.obj = obj
                self.name = f"object_wrapper_{type(obj).__name__}"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'wrapped_{self.name}',
                    'call_type': 'object_wrapper',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def __getattr__(self, name):
                if hasattr(self.obj, name):
                    return getattr(self.obj, name)
                raise AttributeError(f"'{self.name}' has no attribute '{name}'")
        
        return SafeObjectWrapper(obj)
    
    def _create_emergency_wrapper(self, obj: Any, error_msg: str) -> Any:
        """ê¸´ê¸‰ ìƒí™©ìš© ë˜í¼ - DI í˜¸í™˜"""
        class EmergencyWrapper:
            def __init__(self, obj: Any, error: str):
                self.obj = obj
                self.error = error
                self.name = "emergency_wrapper"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'emergency',
                    'model_name': self.name,
                    'result': f'emergency_result',
                    'error': self.error,
                    'call_type': 'emergency',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
        
        return EmergencyWrapper(obj, error_msg)
    
    def _create_dict_wrapper(self, data: Dict[str, Any]) -> Any:
        """Dictë¥¼ callable wrapperë¡œ ë³€í™˜ - DI í˜¸í™˜"""
        class SafeDictWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_type': 'sync',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_type': 'async',
                    'di_compatible': True
                }
            
            def __await__(self):
                return self.async_call().__await__()
        
        return SafeDictWrapper(data)
    
    def _create_coroutine_wrapper(self, coro) -> Any:
        """Coroutineì„ callable wrapperë¡œ ë³€í™˜ - DI í˜¸í™˜"""
        class SafeCoroutineWrapper:
            def __init__(self, coroutine):
                self.coroutine = coroutine
                self.name = "coroutine_wrapper"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        task = asyncio.create_task(self.coroutine)
                        return task
                    else:
                        return loop.run_until_complete(self.coroutine)
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(self.coroutine)
                    finally:
                        loop.close()
            
            async def async_call(self, *args, **kwargs):
                return await self.coroutine
            
            def __await__(self):
                return self.coroutine.__await__()
        
        return SafeCoroutineWrapper(coro)
    
    def _create_callable_wrapper(self, func) -> Any:
        """Callable ê°ì²´ë¥¼ ì•ˆì „í•œ wrapperë¡œ ë³€í™˜ - DI í˜¸í™˜"""
        class SafeCallableWrapper:
            def __init__(self, func):
                self.func = func
                self.is_async = asyncio.iscoroutinefunction(func)
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                if self.is_async:
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            return asyncio.create_task(self.func(*args, **kwargs))
                        else:
                            return loop.run_until_complete(self.func(*args, **kwargs))
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            return loop.run_until_complete(self.func(*args, **kwargs))
                        finally:
                            loop.close()
                else:
                    return self.func(*args, **kwargs)
            
            async def async_call(self, *args, **kwargs):
                if self.is_async:
                    return await self.func(*args, **kwargs)
                else:
                    return self.func(*args, **kwargs)
        
        return SafeCallableWrapper(func)

# ==============================================
# ğŸ”¥ 14. ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì–´ëŒ‘í„° (base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©)
# ==============================================

class MemoryManagerAdapter:
    """MemoryManager ì–´ëŒ‘í„° - base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©"""
    
    def __init__(self, original_manager=None):
        self.original_manager = original_manager
        self.logger = logging.getLogger(f"{__name__}.MemoryManagerAdapter")
        self._ensure_basic_methods()
        self.di_compatible = True
    
    def _ensure_basic_methods(self):
        """ê¸°ë³¸ ë©”ì„œë“œë“¤ì´ í•­ìƒ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥"""
        if not hasattr(self, 'device'):
            self.device = getattr(self.original_manager, 'device', 'cpu')
        if not hasattr(self, 'is_m3_max'):
            self.is_m3_max = getattr(self.original_manager, 'is_m3_max', False)
        if not hasattr(self, 'memory_gb'):
            self.memory_gb = getattr(self.original_manager, 'memory_gb', 16.0)
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ì™„ì „ êµ¬í˜„ëœ optimize_memory ë©”ì„œë“œ - DI í˜¸í™˜"""
        try:
            self.logger.debug("ğŸ§¹ MemoryManagerAdapter ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘")
            optimization_results = []
            
            # ì›ë³¸ ë§¤ë‹ˆì €ì˜ ë©”ëª¨ë¦¬ ì •ë¦¬ ë©”ì„œë“œ ì‹œë„
            if self.original_manager:
                if hasattr(self.original_manager, 'optimize_memory'):
                    try:
                        result = self.original_manager.optimize_memory(aggressive=aggressive)
                        optimization_results.append("ì›ë³¸ ë§¤ë‹ˆì € optimize_memory ì„±ê³µ")
                        self.logger.debug("âœ… ì›ë³¸ ë§¤ë‹ˆì €ì˜ optimize_memory í˜¸ì¶œ ì™„ë£Œ")
                    except Exception as e:
                        optimization_results.append(f"ì›ë³¸ ë§¤ë‹ˆì € optimize_memory ì‹¤íŒ¨: {e}")
                        self.logger.warning(f"âš ï¸ ì›ë³¸ ë§¤ë‹ˆì € optimize_memory ì‹¤íŒ¨: {e}")
                        
                elif hasattr(self.original_manager, 'cleanup_memory'):
                    try:
                        result = self.original_manager.cleanup_memory(aggressive=aggressive)
                        optimization_results.append("ì›ë³¸ ë§¤ë‹ˆì € cleanup_memory ì„±ê³µ")
                        self.logger.debug("âœ… ì›ë³¸ ë§¤ë‹ˆì €ì˜ cleanup_memory í˜¸ì¶œ ì™„ë£Œ")
                    except Exception as e:
                        optimization_results.append(f"ì›ë³¸ ë§¤ë‹ˆì € cleanup_memory ì‹¤íŒ¨: {e}")
                        self.logger.warning(f"âš ï¸ ì›ë³¸ ë§¤ë‹ˆì € cleanup_memory ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™”
            try:
                before_objects = len(gc.get_objects())
                gc.collect()
                after_objects = len(gc.get_objects())
                freed_objects = before_objects - after_objects
                optimization_results.append(f"Python GC: {freed_objects}ê°œ ê°ì²´ ì •ë¦¬")
            except Exception as e:
                optimization_results.append(f"Python GC ì‹¤íŒ¨: {e}")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                if TORCH_AVAILABLE:
                    # CUDA ìºì‹œ ì •ë¦¬
                    if torch.cuda.is_available():
                        before_cuda = torch.cuda.memory_allocated()
                        torch.cuda.empty_cache()
                        after_cuda = torch.cuda.memory_allocated()
                        freed_cuda = (before_cuda - after_cuda) / 1024**3
                        optimization_results.append(f"CUDA ìºì‹œ ì •ë¦¬: {freed_cuda:.2f}GB í•´ì œ")
                        self.logger.debug("âœ… CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                    
                    # MPS ìºì‹œ ì •ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                                optimization_results.append("MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                                self.logger.debug("âœ… MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                            elif hasattr(torch.backends.mps, 'empty_cache'):
                                torch.backends.mps.empty_cache()
                                optimization_results.append("MPS ë°±ì—”ë“œ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                                self.logger.debug("âœ… MPS ë°±ì—”ë“œ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                        except Exception as mps_error:
                            optimization_results.append(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                            self.logger.warning(f"âš ï¸ MPS ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {mps_error}")
                            
            except Exception as torch_error:
                optimization_results.append(f"PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {torch_error}")
                self.logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {torch_error}")
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
            try:
                import psutil
                memory_info = psutil.virtual_memory()
                optimization_results.append(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_info.percent}% ì‚¬ìš©ì¤‘")
            except Exception as e:
                optimization_results.append(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                
            self.logger.debug("âœ… MemoryManagerAdapter ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
            return {
                "success": True, 
                "message": "Memory optimization completed",
                "optimization_results": optimization_results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "di_compatible": True,
                "aggressive": aggressive,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False, 
                "error": str(e),
                "device": getattr(self, 'device', 'unknown'),
                "di_compatible": True,
                "timestamp": time.time()
            }
    
    async def optimize_memory_async(self, aggressive: bool = False):
        """ì™„ì „ êµ¬í˜„ëœ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” - DI í˜¸í™˜"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self.optimize_memory, aggressive)
            await asyncio.sleep(0.01)  # ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ê²Œ ì œì–´ê¶Œ ì–‘ë³´
            return result
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False, 
                "error": str(e),
                "call_type": "async",
                "di_compatible": True,
                "timestamp": time.time()
            }
    
    def cleanup_memory(self, aggressive: bool = False):
        """cleanup_memory ë©”ì„œë“œ - optimize_memoryì™€ ë™ì¼"""
        return self.optimize_memory(aggressive=aggressive)
    
    def get_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
        try:
            if self.original_manager and hasattr(self.original_manager, 'get_memory_stats'):
                return self.original_manager.get_memory_stats()
            else:
                stats = {
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "memory_gb": getattr(self, 'memory_gb', 16.0),
                    "available": True,
                    "di_compatible": True,
                    "adapter_version": "v10.0"
                }
                
                if TORCH_AVAILABLE:
                    if torch.cuda.is_available():
                        stats.update({
                            "cuda_memory_allocated": torch.cuda.memory_allocated() / 1024**3,
                            "cuda_memory_reserved": torch.cuda.memory_reserved() / 1024**3,
                        })
                
                return stats
        except Exception as e:
            self.logger.warning(f"âš ï¸ get_memory_stats ì‹¤íŒ¨: {e}")
            return {"error": str(e), "di_compatible": True, "adapter_version": "v10.0"}
    
    def get_available_memory(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ì¡°íšŒ"""
        try:
            if self.original_manager and hasattr(self.original_manager, 'get_available_memory'):
                return self.original_manager.get_available_memory()
            else:
                if getattr(self, 'is_m3_max', False):
                    return 128.0  # M3 Max 128GB
                else:
                    return 16.0   # ê¸°ë³¸ 16GB
        except Exception as e:
            self.logger.warning(f"âš ï¸ get_available_memory ì‹¤íŒ¨: {e}")
            return 8.0
    
    def check_memory_pressure(self):
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸"""
        try:
            if self.original_manager and hasattr(self.original_manager, 'check_memory_pressure'):
                return self.original_manager.check_memory_pressure()
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.percent > 80  # 80% ì´ìƒ ì‚¬ìš© ì‹œ ì••ë°• ìƒíƒœ
                except ImportError:
                    return False  # psutil ì—†ìœ¼ë©´ ì•ˆì „í•œ ìƒíƒœë¡œ ê°„ì£¼
        except Exception as e:
            self.logger.warning(f"âš ï¸ check_memory_pressure ì‹¤íŒ¨: {e}")
            return False
    
    def __getattr__(self, name):
        """ëˆ„ë½ëœ ì†ì„±ì„ ì›ë³¸ ë§¤ë‹ˆì €ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            critical_methods = [
                'optimize_memory', 'cleanup_memory', 'get_memory_stats', 
                'get_available_memory', 'check_memory_pressure'
            ]
            
            if name in critical_methods:
                raise AttributeError(f"Method '{name}' should be handled directly")
            
            # ì›ë³¸ ë§¤ë‹ˆì €ì—ì„œ ì†ì„± ì°¾ê¸°
            if self.original_manager and hasattr(self.original_manager, name):
                attr = getattr(self.original_manager, name)
                
                if callable(attr):
                    def safe_wrapper(*args, **kwargs):
                        try:
                            return attr(*args, **kwargs)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                            return None
                    return safe_wrapper
                else:
                    return attr
            
            # ê¸°ë³¸ ì†ì„±ë“¤ì— ëŒ€í•œ í´ë°±
            fallback_attrs = {
                'device': 'cpu',
                'is_m3_max': False,
                'memory_gb': 16.0,
                'optimization_enabled': True,
                'auto_cleanup': True,
                'enable_caching': True,
                'di_compatible': True
            }
            
            if name in fallback_attrs:
                return fallback_attrs[name]
            else:
                raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ __getattr__ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({name}): {e}")
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

# ==============================================
# ğŸ”¥ 15. ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class DeviceManager:
    """ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì - DI í˜¸í™˜"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        self.di_compatible = True
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                devices.append("mps")
                self.logger.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - DI í˜¸í™˜"""
    
    def __init__(self, device: str = "mps", memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = self._detect_m3_max()
        self.di_compatible = True
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)  # 128GB ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def cleanup_memory(self, aggressive: bool = False):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 16. AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class BaseModel:
    """ê¸°ë³¸ AI ëª¨ë¸ í´ë˜ìŠ¤ - DI í˜¸í™˜"""
    
    def __init__(self):
        self.model_name = "BaseModel"
        self.device = "cpu"
        self.di_compatible = True
    
    def forward(self, x):
        return x
    
    def __call__(self, x):
        return self.forward(x)

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - DI í˜¸í™˜"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            self.di_compatible = True
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - DI í˜¸í™˜"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            self.di_compatible = True
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            self.paf_head = nn.Conv2d(512, 38, 1)
            self.heatmap_head = nn.Conv2d(512, 19, 1)
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - DI í˜¸í™˜"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            self.di_compatible = True
            
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - DI í˜¸í™˜"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            self.di_compatible = True
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

else:
    # PyTorch ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤ë“¤
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel

# ==============================================
# ğŸ”¥ 17. ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ (base_step_mixin.py íŒ¨í„´ ê°•í™”)
# ==============================================

class SafeModelService:
    """ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ - base_step_mixin.py íŒ¨í„´ ê°•í™”"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.async_lock = asyncio.Lock()
        self.validator = SafeFunctionValidator()
        self.async_manager = AsyncCompatibilityManager()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        self.di_compatible = True
        
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡ - Dictë¥¼ Callableë¡œ ë³€í™˜ (DI í˜¸í™˜)"""
        try:
            with self.lock:
                if isinstance(model, dict):
                    wrapper = self._create_callable_dict_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ë”•ì…”ë„ˆë¦¬ ëª¨ë¸ì„ callable wrapperë¡œ ë“±ë¡: {name}")
                elif callable(model):
                    is_callable, reason, safe_model = self.validator.validate_callable(model, f"register_{name}")
                    if is_callable:
                        safe_wrapped = self.async_manager.make_callable_safe(safe_model)
                        self.models[name] = safe_wrapped
                        self.logger.info(f"ğŸ“ ê²€ì¦ëœ callable ëª¨ë¸ ë“±ë¡: {name}")
                    else:
                        wrapper = self._create_object_wrapper(model)
                        self.models[name] = wrapper
                        self.logger.warning(f"âš ï¸ ì•ˆì „í•˜ì§€ ì•Šì€ callable ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                else:
                    wrapper = self._create_object_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ê°ì²´ ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def _create_callable_dict_wrapper(self, model_dict: Dict[str, Any]) -> Callable:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ callable wrapperë¡œ ë³€í™˜ - DI í˜¸í™˜"""
        class CallableDictWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.type = data.get('type', 'dict_model')
                self.call_count = 0
                self.last_call_time = None
                self.di_compatible = True
            
            def __call__(self, *args, **kwargs):
                self.call_count += 1
                self.last_call_time = time.time()
                
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'dict'
                    },
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.01)
                return self.__call__(*args, **kwargs)
            
            def get_info(self):
                return {
                    **self.data,
                    'wrapper_info': {
                        'type': 'dict_wrapper',
                        'call_count': self.call_count,
                        'last_call_time': self.last_call_time,
                        'di_compatible': True
                    }
                }
            
            def warmup(self):
                try:
                    test_result = self()
                    return test_result.get('status') == 'success'
                except Exception:
                    return False
        
        return CallableDictWrapper(model_dict)
    
    def _create_object_wrapper(self, obj: Any) -> Callable:
        """ì¼ë°˜ ê°ì²´ë¥¼ callable wrapperë¡œ ë³€í™˜ - DI í˜¸í™˜"""
        class ObjectWrapper:
            def __init__(self, wrapped_obj: Any):
                self.wrapped_obj = wrapped_obj
                self.name = getattr(wrapped_obj, 'name', str(type(wrapped_obj).__name__))
                self.type = type(wrapped_obj).__name__
                self.call_count = 0
                self.last_call_time = None
                self.original_callable = callable(wrapped_obj)
                self.di_compatible = True
            
            def __call__(self, *args, **kwargs):
                self.call_count += 1
                self.last_call_time = time.time()
                
                if self.original_callable:
                    validator = SafeFunctionValidator()
                    success, result, message = validator.safe_call(self.wrapped_obj, *args, **kwargs)
                    
                    if success:
                        return result
                    else:
                        return self._create_mock_response("call_failed", message)
                
                return self._create_mock_response("not_callable")
            
            async def async_call(self, *args, **kwargs):
                self.call_count += 1
                self.last_call_time = time.time()
                
                if self.original_callable:
                    validator = SafeFunctionValidator()
                    success, result, message = await validator.safe_call_async(self.wrapped_obj, *args, **kwargs)
                    
                    if success:
                        return result
                    else:
                        return self._create_mock_response("async_call_failed", message)
                
                return self._create_mock_response("not_callable")
            
            def _create_mock_response(self, reason: str, details: str = ""):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'wrapped_type': self.type,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'object',
                        'reason': reason,
                        'details': details
                    },
                    'di_compatible': True
                }
            
            def __getattr__(self, name):
                if hasattr(self.wrapped_obj, name):
                    attr = getattr(self.wrapped_obj, name)
                    if callable(attr):
                        validator = SafeFunctionValidator()
                        return lambda *args, **kwargs: validator.safe_call(attr, *args, **kwargs)[1]
                    else:
                        return attr
                else:
                    raise AttributeError(f"'{self.type}' object has no attribute '{name}'")
        
        return ObjectWrapper(obj)
    
    def call_model(self, name: str, *args, **kwargs) -> Any:
        """ëª¨ë¸ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                if isinstance(model, dict):
                    self.logger.error(f"âŒ ë“±ë¡ëœ ëª¨ë¸ì´ dictì…ë‹ˆë‹¤: {name}")
                    return None
                
                success, result, message = self.validator.safe_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"âœ… ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    async def call_model_async(self, name: str, *args, **kwargs) -> Any:
        """ëª¨ë¸ í˜¸ì¶œ - ë¹„ë™ê¸° ë²„ì „"""
        try:
            async with self.async_lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                if isinstance(model, dict):
                    self.logger.error(f"âŒ ë“±ë¡ëœ ëª¨ë¸ì´ dictì…ë‹ˆë‹¤: {name}")
                    return None
                
                # Coroutine ê°ì²´ ì§ì ‘ ì²´í¬ ë° ì²˜ë¦¬
                if hasattr(model, '__class__') and 'coroutine' in str(type(model)):
                    self.logger.warning(f"âš ï¸ Coroutine ê°ì²´ ê°ì§€, ëŒ€ê¸° ì²˜ë¦¬: {name}")
                    try:
                        result = await model
                        if name in self.call_statistics:
                            self.call_statistics[name]['successes'] += 1
                        self.logger.debug(f"âœ… Coroutine ëŒ€ê¸° ì™„ë£Œ: {name}")
                        return result
                    except Exception as coro_error:
                        self.logger.error(f"âŒ Coroutine ëŒ€ê¸° ì‹¤íŒ¨: {coro_error}")
                        if name in self.call_statistics:
                            self.call_statistics[name]['failures'] += 1
                        return None
                
                # ë¹„ë™ê¸° í˜¸ì¶œ ì‹œë„ (async_call ë©”ì„œë“œ ìš°ì„ )
                if hasattr(model, 'async_call'):
                    try:
                        result = await model.async_call(*args, **kwargs)
                        if name in self.call_statistics:
                            self.call_statistics[name]['successes'] += 1
                        self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ (async_call): {name}")
                        return result
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ async_call ì‹¤íŒ¨, safe_call_async ì‹œë„: {e}")
                
                # ì¼ë°˜ ë¹„ë™ê¸° í˜¸ì¶œ
                success, result, message = await self.validator.safe_call_async(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    
                    # ì¶”ê°€ ì‹œë„: ë™ê¸° í˜¸ì¶œë¡œ í´ë°±
                    if "coroutine" in message.lower():
                        self.logger.info(f"ğŸ”„ Coroutine ì˜¤ë¥˜ë¡œ ì¸í•´ ë™ê¸° í˜¸ì¶œ ì‹œë„: {name}")
                        try:
                            sync_success, sync_result, sync_message = self.validator.safe_call(model, *args, **kwargs)
                            if sync_success:
                                if name in self.call_statistics:
                                    self.call_statistics[name]['successes'] += 1
                                self.logger.info(f"âœ… ë™ê¸° í´ë°± í˜¸ì¶œ ì„±ê³µ: {name}")
                                return sync_result
                        except Exception as sync_error:
                            self.logger.warning(f"âš ï¸ ë™ê¸° í´ë°±ë„ ì‹¤íŒ¨: {sync_error}")
                    
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {}),
                        'di_compatible': True
                    }
                return result
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ 18. Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤ (base_step_mixin.py íŒ¨í„´ ê°•í™”)
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - base_step_mixin.py íŒ¨í„´ ê°•í™”"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        self.async_manager = AsyncCompatibilityManager()
        
        # ëª¨ë¸ ìºì‹œ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = STEP_MODEL_REQUESTS.get(step_name)
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        self.di_compatible = True
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_u2net"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - DI í˜¸í™˜"""
        try:
            async with self._async_lock:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    cached_model = self.loaded_models[model_name]
                    safe_model = self.async_manager.make_callable_safe(cached_model)
                    self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return safe_model
                
                # ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ
                if hasattr(self.model_loader, 'safe_model_service'):
                    service = self.model_loader.safe_model_service
                    
                    model = None
                    try:
                        if hasattr(service, 'call_model_async'):
                            model = await service.call_model_async(model_name)
                    except Exception as async_error:
                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨, ë™ê¸° í˜¸ì¶œ ì‹œë„: {async_error}")
                        try:
                            model = service.call_model(model_name)
                        except Exception as sync_error:
                            self.logger.warning(f"âš ï¸ ë™ê¸° í˜¸ì¶œë„ ì‹¤íŒ¨: {sync_error}")
                            model = None
                    
                    if model:
                        # Coroutine ê°ì²´ ì²´í¬ ì¶”ê°€
                        if hasattr(model, '__class__') and 'coroutine' in str(type(model)):
                            self.logger.warning(f"âš ï¸ Coroutine ê°ì²´ ê°ì§€, ëŒ€ê¸° ì¤‘: {model_name}")
                            try:
                                model = await model
                            except Exception as await_error:
                                self.logger.error(f"âŒ Coroutine ëŒ€ê¸° ì‹¤íŒ¨: {await_error}")
                                model = None
                        
                        if model:
                            safe_model = self.async_manager.make_callable_safe(model)
                            self.loaded_models[model_name] = safe_model
                            self.model_status[model_name] = "loaded"
                            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                            return safe_model
                
                # í´ë°± ëª¨ë¸ ìƒì„±
                fallback = await self._create_fallback_model_async(model_name)
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
                self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            fallback = await self._create_fallback_model_async(model_name or "error")
            async with self._async_lock:
                self.loaded_models[model_name or "error"] = fallback
                self.model_status[model_name or "error"] = "error_fallback"
            return fallback
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        try:
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    return self._get_model_sync_direct(model_name)
                else:
                    return loop.run_until_complete(self.get_model(model_name))
            except RuntimeError:
                return self._get_model_sync_direct(model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_fallback_model_sync(model_name or "error")
    
    def _get_model_sync_direct(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ì§ì ‘ ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                cached_model = self.loaded_models[model_name]
                return self.async_manager.make_callable_safe(cached_model)
            
            # ModelLoaderë¥¼ í†µí•œ ë™ê¸° ëª¨ë¸ ë¡œë“œ
            if hasattr(self.model_loader, 'safe_model_service'):
                service = self.model_loader.safe_model_service
                model = service.call_model(model_name)
                
                if model:
                    safe_model = self.async_manager.make_callable_safe(model)
                    with self._lock:
                        self.loaded_models[model_name] = safe_model
                        self.model_status[model_name] = "loaded"
                    self.logger.info(f"âœ… ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return safe_model
            
            # í´ë°± ëª¨ë¸ ìƒì„±
            fallback = self._create_fallback_model_sync(model_name)
            with self._lock:
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
            self.logger.warning(f"âš ï¸ ë™ê¸° í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
            return fallback
            
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_fallback_model_sync(model_name or "error")
    
    async def _create_fallback_model_async(self, model_name: str) -> Any:
        """ë¹„ë™ê¸° í´ë°± ëª¨ë¸ ìƒì„± - DI í˜¸í™˜"""
        class AsyncSafeFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'async_safe_fallback',
                    'di_compatible': True
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def __await__(self):
                async def _async_result():
                    return self
                return _async_result().__await__()
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return AsyncSafeFallbackModel(model_name)
    
    def _create_fallback_model_sync(self, model_name: str) -> Any:
        """ë™ê¸° í´ë°± ëª¨ë¸ ìƒì„± - DI í˜¸í™˜"""
        class SyncFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                self.di_compatible = True
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'sync_fallback',
                    'di_compatible': True
                }
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return SyncFallbackModel(model_name)
    
    def list_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            with self._lock:
                # ë“±ë¡ëœ ëª¨ë¸ë“¤
                registered_models = list(self.loaded_models.keys())
                
                # ì¶”ì²œ ëª¨ë¸ë“¤
                recommended = self.recommended_models.copy()
                
                # SafeModelServiceì— ë“±ë¡ëœ ëª¨ë¸ë“¤
                safe_models = list(self.model_loader.safe_model_service.models.keys())
                
                # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë°˜í™˜
                all_models = list(set(registered_models + recommended + safe_models))
                
                self.available_models = all_models
                return all_models
                
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.recommended_models
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False

# ==============================================
# ğŸ”¥ 19. ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ì™„ì „ DI ì ìš©)
# ==============================================

class ModelLoader:
    """ì™„ì „ DI ê¸°ë°˜ ModelLoader v10.0 - base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        di_container: Optional['DIContainer'] = None,
        **kwargs
    ):
        """ì™„ì „ DI ê¸°ë°˜ ìƒì„±ì - base_step_mixin.py íŒ¨í„´ ì ìš©"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = SafeConfig(config or {})
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # DI Container ì„¤ì • (base_step_mixin.py íŒ¨í„´)
        self.di_container = di_container or DIHelper.get_di_container()
        self.di_available = self.di_container is not None
        
        # SafeModelService í†µí•©
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        self.async_manager = AsyncCompatibilityManager()
        
        # ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager_raw = ModelMemoryManager(device=self.device)
        
        # Memory Manager ì–´ëŒ‘í„° (AttributeError ì™„ì „ í•´ê²°)
        self.memory_manager = MemoryManagerAdapter(self.memory_manager_raw)
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ
        self.enable_auto_detection = enable_auto_detection
        self.detected_model_registry = {}
        
        # DI í˜¸í™˜ì„±
        self.di_compatible = True
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì„¤ì •
        if self.enable_auto_detection:
            self._setup_auto_detection()
        
        self.logger.info(f"ğŸ¯ ModelLoader v10.0 ì´ˆê¸°í™” ì™„ë£Œ (ì™„ì „ DI ê¸°ë°˜)")
        self.logger.info(f"ğŸ”§ Device: {self.device}, SafeModelService: âœ…, Async: âœ…, DI: {'âœ…' if self.di_available else 'âŒ'}")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” - DI íŒ¨í„´ ì ìš©"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.use_fp16 = True
                if _compat.coreml_available:
                    self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # DI ì˜ì¡´ì„± ë“±ë¡ (base_step_mixin.py íŒ¨í„´)
            if self.di_available:
                self._register_di_dependencies()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _register_di_dependencies(self):
        """DI ì˜ì¡´ì„± ë“±ë¡ - base_step_mixin.py íŒ¨í„´"""
        try:
            if not self.di_container:
                return
            
            # ModelLoader ì–´ëŒ‘í„° ë“±ë¡
            self.di_container.register_instance('IModelLoader', self)
            
            # MemoryManager ì–´ëŒ‘í„° ë“±ë¡
            self.di_container.register_instance('IMemoryManager', self.memory_manager)
            
            # SafeModelService ë“±ë¡
            self.di_container.register_instance('SafeModelService', self.safe_model_service)
            
            # SafeFunctionValidator ë“±ë¡
            self.di_container.register_instance('ISafeFunctionValidator', self.function_validator)
            
            self.logger.info("âœ… DI ì˜ì¡´ì„± ë“±ë¡ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ DI ì˜ì¡´ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _setup_auto_detection(self):
        """ìë™ íƒì§€ ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            self.logger.info("ğŸ” ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            self._detect_available_models()
            self.logger.info("âœ… ìë™ íƒì§€ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ ìë™ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íƒì§€"""
        try:
            detected_count = 0
            search_paths = [
                self.model_cache_dir,
                Path.cwd() / "models",
                Path.cwd() / "checkpoints"
            ]
            
            for search_path in search_paths:
                if search_path.exists():
                    for file_path in search_path.rglob("*.pth"):
                        if file_path.is_file():
                            model_name = file_path.stem
                            model_info = {
                                'path': str(file_path),
                                'size_mb': file_path.stat().st_size / (1024 * 1024),
                                'auto_detected': True,
                                'di_compatible': True
                            }
                            self.detected_model_registry[model_name] = model_info
                            detected_count += 1
            
            self.logger.info(f"ğŸ” {detected_count}ê°œ ëª¨ë¸ íŒŒì¼ íƒì§€ë¨")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡ - DI í˜¸í™˜"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                # SafeModelServiceì—ë„ ë“±ë¡
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device,
                    'di_compatible': True
                }
                self.safe_model_service.register_model(name, model_dict)
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ ModelLoader v10.0 ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            async with self._async_lock:
                # ê¸°ë³¸ ê²€ì¦
                if not hasattr(self, 'device_manager'):
                    self.logger.warning("âš ï¸ ë””ë°”ì´ìŠ¤ ë§¤ë‹ˆì €ê°€ ì—†ìŒ")
                    return False
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë¹„ë™ê¸°) - AttributeError í•´ê²°
                if hasattr(self, 'memory_manager'):
                    try:
                        await self.memory_manager.optimize_memory_async()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info("âœ… ModelLoader v10.0 ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def initialize(self) -> bool:
        """ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ - ìˆœìˆ˜ ë™ê¸° ë²„ì „"""
        try:
            self.logger.info("ğŸš€ ModelLoader v10.0 ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ë³¸ ê²€ì¦
            if not hasattr(self, 'device_manager'):
                self.logger.warning("âš ï¸ ë””ë°”ì´ìŠ¤ ë§¤ë‹ˆì €ê°€ ì—†ìŒ")
                return False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë™ê¸°) - AttributeError í•´ê²°
            if hasattr(self, 'memory_manager'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
            self.logger.info("âœ… ModelLoader v10.0 ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def create_step_interface_async(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë¹„ë™ê¸° ë²„ì „"""
        try:
            async with self._async_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements ì²˜ë¦¬
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ {req_name} ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def create_step_interface(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements ì²˜ë¦¬
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ {req_name} ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ ëª©ë¡"""
        try:
            with self._lock:
                models_info = {}
                
                for model_name in self.model_configs.keys():
                    models_info[model_name] = {
                        'name': model_name,
                        'registered': True,
                        'device': self.device,
                        'config': self.model_configs[model_name],
                        'di_compatible': True
                    }
                
                if hasattr(self, 'detected_model_registry'):
                    for model_name in self.detected_model_registry.keys():
                        if model_name not in models_info:
                            models_info[model_name] = {
                                'name': model_name,
                                'auto_detected': True,
                                'info': self.detected_model_registry[model_name],
                                'di_compatible': True
                            }
                
                safe_models = self.safe_model_service.list_models()
                for model_name, status in safe_models.items():
                    if model_name not in models_info:
                        models_info[model_name] = {
                            'name': model_name,
                            'source': 'SafeModelService',
                            'status': status,
                            'di_compatible': True
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            cache_key = f"{model_name}_{kwargs.get('config_hash', 'default')}"
            
            async with self._async_lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache:
                    cached_model = self.model_cache[cache_key]
                    safe_model = self.async_manager.make_callable_safe(cached_model)
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return safe_model
                
                # SafeModelService ìš°ì„  ì‚¬ìš© (ë¹„ë™ê¸°)
                model = await self.safe_model_service.call_model_async(model_name)
                if model:
                    safe_model = self.async_manager.make_callable_safe(model)
                    self.model_cache[cache_key] = safe_model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return safe_model
                
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def load_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            cache_key = f"{model_name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache:
                    cached_model = self.model_cache[cache_key]
                    safe_model = self.async_manager.make_callable_safe(cached_model)
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return safe_model
                
                # SafeModelService ìš°ì„  ì‚¬ìš©
                model = self.safe_model_service.call_model(model_name)
                if model:
                    safe_model = self.async_manager.make_callable_safe(model)
                    self.model_cache[cache_key] = safe_model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return safe_model
                
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡ - SafeModelServiceì— ìœ„ì„"""
        try:
            return self.safe_model_service.register_model(name, model)
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def cleanup(self):
        """ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    try:
                        if step_name in self.step_interfaces:
                            del self.step_interfaces[step_name]
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            try:
                                model.cpu()
                            except:
                                pass
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ - AttributeError í•´ê²°
            if hasattr(self.memory_manager, 'optimize_memory'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader v10.0 ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”¥ 20. ì „ì—­ ModelLoader ê´€ë¦¬ (DI í˜¸í™˜)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ - DI í˜¸í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            # DI Container ê°€ì ¸ì˜¤ê¸°
            di_container = DIHelper.get_di_container()
            
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True,
                di_container=di_container
            )
            logger.info("ğŸŒ ì „ì—­ ModelLoader v10.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì™„ì „ DI ê¸°ë°˜)")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader async initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        success = loader.initialize()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ModelLoader v10.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ 21. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

def preprocess_image(
    image: Union[Any, Any, Any],
    target_size: Tuple[int, int] = (512, 512),
    device: str = "mps",
    normalize: bool = True,
    to_tensor: bool = True
) -> Any:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        if not CV_AVAILABLE:
            logger.warning("âš ï¸ OpenCV/PIL ì—†ìŒ, ê¸°ë³¸ ì²˜ë¦¬")
            if TORCH_AVAILABLE and to_tensor:
                return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
            else:
                if NUMPY_AVAILABLE:
                    return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
                else:
                    return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
        
        # PIL/OpenCVë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì „ì²˜ë¦¬
        if hasattr(image, 'resize'):  # PIL Image
            image = image.resize(target_size)
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32)
                if normalize:
                    img_array = img_array / 255.0
                
                if to_tensor and TORCH_AVAILABLE:
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return img_tensor.to(device)
                else:
                    return img_array
        
        # í´ë°± ì²˜ë¦¬
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
                
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]

def postprocess_segmentation(output: Any, threshold: float = 0.5) -> Any:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í›„ì²˜ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if NUMPY_AVAILABLE and hasattr(output, 'squeeze'):
            if output.ndim == 4:
                output = output.squeeze(0)
            if output.ndim == 3:
                output = output.squeeze(0)
                
            binary_mask = (output > threshold).astype(np.uint8) * 255
            return binary_mask
        else:
            # NumPy ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
            return [[255 if x > threshold else 0 for x in row] for row in output] if hasattr(output, '__iter__') else output
            
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        if NUMPY_AVAILABLE:
            return np.zeros((512, 512), dtype=np.uint8)
        else:
            return [[0 for _ in range(512)] for _ in range(512)]

# ì¶”ê°€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def preprocess_pose_input(image: Any, target_size: Tuple[int, int] = (368, 368)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_human_parsing_input(image: Any, target_size: Tuple[int, int] = (512, 512)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_cloth_segmentation_input(image: Any, target_size: Tuple[int, int] = (320, 320)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def tensor_to_pil(tensor: Any) -> Any:
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'dim'):
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.dim() == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = tensor.cpu().numpy()
            
        if NUMPY_AVAILABLE and hasattr(tensor, 'dtype'):
            if tensor.dtype != np.uint8:
                tensor = (tensor * 255).astype(np.uint8)
        
        if CV_AVAILABLE:
            return Image.fromarray(tensor)
        else:
            return tensor
    except Exception as e:
        logger.error(f"í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def pil_to_tensor(image: Any, device: str = "mps") -> Any:
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
    try:
        if CV_AVAILABLE and hasattr(image, 'size'):
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32) / 255.0
                if TORCH_AVAILABLE:
                    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return tensor.to(device)
                else:
                    return img_array
        
        # í´ë°±
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return np.zeros((1, 3, 512, 512), dtype=np.float32) if NUMPY_AVAILABLE else None
            
    except Exception as e:
        logger.error(f"PIL->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return None

# ==============================================
# ğŸ”¥ 22. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (DI í˜¸í™˜)
# ==============================================

def get_model_service() -> SafeModelService:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    loader = get_global_model_loader()
    return loader.safe_model_service

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ë¡œ ì•ˆì „í•˜ê²Œ ë“±ë¡"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock ëª¨ë¸ ìƒì„±"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': 'mps',
        'loaded_at': '2025-07-20T12:00:00Z',
        'di_compatible': True
    }
    
    service = get_model_service()
    return service._create_callable_dict_wrapper(mock_dict)

# ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ - ë¹„ë™ê¸° ë²„ì „"""
    return await SafeFunctionValidator.safe_call_async(obj, *args, **kwargs)

def is_safely_callable(obj: Any) -> bool:
    """ì „ì—­ callable ì•ˆì „ì„± ê²€ì¦"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(loader, step_name)

async def create_step_interface_async(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë¹„ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        return await loader.create_step_interface_async(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ ë¹„ë™ê¸° Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(loader, step_name)

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        loader = get_global_model_loader()
        return {
            'device': loader.device,
            'is_m3_max': loader.is_m3_max,
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'memory_gb': loader.memory_gb,
            'optimization_enabled': loader.optimization_enabled,
            'use_fp16': loader.use_fp16,
            'async_compatibility': True,
            'coroutine_fix_applied': True,
            'attributeerror_fix_applied': True,
            'di_compatibility': True,
            'base_step_mixin_pattern_applied': True,
            'version': 'v10.0'
        }
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# ==============================================
# ğŸ”¥ 23. DI í†µí•© í•¨ìˆ˜ë“¤ (base_step_mixin.py íŒ¨í„´)
# ==============================================

def inject_dependencies_to_instance(instance, di_container=None):
    """ì¸ìŠ¤í„´ìŠ¤ì— ì˜ì¡´ì„± ì£¼ì… - base_step_mixin.py íŒ¨í„´"""
    try:
        if not di_container:
            di_container = DIHelper.get_di_container()
        
        if not di_container:
            logger.warning("âš ï¸ DI Container ì‚¬ìš© ë¶ˆê°€")
            return False
        
        # ModelLoader ì£¼ì…
        if not hasattr(instance, 'model_loader') or instance.model_loader is None:
            model_loader = di_container.get('IModelLoader')
            if model_loader:
                instance.model_loader = model_loader
                logger.debug("âœ… ModelLoader ì£¼ì… ì™„ë£Œ")
        
        # MemoryManager ì£¼ì…
        if not hasattr(instance, 'memory_manager') or instance.memory_manager is None:
            memory_manager = di_container.get('IMemoryManager')
            if memory_manager:
                instance.memory_manager = memory_manager
                logger.debug("âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
        
        # SafeFunctionValidator ì£¼ì…
        if not hasattr(instance, 'function_validator') or instance.function_validator is None:
            validator = di_container.get('ISafeFunctionValidator')
            if validator:
                instance.function_validator = validator
                logger.debug("âœ… SafeFunctionValidator ì£¼ì… ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        return False

def create_di_compatible_step(step_class: Type, step_name: str, **kwargs):
    """DI í˜¸í™˜ Step ìƒì„± - base_step_mixin.py íŒ¨í„´"""
    try:
        di_container = DIHelper.get_di_container()
        
        if di_container and DI_CONTAINER_AVAILABLE:
            # DIë¥¼ í†µí•œ ìƒì„±
            model_loader = di_container.get('IModelLoader')
            memory_manager = di_container.get('IMemoryManager')
            function_validator = di_container.get('ISafeFunctionValidator')
            
            step_instance = step_class(
                model_loader=model_loader,
                memory_manager=memory_manager,
                function_validator=function_validator,
                **kwargs
            )
        else:
            # í´ë°±: ê¸°ë³¸ ìƒì„±
            step_instance = step_class(**kwargs)
            
            # ìˆ˜ë™ìœ¼ë¡œ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
            inject_dependencies_to_instance(step_instance)
        
        logger.info(f"âœ… DI í˜¸í™˜ Step ìƒì„± ì™„ë£Œ: {step_name}")
        return step_instance
        
    except Exception as e:
        logger.error(f"âŒ DI í˜¸í™˜ Step ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        # ìµœì¢… í´ë°±
        return step_class(**kwargs)

def setup_di_system():
    """DI ì‹œìŠ¤í…œ ì„¤ì • - base_step_mixin.py íŒ¨í„´"""
    try:
        if not DI_CONTAINER_AVAILABLE:
            logger.warning("âš ï¸ DI Container ì‚¬ìš© ë¶ˆê°€")
            return False
        
        # DI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        initialize_di_system()
        
        # ì „ì—­ ModelLoaderë¡œ ì˜ì¡´ì„± ë“±ë¡
        loader = get_global_model_loader()
        
        logger.info("âœ… DI ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ DI ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ 24. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ì •ì˜ (ì™„ì „ DI ê¸°ë°˜)
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'SafeModelService',
    'SafeFunctionValidator',
    'AsyncCompatibilityManager',
    'MemoryManagerAdapter',
    'DeviceManager',
    'ModelMemoryManager',
    
    # DI ê´€ë ¨ í´ë˜ìŠ¤ë“¤
    'DIHelper',
    'SafeConfig',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType',
    'ModelPriority',
    'ModelConfig',
    'StepModelConfig',
    'QualityLevel',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel',
    'U2NetModel',
    'GeometricMatchingModel',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'cleanup_global_loader',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_model_service',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_call_async',
    'is_safely_callable',
    'create_step_interface',
    'create_step_interface_async',
    'get_device_info',
    
    # DI í†µí•© í•¨ìˆ˜ë“¤
    'inject_dependencies_to_instance',
    'create_di_compatible_step',
    'setup_di_system',
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'tensor_to_pil',
    'pil_to_tensor',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'STEP_MODEL_REQUESTS',
    'DI_CONTAINER_AVAILABLE'
]

# ==============================================
# ğŸ”¥ 25. ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# ğŸ”¥ 26. ëª¨ë“ˆ ë¡œë“œ í™•ì¸ ë©”ì‹œì§€ (ì™„ì „ DI ê¸°ë°˜)
# ==============================================

logger.info("âœ… ModelLoader v10.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì™„ì „ DI ê¸°ë°˜ (base_step_mixin.py íŒ¨í„´)")
logger.info("ğŸ”¥ base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("ğŸš€ ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
logger.info("âš¡ TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("ğŸ”§ ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”")
logger.info("ğŸ’‰ ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„")
logger.info("ğŸ›¡ï¸ ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥/í´ë˜ìŠ¤ëª…/í•¨ìˆ˜ëª… 100% ìœ ì§€")
logger.info("ğŸ”§ MemoryManagerAdapter optimize_memory ì™„ì „ êµ¬í˜„")
logger.info("ğŸš€ ë¹„ë™ê¸°(async/await) ì™„ì „ ì§€ì› ê°•í™”")
logger.info("âš¡ StepModelInterface ë¹„ë™ê¸° í˜¸í™˜ ê°•í™”")
logger.info("ğŸ›¡ï¸ SafeModelService ë¹„ë™ê¸° í™•ì¥ ê°•í™”")
logger.info("ğŸ”„ Coroutine 'not callable' ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("ğŸ“ Dict callable ë¬¸ì œ ê·¼ë³¸ í•´ê²°")
logger.info("âŒ AttributeError ì™„ì „ í•´ê²°")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ìœ ì§€")
logger.info("ğŸ“‹ íŒŒì´ì¬ ìµœì í™”ëœ ìˆœì„œë¡œ ì™„ì „ ì •ë¦¬")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")  
logger.info(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
logger.info(f"   - OpenCV/PIL: {'âœ…' if CV_AVAILABLE else 'âŒ'}")
logger.info(f"   - DI Container: {'âœ…' if DI_CONTAINER_AVAILABLE else 'âŒ'}")

if NUMPY_AVAILABLE and hasattr(_compat, 'numpy_version'):
    numpy_major = int(_compat.numpy_version.split('.')[0])
    if numpy_major >= 2:
        logger.warning("âš ï¸ NumPy 2.x ê°ì§€ë¨ - conda install numpy=1.24.3 ê¶Œì¥")
    else:
        logger.info("âœ… NumPy í˜¸í™˜ì„± í™•ì¸ë¨")

logger.info("ğŸš€ ModelLoader v10.0 ì™„ì „ DI ê¸°ë°˜ ì™„ë£Œ!")
logger.info("   âœ… base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("   âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ í•´ê²°")
logger.info("   âœ… TYPE_CHECKINGìœ¼ë¡œ ëŸ°íƒ€ì„ ìˆœí™˜ì°¸ì¡° ë°©ì§€")  
logger.info("   âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•©")
logger.info("   âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì§€ì›")
logger.info("   âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€")
logger.info("   âœ… ë¹„ë™ê¸° ì™„ì „ ì§€ì›")
logger.info("   âœ… ëª¨ë“  ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("   âœ… M3 Max ìµœì í™” ìœ ì§€")
logger.info("   âœ… DI í˜¸í™˜ì„± ì™„ì „ í™•ë³´")

# DI ì‹œìŠ¤í…œ ìë™ ì„¤ì • ì‹œë„
try:
    if DI_CONTAINER_AVAILABLE:
        setup_di_system()
        logger.info("âœ… DI ì‹œìŠ¤í…œ ìë™ ì„¤ì • ì™„ë£Œ")
    else:
        logger.info("â„¹ï¸ DI Container ì—†ìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")
except Exception as e:
    logger.debug(f"DI ì‹œìŠ¤í…œ ìë™ ì„¤ì • ì‹¤íŒ¨: {e}")

logger.info("ğŸ¯ ModelLoader v10.0 - ì™„ì „ DI ê¸°ë°˜ìœ¼ë¡œ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   ğŸ’‰ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("   ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° í•´ê²°") 
logger.info("   ğŸš€ base_step_mixin.pyì™€ ì™„ë²½ ì—°ë™")
logger.info("   âœ… ëª¨ë“  ê¸°ëŠ¥ ë° ì´ë¦„ 100% ìœ ì§€")
logger.info("   ğŸ¯ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€")