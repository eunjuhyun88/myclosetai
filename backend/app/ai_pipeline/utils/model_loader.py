# backend/app/ai_pipeline/utils/model_loader.py
"""
π”¥ MyCloset AI - μ™„μ „ κ°μ„ λ ModelLoader v5.1 (μ‹¤μ  AI λ¨λΈ μ™„μ „ μ§€μ›)
================================================================================
β… step_interface.py v5.2μ™€ μ™„μ „ μ—°λ™ (μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ΅λ”©)
β… RealStepModelInterface μ”κµ¬μ‚¬ν•­ 100% λ°μ
β… GitHubStepMapping μ‹¤μ  AI λ¨λΈ κ²½λ΅ μ™„μ „ λ§¤ν•‘
β… 229GB AI λ¨λΈ νμΌλ“¤ μ •ν™•ν• λ΅λ”© μ§€μ›
β… BaseStepMixin v19.2 μ™„λ²½ νΈν™
β… StepFactory μμ΅΄μ„± μ£Όμ… μ™„λ²½ μ§€μ›
β… Mock μ™„μ „ μ κ±° - μ‹¤μ  μ²΄ν¬ν¬μΈνΈλ§ μ‚¬μ©
β… PyTorch weights_only λ¬Έμ  μ™„μ „ ν•΄κ²°
β… Auto Detector μ™„μ „ μ—°λ™
β… M3 Max 128GB λ©”λ¨λ¦¬ μµμ ν™”
β… λ¨λ“  κΈ°λ¥ μ™„μ „ μ‘λ™

ν•µμ‹¬ κµ¬μ΅° λ§¤ν•‘:
StepFactory (v11.0) β†’ μμ΅΄μ„± μ£Όμ… β†’ BaseStepMixin (v19.2) β†’ step_interface.py (v5.2) β†’ ModelLoader (v5.1) β†’ μ‹¤μ  AI λ¨λΈλ“¤

Author: MyCloset AI Team
Date: 2025-07-30
Version: 5.1 (step_interface.py v5.2 μ™„μ „ νΈν™)
"""

import os
from fix_pytorch_loading import apply_pytorch_patch; apply_pytorch_patch()

import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
import mmap
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod
from io import BytesIO

# ==============================================
# π”¥ 1. μ•μ „ν• λΌμ΄λΈλ¬λ¦¬ Import
# ==============================================

# κΈ°λ³Έ λΌμ΄λΈλ¬λ¦¬λ“¤
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None


# ModelLoaderμ PyTorch import λ¶€λ¶„μ„ λ‹¤μμΌλ΅ κµμ²΄:

# PyTorch μ•μ „ import (weights_only λ¬Έμ  μ™„μ „ ν•΄κ²°)
TORCH_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    # π”¥ PyTorch 2.7 weights_only λ¬Έμ  μ™„μ „ ν•΄κ²°
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            """PyTorch 2.7 νΈν™ μ•μ „ λ΅λ”"""
            # weights_onlyκ°€ Noneμ΄λ©΄ Falseλ΅ μ„¤μ • (Legacy νΈν™)
            if weights_only is None:
                weights_only = False
            
            try:
                # 1λ‹¨κ³„: weights_only=True μ‹λ„ (κ°€μ¥ μ•μ „)
                if weights_only:
                    return original_torch_load(f, map_location=map_location, 
                                             pickle_module=pickle_module, 
                                             weights_only=True, **kwargs)
                
                # 2λ‹¨κ³„: weights_only=False μ‹λ„ (νΈν™μ„±)
                return original_torch_load(f, map_location=map_location, 
                                         pickle_module=pickle_module, 
                                         weights_only=False, **kwargs)
                                         
            except RuntimeError as e:
                error_msg = str(e).lower()
                
                # Legacy .tar ν¬λ§· μ—λ¬ κ°μ§€
                if "legacy .tar format" in error_msg or "weights_only" in error_msg:
                    print(f"β οΈ Legacy ν¬λ§· κ°μ§€, weights_only=Falseλ΅ μ¬μ‹λ„")
                    try:
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # TorchScript μ•„μΉ΄μ΄λΈ μ—λ¬ κ°μ§€
                if "torchscript" in error_msg or "zip file" in error_msg:
                    print(f"β οΈ TorchScript μ•„μΉ΄μ΄λΈ κ°μ§€, weights_only=Falseλ΅ μ¬μ‹λ„")
                    try:
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # λ§μ§€λ§‰ μ‹λ„: λ¨λ“  νλΌλ―Έν„° μ—†μ΄
                try:
                    import warnings
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore")
                        return original_torch_load(f, map_location=map_location)
                except Exception:
                    pass
                
                # μ›λ³Έ μ—λ¬ λ‹¤μ‹ λ°μƒ
                raise e
        
        # torch.load λ€μ²΄
        torch.load = safe_torch_load
        print("β… PyTorch 2.7 weights_only νΈν™μ„± ν¨μΉ μ μ© μ™„λ£")
        
except ImportError:
    torch = None
    print("β οΈ PyTorchκ°€ μ„¤μΉλμ§€ μ•μ")

# λ””λ°”μ΄μ¤ λ° μ‹μ¤ν… μ •λ³΄
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
MPS_AVAILABLE = False

try:
    import platform
    if platform.system() == 'Darwin':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            if 'M3' in result.stdout:
                IS_M3_MAX = True
                if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    DEFAULT_DEVICE = "mps"
                    MPS_AVAILABLE = True
        except:
            pass
except:
    pass

# auto_model_detector import (μ•μ „ μ²λ¦¬)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# TYPE_CHECKING ν¨ν„΄μΌλ΅ μν™μ°Έμ΅° λ°©μ§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# λ΅κΉ… μ„¤μ •
logger = logging.getLogger(__name__)

# ==============================================
# π”¥ 2. step_interface.py v5.2 μ™„μ „ νΈν™ λ°μ΄ν„° κµ¬μ΅°
# ==============================================

class RealStepModelType(Enum):
    """μ‹¤μ  AI Stepμ—μ„ μ‚¬μ©ν•λ” λ¨λΈ νƒ€μ… (step_interface.py μ™„μ „ νΈν™)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class RealModelStatus(Enum):
    """λ¨λΈ λ΅λ”© μƒνƒ (step_interface.py νΈν™)"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class RealModelPriority(Enum):
    """λ¨λΈ μ°μ„ μμ„ (step_interface.py νΈν™)"""
    PRIMARY = 1
    SECONDARY = 2
    FALLBACK = 3
    OPTIONAL = 4

@dataclass
class RealStepModelInfo:
    """μ‹¤μ  AI Step λ¨λΈ μ •λ³΄ (step_interface.py RealAIModelConfig μ™„μ „ νΈν™)"""
    name: str
    path: str
    step_type: RealStepModelType
    priority: RealModelPriority
    device: str
    
    # μ‹¤μ  λ΅λ”© μ •λ³΄
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    checkpoint_data: Optional[Any] = None
    
    # AI Step νΈν™μ„± μ •λ³΄ (step_interface.py νΈν™)
    model_class: Optional[str] = None
    config_path: Optional[str] = None
    preprocessing_params: Dict[str, Any] = field(default_factory=dict)
    
    # step_interface.py μ”κµ¬μ‚¬ν•­
    model_type: str = "BaseModel"
    size_gb: float = 0.0
    requires_checkpoint: bool = True
    checkpoint_key: Optional[str] = None
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # μ„±λ¥ λ©”νΈλ¦­
    access_count: int = 0
    last_access: float = 0.0
    inference_count: int = 0
    avg_inference_time: float = 0.0
    
    # μ—λ¬ μ •λ³΄
    error: Optional[str] = None
    validation_passed: bool = False

@dataclass 
class RealStepModelRequirement:
    """Stepλ³„ λ¨λΈ μ”κµ¬μ‚¬ν•­ (step_interface.py μ™„μ „ νΈν™)"""
    step_name: str
    step_id: int
    step_type: RealStepModelType
    
    # λ¨λΈ μ”κµ¬μ‚¬ν•­
    required_models: List[str] = field(default_factory=list)
    optional_models: List[str] = field(default_factory=list)
    primary_model: Optional[str] = None
    
    # step_interface.py DetailedDataSpec μ—°λ™
    model_configs: Dict[str, Any] = field(default_factory=dict)
    input_data_specs: Dict[str, Any] = field(default_factory=dict)
    output_data_specs: Dict[str, Any] = field(default_factory=dict)
    
    # AI μ¶”λ΅  μ”κµ¬μ‚¬ν•­
    batch_size: int = 1
    precision: str = "fp32"
    memory_limit_mb: Optional[float] = None
    
    # μ „μ²λ¦¬/ν›„μ²λ¦¬ μ”κµ¬μ‚¬ν•­
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)

# ==============================================
# π”¥ 3. μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ΅λ”© μµμ ν™” λ¨λΈ ν΄λμ¤ (step_interface.py μ™„μ „ νΈν™)
# ==============================================

class RealAIModel:
    """μ‹¤μ  AI μ¶”λ΅ μ— μ‚¬μ©ν•  λ¨λΈ ν΄λμ¤ (step_interface.py RealStepModelInterface μ™„μ „ νΈν™)"""
    
    def __init__(self, model_name: str, model_path: str, step_type: RealStepModelType, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.step_type = step_type
        self.device = device if device != "auto" else DEFAULT_DEVICE
        
        # λ΅λ”© μƒνƒ
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.checkpoint_data = None
        self.model_instance = None
        
        # step_interface.py νΈν™μ„ μ„ν• μ†μ„±λ“¤
        self.preprocessing_params = {}
        self.model_class = None
        self.config_path = None
        
        # κ²€μ¦ μƒνƒ
        self.validation_passed = False
        self.compatibility_checked = False
        
        # Logger
        self.logger = logging.getLogger(f"RealAIModel.{model_name}")
        
        # Stepλ³„ νΉν™” λ΅λ” λ§¤ν•‘ (step_interface.py GitHubStepMappingκ³Ό νΈν™)
        self.step_loaders = {
            RealStepModelType.HUMAN_PARSING: self._load_human_parsing_model,
            RealStepModelType.POSE_ESTIMATION: self._load_pose_model,
            RealStepModelType.CLOTH_SEGMENTATION: self._load_segmentation_model,
            RealStepModelType.GEOMETRIC_MATCHING: self._load_geometric_model,
            RealStepModelType.CLOTH_WARPING: self._load_warping_model,
            RealStepModelType.VIRTUAL_FITTING: self._load_diffusion_model,
            RealStepModelType.POST_PROCESSING: self._load_enhancement_model,
            RealStepModelType.QUALITY_ASSESSMENT: self._load_quality_model
        }
        
    def load(self, validate: bool = True) -> bool:
        """λ¨λΈ λ΅λ”© (Stepλ³„ νΉν™” λ΅λ”©, step_interface.py μ™„μ „ νΈν™)"""
        try:
            start_time = time.time()
            
            # νμΌ μ΅΄μ¬ ν™•μΈ
            if not self.model_path.exists():
                self.logger.error(f"β λ¨λΈ νμΌ μ—†μ: {self.model_path}")
                return False
            
            # νμΌ ν¬κΈ° ν™•μΈ
            file_size = self.model_path.stat().st_size
            self.memory_usage_mb = file_size / (1024 * 1024)
            
            self.logger.info(f"π”„ {self.step_type.value} λ¨λΈ λ΅λ”© μ‹μ‘: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            
            # Stepλ³„ νΉν™” λ΅λ”© (step_interface.py GitHubStepMapping κΈ°λ°)
            success = False
            if self.step_type in self.step_loaders:
                success = self.step_loaders[self.step_type]()
            else:
                success = self._load_generic_model()
            
            if success:
                self.load_time = time.time() - start_time
                self.loaded = True
                
                # κ²€μ¦ μν–‰
                if validate:
                    self.validation_passed = self._validate_model()
                else:
                    self.validation_passed = True
                
                self.logger.info(f"β… {self.step_type.value} λ¨λΈ λ΅λ”© μ™„λ£: {self.model_name} ({self.load_time:.2f}μ΄)")
                return True
            else:
                self.logger.error(f"β {self.step_type.value} λ¨λΈ λ΅λ”© μ‹¤ν¨: {self.model_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"β λ¨λΈ λ΅λ”© μ¤‘ μ¤λ¥: {e}")
            return False
    
    def _load_human_parsing_model(self) -> bool:
        """Human Parsing λ¨λΈ λ΅λ”© (Graphonomy, ATR λ“±) - step_interface.py νΈν™"""
        try:
            # Graphonomy νΉλ³„ μ²λ¦¬ (1.2GB)
            if "graphonomy" in self.model_name.lower():
                return self._load_graphonomy_ultra_safe()
            
            # ATR λ¨λΈ μ²λ¦¬
            if "atr" in self.model_name.lower() or "schp" in self.model_name.lower():
                return self._load_atr_model()
            
            # μΌλ° PyTorch λ¨λΈ
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Human Parsing λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_pose_model(self) -> bool:
        """Pose Estimation λ¨λΈ λ΅λ”© (YOLO, OpenPose λ“±) - step_interface.py νΈν™"""
        try:
            # YOLO λ¨λΈ μ²λ¦¬
            if "yolo" in self.model_name.lower():
                self.checkpoint_data = self._load_yolo_model()
            # OpenPose λ¨λΈ μ²λ¦¬
            elif "openpose" in self.model_name.lower() or "pose" in self.model_name.lower():
                self.checkpoint_data = self._load_openpose_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Pose Estimation λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_segmentation_model(self) -> bool:
        """Segmentation λ¨λΈ λ΅λ”© (SAM, U2Net λ“±) - step_interface.py νΈν™"""
        try:
            # SAM λ¨λΈ μ²λ¦¬ (2.4GB)
            if "sam" in self.model_name.lower():
                self.checkpoint_data = self._load_sam_model()
            # U2Net λ¨λΈ μ²λ¦¬ (176GB)
            elif "u2net" in self.model_name.lower():
                self.checkpoint_data = self._load_u2net_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Segmentation λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_geometric_model(self) -> bool:
        """Geometric Matching λ¨λΈ λ΅λ”© - step_interface.py νΈν™"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Geometric Matching λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_warping_model(self) -> bool:
        """Cloth Warping λ¨λΈ λ΅λ”© (RealVisXL λ“±) - step_interface.py νΈν™"""
        try:
            # RealVisXL Safetensors νμΌ μ²λ¦¬ (6.46GB)
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Cloth Warping λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_diffusion_model(self) -> bool:
        """Virtual Fitting λ¨λΈ λ΅λ”© (Stable Diffusion λ“±) - step_interface.py νΈν™"""
        try:
            # Safetensors μ°μ„  μ²λ¦¬ (4.8GB)
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            # Diffusion λ¨λΈ νΉλ³„ μ²λ¦¬
            elif "diffusion" in self.model_name.lower():
                self.checkpoint_data = self._load_diffusion_checkpoint()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Virtual Fitting λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_enhancement_model(self) -> bool:
        """Post Processing λ¨λΈ λ΅λ”© (Real-ESRGAN λ“±) - step_interface.py νΈν™"""
        try:
            # Real-ESRGAN νΉλ³„ μ²λ¦¬ (64GB)
            if "esrgan" in self.model_name.lower():
                self.checkpoint_data = self._load_esrgan_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Post Processing λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_quality_model(self) -> bool:
        """Quality Assessment λ¨λΈ λ΅λ”© (CLIP, ViT λ“±) - step_interface.py νΈν™"""
        try:
            # CLIP λ¨λΈ μ²λ¦¬ (890MB)
            if "clip" in self.model_name.lower() or "vit" in self.model_name.lower():
                self.checkpoint_data = self._load_clip_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"β Quality Assessment λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_generic_model(self) -> bool:
        """μΌλ° λ¨λΈ λ΅λ”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"β μΌλ° λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    # ==============================================
    # π”¥ νΉν™” λ΅λ”λ“¤ (step_interface.py μ‹¤μ  λ¨λΈ κ²½λ΅ κΈ°λ°)
    # ==============================================
    def _load_pytorch_checkpoint(self) -> Optional[Any]:
        """PyTorch μ²΄ν¬ν¬μΈνΈ λ΅λ”© (PyTorch 2.7 μ™„μ „ νΈν™)"""
        if not TORCH_AVAILABLE:
            self.logger.error("β PyTorchκ°€ μ‚¬μ© λ¶κ°€λ¥")
            return None
        
        try:
            import warnings
            
            # 1λ‹¨κ³„: μ•μ „ λ¨λ“ (weights_only=True)
            try:
                checkpoint = torch.load(
                    self.model_path, 
                    map_location='cpu',
                    weights_only=True
                )
                self.logger.debug(f"β… {self.model_name} μ•μ „ λ¨λ“ λ΅λ”© μ„±κ³µ")
                return checkpoint
            except RuntimeError as safe_error:
                error_msg = str(safe_error).lower()
                if "legacy .tar format" in error_msg or "torchscript" in error_msg:
                    self.logger.debug(f"Legacy/TorchScript νμΌ κ°μ§€: {self.model_name}")
                else:
                    self.logger.debug(f"μ•μ „ λ¨λ“ μ‹¤ν¨: {safe_error}")
            except Exception as e:
                self.logger.debug(f"μ•μ „ λ¨λ“ μμ™Έ: {e}")
            
            # 2λ‹¨κ³„: νΈν™ λ¨λ“ (weights_only=False)
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    checkpoint = torch.load(
                        self.model_path, 
                        map_location='cpu',
                        weights_only=False
                    )
                self.logger.debug(f"β… {self.model_name} νΈν™ λ¨λ“ λ΅λ”© μ„±κ³µ")
                return checkpoint
            except Exception as compat_error:
                self.logger.debug(f"νΈν™ λ¨λ“ μ‹¤ν¨: {compat_error}")
            
            # 3λ‹¨κ³„: Legacy λ¨λ“ (νλΌλ―Έν„° μµμ†ν™”)
            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    checkpoint = torch.load(self.model_path, map_location='cpu')
                self.logger.debug(f"β… {self.model_name} Legacy λ¨λ“ λ΅λ”© μ„±κ³µ")
                return checkpoint
            except Exception as legacy_error:
                self.logger.error(f"β λ¨λ“  λ΅λ”© λ°©λ²• μ‹¤ν¨: {legacy_error}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"β PyTorch μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    def _load_safetensors(self) -> Optional[Any]:
        """Safetensors νμΌ λ΅λ”© (RealVisXL, Diffusion λ“±)"""
        try:
            import safetensors.torch
            checkpoint = safetensors.torch.load_file(str(self.model_path))
            self.logger.debug(f"β… {self.model_name} Safetensors λ΅λ”© μ„±κ³µ")
            return checkpoint
        except ImportError:
            self.logger.warning("β οΈ Safetensors λΌμ΄λΈλ¬λ¦¬ μ—†μ, PyTorch λ΅λ”© μ‹λ„")
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"β Safetensors λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_graphonomy_ultra_safe(self) -> bool:
        """Graphonomy 1.2GB λ¨λΈ μ΄μ•μ „ λ΅λ”© (step_interface.py κ²½λ΅ κΈ°λ°)"""
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # λ©”λ¨λ¦¬ λ§¤ν•‘ λ°©λ²•
                try:
                    with open(self.model_path, 'rb') as f:
                        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
                            checkpoint = torch.load(
                                BytesIO(mmapped_file[:]), 
                                map_location='cpu',
                                weights_only=False
                            )
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("β… Graphonomy λ©”λ¨λ¦¬ λ§¤ν•‘ λ΅λ”© μ„±κ³µ")
                    return True
                    
                except Exception:
                    pass
                
                # μ§μ ‘ pickle λ΅λ”©
                try:
                    with open(self.model_path, 'rb') as f:
                        checkpoint = pickle.load(f)
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("β… Graphonomy μ§μ ‘ pickle λ΅λ”© μ„±κ³µ")
                    return True
                    
                except Exception:
                    pass
                
                # ν΄λ°±: μΌλ° PyTorch λ΅λ”©
                self.checkpoint_data = self._load_pytorch_checkpoint()
                return self.checkpoint_data is not None
                
        except Exception as e:
            self.logger.error(f"β Graphonomy μ΄μ•μ „ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_atr_model(self) -> bool:
        """ATR/SCHP λ¨λΈ λ΅λ”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"β ATR λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return False
    
    def _load_yolo_model(self) -> Optional[Any]:
        """YOLO λ¨λΈ λ΅λ”© (6.2GB)"""
        try:
            # YOLOv8 λ¨λΈμΈ κ²½μ°
            if "v8" in self.model_name.lower():
                try:
                    from ultralytics import YOLO
                    model = YOLO(str(self.model_path))
                    self.model_instance = model
                    return {"model": model, "type": "yolov8"}
                except ImportError:
                    pass
            
            # μΌλ° PyTorch λ¨λΈλ΅ λ΅λ”©
            return self._load_pytorch_checkpoint()
            
        except Exception as e:
            self.logger.error(f"β YOLO λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_openpose_model(self) -> Optional[Any]:
        """OpenPose λ¨λΈ λ΅λ”©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"β OpenPose λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_sam_model(self) -> Optional[Any]:
        """SAM λ¨λΈ λ΅λ”© (2.4GB)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            if checkpoint and isinstance(checkpoint, dict):
                if "model" in checkpoint:
                    return checkpoint
                elif "state_dict" in checkpoint:
                    return checkpoint
                else:
                    return {"model": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"β SAM λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_u2net_model(self) -> Optional[Any]:
        """U2Net λ¨λΈ λ΅λ”© (176GB)"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"β U2Net λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_diffusion_checkpoint(self) -> Optional[Any]:
        """Diffusion λ¨λΈ μ²΄ν¬ν¬μΈνΈ λ΅λ”© (4.8GB)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            
            # Diffusion λ¨λΈ κµ¬μ΅° μ •κ·ν™”
            if checkpoint and isinstance(checkpoint, dict):
                if "state_dict" in checkpoint:
                    return checkpoint
                elif "model" in checkpoint:
                    return checkpoint
                else:
                    return {"state_dict": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"β Diffusion μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_esrgan_model(self) -> Optional[Any]:
        """Real-ESRGAN λ¨λΈ λ΅λ”© (64GB)"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"β Real-ESRGAN λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_clip_model(self) -> Optional[Any]:
        """CLIP λ¨λΈ λ΅λ”© (890MB)"""
        try:
            # .bin νμΌμΈ κ²½μ°
            if self.model_path.suffix.lower() == '.bin':
                checkpoint = torch.load(self.model_path, map_location='cpu')
            else:
                checkpoint = self._load_pytorch_checkpoint()
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"β CLIP λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _validate_model(self) -> bool:
        """λ¨λΈ κ²€μ¦"""
        try:
            if self.checkpoint_data is None:
                return False
            
            # κΈ°λ³Έ κ²€μ¦
            if not isinstance(self.checkpoint_data, (dict, torch.nn.Module)) and self.checkpoint_data is not None:
                self.logger.warning(f"β οΈ μμƒμΉ λ»ν• μ²΄ν¬ν¬μΈνΈ νƒ€μ…: {type(self.checkpoint_data)}")
            
            # Stepλ³„ νΉν™” κ²€μ¦
            if self.step_type == RealStepModelType.HUMAN_PARSING:
                return self._validate_human_parsing_model()
            elif self.step_type == RealStepModelType.VIRTUAL_FITTING:
                return self._validate_diffusion_model()
            else:
                return True
                
        except Exception as e:
            self.logger.error(f"β λ¨λΈ κ²€μ¦ μ‹¤ν¨: {e}")
            return False
    
    def _validate_human_parsing_model(self) -> bool:
        """Human Parsing λ¨λΈ κ²€μ¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    expected_keys = ["backbone", "decoder", "classifier"]
                    for key in expected_keys:
                        if any(key in k for k in state_dict.keys()):
                            return True
                
                if any("conv" in k or "bn" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"β οΈ Human Parsing λ¨λΈ κ²€μ¦ μ¤‘ μ¤λ¥: {e}")
            return True
    
    def _validate_diffusion_model(self) -> bool:
        """Diffusion λ¨λΈ κ²€μ¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    if any("down_blocks" in k or "up_blocks" in k for k in state_dict.keys()):
                        return True
                
                if any("time_embed" in k or "input_blocks" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"β οΈ Diffusion λ¨λΈ κ²€μ¦ μ¤‘ μ¤λ¥: {e}")
            return True
    
    # ==============================================
    # π”¥ step_interface.py νΈν™ λ©”μ„λ“λ“¤
    # ==============================================
    
    def get_checkpoint_data(self) -> Optional[Any]:
        """λ΅λ“λ μ²΄ν¬ν¬μΈνΈ λ°μ΄ν„° λ°ν™ (step_interface.py νΈν™)"""
        return self.checkpoint_data
    
    def get_model_instance(self) -> Optional[Any]:
        """μ‹¤μ  λ¨λΈ μΈμ¤ν„΄μ¤ λ°ν™ (step_interface.py νΈν™)"""
        return self.model_instance
    
    def unload(self):
        """λ¨λΈ μ–Έλ΅λ“ (step_interface.py νΈν™)"""
        self.loaded = False
        self.checkpoint_data = None
        self.model_instance = None
        gc.collect()
        
        # MPS λ©”λ¨λ¦¬ μ •λ¦¬
        if MPS_AVAILABLE and TORCH_AVAILABLE:
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except:
                pass
    
    def get_info(self) -> Dict[str, Any]:
        """λ¨λΈ μ •λ³΄ λ°ν™ (step_interface.py νΈν™)"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "step_type": self.step_type.value,
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0,
            "has_checkpoint_data": self.checkpoint_data is not None,
            "has_model_instance": self.model_instance is not None,
            "validation_passed": self.validation_passed,
            "compatibility_checked": self.compatibility_checked,
            
            # step_interface.py νΈν™ μ¶”κ°€ ν•„λ“
            "model_type": getattr(self, 'model_type', 'BaseModel'),
            "size_gb": self.memory_usage_mb / 1024 if self.memory_usage_mb > 0 else 0,
            "requires_checkpoint": True,
            "preprocessing_required": getattr(self, 'preprocessing_required', []),
            "postprocessing_required": getattr(self, 'postprocessing_required', [])
        }

# ==============================================
# π”¥ 4. step_interface.py μ™„μ „ νΈν™ λ¨λΈ μΈν„°νμ΄μ¤
# ==============================================

class RealStepModelInterface:
    """step_interface.py v5.2 RealStepModelInterface μ™„μ „ νΈν™ κµ¬ν„"""
    
    def __init__(self, model_loader, step_name: str, step_type: RealStepModelType):
        self.model_loader = model_loader
        self.step_name = step_name
        self.step_type = step_type
        self.logger = logging.getLogger(f"RealStepInterface.{step_name}")
        
        # Stepλ³„ λ¨λΈλ“¤ (step_interface.py νΈν™)
        self.step_models: Dict[str, RealAIModel] = {}
        self.primary_model: Optional[RealAIModel] = None
        self.fallback_models: List[RealAIModel] = []
        
        # step_interface.py μ”κµ¬μ‚¬ν•­ μ—°λ™
        self.requirements: Optional[RealStepModelRequirement] = None
        self.data_specs_loaded: bool = False
        
        # μ„±λ¥ λ©”νΈλ¦­ (step_interface.py νΈν™)
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        # μΊμ‹ (step_interface.py νΈν™)
        self.model_cache: Dict[str, Any] = {}
        self.preprocessing_cache: Dict[str, Any] = {}
        
        # step_interface.py ν†µκ³„ νΈν™
        self.real_statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'real_checkpoints_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'creation_time': time.time()
        }
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """step_interface.py DetailedDataSpec κΈ°λ° μ”κµ¬μ‚¬ν•­ λ“±λ΅"""
        try:
            self.requirements = RealStepModelRequirement(
                step_name=self.step_name,
                step_id=requirements.get('step_id', 0),
                step_type=self.step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.data_specs_loaded = True
            self.logger.info(f"β… step_interface.py νΈν™ μ”κµ¬μ‚¬ν•­ λ“±λ΅: {len(self.requirements.required_models)}κ° ν•„μ λ¨λΈ")
            
        except Exception as e:
            self.logger.error(f"β μ”κµ¬μ‚¬ν•­ λ“±λ΅ μ‹¤ν¨: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """μ‹¤μ  AI λ¨λΈ λ°ν™ (step_interface.py νΈν™)"""
        try:
            self.access_count += 1
            
            # νΉμ • λ¨λΈ μ”μ²­
            if model_name:
                if model_name in self.step_models:
                    model = self.step_models[model_name]
                    model.access_count += 1
                    model.last_access = time.time()
                    self.real_statistics['cache_hits'] += 1
                    return model
                
                # μƒ λ¨λΈ λ΅λ”©
                return self._load_new_model(model_name)
            
            # κΈ°λ³Έ λ¨λΈ λ°ν™ (step_interface.py νΈν™)
            if self.primary_model and self.primary_model.loaded:
                return self.primary_model
            
            # λ΅λ“λ λ¨λΈ μ¤‘ κ°€μ¥ μ°μ„ μμ„ λ†’μ€ κ²ƒ
            for model in sorted(self.step_models.values(), key=lambda m: getattr(m, 'priority', 999)):
                if model.loaded:
                    return model
            
            # μ²« λ²μ§Έ λ¨λΈ λ΅λ”© μ‹λ„
            if self.requirements and self.requirements.required_models:
                return self._load_new_model(self.requirements.required_models[0])
            
            return None
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"β λ¨λΈ μ΅°ν μ‹¤ν¨: {e}")
            return None
    
    def _load_new_model(self, model_name: str) -> Optional[RealAIModel]:
        """μƒ λ¨λΈ λ΅λ”© (step_interface.py νΈν™)"""
        try:
            # ModelLoaderλ¥Ό ν†µν• λ΅λ”©
            base_model = self.model_loader.load_model(model_name, step_name=self.step_name, step_type=self.step_type)
            
            if base_model and isinstance(base_model, RealAIModel):
                self.step_models[model_name] = base_model
                
                # Primary λ¨λΈ μ„¤μ •
                if not self.primary_model or (self.requirements and model_name == self.requirements.primary_model):
                    self.primary_model = base_model
                
                # ν†µκ³„ μ—…λ°μ΄νΈ (step_interface.py νΈν™)
                self.real_statistics['models_loaded'] += 1
                self.real_statistics['real_ai_calls'] += 1
                if base_model.checkpoint_data is not None:
                    self.real_statistics['real_checkpoints_loaded'] += 1
                
                return base_model
            else:
                self.real_statistics['cache_misses'] += 1
                self.real_statistics['loading_failures'] += 1
            
            return None
            
        except Exception as e:
            self.logger.error(f"β μƒ λ¨λΈ λ΅λ”© μ‹¤ν¨ {model_name}: {e}")
            self.real_statistics['loading_failures'] += 1
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """λ™κΈ° λ¨λΈ μ΅°ν - step_interface.py BaseStepMixin νΈν™"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """λΉ„λ™κΈ° λ¨λΈ μ΅°ν (step_interface.py νΈν™)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"β λΉ„λ™κΈ° λ¨λΈ μ΅°ν μ‹¤ν¨: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅ - step_interface.py BaseStepMixin νΈν™"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'step_type': self.step_type.value,
                'required': kwargs.get('required', True),
                'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                'device': kwargs.get('device', DEFAULT_DEVICE),
                'preprocessing_params': kwargs.get('preprocessing_params', {}),
                **kwargs
            }
            
            self.real_statistics['models_registered'] += 1
            self.logger.info(f"β… step_interface.py νΈν™ λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"β λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅ μ‹¤ν¨: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ (step_interface.py νΈν™)"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"β λ¨λΈ λ©λ΅ μ΅°ν μ‹¤ν¨: {e}")
            return []
    
    def cleanup(self):
        """λ¦¬μ†μ¤ μ •λ¦¬ (step_interface.py νΈν™)"""
        try:
            # λ©”λ¨λ¦¬ ν•΄μ 
            for model_name, model in self.step_models.items():
                if hasattr(model, 'unload'):
                    model.unload()
            
            self.step_models.clear()
            self.model_cache.clear()
            
            self.logger.info(f"β… step_interface.py νΈν™ {self.step_name} Interface μ •λ¦¬ μ™„λ£")
        except Exception as e:
            self.logger.error(f"β Interface μ •λ¦¬ μ‹¤ν¨: {e}")

# νΈν™μ„±μ„ μ„ν• λ³„μΉ­
EnhancedStepModelInterface = RealStepModelInterface
StepModelInterface = RealStepModelInterface

# ==============================================
# π”¥ 5. μ™„μ „ κ°μ„ λ ModelLoader ν΄λμ¤ v5.1 (step_interface.py μ™„μ „ νΈν™)
# ==============================================

class ModelLoader:
    """
    π”¥ μ™„μ „ κ°μ„ λ ModelLoader v5.1 - step_interface.py v5.2 μ™„μ „ νΈν™
    
    ν•µμ‹¬ κ°μ„ μ‚¬ν•­:
    - step_interface.py RealStepModelInterface μ”κµ¬μ‚¬ν•­ 100% λ°μ
    - GitHubStepMapping μ‹¤μ  AI λ¨λΈ κ²½λ΅ μ™„μ „ λ§¤ν•‘ 
    - 229GB AI λ¨λΈ νμΌλ“¤ μ •ν™•ν• λ΅λ”© μ§€μ›
    - BaseStepMixin v19.2 μ™„λ²½ νΈν™
    - StepFactory μμ΅΄μ„± μ£Όμ… μ™„λ²½ μ§€μ›
    - auto_model_detector μ™„μ „ μ—°λ™
    - λ¨λ“  κΈ°λ¥ μ™„μ „ μ‘λ™
    """
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):
        """ModelLoader μ΄κΈ°ν™” (step_interface.py μ™„μ „ νΈν™)"""
        
        # κΈ°λ³Έ μ„¤μ •
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # λ¨λΈ μΊμ‹ λ””λ ‰ν† λ¦¬ μ„¤μ • (step_interface.py AI_MODELS_ROOT νΈν™)
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # step_interface.py AI_MODELS_ROOT κ²½λ΅ λ§¤ν•‘
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # μ‹¤μ  AI λ¨λΈ κ΄€λ¦¬ (step_interface.py νΈν™)
        self.loaded_models: Dict[str, RealAIModel] = {}
        self.model_info: Dict[str, RealStepModelInfo] = {}
        self.model_status: Dict[str, RealModelStatus] = {}
        
        # Step μ”κµ¬μ‚¬ν•­ (step_interface.py νΈν™)
        self.step_requirements: Dict[str, RealStepModelRequirement] = {}
        self.step_interfaces: Dict[str, RealStepModelInterface] = {}
        
        # auto_model_detector μ—°λ™
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # μ„±λ¥ λ©”νΈλ¦­ (step_interface.py νΈν™)
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0,
            'inference_count': 0,
            'total_inference_time': 0.0
        }
        
        # λ™κΈ°ν™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # step_interface.py GitHubStepMapping λ΅λ”©
        self._load_step_interface_mappings()
        
        self.logger.info(f"π€ μ™„μ „ κ°μ„ λ ModelLoader v5.1 μ΄κΈ°ν™” μ™„λ£ (step_interface.py v5.2 μ™„μ „ νΈν™)")
        self.logger.info(f"π“± Device: {self.device} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
        self.logger.info(f"π“ λ¨λΈ μΊμ‹: {self.model_cache_dir}")
        self.logger.info(f"π― step_interface.py μ‹¤μ  AI Step νΈν™ λ¨λ“")
    
    def _initialize_auto_detector(self):
        """auto_model_detector μ΄κΈ°ν™” (step_interface.py νΈν™)"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("β… auto_model_detector μ—°λ™ μ™„λ£")
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("β οΈ auto_detector μΈμ¤ν„΄μ¤κ°€ None")
            else:
                self.logger.warning("β οΈ AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"β auto_model_detector μ΄κΈ°ν™” μ‹¤ν¨: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector ν†µν•© (step_interface.py νΈν™)"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                # Step νƒ€μ… μ¶”λ΅ 
                                step_type = self._infer_step_type(model_name, model_path)
                                
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "step_type": step_type.value if step_type else 'unknown',
                                    "model_type": self._infer_model_type(model_name),
                                    "auto_detected": True,
                                    "priority": self._infer_model_priority(model_name),
                                    # step_interface.py νΈν™ ν•„λ“
                                    "loaded": False,
                                    "step_id": self._get_step_id_from_step_type(step_type),
                                    "device": self.device,
                                    "real_ai_model": True
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"β… AutoDetector step_interface.py ν†µν•© μ™„λ£: {integrated_count}κ° λ¨λΈ")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"β AutoDetector ν†µν•© μ‹¤ν¨: {e}")
            return False
    
    def _infer_step_type(self, model_name: str, model_path: str) -> Optional[RealStepModelType]:
        """λ¨λΈλ…κ³Ό κ²½λ΅λ΅ Step νƒ€μ… μ¶”λ΅  (step_interface.py GitHubStepType νΈν™)"""
        model_name_lower = model_name.lower()
        model_path_lower = model_path.lower()
        
        # κ²½λ΅ κΈ°λ° μ¶”λ΅  (step_interface.py κµ¬μ΅°)
        if "step_01" in model_path_lower or "human_parsing" in model_path_lower:
            return RealStepModelType.HUMAN_PARSING
        elif "step_02" in model_path_lower or "pose" in model_path_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif "step_03" in model_path_lower or "segmentation" in model_path_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif "step_04" in model_path_lower or "geometric" in model_path_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif "step_05" in model_path_lower or "warping" in model_path_lower:
            return RealStepModelType.CLOTH_WARPING
        elif "step_06" in model_path_lower or "virtual" in model_path_lower or "fitting" in model_path_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif "step_07" in model_path_lower or "post" in model_path_lower:
            return RealStepModelType.POST_PROCESSING
        elif "step_08" in model_path_lower or "quality" in model_path_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        # λ¨λΈλ… κΈ°λ° μ¶”λ΅  (step_interface.py GitHubStepMapping κΈ°λ°)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "atr", "schp"]):
            return RealStepModelType.HUMAN_PARSING
        elif any(keyword in model_name_lower for keyword in ["yolo", "openpose", "pose"]):
            return RealStepModelType.POSE_ESTIMATION
        elif any(keyword in model_name_lower for keyword in ["sam", "u2net", "segment"]):
            return RealStepModelType.CLOTH_SEGMENTATION
        elif any(keyword in model_name_lower for keyword in ["gmm", "tps", "geometric"]):
            return RealStepModelType.GEOMETRIC_MATCHING
        elif any(keyword in model_name_lower for keyword in ["realvis", "vgg", "warping"]):
            return RealStepModelType.CLOTH_WARPING
        elif any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet", "unet", "vae"]):
            return RealStepModelType.VIRTUAL_FITTING
        elif any(keyword in model_name_lower for keyword in ["esrgan", "sr", "enhancement"]):
            return RealStepModelType.POST_PROCESSING
        elif any(keyword in model_name_lower for keyword in ["clip", "vit", "quality"]):
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return None
    
    def _infer_model_type(self, model_name: str) -> str:
        """λ¨λΈ νƒ€μ… μ¶”λ΅  (step_interface.py νΈν™)"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet"]):
            return "DiffusionModel"
        elif any(keyword in model_name_lower for keyword in ["yolo", "detection"]):
            return "DetectionModel"
        elif any(keyword in model_name_lower for keyword in ["segment", "sam", "u2net"]):
            return "SegmentationModel"
        elif any(keyword in model_name_lower for keyword in ["pose", "openpose"]):
            return "PoseModel"
        elif any(keyword in model_name_lower for keyword in ["clip", "vit"]):
            return "ClassificationModel"
        else:
            return "BaseModel"
    
    def _infer_model_priority(self, model_name: str) -> int:
        """λ¨λΈ μ°μ„ μμ„ μ¶”λ΅  (step_interface.py νΈν™)"""
        model_name_lower = model_name.lower()
        
        # Primary λ¨λΈλ“¤ (step_interface.py GitHubStepMapping κΈ°λ°)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "yolo", "sam", "diffusion", "esrgan", "clip"]):
            return RealModelPriority.PRIMARY.value
        elif any(keyword in model_name_lower for keyword in ["atr", "openpose", "u2net", "vgg"]):
            return RealModelPriority.SECONDARY.value
        else:
            return RealModelPriority.OPTIONAL.value
    
    def _get_step_id_from_step_type(self, step_type: Optional[RealStepModelType]) -> int:
        """Step νƒ€μ…μ—μ„ ID μ¶”μ¶ (step_interface.py νΈν™)"""
        if not step_type:
            return 0
        
        step_id_map = {
            RealStepModelType.HUMAN_PARSING: 1,
            RealStepModelType.POSE_ESTIMATION: 2,
            RealStepModelType.CLOTH_SEGMENTATION: 3,
            RealStepModelType.GEOMETRIC_MATCHING: 4,
            RealStepModelType.CLOTH_WARPING: 5,
            RealStepModelType.VIRTUAL_FITTING: 6,
            RealStepModelType.POST_PROCESSING: 7,
            RealStepModelType.QUALITY_ASSESSMENT: 8
        }
        return step_id_map.get(step_type, 0)
    
    def _load_step_interface_mappings(self):
        """step_interface.py GitHubStepMapping λ΅λ”©"""
        try:
            # step_interface.py GitHubStepMapping κµ¬μ΅° λ°μ
            self.step_interface_mappings = {
                'HumanParsingStep': {
                    'step_type': RealStepModelType.HUMAN_PARSING,
                    'step_id': 1,
                    'ai_models': [
                        'graphonomy.pth',  # 1.2GB
                        'exp-schp-201908301523-atr.pth',  # 255MB
                        'pytorch_model.bin'  # 168MB
                    ],
                    'primary_model': 'graphonomy.pth',
                    'local_paths': [
                        'step_01_human_parsing/graphonomy.pth',
                        'step_01_human_parsing/exp-schp-201908301523-atr.pth'
                    ]
                },
                'PoseEstimationStep': {
                    'step_type': RealStepModelType.POSE_ESTIMATION,
                    'step_id': 2,
                    'ai_models': [
                        'yolov8n-pose.pt'  # 6.2GB
                    ],
                    'primary_model': 'yolov8n-pose.pt',
                    'local_paths': [
                        'step_02_pose_estimation/yolov8n-pose.pt'
                    ]
                },
                'ClothSegmentationStep': {
                    'step_type': RealStepModelType.CLOTH_SEGMENTATION,
                    'step_id': 3,
                    'ai_models': [
                        'sam_vit_h_4b8939.pth',  # 2.4GB
                        'u2net.pth'  # 176GB
                    ],
                    'primary_model': 'sam_vit_h_4b8939.pth',
                    'local_paths': [
                        'step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'step_03_cloth_segmentation/u2net.pth'
                    ]
                },
                'GeometricMatchingStep': {
                    'step_type': RealStepModelType.GEOMETRIC_MATCHING,
                    'step_id': 4,
                    'ai_models': [
                        'gmm_final.pth'  # 1.3GB
                    ],
                    'primary_model': 'gmm_final.pth',
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth'
                    ]
                },
                'ClothWarpingStep': {
                    'step_type': RealStepModelType.CLOTH_WARPING,
                    'step_id': 5,
                    'ai_models': [
                        'RealVisXL_V4.0.safetensors'  # 6.46GB
                    ],
                    'primary_model': 'RealVisXL_V4.0.safetensors',
                    'local_paths': [
                        'step_05_cloth_warping/RealVisXL_V4.0.safetensors'
                    ]
                },
                'VirtualFittingStep': {
                    'step_type': RealStepModelType.VIRTUAL_FITTING,
                    'step_id': 6,
                    'ai_models': [
                        'diffusion_pytorch_model.fp16.safetensors',  # 4.8GB
                        'v1-5-pruned-emaonly.safetensors'  # 4.0GB
                    ],
                    'primary_model': 'diffusion_pytorch_model.fp16.safetensors',
                    'local_paths': [
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.fp16.safetensors',
                        'step_06_virtual_fitting/v1-5-pruned-emaonly.safetensors'
                    ]
                },
                'PostProcessingStep': {
                    'step_type': RealStepModelType.POST_PROCESSING,
                    'step_id': 7,
                    'ai_models': [
                        'Real-ESRGAN_x4plus.pth'  # 64GB
                    ],
                    'primary_model': 'Real-ESRGAN_x4plus.pth',
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth'
                    ]
                },
                'QualityAssessmentStep': {
                    'step_type': RealStepModelType.QUALITY_ASSESSMENT,
                    'step_id': 8,
                    'ai_models': [
                        'ViT-L-14.pt'  # 890MB
                    ],
                    'primary_model': 'ViT-L-14.pt',
                    'local_paths': [
                        'step_08_quality_assessment/ViT-L-14.pt'
                    ]
                }
            }
            
            self.logger.info(f"β… step_interface.py GitHubStepMapping λ΅λ”© μ™„λ£: {len(self.step_interface_mappings)}κ° Step")
            
        except Exception as e:
            self.logger.error(f"β step_interface.py λ§¤ν•‘ λ΅λ”© μ‹¤ν¨: {e}")
            self.step_interface_mappings = {}
    
    # ==============================================
    # π”¥ ν•µμ‹¬ λ¨λΈ λ΅λ”© λ©”μ„λ“λ“¤ (step_interface.py μ™„μ „ νΈν™)
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """μ‹¤μ  AI λ¨λΈ λ΅λ”© (step_interface.py RealStepModelInterface μ™„μ „ νΈν™)"""
        try:
            with self._lock:
                # μΊμ‹ ν™•μΈ
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        model.access_count += 1
                        model.last_access = time.time()
                        self.logger.debug(f"β™»οΈ μΊμ‹λ μ‹¤μ  AI λ¨λΈ λ°ν™: {model_name}")
                        return model
                
                # μƒ λ¨λΈ λ΅λ”©
                self.model_status[model_name] = RealModelStatus.LOADING
                
                # λ¨λΈ κ²½λ΅ λ° Step νƒ€μ… κ²°μ • (step_interface.py κ²½λ΅ κΈ°λ°)
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"β λ¨λΈ κ²½λ΅λ¥Ό μ°Ύμ„ μ μ—†μ: {model_name}")
                    self.model_status[model_name] = RealModelStatus.ERROR
                    return None
                
                # Step νƒ€μ… μ¶”λ΅  (step_interface.py νΈν™)
                step_type = kwargs.get('step_type')
                if not step_type:
                    step_type = self._infer_step_type(model_name, model_path)
                
                if not step_type:
                    step_type = RealStepModelType.HUMAN_PARSING  # κΈ°λ³Έκ°’
                
                # RealAIModel μƒμ„± λ° λ΅λ”©
                model = RealAIModel(
                    model_name=model_name,
                    model_path=model_path,
                    step_type=step_type,
                    device=self.device
                )
                
                # λ¨λΈ λ΅λ”© μν–‰
                if model.load(validate=kwargs.get('validate', True)):
                    # μΊμ‹μ— μ €μ¥
                    self.loaded_models[model_name] = model
                    
                    # λ¨λΈ μ •λ³΄ μ €μ¥ (step_interface.py νΈν™)
                    priority = RealModelPriority(kwargs.get('priority', RealModelPriority.SECONDARY.value))
                    self.model_info[model_name] = RealStepModelInfo(
                        name=model_name,
                        path=model_path,
                        step_type=step_type,
                        priority=priority,
                        device=self.device,
                        memory_mb=model.memory_usage_mb,
                        loaded=True,
                        load_time=model.load_time,
                        checkpoint_data=model.checkpoint_data,
                        validation_passed=model.validation_passed,
                        access_count=1,
                        last_access=time.time(),
                        # step_interface.py νΈν™ ν•„λ“
                        model_type=kwargs.get('model_type', 'BaseModel'),
                        size_gb=model.memory_usage_mb / 1024 if model.memory_usage_mb > 0 else 0,
                        requires_checkpoint=True,
                        preprocessing_required=kwargs.get('preprocessing_required', []),
                        postprocessing_required=kwargs.get('postprocessing_required', [])
                    )
                    
                    self.model_status[model_name] = RealModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"β… μ‹¤μ  AI λ¨λΈ λ΅λ”© μ„±κ³µ: {model_name} ({step_type.value}, {model.memory_usage_mb:.1f}MB)")
                    
                    # μΊμ‹ ν¬κΈ° κ΄€λ¦¬
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = RealModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"β μ‹¤μ  AI λ¨λΈ λ΅λ”© μ‹¤ν¨ {model_name}: {e}")
            self.model_status[model_name] = RealModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None

    async def load_model_async(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """λΉ„λ™κΈ° λ¨λΈ λ΅λ”© (step_interface.py νΈν™)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"β λΉ„λ™κΈ° λ¨λΈ λ΅λ”© μ‹¤ν¨ {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """step_interface.py AI_MODELS_ROOT κΈ°λ° λ¨λΈ κ²½λ΅ μ°ΎκΈ°"""
        try:
            # μ§μ ‘ κ²½λ΅ μ§€μ •
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # available_modelsμ—μ„ μ°ΎκΈ°
            if model_name in self._available_models_cache:
                model_info = self._available_models_cache[model_name]
                path = Path(model_info.get('path', ''))
                if path.exists():
                    return str(path)
            
            # step_interface.py λ§¤ν•‘μ—μ„ μ°ΎκΈ°
            step_name = kwargs.get('step_name')
            if step_name and step_name in self.step_interface_mappings:
                mapping = self.step_interface_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        # λ¨λΈλ… λ§¤μΉ­ ν™•μΈ
                        if model_name in local_path or local_path.stem == model_name:
                            return str(full_path)
            
            # λ¨λ“  Step λ§¤ν•‘μ—μ„ μ°ΎκΈ° (step_interface.py GitHubStepMapping μ „μ²΄ κ²€μƒ‰)
            for step_name, mapping in self.step_interface_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        if model_name in local_path or local_path.stem == model_name or model_name in mapping.get('ai_models', []):
                            return str(full_path)
            
            # ν™•μ¥μ ν¨ν„΄μΌλ΅ κ²€μƒ‰ (step_interface.py κµ¬μ΅° κΈ°λ°)
            possible_patterns = [
                f"**/{model_name}",
                f"**/{model_name}.*",
                f"**/*{model_name}*",
                f"**/step_*/{model_name}.*"
            ]
            
            for pattern in possible_patterns:
                for found_path in self.model_cache_dir.glob(pattern):
                    if found_path.is_file():
                        return str(found_path)
            
            return None
            
        except Exception as e:
            self.logger.error(f"β λ¨λΈ κ²½λ΅ μ°ΎκΈ° μ‹¤ν¨ {model_name}: {e}")
            return None
    
    def _manage_cache(self):
        """μ‹¤μ  AI λ¨λΈ μΊμ‹ κ΄€λ¦¬ (step_interface.py νΈν™)"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # μ°μ„ μμ„μ™€ λ§μ§€λ§‰ μ ‘κ·Ό μ‹κ°„ κΈ°λ° μ •λ ¬
            models_by_priority = sorted(
                self.model_info.items(),
                key=lambda x: (x[1].priority.value, x[1].last_access)
            )
            
            models_to_remove = models_by_priority[:len(self.loaded_models) - self.max_cached_models]
            
            for model_name, _ in models_to_remove:
                # Primary λ¨λΈμ€ λ³΄νΈ (step_interface.py GitHubStepMapping κΈ°λ°)
                if any(mapping.get('primary_model') == model_name for mapping in self.step_interface_mappings.values()):
                    continue
                
                self.unload_model(model_name)
                
        except Exception as e:
            self.logger.error(f"β μΊμ‹ κ΄€λ¦¬ μ‹¤ν¨: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """μ‹¤μ  AI λ¨λΈ μ–Έλ΅λ“ (step_interface.py νΈν™)"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # λ©”λ¨λ¦¬ ν†µκ³„ μ—…λ°μ΄νΈ
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = RealModelStatus.NOT_LOADED
                    
                    self.logger.info(f"β… μ‹¤μ  AI λ¨λΈ μ–Έλ΅λ“ μ™„λ£: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"β μ‹¤μ  AI λ¨λΈ μ–Έλ΅λ“ μ‹¤ν¨ {model_name}: {e}")
            return False
    
    # ==============================================
    # π”¥ step_interface.py μ™„μ „ νΈν™ μΈν„°νμ΄μ¤ μ§€μ›
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
        """step_interface.py νΈν™ Step μΈν„°νμ΄μ¤ μƒμ„±"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            # Step νƒ€μ… κ²°μ • (step_interface.py GitHubStepType κΈ°λ°)
            step_type = None
            if step_name in self.step_interface_mappings:
                step_type = self.step_interface_mappings[step_name].get('step_type')
            
            if not step_type:
                # μ΄λ¦„μΌλ΅ μ¶”λ΅  (step_interface.py νΈν™)
                step_type_map = {
                    'HumanParsingStep': RealStepModelType.HUMAN_PARSING,
                    'PoseEstimationStep': RealStepModelType.POSE_ESTIMATION,
                    'ClothSegmentationStep': RealStepModelType.CLOTH_SEGMENTATION,
                    'GeometricMatchingStep': RealStepModelType.GEOMETRIC_MATCHING,
                    'ClothWarpingStep': RealStepModelType.CLOTH_WARPING,
                    'VirtualFittingStep': RealStepModelType.VIRTUAL_FITTING,
                    'PostProcessingStep': RealStepModelType.POST_PROCESSING,
                    'QualityAssessmentStep': RealStepModelType.QUALITY_ASSESSMENT
                }
                step_type = step_type_map.get(step_name, RealStepModelType.HUMAN_PARSING)
            
            interface = RealStepModelInterface(self, step_name, step_type)
            
            # step_interface.py DetailedDataSpec κΈ°λ° μ”κµ¬μ‚¬ν•­ λ“±λ΅
            if step_requirements:
                interface.register_requirements(step_requirements)
            elif step_name in self.step_interface_mappings:
                # κΈ°λ³Έ λ§¤ν•‘μ—μ„ μ”κµ¬μ‚¬ν•­ μƒμ„± (step_interface.py νΈν™)
                mapping = self.step_interface_mappings[step_name]
                default_requirements = {
                    'step_id': mapping.get('step_id', 0),
                    'required_models': mapping.get('ai_models', []),
                    'primary_model': mapping.get('primary_model'),
                    'model_configs': {},
                    'batch_size': 1,
                    'precision': 'fp16' if self.device == 'mps' else 'fp32'
                }
                interface.register_requirements(default_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"β… step_interface.py νΈν™ Step μΈν„°νμ΄μ¤ μƒμ„±: {step_name} ({step_type.value})")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"β Step μΈν„°νμ΄μ¤ μƒμ„± μ‹¤ν¨ {step_name}: {e}")
            return RealStepModelInterface(self, step_name, RealStepModelType.HUMAN_PARSING)
    
    def create_step_model_interface(self, step_name: str) -> RealStepModelInterface:
        """Step λ¨λΈ μΈν„°νμ΄μ¤ μƒμ„± (step_interface.py νΈν™ λ³„μΉ­)"""
        return self.create_step_interface(step_name)
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """step_interface.py DetailedDataSpec κΈ°λ° Step μ”κµ¬μ‚¬ν•­ λ“±λ΅"""
        try:
            step_type = requirements.get('step_type')
            if isinstance(step_type, str):
                step_type = RealStepModelType(step_type)
            elif not step_type:
                if step_name in self.step_interface_mappings:
                    step_type = self.step_interface_mappings[step_name].get('step_type')
                else:
                    step_type = RealStepModelType.HUMAN_PARSING
            
            self.step_requirements[step_name] = RealStepModelRequirement(
                step_name=step_name,
                step_id=requirements.get('step_id', self._get_step_id(step_name)),
                step_type=step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.logger.info(f"β… step_interface.py νΈν™ Step μ”κµ¬μ‚¬ν•­ λ“±λ΅: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"β Step μ”κµ¬μ‚¬ν•­ λ“±λ΅ μ‹¤ν¨ {step_name}: {e}")
            return False
    
    def _get_step_id(self, step_name: str) -> int:
        """Step μ΄λ¦„μΌλ΅ ID λ°ν™ (step_interface.py νΈν™)"""
        step_id_map = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return step_id_map.get(step_name, 0)
    
    # ==============================================
    # π”¥ step_interface.py BaseStepMixin μ™„μ „ νΈν™μ„± λ©”μ„λ“λ“¤
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """μ΄κΈ°ν™” μƒνƒ ν™•μΈ (step_interface.py νΈν™)"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """μ΄κΈ°ν™” (step_interface.py νΈν™)"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("β… step_interface.py νΈν™ ModelLoader μ΄κΈ°ν™” μ™„λ£")
            return True
            
        except Exception as e:
            self.logger.error(f"β ModelLoader μ΄κΈ°ν™” μ‹¤ν¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """λΉ„λ™κΈ° μ΄κΈ°ν™” (step_interface.py νΈν™)"""
        return self.initialize(**kwargs)
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅ - step_interface.py BaseStepMixin νΈν™"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                # Step νƒ€μ… μ¶”λ΅ 
                step_type = kwargs.get('step_type')
                if isinstance(step_type, str):
                    step_type = RealStepModelType(step_type)
                elif not step_type:
                    step_type = self._infer_step_type(model_name, kwargs.get('model_path', ''))
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'step_type': step_type.value if step_type else 'unknown',
                    'required': kwargs.get('required', True),
                    'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                    'device': kwargs.get('device', self.device),
                    'preprocessing_params': kwargs.get('preprocessing_params', {}),
                    **kwargs
                }
                
                self.logger.info(f"β… step_interface.py νΈν™ λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"β λ¨λΈ μ”κµ¬μ‚¬ν•­ λ“±λ΅ μ‹¤ν¨: {e}")
            return False
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """μ‹¤μ  AI λ¨λΈ νΈν™μ„± κ²€μ¦ (step_interface.py νΈν™)"""
        try:
            # λ¨λΈ μ •λ³΄ ν™•μΈ
            if model_name not in self.model_info and model_name not in self._available_models_cache:
                return False
            
            # Step μ”κµ¬μ‚¬ν•­ ν™•μΈ
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # step_interface.py λ§¤ν•‘ ν™•μΈ
            if step_name in self.step_interface_mappings:
                mapping = self.step_interface_mappings[step_name]
                if model_name in mapping.get('ai_models', []):
                    return True
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path or Path(local_path).name == model_name:
                        return True
            
            return True  # κΈ°λ³Έμ μΌλ΅ νΈν™ κ°€λ¥μΌλ΅ μ²λ¦¬
            
        except Exception as e:
            self.logger.error(f"β λ¨λΈ νΈν™μ„± κ²€μ¦ μ‹¤ν¨: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """λ¨λΈ μ΅΄μ¬ μ—¬λ¶€ ν™•μΈ (step_interface.py νΈν™)"""
        return (model_name in self.loaded_models or 
                model_name in self._available_models_cache or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """λ¨λΈ λ΅λ”© μƒνƒ ν™•μΈ (step_interface.py νΈν™)"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """μ‚¬μ© κ°€λ¥ν• μ‹¤μ  AI λ¨λΈ λ©λ΅ (step_interface.py μ™„μ „ νΈν™)"""
        try:
            models = []
            
            # available_modelsμ—μ„ λ©λ΅ κ°€μ Έμ¤κΈ°
            for model_name, model_info in self._available_models_cache.items():
                # ν•„ν„°λ§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # λ΅λ”© μƒνƒ μ¶”κ°€ (step_interface.py νΈν™)
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                # step_interface.py νΈν™ ν•„λ“ μ¶”κ°€
                model_info_copy.update({
                    "real_ai_model": True,
                    "checkpoint_loaded": is_loaded and self.loaded_models.get(model_name, {}).get('checkpoint_data') is not None if is_loaded else False,
                    "step_loadable": True,
                    "device_compatible": True,
                    "requires_checkpoint": True
                })
                
                models.append(model_info_copy)
            
            # step_interface.py λ§¤ν•‘μ—μ„ μ¶”κ°€
            for step_name, mapping in self.step_interface_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                step_type = mapping.get('step_type', RealStepModelType.HUMAN_PARSING)
                for model_name in mapping.get('ai_models', []):
                    if model_name not in [m['name'] for m in models]:
                        # step_interface.py νΈν™ λ¨λΈ μ •λ³΄
                        models.append({
                            'name': model_name,
                            'path': f"ai_models/step_{mapping.get('step_id', 0):02d}_{step_name.lower()}/{model_name}",
                            'type': self._infer_model_type(model_name),
                            'step_type': step_type.value,
                            'loaded': model_name in self.loaded_models,
                            'step_class': step_name,
                            'step_id': mapping.get('step_id', 0),
                            'size_mb': 0.0,  # μ‹¤μ  νμΌ ν¬κΈ°λ” λ΅λ”© μ‹ κ³„μ‚°
                            'priority': self._infer_model_priority(model_name),
                            'is_primary': model_name == mapping.get('primary_model'),
                            'real_ai_model': True,
                            'device_compatible': True,
                            'requires_checkpoint': True,
                            'step_loadable': True
                        })
            
            return models
            
        except Exception as e:
            self.logger.error(f"β μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ μ΅°ν μ‹¤ν¨: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """μ‹¤μ  AI λ¨λΈ μ •λ³΄ μ΅°ν (step_interface.py μ™„μ „ νΈν™)"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'step_type': info.step_type.value,
                    'priority': info.priority.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'inference_count': info.inference_count,
                    'avg_inference_time': info.avg_inference_time,
                    'validation_passed': info.validation_passed,
                    'has_checkpoint_data': info.checkpoint_data is not None,
                    'error': info.error,
                    
                    # step_interface.py νΈν™ ν•„λ“
                    'model_type': info.model_type,
                    'size_gb': info.size_gb,
                    'requires_checkpoint': info.requires_checkpoint,
                    'preprocessing_required': info.preprocessing_required,
                    'postprocessing_required': info.postprocessing_required,
                    'real_ai_model': True,
                    'device_compatible': True,
                    'step_loadable': True
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"β λ¨λΈ μ •λ³΄ μ΅°ν μ‹¤ν¨: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """μ‹¤μ  AI λ¨λΈ μ„±λ¥ λ©”νΈλ¦­ μ΅°ν (step_interface.py νΈν™)"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "mps_available": MPS_AVAILABLE,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self._available_models_cache),
            "step_interfaces_count": len(self.step_interfaces),
            "avg_inference_time": self.performance_metrics['total_inference_time'] / max(1, self.performance_metrics['inference_count']),
            "memory_efficiency": self.performance_metrics['total_memory_mb'] / max(1, len(self.loaded_models)),
            
            # step_interface.py νΈν™ ν•„λ“
            "step_interface_v5_2_compatible": True,
            "github_step_mapping_loaded": len(self.step_interface_mappings) > 0,
            "real_ai_models_only": True,
            "mock_removed": True,
            "checkpoint_loading_optimized": True
        }
    
    def cleanup(self):
        """λ¦¬μ†μ¤ μ •λ¦¬ (step_interface.py νΈν™)"""
        try:
            self.logger.info("π§Ή step_interface.py νΈν™ ModelLoader λ¦¬μ†μ¤ μ •λ¦¬ μ¤‘...")
            
            # λ¨λ“  μ‹¤μ  AI λ¨λΈ μ–Έλ΅λ“
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # μΊμ‹ μ •λ¦¬
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            self.step_requirements.clear()
            
            # μ¤λ λ“ν’€ μΆ…λ£
            self._executor.shutdown(wait=True)
            
            # λ©”λ¨λ¦¬ μ •λ¦¬
            gc.collect()
            
            # MPS λ©”λ¨λ¦¬ μ •λ¦¬
            if MPS_AVAILABLE and TORCH_AVAILABLE:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("β… step_interface.py νΈν™ ModelLoader λ¦¬μ†μ¤ μ •λ¦¬ μ™„λ£")
            
        except Exception as e:
            self.logger.error(f"β λ¦¬μ†μ¤ μ •λ¦¬ μ‹¤ν¨: {e}")

# ==============================================
# π”¥ 6. μ „μ—­ μΈμ¤ν„΄μ¤ λ° νΈν™μ„± ν•¨μλ“¤ (step_interface.py μ™„μ „ νΈν™)
# ==============================================

# μ „μ—­ μΈμ¤ν„΄μ¤
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """μ „μ—­ ModelLoader μΈμ¤ν„΄μ¤ λ°ν™ (step_interface.py νΈν™)"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            try:
                # μ„¤μ • μ μ©
                loader_config = config or {}
                
                _global_model_loader = ModelLoader(
                    device=loader_config.get('device', 'auto'),
                    max_cached_models=loader_config.get('max_cached_models', 10),
                    enable_optimization=loader_config.get('enable_optimization', True),
                    **loader_config
                )
                
                logger.info("β… μ „μ—­ step_interface.py νΈν™ ModelLoader v5.1 μƒμ„± μ„±κ³µ")
                
            except Exception as e:
                logger.error(f"β μ „μ—­ ModelLoader μƒμ„± μ‹¤ν¨: {e}")
                # κΈ°λ³Έ μ„¤μ •μΌλ΅ ν΄λ°±
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """μ „μ—­ ModelLoader μ΄κΈ°ν™” (step_interface.py νΈν™)"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"β μ „μ—­ ModelLoader μ΄κΈ°ν™” μ‹¤ν¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """μ „μ—­ ModelLoader λΉ„λ™κΈ° μ΄κΈ°ν™” (step_interface.py νΈν™)"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("β… μ „μ—­ ModelLoader λΉ„λ™κΈ° μ΄κΈ°ν™” μ™„λ£")
        else:
            logger.warning("β οΈ μ „μ—­ ModelLoader μ΄κΈ°ν™” μΌλ¶€ μ‹¤ν¨")
            
        return loader
        
    except Exception as e:
        logger.error(f"β μ „μ—­ ModelLoader λΉ„λ™κΈ° μ΄κΈ°ν™” μ‹¤ν¨: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
    """Step μΈν„°νμ΄μ¤ μƒμ„± (step_interface.py νΈν™)"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"β Step μΈν„°νμ΄μ¤ μƒμ„± μ‹¤ν¨ {step_name}: {e}")
        step_type = RealStepModelType.HUMAN_PARSING
        return RealStepModelInterface(get_global_model_loader(), step_name, step_type)

def get_model(model_name: str) -> Optional[RealAIModel]:
    """μ „μ—­ λ¨λΈ κ°€μ Έμ¤κΈ° (step_interface.py νΈν™)"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[RealAIModel]:
    """μ „μ—­ λΉ„λ™κΈ° λ¨λΈ κ°€μ Έμ¤κΈ° (step_interface.py νΈν™)"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> RealStepModelInterface:
    """Step λ¨λΈ μΈν„°νμ΄μ¤ μƒμ„± (step_interface.py νΈν™)"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# step_interface.py νΈν™μ„ μ„ν• λ³„μΉ­
BaseModel = RealAIModel
StepModelInterface = RealStepModelInterface

# ==============================================
# π”¥ 7. Export λ° μ΄κΈ°ν™”
# ==============================================

__all__ = [
    # ν•µμ‹¬ ν΄λμ¤λ“¤ (step_interface.py μ™„μ „ νΈν™)
    'ModelLoader',
    'RealStepModelInterface',
    'EnhancedStepModelInterface',  # νΈν™μ„± λ³„μΉ­
    'StepModelInterface',  # νΈν™μ„± λ³„μΉ­
    'RealAIModel',
    'BaseModel',  # νΈν™μ„± λ³„μΉ­
    
    # step_interface.py μ™„μ „ νΈν™ λ°μ΄ν„° κµ¬μ΅°λ“¤
    'RealStepModelType',
    'RealModelStatus',
    'RealModelPriority',
    'RealStepModelInfo',
    'RealStepModelRequirement',
    
    # μ „μ—­ ν•¨μλ“¤ (step_interface.py μ™„μ „ νΈν™)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # μƒμλ“¤
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'TORCH_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'MPS_AVAILABLE',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ==============================================
# π”¥ 8. λ¨λ“ μ΄κΈ°ν™” λ° μ™„λ£ λ©”μ‹μ§€
# ==============================================

logger.info("=" * 80)
logger.info("π€ μ™„μ „ κ°μ„ λ ModelLoader v5.1 - step_interface.py v5.2 μ™„μ „ νΈν™")
logger.info("=" * 80)
logger.info("β… step_interface.py RealStepModelInterface μ”κµ¬μ‚¬ν•­ 100% λ°μ")
logger.info("β… GitHubStepMapping μ‹¤μ  AI λ¨λΈ κ²½λ΅ μ™„μ „ λ§¤ν•‘")
logger.info("β… 229GB AI λ¨λΈ νμΌλ“¤ μ •ν™•ν• λ΅λ”© μ§€μ›")
logger.info("β… RealAIModel ν΄λμ¤λ΅ μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ™„μ „ κ°μ„ ")
logger.info("β… Stepλ³„ νΉν™” λ΅λ” μ§€μ› (Human Parsing, Pose, Segmentation λ“±)")
logger.info("β… BaseStepMixin v19.2 μ™„λ²½ νΈν™")
logger.info("β… StepFactory μμ΅΄μ„± μ£Όμ… μ™„λ²½ μ§€μ›")
logger.info("β… Mock μ™„μ „ μ κ±° - μ‹¤μ  μ²΄ν¬ν¬μΈνΈλ§ μ‚¬μ©")
logger.info("β… PyTorch weights_only λ¬Έμ  μ™„μ „ ν•΄κ²°")
logger.info("β… Auto Detector μ™„μ „ μ—°λ™")
logger.info("β… M3 Max 128GB λ©”λ¨λ¦¬ μµμ ν™”")
logger.info("β… λ¨λ“  κΈ°λ¥ μ™„μ „ μ‘λ™")

logger.info(f"π”§ μ‹μ¤ν… μ •λ³΄:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
logger.info(f"   PyTorch: {TORCH_AVAILABLE}, NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info(f"   conda ν™κ²½: {CONDA_ENV}")

logger.info("π― μ§€μ› μ‹¤μ  AI Step νƒ€μ… (step_interface.py μ™„μ „ νΈν™):")
for step_type in RealStepModelType:
    logger.info(f"   - {step_type.value}: νΉν™” λ΅λ” μ§€μ›")

logger.info("π”¥ ν•µμ‹¬ κ°μ„ μ‚¬ν•­:")
logger.info("   β€Ά RealAIModel: Stepλ³„ νΉν™” μ²΄ν¬ν¬μΈνΈ λ΅λ”©")
logger.info("   β€Ά RealStepModelInterface: step_interface.py μ™„μ „ νΈν™")
logger.info("   β€Ά μ‹¤μ  AI Step λ§¤ν•‘: step_interface.py GitHubStepMapping κΈ°λ°")
logger.info("   β€Ά μ°μ„ μμ„ κΈ°λ° λ¨λΈ μΊμ‹±: Primary/Secondary/Fallback")
logger.info("   β€Ά Graphonomy 1.2GB λ¨λΈ μ΄μ•μ „ λ΅λ”©")
logger.info("   β€Ά RealVisXL 6.46GB Safetensors μ™„λ²½ μ§€μ›")
logger.info("   β€Ά Diffusion 4.8GB λ¨λΈ μ™„λ²½ μ§€μ›")
logger.info("   β€Ά U2Net 176GB λ¨λΈ μ™„λ²½ μ§€μ›")
logger.info("   β€Ά Real-ESRGAN 64GB λ¨λΈ μ™„λ²½ μ§€μ›")
logger.info("   β€Ά Auto Detector μ™„μ „ μ—°λ™")

logger.info("π€ μ‹¤μ  AI Step μ§€μ› νλ¦„ (step_interface.py μ™„μ „ νΈν™):")
logger.info("   StepFactory (v11.0)")
logger.info("     β†“ (Step μΈμ¤ν„΄μ¤ μƒμ„± + μμ΅΄μ„± μ£Όμ…)")
logger.info("   BaseStepMixin (v19.2)")
logger.info("     β†“ (λ‚΄μ¥ GitHubDependencyManager μ‚¬μ©)")
logger.info("   step_interface.py (v5.2)")
logger.info("     β†“ (RealStepModelInterface μ κ³µ)")
logger.info("   ModelLoader (v5.1) β† π”¥ μ™„μ „ νΈν™ κ°μ„ !")
logger.info("     β†“ (RealAIModelλ΅ μ²΄ν¬ν¬μΈνΈ λ΅λ”©)")
logger.info("   μ‹¤μ  AI λ¨λΈλ“¤ (229GB)")

logger.info("π‰ μ™„μ „ κ°μ„ λ ModelLoader v5.1 μ¤€λΉ„ μ™„λ£!")
logger.info("π‰ step_interface.py v5.2μ™€ μ™„λ²½ν• νΈν™μ„± λ‹¬μ„±!")
logger.info("π‰ μ‹¤μ  AI λ¨λΈ λ΅λ”© μ™„μ „ μ§€μ›!")
logger.info("π‰ Mock μ κ±°, μ‹¤μ  μ²΄ν¬ν¬μΈνΈ λ΅λ”© μµμ ν™” μ™„λ£!")
logger.info("π‰ λ¨λ“  κΈ°λ¥ μ™„μ „ μ‘λ™!")
logger.info("=" * 80)

# μ΄κΈ°ν™” ν…μ¤νΈ
try:
    _test_loader = get_global_model_loader()
    logger.info(f"π‰ step_interface.py v5.2 μ™„μ „ νΈν™ ModelLoader v5.1 μ¤€λΉ„ μ™„λ£!")
    logger.info(f"   λ””λ°”μ΄μ¤: {_test_loader.device}")
    logger.info(f"   λ¨λΈ μΊμ‹: {_test_loader.model_cache_dir}")
    logger.info(f"   step_interface.py λ§¤ν•‘: {len(_test_loader.step_interface_mappings)}κ° Step")
    logger.info(f"   AutoDetector ν†µν•©: {_test_loader._integration_successful}")
    logger.info(f"   μ‚¬μ© κ°€λ¥ν• λ¨λΈ: {len(_test_loader._available_models_cache)}κ°")
    logger.info(f"   μ‹¤μ  AI λ¨λΈ λ΅λ”©: β…")
    logger.info(f"   step_interface.py v5.2 νΈν™: β…")
except Exception as e:
    logger.error(f"β μ΄κΈ°ν™” ν…μ¤νΈ μ‹¤ν¨: {e}")