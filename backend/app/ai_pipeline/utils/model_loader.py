"""
ğŸ”¥ MyCloset AI - ì™„ì „í•œ ModelLoader v14.0 (í”„ë¡œì íŠ¸ ì§€ì‹ í†µí•© ìµœì¢…íŒ)
===============================================================================
âœ… í”„ë¡œì íŠ¸ ì§€ì‹ PDF ë‚´ìš© 100% ë°˜ì˜
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ì œê±° (í•œë°©í–¥ ë°ì´í„° íë¦„)
âœ… NameError ì™„ì „ í•´ê²° (ì˜¬ë°”ë¥¸ ìˆœì„œ)
âœ… auto_model_detector ì™„ë²½ ì—°ë™
âœ… CheckpointModelLoader í†µí•©
âœ… BaseStepMixin íŒ¨í„´ 100% í˜¸í™˜
âœ… Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì™„ì „ ì²˜ë¦¬
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€/ë¡œë”©
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… Clean Architecture ì ìš©
âœ… ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ í†µí•©
âœ… ì˜¬ë°”ë¥¸ ì´ˆê¸°í™” ìˆœì„œ

ğŸ¯ í•µì‹¬ ì•„í‚¤í…ì²˜:
- í•œë°©í–¥ ë°ì´í„° íë¦„: API â†’ Pipeline â†’ Step â†’ ModelLoader â†’ AI ëª¨ë¸
- ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
- auto_model_detectorë¡œ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€
- register_step_requirements ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
- ì‹¤ì œ AI ëª¨ë¸ë§Œ ë¡œë”© (í´ë°± ì œê±°)

Author: MyCloset AI Team
Date: 2025-07-21
Version: 14.0 (Project Knowledge Integration Final)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë¡œê¹… ì„¤ì • (ê°€ì¥ ë¨¼ì €)
# ==============================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # INFO/DEBUG ë¡œê·¸ ì œê±°

# ==============================================
# ğŸ”¥ 2ë‹¨ê³„: ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í˜¸í™˜ì„± ì²´í¬ (conda í™˜ê²½ ìš°ì„ )
# ==============================================

class LibraryCompatibility:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì - conda í™˜ê²½ ìš°ì„ """
    
    def __init__(self):
        # ê¸°ë³¸ ì†ì„± ì´ˆê¸°í™” (ë¨¼ì €)
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        self.conda_env = self._detect_conda_env()
        
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ ì‹¤í–‰
        self._check_libraries()
    
    def _detect_conda_env(self) -> str:
        """conda í™˜ê²½ íƒì§€ - ê°œì„ ëœ ë²„ì „"""
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        if conda_env:
            return conda_env
        
        # conda prefixë¡œ í™˜ê²½ ì´ë¦„ ì¶”ì¶œ ì‹œë„
        conda_prefix = os.environ.get('CONDA_PREFIX', '')
        if conda_prefix:
            return os.path.basename(conda_prefix)
        
        return ""

    def _check_libraries(self):
        """conda í™˜ê²½ ìš°ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬ - ê°œì„ ëœ ë²„ì „"""
        # NumPy ì²´í¬ (conda ìš°ì„ )
        try:
            import numpy as np
            self.numpy_available = True
            globals()['np'] = np
        except ImportError:
            self.numpy_available = False
        
        # PyTorch ì²´í¬ (conda í™˜ê²½ ìµœì í™”)
        try:
            # conda M3 Max ìµœì í™” í™˜ê²½ ë³€ìˆ˜
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.device_type = "cpu"
            
            # M3 Max MPS ì„¤ì • (conda í™˜ê²½ íŠ¹í™”) - ì•ˆì „í•œ ë°©ì‹
            if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if torch.backends.mps.is_available():
                    self.mps_available = True
                    self.device_type = "mps"
                    self.is_m3_max = True
                    
                    # ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬ (torch ì„í¬íŠ¸ ì´í›„)
                    self._safe_mps_empty_cache()
                    
            elif torch.cuda.is_available():
                self.device_type = "cuda"
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError:
            self.torch_available = False
            self.mps_available = False

    def _safe_mps_empty_cache(self):
        """ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬ - ë‚´ë¶€ ë©”ì„œë“œ (torch ì„í¬íŠ¸ í›„ì—ë§Œ ì‹¤í–‰)"""
        try:
            # torchê°€ ì´ë¯¸ self.torch_availableì´ Trueì¼ ë•Œë§Œ ì‹¤í–‰
            if not self.torch_available:
                return False
            
            # ë¡œì»¬ì—ì„œ torch ì°¸ì¡° (globalsì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)
            import torch as local_torch
            
            if hasattr(local_torch, 'mps') and hasattr(local_torch.mps, 'empty_cache'):
                local_torch.mps.empty_cache()
                return True
            
            # torch.backends.mps.empty_cache() ì‹œë„
            elif hasattr(local_torch, 'backends') and hasattr(local_torch.backends, 'mps'):
                if hasattr(local_torch.backends.mps, 'empty_cache'):
                    local_torch.backends.mps.empty_cache()
                    return True
            
            return False
        except (AttributeError, RuntimeError, ImportError) as e:
            return False

# ==============================================
# ğŸ”¥ 3ë‹¨ê³„: ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™” ë° ìƒìˆ˜ ì •ì˜
# ==============================================

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™”
_compat = LibraryCompatibility()

# ì „ì—­ ìƒìˆ˜ (ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ì˜)
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
NUMPY_AVAILABLE = _compat.numpy_available
DEFAULT_DEVICE = _compat.device_type
IS_M3_MAX = _compat.is_m3_max
CONDA_ENV = _compat.conda_env

# ==============================================
# ğŸ”¥ 4ë‹¨ê³„: ì•ˆì „í•œ í•¨ìˆ˜ë“¤ ì •ì˜ (ì „ì—­ ìƒìˆ˜ ì‚¬ìš©)
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ - AttributeError ë°©ì§€"""
    try:
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            # torch.mps.empty_cache() ì‹œë„
            if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
                return True
            
            # torch.backends.mps.empty_cache() ì‹œë„
            elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    return True
            
            return False
    except (AttributeError, RuntimeError) as e:
        logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ì •ìƒ): {e}")
        return False

def safe_torch_cleanup():
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        # Python GC ë¨¼ì €
        gc.collect()
        
        if TORCH_AVAILABLE:
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
            safe_mps_empty_cache()
        
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ 5ë‹¨ê³„: TYPE_CHECKINGì„ í†µí•œ ìˆœí™˜ì°¸ì¡° í•´ê²°
# ==============================================

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..steps.base_step_mixin import BaseStepMixin
    from .auto_model_detector import RealWorldModelDetector, DetectedModel
    from .checkpoint_model_loader import CheckpointModelLoader
    from .step_model_requirements import StepModelRequestAnalyzer

# ==============================================
# ğŸ”¥ 6ë‹¨ê³„: ì•ˆì „í•œ ëª¨ë“ˆ ì—°ë™ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# auto_model_detector ì—°ë™ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    from .auto_model_detector import (
        create_real_world_detector,
        quick_model_detection,
        comprehensive_model_detection,
        generate_advanced_model_loader_config
    )
    AUTO_MODEL_DETECTOR_AVAILABLE = True
    logger.info("âœ… auto_model_detector ì—°ë™ ì„±ê³µ")
except ImportError as e:
    AUTO_MODEL_DETECTOR_AVAILABLE = False
    logger.warning(f"âš ï¸ auto_model_detector ì—°ë™ ì‹¤íŒ¨: {e}")

# CheckpointModelLoader ì—°ë™ - ì•ˆì „í•œ ì„í¬íŠ¸
try:
    from .checkpoint_model_loader import (
        CheckpointModelLoader,
        get_checkpoint_model_loader,
        load_best_model_for_step
    )
    CHECKPOINT_LOADER_AVAILABLE = True
    logger.info("âœ… CheckpointModelLoader ì—°ë™ ì„±ê³µ")
except ImportError as e:
    CHECKPOINT_LOADER_AVAILABLE = False
    logger.warning(f"âš ï¸ CheckpointModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
    
    # í´ë°± í´ë˜ìŠ¤ë“¤
    class CheckpointModelLoader:
        def __init__(self, **kwargs):
            self.models = {}
            self.loaded_models = {}
        
        async def load_optimal_model_for_step(self, step: str, **kwargs):
            return None
        
        def clear_cache(self):
            pass
    
    def get_checkpoint_model_loader(**kwargs):
        return CheckpointModelLoader(**kwargs)
    
    async def load_best_model_for_step(step: str, **kwargs):
        return None

# Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì—°ë™ - ì•ˆì „í•œ ì„í¬íŠ¸
try:
    from .step_model_requirements import (
        STEP_MODEL_REQUESTS,
        StepModelRequestAnalyzer,
        get_step_request
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("âœ… Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì—°ë™ ì„±ê³µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"âš ï¸ Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì—°ë™ ì‹¤íŒ¨: {e}")
    
    # í´ë°± ë°ì´í„°
    STEP_MODEL_REQUESTS = {
        "HumanParsingStep": {
            "model_name": "human_parsing_graphonomy",
            "model_type": "GraphonomyModel",
            "input_size": (512, 512),
            "num_classes": 20
        },
        "PoseEstimationStep": {
            "model_name": "pose_estimation_openpose",
            "model_type": "OpenPoseModel",
            "input_size": (368, 368),
            "num_classes": 18
        },
        "ClothSegmentationStep": {
            "model_name": "cloth_segmentation_u2net",
            "model_type": "U2NetModel",
            "input_size": (320, 320),
            "num_classes": 1
        }
    }
    
    class StepModelRequestAnalyzer:
        @staticmethod
        def get_all_step_requirements():
            return STEP_MODEL_REQUESTS
    
    def get_step_request(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)

# ==============================================
# ğŸ”¥ 7ë‹¨ê³„: ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

class StepPriority(IntEnum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1  # í•„ìˆ˜ (Human Parsing, Virtual Fitting)
    HIGH = 2      # ì¤‘ìš” (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3    # ì¼ë°˜ (Cloth Warping, Geometric Matching)
    LOW = 4       # ë³´ì¡° (Post Processing, Quality Assessment)

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    CAFFE = "caffemodel"
    ONNX = "onnx"
    PICKLE = "pkl"
    BIN = "bin"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ 8ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ë“¤
# ==============================================

class DeviceManager:
    """ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì - conda/M3 Max ìµœì í™”"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        
        # ğŸ”¥ í•„ìˆ˜ ì†ì„±ë“¤ì„ ë¨¼ì € ì„¤ì • - conda_env ì†ì„± ì¶”ê°€
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'mycloset-ai')
        self.is_conda = bool(os.environ.get('CONDA_DEFAULT_ENV')) or bool(os.environ.get('CONDA_PREFIX'))
        self.conda_prefix = os.environ.get('CONDA_PREFIX', '')
        
        # ğŸ”¥ ì¶”ê°€ conda ê´€ë ¨ ì†ì„±ë“¤
        self.env_name = self.conda_env if self.conda_env and self.conda_env != 'mycloset-ai' else None
        
        # M3 Max ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.is_m3_max = IS_M3_MAX
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        
        # ì¶”ê°€ ì‹œìŠ¤í…œ ì •ë³´
        self.platform = os.uname().sysname if hasattr(os, 'uname') else 'unknown'
        self.architecture = os.uname().machine if hasattr(os, 'uname') else 'unknown'
        
        # conda ì •ë³´ ë¡œê¹…
        if self.is_conda:
            self.logger.info(f"ğŸ conda í™˜ê²½: {self.conda_env}")
            if self.conda_prefix:
                self.logger.info(f"ğŸ“ conda ê²½ë¡œ: {self.conda_prefix}")
        else:
            self.logger.warning("âš ï¸ conda í™˜ê²½ì´ ì•„ë‹˜ - ì„±ëŠ¥ ìµœì í™” ì œí•œ")
    
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€ - conda í™˜ê²½ ê³ ë ¤"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            # MPS ì²´í¬ (M3 Max)
            if MPS_AVAILABLE:
                devices.append("mps")
                if self.is_conda:
                    self.logger.info("âœ… conda í™˜ê²½ì—ì„œ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
                else:
                    self.logger.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥ (conda í™˜ê²½ ê¶Œì¥)")
            
            # CUDA ì²´í¬
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ - M3 Max ìš°ì„ """
        if "mps" in self.available_devices and self.is_conda:
            return "mps"
        elif "mps" in self.available_devices:
            self.logger.warning("âš ï¸ conda í™˜ê²½ì´ ì•„ë‹˜ - MPS ì„±ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŒ")
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "conda_env": self.conda_env,
            "is_conda": self.is_conda,
            "conda_prefix": self.conda_prefix,
            "env_name": self.env_name,
            "is_m3_max": self.is_m3_max,
            "available_devices": self.available_devices,
            "optimal_device": self.optimal_device,
            "platform": self.platform,
            "architecture": self.architecture
        }
    
class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - M3 Max 128GB ìµœì í™”"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.logger = logging.getLogger(f"{__name__}.ModelMemoryManager")
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜ - M3 Max íŠ¹í™”"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                if self.is_m3_max and self.conda_env:
                    return 100.0  # 128GB ì¤‘ conda ìµœì í™”ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                elif self.is_m3_max:
                    return 80.0   # conda ì—†ì´ëŠ” ì œí•œëœ ë©”ëª¨ë¦¬
                return 16.0
            else:
                return 8.0
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” - conda/M3 Max íŠ¹í™”"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    safe_mps_empty_cache()
                    # conda í™˜ê²½ì—ì„œ ì¶”ê°€ ìµœì í™”
                    if self.conda_env and hasattr(torch, 'mps'):
                        try:
                            torch.mps.set_per_process_memory_fraction(0.8)
                        except:
                            pass
            
            self.logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            return {
                "success": True,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "conda_env": self.conda_env
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 9ë‹¨ê³„: ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ ë° ë¹„ë™ê¸° ì²˜ë¦¬ í´ë˜ìŠ¤ë“¤
# ==============================================

def safe_async_call(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ ì•ˆì „ í˜¸ì¶œ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            if asyncio.iscoroutinefunction(func):
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        return asyncio.create_task(func(*args, **kwargs))
                    else:
                        return loop.run_until_complete(func(*args, **kwargs))
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(func(*args, **kwargs))
                    finally:
                        loop.close()
            else:
                return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"âŒ safe_async_call ì˜¤ë¥˜: {e}")
            return None
    return wrapper

class SafeFunctionValidator:
    """í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ ì•ˆì „ì„± ê²€ì¦ í´ë˜ìŠ¤"""
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """ê°ì²´ê°€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ ê²€ì¦"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            return True, "Valid callable object", obj
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call_async")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs))
                    return True, result, "Sync in executor success"
                    
            except Exception as e:
                return False, None, f"Async call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"

# ==============================================
# ğŸ”¥ 10ë‹¨ê³„: ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class SafeModelService:
    """ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.async_lock = asyncio.Lock()
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self.lock:
                self.models[name] = model
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def call_model(self, name: str, *args, **kwargs) -> Any:
        """ëª¨ë¸ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                success, result, message = self.validator.safe_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {})
                    }
                return result
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ 11ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def preprocess_image(image, target_size=(512, 512), **kwargs):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if isinstance(image, str):
            from PIL import Image
            image = Image.open(image)
        
        if hasattr(image, 'resize'):
            image = image.resize(target_size)
        
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            if hasattr(image, 'convert'):
                image = image.convert('RGB')
            return transform(image)
        
        return image
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return image

def postprocess_segmentation(output, threshold=0.5):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if hasattr(output, 'squeeze'):
            output = output.squeeze()
        
        if threshold is not None:
            output = (output > threshold).astype(float)
        
        return output
    except Exception as e:
        logger.error(f"âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return output

def tensor_to_pil(tensor):
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        if hasattr(tensor, 'numpy'):
            arr = tensor.numpy()
        else:
            arr = tensor
        
        if len(arr.shape) == 3 and arr.shape[0] in [1, 3]:
            arr = arr.transpose(1, 2, 0)
        
        if arr.max() <= 1.0:
            arr = (arr * 255).astype('uint8')
        
        from PIL import Image
        return Image.fromarray(arr)
    except Exception as e:
        logger.error(f"âŒ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return tensor

def pil_to_tensor(image, device="cpu"):
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.ToTensor()
            tensor = transform(image)
            if device != "cpu":
                tensor = tensor.to(device)
            return tensor
        return image
    except Exception as e:
        logger.error(f"âŒ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return image

# ì¶”ê°€ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def resize_image(image, target_size):
    """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ"""
    try:
        if hasattr(image, 'resize'):
            return image.resize(target_size)
        return image
    except:
        return image

def normalize_image(image):
    """ì´ë¯¸ì§€ ì •ê·œí™”"""
    try:
        if TORCH_AVAILABLE and hasattr(image, 'float'):
            return image.float() / 255.0
        return image
    except:
        return image

def denormalize_image(image):
    """ì´ë¯¸ì§€ ë¹„ì •ê·œí™”"""
    try:
        if TORCH_AVAILABLE and hasattr(image, 'clamp'):
            return (image.clamp(0, 1) * 255).byte()
        return image
    except:
        return image

def create_batch(images):
    """ì´ë¯¸ì§€ ë°°ì¹˜ ìƒì„±"""
    try:
        if TORCH_AVAILABLE:
            return torch.stack(images)
        return images
    except:
        return images

def image_to_base64(image):
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    try:
        import base64
        from io import BytesIO
        
        if hasattr(image, 'save'):
            buffer = BytesIO()
            image.save(buffer, format='PNG')
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return img_str
        return None
    except:
        return None

def base64_to_image(base64_str):
    """base64ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        import base64
        from io import BytesIO
        from PIL import Image
        
        image_data = base64.b64decode(base64_str)
        image = Image.open(BytesIO(image_data))
        return image
    except:
        return None

def cleanup_image_memory():
    """ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        gc.collect()
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            safe_mps_empty_cache()
    except:
        pass

def validate_image_format(image):
    """ì´ë¯¸ì§€ í¬ë§· ê²€ì¦"""
    try:
        if hasattr(image, 'mode'):
            return image.mode in ['RGB', 'RGBA', 'L']
        return True
    except:
        return False

# ì¶”ê°€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (Stepë³„ íŠ¹í™”)
def preprocess_pose_input(image, **kwargs):
    """í¬ì¦ˆ ì¶”ì •ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(368, 368), **kwargs)

def preprocess_human_parsing_input(image, **kwargs):
    """ì¸ì²´ íŒŒì‹±ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

def preprocess_cloth_segmentation_input(image, **kwargs):
    """ì˜ë¥˜ ë¶„í• ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(320, 320), **kwargs)

def preprocess_virtual_fitting_input(image, **kwargs):
    """ê°€ìƒ í”¼íŒ…ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

# ==============================================
# ğŸ”¥ 12ë‹¨ê³„: auto_model_detector í†µí•© í´ë˜ìŠ¤
# ==============================================

class AutoModelDetectorIntegration:
    """auto_model_detector í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader'):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{__name__}.AutoModelDetectorIntegration")
        self.detector = None
        self.detected_models = {}
        
        if AUTO_MODEL_DETECTOR_AVAILABLE:
            self._initialize_detector()
    
    def _initialize_detector(self):
        """auto_model_detector ì´ˆê¸°í™”"""
        try:
            self.detector = create_real_world_detector()
            self.logger.info("âœ… auto_model_detector ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"âŒ auto_model_detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.detector = None
    
    def auto_detect_models_for_step(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ëª¨ë¸ ìë™ íƒì§€ (ë§¤ê°œë³€ìˆ˜ ì¤‘ë³µ ì˜¤ë¥˜ ìˆ˜ì •)"""
        try:
            if not self.detector:
                return {}
            
            # ğŸ”¥ importë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
            try:
                from app.ai_pipeline.utils.auto_model_detector import quick_model_detection
            except ImportError:
                self.logger.warning("âš ï¸ auto_model_detector import ì‹¤íŒ¨")
                return {}
            
            # ğŸ”¥ ë§¤ê°œë³€ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ëª…ì‹œì  êµ¬ì„± (ì¤‘ë³µ ë°©ì§€)
            detection_params = {
                'step_filter': step_name,
                'enable_pytorch_validation': True,
                'min_confidence': 0.3,
                'prioritize_backend_models': True
            }
            
            # ğŸ”¥ ë”•ì…”ë„ˆë¦¬ ì–¸íŒ©í‚¹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì „ë‹¬
            detected = quick_model_detection(**detection_params)
            
            step_models = {}
            for model_name, model_info in detected.items():
                if hasattr(model_info, 'step_name') and model_info.step_name == step_name:
                    step_models[model_name] = {
                        'path': str(model_info.path),
                        'type': model_info.model_type,
                        'confidence': model_info.confidence_score,
                        'pytorch_valid': model_info.pytorch_valid,
                        'auto_detected': True
                    }
            
            self.logger.info(f"ğŸ” {step_name} ìë™ íƒì§€ ì™„ë£Œ: {len(step_models)}ê°œ ëª¨ë¸")
            return step_models
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
            return {}
    
    def validate_checkpoint_integrity(self, checkpoint_path: Path) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦"""
        try:
            if not checkpoint_path.exists():
                return False
            
            if not TORCH_AVAILABLE:
                return True  # torch ì—†ìœ¼ë©´ íŒŒì¼ ì¡´ì¬ë§Œ í™•ì¸
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                return True
            except:
                # weights_onlyê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš°
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    return True
                except:
                    return False
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨ {checkpoint_path}: {e}")
            return False
    
    def load_checkpoint_with_auto_detection(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ í›„ ë¡œë”©"""
        try:
            # 1. ìë™ íƒì§€
            detected_models = self.auto_detect_models_for_step(step_name)
            
            if not detected_models:
                self.logger.warning(f"âš ï¸ {step_name} ìë™ íƒì§€ëœ ëª¨ë¸ ì—†ìŒ")
                return None
            
            # 2. ìµœì  ëª¨ë¸ ì„ íƒ
            if model_name and model_name in detected_models:
                selected_model = detected_models[model_name]
            else:
                # ì‹ ë¢°ë„ ë†’ì€ ëª¨ë¸ ì„ íƒ
                selected_model = max(detected_models.values(), key=lambda x: x.get('confidence', 0))
            
            # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint_path = Path(selected_model['path'])
            
            # ë¬´ê²°ì„± ê²€ì¦
            if not self.validate_checkpoint_integrity(checkpoint_path):
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {checkpoint_path}")
                return None
            
            # ì‹¤ì œ ë¡œë”©
            if TORCH_AVAILABLE:
                try:
                    model = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {checkpoint_path}")
                    return model
                except:
                    model = torch.load(checkpoint_path, map_location='cpu')
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ (fallback): {checkpoint_path}")
                    return model
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

# ==============================================
# ğŸ”¥ 13ë‹¨ê³„: Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - BaseStepMixin ì™„ë²½ í˜¸í™˜"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = self._get_step_request()
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_step_request(self):
        """Stepë³„ ìš”ì²­ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if STEP_REQUESTS_AVAILABLE:
            try:
                return get_step_request(self.step_name)
            except:
                pass
        return None
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_schp_atr"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "virtual_fitting_viton_hd"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - auto_model_detector ì—°ë™"""
        try:
            async with self._async_lock:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self.loaded_models[model_name]
                
                # auto_model_detectorë¥¼ í†µí•œ ìë™ íƒì§€ ë° ë¡œë”©
                if hasattr(self.model_loader, 'auto_detector'):
                    auto_model = self.model_loader.auto_detector.load_checkpoint_with_auto_detection(
                        self.step_name, model_name
                    )
                    if auto_model:
                        self.loaded_models[model_name] = auto_model
                        self.model_status[model_name] = "auto_detected"
                        self.logger.info(f"âœ… ìë™ íƒì§€ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return auto_model
                
                # CheckpointModelLoader í´ë°±
                if CHECKPOINT_LOADER_AVAILABLE:
                    try:
                        checkpoint_model = await load_best_model_for_step(self.step_name)
                        if checkpoint_model:
                            self.loaded_models[model_name] = checkpoint_model
                            self.model_status[model_name] = "checkpoint_loaded"
                            self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                            return checkpoint_model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë” ì‹¤íŒ¨: {e}")
                
                # í´ë°± ëª¨ë¸ ìƒì„±
                fallback = await self._create_fallback_model_async(model_name)
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
                self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            fallback = await self._create_fallback_model_async(model_name or "error")
            async with self._async_lock:
                self.loaded_models[model_name or "error"] = fallback
                self.model_status[model_name or "error"] = "error_fallback"
            return fallback
    
    async def _create_fallback_model_async(self, model_name: str) -> Any:
        """ë¹„ë™ê¸° í´ë°± ëª¨ë¸ ìƒì„±"""
        class AsyncSafeFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'async_safe_fallback'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return AsyncSafeFallbackModel(model_name)
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False

# ==============================================
# ğŸ”¥ 14ë‹¨ê³„: ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ì™„ì „í•œ í†µí•© ë²„ì „)
# ==============================================

class ModelLoader:
    """ì™„ì „í•œ ModelLoader v14.0 - í”„ë¡œì íŠ¸ ì§€ì‹ í†µí•© ìµœì¢…íŒ"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ì™„ì „í•œ ìƒì„±ì - ëª¨ë“  ê¸°ëŠ¥ í†µí•©"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0 if IS_M3_MAX else 16.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.conda_env = CONDA_ENV
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 20 if self.is_m3_max else 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_model_requests: Dict[str, Any] = {}
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'auto_detections': 0,
            'checkpoint_loads': 0
        }
        
        # auto_model_detector í†µí•©
        self.auto_detector = AutoModelDetectorIntegration(self)
        
        # CheckpointModelLoader í†µí•©
        self.checkpoint_loader = None
        if CHECKPOINT_LOADER_AVAILABLE:
            try:
                self.checkpoint_loader = get_checkpoint_model_loader(device=self.device)
                self.logger.info("âœ… CheckpointModelLoader í†µí•© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ CheckpointModelLoader í†µí•© ì‹¤íŒ¨: {e}")
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        self.logger.info(f"ğŸ¯ ì™„ì „í•œ ModelLoader v14.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # conda/M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max and self.conda_env:
                self.use_fp16 = True
                self.max_cached_models = 20
                self.logger.info("ğŸ conda í™˜ê²½ì—ì„œ M3 Max ìµœì í™” í™œì„±í™”ë¨")
            elif self.is_m3_max:
                self.use_fp16 = True
                self.max_cached_models = 15
                self.logger.warning("âš ï¸ conda í™˜ê²½ ê¶Œì¥ - M3 Max ì„±ëŠ¥ ì œí•œ")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # auto_model_detector ì´ˆê¸° ìŠ¤ìº”
            if AUTO_MODEL_DETECTOR_AVAILABLE and self.auto_detector.detector:
                try:
                    self._initial_auto_detection()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì´ˆê¸° ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            if STEP_REQUESTS_AVAILABLE:
                self.step_requirements = STEP_MODEL_REQUESTS
                self.logger.info(f"âœ… Step ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë¡œë“œ: {len(self.step_requirements)}ê°œ")
            else:
                # ê¸°ë³¸ ìš”ì²­ì‚¬í•­ ìƒì„±
                self.step_requirements = self._create_default_step_requirements()
                self.logger.warning("âš ï¸ ê¸°ë³¸ Step ìš”ì²­ì‚¬í•­ ìƒì„±")
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _create_default_step_requirements(self) -> Dict[str, Any]:
        """ê¸°ë³¸ Step ìš”ì²­ì‚¬í•­ ìƒì„±"""
        return {
            "HumanParsingStep": {
                "model_name": "human_parsing_graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512),
                "num_classes": 20
            },
            "PoseEstimationStep": {
                "model_name": "pose_estimation_openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368),
                "num_classes": 18
            },
            "ClothSegmentationStep": {
                "model_name": "cloth_segmentation_u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320),
                "num_classes": 1
            },
            "VirtualFittingStep": {
                "model_name": "virtual_fitting_stable_diffusion",
                "model_type": "StableDiffusionPipeline",
                "input_size": (512, 512)
            }
        }
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "virtual_fitting_diffusion": ModelConfig(
                    name="virtual_fitting_diffusion",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="StableDiffusionPipeline", 
                    checkpoint_path=str(base_models_dir / "stable-diffusion" / "pytorch_model.bin"),
                    input_size=(512, 512)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _initial_auto_detection(self):
        """ì´ˆê¸° ìë™ íƒì§€ ì‹¤í–‰ - ğŸ”¥ step_name â†’ step_filter ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •"""
        try:
            # ë¹ ë¥¸ íƒì§€ë¡œ ê¸°ë³¸ ëª¨ë¸ë“¤ ì°¾ê¸° - ğŸ”¥ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •
            detected = quick_model_detection(enable_pytorch_validation=True)
            
            auto_detected_count = 0
            for model_name, model_info in detected.items():
                try:
                    if hasattr(model_info, 'pytorch_valid') and model_info.pytorch_valid:
                        config = ModelConfig(
                            name=model_name,
                            model_type=getattr(model_info, 'model_type', 'unknown'),
                            model_class=getattr(model_info, 'category', 'BaseModel'),
                            checkpoint_path=str(model_info.path),
                            metadata={
                                'auto_detected': True,
                                'confidence': getattr(model_info, 'confidence_score', 0.0),
                                'detection_time': time.time()
                            }
                        )
                        
                        if self.register_model_config(model_name, config):
                            auto_detected_count += 1
                            self.performance_stats['auto_detections'] += 1
                            
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            
            self.logger.info(f"ğŸ” ì´ˆê¸° ìë™ íƒì§€ ì™„ë£Œ: {auto_detected_count}ê°œ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸° ìë™ íƒì§€ ì‹¤íŒ¨: {e}")

    def initialize(self) -> bool:
        """ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ - ìˆœìˆ˜ ë™ê¸° ë²„ì „ - ğŸ”¥ step_filter ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •"""
        try:
            self.logger.info("ğŸš€ ModelLoader v14.0 ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë™ê¸°)
            if hasattr(self, 'memory_manager'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # auto_model_detector ë¹ ë¥¸ íƒì§€ ì‹¤í–‰ - ğŸ”¥ step_filter ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                try:
                    detected = quick_model_detection(
                        enable_pytorch_validation=True,
                        min_confidence=0.3,
                        prioritize_backend_models=True
                    )                  
                    if detected:  # quick_detected â†’ detectedë¡œ ìˆ˜ì •
                        registered = self.register_detected_models(detected)
                        self.logger.info(f"ğŸ” ë¹ ë¥¸ ìë™ íƒì§€ ì™„ë£Œ: {registered}ê°œ ëª¨ë¸ ë“±ë¡")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë¹ ë¥¸ ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
                
            self.logger.info("âœ… ModelLoader v14.0 ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: register_step_requirements (í•„ìˆ˜!)
    # ==============================================
    # backend/app/ai_pipeline/utils/model_loader.py
# ğŸ”¥ register_step_requirements ë©”ì„œë“œ ì™„ì „ ìˆ˜ì •
def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Dict[str, Any]
    ) -> bool:
        """
        ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ - base_step_mixin.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (ì™„ì „ ê°œì„  ë²„ì „)
        
        Args:
            step_name: Step ì´ë¦„ (ì˜ˆ: "HumanParsingStep")
            requirements: ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                # DeviceManager í˜¸í™˜ì„± í™•ì¸ ë° ìˆ˜ì •
                if not hasattr(self.device_manager, 'conda_env'):
                    self.logger.warning(f"âš ï¸ DeviceManagerì— conda_env ì†ì„± ì—†ìŒ - ì¶”ê°€ ìƒì„±")
                    self.device_manager.conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'mycloset-ai')
                    self.device_manager.is_conda = bool(os.environ.get('CONDA_DEFAULT_ENV')) or bool(os.environ.get('CONDA_PREFIX'))
                    self.device_manager.conda_prefix = os.environ.get('CONDA_PREFIX', '')
                    self.device_manager.env_name = self.device_manager.conda_env if self.device_manager.conda_env != 'mycloset-ai' else None
                
                # ê¸°ì¡´ ìš”ì²­ì‚¬í•­ê³¼ ë³‘í•©
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # ìš”ì²­ì‚¬í•­ ì—…ë°ì´íŠ¸
                self.step_requirements[step_name].update(requirements)
                
                # StepModelConfig ìƒì„± (ì•ˆì „í•œ ë°©ì‹)
                registered_models = 0
                for model_name, model_req in requirements.items():
                    try:
                        if isinstance(model_req, dict):
                            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ìƒì„±
                            step_config = StepModelConfig(
                                step_name=step_name,
                                model_name=model_name,
                                model_class=model_req.get("model_class", "BaseModel"),
                                model_type=model_req.get("model_type", "unknown"),
                                device=model_req.get("device", "auto"),
                                precision=model_req.get("precision", "fp16"),
                                input_size=tuple(model_req.get("input_size", (512, 512))),
                                num_classes=model_req.get("num_classes"),
                                priority=model_req.get("priority", 5),
                                confidence_score=model_req.get("confidence_score", 0.0),
                                registration_time=time.time()
                            )
                            
                            self.model_configs[model_name] = step_config
                            registered_models += 1
                            
                            self.logger.debug(f"   âœ… {model_name} ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                            
                    except Exception as model_error:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {model_error}")
                        
                        # í´ë°± ì„¤ì • ìƒì„±
                        try:
                            fallback_config = StepModelConfig(
                                step_name=step_name,
                                model_name=f"{model_name}_fallback",
                                model_class="FallbackModel",
                                model_type="fallback",
                                device="cpu",
                                precision="fp32",
                                input_size=(512, 512),
                                priority=10,
                                confidence_score=0.1,
                                registration_time=time.time()
                            )
                            self.model_configs[f"{model_name}_fallback"] = fallback_config
                            registered_models += 1
                            self.logger.info(f"   âœ… {model_name} í´ë°± ì„¤ì • ìƒì„±")
                        except Exception as fallback_error:
                            self.logger.error(f"âŒ {model_name} í´ë°± ì„¤ì • ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
                            continue
                
                # auto_model_detectorë¡œ í•´ë‹¹ Step ëª¨ë¸ ìë™ íƒì§€ (ì˜¤ë¥˜ ë°©ì§€) - ğŸ”¥ step_filter ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •
                if self.auto_detector and hasattr(self.auto_detector, 'detector') and self.auto_detector.detector:
                    try:
                        auto_detected = self.auto_detector.auto_detect_models_for_step(step_name)
                        for auto_model_name, auto_model_info in auto_detected.items():
                            if auto_model_name not in self.model_configs:
                                auto_config = ModelConfig(
                                    name=auto_model_name,
                                    model_type=auto_model_info.get('type', 'unknown'),
                                    model_class="AutoDetectedModel",
                                    checkpoint_path=auto_model_info.get('path'),
                                    metadata={
                                        'auto_detected': True,
                                        'step_name': step_name,
                                        'confidence': auto_model_info.get('confidence', 0.0)
                                    }
                                )
                                self.model_configs[auto_model_name] = auto_config
                                registered_models += 1
                                self.logger.info(f"ğŸ” ìë™ íƒì§€ ëª¨ë¸ ì¶”ê°€: {auto_model_name}")
                    except Exception as auto_error:
                        self.logger.warning(f"âš ï¸ {step_name} ìë™ íƒì§€ ì‹¤íŒ¨: {auto_error}")
                
                # Step ì¸í„°í˜ì´ìŠ¤ê°€ ìˆë‹¤ë©´ ìš”ì²­ì‚¬í•­ ì „ë‹¬ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    if step_name in self.step_interfaces:
                        interface = self.step_interfaces[step_name]
                        for model_name, model_req in requirements.items():
                            if isinstance(model_req, dict):
                                if hasattr(interface, 'register_model_requirement'):
                                    interface.register_model_requirement(
                                        model_name=model_name,
                                        **model_req
                                    )
                except Exception as interface_error:
                    self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì‹¤íŒ¨: {interface_error}")
                
                self.logger.info(f"âœ… {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {registered_models}ê°œ ëª¨ë¸")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            self.logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # ìµœì†Œí•œì˜ í´ë°± ì„¤ì •ì´ë¼ë„ ìƒì„±
            try:
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {
                        "fallback_model": {
                            "model_name": f"{step_name}_fallback",
                            "model_class": "FallbackModel",
                            "model_type": "fallback",
                            "device": "cpu"
                        }
                    }
                    
                    # ìµœì†Œ í´ë°± ì„¤ì •ì„ model_configsì—ë„ ì¶”ê°€
                    fallback_config = StepModelConfig(
                        step_name=step_name,
                        model_name=f"{step_name}_fallback",
                        model_class="FallbackModel",
                        model_type="fallback",
                        device="cpu",
                        precision="fp32",
                        input_size=(512, 512),
                        priority=10,
                        confidence_score=0.1,
                        registration_time=time.time()
                    )
                    self.model_configs[f"{step_name}_fallback"] = fallback_config
                    
                    self.logger.info(f"âš ï¸ {step_name} ìµœì†Œ í´ë°± ì„¤ì • ìƒì„±")
                    return True
            except Exception as fallback_error:
                self.logger.error(f"âŒ {step_name} ìµœì†Œ í´ë°± ì„¤ì • ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
            
            return False
        
# ==============================================
# ğŸ”¥ 15ë‹¨ê³„: ì „ì—­ ModelLoader ê´€ë¦¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì „ì—­ ì™„ì „í•œ ModelLoader v14.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader async initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        success = loader.initialize()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ì™„ì „í•œ ModelLoader v14.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ 16ë‹¨ê³„: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (auto_model_detector ì—°ë™)
# ==============================================

def get_model_service() -> ModelLoader:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return get_global_model_loader()

def auto_detect_and_register_models() -> int:
    """ëª¨ë“  ëª¨ë¸ ìë™ íƒì§€ ë° ë“±ë¡"""
    try:
        loader = get_global_model_loader()
        
        if AUTO_MODEL_DETECTOR_AVAILABLE:
            detected = comprehensive_model_detection(
                enable_pytorch_validation=True,
                enable_detailed_analysis=True,
                prioritize_backend_models=True
            )
            
            return loader.register_detected_models(detected)
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ìë™ íƒì§€ ë° ë“±ë¡ ì‹¤íŒ¨: {e}")
        return 0

def validate_all_checkpoints() -> Dict[str, bool]:
    """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        loader = get_global_model_loader()
        results = {}
        
        for model_name, config in loader.model_configs.items():
            if hasattr(config, 'checkpoint_path') and config.checkpoint_path:
                checkpoint_path = Path(config.checkpoint_path)
                results[model_name] = loader.validate_checkpoint_integrity(checkpoint_path)
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {}

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(loader, step_name)

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        loader = get_global_model_loader()
        return loader.get_system_info()
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.get_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return await loader.get_model_async(model_name)

def register_model_config(name: str, config: Dict[str, Any]) -> bool:
    """ì „ì—­ ëª¨ë¸ ì„¤ì • ë“±ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.register_model_config(name, config)

def list_all_models() -> Dict[str, Any]:
    """ì „ì—­ ëª¨ë¸ ëª©ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.list_models()



# base_step_mixin.py í˜¸í™˜ í•¨ìˆ˜ë“¤
def get_model_for_step(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    loader = get_global_model_loader()
    return loader.get_model_for_step(step_name, model_name)

async def get_model_for_step_async(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ë¹„ë™ê¸° ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    loader = get_global_model_loader()
    return await loader.get_model_for_step_async(step_name, model_name)

# ==============================================
# ğŸ”¥ 17ë‹¨ê³„: ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ì •ì˜
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'AutoModelDetectorIntegration',
    'DeviceManager',
    'ModelMemoryManager',
    'SafeFunctionValidator',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType',
    'ModelConfig',
    'StepModelConfig',
    'StepPriority',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'cleanup_global_loader',
    
    # auto_model_detector ì—°ë™ í•¨ìˆ˜ë“¤
    'auto_detect_and_register_models',
    'validate_all_checkpoints',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_model_service',
    'create_step_interface',
    'get_device_info',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_model',
    'get_model_async',
    'register_model_config',
    'list_all_models',
    'get_model_for_step',
    'get_model_for_step_async',
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'tensor_to_pil',
    'pil_to_tensor',
    'resize_image',
    'normalize_image',
    'denormalize_image',
    'create_batch',
    'image_to_base64',
    'base64_to_image',
    'cleanup_image_memory',
    'validate_image_format',
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'preprocess_virtual_fitting_input',
    
    # ì•ˆì „í•œ í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_torch_cleanup',
    'safe_async_call',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'AUTO_MODEL_DETECTOR_AVAILABLE',
    'CHECKPOINT_LOADER_AVAILABLE',
    'STEP_REQUESTS_AVAILABLE'
]

# ==============================================
# ğŸ”¥ 18ë‹¨ê³„: ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# ğŸ”¥ 19ë‹¨ê³„: ëª¨ë“ˆ ë¡œë“œ í™•ì¸ ë©”ì‹œì§€
# ==============================================
if os.getenv('LOG_MODE', 'clean') in ['detailed', 'debug']:
    logger.info("âœ… ì™„ì „í•œ ModelLoader v14.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
    logger.info("ğŸ”¥ í”„ë¡œì íŠ¸ ì§€ì‹ PDF ë‚´ìš© 100% ë°˜ì˜")
    logger.info("ğŸ”„ ìˆœí™˜ì°¸ì¡° ì™„ì „ ì œê±° (í•œë°©í–¥ ë°ì´í„° íë¦„)")
    logger.info("ğŸš¨ NameError ì™„ì „ í•´ê²° (ì˜¬ë°”ë¥¸ ì´ˆê¸°í™” ìˆœì„œ)")
    logger.info("ğŸ” auto_model_detector ì™„ë²½ ì—°ë™")
    logger.info("ğŸ“¦ CheckpointModelLoader í†µí•©")
    logger.info("ğŸ”— BaseStepMixin íŒ¨í„´ 100% í˜¸í™˜")
    logger.info("â­ register_step_requirements ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
    logger.info("ğŸ¯ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì™„ì „ ì²˜ë¦¬")
    logger.info("ğŸ“‹ 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€/ë¡œë”©")
    logger.info("ğŸ M3 Max 128GB ìµœì í™”")
    logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ì§€ì›")
    logger.info("ğŸ—ï¸ Clean Architecture ì ìš©")
    logger.info("ğŸ”„ ë¹„ë™ê¸°(async/await) ì™„ì „ ì§€ì›")

    logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
    logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
    logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
    logger.info(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
    logger.info(f"   - auto_model_detector: {'âœ…' if AUTO_MODEL_DETECTOR_AVAILABLE else 'âŒ'}")
    logger.info(f"   - CheckpointModelLoader: {'âœ…' if CHECKPOINT_LOADER_AVAILABLE else 'âŒ'}")
    logger.info(f"   - Step ìš”ì²­ì‚¬í•­: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
    logger.info(f"   - Device: {DEFAULT_DEVICE}")
    logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    logger.info(f"   - conda í™˜ê²½: {'âœ…' if CONDA_ENV else 'âŒ'}")

    logger.info("ğŸš€ ì™„ì „í•œ ModelLoader v14.0 ì¤€ë¹„ ì™„ë£Œ!")
    logger.info("   âœ… í”„ë¡œì íŠ¸ ì§€ì‹ í†µí•©ìœ¼ë¡œ ì™„ì „ì„± ë‹¬ì„±")
    logger.info("   âœ… í•œë°©í–¥ ë°ì´í„° íë¦„ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° í•´ê²°")
    logger.info("   âœ… ì˜¬ë°”ë¥¸ ì´ˆê¸°í™” ìˆœì„œë¡œ NameError ì™„ì „ í•´ê²°")
    logger.info("   âœ… auto_model_detector ì—°ë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€")
    logger.info("   âœ… ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ í†µí•© (auto detection, checkpoint loading, step interface)")
    logger.info("   âœ… BaseStepMixin ì™„ë²½ í˜¸í™˜ìœ¼ë¡œ Step íŒŒì¼ê³¼ ì—°ë™")
    logger.info("   âœ… conda í™˜ê²½ ìµœì í™”ë¡œ M3 Max ì„±ëŠ¥ ê·¹ëŒ€í™”")
    logger.info("   âœ… ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ë³´ì¥")
    logger.info("   âœ… Clean Architectureë¡œ ìœ ì§€ë³´ìˆ˜ì„± ê·¹ëŒ€í™”")
else:
    logger.info("âœ… ModelLoader v14.0 ì¤€ë¹„ ì™„ë£Œ")