# backend/app/ai_pipeline/utils/model_loader.py
"""
üî• MyCloset AI - Í∞úÏÑ†Îêú ModelLoader v3.1 (Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî)
================================================================================
‚úÖ Ïã§Ï†ú AI Step ÌååÏùºÎì§Í≥ºÏùò Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî
‚úÖ StepFactory ‚Üí BaseStepMixin ‚Üí StepInterface ‚Üí ModelLoader ÌùêÎ¶Ñ ÏôÑÎ≤Ω ÏßÄÏõê
‚úÖ DetailedDataSpec Í∏∞Î∞ò Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Ï†ïÌôï Îß§Ìïë
‚úÖ GitHub ÌîÑÎ°úÏ†ùÌä∏ Step ÌÅ¥ÎûòÏä§Îì§Í≥º 100% Ìò∏Ìôò
‚úÖ Ìï®ÏàòÎ™Ö/ÌÅ¥ÎûòÏä§Î™Ö/Î©îÏÑúÎìúÎ™Ö 100% Ïú†ÏßÄ + Íµ¨Ï°∞ Í∏∞Îä• Í∞úÏÑ†
‚úÖ Mock Ï†úÍ±∞, Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏµúÏ†ÅÌôî
‚úÖ BaseStepMixin v19.2 ÏôÑÎ≤Ω Ìò∏Ìôò
================================================================================

Author: MyCloset AI Team
Date: 2025-07-30
Version: 3.1 (Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod

# ==============================================
# üî• 1. ÏïàÏ†ÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Import
# ==============================================

# Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# PyTorch ÏïàÏ†Ñ import (weights_only Î¨∏Ï†ú Ìï¥Í≤∞)
TORCH_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    # weights_only Î¨∏Ï†ú Ìï¥Í≤∞
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            if weights_only is None:
                weights_only = False
            return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=weights_only, **kwargs)
        torch.load = safe_torch_load
        
except ImportError:
    torch = None

# ÎîîÎ∞îÏù¥Ïä§ Î∞è ÏãúÏä§ÌÖú Ï†ïÎ≥¥
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
MPS_AVAILABLE = False

try:
    import platform
    if platform.system() == 'Darwin':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            if 'M3' in result.stdout:
                IS_M3_MAX = True
                if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    DEFAULT_DEVICE = "mps"
                    MPS_AVAILABLE = True
        except:
            pass
except:
    pass

# auto_model_detector import (ÏïàÏ†Ñ Ï≤òÎ¶¨)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# TYPE_CHECKING Ìå®ÌÑ¥ÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# Î°úÍπÖ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)

# ==============================================
# üî• 2. Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Ï†ïÏùò
# ==============================================

class StepModelType(Enum):
    """Ïã§Ï†ú AI StepÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Î™®Îç∏ ÌÉÄÏûÖ"""
    HUMAN_PARSING = "human_parsing"           # Step 01
    POSE_ESTIMATION = "pose_estimation"       # Step 02
    CLOTH_SEGMENTATION = "cloth_segmentation" # Step 03
    GEOMETRIC_MATCHING = "geometric_matching" # Step 04
    CLOTH_WARPING = "cloth_warping"          # Step 05
    VIRTUAL_FITTING = "virtual_fitting"       # Step 06
    POST_PROCESSING = "post_processing"       # Step 07
    QUALITY_ASSESSMENT = "quality_assessment" # Step 08

class ModelStatus(Enum):
    """Î™®Îç∏ Î°úÎî© ÏÉÅÌÉú"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class ModelPriority(Enum):
    """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ (Ïã§Ï†ú AI StepÏóêÏÑú ÏÇ¨Ïö©)"""
    PRIMARY = 1      # Ï£º Î™®Îç∏ (ÌïÑÏàò)
    SECONDARY = 2    # Î≥¥Ï°∞ Î™®Îç∏
    FALLBACK = 3     # Ìè¥Î∞± Î™®Îç∏
    OPTIONAL = 4     # ÏÑ†ÌÉùÏ†Å Î™®Îç∏

@dataclass
class RealStepModelInfo:
    """Ïã§Ï†ú AI StepÏóêÏÑú ÌïÑÏöîÌïú Î™®Îç∏ Ï†ïÎ≥¥"""
    name: str
    path: str
    step_type: StepModelType
    priority: ModelPriority
    device: str
    
    # Ïã§Ï†ú Î°úÎî© Ï†ïÎ≥¥
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    checkpoint_data: Optional[Any] = None
    
    # AI Step Ìò∏ÌôòÏÑ± Ï†ïÎ≥¥
    model_class: Optional[str] = None  # Î™®Îç∏ ÌÅ¥ÎûòÏä§Î™Ö
    config_path: Optional[str] = None  # ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú
    preprocessing_params: Dict[str, Any] = field(default_factory=dict)
    
    # ÏÑ±Îä• Î©îÌä∏Î¶≠
    access_count: int = 0
    last_access: float = 0.0
    inference_count: int = 0
    avg_inference_time: float = 0.0
    
    # ÏóêÎü¨ Ï†ïÎ≥¥
    error: Optional[str] = None
    validation_passed: bool = False

@dataclass 
class StepModelRequirement:
    """StepÎ≥Ñ Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ (DetailedDataSpec Í∏∞Î∞ò)"""
    step_name: str
    step_id: int
    step_type: StepModelType
    
    # Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠
    required_models: List[str] = field(default_factory=list)
    optional_models: List[str] = field(default_factory=list)
    primary_model: Optional[str] = None
    
    # DetailedDataSpec Ïó∞Îèô
    model_configs: Dict[str, Any] = field(default_factory=dict)
    input_data_specs: Dict[str, Any] = field(default_factory=dict)
    output_data_specs: Dict[str, Any] = field(default_factory=dict)
    
    # AI Ï∂îÎ°† ÏöîÍµ¨ÏÇ¨Ìï≠
    batch_size: int = 1
    precision: str = "fp32"  # fp32, fp16, int8
    memory_limit_mb: Optional[float] = None
    
    # Ï†ÑÏ≤òÎ¶¨/ÌõÑÏ≤òÎ¶¨ ÏöîÍµ¨ÏÇ¨Ìï≠
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)

# ==============================================
# üî• 3. Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏµúÏ†ÅÌôî Î™®Îç∏ ÌÅ¥ÎûòÏä§
# ==============================================

class RealAIModel:
    """Ïã§Ï†ú AI Ï∂îÎ°†Ïóê ÏÇ¨Ïö©Ìï† Î™®Îç∏ ÌÅ¥ÎûòÏä§ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏµúÏ†ÅÌôî)"""
    
    def __init__(self, model_name: str, model_path: str, step_type: StepModelType, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.step_type = step_type
        self.device = device if device != "auto" else DEFAULT_DEVICE
        
        # Î°úÎî© ÏÉÅÌÉú
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.checkpoint_data = None
        self.model_instance = None  # Ïã§Ï†ú Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§
        
        # Í≤ÄÏ¶ù ÏÉÅÌÉú
        self.validation_passed = False
        self.compatibility_checked = False
        
        # Logger
        self.logger = logging.getLogger(f"RealAIModel.{model_name}")
        
        # StepÎ≥Ñ ÌäπÌôî Î°úÎçî Îß§Ìïë
        self.step_loaders = {
            StepModelType.HUMAN_PARSING: self._load_human_parsing_model,
            StepModelType.POSE_ESTIMATION: self._load_pose_model,
            StepModelType.CLOTH_SEGMENTATION: self._load_segmentation_model,
            StepModelType.GEOMETRIC_MATCHING: self._load_geometric_model,
            StepModelType.CLOTH_WARPING: self._load_warping_model,
            StepModelType.VIRTUAL_FITTING: self._load_diffusion_model,
            StepModelType.POST_PROCESSING: self._load_enhancement_model,
            StepModelType.QUALITY_ASSESSMENT: self._load_quality_model
        }
        
    def load(self, validate: bool = True) -> bool:
        """Î™®Îç∏ Î°úÎî© (StepÎ≥Ñ ÌäπÌôî Î°úÎî©)"""
        try:
            start_time = time.time()
            
            # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
            if not self.model_path.exists():
                self.logger.error(f"‚ùå Î™®Îç∏ ÌååÏùº ÏóÜÏùå: {self.model_path}")
                return False
            
            # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
            file_size = self.model_path.stat().st_size
            self.memory_usage_mb = file_size / (1024 * 1024)
            
            self.logger.info(f"üîÑ {self.step_type.value} Î™®Îç∏ Î°úÎî© ÏãúÏûë: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            
            # StepÎ≥Ñ ÌäπÌôî Î°úÎî©
            success = False
            if self.step_type in self.step_loaders:
                success = self.step_loaders[self.step_type]()
            else:
                success = self._load_generic_model()
            
            if success:
                self.load_time = time.time() - start_time
                self.loaded = True
                
                # Í≤ÄÏ¶ù ÏàòÌñâ
                if validate:
                    self.validation_passed = self._validate_model()
                else:
                    self.validation_passed = True
                
                self.logger.info(f"‚úÖ {self.step_type.value} Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {self.model_name} ({self.load_time:.2f}Ï¥à)")
                return True
            else:
                self.logger.error(f"‚ùå {self.step_type.value} Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {self.model_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎî© Ï§ë Ïò§Î•ò: {e}")
            return False
    
    def _load_human_parsing_model(self) -> bool:
        """Human Parsing Î™®Îç∏ Î°úÎî© (Graphonomy, ATR Îì±)"""
        try:
            # Graphonomy ÌäπÎ≥Ñ Ï≤òÎ¶¨
            if "graphonomy" in self.model_name.lower():
                return self._load_graphonomy_ultra_safe()
            
            # ÏùºÎ∞ò PyTorch Î™®Îç∏
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Human Parsing Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_pose_model(self) -> bool:
        """Pose Estimation Î™®Îç∏ Î°úÎî© (YOLO, OpenPose Îì±)"""
        try:
            # YOLO Î™®Îç∏ Ï≤òÎ¶¨
            if "yolo" in self.model_name.lower():
                self.checkpoint_data = self._load_yolo_model()
            # OpenPose Î™®Îç∏ Ï≤òÎ¶¨
            elif "openpose" in self.model_name.lower():
                self.checkpoint_data = self._load_openpose_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Pose Estimation Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_segmentation_model(self) -> bool:
        """Segmentation Î™®Îç∏ Î°úÎî© (SAM, U2Net Îì±)"""
        try:
            # SAM Î™®Îç∏ Ï≤òÎ¶¨
            if "sam" in self.model_name.lower():
                self.checkpoint_data = self._load_sam_model()
            # U2Net Î™®Îç∏ Ï≤òÎ¶¨  
            elif "u2net" in self.model_name.lower():
                self.checkpoint_data = self._load_u2net_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Segmentation Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_geometric_model(self) -> bool:
        """Geometric Matching Î™®Îç∏ Î°úÎî©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Geometric Matching Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_warping_model(self) -> bool:
        """Cloth Warping Î™®Îç∏ Î°úÎî© (Diffusion, VGG Îì±)"""
        try:
            # Safetensors ÌååÏùº Ï≤òÎ¶¨
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Cloth Warping Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_diffusion_model(self) -> bool:
        """Virtual Fitting Î™®Îç∏ Î°úÎî© (Stable Diffusion Îì±)"""
        try:
            # Safetensors Ïö∞ÏÑ† Ï≤òÎ¶¨
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            # Diffusion Î™®Îç∏ ÌäπÎ≥Ñ Ï≤òÎ¶¨
            elif "diffusion" in self.model_name.lower():
                self.checkpoint_data = self._load_diffusion_checkpoint()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Virtual Fitting Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_enhancement_model(self) -> bool:
        """Post Processing Î™®Îç∏ Î°úÎî© (Super Resolution Îì±)"""
        try:
            # Real-ESRGAN ÌäπÎ≥Ñ Ï≤òÎ¶¨
            if "esrgan" in self.model_name.lower():
                self.checkpoint_data = self._load_esrgan_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Post Processing Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_quality_model(self) -> bool:
        """Quality Assessment Î™®Îç∏ Î°úÎî© (CLIP, ViT Îì±)"""
        try:
            # CLIP Î™®Îç∏ Ï≤òÎ¶¨
            if "clip" in self.model_name.lower():
                self.checkpoint_data = self._load_clip_model()
            # ViT Î™®Îç∏ Ï≤òÎ¶¨
            elif "vit" in self.model_name.lower():
                self.checkpoint_data = self._load_vit_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"‚ùå Quality Assessment Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _load_generic_model(self) -> bool:
        """ÏùºÎ∞ò Î™®Îç∏ Î°úÎî©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"‚ùå ÏùºÎ∞ò Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    # ==============================================
    # üî• ÌäπÌôî Î°úÎçîÎì§
    # ==============================================
    
    def _load_pytorch_checkpoint(self) -> Optional[Any]:
        """PyTorch Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© (weights_only Î¨∏Ï†ú Ìï¥Í≤∞)"""
        if not TORCH_AVAILABLE:
            self.logger.error("‚ùå PyTorchÍ∞Ä ÏÇ¨Ïö© Î∂àÍ∞ÄÎä•")
            return None
        
        try:
            # 1Îã®Í≥Ñ: ÏïàÏ†Ñ Î™®Îìú (weights_only=True)
            try:
                checkpoint = torch.load(
                    self.model_path, 
                    map_location='cpu',
                    weights_only=True
                )
                self.logger.debug(f"‚úÖ {self.model_name} ÏïàÏ†Ñ Î™®Îìú Î°úÎî© ÏÑ±Í≥µ")
                return checkpoint
            except:
                pass
            
            # 2Îã®Í≥Ñ: Ìò∏Ìôò Î™®Îìú (weights_only=False)
            try:
                checkpoint = torch.load(
                    self.model_path, 
                    map_location='cpu',
                    weights_only=False
                )
                self.logger.debug(f"‚úÖ {self.model_name} Ìò∏Ìôò Î™®Îìú Î°úÎî© ÏÑ±Í≥µ")
                return checkpoint
            except:
                pass
            
            # 3Îã®Í≥Ñ: Legacy Î™®Îìú
            checkpoint = torch.load(self.model_path, map_location='cpu')
            self.logger.debug(f"‚úÖ {self.model_name} Legacy Î™®Îìú Î°úÎî© ÏÑ±Í≥µ")
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"‚ùå PyTorch Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_safetensors(self) -> Optional[Any]:
        """Safetensors ÌååÏùº Î°úÎî©"""
        try:
            import safetensors.torch
            checkpoint = safetensors.torch.load_file(str(self.model_path))
            self.logger.debug(f"‚úÖ {self.model_name} Safetensors Î°úÎî© ÏÑ±Í≥µ")
            return checkpoint
        except ImportError:
            self.logger.warning("‚ö†Ô∏è Safetensors ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏóÜÏùå, PyTorch Î°úÎî© ÏãúÎèÑ")
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"‚ùå Safetensors Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_graphonomy_ultra_safe(self) -> Optional[Any]:
        """Graphonomy 1.2GB Î™®Îç∏ Ï¥àÏïàÏ†Ñ Î°úÎî©"""
        try:
            import mmap
            import warnings
            from io import BytesIO
            
            self.logger.info(f"üîß Graphonomy Ï¥àÏïàÏ†Ñ Î°úÎî©: {self.model_path.name}")
            
            # Î©îÎ™®Î¶¨ Îß§Ìïë Î∞©Î≤ï
            try:
                with open(self.model_path, 'rb') as f:
                    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            checkpoint = torch.load(
                                BytesIO(mmapped_file[:]), 
                                map_location='cpu',
                                weights_only=False
                            )
                
                self.logger.info("‚úÖ Graphonomy Î©îÎ™®Î¶¨ Îß§Ìïë Î°úÎî© ÏÑ±Í≥µ")
                return checkpoint
                
            except Exception as e1:
                self.logger.debug(f"Î©îÎ™®Î¶¨ Îß§Ìïë Ïã§Ìå®: {str(e1)[:50]}")
            
            # ÏßÅÏ†ë pickle Î°úÎî©
            try:
                with open(self.model_path, 'rb') as f:
                    checkpoint = pickle.load(f)
                
                self.logger.info("‚úÖ Graphonomy ÏßÅÏ†ë pickle Î°úÎî© ÏÑ±Í≥µ")
                return checkpoint
                
            except Exception as e2:
                self.logger.debug(f"ÏßÅÏ†ë pickle Ïã§Ìå®: {str(e2)[:50]}")
            
            # Ìè¥Î∞±: ÏùºÎ∞ò PyTorch Î°úÎî©
            return self._load_pytorch_checkpoint()
            
        except Exception as e:
            self.logger.error(f"‚ùå Graphonomy Ï¥àÏïàÏ†Ñ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_yolo_model(self) -> Optional[Any]:
        """YOLO Î™®Îç∏ Î°úÎî©"""
        try:
            # YOLOv8 Î™®Îç∏Ïù∏ Í≤ΩÏö∞
            if "v8" in self.model_name.lower():
                try:
                    from ultralytics import YOLO
                    model = YOLO(str(self.model_path))
                    self.model_instance = model
                    return {"model": model, "type": "yolov8"}
                except ImportError:
                    pass
            
            # ÏùºÎ∞ò PyTorch Î™®Îç∏Î°ú Î°úÎî©
            return self._load_pytorch_checkpoint()
            
        except Exception as e:
            self.logger.error(f"‚ùå YOLO Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_openpose_model(self) -> Optional[Any]:
        """OpenPose Î™®Îç∏ Î°úÎî©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_sam_model(self) -> Optional[Any]:
        """SAM Î™®Îç∏ Î°úÎî©"""
        try:
            # SAM ÌäπÎ≥Ñ Ï≤òÎ¶¨ Î°úÏßÅ
            checkpoint = self._load_pytorch_checkpoint()
            if checkpoint and isinstance(checkpoint, dict):
                # SAM Î™®Îç∏ Íµ¨Ï°∞ ÌôïÏù∏
                if "model" in checkpoint:
                    return checkpoint
                elif "state_dict" in checkpoint:
                    return checkpoint
                else:
                    return {"model": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"‚ùå SAM Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_u2net_model(self) -> Optional[Any]:
        """U2Net Î™®Îç∏ Î°úÎî©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"‚ùå U2Net Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_diffusion_checkpoint(self) -> Optional[Any]:
        """Diffusion Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            
            # Diffusion Î™®Îç∏ Íµ¨Ï°∞ Ï†ïÍ∑úÌôî
            if checkpoint and isinstance(checkpoint, dict):
                if "state_dict" in checkpoint:
                    return checkpoint
                elif "model" in checkpoint:
                    return checkpoint
                else:
                    return {"state_dict": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"‚ùå Diffusion Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_esrgan_model(self) -> Optional[Any]:
        """Real-ESRGAN Î™®Îç∏ Î°úÎî©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"‚ùå Real-ESRGAN Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_clip_model(self) -> Optional[Any]:
        """CLIP Î™®Îç∏ Î°úÎî©"""
        try:
            # .bin ÌååÏùºÏù∏ Í≤ΩÏö∞
            if self.model_path.suffix.lower() == '.bin':
                checkpoint = torch.load(self.model_path, map_location='cpu')
            else:
                checkpoint = self._load_pytorch_checkpoint()
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"‚ùå CLIP Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _load_vit_model(self) -> Optional[Any]:
        """ViT Î™®Îç∏ Î°úÎî©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"‚ùå ViT Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    def _validate_model(self) -> bool:
        """Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            if self.checkpoint_data is None:
                return False
            
            # Í∏∞Î≥∏ Í≤ÄÏ¶ù: Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌôïÏù∏
            if not isinstance(self.checkpoint_data, (dict, torch.nn.Module)) and self.checkpoint_data is not None:
                self.logger.warning(f"‚ö†Ô∏è ÏòàÏÉÅÏπò Î™ªÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉÄÏûÖ: {type(self.checkpoint_data)}")
            
            # StepÎ≥Ñ ÌäπÌôî Í≤ÄÏ¶ù
            if self.step_type == StepModelType.HUMAN_PARSING:
                return self._validate_human_parsing_model()
            elif self.step_type == StepModelType.VIRTUAL_FITTING:
                return self._validate_diffusion_model()
            else:
                return True  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌÜµÍ≥º
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False
    
    def _validate_human_parsing_model(self) -> bool:
        """Human Parsing Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            if isinstance(self.checkpoint_data, dict):
                # Graphonomy Î™®Îç∏ ÌôïÏù∏
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    # ÏòàÏÉÅ ÌÇ§ ÌôïÏù∏
                    expected_keys = ["backbone", "decoder", "classifier"]
                    for key in expected_keys:
                        if any(key in k for k in state_dict.keys()):
                            return True
                
                # ÏßÅÏ†ë state_dictÏù∏ Í≤ΩÏö∞
                if any("conv" in k or "bn" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌÜµÍ≥º
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Human Parsing Î™®Îç∏ Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {e}")
            return True
    
    def _validate_diffusion_model(self) -> bool:
        """Diffusion Î™®Îç∏ Í≤ÄÏ¶ù"""
        try:
            if isinstance(self.checkpoint_data, dict):
                # U-Net Íµ¨Ï°∞ ÌôïÏù∏
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    if any("down_blocks" in k or "up_blocks" in k for k in state_dict.keys()):
                        return True
                
                # ÏßÅÏ†ë state_dictÏù∏ Í≤ΩÏö∞
                if any("time_embed" in k or "input_blocks" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌÜµÍ≥º
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Diffusion Î™®Îç∏ Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {e}")
            return True
    
    def get_checkpoint_data(self) -> Optional[Any]:
        """Î°úÎìúÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        return self.checkpoint_data
    
    def get_model_instance(self) -> Optional[Any]:
        """Ïã§Ï†ú Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (YOLO Îì±)"""
        return self.model_instance
    
    def unload(self):
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        self.loaded = False
        self.checkpoint_data = None
        self.model_instance = None
        gc.collect()
        
        # MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        if MPS_AVAILABLE and TORCH_AVAILABLE:
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except:
                pass
    
    def get_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "step_type": self.step_type.value,
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0,
            "has_checkpoint_data": self.checkpoint_data is not None,
            "has_model_instance": self.model_instance is not None,
            "validation_passed": self.validation_passed,
            "compatibility_checked": self.compatibility_checked
        }

# ==============================================
# üî• 4. Ïã§Ï†ú AI Step Ìò∏Ìôò Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í∞úÏÑ†
# ==============================================

class EnhancedStepModelInterface:
    """Ïã§Ï†ú AI StepÍ≥º ÏôÑÎ≤Ω Ìò∏ÌôòÎêòÎäî Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    
    def __init__(self, model_loader, step_name: str, step_type: StepModelType):
        self.model_loader = model_loader
        self.step_name = step_name
        self.step_type = step_type
        self.logger = logging.getLogger(f"EnhancedStepInterface.{step_name}")
        
        # StepÎ≥Ñ Î™®Îç∏Îì§
        self.step_models: Dict[str, RealAIModel] = {}
        self.primary_model: Optional[RealAIModel] = None
        self.fallback_models: List[RealAIModel] = []
        
        # DetailedDataSpec Ïó∞Îèô
        self.requirements: Optional[StepModelRequirement] = None
        self.data_specs_loaded: bool = False
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        # Ï∫êÏãú
        self.model_cache: Dict[str, Any] = {}
        self.preprocessing_cache: Dict[str, Any] = {}
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """DetailedDataSpec Í∏∞Î∞ò ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            self.requirements = StepModelRequirement(
                step_name=self.step_name,
                step_id=requirements.get('step_id', 0),
                step_type=self.step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.data_specs_loaded = True
            self.logger.info(f"‚úÖ DetailedDataSpec Í∏∞Î∞ò ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {len(self.requirements.required_models)}Í∞ú ÌïÑÏàò Î™®Îç∏")
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """Ïã§Ï†ú AI Î™®Îç∏ Î∞òÌôò (Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò)"""
        try:
            self.access_count += 1
            
            # ÌäπÏ†ï Î™®Îç∏ ÏöîÏ≤≠
            if model_name:
                if model_name in self.step_models:
                    model = self.step_models[model_name]
                    model.access_count += 1
                    model.last_access = time.time()
                    return model
                
                # ÏÉà Î™®Îç∏ Î°úÎî©
                return self._load_new_model(model_name)
            
            # Í∏∞Î≥∏ Î™®Îç∏ Î∞òÌôò (Ïö∞ÏÑ†ÏàúÏúÑ Ïàú)
            if self.primary_model and self.primary_model.loaded:
                return self.primary_model
            
            # Î°úÎìúÎêú Î™®Îç∏ Ï§ë Í∞ÄÏû• Ïö∞ÏÑ†ÏàúÏúÑ ÎÜíÏùÄ Í≤É
            for model in sorted(self.step_models.values(), key=lambda m: m.priority if hasattr(m, 'priority') else 999):
                if model.loaded:
                    return model
            
            # Ï≤´ Î≤àÏß∏ Î™®Îç∏ Î°úÎî© ÏãúÎèÑ
            if self.requirements and self.requirements.required_models:
                return self._load_new_model(self.requirements.required_models[0])
            
            return None
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"‚ùå Î™®Îç∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return None
    
    def _load_new_model(self, model_name: str) -> Optional[RealAIModel]:
        """ÏÉà Î™®Îç∏ Î°úÎî©"""
        try:
            # ModelLoaderÎ•º ÌÜµÌïú Î°úÎî©
            base_model = self.model_loader.load_model(model_name, step_name=self.step_name, step_type=self.step_type)
            
            if base_model and isinstance(base_model, RealAIModel):
                self.step_models[model_name] = base_model
                
                # Primary Î™®Îç∏ ÏÑ§Ï†ï
                if not self.primary_model or (self.requirements and model_name == self.requirements.primary_model):
                    self.primary_model = base_model
                
                return base_model
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÉà Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ÎèôÍ∏∞ Î™®Îç∏ Ï°∞Ìöå - BaseStepMixin Ìò∏Ìôò"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ï°∞Ìöå"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù - BaseStepMixin Ìò∏Ìôò"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'step_type': self.step_type.value,
                'required': kwargs.get('required', True),
                'priority': kwargs.get('priority', ModelPriority.SECONDARY.value),
                'device': kwargs.get('device', DEFAULT_DEVICE),
                'preprocessing_params': kwargs.get('preprocessing_params', {}),
                **kwargs
            }
            
            self.logger.info(f"‚úÖ Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def get_preprocessing_params(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏Î≥Ñ Ï†ÑÏ≤òÎ¶¨ ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ìöå"""
        try:
            if model_name in self.step_models:
                model = self.step_models[model_name]
                if hasattr(model, 'preprocessing_params'):
                    return model.preprocessing_params
            
            # RequirementsÏóêÏÑú Ï°∞Ìöå
            if self.requirements and model_name in self.requirements.model_configs:
                config = self.requirements.model_configs[model_name]
                return config.get('preprocessing_params', {})
            
            # StepÎ≥Ñ Í∏∞Î≥∏ Ï†ÑÏ≤òÎ¶¨ ÌååÎùºÎØ∏ÌÑ∞
            default_params = self._get_default_preprocessing_params()
            return default_params
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï†ÑÏ≤òÎ¶¨ ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}
    
    def _get_default_preprocessing_params(self) -> Dict[str, Any]:
        """StepÎ≥Ñ Í∏∞Î≥∏ Ï†ÑÏ≤òÎ¶¨ ÌååÎùºÎØ∏ÌÑ∞"""
        defaults = {
            StepModelType.HUMAN_PARSING: {
                'input_size': (512, 512),
                'normalize': True,
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            },
            StepModelType.POSE_ESTIMATION: {
                'input_size': (256, 192),
                'normalize': True,
                'confidence_threshold': 0.3
            },
            StepModelType.CLOTH_SEGMENTATION: {
                'input_size': (1024, 1024),
                'normalize': False
            },
            StepModelType.VIRTUAL_FITTING: {
                'input_size': (512, 512),
                'normalize': True,
                'guidance_scale': 7.5,
                'num_inference_steps': 20
            }
        }
        
        return defaults.get(self.step_type, {})
    
    def get_step_status(self) -> Dict[str, Any]:
        """Step ÏÉÅÌÉú Ï°∞Ìöå (DetailedDataSpec Ìè¨Ìï®)"""
        return {
            "step_name": self.step_name,
            "step_type": self.step_type.value,
            "creation_time": self.creation_time,
            "models_loaded": len(self.step_models),
            "primary_model": self.primary_model.model_name if self.primary_model else None,
            "access_count": self.access_count,
            "error_count": self.error_count,
            "inference_count": self.inference_count,
            "avg_inference_time": self.total_inference_time / max(1, self.inference_count),
            "available_models": list(self.step_models.keys()),
            "data_specs_loaded": self.data_specs_loaded,
            "requirements": {
                "required_models": self.requirements.required_models if self.requirements else [],
                "optional_models": self.requirements.optional_models if self.requirements else [],
                "primary_model": self.requirements.primary_model if self.requirements else None,
                "batch_size": self.requirements.batch_size if self.requirements else 1,
                "precision": self.requirements.precision if self.requirements else "fp32"
            }
        }

# Ïù¥Ï†Ñ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ÏôÄÏùò Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Î≥ÑÏπ≠
StepModelInterface = EnhancedStepModelInterface

# ==============================================
# üî• 5. Í∞úÏÑ†Îêú ModelLoader ÌÅ¥ÎûòÏä§ v3.1
# ==============================================

class ModelLoader:
    """
    üî• Í∞úÏÑ†Îêú ModelLoader v3.1 - Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî
    
    ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠:
    - RealAIModel ÌÅ¥ÎûòÏä§Î°ú Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏµúÏ†ÅÌôî
    - StepÎ≥Ñ ÌäπÌôî Î°úÎçî ÏßÄÏõê (Human Parsing, Pose, Segmentation Îì±)
    - DetailedDataSpec Í∏∞Î∞ò Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Ï≤òÎ¶¨
    - BaseStepMixin v19.2 ÏôÑÎ≤Ω Ìò∏Ìôò
    - StepFactory ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ≤Ω ÏßÄÏõê
    """
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):
        """ModelLoader Ï¥àÍ∏∞Ìôî"""
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # Î™®Îç∏ Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # ÏûêÎèô Í∞êÏßÄ: backend/ai_models
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Ïã§Ï†ú AI Î™®Îç∏ Í¥ÄÎ¶¨
        self.loaded_models: Dict[str, RealAIModel] = {}
        self.model_info: Dict[str, RealStepModelInfo] = {}
        self.model_status: Dict[str, ModelStatus] = {}
        
        # Step ÏöîÍµ¨ÏÇ¨Ìï≠ (DetailedDataSpec Í∏∞Î∞ò)
        self.step_requirements: Dict[str, StepModelRequirement] = {}
        self.step_interfaces: Dict[str, EnhancedStepModelInterface] = {}
        
        # auto_model_detector Ïó∞Îèô
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0,
            'inference_count': 0,
            'total_inference_time': 0.0
        }
        
        # ÎèôÍ∏∞Ìôî
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # Ïã§Ï†ú AI Step Îß§Ìïë Î°úÎî©
        self._load_real_step_mappings()
        
        # ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î°úÍπÖ
        self.logger.info(f"üöÄ Í∞úÏÑ†Îêú ModelLoader v3.1 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"üì± Device: {self.device} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
        self.logger.info(f"üìÅ Î™®Îç∏ Ï∫êÏãú: {self.model_cache_dir}")
        self.logger.info(f"üéØ Ïã§Ï†ú AI Step ÏµúÏ†ÅÌôî Î™®Îìú")
    
    def _initialize_auto_detector(self):
        """auto_model_detector Ï¥àÍ∏∞Ìôî"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("‚úÖ auto_model_detector Ïó∞Îèô ÏôÑÎ£å")
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("‚ö†Ô∏è auto_detector Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä None")
            else:
                self.logger.warning("‚ö†Ô∏è AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"‚ùå auto_model_detector Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector ÌÜµÌï© (Ïã§Ï†ú AI Step Ï†ïÎ≥¥ Ìè¨Ìï®)"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                # Step ÌÉÄÏûÖ Ï∂îÎ°†
                                step_type = self._infer_step_type(model_name, model_path)
                                
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "step_type": step_type.value if step_type else 'unknown',
                                    "model_type": self._infer_model_type(model_name),
                                    "auto_detected": True,
                                    "priority": self._infer_model_priority(model_name)
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"‚úÖ AutoDetector Ïã§Ï†ú AI Step ÌÜµÌï© ÏôÑÎ£å: {integrated_count}Í∞ú Î™®Îç∏")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå AutoDetector ÌÜµÌï© Ïã§Ìå®: {e}")
            return False
    
    def _infer_step_type(self, model_name: str, model_path: str) -> Optional[StepModelType]:
        """Î™®Îç∏Î™ÖÍ≥º Í≤ΩÎ°úÎ°ú Step ÌÉÄÏûÖ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        model_path_lower = model_path.lower()
        
        # Í≤ΩÎ°ú Í∏∞Î∞ò Ï∂îÎ°†
        if "step_01" in model_path_lower or "human_parsing" in model_path_lower:
            return StepModelType.HUMAN_PARSING
        elif "step_02" in model_path_lower or "pose" in model_path_lower:
            return StepModelType.POSE_ESTIMATION
        elif "step_03" in model_path_lower or "segmentation" in model_path_lower:
            return StepModelType.CLOTH_SEGMENTATION
        elif "step_04" in model_path_lower or "geometric" in model_path_lower:
            return StepModelType.GEOMETRIC_MATCHING
        elif "step_05" in model_path_lower or "warping" in model_path_lower:
            return StepModelType.CLOTH_WARPING
        elif "step_06" in model_path_lower or "virtual" in model_path_lower or "fitting" in model_path_lower:
            return StepModelType.VIRTUAL_FITTING
        elif "step_07" in model_path_lower or "post" in model_path_lower or "enhancement" in model_path_lower:
            return StepModelType.POST_PROCESSING
        elif "step_08" in model_path_lower or "quality" in model_path_lower:
            return StepModelType.QUALITY_ASSESSMENT
        
        # Î™®Îç∏Î™Ö Í∏∞Î∞ò Ï∂îÎ°†
        if any(keyword in model_name_lower for keyword in ["graphonomy", "atr", "schp", "parsing"]):
            return StepModelType.HUMAN_PARSING
        elif any(keyword in model_name_lower for keyword in ["yolo", "openpose", "pose"]):
            return StepModelType.POSE_ESTIMATION
        elif any(keyword in model_name_lower for keyword in ["sam", "u2net", "segment"]):
            return StepModelType.CLOTH_SEGMENTATION
        elif any(keyword in model_name_lower for keyword in ["gmm", "tps", "geometric"]):
            return StepModelType.GEOMETRIC_MATCHING
        elif any(keyword in model_name_lower for keyword in ["realvis", "vgg", "warping"]):
            return StepModelType.CLOTH_WARPING
        elif any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet", "unet", "vae"]):
            return StepModelType.VIRTUAL_FITTING
        elif any(keyword in model_name_lower for keyword in ["esrgan", "sr", "enhancement"]):
            return StepModelType.POST_PROCESSING
        elif any(keyword in model_name_lower for keyword in ["clip", "vit", "quality", "assessment"]):
            return StepModelType.QUALITY_ASSESSMENT
        
        return None
    
    def _infer_model_type(self, model_name: str) -> str:
        """Î™®Îç∏ ÌÉÄÏûÖ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet"]):
            return "DiffusionModel"
        elif any(keyword in model_name_lower for keyword in ["yolo", "detection"]):
            return "DetectionModel"
        elif any(keyword in model_name_lower for keyword in ["segment", "sam", "u2net"]):
            return "SegmentationModel"
        elif any(keyword in model_name_lower for keyword in ["pose", "openpose"]):
            return "PoseModel"
        elif any(keyword in model_name_lower for keyword in ["clip", "vit", "classification"]):
            return "ClassificationModel"
        else:
            return "BaseModel"
    
    def _infer_model_priority(self, model_name: str) -> int:
        """Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ Ï∂îÎ°†"""
        model_name_lower = model_name.lower()
        
        # Primary Î™®Îç∏Îì§
        if any(keyword in model_name_lower for keyword in ["graphonomy", "yolo", "sam", "diffusion", "esrgan", "clip"]):
            return ModelPriority.PRIMARY.value
        # Secondary Î™®Îç∏Îì§
        elif any(keyword in model_name_lower for keyword in ["atr", "openpose", "u2net", "vgg"]):
            return ModelPriority.SECONDARY.value
        else:
            return ModelPriority.OPTIONAL.value
    
    def _load_real_step_mappings(self):
        """Ïã§Ï†ú AI Step Îß§Ìïë Î°úÎî©"""
        try:
            # Ïã§Ï†ú GitHub ÌîÑÎ°úÏ†ùÌä∏ StepÎ≥Ñ Î™®Îç∏ Îß§Ìïë
            self.real_step_mappings = {
                'HumanParsingStep': {
                    'step_type': StepModelType.HUMAN_PARSING,
                    'local_paths': [
                        'step_01_human_parsing/graphonomy.pth',
                        'step_01_human_parsing/atr_model.pth',
                        'step_01_human_parsing/human_parsing_schp.pth'
                    ],
                    'primary_model': 'graphonomy.pth',
                    'model_configs': {
                        'graphonomy.pth': {
                            'model_class': 'GraphonomyNet',
                            'num_classes': 20,
                            'input_size': (512, 512)
                        }
                    }
                },
                'PoseEstimationStep': {
                    'step_type': StepModelType.POSE_ESTIMATION,
                    'local_paths': [
                        'step_02_pose_estimation/yolov8n-pose.pt',
                        'step_02_pose_estimation/openpose_pose_coco.pth'
                    ],
                    'primary_model': 'yolov8n-pose.pt',
                    'model_configs': {
                        'yolov8n-pose.pt': {
                            'model_class': 'YOLOv8',
                            'confidence_threshold': 0.3,
                            'input_size': (640, 640)
                        }
                    }
                },
                'ClothSegmentationStep': {
                    'step_type': StepModelType.CLOTH_SEGMENTATION,
                    'local_paths': [
                        'step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'step_03_cloth_segmentation/u2net.pth',
                        'step_03_cloth_segmentation/mobile_sam.pt'
                    ],
                    'primary_model': 'sam_vit_h_4b8939.pth',
                    'model_configs': {
                        'sam_vit_h_4b8939.pth': {
                            'model_class': 'SAM',
                            'encoder': 'vit_h',
                            'input_size': (1024, 1024)
                        }
                    }
                },
                'GeometricMatchingStep': {
                    'step_type': StepModelType.GEOMETRIC_MATCHING,
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth',
                        'step_04_geometric_matching/tps_model.pth'
                    ],
                    'primary_model': 'gmm_final.pth'
                },
                'ClothWarpingStep': {
                    'step_type': StepModelType.CLOTH_WARPING,
                    'local_paths': [
                        'step_05_cloth_warping/RealVisXL_V4.0.safetensors',
                        'step_05_cloth_warping/vgg19_warping.pth'
                    ],
                    'primary_model': 'RealVisXL_V4.0.safetensors'
                },
                'VirtualFittingStep': {
                    'step_type': StepModelType.VIRTUAL_FITTING,
                    'local_paths': [
                        'step_06_virtual_fitting/diffusion_pytorch_model.safetensors',
                        'step_06_virtual_fitting/v1-5-pruned.safetensors',
                        'step_06_virtual_fitting/v1-5-pruned-emaonly.safetensors',
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.bin'
                    ],
                    'primary_model': 'diffusion_pytorch_model.safetensors',
                    'model_configs': {
                        'diffusion_pytorch_model.safetensors': {
                            'model_class': 'UNet2DConditionModel',
                            'guidance_scale': 7.5,
                            'num_inference_steps': 20
                        }
                    }
                },
                'PostProcessingStep': {
                    'step_type': StepModelType.POST_PROCESSING,
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth',
                        'step_07_post_processing/sr_model.pth'
                    ],
                    'primary_model': 'Real-ESRGAN_x4plus.pth'
                },
                'QualityAssessmentStep': {
                    'step_type': StepModelType.QUALITY_ASSESSMENT,
                    'local_paths': [
                        'step_08_quality_assessment/ViT-L-14.pt',
                        'step_08_quality_assessment/open_clip_pytorch_model.bin'
                    ],
                    'primary_model': 'ViT-L-14.pt',
                    'model_configs': {
                        'ViT-L-14.pt': {
                            'model_class': 'CLIP',
                            'vision_model': 'ViT-L/14'
                        }
                    }
                }
            }
            
            self.logger.info(f"‚úÖ Ïã§Ï†ú AI Step Îß§Ìïë Î°úÎî© ÏôÑÎ£å: {len(self.real_step_mappings)}Í∞ú Step")
            
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Step Îß§Ìïë Î°úÎî© Ïã§Ìå®: {e}")
            self.real_step_mappings = {}
    
    # ==============================================
    # üî• ÌïµÏã¨ Î™®Îç∏ Î°úÎî© Î©îÏÑúÎìúÎì§ (Í∞úÏÑ†)
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© (StepÎ≥Ñ ÌäπÌôî Î°úÎî©)"""
        try:
            with self._lock:
                # Ï∫êÏãú ÌôïÏù∏
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        model.access_count += 1
                        model.last_access = time.time()
                        self.logger.debug(f"‚ôªÔ∏è Ï∫êÏãúÎêú Ïã§Ï†ú AI Î™®Îç∏ Î∞òÌôò: {model_name}")
                        return model
                
                # ÏÉà Î™®Îç∏ Î°úÎî©
                self.model_status[model_name] = ModelStatus.LOADING
                
                # Î™®Îç∏ Í≤ΩÎ°ú Î∞è Step ÌÉÄÏûÖ Í≤∞Ï†ï
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°úÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {model_name}")
                    self.model_status[model_name] = ModelStatus.ERROR
                    return None
                
                # Step ÌÉÄÏûÖ Ï∂îÎ°†
                step_type = kwargs.get('step_type')
                if not step_type:
                    step_type = self._infer_step_type(model_name, model_path)
                
                if not step_type:
                    step_type = StepModelType.HUMAN_PARSING  # Í∏∞Î≥∏Í∞í
                
                # RealAIModel ÏÉùÏÑ± Î∞è Î°úÎî©
                model = RealAIModel(
                    model_name=model_name,
                    model_path=model_path,
                    step_type=step_type,
                    device=self.device
                )
                
                # Î™®Îç∏ Î°úÎî© ÏàòÌñâ
                if model.load(validate=kwargs.get('validate', True)):
                    # Ï∫êÏãúÏóê Ï†ÄÏû•
                    self.loaded_models[model_name] = model
                    
                    # Î™®Îç∏ Ï†ïÎ≥¥ Ï†ÄÏû•
                    priority = ModelPriority(kwargs.get('priority', ModelPriority.SECONDARY.value))
                    self.model_info[model_name] = RealStepModelInfo(
                        name=model_name,
                        path=model_path,
                        step_type=step_type,
                        priority=priority,
                        device=self.device,
                        memory_mb=model.memory_usage_mb,
                        loaded=True,
                        load_time=model.load_time,
                        checkpoint_data=model.checkpoint_data,
                        validation_passed=model.validation_passed,
                        access_count=1,
                        last_access=time.time()
                    )
                    
                    self.model_status[model_name] = ModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"‚úÖ Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ: {model_name} ({step_type.value}, {model.memory_usage_mb:.1f}MB)")
                    
                    # Ï∫êÏãú ÌÅ¨Í∏∞ Í¥ÄÎ¶¨
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = ModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            self.model_status[model_name] = ModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None

    async def load_model_async(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎî©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"‚ùå ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎî© Ïã§Ìå® {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """Ïã§Ï†ú AI StepÏö© Î™®Îç∏ Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        try:
            # ÏßÅÏ†ë Í≤ΩÎ°ú ÏßÄÏ†ï
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # available_modelsÏóêÏÑú Ï∞æÍ∏∞
            if model_name in self._available_models_cache:
                model_info = self._available_models_cache[model_name]
                path = Path(model_info.get('path', ''))
                if path.exists():
                    return str(path)
            
            # Step Í∏∞Î∞ò Îß§ÌïëÏóêÏÑú Ï∞æÍ∏∞ (Ìñ•ÏÉÅÎêú Î°úÏßÅ)
            step_name = kwargs.get('step_name')
            if step_name and step_name in self.real_step_mappings:
                mapping = self.real_step_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        # Î™®Îç∏Î™Ö Îß§Ïπ≠ ÌôïÏù∏
                        if model_name in local_path or local_path.stem == model_name:
                            return str(full_path)
            
            # Î™®Îì† Step Îß§ÌïëÏóêÏÑú Ï∞æÍ∏∞
            for step_name, mapping in self.real_step_mappings.items():
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        if model_name in local_path or local_path.stem == model_name:
                            return str(full_path)
            
            # ÌôïÏû•Ïûê Ìå®ÌÑ¥ÏúºÎ°ú Í≤ÄÏÉâ
            possible_patterns = [
                f"**/{model_name}",
                f"**/{model_name}.*",
                f"**/*{model_name}*",
                f"**/step_*/{model_name}.*"
            ]
            
            for pattern in possible_patterns:
                for found_path in self.model_cache_dir.glob(pattern):
                    if found_path.is_file():
                        return str(found_path)
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Í≤ΩÎ°ú Ï∞æÍ∏∞ Ïã§Ìå® {model_name}: {e}")
            return None
    
    def _manage_cache(self):
        """Ïã§Ï†ú AI Î™®Îç∏ Ï∫êÏãú Í¥ÄÎ¶¨"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # Ïö∞ÏÑ†ÏàúÏúÑÏôÄ ÎßàÏßÄÎßâ Ï†ëÍ∑º ÏãúÍ∞Ñ Í∏∞Î∞ò Ï†ïÎ†¨
            models_by_priority = sorted(
                self.model_info.items(),
                key=lambda x: (x[1].priority.value, x[1].last_access)
            )
            
            models_to_remove = models_by_priority[:len(self.loaded_models) - self.max_cached_models]
            
            for model_name, _ in models_to_remove:
                # Primary Î™®Îç∏ÏùÄ Î≥¥Ìò∏
                if any(mapping.get('primary_model') == model_name for mapping in self.real_step_mappings.values()):
                    continue
                
                self.unload_model(model_name)
                
        except Exception as e:
            self.logger.error(f"‚ùå Ï∫êÏãú Í¥ÄÎ¶¨ Ïã§Ìå®: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """Ïã§Ï†ú AI Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = ModelStatus.NOT_LOADED
                    
                    self.logger.info(f"‚úÖ Ïã§Ï†ú AI Î™®Îç∏ Ïñ∏Î°úÎìú ÏôÑÎ£å: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return False
    
    # ==============================================
    # üî• Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏßÄÏõê (Í∞úÏÑ†)
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> EnhancedStepModelInterface:
        """Ïã§Ï†ú AI Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± (DetailedDataSpec ÏßÄÏõê)"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            # Step ÌÉÄÏûÖ Í≤∞Ï†ï
            step_type = None
            if step_name in self.real_step_mappings:
                step_type = self.real_step_mappings[step_name].get('step_type')
            
            if not step_type:
                # Ïù¥Î¶ÑÏúºÎ°ú Ï∂îÎ°†
                step_type_map = {
                    'HumanParsingStep': StepModelType.HUMAN_PARSING,
                    'PoseEstimationStep': StepModelType.POSE_ESTIMATION,
                    'ClothSegmentationStep': StepModelType.CLOTH_SEGMENTATION,
                    'GeometricMatchingStep': StepModelType.GEOMETRIC_MATCHING,
                    'ClothWarpingStep': StepModelType.CLOTH_WARPING,
                    'VirtualFittingStep': StepModelType.VIRTUAL_FITTING,
                    'PostProcessingStep': StepModelType.POST_PROCESSING,
                    'QualityAssessmentStep': StepModelType.QUALITY_ASSESSMENT
                }
                step_type = step_type_map.get(step_name, StepModelType.HUMAN_PARSING)
            
            interface = EnhancedStepModelInterface(self, step_name, step_type)
            
            # DetailedDataSpec Í∏∞Î∞ò ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù
            if step_requirements:
                interface.register_requirements(step_requirements)
            elif step_name in self.real_step_mappings:
                # Í∏∞Î≥∏ Îß§ÌïëÏóêÏÑú ÏöîÍµ¨ÏÇ¨Ìï≠ ÏÉùÏÑ±
                mapping = self.real_step_mappings[step_name]
                default_requirements = {
                    'step_id': self._get_step_id(step_name),
                    'required_models': [Path(p).name for p in mapping.get('local_paths', [])],
                    'primary_model': mapping.get('primary_model'),
                    'model_configs': mapping.get('model_configs', {}),
                    'batch_size': 1,
                    'precision': 'fp16' if self.device == 'mps' else 'fp32'
                }
                interface.register_requirements(default_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"‚úÖ Ïã§Ï†ú AI Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±: {step_name} ({step_type.value})")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
            return EnhancedStepModelInterface(self, step_name, StepModelType.HUMAN_PARSING)
    
    def _get_step_id(self, step_name: str) -> int:
        """Step Ïù¥Î¶ÑÏúºÎ°ú ID Î∞òÌôò"""
        step_id_map = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return step_id_map.get(step_name, 0)
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """DetailedDataSpec Í∏∞Î∞ò Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            step_type = requirements.get('step_type')
            if isinstance(step_type, str):
                step_type = StepModelType(step_type)
            elif not step_type:
                if step_name in self.real_step_mappings:
                    step_type = self.real_step_mappings[step_name].get('step_type')
                else:
                    step_type = StepModelType.HUMAN_PARSING
            
            self.step_requirements[step_name] = StepModelRequirement(
                step_name=step_name,
                step_id=requirements.get('step_id', self._get_step_id(step_name)),
                step_type=step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.logger.info(f"‚úÖ DetailedDataSpec Í∏∞Î∞ò Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå® {step_name}: {e}")
            return False
    
    # ==============================================
    # üî• BaseStepMixin Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§ (Î™®Îëê Ïú†ÏßÄ)
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """Ï¥àÍ∏∞Ìôî ÏÉÅÌÉú ÌôïÏù∏"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """Ï¥àÍ∏∞Ìôî"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("‚úÖ Í∞úÏÑ†Îêú ModelLoader Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî"""
        return self.initialize(**kwargs)
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù - BaseStepMixin Ìò∏Ìôò"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                # Step ÌÉÄÏûÖ Ï∂îÎ°†
                step_type = kwargs.get('step_type')
                if isinstance(step_type, str):
                    step_type = StepModelType(step_type)
                elif not step_type:
                    step_type = self._infer_step_type(model_name, kwargs.get('model_path', ''))
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'step_type': step_type.value if step_type else 'unknown',
                    'required': kwargs.get('required', True),
                    'priority': kwargs.get('priority', ModelPriority.SECONDARY.value),
                    'device': kwargs.get('device', self.device),
                    'preprocessing_params': kwargs.get('preprocessing_params', {}),
                    **kwargs
                }
                
                self.logger.info(f"‚úÖ Ïã§Ï†ú AI Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """Ïã§Ï†ú AI Î™®Îç∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"""
        try:
            # Î™®Îç∏ Ï†ïÎ≥¥ ÌôïÏù∏
            if model_name not in self.model_info and model_name not in self._available_models_cache:
                return False
            
            # Step ÏöîÍµ¨ÏÇ¨Ìï≠ ÌôïÏù∏
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # Ïã§Ï†ú Step Îß§Ìïë ÌôïÏù∏
            if step_name in self.real_step_mappings:
                mapping = self.real_step_mappings[step_name]
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path or Path(local_path).name == model_name:
                        return True
            
            return True  # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ìò∏Ìôò Í∞ÄÎä•ÏúºÎ°ú Ï≤òÎ¶¨
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """Î™®Îç∏ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏"""
        return (model_name in self.loaded_models or 
                model_name in self._available_models_cache or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """Î™®Îç∏ Î°úÎî© ÏÉÅÌÉú ÌôïÏù∏"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def create_step_model_interface(self, step_name: str) -> EnhancedStepModelInterface:
        """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
        return self.create_step_interface(step_name)
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïã§Ï†ú AI Î™®Îç∏ Î™©Î°ù"""
        try:
            models = []
            
            # available_modelsÏóêÏÑú Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
            for model_name, model_info in self._available_models_cache.items():
                # ÌïÑÌÑ∞ÎßÅ
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # Î°úÎî© ÏÉÅÌÉú Ï∂îÍ∞Ä
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                models.append(model_info_copy)
            
            # Ïã§Ï†ú Step Îß§ÌïëÏóêÏÑú Ï∂îÍ∞Ä
            for step_name, mapping in self.real_step_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                step_type = mapping.get('step_type', StepModelType.HUMAN_PARSING)
                for local_path in mapping.get('local_paths', []):
                    full_path = self.model_cache_dir / local_path
                    if full_path.exists():
                        model_name = Path(local_path).name
                        if model_name not in [m['name'] for m in models]:
                            models.append({
                                'name': model_name,
                                'path': str(full_path),
                                'type': self._infer_model_type(model_name),
                                'step_type': step_type.value,
                                'loaded': model_name in self.loaded_models,
                                'step_class': step_name,
                                'size_mb': full_path.stat().st_size / (1024 * 1024),
                                'priority': self._infer_model_priority(model_name),
                                'is_primary': model_name == mapping.get('primary_model')
                            })
            
            return models
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Ïã§Ï†ú AI Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'step_type': info.step_type.value,
                    'priority': info.priority.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'inference_count': info.inference_count,
                    'avg_inference_time': info.avg_inference_time,
                    'validation_passed': info.validation_passed,
                    'has_checkpoint_data': info.checkpoint_data is not None,
                    'error': info.error
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Ïã§Ï†ú AI Î™®Îç∏ ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "mps_available": MPS_AVAILABLE,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self._available_models_cache),
            "step_interfaces_count": len(self.step_interfaces),
            "avg_inference_time": self.performance_metrics['total_inference_time'] / max(1, self.performance_metrics['inference_count']),
            "memory_efficiency": self.performance_metrics['total_memory_mb'] / max(1, len(self.loaded_models))
        }
    
    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            self.logger.info("üßπ Í∞úÏÑ†Îêú ModelLoader Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ï§ë...")
            
            # Î™®Îì† Ïã§Ï†ú AI Î™®Îç∏ Ïñ∏Î°úÎìú
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            self.step_requirements.clear()
            
            # Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å
            self._executor.shutdown(wait=True)
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            gc.collect()
            
            # MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if MPS_AVAILABLE and TORCH_AVAILABLE:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("‚úÖ Í∞úÏÑ†Îêú ModelLoader Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

# ==============================================
# üî• 6. Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞è Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§ (Î™®Îëê Ïú†ÏßÄ)
# ==============================================

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            try:
                # ÏÑ§Ï†ï Ï†ÅÏö©
                loader_config = config or {}
                
                _global_model_loader = ModelLoader(
                    device=loader_config.get('device', 'auto'),
                    max_cached_models=loader_config.get('max_cached_models', 10),
                    enable_optimization=loader_config.get('enable_optimization', True),
                    **loader_config
                )
                
                logger.info("‚úÖ Ï†ÑÏó≠ Í∞úÏÑ†Îêú ModelLoader v3.1 ÏÉùÏÑ± ÏÑ±Í≥µ")
                
            except Exception as e:
                logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÏÉùÏÑ± Ïã§Ìå®: {e}")
                # Í∏∞Î≥∏ ÏÑ§Ï†ïÏúºÎ°ú Ìè¥Î∞±
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        else:
            logger.warning("‚ö†Ô∏è Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî ÏùºÎ∂Ä Ïã§Ìå®")
            
        return loader
        
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> EnhancedStepModelInterface:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
        step_type = StepModelType.HUMAN_PARSING
        return EnhancedStepModelInterface(get_global_model_loader(), step_name, step_type)

def get_model(model_name: str) -> Optional[RealAIModel]:
    """Ï†ÑÏó≠ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[RealAIModel]:
    """Ï†ÑÏó≠ ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> EnhancedStepModelInterface:
    """Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# ==============================================
# üî• 7. Export Î∞è Ï¥àÍ∏∞Ìôî
# ==============================================

__all__ = [
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§ (Í∞úÏÑ†)
    'ModelLoader',
    'EnhancedStepModelInterface',
    'StepModelInterface',  # Ìò∏ÌôòÏÑ± Î≥ÑÏπ≠
    'RealAIModel',
    
    # Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îì§
    'StepModelType',
    'ModelStatus',
    'ModelPriority',
    'RealStepModelInfo',
    'StepModelRequirement',
    
    # Ï†ÑÏó≠ Ìï®ÏàòÎì§ (Î™®Îëê Ïú†ÏßÄ)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # ÏÉÅÏàòÎì§
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'TORCH_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'MPS_AVAILABLE',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ==============================================
# üî• 8. Î™®Îìà Ï¥àÍ∏∞Ìôî Î∞è ÏôÑÎ£å Î©îÏãúÏßÄ
# ==============================================

logger.info("=" * 80)
logger.info("üöÄ Í∞úÏÑ†Îêú ModelLoader v3.1 - Ïã§Ï†ú AI Step Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî")
logger.info("=" * 80)
logger.info("‚úÖ Ïã§Ï†ú AI Step ÌååÏùºÎì§Í≥ºÏùò Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨ Íµ¨Ï°∞ ÏµúÏ†ÅÌôî")
logger.info("‚úÖ RealAIModel ÌÅ¥ÎûòÏä§Î°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÏ†Ñ Í∞úÏÑ†")
logger.info("‚úÖ StepÎ≥Ñ ÌäπÌôî Î°úÎçî ÏßÄÏõê (Human Parsing, Pose, Segmentation Îì±)")
logger.info("‚úÖ DetailedDataSpec Í∏∞Î∞ò Î™®Îç∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Ï†ïÌôï Îß§Ìïë")
logger.info("‚úÖ StepFactory ‚Üí BaseStepMixin ‚Üí StepInterface ‚Üí ModelLoader ÌùêÎ¶Ñ ÏôÑÎ≤Ω ÏßÄÏõê")
logger.info("‚úÖ GitHub ÌîÑÎ°úÏ†ùÌä∏ Step ÌÅ¥ÎûòÏä§Îì§Í≥º 100% Ìò∏Ìôò")
logger.info("‚úÖ Ìï®ÏàòÎ™Ö/ÌÅ¥ÎûòÏä§Î™Ö/Î©îÏÑúÎìúÎ™Ö 100% Ïú†ÏßÄ + Íµ¨Ï°∞ Í∏∞Îä• Í∞úÏÑ†")
logger.info("‚úÖ BaseStepMixin v19.2 ÏôÑÎ≤Ω Ìò∏Ìôò")

logger.info(f"üîß ÏãúÏä§ÌÖú Ï†ïÎ≥¥:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
logger.info(f"   PyTorch: {TORCH_AVAILABLE}, NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info(f"   conda ÌôòÍ≤Ω: {CONDA_ENV}")

logger.info("üéØ ÏßÄÏõê Ïã§Ï†ú AI Step ÌÉÄÏûÖ:")
for step_type in StepModelType:
    logger.info(f"   - {step_type.value}: ÌäπÌôî Î°úÎçî ÏßÄÏõê")

logger.info("üî• ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠:")
logger.info("   ‚Ä¢ RealAIModel: StepÎ≥Ñ ÌäπÌôî Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©")
logger.info("   ‚Ä¢ EnhancedStepModelInterface: DetailedDataSpec ÏôÑÏ†Ñ ÏßÄÏõê")
logger.info("   ‚Ä¢ Ïã§Ï†ú AI Step Îß§Ìïë: GitHub ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞ Í∏∞Î∞ò")
logger.info("   ‚Ä¢ Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò Î™®Îç∏ Ï∫êÏã±: Primary/Secondary/Fallback")
logger.info("   ‚Ä¢ Graphonomy 1.2GB Î™®Îç∏ Ï¥àÏïàÏ†Ñ Î°úÎî©")
logger.info("   ‚Ä¢ Safetensors + PyTorch weights_only ÏôÑÎ≤Ω ÏßÄÏõê")

logger.info("üöÄ Ïã§Ï†ú AI Step ÏßÄÏõê ÌùêÎ¶Ñ:")
logger.info("   StepFactory (v11.0)")
logger.info("     ‚Üì (Step Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± + ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ)")
logger.info("   BaseStepMixin (v19.2)")
logger.info("     ‚Üì (ÎÇ¥Ïû• GitHubDependencyManager ÏÇ¨Ïö©)")
logger.info("   step_interface.py (v5.1)")
logger.info("     ‚Üì (ModelLoader, MemoryManager Îì± Ï†úÍ≥µ)")
logger.info("   ModelLoader (v3.1) ‚Üê üî• Ïó¨Í∏∞ÏÑú ÏµúÏ†ÅÌôî!")
logger.info("     ‚Üì (RealAIModelÎ°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©)")
logger.info("   Ïã§Ï†ú AI Î™®Îç∏Îì§ (Graphonomy, YOLO, SAM, Diffusion Îì±)")

logger.info("üéâ Í∞úÏÑ†Îêú ModelLoader v3.1 Ï§ÄÎπÑ ÏôÑÎ£å!")
logger.info("üéâ Ïã§Ï†ú AI Step ÌååÏùºÎì§Í≥ºÏùò ÏôÑÎ≤ΩÌïú Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨ Íµ¨Ï°∞ ÏôÑÏÑ±!")
logger.info("üéâ Mock Ï†úÍ±∞, Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏµúÏ†ÅÌôî ÏôÑÎ£å!")
logger.info("=" * 80)

# Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏
try:
    _test_loader = get_global_model_loader()
    logger.info(f"üéâ Í∞úÏÑ†Îêú ModelLoader v3.1 Ï§ÄÎπÑ ÏôÑÎ£å!")
    logger.info(f"   ÎîîÎ∞îÏù¥Ïä§: {_test_loader.device}")
    logger.info(f"   Î™®Îç∏ Ï∫êÏãú: {_test_loader.model_cache_dir}")
    logger.info(f"   Ïã§Ï†ú Step Îß§Ìïë: {len(_test_loader.real_step_mappings)}Í∞ú Step")
    logger.info(f"   AutoDetector ÌÜµÌï©: {_test_loader._integration_successful}")
    logger.info(f"   ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏: {len(_test_loader._available_models_cache)}Í∞ú")
except Exception as e:
    logger.error(f"‚ùå Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")