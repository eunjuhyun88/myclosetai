# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI - ê°„ì†Œí™”ëœ ModelLoader v13.0 (auto_model_detector ì œê±°)
=====================================================================
âœ… auto_model_detector ê¸°ëŠ¥ ì™„ì „ ì œê±°
âœ… AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬
âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… register_step_requirements ë©”ì„œë“œ ìœ ì§€
âœ… base_step_mixin.py íŒ¨í„´ 100% í˜¸í™˜
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
âœ… ì½”ë“œ í¬ê¸° ëŒ€í­ ê°ì†Œ (11,000 â†’ 3,000 ë¼ì¸)

Author: MyCloset AI Team
Date: 2025-07-21
Version: 13.0 (Simplified, Clean Architecture)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================================

class LibraryCompatibility:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        
        self._check_libraries()
    
    def _check_libraries(self):
        """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬"""
        # NumPy ì²´í¬
        try:
            import numpy as np
            self.numpy_available = True
            globals()['np'] = np
        except ImportError:
            self.numpy_available = False
        
        # PyTorch ì²´í¬
        try:
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.device_type = "cpu"
            
            # M3 Max MPS ì„¤ì •
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.mps_available = True
                self.device_type = "mps"
                self.is_m3_max = True
                
                # ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except (AttributeError, RuntimeError):
                    pass
            elif torch.cuda.is_available():
                self.device_type = "cuda"
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError:
            self.torch_available = False
            self.mps_available = False

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì ì´ˆê¸°í™”
_compat = LibraryCompatibility()

# ì „ì—­ ìƒìˆ˜
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
NUMPY_AVAILABLE = _compat.numpy_available
DEFAULT_DEVICE = _compat.device_type
IS_M3_MAX = _compat.is_m3_max

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë³„ë„ ëª¨ë“ˆ ì„í¬íŠ¸ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸
try:
    from ..models.ai_models import (
        BaseModel, GraphonomyModel, OpenPoseModel, U2NetModel, 
        GeometricMatchingModel, VirtualFittingModel, ModelFactory,
        create_model_by_step, validate_model_compatibility
    )
    AI_MODELS_AVAILABLE = True
    logger.info("âœ… AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    AI_MODELS_AVAILABLE = False
    logger.warning(f"âš ï¸ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í´ë°± í´ë˜ìŠ¤ë“¤
    class BaseModel:
        def __init__(self):
            self.model_name = "BaseModel"
            self.device = "cpu"
        def forward(self, x): return x
        def __call__(self, x): return self.forward(x)
    
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel
    VirtualFittingModel = BaseModel

# ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
try:
    from .image_processing import (
        preprocess_image, postprocess_segmentation,
        preprocess_pose_input, preprocess_human_parsing_input,
        preprocess_cloth_segmentation_input, preprocess_virtual_fitting_input,
        tensor_to_pil, pil_to_tensor, resize_image, normalize_image,
        denormalize_image, create_batch, image_to_base64, base64_to_image,
        cleanup_image_memory, validate_image_format
    )
    IMAGE_PROCESSING_AVAILABLE = True
    logger.info("âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    IMAGE_PROCESSING_AVAILABLE = False
    logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í´ë°± í•¨ìˆ˜ë“¤
    def preprocess_image(image, target_size=(512, 512), **kwargs):
        return image
    def postprocess_segmentation(output, threshold=0.5):
        return output
    def tensor_to_pil(tensor): return tensor
    def pil_to_tensor(image, device="cpu"): return image

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

class StepPriority(IntEnum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    CAFFE = "caffemodel"
    ONNX = "onnx"
    PICKLE = "pkl"
    BIN = "bin"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì •ì˜ (ë‹¨ìˆœí™”)
# ==============================================

STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "ClothWarpingStep": {
        "model_name": "cloth_warping_net",
        "model_type": "ClothWarpingModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*warping*.pth", "*flow*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

# ==============================================
# ğŸ”¥ ì•ˆì „ì„± ë° ë¹„ë™ê¸° ì²˜ë¦¬ í´ë˜ìŠ¤ë“¤
# ==============================================

def safe_async_call(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ ì•ˆì „ í˜¸ì¶œ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            if asyncio.iscoroutinefunction(func):
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        return asyncio.create_task(func(*args, **kwargs))
                    else:
                        return loop.run_until_complete(func(*args, **kwargs))
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(func(*args, **kwargs))
                    finally:
                        loop.close()
            else:
                return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"âŒ safe_async_call ì˜¤ë¥˜: {e}")
            return None
    return wrapper

class SafeFunctionValidator:
    """í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ ì•ˆì „ì„± ê²€ì¦ í´ë˜ìŠ¤"""
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """ê°ì²´ê°€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ ê²€ì¦"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            return True, "Valid callable object", obj
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call_async")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs))
                    return True, result, "Sync in executor success"
                    
            except Exception as e:
                return False, None, f"Async call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"

class AsyncCompatibilityManager:
    """ë¹„ë™ê¸° í˜¸í™˜ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AsyncCompatibilityManager")
        
    def make_callable_safe(self, obj: Any) -> Any:
        """ê°ì²´ë¥¼ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜"""
        try:
            if obj is None:
                return self._create_none_wrapper()
            
            if isinstance(obj, dict):
                return self._create_dict_wrapper(obj)
            
            if callable(obj):
                return self._create_callable_wrapper(obj)
            
            return self._create_object_wrapper(obj)
            
        except Exception as e:
            self.logger.error(f"âŒ make_callable_safe ì˜¤ë¥˜: {e}")
            return self._create_emergency_wrapper(obj, str(e))
    
    def _create_none_wrapper(self) -> Any:
        """None ê°ì²´ìš© ë˜í¼"""
        class SafeNoneWrapper:
            def __init__(self):
                self.name = "none_wrapper"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': None,
                    'call_type': 'none_wrapper'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
        
        return SafeNoneWrapper()
    
    def _create_dict_wrapper(self, data: Dict[str, Any]) -> Any:
        """Dictë¥¼ callable wrapperë¡œ ë³€í™˜"""
        class SafeDictWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_type': 'sync'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_type': 'async'
                }
        
        return SafeDictWrapper(data)
    
    def _create_callable_wrapper(self, func) -> Any:
        """Callable ê°ì²´ë¥¼ ì•ˆì „í•œ wrapperë¡œ ë³€í™˜"""
        class SafeCallableWrapper:
            def __init__(self, func):
                self.func = func
                self.is_async = asyncio.iscoroutinefunction(func)
                
            def __call__(self, *args, **kwargs):
                if self.is_async:
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            return asyncio.create_task(self.func(*args, **kwargs))
                        else:
                            return loop.run_until_complete(self.func(*args, **kwargs))
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            return loop.run_until_complete(self.func(*args, **kwargs))
                        finally:
                            loop.close()
                else:
                    return self.func(*args, **kwargs)
            
            async def async_call(self, *args, **kwargs):
                if self.is_async:
                    return await self.func(*args, **kwargs)
                else:
                    return self.func(*args, **kwargs)
        
        return SafeCallableWrapper(func)
    
    def _create_object_wrapper(self, obj: Any) -> Any:
        """ì¼ë°˜ ê°ì²´ìš© ë˜í¼"""
        class SafeObjectWrapper:
            def __init__(self, obj: Any):
                self.obj = obj
                self.name = f"object_wrapper_{type(obj).__name__}"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'wrapped_{self.name}',
                    'call_type': 'object_wrapper'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def __getattr__(self, name):
                if hasattr(self.obj, name):
                    return getattr(self.obj, name)
                raise AttributeError(f"'{self.name}' has no attribute '{name}'")
        
        return SafeObjectWrapper(obj)
    
    def _create_emergency_wrapper(self, obj: Any, error_msg: str) -> Any:
        """ê¸´ê¸‰ ìƒí™©ìš© ë˜í¼"""
        class EmergencyWrapper:
            def __init__(self, obj: Any, error: str):
                self.obj = obj
                self.error = error
                self.name = "emergency_wrapper"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'emergency',
                    'model_name': self.name,
                    'result': f'emergency_result',
                    'error': self.error,
                    'call_type': 'emergency'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
        
        return EmergencyWrapper(obj, error_msg)

# ==============================================
# ğŸ”¥ ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ë“¤
# ==============================================

class DeviceManager:
    """ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = IS_M3_MAX
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                devices.append("mps")
                self.logger.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = IS_M3_MAX
        self.logger = logging.getLogger(f"{__name__}.ModelMemoryManager")
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                if self.is_m3_max:
                    return 100.0  # 128GB ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                return 16.0
            else:
                return 8.0
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            self.logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            return {
                "success": True,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class SafeModelService:
    """ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.async_lock = asyncio.Lock()
        self.validator = SafeFunctionValidator()
        self.async_manager = AsyncCompatibilityManager()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self.lock:
                if isinstance(model, dict):
                    wrapper = self.async_manager.make_callable_safe(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ë”•ì…”ë„ˆë¦¬ ëª¨ë¸ì„ callable wrapperë¡œ ë“±ë¡: {name}")
                elif callable(model):
                    is_callable, reason, safe_model = self.validator.validate_callable(model, f"register_{name}")
                    if is_callable:
                        safe_wrapped = self.async_manager.make_callable_safe(safe_model)
                        self.models[name] = safe_wrapped
                        self.logger.info(f"ğŸ“ ê²€ì¦ëœ callable ëª¨ë¸ ë“±ë¡: {name}")
                    else:
                        wrapper = self.async_manager.make_callable_safe(model)
                        self.models[name] = wrapper
                        self.logger.warning(f"âš ï¸ ì•ˆì „í•˜ì§€ ì•Šì€ callable ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                else:
                    wrapper = self.async_manager.make_callable_safe(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ê°ì²´ ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def call_model(self, name: str, *args, **kwargs) -> Any:
        """ëª¨ë¸ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                success, result, message = self.validator.safe_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"âœ… ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    async def call_model_async(self, name: str, *args, **kwargs) -> Any:
        """ëª¨ë¸ í˜¸ì¶œ - ë¹„ë™ê¸° ë²„ì „"""
        try:
            async with self.async_lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                # ë¹„ë™ê¸° í˜¸ì¶œ ì‹œë„ (async_call ë©”ì„œë“œ ìš°ì„ )
                if hasattr(model, 'async_call'):
                    try:
                        result = await model.async_call(*args, **kwargs)
                        if name in self.call_statistics:
                            self.call_statistics[name]['successes'] += 1
                        self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ (async_call): {name}")
                        return result
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ async_call ì‹¤íŒ¨, safe_call_async ì‹œë„: {e}")
                
                # ì¼ë°˜ ë¹„ë™ê¸° í˜¸ì¶œ
                success, result, message = await self.validator.safe_call_async(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {})
                    }
                return result
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        self.async_manager = AsyncCompatibilityManager()
        
        # ëª¨ë¸ ìºì‹œ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = STEP_MODEL_REQUESTS.get(step_name)
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_schp_atr"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "virtual_fitting_viton_hd"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            async with self._async_lock:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    cached_model = self.loaded_models[model_name]
                    safe_model = self.async_manager.make_callable_safe(cached_model)
                    self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return safe_model
                
                # ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ
                if hasattr(self.model_loader, 'safe_model_service'):
                    service = self.model_loader.safe_model_service
                    
                    model = None
                    try:
                        if hasattr(service, 'call_model_async'):
                            model = await service.call_model_async(model_name)
                    except Exception as async_error:
                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨, ë™ê¸° í˜¸ì¶œ ì‹œë„: {async_error}")
                        try:
                            model = service.call_model(model_name)
                        except Exception as sync_error:
                            self.logger.warning(f"âš ï¸ ë™ê¸° í˜¸ì¶œë„ ì‹¤íŒ¨: {sync_error}")
                            model = None
                    
                    if model:
                        safe_model = self.async_manager.make_callable_safe(model)
                        self.loaded_models[model_name] = safe_model
                        self.model_status[model_name] = "loaded"
                        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return safe_model
                
                # í´ë°± ëª¨ë¸ ìƒì„±
                fallback = await self._create_fallback_model_async(model_name)
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
                self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            fallback = await self._create_fallback_model_async(model_name or "error")
            async with self._async_lock:
                self.loaded_models[model_name or "error"] = fallback
                self.model_status[model_name or "error"] = "error_fallback"
            return fallback
    
    async def _create_fallback_model_async(self, model_name: str) -> Any:
        """ë¹„ë™ê¸° í´ë°± ëª¨ë¸ ìƒì„±"""
        class AsyncSafeFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'async_safe_fallback'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return AsyncSafeFallbackModel(model_name)
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        try:
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    return self._get_model_sync_direct(model_name)
                else:
                    return loop.run_until_complete(self.get_model(model_name))
            except RuntimeError:
                return self._get_model_sync_direct(model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_fallback_model_sync(model_name or "error")
    
    def _get_model_sync_direct(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ì§ì ‘ ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                cached_model = self.loaded_models[model_name]
                return self.async_manager.make_callable_safe(cached_model)
            
            # ModelLoaderë¥¼ í†µí•œ ë™ê¸° ëª¨ë¸ ë¡œë“œ
            if hasattr(self.model_loader, 'safe_model_service'):
                service = self.model_loader.safe_model_service
                model = service.call_model(model_name)
                
                if model:
                    safe_model = self.async_manager.make_callable_safe(model)
                    with self._lock:
                        self.loaded_models[model_name] = safe_model
                        self.model_status[model_name] = "loaded"
                    self.logger.info(f"âœ… ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return safe_model
            
            # í´ë°± ëª¨ë¸ ìƒì„±
            fallback = self._create_fallback_model_sync(model_name)
            with self._lock:
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
            self.logger.warning(f"âš ï¸ ë™ê¸° í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
            return fallback
            
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_fallback_model_sync(model_name or "error")
    
    def _create_fallback_model_sync(self, model_name: str) -> Any:
        """ë™ê¸° í´ë°± ëª¨ë¸ ìƒì„±"""
        class SyncFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'sync_fallback'
                }
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return SyncFallbackModel(model_name)
    
    def list_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            with self._lock:
                # ë“±ë¡ëœ ëª¨ë¸ë“¤
                registered_models = list(self.loaded_models.keys())
                
                # ì¶”ì²œ ëª¨ë¸ë“¤
                recommended = self.recommended_models.copy()
                
                # SafeModelServiceì— ë“±ë¡ëœ ëª¨ë¸ë“¤
                safe_models = list(self.model_loader.safe_model_service.models.keys())
                
                # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë°˜í™˜
                all_models = list(set(registered_models + recommended + safe_models))
                
                self.available_models = all_models
                return all_models
                
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.recommended_models
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ê°„ì†Œí™”ëœ ë²„ì „)
# ==============================================

class ModelLoader:
    """ê°„ì†Œí™”ëœ ModelLoader v13.0 - auto_model_detector ì œê±°"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ê°„ì†Œí™”ëœ ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # í•µì‹¬ ì„œë¹„ìŠ¤ë“¤
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        self.async_manager = AsyncCompatibilityManager()
        
        # ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0 if IS_M3_MAX else 16.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_model_requests: Dict[str, Any] = {}
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {}
        }
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        self.logger.info(f"ğŸ¯ ê°„ì†Œí™”ëœ ModelLoader v13.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, SafeModelService: âœ…, Async: âœ…")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.use_fp16 = True
                self.logger.info("ğŸ M3 Max ìµœì í™” í™œì„±í™”ë¨")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            # ê¸°ë³¸ ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: register_step_requirements (í•„ìˆ˜!)
    # ==============================================
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Dict[str, Any]
    ) -> bool:
        """
        ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ - base_step_mixin.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ
        
        Args:
            step_name: Step ì´ë¦„ (ì˜ˆ: "HumanParsingStep")
            requirements: ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                # ê¸°ì¡´ ìš”ì²­ì‚¬í•­ê³¼ ë³‘í•©
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # ìš”ì²­ì‚¬í•­ ì—…ë°ì´íŠ¸
                self.step_requirements[step_name].update(requirements)
                
                # StepModelConfig ìƒì„±
                for model_name, model_req in requirements.items():
                    try:
                        if isinstance(model_req, dict):
                            step_config = StepModelConfig(
                                step_name=step_name,
                                model_name=model_name,
                                model_class=model_req.get("model_class", "BaseModel"),
                                model_type=model_req.get("model_type", "unknown"),
                                device=model_req.get("device", "auto"),
                                precision=model_req.get("precision", "fp16"),
                                input_size=tuple(model_req.get("input_size", (512, 512))),
                                num_classes=model_req.get("num_classes"),
                                priority=model_req.get("priority", 5),
                                confidence_score=model_req.get("confidence_score", 0.0),
                                registration_time=time.time()
                            )
                            
                            self.model_configs[model_name] = step_config
                            
                            # SafeModelServiceì—ë„ ë“±ë¡
                            model_dict = {
                                'name': model_name,
                                'step_name': step_name,
                                'config': step_config,
                                'type': model_req.get("model_type", "unknown"),
                                'device': self.device,
                                'registered_via': 'register_step_requirements'
                            }
                            self.safe_model_service.register_model(model_name, model_dict)
                            
                            self.logger.debug(f"   âœ… {model_name} ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                            
                    except Exception as model_error:
                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {model_error}")
                        continue
                
                # Step ì¸í„°í˜ì´ìŠ¤ê°€ ìˆë‹¤ë©´ ìš”ì²­ì‚¬í•­ ì „ë‹¬
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    for model_name, model_req in requirements.items():
                        if isinstance(model_req, dict):
                            interface.register_model_requirement(
                                model_name=model_name,
                                **model_req
                            )
                
                self.logger.info(f"âœ… {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {len(requirements)}ê°œ ëª¨ë¸")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def get_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìš”ì²­ì‚¬í•­ ì¡°íšŒ"""
        try:
            with self._lock:
                return self.step_requirements.get(step_name, {})
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ìš”ì²­ì‚¬í•­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model_for_step(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """Stepìš© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                cache_key = f"{step_name}_{model_name or 'default'}"
                if cache_key in self.model_cache:
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"ğŸ“¦ ìºì‹œì—ì„œ ëª¨ë¸ ë°˜í™˜: {cache_key}")
                    return self.model_cache[cache_key]
                
                # SafeModelService ì‚¬ìš©
                if hasattr(self, 'safe_model_service'):
                    model = self.safe_model_service.call_model(model_name or step_name)
                    if model:
                        safe_model = self.async_manager.make_callable_safe(model)
                        self.model_cache[cache_key] = safe_model
                        self.performance_stats['models_loaded'] += 1
                        self.logger.info(f"âœ… {step_name} SafeModelService ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                        return safe_model
                
                # AI ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œ í´ë°±
                if AI_MODELS_AVAILABLE:
                    try:
                        model = create_model_by_step(step_name)
                        safe_model = self.async_manager.make_callable_safe(model)
                        self.model_cache[cache_key] = safe_model
                        self.performance_stats['models_loaded'] += 1
                        self.logger.info(f"âœ… {step_name} AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± ì„±ê³µ")
                        return safe_model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                
                self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_for_step_async(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """Stepìš© ëª¨ë¸ ë¹„ë™ê¸° ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self._async_lock:
                # ë™ê¸° ë²„ì „ í˜¸ì¶œ (ìŠ¤ë ˆë“œí’€ì—ì„œ)
                loop = asyncio.get_event_loop()
                model = await loop.run_in_executor(
                    None, 
                    self.get_model_for_step, 
                    step_name, 
                    model_name
                )
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                # SafeModelServiceì—ë„ ë“±ë¡
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device
                }
                self.safe_model_service.register_model(name, model_dict)
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ ModelLoader v13.0 ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            async with self._async_lock:
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë¹„ë™ê¸°)
                if hasattr(self, 'memory_manager'):
                    try:
                        self.memory_manager.optimize_memory()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info("âœ… ModelLoader v13.0 ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def initialize(self) -> bool:
        """ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ - ìˆœìˆ˜ ë™ê¸° ë²„ì „"""
        try:
            self.logger.info("ğŸš€ ModelLoader v13.0 ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë™ê¸°)
            if hasattr(self, 'memory_manager'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
            self.logger.info("âœ… ModelLoader v13.0 ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def create_step_interface(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements ì²˜ë¦¬
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ {req_name} ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ ëª©ë¡"""
        try:
            with self._lock:
                models_info = {}
                
                # ë“±ë¡ëœ ëª¨ë¸ ì„¤ì •ë“¤
                for model_name in self.model_configs.keys():
                    models_info[model_name] = {
                        'name': model_name,
                        'registered': True,
                        'device': self.device,
                        'config': self.model_configs[model_name]
                    }
                
                # SafeModelService ëª¨ë¸ë“¤
                safe_models = self.safe_model_service.list_models()
                for model_name, status in safe_models.items():
                    if model_name not in models_info:
                        models_info[model_name] = {
                            'name': model_name,
                            'source': 'SafeModelService',
                            'status': status
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        return {
            "device": self.device,
            "is_m3_max": self.is_m3_max,
            "torch_available": TORCH_AVAILABLE,
            "numpy_available": NUMPY_AVAILABLE,
            "ai_models_available": AI_MODELS_AVAILABLE,
            "image_processing_available": IMAGE_PROCESSING_AVAILABLE,
            "performance_stats": self.performance_stats.copy(),
            "model_counts": {
                "loaded": len(self.model_cache),
                "cached": len(self.model_configs)
            },
            "version": "13.0",
            "simplified": True,
            "auto_detector_removed": True,
            "register_step_requirements_available": True
        }
    
    def cleanup(self):
        """ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    try:
                        if step_name in self.step_interfaces:
                            del self.step_interfaces[step_name]
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            try:
                                model.cpu()
                            except:
                                pass
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(self.memory_manager, 'optimize_memory'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ê°„ì†Œí™”ëœ ModelLoader v13.0 ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (í•˜ìœ„ í˜¸í™˜ì„± ì™„ë²½ ìœ ì§€)
    # ==============================================
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """ê¸°ì¡´ ModelLoader.get_model() ë©”ì„œë“œ - ì™„ë²½ í˜¸í™˜"""
        try:
            # Step ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ ì‹œë„
            step_mapping = {
                'human_parsing': 'HumanParsingStep',
                'pose_estimation': 'PoseEstimationStep', 
                'cloth_segmentation': 'ClothSegmentationStep',
                'geometric_matching': 'GeometricMatchingStep',
                'cloth_warping': 'ClothWarpingStep',
                'virtual_fitting': 'VirtualFittingStep',
                'post_processing': 'PostProcessingStep',
                'quality_assessment': 'QualityAssessmentStep'
            }
            
            # 1. ì§ì ‘ Step ì´ë¦„ì¸ ê²½ìš°
            if model_name in STEP_MODEL_REQUESTS:
                return self.get_model_for_step(model_name, None)
            
            # 2. ëª¨ë¸ëª…ìœ¼ë¡œ Step ë§¤í•‘
            for key, step_name in step_mapping.items():
                if key in model_name.lower():
                    return self.get_model_for_step(step_name, model_name)
            
            # 3. ìºì‹œì—ì„œ í™•ì¸
            if model_name in self.model_cache:
                return self.model_cache[model_name]
            
            # 4. SafeModelService í´ë°±
            model = self.safe_model_service.call_model(model_name)
            if model:
                safe_model = self.async_manager.make_callable_safe(model)
                self.model_cache[model_name] = safe_model
                return safe_model
            
            self.logger.warning(f"âš ï¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ì¡´ get_model ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """ê¸°ì¡´ ModelLoader.get_model_async() ë©”ì„œë“œ - ì™„ë²½ í˜¸í™˜"""
        try:
            # ë™ê¸° ë²„ì „ê³¼ ë™ì¼í•œ ë¡œì§, ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            return await asyncio.get_event_loop().run_in_executor(
                None, self.get_model, model_name
            )
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ì¡´ get_model_async ì‹¤íŒ¨ {model_name}: {e}")
            return None

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì „ì—­ ê°„ì†Œí™”ëœ ModelLoader v13.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader async initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        success = loader.initialize()
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return loader
        else:
            logger.error("âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ê°„ì†Œí™”ëœ ModelLoader v13.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def get_model_service() -> SafeModelService:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    loader = get_global_model_loader()
    return loader.safe_model_service

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ë¡œ ì•ˆì „í•˜ê²Œ ë“±ë¡"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock ëª¨ë¸ ìƒì„±"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': DEFAULT_DEVICE,
        'loaded_at': '2025-07-21T12:00:00Z'
    }
    
    loader = get_global_model_loader()
    return loader.async_manager._create_dict_wrapper(mock_dict)

# ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ - ë™ê¸° ë²„ì „"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ - ë¹„ë™ê¸° ë²„ì „"""
    return await SafeFunctionValidator.safe_call_async(obj, *args, **kwargs)

def is_safely_callable(obj: Any) -> bool:
    """ì „ì—­ callable ì•ˆì „ì„± ê²€ì¦"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ë™ê¸° ë²„ì „"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(loader, step_name)

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        loader = get_global_model_loader()
        return loader.get_system_info()
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.get_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return await loader.get_model_async(model_name)

def register_model_config(name: str, config: Dict[str, Any]) -> bool:
    """ì „ì—­ ëª¨ë¸ ì„¤ì • ë“±ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.register_model_config(name, config)

def list_all_models() -> Dict[str, Any]:
    """ì „ì—­ ëª¨ë¸ ëª©ë¡ í•¨ìˆ˜ - ê¸°ì¡´ í˜¸í™˜"""
    loader = get_global_model_loader()
    return loader.list_models()

# base_step_mixin.py í˜¸í™˜ í•¨ìˆ˜ë“¤
def get_model_for_step(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    loader = get_global_model_loader()
    return loader.get_model_for_step(step_name, model_name)

async def get_model_for_step_async(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ë¹„ë™ê¸° ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    loader = get_global_model_loader()
    return await loader.get_model_for_step_async(step_name, model_name)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ì •ì˜
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'SafeModelService',
    'SafeFunctionValidator',
    'AsyncCompatibilityManager',
    'DeviceManager',
    'ModelMemoryManager',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType',
    'ModelConfig',
    'StepModelConfig',
    'StepPriority',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'cleanup_global_loader',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_model_service',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_call_async',
    'is_safely_callable',
    'create_step_interface',
    'get_device_info',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_model',
    'get_model_async',
    'register_model_config',
    'list_all_models',
    'get_model_for_step',
    'get_model_for_step_async',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'STEP_MODEL_REQUESTS',
    'AI_MODELS_AVAILABLE',
    'IMAGE_PROCESSING_AVAILABLE'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ í™•ì¸ ë©”ì‹œì§€
# ==============================================

logger.info("âœ… ê°„ì†Œí™”ëœ ModelLoader v13.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”¥ auto_model_detector ê¸°ëŠ¥ ì™„ì „ ì œê±°")
logger.info("ğŸ“¦ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬")
logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬")
logger.info("â­ register_step_requirements ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info("ğŸ”— base_step_mixin.py íŒ¨í„´ 100% í˜¸í™˜")
logger.info("ğŸ M3 Max 128GB ìµœì í™”")
logger.info("ğŸ”„ ë¹„ë™ê¸°(async/await) ì™„ì „ ì§€ì›")
logger.info("ğŸ›¡ï¸ Coroutine/AttributeError ì™„ì „ í•´ê²°")
logger.info("ğŸ“‹ ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ì§€ì›")
logger.info("ğŸ”„ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
logger.info(f"   - AI Models: {'âœ…' if AI_MODELS_AVAILABLE else 'âŒ'}")
logger.info(f"   - Image Processing: {'âœ…' if IMAGE_PROCESSING_AVAILABLE else 'âŒ'}")
logger.info(f"   - Device: {DEFAULT_DEVICE}")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")

logger.info("ğŸš€ ê°„ì†Œí™”ëœ ModelLoader v13.0 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… ëª¨ë“ˆ ë¶„ë¦¬ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   âœ… ì½”ë“œ í¬ê¸° ëŒ€í­ ê°ì†Œ (11,000 â†’ 3,000 ë¼ì¸)")
logger.info("   âœ… register_step_requirements ë©”ì„œë“œ í¬í•¨")
logger.info("   âœ… base_step_mixin.py ì™„ë²½ ì—°ë™")
logger.info("   âœ… ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ë³´ì¥")
logger.info("   âœ… conda í™˜ê²½ ìµœì í™”")
logger.info("   âœ… M3 Max 128GB ìµœëŒ€ í™œìš©")
logger.info("   âœ… Clean Architecture ì ìš©")