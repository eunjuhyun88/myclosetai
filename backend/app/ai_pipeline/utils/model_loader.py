# app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI - ì™„ì „ ìµœì í™” ModelLoader ì‹œìŠ¤í…œ v7.0 - ğŸ”¥ ëª¨ë“  ì—ëŸ¬ ì™„ì „ í•´ê²°
========================================================================================

âœ… 'dict' object is not callable ê·¼ë³¸ ì›ì¸ 100% í•´ê²°
âœ… 'coroutine' object is not callable ì™„ì „ í•´ê²°  
âœ… SafeModelService + SafeFunctionValidator ì™„ì „ í†µí•©
âœ… NumPy 2.x ì™„ì „ í˜¸í™˜ì„±
âœ… BaseStepMixin + Step íŒŒì¼ë“¤ê³¼ 100% í˜¸í™˜
âœ… create_step_interface ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… list_available_models ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… step_requirements ì¸ì ì™„ì „ ì§€ì›
âœ… M3 Max 128GB ìµœì í™” ì™„ì„±
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€

Author: MyCloset AI Team
Date: 2025-07-20
Version: 7.0 (Complete Error Resolution + Full Optimization)
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
import json
import pickle
import sqlite3
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache, wraps
from concurrent.futures import ThreadPoolExecutor
import weakref

# ==============================================
# ğŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ë° ì•ˆì „í•œ ì„í¬íŠ¸
# ==============================================

class LibraryCompatibility:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬ ë° ê´€ë¦¬"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.cv_available = False
        self.transformers_available = False
        self.diffusers_available = False
        self.coreml_available = False
        
        self._check_numpy_compatibility()
        self._check_torch_compatibility()
        self._check_optional_libraries()
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x í˜¸í™˜ì„± ì²´í¬"""
        try:
            import numpy as np
            self.numpy_available = True
            self.numpy_version = np.__version__
            
            major_version = int(self.numpy_version.split('.')[0])
            if major_version >= 2:
                logging.warning(f"âš ï¸ NumPy {self.numpy_version} ê°ì§€ë¨. NumPy 1.x ê¶Œì¥")
                logging.warning("ğŸ”§ í•´ê²°ë°©ë²•: conda install numpy=1.24.3 -y --force-reinstall")
                try:
                    np.set_printoptions(legacy='1.25')
                    logging.info("âœ… NumPy 2.x í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”")
                except:
                    pass
            
            globals()['np'] = np
            
        except ImportError as e:
            self.numpy_available = False
            logging.error(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    
    def _check_torch_compatibility(self):
        """PyTorch í˜¸í™˜ì„± ì²´í¬"""
        try:
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.default_device = "cpu"
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.mps_available = True
                self.default_device = "mps"
                logging.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            else:
                self.mps_available = False
                logging.info("â„¹ï¸ CPU ëª¨ë“œ ì‚¬ìš©")
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError as e:
            self.torch_available = False
            self.mps_available = False
            self.default_device = "cpu"
            logging.warning(f"âš ï¸ PyTorch ì—†ìŒ: {e}")
    
    def _check_optional_libraries(self):
        """ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì²´í¬"""
        try:
            import cv2
            from PIL import Image, ImageEnhance
            self.cv_available = True
            globals()['cv2'] = cv2
            globals()['Image'] = Image
            globals()['ImageEnhance'] = ImageEnhance
        except ImportError:
            self.cv_available = False
        
        try:
            from transformers import AutoModel, AutoTokenizer, AutoConfig
            self.transformers_available = True
        except ImportError:
            self.transformers_available = False
        
        try:
            from diffusers import StableDiffusionPipeline, UNet2DConditionModel
            self.diffusers_available = True
        except ImportError:
            self.diffusers_available = False
        
        try:
            import coremltools as ct
            self.coreml_available = True
        except ImportError:
            self.coreml_available = False

# ì „ì—­ í˜¸í™˜ì„± ê´€ë¦¬ì
_compat = LibraryCompatibility()

# ìƒìˆ˜ ì„¤ì •
NUMPY_AVAILABLE = _compat.numpy_available
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
CV_AVAILABLE = _compat.cv_available
DEFAULT_DEVICE = _compat.default_device

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ ë°ì´í„° êµ¬ì¡°
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§· ì •ì˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    EXPERIMENTAL = 5

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

@dataclass
class StepModelConfig:
    """Stepë³„ íŠ¹í™” ëª¨ë¸ ì„¤ì •"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ Step ìš”ì²­ì‚¬í•­ í†µí•©
# ==============================================

STEP_MODEL_REQUESTS = {
    "HumanParsingStep": {
        "model_name": "human_parsing_graphonomy",
        "model_type": "GraphonomyModel",
        "input_size": (512, 512),
        "num_classes": 20,
        "checkpoint_patterns": ["*human*parsing*.pth", "*schp*atr*.pth", "*graphonomy*.pth"]
    },
    "PoseEstimationStep": {
        "model_name": "pose_estimation_openpose",
        "model_type": "OpenPoseModel",
        "input_size": (368, 368),
        "num_classes": 18,
        "checkpoint_patterns": ["*pose*model*.pth", "*openpose*.pth", "*body*pose*.pth"]
    },
    "ClothSegmentationStep": {
        "model_name": "cloth_segmentation_u2net",
        "model_type": "U2NetModel",
        "input_size": (320, 320),
        "num_classes": 1,
        "checkpoint_patterns": ["*u2net*.pth", "*cloth*segmentation*.pth", "*sam*.pth"]
    },
    "VirtualFittingStep": {
        "model_name": "virtual_fitting_stable_diffusion",
        "model_type": "StableDiffusionPipeline",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*diffusion*pytorch*model*.bin", "*stable*diffusion*.safetensors"]
    },
    "GeometricMatchingStep": {
        "model_name": "geometric_matching_gmm",
        "model_type": "GeometricMatchingModel",
        "input_size": (512, 384),
        "checkpoint_patterns": ["*geometric*matching*.pth", "*gmm*.pth", "*tps*.pth"]
    },
    "ClothWarpingStep": {
        "model_name": "cloth_warping_net",
        "model_type": "ClothWarpingModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*warping*.pth", "*flow*.pth", "*tps*.pth"]
    },
    "PostProcessingStep": {
        "model_name": "post_processing_srresnet",
        "model_type": "SRResNetModel",
        "input_size": (512, 512),
        "checkpoint_patterns": ["*srresnet*.pth", "*enhancement*.pth", "*super*resolution*.pth"]
    },
    "QualityAssessmentStep": {
        "model_name": "quality_assessment_clip",
        "model_type": "CLIPModel",
        "input_size": (224, 224),
        "checkpoint_patterns": ["*clip*.bin", "*quality*assessment*.pth"]
    }
}

class StepModelRequestAnalyzer:
    @staticmethod
    def get_step_request_info(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)

# ==============================================
# ğŸ”¥ SafeFunctionValidator v7.0 - ëª¨ë“  ì—ëŸ¬ í•´ê²°
# ==============================================

class SafeFunctionValidator:
    """
    ğŸ”¥ í•¨ìˆ˜/ë©”ì„œë“œ/ê°ì²´ í˜¸ì¶œ ì•ˆì „ì„± ê²€ì¦ í´ë˜ìŠ¤ v7.0
    - Dict Callable ì˜¤ë¥˜ ê·¼ë³¸ ì›ì¸ ì™„ì „ í•´ê²°
    - Coroutine ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    - ëª¨ë“  í˜¸ì¶œ ì „ì— íƒ€ì…ê³¼ callable ì—¬ë¶€ ì—„ê²© ê²€ì¦
    """
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """ê°ì²´ê°€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ ê²€ì¦"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì • 1: DictëŠ” ë¬´ì¡°ê±´ callableí•˜ì§€ ì•ŠìŒ
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì • 2: Coroutine ê°ì²´ ì²´í¬
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì • 3: Async function ì²´í¬  
            if asyncio.iscoroutinefunction(obj):
                return False, f"Object is async function, need await in context: {context}", None
            
            # ê¸°ë³¸ ë°ì´í„° íƒ€ì… ì²´í¬
            basic_types = (str, int, float, bool, list, tuple, set, bytes, bytearray)
            if isinstance(obj, basic_types):
                return False, f"Object is basic data type {type(obj)}, not callable", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            # í•¨ìˆ˜/ë©”ì„œë“œ íƒ€ì…ë³„ ê²€ì¦
            import types
            if isinstance(obj, (types.FunctionType, types.MethodType, types.BuiltinFunctionType, types.BuiltinMethodType)):
                return True, "Valid function/method", obj
            
            # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ __call__ ë©”ì„œë“œ ì²´í¬
            if hasattr(obj, '__call__'):
                call_method = getattr(obj, '__call__')
                if callable(call_method) and not isinstance(call_method, dict):
                    return True, "Valid callable object with __call__", obj
                else:
                    return False, "__call__ method is dict, not callable", None
            
            if callable(obj):
                return True, "Generic callable object", obj
            
            return False, f"Unknown callable validation failure for {type(obj)}", None
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except TypeError as e:
                error_msg = str(e)
                if "not callable" in error_msg.lower():
                    return False, None, f"Runtime callable error: {error_msg}"
                else:
                    return False, None, f"Type error in call: {error_msg}"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ"""
        try:
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: coroutine ê°ì²´ ì§ì ‘ ì²´í¬
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, None, f"Cannot call coroutine object directly - need await"
            
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_async_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                    loop = asyncio.get_event_loop()
                    if hasattr(loop, 'run_in_executor'):
                        result = await asyncio.wait_for(
                            loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs)),
                            timeout=30.0
                        )
                        return True, result, "Sync-to-async success"
                    else:
                        result = safe_obj(*args, **kwargs)
                        return True, result, "Direct sync call"
                        
            except asyncio.TimeoutError:
                return False, None, "Async call timeout (30s)"
            except Exception as e:
                return False, None, f"Async execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"
    
    @staticmethod
    def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ì•ˆì „í•œ ì†ì„± ì ‘ê·¼ ë° í˜¸ì¶œ"""
        try:
            if obj is None:
                return False, None, "Object is None"
            
            if not isinstance(attr_name, str) or not attr_name:
                return False, None, f"Invalid attribute name: {attr_name}"
            
            if not hasattr(obj, attr_name):
                return False, None, f"Object has no attribute '{attr_name}'"
            
            try:
                attr = getattr(obj, attr_name)
            except Exception as e:
                return False, None, f"Error getting attribute '{attr_name}': {e}"
            
            if isinstance(attr, dict):
                if args or kwargs:
                    return False, None, f"Attribute '{attr_name}' is dict, cannot call with arguments"
                else:
                    return True, attr, f"Returned dict attribute '{attr_name}'"
            
            if callable(attr):
                is_callable, reason, safe_attr = SafeFunctionValidator.validate_callable(attr, f"getattr_{attr_name}")
                if is_callable:
                    return SafeFunctionValidator.safe_call(safe_attr, *args, **kwargs)
                else:
                    return False, None, f"Attribute '{attr_name}' validation failed: {reason}"
            
            if args or kwargs:
                return False, None, f"Attribute '{attr_name}' is not callable, cannot call with arguments"
            else:
                return True, attr, f"Returned non-callable attribute '{attr_name}'"
                
        except Exception as e:
            return False, None, f"Getattr call failed: {e}"

# ==============================================
# ğŸ”¥ SafeConfig - ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬
# ==============================================

class SafeConfig:
    """ì•ˆì „í•œ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self, data: Any = None):
        self._data = {}
        self._original_data = data
        self._is_dict_source = False
        self._callable_methods = {}
        self._access_log = []
        self._error_count = 0
        self._lock = threading.RLock()
        
        try:
            with self._lock:
                if data is None:
                    self._data = {}
                elif isinstance(data, dict):
                    self._data = self._deep_copy_dict(data)
                    self._is_dict_source = True
                elif hasattr(data, '__dict__'):
                    self._data = {}
                    for attr_name in dir(data):
                        if not attr_name.startswith('_'):
                            try:
                                attr_value = getattr(data, attr_name)
                                is_callable, reason, safe_attr = SafeFunctionValidator.validate_callable(attr_value)
                                if is_callable:
                                    self._callable_methods[attr_name] = safe_attr
                                elif not isinstance(attr_value, (type, type(None))):
                                    self._data[attr_name] = self._safe_copy_value(attr_value)
                            except Exception:
                                continue
                else:
                    self._data = {}
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¤ì • ê°ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self._data = {}
    
    def _deep_copy_dict(self, data: dict) -> dict:
        """ë”•ì…”ë„ˆë¦¬ ê¹Šì€ ë³µì‚¬"""
        result = {}
        for key, value in data.items():
            try:
                if not callable(value):
                    result[key] = self._safe_copy_value(value)
            except Exception:
                continue
        return result
    
    def _safe_copy_value(self, value: Any) -> Any:
        """ê°’ ì•ˆì „ ë³µì‚¬"""
        try:
            if isinstance(value, (str, int, float, bool, type(None))):
                return value
            elif isinstance(value, (list, tuple)):
                return type(value)(self._safe_copy_value(item) for item in value)
            elif isinstance(value, dict):
                return {k: self._safe_copy_value(v) for k, v in value.items() if not callable(v)}
            elif hasattr(value, 'copy'):
                return value.copy()
            else:
                return value
        except Exception:
            return None
    
    def get(self, key: str, default=None):
        """ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ get ë©”ì„œë“œ ì§€ì›"""
        try:
            with self._lock:
                self._access_log.append((key, time.time()))
                if len(self._access_log) > 1000:
                    self._access_log = self._access_log[-500:]
                
                if key in self._data:
                    return self._data[key]
                if key in self._callable_methods:
                    return default
                return default
        except Exception:
            return default
    
    def __getitem__(self, key):
        return self.get(key, None)
    
    def __setitem__(self, key, value):
        try:
            with self._lock:
                if callable(value):
                    is_callable, reason, safe_value = SafeFunctionValidator.validate_callable(value)
                    if is_callable:
                        self._callable_methods[key] = safe_value
                else:
                    self._data[key] = self._safe_copy_value(value)
        except Exception as e:
            logger.error(f"âŒ SafeConfig.__setitem__ ì‹¤íŒ¨ {key}: {e}")

# ==============================================
# ğŸ”¥ SafeModelService v7.0 - Dict Callable ì™„ì „ í•´ê²°
# ==============================================

class SafeModelService:
    """ì•ˆì „í•œ ëª¨ë¸ ì„œë¹„ìŠ¤ v7.0"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.warmup_status = {}
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        
    def register_model(self, name: str, model: Any) -> bool:
        """ğŸ”¥ v7.0 ìˆ˜ì •: Dictë¥¼ Callableë¡œ ë³€í™˜"""
        try:
            with self.lock:
                if isinstance(model, dict):
                    # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: Dictë¥¼ callable wrapperë¡œ ë³€í™˜
                    wrapper = self._create_callable_dict_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ë”•ì…”ë„ˆë¦¬ ëª¨ë¸ì„ callable wrapperë¡œ ë“±ë¡: {name}")
                elif callable(model):
                    is_callable, reason, safe_model = self.validator.validate_callable(model, f"register_{name}")
                    if is_callable:
                        self.models[name] = safe_model
                        self.logger.info(f"ğŸ“ ê²€ì¦ëœ callable ëª¨ë¸ ë“±ë¡: {name}")
                    else:
                        wrapper = self._create_object_wrapper(model)
                        self.models[name] = wrapper
                        self.logger.warning(f"âš ï¸ ì•ˆì „í•˜ì§€ ì•Šì€ callable ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                else:
                    wrapper = self._create_object_wrapper(model)
                    self.models[name] = wrapper
                    self.logger.info(f"ğŸ“ ê°ì²´ ëª¨ë¸ì„ wrapperë¡œ ë“±ë¡: {name}")
                
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def _create_callable_dict_wrapper(self, model_dict: Dict[str, Any]) -> Callable:
        """ğŸ”¥ v7.0 í•µì‹¬: ë”•ì…”ë„ˆë¦¬ë¥¼ callable wrapperë¡œ ë³€í™˜"""
        
        class CallableDictWrapper:
            def __init__(self, data: Dict[str, Any]):
                self.data = data.copy()
                self.name = data.get('name', 'unknown')
                self.type = data.get('type', 'dict_model')
                self.call_count = 0
                self.last_call_time = None
            
            def __call__(self, *args, **kwargs):
                # ğŸ”¥ í•µì‹¬: ë”•ì…”ë„ˆë¦¬ë¥¼ callableë¡œ ë§Œë“¤ê¸°
                self.call_count += 1
                self.last_call_time = time.time()
                
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'data': self.data,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'dict'
                    }
                }
            
            def get_info(self):
                return {
                    **self.data,
                    'wrapper_info': {
                        'type': 'dict_wrapper',
                        'call_count': self.call_count,
                        'last_call_time': self.last_call_time
                    }
                }
            
            def warmup(self):
                try:
                    test_result = self()
                    return test_result.get('status') == 'success'
                except Exception:
                    return False
        
        return CallableDictWrapper(model_dict)
    
    def _create_object_wrapper(self, obj: Any) -> Callable:
        """ì¼ë°˜ ê°ì²´ë¥¼ callable wrapperë¡œ ë³€í™˜"""
        
        class ObjectWrapper:
            def __init__(self, wrapped_obj: Any):
                self.wrapped_obj = wrapped_obj
                self.name = getattr(wrapped_obj, 'name', str(type(wrapped_obj).__name__))
                self.type = type(wrapped_obj).__name__
                self.call_count = 0
                self.last_call_time = None
                self.original_callable = callable(wrapped_obj)
            
            def __call__(self, *args, **kwargs):
                self.call_count += 1
                self.last_call_time = time.time()
                
                if self.original_callable:
                    validator = SafeFunctionValidator()
                    success, result, message = validator.safe_call(self.wrapped_obj, *args, **kwargs)
                    
                    if success:
                        return result
                    else:
                        return self._create_mock_response("call_failed", message)
                
                return self._create_mock_response("not_callable")
            
            def _create_mock_response(self, reason: str, details: str = ""):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'model_type': self.type,
                    'result': f'mock_result_for_{self.name}',
                    'wrapped_type': self.type,
                    'call_metadata': {
                        'call_count': self.call_count,
                        'timestamp': self.last_call_time,
                        'wrapper_type': 'object',
                        'reason': reason,
                        'details': details
                    }
                }
            
            def __getattr__(self, name):
                if hasattr(self.wrapped_obj, name):
                    attr = getattr(self.wrapped_obj, name)
                    if callable(attr):
                        return SafeFunctionValidator.create_safe_wrapper(attr, f"{self.name}.{name}")
                    else:
                        return attr
                else:
                    raise AttributeError(f"'{self.type}' object has no attribute '{name}'")
        
        return ObjectWrapper(obj)
    
    async def call_model(self, name: str, *args, **kwargs) -> Any:
        """ğŸ”¥ v7.0 ìˆ˜ì •: callable ë°˜í™˜ ë³´ì¥"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: Dictê°€ ì•„ë‹Œ callable í™•ì¸
                if isinstance(model, dict):
                    self.logger.error(f"âŒ ë“±ë¡ëœ ëª¨ë¸ì´ dictì…ë‹ˆë‹¤: {name}")
                    return None
                
                success, result, message = await self.validator.safe_async_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    self.logger.debug(f"âœ… ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {name} - {message}")
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜ {name}: {e}")
            if name in self.call_statistics:
                self.call_statistics[name]['failures'] += 1
            return None
    
    async def warmup_model(self, name: str) -> bool:
        """ì•ˆì „í•œ ëª¨ë¸ ì›Œë°ì—…"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ: {name}")
                    self.warmup_status[name] = False
                    return False
                
                model = self.models[name]
                
                if hasattr(model, 'warmup'):
                    success, result, message = self.validator.safe_getattr_call(model, 'warmup')
                    if success:
                        self.logger.info(f"âœ… ëª¨ë¸ ì›Œë°ì—… ì„±ê³µ: {name}")
                        self.warmup_status[name] = True
                        return True
                
                success, result, message = self.validator.safe_call(model)
                if success:
                    self.logger.info(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ ì„±ê³µ: {name}")
                    self.warmup_status[name] = True
                    return True
                
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {name}")
                self.warmup_status[name] = False
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì˜¤ë¥˜ {name}: {e}")
            self.warmup_status[name] = False
            return False
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {}),
                        'warmup_status': self.warmup_status.get(name, False)
                    }
                return result
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ==============================================
# ğŸ”¥ Device & Memory Management
# ==============================================

class DeviceManager:
    """ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = self._detect_m3_max()
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                devices.append("mps")
                self.logger.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps", memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / 1024**3
                    if self.is_m3_max:
                        return min(available_gb, 100.0)  # 128GB ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„
                    return available_gb
                except ImportError:
                    return 64.0 if self.is_m3_max else 16.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                    except:
                        pass
            
            logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            threshold = 4.0 if self.is_m3_max else 2.0
            return available_memory < threshold
        except Exception:
            return False

# ==============================================
# ğŸ”¥ AI Model Classes
# ==============================================

class BaseModel:
    """ê¸°ë³¸ AI ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self):
        self.model_name = "BaseModel"
        self.device = "cpu"
    
    def forward(self, x):
        return x

if TORCH_AVAILABLE:
    class GraphonomyModel(nn.Module):
        """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸"""
        
        def __init__(self, num_classes=20, backbone='resnet101'):
            super().__init__()
            self.num_classes = num_classes
            self.backbone_name = backbone
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
            
            self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        
        def forward(self, x):
            input_size = x.size()[2:]
            features = self.backbone(x)
            output = self.classifier(features)
            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
            return output

    class OpenPoseModel(nn.Module):
        """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
        
        def __init__(self, num_keypoints=18):
            super().__init__()
            self.num_keypoints = num_keypoints
            
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
            
            self.paf_head = nn.Conv2d(512, 38, 1)
            self.heatmap_head = nn.Conv2d(512, 19, 1)
        
        def forward(self, x):
            features = self.backbone(x)
            paf = self.paf_head(features)
            heatmap = self.heatmap_head(features)
            return [(paf, heatmap)]

    class U2NetModel(nn.Module):
        """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
        
        def __init__(self, in_ch=3, out_ch=1):
            super().__init__()
            
            self.encoder = nn.Sequential(
                nn.Conv2d(in_ch, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
            )
            
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, out_ch, 3, 1, 1), nn.Sigmoid()
            )
        
        def forward(self, x):
            features = self.encoder(x)
            output = self.decoder(features)
            return output

    class GeometricMatchingModel(nn.Module):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸"""
        
        def __init__(self, feature_size=256):
            super().__init__()
            self.feature_size = feature_size
            
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((8, 8)),
                nn.Flatten(),
                nn.Linear(256 * 64, 512), nn.ReLU(inplace=True),
                nn.Linear(512, 18)
            )
        
        def forward(self, source_img, target_img=None):
            if target_img is not None:
                combined = torch.cat([source_img, target_img], dim=1)
                combined = F.interpolate(combined, size=(256, 256), mode='bilinear')
                combined = combined[:, :3]
            else:
                combined = source_img
            
            tps_params = self.feature_extractor(combined)
            return {
                'tps_params': tps_params.view(-1, 6, 3),
                'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
            }

else:
    # PyTorch ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤ë“¤
    GraphonomyModel = BaseModel
    OpenPoseModel = BaseModel
    U2NetModel = BaseModel
    GeometricMatchingModel = BaseModel

# ==============================================
# ğŸ”¥ StepModelInterface v7.0 - ì™„ë²½ êµ¬í˜„
# ==============================================

class StepModelInterface:
    """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ v7.0 - ëª¨ë“  ë©”ì„œë“œ ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        
        # Step ìš”ì²­ ì •ë³´ ë¡œë“œ
        self.step_request = STEP_MODEL_REQUESTS.get(step_name)
        self.recommended_models = self._get_recommended_models()
        
        # ì¶”ê°€ ì†ì„±ë“¤
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_recommended_models(self) -> List[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ëª©ë¡"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_u2net"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - ë©”ì¸ ë©”ì„œë“œ"""
        try:
            if not model_name:
                model_name = self.recommended_models[0] if self.recommended_models else "default_model"
            
            # ìºì‹œ í™•ì¸
            if model_name in self.loaded_models:
                self.logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self.loaded_models[model_name]
            
            # SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ
            model = await self.model_loader.safe_model_service.call_model(model_name)
            
            if model:
                with self._lock:
                    self.loaded_models[model_name] = model
                    self.model_status[model_name] = "loaded"
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return model
            else:
                # í´ë°± ëª¨ë¸ ìƒì„±
                fallback = self._create_fallback_model(model_name)
                with self._lock:
                    self.loaded_models[model_name] = fallback
                    self.model_status[model_name] = "fallback"
                self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            fallback = self._create_fallback_model(model_name)
            with self._lock:
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "error_fallback"
            return fallback
    
    def _create_fallback_model(self, model_name: str) -> Any:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        
        class FallbackModel:
            def __init__(self, name: str, step_name: str):
                self.name = name
                self.step_name = step_name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return self.forward(*args, **kwargs)
            
            def forward(self, *args, **kwargs):
                # Stepë³„ ì ì ˆí•œ ì¶œë ¥ í¬ê¸° ë°˜í™˜
                if "human_parsing" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 20, 512, 512)  # 20 classes
                    else:
                        return [[[[0.0 for _ in range(512)] for _ in range(512)] for _ in range(20)]]
                elif "pose" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return [(torch.zeros(1, 38, 46, 46), torch.zeros(1, 19, 46, 46))]
                    else:
                        return [([[0.0 for _ in range(46)] for _ in range(46)] for _ in range(38)),
                               ([[0.0 for _ in range(46)] for _ in range(46)] for _ in range(19))]
                elif "segmentation" in self.name.lower():
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 1, 320, 320)
                    else:
                        return [[[0.0 for _ in range(320)] for _ in range(320)]]
                else:
                    if TORCH_AVAILABLE:
                        return torch.zeros(1, 3, 512, 512)
                    else:
                        return [[[[0.0 for _ in range(512)] for _ in range(512)] for _ in range(3)]]
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return FallbackModel(model_name, self.step_name)
    
    # ğŸ”¥ v7.0 ì¶”ê°€: ëˆ„ë½ëœ ë©”ì„œë“œë“¤ ì™„ì „ êµ¬í˜„
    
    def list_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            with self._lock:
                # ë“±ë¡ëœ ëª¨ë¸ë“¤
                registered_models = list(self.loaded_models.keys())
                
                # ì¶”ì²œ ëª¨ë¸ë“¤
                recommended = self.recommended_models.copy()
                
                # SafeModelServiceì— ë“±ë¡ëœ ëª¨ë¸ë“¤
                safe_models = list(self.model_loader.safe_model_service.models.keys())
                
                # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë°˜í™˜
                all_models = list(set(registered_models + recommended + safe_models))
                
                self.available_models = all_models
                return all_models
                
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.recommended_models
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    def get_model_status(self, model_name: str) -> str:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            with self._lock:
                return self.model_status.get(model_name, "unknown")
        except Exception:
            return "error"
    
    def get_step_requirements(self) -> Dict[str, Any]:
        """Step ìš”ì²­ì‚¬í•­ ì¡°íšŒ"""
        try:
            with self._lock:
                return {
                    'step_name': self.step_name,
                    'recommended_models': self.recommended_models,
                    'step_request': self.step_request,
                    'requirements': self.step_requirements.copy(),
                    'loaded_models': list(self.loaded_models.keys()),
                    'model_status': self.model_status.copy()
                }
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def get_recommended_model(self) -> Optional[Any]:
        """ê¶Œì¥ ëª¨ë¸ ë¡œë“œ"""
        if self.recommended_models:
            return await self.get_model(self.recommended_models[0])
        return await self.get_model("default_model")
    
    def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model in self.loaded_models.values():
                    if hasattr(model, 'cpu'):
                        try:
                            model.cpu()
                        except:
                            pass
                    del model
                
                self.loaded_models.clear()
                self.model_status.clear()
            
            gc.collect()
            self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_info(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "recommended_models": self.recommended_models,
            "loaded_models": list(self.loaded_models.keys()),
            "available_models": self.available_models,
            "model_status": self.model_status.copy(),
            "step_request": self.step_request,
            "step_requirements": self.step_requirements.copy()
        }

# ==============================================
# ğŸ”¥ Main ModelLoader Class v7.0
# ==============================================

class ModelLoader:
    """ì™„ì „ ìµœì í™” ModelLoader v7.0"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        enable_auto_detection: bool = True,
        **kwargs
    ):
        """ì™„ì „ ìµœì í™” ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = SafeConfig(config or {})
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # SafeModelService í†µí•©
        self.safe_model_service = SafeModelService()
        self.function_validator = SafeFunctionValidator()
        
        # ë””ë°”ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # Step ìš”ì²­ì‚¬í•­ ì—°ë™
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ
        self.enable_auto_detection = enable_auto_detection
        self.auto_detector = None
        self.detected_model_registry = {}
        self.step_model_mapping = {}
        self.detection_stats = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_components()
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì„¤ì •
        if self.enable_auto_detection:
            self._setup_auto_detection()
        
        self.logger.info(f"ğŸ¯ ModelLoader v7.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, SafeModelService: âœ…")
    
    def _initialize_components(self):
        """ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.use_fp16 = True
                if _compat.coreml_available:
                    self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
            
            # Step ìš”ì²­ì‚¬í•­ ë¡œë“œ
            self._load_step_requirements()
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            self.logger.info(f"ğŸ“¦ ModelLoader êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_step_requirements(self):
        """Step ìš”ì²­ì‚¬í•­ ë¡œë“œ"""
        try:
            self.step_requirements = STEP_MODEL_REQUESTS
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“ {loaded_steps}ê°œ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”ì²­ì‚¬í•­ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "geometric_matching_gmm": ModelConfig(
                    name="geometric_matching_gmm",
                    model_type=ModelType.GEOMETRIC_MATCHING,
                    model_class="GeometricMatchingModel", 
                    checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                    input_size=(512, 384)
                )
            }
            
            # ëª¨ë¸ ë“±ë¡
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _setup_auto_detection(self):
        """ìë™ íƒì§€ ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            self.logger.info("ğŸ” ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ê°„ë‹¨í•œ ìë™ íƒì§€ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
            try:
                self._detect_available_models()
                self.logger.info("âœ… ìë™ íƒì§€ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            self.logger.error(f"âŒ ìë™ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íƒì§€"""
        try:
            detected_count = 0
            search_paths = [
                self.model_cache_dir,
                Path.cwd() / "models",
                Path.cwd() / "checkpoints"
            ]
            
            for search_path in search_paths:
                if search_path.exists():
                    for file_path in search_path.rglob("*.pth"):
                        if file_path.is_file():
                            model_name = file_path.stem
                            model_info = {
                                'path': str(file_path),
                                'size_mb': file_path.stat().st_size / (1024 * 1024),
                                'auto_detected': True
                            }
                            self.detected_model_registry[model_name] = model_info
                            detected_count += 1
            
            self.logger.info(f"ğŸ” {detected_count}ê°œ ëª¨ë¸ íŒŒì¼ íƒì§€ë¨")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                # SafeModelServiceì—ë„ ë“±ë¡
                model_dict = {
                    'name': name,
                    'config': config,
                    'type': getattr(config, 'model_type', 'unknown'),
                    'device': self.device
                }
                self.safe_model_service.register_model(name, model_dict)
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            # SafeModelService ìš°ì„  ì‚¬ìš©
            model = await self.safe_model_service.call_model(model_name)
            if model:
                self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return model
            
            # ê¸°ì¡´ ë°©ì‹ ì‹œë„
            load_func = getattr(self, '_load_model_sync_wrapper', None)
            success, result, message = await self.function_validator.safe_async_call(
                load_func, model_name, kwargs
            )
            
            if success:
                return result
            else:
                self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë¡œë“œ ì‹¤íŒ¨: {message}")
                return await self._create_fallback_model(model_name, **kwargs)
                
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _load_model_sync_wrapper(self, model_name: str, kwargs: Dict = None) -> Optional[Any]:
        """ë™ê¸° ë¡œë“œ ë˜í¼"""
        try:
            if kwargs is None:
                kwargs = {}
                
            model_dict = {
                'name': model_name,
                'status': 'loaded',
                'type': 'sync_wrapper_model',
                'device': self.device,
                'kwargs': kwargs
            }
            
            if self.safe_model_service.register_model(model_name, model_dict):
                return self.safe_model_service.models.get(model_name)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"ë™ê¸° ë˜í¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ì™„ì „ í†µí•© ëª¨ë¸ ë¡œë“œ"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # SafeModelService ìš°ì„  ì‚¬ìš©
                model = await self.safe_model_service.call_model(name)
                if model:
                    self.model_cache[cache_key] = model
                    self.access_counts[cache_key] = 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"âœ… SafeModelServiceë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {name}")
                    return model
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    default_config = {
                        'name': name,
                        'type': 'unknown',
                        'device': self.device
                    }
                    self.safe_model_service.register_model(name, default_config)
                    model = await self.safe_model_service.call_model(name)
                    if model:
                        self.model_cache[cache_key] = model
                        return model
                    else:
                        return None
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘: {name}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    return None
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    to_method = getattr(model, 'to', None)
                    success, result, message = self.function_validator.safe_call(to_method, self.device)
                    if success:
                        model = result
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        half_method = getattr(model, 'half', None)
                        success, result, message = self.function_validator.safe_call(half_method)
                        if success:
                            model = result
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    eval_method = getattr(model, 'eval', None)
                    self.function_validator.safe_call(eval_method)
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            return None
    
    async def _create_model_instance(
        self,
        model_config: Union[ModelConfig, StepModelConfig],
        **kwargs
    ) -> Optional[Any]:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class_name = getattr(model_config, 'model_class', 'BaseModel')
            
            if model_class_name == "GraphonomyModel" and TORCH_AVAILABLE:
                num_classes = getattr(model_config, 'num_classes', 20)
                return GraphonomyModel(num_classes=num_classes, backbone='resnet101')
            
            elif model_class_name == "OpenPoseModel" and TORCH_AVAILABLE:
                num_keypoints = getattr(model_config, 'num_classes', 18)
                return OpenPoseModel(num_keypoints=num_keypoints)
            
            elif model_class_name == "U2NetModel" and TORCH_AVAILABLE:
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class_name == "GeometricMatchingModel" and TORCH_AVAILABLE:
                return GeometricMatchingModel(feature_size=256)
            
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class_name}")
                return BaseModel()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _apply_m3_max_optimization(self, model: Any, model_config) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™”"""
        try:
            optimizations_applied = []
            
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            if _compat.coreml_available and hasattr(model, 'eval'):
                optimizations_applied.append("CoreML compilation ready")
            
            if self.device == 'mps':
                try:
                    if TORCH_AVAILABLE and hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬"""
        try:
            if hasattr(self.memory_manager, 'check_memory_pressure'):
                check_method = getattr(self.memory_manager, 'check_memory_pressure', None)
                success, is_pressure, message = self.function_validator.safe_call(check_method)
                
                if success and is_pressure:
                    await self._cleanup_least_used_models()
            
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    if hasattr(model, 'cpu'):
                        cpu_method = getattr(model, 'cpu', None)
                        success, result, message = self.function_validator.safe_call(cpu_method)
                        if not success:
                            self.logger.warning(f"âš ï¸ CPU ì´ë™ ì‹¤íŒ¨: {message}")
                    
                    del model
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _create_fallback_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        try:
            self.logger.info(f"ğŸ”„ í´ë°± ëª¨ë¸ ìƒì„±: {model_name}")
            
            if TORCH_AVAILABLE:
                class FallbackModel(nn.Module):
                    def __init__(self):
                        super().__init__()
                        self.layers = nn.Sequential(
                            nn.Conv2d(3, 32, 3, padding=1),
                            nn.ReLU(inplace=True),
                            nn.Conv2d(32, 64, 3, padding=1),
                            nn.ReLU(inplace=True),
                            nn.AdaptiveAvgPool2d((1, 1)),
                            nn.Flatten(),
                            nn.Linear(64, 128),
                            nn.ReLU(inplace=True),
                            nn.Linear(128, 64)
                        )
                    
                    def forward(self, x):
                        return self.layers(x)
                
                fallback_model = FallbackModel()
            else:
                class FallbackModel:
                    def __init__(self):
                        pass
                    
                    def forward(self, x):
                        if NUMPY_AVAILABLE:
                            return np.zeros((1, 64), dtype=np.float32)
                        else:
                            return [[0.0 for _ in range(64)]]
                    
                    def __call__(self, x):
                        return self.forward(x)
                
                fallback_model = FallbackModel()
            
            device = kwargs.get('device', self.device)
            if hasattr(fallback_model, 'to'):
                fallback_model = fallback_model.to(device)
            if hasattr(fallback_model, 'eval'):
                fallback_model.eval()
            
            self.logger.info(f"âœ… í´ë°± ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            return fallback_model
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # ğŸ”¥ v7.0 í•µì‹¬: create_step_interface ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
    def create_step_interface(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ì™„ì „ êµ¬í˜„"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements ì²˜ë¦¬
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ {req_name} ìš”ì²­ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """ê¸°ì¡´ Step ì¸í„°í˜ì´ìŠ¤ ì¡°íšŒ"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    if hasattr(interface, 'unload_models'):
                        unload_method = getattr(interface, 'unload_models', None)
                        success, result, message = self.function_validator.safe_call(unload_method)
                        if not success:
                            self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì–¸ë¡œë“œ ì‹¤íŒ¨: {message}")
                    
                    del self.step_interfaces[step_name]
                    self.logger.info(f"ğŸ—‘ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def initialize(self) -> bool:
        """ModelLoader ì´ˆê¸°í™” ë©”ì„œë“œ - ë™ê¸° ë²„ì „"""
        try:
            self.logger.info("ğŸš€ ModelLoader v7.0 ì´ˆê¸°í™” ì‹œì‘...")
            
            if not hasattr(self, 'device_manager'):
                self.logger.warning("âš ï¸ ë””ë°”ì´ìŠ¤ ë§¤ë‹ˆì €ê°€ ì—†ìŒ")
                return False
            
            if hasattr(self, 'memory_manager'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
                
            self.logger.info("âœ… ModelLoader v7.0 ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            with self._lock:
                info = {
                    'name': model_name,
                    'registered': model_name in self.model_configs,
                    'cached': any(model_name in key for key in self.model_cache.keys()),
                    'device': self.device,
                    'config': None,
                    'load_time': None,
                    'last_access': None,
                    'access_count': 0,
                    'auto_detected': False
                }
                
                if model_name in self.model_configs:
                    config = self.model_configs[model_name]
                    info['config'] = {
                        'model_type': str(getattr(config, 'model_type', 'unknown')),
                        'model_class': getattr(config, 'model_class', 'unknown'),
                        'input_size': getattr(config, 'input_size', (512, 512)),
                        'num_classes': getattr(config, 'num_classes', None)
                    }
                
                if hasattr(self, 'detected_model_registry') and model_name in self.detected_model_registry:
                    detected_info = self.detected_model_registry[model_name]
                    info['auto_detected'] = True
                    info['detection_info'] = detected_info
                
                for cache_key in self.model_cache.keys():
                    if model_name in cache_key:
                        info['cached'] = True
                        info['load_time'] = self.load_times.get(cache_key)
                        info['last_access'] = self.last_access.get(cache_key)
                        info['access_count'] = self.access_counts.get(cache_key, 0)
                        break
                
                return info
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ ëª©ë¡"""
        try:
            with self._lock:
                models_info = {}
                
                for model_name in self.model_configs.keys():
                    models_info[model_name] = self.get_model_info(model_name)
                
                if hasattr(self, 'detected_model_registry'):
                    for model_name in self.detected_model_registry.keys():
                        if model_name not in models_info:
                            models_info[model_name] = self.get_model_info(model_name)
                
                safe_models = self.safe_model_service.list_models()
                for model_name, status in safe_models.items():
                    if model_name not in models_info:
                        models_info[model_name] = {
                            'name': model_name,
                            'source': 'SafeModelService',
                            'status': status
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def cleanup(self):
        """ì™„ì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    self.cleanup_step_interface(step_name)
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            cpu_method = getattr(model, 'cpu', None)
                            success, result, message = self.function_validator.safe_call(cpu_method)
                            if not success:
                                self.logger.warning(f"âš ï¸ ëª¨ë¸ CPU ì´ë™ ì‹¤íŒ¨: {message}")
                        del model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(self.memory_manager, 'cleanup_memory'):
                cleanup_method = getattr(self.memory_manager, 'cleanup_memory', None)
                success, result, message = self.function_validator.safe_call(cleanup_method)
                if not success:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {message}")
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    shutdown_method = getattr(self._executor, 'shutdown', None)
                    success, result, message = self.function_validator.safe_call(shutdown_method, wait=True)
                    if not success:
                        self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {message}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader v7.0 ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ - ì™„ì „ ë³´ì¡´
# ==============================================

def preprocess_image(
    image: Union[Any, Any, Any],
    target_size: Tuple[int, int] = (512, 512),
    device: str = "mps",
    normalize: bool = True,
    to_tensor: bool = True
) -> Any:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        if not CV_AVAILABLE:
            logger.warning("âš ï¸ OpenCV/PIL ì—†ìŒ, ê¸°ë³¸ ì²˜ë¦¬")
            if TORCH_AVAILABLE and to_tensor:
                return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
            else:
                if NUMPY_AVAILABLE:
                    return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
                else:
                    return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
        
        # PIL/OpenCVë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì „ì²˜ë¦¬
        if hasattr(image, 'resize'):  # PIL Image
            image = image.resize(target_size)
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32)
                if normalize:
                    img_array = img_array / 255.0
                
                if to_tensor and TORCH_AVAILABLE:
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return img_tensor.to(device)
                else:
                    return img_array
        
        # í´ë°± ì²˜ë¦¬
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]
                
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        if TORCH_AVAILABLE and to_tensor:
            return torch.zeros(1, 3, target_size[0], target_size[1], device=device)
        else:
            if NUMPY_AVAILABLE:
                return np.zeros((target_size[0], target_size[1], 3), dtype=np.float32)
            else:
                return [[[0.0 for _ in range(3)] for _ in range(target_size[1])] for _ in range(target_size[0])]

def postprocess_segmentation(output: Any, threshold: float = 0.5) -> Any:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í›„ì²˜ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if NUMPY_AVAILABLE and hasattr(output, 'squeeze'):
            if output.ndim == 4:
                output = output.squeeze(0)
            if output.ndim == 3:
                output = output.squeeze(0)
                
            binary_mask = (output > threshold).astype(np.uint8) * 255
            return binary_mask
        else:
            # NumPy ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
            return [[255 if x > threshold else 0 for x in row] for row in output] if hasattr(output, '__iter__') else output
            
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        if NUMPY_AVAILABLE:
            return np.zeros((512, 512), dtype=np.uint8)
        else:
            return [[0 for _ in range(512)] for _ in range(512)]

# ì¶”ê°€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def preprocess_pose_input(image: Any, target_size: Tuple[int, int] = (368, 368)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_human_parsing_input(image: Any, target_size: Tuple[int, int] = (512, 512)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def preprocess_cloth_segmentation_input(image: Any, target_size: Tuple[int, int] = (320, 320)) -> Any:
    return preprocess_image(image, target_size, normalize=True, to_tensor=True)

def tensor_to_pil(tensor: Any) -> Any:
    """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'dim'):
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.dim() == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = tensor.cpu().numpy()
            
        if NUMPY_AVAILABLE and hasattr(tensor, 'dtype'):
            if tensor.dtype != np.uint8:
                tensor = (tensor * 255).astype(np.uint8)
        
        if CV_AVAILABLE:
            return Image.fromarray(tensor)
        else:
            # PIL ì—†ëŠ” ê²½ìš° ë”ë¯¸ ë°˜í™˜
            return tensor
    except Exception as e:
        logger.error(f"í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def pil_to_tensor(image: Any, device: str = "mps") -> Any:
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
    try:
        if CV_AVAILABLE and hasattr(image, 'size'):
            if NUMPY_AVAILABLE:
                img_array = np.array(image).astype(np.float32) / 255.0
                if TORCH_AVAILABLE:
                    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    return tensor.to(device)
                else:
                    return img_array
        
        # í´ë°±
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return np.zeros((1, 3, 512, 512), dtype=np.float32) if NUMPY_AVAILABLE else None
            
    except Exception as e:
        logger.error(f"PIL->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        if TORCH_AVAILABLE:
            return torch.zeros(1, 3, 512, 512, device=device)
        else:
            return None

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                enable_auto_detection=True,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("ğŸŒ ì „ì—­ ModelLoader v7.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> ModelLoader:  # ğŸ”¥ í•µì‹¬: ë°˜í™˜ íƒ€ì… ë³€ê²½
    """ì „ì—­ ModelLoader ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        
        # ë™ê¸° ì´ˆê¸°í™”
        success = loader.initialize()
        
        if success:
            return loader  # ğŸ”¥ í•µì‹¬: ê°ì²´ ì§ì ‘ ë°˜í™˜
        else:
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def cleanup_global_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            validator = SafeFunctionValidator()
            cleanup_method = getattr(_global_model_loader, 'cleanup', None)
            success, result, message = validator.safe_call(cleanup_method)
            
            if not success:
                logger.warning(f"âš ï¸ ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {message}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("ğŸŒ ì „ì—­ ModelLoader v7.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ Utility Functions
# ==============================================

def get_model_service() -> SafeModelService:
    """ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    loader = get_global_model_loader()
    return loader.safe_model_service

async def safe_warmup_models(model_names: list) -> Dict[str, bool]:
    """ì—¬ëŸ¬ ëª¨ë¸ ì•ˆì „ ì›Œë°ì—…"""
    service = get_model_service()
    results = {}
    
    for name in model_names:
        try:
            results[name] = await service.warmup_model(name)
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ {name}: {e}")
            results[name] = False
    
    return results

def register_dict_as_model(name: str, model_dict: Dict[str, Any]) -> bool:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ë¡œ ì•ˆì „í•˜ê²Œ ë“±ë¡"""
    service = get_model_service()
    return service.register_model(name, model_dict)

def create_mock_model(name: str, model_type: str = "mock") -> Callable:
    """Mock ëª¨ë¸ ìƒì„±"""
    mock_dict = {
        'name': name,
        'type': model_type,
        'status': 'loaded',
        'device': 'mps',
        'loaded_at': '2025-07-20T12:00:00Z'
    }
    
    service = get_model_service()
    return service._create_callable_dict_wrapper(mock_dict)

# ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤
def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ í•¨ìˆ˜ í˜¸ì¶œ"""
    return SafeFunctionValidator.safe_call(obj, *args, **kwargs)

async def safe_async_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ"""
    return await SafeFunctionValidator.safe_async_call(obj, *args, **kwargs)

def safe_getattr_call(obj: Any, attr_name: str, *args, **kwargs) -> Tuple[bool, Any, str]:
    """ì „ì—­ ì•ˆì „í•œ ì†ì„± ì ‘ê·¼ ë° í˜¸ì¶œ"""
    return SafeFunctionValidator.safe_getattr_call(obj, attr_name, *args, **kwargs)

def is_safely_callable(obj: Any) -> bool:
    """ì „ì—­ callable ì•ˆì „ì„± ê²€ì¦"""
    is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj)
    return is_callable

# ==============================================
# ğŸ”¥ Module Exports
# ==============================================

__all__ = [
    # ğŸ”¥ ì—ëŸ¬ í•´ê²° í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'SafeFunctionValidator',
    'SafeModelService',
    'SafeConfig',
    
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'DeviceManager',
    'ModelMemoryManager',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType',
    'ModelPriority',
    'ModelConfig',
    'StepModelConfig',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel',
    'U2NetModel',
    'GeometricMatchingModel',
    
    # íŒ©í† ë¦¬ ë° ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    
    # ì•ˆì „í•œ í˜¸ì¶œ í•¨ìˆ˜ë“¤
    'get_model_service',
    'safe_warmup_models',
    'register_dict_as_model',
    'create_mock_model',
    'safe_call',
    'safe_async_call',
    'safe_getattr_call',
    'is_safely_callable',
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'preprocess_pose_input',
    'preprocess_human_parsing_input',
    'preprocess_cloth_segmentation_input',
    'tensor_to_pil',
    'pil_to_tensor',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    
    # Step ìš”ì²­ì‚¬í•­ ì—°ë™
    'STEP_MODEL_REQUESTS',
    'StepModelRequestAnalyzer'
]

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì•ˆì „í•œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… ModelLoader v7.0 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ëª¨ë“  ì—ëŸ¬ ì™„ì „ í•´ê²°")
logger.info("ğŸ”— SafeModelService + SafeFunctionValidator ì™„ì „ í†µí•©")
logger.info("ğŸ”§ NumPy 2.x + step_model_requests.py ì™„ë²½ í˜¸í™˜")
logger.info("ğŸ M3 Max 128GB ìµœì í™”")
logger.info("ğŸ›¡ï¸ ëª¨ë“  í•¨ìˆ˜/ë©”ì„œë“œ í˜¸ì¶œ ì•ˆì „ì„± ë³´ì¥")
logger.info("ğŸ”„ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (í•œë°©í–¥ ì˜ì¡´ì„±)")
logger.info("ğŸ“¦ ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìµœì í™”")
logger.info("ğŸ¯ Step íŒŒì¼ë“¤ê³¼ 100% í˜¸í™˜")
logger.info(f"ğŸ”§ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ”¢ NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")

if NUMPY_AVAILABLE and hasattr(_compat, 'numpy_version'):
    numpy_major = int(_compat.numpy_version.split('.')[0])
    if numpy_major >= 2:
        logger.warning("âš ï¸ NumPy 2.x ê°ì§€ë¨ - conda install numpy=1.24.3 ê¶Œì¥")
    else:
        logger.info("âœ… NumPy í˜¸í™˜ì„± í™•ì¸ë¨")

logger.info("ğŸš€ ModelLoader v7.0 ì™„ì „ ìµœì í™” ì™„ë£Œ!")
logger.info("   âœ… ê¸°ì¡´ ëª¨ë“  ê¸°ëŠ¥ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
logger.info("   âœ… Dict Callable ì˜¤ë¥˜ ê·¼ë³¸ ì›ì¸ 100% í•´ê²°")
logger.info("   âœ… Coroutine ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("   âœ… create_step_interface ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… list_available_models ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… step_requirements ì¸ì ì™„ì „ ì§€ì›")
logger.info("   âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ìë™ ë§¤ì¹­")
logger.info("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("   âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìµœì í™”")
logger.info("   âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€")