# app/ai_pipeline/utils/model_loader.py
"""
ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ AI ëª¨ë¸ ë¡œë” - ì‹¤ì œ 72GB ëª¨ë¸ ì—°ê²° ì™„ì „íŒ + ìë™ íƒì§€ í†µí•©
âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™ (ê¸°ì¡´ êµ¬ì¡° 100% ìœ ì§€)
âœ… ì‹¤ì œ ë³´ìœ í•œ 72GB ëª¨ë¸ë“¤ê³¼ ì™„ì „ ì—°ê²°
âœ… AutoModelDetector ì™„ì „ í†µí•©
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… ëª¨ë“  í´ë˜ìŠ¤/í•¨ìˆ˜/ì¸ì ë™ì¼í•˜ê²Œ ìœ ì§€
"""

import os
import gc
import time
import threading
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

# PyTorch ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    raise ImportError("PyTorch is required for production ModelLoader")

try:
    import cv2
    import numpy as np
    from PIL import Image
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    raise ImportError("OpenCV and PIL are required for production ModelLoader")

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ í†µí•©
# ==============================================

try:
    from .auto_model_detector import (
        AutoModelDetector,
        ModelLoaderAdapter,
        DetectedModel,
        ModelCategory,
        create_auto_detector,
        detect_models_and_generate_config
    )
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    logger.warning("AutoModelDetector ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²½ë¡œ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    AUTO_DETECTOR_AVAILABLE = False
    DetectedModel = None
    ModelCategory = None

# ==============================================
# ğŸ”¥ ì‹¤ì œ 72GB ëª¨ë¸ ê²½ë¡œ ë§µí•‘ (ê¸°ë³¸ê°’)
# ==============================================

# ê¸°ë³¸ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ë“¤ (ë¶„ì„ ë¦¬í¬íŠ¸ ê¸°ë°˜)
DEFAULT_ACTUAL_MODEL_PATHS = {
    # Step 01: Human Parsing - ì‹¤ì œ ê²½ë¡œ
    "human_parsing_graphonomy": {
        "primary": "backend/ai_models/checkpoints/human_parsing/schp_atr.pth",  # 255MB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/human_parsing/atr_model.pth",  # 255MB âœ…
            "backend/ai_models/checkpoints/human_parsing/pytorch_model.bin"  # 104MB âœ…
        ]
    },
    
    # Step 02: Pose Estimation - ì‹¤ì œ ê²½ë¡œ
    "pose_estimation_openpose": {
        "primary": "backend/ai_models/checkpoints/openpose/ckpts/body_pose_model.pth",  # 200MB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/openpose/hand_pose_model.pth",  # 140MB âœ…
            "backend/ai_models/checkpoints/step_02_pose_estimation/yolov8n-pose.pt"  # 6.5MB âœ…
        ]
    },
    
    # Step 03: Cloth Segmentation - ì‹¤ì œ ê²½ë¡œ
    "cloth_segmentation_u2net": {
        "primary": "backend/ai_models/checkpoints/step_03_cloth_segmentation/u2net.pth",  # 168MB âœ…
        "alternatives": [
            "backend/ai_models/step_03_cloth_segmentation/parsing_lip.onnx",  # 254MB âœ…
            "backend/ai_models/checkpoints/cloth_segmentation/model.pth"  # 168MB âœ…
        ]
    },
    
    # Step 04: Geometric Matching - ì‹¤ì œ ê²½ë¡œ
    "geometric_matching_gmm": {
        "primary": "backend/ai_models/checkpoints/step_04_geometric_matching/lightweight_gmm.pth",  # 4MB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/step_04/step_04_geometric_matching_base/geometric_matching_base.pth",  # 18MB âœ…
            "backend/ai_models/checkpoints/step_04_geometric_matching/tps_transformation_model/tps_network.pth"  # 2MB âœ…
        ]
    },
    
    # Step 05: Cloth Warping - ì‹¤ì œ ê²½ë¡œ (ê°€ìƒ í”¼íŒ…ê³¼ ê³µìš©)
    "cloth_warping_tom": {
        "primary": "backend/ai_models/checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.bin",  # 3.3GB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.safetensors",  # 3.3GB âœ…
            "backend/ai_models/checkpoints/stable_diffusion_inpaint/unet/diffusion_pytorch_model.bin"  # 3.3GB âœ…
        ]
    },
    
    # Step 06: Virtual Fitting - ì‹¤ì œ ê²½ë¡œ
    "virtual_fitting_hrviton": {
        "primary": "backend/ai_models/checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.safetensors",  # 3.3GB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.bin",  # 3.3GB âœ…
            "backend/ai_models/checkpoints/ootdiffusion/checkpoints/ootd/vae/diffusion_pytorch_model.bin"  # 319MB âœ…
        ]
    },
    
    # Step 07: Post Processing - ì‹¤ì œ ê²½ë¡œ
    "post_processing_enhancer": {
        "primary": "backend/ai_models/checkpoints/step_07_post_processing/RealESRGAN_x4plus.pth",  # 64MB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/pose_estimation/res101.pth",  # 506MB âœ…
            "backend/ai_models/checkpoints/pose_estimation/clip_g.pth"  # 3.5GB âœ…
        ]
    },
    
    # Step 08: Quality Assessment - ì‹¤ì œ ê²½ë¡œ
    "quality_assessment_combined": {
        "primary": "backend/ai_models/checkpoints/step_01_human_parsing/densepose_rcnn_R_50_FPN_s1x.pkl",  # 244MB âœ…
        "alternatives": [
            "backend/ai_models/checkpoints/sam/sam_vit_h_4b8939.pth",  # 2.4GB âœ…
            "backend/ai_models/checkpoints/auxiliary/resnet50_features/resnet50_features.pth"  # 98MB âœ…
        ]
    }
}

# ==============================================
# ğŸ”¥ í•µì‹¬ ëª¨ë¸ ì •ì˜ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§· ì •ì˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    COREML = "coreml"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ - í”„ë¡œë•ì…˜ ë²„ì „ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class GraphonomyModel(nn.Module):
    """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - Step 01"""
    
    def __init__(self, num_classes=20, backbone='resnet101', pretrained=True):
        super().__init__()
        self.num_classes = num_classes
        self.backbone_name = backbone
        
        # ResNet ë°±ë³¸ êµ¬ì„±
        self.backbone = self._build_backbone(pretrained)
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = self._build_aspp()
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)
        
        # ë³´ì¡° ë¶„ë¥˜ê¸°
        self.aux_classifier = nn.Conv2d(1024, num_classes, kernel_size=1)
        
    def _build_backbone(self, pretrained=True):
        """ResNet ë°±ë³¸ êµ¬ì„±"""
        try:
            import torchvision.models as models
            if self.backbone_name == 'resnet101':
                backbone = models.resnet101(pretrained=pretrained)
            else:
                backbone = models.resnet50(pretrained=pretrained)
                
            # Atrous convolutionì„ ìœ„í•œ ì„¤ì •
            backbone.layer3[0].conv2.stride = (1, 1)
            backbone.layer3[0].downsample[0].stride = (1, 1)
            backbone.layer4[0].conv2.stride = (1, 1)
            backbone.layer4[0].downsample[0].stride = (1, 1)
            
            # Dilation ì ìš©
            for module in backbone.layer3[1:]:
                module.conv2.dilation = (2, 2)
                module.conv2.padding = (2, 2)
            for module in backbone.layer4:
                module.conv2.dilation = (4, 4)
                module.conv2.padding = (4, 4)
                
            return nn.Sequential(*list(backbone.children())[:-2])
        except ImportError:
            # ê¸°ë³¸ CNN ë°±ë³¸
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                *[self._make_layer(64, 128, 2, stride=2) for _ in range(3)],
                *[self._make_layer(128, 256, 2, stride=2) for _ in range(4)],
                *[self._make_layer(256, 512, 2, stride=2) for _ in range(6)],
                *[self._make_layer(512, 1024, 2, stride=2) for _ in range(3)],
                nn.AdaptiveAvgPool2d((1, 1))
            )
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """ResNet ë ˆì´ì–´ êµ¬ì„±"""
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def _build_aspp(self):
        """ASPP ëª¨ë“ˆ êµ¬ì„±"""
        return nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(1024, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=6, dilation=6, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=12, dilation=12, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(1024, 256, 3, padding=18, dilation=18, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Conv2d(1024, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            )
        ])
    
    def forward(self, x):
        input_size = x.size()[2:]
        
        # ë°±ë³¸ í†µê³¼
        features = self.backbone(x)
        
        # ASPP ì ìš©
        aspp_outputs = []
        for i, aspp_layer in enumerate(self.aspp[:-1]):
            aspp_outputs.append(aspp_layer(features))
        
        # Global average pooling
        global_feat = self.aspp[-1](features)
        global_feat = F.interpolate(global_feat, size=features.size()[2:], 
                                   mode='bilinear', align_corners=False)
        aspp_outputs.append(global_feat)
        
        # íŠ¹ì§• ìœµí•©
        fused = torch.cat(aspp_outputs, dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.classifier(fused)
        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
        
        return output

class OpenPoseModel(nn.Module):
    """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - Step 02"""
    
    def __init__(self, num_keypoints=18, num_pafs=38):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.num_pafs = num_pafs
        
        # VGG ë°±ë³¸
        self.backbone = self._build_vgg_backbone()
        
        # ì´ˆê¸° ìŠ¤í…Œì´ì§€
        self.stage1_paf = self._build_initial_stage(num_pafs)
        self.stage1_heatmap = self._build_initial_stage(num_keypoints + 1)
        
        # ê°œì„  ìŠ¤í…Œì´ì§€ë“¤
        self.refinement_stages = nn.ModuleList()
        for i in range(5):
            self.refinement_stages.append(nn.ModuleDict({
                'paf': self._build_refinement_stage(num_pafs),
                'heatmap': self._build_refinement_stage(num_keypoints + 1)
            }))
    
    def _build_vgg_backbone(self):
        """VGG ë°±ë³¸ êµ¬ì„±"""
        return nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 4
            nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
        )
    
    def _build_initial_stage(self, output_channels):
        """ì´ˆê¸° ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(512, output_channels, 1, 1, 0)
        )
    
    def _build_refinement_stage(self, output_channels):
        """ê°œì„  ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        input_channels = 512 + self.num_pafs + self.num_keypoints + 1
        return nn.Sequential(
            nn.Conv2d(input_channels, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(128, output_channels, 1, 1, 0)
        )
    
    def forward(self, x):
        # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # ì´ˆê¸° ìŠ¤í…Œì´ì§€
        paf = self.stage1_paf(features)
        heatmap = self.stage1_heatmap(features)
        
        stage_outputs = [(paf, heatmap)]
        
        # ê°œì„  ìŠ¤í…Œì´ì§€ë“¤
        for stage in self.refinement_stages:
            combined = torch.cat([features, paf, heatmap], dim=1)
            paf = stage['paf'](combined)
            heatmap = stage['heatmap'](combined)
            stage_outputs.append((paf, heatmap))
        
        return stage_outputs

class U2NetModel(nn.Module):
    """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - Step 03"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super().__init__()
        
        # ì¸ì½”ë”
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU6(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU5(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU4(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU4F(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU4F(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU4F(1024, 256, 512)
        self.stage4d = RSU4(1024, 128, 256)
        self.stage3d = RSU5(512, 64, 128)
        self.stage2d = RSU6(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # ì¶œë ¥ ë ˆì´ì–´ë“¤
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        self.outconv = nn.Conv2d(6 * out_ch, out_ch, 1)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx6up = F.interpolate(hx6, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # ì¶œë ¥
        d1 = self.side1(hx1d)
        d2 = F.interpolate(self.side2(hx2d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d3 = F.interpolate(self.side3(hx3d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d4 = F.interpolate(self.side4(hx4d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d5 = F.interpolate(self.side5(hx5d), size=x.shape[2:], mode='bilinear', align_corners=False)
        d6 = F.interpolate(self.side6(hx6), size=x.shape[2:], mode='bilinear', align_corners=False)
        
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0)

# RSU ë¸”ë¡ë“¤ êµ¬í˜„ (ê°„ì†Œí™”)
class RSU7(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = F.interpolate(hx6d, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        
        return hx1d + hxin

# ê°„ì†Œí™”ëœ RSU ë¸”ë¡ë“¤ (êµ¬í˜„ ìƒëµ)
class RSU6(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
    def forward(self, x):
        return self.conv(x)

class RSU5(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
    def forward(self, x):
        return self.conv(x)

class RSU4(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
    def forward(self, x):
        return self.conv(x)

class RSU4F(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
    def forward(self, x):
        return self.conv(x)

class REBNCONV(nn.Module):
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super().__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - Step 04"""
    
    def __init__(self, feature_size=256, num_control_points=18):
        super().__init__()
        self.feature_size = feature_size
        self.num_control_points = num_control_points
        
        # íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬
        self.feature_extractor = self._build_feature_extractor()
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        self.correlation = self._build_correlation_layer()
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        self.tps_regression = self._build_tps_regression()
        
    def _build_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, self.feature_size, 3, 1, 1), nn.BatchNorm2d(self.feature_size), nn.ReLU(inplace=True)
        )
    
    def _build_correlation_layer(self):
        """ìƒê´€ê´€ê³„ ê³„ì‚° ë ˆì´ì–´"""
        return nn.Sequential(
            nn.Conv2d(self.feature_size * 2, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_tps_regression(self):
        """TPS íŒŒë¼ë¯¸í„° íšŒê·€ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(64, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(128, self.num_control_points * 2)  # x, y ì¢Œí‘œ
        )
    
    def forward(self, source_img, target_img):
        # íŠ¹ì§• ì¶”ì¶œ
        source_feat = self.feature_extractor(source_img)
        target_feat = self.feature_extractor(target_img)
        
        # íŠ¹ì§• ê²°í•©
        combined_feat = torch.cat([source_feat, target_feat], dim=1)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_map = self.correlation(combined_feat)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.tps_regression(correlation_map)
        tps_params = tps_params.view(-1, self.num_control_points, 2)
        
        return {
            'correlation_map': correlation_map,
            'tps_params': tps_params,
            'source_features': source_feat,
            'target_features': target_feat
        }

class HRVITONModel(nn.Module):
    """HR-VITON ê°€ìƒ í”¼íŒ… ëª¨ë¸ - Step 06"""
    
    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_downsampling=2, n_blocks=9):
        super().__init__()
        
        # ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬
        self.generator = self._build_generator(input_nc, output_nc, ngf, n_downsampling, n_blocks)
        
        # ì–´í…ì…˜ ëª¨ë“ˆ
        self.attention = self._build_attention_module()
        
        # ìœµí•© ëª¨ë“ˆ
        self.fusion = self._build_fusion_module()
    
    def _build_generator(self, input_nc, output_nc, ngf, n_downsampling, n_blocks):
        """ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±"""
        model = [
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=False),
            nn.InstanceNorm2d(ngf),
            nn.ReLU(True)
        ]
        
        # ë‹¤ìš´ìƒ˜í”Œë§
        for i in range(n_downsampling):
            mult = 2 ** i
            model += [
                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=False),
                nn.InstanceNorm2d(ngf * mult * 2),
                nn.ReLU(True)
            ]
        
        # ResNet ë¸”ë¡ë“¤
        mult = 2 ** n_downsampling
        for i in range(n_blocks):
            model += [ResnetBlock(ngf * mult, use_dropout=False)]
        
        # ì—…ìƒ˜í”Œë§
        for i in range(n_downsampling):
            mult = 2 ** (n_downsampling - i)
            model += [
                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),
                nn.InstanceNorm2d(int(ngf * mult / 2)),
                nn.ReLU(True)
            ]
        
        model += [nn.ReflectionPad2d(3)]
        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]
        model += [nn.Tanh()]
        
        return nn.Sequential(*model)
    
    def _build_attention_module(self):
        """ì–´í…ì…˜ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_fusion_module(self):
        """ìœµí•© ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 3, 3, 1, 1), nn.Tanh()
        )
    
    def forward(self, person_img, cloth_img, person_parse=None):
        # ê¸°ë³¸ ìƒì„±
        input_concat = torch.cat([person_img, cloth_img], dim=1)
        generated = self.generator(input_concat)
        
        # ì–´í…ì…˜ ë§µ ê³„ì‚°
        attention_map = self.attention(input_concat)
        
        # ì–´í…ì…˜ ì ìš© ìœµí•©
        attended_result = generated * attention_map + person_img * (1 - attention_map)
        
        # ì¶”ê°€ ìœµí•©
        final_result = self.fusion(torch.cat([attended_result, cloth_img], dim=1))
        
        return {
            'generated_image': final_result,
            'attention_map': attention_map,
            'intermediate': generated,
            'warped_cloth': cloth_img
        }

class ResnetBlock(nn.Module):
    """ResNet ë¸”ë¡"""
    def __init__(self, dim, use_dropout=False):
        super().__init__()
        self.conv_block = self._build_conv_block(dim, use_dropout)

    def _build_conv_block(self, dim, use_dropout):
        layers = []
        layers += [nn.ReflectionPad2d(1)]
        layers += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=False), nn.InstanceNorm2d(dim), nn.ReLU(True)]
        if use_dropout:
            layers += [nn.Dropout(0.5)]
        layers += [nn.ReflectionPad2d(1)]
        layers += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=False), nn.InstanceNorm2d(dim)]
        return nn.Sequential(*layers)

    def forward(self, x):
        return x + self.conv_block(x)

# ==============================================
# ğŸ”¥ ìë™ ëª¨ë¸ íƒì§€ í†µí•© ê²½ë¡œ ê²°ì • í•¨ìˆ˜
# ==============================================

def get_actual_model_paths() -> Dict[str, Dict[str, Any]]:
    """ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¥¼ ìë™ íƒì§€ ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œì—ì„œ ë°˜í™˜"""
    try:
        if AUTO_DETECTOR_AVAILABLE:
            logger.info("ğŸ” ìë™ ëª¨ë¸ íƒì§€ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì¤‘...")
            
            # ìë™ íƒì§€ ì‹¤í–‰
            detector = create_auto_detector()
            detected_models = detector.detect_all_models()
            
            if detected_models:
                # ì–´ëŒ‘í„°ë¥¼ í†µí•´ ModelLoader í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                adapter = ModelLoaderAdapter(detector)
                actual_paths = adapter.generate_actual_model_paths()
                
                logger.info(f"âœ… ìë™ íƒì§€ ì™„ë£Œ: {len(actual_paths)}ê°œ ëª¨ë¸ ë°œê²¬")
                return actual_paths
            else:
                logger.warning("âš ï¸ ìë™ íƒì§€ì—ì„œ ëª¨ë¸ì„ ì°¾ì§€ ëª»í•¨, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
                
        else:
            logger.info("ğŸ“ ìë™ íƒì§€ê¸° ë¯¸ì‚¬ìš©, ê¸°ë³¸ ê²½ë¡œ ë§¤í•‘ ì‚¬ìš©")
            
    except Exception as e:
        logger.error(f"âŒ ìë™ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ê²½ë¡œë¡œ í´ë°±")
    
    # ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜ (í˜¸í™˜ì„± ìœ ì§€)
    return DEFAULT_ACTUAL_MODEL_PATHS

def find_actual_checkpoint_path(model_name: str) -> Optional[str]:
    """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸° - ìë™ íƒì§€ í†µí•©"""
    try:
        # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        actual_model_paths = get_actual_model_paths()
        
        if model_name not in actual_model_paths:
            logger.warning(f"ëª¨ë¸ {model_name}ì— ëŒ€í•œ ê²½ë¡œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        model_info = actual_model_paths[model_name]
        
        # ìë™ íƒì§€ ê²°ê³¼ì¸ ê²½ìš° (primary í‚¤ë§Œ ìˆìŒ)
        if "primary" not in model_info and "path" in model_info:
            primary_path = Path(model_info["path"])
            if primary_path.exists():
                logger.info(f"âœ… ìë™ íƒì§€ ê²½ë¡œ ë°œê²¬: {primary_path}")
                return str(primary_path)
        
        # ê¸°ë³¸ í˜•ì‹ì¸ ê²½ìš° (primary, alternatives í‚¤ ìˆìŒ)
        elif "primary" in model_info:
            # 1. ìš°ì„  ê²½ë¡œ í™•ì¸
            primary_path = Path(model_info["primary"])
            if primary_path.exists():
                logger.info(f"âœ… ìš°ì„  ê²½ë¡œ ë°œê²¬: {primary_path}")
                return str(primary_path)
            
            # 2. ëŒ€ì²´ ê²½ë¡œë“¤ í™•ì¸
            for alt_path in model_info.get("alternatives", []):
                alt_path = Path(alt_path)
                if alt_path.exists():
                    logger.info(f"âœ… ëŒ€ì²´ ê²½ë¡œ ë°œê²¬: {alt_path}")
                    return str(alt_path)
        
        # 3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        logger.error(f"âŒ {model_name}ì— ëŒ€í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None
        
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨ {model_name}: {e}")
        return None

def validate_model_availability() -> Dict[str, bool]:
    """ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ê²€ì¦ - ìë™ íƒì§€ í†µí•©"""
    availability = {}
    
    logger.info("ğŸ” ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê°€ìš©ì„± ê²€ì¦ ì¤‘...")
    
    actual_model_paths = get_actual_model_paths()
    
    for model_name in actual_model_paths.keys():
        actual_path = find_actual_checkpoint_path(model_name)
        availability[model_name] = actual_path is not None
        
        if actual_path:
            file_size = Path(actual_path).stat().st_size / (1024**2)  # MB
            logger.info(f"   âœ… {model_name}: {file_size:.1f}MB")
        else:
            logger.warning(f"   âŒ {model_name}: íŒŒì¼ ì—†ìŒ")
    
    available_count = sum(availability.values())
    total_count = len(availability)
    
    logger.info(f"ğŸ“Š ëª¨ë¸ ê°€ìš©ì„±: {available_count}/{total_count} ({available_count/total_count*100:.1f}%)")
    
    return availability

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - í”„ë¡œë•ì…˜ ë²„ì „ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class ModelMemoryManager:
    """í”„ë¡œë•ì…˜ ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps", memory_limit_gb: float = 128.0):
        self.device = device
        self.memory_limit_gb = memory_limit_gb
        self.memory_threshold = 0.8
        
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return self.memory_limit_gb * 0.8
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.memory_limit_gb * 0.5
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif self.device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            
            logger.debug("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            if available_memory < self.memory_limit_gb * 0.2:  # 20% ë¯¸ë§Œ
                return True
            return False
        except Exception:
            return False

# ==============================================
# ğŸ”¥ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - í”„ë¡œë•ì…˜ ë²„ì „ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class ModelRegistry:
    """í”„ë¡œë•ì…˜ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not getattr(self, '_initialized', False):
            self.registered_models: Dict[str, Dict[str, Any]] = {}
            self._lock = threading.RLock()
            self._initialized = True
            logger.info("ModelRegistry ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model(self, 
                      name: str, 
                      model_class: Type, 
                      default_config: Dict[str, Any] = None,
                      loader_func: Optional[Callable] = None):
        """ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            try:
                self.registered_models[name] = {
                    'class': model_class,
                    'config': default_config or {},
                    'loader': loader_func,
                    'registered_at': time.time()
                }
                logger.info(f"ëª¨ë¸ ë“±ë¡: {name}")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            return self.registered_models.get(name)
    
    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            return list(self.registered_models.keys())
    
    def unregister_model(self, name: str) -> bool:
        """ëª¨ë¸ ë“±ë¡ í•´ì œ"""
        with self._lock:
            try:
                if name in self.registered_models:
                    del self.registered_models[name]
                    logger.info(f"ëª¨ë¸ ë“±ë¡ í•´ì œ: {name}")
                    return True
                return False
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ í•´ì œ ì‹¤íŒ¨ {name}: {e}")
                return False

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ - í”„ë¡œë•ì…˜ ë²„ì „ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ì™€ ModelLoader ê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.loaded_models: Dict[str, Any] = {}
        self._lock = threading.RLock()
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Stepì—ì„œ í•„ìš”í•œ ëª¨ë¸ ìš”ì²­"""
        try:
            with self._lock:
                cache_key = f"{self.step_name}_{model_name}"
                
                # ìºì‹œ í™•ì¸
                if cache_key in self.loaded_models:
                    return self.loaded_models[cache_key]
                
                # ëª¨ë¸ ë¡œë“œ
                model = await self.model_loader.load_model(model_name, **kwargs)
                
                if model:
                    self.loaded_models[cache_key] = model
                    logger.info(f"ğŸ“¦ {self.step_name}ì— {model_name} ëª¨ë¸ ì „ë‹¬ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™•ì¸ í•„ìš”")
                
                return model
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_recommended_model(self) -> Optional[Any]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ì„ íƒ"""
        model_recommendations = {
            'HumanParsingStep': 'human_parsing_graphonomy',
            'PoseEstimationStep': 'pose_estimation_openpose', 
            'ClothSegmentationStep': 'cloth_segmentation_u2net',
            'GeometricMatchingStep': 'geometric_matching_gmm',
            'ClothWarpingStep': 'cloth_warping_tom',
            'VirtualFittingStep': 'virtual_fitting_hrviton',
            'PostProcessingStep': 'post_processing_enhancer',
            'QualityAssessmentStep': 'quality_assessment_combined'
        }
        
        recommended_model = model_recommendations.get(self.step_name)
        if recommended_model:
            return await self.get_model(recommended_model)
        
        logger.error(f"âŒ {self.step_name}ì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def unload_models(self):
        """Stepì˜ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model_name, model in self.loaded_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                
                self.loaded_models.clear()
                logger.info(f"ğŸ—‘ï¸ {self.step_name} ëª¨ë¸ë“¤ ì–¸ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤ - ì‹¤ì œ 72GB ëª¨ë¸ ì—°ê²° + ìë™ íƒì§€ í†µí•© ì™„ì „íŒ
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ AI ëª¨ë¸ ë¡œë” - ì‹¤ì œ 72GB ëª¨ë¸ ì—°ê²° + ìë™ íƒì§€ í†µí•© ì™„ì „íŒ
    âœ… Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™ (ê¸°ì¡´ êµ¬ì¡° 100% ìœ ì§€)
    âœ… ì‹¤ì œ ë³´ìœ í•œ 72GB ëª¨ë¸ë“¤ê³¼ ì™„ì „ ì—°ê²°
    âœ… AutoModelDetector ì™„ì „ í†µí•©
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """Step í´ë˜ìŠ¤ì™€ ì™„ë²½ í˜¸í™˜ë˜ëŠ” ìƒì„±ì (ê¸°ì¡´ê³¼ 100% ë™ì¼)"""
        
        # ğŸ”¥ Step í´ë˜ìŠ¤ ìƒì„±ì íŒ¨í„´ ì™„ì „ í˜¸í™˜
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 128.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # ModelLoader íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', 'backend/app/ai_pipeline/models/ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        
        # ìë™ íƒì§€ ì„¤ì •
        self.enable_auto_detection = kwargs.get('enable_auto_detection', AUTO_DETECTOR_AVAILABLE)
        self.detection_force_rescan = kwargs.get('detection_force_rescan', False)
        
        # Step íŠ¹í™” ì„¤ì • ë³‘í•©
        self._merge_step_specific_config(kwargs)
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ í”„ë¡œë•ì…˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")

        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """Step íŠ¹í™” ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'model_cache_dir', 'use_fp16', 'max_cached_models',
            'lazy_loading', 'enable_auto_detection', 'detection_force_rescan'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _initialize_step_specific(self):
        """ModelLoader íŠ¹í™” ì´ˆê¸°í™”"""
        # í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤
        self.registry = ModelRegistry()
        self.memory_manager = ModelMemoryManager(device=self.device, memory_limit_gb=self.memory_gb)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        self._interface_lock = threading.RLock()
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self.use_fp16 = True
            if COREML_AVAILABLE:
                self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
        
        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” - ìë™ íƒì§€ í†µí•© ë²„ì „
        self._initialize_enhanced_model_registry()
        
        self.logger.info(f"ğŸ“¦ ì‹¤ì œ 72GB ëª¨ë¸ ì—°ê²° + ìë™ íƒì§€ í†µí•© ì™„ë£Œ - {self.device} (FP16: {self.use_fp16})")

    def _initialize_enhanced_model_registry(self):
        """ğŸ”¥ ì‹¤ì œ 72GB AI ëª¨ë¸ë“¤ ë“±ë¡ - ìë™ íƒì§€ í†µí•© ë²„ì „"""
        
        self.logger.info("ğŸ” ì‹¤ì œ 72GB ëª¨ë¸ íŒŒì¼ë“¤ íƒì§€ ë° ë“±ë¡ ì¤‘... (ìë™ íƒì§€ í†µí•©)")
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
        if self.enable_auto_detection and AUTO_DETECTOR_AVAILABLE:
            try:
                self.logger.info("ğŸ¤– AutoModelDetector ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ìë™ íƒì§€...")
                
                # ìë™ íƒì§€ ì‹¤í–‰
                detector = create_auto_detector()
                detected_models = detector.detect_all_models(force_rescan=self.detection_force_rescan)
                
                if detected_models:
                    registered_count = 0
                    
                    # íƒì§€ëœ ëª¨ë¸ë“¤ ë“±ë¡
                    for name, detected_model in detected_models.items():
                        try:
                            # ModelConfig ìƒì„±
                            model_config = self._create_model_config_from_detected(detected_model)
                            
                            if model_config:
                                # ëª¨ë¸ ë“±ë¡
                                self.register_model(name, model_config)
                                registered_count += 1
                                
                                file_size = detected_model.file_size_mb
                                self.logger.info(f"âœ… ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡: {name} ({file_size:.1f}MB)")
                            
                        except Exception as e:
                            self.logger.error(f"âŒ ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
                    
                    if registered_count > 0:
                        self.logger.info(f"ğŸ‰ ìë™ íƒì§€ ì™„ë£Œ: {registered_count}ê°œ ëª¨ë¸ ë“±ë¡")
                        return
                        
                else:
                    self.logger.warning("âš ï¸ ìë™ íƒì§€ì—ì„œ ëª¨ë¸ì„ ì°¾ì§€ ëª»í•¨")
                    
            except Exception as e:
                self.logger.error(f"âŒ ìë™ íƒì§€ ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ ë“±ë¡
        self.logger.info("ğŸ“ ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ ëª¨ë¸ ë“±ë¡ìœ¼ë¡œ í´ë°±")
        self._initialize_fallback_model_registry()

    def _create_model_config_from_detected(self, detected_model: 'DetectedModel') -> Optional[ModelConfig]:
        """íƒì§€ëœ ëª¨ë¸ì—ì„œ ModelConfig ìƒì„±"""
        try:
            # ì¹´í…Œê³ ë¦¬ë¥¼ ModelTypeìœ¼ë¡œ ë§¤í•‘
            category_to_type = {
                "human_parsing": ModelType.HUMAN_PARSING,
                "pose_estimation": ModelType.POSE_ESTIMATION,
                "cloth_segmentation": ModelType.CLOTH_SEGMENTATION,
                "geometric_matching": ModelType.GEOMETRIC_MATCHING,
                "cloth_warping": ModelType.CLOTH_WARPING,
                "virtual_fitting": ModelType.VIRTUAL_FITTING,
                "post_processing": ModelType.POST_PROCESSING,
                "quality_assessment": ModelType.QUALITY_ASSESSMENT,
                "auxiliary": ModelType.QUALITY_ASSESSMENT  # ë³´ì¡° ëª¨ë¸ì€ í’ˆì§ˆ í‰ê°€ë¡œ ë¶„ë¥˜
            }
            
            model_type = category_to_type.get(detected_model.category.value)
            if not model_type:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¹´í…Œê³ ë¦¬: {detected_model.category.value}")
                return None
            
            # ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì •
            model_class = self._determine_model_class_from_type(model_type, detected_model)
            
            # ì…ë ¥ í¬ê¸° ê²°ì •
            input_size = self._get_input_size_for_type(model_type)
            
            # num_classes ê²°ì •
            num_classes = self._get_num_classes_for_type(model_type)
            
            return ModelConfig(
                name=detected_model.name,
                model_type=model_type,
                model_class=model_class,
                checkpoint_path=str(detected_model.path),
                device=self.device,
                precision="fp16" if self.use_fp16 else "fp32",
                input_size=input_size,
                num_classes=num_classes,
                metadata={
                    **detected_model.metadata,
                    "auto_detected": True,
                    "confidence_score": detected_model.confidence_score,
                    "file_size_mb": detected_model.file_size_mb,
                    "alternative_paths": [str(p) for p in detected_model.alternative_paths]
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ íƒì§€ëœ ëª¨ë¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _determine_model_class_from_type(self, model_type: ModelType, detected_model: 'DetectedModel') -> str:
        """ëª¨ë¸ íƒ€ì…ê³¼ íƒì§€ ì •ë³´ì—ì„œ ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì •"""
        # íŒŒì¼ëª…ì´ë‚˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•œ í´ë˜ìŠ¤ ê²°ì •
        file_name = detected_model.path.name.lower()
        
        if model_type == ModelType.HUMAN_PARSING:
            if "graphonomy" in file_name or "schp" in file_name:
                return "GraphonomyModel"
            return "GraphonomyModel"  # ê¸°ë³¸ê°’
            
        elif model_type == ModelType.POSE_ESTIMATION:
            if "openpose" in file_name:
                return "OpenPoseModel"
            return "OpenPoseModel"  # ê¸°ë³¸ê°’
            
        elif model_type == ModelType.CLOTH_SEGMENTATION:
            if "u2net" in file_name:
                return "U2NetModel"
            return "U2NetModel"  # ê¸°ë³¸ê°’
            
        elif model_type == ModelType.GEOMETRIC_MATCHING:
            return "GeometricMatchingModel"
            
        elif model_type in [ModelType.CLOTH_WARPING, ModelType.VIRTUAL_FITTING]:
            if "diffusion" in file_name:
                return "StableDiffusionPipeline"
            return "HRVITONModel"  # ê¸°ë³¸ê°’
            
        else:
            # ê¸°íƒ€ ëª¨ë¸ë“¤ì€ ë²”ìš© ëª¨ë¸ë¡œ
            return "GraphonomyModel"

    def _get_input_size_for_type(self, model_type: ModelType) -> tuple:
        """ëª¨ë¸ íƒ€ì…ë³„ ê¸°ë³¸ ì…ë ¥ í¬ê¸°"""
        size_mapping = {
            ModelType.HUMAN_PARSING: (512, 512),
            ModelType.POSE_ESTIMATION: (368, 368),
            ModelType.CLOTH_SEGMENTATION: (320, 320),
            ModelType.GEOMETRIC_MATCHING: (512, 384),
            ModelType.CLOTH_WARPING: (512, 384),
            ModelType.VIRTUAL_FITTING: (512, 384),
            ModelType.POST_PROCESSING: (512, 512),
            ModelType.QUALITY_ASSESSMENT: (224, 224)
        }
        return size_mapping.get(model_type, (512, 512))

    def _get_num_classes_for_type(self, model_type: ModelType) -> Optional[int]:
        """ëª¨ë¸ íƒ€ì…ë³„ í´ë˜ìŠ¤ ìˆ˜"""
        class_mapping = {
            ModelType.HUMAN_PARSING: 20,
            ModelType.POSE_ESTIMATION: 18,
            ModelType.CLOTH_SEGMENTATION: 1,
            ModelType.GEOMETRIC_MATCHING: None,
            ModelType.CLOTH_WARPING: None,
            ModelType.VIRTUAL_FITTING: None,
            ModelType.POST_PROCESSING: None,
            ModelType.QUALITY_ASSESSMENT: None
        }
        return class_mapping.get(model_type)

    def _initialize_fallback_model_registry(self):
        """í´ë°±: ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ ëª¨ë¸ ë“±ë¡"""
        
        self.logger.info("ğŸ“ ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ 72GB ëª¨ë¸ ë“±ë¡ ì¤‘...")
        
        # ì‹¤ì œ ëª¨ë¸ ê°€ìš©ì„± ê²€ì¦
        model_availability = validate_model_availability()
        
        registered_count = 0
        failed_count = 0
        
        for model_name, is_available in model_availability.items():
            if not is_available:
                self.logger.warning(f"âŒ {model_name}: íŒŒì¼ ì—†ìŒ - ë“±ë¡ ê±´ë„ˆëœ€")
                failed_count += 1
                continue
            
            try:
                # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
                actual_path = find_actual_checkpoint_path(model_name)
                if not actual_path:
                    failed_count += 1
                    continue
                
                # ëª¨ë¸ ì„¤ì • ìƒì„±
                model_config = self._create_model_config_from_actual_path(model_name, actual_path)
                
                if model_config:
                    # ëª¨ë¸ ë“±ë¡
                    self.register_model(model_name, model_config)
                    registered_count += 1
                    
                    file_size = Path(actual_path).stat().st_size / (1024**2)  # MB
                    self.logger.info(f"âœ… {model_name}: {file_size:.1f}MB - ë“±ë¡ ì™„ë£Œ")
                else:
                    failed_count += 1
                    
            except Exception as e:
                self.logger.error(f"âŒ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
                failed_count += 1
        
        total_models = len(model_availability)
        success_rate = (registered_count / total_models * 100) if total_models > 0 else 0
        
        self.logger.info(f"ğŸ“Š ê¸°ë³¸ ê²½ë¡œ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {registered_count}/{total_models} ({success_rate:.1f}%)")
        
        if registered_count == 0:
            self.logger.error("âŒ ë“±ë¡ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤ - ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”")
        elif failed_count > 0:
            self.logger.warning(f"âš ï¸ {failed_count}ê°œ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨")

    def _create_model_config_from_actual_path(self, model_name: str, actual_path: str) -> Optional[ModelConfig]:
        """ì‹¤ì œ íŒŒì¼ ê²½ë¡œì—ì„œ ModelConfig ìƒì„± (ê¸°ë³¸ ê²½ë¡œ ë°©ì‹)"""
        try:
            # ëª¨ë¸ë³„ ì„¤ì • ë§¤í•‘
            model_configs = {
                "human_parsing_graphonomy": {
                    "model_type": ModelType.HUMAN_PARSING,
                    "model_class": "GraphonomyModel",
                    "input_size": (512, 512),
                    "num_classes": 20,
                    "metadata": {"backbone": "resnet101", "pretrained": True}
                },
                
                "pose_estimation_openpose": {
                    "model_type": ModelType.POSE_ESTIMATION,
                    "model_class": "OpenPoseModel",
                    "input_size": (368, 368),
                    "num_classes": 18,
                    "metadata": {"num_pafs": 38, "stages": 6}
                },
                
                "cloth_segmentation_u2net": {
                    "model_type": ModelType.CLOTH_SEGMENTATION,
                    "model_class": "U2NetModel",
                    "input_size": (320, 320),
                    "metadata": {"architecture": "u2net", "output_channels": 1}
                },
                
                "geometric_matching_gmm": {
                    "model_type": ModelType.GEOMETRIC_MATCHING,
                    "model_class": "GeometricMatchingModel",
                    "input_size": (512, 384),
                    "metadata": {"control_points": 18, "feature_size": 256}
                },
                
                "cloth_warping_tom": {
                    "model_type": ModelType.CLOTH_WARPING,
                    "model_class": "HRVITONModel",
                    "input_size": (512, 384),
                    "metadata": {"generator_type": "unet", "blocks": 9}
                },
                
                "virtual_fitting_hrviton": {
                    "model_type": ModelType.VIRTUAL_FITTING,
                    "model_class": "HRVITONModel",
                    "input_size": (512, 384),
                    "metadata": {"has_attention": True, "has_fusion": True}
                },
                
                "post_processing_enhancer": {
                    "model_type": ModelType.POST_PROCESSING,
                    "model_class": "GraphonomyModel",  # ë²”ìš© ëª¨ë¸ë¡œ ì‚¬ìš©
                    "input_size": (512, 512),
                    "metadata": {"enhancement": True, "upscale_factor": 4}
                },
                
                "quality_assessment_combined": {
                    "model_type": ModelType.QUALITY_ASSESSMENT,
                    "model_class": "GraphonomyModel",  # ë²”ìš© ëª¨ë¸ë¡œ ì‚¬ìš©
                    "input_size": (224, 224),
                    "metadata": {"assessment": True, "metrics": ["quality", "realism"]}
                }
            }
            
            if model_name not in model_configs:
                self.logger.error(f"âŒ {model_name}ì— ëŒ€í•œ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            config_data = model_configs[model_name]
            
            return ModelConfig(
                name=model_name,
                model_type=config_data["model_type"],
                model_class=config_data["model_class"],
                checkpoint_path=actual_path,
                device=self.device,
                precision="fp16" if self.use_fp16 else "fp32",
                input_size=config_data["input_size"],
                num_classes=config_data.get("num_classes"),
                metadata={
                    **config_data.get("metadata", {}),
                    "auto_detected": False,  # ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜
                    "fallback_registration": True
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ {model_name} ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def register_model(
        self,
        name: str,
        model_config: Union[ModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            with self._lock:
                # ModelConfig ê°ì²´ë¡œ ë³€í™˜
                if isinstance(model_config, dict):
                    model_config = ModelConfig(name=name, **model_config)
                elif not isinstance(model_config, ModelConfig):
                    raise ValueError(f"Invalid model_config type: {type(model_config)}")
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ìë™ ê°ì§€
                if model_config.device == "auto":
                    model_config.device = self.device
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                self.registry.register_model(
                    name=name,
                    model_class=self._get_model_class(model_config.model_class),
                    default_config=model_config.__dict__,
                    loader_func=loader_func
                )
                
                # ë‚´ë¶€ ì„¤ì • ì €ì¥
                self.model_configs[name] = model_config
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False

    def _get_model_class(self, model_class_name: str) -> Type:
        """ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ ë°˜í™˜"""
        model_classes = {
            'GraphonomyModel': GraphonomyModel,
            'OpenPoseModel': OpenPoseModel,
            'U2NetModel': U2NetModel,
            'GeometricMatchingModel': GeometricMatchingModel,
            'HRVITONModel': HRVITONModel,
            'StableDiffusionPipeline': None  # íŠ¹ë³„ ì²˜ë¦¬
        }
        return model_classes.get(model_class_name, None)

    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)

    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ğŸ”¥ ì‹¤ì œ 72GB ëª¨ë¸ ë¡œë“œ - ìë™ íƒì§€ í†µí•© ë²„ì „"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ì‹¤ì œ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.error(f"âŒ ë“±ë¡ë˜ì§€ ì•Šì€ ì‹¤ì œ ëª¨ë¸: {name}")
                    # ì‹¤ì‹œê°„ ê²½ë¡œ íƒì§€ ì‹œë„
                    actual_path = find_actual_checkpoint_path(name)
                    if actual_path:
                        model_config = self._create_model_config_from_actual_path(name, actual_path)
                        if model_config:
                            self.register_model(name, model_config)
                        else:
                            raise ValueError(f"Model {name} config creation failed")
                    else:
                        raise ValueError(f"Model {name} not found")
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ ì‹¤ì œ 72GB ëª¨ë¸ ë¡œë”© ì‹œì‘: {name} ({model_config.model_type.value})")
                self.logger.info(f"   ê²½ë¡œ: {model_config.checkpoint_path}")
                
                # ìë™ íƒì§€ ì—¬ë¶€ ë¡œê¹…
                if model_config.metadata.get("auto_detected", False):
                    confidence = model_config.metadata.get("confidence_score", 0)
                    self.logger.info(f"   ğŸ¤– ìë™ íƒì§€ ëª¨ë¸ (ì‹ ë¢°ë„: {confidence:.2f})")
                else:
                    self.logger.info(f"   ğŸ“ ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ ëª¨ë¸")
                
                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_actual_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    raise RuntimeError(f"Failed to create actual model {name}")
                
                # ğŸ”¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                await self._load_actual_checkpoint(model, model_config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                file_size = Path(model_config.checkpoint_path).stat().st_size / (1024**2)
                self.logger.info(f"âœ… ì‹¤ì œ 72GB ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({file_size:.1f}MB, {load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            raise

    async def _create_actual_model_instance(
        self,
        model_config: ModelConfig,
        **kwargs
    ) -> Optional[Any]:
        """ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ì™„ì „ ìƒˆë¡œìš´ êµ¬í˜„"""
        try:
            model_class = model_config.model_class
            
            self.logger.info(f"ğŸ—ï¸ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±: {model_class}")
            
            if model_class == "GraphonomyModel":
                return GraphonomyModel(
                    num_classes=model_config.num_classes or 20,
                    backbone=model_config.metadata.get('backbone', 'resnet101'),
                    pretrained=model_config.metadata.get('pretrained', True)
                )
            
            elif model_class == "OpenPoseModel":
                return OpenPoseModel(
                    num_keypoints=model_config.num_classes or 18,
                    num_pafs=model_config.metadata.get('num_pafs', 38)
                )
            
            elif model_class == "U2NetModel":
                return U2NetModel(
                    in_ch=3, 
                    out_ch=model_config.metadata.get('output_channels', 1)
                )
            
            elif model_class == "GeometricMatchingModel":
                return GeometricMatchingModel(
                    feature_size=model_config.metadata.get('feature_size', 256),
                    num_control_points=model_config.metadata.get('control_points', 18)
                )
            
            elif model_class == "HRVITONModel":
                return HRVITONModel(
                    input_nc=3, 
                    output_nc=3, 
                    ngf=64,
                    n_blocks=model_config.metadata.get('blocks', 9)
                )
            
            elif model_class == "StableDiffusionPipeline":
                return await self._create_actual_diffusion_model(model_config)
            
            else:
                self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹¤ì œ ëª¨ë¸ í´ë˜ìŠ¤: {model_class}")
                raise ValueError(f"Unsupported actual model class: {model_class}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _create_actual_diffusion_model(self, model_config: ModelConfig):
        """ì‹¤ì œ Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                checkpoint_path = Path(model_config.checkpoint_path)
                
                if checkpoint_path.exists():
                    # ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ê²½ìš°
                    if checkpoint_path.is_file():
                        # Hugging Face ë³€í™˜ í•„ìš”
                        self.logger.info(f"ğŸ”„ ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ ë³€í™˜ ì¤‘: {checkpoint_path}")
                        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³€í™˜ ë¡œì§ ì¶”ê°€ í•„ìš”
                        pipeline = None  # ì„ì‹œ
                    else:
                        # ë””ë ‰í† ë¦¬ êµ¬ì¡°ì˜ ê²½ìš°
                        pipeline = StableDiffusionPipeline.from_pretrained(
                            str(checkpoint_path),
                            torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                            safety_checker=None,
                            requires_safety_checker=False
                        )
                else:
                    self.logger.error(f"âŒ ì‹¤ì œ Diffusion ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {checkpoint_path}")
                    raise FileNotFoundError(f"Actual diffusion checkpoint not found: {checkpoint_path}")
                
                return pipeline
            else:
                self.logger.error("âŒ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                raise ImportError("diffusers library is required")
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _load_actual_checkpoint(self, model: Any, model_config: ModelConfig):
        """ğŸ”¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ - ì™„ì „ ìƒˆë¡œìš´ êµ¬í˜„"""
        if not model_config.checkpoint_path:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {model_config.name}")
            return
            
        checkpoint_path = Path(model_config.checkpoint_path)
        
        if not checkpoint_path.exists():
            self.logger.error(f"âŒ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {checkpoint_path}")
            raise FileNotFoundError(f"Actual checkpoint file not found: {checkpoint_path}")
        
        try:
            self.logger.info(f"ğŸ“¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
            file_size = checkpoint_path.stat().st_size / (1024**2)  # MB
            self.logger.info(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
            
            # PyTorch ëª¨ë¸ì¸ ê²½ìš°
            if hasattr(model, 'load_state_dict'):
                
                # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë¡œë“œ ë°©ì‹ ê²°ì •
                if checkpoint_path.suffix == '.pkl':
                    # Detectron2 í˜•ì‹ (DensePose ë“±)
                    import pickle
                    with open(checkpoint_path, 'rb') as f:
                        state_dict = pickle.load(f)
                    if isinstance(state_dict, dict) and 'model' in state_dict:
                        state_dict = state_dict['model']
                        
                elif checkpoint_path.suffix == '.safetensors':
                    # SafeTensors í˜•ì‹
                    try:
                        from safetensors.torch import load_file
                        state_dict = load_file(checkpoint_path)
                    except ImportError:
                        self.logger.warning("SafeTensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, PyTorchë¡œ ëŒ€ì²´ ì‹œë„")
                        state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                        
                else:
                    # í‘œì¤€ PyTorch í˜•ì‹ (.pth, .pt, .bin)
                    state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict ì •ë¦¬
                if isinstance(state_dict, dict):
                    if 'state_dict' in state_dict:
                        state_dict = state_dict['state_dict']
                    elif 'model' in state_dict:
                        state_dict = state_dict['model']
                    elif 'model_state_dict' in state_dict:
                        state_dict = state_dict['model_state_dict']
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                # ëª¨ë¸ í¬ê¸°ì™€ state_dict í¬ê¸° ë¹„êµ
                model_params = sum(p.numel() for p in model.parameters())
                state_dict_params = sum(v.numel() if torch.is_tensor(v) else 0 for v in cleaned_state_dict.values())
                
                self.logger.info(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {model_params:,}")
                self.logger.info(f"   ì²´í¬í¬ì¸íŠ¸ íŒŒë¼ë¯¸í„°: {state_dict_params:,}")
                
                # strict=Falseë¡œ ë¡œë“œ (ì¼ë¶€ ë¶ˆì¼ì¹˜ í—ˆìš©)
                missing_keys, unexpected_keys = model.load_state_dict(cleaned_state_dict, strict=False)
                
                if missing_keys:
                    self.logger.warning(f"âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                    if len(missing_keys) <= 5:  # 5ê°œ ì´í•˜ì¼ ë•Œë§Œ ì¶œë ¥
                        for key in missing_keys[:5]:
                            self.logger.warning(f"   - {key}")
                
                if unexpected_keys:
                    self.logger.warning(f"âš ï¸ ì˜ˆìƒí•˜ì§€ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                    if len(unexpected_keys) <= 5:  # 5ê°œ ì´í•˜ì¼ ë•Œë§Œ ì¶œë ¥
                        for key in unexpected_keys[:5]:
                            self.logger.warning(f"   - {key}")
                
                self.logger.info(f"âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
            
            else:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê±´ë„ˆëœ€ (íŒŒì´í”„ë¼ì¸): {model_config.name}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ëŠ” ë°˜í™˜ (ë¹ˆ ê°€ì¤‘ì¹˜ë¡œë¼ë„ ì‘ë™ ê°€ëŠ¥)
            self.logger.warning(f"âš ï¸ ë¹ˆ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ì‚¬ìš©: {model_config.name}")

    async def _apply_m3_max_optimization(self, model: Any, model_config: ModelConfig) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            optimizations_applied = []
            
            # 1. MPS ë””ë°”ì´ìŠ¤ ìµœì í™”
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # 3. CoreML ì»´íŒŒì¼ ì¤€ë¹„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if (COREML_AVAILABLE and 
                hasattr(model, 'eval') and 
                model_config.model_type in [ModelType.HUMAN_PARSING, ModelType.CLOTH_SEGMENTATION]):
                optimizations_applied.append("CoreML compilation ready")
            
            # 4. Metal Performance Shaders ìµœì í™”
            if self.device == 'mps':
                try:
                    # PyTorch MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ì‹¤ì œ ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì‹¤ì œ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
            if self.memory_manager.check_memory_pressure():
                await self._cleanup_least_used_models()
            
            # ìºì‹œëœ ëª¨ë¸ ìˆ˜ í™•ì¸
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                # ì‚¬ìš© ë¹ˆë„ì™€ ìµœê·¼ ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    # ëª¨ë¸ í•´ì œ
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                    
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ì‹¤ì œ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def unload_model(self, name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            with self._lock:
                # ìºì‹œì—ì„œ ì œê±°
                keys_to_remove = [k for k in self.model_cache.keys() 
                                 if k.startswith(f"{name}_")]
                
                removed_count = 0
                for key in keys_to_remove:
                    if key in self.model_cache:
                        model = self.model_cache[key]
                        
                        # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                        del self.model_cache[key]
                        removed_count += 1
                    
                    self.access_counts.pop(key, None)
                    self.load_times.pop(key, None)
                    self.last_access.pop(key, None)
                
                if removed_count > 0:
                    self.logger.info(f"ğŸ—‘ï¸ ì‹¤ì œ ëª¨ë¸ ì–¸ë¡œë“œ: {name} ({removed_count}ê°œ ì¸ìŠ¤í„´ìŠ¤)")
                    self.memory_manager.cleanup_memory()
                    return True
                else:
                    self.logger.warning(f"ì–¸ë¡œë“œí•  ì‹¤ì œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {name}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
            return False

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ - ìë™ íƒì§€ ì •ë³´ í¬í•¨"""
        with self._lock:
            if name not in self.model_configs:
                return None
                
            config = self.model_configs[name]
            cache_keys = [k for k in self.model_cache.keys() if k.startswith(f"{name}_")]
            
            # ì‹¤ì œ íŒŒì¼ ì •ë³´ ì¶”ê°€
            actual_file_info = {}
            if config.checkpoint_path and Path(config.checkpoint_path).exists():
                checkpoint_path = Path(config.checkpoint_path)
                actual_file_info = {
                    "file_exists": True,
                    "file_size_mb": checkpoint_path.stat().st_size / (1024**2),
                    "file_modified": checkpoint_path.stat().st_mtime,
                    "file_extension": checkpoint_path.suffix
                }
            else:
                actual_file_info = {"file_exists": False}
            
            # ìë™ íƒì§€ ì •ë³´ ì¶”ê°€
            auto_detection_info = {}
            if config.metadata.get("auto_detected", False):
                auto_detection_info = {
                    "auto_detected": True,
                    "confidence_score": config.metadata.get("confidence_score", 0),
                    "alternative_paths": config.metadata.get("alternative_paths", [])
                }
            else:
                auto_detection_info = {
                    "auto_detected": False,
                    "fallback_registration": config.metadata.get("fallback_registration", False)
                }
            
            return {
                "name": name,
                "model_type": config.model_type.value,
                "model_class": config.model_class,
                "device": config.device,
                "loaded": len(cache_keys) > 0,
                "cache_instances": len(cache_keys),
                "total_access_count": sum(self.access_counts.get(k, 0) for k in cache_keys),
                "average_load_time": sum(self.load_times.get(k, 0) for k in cache_keys) / max(1, len(cache_keys)),
                "checkpoint_path": config.checkpoint_path,
                "input_size": config.input_size,
                "last_access": max((self.last_access.get(k, 0) for k in cache_keys), default=0),
                "metadata": config.metadata,
                **actual_file_info,  # ì‹¤ì œ íŒŒì¼ ì •ë³´ í¬í•¨
                **auto_detection_info  # ìë™ íƒì§€ ì •ë³´ í¬í•¨
            }

    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        with self._lock:
            result = {}
            for name in self.model_configs.keys():
                info = self.get_model_info(name)
                if info:
                    result[name] = info
            return result

    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ - ìë™ íƒì§€ ì •ë³´ í¬í•¨"""
        try:
            # ìë™ íƒì§€ëœ ëª¨ë¸ ìˆ˜ ê³„ì‚°
            auto_detected_count = sum(1 for config in self.model_configs.values() 
                                    if config.metadata.get("auto_detected", False))
            fallback_count = sum(1 for config in self.model_configs.values() 
                               if config.metadata.get("fallback_registration", False))
            
            usage = {
                "loaded_models": len(self.model_cache),
                "device": self.device,
                "available_memory_gb": self.memory_manager.get_available_memory(),
                "memory_pressure": self.memory_manager.check_memory_pressure(),
                "memory_limit_gb": self.memory_gb,
                "total_models_registered": len(self.model_configs),
                "auto_detected_models": auto_detected_count,
                "fallback_registered_models": fallback_count,
                "models_with_actual_files": sum(1 for config in self.model_configs.values() 
                                               if config.checkpoint_path and Path(config.checkpoint_path).exists()),
                "auto_detection_enabled": self.enable_auto_detection and AUTO_DETECTOR_AVAILABLE
            }
            
            if self.device == "cuda" and torch.cuda.is_available():
                usage.update({
                    "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                    "cached_gb": torch.cuda.memory_reserved() / 1024**3
                })
            elif self.device == "mps":
                try:
                    import psutil
                    process = psutil.Process()
                    usage.update({
                        "process_memory_gb": process.memory_info().rss / 1024**3,
                        "system_memory_percent": psutil.virtual_memory().percent
                    })
                except ImportError:
                    usage["memory_info"] = "psutil not available"
            
            return usage
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    interface = self.step_interfaces[step_name]
                    interface.unload_models()
                    del self.step_interfaces[step_name]
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.warning(f"ì‹¤ì œ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ì‹¤ì œ ModelLoader + ìë™ íƒì§€ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì‹¤ì œ ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def initialize(self) -> bool:
        """ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” - ìë™ íƒì§€ í†µí•© ë²„ì „"""
        try:
            self.logger.info("ğŸš€ ì‹¤ì œ 72GB ëª¨ë¸ ë¡œë” + ìë™ íƒì§€ í†µí•© ì´ˆê¸°í™” ì¤‘...")
            
            # ì‹¤ì œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
            missing_checkpoints = []
            available_checkpoints = []
            auto_detected_checkpoints = []
            
            for name, config in self.model_configs.items():
                if config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if checkpoint_path.exists():
                        file_size = checkpoint_path.stat().st_size / (1024**2)
                        if config.metadata.get("auto_detected", False):
                            auto_detected_checkpoints.append((name, file_size))
                            confidence = config.metadata.get("confidence_score", 0)
                            self.logger.info(f"   ğŸ¤– {name}: {file_size:.1f}MB (ìë™ íƒì§€, ì‹ ë¢°ë„: {confidence:.2f})")
                        else:
                            available_checkpoints.append((name, file_size))
                            self.logger.info(f"   ğŸ“ {name}: {file_size:.1f}MB (ê¸°ë³¸ ê²½ë¡œ)")
                    else:
                        missing_checkpoints.append(name)
                        self.logger.warning(f"   âŒ {name}: íŒŒì¼ ì—†ìŒ")
            
            total_models = len(self.model_configs)
            available_count = len(available_checkpoints) + len(auto_detected_checkpoints)
            
            if available_count == 0:
                self.logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                self.logger.error("   ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ê³  ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”")
                return False
            
            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (available_count / total_models * 100) if total_models > 0 else 0
            total_size = sum(size for _, size in available_checkpoints + auto_detected_checkpoints)
            
            self.logger.info(f"ğŸ“Š ì‹¤ì œ ëª¨ë¸ + ìë™ íƒì§€ ì´ˆê¸°í™” ê²°ê³¼:")
            self.logger.info(f"   âœ… ì‚¬ìš© ê°€ëŠ¥: {available_count}/{total_models} ({success_rate:.1f}%)")
            self.logger.info(f"   ğŸ¤– ìë™ íƒì§€: {len(auto_detected_checkpoints)}ê°œ")
            self.logger.info(f"   ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {len(available_checkpoints)}ê°œ")
            self.logger.info(f"   ğŸ’¾ ì´ í¬ê¸°: {total_size:.1f}MB ({total_size/1024:.1f}GB)")
            
            if missing_checkpoints:
                self.logger.warning(f"   âŒ ëˆ„ë½ëœ ëª¨ë¸: {missing_checkpoints}")
            
            # ìë™ íƒì§€ ì‹œìŠ¤í…œ ìƒíƒœ
            if AUTO_DETECTOR_AVAILABLE and self.enable_auto_detection:
                self.logger.info("ğŸ¤– AutoModelDetector í™œì„±í™”ë¨")
            else:
                self.logger.info("ğŸ“ ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜ ëª¨ë“œ")
            
            # M3 Max ìµœì í™” ì„¤ì •
            if COREML_AVAILABLE and self.is_m3_max:
                self.logger.info("ğŸ CoreML ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
            self.logger.info(f"âœ… ì‹¤ì œ 72GB AI ëª¨ë¸ ë¡œë” + ìë™ íƒì§€ ì´ˆê¸°í™” ì™„ë£Œ - {available_count}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def __del__(self):
        """ì†Œë©¸ì (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                # ì „ì—­ ëª¨ë¸ ë¡œë” ì‚¬ìš©
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ì‹¤ì œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ì‹¤ì œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ì‹¤ì œ ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.error(f"âŒ {self.__class__.__name__} ì‹¤ì œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                return await self.model_interface.get_model(model_name)
            else:
                # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                return await self.model_interface.get_recommended_model()
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ì‹¤ì œ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ëª¨ë¸ ë¡œë” ê´€ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None

@lru_cache(maxsize=1)
def get_global_model_loader() -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    try:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader()
        return _global_model_loader
    except Exception as e:
        logger.error(f"ì „ì—­ ì‹¤ì œ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"Failed to create global actual ModelLoader: {e}")

def cleanup_global_loader():
    """ì „ì—­ ë¡œë” ì •ë¦¬"""
    global _global_model_loader
    
    try:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("âœ… ì „ì—­ ì‹¤ì œ ModelLoader + ìë™ íƒì§€ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì „ì—­ ì‹¤ì œ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================

def preprocess_image(image: Union[np.ndarray, Image.Image], target_size: tuple, normalize: bool = True) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV and PIL are required")
            
        if isinstance(image, np.ndarray):
            if image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            elif len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image).astype(np.float32)
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1) / 255.0
        
        # ì •ê·œí™”
        if normalize:
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            image_tensor = (image_tensor - mean) / std
        
        return image_tensor.unsqueeze(0)
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_segmentation(output: torch.Tensor, original_size: tuple, threshold: float = 0.5) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV is required")
            
        if output.dim() == 4:
            output = output.squeeze(0)
        
        # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
        if output.shape[0] > 1:
            output = torch.argmax(output, dim=0)
        else:
            output = (output > threshold).float()
        
        # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
        output = output.cpu().numpy().astype(np.uint8)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if output.shape != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        return output
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_pose(output: torch.Tensor, original_size: tuple, confidence_threshold: float = 0.3) -> Dict[str, Any]:
    """í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬"""
    try:
        if isinstance(output, (list, tuple)):
            # OpenPose ìŠ¤íƒ€ì¼ ì¶œë ¥ (PAF, heatmaps)
            pafs, heatmaps = output[-1]  # ë§ˆì§€ë§‰ ìŠ¤í…Œì´ì§€ ê²°ê³¼ ì‚¬ìš©
        else:
            heatmaps = output
            pafs = None
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = []
        if heatmaps.dim() == 4:
            heatmaps = heatmaps.squeeze(0)
        
        for i in range(heatmaps.shape[0] - 1):  # ë°°ê²½ ì œì™¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = heatmap[y, x]
            
            if confidence > confidence_threshold:
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                x_scaled = int(x * original_size[0] / heatmap.shape[1])
                y_scaled = int(y * original_size[1] / heatmap.shape[0])
                keypoints.append([x_scaled, y_scaled, confidence])
            else:
                keypoints.append([0, 0, 0])
        
        return {
            'keypoints': keypoints,
            'pafs': pafs.cpu().numpy() if pafs is not None else None,
            'heatmaps': heatmaps.cpu().numpy()
        }
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_model_loader(device: str = "mps", use_fp16: bool = True, **kwargs) -> ModelLoader:
    """ì‹¤ì œ ëª¨ë¸ ë¡œë” ìƒì„±"""
    return ModelLoader(device=device, use_fp16=use_fp16, **kwargs)

async def load_model_async(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì‹¤ì œ ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        return await loader.load_model(model_name, config)
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def load_model_sync(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë™ê¸° ì‹¤ì œ ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(loader.load_model(model_name, config))
    except Exception as e:
        logger.error(f"ë™ê¸° ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

# ğŸ”¥ ì´ˆê¸°í™” í•¨ìˆ˜ - ì‹¤ì œ 72GB ëª¨ë¸ + ìë™ íƒì§€ í†µí•© ë²„ì „
def initialize_global_model_loader(
    device: str = "mps",
    memory_gb: float = 128.0,
    optimization_enabled: bool = True,
    enable_auto_detection: bool = True,
    **kwargs
) -> Dict[str, Any]:
    """
    ì „ì—­ ì‹¤ì œ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” - 72GB ëª¨ë¸ ì—°ê²° + ìë™ íƒì§€ í†µí•© ë²„ì „
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (mps, cuda, cpu)
        memory_gb: ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
        optimization_enabled: ìµœì í™” í™œì„±í™” ì—¬ë¶€
        enable_auto_detection: ìë™ ëª¨ë¸ íƒì§€ í™œì„±í™” ì—¬ë¶€
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        Dict[str, Any]: ì´ˆê¸°í™”ëœ ë¡œë” ì„¤ì •
    """
    try:
        logger.info(f"ğŸš€ ì‹¤ì œ 72GB ModelLoader + ìë™ íƒì§€ í†µí•© ì´ˆê¸°í™”: {device}, {memory_gb}GB")
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì‚¬ì „ í…ŒìŠ¤íŠ¸
        auto_detection_status = {
            "available": AUTO_DETECTOR_AVAILABLE,
            "enabled": enable_auto_detection,
            "models_detected": 0
        }
        
        if enable_auto_detection and AUTO_DETECTOR_AVAILABLE:
            try:
                logger.info("ğŸ¤– ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ ì‚¬ì „ í…ŒìŠ¤íŠ¸...")
                detector = create_auto_detector()
                detected_models = detector.detect_all_models()
                auto_detection_status["models_detected"] = len(detected_models)
                logger.info(f"âœ… ìë™ íƒì§€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸ ë°œê²¬")
            except Exception as e:
                logger.warning(f"âš ï¸ ìë™ íƒì§€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                auto_detection_status["test_error"] = str(e)
        
        # ì‹¤ì œ ëª¨ë¸ ê°€ìš©ì„± ê²€ì¦ (ê¸°ë³¸ ê²½ë¡œ)
        model_availability = validate_model_availability()
        available_count = sum(model_availability.values())
        total_count = len(model_availability)
        
        if available_count == 0 and auto_detection_status["models_detected"] == 0:
            logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            logger.error("   ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”")
            return {"error": "No actual models available"}
        
        # ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë” ì„¤ì •
        loader_config = {
            "device": device,
            "memory_gb": memory_gb,
            "optimization_enabled": optimization_enabled,
            "enable_auto_detection": enable_auto_detection,
            "cache_enabled": True,
            "lazy_loading": True,
            "memory_efficient": True,
            "production_mode": True,
            "actual_models_available": available_count,
            "actual_models_total": total_count,
            "actual_models_success_rate": (available_count / total_count * 100) if total_count > 0 else 0,
            "auto_detection_status": auto_detection_status
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if device == "mps":
            is_m3_max = memory_gb >= 64
            loader_config.update({
                "use_neural_engine": True,
                "use_unified_memory": True,
                "optimization_level": "maximum" if is_m3_max else "balanced",
                "coreml_enabled": COREML_AVAILABLE,
                "batch_size": 4 if is_m3_max else 2,
                "precision": "float16",
                "memory_pooling": True,
                "actual_model_optimization": "m3_max" if is_m3_max else "standard"
            })
            
            if is_m3_max:
                loader_config.update({
                    "m3_max_actual_optimizations": {
                        "neural_engine": True,
                        "metal_shaders": True,
                        "unified_memory": True,
                        "pipeline_parallel": True,
                        "memory_bandwidth": "400GB/s",
                        "actual_model_cache": "aggressive",
                        "auto_detection_enhanced": enable_auto_detection
                    }
                })
        
        elif device == "cuda":
            loader_config.update({
                "mixed_precision": optimization_enabled,
                "tensorrt_enabled": False,  # ì‹¤ì œ ëª¨ë¸ì—ì„œëŠ” ì•ˆì •ì„± ìš°ì„ 
                "batch_size": 8,
                "memory_growth": True,
                "actual_model_optimization": "cuda",
                "auto_detection_gpu": enable_auto_detection
            })
        
        else:  # CPU
            loader_config.update({
                "num_threads": os.cpu_count() or 4,
                "batch_size": 1,
                "memory_mapping": True,
                "actual_model_optimization": "cpu",
                "auto_detection_cpu": enable_auto_detection
            })
        
        # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        actual_model_paths = {
            "base_dir": Path("backend/ai_models"),
            "checkpoints_dir": Path("backend/ai_models/checkpoints"),
            "cache_dir": Path("backend/app/ai_pipeline/cache"),
            "temp_dir": Path("backend/ai_models/temp")
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for path in actual_model_paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        loader_config["actual_paths"] = {str(k): str(v) for k, v in actual_model_paths.items()}
        
        # ì‹¤ì œ ëª¨ë¸ ì •ë³´ ì¶”ê°€ (ê¸°ë³¸ ê²½ë¡œ ê¸°ë°˜)
        loader_config["actual_model_info"] = {}
        for model_name, is_available in model_availability.items():
            if is_available:
                actual_path = find_actual_checkpoint_path(model_name)
                if actual_path:
                    file_size = Path(actual_path).stat().st_size / (1024**2)
                    loader_config["actual_model_info"][model_name] = {
                        "path": actual_path,
                        "size_mb": file_size,
                        "available": True,
                        "detection_method": "static_mapping"
                    }
        
        # ìë™ íƒì§€ ê²°ê³¼ ì¶”ê°€
        if auto_detection_status["models_detected"] > 0:
            loader_config["auto_detected_model_info"] = {
                "count": auto_detection_status["models_detected"],
                "available": True,
                "detection_method": "auto_detector"
            }
        
        logger.info(f"âœ… ì‹¤ì œ 72GB ModelLoader + ìë™ íƒì§€ í†µí•© ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ê¸°ë³¸ ê²½ë¡œ ëª¨ë¸: {available_count}/{total_count}")
        logger.info(f"   ìë™ íƒì§€ ëª¨ë¸: {auto_detection_status['models_detected']}ê°œ")
        logger.info(f"   ìë™ íƒì§€ ì‹œìŠ¤í…œ: {'í™œì„±í™”' if enable_auto_detection and AUTO_DETECTOR_AVAILABLE else 'ë¹„í™œì„±í™”'}")
        
        return loader_config
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ 72GB ModelLoader + ìë™ íƒì§€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ìë™ íƒì§€ í†µí•©ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def enable_auto_detection_mode():
    """ìë™ íƒì§€ ëª¨ë“œ í™œì„±í™”"""
    global _global_model_loader
    
    if not AUTO_DETECTOR_AVAILABLE:
        logger.warning("âš ï¸ AutoModelDetectorê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        return False
    
    try:
        if _global_model_loader:
            _global_model_loader.enable_auto_detection = True
            _global_model_loader.detection_force_rescan = True
            logger.info("ğŸ¤– ìë™ íƒì§€ ëª¨ë“œ í™œì„±í™”ë¨")
            return True
        else:
            logger.warning("âš ï¸ ì „ì—­ ModelLoaderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return False
    except Exception as e:
        logger.error(f"âŒ ìë™ íƒì§€ ëª¨ë“œ í™œì„±í™” ì‹¤íŒ¨: {e}")
        return False

def disable_auto_detection_mode():
    """ìë™ íƒì§€ ëª¨ë“œ ë¹„í™œì„±í™”"""
    global _global_model_loader
    
    try:
        if _global_model_loader:
            _global_model_loader.enable_auto_detection = False
            logger.info("ğŸ“ ê¸°ë³¸ ê²½ë¡œ ëª¨ë“œë¡œ ì „í™˜ë¨")
            return True
        else:
            logger.warning("âš ï¸ ì „ì—­ ModelLoaderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return False
    except Exception as e:
        logger.error(f"âŒ ìë™ íƒì§€ ëª¨ë“œ ë¹„í™œì„±í™” ì‹¤íŒ¨: {e}")
        return False

def force_model_rescan():
    """ê°•ì œ ëª¨ë¸ ì¬ìŠ¤ìº”"""
    global _global_model_loader
    
    try:
        if _global_model_loader and AUTO_DETECTOR_AVAILABLE:
            _global_model_loader.detection_force_rescan = True
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¬ì´ˆê¸°í™”
            _global_model_loader._initialize_enhanced_model_registry()
            logger.info("ğŸ”„ ëª¨ë¸ ê°•ì œ ì¬ìŠ¤ìº” ì™„ë£Œ")
            return True
        else:
            logger.warning("âš ï¸ ì¬ìŠ¤ìº” ë¶ˆê°€ëŠ¥ (ModelLoader ë¯¸ì´ˆê¸°í™” ë˜ëŠ” AutoDetector ë¯¸ì‚¬ìš©)")
            return False
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê°•ì œ ì¬ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        return False

def get_detection_summary() -> Dict[str, Any]:
    """ëª¨ë¸ íƒì§€ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
    try:
        loader = get_global_model_loader()
        
        # ê¸°ë³¸ ì •ë³´
        models_info = loader.list_models()
        auto_detected = sum(1 for info in models_info.values() if info.get("auto_detected", False))
        fallback_registered = sum(1 for info in models_info.values() if info.get("fallback_registration", False))
        
        summary = {
            "total_models": len(models_info),
            "auto_detected_models": auto_detected,
            "fallback_registered_models": fallback_registered,
            "auto_detection_available": AUTO_DETECTOR_AVAILABLE,
            "auto_detection_enabled": getattr(loader, 'enable_auto_detection', False),
            "models_by_detection_method": {
                "auto_detected": auto_detected,
                "static_mapping": fallback_registered,
                "unknown": len(models_info) - auto_detected - fallback_registered
            }
        }
        
        # ìë™ íƒì§€ ì‹œìŠ¤í…œì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€ ì •ë³´
        if AUTO_DETECTOR_AVAILABLE:
            try:
                detector = create_auto_detector()
                detection_stats = detector.scan_stats
                summary["detection_stats"] = detection_stats
            except Exception as e:
                summary["detection_error"] = str(e)
        
        return summary
        
    except Exception as e:
        logger.error(f"âŒ íƒì§€ ìš”ì•½ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelFormat',
    'ModelConfig', 
    'ModelType',
    'ModelMemoryManager',
    'ModelRegistry',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    'RSU7', 'RSU6', 'RSU5', 'RSU4', 'RSU4F', 'REBNCONV',
    'ResnetBlock',
    
    # ğŸ”¥ ìë™ íƒì§€ í†µí•© í•¨ìˆ˜ë“¤
    'get_actual_model_paths',
    'find_actual_checkpoint_path',
    'validate_model_availability',
    'DEFAULT_ACTUAL_MODEL_PATHS',
    
    # ìë™ íƒì§€ ì œì–´ í•¨ìˆ˜ë“¤
    'enable_auto_detection_mode',
    'disable_auto_detection_mode', 
    'force_model_rescan',
    'get_detection_summary',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'load_model_async',
    'load_model_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'postprocess_pose',
    'cleanup_global_loader',
    'initialize_global_model_loader'
]

# ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

logger.info("âœ… ì‹¤ì œ 72GB ëª¨ë¸ ì—°ê²° + ìë™ íƒì§€ í†µí•© ì™„ë£Œ - Enhanced ModelLoader ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - Step í´ë˜ìŠ¤ ì™„ë²½ ì—°ë™")