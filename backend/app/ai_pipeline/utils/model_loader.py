# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ AI ì¶”ë¡  ê¸°ë°˜ ModelLoader v5.1 (torch ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
================================================================================
âœ… torch ì´ˆê¸°í™” ë¬¸ì œ ì™„ì „ í•´ê²° - 'NoneType' object has no attribute 'Tensor' í•´ê²°
âœ… ì‹¤ì œ 229GB AI ëª¨ë¸ì„ AI í´ë˜ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì™„ì „í•œ ì¶”ë¡  ì‹¤í–‰
âœ… auto_model_detector.pyì™€ ì™„ë²½ ì—°ë™ (integrate_auto_detector ë©”ì„œë“œ ì¶”ê°€)
âœ… BaseStepMixinê³¼ 100% í˜¸í™˜ë˜ëŠ” ì‹¤ì œ AI ëª¨ë¸ ì œê³µ
âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ AI í´ë˜ìŠ¤ ìë™ ë³€í™˜
âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”
âœ… í¬ê¸° ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë™ì  ë¡œë”© (RealVisXL 6.6GB, CLIP 5.2GB ë“±)
âœ… ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ ë‚´ì¥ (ëª©ì—…/ê°€ìƒ ëª¨ë¸ ì™„ì „ ì œê±°)
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€
================================================================================

Author: MyCloset AI Team
Date: 2025-07-25
Version: 5.1 (torch ì˜¤ë¥˜ ì™„ì „ í•´ê²° + AutoDetector ì™„ì „ ì—°ë™)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
import importlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager, contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod
import sys

# ==============================================
# ğŸ”¥ 1. ì•ˆì „í•œ PyTorch Import (torch ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
# ==============================================

# í™˜ê²½ ìµœì í™” ë¨¼ì € ì„¤ì •
os.environ.update({
    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
    'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
    'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1',
    'OMP_NUM_THREADS': '16',
    'MKL_NUM_THREADS': '16'
})

# ê¸€ë¡œë²Œ ìƒìˆ˜ ì´ˆê¸°í™”
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
CUDA_AVAILABLE = False
NUMPY_AVAILABLE = False
PIL_AVAILABLE = False
CV2_AVAILABLE = False
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = "none"

# torch ë³€ìˆ˜ë¥¼ Noneìœ¼ë¡œ ëª…ì‹œì  ì´ˆê¸°í™”
torch = None
nn = None
F = None

try:
    # PyTorch import ì‹œë„
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    # torchê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œëëŠ”ì§€ í™•ì¸
    if torch is not None and hasattr(torch, 'Tensor'):
        TORCH_AVAILABLE = True
        
        # ë””ë°”ì´ìŠ¤ ì§€ì› í™•ì¸
        if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
            if torch.backends.mps.is_available():
                MPS_AVAILABLE = True
                DEFAULT_DEVICE = "mps"
        
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            CUDA_AVAILABLE = True
            if DEFAULT_DEVICE == "cpu":
                DEFAULT_DEVICE = "cuda"
        
        # M3 Max ê°ì§€
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                IS_M3_MAX = 'M3' in result.stdout
        except:
            pass
        
        logging.getLogger(__name__).info(f"âœ… PyTorch {torch.__version__} ë¡œë“œ ì„±ê³µ (MPS: {MPS_AVAILABLE}, CUDA: {CUDA_AVAILABLE})")
    else:
        raise ImportError("torch ëª¨ë“ˆì´ None ë˜ëŠ” Tensor ì†ì„± ì—†ìŒ")
        
except ImportError as e:
    logging.getLogger(__name__).error(f"âŒ PyTorch import ì‹¤íŒ¨: {e}")
    
    # ì•ˆì „í•œ ë”ë¯¸ torch ê°ì²´ ìƒì„±
    class DummyTensor:
        pass
    
    class DummyNN:
        class Module:
            def __init__(self):
                pass
            def to(self, device):
                return self
            def eval(self):
                return self
        
        class Conv2d(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        class BatchNorm2d(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        class ReLU(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        class Sequential(Module):
            def __init__(self, *args):
                super().__init__()
        
        class Linear(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        class TransformerEncoder(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        class TransformerEncoderLayer(Module):
            def __init__(self, *args, **kwargs):
                super().__init__()
        
        Parameter = lambda x: x
    
    class DummyF:
        @staticmethod
        def interpolate(*args, **kwargs):
            return None
        
        @staticmethod
        def conv2d(*args, **kwargs):
            return None
        
        @staticmethod
        def max_pool2d(*args, **kwargs):
            return None
        
        @staticmethod
        def normalize(*args, **kwargs):
            return None
        
        @staticmethod
        def softmax(*args, **kwargs):
            return None
    
    class DummyTorch:
        Tensor = DummyTensor
        
        @staticmethod
        def load(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def from_numpy(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def randn(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def tensor(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def no_grad():
            class NoGrad:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    pass
            return NoGrad()
        
        @staticmethod
        def cat(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def argmax(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def clamp(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def norm(*args, **kwargs):
            raise RuntimeError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        class backends:
            class mps:
                @staticmethod
                def is_available():
                    return False
        
        class cuda:
            @staticmethod
            def is_available():
                return False
            
            @staticmethod
            def empty_cache():
                pass
    
    # ë”ë¯¸ ê°ì²´ë“¤ í• ë‹¹
    torch = DummyTorch()
    nn = DummyNN()
    F = DummyF()

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì•ˆì „ import
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    class DummyNumpy:
        @staticmethod
        def array(*args, **kwargs):
            raise ImportError("NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        @staticmethod
        def zeros(*args, **kwargs):
            raise ImportError("NumPyê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        ndarray = object
    
    np = DummyNumpy()

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    class DummyImage:
        @staticmethod
        def open(*args, **kwargs):
            raise ImportError("PILì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    Image = DummyImage()

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# conda í™˜ê²½ ê°ì§€
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')

# TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# auto_model_detector import
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector, DetectedModel
    AUTO_DETECTOR_AVAILABLE = True
    logger.info("âœ… auto_model_detector import ì„±ê³µ")
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False
    logger.warning("âš ï¸ auto_model_detector import ì‹¤íŒ¨")
    
    class DetectedModel:
        def __init__(self, **kwargs):
            self.__dict__.update(kwargs)


# ìœ„ì¹˜ 1 (ê¶Œì¥): backend/app/ai_pipeline/utils/model_loader.py
# íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€ (import ì„¹ì…˜ ë‹¤ìŒ)

# =============================================================================
# ğŸ”¥ ëˆ„ë½ëœ ëª¨ë¸ ë§¤í•‘ ì¶”ê°€ (ì›Œë‹ í•´ê²°)
# =============================================================================

# ëˆ„ë½ëœ ëª¨ë¸ë“¤ì„ ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§¤í•‘
MISSING_MODEL_MAPPING = {
    # Step 05 ClothWarping ëˆ„ë½ ëª¨ë¸ë“¤
    'realvis_xl': 'step_05_cloth_warping/RealVisXL_V4.0.safetensors',
    'vgg16_warping': 'step_05_cloth_warping/vgg16_warping.pth',
    'vgg19_warping': 'step_05_cloth_warping/vgg19_warping.pth', 
    'densenet121': 'step_05_cloth_warping/densenet121_warping.pth',
    
    # Step 07 PostProcessing ëˆ„ë½ ëª¨ë¸ë“¤
    'post_processing_model': 'step_07_post_processing/sr_model.pth',
    'super_resolution': 'step_07_post_processing/Real-ESRGAN_x4plus.pth',
    
    # Step 08 QualityAssessment ëˆ„ë½ ëª¨ë¸ë“¤
    'clip_vit_large': 'step_08_quality_assessment/ViT-L-14.pt',
    'quality_assessment': 'step_08_quality_assessment/quality_model.pth',
    
    # ê³µìœ  ëª¨ë¸ë“¤ (ì—¬ëŸ¬ Stepì—ì„œ ì‚¬ìš©)
    'sam_vit_h': 'step_04_geometric_matching/sam_vit_h_4b8939.pth',
    'vit_large_patch14': 'step_08_quality_assessment/ViT-L-14.pt',
}

def resolve_missing_model_path(model_name: str, ai_models_root: str) -> Optional[str]:
    """ëˆ„ë½ëœ ëª¨ë¸ì˜ ì‹¤ì œ ê²½ë¡œ ì°¾ê¸°"""
    try:
        from pathlib import Path
        import logging
        
        logger = logging.getLogger(__name__)
        
        # 1. ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì°¾ê¸°
        if model_name in MISSING_MODEL_MAPPING:
            mapped_path = Path(ai_models_root) / MISSING_MODEL_MAPPING[model_name]
            if mapped_path.exists():
                logger.info(f"âœ… ëˆ„ë½ ëª¨ë¸ í•´ê²°: {model_name} â†’ {mapped_path}")
                return str(mapped_path)
        
        # 2. ë™ì  ê²€ìƒ‰ (íŒŒì¼ëª… ê¸°ë°˜)
        search_patterns = [
            f"**/{model_name}.pth",
            f"**/{model_name}.safetensors", 
            f"**/{model_name}.pt",
            f"**/{model_name}.bin",
            f"**/model.safetensors",
            f"**/pytorch_model.bin",
        ]
        
        ai_models_path = Path(ai_models_root)
        for pattern in search_patterns:
            for found_path in ai_models_path.glob(pattern):
                if found_path.is_file() and found_path.stat().st_size > 50 * 1024 * 1024:  # 50MB ì´ìƒ
                    logger.info(f"âœ… ë™ì  ê²€ìƒ‰ ì„±ê³µ: {model_name} â†’ {found_path}")
                    return str(found_path)
        
        logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ í•´ê²° ì‹¤íŒ¨: {model_name}")
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ í•´ê²° ì‹¤íŒ¨ ({model_name}): {e}")
        return None

# ModelLoader í´ë˜ìŠ¤ ë‚´ë¶€ì— ì´ ë©”ì„œë“œ ì¶”ê°€
def load_model_with_fallback(self, model_name: str, **kwargs):
    """ëˆ„ë½ëœ ëª¨ë¸ì— ëŒ€í•œ í´ë°± ì²˜ë¦¬ (ModelLoader í´ë˜ìŠ¤ ë‚´ë¶€)"""
    try:
        # ê¸°ë³¸ ë¡œë”© ì‹œë„
        return self.load_model(model_name, **kwargs)
        
    except Exception as e:
        self.logger.warning(f"âš ï¸ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
        
        # ëˆ„ë½ëœ ëª¨ë¸ ê²½ë¡œ í•´ê²° ì‹œë„
        resolved_path = resolve_missing_model_path(model_name, str(self.model_cache_dir))
        
        if resolved_path:
            self.logger.info(f"ğŸ”„ í´ë°± ê²½ë¡œë¡œ ì¬ì‹œë„: {model_name}")
            # ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•´ì„œ ë¡œë”© ì‹œë„
            return self.load_model_from_path(resolved_path, **kwargs)
        else:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {model_name}")
            raise

# ============================================================================= 
# ìœ„ì¹˜ 2 (ëŒ€ì•ˆ): backend/app/core/config.py
# ì „ì—­ ì„¤ì •ìœ¼ë¡œ ì¶”ê°€í•˜ê³  ì‹¶ë‹¤ë©´ ì´ íŒŒì¼ì— ë„£ì–´ë„ ë©ë‹ˆë‹¤

# ============================================================================= 
# ìœ„ì¹˜ 3 (Stepë³„): ê° Step íŒŒì¼ì— ê°œë³„ ì¶”ê°€
# ì˜ˆ: backend/app/ai_pipeline/steps/step_05_cloth_warping.py
# ê° Stepì—ì„œ í•„ìš”í•œ ëª¨ë¸ë§Œ ê°œë³„ì ìœ¼ë¡œ ë§¤í•‘
# ==============================================
# ğŸ”¥ 2. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (torch ì•ˆì „ ì²˜ë¦¬)
# ==============================================

class BaseRealAIModel(ABC):
    """ì‹¤ì œ AI ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤ (torch ì•ˆì „ ì²˜ë¦¬)"""
    
    def __init__(self, checkpoint_path: str, device: str = "auto"):
        self.checkpoint_path = Path(checkpoint_path)
        self.device = self._resolve_device(device)
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        self.load_time = 0.0
        self.memory_usage_mb = 0.0

        # torch ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.torch_available = TORCH_AVAILABLE and torch is not None
        self._setup_mps_safety()

    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device == "auto":
            return DEFAULT_DEVICE
        return device
    
    @abstractmethod
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë”© (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    @abstractmethod
    def predict(self, *args, **kwargs) -> Any:
        """AI ì¶”ë¡  (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.model is not None:
            del self.model
            self.model = None
            self.loaded = False
            
            if self.torch_available:
                try:
                    if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif self.device == "mps" and MPS_AVAILABLE:
                        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                except Exception as e:
                    self.logger.debug(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            gc.collect()
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "class_name": self.__class__.__name__,
            "checkpoint_path": str(self.checkpoint_path),
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "torch_available": self.torch_available,
            "file_size_mb": self.checkpoint_path.stat().st_size / (1024 * 1024) if self.checkpoint_path.exists() else 0
        }

    def _setup_mps_safety(self):
        """MPS ì•ˆì „ ì²˜ë¦¬ ì„¤ì •"""
        try:
            if self.device == "mps" and self.torch_available:
                # MPS í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                import os
                os.environ.update({
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                    'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
                    'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1'
                })
                
                # MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì¬í™•ì¸
                if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    self.logger.warning("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í´ë°±")
                    self.device = "cpu"
                else:
                    self.logger.info(f"âœ… MPS ì•ˆì „ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ MPS ì•ˆì „ ì²˜ë¦¬ ì‹¤íŒ¨ - CPUë¡œ í´ë°±: {e}")
            self.device = "cpu"

    def _safe_load_checkpoint(self, checkpoint_path: Path) -> Optional[Any]:
        """MPS ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            self.logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘: {checkpoint_path}")
            
            # ğŸ”¥ MPS ì•ˆì „ ë¡œë”© ì „ëµ
            if self.device == "mps":
                # 1ë‹¨ê³„: CPUë¡œ ë¨¼ì € ë¡œë”©
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                self.logger.debug("âœ… CPU ë¡œë”© ì™„ë£Œ")
                
                # 2ë‹¨ê³„: ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
                checkpoint = self._convert_to_mps_compatible(checkpoint)
                
                return checkpoint
            else:
                # ì¼ë°˜ ë¡œë”©
                return torch.load(checkpoint_path, map_location=self.device)
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # CPU í´ë°± ì‹œë„
            try:
                self.logger.info("ğŸ”„ CPU í´ë°± ì‹œë„...")
                self.device = "cpu"
                return torch.load(checkpoint_path, map_location='cpu')
            except Exception as fallback_error:
                self.logger.error(f"âŒ CPU í´ë°±ë„ ì‹¤íŒ¨: {fallback_error}")
                return None

    def _convert_to_mps_compatible(self, checkpoint: Any) -> Any:
        """MPS í˜¸í™˜ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        try:
            if isinstance(checkpoint, dict):
                converted = {}
                for key, value in checkpoint.items():
                    if isinstance(value, torch.Tensor):
                        # float64 â†’ float32 ë³€í™˜ (MPSëŠ” float64 ë¯¸ì§€ì›)
                        if value.dtype == torch.float64:
                            converted[key] = value.to(torch.float32)
                            self.logger.debug(f"âœ… {key}: float64 â†’ float32 ë³€í™˜")
                        # int64 â†’ int32 ë³€í™˜ (ì•ˆì „ì„±)
                        elif value.dtype == torch.int64:
                            converted[key] = value.to(torch.int32)
                            self.logger.debug(f"âœ… {key}: int64 â†’ int32 ë³€í™˜")
                        else:
                            converted[key] = value
                    elif isinstance(value, dict):
                        converted[key] = self._convert_to_mps_compatible(value)
                    else:
                        converted[key] = value
                return converted
            elif isinstance(checkpoint, torch.Tensor):
                # ë‹¨ì¼ í…ì„œì¸ ê²½ìš°
                if checkpoint.dtype == torch.float64:
                    return checkpoint.to(torch.float32)
                elif checkpoint.dtype == torch.int64:
                    return checkpoint.to(torch.int32)
                return checkpoint
            else:
                return checkpoint
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ MPS í˜¸í™˜ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return checkpoint

class RealGraphonomyModel(BaseRealAIModel):
    """ì‹¤ì œ Graphonomy Human Parsing ëª¨ë¸ (1.2GB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """Graphonomy ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  Graphonomy ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            # Graphonomy ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”ëœ ë²„ì „)
            class GraphonomyNetwork(nn.Module):
                def __init__(self, num_classes=20):
                    super().__init__()
                    # ResNet ë°±ë³¸ (ê°„ì†Œí™”)
                    self.backbone = nn.Sequential(
                        nn.Conv2d(3, 64, 7, 2, 3),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        # ê°„ì†Œí™”ëœ ë ˆì´ì–´ë“¤
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 512, 3, 1, 1),
                        nn.BatchNorm2d(512),
                        nn.ReLU(inplace=True)
                    )
                    
                    # ìµœì¢… ë¶„ë¥˜ê¸°
                    self.classifier = nn.Conv2d(512, num_classes, 1)
                    
                def forward(self, x):
                    x = self.backbone(x)
                    x = self.classifier(x)
                    return F.interpolate(x, size=(512, 512), mode='bilinear', align_corners=True)
            
            # ëª¨ë¸ ìƒì„± ë° ë¡œë”©
            self.model = GraphonomyNetwork()
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ state_dict ì¶”ì¶œ
            if isinstance(checkpoint, dict):
                if 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # í‚¤ ì´ë¦„ ë§¤í•‘ (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                self.model.load_state_dict(state_dict, strict=False)
                self.logger.info("âœ… state_dict ë¡œë”© ì„±ê³µ (strict=False)")
            except Exception as e:
                self.logger.warning(f"âš ï¸ state_dict ë¡œë”© ì‹¤íŒ¨, í˜¸í™˜ ë ˆì´ì–´ë§Œ ì‚¬ìš©: {e}")
                # í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë”©
                model_dict = self.model.state_dict()
                pretrained_dict = {}
                for k, v in state_dict.items():
                    if k in model_dict and model_dict[k].shape == v.shape:
                        pretrained_dict[k] = v
                model_dict.update(pretrained_dict)
                self.model.load_state_dict(model_dict)
                self.logger.info(f"âœ… í˜¸í™˜ ë ˆì´ì–´ {len(pretrained_dict)}ê°œ ë¡œë”© ì™„ë£Œ")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… Graphonomy ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ, {self.memory_usage_mb:.1f}MB)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """Human Parsing ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    # numpy â†’ tensor
                    image_tensor = torch.from_numpy(image).float()
                    if image_tensor.dim() == 3:
                        image_tensor = image_tensor.unsqueeze(0)  # batch ì°¨ì› ì¶”ê°€
                    if image_tensor.shape[1] != 3:  # HWC â†’ CHW
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                # ì •ê·œí™”
                image_tensor = image_tensor / 255.0
                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
                image_tensor = (image_tensor - mean) / std
                
                # í¬ê¸° ì¡°ì •
                image_tensor = F.interpolate(image_tensor, size=(512, 512), mode='bilinear', align_corners=True)
                image_tensor = image_tensor.to(self.device)
                
                # ì¶”ë¡  ì‹¤í–‰
                output = self.model(image_tensor)
                
                # í›„ì²˜ë¦¬
                prediction = torch.argmax(output, dim=1).squeeze().cpu().numpy()
                confidence = torch.softmax(output, dim=1).max(dim=1)[0].squeeze().cpu().numpy()
                
                return {
                    "success": True,
                    "parsing_map": prediction,
                    "confidence": float(confidence.mean()) if hasattr(confidence, 'mean') else 0.8,
                    "num_classes": output.shape[1],
                    "output_shape": prediction.shape,
                    "device": self.device,
                    "inference_time": time.time()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 1200.0  # ê¸°ë³¸ ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)  # 4ë°”ì´íŠ¸(float32) â†’ MB
        except:
            return 1200.0

class RealSAMModel(BaseRealAIModel):
    """ì‹¤ì œ SAM (Segment Anything Model) í´ë˜ìŠ¤ (2.4GB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """SAM ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  SAM ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # SAM ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”ëœ ë²„ì „)
            class SAMNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ê°„ì†Œí™”ëœ ì´ë¯¸ì§€ ì¸ì½”ë”
                    self.image_encoder = nn.Sequential(
                        nn.Conv2d(3, 64, 16, 16),  # Patch embedding
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 512, 3, 1, 1),
                        nn.BatchNorm2d(512),
                        nn.ReLU(inplace=True)
                    )
                    
                    # ë§ˆìŠ¤í¬ ë””ì½”ë” (ê°„ì†Œí™”)
                    self.mask_decoder = nn.Sequential(
                        nn.Conv2d(512, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 1, 1)
                    )
                
                def forward(self, x):
                    # ì´ë¯¸ì§€ ì¸ì½”ë”©
                    features = self.image_encoder(x)
                    
                    # ë§ˆìŠ¤í¬ ìƒì„±
                    mask = self.mask_decoder(features)
                    mask = F.interpolate(mask, size=(1024, 1024), mode='bilinear', align_corners=True)
                    
                    return torch.sigmoid(mask)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            except Exception as e:
                self.logger.error(f"âŒ SAM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            self.model = SAMNetwork()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            if isinstance(checkpoint, dict) and 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint
            
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ SAM state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                # í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë”©
                pass
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… SAM ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"], prompts: Optional[List] = None) -> Dict[str, Any]:
        """Cloth Segmentation ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float().unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                image_tensor = image_tensor / 255.0
                image_tensor = F.interpolate(image_tensor, size=(1024, 1024), mode='bilinear')
                image_tensor = image_tensor.to(self.device)
                
                # SAM ì¶”ë¡ 
                mask = self.model(image_tensor)
                
                # í›„ì²˜ë¦¬
                mask_binary = (mask > 0.5).float()
                confidence = mask.mean().item()
                
                result_mask = mask_binary.squeeze().cpu().numpy() if NUMPY_AVAILABLE else None
                
                return {
                    "success": True,
                    "mask": result_mask,
                    "confidence": confidence,
                    "output_shape": mask.shape,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ SAM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 2400.0
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)
        except:
            return 2400.0

class RealVisXLModel(BaseRealAIModel):
    """ì‹¤ì œ RealVis XL Cloth Warping ëª¨ë¸ (6.6GB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """RealVis XL ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - RealVis XL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  RealVis XL ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # RealVis XL ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”ëœ U-Net)
            class RealVisXLNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ê°„ì†Œí™”ëœ ì¸ì½”ë”
                    self.encoder = nn.Sequential(
                        nn.Conv2d(3, 64, 3, 1, 1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 128, 3, 2, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 2, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 512, 3, 2, 1),
                        nn.BatchNorm2d(512),
                        nn.ReLU(inplace=True)
                    )
                    
                    # ê°„ì†Œí™”ëœ ë””ì½”ë”
                    self.decoder = nn.Sequential(
                        nn.ConvTranspose2d(512, 256, 4, 2, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(256, 128, 4, 2, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(128, 64, 4, 2, 1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 3, 1)
                    )
                
                def forward(self, x):
                    # ì¸ì½”ë”
                    encoded = self.encoder(x)
                    
                    # ë””ì½”ë”
                    decoded = self.decoder(encoded)
                    
                    # ìµœì¢… ì¶œë ¥
                    output = torch.tanh(decoded)
                    return output
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (.safetensors ì§€ì›)
            try:
                if self.checkpoint_path.suffix == '.safetensors':
                    try:
                        from safetensors.torch import load_file
                        state_dict = load_file(str(self.checkpoint_path), device=self.device)
                    except ImportError:
                        self.logger.warning("âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, torch.load ì‚¬ìš©")
                        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
                        state_dict = checkpoint
                else:
                    checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
                    if isinstance(checkpoint, dict):
                        state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
                    else:
                        state_dict = checkpoint
            except Exception as e:
                self.logger.error(f"âŒ RealVis XL ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            self.model = RealVisXLNetwork()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ RealVis XL state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                # ëŒ€í˜• ëª¨ë¸ì´ë¯€ë¡œ í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë”©
                pass
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… RealVis XL ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ, {self.memory_usage_mb:.1f}MB)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ RealVis XL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, person_image: Union[np.ndarray, "torch.Tensor"], 
                garment_image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """Cloth Warping ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                def preprocess_image(img):
                    if isinstance(img, np.ndarray) and NUMPY_AVAILABLE:
                        img_tensor = torch.from_numpy(img).float()
                        if img_tensor.dim() == 3:
                            img_tensor = img_tensor.unsqueeze(0)
                        if img_tensor.shape[1] != 3:
                            img_tensor = img_tensor.permute(0, 3, 1, 2)
                    else:
                        img_tensor = img
                    
                    img_tensor = img_tensor / 255.0
                    img_tensor = F.interpolate(img_tensor, size=(512, 512), mode='bilinear')
                    return img_tensor.to(self.device)
                
                person_tensor = preprocess_image(person_image)
                
                # Cloth Warping ì¶”ë¡  (person ì´ë¯¸ì§€ë§Œ ì‚¬ìš©)
                warped_result = self.model(person_tensor)
                
                # í›„ì²˜ë¦¬
                output = (warped_result + 1) / 2  # tanh â†’ [0,1]
                output = torch.clamp(output, 0, 1)
                
                result_image = output.squeeze().cpu().numpy() if NUMPY_AVAILABLE else None
                
                return {
                    "success": True,
                    "warped_image": result_image,
                    "output_shape": output.shape,
                    "device": self.device,
                    "model_size": "6.6GB"
                }
                
        except Exception as e:
            self.logger.error(f"âŒ RealVis XL ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 6600.0  # 6.6GB ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)  # ëŒ€í˜• ëª¨ë¸ì´ë¯€ë¡œ ì •í™•í•œ ì¶”ì •
        except:
            return 6600.0

class RealOOTDDiffusionModel(BaseRealAIModel):
    """ì‹¤ì œ OOTD Diffusion Virtual Fitting ëª¨ë¸ (3.2GB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """OOTD Diffusion ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - OOTD Diffusion ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  OOTD Diffusion ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # OOTD Diffusion U-Net êµ¬ì¡° (ê°„ì†Œí™”)
            class OOTDDiffusionUNet(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ê°„ì†Œí™”ëœ ë‹¤ìš´ìƒ˜í”Œë§
                    self.down_blocks = nn.Sequential(
                        nn.Conv2d(4, 64, 3, 1, 1),   # input + noise
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 128, 3, 2, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 2, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 512, 3, 2, 1),
                        nn.ReLU(inplace=True)
                    )
                    
                    # ê°„ì†Œí™”ëœ ì—…ìƒ˜í”Œë§
                    self.up_blocks = nn.Sequential(
                        nn.ConvTranspose2d(512, 256, 4, 2, 1),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(256, 128, 4, 2, 1),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(128, 64, 4, 2, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 3, 3, 1, 1)
                    )
                    
                def forward(self, x, timestep=None):
                    # ë‹¤ìš´ìƒ˜í”Œë§
                    x = self.down_blocks(x)
                    
                    # ì—…ìƒ˜í”Œë§
                    x = self.up_blocks(x)
                    
                    return x
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                if self.checkpoint_path.suffix == '.safetensors':
                    try:
                        from safetensors.torch import load_file
                        state_dict = load_file(str(self.checkpoint_path), device=self.device)
                    except ImportError:
                        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
                        state_dict = checkpoint
                else:
                    checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
                    state_dict = checkpoint
            except Exception as e:
                self.logger.error(f"âŒ OOTD Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            self.model = OOTDDiffusionUNet()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ OOTD Diffusion state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                # í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë”©
                pass
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… OOTD Diffusion ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OOTD Diffusion ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, person_image: Union[np.ndarray, "torch.Tensor"], 
                garment_image: Union[np.ndarray, "torch.Tensor"],
                num_steps: int = 10) -> Dict[str, Any]:
        """Virtual Fitting ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                def preprocess_image(img):
                    if isinstance(img, np.ndarray) and NUMPY_AVAILABLE:
                        img_tensor = torch.from_numpy(img).float()
                        if img_tensor.dim() == 3:
                            img_tensor = img_tensor.unsqueeze(0)
                        if img_tensor.shape[1] != 3:
                            img_tensor = img_tensor.permute(0, 3, 1, 2)
                    else:
                        img_tensor = img
                    
                    img_tensor = (img_tensor / 255.0) * 2 - 1  # [-1, 1] ì •ê·œí™”
                    img_tensor = F.interpolate(img_tensor, size=(512, 512), mode='bilinear')
                    return img_tensor.to(self.device)
                
                person_tensor = preprocess_image(person_image)
                
                # ë…¸ì´ì¦ˆ ì´ˆê¸°í™”
                noise = torch.randn_like(person_tensor)
                
                # ê°„ì†Œí™”ëœ Diffusion í”„ë¡œì„¸ìŠ¤
                x = person_tensor
                for step in range(min(num_steps, 5)):  # ìµœëŒ€ 5ìŠ¤í…ìœ¼ë¡œ ì œí•œ
                    # ì¡°ê±´ ì…ë ¥ ê²°í•©
                    model_input = torch.cat([x, noise], dim=1)
                    
                    # U-Net ì¶”ë¡ 
                    noise_pred = self.model(model_input)
                    
                    # ë…¸ì´ì¦ˆ ì œê±° (ê°„ì†Œí™”)
                    alpha = 1 - step / num_steps
                    x = alpha * x + (1 - alpha) * noise_pred
                
                # í›„ì²˜ë¦¬
                output = (x + 1) / 2  # [-1,1] â†’ [0,1]
                output = torch.clamp(output, 0, 1)
                
                result_image = output.squeeze().cpu().numpy() if NUMPY_AVAILABLE else None
                
                return {
                    "success": True,
                    "fitted_image": result_image,
                    "output_shape": output.shape,
                    "num_steps": num_steps,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ OOTD Diffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 3200.0  # 3.2GB ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)
        except:
            return 3200.0

class RealCLIPModel(BaseRealAIModel):
    """ì‹¤ì œ CLIP Quality Assessment ëª¨ë¸ (5.2GB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """CLIP ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - CLIP ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  CLIP ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # CLIP êµ¬ì¡° (ê°„ì†Œí™”ëœ ViT)
            class CLIPVisionModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ê°„ì†Œí™”ëœ Vision Transformer
                    self.patch_embedding = nn.Conv2d(3, 768, 16, 16)  # íŒ¨ì¹˜ ì„ë² ë”©
                    self.pos_embedding = nn.Parameter(torch.randn(1, 197, 768))  # 14x14 + cls
                    
                    # ê°„ì†Œí™”ëœ Transformer ë ˆì´ì–´
                    encoder_layer = nn.TransformerEncoderLayer(
                        d_model=768, nhead=12, dim_feedforward=3072, batch_first=True
                    )
                    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=12)
                    
                    # Projection head
                    self.projection = nn.Linear(768, 512)
                    
                def forward(self, x):
                    # Patch embedding
                    x = self.patch_embedding(x)  # (B, 768, 14, 14)
                    x = x.flatten(2).transpose(1, 2)  # (B, 196, 768)
                    
                    # Add position embedding
                    cls_token = torch.zeros(x.shape[0], 1, 768, device=x.device)
                    x = torch.cat([cls_token, x], dim=1)  # (B, 197, 768)
                    x = x + self.pos_embedding
                    
                    # Transformer
                    x = self.transformer(x)
                    
                    # Use class token for representation
                    cls_output = x[:, 0]  # (B, 768)
                    
                    # Project to common space
                    features = self.projection(cls_output)  # (B, 512)
                    features = F.normalize(features, dim=-1)
                    
                    return features
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            except Exception as e:
                self.logger.error(f"âŒ CLIP ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            if isinstance(checkpoint, dict):
                if 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'visual' in checkpoint:
                    state_dict = checkpoint['visual']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            self.model = CLIPVisionModel()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ CLIP state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
                # CLIPì€ ë³µì¡í•˜ë¯€ë¡œ í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë”©
                pass
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… CLIP ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CLIP ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """Quality Assessment ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float()
                    if image_tensor.dim() == 3:
                        image_tensor = image_tensor.unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                # CLIP ì •ê·œí™”
                image_tensor = image_tensor / 255.0
                mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1)
                std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1)
                image_tensor = (image_tensor - mean) / std
                
                # í¬ê¸° ì¡°ì • (ViTëŠ” 224x224)
                image_tensor = F.interpolate(image_tensor, size=(224, 224), mode='bilinear')
                image_tensor = image_tensor.to(self.device)
                
                # CLIP ì¶”ë¡ 
                features = self.model(image_tensor)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°„ì†Œí™”)
                quality_score = torch.norm(features, dim=-1).mean().item()
                
                # íŠ¹ì„± ë¶„ì„
                feature_stats = {
                    "mean": features.mean().item(),
                    "std": features.std().item(),
                    "max": features.max().item(),
                    "min": features.min().item()
                }
                
                result_features = features.squeeze().cpu().numpy() if NUMPY_AVAILABLE else None
                
                return {
                    "success": True,
                    "quality_score": quality_score,
                    "features": result_features,
                    "feature_stats": feature_stats,
                    "device": self.device,
                    "model_size": "5.2GB"
                }
                
        except Exception as e:
            self.logger.error(f"âŒ CLIP ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 5200.0  # 5.2GB ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)
        except:
            return 5200.0

# model_loader.pyì— ì¶”ê°€í•  AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤

class RealVGGModel(BaseRealAIModel):
    """ì‹¤ì œ VGG Warping ëª¨ë¸ (vgg16, vgg19)"""
    
    def load_model(self) -> bool:
        """VGG ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - VGG ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  VGG ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # VGG ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”)
            class VGGWarpingNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # VGG ë°±ë³¸ (ê°„ì†Œí™”)
                    self.features = nn.Sequential(
                        # Block 1
                        nn.Conv2d(3, 64, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 64, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2, 2),
                        
                        # Block 2
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2, 2),
                        
                        # Block 3
                        nn.Conv2d(128, 256, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(256, 256, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2, 2)
                    )
                    
                    # Warping Head
                    self.warping_head = nn.Sequential(
                        nn.Conv2d(256, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 2, 1)  # x, y displacement
                    )
                
                def forward(self, x):
                    features = self.features(x)
                    warping_field = self.warping_head(features)
                    return warping_field
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = self._safe_load_checkpoint(self.checkpoint_path)
            if checkpoint is None:
                return False
            
            self.model = VGGWarpingNetwork()
            
            # state_dict ë¡œë”©
            if isinstance(checkpoint, dict):
                state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
            else:
                state_dict = checkpoint
            
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ VGG state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… VGG ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ VGG ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, person_image: Union[np.ndarray, "torch.Tensor"], 
                cloth_image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """VGG Warping ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                def preprocess_image(img):
                    if isinstance(img, np.ndarray) and NUMPY_AVAILABLE:
                        img_tensor = torch.from_numpy(img).float()
                        if img_tensor.dim() == 3:
                            img_tensor = img_tensor.unsqueeze(0)
                        if img_tensor.shape[1] != 3:
                            img_tensor = img_tensor.permute(0, 3, 1, 2)
                    else:
                        img_tensor = img
                    
                    img_tensor = img_tensor / 255.0
                    img_tensor = F.interpolate(img_tensor, size=(256, 192), mode='bilinear')
                    return img_tensor.to(self.device)
                
                person_tensor = preprocess_image(person_image)
                
                # VGG Warping ì¶”ë¡ 
                warping_field = self.model(person_tensor)
                
                return {
                    "success": True,
                    "warping_field": warping_field.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ VGG ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        return 500.0  # 500MB ì¶”ì •

class RealDenseNetModel(BaseRealAIModel):
    """ì‹¤ì œ DenseNet ëª¨ë¸"""
    
    def load_model(self) -> bool:
        """DenseNet ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # DenseNet êµ¬ì¡° (ê°„ì†Œí™”)
            class DenseNetWarping(nn.Module):
                def __init__(self):
                    super().__init__()
                    # Dense Block (ê°„ì†Œí™”)
                    self.features = nn.Sequential(
                        nn.Conv2d(3, 64, 7, 2, 3),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(3, 2, 1),
                        
                        # Dense layers (ê°„ì†Œí™”)
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    )
                    
                    self.classifier = nn.Conv2d(256, 2, 1)
                
                def forward(self, x):
                    features = self.features(x)
                    output = self.classifier(features)
                    return output
            
            checkpoint = self._safe_load_checkpoint(self.checkpoint_path)
            if checkpoint is None:
                return False
            
            self.model = DenseNetWarping()
            
            try:
                if isinstance(checkpoint, dict):
                    state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
                else:
                    state_dict = checkpoint
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ DenseNet state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… DenseNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DenseNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """DenseNet ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        try:
            with torch.no_grad():
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float().unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                image_tensor = image_tensor / 255.0
                image_tensor = F.interpolate(image_tensor, size=(224, 224), mode='bilinear')
                image_tensor = image_tensor.to(self.device)
                
                output = self.model(image_tensor)
                
                return {
                    "success": True,
                    "features": output.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ DenseNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        return 120.0  # 120MB

class RealESRGANModel(BaseRealAIModel):
    """ì‹¤ì œ ESRGAN Super Resolution ëª¨ë¸"""
    
    def load_model(self) -> bool:
        """ESRGAN ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ESRGAN êµ¬ì¡° (ê°„ì†Œí™”)
            class ESRGANNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # Generator (ê°„ì†Œí™”)
                    self.conv_first = nn.Conv2d(3, 64, 3, 1, 1)
                    
                    # Residual blocks (ê°„ì†Œí™”)
                    self.trunk_conv = nn.Sequential(
                        nn.Conv2d(64, 64, 3, 1, 1),
                        nn.ReLU(),
                        nn.Conv2d(64, 64, 3, 1, 1),
                        nn.ReLU()
                    )
                    
                    # Upsampling
                    self.upconv1 = nn.Conv2d(64, 256, 3, 1, 1)
                    self.pixel_shuffle1 = nn.PixelShuffle(2)
                    
                    self.upconv2 = nn.Conv2d(64, 256, 3, 1, 1) 
                    self.pixel_shuffle2 = nn.PixelShuffle(2)
                    
                    self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)
                
                def forward(self, x):
                    feat = self.conv_first(x)
                    trunk = self.trunk_conv(feat)
                    feat = feat + trunk
                    
                    # Upsampling
                    feat = self.pixel_shuffle1(self.upconv1(feat))
                    feat = self.pixel_shuffle2(self.upconv2(feat))
                    
                    out = self.conv_last(feat)
                    return out
            
            # íŒŒì¼ ë˜ëŠ” í´ë” ì²˜ë¦¬
            if self.checkpoint_path.is_dir():
                # í´ë”ì¸ ê²½ìš° ë‚´ë¶€ íŒŒì¼ ì°¾ê¸°
                model_files = list(self.checkpoint_path.glob("*.pth"))
                if not model_files:
                    self.logger.warning(f"âš ï¸ í´ë” ë‚´ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.checkpoint_path}")
                    # ë”ë¯¸ ëª¨ë¸ ìƒì„±
                    self.model = ESRGANNetwork()
                else:
                    checkpoint = torch.load(model_files[0], map_location=self.device)
                    self.model = ESRGANNetwork()
                    try:
                        if isinstance(checkpoint, dict):
                            state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
                        else:
                            state_dict = checkpoint
                        self.model.load_state_dict(state_dict, strict=False)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ESRGAN state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            else:
                checkpoint = self._safe_load_checkpoint(self.checkpoint_path)
                if checkpoint is None:
                    return False
                
                self.model = ESRGANNetwork()
                try:
                    if isinstance(checkpoint, dict):
                        state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
                    else:
                        state_dict = checkpoint
                    self.model.load_state_dict(state_dict, strict=False)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ESRGAN state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… ESRGAN ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ESRGAN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """Super Resolution ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        try:
            with torch.no_grad():
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float().unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                image_tensor = image_tensor / 255.0
                image_tensor = image_tensor.to(self.device)
                
                enhanced = self.model(image_tensor)
                enhanced = torch.clamp(enhanced, 0, 1)
                
                return {
                    "success": True,
                    "enhanced_image": enhanced.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ESRGAN ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        return 150.0  # 150MB

# ==============================================
# ğŸ”¥ ì¶”ê°€: ëˆ„ë½ëœ Stepë³„ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (torch ì•ˆì „ ì²˜ë¦¬)
# ==============================================

class RealOpenPoseModel(BaseRealAIModel):
    """ì‹¤ì œ OpenPose ëª¨ë¸ (97.8MB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """OpenPose ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  OpenPose ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”ëœ ë²„ì „)
            class OpenPoseNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # VGG ë°±ë³¸ (ê°„ì†Œí™”)
                    self.backbone = nn.Sequential(
                        nn.Conv2d(3, 64, 3, 1, 1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    )
                    
                    # PAF (Part Affinity Fields) ë¸Œëœì¹˜
                    self.paf_branch = nn.Sequential(
                        nn.Conv2d(256, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 38, 1)  # 19 connections * 2
                    )
                    
                    # í‚¤í¬ì¸íŠ¸ ë¸Œëœì¹˜
                    self.keypoint_branch = nn.Sequential(
                        nn.Conv2d(256, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 19, 1)  # 18 keypoints + background
                    )
                
                def forward(self, x):
                    features = self.backbone(x)
                    paf_output = self.paf_branch(features)
                    keypoint_output = self.keypoint_branch(features)
                    return paf_output, keypoint_output
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                # ğŸ”¥ MPS ì•ˆì „ ë¡œë”©
                checkpoint = self._safe_load_checkpoint(self.checkpoint_path)
                if checkpoint is None:
                    self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: ì•ˆì „ ë¡œë”© ì‹¤íŒ¨")
                    return False
            except Exception as e:
                self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False


            self.model = OpenPoseNetwork()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            if isinstance(checkpoint, dict):
                state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
            else:
                state_dict = checkpoint
            
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ OpenPose state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… OpenPose ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float()
                    if image_tensor.dim() == 3:
                        image_tensor = image_tensor.unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                # ì •ê·œí™”
                image_tensor = image_tensor / 255.0
                image_tensor = F.interpolate(image_tensor, size=(368, 368), mode='bilinear')
                image_tensor = image_tensor.to(self.device)
                
                # OpenPose ì¶”ë¡ 
                paf_output, keypoint_output = self.model(image_tensor)
                
                # í›„ì²˜ë¦¬ (ê°„ì†Œí™”ëœ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ)
                keypoints = torch.argmax(keypoint_output, dim=1).float()
                confidence = torch.softmax(keypoint_output, dim=1).max(dim=1)[0]
                
                return {
                    "success": True,
                    "keypoints": keypoints.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "confidence": confidence.mean().item(),
                    "paf_output": paf_output.shape,
                    "keypoint_output": keypoint_output.shape,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 97.8  # ê¸°ë³¸ ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)  # 4ë°”ì´íŠ¸(float32) â†’ MB
        except:
            return 97.8

class RealGMMModel(BaseRealAIModel):
    """ì‹¤ì œ GMM (Geometric Matching Module) ëª¨ë¸ - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """GMM ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            if not self.checkpoint_path.exists():
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {self.checkpoint_path}")
                return False
            
            self.logger.info(f"ğŸ§  GMM ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # GMM ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ê°„ì†Œí™”ëœ ë²„ì „)
            class GMMNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    # íŠ¹ì§• ì¶”ì¶œê¸°
                    self.feature_extractor = nn.Sequential(
                        nn.Conv2d(3, 64, 3, 1, 1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, 3, 1, 1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True)
                    )
                    
                    # ê¸°í•˜í•™ì  ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬
                    self.matching_network = nn.Sequential(
                        nn.Conv2d(256, 128, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 64, 3, 1, 1),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(64, 2, 1)  # x, y ì¢Œí‘œ
                    )
                
                def forward(self, person_img, cloth_img):
                    # íŠ¹ì§• ì¶”ì¶œ
                    person_features = self.feature_extractor(person_img)
                    cloth_features = self.feature_extractor(cloth_img)
                    
                    # íŠ¹ì§• ê²°í•©
                    combined_features = person_features + cloth_features
                    
                    # ê¸°í•˜í•™ì  ë³€í™˜ ì¶”ì •
                    transform_params = self.matching_network(combined_features)
                    
                    return transform_params
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            try:
                # ğŸ”¥ MPS ì•ˆì „ ë¡œë”©
                checkpoint = self._safe_load_checkpoint(self.checkpoint_path)
                if checkpoint is None:
                    self.logger.error(f"âŒ GMM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: ì•ˆì „ ë¡œë”© ì‹¤íŒ¨")
                    return False
            except Exception as e:
                self.logger.error(f"âŒ GMM ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False


            self.model = GMMNetwork()
            
            # state_dict ë¡œë”© (í˜¸í™˜ì„± ì²˜ë¦¬)
            if isinstance(checkpoint, dict):
                state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
            else:
                state_dict = checkpoint
            
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                self.logger.warning(f"âš ï¸ GMM state_dict ë¡œë”© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… GMM ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ GMM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, person_image: Union[np.ndarray, "torch.Tensor"], 
                cloth_image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ì¶”ë¡ """
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        if not self.torch_available:
            return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            with torch.no_grad():
                # ì…ë ¥ ì „ì²˜ë¦¬
                def preprocess_image(img):
                    if isinstance(img, np.ndarray) and NUMPY_AVAILABLE:
                        img_tensor = torch.from_numpy(img).float()
                        if img_tensor.dim() == 3:
                            img_tensor = img_tensor.unsqueeze(0)
                        if img_tensor.shape[1] != 3:
                            img_tensor = img_tensor.permute(0, 3, 1, 2)
                    else:
                        img_tensor = img
                    
                    img_tensor = img_tensor / 255.0
                    img_tensor = F.interpolate(img_tensor, size=(256, 192), mode='bilinear')
                    return img_tensor.to(self.device)
                
                person_tensor = preprocess_image(person_image)
                cloth_tensor = preprocess_image(cloth_image)
                
                # GMM ì¶”ë¡ 
                transform_params = self.model(person_tensor, cloth_tensor)
                
                # í›„ì²˜ë¦¬
                transform_grid = F.interpolate(transform_params, size=(256, 192), mode='bilinear')
                
                return {
                    "success": True,
                    "transform_params": transform_params.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "transform_grid": transform_grid.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "output_shape": transform_params.shape,
                    "device": self.device
                }
                
        except Exception as e:
            self.logger.error(f"âŒ GMM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        if not self.torch_available or not self.model:
            return 250.0  # ê¸°ë³¸ ì¶”ì •ê°’
        
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            return total_params * 4 / (1024 * 1024)
        except:
            return 250.0

# ì¶”ê°€ë¡œ í•„ìš”í•œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë„ ê°™ì€ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥
class RealYOLOv8PoseModel(BaseRealAIModel):
    """ì‹¤ì œ YOLOv8 Pose ëª¨ë¸ (6.5MB) - torch ì•ˆì „ ì²˜ë¦¬"""
    
    def load_model(self) -> bool:
        """YOLOv8 ëª¨ë¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            if not self.torch_available:
                self.logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"ğŸ§  YOLOv8 Pose ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.checkpoint_path}")
            
            # YOLOv8 êµ¬ì¡° (ë§¤ìš° ê°„ì†Œí™”)
            class YOLOv8PoseNetwork(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.backbone = nn.Sequential(
                        nn.Conv2d(3, 32, 6, 2, 2),
                        nn.BatchNorm2d(32),
                        nn.ReLU(),
                        nn.Conv2d(32, 64, 3, 2, 1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(),
                        nn.Conv2d(64, 128, 3, 1, 1),
                        nn.BatchNorm2d(128),
                        nn.ReLU()
                    )
                    
                    self.pose_head = nn.Conv2d(128, 51, 1)  # 17 keypoints * 3 (x,y,conf)
                
                def forward(self, x):
                    features = self.backbone(x)
                    pose_output = self.pose_head(features)
                    return pose_output
            
            self.model = YOLOv8PoseNetwork()
            self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.load_time = time.time() - start_time
            self.memory_usage_mb = self._estimate_memory_usage()
            
            self.logger.info(f"âœ… YOLOv8 Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({self.load_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, image: Union[np.ndarray, "torch.Tensor"]) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ì¶”ì •"""
        if not self.loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        try:
            with torch.no_grad():
                # ê°„ì†Œí™”ëœ ì¶”ë¡ 
                if isinstance(image, np.ndarray) and NUMPY_AVAILABLE:
                    image_tensor = torch.from_numpy(image).float().unsqueeze(0)
                    if image_tensor.shape[1] != 3:
                        image_tensor = image_tensor.permute(0, 3, 1, 2)
                else:
                    image_tensor = image
                
                image_tensor = image_tensor / 255.0
                image_tensor = F.interpolate(image_tensor, size=(640, 640), mode='bilinear')
                image_tensor = image_tensor.to(self.device)
                
                pose_output = self.model(image_tensor)
                
                return {
                    "success": True,
                    "poses": pose_output.cpu().numpy() if NUMPY_AVAILABLE else None,
                    "device": self.device,
                    "model_type": "yolov8_pose"
                }
                
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _estimate_memory_usage(self) -> float:
        return 6.5  # 6.5MB
# ==============================================
# ğŸ”¥ 3. ì‹¤ì œ AI ëª¨ë¸ íŒ©í† ë¦¬ (torch ì•ˆì „ ì²˜ë¦¬)
# ==============================================

class RealAIModelFactory:
    """ì‹¤ì œ AI ëª¨ë¸ íŒ©í† ë¦¬ (torch ì•ˆì „ ì²˜ë¦¬)"""
    
    MODEL_CLASSES = {
        "RealGraphonomyModel": RealGraphonomyModel,
        "RealSAMModel": RealSAMModel,
        "RealVisXLModel": RealVisXLModel,
        "RealOOTDDiffusionModel": RealOOTDDiffusionModel,
        "RealCLIPModel": RealCLIPModel,

        # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë¸ë“¤
        "RealOpenPoseModel": RealOpenPoseModel,
        "RealGMMModel": RealGMMModel,
        "RealYOLOv8PoseModel": RealYOLOv8PoseModel,
        "RealVGGModel": RealVGGModel,           # VGG16, VGG19 warping
        "RealDenseNetModel": RealDenseNetModel, # DenseNet121
        "RealESRGANModel": RealESRGANModel,     # ESRGAN
       
        # ì¶”ê°€ ëª¨ë¸ë“¤ (ë³„ì¹­)
        "RealSCHPModel": RealGraphonomyModel,  # SCHPëŠ” Graphonomyì™€ ìœ ì‚¬
        "RealU2NetModel": RealSAMModel,        # U2Netì€ SAMê³¼ ìœ ì‚¬
        "RealTextEncoderModel": RealCLIPModel, # TextEncoderëŠ” CLIPê³¼ ìœ ì‚¬
        "RealViTLargeModel": RealCLIPModel,    # ViT-LargeëŠ” CLIPê³¼ ìœ ì‚¬
        "RealGFPGANModel": RealCLIPModel,      # GFPGANì€ CLIPê³¼ ìœ ì‚¬
        "RealESRGANModel": RealCLIPModel,      # ESRGANì€ CLIPê³¼ ìœ ì‚¬
        "BaseRealAIModel": BaseRealAIModel     # ê¸°ë³¸ ëª¨ë¸
    }
    
    @classmethod
    def create_model(cls, ai_class: str, checkpoint_path: str, device: str = "auto") -> Optional[BaseRealAIModel]:
        """AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±"""
        try:
            if not TORCH_AVAILABLE:
                logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜")
                return BaseRealAIModel(checkpoint_path, device)
            
            if ai_class in cls.MODEL_CLASSES:
                model_class = cls.MODEL_CLASSES[ai_class]
                return model_class(checkpoint_path, device)
            else:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” AI í´ë˜ìŠ¤: {ai_class} â†’ BaseRealAIModel ì‚¬ìš©")
                return BaseRealAIModel(checkpoint_path, device)
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ {ai_class}: {e}")
            return None

# ==============================================
# ğŸ”¥ 4. ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class LoadingStatus(Enum):
    """ë¡œë”© ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

@dataclass
class RealModelCacheEntry:
    """ì‹¤ì œ AI ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    ai_model: BaseRealAIModel
    load_time: float
    last_access: float
    access_count: int
    memory_usage_mb: float
    device: str
    step_name: Optional[str] = None
    is_healthy: bool = True
    error_count: int = 0
    
    def update_access(self):
        """ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.last_access = time.time()
        self.access_count += 1

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ ì‹¤ì œ AI ModelLoader í´ë˜ìŠ¤ v5.1 (torch ì•ˆì „ ì²˜ë¦¬)
# ==============================================

class RealAIModelLoader:
    """ì‹¤ì œ AI ì¶”ë¡  ê¸°ë°˜ ModelLoader v5.1 (torch ì˜¤ë¥˜ ì™„ì „ í•´ê²°)"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ì‹¤ì œ AI ModelLoader ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"RealAIModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._resolve_device(device or "auto")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.torch_available = TORCH_AVAILABLE
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬
        self.model_cache_dir = self._resolve_model_cache_dir(kwargs.get('model_cache_dir'))
        
        # ì„¤ì • íŒŒë¼ë¯¸í„°
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu' and TORCH_AVAILABLE)
        self.max_cached_models = kwargs.get('max_cached_models', 10 if self.is_m3_max else 5)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        self.min_model_size_mb = kwargs.get('min_model_size_mb', 50)
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ê´€ë ¨
        self.loaded_ai_models: Dict[str, BaseRealAIModel] = {}
        self.model_cache: Dict[str, RealModelCacheEntry] = {}
        self.model_status: Dict[str, LoadingStatus] = {}
        self.step_interfaces: Dict[str, Any] = {}
        
        # ğŸ”¥ AutoDetector ì—°ë™ (í•µì‹¬ ì¶”ê°€)
        self.auto_detector = None
        self._last_integration_time = 0.0
        self._integration_successful = False
        self._available_models_cache: Dict[str, Any] = {}

        self._initialize_auto_detector()

        # ì„±ëŠ¥ ì¶”ì 
        self.performance_stats = {
            'ai_models_loaded': 0,
            'cache_hits': 0,
            'ai_inference_count': 0,
            'total_inference_time': 0.0,
            'memory_usage_mb': 0.0,
            'large_models_loaded': 0,
            'integration_attempts': 0,
            'integration_success': 0,
            'torch_errors': 0,
            'torch_available': TORCH_AVAILABLE
        }
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="real_ai_loader")
        
        # ì´ˆê¸°í™”
        self._safe_initialize()
        
        # ğŸ”¥ ìë™ìœ¼ë¡œ AutoDetector í†µí•© ì‹œë„
        self._auto_integrate_on_init()
        
        self.logger.info(f"ğŸ§  ì‹¤ì œ AI ModelLoader v5.1 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, M3 Max: {self.is_m3_max}, conda: {self.conda_env}")
        self.logger.info(f"âš¡ PyTorch: {self.torch_available}, MPS: {MPS_AVAILABLE}, CUDA: {CUDA_AVAILABLE}")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬: {self.model_cache_dir}")
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device == "auto":
            return DEFAULT_DEVICE
        return device
    
    def _resolve_model_cache_dir(self, model_cache_dir_raw) -> Path:
        """ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ í•´ê²°"""
        try:
            if model_cache_dir_raw is None:
                # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìë™ ê³„ì‚°
                current_file = Path(__file__).resolve()
                current_path = current_file.parent
                for i in range(10):
                    if current_path.name == 'backend':
                        ai_models_path = current_path / "ai_models"
                        return ai_models_path
                    if current_path.parent == current_path:
                        break
                    current_path = current_path.parent
                
                # í´ë°±
                return Path.cwd() / "ai_models"
            else:
                path = Path(model_cache_dir_raw)
                # backend/backend íŒ¨í„´ ì œê±°
                path_str = str(path)
                if "backend/backend" in path_str:
                    path = Path(path_str.replace("backend/backend", "backend"))
                return path.resolve()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ í•´ê²° ì‹¤íŒ¨: {e}")
            return Path.cwd() / "ai_models"
    
    def _initialize_auto_detector(self):
        """auto_model_detector ì´ˆê¸°í™”"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                self.logger.info("âœ… auto_model_detector ì—°ë™ ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ auto_model_detector ì—†ìŒ")
        except Exception as e:
            self.logger.error(f"âŒ auto_model_detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _auto_integrate_on_init(self):
        """ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ AutoDetector í†µí•© ì‹œë„"""
        try:
            if self.auto_detector:
                success = self.integrate_auto_detector()
                if success:
                    self.logger.info("ğŸ‰ ì´ˆê¸°í™” ì‹œ AutoDetector ìë™ í†µí•© ì„±ê³µ")
                else:
                    self.logger.warning("âš ï¸ ì´ˆê¸°í™” ì‹œ AutoDetector ìë™ í†µí•© ì‹¤íŒ¨")
        except Exception as e:
            self.logger.debug(f"ìë™ í†µí•© ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ì¶”ê°€: integrate_auto_detector ë©”ì„œë“œ
    # ==============================================
    
    def integrate_auto_detector(self) -> bool:
        """ğŸ”¥ AutoDetector ì™„ì „ í†µí•© - available_models ì—°ë™"""
        integration_start = time.time()
        
        try:
            with self._lock:
                self.performance_stats['integration_attempts'] += 1
                
                if not AUTO_DETECTOR_AVAILABLE:
                    self.logger.warning("âš ï¸ AutoDetector ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                    return False
                
                if not self.auto_detector:
                    self.logger.warning("âš ï¸ AutoDetector ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ")
                    return False
                
                self.logger.info("ğŸ”¥ AutoDetector í†µí•© ì‹œì‘...")
                
                # 1ë‹¨ê³„: ëª¨ë¸ íƒì§€ ì‹¤í–‰
                try:
                    detected_models = self.auto_detector.detect_all_models()
                    if not detected_models:
                        self.logger.warning("âš ï¸ íƒì§€ëœ ëª¨ë¸ ì—†ìŒ")
                        return False
                        
                    self.logger.info(f"ğŸ“Š AutoDetector íƒì§€ ì™„ë£Œ: {len(detected_models)}ê°œ ëª¨ë¸")
                    
                except Exception as detect_error:
                    self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤í–‰ ì‹¤íŒ¨: {detect_error}")
                    return False
                
                # 2ë‹¨ê³„: ëª¨ë¸ ì •ë³´ í†µí•©
                integrated_count = 0
                failed_count = 0
                
                for model_name, detected_model in detected_models.items():
                    try:
                        # DetectedModelì„ available_models í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        model_info = self._convert_detected_model_to_available_format(model_name, detected_model)
                        
                        if model_info:
                            # ê¸°ì¡´ ëª¨ë¸ê³¼ ì¶©ëŒ í™•ì¸
                            if model_name in self._available_models_cache:
                                existing = self._available_models_cache[model_name]
                                if existing.get("size_mb", 0) > model_info["size_mb"]:
                                    self.logger.debug(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì´ ë” í¼ - ìœ ì§€: {model_name}")
                                    continue
                            
                            self._available_models_cache[model_name] = model_info
                            integrated_count += 1
                            
                            self.logger.debug(f"âœ… ëª¨ë¸ í†µí•©: {model_name} ({model_info['size_mb']:.1f}MB)")
                            
                    except Exception as model_error:
                        failed_count += 1
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} í†µí•© ì‹¤íŒ¨: {model_error}")
                        continue
                
                # 3ë‹¨ê³„: í†µí•© ê²°ê³¼ í‰ê°€
                integration_time = time.time() - integration_start
                self._last_integration_time = integration_time
                
                if integrated_count > 0:
                    self._integration_successful = True
                    self.performance_stats['integration_success'] += 1
                    
                    self.logger.info(f"âœ… AutoDetector í†µí•© ì„±ê³µ:")
                    self.logger.info(f"   í†µí•©ëœ ëª¨ë¸: {integrated_count}ê°œ")
                    self.logger.info(f"   ì‹¤íŒ¨í•œ ëª¨ë¸: {failed_count}ê°œ")
                    self.logger.info(f"   ì†Œìš” ì‹œê°„: {integration_time:.2f}ì´ˆ")
                    
                    # ìš°ì„ ìˆœìœ„ë³„ ìƒìœ„ 5ê°œ ëª¨ë¸ í‘œì‹œ
                    sorted_models = sorted(
                        self._available_models_cache.items(),
                        key=lambda x: x[1].get("priority_score", 0),
                        reverse=True
                    )
                    
                    self.logger.info("ğŸ† ìƒìœ„ 5ê°œ ëª¨ë¸:")
                    for i, (name, info) in enumerate(sorted_models[:5]):
                        size_mb = info.get("size_mb", 0)
                        score = info.get("priority_score", 0)
                        ai_class = info.get("ai_model_info", {}).get("ai_class", "Unknown")
                        self.logger.info(f"   {i+1}. {name}: {size_mb:.1f}MB (ì ìˆ˜: {score:.1f}) â†’ {ai_class}")
                    
                    return True
                else:
                    self.logger.warning(f"âš ï¸ AutoDetector í†µí•© ì‹¤íŒ¨: í†µí•©ëœ ëª¨ë¸ ì—†ìŒ")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ AutoDetector í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def _convert_detected_model_to_available_format(self, model_name: str, detected_model: DetectedModel) -> Optional[Dict[str, Any]]:
        """DetectedModelì„ available_models í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # DetectedModelì˜ to_dict() í™œìš©
            if hasattr(detected_model, 'to_dict'):
                base_dict = detected_model.to_dict()
            else:
                # ì§ì ‘ ì ‘ê·¼
                base_dict = {
                    "name": getattr(detected_model, 'name', model_name),
                    "path": str(getattr(detected_model, 'path', '')),
                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                    "model_type": getattr(detected_model, 'model_type', 'unknown'),
                    "confidence": getattr(detected_model, 'confidence_score', 0.5)
                }
            
            # ModelLoader í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            model_info = {
                "name": model_name,
                "path": base_dict.get("checkpoint_path", base_dict.get("path", "")),
                "checkpoint_path": base_dict.get("checkpoint_path", base_dict.get("path", "")),
                "size_mb": base_dict.get("size_mb", 0),
                "model_type": base_dict.get("model_type", "unknown"),
                "step_class": base_dict.get("step_class", "UnknownStep"),
                "loaded": False,
                "device": self.device,
                "priority_score": base_dict.get("priority_info", {}).get("priority_score", 0),
                "is_large_model": base_dict.get("priority_info", {}).get("is_large_model", False),
                "can_load_by_step": base_dict.get("step_implementation", {}).get("load_ready", False),
                
                # AI ëª¨ë¸ ì •ë³´
                "ai_model_info": {
                    "ai_class": self._determine_ai_class(detected_model, base_dict),
                    "can_create_ai_model": True,
                    "device_compatible": base_dict.get("device_config", {}).get("device_compatible", True),
                    "recommended_device": base_dict.get("device_config", {}).get("recommended_device", self.device),
                    "torch_available": self.torch_available
                },
                
                # ë©”íƒ€ë°ì´í„°
                "metadata": {
                    "detection_source": "auto_detector_v5.1",
                    "confidence": base_dict.get("confidence", 0.5),
                    "step_class_name": base_dict.get("step_implementation", {}).get("step_class_name", "UnknownStep"),
                    "model_load_method": base_dict.get("step_implementation", {}).get("model_load_method", "load_models"),
                    "full_path": base_dict.get("path", ""),
                    "size_category": base_dict.get("priority_info", {}).get("size_category", "medium"),
                    "integration_time": time.time(),
                    "torch_compatible": self.torch_available
                }
            }
            
            return model_info
            
        except Exception as e:
            self.logger.error(f"âŒ DetectedModel ë³€í™˜ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _determine_ai_class(self, detected_model: DetectedModel, base_dict: Dict[str, Any]) -> str:
        """AI í´ë˜ìŠ¤ ê²°ì •"""
        try:
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ê¸°ë³¸ í´ë˜ìŠ¤
            if not self.torch_available:
                return "BaseRealAIModel"
            
            # 1. DetectedModelì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            if hasattr(detected_model, 'ai_class') and detected_model.ai_class:
                return detected_model.ai_class
            
            # 2. base_dictì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if base_dict.get("ai_model_info", {}).get("ai_class"):
                return base_dict["ai_model_info"]["ai_class"]
            
            # 3. Step ê¸°ë°˜ ë§¤í•‘
            step_name = getattr(detected_model, 'step_name', 'UnknownStep')
            step_ai_mapping = {
                "HumanParsingStep": "RealGraphonomyModel",
                "ClothSegmentationStep": "RealSAMModel", 
                "ClothWarpingStep": "RealVisXLModel",
                "VirtualFittingStep": "RealOOTDDiffusionModel",
                "QualityAssessmentStep": "RealCLIPModel",
                "PostProcessingStep": "RealGFPGANModel"
            }
            
            if step_name in step_ai_mapping:
                return step_ai_mapping[step_name]
            
            # 4. íŒŒì¼ëª… ê¸°ë°˜ ì¶”ë¡ 
            file_name = getattr(detected_model, 'name', '').lower()
            if 'graphonomy' in file_name or 'schp' in file_name or 'atr' in file_name:
                return "RealGraphonomyModel"
            elif 'sam' in file_name:
                return "RealSAMModel"
            elif 'visxl' in file_name or 'realvis' in file_name:
                return "RealVisXLModel"
            elif 'diffusion' in file_name or 'ootd' in file_name:
                return "RealOOTDDiffusionModel"
            elif 'clip' in file_name or 'vit' in file_name:
                return "RealCLIPModel"
            elif 'gfpgan' in file_name:
                return "RealGFPGANModel"
            elif 'esrgan' in file_name:
                return "RealESRGANModel"
            else:
                return "BaseRealAIModel"
                
        except Exception as e:
            self.logger.debug(f"AI í´ë˜ìŠ¤ ê²°ì • ì‹¤íŒ¨: {e}")
            return "BaseRealAIModel"
    
    # ==============================================
    # ğŸ”¥ available_models ì†ì„± ì™„ì „ ì—°ë™
    # ==============================================
    
    @property
    def available_models(self) -> Dict[str, Any]:
        """ğŸ”¥ AutoDetector ì—°ë™ëœ available_models ì†ì„±"""
        try:
            # ìºì‹œ í™•ì¸
            if self._available_models_cache and self._integration_successful:
                return self._available_models_cache
            
            # AutoDetector í†µí•© ì‹œë„
            if self.auto_detector and not self._integration_successful:
                self.logger.info("ğŸ”„ available_models ì ‘ê·¼ ì‹œ AutoDetector í†µí•© ì‹œë„")
                success = self.integrate_auto_detector()
                if success and self._available_models_cache:
                    return self._available_models_cache
            
            # í´ë°±: ë¹ˆ ë”•ì…”ë„ˆë¦¬
            return {}
            
        except Exception as e:
            self.logger.error(f"âŒ available_models ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return {}
    
    @available_models.setter
    def available_models(self, value: Dict[str, Any]):
        """available_models ì„¤ì •"""
        try:
            with self._lock:
                self._available_models_cache = value
                self.logger.debug(f"ğŸ“ available_models ì—…ë°ì´íŠ¸: {len(value)}ê°œ ëª¨ë¸")
        except Exception as e:
            self.logger.error(f"âŒ available_models ì„¤ì • ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ list_available_models ë©”ì„œë“œ AutoDetector ì—°ë™
    # ==============================================
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ğŸ”¥ AutoDetector ì—°ë™ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ AI ëª¨ë¸ ëª©ë¡"""
        try:
            # AutoDetector ì—°ë™ í™•ì¸
            if not self._integration_successful and self.auto_detector:
                self.logger.info("ğŸ”„ list_available_models í˜¸ì¶œ ì‹œ AutoDetector í†µí•© ì‹œë„")
                self.integrate_auto_detector()
            
            # available_modelsì—ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_dict = self.available_models
            if not available_dict:
                self.logger.warning("âš ï¸ available_models ì—†ìŒ")
                return []
            
            available_models = []
            
            for model_name, model_info in available_dict.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # ë¡œë”© ìƒíƒœ ì¶”ê°€
                is_loaded = model_name in self.loaded_ai_models
                model_info_copy = model_info.copy()
                
                if is_loaded:
                    cache_entry = self.model_cache.get(model_name)
                    model_info_copy["loaded"] = True
                    model_info_copy["ai_loaded"] = True
                    model_info_copy["access_count"] = cache_entry.access_count if cache_entry else 0
                    model_info_copy["last_access"] = cache_entry.last_access if cache_entry else 0
                else:
                    model_info_copy["loaded"] = False
                    model_info_copy["ai_loaded"] = False
                    model_info_copy["access_count"] = 0
                    model_info_copy["last_access"] = 0
                
                # torch í˜¸í™˜ì„± ì •ë³´ ì¶”ê°€
                model_info_copy["torch_compatible"] = self.torch_available
                model_info_copy["can_load"] = self.torch_available or model_info_copy.get("ai_model_info", {}).get("ai_class") == "BaseRealAIModel"
                
                available_models.append(model_info_copy)
            
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
            available_models.sort(key=lambda x: x.get("priority_score", 0), reverse=True)
            
            self.logger.info(f"ğŸ“Š list_available_models ë°˜í™˜: {len(available_models)}ê°œ ëª¨ë¸ (torch: {self.torch_available})")
            return available_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    

    # ==============================================
    # ğŸ”¥ Stepë³„ ìµœì  ëª¨ë¸ ë§¤í•‘ ë° ì „ë‹¬ (torch ì•ˆì „ ì²˜ë¦¬)
    # ==============================================
    
    def get_model_for_step(self, step_name: str, model_type: Optional[str] = None) -> Optional[BaseRealAIModel]:
        """ğŸ”¥ Stepë³„ ìµœì  AI ëª¨ë¸ ë°˜í™˜ (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            self.logger.info(f"ğŸ¯ Stepë³„ ëª¨ë¸ ìš”ì²­: {step_name} (torch: {self.torch_available})")
            
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²½ê³ 
            if not self.torch_available:
                self.logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ AI ëª¨ë¸ë§Œ ê°€ëŠ¥")
            
            # AutoDetector ì—°ë™ í™•ì¸
            if not self._integration_successful and self.auto_detector:
                self.integrate_auto_detector()
            
            # Step ID ì¶”ì¶œ
            step_id = self._extract_step_id(step_name)
            if step_id == 0:
                self.logger.warning(f"âš ï¸ Step ID ì¶”ì¶œ ì‹¤íŒ¨: {step_name}")
                return None
            
            # í•´ë‹¹ Stepì˜ ëª¨ë¸ë“¤ ê°€ì ¸ì˜¤ê¸°
            step_models = self.list_available_models(step_class=step_name)
            if not step_models:
                self.logger.warning(f"âš ï¸ {step_name}ì— ëŒ€í•œ ëª¨ë¸ ì—†ìŒ")
                return None
            
            # torch í˜¸í™˜ ëª¨ë¸ ìš°ì„  ì„ íƒ
            compatible_models = [m for m in step_models if m.get("can_load", False)]
            if not compatible_models:
                self.logger.warning(f"âš ï¸ {step_name}ì— ëŒ€í•œ í˜¸í™˜ ëª¨ë¸ ì—†ìŒ")
                return None
            
            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ëª¨ë¸ë¶€í„° ì‹œë„ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŒ)
            for model_info in compatible_models:
                try:
                    model_name = model_info["name"]
                    ai_model = self.load_model(model_name)
                    if ai_model and ai_model.loaded:
                        self.logger.info(f"âœ… Step {step_name}ì— {model_name} AI ëª¨ë¸ ì—°ê²°")
                        return ai_model
                except Exception as e:
                    self.logger.debug(f"âŒ {model_info['name']} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.warning(f"âš ï¸ {step_name}ì— ë¡œë”© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {step_name}: {e}")
            return None
    
    def _extract_step_id(self, step_name: str) -> int:
        """Step ì´ë¦„ì—ì„œ ID ì¶”ì¶œ"""
        try:
            # "Step01HumanParsing" â†’ 1
            if "Step" in step_name:
                import re
                match = re.search(r'Step(\d+)', step_name)
                if match:
                    return int(match.group(1))
            
            # "HumanParsingStep" â†’ 1
            step_mapping = {
                "HumanParsingStep": 1, "HumanParsing": 1,
                "PoseEstimationStep": 2, "PoseEstimation": 2,
                "ClothSegmentationStep": 3, "ClothSegmentation": 3,
                "GeometricMatchingStep": 4, "GeometricMatching": 4,
                "ClothWarpingStep": 5, "ClothWarping": 5,
                "VirtualFittingStep": 6, "VirtualFitting": 6,
                "PostProcessingStep": 7, "PostProcessing": 7,
                "QualityAssessmentStep": 8, "QualityAssessment": 8
            }
            
            for key, step_id in step_mapping.items():
                if key in step_name:
                    return step_id
            
            return 0
            
        except Exception as e:
            self.logger.debug(f"Step ID ì¶”ì¶œ ì‹¤íŒ¨ {step_name}: {e}")
            return 0
    
        # ğŸ“ ìœ„ì¹˜: RealAIModelLoader í´ë˜ìŠ¤ ë‚´ë¶€ (ì•½ 1800ë¼ì¸ ê·¼ì²˜, ê¸°ì¡´ ë©”ì„œë“œë“¤ ì•„ë˜)

    # ==============================================
    # ğŸ”¥ BaseStepMixin v18.0 í˜¸í™˜ì„± ë©”ì„œë“œ ì¶”ê°€
    # ==============================================

    @property 
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ - BaseStepMixin í˜¸í™˜"""
        try:
            return (
                hasattr(self, 'model_cache') and 
                hasattr(self, 'loaded_ai_models') and 
                hasattr(self, 'available_models') and
                self.torch_available is not None
            )
        except Exception as e:
            self.logger.debug(f"ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def initialize(self, **kwargs) -> bool:
        """ModelLoader ì´ˆê¸°í™” - BaseStepMixin í˜¸í™˜ (ê¸°ì¡´ ë©”ì„œë“œ ê°œì„ )"""
        try:
            # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
            if self.is_initialized:
                return True
            
            # kwargsë¡œ ì „ë‹¬ëœ ì„¤ì • ì ìš©
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            # ê¸°ë³¸ ì´ˆê¸°í™” ë¡œì§ ì¬ì‹¤í–‰
            self._safe_initialize()
            
            # AutoDetector ì¬í†µí•© ì‹œë„
            if self.auto_detector and not self._integration_successful:
                self.integrate_auto_detector()
            
            self.logger.info("âœ… ModelLoader BaseStepMixin í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ ë©”ì„œë“œë“¤ (torch ì•ˆì „ ì²˜ë¦¬ ê°•í™”)
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[BaseRealAIModel]:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                if cache_entry.is_healthy:
                    cache_entry.update_access()
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ AI ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return cache_entry.ai_model
                else:
                    # ì†ìƒëœ ìºì‹œ ì œê±°
                    del self.model_cache[model_name]
            
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ì²˜ë¦¬
            if not self.torch_available:
                self.performance_stats['torch_errors'] += 1
                self.logger.warning(f"âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€ - {model_name} ë¡œë”© ì‹¤íŒ¨")
                return None
            
            # available_modelsì—ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (AutoDetector ì—°ë™)
            available_dict = self.available_models
            model_info = available_dict.get(model_name)
            
            if not model_info:
                self.logger.warning(f"âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´ ì—†ìŒ: {model_name}")
                return None
            
            # torch í˜¸í™˜ì„± í™•ì¸
            if not model_info.get("torch_compatible", True):
                self.logger.warning(f"âš ï¸ torch ë¹„í˜¸í™˜ ëª¨ë¸: {model_name}")
                return None
            
            # ì‹¤ì œ AI ëª¨ë¸ ìƒì„±
            ai_model = self._create_real_ai_model_from_info(model_name, model_info)
            if not ai_model:
                return None
            
            # AI ëª¨ë¸ ë¡œë”©
            if not ai_model.load_model():
                self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
                return None
            
            # ìºì‹œì— ì €ì¥
            cache_entry = RealModelCacheEntry(
                ai_model=ai_model,
                load_time=ai_model.load_time,
                last_access=time.time(),
                access_count=1,
                memory_usage_mb=ai_model.memory_usage_mb,
                device=ai_model.device,
                is_healthy=True,
                error_count=0
            )
            
            with self._lock:
                self.model_cache[model_name] = cache_entry
                self.loaded_ai_models[model_name] = ai_model
                self.model_status[model_name] = LoadingStatus.LOADED
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats['ai_models_loaded'] += 1
            self.performance_stats['memory_usage_mb'] += ai_model.memory_usage_mb
            
            if ai_model.memory_usage_mb >= 1000:  # 1GB ì´ìƒ
                self.performance_stats['large_models_loaded'] += 1
            
            self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({ai_model.memory_usage_mb:.1f}MB)")
            return ai_model
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.model_status[model_name] = LoadingStatus.ERROR
            return None
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[BaseRealAIModel]:
        """ë¹„ë™ê¸° ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor, 
                self.load_model, 
                model_name
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _create_real_ai_model_from_info(self, model_name: str, model_info: Dict[str, Any]) -> Optional[BaseRealAIModel]:
        """ëª¨ë¸ ì •ë³´ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ ìƒì„±"""
        try:
            ai_class = model_info.get("ai_model_info", {}).get("ai_class", "BaseRealAIModel")
            checkpoint_path = model_info.get("checkpoint_path") or model_info.get("path")
            
            if not checkpoint_path:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {model_name}")
                return None
            
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©
            if not self.torch_available and ai_class != "BaseRealAIModel":
                self.logger.warning(f"âš ï¸ PyTorch ì—†ìŒ - BaseRealAIModel ì‚¬ìš©: {model_name}")
                ai_class = "BaseRealAIModel"
            
            # RealAIModelFactoryë¡œ AI ëª¨ë¸ ìƒì„±
            ai_model = RealAIModelFactory.create_model(
                ai_class=ai_class,
                checkpoint_path=checkpoint_path,
                device=self.device
            )
            
            if not ai_model:
                self.logger.error(f"âŒ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {ai_class}")
                return None
            
            self.logger.info(f"âœ… AI ëª¨ë¸ ìƒì„± ì„±ê³µ: {ai_class} â†’ {type(ai_model).__name__}")
            return ai_model
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # ==============================================
    # ğŸ”¥ AI ì¶”ë¡  ì‹¤í–‰ ë©”ì„œë“œë“¤ (torch ì•ˆì „ ì²˜ë¦¬)
    # ==============================================
    
    def run_inference(self, model_name: str, *args, **kwargs) -> Dict[str, Any]:
        """ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ì²˜ë¦¬
            if not self.torch_available:
                return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€ - AI ì¶”ë¡  ì‹¤í–‰ ë¶ˆê°€"}
            
            # AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            ai_model = self.load_model(model_name)
            if not ai_model:
                return {"error": f"AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}"}
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = ai_model.predict(*args, **kwargs)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            inference_time = time.time() - start_time
            self.performance_stats['ai_inference_count'] += 1
            self.performance_stats['total_inference_time'] += inference_time
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if isinstance(result, dict) and "error" not in result:
                result["inference_metadata"] = {
                    "model_name": model_name,
                    "ai_class": type(ai_model).__name__,
                    "inference_time": inference_time,
                    "device": ai_model.device,
                    "memory_usage_mb": ai_model.memory_usage_mb,
                    "torch_available": self.torch_available
                }
            
            self.logger.info(f"âœ… AI ì¶”ë¡  ì™„ë£Œ: {model_name} ({inference_time:.3f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨ {model_name}: {e}")
            return {"error": str(e)}
    
    async def run_inference_async(self, model_name: str, *args, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.run_inference,
                model_name,
                *args
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° AI ì¶”ë¡  ì‹¤íŒ¨ {model_name}: {e}")
            return {"error": str(e)}
    
    # ==============================================
    # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ì—°ë™ (torch ì•ˆì „ ì²˜ë¦¬)
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> 'RealStepModelInterface':
        """ì‹¤ì œ AI ê¸°ë°˜ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            with self._lock:
                # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if step_name in self.step_interfaces:
                    return self.step_interfaces[step_name]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                interface = RealStepModelInterface(self, step_name)
                
                # Step ìš”êµ¬ì‚¬í•­ ë“±ë¡
                if step_requirements:
                    interface.register_step_requirements(step_requirements)
                
                self.step_interfaces[step_name] = interface
                
                self.logger.info(f"âœ… ì‹¤ì œ AI Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name} (torch: {self.torch_available})")
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return RealStepModelInterface(self, step_name)
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ê´€ë¦¬ ë©”ì„œë“œë“¤ (torch ì•ˆì „ ì²˜ë¦¬)
    # ==============================================
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """AI ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                ai_model = cache_entry.ai_model
                
                return {
                    "name": model_name,
                    "status": "loaded",
                    "ai_class": type(ai_model).__name__,
                    "device": ai_model.device,
                    "memory_usage_mb": ai_model.memory_usage_mb,
                    "load_time": ai_model.load_time,
                    "last_access": cache_entry.last_access,
                    "access_count": cache_entry.access_count,
                    "is_healthy": cache_entry.is_healthy,
                    "error_count": cache_entry.error_count,
                    "file_size_mb": ai_model.checkpoint_path.stat().st_size / (1024 * 1024) if ai_model.checkpoint_path.exists() else 0,
                    "checkpoint_path": str(ai_model.checkpoint_path),
                    "torch_available": ai_model.torch_available,
                    "torch_compatible": self.torch_available
                }
            else:
                status = self.model_status.get(model_name, LoadingStatus.NOT_LOADED)
                return {
                    "name": model_name,
                    "status": status.value,
                    "ai_class": None,
                    "device": self.device,
                    "memory_usage_mb": 0,
                    "load_time": 0,
                    "last_access": 0,
                    "access_count": 0,
                    "is_healthy": False,
                    "error_count": 0,
                    "file_size_mb": 0,
                    "checkpoint_path": None,
                    "torch_available": self.torch_available,
                    "torch_compatible": self.torch_available
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"name": model_name, "status": "error", "error": str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (torch ìƒíƒœ ì •ë³´ í¬í•¨)"""
        try:
            total_memory = sum(entry.memory_usage_mb for entry in self.model_cache.values())
            avg_inference_time = (
                self.performance_stats['total_inference_time'] / 
                max(1, self.performance_stats['ai_inference_count'])
            )
            
            return {
                "ai_model_counts": {
                    "loaded": len(self.loaded_ai_models),
                    "cached": len(self.model_cache),
                    "large_models": self.performance_stats['large_models_loaded'],
                    "available": len(self.available_models)
                },
                "memory_usage": {
                    "total_mb": total_memory,
                    "average_per_model_mb": total_memory / len(self.model_cache) if self.model_cache else 0,
                    "device": self.device,
                    "is_m3_max": self.is_m3_max
                },
                "ai_performance": {
                    "inference_count": self.performance_stats['ai_inference_count'],
                    "total_inference_time": self.performance_stats['total_inference_time'],
                    "average_inference_time": avg_inference_time,
                    "cache_hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['ai_models_loaded']),
                    "torch_errors": self.performance_stats['torch_errors']
                },
                "auto_detector_integration": {
                    "integration_attempts": self.performance_stats['integration_attempts'],
                    "integration_success": self.performance_stats['integration_success'],
                    "last_integration_time": self._last_integration_time,
                    "integration_successful": self._integration_successful,
                    "available_models_count": len(self._available_models_cache)
                },
                "system_info": {
                    "conda_env": self.conda_env,
                    "torch_available": self.torch_available,
                    "mps_available": MPS_AVAILABLE,
                    "cuda_available": CUDA_AVAILABLE,
                    "numpy_available": NUMPY_AVAILABLE,
                    "pil_available": PIL_AVAILABLE,
                    "auto_detector_available": AUTO_DETECTOR_AVAILABLE,
                    "default_device": DEFAULT_DEVICE
                },
                "torch_status": {
                    "torch_module": torch is not None,
                    "torch_tensor": hasattr(torch, 'Tensor') if torch else False,
                    "functional_status": TORCH_AVAILABLE,
                    "error_count": self.performance_stats['torch_errors']
                },
                "version": "5.1_torch_error_fixed"
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def unload_model(self, model_name: str) -> bool:
        """AI ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    
                    # AI ëª¨ë¸ ì–¸ë¡œë“œ
                    cache_entry.ai_model.unload_model()
                    
                    # ìºì‹œì—ì„œ ì œê±°
                    del self.model_cache[model_name]
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.performance_stats['memory_usage_mb'] -= cache_entry.memory_usage_mb
                
                if model_name in self.loaded_ai_models:
                    del self.loaded_ai_models[model_name]
                
                if model_name in self.model_status:
                    self.model_status[model_name] = LoadingStatus.NOT_LOADED
                
                self._safe_memory_cleanup()
                
                self.logger.info(f"âœ… AI ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {model_name} - {e}")
            return True  # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ì‹¤ì œ AI ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë“  AI ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.model_cache.keys()):
                self.unload_model(model_name)
            
            # ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_ai_models.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            self._safe_memory_cleanup()
            
            self.logger.info("âœ… ì‹¤ì œ AI ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def register_model_requirement(self, model_name: str, requirement: Dict[str, Any]) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                self.model_requirements[model_name] = requirement
                
                # ModelLoaderì—ë„ ì „ë‹¬
                if self.model_loader and hasattr(self.model_loader, 'register_step_requirements'):
                    self.model_loader.register_step_requirements(model_name, requirement)
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
        
    # ==============================================
    # ğŸ”¥ ì¶”ê°€: ëˆ„ë½ëœ í•µì‹¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (main.pyì—ì„œ í•„ìš”)"""
        try:
            with self._lock:
                if not hasattr(self, 'step_requirements'):
                    self.step_requirements = {}
                
                self.step_requirements[step_name] = requirements
                self.logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_name} ({len(requirements)}ê°œ)")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
    
    def get_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """Step ìš”êµ¬ì‚¬í•­ ì¡°íšŒ"""
        try:
            if hasattr(self, 'step_requirements'):
                return self.step_requirements.get(step_name, {})
            return {}
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ì¡°íšŒ ì‹¤íŒ¨ {step_name}: {e}")
            return {}
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦"""
        try:
            # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            available_dict = self.available_models
            model_info = available_dict.get(model_name)
            
            if not model_info:
                return False
            
            # Step í˜¸í™˜ì„± í™•ì¸
            model_step_class = model_info.get("step_class", "")
            if step_name not in model_step_class and model_step_class not in step_name:
                return False
            
            # torch í˜¸í™˜ì„± í™•ì¸
            if not self.torch_available and model_info.get("ai_model_info", {}).get("ai_class") != "BaseRealAIModel":
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨ {model_name}-{step_name}: {e}")
            return False
    
    def get_model_metadata(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        try:
            available_dict = self.available_models
            model_info = available_dict.get(model_name, {})
            
            return {
                "name": model_name,
                "exists": model_name in available_dict,
                "size_mb": model_info.get("size_mb", 0),
                "step_class": model_info.get("step_class", "Unknown"),
                "ai_class": model_info.get("ai_model_info", {}).get("ai_class", "Unknown"),
                "device_compatible": model_info.get("ai_model_info", {}).get("device_compatible", True),
                "torch_compatible": model_info.get("torch_compatible", self.torch_available),
                "can_load": model_info.get("can_load", False),
                "metadata": model_info.get("metadata", {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"name": model_name, "exists": False, "error": str(e)}
    
    def force_reload_model(self, model_name: str) -> Optional[BaseRealAIModel]:
        """ëª¨ë¸ ê°•ì œ ì¬ë¡œë“œ"""
        try:
            # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
            if model_name in self.model_cache:
                self.unload_model(model_name)
            
            # ìƒíƒœ ì´ˆê¸°í™”
            if model_name in self.model_status:
                del self.model_status[model_name]
            
            # ì¬ë¡œë“œ
            return self.load_model(model_name)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°•ì œ ì¬ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def get_loaded_models_info(self) -> Dict[str, Dict[str, Any]]:
        """ë¡œë“œëœ ëª¨ë¸ë“¤ ì •ë³´ ì¡°íšŒ"""
        try:
            loaded_info = {}
            
            for model_name, cache_entry in self.model_cache.items():
                loaded_info[model_name] = {
                    "ai_class": type(cache_entry.ai_model).__name__,
                    "device": cache_entry.device,
                    "memory_usage_mb": cache_entry.memory_usage_mb,
                    "load_time": cache_entry.load_time,
                    "last_access": cache_entry.last_access,
                    "access_count": cache_entry.access_count,
                    "is_healthy": cache_entry.is_healthy,
                    "error_count": cache_entry.error_count
                }
            
            return loaded_info
            
        except Exception as e:
            self.logger.error(f"âŒ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        try:
            initial_memory = sum(entry.memory_usage_mb for entry in self.model_cache.values())
            
            # ì˜¤ë˜ëœ ëª¨ë¸ë“¤ ì–¸ë¡œë“œ (ì ‘ê·¼í•œì§€ 1ì‹œê°„ ì´ìƒ)
            current_time = time.time()
            models_to_unload = []
            
            for model_name, cache_entry in self.model_cache.items():
                if current_time - cache_entry.last_access > 3600:  # 1ì‹œê°„
                    models_to_unload.append(model_name)
            
            unloaded_count = 0
            for model_name in models_to_unload:
                if self.unload_model(model_name):
                    unloaded_count += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self._safe_memory_cleanup()
            
            final_memory = sum(entry.memory_usage_mb for entry in self.model_cache.values())
            freed_memory = initial_memory - final_memory
            
            optimization_result = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "freed_memory_mb": freed_memory,
                "unloaded_models": unloaded_count,
                "remaining_models": len(self.model_cache),
                "optimization_successful": freed_memory > 0
            }
            
            self.logger.info(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {freed_memory:.1f}MB í•´ì œ, {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ")
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e), "optimization_successful": False}
    
    def health_check(self) -> Dict[str, Any]:
        """ModelLoader ê±´ê°•ìƒíƒœ ì²´í¬"""
        try:
            health_status = {
                "status": "healthy",
                "timestamp": time.time(),
                "system_info": {
                    "torch_available": self.torch_available,
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "conda_env": self.conda_env
                },
                "models": {
                    "loaded_count": len(self.loaded_ai_models),
                    "cached_count": len(self.model_cache),
                    "available_count": len(self.available_models),
                    "total_memory_mb": sum(entry.memory_usage_mb for entry in self.model_cache.values())
                },
                "auto_detector": {
                    "available": AUTO_DETECTOR_AVAILABLE,
                    "integration_successful": self._integration_successful,
                    "last_integration_time": self._last_integration_time
                },
                "performance": {
                    "ai_inference_count": self.performance_stats['ai_inference_count'],
                    "cache_hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['ai_models_loaded']),
                    "torch_errors": self.performance_stats['torch_errors']
                },
                "issues": []
            }
            
            # ë¬¸ì œ í™•ì¸
            if not self.torch_available:
                health_status["issues"].append("PyTorch ì‚¬ìš© ë¶ˆê°€")
                health_status["status"] = "warning"
            
            if self.performance_stats['torch_errors'] > 0:
                health_status["issues"].append(f"torch ì˜¤ë¥˜ {self.performance_stats['torch_errors']}ê°œ")
                health_status["status"] = "warning"
            
            if not self._integration_successful and AUTO_DETECTOR_AVAILABLE:
                health_status["issues"].append("AutoDetector í†µí•© ì‹¤íŒ¨")
                health_status["status"] = "warning"
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            total_memory = health_status["models"]["total_memory_mb"]
            if total_memory > 50000:  # 50GB ì´ìƒ
                health_status["issues"].append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {total_memory:.1f}MB")
                health_status["status"] = "warning"
            
            if health_status["issues"]:
                self.logger.warning(f"âš ï¸ ModelLoader ê±´ê°•ìƒíƒœ ê²½ê³ : {len(health_status['issues'])}ê°œ ë¬¸ì œ")
            else:
                self.logger.info("âœ… ModelLoader ê±´ê°•ìƒíƒœ ì–‘í˜¸")
            
            return health_status
            
        except Exception as e:
            self.logger.error(f"âŒ ê±´ê°•ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": time.time()
            }
    # ==============================================
    # ğŸ”¥ BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜ì„± 
    # ==============================================
    
    @property 
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ - BaseStepMixin í˜¸í™˜"""
        return (
            hasattr(self, 'model_cache') and 
            len(self.model_cache) >= 0 and
            hasattr(self, 'available_models') and
            self.torch_available is not None
        )
    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (torch ì•ˆì „ ì²˜ë¦¬)
    # ==============================================
    
    def _safe_initialize(self):
        """ì•ˆì „í•œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
            if not self.model_cache_dir.exists():
                self.model_cache_dir.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.model_cache_dir}")
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.optimization_enabled:
                self._safe_memory_cleanup()
            
            self.logger.info(f"ğŸ“¦ ì‹¤ì œ AI ModelLoader ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ (torch: {self.torch_available})")
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.torch_available:
                try:
                    if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif self.device == "mps" and MPS_AVAILABLE:
                        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                except Exception as e:
                    self.logger.debug(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    
    # ==============================================
    # ğŸ”¥ í˜¸í™˜ì„± ì†ì„± ë° ë©”ì„œë“œ ì¶”ê°€
    # ==============================================
    
    @property
    def loaded_models(self) -> Dict[str, BaseRealAIModel]:
        """í˜¸í™˜ì„±ì„ ìœ„í•œ loaded_models ì†ì„±"""
        return self.loaded_ai_models
    
    def initialize(self, **kwargs) -> bool:
        """ModelLoader ì´ˆê¸°í™” (í˜¸í™˜ì„±)"""
        try:
            if kwargs:
                for key, value in kwargs.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
            
            self._safe_initialize()
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” (í˜¸í™˜ì„±)"""
        try:
            result = self.initialize(**kwargs)
            return result
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ê¸°ë°˜ Step ì¸í„°í˜ì´ìŠ¤ (torch ì•ˆì „ ì²˜ë¦¬)
# ==============================================

class RealStepModelInterface:
    """ì‹¤ì œ AI ê¸°ë°˜ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (torch ì•ˆì „ ì²˜ë¦¬)"""
    
    def __init__(self, model_loader: RealAIModelLoader, step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"RealStepInterface.{step_name}")
        
        # Stepë³„ AI ëª¨ë¸ë“¤
        self.step_ai_models: Dict[str, BaseRealAIModel] = {}
        self.primary_ai_model: Optional[BaseRealAIModel] = None
        
        # ìš”êµ¬ì‚¬í•­ ë° ìƒíƒœ
        self.step_requirements: Dict[str, Any] = {}
        self.creation_time = time.time()
        self.error_count = 0
        self.last_error = None
        self.torch_available = self.model_loader.torch_available
        
        self._lock = threading.RLock()
        
        # Stepë³„ ìµœì  AI ëª¨ë¸ ìë™ ë¡œë”© (torch ì•ˆì „ ì²˜ë¦¬)
        self._load_step_ai_models()
        
        self.logger.info(f"ğŸ§  ì‹¤ì œ AI Step ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”: {step_name} (torch: {self.torch_available})")
    
    def _load_step_ai_models(self):
        """Stepë³„ AI ëª¨ë¸ë“¤ ìë™ ë¡œë”© (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²½ê³ 
            if not self.torch_available:
                self.logger.warning(f"âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€ - {self.step_name} AI ëª¨ë¸ ì œí•œì  ë¡œë”©")
            
            # ì£¼ AI ëª¨ë¸ ë¡œë”© (AutoDetectorì—ì„œ ìµœì  ëª¨ë¸ ì„ íƒ)
            primary_model = self.model_loader.get_model_for_step(self.step_name)
            if primary_model:
                self.primary_ai_model = primary_model
                self.step_ai_models["primary"] = primary_model
                self.logger.info(f"âœ… ì£¼ AI ëª¨ë¸ ë¡œë”©: {type(primary_model).__name__}")
            else:
                self.logger.warning(f"âš ï¸ {self.step_name}ì— ëŒ€í•œ ì£¼ AI ëª¨ë¸ ì—†ìŒ")
            
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ Step AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤ (torch ì•ˆì „ ì²˜ë¦¬)
    def get_model(self, model_name: Optional[str] = None) -> Optional[BaseRealAIModel]:
        """AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            if not model_name or model_name == "default":
                return self.primary_ai_model
            
            # íŠ¹ì • ëª¨ë¸ ìš”ì²­
            if model_name in self.step_ai_models:
                return self.step_ai_models[model_name]
            
            # ModelLoaderì—ì„œ ë¡œë”© ì‹œë„
            ai_model = self.model_loader.load_model(model_name)
            if ai_model and ai_model.loaded:
                self.step_ai_models[model_name] = ai_model
                return ai_model
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[BaseRealAIModel]:
        """ë¹„ë™ê¸° AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.get_model(model_name))
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def run_ai_inference(self, input_data: Any, model_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰ (torch ì•ˆì „ ì²˜ë¦¬)"""
        try:
            # torch ì‚¬ìš© ë¶ˆê°€ ì‹œ ì²˜ë¦¬
            if not self.torch_available:
                return {"error": "PyTorch ì‚¬ìš© ë¶ˆê°€ - AI ì¶”ë¡  ì‹¤í–‰ ë¶ˆê°€"}
            
            # AI ëª¨ë¸ ì„ íƒ
            ai_model = self.get_model(model_name)
            if not ai_model:
                return {"error": f"AI ëª¨ë¸ ì—†ìŒ: {model_name or 'default'}"}
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = ai_model.predict(input_data, **kwargs)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if isinstance(result, dict) and "error" not in result:
                result["step_info"] = {
                    "step_name": self.step_name,
                    "ai_model": type(ai_model).__name__,
                    "device": ai_model.device,
                    "torch_available": self.torch_available
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def run_ai_inference_async(self, input_data: Any, model_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None,
                self.run_ai_inference,
                input_data,
                model_name
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def register_step_requirements(self, requirements: Dict[str, Any]):
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                self.step_requirements.update(requirements)
                self.logger.info(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {len(requirements)}ê°œ")
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def get_step_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ"""
        try:
            return {
                "step_name": self.step_name,
                "ai_models_loaded": len(self.step_ai_models),
                "primary_model": type(self.primary_ai_model).__name__ if self.primary_ai_model else None,
                "creation_time": self.creation_time,
                "error_count": self.error_count,
                "last_error": self.last_error,
                "available_models": list(self.step_ai_models.keys()),
                "torch_available": self.torch_available,
                "torch_compatible": self.torch_available
            }
        except Exception as e:
            self.logger.error(f"âŒ Step ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    # BaseStepMixin v18.0 í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[BaseRealAIModel]:
        """ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - BaseStepMixin í˜¸í™˜"""
        return self.get_model(model_name)
    
    def register_model_requirement(self, model_name: str, requirement: Dict[str, Any]) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin í˜¸í™˜"""
        try:
            self.register_step_requirements({model_name: requirement})
            return True
        except:
            return False


# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… 100% ìœ ì§€)
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_real_model_loader: Optional[RealAIModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> RealAIModelLoader:
    """ì „ì—­ ì‹¤ì œ AI ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    global _global_real_model_loader
    
    with _loader_lock:
        if _global_real_model_loader is None:
            # ì˜¬ë°”ë¥¸ AI ëª¨ë¸ ê²½ë¡œ ê³„ì‚°
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            ai_models_path = backend_root / "ai_models"
            
            try:
                _global_real_model_loader = RealAIModelLoader(
                    config=config,
                    device="auto",
                    model_cache_dir=str(ai_models_path),
                    use_fp16=True and TORCH_AVAILABLE,
                    optimization_enabled=True,
                    enable_fallback=True,
                    min_model_size_mb=50  # 50MB ì´ìƒ
                )
                logger.info("âœ… ì „ì—­ ì‹¤ì œ AI ModelLoader ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ ì‹¤ì œ AI ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
                _global_real_model_loader = RealAIModelLoader(device="cpu")
                
        return _global_real_model_loader

# ì „ì—­ ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> RealAIModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info(f"âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return RealStepModelInterface(get_global_model_loader(), step_name)

def get_model(model_name: str) -> Optional[BaseRealAIModel]:
    """ì „ì—­ AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[BaseRealAIModel]:
    """ì „ì—­ ë¹„ë™ê¸° AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def run_ai_inference(model_name: str, *args, **kwargs) -> Dict[str, Any]:
    """ì „ì—­ AI ì¶”ë¡  ì‹¤í–‰"""
    loader = get_global_model_loader()
    return loader.run_inference(model_name, *args, **kwargs)

async def run_ai_inference_async(model_name: str, *args, **kwargs) -> Dict[str, Any]:
    """ì „ì—­ ë¹„ë™ê¸° AI ì¶”ë¡  ì‹¤í–‰"""
    loader = get_global_model_loader()
    return await loader.run_inference_async(model_name, *args, **kwargs)

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
ModelLoader = RealAIModelLoader
StepModelInterface = RealStepModelInterface

def get_step_model_interface(step_name: str, model_loader_instance=None) -> RealStepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# ==============================================
# ğŸ”¥ ì¶”ê°€: main.py ì™„ì „ í˜¸í™˜ í•¨ìˆ˜ë“¤
# ==============================================

def ensure_global_model_loader_initialized(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ê°•ì œ ì´ˆê¸°í™” ë° ê²€ì¦ (main.py í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        if loader and hasattr(loader, 'initialize'):
            success = loader.initialize(**kwargs)
            if success:
                logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ê²€ì¦ ì™„ë£Œ")
                return True
            else:
                logger.error("âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
        else:
            logger.error("âŒ ModelLoader ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ê±°ë‚˜ initialize ë©”ì„œë“œ ì—†ìŒ")
            return False
    except Exception as e:
        logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> Dict[str, Any]:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ í•¨ìˆ˜"""
    try:
        path = Path(checkpoint_path)
        
        validation = {
            "path": str(path),
            "exists": path.exists(),
            "is_file": path.is_file() if path.exists() else False,
            "size_mb": 0,
            "readable": False,
            "valid_extension": False,
            "torch_loadable": False,
            "is_valid": False,
            "errors": []
        }
        
        if not path.exists():
            validation["errors"].append("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            return validation
        
        if not path.is_file():
            validation["errors"].append("íŒŒì¼ì´ ì•„ë‹˜")
            return validation
        
        # í¬ê¸° í™•ì¸
        try:
            size_bytes = path.stat().st_size
            validation["size_mb"] = size_bytes / (1024 * 1024)
        except Exception as e:
            validation["errors"].append(f"í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ì½ê¸° ê¶Œí•œ í™•ì¸
        try:
            validation["readable"] = os.access(path, os.R_OK)
            if not validation["readable"]:
                validation["errors"].append("ì½ê¸° ê¶Œí•œ ì—†ìŒ")
        except Exception as e:
            validation["errors"].append(f"ê¶Œí•œ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # í™•ì¥ì í™•ì¸
        valid_extensions = ['.pth', '.pt', '.ckpt', '.safetensors', '.bin']
        validation["valid_extension"] = path.suffix.lower() in valid_extensions
        if not validation["valid_extension"]:
            validation["errors"].append(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {path.suffix}")
        
        # torch ë¡œë”© ê°€ëŠ¥ ì—¬ë¶€ (ê°„ë‹¨í•œ ì²´í¬)
        if TORCH_AVAILABLE and validation["readable"] and validation["valid_extension"]:
            try:
                # í—¤ë”ë§Œ ì½ì–´ì„œ ê¸°ë³¸ ê²€ì¦
                with open(path, 'rb') as f:
                    header = f.read(1024)  # ì²« 1KBë§Œ ì½ê¸°
                    if header:
                        validation["torch_loadable"] = True
            except Exception as e:
                validation["errors"].append(f"torch ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ìœ íš¨ì„± íŒë‹¨
        validation["is_valid"] = (
            validation["exists"] and 
            validation["is_file"] and 
            validation["readable"] and 
            validation["valid_extension"] and
            validation["size_mb"] > 0 and
            len(validation["errors"]) == 0
        )
        
        return validation
        
    except Exception as e:
        return {
            "path": str(checkpoint_path),
            "exists": False,
            "is_valid": False,
            "errors": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}"]
        }

def safe_load_checkpoint(checkpoint_path: Union[str, Path], device: str = "cpu") -> Optional[Any]:
    """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        # ê²€ì¦ ë¨¼ì € ì‹¤í–‰
        validation = validate_checkpoint_file(checkpoint_path)
        if not validation["is_valid"]:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {validation['errors']}")
            return None
        
        if not TORCH_AVAILABLE:
            logger.error("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€ - ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨")
            return None
        
        # ì•ˆì „í•œ ë¡œë”©
        path = Path(checkpoint_path)
        logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {path} ({validation['size_mb']:.1f}MB)")
        
        checkpoint = torch.load(path, map_location=device)
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {path}")
        
        return checkpoint
        
    except Exception as e:
        logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

def get_system_capabilities() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ëŠ¥ë ¥ ì¡°íšŒ"""
    try:
        return {
            "torch_available": TORCH_AVAILABLE,
            "mps_available": MPS_AVAILABLE,
            "cuda_available": CUDA_AVAILABLE,
            "numpy_available": NUMPY_AVAILABLE,
            "pil_available": PIL_AVAILABLE,
            "cv2_available": CV2_AVAILABLE,
            "auto_detector_available": AUTO_DETECTOR_AVAILABLE,
            "default_device": DEFAULT_DEVICE,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV,
            "python_version": sys.version,
            "torch_version": torch.__version__ if TORCH_AVAILABLE else "Not Available"
        }
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ëŠ¥ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def emergency_cleanup() -> bool:
    """ë¹„ìƒ ì •ë¦¬ í•¨ìˆ˜"""
    try:
        logger.warning("ğŸš¨ ë¹„ìƒ ì •ë¦¬ ì‹œì‘...")
        
        # ì „ì—­ ModelLoader ì •ë¦¬
        global _global_real_model_loader
        if _global_real_model_loader:
            _global_real_model_loader.cleanup()
            _global_real_model_loader = None
        
        # torch ìºì‹œ ì •ë¦¬
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                if MPS_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            except:
                pass
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        logger.info("âœ… ë¹„ìƒ ì •ë¦¬ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë¹„ìƒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ Export ë° ì´ˆê¸°í™”
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'RealAIModelLoader',
    'RealStepModelInterface',
    'BaseRealAIModel',
    'RealAIModelFactory',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'RealGraphonomyModel',
    'RealSAMModel', 
    'RealVisXLModel',
    'RealOOTDDiffusionModel',
    'RealCLIPModel',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'LoadingStatus',
    'RealModelCacheEntry',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'get_global_model_loader',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'run_ai_inference',
    'run_ai_inference_async',
    'get_step_model_interface',
    
    # ğŸ”¥ ì¶”ê°€ëœ main.py í˜¸í™˜ í•¨ìˆ˜ë“¤
    'ensure_global_model_loader_initialized',
    'validate_checkpoint_file',
    'safe_load_checkpoint',
    'get_system_capabilities',
    'emergency_cleanup',
    
    # í˜¸í™˜ì„± ë³„ì¹­ë“¤
    'ModelLoader',
    'StepModelInterface',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CUDA_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CV2_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ
logger.info("=" * 80)
logger.info("âœ… ì‹¤ì œ AI ì¶”ë¡  ê¸°ë°˜ ModelLoader v5.1 ë¡œë“œ ì™„ë£Œ (torch ì˜¤ë¥˜ í•´ê²°)")
logger.info("=" * 80)
logger.info("ğŸ”¥ torch ì´ˆê¸°í™” ë¬¸ì œ ì™„ì „ í•´ê²° - 'NoneType' object has no attribute 'Tensor'")
logger.info("ğŸ§  ì‹¤ì œ 229GB AI ëª¨ë¸ì„ AI í´ë˜ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì™„ì „í•œ ì¶”ë¡  ì‹¤í–‰")
logger.info("ğŸ”— auto_model_detector.pyì™€ ì™„ë²½ ì—°ë™ (integrate_auto_detector)")
logger.info("âœ… BaseStepMixinê³¼ 100% í˜¸í™˜ë˜ëŠ” ì‹¤ì œ AI ëª¨ë¸ ì œê³µ")
logger.info("ğŸš€ PyTorch ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ AI í´ë˜ìŠ¤ ìë™ ë³€í™˜")
logger.info("âš¡ M3 Max 128GB + conda í™˜ê²½ ìµœì í™”")
logger.info("ğŸ¯ ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ ë‚´ì¥ (ëª©ì—…/ê°€ìƒ ëª¨ë¸ ì™„ì „ ì œê±°)")
logger.info("ğŸ”„ ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€")
logger.info(f"ğŸ”§ PyTorch ìƒíƒœ: {TORCH_AVAILABLE}, MPS: {MPS_AVAILABLE}, CUDA: {CUDA_AVAILABLE}")
logger.info("=" * 80)

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_loader = get_global_model_loader()
    logger.info(f"ğŸš€ ì‹¤ì œ AI ModelLoader v5.1 ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   ë””ë°”ì´ìŠ¤: {_test_loader.device}")
    logger.info(f"   M3 Max: {_test_loader.is_m3_max}")
    logger.info(f"   PyTorch: {_test_loader.torch_available}")
    logger.info(f"   AI ëª¨ë¸ ë£¨íŠ¸: {_test_loader.model_cache_dir}")
    logger.info(f"   auto_detector ì—°ë™: {_test_loader.auto_detector is not None}")
    logger.info(f"   AutoDetector í†µí•©: {_test_loader._integration_successful}")
    logger.info(f"   available_models: {len(_test_loader.available_models)}ê°œ")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸ§  ì‹¤ì œ AI ì¶”ë¡  ê¸°ë°˜ ModelLoader v5.1 í…ŒìŠ¤íŠ¸ (torch ì˜¤ë¥˜ í•´ê²°)")
    print("=" * 80)
    
    async def test_real_ai_loader():
        # ModelLoader ìƒì„±
        loader = get_global_model_loader()
        print(f"âœ… ì‹¤ì œ AI ModelLoader ìƒì„±: {type(loader).__name__}")
        print(f"ğŸ”§ PyTorch ìƒíƒœ: {loader.torch_available}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        models = loader.list_available_models()
        print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models)}ê°œ")
        
        if models:
            # ìƒìœ„ 3ê°œ ëª¨ë¸ í‘œì‹œ
            print("\nğŸ† ìƒìœ„ AI ëª¨ë¸:")
            for i, model in enumerate(models[:3]):
                ai_class = model.get("ai_model_info", {}).get("ai_class", "Unknown")
                size_mb = model.get("size_mb", 0)
                torch_compatible = model.get("torch_compatible", False)
                print(f"   {i+1}. {model['name']}: {size_mb:.1f}MB â†’ {ai_class} (torch: {torch_compatible})")
        
        # Step ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        step_interface = create_step_interface("HumanParsingStep")
        print(f"\nğŸ”— Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {type(step_interface).__name__}")
        print(f"ğŸ”§ Step torch ìƒíƒœ: {step_interface.torch_available}")
        
        step_status = step_interface.get_step_status()
        print(f"ğŸ“Š Step ìƒíƒœ: {step_status.get('ai_models_loaded', 0)}ê°œ AI ëª¨ë¸ ë¡œë”©ë¨")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics = loader.get_performance_metrics()
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   ë¡œë”©ëœ AI ëª¨ë¸: {metrics['ai_model_counts']['loaded']}ê°œ")
        print(f"   ëŒ€í˜• ëª¨ë¸: {metrics['ai_model_counts']['large_models']}ê°œ")
        print(f"   ì´ ë©”ëª¨ë¦¬: {metrics['memory_usage']['total_mb']:.1f}MB")
        print(f"   M3 Max ìµœì í™”: {metrics['memory_usage']['is_m3_max']}")
        print(f"   AutoDetector í†µí•©: {metrics['auto_detector_integration']['integration_successful']}")
        print(f"   torch ìƒíƒœ: {metrics['torch_status']['functional_status']}")
        print(f"   torch ì˜¤ë¥˜: {metrics['torch_status']['error_count']}ê°œ")
    
    try:
        asyncio.run(test_real_ai_loader())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ ì‹¤ì œ AI ì¶”ë¡  ModelLoader v5.1 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ”¥ torch ì´ˆê¸°í™” ë¬¸ì œ ì™„ì „ í•´ê²°")
    print("ğŸ§  ì²´í¬í¬ì¸íŠ¸ â†’ AI í´ë˜ìŠ¤ ë³€í™˜ ì™„ë£Œ")
    print("âš¡ ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ")
    print("ğŸ”— AutoDetector ì™„ì „ ì—°ë™ ì™„ë£Œ")
    print("ğŸ”„ BaseStepMixin í˜¸í™˜ì„± ì™„ë£Œ")
    print("âœ… 'NoneType' object has no attribute 'Tensor' ì˜¤ë¥˜ í•´ê²°")