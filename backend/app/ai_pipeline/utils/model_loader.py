# app/ai_pipeline/utils/model_loader.py (ì™„ì „íˆ ìƒˆë¡œìš´ ë²„ì „)
"""
ğŸ”„ ì™„ì „íˆ ìƒˆë¡œìš´ ModelLoader v3.0
âœ… Stepë³„ ìš”ì²­ ì •ë³´ (3ë²ˆ íŒŒì¼)ì— ì •í™•íˆ ë§ì¶¤
âœ… auto_model_detectorì™€ ì™„ë²½í•œ ë°ì´í„° êµí™˜
âœ… ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì™„ì „ ì²˜ë¦¬
âœ… M3 Max 128GB ìµœì í™”
"""

import os
import logging
import threading
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Type, Callable
from dataclasses import dataclass, field
from enum import Enum
import weakref

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    from PIL import Image
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# Stepë³„ ìš”ì²­ ì •ë³´ ì„í¬íŠ¸
from .step_model_requests import (
    STEP_MODEL_REQUESTS,
    StepModelRequestAnalyzer,
    ModelRequestInfo
)

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ Step ìš”ì²­ì— ë§ì¶˜ ë°ì´í„° êµ¬ì¡°ë“¤
# ==============================================

@dataclass
class ModelCheckpointInfo:
    """ModelLoaderê°€ ì²˜ë¦¬í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ì •ë³´"""
    primary_path: str                               # ì£¼ ëª¨ë¸ íŒŒì¼
    config_files: List[str] = field(default_factory=list)        # config.json ë“±
    required_files: List[str] = field(default_factory=list)      # í•„ìˆ˜ íŒŒì¼ë“¤
    optional_files: List[str] = field(default_factory=list)      # ì„ íƒì  íŒŒì¼ë“¤
    tokenizer_files: List[str] = field(default_factory=list)     # tokenizer ê´€ë ¨
    scheduler_files: List[str] = field(default_factory=list)     # scheduler ê´€ë ¨
    
    # Stepë³„ íŠ¹ìˆ˜ ì²´í¬í¬ì¸íŠ¸
    unet_model: Optional[str] = None                # VirtualFittingStepìš©
    vae_model: Optional[str] = None                 # VirtualFittingStepìš©
    text_encoder: Optional[str] = None              # VirtualFittingStepìš©
    body_model: Optional[str] = None                # PoseEstimationStepìš©
    hand_model: Optional[str] = None                # PoseEstimationStepìš©
    face_model: Optional[str] = None                # PoseEstimationStepìš©
    
    # ë©”íƒ€ë°ì´í„°
    total_size_mb: float = 0.0
    validation_passed: bool = False

@dataclass
class StepModelConfig:
    """Stepë³„ ëª¨ë¸ ì„¤ì • (Step ìš”ì²­ ì •ë³´ ê¸°ë°˜)"""
    # Step ê¸°ë³¸ ì •ë³´
    step_name: str                                  # Step í´ë˜ìŠ¤ëª…
    model_name: str                                 # ëª¨ë¸ ì´ë¦„
    model_class: str                                # AI ëª¨ë¸ í´ë˜ìŠ¤
    model_type: str                                 # ëª¨ë¸ íƒ€ì…
    
    # ë””ë°”ì´ìŠ¤ ë° ìµœì í™” (Step ìš”ì²­ ê·¸ëŒ€ë¡œ)
    device: str                                     # 'auto', 'mps', 'cuda', 'cpu'
    precision: str                                  # 'fp16', 'fp32'
    input_size: Tuple[int, int]                     # ì…ë ¥ í¬ê¸°
    num_classes: Optional[int]                      # í´ë˜ìŠ¤ ìˆ˜
    
    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
    checkpoints: ModelCheckpointInfo                # ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
    
    # Stepë³„ íŒŒë¼ë¯¸í„° (Step ìš”ì²­ ì •ë³´ ê·¸ëŒ€ë¡œ)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    
    # ëŒ€ì²´ ë° í´ë°±
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

class DeviceManager:
    """M3 Max íŠ¹í™” ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        
    def _detect_available_devices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íƒì§€"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            # M3 Max MPS ì§€ì› í™•ì¸
            if torch.backends.mps.is_available():
                devices.append("mps")
                self.logger.info("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
            
            # CUDA ì§€ì› í™•ì¸ (ì™¸ë¶€ GPU)
            if torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"ğŸ”¥ CUDA ë””ë°”ì´ìŠ¤: {cuda_devices}")
        
        self.logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        # M3 Max í™˜ê²½ì—ì„œëŠ” MPS ìš°ì„ 
        if "mps" in self.available_devices:
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def resolve_device(self, requested_device: str) -> str:
        """ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ë¥¼ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¡œ ë³€í™˜"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"âš ï¸ ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ {requested_device} ì‚¬ìš© ë¶ˆê°€, {self.optimal_device} ì‚¬ìš©")
            return self.optimal_device

class MemoryOptimizer:
    """M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, total_memory_gb: float = 128.0):
        self.total_memory_gb = total_memory_gb
        self.allocated_memory_gb = 0.0
        self.memory_budget = total_memory_gb * 0.7  # 70% ê¹Œì§€ë§Œ ì‚¬ìš©
        self.logger = logging.getLogger(f"{__name__}.MemoryOptimizer")
        
    def can_allocate(self, required_memory_gb: float) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return (self.allocated_memory_gb + required_memory_gb) <= self.memory_budget
    
    def allocate_memory(self, memory_gb: float) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        if self.can_allocate(memory_gb):
            self.allocated_memory_gb += memory_gb
            self.logger.debug(f"ğŸ’¾ ë©”ëª¨ë¦¬ í• ë‹¹: {memory_gb:.2f}GB (ì´: {self.allocated_memory_gb:.2f}GB)")
            return True
        return False
    
    def deallocate_memory(self, memory_gb: float):
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        self.allocated_memory_gb = max(0, self.allocated_memory_gb - memory_gb)
        self.logger.debug(f"ğŸ—‘ï¸ ë©”ëª¨ë¦¬ í•´ì œ: {memory_gb:.2f}GB (ë‚¨ì€: {self.allocated_memory_gb:.2f}GB)")
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if TORCH_AVAILABLE:
                # MPS ìºì‹œ ì •ë¦¬
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
                
                # CUDA ìºì‹œ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            
            self.logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”„ ìƒˆë¡œìš´ ModelLoader í´ë˜ìŠ¤
# ==============================================

class ModelLoader:
    """
    ğŸ”„ ì™„ì „íˆ ìƒˆë¡œìš´ ModelLoader v3.0
    âœ… Stepë³„ ìš”ì²­ ì •ë³´ì— ì •í™•íˆ ë§ì¶¤
    âœ… auto_model_detectorì™€ ì™„ë²½ ì—°ë™
    âœ… M3 Max 128GB ìµœì í™”
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.logger = logging.getLogger(f"{__name__}.ModelLoader")
        
        # ì„¤ì •
        self.config = config or {}
        
        # í•µì‹¬ ë§¤ë‹ˆì €ë“¤
        self.device_manager = DeviceManager()
        self.memory_optimizer = MemoryOptimizer()
        
        # ëª¨ë¸ ë“±ë¡ ë° ìºì‹œ
        self.registered_models: Dict[str, StepModelConfig] = {}
        self.loaded_models: Dict[str, Any] = {}
        self.model_instances: Dict[str, Any] = {}
        
        # Stepë³„ ì¸í„°í˜ì´ìŠ¤
        self.step_interfaces: Dict[str, 'StepModelInterface'] = {}
        
        # ì„±ëŠ¥ í†µê³„
        self.load_stats = {
            "total_loads": 0,
            "successful_loads": 0,
            "failed_loads": 0,
            "cache_hits": 0,
            "average_load_time": 0.0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        self.logger.info("ğŸ”„ ìƒˆë¡œìš´ ModelLoader v3.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model(
        self, 
        model_name: str, 
        model_config: Union[Dict[str, Any], StepModelConfig]
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡ (auto_model_detectorì—ì„œ í˜¸ì¶œ)"""
        try:
            with self._lock:
                # Dictë¥¼ StepModelConfigë¡œ ë³€í™˜
                if isinstance(model_config, dict):
                    step_model_config = self._dict_to_step_model_config(model_config)
                else:
                    step_model_config = model_config
                
                if not step_model_config:
                    self.logger.error(f"âŒ ì˜ëª»ëœ ëª¨ë¸ ì„¤ì •: {model_name}")
                    return False
                
                # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
                if not self._validate_checkpoints(step_model_config.checkpoints):
                    self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {model_name}")
                    return False
                
                # ë“±ë¡
                self.registered_models[model_name] = step_model_config
                
                self.logger.info(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name} ({step_model_config.step_name})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    def _dict_to_step_model_config(self, config_dict: Dict[str, Any]) -> Optional[StepModelConfig]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ StepModelConfigë¡œ ë³€í™˜"""
        try:
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë³€í™˜
            checkpoints_data = config_dict.get("checkpoints", {})
            if isinstance(checkpoints_data, dict):
                checkpoints = ModelCheckpointInfo(
                    primary_path=checkpoints_data.get("primary", ""),
                    config_files=checkpoints_data.get("config", []),
                    required_files=checkpoints_data.get("required", []),
                    optional_files=checkpoints_data.get("optional", []),
                    tokenizer_files=checkpoints_data.get("tokenizer_files", []),
                    scheduler_files=checkpoints_data.get("scheduler_files", []),
                    unet_model=checkpoints_data.get("unet"),
                    vae_model=checkpoints_data.get("vae"),
                    text_encoder=checkpoints_data.get("text_encoder"),
                    body_model=checkpoints_data.get("body_model"),
                    hand_model=checkpoints_data.get("hand_model"),
                    face_model=checkpoints_data.get("face_model"),
                    total_size_mb=checkpoints_data.get("total_size_mb", 0.0),
                    validation_passed=checkpoints_data.get("validation_passed", False)
                )
            else:
                # ê°„ë‹¨í•œ ê²½ë¡œë§Œ ì œê³µëœ ê²½ìš°
                primary_path = config_dict.get("checkpoint_path", "")
                checkpoints = ModelCheckpointInfo(
                    primary_path=primary_path,
                    validation_passed=bool(primary_path and Path(primary_path).exists())
                )
            
            # StepModelConfig ìƒì„±
            step_config = StepModelConfig(
                step_name=config_dict.get("step_name", "UnknownStep"),
                model_name=config_dict.get("name", "unknown_model"),
                model_class=config_dict.get("model_class", "BaseModel"),
                model_type=config_dict.get("model_type", "unknown"),
                device=config_dict.get("device", "auto"),
                precision=config_dict.get("precision", "fp16"),
                input_size=tuple(config_dict.get("input_size", (512, 512))),
                num_classes=config_dict.get("num_classes"),
                checkpoints=checkpoints,
                optimization_params=config_dict.get("optimization_params", {}),
                special_params=config_dict.get("special_params", {}),
                alternative_models=config_dict.get("alternative_models", []),
                fallback_config=config_dict.get("fallback_config", {}),
                priority=config_dict.get("priority", 5),
                confidence_score=config_dict.get("confidence", 0.0),
                auto_detected=config_dict.get("auto_detected", True)
            )
            
            return step_config
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _validate_checkpoints(self, checkpoints: ModelCheckpointInfo) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ê²€ì¦"""
        try:
            # ì£¼ ëª¨ë¸ íŒŒì¼ í™•ì¸
            if not checkpoints.primary_path:
                return False
            
            primary_path = Path(checkpoints.primary_path)
            if not primary_path.exists():
                self.logger.warning(f"âš ï¸ ì£¼ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {primary_path}")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size_mb = primary_path.stat().st_size / (1024 * 1024)
            if file_size_mb < 0.1:  # 100KB ë¯¸ë§Œ
                self.logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ: {file_size_mb}MB")
                return False
            
            # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
            for required_file in checkpoints.required_files:
                if required_file and not Path(required_file).exists():
                    self.logger.warning(f"âš ï¸ í•„ìˆ˜ íŒŒì¼ ì—†ìŒ: {required_file}")
                    # í•„ìˆ˜ íŒŒì¼ì´ ì—†ì–´ë„ ì¼ë‹¨ í†µê³¼ (ìœ ì—°ì„±ì„ ìœ„í•´)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    async def get_model(
        self, 
        model_name: str, 
        step_name: Optional[str] = None,
        **kwargs
    ) -> Optional[Any]:
        """
        ëª¨ë¸ ë¡œë“œ ë° ë°˜í™˜ (Stepì—ì„œ í˜¸ì¶œ)
        
        Args:
            model_name: ìš”ì²­í•  ëª¨ë¸ ì´ë¦„
            step_name: í˜¸ì¶œí•˜ëŠ” Step ì´ë¦„
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤
            
        Returns:
            ë¡œë“œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            start_time = time.time()
            
            with self._lock:
                self.load_stats["total_loads"] += 1
                
                # ìºì‹œ í™•ì¸
                cache_key = f"{model_name}_{step_name}" if step_name else model_name
                if cache_key in self.loaded_models:
                    self.load_stats["cache_hits"] += 1
                    load_time = time.time() - start_time
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name} ({load_time:.3f}ì´ˆ)")
                    return self.loaded_models[cache_key]
                
                # ë“±ë¡ëœ ëª¨ë¸ í™•ì¸
                if model_name not in self.registered_models:
                    # Stepë³„ ê¶Œì¥ ëª¨ë¸ ì‹œë„
                    if step_name:
                        recommended_model = self._get_recommended_model_for_step(step_name)
                        if recommended_model and recommended_model in self.registered_models:
                            model_name = recommended_model
                            self.logger.info(f"ğŸ¯ Step ê¶Œì¥ ëª¨ë¸ ì‚¬ìš©: {model_name}")
                        else:
                            self.logger.error(f"âŒ ëª¨ë¸ ì—†ìŒ: {model_name}, Step: {step_name}")
                            self.load_stats["failed_loads"] += 1
                            return None
                    else:
                        self.logger.error(f"âŒ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
                        self.load_stats["failed_loads"] += 1
                        return None
                
                model_config = self.registered_models[model_name]
                
                # ë””ë°”ì´ìŠ¤ ê²°ì •
                device = self.device_manager.resolve_device(model_config.device)
                
                # ë©”ëª¨ë¦¬ í™•ì¸
                estimated_memory = self._estimate_model_memory(model_config)
                if not self.memory_optimizer.can_allocate(estimated_memory):
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±: {estimated_memory:.2f}GB í•„ìš”")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                    self._cleanup_least_used_models()
                    if not self.memory_optimizer.can_allocate(estimated_memory):
                        self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                        self.load_stats["failed_loads"] += 1
                        return None
                
                # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
                model_instance = await self._load_model_instance(model_config, device, **kwargs)
                
                if model_instance:
                    # ìºì‹œì— ì €ì¥
                    self.loaded_models[cache_key] = model_instance
                    self.model_instances[model_name] = model_instance
                    
                    # ë©”ëª¨ë¦¬ í• ë‹¹ ê¸°ë¡
                    self.memory_optimizer.allocate_memory(estimated_memory)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.load_stats["successful_loads"] += 1
                    load_time = time.time() - start_time
                    self.load_stats["average_load_time"] = (
                        (self.load_stats["average_load_time"] * (self.load_stats["successful_loads"] - 1) + load_time) 
                        / self.load_stats["successful_loads"]
                    )
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} ({load_time:.2f}ì´ˆ)")
                    return model_instance
                else:
                    self.load_stats["failed_loads"] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            self.load_stats["failed_loads"] += 1
            return None
    
    async def _load_model_instance(
        self, 
        model_config: StepModelConfig, 
        device: str,
        **kwargs
    ) -> Optional[Any]:
        """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ"""
        try:
            primary_path = Path(model_config.checkpoints.primary_path)
            
            # Stepë³„ íŠ¹í™” ë¡œë”©
            if model_config.step_name == "VirtualFittingStep":
                return await self._load_diffusion_model(model_config, device, **kwargs)
            elif model_config.step_name == "HumanParsingStep":
                return await self._load_pytorch_model(model_config, device, **kwargs)
            elif model_config.step_name == "PoseEstimationStep":
                return await self._load_pose_model(model_config, device, **kwargs)
            elif model_config.step_name == "ClothSegmentationStep":
                return await self._load_segmentation_model(model_config, device, **kwargs)
            else:
                # ê¸°ë³¸ PyTorch ëª¨ë¸ ë¡œë”©
                return await self._load_pytorch_model(model_config, device, **kwargs)
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_pytorch_model(
        self, 
        model_config: StepModelConfig, 
        device: str,
        **kwargs
    ) -> Optional[Any]:
        """PyTorch ëª¨ë¸ ë¡œë”©"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.error("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return None
            
            checkpoint_path = Path(model_config.checkpoints.primary_path)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(
                checkpoint_path, 
                map_location='cpu',
                weights_only=True
            )
            
            # ëª¨ë¸ êµ¬ì¡°ëŠ” ì‹¤ì œ Stepì—ì„œ ì •ì˜í•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ë§Œ ë°˜í™˜
            model_data = {
                "checkpoint": checkpoint,
                "config": model_config,
                "device": device,
                "checkpoints_info": model_config.checkpoints
            }
            
            self.logger.debug(f"âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path.name}")
            return model_data
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_diffusion_model(
        self, 
        model_config: StepModelConfig, 
        device: str,
        **kwargs
    ) -> Optional[Any]:
        """Diffusion ëª¨ë¸ ë¡œë”© (VirtualFittingStepìš©)"""
        try:
            checkpoints = model_config.checkpoints
            
            # Diffusion ëª¨ë¸ì€ ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ë¨
            diffusion_components = {
                "primary_model": checkpoints.primary_path,
                "unet_model": checkpoints.unet_model,
                "vae_model": checkpoints.vae_model,
                "text_encoder": checkpoints.text_encoder,
                "config_files": checkpoints.config_files,
                "tokenizer_files": checkpoints.tokenizer_files,
                "scheduler_files": checkpoints.scheduler_files
            }
            
            # ì‹¤ì œ Diffusion íŒŒì´í”„ë¼ì¸ì€ Stepì—ì„œ êµ¬ì„±
            model_data = {
                "components": diffusion_components,
                "config": model_config,
                "device": device,
                "optimization_params": model_config.optimization_params,
                "special_params": model_config.special_params
            }
            
            self.logger.debug(f"âœ… Diffusion ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„: {len([v for v in diffusion_components.values() if v])}ê°œ")
            return model_data
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_pose_model(
        self, 
        model_config: StepModelConfig, 
        device: str,
        **kwargs
    ) -> Optional[Any]:
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë”© (PoseEstimationStepìš©)"""
        try:
            checkpoints = model_config.checkpoints
            
            # í¬ì¦ˆ ëª¨ë¸ì€ body, hand, face ëª¨ë¸ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŒ
            pose_components = {
                "primary_model": checkpoints.primary_path,
                "body_model": checkpoints.body_model,
                "hand_model": checkpoints.hand_model,
                "face_model": checkpoints.face_model,
                "config_files": checkpoints.config_files
            }
            
            model_data = {
                "components": pose_components,
                "config": model_config,
                "device": device,
                "special_params": model_config.special_params
            }
            
            self.logger.debug(f"âœ… í¬ì¦ˆ ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„")
            return model_data
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_segmentation_model(
        self, 
        model_config: StepModelConfig, 
        device: str,
        **kwargs
    ) -> Optional[Any]:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”© (ClothSegmentationStepìš©)"""
        try:
            return await self._load_pytorch_model(model_config, device, **kwargs)
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_recommended_model_for_step(self, step_name: str) -> Optional[str]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ì¡°íšŒ"""
        try:
            # Stepë³„ ê¶Œì¥ ëª¨ë¸ ë§¤í•‘
            step_recommendations = {
                'HumanParsingStep': 'human_parsing_graphonomy',
                'PoseEstimationStep': 'pose_estimation_openpose',
                'ClothSegmentationStep': 'cloth_segmentation_u2net',
                'GeometricMatchingStep': 'geometric_matching_gmm',
                'ClothWarpingStep': 'cloth_warping_tom',
                'VirtualFittingStep': 'virtual_fitting_stable_diffusion',
                'PostProcessingStep': 'post_processing_realesrgan',
                'QualityAssessmentStep': 'quality_assessment_clip'
            }
            
            recommended = step_recommendations.get(step_name)
            if recommended:
                return recommended
            
            # ë“±ë¡ëœ ëª¨ë¸ ì¤‘ì—ì„œ í•´ë‹¹ Stepì˜ ëª¨ë¸ ì°¾ê¸°
            for model_name, config in self.registered_models.items():
                if config.step_name == step_name:
                    return model_name
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ Step ê¶Œì¥ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _estimate_model_memory(self, model_config: StepModelConfig) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (GB)"""
        try:
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •
            base_memory = model_config.checkpoints.total_size_mb / 1024
            
            # Stepë³„ ë©”ëª¨ë¦¬ ìŠ¹ìˆ˜
            step_multipliers = {
                'VirtualFittingStep': 3.0,  # Diffusion ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
                'HumanParsingStep': 2.0,
                'PoseEstimationStep': 1.5,
                'ClothSegmentationStep': 2.5,
                'GeometricMatchingStep': 1.2,
                'ClothWarpingStep': 2.0,
                'PostProcessingStep': 1.5,
                'QualityAssessmentStep': 2.0
            }
            
            multiplier = step_multipliers.get(model_config.step_name, 1.5)
            estimated_memory = base_memory * multiplier
            
            # ìµœì†Œ ë©”ëª¨ë¦¬ ë³´ì¥
            return max(estimated_memory, 0.5)  # ìµœì†Œ 500MB
            
        except Exception as e:
            return 2.0  # ê¸°ë³¸ê°’ 2GB
    
    def _cleanup_least_used_models(self):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ë“¤ ì •ë¦¬"""
        try:
            # ê°„ë‹¨í•œ LRU ë°©ì‹ìœ¼ë¡œ ì •ë¦¬
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì •ë¦¬ ì „ëµ í•„ìš”
            if len(self.loaded_models) > 5:  # 5ê°œ ì´ìƒì´ë©´ ì •ë¦¬
                # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±°
                oldest_key = next(iter(self.loaded_models))
                removed_model = self.loaded_models.pop(oldest_key)
                
                # ë©”ëª¨ë¦¬ í•´ì œ
                if hasattr(removed_model, 'cpu'):
                    removed_model.cpu()
                
                self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {oldest_key}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str) -> 'StepModelInterface':
        """Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            if step_name not in self.step_interfaces:
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                self.logger.debug(f"ğŸ”— Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name}")
            
            return self.step_interfaces[step_name]
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            raise
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            if model_name not in self.registered_models:
                return None
            
            config = self.registered_models[model_name]
            return {
                "name": config.model_name,
                "step_name": config.step_name,
                "model_class": config.model_class,
                "model_type": config.model_type,
                "device": config.device,
                "precision": config.precision,
                "input_size": config.input_size,
                "num_classes": config.num_classes,
                "checkpoints": {
                    "primary_path": config.checkpoints.primary_path,
                    "total_size_mb": config.checkpoints.total_size_mb,
                    "validation_passed": config.checkpoints.validation_passed
                },
                "priority": config.priority,
                "confidence_score": config.confidence_score,
                "auto_detected": config.auto_detected
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def list_models(self, step_name: Optional[str] = None) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            if step_name:
                return [
                    name for name, config in self.registered_models.items()
                    if config.step_name == step_name
                ]
            else:
                return list(self.registered_models.keys())
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ModelLoader í†µê³„ ì •ë³´"""
        return {
            "registered_models": len(self.registered_models),
            "loaded_models": len(self.loaded_models),
            "memory_usage": {
                "allocated_gb": self.memory_optimizer.allocated_memory_gb,
                "budget_gb": self.memory_optimizer.memory_budget,
                "utilization_percent": (self.memory_optimizer.allocated_memory_gb / self.memory_optimizer.memory_budget) * 100
            },
            "load_statistics": self.load_stats.copy(),
            "available_devices": self.device_manager.available_devices,
            "optimal_device": self.device_manager.optimal_device
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                # ë¡œë“œëœ ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.loaded_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except:
                        pass
                
                self.loaded_models.clear()
                self.model_instances.clear()
                
                # Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
                for interface in self.step_interfaces.values():
                    try:
                        interface.cleanup()
                    except:
                        pass
                self.step_interfaces.clear()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                self.memory_optimizer.cleanup_memory()
                
                self.logger.info("âœ… ModelLoader ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”— Step ì¸í„°í˜ì´ìŠ¤ í´ë˜ìŠ¤
# ==============================================

class StepModelInterface:
    """Stepê³¼ ModelLoader ê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: ModelLoader, step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"{__name__}.StepModelInterface.{step_name}")
        
        # Stepë³„ ìºì‹œ
        self.step_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """
        Stepì—ì„œ ëª¨ë¸ ìš”ì²­
        
        Args:
            model_name: ìš”ì²­í•  ëª¨ë¸ ì´ë¦„
            **kwargs: Stepë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¡œë“œëœ ëª¨ë¸ ë˜ëŠ” ëª¨ë¸ ë°ì´í„°
        """
        try:
            # Stepë³„ ìºì‹œ í™•ì¸
            cache_key = f"{model_name}_{hash(str(kwargs))}" if kwargs else model_name
            
            with self._lock:
                if cache_key in self.step_cache:
                    self.logger.debug(f"ğŸ“¦ Step ìºì‹œ íˆíŠ¸: {model_name}")
                    return self.step_cache[cache_key]
            
            # ModelLoaderì—ì„œ ëª¨ë¸ ë¡œë“œ
            model = await self.model_loader.get_model(
                model_name=model_name,
                step_name=self.step_name,
                **kwargs
            )
            
            if model:
                with self._lock:
                    self.step_cache[cache_key] = model
                
                self.logger.info(f"âœ… Step ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                return model
            else:
                self.logger.error(f"âŒ Step ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ìš”ì²­ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def get_recommended_model(self) -> Optional[Any]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ"""
        try:
            # Stepë³„ ê¶Œì¥ ëª¨ë¸ëª… ì¡°íšŒ
            recommended_models = self.model_loader.list_models(self.step_name)
            
            if not recommended_models:
                self.logger.warning(f"âš ï¸ {self.step_name}ì— ëŒ€í•œ ë“±ë¡ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # ê°€ì¥ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ëª¨ë¸ ì„ íƒ
            best_model_name = None
            best_priority = float('inf')
            
            for model_name in recommended_models:
                model_info = self.model_loader.get_model_info(model_name)
                if model_info and model_info["priority"] < best_priority:
                    best_priority = model_info["priority"]
                    best_model_name = model_name
            
            if best_model_name:
                return await self.get_model(best_model_name)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Step ê¶Œì¥ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_step_model_config(self, model_name: str) -> Optional[Dict[str, Any]]:
        """Stepë³„ ëª¨ë¸ ì„¤ì • ì¡°íšŒ"""
        try:
            model_info = self.model_loader.get_model_info(model_name)
            if not model_info or model_info["step_name"] != self.step_name:
                return None
            
            # Step ìš”ì²­ ì •ë³´ì™€ ê²°í•©
            step_request_info = StepModelRequestAnalyzer.get_step_request_info(self.step_name)
            if step_request_info:
                default_request = step_request_info["default_request"]
                model_info.update({
                    "optimization_params": default_request.optimization_params,
                    "special_params": default_request.special_params,
                    "checkpoint_requirements": default_request.checkpoint_requirements
                })
            
            return model_info
            
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup(self):
        """Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ë“¤ ì •ë¦¬
                for model in self.step_cache.values():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except:
                        pass
                
                self.step_cache.clear()
                
            self.logger.debug(f"âœ… {self.step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(config)
            logger.info("ğŸŒ ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_model_loader

def cleanup_global_model_loader():
    """ì „ì—­ ModelLoader ì •ë¦¬"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
            logger.info("ğŸŒ ì „ì—­ ModelLoader ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

async def load_model_for_step(
    step_name: str, 
    model_name: Optional[str] = None,
    **kwargs
) -> Optional[Any]:
    """Stepë³„ ëª¨ë¸ ë¡œë“œ í¸ì˜ í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        interface = loader.create_step_interface(step_name)
        
        if model_name:
            return await interface.get_model(model_name, **kwargs)
        else:
            return await interface.get_recommended_model()
            
    except Exception as e:
        logger.error(f"âŒ Step ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {step_name}: {e}")
        return None

def register_auto_detected_models(detected_models: Dict[str, Any]) -> Dict[str, bool]:
    """auto_model_detectorì—ì„œ íƒì§€ëœ ëª¨ë¸ë“¤ ì¼ê´„ ë“±ë¡"""
    try:
        loader = get_global_model_loader()
        registration_results = {}
        
        for model_name, model_config in detected_models.items():
            success = loader.register_model(model_name, model_config)
            registration_results[model_name] = success
        
        successful_count = sum(registration_results.values())
        logger.info(f"ğŸ”— ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡: {successful_count}/{len(detected_models)}ê°œ ì„±ê³µ")
        
        return registration_results
        
    except Exception as e:
        logger.error(f"âŒ ìë™ íƒì§€ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return {}

# ==============================================
# ğŸ”¥ ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤ ì¶”ê°€
# ==============================================

class ModelCache:
    """ëª¨ë¸ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, max_models: int = 10, max_memory_gb: float = 32.0):
        self.max_models = max_models
        self.max_memory_gb = max_memory_gb
        self.cached_models: Dict[str, Any] = {}
        self.access_times: Dict[str, float] = {}
        self.memory_usage: Dict[str, float] = {}
        self.load_times: Dict[str, float] = {}
        self._lock = threading.RLock()
        self.logger = logging.getLogger(f"{__name__}.ModelCache")
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ëª¨ë¸ ì¡°íšŒ"""
        with self._lock:
            if key in self.cached_models:
                self.access_times[key] = time.time()
                self.logger.debug(f"ğŸ“¦ ìºì‹œ íˆíŠ¸: {key}")
                return self.cached_models[key]
            return None
    
    def put(self, key: str, model: Any, memory_usage_gb: float = 0.0):
        """ìºì‹œì— ëª¨ë¸ ì €ì¥"""
        with self._lock:
            # ë©”ëª¨ë¦¬ í•œë„ í™•ì¸
            current_memory = sum(self.memory_usage.values())
            if current_memory + memory_usage_gb > self.max_memory_gb:
                self._evict_models(memory_usage_gb)
            
            # ëª¨ë¸ ìˆ˜ í•œë„ í™•ì¸
            if len(self.cached_models) >= self.max_models:
                self._evict_oldest()
            
            self.cached_models[key] = model
            self.access_times[key] = time.time()
            self.memory_usage[key] = memory_usage_gb
            self.load_times[key] = time.time()
            
            self.logger.debug(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key} ({memory_usage_gb:.2f}GB)")
    
    def _evict_models(self, required_memory: float):
        """ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•œ ëª¨ë¸ ì œê±°"""
        # LRU ë°©ì‹ìœ¼ë¡œ ì œê±°
        sorted_models = sorted(
            self.access_times.items(),
            key=lambda x: x[1]
        )
        
        freed_memory = 0.0
        for key, _ in sorted_models:
            if freed_memory >= required_memory:
                break
            
            freed_memory += self.memory_usage.get(key, 0.0)
            self._remove_model(key)
    
    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ ëª¨ë¸ ì œê±°"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        self._remove_model(oldest_key)
    
    def _remove_model(self, key: str):
        """ëª¨ë¸ ì œê±°"""
        if key in self.cached_models:
            model = self.cached_models[key]
            if hasattr(model, 'cpu'):
                model.cpu()
            
            del self.cached_models[key]
            del self.access_times[key]
            del self.memory_usage[key]
            if key in self.load_times:
                del self.load_times[key]
            
            self.logger.debug(f"ğŸ—‘ï¸ ìºì‹œ ì œê±°: {key}")
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì •ë¦¬"""
        with self._lock:
            for model in self.cached_models.values():
                if hasattr(model, 'cpu'):
                    model.cpu()
            
            self.cached_models.clear()
            self.access_times.clear()
            self.memory_usage.clear()
            self.load_times.clear()
            
            self.logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        with self._lock:
            total_memory = sum(self.memory_usage.values())
            return {
                "cached_models": len(self.cached_models),
                "max_models": self.max_models,
                "total_memory_gb": total_memory,
                "max_memory_gb": self.max_memory_gb,
                "memory_utilization": (total_memory / self.max_memory_gb) * 100,
                "models": list(self.cached_models.keys())
            }

class BatchModelLoader:
    """ë°°ì¹˜ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_loader: 'ModelLoader'):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{__name__}.BatchModelLoader")
        
    async def load_models_batch(
        self, 
        model_requests: List[Tuple[str, str]], # (model_name, step_name)
        max_concurrent: int = 3
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ë¡œë”©"""
        try:
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def load_single_model(model_name: str, step_name: str):
                async with semaphore:
                    try:
                        model = await self.model_loader.get_model(model_name, step_name)
                        return model_name, model, None
                    except Exception as e:
                        return model_name, None, str(e)
            
            # ë™ì‹œ ë¡œë”© ì‹¤í–‰
            tasks = [
                load_single_model(model_name, step_name)
                for model_name, step_name in model_requests
            ]
            
            results = await asyncio.gather(*tasks)
            
            # ê²°ê³¼ ì •ë¦¬
            loaded_models = {}
            failed_models = {}
            
            for model_name, model, error in results:
                if model is not None:
                    loaded_models[model_name] = model
                else:
                    failed_models[model_name] = error
            
            batch_result = {
                "loaded_models": loaded_models,
                "failed_models": failed_models,
                "success_rate": len(loaded_models) / len(model_requests) if model_requests else 0.0,
                "total_requested": len(model_requests)
            }
            
            self.logger.info(f"ğŸ“¦ ë°°ì¹˜ ë¡œë”© ì™„ë£Œ: {len(loaded_models)}/{len(model_requests)}ê°œ ì„±ê³µ")
            return batch_result
            
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

class ModelWarmer:
    """ëª¨ë¸ ì›Œë°ì—… ë° í”„ë¦¬ë¡œë”©"""
    
    def __init__(self, model_loader: 'ModelLoader'):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{__name__}.ModelWarmer")
        self.warmed_models: Set[str] = set()
        
    async def warmup_critical_models(self) -> Dict[str, Any]:
        """ì¤‘ìš” ëª¨ë¸ë“¤ ì›Œë°ì—…"""
        try:
            critical_models = [
                ("human_parsing_graphonomy", "HumanParsingStep"),
                ("virtual_fitting_stable_diffusion", "VirtualFittingStep"),
                ("pose_estimation_openpose", "PoseEstimationStep")
            ]
            
            warmup_results = {
                "warmed_models": [],
                "failed_models": [],
                "warmup_time": 0.0
            }
            
            start_time = time.time()
            
            for model_name, step_name in critical_models:
                try:
                    # ëª¨ë¸ ë¡œë“œë§Œ ìˆ˜í–‰ (ì‹¤ì œ ì¶”ë¡ ì€ í•˜ì§€ ì•ŠìŒ)
                    model = await self.model_loader.get_model(model_name, step_name)
                    if model:
                        self.warmed_models.add(model_name)
                        warmup_results["warmed_models"].append(model_name)
                        self.logger.debug(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {model_name}")
                    
                except Exception as e:
                    warmup_results["failed_models"].append({
                        "model": model_name,
                        "error": str(e)
                    })
                    self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨ {model_name}: {e}")
            
            warmup_results["warmup_time"] = time.time() - start_time
            
            self.logger.info(f"ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ: {len(warmup_results['warmed_models'])}ê°œ")
            return warmup_results
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def is_warmed(self, model_name: str) -> bool:
        """ëª¨ë¸ ì›Œë°ì—… ìƒíƒœ í™•ì¸"""
        return model_name in self.warmed_models

class ModelHealthChecker:
    """ëª¨ë¸ ìƒíƒœ ë° ê±´ê°•ì„± ì²´í¬"""
    
    def __init__(self, model_loader: 'ModelLoader'):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{__name__}.ModelHealthChecker")
        
    async def check_all_models(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ ìƒíƒœ ì²´í¬"""
        try:
            health_report = {
                "healthy_models": [],
                "unhealthy_models": [],
                "missing_models": [],
                "total_models": 0,
                "overall_health": 0.0,
                "recommendations": []
            }
            
            model_names = self.model_loader.list_models()
            health_report["total_models"] = len(model_names)
            
            for model_name in model_names:
                try:
                    model_info = self.model_loader.get_model_info(model_name)
                    if not model_info:
                        health_report["missing_models"].append(model_name)
                        continue
                    
                    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
                    checkpoint_path = Path(model_info["checkpoints"]["primary_path"])
                    if not checkpoint_path.exists():
                        health_report["unhealthy_models"].append({
                            "model": model_name,
                            "issue": "Checkpoint file missing",
                            "path": str(checkpoint_path)
                        })
                        continue
                    
                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
                    expected_size = model_info["checkpoints"]["total_size_mb"]
                    
                    if abs(file_size_mb - expected_size) > expected_size * 0.1:  # 10% ì˜¤ì°¨
                        health_report["unhealthy_models"].append({
                            "model": model_name,
                            "issue": "File size mismatch",
                            "expected": expected_size,
                            "actual": file_size_mb
                        })
                        continue
                    
                    health_report["healthy_models"].append(model_name)
                    
                except Exception as e:
                    health_report["unhealthy_models"].append({
                        "model": model_name,
                        "issue": str(e)
                    })
            
            # ì „ì²´ ê±´ê°•ë„ ê³„ì‚°
            total_checked = len(health_report["healthy_models"]) + len(health_report["unhealthy_models"])
            if total_checked > 0:
                health_report["overall_health"] = len(health_report["healthy_models"]) / total_checked
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if health_report["unhealthy_models"]:
                health_report["recommendations"].append("Fix unhealthy models before production use")
            
            if health_report["missing_models"]:
                health_report["recommendations"].append("Re-run model detection to find missing models")
            
            if health_report["overall_health"] < 0.8:
                health_report["recommendations"].append("Overall model health is low - consider system maintenance")
            
            self.logger.info(f"ğŸ¥ ëª¨ë¸ ê±´ê°•ì„± ì²´í¬ ì™„ë£Œ: {len(health_report['healthy_models'])}/{health_report['total_models']}ê°œ ì •ìƒ")
            return health_report
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê±´ê°•ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ModelLoader í´ë˜ìŠ¤ì— ì¶”ê°€í•  ë©”ì„œë“œë“¤
def add_missing_methods_to_modelloader():
    """ModelLoaderì— ëˆ„ë½ëœ ë©”ì„œë“œë“¤ ì¶”ê°€"""
    
    def preload_models(self, model_names: List[str]) -> Dict[str, Any]:
        """ëª¨ë¸ë“¤ ì‚¬ì „ ë¡œë”©"""
        try:
            preload_results = {
                "preloaded": [],
                "failed": [],
                "total_time": 0.0
            }
            
            start_time = time.time()
            
            for model_name in model_names:
                try:
                    # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    model = loop.run_until_complete(self.get_model(model_name))
                    if model:
                        preload_results["preloaded"].append(model_name)
                    
                    loop.close()
                    
                except Exception as e:
                    preload_results["failed"].append({
                        "model": model_name,
                        "error": str(e)
                    })
            
            preload_results["total_time"] = time.time() - start_time
            
            self.logger.info(f"ğŸ“¦ ëª¨ë¸ ì‚¬ì „ ë¡œë”©: {len(preload_results['preloaded'])}/{len(model_names)}ê°œ ì„±ê³µ")
            return preload_results
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
        try:
            status = {
                "registered": model_name in self.registered_models,
                "loaded": False,
                "cached": False,
                "memory_usage_gb": 0.0,
                "last_access": None,
                "load_count": 0
            }
            
            # ìºì‹œ ìƒíƒœ í™•ì¸
            for cache_key in self.loaded_models.keys():
                if model_name in cache_key:
                    status["loaded"] = True
                    status["cached"] = True
                    break
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            if model_name in self.registered_models:
                config = self.registered_models[model_name]
                status["memory_usage_gb"] = self._estimate_model_memory(config)
            
            return status
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        try:
            optimization_result = {
                "before_memory_gb": self.memory_optimizer.allocated_memory_gb,
                "cleaned_models": [],
                "memory_freed_gb": 0.0,
                "after_memory_gb": 0.0
            }
            
            # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ ì •ë¦¬
            self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_optimizer.cleanup_memory()
            
            optimization_result["after_memory_gb"] = self.memory_optimizer.allocated_memory_gb
            optimization_result["memory_freed_gb"] = (
                optimization_result["before_memory_gb"] - 
                optimization_result["after_memory_gb"]
            )
            
            self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™”: {optimization_result['memory_freed_gb']:.2f}GB í•´ì œ")
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    # ModelLoader í´ë˜ìŠ¤ì— ë©”ì„œë“œ ë™ì  ì¶”ê°€
    ModelLoader.preload_models = preload_models
    ModelLoader.get_model_status = get_model_status
    ModelLoader.optimize_memory_usage = optimize_memory_usage

# ì´ˆê¸°í™” ì‹œ ë©”ì„œë“œ ì¶”ê°€
add_missing_methods_to_modelloader()

# ì „ì—­ í•¨ìˆ˜ë“¤
async def batch_load_models_for_steps(step_models: Dict[str, str]) -> Dict[str, Any]:
    """Stepë³„ ëª¨ë¸ ë°°ì¹˜ ë¡œë”©"""
    try:
        loader = get_global_model_loader()
        batch_loader = BatchModelLoader(loader)
        
        model_requests = [(model_name, step_name) for step_name, model_name in step_models.items()]
        return await batch_loader.load_models_batch(model_requests)
        
    except Exception as e:
        logger.error(f"âŒ Stepë³„ ë°°ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def warmup_system_models() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ëª¨ë¸ë“¤ ì›Œë°ì—…"""
    try:
        loader = get_global_model_loader()
        warmer = ModelWarmer(loader)
        
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(warmer.warmup_critical_models())
        loop.close()
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def check_system_model_health() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ëª¨ë¸ ê±´ê°•ì„± ì²´í¬"""
    try:
        loader = get_global_model_loader()
        health_checker = ModelHealthChecker(loader)
        
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(health_checker.check_all_models())
        loop.close()
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ëª¨ë¸ ê±´ê°•ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ ì—…ë°ì´íŠ¸
__all__ = [
    # ê¸°ì¡´ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'StepModelConfig',
    'ModelCheckpointInfo',
    'DeviceManager',
    'MemoryOptimizer',
    
    # ìƒˆë¡œ ì¶”ê°€ëœ í´ë˜ìŠ¤ë“¤
    'ModelCache',
    'BatchModelLoader',
    'ModelWarmer',
    'ModelHealthChecker',
    
    # ê¸°ì¡´ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'cleanup_global_model_loader',
    'load_model_for_step',
    'register_auto_detected_models',
    
    # ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜ë“¤
    'batch_load_models_for_steps',
    'warmup_system_models',
    'check_system_model_health'
]