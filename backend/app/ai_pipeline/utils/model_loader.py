# app/ai_pipeline/utils/model_loader.py
"""
ğŸ M3 Max ìµœì í™” ì‹¤ì œ AI ëª¨ë¸ ë¡œë” - ì™„ì „í•œ ê¸°ëŠ¥ ë³µì› + Step ì—°ë™
âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš© + ëª¨ë“  ê¸°ëŠ¥ ë³µì› + Step í´ë˜ìŠ¤ í†µí•© + ì‹¤ì œ AI ëª¨ë¸ë“¤
- 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ë“¤
- M3 Max MPS ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”©
- ModelRegistry, ModelMemoryManager í¬í•¨
- ì‹¤ì œ PyTorch ëª¨ë¸ í´ë˜ìŠ¤ë“¤ í¬í•¨
- Step í´ë˜ìŠ¤ì™€ ì™„ë²½ ì—°ë™
"""

import os
import gc
import time
import threading
import asyncio
import hashlib
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable
from abc import ABC, abstractmethod
import json
import pickle
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import weakref

# PyTorch import (ì•ˆì „)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None

# ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import cv2
    import numpy as np
    from PIL import Image, ImageEnhance
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False

# ì™¸ë¶€ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬: ModelFormat í´ë˜ìŠ¤ - main.pyì—ì„œ ìš”êµ¬
# ==============================================

class ModelFormat(Enum):
    """ğŸ”¥ ëª¨ë¸ í¬ë§· ì •ì˜ - main.pyì—ì„œ í•„ìˆ˜"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    ONNX = "onnx"
    DIFFUSERS = "diffusers"
    TRANSFORMERS = "transformers"
    CHECKPOINT = "checkpoint"
    PICKLE = "pickle"
    COREML = "coreml"
    TENSORRT = "tensorrt"

class ModelPrecision(Enum):
    """ëª¨ë¸ ì •ë°€ë„ ì •ì˜"""
    FP32 = "fp32"
    FP16 = "fp16"
    BF16 = "bf16"
    INT8 = "int8"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    DIFFUSION = "diffusion"
    SEGMENTATION = "segmentation"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    optimization_level: str = "balanced"
    cache_enabled: bool = True
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if isinstance(self.model_type, str):
            self.model_type = ModelType(self.model_type)

# ==============================================
# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€
# ==============================================

class ModelRegistry:
    """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - ì•ˆì „í•œ ë²„ì „"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not getattr(self, '_initialized', False):
            self.registered_models: Dict[str, Dict[str, Any]] = {}
            self._lock = threading.RLock()
            self._initialized = True
            logger.info("ModelRegistry ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model(self, 
                      name: str, 
                      model_class: Type, 
                      default_config: Dict[str, Any] = None,
                      loader_func: Optional[Callable] = None):
        """ëª¨ë¸ ë“±ë¡"""
        with self._lock:
            try:
                self.registered_models[name] = {
                    'class': model_class,
                    'config': default_config or {},
                    'loader': loader_func,
                    'registered_at': time.time()
                }
                logger.info(f"ëª¨ë¸ ë“±ë¡: {name}")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            try:
                return self.registered_models.get(name)
            except Exception as e:
                logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ {name}: {e}")
                return None
    
    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            try:
                return list(self.registered_models.keys())
            except Exception as e:
                logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return []
    
    def unregister_model(self, name: str) -> bool:
        """ëª¨ë¸ ë“±ë¡ í•´ì œ"""
        with self._lock:
            try:
                if name in self.registered_models:
                    del self.registered_models[name]
                    logger.info(f"ëª¨ë¸ ë“±ë¡ í•´ì œ: {name}")
                    return True
                return False
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë“±ë¡ í•´ì œ ì‹¤íŒ¨ {name}: {e}")
                return False

# ==============================================
# ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€
# ==============================================

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.memory_threshold = 0.8
        
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ë°˜í™˜"""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 64.0
            else:
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    return memory.available / 1024**3
                except ImportError:
                    return 8.0
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 8.0
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif self.device == "mps" and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except:
                    pass
            
            logger.debug("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            available_memory = self.get_available_memory()
            if available_memory < 2.0:  # 2GB ë¯¸ë§Œ
                return True
            return False
        except Exception:
            return False

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ì™€ ModelLoader ì—°ë™ ì¸í„°í˜ì´ìŠ¤
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ì™€ ModelLoader ê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.loaded_models: Dict[str, Any] = {}
        self._lock = threading.RLock()
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Stepì—ì„œ í•„ìš”í•œ ëª¨ë¸ ìš”ì²­"""
        try:
            with self._lock:
                cache_key = f"{self.step_name}_{model_name}"
                
                # ìºì‹œ í™•ì¸
                if cache_key in self.loaded_models:
                    return self.loaded_models[cache_key]
                
                # ëª¨ë¸ ë¡œë“œ
                model = await self.model_loader.load_model(model_name, **kwargs)
                
                if model:
                    self.loaded_models[cache_key] = model
                    logger.info(f"ğŸ“¦ {self.step_name}ì— {model_name} ëª¨ë¸ ì „ë‹¬ ì™„ë£Œ")
                
                return model
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_recommended_model(self) -> Optional[Any]:
        """Stepë³„ ê¶Œì¥ ëª¨ë¸ ìë™ ì„ íƒ"""
        model_recommendations = {
            'HumanParsingStep': 'human_parsing_graphonomy',
            'PoseEstimationStep': 'pose_estimation_openpose', 
            'ClothSegmentationStep': 'cloth_segmentation_u2net',
            'GeometricMatchingStep': 'geometric_matching_gmm',
            'ClothWarpingStep': 'cloth_warping_tom',
            'VirtualFittingStep': 'virtual_fitting_hrviton',
            'PostProcessingStep': 'post_processing_enhancer',
            'QualityAssessmentStep': 'quality_assessment_combined'
        }
        
        recommended_model = model_recommendations.get(self.step_name)
        if recommended_model:
            return await self.get_model(recommended_model)
        
        logger.warning(f"âš ï¸ {self.step_name}ì— ëŒ€í•œ ê¶Œì¥ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def unload_models(self):
        """Stepì˜ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                for model_name, model in self.loaded_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                
                self.loaded_models.clear()
                logger.info(f"ğŸ—‘ï¸ {self.step_name} ëª¨ë¸ë“¤ ì–¸ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ {self.step_name} ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==============================================
# ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ - ì›ë³¸ ì „ì²´ ë³µì›
# ==============================================

class GraphonomyModel(nn.Module):
    """Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ - Step 01"""
    
    def __init__(self, num_classes=20, backbone='resnet101'):
        super().__init__()
        self.num_classes = num_classes
        self.backbone_name = backbone
        
        # ResNet ë°±ë³¸ êµ¬ì„±
        self.backbone = self._build_backbone()
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = self._build_aspp()
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)
        
        # ë³´ì¡° ë¶„ë¥˜ê¸°
        self.aux_classifier = nn.Conv2d(1024, num_classes, kernel_size=1)
        
    def _build_backbone(self):
        """ResNet ë°±ë³¸ êµ¬ì„±"""
        try:
            import torchvision.models as models
            if self.backbone_name == 'resnet101':
                backbone = models.resnet101(pretrained=True)
            else:
                backbone = models.resnet50(pretrained=True)
                
            # Atrous convolutionì„ ìœ„í•œ stride ìˆ˜ì •
            backbone.layer3[0].conv2.stride = (1, 1)
            backbone.layer3[0].downsample[0].stride = (1, 1)
            backbone.layer4[0].conv2.stride = (1, 1)
            backbone.layer4[0].downsample[0].stride = (1, 1)
            
            # Dilation ì ìš©
            for module in backbone.layer3[1:]:
                module.conv2.dilation = (2, 2)
                module.conv2.padding = (2, 2)
            for module in backbone.layer4:
                module.conv2.dilation = (4, 4)
                module.conv2.padding = (4, 4)
                
            return nn.Sequential(*list(backbone.children())[:-2])
        except ImportError:
            # torchvision ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ë°±ë³¸
            return nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, 2, 1),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, 3, 1, 1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 1024, 3, 1, 1),
                nn.BatchNorm2d(1024),
                nn.ReLU(inplace=True),
                nn.Conv2d(1024, 2048, 3, 1, 1),
                nn.BatchNorm2d(2048),
                nn.ReLU(inplace=True)
            )
    
    def _build_aspp(self):
        """ASPP ëª¨ë“ˆ êµ¬ì„±"""
        return nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(2048, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(2048, 256, 3, padding=6, dilation=6, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(2048, 256, 3, padding=12, dilation=12, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.Conv2d(2048, 256, 3, padding=18, dilation=18, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ),
            nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Conv2d(2048, 256, 1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            )
        ])
    
    def forward(self, x):
        input_size = x.size()[2:]
        
        # ë°±ë³¸ í†µê³¼
        features = self.backbone(x)
        
        # ASPP ì ìš©
        aspp_outputs = []
        for aspp_layer in self.aspp[:-1]:
            aspp_outputs.append(aspp_layer(features))
        
        # Global average pooling
        global_feat = self.aspp[-1](features)
        global_feat = F.interpolate(global_feat, size=features.size()[2:], 
                                   mode='bilinear', align_corners=False)
        aspp_outputs.append(global_feat)
        
        # íŠ¹ì§• ìœµí•©
        fused = torch.cat(aspp_outputs, dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.classifier(fused)
        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
        
        return output

class OpenPoseModel(nn.Module):
    """OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸ - Step 02"""
    
    def __init__(self, num_keypoints=18):
        super().__init__()
        self.num_keypoints = num_keypoints
        
        # VGG-19 ë°±ë³¸
        self.backbone = self._build_vgg_backbone()
        
        # 6ë‹¨ê³„ ë°˜ë³µ ì²˜ë¦¬
        self.stages = nn.ModuleList()
        for i in range(6):
            if i == 0:
                # ì²« ë²ˆì§¸ ìŠ¤í…Œì´ì§€
                stage = nn.ModuleDict({
                    'paf': self._build_initial_stage(38),  # 19 limbs * 2 (x,y)
                    'heatmap': self._build_initial_stage(19)  # 18 keypoints + 1 background
                })
            else:
                # í›„ì† ìŠ¤í…Œì´ì§€
                stage = nn.ModuleDict({
                    'paf': self._build_refinement_stage(38),
                    'heatmap': self._build_refinement_stage(19)
                })
            self.stages.append(stage)
    
    def _build_vgg_backbone(self):
        """VGG-19 ë°±ë³¸ êµ¬ì„±"""
        try:
            import torchvision.models as models
            vgg = models.vgg19(pretrained=True).features
            # Conv4_4ê¹Œì§€ë§Œ ì‚¬ìš©
            return nn.Sequential(*list(vgg.children())[:23])
        except ImportError:
            # VGG ëŒ€ì²´ ë°±ë³¸
            return nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True)
            )
    
    def _build_initial_stage(self, output_channels):
        """ì´ˆê¸° ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(512, output_channels, 1, 1, 0)
        )
    
    def _build_refinement_stage(self, output_channels):
        """ê°œì„  ìŠ¤í…Œì´ì§€ êµ¬ì„±"""
        return nn.Sequential(
            nn.Conv2d(512 + 38 + 19, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 7, 1, 3), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(128, output_channels, 1, 1, 0)
        )
    
    def forward(self, x):
        # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        stage_outputs = []
        
        for i, stage in enumerate(self.stages):
            if i == 0:
                # ì²« ë²ˆì§¸ ìŠ¤í…Œì´ì§€
                paf = stage['paf'](features)
                heatmap = stage['heatmap'](features)
            else:
                # ì´ì „ ê²°ê³¼ì™€ íŠ¹ì§• ê²°í•©
                combined = torch.cat([features, prev_paf, prev_heatmap], dim=1)
                paf = stage['paf'](combined)
                heatmap = stage['heatmap'](combined)
            
            stage_outputs.append((paf, heatmap))
            prev_paf, prev_heatmap = paf, heatmap
        
        return stage_outputs

class U2NetModel(nn.Module):
    """UÂ²-Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ - Step 03"""
    
    def __init__(self, in_ch=3, out_ch=1):
        super().__init__()
        
        # ì¸ì½”ë” (6ë‹¨ê³„ RSU ë¸”ë¡)
        self.stage1 = RSU7(in_ch, 32, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage2 = RSU6(64, 32, 128)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage3 = RSU5(128, 64, 256)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage4 = RSU4(256, 128, 512)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage5 = RSU4F(512, 256, 512)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        
        self.stage6 = RSU4F(512, 256, 512)
        
        # ë””ì½”ë”
        self.stage5d = RSU4F(1024, 256, 512)
        self.stage4d = RSU4(1024, 128, 256)
        self.stage3d = RSU5(512, 64, 128)
        self.stage2d = RSU6(256, 32, 64)
        self.stage1d = RSU7(128, 16, 64)
        
        # ì‚¬ì´ë“œ ì¶œë ¥ë“¤
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)
        
        # ìµœì¢… ìœµí•©
        self.outconv = nn.Conv2d(6 * out_ch, out_ch, 1)
    
    def forward(self, x):
        hx = x
        
        # ì¸ì½”ë”
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        
        hx6 = self.stage6(hx)
        
        # ë””ì½”ë”
        hx6up = F.interpolate(hx6, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))
        
        # ì‚¬ì´ë“œ ì¶œë ¥ë“¤
        d1 = self.side1(hx1d)
        d2 = self.side2(hx2d)
        d2 = F.interpolate(d2, size=x.shape[2:], mode='bilinear', align_corners=False)
        d3 = self.side3(hx3d)
        d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=False)
        d4 = self.side4(hx4d)
        d4 = F.interpolate(d4, size=x.shape[2:], mode='bilinear', align_corners=False)
        d5 = self.side5(hx5d)
        d5 = F.interpolate(d5, size=x.shape[2:], mode='bilinear', align_corners=False)
        d6 = self.side6(hx6)
        d6 = F.interpolate(d6, size=x.shape[2:], mode='bilinear', align_corners=False)
        
        # ìµœì¢… ìœµí•©
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))
        
        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)

# RSU ë¸”ë¡ë“¤ (UÂ²-Net êµ¬ì„± ìš”ì†Œ) - ì „ì²´ êµ¬í˜„
class RSU7(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv6d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = F.interpolate(hx6d, size=hx5.shape[2:], mode='bilinear', align_corners=False)
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin

class RSU6(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU6, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv5d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        hx5 = self.rebnconv5(hx)
        hx6 = self.rebnconv6(hx5)
        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
        hx5dup = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin

class RSU5(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU5, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv4d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx5 = self.rebnconv5(hx4)
        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), 1))
        hx4dup = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin

class RSU4(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx4 = self.rebnconv4(hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx3dup = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin

class RSU4F(nn.Module):
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4F, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=4)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=8)
        self.rebnconv3d = REBNCONV(mid_ch*2, mid_ch, dirate=4)
        self.rebnconv2d = REBNCONV(mid_ch*2, mid_ch, dirate=2)
        self.rebnconv1d = REBNCONV(mid_ch*2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx2 = self.rebnconv2(hx1)
        hx3 = self.rebnconv3(hx2)
        hx4 = self.rebnconv4(hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx2d = self.rebnconv2d(torch.cat((hx3d, hx2), 1))
        hx1d = self.rebnconv1d(torch.cat((hx2d, hx1), 1))
        return hx1d + hxin

class REBNCONV(nn.Module):
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1*dirate, dilation=1*dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x):
        hx = self.relu_s1(self.bn_s1(self.conv_s1(x)))
        return hx

class GeometricMatchingModel(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ - Step 04"""
    
    def __init__(self, feature_size=256):
        super().__init__()
        self.feature_size = feature_size
        
        # íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬
        self.feature_extractor = self._build_feature_extractor()
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        self.correlation = self._build_correlation_layer()
        
        # íšŒê·€ ë„¤íŠ¸ì›Œí¬
        self.regression = self._build_regression_network()
        
    def _build_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True)
        )
    
    def _build_correlation_layer(self):
        """ìƒê´€ê´€ê³„ ê³„ì‚° ë ˆì´ì–´"""
        return nn.Sequential(
            nn.Conv2d(512, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_regression_network(self):
        """íšŒê·€ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(64, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),
            nn.Linear(128, 18)  # 6ê°œ TPS ì œì–´ì  * 3 (x, y, confidence)
        )
    
    def forward(self, source_img, target_img):
        # íŠ¹ì§• ì¶”ì¶œ
        source_feat = self.feature_extractor(source_img)
        target_feat = self.feature_extractor(target_img)
        
        # íŠ¹ì§• ê²°í•©
        combined_feat = torch.cat([source_feat, target_feat], dim=1)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_map = self.correlation(combined_feat)
        
        # TPS íŒŒë¼ë¯¸í„° íšŒê·€
        tps_params = self.regression(correlation_map)
        
        return {
            'correlation_map': correlation_map,
            'tps_params': tps_params.view(-1, 6, 3),  # [batch, 6_points, (x,y,conf)]
            'source_features': source_feat,
            'target_features': target_feat
        }

class HRVITONModel(nn.Module):
    """HR-VITON ê°€ìƒ í”¼íŒ… ëª¨ë¸ - Step 06"""
    
    def __init__(self, input_nc=3, output_nc=3, ngf=64):
        super().__init__()
        
        # ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬
        self.generator = self._build_generator(input_nc, output_nc, ngf)
        
        # ì–´í…ì…˜ ëª¨ë“ˆ
        self.attention = self._build_attention_module()
        
        # ìœµí•© ëª¨ë“ˆ
        self.fusion = self._build_fusion_module()
    
    def _build_generator(self, input_nc, output_nc, ngf):
        """ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±"""
        return nn.Sequential(
            # ì¸ì½”ë”
            nn.Conv2d(input_nc, ngf, 7, 1, 3), nn.InstanceNorm2d(ngf), nn.ReLU(True),
            nn.Conv2d(ngf, ngf*2, 3, 2, 1), nn.InstanceNorm2d(ngf*2), nn.ReLU(True),
            nn.Conv2d(ngf*2, ngf*4, 3, 2, 1), nn.InstanceNorm2d(ngf*4), nn.ReLU(True),
            
            # ResNet ë¸”ë¡ë“¤
            *[ResnetBlock(ngf*4) for _ in range(9)],
            
            # ë””ì½”ë”
            nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, 1), nn.InstanceNorm2d(ngf*2), nn.ReLU(True),
            nn.ConvTranspose2d(ngf*2, ngf, 3, 2, 1, 1), nn.InstanceNorm2d(ngf), nn.ReLU(True),
            nn.Conv2d(ngf, output_nc, 7, 1, 3), nn.Tanh()
        )
    
    def _build_attention_module(self):
        """ì–´í…ì…˜ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
        )
    
    def _build_fusion_module(self):
        """ìœµí•© ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(64, 32, 3, 1, 1), nn.ReLU(True),
            nn.Conv2d(32, 3, 3, 1, 1), nn.Tanh()
        )
    
    def forward(self, person_img, cloth_img, person_parse=None):
        # ê¸°ë³¸ ìƒì„±
        input_concat = torch.cat([person_img, cloth_img], dim=1)
        generated = self.generator(input_concat)
        
        # ì–´í…ì…˜ ë§µ ê³„ì‚°
        attention_map = self.attention(input_concat)
        
        # ì–´í…ì…˜ ì ìš© ìœµí•©
        attended_result = generated * attention_map + person_img * (1 - attention_map)
        
        # ì¶”ê°€ ìœµí•©
        final_result = self.fusion(torch.cat([attended_result, cloth_img], dim=1))
        
        return {
            'generated_image': final_result,
            'attention_map': attention_map,
            'intermediate': generated
        }

class ResnetBlock(nn.Module):
    """ResNet ë¸”ë¡"""
    def __init__(self, dim, use_dropout=False):
        super().__init__()
        self.conv_block = self._build_conv_block(dim, use_dropout)

    def _build_conv_block(self, dim, use_dropout):
        layers = []
        layers += [nn.Conv2d(dim, dim, 3, 1, 1), nn.InstanceNorm2d(dim), nn.ReLU(True)]
        if use_dropout:
            layers += [nn.Dropout(0.5)]
        layers += [nn.Conv2d(dim, dim, 3, 1, 1), nn.InstanceNorm2d(dim)]
        return nn.Sequential(*layers)

    def forward(self, x):
        return x + self.conv_block(x)

# ==============================================
# ë©”ì¸ ModelLoader í´ë˜ìŠ¤ - ì™„ì „í•œ ê¸°ëŠ¥ + Step ì—°ë™
# ==============================================

class ModelLoader:
    """
    ğŸ M3 Max ìµœì í™” ì‹¤ì œ AI ëª¨ë¸ ë¡œë” - ì™„ì „í•œ ê¸°ëŠ¥ + Step ì—°ë™
    âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš© + ì™„ì „í•œ ê¸°ëŠ¥ ë³µì› + Step í´ë˜ìŠ¤ í†µí•©
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… ìµœì í™”ëœ ìƒì„±ì - Step í´ë˜ìŠ¤ ì—°ë™ ê°œì„ """
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # ëª¨ë¸ ë¡œë” íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # ğŸ”¥ ìƒˆë¡œìš´ ê¸°ëŠ¥: Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        self._interface_lock = threading.RLock()
        
        # ê¸°ì¡´ ì´ˆê¸°í™”
        self._merge_step_specific_config(kwargs)
        self.is_initialized = False
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def get_step_interface(self, step_name: str) -> Optional[StepModelInterface]:
        """ê¸°ì¡´ Step ì¸í„°í˜ì´ìŠ¤ ì¡°íšŒ"""
        with self._interface_lock:
            return self.step_interfaces.get(step_name)
    
    def cleanup_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬"""
        try:
            with self._interface_lock:
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    interface.unload_models()
                    del self.step_interfaces[step_name]
                    self.logger.info(f"ğŸ—‘ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            return 'cpu'

        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'model_cache_dir', 'use_fp16', 'max_cached_models',
            'lazy_loading', 'enable_fallback'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _initialize_step_specific(self):
        """ê¸°ë³¸ ì´ˆê¸°í™”"""
        # í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤
        self.registry = ModelRegistry()
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self.use_fp16 = True
            if COREML_AVAILABLE:
                self.logger.info("ğŸ CoreML ìµœì í™” í™œì„±í™”ë¨")
        
        # ì‹¤ì œ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
        self._initialize_model_registry()
        
        self.logger.info(f"ğŸ“¦ ì‹¤ì œ AI ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” - {self.device} (FP16: {self.use_fp16})")
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True

    def _initialize_model_registry(self):
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ ë“±ë¡ - ì •í™•í•œ ê²½ë¡œ í¬í•¨"""
        base_models_dir = self.model_cache_dir
        
        model_configs = {
            # Step 01: Human Parsing - Graphonomy
            "human_parsing_graphonomy": ModelConfig(
                name="human_parsing_graphonomy",
                model_type=ModelType.HUMAN_PARSING,
                model_class="GraphonomyModel",
                checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                input_size=(512, 512),
                num_classes=20
            ),
            
            # Step 02: Pose Estimation - OpenPose
            "pose_estimation_openpose": ModelConfig(
                name="pose_estimation_openpose", 
                model_type=ModelType.POSE_ESTIMATION,
                model_class="OpenPoseModel",
                checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                input_size=(368, 368),
                num_classes=18
            ),
            
            # Step 03: Cloth Segmentation - U2Net
            "cloth_segmentation_u2net": ModelConfig(
                name="cloth_segmentation_u2net",
                model_type=ModelType.CLOTH_SEGMENTATION, 
                model_class="U2NetModel",
                checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                input_size=(320, 320)
            ),
            
            # Step 04: Geometric Matching - GMM
            "geometric_matching_gmm": ModelConfig(
                name="geometric_matching_gmm",
                model_type=ModelType.GEOMETRIC_MATCHING,
                model_class="GeometricMatchingModel", 
                checkpoint_path=str(base_models_dir / "HR-VITON" / "gmm_final.pth"),
                input_size=(512, 384)
            ),
            
            # Step 05: Cloth Warping - TOM
            "cloth_warping_tom": ModelConfig(
                name="cloth_warping_tom",
                model_type=ModelType.CLOTH_WARPING,
                model_class="HRVITONModel",
                checkpoint_path=str(base_models_dir / "HR-VITON" / "tom_final.pth"),
                input_size=(512, 384)
            ),
            
            # Step 06: Virtual Fitting - HR-VITON
            "virtual_fitting_hrviton": ModelConfig(
                name="virtual_fitting_hrviton",
                model_type=ModelType.VIRTUAL_FITTING,
                model_class="HRVITONModel",
                checkpoint_path=str(base_models_dir / "HR-VITON" / "final.pth"),
                input_size=(512, 384)
            ),
            
            # OOTD ëŒ€ì²´ ëª¨ë¸
            "virtual_fitting_ootd": ModelConfig(
                name="virtual_fitting_ootd",
                model_type=ModelType.DIFFUSION,
                model_class="StableDiffusionPipeline",
                checkpoint_path=str(base_models_dir / "OOTDiffusion"),
                input_size=(512, 512)
            )
        }
        
        # ëª¨ë¸ ë“±ë¡
        for name, config in model_configs.items():
            self.register_model(name, config)

    def register_model(
        self,
        name: str,
        model_config: Union[ModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self._lock:
                # ModelConfig ê°ì²´ë¡œ ë³€í™˜
                if isinstance(model_config, dict):
                    model_config = ModelConfig(name=name, **model_config)
                elif not isinstance(model_config, ModelConfig):
                    raise ValueError(f"Invalid model_config type: {type(model_config)}")
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ìë™ ê°ì§€
                if model_config.device == "auto":
                    model_config.device = self.device
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                self.registry.register_model(
                    name=name,
                    model_class=self._get_model_class(model_config.model_class),
                    default_config=model_config.__dict__,
                    loader_func=loader_func
                )
                
                # ë‚´ë¶€ ì„¤ì • ì €ì¥
                self.model_configs[name] = model_config
                
                self.logger.info(f"ğŸ“ ì‹¤ì œ AI ëª¨ë¸ ë“±ë¡: {name} ({model_config.model_type.value})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False

    def _get_model_class(self, model_class_name: str) -> Type:
        """ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ ë°˜í™˜"""
        model_classes = {
            'GraphonomyModel': GraphonomyModel,
            'OpenPoseModel': OpenPoseModel,
            'U2NetModel': U2NetModel,
            'GeometricMatchingModel': GeometricMatchingModel,
            'HRVITONModel': HRVITONModel,
            'StableDiffusionPipeline': None  # íŠ¹ë³„ ì²˜ë¦¬
        }
        return model_classes.get(model_class_name, None)

    async def load_model(
        self,
        name: str,
        force_reload: bool = False,
        **kwargs
    ) -> Optional[Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ"""
        try:
            cache_key = f"{name}_{kwargs.get('config_hash', 'default')}"
            
            with self._lock:
                # ìºì‹œëœ ëª¨ë¸ í™•ì¸
                if cache_key in self.model_cache and not force_reload:
                    self.access_counts[cache_key] = self.access_counts.get(cache_key, 0) + 1
                    self.last_access[cache_key] = time.time()
                    self.logger.info(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return self.model_cache[cache_key]
                
                # ëª¨ë¸ ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    if self.enable_fallback:
                        return await self._load_fallback_model(name)
                    return None
                
                start_time = time.time()
                model_config = self.model_configs[name]
                
                self.logger.info(f"ğŸ“¦ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘: {name} ({model_config.model_type.value})")
                
                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ì •ë¦¬
                await self._check_memory_and_cleanup()
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                model = await self._create_model_instance(model_config, **kwargs)
                
                if model is None:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {name}")
                    if self.enable_fallback:
                        return await self._load_fallback_model(name)
                    return None
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                await self._load_checkpoint(model, model_config)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # M3 Max ìµœì í™” ì ìš©
                if self.is_m3_max and self.optimization_enabled:
                    model = await self._apply_m3_max_optimization(model, model_config)
                
                # FP16 ìµœì í™”
                if self.use_fp16 and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ìºì‹œì— ì €ì¥
                self.model_cache[cache_key] = model
                self.load_times[cache_key] = time.time() - start_time
                self.access_counts[cache_key] = 1
                self.last_access[cache_key] = time.time()
                
                load_time = self.load_times[cache_key]
                self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({load_time:.2f}s)")
                
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            if self.enable_fallback:
                return await self._load_fallback_model(name)
            return None

    async def _create_model_instance(
        self,
        model_config: ModelConfig,
        **kwargs
    ) -> Optional[Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class = model_config.model_class
            
            if model_class == "GraphonomyModel":
                return GraphonomyModel(
                    num_classes=model_config.num_classes or 20,
                    backbone='resnet101'
                )
            
            elif model_class == "OpenPoseModel":
                return OpenPoseModel(
                    num_keypoints=model_config.num_classes or 18
                )
            
            elif model_class == "U2NetModel":
                return U2NetModel(in_ch=3, out_ch=1)
            
            elif model_class == "GeometricMatchingModel":
                return GeometricMatchingModel(feature_size=256)
            
            elif model_class == "HRVITONModel":
                return HRVITONModel(input_nc=3, output_nc=3, ngf=64)
            
            elif model_class == "StableDiffusionPipeline":
                return await self._create_diffusion_model(model_config)
            
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    async def _create_diffusion_model(self, model_config: ModelConfig):
        """Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if DIFFUSERS_AVAILABLE:
                from diffusers import StableDiffusionPipeline
                
                if model_config.checkpoint_path and Path(model_config.checkpoint_path).exists():
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        model_config.checkpoint_path,
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                else:
                    pipeline = StableDiffusionPipeline.from_pretrained(
                        "runwayml/stable-diffusion-v1-5",
                        torch_dtype=torch.float16 if self.use_fp16 else torch.float32,
                        safety_checker=None,
                        requires_safety_checker=False
                    )
                
                return pipeline
            else:
                self.logger.warning("âš ï¸ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    async def _load_checkpoint(self, model: Any, model_config: ModelConfig):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not model_config.checkpoint_path:
            self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—†ìŒ: {model_config.name}")
            return
            
        checkpoint_path = Path(model_config.checkpoint_path)
        
        if not checkpoint_path.exists():
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {checkpoint_path}")
            return
        
        try:
            # PyTorch ëª¨ë¸ì¸ ê²½ìš°
            if hasattr(model, 'load_state_dict'):
                state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                
                # state_dict ì •ë¦¬
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif isinstance(state_dict, dict) and 'model' in state_dict:
                    state_dict = state_dict['model']
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key.replace('module.', '') if key.startswith('module.') else key
                    cleaned_state_dict[new_key] = value
                
                model.load_state_dict(cleaned_state_dict, strict=False)
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
            
            else:
                self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê±´ë„ˆëœ€ (íŒŒì´í”„ë¼ì¸): {model_config.name}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    async def _apply_m3_max_optimization(self, model: Any, model_config: ModelConfig) -> Any:
        """M3 Max íŠ¹í™” ëª¨ë¸ ìµœì í™”"""
        try:
            optimizations_applied = []
            
            # 1. MPS ë””ë°”ì´ìŠ¤ ìµœì í™”
            if self.device == 'mps' and hasattr(model, 'to'):
                optimizations_applied.append("MPS device optimization")
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (128GB M3 Max)
            if self.memory_gb >= 64:
                optimizations_applied.append("High memory optimization")
            
            # 3. CoreML ì»´íŒŒì¼ ì¤€ë¹„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if (COREML_AVAILABLE and 
                hasattr(model, 'eval') and 
                model_config.model_type in [ModelType.HUMAN_PARSING, ModelType.CLOTH_SEGMENTATION]):
                optimizations_applied.append("CoreML compilation ready")
            
            # 4. Metal Performance Shaders ìµœì í™”
            if self.device == 'mps':
                try:
                    # PyTorch MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                        torch.backends.mps.set_per_process_memory_fraction(0.8)
                    optimizations_applied.append("Metal Performance Shaders")
                except:
                    pass
            
            if optimizations_applied:
                self.logger.info(f"ğŸ M3 Max ëª¨ë¸ ìµœì í™” ì ìš©: {', '.join(optimizations_applied)}")
            
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

    async def _load_fallback_model(self, model_name: str) -> Optional[Any]:
        """ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ (MediaPipe, RemBG ë“±)"""
        try:
            fallback_model = None
            
            # ëª¨ë¸ íƒ€ì…ë³„ ëŒ€ì²´ ëª¨ë¸ (ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
            if "pose" in model_name.lower() or "pose_estimation" in model_name:
                fallback_model = self._load_mediapipe_pose()
                
            elif "parsing" in model_name.lower() or "human_parsing" in model_name:
                fallback_model = self._load_mediapipe_selfie()
                
            elif "segmentation" in model_name.lower() or "cloth_segmentation" in model_name:
                fallback_model = self._load_rembg_model()
                
            elif "matching" in model_name.lower() or "geometric" in model_name:
                fallback_model = self._create_simple_matching_model()
                
            elif "warping" in model_name.lower() or "fitting" in model_name or "viton" in model_name:
                fallback_model = self._create_simple_generation_model(model_name)
            
            if fallback_model:
                self.logger.info(f"âœ… ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            
            return fallback_model
            
        except Exception as e:
            self.logger.error(f"âŒ ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _load_mediapipe_pose(self):
        """MediaPipe Pose ëª¨ë¸"""
        try:
            if MEDIAPIPE_AVAILABLE:
                return mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=0.5
                )
            else:
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ MediaPipe Pose ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _load_mediapipe_selfie(self):
        """MediaPipe Selfie Segmentation"""
        try:
            if MEDIAPIPE_AVAILABLE:
                return mp.solutions.selfie_segmentation.SelfieSegmentation(
                    model_selection=1
                )
            else:
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ MediaPipe Selfie ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _load_rembg_model(self):
        """RemBG ë°°ê²½ ì œê±° ëª¨ë¸"""
        try:
            # rembg ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            try:
                from rembg import new_session
                return new_session("u2net")
            except ImportError:
                # ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸
                return self._create_simple_segmentation_model()
        except Exception as e:
            self.logger.warning(f"âš ï¸ RemBG ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_simple_segmentation_model()

    def _create_simple_segmentation_model(self):
        """ê°„ë‹¨í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
        if not TORCH_AVAILABLE:
            return None
            
        class SimpleSegmentationModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                )
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 1, 3, 1, 1), nn.Sigmoid()
                )
            
            def forward(self, x):
                features = self.encoder(x)
                output = self.decoder(features)
                return output
        
        model = SimpleSegmentationModel()
        model = model.to(self.device)
        return model

    def _create_simple_matching_model(self):
        """ê°„ë‹¨í•œ ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸"""
        if not TORCH_AVAILABLE:
            return None
            
        class SimpleMatchingModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.feature_net = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                    nn.AdaptiveAvgPool2d((8, 8)),
                    nn.Flatten(),
                    nn.Linear(128 * 64, 256), nn.ReLU(inplace=True),
                    nn.Linear(256, 18)  # 6ê°œ ì œì–´ì  * 3
                )
            
            def forward(self, source_img, target_img=None):
                if target_img is not None:
                    # ë‘ ì´ë¯¸ì§€ë¥¼ ê²°í•©
                    combined = torch.cat([source_img, target_img], dim=1)
                    combined = nn.functional.interpolate(combined, size=(256, 256), mode='bilinear')
                    # ì²« 3ì±„ë„ë§Œ ì‚¬ìš©
                    combined = combined[:, :3]
                else:
                    combined = source_img
                
                tps_params = self.feature_net(combined)
                return {
                    'tps_params': tps_params.view(-1, 6, 3),
                    'correlation_map': torch.ones(combined.shape[0], 1, 64, 64).to(combined.device)
                }
        
        model = SimpleMatchingModel()
        model = model.to(self.device)
        return model

    def _create_simple_generation_model(self, model_name: str):
        """ê°„ë‹¨í•œ ìƒì„± ëª¨ë¸"""
        if not TORCH_AVAILABLE:
            return None
            
        class SimpleGenerationModel(nn.Module):
            def __init__(self, model_name: str):
                super().__init__()
                self.model_name = model_name
                
                # U-Net ìŠ¤íƒ€ì¼ ìƒì„±ê¸°
                self.encoder = nn.Sequential(
                    nn.Conv2d(6, 64, 3, 1, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.ReLU(inplace=True)
                )
                
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 3, 3, 1, 1), nn.Tanh()
                )
                
                # ì–´í…ì…˜ ëª¨ë“ˆ
                self.attention = nn.Sequential(
                    nn.Conv2d(6, 32, 3, 1, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(32, 1, 1, 1, 0), nn.Sigmoid()
                )
            
            def forward(self, person_img, cloth_img, **kwargs):
                # ì…ë ¥ ê²°í•©
                combined_input = torch.cat([person_img, cloth_img], dim=1)
                
                # ìƒì„±
                features = self.encoder(combined_input)
                generated = self.decoder(features)
                
                # ì–´í…ì…˜ ì ìš©
                attention_map = self.attention(combined_input)
                
                # ìµœì¢… ê²°ê³¼
                result = generated * attention_map + person_img * (1 - attention_map)
                
                return {
                    'generated_image': result,
                    'attention_map': attention_map,
                    'warped_cloth': cloth_img,  # ê°„ë‹¨í•œ ê²½ìš°
                    'intermediate': generated
                }
        
        model = SimpleGenerationModel(model_name)
        model = model.to(self.device)
        return model

    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
            if self.memory_manager.check_memory_pressure():
                await self._cleanup_least_used_models()
            
            # ìºì‹œëœ ëª¨ë¸ ìˆ˜ í™•ì¸
            if len(self.model_cache) >= self.max_cached_models:
                await self._cleanup_least_used_models()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def _cleanup_least_used_models(self, keep_count: int = 5):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.model_cache) <= keep_count:
                    return
                
                # ì‚¬ìš© ë¹ˆë„ì™€ ìµœê·¼ ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                sorted_models = sorted(
                    self.model_cache.items(),
                    key=lambda x: (
                        self.access_counts.get(x[0], 0),
                        self.last_access.get(x[0], 0)
                    )
                )
                
                cleanup_count = len(self.model_cache) - keep_count
                cleaned_models = []
                
                for i in range(min(cleanup_count, len(sorted_models))):
                    cache_key, model = sorted_models[i]
                    
                    # ëª¨ë¸ í•´ì œ
                    del self.model_cache[cache_key]
                    self.access_counts.pop(cache_key, None)
                    self.load_times.pop(cache_key, None)
                    self.last_access.pop(cache_key, None)
                    
                    # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                    
                    cleaned_models.append(cache_key)
                
                if cleaned_models:
                    self.logger.info(f"ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬: {len(cleaned_models)}ê°œ ëª¨ë¸ í•´ì œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def unload_model(self, name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                # ìºì‹œì—ì„œ ì œê±°
                keys_to_remove = [k for k in self.model_cache.keys() 
                                 if k.startswith(f"{name}_")]
                
                removed_count = 0
                for key in keys_to_remove:
                    if key in self.model_cache:
                        model = self.model_cache[key]
                        
                        # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                        del self.model_cache[key]
                        removed_count += 1
                    
                    self.access_counts.pop(key, None)
                    self.load_times.pop(key, None)
                    self.last_access.pop(key, None)
                
                if removed_count > 0:
                    self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {name} ({removed_count}ê°œ ì¸ìŠ¤í„´ìŠ¤)")
                    self.memory_manager.cleanup_memory()
                    return True
                else:
                    self.logger.warning(f"ì–¸ë¡œë“œí•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {name}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {name}: {e}")
            return False

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            if name not in self.model_configs:
                return None
                
            config = self.model_configs[name]
            cache_keys = [k for k in self.model_cache.keys() if k.startswith(f"{name}_")]
            
            return {
                "name": name,
                "model_type": config.model_type.value,
                "model_class": config.model_class,
                "device": config.device,
                "loaded": len(cache_keys) > 0,
                "cache_instances": len(cache_keys),
                "total_access_count": sum(self.access_counts.get(k, 0) for k in cache_keys),
                "average_load_time": sum(self.load_times.get(k, 0) for k in cache_keys) / max(1, len(cache_keys)),
                "checkpoint_path": config.checkpoint_path,
                "input_size": config.input_size,
                "last_access": max((self.last_access.get(k, 0) for k in cache_keys), default=0)
            }

    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            result = {}
            for name in self.model_configs.keys():
                info = self.get_model_info(name)
                if info:
                    result[name] = info
            return result

    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            usage = {
                "loaded_models": len(self.model_cache),
                "device": self.device,
                "available_memory_gb": self.memory_manager.get_available_memory(),
                "memory_pressure": self.memory_manager.check_memory_pressure()
            }
            
            if self.device == "cuda" and torch.cuda.is_available():
                usage.update({
                    "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                    "cached_gb": torch.cuda.memory_reserved() / 1024**3
                })
            elif self.device == "mps":
                try:
                    import psutil
                    process = psutil.Process()
                    usage.update({
                        "process_memory_gb": process.memory_info().rss / 1024**3,
                        "system_memory_percent": psutil.virtual_memory().percent
                    })
                except ImportError:
                    usage["memory_info"] = "psutil not available"
            else:
                usage["memory_info"] = "cpu mode"
                
            return usage
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ë“¤ ì •ë¦¬
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    self.cleanup_step_interface(step_name)
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.warning(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ModelLoader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ModelLoader ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def initialize(self) -> bool:
        """ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
            missing_checkpoints = []
            for name, config in self.model_configs.items():
                if config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if not checkpoint_path.exists():
                        missing_checkpoints.append(name)
            
            if missing_checkpoints:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ëŠ” ëª¨ë¸ë“¤: {missing_checkpoints}")
                self.logger.info("ğŸ“ í•´ë‹¹ ëª¨ë¸ë“¤ì€ ëŒ€ì²´ ëª¨ë¸ë¡œ ë¡œë“œë©ë‹ˆë‹¤")
            
            # M3 Max ìµœì í™” ì„¤ì •
            if COREML_AVAILABLE and self.is_m3_max:
                self.logger.info("ğŸ CoreML ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
            self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ - {len(self.model_configs)}ê°œ ëª¨ë¸ ë“±ë¡ë¨")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def get_step_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë” ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "specialized_features": {
                "use_fp16": self.use_fp16,
                "lazy_loading": self.lazy_loading,
                "max_cached_models": self.max_cached_models,
                "enable_fallback": self.enable_fallback
            },
            "model_stats": {
                "registered_models": len(self.model_configs),
                "loaded_models": len(self.model_cache),
                "total_access_count": sum(self.access_counts.values()),
                "average_load_time": sum(self.load_times.values()) / len(self.load_times) if self.load_times else 0
            },
            "library_availability": {
                "torch": TORCH_AVAILABLE,
                "opencv": CV_AVAILABLE,
                "mediapipe": MEDIAPIPE_AVAILABLE,
                "transformers": TRANSFORMERS_AVAILABLE,
                "diffusers": DIFFUSERS_AVAILABLE,
                "onnx": ONNX_AVAILABLE,
                "coreml": COREML_AVAILABLE
            },
            "memory_usage": self.get_memory_usage()
        }

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                # ì „ì—­ ëª¨ë¸ ë¡œë” ì‚¬ìš©
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.warning(f"âš ï¸ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if model_name:
                return await self.model_interface.get_model(model_name)
            else:
                # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                return await self.model_interface.get_recommended_model()
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì›ë³¸ ê¸°ëŠ¥ ìœ ì§€)
# ==============================================

def preprocess_image(image: Union[np.ndarray, Image.Image], target_size: tuple, normalize: bool = True) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV not available")
            
        if isinstance(image, np.ndarray):
            if image.shape[2] == 4:  # RGBA
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
            elif len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image).astype(np.float32)
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1) / 255.0
        
        # ì •ê·œí™”
        if normalize:
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            image_tensor = (image_tensor - mean) / std
        
        return image_tensor.unsqueeze(0)
        
    except Exception as e:
        logging.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # ë”ë¯¸ í…ì„œ ë°˜í™˜
        return torch.randn(1, 3, target_size[1], target_size[0])

def postprocess_segmentation(output: torch.Tensor, original_size: tuple, threshold: float = 0.5) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCV not available")
            
        if output.dim() == 4:
            output = output.squeeze(0)
        
        # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
        if output.shape[0] > 1:
            output = torch.argmax(output, dim=0)
        else:
            output = (output > threshold).float()
        
        # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
        output = output.cpu().numpy().astype(np.uint8)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if output.shape != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        return output
        
    except Exception as e:
        logging.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return np.zeros(original_size[::-1], dtype=np.uint8)

def postprocess_pose(output: torch.Tensor, original_size: tuple, confidence_threshold: float = 0.3) -> Dict[str, Any]:
    """í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬"""
    try:
        if isinstance(output, (list, tuple)):
            # OpenPose ìŠ¤íƒ€ì¼ ì¶œë ¥ (PAF, heatmaps)
            pafs, heatmaps = output[-1]  # ë§ˆì§€ë§‰ ìŠ¤í…Œì´ì§€ ê²°ê³¼ ì‚¬ìš©
        else:
            heatmaps = output
            pafs = None
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = []
        if heatmaps.dim() == 4:
            heatmaps = heatmaps.squeeze(0)
        
        for i in range(heatmaps.shape[0] - 1):  # ë°°ê²½ ì œì™¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = heatmap[y, x]
            
            if confidence > confidence_threshold:
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                x_scaled = int(x * original_size[0] / heatmap.shape[1])
                y_scaled = int(y * original_size[1] / heatmap.shape[0])
                keypoints.append([x_scaled, y_scaled, confidence])
            else:
                keypoints.append([0, 0, 0])
        
        return {
            'keypoints': keypoints,
            'pafs': pafs.cpu().numpy() if pafs is not None else None,
            'heatmaps': heatmaps.cpu().numpy()
        }
        
    except Exception as e:
        logging.error(f"í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {'keypoints': [], 'pafs': None, 'heatmaps': None}

# ==============================================
# ì „ì—­ ëª¨ë¸ ë¡œë” ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None

@lru_cache(maxsize=1)
def get_global_model_loader() -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    try:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader()
        return _global_model_loader
    except Exception as e:
        logger.error(f"ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì†Œí•œì˜ ModelLoader ìƒì„± ì‹œë„
        return ModelLoader(device="cpu", enable_fallback=True)

def cleanup_global_loader():
    """ì „ì—­ ë¡œë” ì •ë¦¬"""
    global _global_model_loader
    
    try:
        if _global_model_loader:
            _global_model_loader.cleanup()
            _global_model_loader = None
        # ìºì‹œ í´ë¦¬ì–´
        get_global_model_loader.cache_clear()
        logger.info("âœ… ì „ì—­ ModelLoader ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_model_loader(device: str = "mps", use_fp16: bool = True, **kwargs) -> ModelLoader:
    """ëª¨ë¸ ë¡œë” ìƒì„± (í•˜ìœ„ í˜¸í™˜)"""
    return ModelLoader(device=device, use_fp16=use_fp16, **kwargs)

async def load_model_async(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        return await loader.load_model(model_name, config)
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_model_sync(model_name: str, config: Optional[ModelConfig] = None) -> Optional[Any]:
    """ì „ì—­ ë¡œë”ë¥¼ ì‚¬ìš©í•œ ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(loader.load_model(model_name, config))
    except Exception as e:
        logger.error(f"ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ğŸ”¥ í•µì‹¬: ëª¨ë¸ í¬ë§· ê°ì§€ ë° ë³€í™˜ í•¨ìˆ˜ë“¤
def detect_model_format(model_path: Union[str, Path]) -> ModelFormat:
    """íŒŒì¼ í™•ì¥ìë¡œ ëª¨ë¸ í¬ë§· ê°ì§€"""
    path = Path(model_path)
    
    if path.suffix == '.pth' or path.suffix == '.pt':
        return ModelFormat.PYTORCH
    elif path.suffix == '.safetensors':
        return ModelFormat.SAFETENSORS
    elif path.suffix == '.onnx':
        return ModelFormat.ONNX
    elif path.suffix == '.mlmodel':
        return ModelFormat.COREML
    elif path.is_dir():
        # ë””ë ‰í† ë¦¬ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨
        if (path / "config.json").exists():
            if (path / "model.safetensors").exists():
                return ModelFormat.TRANSFORMERS
            elif any(path.glob("*.bin")):
                return ModelFormat.DIFFUSERS
        return ModelFormat.DIFFUSERS  # ê¸°ë³¸ê°’
    else:
        return ModelFormat.PYTORCH  # ê¸°ë³¸ê°’

def load_model_with_format(
    model_path: Union[str, Path],
    model_format: ModelFormat,
    device: str = "mps"
) -> Any:
    """ê°„í¸í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        loader = get_global_model_loader()
        
        # ëª¨ë¸ ì„¤ì • ìƒì„±
        config = ModelConfig(
            name=Path(model_path).stem,
            model_type=ModelType.VIRTUAL_FITTING,  # ê¸°ë³¸ê°’
            model_class="HRVITONModel",
            checkpoint_path=str(model_path),
            device=device
        )
        
        # ë™ê¸° ë¡œë”©
        return load_model_sync(config.name, config)
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì•ˆì „í•œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelFormat',  # ğŸ”¥ main.py í•„ìˆ˜
    'ModelConfig', 
    'ModelType',
    'ModelPrecision',
    'ModelRegistry',
    'ModelMemoryManager',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'GraphonomyModel',
    'OpenPoseModel', 
    'U2NetModel',
    'GeometricMatchingModel',
    'HRVITONModel',
    'RSU7', 'RSU6', 'RSU5', 'RSU4', 'RSU4F', 'REBNCONV',
    'ResnetBlock',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'load_model_async',
    'load_model_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'detect_model_format',
    'load_model_with_format',
    'preprocess_image',
    'postprocess_segmentation',
    'postprocess_pose',
    'cleanup_global_loader'
]

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… ModelLoader ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ëª¨ë“  AI ëª¨ë¸ í´ë˜ìŠ¤ ë° íŒ©í† ë¦¬ í•¨ìˆ˜ í¬í•¨")