# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ê°œì„ ëœ ModelLoader v5.1 (ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì§€ì›)
================================================================================
âœ… step_interface.py v5.2ì™€ ì™„ì „ ì—°ë™ (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©)
âœ… RealStepModelInterface ìš”êµ¬ì‚¬í•­ 100% ë°˜ì˜
âœ… GitHubStepMapping ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ ì™„ì „ ë§¤í•‘
âœ… 229GB AI ëª¨ë¸ íŒŒì¼ë“¤ ì •í™•í•œ ë¡œë”© ì§€ì›
âœ… BaseStepMixin v19.2 ì™„ë²½ í˜¸í™˜
âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë²½ ì§€ì›
âœ… Mock ì™„ì „ ì œê±° - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
âœ… PyTorch weights_only ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… Auto Detector ì™„ì „ ì—°ë™
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ì‘ë™

í•µì‹¬ êµ¬ì¡° ë§¤í•‘:
StepFactory (v11.0) â†’ ì˜ì¡´ì„± ì£¼ì… â†’ BaseStepMixin (v19.2) â†’ step_interface.py (v5.2) â†’ ModelLoader (v5.1) â†’ ì‹¤ì œ AI ëª¨ë¸ë“¤

Author: MyCloset AI Team
Date: 2025-07-30
Version: 5.1 (step_interface.py v5.2 ì™„ì „ í˜¸í™˜)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
import mmap
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod
from io import BytesIO

# ==============================================
# ğŸ”¥ 1. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None


# ModelLoaderì˜ PyTorch import ë¶€ë¶„ì„ ë‹¤ìŒìœ¼ë¡œ êµì²´:

# PyTorch ì•ˆì „ import (weights_only ë¬¸ì œ ì™„ì „ í•´ê²°)
TORCH_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    # ğŸ”¥ PyTorch 2.7 weights_only ë¬¸ì œ ì™„ì „ í•´ê²°
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            """PyTorch 2.7 í˜¸í™˜ ì•ˆì „ ë¡œë”"""
            # weights_onlyê°€ Noneì´ë©´ Falseë¡œ ì„¤ì • (Legacy í˜¸í™˜)
            if weights_only is None:
                weights_only = False
            
            try:
                # 1ë‹¨ê³„: weights_only=True ì‹œë„ (ê°€ì¥ ì•ˆì „)
                if weights_only:
                    return original_torch_load(f, map_location=map_location, 
                                             pickle_module=pickle_module, 
                                             weights_only=True, **kwargs)
                
                # 2ë‹¨ê³„: weights_only=False ì‹œë„ (í˜¸í™˜ì„±)
                return original_torch_load(f, map_location=map_location, 
                                         pickle_module=pickle_module, 
                                         weights_only=False, **kwargs)
                                         
            except RuntimeError as e:
                error_msg = str(e).lower()
                
                # Legacy .tar í¬ë§· ì—ëŸ¬ ê°ì§€
                if "legacy .tar format" in error_msg or "weights_only" in error_msg:
                    print(f"âš ï¸ Legacy í¬ë§· ê°ì§€, weights_only=Falseë¡œ ì¬ì‹œë„")
                    try:
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # TorchScript ì•„ì¹´ì´ë¸Œ ì—ëŸ¬ ê°ì§€
                if "torchscript" in error_msg or "zip file" in error_msg:
                    print(f"âš ï¸ TorchScript ì•„ì¹´ì´ë¸Œ ê°ì§€, weights_only=Falseë¡œ ì¬ì‹œë„")
                    try:
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  íŒŒë¼ë¯¸í„° ì—†ì´
                try:
                    import warnings
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore")
                        return original_torch_load(f, map_location=map_location)
                except Exception:
                    pass
                
                # ì›ë³¸ ì—ëŸ¬ ë‹¤ì‹œ ë°œìƒ
                raise e
        
        # torch.load ëŒ€ì²´
        torch.load = safe_torch_load
        print("âœ… PyTorch 2.7 weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
        
except ImportError:
    torch = None
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

# ë””ë°”ì´ìŠ¤ ë° ì‹œìŠ¤í…œ ì •ë³´
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
MPS_AVAILABLE = False

try:
    import platform
    if platform.system() == 'Darwin':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            if 'M3' in result.stdout:
                IS_M3_MAX = True
                if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    DEFAULT_DEVICE = "mps"
                    MPS_AVAILABLE = True
        except:
            pass
except:
    pass

# auto_model_detector import (ì•ˆì „ ì²˜ë¦¬)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 2. step_interface.py v5.2 ì™„ì „ í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
# ==============================================

class RealStepModelType(Enum):
    """ì‹¤ì œ AI Stepì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ íƒ€ì… (step_interface.py ì™„ì „ í˜¸í™˜)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class RealModelStatus(Enum):
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ (step_interface.py í˜¸í™˜)"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class RealModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ (step_interface.py í˜¸í™˜)"""
    PRIMARY = 1
    SECONDARY = 2
    FALLBACK = 3
    OPTIONAL = 4

@dataclass
class RealStepModelInfo:
    """ì‹¤ì œ AI Step ëª¨ë¸ ì •ë³´ (step_interface.py RealAIModelConfig ì™„ì „ í˜¸í™˜)"""
    name: str
    path: str
    step_type: RealStepModelType
    priority: RealModelPriority
    device: str
    
    # ì‹¤ì œ ë¡œë”© ì •ë³´
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    checkpoint_data: Optional[Any] = None
    
    # AI Step í˜¸í™˜ì„± ì •ë³´ (step_interface.py í˜¸í™˜)
    model_class: Optional[str] = None
    config_path: Optional[str] = None
    preprocessing_params: Dict[str, Any] = field(default_factory=dict)
    
    # step_interface.py ìš”êµ¬ì‚¬í•­
    model_type: str = "BaseModel"
    size_gb: float = 0.0
    requires_checkpoint: bool = True
    checkpoint_key: Optional[str] = None
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    access_count: int = 0
    last_access: float = 0.0
    inference_count: int = 0
    avg_inference_time: float = 0.0
    
    # ì—ëŸ¬ ì •ë³´
    error: Optional[str] = None
    validation_passed: bool = False

@dataclass 
class RealStepModelRequirement:
    """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ (step_interface.py ì™„ì „ í˜¸í™˜)"""
    step_name: str
    step_id: int
    step_type: RealStepModelType
    
    # ëª¨ë¸ ìš”êµ¬ì‚¬í•­
    required_models: List[str] = field(default_factory=list)
    optional_models: List[str] = field(default_factory=list)
    primary_model: Optional[str] = None
    
    # step_interface.py DetailedDataSpec ì—°ë™
    model_configs: Dict[str, Any] = field(default_factory=dict)
    input_data_specs: Dict[str, Any] = field(default_factory=dict)
    output_data_specs: Dict[str, Any] = field(default_factory=dict)
    
    # AI ì¶”ë¡  ìš”êµ¬ì‚¬í•­
    batch_size: int = 1
    precision: str = "fp32"
    memory_limit_mb: Optional[float] = None
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)

# ==============================================
# ğŸ”¥ 3. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ (step_interface.py ì™„ì „ í˜¸í™˜)
# ==============================================

class RealAIModel:
    """ì‹¤ì œ AI ì¶”ë¡ ì— ì‚¬ìš©í•  ëª¨ë¸ í´ë˜ìŠ¤ (step_interface.py RealStepModelInterface ì™„ì „ í˜¸í™˜)"""
    
    def __init__(self, model_name: str, model_path: str, step_type: RealStepModelType, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.step_type = step_type
        self.device = device if device != "auto" else DEFAULT_DEVICE
        
        # ë¡œë”© ìƒíƒœ
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.checkpoint_data = None
        self.model_instance = None
        
        # step_interface.py í˜¸í™˜ì„ ìœ„í•œ ì†ì„±ë“¤
        self.preprocessing_params = {}
        self.model_class = None
        self.config_path = None
        
        # ê²€ì¦ ìƒíƒœ
        self.validation_passed = False
        self.compatibility_checked = False
        
        # Logger
        self.logger = logging.getLogger(f"RealAIModel.{model_name}")
        
        # Stepë³„ íŠ¹í™” ë¡œë” ë§¤í•‘ (step_interface.py GitHubStepMappingê³¼ í˜¸í™˜)
        self.step_loaders = {
            RealStepModelType.HUMAN_PARSING: self._load_human_parsing_model,
            RealStepModelType.POSE_ESTIMATION: self._load_pose_model,
            RealStepModelType.CLOTH_SEGMENTATION: self._load_segmentation_model,
            RealStepModelType.GEOMETRIC_MATCHING: self._load_geometric_model,
            RealStepModelType.CLOTH_WARPING: self._load_warping_model,
            RealStepModelType.VIRTUAL_FITTING: self._load_diffusion_model,
            RealStepModelType.POST_PROCESSING: self._load_enhancement_model,
            RealStepModelType.QUALITY_ASSESSMENT: self._load_quality_model
        }
        
    def load(self, validate: bool = True) -> bool:
        """ëª¨ë¸ ë¡œë”© (Stepë³„ íŠ¹í™” ë¡œë”©, step_interface.py ì™„ì „ í˜¸í™˜)"""
        try:
            start_time = time.time()
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.model_path.exists():
                self.logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = self.model_path.stat().st_size
            self.memory_usage_mb = file_size / (1024 * 1024)
            
            self.logger.info(f"ğŸ”„ {self.step_type.value} ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            
            # Stepë³„ íŠ¹í™” ë¡œë”© (step_interface.py GitHubStepMapping ê¸°ë°˜)
            success = False
            if self.step_type in self.step_loaders:
                success = self.step_loaders[self.step_type]()
            else:
                success = self._load_generic_model()
            
            if success:
                self.load_time = time.time() - start_time
                self.loaded = True
                
                # ê²€ì¦ ìˆ˜í–‰
                if validate:
                    self.validation_passed = self._validate_model()
                else:
                    self.validation_passed = True
                
                self.logger.info(f"âœ… {self.step_type.value} ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name} ({self.load_time:.2f}ì´ˆ)")
                return True
            else:
                self.logger.error(f"âŒ {self.step_type.value} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {self.model_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _load_human_parsing_model(self) -> bool:
        """Human Parsing ëª¨ë¸ ë¡œë”© (Graphonomy, ATR ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # Graphonomy íŠ¹ë³„ ì²˜ë¦¬ (1.2GB)
            if "graphonomy" in self.model_name.lower():
                return self._load_graphonomy_ultra_safe()
            
            # ATR ëª¨ë¸ ì²˜ë¦¬
            if "atr" in self.model_name.lower() or "schp" in self.model_name.lower():
                return self._load_atr_model()
            
            # ì¼ë°˜ PyTorch ëª¨ë¸
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Human Parsing ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_pose_model(self) -> bool:
        """Pose Estimation ëª¨ë¸ ë¡œë”© (YOLO, OpenPose ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # YOLO ëª¨ë¸ ì²˜ë¦¬
            if "yolo" in self.model_name.lower():
                self.checkpoint_data = self._load_yolo_model()
            # OpenPose ëª¨ë¸ ì²˜ë¦¬
            elif "openpose" in self.model_name.lower() or "pose" in self.model_name.lower():
                self.checkpoint_data = self._load_openpose_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Pose Estimation ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_segmentation_model(self) -> bool:
        """Segmentation ëª¨ë¸ ë¡œë”© (SAM, U2Net ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # SAM ëª¨ë¸ ì²˜ë¦¬ (2.4GB)
            if "sam" in self.model_name.lower():
                self.checkpoint_data = self._load_sam_model()
            # U2Net ëª¨ë¸ ì²˜ë¦¬ (176GB)
            elif "u2net" in self.model_name.lower():
                self.checkpoint_data = self._load_u2net_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Segmentation ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_geometric_model(self) -> bool:
        """Geometric Matching ëª¨ë¸ ë¡œë”© - step_interface.py í˜¸í™˜"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Geometric Matching ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_warping_model(self) -> bool:
        """Cloth Warping ëª¨ë¸ ë¡œë”© (RealVisXL ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # RealVisXL Safetensors íŒŒì¼ ì²˜ë¦¬ (6.46GB)
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Cloth Warping ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_diffusion_model(self) -> bool:
        """Virtual Fitting ëª¨ë¸ ë¡œë”© (Stable Diffusion ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # Safetensors ìš°ì„  ì²˜ë¦¬ (4.8GB)
            if self.model_path.suffix.lower() == '.safetensors':
                self.checkpoint_data = self._load_safetensors()
            # Diffusion ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬
            elif "diffusion" in self.model_name.lower():
                self.checkpoint_data = self._load_diffusion_checkpoint()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Virtual Fitting ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_enhancement_model(self) -> bool:
        """Post Processing ëª¨ë¸ ë¡œë”© (Real-ESRGAN ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # Real-ESRGAN íŠ¹ë³„ ì²˜ë¦¬ (64GB)
            if "esrgan" in self.model_name.lower():
                self.checkpoint_data = self._load_esrgan_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Post Processing ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_quality_model(self) -> bool:
        """Quality Assessment ëª¨ë¸ ë¡œë”© (CLIP, ViT ë“±) - step_interface.py í˜¸í™˜"""
        try:
            # CLIP ëª¨ë¸ ì²˜ë¦¬ (890MB)
            if "clip" in self.model_name.lower() or "vit" in self.model_name.lower():
                self.checkpoint_data = self._load_clip_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Quality Assessment ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_generic_model(self) -> bool:
        """ì¼ë°˜ ëª¨ë¸ ë¡œë”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"âŒ ì¼ë°˜ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ íŠ¹í™” ë¡œë”ë“¤ (step_interface.py ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ê¸°ë°˜)
    # ==============================================
    def _load_pytorch_checkpoint(self) -> Optional[Any]:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (PyTorch 2.7 ì™„ì „ í˜¸í™˜)"""
        if not TORCH_AVAILABLE:
            self.logger.error("âŒ PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
            return None
        
        try:
            import warnings
            
            # 1ë‹¨ê³„: ì•ˆì „ ëª¨ë“œ (weights_only=True)
            try:
                checkpoint = torch.load(
                    self.model_path, 
                    map_location='cpu',
                    weights_only=True
                )
                self.logger.debug(f"âœ… {self.model_name} ì•ˆì „ ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except RuntimeError as safe_error:
                error_msg = str(safe_error).lower()
                if "legacy .tar format" in error_msg or "torchscript" in error_msg:
                    self.logger.debug(f"Legacy/TorchScript íŒŒì¼ ê°ì§€: {self.model_name}")
                else:
                    self.logger.debug(f"ì•ˆì „ ëª¨ë“œ ì‹¤íŒ¨: {safe_error}")
            except Exception as e:
                self.logger.debug(f"ì•ˆì „ ëª¨ë“œ ì˜ˆì™¸: {e}")
            
            # 2ë‹¨ê³„: í˜¸í™˜ ëª¨ë“œ (weights_only=False)
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    checkpoint = torch.load(
                        self.model_path, 
                        map_location='cpu',
                        weights_only=False
                    )
                self.logger.debug(f"âœ… {self.model_name} í˜¸í™˜ ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except Exception as compat_error:
                self.logger.debug(f"í˜¸í™˜ ëª¨ë“œ ì‹¤íŒ¨: {compat_error}")
            
            # 3ë‹¨ê³„: Legacy ëª¨ë“œ (íŒŒë¼ë¯¸í„° ìµœì†Œí™”)
            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    checkpoint = torch.load(self.model_path, map_location='cpu')
                self.logger.debug(f"âœ… {self.model_name} Legacy ëª¨ë“œ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except Exception as legacy_error:
                self.logger.error(f"âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {legacy_error}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
        
    def _load_safetensors(self) -> Optional[Any]:
        """Safetensors íŒŒì¼ ë¡œë”© (RealVisXL, Diffusion ë“±)"""
        try:
            import safetensors.torch
            checkpoint = safetensors.torch.load_file(str(self.model_path))
            self.logger.debug(f"âœ… {self.model_name} Safetensors ë¡œë”© ì„±ê³µ")
            return checkpoint
        except ImportError:
            self.logger.warning("âš ï¸ Safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, PyTorch ë¡œë”© ì‹œë„")
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ Safetensors ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_graphonomy_ultra_safe(self) -> bool:
        """Graphonomy 1.2GB ëª¨ë¸ ì´ˆì•ˆì „ ë¡œë”© (step_interface.py ê²½ë¡œ ê¸°ë°˜)"""
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # ë©”ëª¨ë¦¬ ë§¤í•‘ ë°©ë²•
                try:
                    with open(self.model_path, 'rb') as f:
                        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
                            checkpoint = torch.load(
                                BytesIO(mmapped_file[:]), 
                                map_location='cpu',
                                weights_only=False
                            )
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("âœ… Graphonomy ë©”ëª¨ë¦¬ ë§¤í•‘ ë¡œë”© ì„±ê³µ")
                    return True
                    
                except Exception:
                    pass
                
                # ì§ì ‘ pickle ë¡œë”©
                try:
                    with open(self.model_path, 'rb') as f:
                        checkpoint = pickle.load(f)
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("âœ… Graphonomy ì§ì ‘ pickle ë¡œë”© ì„±ê³µ")
                    return True
                    
                except Exception:
                    pass
                
                # í´ë°±: ì¼ë°˜ PyTorch ë¡œë”©
                self.checkpoint_data = self._load_pytorch_checkpoint()
                return self.checkpoint_data is not None
                
        except Exception as e:
            self.logger.error(f"âŒ Graphonomy ì´ˆì•ˆì „ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_atr_model(self) -> bool:
        """ATR/SCHP ëª¨ë¸ ë¡œë”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"âŒ ATR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_yolo_model(self) -> Optional[Any]:
        """YOLO ëª¨ë¸ ë¡œë”© (6.2GB)"""
        try:
            # YOLOv8 ëª¨ë¸ì¸ ê²½ìš°
            if "v8" in self.model_name.lower():
                try:
                    from ultralytics import YOLO
                    model = YOLO(str(self.model_path))
                    self.model_instance = model
                    return {"model": model, "type": "yolov8"}
                except ImportError:
                    pass
            
            # ì¼ë°˜ PyTorch ëª¨ë¸ë¡œ ë¡œë”©
            return self._load_pytorch_checkpoint()
            
        except Exception as e:
            self.logger.error(f"âŒ YOLO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_openpose_model(self) -> Optional[Any]:
        """OpenPose ëª¨ë¸ ë¡œë”©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_sam_model(self) -> Optional[Any]:
        """SAM ëª¨ë¸ ë¡œë”© (2.4GB)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            if checkpoint and isinstance(checkpoint, dict):
                if "model" in checkpoint:
                    return checkpoint
                elif "state_dict" in checkpoint:
                    return checkpoint
                else:
                    return {"model": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_u2net_model(self) -> Optional[Any]:
        """U2Net ëª¨ë¸ ë¡œë”© (176GB)"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_diffusion_checkpoint(self) -> Optional[Any]:
        """Diffusion ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (4.8GB)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            
            # Diffusion ëª¨ë¸ êµ¬ì¡° ì •ê·œí™”
            if checkpoint and isinstance(checkpoint, dict):
                if "state_dict" in checkpoint:
                    return checkpoint
                elif "model" in checkpoint:
                    return checkpoint
                else:
                    return {"state_dict": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_esrgan_model(self) -> Optional[Any]:
        """Real-ESRGAN ëª¨ë¸ ë¡œë”© (64GB)"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ Real-ESRGAN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_clip_model(self) -> Optional[Any]:
        """CLIP ëª¨ë¸ ë¡œë”© (890MB)"""
        try:
            # .bin íŒŒì¼ì¸ ê²½ìš°
            if self.model_path.suffix.lower() == '.bin':
                checkpoint = torch.load(self.model_path, map_location='cpu')
            else:
                checkpoint = self._load_pytorch_checkpoint()
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ CLIP ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _validate_model(self) -> bool:
        """ëª¨ë¸ ê²€ì¦"""
        try:
            if self.checkpoint_data is None:
                return False
            
            # ê¸°ë³¸ ê²€ì¦
            if not isinstance(self.checkpoint_data, (dict, torch.nn.Module)) and self.checkpoint_data is not None:
                self.logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì²´í¬í¬ì¸íŠ¸ íƒ€ì…: {type(self.checkpoint_data)}")
            
            # Stepë³„ íŠ¹í™” ê²€ì¦
            if self.step_type == RealStepModelType.HUMAN_PARSING:
                return self._validate_human_parsing_model()
            elif self.step_type == RealStepModelType.VIRTUAL_FITTING:
                return self._validate_diffusion_model()
            else:
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _validate_human_parsing_model(self) -> bool:
        """Human Parsing ëª¨ë¸ ê²€ì¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    expected_keys = ["backbone", "decoder", "classifier"]
                    for key in expected_keys:
                        if any(key in k for k in state_dict.keys()):
                            return True
                
                if any("conv" in k or "bn" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Human Parsing ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return True
    
    def _validate_diffusion_model(self) -> bool:
        """Diffusion ëª¨ë¸ ê²€ì¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    if any("down_blocks" in k or "up_blocks" in k for k in state_dict.keys()):
                        return True
                
                if any("time_embed" in k or "input_blocks" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Diffusion ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return True
    
    # ==============================================
    # ğŸ”¥ step_interface.py í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_checkpoint_data(self) -> Optional[Any]:
        """ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
        return self.checkpoint_data
    
    def get_model_instance(self) -> Optional[Any]:
        """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
        return self.model_instance
    
    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (step_interface.py í˜¸í™˜)"""
        self.loaded = False
        self.checkpoint_data = None
        self.model_instance = None
        gc.collect()
        
        # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
        if MPS_AVAILABLE and TORCH_AVAILABLE:
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except:
                pass
    
    def get_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "step_type": self.step_type.value,
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0,
            "has_checkpoint_data": self.checkpoint_data is not None,
            "has_model_instance": self.model_instance is not None,
            "validation_passed": self.validation_passed,
            "compatibility_checked": self.compatibility_checked,
            
            # step_interface.py í˜¸í™˜ ì¶”ê°€ í•„ë“œ
            "model_type": getattr(self, 'model_type', 'BaseModel'),
            "size_gb": self.memory_usage_mb / 1024 if self.memory_usage_mb > 0 else 0,
            "requires_checkpoint": True,
            "preprocessing_required": getattr(self, 'preprocessing_required', []),
            "postprocessing_required": getattr(self, 'postprocessing_required', [])
        }

# ==============================================
# ğŸ”¥ 4. step_interface.py ì™„ì „ í˜¸í™˜ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
# ==============================================

class RealStepModelInterface:
    """step_interface.py v5.2 RealStepModelInterface ì™„ì „ í˜¸í™˜ êµ¬í˜„"""
    
    def __init__(self, model_loader, step_name: str, step_type: RealStepModelType):
        self.model_loader = model_loader
        self.step_name = step_name
        self.step_type = step_type
        self.logger = logging.getLogger(f"RealStepInterface.{step_name}")
        
        # Stepë³„ ëª¨ë¸ë“¤ (step_interface.py í˜¸í™˜)
        self.step_models: Dict[str, RealAIModel] = {}
        self.primary_model: Optional[RealAIModel] = None
        self.fallback_models: List[RealAIModel] = []
        
        # step_interface.py ìš”êµ¬ì‚¬í•­ ì—°ë™
        self.requirements: Optional[RealStepModelRequirement] = None
        self.data_specs_loaded: bool = False
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (step_interface.py í˜¸í™˜)
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        # ìºì‹œ (step_interface.py í˜¸í™˜)
        self.model_cache: Dict[str, Any] = {}
        self.preprocessing_cache: Dict[str, Any] = {}
        
        # step_interface.py í†µê³„ í˜¸í™˜
        self.real_statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'real_checkpoints_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'creation_time': time.time()
        }
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """step_interface.py DetailedDataSpec ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            self.requirements = RealStepModelRequirement(
                step_name=self.step_name,
                step_id=requirements.get('step_id', 0),
                step_type=self.step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.data_specs_loaded = True
            self.logger.info(f"âœ… step_interface.py í˜¸í™˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {len(self.requirements.required_models)}ê°œ í•„ìˆ˜ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ì‹¤ì œ AI ëª¨ë¸ ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
        try:
            self.access_count += 1
            
            # íŠ¹ì • ëª¨ë¸ ìš”ì²­
            if model_name:
                if model_name in self.step_models:
                    model = self.step_models[model_name]
                    model.access_count += 1
                    model.last_access = time.time()
                    self.real_statistics['cache_hits'] += 1
                    return model
                
                # ìƒˆ ëª¨ë¸ ë¡œë”©
                return self._load_new_model(model_name)
            
            # ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜ (step_interface.py í˜¸í™˜)
            if self.primary_model and self.primary_model.loaded:
                return self.primary_model
            
            # ë¡œë“œëœ ëª¨ë¸ ì¤‘ ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒ
            for model in sorted(self.step_models.values(), key=lambda m: getattr(m, 'priority', 999)):
                if model.loaded:
                    return model
            
            # ì²« ë²ˆì§¸ ëª¨ë¸ ë¡œë”© ì‹œë„
            if self.requirements and self.requirements.required_models:
                return self._load_new_model(self.requirements.required_models[0])
            
            return None
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_new_model(self, model_name: str) -> Optional[RealAIModel]:
        """ìƒˆ ëª¨ë¸ ë¡œë”© (step_interface.py í˜¸í™˜)"""
        try:
            # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
            base_model = self.model_loader.load_model(model_name, step_name=self.step_name, step_type=self.step_type)
            
            if base_model and isinstance(base_model, RealAIModel):
                self.step_models[model_name] = base_model
                
                # Primary ëª¨ë¸ ì„¤ì •
                if not self.primary_model or (self.requirements and model_name == self.requirements.primary_model):
                    self.primary_model = base_model
                
                # í†µê³„ ì—…ë°ì´íŠ¸ (step_interface.py í˜¸í™˜)
                self.real_statistics['models_loaded'] += 1
                self.real_statistics['real_ai_calls'] += 1
                if base_model.checkpoint_data is not None:
                    self.real_statistics['real_checkpoints_loaded'] += 1
                
                return base_model
            else:
                self.real_statistics['cache_misses'] += 1
                self.real_statistics['loading_failures'] += 1
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.real_statistics['loading_failures'] += 1
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ë™ê¸° ëª¨ë¸ ì¡°íšŒ - step_interface.py BaseStepMixin í˜¸í™˜"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ (step_interface.py í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - step_interface.py BaseStepMixin í˜¸í™˜"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'step_type': self.step_type.value,
                'required': kwargs.get('required', True),
                'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                'device': kwargs.get('device', DEFAULT_DEVICE),
                'preprocessing_params': kwargs.get('preprocessing_params', {}),
                **kwargs
            }
            
            self.real_statistics['models_registered'] += 1
            self.logger.info(f"âœ… step_interface.py í˜¸í™˜ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (step_interface.py í˜¸í™˜)"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (step_interface.py í˜¸í™˜)"""
        try:
            # ë©”ëª¨ë¦¬ í•´ì œ
            for model_name, model in self.step_models.items():
                if hasattr(model, 'unload'):
                    model.unload()
            
            self.step_models.clear()
            self.model_cache.clear()
            
            self.logger.info(f"âœ… step_interface.py í˜¸í™˜ {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
EnhancedStepModelInterface = RealStepModelInterface
StepModelInterface = RealStepModelInterface

# ==============================================
# ğŸ”¥ 5. ì™„ì „ ê°œì„ ëœ ModelLoader í´ë˜ìŠ¤ v5.1 (step_interface.py ì™„ì „ í˜¸í™˜)
# ==============================================
    
class ModelLoader:
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):  # ğŸ”¥ ìˆ˜ì •: di_containerë¥¼ kwargsë¡œ ë°›ìŒ
        """ModelLoader ì´ˆê¸°í™” (step_interface.py ì™„ì „ í˜¸í™˜)"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # ğŸ”¥ DI Container ì²˜ë¦¬ (kwargsì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ)
        self._di_container = kwargs.pop('di_container', None)  # popì„ ì‚¬ìš©í•´ì„œ ì¤‘ë³µ ë°©ì§€
        
        # DI Container ìë™ ë“±ë¡ ì‹œë„
        if self._di_container:
            try:
                # ìê¸° ìì‹ ì„ DI Containerì— ë“±ë¡
                success = self._di_container.force_register_model_loader(self)
                if success:
                    self.logger.info("âœ… ModelLoaderê°€ DI Containerì— ìë™ ë“±ë¡ë¨")
                else:
                    self.logger.warning("âš ï¸ ModelLoader DI Container ìë™ ë“±ë¡ ì‹¤íŒ¨")
            except Exception as e:
                self.logger.debug(f"âš ï¸ DI Container ìë™ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë‚˜ë¨¸ì§€ kwargs ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒë¼ë¯¸í„° ë¬´ì‹œ: {key}={value}")
                # ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (step_interface.py AI_MODELS_ROOT í˜¸í™˜)
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # step_interface.py AI_MODELS_ROOT ê²½ë¡œ ë§¤í•‘
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ DI Containerë¡œë¶€í„° ì˜ì¡´ì„±ë“¤ ì´ˆê¸°í™”
        self._initialize_dependencies_from_di_container()
        
        # ì‹¤ì œ AI ëª¨ë¸ ê´€ë¦¬ (step_interface.py í˜¸í™˜)
        self.loaded_models: Dict[str, RealAIModel] = {}
        self.model_info: Dict[str, RealStepModelInfo] = {}
        self.model_status: Dict[str, RealModelStatus] = {}
        
        # Step ìš”êµ¬ì‚¬í•­ (step_interface.py í˜¸í™˜)
        self.step_requirements: Dict[str, RealStepModelRequirement] = {}
        self.step_interfaces: Dict[str, RealStepModelInterface] = {}
        
        # auto_model_detector ì—°ë™
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (step_interface.py í˜¸í™˜)
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0,
            'inference_count': 0,
            'total_inference_time': 0.0
        }
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # step_interface.py GitHubStepMapping ë¡œë”©
        self._load_step_interface_mappings()
        
        self.logger.info(f"ğŸš€ ì™„ì „ ê°œì„ ëœ ModelLoader v5.1 ì´ˆê¸°í™” ì™„ë£Œ (step_interface.py v5.2 ì™„ì „ í˜¸í™˜)")
        self.logger.info(f"ğŸ“± Device: {self.device} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ: {self.model_cache_dir}")
        self.logger.info(f"ğŸ¯ step_interface.py ì‹¤ì œ AI Step í˜¸í™˜ ëª¨ë“œ")
        
        # ğŸ”¥ DI Container ì—°ë™ ë¡œê·¸
        if self._di_container:
            self.logger.info("âœ… DI Container ì—°ë™ ì™„ë£Œ")
        else:
            self.logger.debug("âš ï¸ DI Container ì—†ìŒ, ê¸°ë³¸ ëª¨ë“œ")

    def _initialize_dependencies_from_di_container(self):
        """ğŸ”¥ DI Containerë¡œë¶€í„° ì˜ì¡´ì„±ë“¤ ì¡°íšŒ ë° ì´ˆê¸°í™”"""
        try:
            if self._di_container:
                # MemoryManager ì¡°íšŒ
                memory_manager = self._di_container.get('memory_manager')
                if memory_manager:
                    self.memory_manager = memory_manager
                    self.logger.debug("âœ… DI Containerë¡œë¶€í„° MemoryManager ì¡°íšŒ ì„±ê³µ")
                else:
                    self.memory_manager = None
                    self.logger.debug("âš ï¸ DI Containerì— MemoryManager ì—†ìŒ")
                
                # DataConverter ì¡°íšŒ
                data_converter = self._di_container.get('data_converter')
                if data_converter:
                    self.data_converter = data_converter
                    self.logger.debug("âœ… DI Containerë¡œë¶€í„° DataConverter ì¡°íšŒ ì„±ê³µ")
                else:
                    self.data_converter = None
                    self.logger.debug("âš ï¸ DI Containerì— DataConverter ì—†ìŒ")
                
                # ì‹œìŠ¤í…œ ì •ë³´ë„ DI Containerë¡œë¶€í„°
                self.device_info = self._di_container.get('device') or self.device
                self.memory_gb = self._di_container.get('memory_gb') or 16.0
                self.is_m3_max = self._di_container.get('is_m3_max') or False
                
                self.logger.debug("âœ… DI Containerë¡œë¶€í„° ì˜ì¡´ì„± ì¡°íšŒ ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ DI Container ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.memory_manager = None
                self.data_converter = None
                
        except Exception as e:
            self.logger.debug(f"âš ï¸ DI Container ì˜ì¡´ì„± ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.memory_manager = None
            self.data_converter = None

    def set_di_container(self, di_container):
        """ğŸ”¥ DI Container ì„¤ì • (ë‚˜ì¤‘ì— ì£¼ì…ë°›ì„ ë•Œ)"""
        try:
            self._di_container = di_container
            self._initialize_dependencies_from_di_container()
            self.logger.debug("âœ… DI Container ì¬ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ DI Container ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_service(self, service_key: str):
        """ğŸ”¥ DI Containerë¡œë¶€í„° ì„œë¹„ìŠ¤ ì¡°íšŒ"""
        try:
            if self._di_container:
                return self._di_container.get(service_key)
            return None
        except Exception as e:
            self.logger.debug(f"ì„œë¹„ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨ ({service_key}): {e}")
            return None

    # ... ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€ ...


    def _initialize_auto_detector(self):
        """auto_model_detector ì´ˆê¸°í™” (step_interface.py í˜¸í™˜)"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("âœ… auto_model_detector ì—°ë™ ì™„ë£Œ")
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("âš ï¸ auto_detector ì¸ìŠ¤í„´ìŠ¤ê°€ None")
            else:
                self.logger.warning("âš ï¸ AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"âŒ auto_model_detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector í†µí•© (step_interface.py í˜¸í™˜)"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                # Step íƒ€ì… ì¶”ë¡ 
                                step_type = self._infer_step_type(model_name, model_path)
                                
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "step_type": step_type.value if step_type else 'unknown',
                                    "model_type": self._infer_model_type(model_name),
                                    "auto_detected": True,
                                    "priority": self._infer_model_priority(model_name),
                                    # step_interface.py í˜¸í™˜ í•„ë“œ
                                    "loaded": False,
                                    "step_id": self._get_step_id_from_step_type(step_type),
                                    "device": self.device,
                                    "real_ai_model": True
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"âœ… AutoDetector step_interface.py í†µí•© ì™„ë£Œ: {integrated_count}ê°œ ëª¨ë¸")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ AutoDetector í†µí•© ì‹¤íŒ¨: {e}")
            return False
    
    def _infer_step_type(self, model_name: str, model_path: str) -> Optional[RealStepModelType]:
        """ëª¨ë¸ëª…ê³¼ ê²½ë¡œë¡œ Step íƒ€ì… ì¶”ë¡  (step_interface.py GitHubStepType í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        model_path_lower = model_path.lower()
        
        # ê²½ë¡œ ê¸°ë°˜ ì¶”ë¡  (step_interface.py êµ¬ì¡°)
        if "step_01" in model_path_lower or "human_parsing" in model_path_lower:
            return RealStepModelType.HUMAN_PARSING
        elif "step_02" in model_path_lower or "pose" in model_path_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif "step_03" in model_path_lower or "segmentation" in model_path_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif "step_04" in model_path_lower or "geometric" in model_path_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif "step_05" in model_path_lower or "warping" in model_path_lower:
            return RealStepModelType.CLOTH_WARPING
        elif "step_06" in model_path_lower or "virtual" in model_path_lower or "fitting" in model_path_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif "step_07" in model_path_lower or "post" in model_path_lower:
            return RealStepModelType.POST_PROCESSING
        elif "step_08" in model_path_lower or "quality" in model_path_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        # ëª¨ë¸ëª… ê¸°ë°˜ ì¶”ë¡  (step_interface.py GitHubStepMapping ê¸°ë°˜)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "atr", "schp"]):
            return RealStepModelType.HUMAN_PARSING
        elif any(keyword in model_name_lower for keyword in ["yolo", "openpose", "pose"]):
            return RealStepModelType.POSE_ESTIMATION
        elif any(keyword in model_name_lower for keyword in ["sam", "u2net", "segment"]):
            return RealStepModelType.CLOTH_SEGMENTATION
        elif any(keyword in model_name_lower for keyword in ["gmm", "tps", "geometric"]):
            return RealStepModelType.GEOMETRIC_MATCHING
        elif any(keyword in model_name_lower for keyword in ["realvis", "vgg", "warping"]):
            return RealStepModelType.CLOTH_WARPING
        elif any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet", "unet", "vae"]):
            return RealStepModelType.VIRTUAL_FITTING
        elif any(keyword in model_name_lower for keyword in ["esrgan", "sr", "enhancement"]):
            return RealStepModelType.POST_PROCESSING
        elif any(keyword in model_name_lower for keyword in ["clip", "vit", "quality"]):
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return None
    
    def _infer_model_type(self, model_name: str) -> str:
        """ëª¨ë¸ íƒ€ì… ì¶”ë¡  (step_interface.py í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet"]):
            return "DiffusionModel"
        elif any(keyword in model_name_lower for keyword in ["yolo", "detection"]):
            return "DetectionModel"
        elif any(keyword in model_name_lower for keyword in ["segment", "sam", "u2net"]):
            return "SegmentationModel"
        elif any(keyword in model_name_lower for keyword in ["pose", "openpose"]):
            return "PoseModel"
        elif any(keyword in model_name_lower for keyword in ["clip", "vit"]):
            return "ClassificationModel"
        else:
            return "BaseModel"
    
    def _infer_model_priority(self, model_name: str) -> int:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì¶”ë¡  (step_interface.py í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        
        # Primary ëª¨ë¸ë“¤ (step_interface.py GitHubStepMapping ê¸°ë°˜)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "yolo", "sam", "diffusion", "esrgan", "clip"]):
            return RealModelPriority.PRIMARY.value
        elif any(keyword in model_name_lower for keyword in ["atr", "openpose", "u2net", "vgg"]):
            return RealModelPriority.SECONDARY.value
        else:
            return RealModelPriority.OPTIONAL.value
    
    def _get_step_id_from_step_type(self, step_type: Optional[RealStepModelType]) -> int:
        """Step íƒ€ì…ì—ì„œ ID ì¶”ì¶œ (step_interface.py í˜¸í™˜)"""
        if not step_type:
            return 0
        
        step_id_map = {
            RealStepModelType.HUMAN_PARSING: 1,
            RealStepModelType.POSE_ESTIMATION: 2,
            RealStepModelType.CLOTH_SEGMENTATION: 3,
            RealStepModelType.GEOMETRIC_MATCHING: 4,
            RealStepModelType.CLOTH_WARPING: 5,
            RealStepModelType.VIRTUAL_FITTING: 6,
            RealStepModelType.POST_PROCESSING: 7,
            RealStepModelType.QUALITY_ASSESSMENT: 8
        }
        return step_id_map.get(step_type, 0)
    
    def _load_step_interface_mappings(self):
        """step_interface.py GitHubStepMapping ë¡œë”©"""
        try:
            # step_interface.py GitHubStepMapping êµ¬ì¡° ë°˜ì˜
            self.step_interface_mappings = {
                'HumanParsingStep': {
                    'step_type': RealStepModelType.HUMAN_PARSING,
                    'step_id': 1,
                    'ai_models': [
                        'graphonomy.pth',  # 1.2GB
                        'exp-schp-201908301523-atr.pth',  # 255MB
                        'pytorch_model.bin'  # 168MB
                    ],
                    'primary_model': 'graphonomy.pth',
                    'local_paths': [
                        'step_01_human_parsing/graphonomy.pth',
                        'step_01_human_parsing/exp-schp-201908301523-atr.pth'
                    ]
                },
                'PoseEstimationStep': {
                    'step_type': RealStepModelType.POSE_ESTIMATION,
                    'step_id': 2,
                    'ai_models': [
                        'yolov8n-pose.pt'  # 6.2GB
                    ],
                    'primary_model': 'yolov8n-pose.pt',
                    'local_paths': [
                        'step_02_pose_estimation/yolov8n-pose.pt'
                    ]
                },
                'ClothSegmentationStep': {
                    'step_type': RealStepModelType.CLOTH_SEGMENTATION,
                    'step_id': 3,
                    'ai_models': [
                        'sam_vit_h_4b8939.pth',  # 2.4GB
                        'u2net.pth'  # 176GB
                    ],
                    'primary_model': 'sam_vit_h_4b8939.pth',
                    'local_paths': [
                        'step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'step_03_cloth_segmentation/u2net.pth'
                    ]
                },
                'GeometricMatchingStep': {
                    'step_type': RealStepModelType.GEOMETRIC_MATCHING,
                    'step_id': 4,
                    'ai_models': [
                        'gmm_final.pth'  # 1.3GB
                    ],
                    'primary_model': 'gmm_final.pth',
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth'
                    ]
                },
                'ClothWarpingStep': {
                    'step_type': RealStepModelType.CLOTH_WARPING,
                    'step_id': 5,
                    'ai_models': [
                        'RealVisXL_V4.0.safetensors'  # 6.46GB
                    ],
                    'primary_model': 'RealVisXL_V4.0.safetensors',
                    'local_paths': [
                        'step_05_cloth_warping/RealVisXL_V4.0.safetensors'
                    ]
                },
                'VirtualFittingStep': {
                    'step_type': RealStepModelType.VIRTUAL_FITTING,
                    'step_id': 6,
                    'ai_models': [
                        'diffusion_pytorch_model.fp16.safetensors',  # 4.8GB
                        'v1-5-pruned-emaonly.safetensors'  # 4.0GB
                    ],
                    'primary_model': 'diffusion_pytorch_model.fp16.safetensors',
                    'local_paths': [
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.fp16.safetensors',
                        'step_06_virtual_fitting/v1-5-pruned-emaonly.safetensors'
                    ]
                },
                'PostProcessingStep': {
                    'step_type': RealStepModelType.POST_PROCESSING,
                    'step_id': 7,
                    'ai_models': [
                        'Real-ESRGAN_x4plus.pth'  # 64GB
                    ],
                    'primary_model': 'Real-ESRGAN_x4plus.pth',
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth'
                    ]
                },
                'QualityAssessmentStep': {
                    'step_type': RealStepModelType.QUALITY_ASSESSMENT,
                    'step_id': 8,
                    'ai_models': [
                        'ViT-L-14.pt'  # 890MB
                    ],
                    'primary_model': 'ViT-L-14.pt',
                    'local_paths': [
                        'step_08_quality_assessment/ViT-L-14.pt'
                    ]
                }
            }
            
            self.logger.info(f"âœ… step_interface.py GitHubStepMapping ë¡œë”© ì™„ë£Œ: {len(self.step_interface_mappings)}ê°œ Step")
            
        except Exception as e:
            self.logger.error(f"âŒ step_interface.py ë§¤í•‘ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.step_interface_mappings = {}
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤ (step_interface.py ì™„ì „ í˜¸í™˜)
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (step_interface.py RealStepModelInterface ì™„ì „ í˜¸í™˜)"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        model.access_count += 1
                        model.last_access = time.time()
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì‹¤ì œ AI ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return model
                
                # ìƒˆ ëª¨ë¸ ë¡œë”©
                self.model_status[model_name] = RealModelStatus.LOADING
                
                # ëª¨ë¸ ê²½ë¡œ ë° Step íƒ€ì… ê²°ì • (step_interface.py ê²½ë¡œ ê¸°ë°˜)
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                    self.model_status[model_name] = RealModelStatus.ERROR
                    return None
                
                # Step íƒ€ì… ì¶”ë¡  (step_interface.py í˜¸í™˜)
                step_type = kwargs.get('step_type')
                if not step_type:
                    step_type = self._infer_step_type(model_name, model_path)
                
                if not step_type:
                    step_type = RealStepModelType.HUMAN_PARSING  # ê¸°ë³¸ê°’
                
                # RealAIModel ìƒì„± ë° ë¡œë”©
                model = RealAIModel(
                    model_name=model_name,
                    model_path=model_path,
                    step_type=step_type,
                    device=self.device
                )
                
                # ëª¨ë¸ ë¡œë”© ìˆ˜í–‰
                if model.load(validate=kwargs.get('validate', True)):
                    # ìºì‹œì— ì €ì¥
                    self.loaded_models[model_name] = model
                    
                    # ëª¨ë¸ ì •ë³´ ì €ì¥ (step_interface.py í˜¸í™˜)
                    priority = RealModelPriority(kwargs.get('priority', RealModelPriority.SECONDARY.value))
                    self.model_info[model_name] = RealStepModelInfo(
                        name=model_name,
                        path=model_path,
                        step_type=step_type,
                        priority=priority,
                        device=self.device,
                        memory_mb=model.memory_usage_mb,
                        loaded=True,
                        load_time=model.load_time,
                        checkpoint_data=model.checkpoint_data,
                        validation_passed=model.validation_passed,
                        access_count=1,
                        last_access=time.time(),
                        # step_interface.py í˜¸í™˜ í•„ë“œ
                        model_type=kwargs.get('model_type', 'BaseModel'),
                        size_gb=model.memory_usage_mb / 1024 if model.memory_usage_mb > 0 else 0,
                        requires_checkpoint=True,
                        preprocessing_required=kwargs.get('preprocessing_required', []),
                        postprocessing_required=kwargs.get('postprocessing_required', [])
                    )
                    
                    self.model_status[model_name] = RealModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({step_type.value}, {model.memory_usage_mb:.1f}MB)")
                    
                    # ìºì‹œ í¬ê¸° ê´€ë¦¬
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = RealModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.model_status[model_name] = RealModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None


    def validate_di_container_integration(self) -> Dict[str, Any]:
        """DI Container ì—°ë™ ìƒíƒœ ê²€ì¦"""
        try:
            validation_result = {
                'di_container_available': self._di_container is not None,
                'registered_in_container': False,
                'can_inject_to_steps': False,
                'container_stats': {}
            }
            
            if self._di_container:
                # Containerì— ë“±ë¡ í™•ì¸
                model_loader_from_container = self._di_container.get('model_loader')
                validation_result['registered_in_container'] = model_loader_from_container is not None
                
                # Step ì£¼ì… í…ŒìŠ¤íŠ¸ (ê°€ìƒ)
                validation_result['can_inject_to_steps'] = hasattr(self._di_container, 'inject_to_step')
                
                # Container í†µê³„
                if hasattr(self._di_container, 'get_stats'):
                    validation_result['container_stats'] = self._di_container.get_stats()
            
            return validation_result
            
        except Exception as e:
            return {'error': str(e), 'di_container_available': False}

    async def load_model_async(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© (step_interface.py í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° - ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì """
        try:
            # ì§ì ‘ ê²½ë¡œ ì§€ì •ëœ ê²½ìš°
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # ìºì‹œëœ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
            if hasattr(self, '_model_path_cache') and model_name in self._model_path_cache:
                cached_path = Path(self._model_path_cache[model_name])
                if cached_path.exists():
                    return str(cached_path)
            
            # ìºì‹œ ì´ˆê¸°í™”
            if not hasattr(self, '_model_path_cache'):
                self._model_path_cache = {}
            
            # ì‹¤ì œ íŒŒì¼ëª… ë§¤í•‘ (í˜„ì¬ êµ¬ì¡° ê¸°ë°˜)
            model_name_mappings = {
                # Human Parsing ëª¨ë¸ë“¤
                'graphonomy': [
                    'checkpoints/step_01_human_parsing/graphonomy_alternative.pth',
                    'step_01_human_parsing/graphonomy_fixed.pth',
                    'step_01_human_parsing/graphonomy_new.pth',
                    'Graphonomy/inference.pth'
                ],
                'schp_atr': [
                    'exp-schp-201908301523-atr.pth',
                    'Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth'
                ],
                'atr_model': [
                    'exp-schp-201908301523-atr.pth',
                    'Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth'
                ],
                'schp_lip': [
                    'step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing/exp-schp-201908261155-lip.pth'
                ],
                
                # Pose Estimation ëª¨ë¸ë“¤
                'hrnet': [
                    'step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/ckpts/body_pose_model.pth',
                    'step_02_pose_estimation/yolov8s-pose.pt'
                ],
                'openpose': [
                    'step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/ckpts/body_pose_model.pth'
                ],
                'body_pose_model': [
                    'step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/ckpts/body_pose_model.pth'
                ],
                
                # Cloth Segmentation ëª¨ë¸ë“¤
                'sam': [
                    'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                    'step_04_geometric_matching/ultra_models/sam_vit_h_4b8939.pth',
                    'step_03_cloth_segmentation/ultra_models/sam_vit_h_4b8939.pth'
                ],
                'sam_vit_h': [
                    'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                    'step_04_geometric_matching/ultra_models/sam_vit_h_4b8939.pth'
                ],
                'sam_vit_l': [
                    'checkpoints/step_03_cloth_segmentation/sam_vit_l_0b3195.pth'
                ],
                'mobile_sam': [
                    'checkpoints/step_03_cloth_segmentation/mobile_sam_alternative.pt',
                    'step_03_cloth_segmentation/mobile_sam.pt'
                ],
                'u2net': [
                    'checkpoints/step_03_cloth_segmentation/u2net_fallback.pth',
                    'step_03_cloth_segmentation/u2net.pth'
                ],
                
                # Geometric Matching ëª¨ë¸ë“¤
                'resnet': [
                    'step_04_geometric_matching/ultra_models/resnet101_geometric.pth'
                ],
                'raft': [
                    'step_04_geometric_matching/ultra_models/raft-things.pth'
                ],
                'vit': [
                    'step_04_geometric_matching/ultra_models/ViT-L-14.pt',
                    'step_08_quality_assessment/ultra_models/ViT-L-14.pt'
                ],
                'efficientnet': [
                    'step_04_geometric_matching/ultra_models/efficientnet_b0_ultra.pth'
                ],
                
                # Cloth Warping ëª¨ë¸ë“¤
                'tom': [
                    'checkpoints/step_05_cloth_warping/tom_final.pth'
                ],
                'hrviton': [
                    'checkpoints/step_06_virtual_fitting/hrviton_final.pth'
                ],
                'vgg': [
                    'step_05_cloth_warping/ultra_models/vgg19_warping.pth',
                    'step_05_cloth_warping/ultra_models/vgg16_warping_ultra.pth'
                ],
                
                # Virtual Fitting ëª¨ë¸ë“¤
                'ootdiffusion': [
                    'step_06_virtual_fitting/ootdiffusion/diffusion_pytorch_model.bin',
                    'checkpoints/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors'
                ],
                'stable_diffusion': [
                    'checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors',
                    'checkpoints/stable-diffusion-v1-5/v1-5-pruned.safetensors'
                ],
                
                # Post Processing ëª¨ë¸ë“¤
                'esrgan': [
                    'step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth',
                    'step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth'
                ],
                'gfpgan': [
                    'checkpoints/step_07_post_processing/GFPGAN.pth'
                ],
                
                # Quality Assessment ëª¨ë¸ë“¤
                'clip': [
                    'step_08_quality_assessment/clip_vit_g14/open_clip_pytorch_model.bin',
                    'step_08_quality_assessment/clip_vit_b32.pth'
                ],
                'lpips': [
                    'checkpoints/step_08_quality_assessment/lpips_vgg.pth',
                    'checkpoints/step_08_quality_assessment/lpips_alex.pth'
                ]
            }
            
            # ë§¤í•‘ëœ ê²½ë¡œì—ì„œ ì°¾ê¸°
            if model_name in model_name_mappings:
                for relative_path in model_name_mappings[model_name]:
                    full_path = self.model_cache_dir / relative_path
                    if full_path.exists():
                        self._model_path_cache[model_name] = str(full_path)
                        self.logger.info(f"âœ… ëª¨ë¸ ë°œê²¬: {model_name} â†’ {full_path}")
                        return str(full_path)
            
            # ë§¤í•‘ì— ì—†ëŠ” ê²½ìš° - íŒŒì¼ëª…ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰ (ìµœí›„ ìˆ˜ë‹¨)
            search_patterns = [
                f"**/{model_name}.pth",
                f"**/{model_name}.pt", 
                f"**/{model_name}.safetensors",
                f"**/{model_name}.bin",
                f"**/*{model_name}*.pth",
                f"**/*{model_name}*.pt"
            ]
            
            for pattern in search_patterns:
                try:
                    for found_path in self.model_cache_dir.glob(pattern):
                        if found_path.is_file() and found_path.stat().st_size > 1024:  # 1KB ì´ìƒ
                            self._model_path_cache[model_name] = str(found_path)
                            self.logger.info(f"ğŸ” íŒ¨í„´ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë¸ ë°œê²¬: {model_name} â†’ {found_path}")
                            return str(found_path)
                except Exception as e:
                    self.logger.debug(f"íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨ {pattern}: {e}")
                    continue
            
            # ëª» ì°¾ì€ ê²½ìš°
            self.logger.warning(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None

    # ì¶”ê°€: ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜
    def clear_model_cache(self):
        """ëª¨ë¸ ê²½ë¡œ ìºì‹œ ì´ˆê¸°í™”"""
        if hasattr(self, '_model_path_cache'):
            self._model_path_cache.clear()
            self.logger.info("ğŸ—‘ï¸ ëª¨ë¸ ê²½ë¡œ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    # ì¶”ê°€: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í•¨ìˆ˜  
    def list_available_models(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ëª©ë¡ ë°˜í™˜"""
        available = {}
        
        # ì£¼ìš” ëª¨ë¸ë“¤ ì²´í¬
        important_models = [
            'graphonomy', 'schp_atr', 'hrnet', 'openpose', 'sam', 'sam_vit_h', 
            'u2net', 'resnet', 'raft', 'vit', 'hrviton', 'ootdiffusion', 
            'stable_diffusion', 'esrgan', 'gfpgan', 'clip'
        ]
        
        for model_name in important_models:
            path = self._find_model_path(model_name)
            if path:
                available[model_name] = path
        
        self.logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(available)}ê°œ")
        return available


    def _manage_cache(self):
        """ì‹¤ì œ AI ëª¨ë¸ ìºì‹œ ê´€ë¦¬ (step_interface.py í˜¸í™˜)"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # ìš°ì„ ìˆœìœ„ì™€ ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ê¸°ë°˜ ì •ë ¬
            models_by_priority = sorted(
                self.model_info.items(),
                key=lambda x: (x[1].priority.value, x[1].last_access)
            )
            
            models_to_remove = models_by_priority[:len(self.loaded_models) - self.max_cached_models]
            
            for model_name, _ in models_to_remove:
                # Primary ëª¨ë¸ì€ ë³´í˜¸ (step_interface.py GitHubStepMapping ê¸°ë°˜)
                if any(mapping.get('primary_model') == model_name for mapping in self.step_interface_mappings.values()):
                    continue
                
                self.unload_model(model_name)
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ (step_interface.py í˜¸í™˜)"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = RealModelStatus.NOT_LOADED
                    
                    self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ step_interface.py ì™„ì „ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ì§€ì›
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
        """step_interface.py í˜¸í™˜ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            # Step íƒ€ì… ê²°ì • (step_interface.py GitHubStepType ê¸°ë°˜)
            step_type = None
            if step_name in self.step_interface_mappings:
                step_type = self.step_interface_mappings[step_name].get('step_type')
            
            if not step_type:
                # ì´ë¦„ìœ¼ë¡œ ì¶”ë¡  (step_interface.py í˜¸í™˜)
                step_type_map = {
                    'HumanParsingStep': RealStepModelType.HUMAN_PARSING,
                    'PoseEstimationStep': RealStepModelType.POSE_ESTIMATION,
                    'ClothSegmentationStep': RealStepModelType.CLOTH_SEGMENTATION,
                    'GeometricMatchingStep': RealStepModelType.GEOMETRIC_MATCHING,
                    'ClothWarpingStep': RealStepModelType.CLOTH_WARPING,
                    'VirtualFittingStep': RealStepModelType.VIRTUAL_FITTING,
                    'PostProcessingStep': RealStepModelType.POST_PROCESSING,
                    'QualityAssessmentStep': RealStepModelType.QUALITY_ASSESSMENT
                }
                step_type = step_type_map.get(step_name, RealStepModelType.HUMAN_PARSING)
            
            interface = RealStepModelInterface(self, step_name, step_type)
            
            # step_interface.py DetailedDataSpec ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡
            if step_requirements:
                interface.register_requirements(step_requirements)
            elif step_name in self.step_interface_mappings:
                # ê¸°ë³¸ ë§¤í•‘ì—ì„œ ìš”êµ¬ì‚¬í•­ ìƒì„± (step_interface.py í˜¸í™˜)
                mapping = self.step_interface_mappings[step_name]
                default_requirements = {
                    'step_id': mapping.get('step_id', 0),
                    'required_models': mapping.get('ai_models', []),
                    'primary_model': mapping.get('primary_model'),
                    'model_configs': {},
                    'batch_size': 1,
                    'precision': 'fp16' if self.device == 'mps' else 'fp32'
                }
                interface.register_requirements(default_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"âœ… step_interface.py í˜¸í™˜ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name} ({step_type.value})")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            return RealStepModelInterface(self, step_name, RealStepModelType.HUMAN_PARSING)
    
    def create_step_model_interface(self, step_name: str) -> RealStepModelInterface:
        """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (step_interface.py í˜¸í™˜ ë³„ì¹­)"""
        return self.create_step_interface(step_name)
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """step_interface.py DetailedDataSpec ê¸°ë°˜ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            step_type = requirements.get('step_type')
            if isinstance(step_type, str):
                step_type = RealStepModelType(step_type)
            elif not step_type:
                if step_name in self.step_interface_mappings:
                    step_type = self.step_interface_mappings[step_name].get('step_type')
                else:
                    step_type = RealStepModelType.HUMAN_PARSING
            
            self.step_requirements[step_name] = RealStepModelRequirement(
                step_name=step_name,
                step_id=requirements.get('step_id', self._get_step_id(step_name)),
                step_type=step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.logger.info(f"âœ… step_interface.py í˜¸í™˜ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
    
    def _get_step_id(self, step_name: str) -> int:
        """Step ì´ë¦„ìœ¼ë¡œ ID ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
        step_id_map = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return step_id_map.get(step_name, 0)
    
    # ==============================================
    # ğŸ”¥ step_interface.py BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ (step_interface.py í˜¸í™˜)"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """ì´ˆê¸°í™” (step_interface.py í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("âœ… step_interface.py í˜¸í™˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” (step_interface.py í˜¸í™˜)"""
        return self.initialize(**kwargs)
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - step_interface.py BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                # Step íƒ€ì… ì¶”ë¡ 
                step_type = kwargs.get('step_type')
                if isinstance(step_type, str):
                    step_type = RealStepModelType(step_type)
                elif not step_type:
                    step_type = self._infer_step_type(model_name, kwargs.get('model_path', ''))
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'step_type': step_type.value if step_type else 'unknown',
                    'required': kwargs.get('required', True),
                    'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                    'device': kwargs.get('device', self.device),
                    'preprocessing_params': kwargs.get('preprocessing_params', {}),
                    **kwargs
                }
                
                self.logger.info(f"âœ… step_interface.py í˜¸í™˜ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def force_register_to_di_container(self) -> bool:
        """DI Containerì— ê°•ì œ ë“±ë¡"""
        try:
            if not self._di_container:
                return False
            
            return self._di_container.force_register_model_loader(self)
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê°•ì œ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ (step_interface.py í˜¸í™˜)"""
        try:
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            if model_name not in self.model_info and model_name not in self._available_models_cache:
                return False
            
            # Step ìš”êµ¬ì‚¬í•­ í™•ì¸
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # step_interface.py ë§¤í•‘ í™•ì¸
            if step_name in self.step_interface_mappings:
                mapping = self.step_interface_mappings[step_name]
                if model_name in mapping.get('ai_models', []):
                    return True
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path or Path(local_path).name == model_name:
                        return True
            
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í˜¸í™˜ ê°€ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (step_interface.py í˜¸í™˜)"""
        return (model_name in self.loaded_models or 
                model_name in self._available_models_cache or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ (step_interface.py í˜¸í™˜)"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ AI ëª¨ë¸ ëª©ë¡ (step_interface.py ì™„ì „ í˜¸í™˜)"""
        try:
            models = []
            
            # available_modelsì—ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            for model_name, model_info in self._available_models_cache.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # ë¡œë”© ìƒíƒœ ì¶”ê°€ (step_interface.py í˜¸í™˜)
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                # step_interface.py í˜¸í™˜ í•„ë“œ ì¶”ê°€
                model_info_copy.update({
                    "real_ai_model": True,
                    "checkpoint_loaded": is_loaded and self.loaded_models.get(model_name, {}).get('checkpoint_data') is not None if is_loaded else False,
                    "step_loadable": True,
                    "device_compatible": True,
                    "requires_checkpoint": True
                })
                
                models.append(model_info_copy)
            
            # step_interface.py ë§¤í•‘ì—ì„œ ì¶”ê°€
            for step_name, mapping in self.step_interface_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                step_type = mapping.get('step_type', RealStepModelType.HUMAN_PARSING)
                for model_name in mapping.get('ai_models', []):
                    if model_name not in [m['name'] for m in models]:
                        # step_interface.py í˜¸í™˜ ëª¨ë¸ ì •ë³´
                        models.append({
                            'name': model_name,
                            'path': f"ai_models/step_{mapping.get('step_id', 0):02d}_{step_name.lower()}/{model_name}",
                            'type': self._infer_model_type(model_name),
                            'step_type': step_type.value,
                            'loaded': model_name in self.loaded_models,
                            'step_class': step_name,
                            'step_id': mapping.get('step_id', 0),
                            'size_mb': 0.0,  # ì‹¤ì œ íŒŒì¼ í¬ê¸°ëŠ” ë¡œë”© ì‹œ ê³„ì‚°
                            'priority': self._infer_model_priority(model_name),
                            'is_primary': model_name == mapping.get('primary_model'),
                            'real_ai_model': True,
                            'device_compatible': True,
                            'requires_checkpoint': True,
                            'step_loadable': True
                        })
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì •ë³´ ì¡°íšŒ (step_interface.py ì™„ì „ í˜¸í™˜)"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'step_type': info.step_type.value,
                    'priority': info.priority.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'inference_count': info.inference_count,
                    'avg_inference_time': info.avg_inference_time,
                    'validation_passed': info.validation_passed,
                    'has_checkpoint_data': info.checkpoint_data is not None,
                    'error': info.error,
                    
                    # step_interface.py í˜¸í™˜ í•„ë“œ
                    'model_type': info.model_type,
                    'size_gb': info.size_gb,
                    'requires_checkpoint': info.requires_checkpoint,
                    'preprocessing_required': info.preprocessing_required,
                    'postprocessing_required': info.postprocessing_required,
                    'real_ai_model': True,
                    'device_compatible': True,
                    'step_loadable': True
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (step_interface.py í˜¸í™˜)"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "mps_available": MPS_AVAILABLE,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self._available_models_cache),
            "step_interfaces_count": len(self.step_interfaces),
            "avg_inference_time": self.performance_metrics['total_inference_time'] / max(1, self.performance_metrics['inference_count']),
            "memory_efficiency": self.performance_metrics['total_memory_mb'] / max(1, len(self.loaded_models)),
            
            # step_interface.py í˜¸í™˜ í•„ë“œ
            "step_interface_v5_2_compatible": True,
            "github_step_mapping_loaded": len(self.step_interface_mappings) > 0,
            "real_ai_models_only": True,
            "mock_removed": True,
            "checkpoint_loading_optimized": True
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (step_interface.py í˜¸í™˜)"""
        try:
            self.logger.info("ğŸ§¹ step_interface.py í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # ìºì‹œ ì •ë¦¬
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            self.step_requirements.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE and TORCH_AVAILABLE:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… step_interface.py í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 6. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (step_interface.py ì™„ì „ í˜¸í™˜)
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
   """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (step_interface.py í˜¸í™˜)"""
   global _global_model_loader
   
   with _loader_lock:
       if _global_model_loader is None:
           try:
               # ì„¤ì • ì ìš©
               loader_config = config or {}
               
               # ğŸ”¥ DI Container ì¡°íšŒí•´ì„œ ì£¼ì…
               di_container = None
               try:
                   # ìˆœí™˜ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ë™ì  import
                   import importlib
                   di_module = importlib.import_module('app.core.di_container')
                   get_global_container = getattr(di_module, 'get_global_container', None)
                   if get_global_container:
                       di_container = get_global_container()
                       logger.debug("âœ… DI Container ì¡°íšŒ ì„±ê³µ")
                   else:
                       logger.debug("âš ï¸ get_global_container í•¨ìˆ˜ ì—†ìŒ")
               except ImportError as e:
                   logger.debug(f"âš ï¸ DI Container ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
               except Exception as e:
                   logger.debug(f"âš ï¸ DI Container ì¡°íšŒ ì‹¤íŒ¨: {e}")
               
               # ğŸ”¥ ModelLoader ìƒì„± ì‹œ DI Container ì£¼ì…
               _global_model_loader = ModelLoader(
                   device=loader_config.get('device', 'auto'),
                   max_cached_models=loader_config.get('max_cached_models', 10),
                   enable_optimization=loader_config.get('enable_optimization', True),
                   di_container=di_container,  # ğŸ”¥ DI Container ì£¼ì…
                   **loader_config
               )
               
               # ğŸ”¥ DI Containerì— ë“±ë¡ í™•ì¸ ë° ê°•ì œ ë“±ë¡
               if di_container:
                   try:
                       # ModelLoaderë¥¼ DI Containerì— ê°•ì œ ë“±ë¡
                       success = di_container.force_register_model_loader(_global_model_loader)
                       if success:
                           logger.info("âœ… ModelLoaderê°€ DI Containerì— ê°•ì œ ë“±ë¡ë¨")
                       else:
                           logger.warning("âš ï¸ ModelLoader DI Container ê°•ì œ ë“±ë¡ ì‹¤íŒ¨")
                       
                       # ë“±ë¡ í™•ì¸
                       ensure_model_loader_registration = getattr(di_module, 'ensure_model_loader_registration', None)
                       if ensure_model_loader_registration:
                           ensure_model_loader_registration()
                           
                   except Exception as e:
                       logger.debug(f"âš ï¸ ModelLoader ë“±ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
               
               logger.info("âœ… ì „ì—­ step_interface.py í˜¸í™˜ ModelLoader v5.1 ìƒì„± ì„±ê³µ (DI Container ì—°ë™)")
               
           except Exception as e:
               logger.error(f"âŒ ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
               # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°± (DI Container ì—†ì´)
               _global_model_loader = ModelLoader(device="cpu", di_container=None)
               
       return _global_model_loader
   
def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” (step_interface.py í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” (step_interface.py í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (step_interface.py í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        step_type = RealStepModelType.HUMAN_PARSING
        return RealStepModelInterface(get_global_model_loader(), step_name, step_type)

def get_model(model_name: str) -> Optional[RealAIModel]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (step_interface.py í˜¸í™˜)"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[RealAIModel]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (step_interface.py í˜¸í™˜)"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> RealStepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (step_interface.py í˜¸í™˜)"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# step_interface.py í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
BaseModel = RealAIModel
StepModelInterface = RealStepModelInterface

# ==============================================
# ğŸ”¥ 7. Export ë° ì´ˆê¸°í™”
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤ (step_interface.py ì™„ì „ í˜¸í™˜)
    'ModelLoader',
    'RealStepModelInterface',
    'EnhancedStepModelInterface',  # í˜¸í™˜ì„± ë³„ì¹­
    'StepModelInterface',  # í˜¸í™˜ì„± ë³„ì¹­
    'RealAIModel',
    'BaseModel',  # í˜¸í™˜ì„± ë³„ì¹­
    
    # step_interface.py ì™„ì „ í˜¸í™˜ ë°ì´í„° êµ¬ì¡°ë“¤
    'RealStepModelType',
    'RealModelStatus',
    'RealModelPriority',
    'RealStepModelInfo',
    'RealStepModelRequirement',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤ (step_interface.py ì™„ì „ í˜¸í™˜)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # ìƒìˆ˜ë“¤
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'TORCH_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'MPS_AVAILABLE',
    'CONDA_ENV',
    'DEFAULT_DEVICE'
]

# ==============================================
# ğŸ”¥ 8. ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("ğŸš€ ì™„ì „ ê°œì„ ëœ ModelLoader v5.1 - step_interface.py v5.2 ì™„ì „ í˜¸í™˜")
logger.info("=" * 80)
logger.info("âœ… step_interface.py RealStepModelInterface ìš”êµ¬ì‚¬í•­ 100% ë°˜ì˜")
logger.info("âœ… GitHubStepMapping ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ ì™„ì „ ë§¤í•‘")
logger.info("âœ… 229GB AI ëª¨ë¸ íŒŒì¼ë“¤ ì •í™•í•œ ë¡œë”© ì§€ì›")
logger.info("âœ… RealAIModel í´ë˜ìŠ¤ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ì „ ê°œì„ ")
logger.info("âœ… Stepë³„ íŠ¹í™” ë¡œë” ì§€ì› (Human Parsing, Pose, Segmentation ë“±)")
logger.info("âœ… BaseStepMixin v19.2 ì™„ë²½ í˜¸í™˜")
logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë²½ ì§€ì›")
logger.info("âœ… Mock ì™„ì „ ì œê±° - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë§Œ ì‚¬ìš©")
logger.info("âœ… PyTorch weights_only ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… Auto Detector ì™„ì „ ì—°ë™")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ì‘ë™")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
logger.info(f"   PyTorch: {TORCH_AVAILABLE}, NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info(f"   conda í™˜ê²½: {CONDA_ENV}")

logger.info("ğŸ¯ ì§€ì› ì‹¤ì œ AI Step íƒ€ì… (step_interface.py ì™„ì „ í˜¸í™˜):")
for step_type in RealStepModelType:
    logger.info(f"   - {step_type.value}: íŠ¹í™” ë¡œë” ì§€ì›")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ RealAIModel: Stepë³„ íŠ¹í™” ì²´í¬í¬ì¸íŠ¸ ë¡œë”©")
logger.info("   â€¢ RealStepModelInterface: step_interface.py ì™„ì „ í˜¸í™˜")
logger.info("   â€¢ ì‹¤ì œ AI Step ë§¤í•‘: step_interface.py GitHubStepMapping ê¸°ë°˜")
logger.info("   â€¢ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëª¨ë¸ ìºì‹±: Primary/Secondary/Fallback")
logger.info("   â€¢ Graphonomy 1.2GB ëª¨ë¸ ì´ˆì•ˆì „ ë¡œë”©")
logger.info("   â€¢ RealVisXL 6.46GB Safetensors ì™„ë²½ ì§€ì›")
logger.info("   â€¢ Diffusion 4.8GB ëª¨ë¸ ì™„ë²½ ì§€ì›")
logger.info("   â€¢ U2Net 176GB ëª¨ë¸ ì™„ë²½ ì§€ì›")
logger.info("   â€¢ Real-ESRGAN 64GB ëª¨ë¸ ì™„ë²½ ì§€ì›")
logger.info("   â€¢ Auto Detector ì™„ì „ ì—°ë™")

logger.info("ğŸš€ ì‹¤ì œ AI Step ì§€ì› íë¦„ (step_interface.py ì™„ì „ í˜¸í™˜):")
logger.info("   StepFactory (v11.0)")
logger.info("     â†“ (Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± + ì˜ì¡´ì„± ì£¼ì…)")
logger.info("   BaseStepMixin (v19.2)")
logger.info("     â†“ (ë‚´ì¥ GitHubDependencyManager ì‚¬ìš©)")
logger.info("   step_interface.py (v5.2)")
logger.info("     â†“ (RealStepModelInterface ì œê³µ)")
logger.info("   ModelLoader (v5.1) â† ğŸ”¥ ì™„ì „ í˜¸í™˜ ê°œì„ !")
logger.info("     â†“ (RealAIModelë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©)")
logger.info("   ì‹¤ì œ AI ëª¨ë¸ë“¤ (229GB)")

logger.info("ğŸ‰ ì™„ì „ ê°œì„ ëœ ModelLoader v5.1 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ‰ step_interface.py v5.2ì™€ ì™„ë²½í•œ í˜¸í™˜ì„± ë‹¬ì„±!")
logger.info("ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ì „ ì§€ì›!")
logger.info("ğŸ‰ Mock ì œê±°, ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìµœì í™” ì™„ë£Œ!")
logger.info("ğŸ‰ ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ì‘ë™!")
logger.info("=" * 80)

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_loader = get_global_model_loader()
    if hasattr(_test_loader, 'validate_di_container_integration'):
        di_integration = _test_loader.validate_di_container_integration()
        logger.info(f"ğŸ”— DI Container ì—°ë™ ìƒíƒœ: {di_integration.get('registered_in_container', False)}")
   
    logger.info(f"ğŸ‰ step_interface.py v5.2 ì™„ì „ í˜¸í™˜ ModelLoader v5.1 ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   ë””ë°”ì´ìŠ¤: {_test_loader.device}")
    logger.info(f"   ëª¨ë¸ ìºì‹œ: {_test_loader.model_cache_dir}")
    logger.info(f"   step_interface.py ë§¤í•‘: {len(_test_loader.step_interface_mappings)}ê°œ Step")
    logger.info(f"   AutoDetector í†µí•©: {_test_loader._integration_successful}")
    logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(_test_loader._available_models_cache)}ê°œ")
    logger.info(f"   ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©: âœ…")
    logger.info(f"   step_interface.py v5.2 í˜¸í™˜: âœ…")
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")