# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ ModelLoader v21.0 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ì•ˆì •ì„± ê°•í™”
========================================================

âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… StepModelInterface ê°œì„ 
âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
âœ… í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬
âœ… M3 Max 128GB ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-24
Version: 21.0 (Circular Reference Complete Solution)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager, contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ==============================================

logger = logging.getLogger(__name__)

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
NUMPY_AVAILABLE = False
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False
CONDA_ENV = "none"

try:
    os.environ.update({
        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
        'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
        'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1'
    })
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
        if torch.backends.mps.is_available():
            MPS_AVAILABLE = True
            DEFAULT_DEVICE = "mps"
            
            # M3 Max ê°ì§€
            try:
                import platform
                import subprocess
                if platform.system() == 'Darwin':
                    result = subprocess.run(
                        ['sysctl', '-n', 'machdep.cpu.brand_string'],
                        capture_output=True, text=True, timeout=5
                    )
                    IS_M3_MAX = 'M3' in result.stdout
            except:
                pass
                
    elif torch.cuda.is_available():
        DEFAULT_DEVICE = "cuda"
        
except ImportError:
    torch = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# conda í™˜ê²½ ê°ì§€
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class LoadingStatus(Enum):
    """ë¡œë”© ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    TENSORFLOW = "bin"
    ONNX = "onnx"
    PICKLE = "pkl"
    CHECKPOINT = "ckpt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    file_exists: bool
    size_mb: float
    checksum: Optional[str] = None
    error_message: Optional[str] = None
    validation_time: float = 0.0

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    file_size_mb: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation: Optional[CheckpointValidation] = None
    loading_status: LoadingStatus = LoadingStatus.NOT_LOADED
    last_validated: float = 0.0

@dataclass
class SafeModelCacheEntry:
    """ì•ˆì „í•œ ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    load_time: float
    last_access: float
    access_count: int
    memory_usage_mb: float
    device: str
    step_name: Optional[str] = None
    validation: Optional[CheckpointValidation] = None
    is_healthy: bool = True
    error_count: int = 0

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸°
# ==============================================

class CheckpointValidator:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ê¸°"""
    
    @staticmethod
    def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦"""
        start_time = time.time()
        checkpoint_path = Path(checkpoint_path)
        
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not checkpoint_path.exists():
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=False,
                    size_mb=0.0,
                    error_message=f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {checkpoint_path}",
                    validation_time=time.time() - start_time
                )
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            size_bytes = checkpoint_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            if size_bytes == 0:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=0.0,
                    error_message="íŒŒì¼ í¬ê¸°ê°€ 0ë°”ì´íŠ¸",
                    validation_time=time.time() - start_time
                )
            
            # ìµœì†Œ í¬ê¸° í™•ì¸ (10MB ì´ìƒ)
            if size_mb < 10:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=size_mb,
                    error_message=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {size_mb:.1f}MB",
                    validation_time=time.time() - start_time
                )
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            if TORCH_AVAILABLE:
                validation_result = CheckpointValidator._validate_pytorch_checkpoint(checkpoint_path)
                if not validation_result.is_valid:
                    return validation_result
            
            # ì²´í¬ì„¬ ê³„ì‚° (1GB ë¯¸ë§Œì¸ ê²½ìš°ë§Œ)
            checksum = None
            if size_mb < 1000:
                try:
                    checksum = CheckpointValidator._calculate_checksum(checkpoint_path)
                except:
                    pass
            
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=size_mb,
                checksum=checksum,
                validation_time=time.time() - start_time
            )
            
        except Exception as e:
            return CheckpointValidation(
                is_valid=False,
                file_exists=checkpoint_path.exists(),
                size_mb=0.0,
                error_message=f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                validation_time=time.time() - start_time
            )
    
    @staticmethod
    def _validate_pytorch_checkpoint(checkpoint_path: Path) -> CheckpointValidation:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦"""
        start_time = time.time()
        
        try:
            import torch
            
            # ì•ˆì „í•œ ë¡œë”© ì‹œë„
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {checkpoint_path.name}")
            except Exception:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                    logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {checkpoint_path.name}")
                except Exception as load_error:
                    return CheckpointValidation(
                        is_valid=False,
                        file_exists=True,
                        size_mb=checkpoint_path.stat().st_size / (1024**2),
                        error_message=f"PyTorch ë¡œë”© ì‹¤íŒ¨: {str(load_error)}",
                        validation_time=time.time() - start_time
                    )
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ê²€ì¦
            if checkpoint is None:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=checkpoint_path.stat().st_size / (1024**2),
                    error_message="ì²´í¬í¬ì¸íŠ¸ê°€ None",
                    validation_time=time.time() - start_time
                )
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ í™•ì¸
            if isinstance(checkpoint, dict) and len(checkpoint) == 0:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=checkpoint_path.stat().st_size / (1024**2),
                    error_message="ë¹ˆ ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬",
                    validation_time=time.time() - start_time
                )
            
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                validation_time=time.time() - start_time
            )
            
        except ImportError:
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                error_message="PyTorch ê²€ì¦ ë¶ˆê°€",
                validation_time=time.time() - start_time
            )
        except Exception as e:
            return CheckpointValidation(
                is_valid=False,
                file_exists=True,
                size_mb=checkpoint_path.stat().st_size / (1024**2),
                error_message=f"PyTorch ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                validation_time=time.time() - start_time
            )
    
    @staticmethod
    def _calculate_checksum(file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# ==============================================

class SafeAsyncContextManager:
    """ì•ˆì „í•œ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, resource_name: str = "ModelLoader"):
        self.resource_name = resource_name
        self.is_entered = False
        self.logger = logging.getLogger(f"SafeAsyncCM.{resource_name}")
    
    async def __aenter__(self):
        """ì•ˆì „í•œ ë¹„ë™ê¸° ì§„ì…"""
        try:
            self.logger.debug(f"ğŸ”„ {self.resource_name} ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ì§„ì…")
            self.is_entered = True
            return self
        except Exception as e:
            self.logger.error(f"âŒ {self.resource_name} ë¹„ë™ê¸° ì§„ì… ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Async context enter failed for {self.resource_name}: {e}")
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì•ˆì „í•œ ë¹„ë™ê¸° ì¢…ë£Œ"""
        try:
            if self.is_entered:
                self.logger.debug(f"ğŸ”„ {self.resource_name} ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ")
                self.is_entered = False
                
                if exc_type is not None:
                    self.logger.warning(f"âš ï¸ {self.resource_name} ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {exc_type.__name__}: {exc_val}")
                    
            return False
        except Exception as e:
            self.logger.error(f"âŒ {self.resource_name} ë¹„ë™ê¸° ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ ê°œì„ ëœ StepModelInterface
# ==============================================

class StepModelInterface:
    """ê°œì„ ëœ Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, SafeModelCacheEntry] = {}
        self.model_status: Dict[str, LoadingStatus] = {}
        self._lock = threading.RLock()
        
        # Step ìš”êµ¬ì‚¬í•­
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.creation_time = time.time()
        self.error_count = 0
        self.last_error = None
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹œì‘: {model_name}")
                
                # ModelLoaderì˜ register_model_requirement í˜¸ì¶œ
                if hasattr(self.model_loader, 'register_model_requirement'):
                    success = self.model_loader.register_model_requirement(
                        model_name=model_name,
                        model_type=model_type,
                        step_name=self.step_name,
                        **kwargs
                    )
                    if success:
                        self.step_requirements[model_name] = {
                            "model_name": model_name,
                            "model_type": model_type,
                            "step_name": self.step_name,
                            "registered_at": time.time(),
                            **kwargs
                        }
                        self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                        return True
                else:
                    self.step_requirements[model_name] = {
                        "model_name": model_name,
                        "model_type": model_type,
                        "step_name": self.step_name,
                        "registered_at": time.time(),
                        **kwargs
                    }
                    self.logger.info(f"âœ… ë¡œì»¬ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                    return True
               
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        async with SafeAsyncContextManager(f"GetModel.{self.step_name}"):
            try:
                if not model_name:
                    model_name = "default_model"
                
                # ìºì‹œ í™•ì¸
                with self._lock:
                    if model_name in self.model_cache:
                        cache_entry = self.model_cache[model_name]
                        if cache_entry.is_healthy:
                            cache_entry.last_access = time.time()
                            cache_entry.access_count += 1
                            self.logger.debug(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                            return cache_entry.model
                        else:
                            del self.model_cache[model_name]
                
                # ë¡œë”© ìƒíƒœ ì„¤ì •
                self.model_status[model_name] = LoadingStatus.LOADING
                
                # ModelLoaderë¥¼ í†µí•œ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                checkpoint = await self._safe_load_checkpoint(model_name)
                
                if checkpoint:
                    # ì•ˆì „í•œ ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                        step_name=self.step_name,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    with self._lock:
                        self.model_cache[model_name] = cache_entry
                        self.loaded_models[model_name] = checkpoint
                        self.model_status[model_name] = LoadingStatus.LOADED
                    
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return checkpoint
                
                self.model_status[model_name] = LoadingStatus.ERROR
                return None
                
            except Exception as e:
                self.error_count += 1
                self.last_error = str(e)
                self.model_status[model_name] = LoadingStatus.ERROR
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return None
    
    async def _safe_load_checkpoint(self, model_name: str) -> Optional[Any]:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if hasattr(self.model_loader, 'load_model_async'):
                return await self.model_loader.load_model_async(model_name)
            elif hasattr(self.model_loader, 'load_model'):
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None, 
                    self.model_loader.load_model, 
                    model_name
                )
            else:
                self.logger.error(f"âŒ ModelLoaderì— ë¡œë”© ë©”ì„œë“œ ì—†ìŒ")
                return None
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            if not model_name:
                model_name = "default_model"
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.is_healthy:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        return cache_entry.model
                    else:
                        del self.model_cache[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(model_name)
            
            if checkpoint:
                with self._lock:
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                        step_name=self.step_name,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = LoadingStatus.LOADED
                return checkpoint
            
            self.model_status[model_name] = LoadingStatus.ERROR
            return None
            
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ í¬ê¸° ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)
                elif hasattr(checkpoint, 'parameters'):
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0
    
    def list_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        models = []
        
        # ë¡œì»¬ ëª¨ë¸ë“¤ ì¶”ê°€
        for model_name in self.step_requirements.keys():
            is_loaded = model_name in self.loaded_models
            cache_entry = self.model_cache.get(model_name)
            
            models.append({
                "name": model_name,
                "path": f"step_models/{model_name}",
                "size_mb": cache_entry.memory_usage_mb if cache_entry else 0.0,
                "model_type": self.step_name.lower(),
                "step_class": self.step_name,
                "loaded": is_loaded,
                "device": cache_entry.device if cache_entry else "auto",
                "metadata": {
                    "step_name": self.step_name,
                    "access_count": cache_entry.access_count if cache_entry else 0
                }
            })
        
        # í¬ê¸°ìˆœ ì •ë ¬
        models.sort(key=lambda x: x["size_mb"], reverse=True)
        return models

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤
# ==============================================

class ModelLoader:
    """ê°œì„ ëœ ModelLoader v21.0 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ê°œì„ ëœ ModelLoader ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._resolve_device(device or "auto")
        
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = self._get_memory_info()
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬
        self.model_cache_dir = self._resolve_model_cache_dir(kwargs.get('model_cache_dir'))
        
        # ì„¤ì • íŒŒë¼ë¯¸í„°
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 30 if self.is_m3_max else 15)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        self.min_model_size_mb = kwargs.get('min_model_size_mb', 10)
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # í•µì‹¬ ì†ì„±ë“¤
        self.loaded_models: Dict[str, Any] = {}
        self.model_configs: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self.available_models: Dict[str, Any] = {}
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_interfaces: Dict[str, Any] = {}
        self._loaded_models = self.loaded_models
        self._is_initialized = False
        
        # ì„±ëŠ¥ ì¶”ì 
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'validation_count': 0,
            'validation_success': 0,
            'checkpoint_loads': 0
        }
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="model_loader_v21")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸°
        self.validator = CheckpointValidator()
        
        # ì•ˆì „í•œ ì´ˆê¸°í™” ì‹¤í–‰
        self._safe_initialize()
        
        self.logger.info(f"ğŸ¯ ê°œì„ ëœ ModelLoader v21.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
        self.logger.info(f"ğŸ’¾ Memory: {self.memory_gb:.1f}GB")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬: {self.model_cache_dir}")
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device == "auto":
            return DEFAULT_DEVICE
        return device
    
    def _resolve_model_cache_dir(self, model_cache_dir_raw) -> Path:
        """ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ í•´ê²°"""
        try:
            if model_cache_dir_raw is None:
                # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìë™ ê³„ì‚°
                current_file = Path(__file__).resolve()
                # backend/app/ai_pipeline/utils/model_loader.pyì—ì„œ backend/ ì°¾ê¸°
                current_path = current_file.parent
                for i in range(10):
                    if current_path.name == 'backend':
                        ai_models_path = current_path / "ai_models"
                        return ai_models_path
                    if current_path.parent == current_path:
                        break
                    current_path = current_path.parent
                
                # í´ë°±
                return Path.cwd() / "ai_models"
            else:
                path = Path(model_cache_dir_raw)
                # backend/backend íŒ¨í„´ ì œê±°
                path_str = str(path)
                if "backend/backend" in path_str:
                    path = Path(path_str.replace("backend/backend", "backend"))
                return path.resolve()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ í•´ê²° ì‹¤íŒ¨: {e}")
            return Path.cwd() / "ai_models"
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return memory.total / (1024**3)
        except ImportError:
            return 128.0 if IS_M3_MAX else 16.0
    
    def _safe_initialize(self):
        """ì•ˆì „í•œ ì´ˆê¸°í™”"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            if not self.model_cache_dir.exists():
                self.model_cache_dir.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.model_cache_dir}")
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº”
            self._scan_available_models()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.optimization_enabled:
                self._safe_memory_cleanup()
            
            self.logger.info(f"ğŸ“¦ ModelLoader ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”"""
        try:
            # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •ë“¤
            default_models = {
                "human_parsing_model": ModelConfig(
                    name="human_parsing_model",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="HumanParsingModel",
                    device="auto",
                    precision="fp16",
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_model": ModelConfig(
                    name="pose_estimation_model",
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="PoseEstimationModel",
                    device="auto",
                    precision="fp16",
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_model": ModelConfig(
                    name="cloth_segmentation_model",
                    model_type=ModelType.CLOTH_SEGMENTATION,
                    model_class="ClothSegmentationModel",
                    device="auto",
                    precision="fp16",
                    input_size=(320, 320),
                    num_classes=1
                ),
                "virtual_fitting_model": ModelConfig(
                    name="virtual_fitting_model",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="VirtualFittingModel",
                    device="auto",
                    precision="fp16",
                    input_size=(512, 512)
                )
            }
            
            for name, config in default_models.items():
                self.model_configs[name] = config
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”: {len(default_models)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _scan_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº”"""
        try:
            if not self.model_cache_dir.exists():
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_cache_dir}")
                return
            
            # ê²€ìƒ‰ ê²½ë¡œë“¤
            search_paths = [
                self.model_cache_dir,
                self.model_cache_dir / "checkpoints",
                self.model_cache_dir / "models",
                self.model_cache_dir / "step_01_human_parsing",
                self.model_cache_dir / "step_02_pose_estimation",
                self.model_cache_dir / "step_03_cloth_segmentation",
                self.model_cache_dir / "step_06_virtual_fitting",
            ]
            
            # ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ í•„í„°ë§
            existing_paths = [p for p in search_paths if p.exists()]
            
            scanned_count = 0
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            
            for search_path in existing_paths:
                try:
                    for ext in extensions:
                        for model_file in search_path.rglob(f"*{ext}"):
                            try:
                                if not model_file.is_file():
                                    continue
                                
                                size_mb = model_file.stat().st_size / (1024 * 1024)
                                if size_mb < self.min_model_size_mb:
                                    continue
                                
                                # ê°„ë‹¨í•œ ê²€ì¦
                                if not self._quick_validate_file(model_file):
                                    continue
                                
                                relative_path = model_file.relative_to(self.model_cache_dir)
                                model_type, step_class = self._detect_model_info(model_file)
                                
                                model_info = {
                                    "name": model_file.stem,
                                    "path": str(relative_path),
                                    "size_mb": round(size_mb, 2),
                                    "model_type": model_type,
                                    "step_class": step_class,
                                    "loaded": False,
                                    "device": self.device,
                                    "is_valid": True,
                                    "metadata": {
                                        "extension": ext,
                                        "parent_dir": model_file.parent.name,
                                        "full_path": str(model_file),
                                        "is_large": size_mb > 500,
                                        "detected_from": str(search_path.name)
                                    }
                                }
                                
                                self.available_models[model_file.stem] = model_info
                                scanned_count += 1
                                
                            except Exception as e:
                                self.logger.debug(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {model_file}: {e}")
                                continue
                                
                except Exception as path_error:
                    self.logger.debug(f"âš ï¸ ê²½ë¡œ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {path_error}")
                    continue
            
            self.logger.info(f"âœ… ëª¨ë¸ ìŠ¤ìº” ì™„ë£Œ: {scanned_count}ê°œ ë“±ë¡")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
    
    def _quick_validate_file(self, file_path: Path) -> bool:
        """ë¹ ë¥¸ íŒŒì¼ ê²€ì¦"""
        try:
            if not file_path.exists():
                return False
            
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb < 1:
                return False
                
            valid_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt'}
            if file_path.suffix.lower() not in valid_extensions:
                return False
                
            return True
        except Exception:
            return False
    
    def _detect_model_info(self, model_file: Path) -> tuple:
        """ëª¨ë¸ íƒ€ì… ë° Step í´ë˜ìŠ¤ ê°ì§€"""
        filename = model_file.name.lower()
        path_str = str(model_file).lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ê°ì§€
        if "schp" in filename or "human" in filename or "parsing" in filename:
            return "human_parsing", "HumanParsingStep"
        elif "openpose" in filename or "pose" in filename:
            return "pose_estimation", "PoseEstimationStep"
        elif "u2net" in filename or "segment" in filename or "cloth" in filename:
            return "cloth_segmentation", "ClothSegmentationStep"
        elif "diffusion" in filename or "pytorch_model" in filename:
            return "virtual_fitting", "VirtualFittingStep"
        
        # ê²½ë¡œ ê¸°ë°˜ ê°ì§€
        if "step_01" in path_str or "human_parsing" in path_str:
            return "human_parsing", "HumanParsingStep"
        elif "step_02" in path_str or "pose" in path_str:
            return "pose_estimation", "PoseEstimationStep"
        elif "step_03" in path_str or "cloth" in path_str:
            return "cloth_segmentation", "ClothSegmentationStep"
        elif "step_06" in path_str or "virtual" in path_str:
            return "virtual_fitting", "VirtualFittingStep"
        
        return "unknown", "UnknownStep"
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    
    # ==============================================
    # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            with self._interface_lock:
                # Step ìš”êµ¬ì‚¬í•­ ë“±ë¡
                if step_requirements:
                    self.register_step_requirements(step_name, step_requirements)
                
                # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if step_name in self.step_interfaces:
                    return self.step_interfaces[step_name]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                
                self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(self, step_name)
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Union[Dict[str, Any], List[Dict[str, Any]]]
    ) -> bool:
        """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # requirementsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(requirements, list):
                    processed_requirements = {}
                    for i, req in enumerate(requirements):
                        if isinstance(req, dict):
                            model_name = req.get("model_name", f"{step_name}_model_{i}")
                            processed_requirements[model_name] = req
                    requirements = processed_requirements
                
                # ìš”êµ¬ì‚¬í•­ ì—…ë°ì´íŠ¸
                if isinstance(requirements, dict):
                    self.step_requirements[step_name].update(requirements)
                
                self.logger.info(f"âœ… {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                model_config = {
                    "name": model_name,
                    "model_type": model_type,
                    "model_class": kwargs.get("model_class", model_type),
                    "device": kwargs.get("device", "auto"),
                    "precision": kwargs.get("precision", "fp16"),
                    "input_size": tuple(kwargs.get("input_size", (512, 512))),
                    "num_classes": kwargs.get("num_classes"),
                    "file_size_mb": kwargs.get("file_size_mb", 0.0),
                    "metadata": kwargs.get("metadata", {})
                }
                
                self.model_configs[model_name] = model_config
                
                self.available_models[model_name] = {
                    "name": model_name,
                    "path": f"requirements/{model_name}",
                    "size_mb": kwargs.get("file_size_mb", 0.0),
                    "model_type": model_type,
                    "step_class": kwargs.get("model_class", model_type),
                    "loaded": False,
                    "device": kwargs.get("device", "auto"),
                    "metadata": {
                        "source": "requirement_registration",
                        "registered_at": time.time(),
                        "step_name": kwargs.get("step_name", "unknown"),
                        **kwargs.get("metadata", {})
                    }
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def register_model_config(self, name: str, config: Union[ModelConfig, Dict[str, Any]]) -> bool:
        """ëª¨ë¸ ì„¤ì • ë“±ë¡"""
        try:
            with self._lock:
                if isinstance(config, dict):
                    model_config = config
                else:
                    model_config = config.__dict__ if hasattr(config, '__dict__') else config
                
                self.model_configs[name] = model_config
                
                self.available_models[name] = {
                    "name": name,
                    "path": model_config.get("checkpoint_path", f"config/{name}"),
                    "size_mb": model_config.get("file_size_mb", 0.0),
                    "model_type": str(model_config.get("model_type", "unknown")),
                    "step_class": model_config.get("model_class", "BaseModel"),
                    "loaded": False,
                    "device": model_config.get("device", "auto"),
                    "metadata": model_config.get("metadata", {})
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ì„¤ì • ë“±ë¡ ì™„ë£Œ: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„¤ì • ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            models = []
            
            for model_name, model_info in self.available_models.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                    
                models.append({
                    "name": model_info["name"],
                    "path": model_info["path"],
                    "size_mb": model_info["size_mb"],
                    "model_type": model_info["model_type"],
                    "step_class": model_info["step_class"],
                    "loaded": model_info["loaded"],
                    "device": model_info["device"],
                    "is_valid": model_info.get("is_valid", True),
                    "metadata": model_info["metadata"]
                })
            
            # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
            models.sort(key=lambda x: x["size_mb"], reverse=True)
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    # ==============================================
    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        async with SafeAsyncContextManager(f"LoadModel.{model_name}"):
            try:
                # ìºì‹œ í™•ì¸
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.get('is_healthy', True):
                        cache_entry['last_access'] = time.time()
                        cache_entry['access_count'] = cache_entry.get('access_count', 0) + 1
                        self.performance_stats['cache_hits'] += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                        return cache_entry['model']
                
                if model_name not in self.available_models and model_name not in self.model_configs:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                    return None
                
                # ë¹„ë™ê¸°ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤í–‰
                loop = asyncio.get_event_loop()
                checkpoint = await loop.run_in_executor(
                    self._executor, 
                    self._safe_load_checkpoint_sync,
                    model_name,
                    kwargs
                )
                
                if checkpoint is not None:
                    # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    cache_entry = {
                        'model': checkpoint,
                        'load_time': time.time(),
                        'last_access': time.time(),
                        'access_count': 1,
                        'memory_usage_mb': self._estimate_checkpoint_size(checkpoint),
                        'device': getattr(checkpoint, 'device', self.device) if hasattr(checkpoint, 'device') else self.device,
                        'is_healthy': True,
                        'error_count': 0
                    }
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    
                    if model_name in self.available_models:
                        self.available_models[model_name]["loaded"] = True
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                    
                return checkpoint
                
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                if cache_entry.get('is_healthy', True):
                    cache_entry['last_access'] = time.time()
                    cache_entry['access_count'] = cache_entry.get('access_count', 0) + 1
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                    return cache_entry['model']
            
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
            
            return self._safe_load_checkpoint_sync(model_name, kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _safe_load_checkpoint_sync(self, model_name: str, kwargs: Dict[str, Any]) -> Optional[Any]:
        """ì•ˆì „í•œ ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            checkpoint_path = self._find_checkpoint_file(model_name)
            if not checkpoint_path or not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            validation = self.validator.validate_checkpoint_file(checkpoint_path)
            if not validation.is_valid:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {model_name} - {validation.error_message}")
                return None
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if TORCH_AVAILABLE:
                try:
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    if self.device in ["mps", "cuda"]:
                        self._safe_memory_cleanup()
                    
                    self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”©: {checkpoint_path}")
                    
                    # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    checkpoint = None
                    
                    try:
                        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                        self.logger.debug(f"âœ… ì•ˆì „í•œ ë¡œë”© ì„±ê³µ (weights_only=True): {model_name}")
                    except Exception:
                        try:
                            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                            self.logger.debug(f"âœ… ì¼ë°˜ ë¡œë”© ì„±ê³µ (weights_only=False): {model_name}")
                        except Exception as load_error:
                            self.logger.error(f"âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {load_error}")
                            return None
                    
                    if checkpoint is None:
                        self.logger.error(f"âŒ ë¡œë”©ëœ ì²´í¬í¬ì¸íŠ¸ê°€ None: {model_name}")
                        return None
                    
                    # ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬
                    processed_checkpoint = self._post_process_checkpoint(checkpoint, model_name)
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    load_time = time.time() - start_time
                    self.load_times[model_name] = load_time
                    self.last_access[model_name] = time.time()
                    self.access_counts[model_name] = self.access_counts.get(model_name, 0) + 1
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_name} ({load_time:.2f}ì´ˆ)")
                    return processed_checkpoint
                    
                except Exception as e:
                    self.logger.error(f"âŒ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                    return None
            
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë¶ˆê°€: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_checkpoint_file(self, model_name: str) -> Optional[Path]:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°"""
        try:
            # ëª¨ë¸ ì„¤ì •ì—ì„œ ì§ì ‘ ê²½ë¡œ í™•ì¸
            if model_name in self.model_configs:
                config = self.model_configs[model_name]
                if config.get('checkpoint_path'):
                    checkpoint_path = Path(config['checkpoint_path'])
                    if checkpoint_path.exists():
                        return checkpoint_path
            
            # available_modelsì—ì„œ ì°¾ê¸°
            if model_name in self.available_models:
                model_info = self.available_models[model_name]
                if "full_path" in model_info["metadata"]:
                    full_path = Path(model_info["metadata"]["full_path"])
                    if full_path.exists():
                        return full_path
            
            # ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            for ext in extensions:
                direct_path = self.model_cache_dir / f"{model_name}{ext}"
                if direct_path.exists():
                    return direct_path
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
            return self._find_via_pattern_matching(model_name, extensions)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_via_pattern_matching(self, model_name: str, extensions: List[str]) -> Optional[Path]:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°"""
        try:
            candidates = []
            for model_file in self.model_cache_dir.rglob("*"):
                if model_file.is_file() and model_file.suffix.lower() in extensions:
                    if model_name.lower() in model_file.name.lower():
                        try:
                            size_mb = model_file.stat().st_size / (1024 * 1024)
                            if size_mb >= self.min_model_size_mb:
                                candidates.append((model_file, size_mb))
                        except:
                            continue
            
            if candidates:
                # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
                candidates.sort(key=lambda x: x[1], reverse=True)
                best_candidate = candidates[0][0]
                return best_candidate
            
            return None
        except Exception:
            return None
    
    def _post_process_checkpoint(self, checkpoint: Any, model_name: str) -> Any:
        """ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬"""
        try:
            if isinstance(checkpoint, dict):
                if 'model' in checkpoint:
                    return checkpoint['model']
                elif 'state_dict' in checkpoint:
                    return checkpoint['state_dict']
                else:
                    return checkpoint
            return checkpoint
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return checkpoint
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)
                elif hasattr(checkpoint, 'parameters'):
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ë° ì„±ëŠ¥ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                return {
                    "status": "loaded",
                    "device": cache_entry.get('device', 'unknown'),
                    "memory_usage_mb": cache_entry.get('memory_usage_mb', 0.0),
                    "last_used": cache_entry.get('last_access', 0),
                    "load_time": cache_entry.get('load_time', 0),
                    "access_count": cache_entry.get('access_count', 0),
                    "model_type": type(cache_entry['model']).__name__,
                    "loaded": True,
                    "is_healthy": cache_entry.get('is_healthy', True),
                    "error_count": cache_entry.get('error_count', 0)
                }
            elif model_name in self.model_configs:
                return {
                    "status": "registered",
                    "device": self.device,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": "Not Loaded",
                    "loaded": False,
                    "is_healthy": True,
                    "error_count": 0
                }
            else:
                return {
                    "status": "not_found",
                    "device": None,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": None,
                    "loaded": False,
                    "is_healthy": False,
                    "error_count": 0
                }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"status": "error", "error": str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            total_memory = sum(
                entry.get('memory_usage_mb', 0) for entry in self.model_cache.values()
            )
            
            load_times = list(self.load_times.values())
            avg_load_time = sum(load_times) / len(load_times) if load_times else 0
            
            validation_rate = (
                self.performance_stats['validation_success'] / 
                max(1, self.performance_stats['validation_count'])
            )
            
            return {
                "model_counts": {
                    "loaded": len(self.model_cache),
                    "registered": len(self.model_configs),
                    "available": len(self.available_models)
                },
                "memory_usage": {
                    "total_mb": total_memory,
                    "average_per_model_mb": total_memory / len(self.model_cache) if self.model_cache else 0,
                    "device": self.device,
                    "available_memory_gb": self.memory_gb
                },
                "performance_stats": {
                    "cache_hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['models_loaded']),
                    "average_load_time_sec": avg_load_time,
                    "total_models_loaded": self.performance_stats['models_loaded'],
                    "checkpoint_loads": self.performance_stats.get('checkpoint_loads', 0),
                    "validation_rate": validation_rate,
                    "validation_count": self.performance_stats['validation_count'],
                    "validation_success": self.performance_stats['validation_success']
                },
                "step_interfaces": len(self.step_interfaces),
                "system_info": {
                    "conda_env": self.conda_env,
                    "is_m3_max": self.is_m3_max,
                    "torch_available": TORCH_AVAILABLE,
                    "mps_available": MPS_AVAILABLE,
                    "min_model_size_mb": self.min_model_size_mb
                },
                "version": "21.0"
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            if model_name in self.model_cache:
                del self.model_cache[model_name]
                
            if model_name in self.loaded_models:
                del self.loaded_models[model_name]
                
            if model_name in self.available_models:
                self.available_models[model_name]["loaded"] = False
                
            self._safe_memory_cleanup()
            
            self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {model_name} - {e}")
            return True  # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.model_cache.keys()):
                self.unload_model(model_name)
                
            # ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            self._safe_memory_cleanup()
            
            self.logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def initialize(self, **kwargs) -> bool:
        """ModelLoader ì´ˆê¸°í™”"""
        try:
            if self._is_initialized:
                return True
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            if kwargs:
                for key, value in kwargs.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
            
            # ì¬ì´ˆê¸°í™” ì‹¤í–‰
            self._safe_initialize()
            
            self._is_initialized = True
            self.logger.info(f"âœ… ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            result = self.initialize(**kwargs)
            if result:
                self.logger.info("âœ… ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return result
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸"""
        return getattr(self, '_is_initialized', False)
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            # ì˜¬ë°”ë¥¸ AI ëª¨ë¸ ê²½ë¡œ ê³„ì‚°
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            ai_models_path = backend_root / "ai_models"
            
            try:
                _global_model_loader = ModelLoader(
                    config=config,
                    device="auto",
                    model_cache_dir=str(ai_models_path),
                    use_fp16=True,
                    optimization_enabled=True,
                    enable_fallback=True,
                    min_model_size_mb=10
                )
                logger.info("âœ… ì „ì—­ ModelLoader ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™”"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info(f"âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    try:
        loader = get_global_model_loader()
        if step_requirements:
            loader.register_step_requirements(step_name, step_requirements)
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(get_global_model_loader(), step_name)

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ í•¨ìˆ˜"""
    return CheckpointValidator.validate_checkpoint_file(checkpoint_path)

def safe_load_checkpoint(checkpoint_path: Union[str, Path], device: str = "cpu") -> Optional[Any]:
    """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        validation = validate_checkpoint_file(checkpoint_path)
        if not validation.is_valid:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {validation.error_message}")
            return None
        
        if TORCH_AVAILABLE:
            try:
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
                    logger.debug(f"âœ… ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    return checkpoint
                except Exception as e:
                    logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    return None
        
        return None
    except Exception as e:
        logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    try:
        if model_loader_instance is None:
            model_loader_instance = get_global_model_loader()
        
        return model_loader_instance.create_step_interface(step_name)
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return StepModelInterface(model_loader_instance or get_global_model_loader(), step_name)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            if hasattr(torch, 'mps'):
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                if hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                return True
        return False
    except Exception:
        return False

def safe_torch_cleanup():
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        gc.collect()
        
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            if MPS_AVAILABLE:
                safe_mps_empty_cache()
        
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def get_enhanced_memory_info() -> Dict[str, Any]:
    """í–¥ìƒëœ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        
        memory_info = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "percent": memory.percent,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                memory_info["gpu"] = {
                    "allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                    "reserved_mb": torch.cuda.memory_reserved() / (1024**2),
                    "max_allocated_mb": torch.cuda.max_memory_allocated() / (1024**2)
                }
            elif MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'current_allocated_memory'):
                        memory_info["mps"] = {
                            "allocated_mb": torch.mps.current_allocated_memory() / (1024**2)
                        }
                except:
                    memory_info["mps"] = {"status": "available"}
        
        return memory_info
        
    except ImportError:
        return {
            "total_gb": 128.0 if IS_M3_MAX else 16.0,
            "available_gb": 100.0 if IS_M3_MAX else 12.0,
            "used_gb": 28.0 if IS_M3_MAX else 4.0,
            "percent": 22.0 if IS_M3_MAX else 25.0,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV
        }

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface',
    'CheckpointValidator',
    'SafeAsyncContextManager',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType', 
    'ModelConfig',
    'SafeModelCacheEntry',
    'CheckpointValidation',
    'LoadingStatus',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'create_step_interface',
    'validate_checkpoint_file',
    'safe_load_checkpoint',
    'get_step_model_interface',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_model',
    'get_model_async',
    
    # ì•ˆì „í•œ í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_torch_cleanup',
    'get_enhanced_memory_info',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ
logger.info("=" * 80)
logger.info("âœ… ModelLoader v21.0 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ì•ˆì •ì„± ê°•í™”")
logger.info("=" * 80)
logger.info("ğŸ”¥ TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… StepModelInterface ê°œì„ ")
logger.info("âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©")
logger.info("âœ… í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬")
logger.info("âœ… M3 Max 128GB ìµœì í™”")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("=" * 80)