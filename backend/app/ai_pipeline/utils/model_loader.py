# app/ai_pipeline/utils/model_loader.py
"""
ğŸ MyCloset AI ì™„ì „ ì¬êµ¬ì„±ëœ ModelLoader ì‹œìŠ¤í…œ v2.0
âœ… M3 Max 128GB ìµœì í™” ì„¤ê³„
âœ… Step í´ë˜ìŠ¤ ì™„ë²½ í˜¸í™˜
âœ… ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ìë™ íƒì§€
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
âœ… ê¹”ë”í•œ ì•„í‚¤í…ì²˜
"""

import os
import gc
import time
import threading
import asyncio
import logging
import hashlib
import json
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import weakref

# ==============================================
# ğŸ”¥ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None

try:
    import cv2
    import numpy as np
    from PIL import Image, ImageDraw, ImageFont
    CV_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í•µì‹¬ íƒ€ì… ì •ì˜
# ==============================================

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pytorch"
    SAFETENSORS = "safetensors"
    DIFFUSERS = "diffusers"
    ONNX = "onnx"
    TRANSFORMERS = "transformers"

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class ModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    name: str
    model_type: ModelType
    model_class: str
    checkpoint_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    priority: ModelPriority = ModelPriority.MEDIUM
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LoadedModel:
    """ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
    model: Any
    config: ModelConfig
    load_time: float
    memory_usage_mb: float
    last_access: float = field(default_factory=time.time)
    access_count: int = 0

# ==============================================
# ğŸ”¥ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ
# ==============================================

class ModelScanner:
    """ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ìŠ¤ìºë„ˆ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.ModelScanner")
        
        # ê²€ìƒ‰ ê²½ë¡œ
        self.search_paths = [
            Path("backend/ai_models"),
            Path("ai_models"),
            Path("models"),
            Path("checkpoints")
        ]
        
        # ëª¨ë¸ íŒ¨í„´ ë§¤í•‘
        self.model_patterns = {
            "human_parsing_graphonomy": {
                "patterns": [r".*schp.*atr.*\.pth$", r".*graphonomy.*\.pth$", r".*human.*parsing.*\.pth$"],
                "model_type": ModelType.HUMAN_PARSING,
                "model_class": "GraphonomyModel",
                "priority": ModelPriority.CRITICAL
            },
            "pose_estimation_openpose": {
                "patterns": [r".*body.*pose.*\.pth$", r".*openpose.*\.pth$", r".*pose.*model.*\.pth$"],
                "model_type": ModelType.POSE_ESTIMATION,
                "model_class": "OpenPoseModel",
                "priority": ModelPriority.HIGH
            },
            "cloth_segmentation_u2net": {
                "patterns": [r".*u2net.*\.pth$", r".*cloth.*seg.*\.pth$", r".*sam.*\.pth$"],
                "model_type": ModelType.CLOTH_SEGMENTATION,
                "model_class": "U2NetModel",
                "priority": ModelPriority.HIGH
            },
            "geometric_matching_gmm": {
                "patterns": [r".*geometric.*\.pth$", r".*gmm.*\.pth$", r".*tps.*\.pth$"],
                "model_type": ModelType.GEOMETRIC_MATCHING,
                "model_class": "GeometricMatchingModel",
                "priority": ModelPriority.MEDIUM
            },
            "virtual_fitting_diffusion": {
                "patterns": [r".*diffusion.*\.bin$", r".*diffusion.*\.safetensors$", r".*stable.*diffusion.*\.safetensors$"],
                "model_type": ModelType.VIRTUAL_FITTING,
                "model_class": "StableDiffusionPipeline",
                "priority": ModelPriority.CRITICAL
            },
            "post_processing_enhancer": {
                "patterns": [r".*esrgan.*\.pth$", r".*enhance.*\.pth$", r".*upscale.*\.pth$"],
                "model_type": ModelType.POST_PROCESSING,
                "model_class": "EnhancementModel",
                "priority": ModelPriority.MEDIUM
            },
            "quality_assessment_clip": {
                "patterns": [r".*clip.*\.bin$", r".*quality.*\.pth$", r".*assessment.*\.pth$"],
                "model_type": ModelType.QUALITY_ASSESSMENT,
                "model_class": "CLIPModel",
                "priority": ModelPriority.MEDIUM
            }
        }
    
    def scan_models(self) -> Dict[str, ModelConfig]:
        """ëª¨ë¸ ìŠ¤ìº” ì‹¤í–‰"""
        self.logger.info("ğŸ” ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì‹œì‘...")
        
        found_models = {}
        
        for search_path in self.search_paths:
            if not search_path.exists():
                continue
                
            self.logger.debug(f"ğŸ“ ìŠ¤ìº” ì¤‘: {search_path}")
            
            # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰
            for file_path in search_path.rglob("*"):
                if not file_path.is_file():
                    continue
                
                # íŒŒì¼ í¬ê¸° ì²´í¬ (1MB ì´ìƒ)
                if file_path.stat().st_size < 1024 * 1024:
                    continue
                
                # ëª¨ë¸ ì‹ë³„
                model_config = self._identify_model(file_path)
                if model_config:
                    # ì¤‘ë³µ ì²´í¬ (ë” ì¢‹ì€ ëª¨ë¸ë¡œ êµì²´)
                    if (model_config.name not in found_models or 
                        self._is_better_model(model_config, found_models[model_config.name])):
                        found_models[model_config.name] = model_config
                        
                        file_size_mb = file_path.stat().st_size / (1024 * 1024)
                        self.logger.info(f"âœ… ë°œê²¬: {model_config.name} ({file_size_mb:.1f}MB)")
        
        self.logger.info(f"ğŸ“Š ìŠ¤ìº” ì™„ë£Œ: {len(found_models)}ê°œ ëª¨ë¸ ë°œê²¬")
        return found_models
    
    def _identify_model(self, file_path: Path) -> Optional[ModelConfig]:
        """íŒŒì¼ì„ í†µí•´ ëª¨ë¸ ì‹ë³„"""
        import re
        
        file_str = str(file_path).lower()
        
        for model_name, config in self.model_patterns.items():
            for pattern in config["patterns"]:
                if re.search(pattern, file_str, re.IGNORECASE):
                    return ModelConfig(
                        name=model_name,
                        model_type=config["model_type"],
                        model_class=config["model_class"],
                        checkpoint_path=str(file_path),
                        priority=config["priority"],
                        metadata={
                            "file_size_mb": file_path.stat().st_size / (1024 * 1024),
                            "last_modified": file_path.stat().st_mtime,
                            "auto_detected": True
                        }
                    )
        
        return None
    
    def _is_better_model(self, new_config: ModelConfig, existing_config: ModelConfig) -> bool:
        """ìƒˆ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨"""
        # ìš°ì„ ìˆœìœ„ ë¹„êµ
        if new_config.priority.value < existing_config.priority.value:
            return True
        elif new_config.priority.value > existing_config.priority.value:
            return False
        
        # íŒŒì¼ í¬ê¸° ë¹„êµ (ë” í° ê²ƒì´ ë³´í†µ ë” ì¢‹ìŒ)
        new_size = new_config.metadata.get("file_size_mb", 0)
        existing_size = existing_config.metadata.get("file_size_mb", 0)
        
        return new_size > existing_size

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
# ==============================================

class MemoryManager:
    """M3 Max ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: str = "mps", memory_limit_gb: float = 128.0):
        self.device = device
        self.memory_limit_gb = memory_limit_gb
        self.logger = logging.getLogger(f"{__name__}.MemoryManager")
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = self._detect_m3_max()
        if self.is_m3_max:
            self._setup_m3_max_optimization()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            if platform.system() == 'Darwin' and self.memory_limit_gb >= 64:
                return True
        except:
            pass
        return False
    
    def _setup_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì„¤ì •"""
        try:
            # PyTorch MPS ìµœì í™”
            if TORCH_AVAILABLE and torch.backends.mps.is_available():
                os.environ.update({
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                    'OMP_NUM_THREADS': '16',
                    'MKL_NUM_THREADS': '16'
                })
                self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def get_available_memory_gb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ë°˜í™˜"""
        try:
            if self.device == "mps":
                import psutil
                memory = psutil.virtual_memory()
                return memory.available / (1024**3)
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                total = torch.cuda.get_device_properties(0).total_memory
                allocated = torch.cuda.memory_allocated()
                return (total - allocated) / (1024**3)
            else:
                import psutil
                memory = psutil.virtual_memory()
                return memory.available / (1024**3)
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.memory_limit_gb * 0.5
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif self.device == "mps" and TORCH_AVAILABLE and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            
            self.logger.debug("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def estimate_model_memory(self, file_size_mb: float) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        # ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ íŒŒì¼ í¬ê¸°ì˜ 1.5~2ë°° ë©”ëª¨ë¦¬ ì‚¬ìš©
        base_memory = file_size_mb / 1024  # GB ë³€í™˜
        
        if self.device == "mps":
            # M3 MaxëŠ” í†µí•© ë©”ëª¨ë¦¬ë¡œ íš¨ìœ¨ì 
            return base_memory * 1.3
        elif self.device == "cuda":
            return base_memory * 1.8
        else:
            return base_memory * 2.0

# ==============================================
# ğŸ”¥ ê°„ë‹¨í•œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class BaseModel(nn.Module):
    """ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self, num_classes: int = 20, input_size: Tuple[int, int] = (512, 512)):
        super().__init__()
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required")
        
        self.num_classes = num_classes
        self.input_size = input_size
        
        # ê°„ë‹¨í•œ CNN êµ¬ì¡°
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(True),
            nn.MaxPool2d(3, 2, 1),
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(True),
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.classifier = nn.Linear(256, num_classes)
    
    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        return self.classifier(features)

# íŠ¹í™” ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (BaseModel ìƒì†)
class GraphonomyModel(BaseModel):
    """ì¸ê°„ íŒŒì‹± ëª¨ë¸"""
    def __init__(self, num_classes: int = 20, **kwargs):
        super().__init__(num_classes, **kwargs)

class OpenPoseModel(BaseModel):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    def __init__(self, num_classes: int = 18, **kwargs):
        super().__init__(num_classes, **kwargs)

class U2NetModel(BaseModel):
    """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
    def __init__(self, num_classes: int = 1, **kwargs):
        super().__init__(num_classes, **kwargs)

class GeometricMatchingModel(BaseModel):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸"""
    def __init__(self, num_classes: int = 3, **kwargs):
        super().__init__(num_classes, **kwargs)

class EnhancementModel(BaseModel):
    """í›„ì²˜ë¦¬ ëª¨ë¸"""
    def __init__(self, num_classes: int = 3, **kwargs):
        super().__init__(num_classes, **kwargs)

class CLIPModel(BaseModel):
    """CLIP ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ëª¨ë¸"""
    def __init__(self, num_classes: int = 512, **kwargs):
        super().__init__(num_classes, **kwargs)

# ==============================================
# ğŸ”¥ ëª¨ë¸ íŒ©í† ë¦¬
# ==============================================

class ModelFactory:
    """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± íŒ©í† ë¦¬"""
    
    MODEL_CLASSES = {
        "GraphonomyModel": GraphonomyModel,
        "OpenPoseModel": OpenPoseModel,
        "U2NetModel": U2NetModel,
        "GeometricMatchingModel": GeometricMatchingModel,
        "EnhancementModel": EnhancementModel,
        "CLIPModel": CLIPModel,
        "StableDiffusionPipeline": None  # íŠ¹ë³„ ì²˜ë¦¬
    }
    
    @classmethod
    def create_model(cls, config: ModelConfig) -> Optional[Any]:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            model_class_name = config.model_class
            
            if model_class_name == "StableDiffusionPipeline":
                return cls._create_diffusion_model(config)
            
            model_class = cls.MODEL_CLASSES.get(model_class_name)
            if not model_class:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class_name}")
                return None
            
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            kwargs = {
                "num_classes": config.num_classes or 20,
                "input_size": config.input_size
            }
            
            return model_class(**kwargs)
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    @classmethod
    def _create_diffusion_model(cls, config: ModelConfig) -> Optional[Any]:
        """Diffusion ëª¨ë¸ ìƒì„±"""
        try:
            if not DIFFUSERS_AVAILABLE:
                logger.error("diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”")
                return None
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
            # í˜„ì¬ëŠ” ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜
            return BaseModel(num_classes=3, input_size=config.input_size)
            
        except Exception as e:
            logger.error(f"Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤
# ==============================================

class StepModelInterface:
    """Step í´ë˜ìŠ¤ë¥¼ ìœ„í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"{__name__}.StepInterface.{step_name}")
        
        # Stepë³„ ê¶Œì¥ ëª¨ë¸ ë§¤í•‘
        self.recommended_models = {
            'HumanParsingStep': 'human_parsing_graphonomy',
            'PoseEstimationStep': 'pose_estimation_openpose',
            'ClothSegmentationStep': 'cloth_segmentation_u2net',
            'GeometricMatchingStep': 'geometric_matching_gmm',
            'ClothWarpingStep': 'virtual_fitting_diffusion',
            'VirtualFittingStep': 'virtual_fitting_diffusion',
            'PostProcessingStep': 'post_processing_enhancer',
            'QualityAssessmentStep': 'quality_assessment_clip'
        }
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if model_name is None:
                model_name = self.recommended_models.get(self.step_name)
                if not model_name:
                    self.logger.error(f"ê¶Œì¥ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {self.step_name}")
                    return None
            
            model = await self.model_loader.load_model(model_name)
            if model:
                self.logger.info(f"âœ… {self.step_name}ì— {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ {self.step_name}ì—ì„œ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup(self):
        """ì •ë¦¬"""
        self.logger.debug(f"ğŸ§¹ {self.step_name} ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬")

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤
# ==============================================

class ModelLoader:
    """
    ğŸ ì™„ì „ ì¬êµ¬ì„±ëœ ModelLoader v2.0
    âœ… M3 Max 128GB ìµœì í™”
    âœ… ê¹”ë”í•œ ì•„í‚¤í…ì²˜
    âœ… Step í´ë˜ìŠ¤ ì™„ë²½ í˜¸í™˜
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        memory_limit_gb: float = 128.0,
        auto_scan: bool = True,
        **kwargs
    ):
        """ModelLoader ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(f"{__name__}.ModelLoader")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._detect_device(device)
        self.memory_limit_gb = memory_limit_gb
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.memory_manager = MemoryManager(self.device, memory_limit_gb)
        self.model_scanner = ModelScanner()
        
        # ìƒíƒœ ê´€ë¦¬
        self.model_configs: Dict[str, ModelConfig] = {}
        self.loaded_models: Dict[str, LoadedModel] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._is_initialized = False
        
        # Step í´ë˜ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
        self.step_name = self.__class__.__name__
        
        self.logger.info(f"ğŸ¯ ModelLoader v2.0 ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ìë™ ìŠ¤ìº”
        if auto_scan:
            asyncio.create_task(self._initialize_async())
    
    def _detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device
        
        if not TORCH_AVAILABLE:
            return "cpu"
        
        try:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except:
            return "cpu"
    
    async def _initialize_async(self):
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            if self._is_initialized:
                return
            
            self.logger.info("ğŸ” ìë™ ëª¨ë¸ ìŠ¤ìº” ì‹œì‘...")
            
            # ëª¨ë¸ ìŠ¤ìº” ì‹¤í–‰
            scanned_models = self.model_scanner.scan_models()
            
            # ëª¨ë¸ ë“±ë¡
            for name, config in scanned_models.items():
                self.register_model(name, config)
            
            self._is_initialized = True
            self.logger.info(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ: {len(self.model_configs)}ê°œ ëª¨ë¸ ë“±ë¡")
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def register_model(self, name: str, config: ModelConfig) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            with self._lock:
                if config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                self.logger.debug(f"ğŸ“ ëª¨ë¸ ë“±ë¡: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    async def load_model(self, name: str, force_reload: bool = False) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if name in self.loaded_models and not force_reload:
                    loaded = self.loaded_models[name]
                    loaded.last_access = time.time()
                    loaded.access_count += 1
                    self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {name}")
                    return loaded.model
                
                # ì„¤ì • í™•ì¸
                if name not in self.model_configs:
                    self.logger.error(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
                    return None
                
                config = self.model_configs[name]
                start_time = time.time()
                
                self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì‹œì‘: {name}")
                
                # ë©”ëª¨ë¦¬ í™•ì¸
                await self._ensure_memory_available(config)
                
                # ëª¨ë¸ ìƒì„±
                model = ModelFactory.create_model(config)
                if not model:
                    return None
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                if config.checkpoint_path:
                    await self._load_checkpoint(model, config)
                
                # ë””ë°”ì´ìŠ¤ ì´ë™
                if hasattr(model, 'to'):
                    model = model.to(self.device)
                
                # FP16 ìµœì í™”
                if config.precision == "fp16" and hasattr(model, 'half') and self.device != 'cpu':
                    try:
                        model = model.half()
                    except:
                        pass
                
                # í‰ê°€ ëª¨ë“œ
                if hasattr(model, 'eval'):
                    model.eval()
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                file_size_mb = config.metadata.get("file_size_mb", 100)
                memory_usage_mb = self.memory_manager.estimate_model_memory(file_size_mb) * 1024
                
                # ë¡œë“œëœ ëª¨ë¸ ë“±ë¡
                loaded_model = LoadedModel(
                    model=model,
                    config=config,
                    load_time=time.time() - start_time,
                    memory_usage_mb=memory_usage_mb
                )
                
                self.loaded_models[name] = loaded_model
                
                self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {name} ({loaded_model.load_time:.2f}s)")
                return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
            return None
    
    async def _ensure_memory_available(self, config: ModelConfig):
        """ë©”ëª¨ë¦¬ í™•ë³´"""
        try:
            file_size_mb = config.metadata.get("file_size_mb", 100)
            required_memory_gb = self.memory_manager.estimate_model_memory(file_size_mb)
            available_memory_gb = self.memory_manager.get_available_memory_gb()
            
            if available_memory_gb < required_memory_gb:
                self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ë¶€ì¡±, ì •ë¦¬ ì‹¤í–‰ (í•„ìš”: {required_memory_gb:.1f}GB, ì‚¬ìš©ê°€ëŠ¥: {available_memory_gb:.1f}GB)")
                await self._cleanup_least_used_models()
                self.memory_manager.cleanup_memory()
                
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨: {e}")
    
    async def _load_checkpoint(self, model: Any, config: ModelConfig):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            if not config.checkpoint_path or not Path(config.checkpoint_path).exists():
                self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {config.checkpoint_path}")
                return
            
            checkpoint_path = Path(config.checkpoint_path)
            file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
            
            self.logger.info(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path.name} ({file_size_mb:.1f}MB)")
            
            if hasattr(model, 'load_state_dict'):
                # PyTorch ëª¨ë¸
                try:
                    if checkpoint_path.suffix == '.safetensors':
                        try:
                            from safetensors.torch import load_file
                            state_dict = load_file(checkpoint_path)
                        except ImportError:
                            state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                    else:
                        state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                    
                    # state_dict ì •ë¦¬
                    if isinstance(state_dict, dict):
                        if 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                        elif 'model' in state_dict:
                            state_dict = state_dict['model']
                    
                    # í‚¤ ì •ë¦¬
                    cleaned_state_dict = {}
                    for key, value in state_dict.items():
                        new_key = key.replace('module.', '') if key.startswith('module.') else key
                        cleaned_state_dict[new_key] = value
                    
                    # ë¡œë“œ (strict=False)
                    missing_keys, unexpected_keys = model.load_state_dict(cleaned_state_dict, strict=False)
                    
                    if missing_keys:
                        self.logger.debug(f"ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                    if unexpected_keys:
                        self.logger.debug(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                    
                    self.logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e} (ë¹ˆ ê°€ì¤‘ì¹˜ë¡œ ê³„ì†)")
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _cleanup_least_used_models(self, keep_count: int = 3):
        """ì‚¬ìš©ëŸ‰ì´ ì ì€ ëª¨ë¸ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self.loaded_models) <= keep_count:
                    return
                
                # ì•¡ì„¸ìŠ¤ ë¹ˆë„ì™€ ì‹œê°„ìœ¼ë¡œ ì •ë ¬
                sorted_models = sorted(
                    self.loaded_models.items(),
                    key=lambda x: (x[1].access_count, x[1].last_access)
                )
                
                cleanup_count = len(self.loaded_models) - keep_count
                
                for i in range(cleanup_count):
                    name, loaded_model = sorted_models[i]
                    
                    # ëª¨ë¸ ì •ë¦¬
                    if hasattr(loaded_model.model, 'cpu'):
                        loaded_model.model.cpu()
                    
                    del self.loaded_models[name]
                    del loaded_model
                    
                    self.logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {name}")
                
                self.memory_manager.cleanup_memory()
                
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_step_interface(self, step_name: str) -> StepModelInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            if step_name not in self.step_interfaces:
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                self.logger.debug(f"ğŸ”— Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name}")
            
            return self.step_interfaces[step_name]
            
        except Exception as e:
            self.logger.error(f"Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(self, step_name)
    
    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._lock:
            return list(self.model_configs.keys())
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            if name not in self.model_configs:
                return None
            
            config = self.model_configs[name]
            loaded = self.loaded_models.get(name)
            
            info = {
                "name": name,
                "model_type": config.model_type.value,
                "model_class": config.model_class,
                "device": config.device,
                "loaded": loaded is not None,
                "checkpoint_path": config.checkpoint_path,
                "priority": config.priority.name,
                "metadata": config.metadata
            }
            
            if loaded:
                info.update({
                    "load_time": loaded.load_time,
                    "memory_usage_mb": loaded.memory_usage_mb,
                    "last_access": loaded.last_access,
                    "access_count": loaded.access_count
                })
            
            return info
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´"""
        return {
            "device": self.device,
            "memory_limit_gb": self.memory_limit_gb,
            "available_memory_gb": self.memory_manager.get_available_memory_gb(),
            "is_m3_max": self.memory_manager.is_m3_max,
            "registered_models": len(self.model_configs),
            "loaded_models": len(self.loaded_models),
            "torch_available": TORCH_AVAILABLE,
            "mps_available": TORCH_AVAILABLE and torch.backends.mps.is_available() if TORCH_AVAILABLE else False
        }
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” (Step í´ë˜ìŠ¤ í˜¸í™˜ìš©)"""
        try:
            if not self._is_initialized:
                await self._initialize_async()
            
            available_models = len([name for name, config in self.model_configs.items() 
                                  if config.checkpoint_path and Path(config.checkpoint_path).exists()])
            
            self.logger.info(f"âœ… ModelLoader ì¤€ë¹„ ì™„ë£Œ - {available_models}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
            return True
            
        except Exception as e:
            self.logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            for interface in self.step_interfaces.values():
                interface.cleanup()
            self.step_interfaces.clear()
            
            # ë¡œë“œëœ ëª¨ë¸ ì •ë¦¬
            with self._lock:
                for name, loaded_model in self.loaded_models.items():
                    if hasattr(loaded_model.model, 'cpu'):
                        loaded_model.model.cpu()
                    del loaded_model
                
                self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            self.logger.info("âœ… ModelLoader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ Step í´ë˜ìŠ¤ ì—°ë™ ë¯¹ìŠ¤ì¸
# ==============================================

class BaseStepMixin:
    """Step í´ë˜ìŠ¤ë“¤ì´ ìƒì†ë°›ì„ ModelLoader ì—°ë™ ë¯¹ìŠ¤ì¸"""
    
    def _setup_model_interface(self, model_loader: Optional[ModelLoader] = None):
        """ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if model_loader is None:
                model_loader = get_global_model_loader()
            
            self.model_interface = model_loader.create_step_interface(
                self.__class__.__name__
            )
            
            logger.info(f"ğŸ”— {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (Stepì—ì„œ ì‚¬ìš©)"""
        try:
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            return await self.model_interface.get_model(model_name)
                
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.cleanup()
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ëª¨ë¸ ë¡œë” ê´€ë¦¬
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader() -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader()
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> Dict[str, Any]:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™”"""
    global _global_model_loader
    
    try:
        with _loader_lock:
            if _global_model_loader is not None:
                _global_model_loader.cleanup()
            
            _global_model_loader = ModelLoader(**kwargs)
            
            # ì´ˆê¸°í™” ì‹¤í–‰
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            if not loop.is_running():
                result = loop.run_until_complete(_global_model_loader.initialize())
            else:
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                result = True
            
            system_info = _global_model_loader.get_system_info()
            
            return {
                "success": result,
                "system_info": system_info,
                "message": f"ModelLoader v2.0 ì´ˆê¸°í™” ì™„ë£Œ - {system_info['registered_models']}ê°œ ëª¨ë¸"
            }
            
    except Exception as e:
        logger.error(f"ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def cleanup_global_loader():
    """ì „ì—­ ë¡œë” ì •ë¦¬"""
    global _global_model_loader
    
    try:
        with _loader_lock:
            if _global_model_loader:
                _global_model_loader.cleanup()
                _global_model_loader = None
        
        logger.info("âœ… ì „ì—­ ModelLoader ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì „ì—­ ë¡œë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_model_loader(device: str = "mps", **kwargs) -> ModelLoader:
    """ModelLoader ìƒì„±"""
    return ModelLoader(device=device, **kwargs)

async def load_model_async(model_name: str) -> Optional[Any]:
    """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        return await loader.load_model(model_name)
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_model_sync(model_name: str) -> Optional[Any]:
    """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
    try:
        loader = get_global_model_loader()
        
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(loader.load_model(model_name))
    except Exception as e:
        logger.error(f"ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def preprocess_image(
    image: Union[np.ndarray, Image.Image, str, Path], 
    target_size: Tuple[int, int] = (512, 512), 
    normalize: bool = True,
    device: str = "cpu"
) -> torch.Tensor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    try:
        if not (TORCH_AVAILABLE and CV_AVAILABLE):
            raise ImportError("PyTorchì™€ OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, (str, Path)):
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            if image.ndim == 3 and image.shape[2] == 3:
                image = Image.fromarray(image.astype(np.uint8))
            else:
                image = Image.fromarray(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image)}")
        
        # í¬ê¸° ì¡°ì •
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        image_array = np.array(image).astype(np.float32) / 255.0
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0)
        
        # ì •ê·œí™”
        if normalize:
            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
            image_tensor = (image_tensor - mean) / std
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if device != "cpu":
            image_tensor = image_tensor.to(device)
        
        return image_tensor
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_segmentation(
    output: torch.Tensor, 
    original_size: Tuple[int, int], 
    threshold: float = 0.5
) -> np.ndarray:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬"""
    try:
        if not CV_AVAILABLE:
            raise ImportError("OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ì°¨ì› ì •ë¦¬
        if output.dim() == 4:
            output = output.squeeze(0)
        
        if output.dim() == 3:
            if output.shape[0] > 1:
                output = torch.argmax(output, dim=0)
            else:
                output = output.squeeze(0)
        
        # CPUë¡œ ì´ë™ ë° ì´ì§„í™”
        output = output.cpu().numpy()
        if output.dtype in [np.float32, np.float64]:
            output = (output > threshold).astype(np.uint8)
        else:
            output = output.astype(np.uint8)
        
        # í¬ê¸° ì¡°ì •
        if output.shape != original_size[::-1]:
            output = cv2.resize(output, original_size, interpolation=cv2.INTER_NEAREST)
        
        return output
        
    except Exception as e:
        logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def postprocess_pose(
    output: Union[torch.Tensor, List, Tuple], 
    original_size: Tuple[int, int],
    confidence_threshold: float = 0.3
) -> Dict[str, Any]:
    """í¬ì¦ˆ ì¶”ì • í›„ì²˜ë¦¬"""
    try:
        keypoints = []
        
        # ì¶œë ¥ í˜•ì‹ ì²˜ë¦¬
        if isinstance(output, (list, tuple)):
            # OpenPose ìŠ¤íƒ€ì¼: (PAFs, heatmaps) ë¦¬ìŠ¤íŠ¸
            heatmaps = output[-1][1] if len(output[-1]) > 1 else output[-1]
        else:
            heatmaps = output
        
        # í…ì„œ ì²˜ë¦¬
        if torch.is_tensor(heatmaps):
            if heatmaps.dim() == 4:
                heatmaps = heatmaps.squeeze(0)
            heatmaps = heatmaps.cpu().numpy()
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        if heatmaps.ndim == 3:
            for i in range(min(18, heatmaps.shape[0] - 1)):  # ë°°ê²½ ì œì™¸
                heatmap = heatmaps[i]
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                confidence = heatmap[y, x]
                
                if confidence > confidence_threshold:
                    # ì›ë³¸ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                    x_scaled = int(x * original_size[0] / heatmap.shape[1])
                    y_scaled = int(y * original_size[1] / heatmap.shape[0])
                    keypoints.append([x_scaled, y_scaled, float(confidence)])
                else:
                    keypoints.append([0, 0, 0])
        
        return {
            "keypoints": keypoints,
            "num_keypoints": len([kp for kp in keypoints if kp[2] > confidence_threshold]),
            "confidence_avg": np.mean([kp[2] for kp in keypoints if kp[2] > 0]) if keypoints else 0
        }
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'ModelConfig',
    'ModelType',
    'ModelFormat',
    'ModelPriority',
    'LoadedModel',
    'StepModelInterface',
    'BaseStepMixin',
    
    # ì»´í¬ë„ŒíŠ¸ í´ë˜ìŠ¤ë“¤
    'MemoryManager',
    'ModelScanner',
    'ModelFactory',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'BaseModel',
    'GraphonomyModel',
    'OpenPoseModel',
    'U2NetModel',
    'GeometricMatchingModel',
    'EnhancementModel',
    'CLIPModel',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_model_loader',
    'get_global_model_loader',
    'initialize_global_model_loader',
    'cleanup_global_loader',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'load_model_async',
    'load_model_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'preprocess_image',
    'postprocess_segmentation',
    'postprocess_pose'
]

# ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit
atexit.register(cleanup_global_loader)

logger.info("âœ… ModelLoader v2.0 ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ - ì™„ì „ ì¬êµ¬ì„±")