#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì™„ì „ ê°œì„ ëœ ModelLoader v22.0
================================================================
âœ… 2ë²ˆ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ 1ë²ˆ íŒŒì¼ê³¼ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ ì°¸ì¡°í•˜ì—¬ ì™„ì „ ê°œì„ 
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + BaseStepMixin 100% í˜¸í™˜
âœ… ì‹¤ì œ 229GB AI ëª¨ë¸ íŒŒì¼ 100% í™œìš©
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ mycloset-ai-clean ì™„ì „ ì§€ì›
âœ… í¬ê¸° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ (50MB ì´ìƒ ìš°ì„ )

Author: MyCloset AI Team
Date: 2025-07-25
Version: 22.0 (Complete Improvement)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager, contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (conda í™˜ê²½ ìµœì í™”)
# ==============================================

logger = logging.getLogger(__name__)

# conda í™˜ê²½ ê°ì§€ ë° ìµœì í™”
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
IS_MYCLOSET_ENV = 'mycloset' in CONDA_ENV.lower()

# PyTorch ì•ˆì „ import (M3 Max ìµœì í™”)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
NUMPY_AVAILABLE = False
DEFAULT_DEVICE = "cpu"
IS_M3_MAX = False

if IS_MYCLOSET_ENV:
    logger.info(f"ğŸ MyCloset conda í™˜ê²½ ê°ì§€: {CONDA_ENV}")
    # conda í™˜ê²½ ìµœì í™” ì„¤ì •
    os.environ.update({
        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
        'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
        'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1',
        'OMP_NUM_THREADS': '8',  # M3 Max 8ì½”ì–´ ìµœì í™”
        'MKL_NUM_THREADS': '8'
    })

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):
        if torch.backends.mps.is_available():
            MPS_AVAILABLE = True
            DEFAULT_DEVICE = "mps"
            
            # M3 Max ê°ì§€
            try:
                import platform
                import subprocess
                if platform.system() == 'Darwin':
                    result = subprocess.run(
                        ['sysctl', '-n', 'machdep.cpu.brand_string'],
                        capture_output=True, text=True, timeout=5
                    )
                    IS_M3_MAX = 'M3' in result.stdout and 'Max' in result.stdout
                    if IS_M3_MAX:
                        logger.info("ğŸ M3 Max ê°ì§€ë¨ - 128GB í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
            except:
                pass
                
    elif torch.cuda.is_available():
        DEFAULT_DEVICE = "cuda"
        
except ImportError:
    torch = None
    logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class LoadingStatus(Enum):
    """ë¡œë”© ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"
    UNLOADING = "unloading"

class ModelFormat(Enum):
    """ëª¨ë¸ í¬ë§·"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    TENSORFLOW = "bin"
    ONNX = "onnx"
    PICKLE = "pkl"
    CHECKPOINT = "ckpt"

class ModelType(Enum):
    """AI ëª¨ë¸ íƒ€ì… (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"
    UNKNOWN = "unknown"

@dataclass
class CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    file_exists: bool
    size_mb: float
    checksum: Optional[str] = None
    error_message: Optional[str] = None
    validation_time: float = 0.0
    pytorch_loadable: bool = False
    parameter_count: int = 0

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´ (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    file_size_mb: float = 0.0
    priority_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation: Optional[CheckpointValidation] = None
    loading_status: LoadingStatus = LoadingStatus.NOT_LOADED
    last_validated: float = 0.0

@dataclass
class SafeModelCacheEntry:
    """ì•ˆì „í•œ ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    load_time: float
    last_access: float
    access_count: int
    memory_usage_mb: float
    device: str
    step_name: Optional[str] = None
    validation: Optional[CheckpointValidation] = None
    is_healthy: bool = True
    error_count: int = 0

# ==============================================
# ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸° (M3 Max ìµœì í™”)
# ==============================================

class CheckpointValidator:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ê¸° (M3 Max + conda ìµœì í™”)"""
    
    @staticmethod
    def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ì „ ê²€ì¦"""
        start_time = time.time()
        checkpoint_path = Path(checkpoint_path)
        
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not checkpoint_path.exists():
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=False,
                    size_mb=0.0,
                    error_message=f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {checkpoint_path}",
                    validation_time=time.time() - start_time
                )
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            size_bytes = checkpoint_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            if size_bytes == 0:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=0.0,
                    error_message="íŒŒì¼ í¬ê¸°ê°€ 0ë°”ì´íŠ¸",
                    validation_time=time.time() - start_time
                )
            
            # ìµœì†Œ í¬ê¸° í™•ì¸ (50MB ì´ìƒ ìš°ì„ ìˆœìœ„)
            if size_mb < 50:
                return CheckpointValidation(
                    is_valid=False,
                    file_exists=True,
                    size_mb=size_mb,
                    error_message=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {size_mb:.1f}MB (50MB ë¯¸ë§Œ)",
                    validation_time=time.time() - start_time
                )
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            pytorch_valid = False
            parameter_count = 0
            
            if TORCH_AVAILABLE:
                pytorch_valid, parameter_count = CheckpointValidator._validate_pytorch_checkpoint(checkpoint_path)
            
            # ì²´í¬ì„¬ ê³„ì‚° (1GB ë¯¸ë§Œì¸ ê²½ìš°ë§Œ)
            checksum = None
            if size_mb < 1000:
                try:
                    checksum = CheckpointValidator._calculate_checksum(checkpoint_path)
                except:
                    pass
            
            return CheckpointValidation(
                is_valid=True,
                file_exists=True,
                size_mb=size_mb,
                checksum=checksum,
                validation_time=time.time() - start_time,
                pytorch_loadable=pytorch_valid,
                parameter_count=parameter_count
            )
            
        except Exception as e:
            return CheckpointValidation(
                is_valid=False,
                file_exists=checkpoint_path.exists(),
                size_mb=0.0,
                error_message=f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                validation_time=time.time() - start_time
            )
    
    @staticmethod
    def _validate_pytorch_checkpoint(checkpoint_path: Path) -> Tuple[bool, int]:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (M3 Max ìµœì í™”)"""
        try:
            import torch
            
            # M3 Max MPS ì•ˆì „ ë¡œë”©
            device_map = "cpu"  # ê²€ì¦ ì‹œì—ëŠ” CPU ì‚¬ìš©
            
            try:
                # ì•ˆì „í•œ ë¡œë”© ì‹œë„ (weights_only=True)
                checkpoint = torch.load(checkpoint_path, map_location=device_map, weights_only=True)
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {checkpoint_path.name}")
            except Exception:
                try:
                    # ì¼ë°˜ ë¡œë”© ì‹œë„ (weights_only=False)
                    checkpoint = torch.load(checkpoint_path, map_location=device_map, weights_only=False)
                    logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {checkpoint_path.name}")
                except Exception as load_error:
                    logger.warning(f"âš ï¸ PyTorch ë¡œë”© ì‹¤íŒ¨: {checkpoint_path.name} - {load_error}")
                    return False, 0
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            parameter_count = 0
            if isinstance(checkpoint, dict):
                for param in checkpoint.values():
                    if hasattr(param, 'numel'):
                        parameter_count += param.numel()
            elif hasattr(checkpoint, 'parameters'):
                parameter_count = sum(p.numel() for p in checkpoint.parameters())
            
            return True, parameter_count
            
        except ImportError:
            return False, 0
        except Exception as e:
            logger.warning(f"âš ï¸ PyTorch ê²€ì¦ ì‹¤íŒ¨: {checkpoint_path.name} - {e}")
            return False, 0
    
    @staticmethod
    def _calculate_checksum(file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

# ==============================================
# ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ (BaseStepMixin ì™„ì „ í˜¸í™˜)
# ==============================================

class StepModelInterface:
    """BaseStepMixin ì™„ì „ í˜¸í™˜ Step ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ ë° ìƒíƒœ
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, SafeModelCacheEntry] = {}
        self.model_status: Dict[str, LoadingStatus] = {}
        self._lock = threading.RLock()
        
        # Step ìš”êµ¬ì‚¬í•­
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.creation_time = time.time()
        self.error_count = 0
        self.last_error = None
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (BaseStepMixin í˜¸í™˜)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name}")
                
                # ModelLoaderì˜ register_model_requirement í˜¸ì¶œ
                if hasattr(self.model_loader, 'register_model_requirement'):
                    success = self.model_loader.register_model_requirement(
                        model_name=model_name,
                        model_type=model_type,
                        step_name=self.step_name,
                        **kwargs
                    )
                    if success:
                        self.step_requirements[model_name] = {
                            "model_name": model_name,
                            "model_type": model_type,
                            "step_name": self.step_name,
                            "registered_at": time.time(),
                            **kwargs
                        }
                        self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                        return True
                
                # ë¡œì»¬ ë“±ë¡ (í´ë°±)
                self.step_requirements[model_name] = {
                    "model_name": model_name,
                    "model_type": model_type,
                    "step_name": self.step_name,
                    "registered_at": time.time(),
                    **kwargs
                }
                self.logger.info(f"âœ… ë¡œì»¬ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name}")
                return True
               
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ (BaseStepMixin í˜¸í™˜)"""
        try:
            if not model_name:
                model_name = "default_model"
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.is_healthy:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        self.logger.debug(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return cache_entry.model
                    else:
                        del self.model_cache[model_name]
            
            # ë¡œë”© ìƒíƒœ ì„¤ì •
            self.model_status[model_name] = LoadingStatus.LOADING
            
            # ModelLoaderë¥¼ í†µí•œ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = await self._safe_load_checkpoint(model_name)
            
            if checkpoint:
                # ì•ˆì „í•œ ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                cache_entry = SafeModelCacheEntry(
                    model=checkpoint,
                    load_time=time.time(),
                    last_access=time.time(),
                    access_count=1,
                    memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                    device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                    step_name=self.step_name,
                    is_healthy=True,
                    error_count=0
                )
                
                with self._lock:
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = LoadingStatus.LOADED
                
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return checkpoint
            
            self.model_status[model_name] = LoadingStatus.ERROR
            return None
            
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.model_status[model_name] = LoadingStatus.ERROR
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ (BaseStepMixin í˜¸í™˜)"""
        try:
            if not model_name:
                model_name = "default_model"
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self.model_cache:
                    cache_entry = self.model_cache[model_name]
                    if cache_entry.is_healthy:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        return cache_entry.model
                    else:
                        del self.model_cache[model_name]
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(model_name)
            
            if checkpoint:
                with self._lock:
                    cache_entry = SafeModelCacheEntry(
                        model=checkpoint,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                        device=getattr(checkpoint, 'device', DEFAULT_DEVICE) if hasattr(checkpoint, 'device') else DEFAULT_DEVICE,
                        step_name=self.step_name,
                        is_healthy=True,
                        error_count=0
                    )
                    
                    self.model_cache[model_name] = cache_entry
                    self.loaded_models[model_name] = checkpoint
                    self.model_status[model_name] = LoadingStatus.LOADED
                return checkpoint
            
            self.model_status[model_name] = LoadingStatus.ERROR
            return None
            
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def _safe_load_checkpoint(self, model_name: str) -> Optional[Any]:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if hasattr(self.model_loader, 'load_model_async'):
                return await self.model_loader.load_model_async(model_name)
            elif hasattr(self.model_loader, 'load_model'):
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None, 
                    self.model_loader.load_model, 
                    model_name
                )
            else:
                self.logger.error(f"âŒ ModelLoaderì— ë¡œë”© ë©”ì„œë“œ ì—†ìŒ")
                return None
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ í¬ê¸° ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)
                elif hasattr(checkpoint, 'parameters'):
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0

# ==============================================
# ğŸ”¥ ë©”ì¸ ModelLoader í´ë˜ìŠ¤ (ì™„ì „ ê°œì„ )
# ==============================================

class ModelLoader:
    """ì™„ì „ ê°œì„ ëœ ModelLoader v22.0 - ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ 229GB ëª¨ë¸ í™œìš©"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ì™„ì „ ê°œì„ ëœ ìƒì„±ì"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
        self.device = self._resolve_device(device or "auto")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.memory_gb = self._get_memory_info()
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.is_mycloset_env = IS_MYCLOSET_ENV
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)
        self.model_cache_dir = self._resolve_model_cache_dir(kwargs.get('model_cache_dir'))
        
        # ì„±ëŠ¥ ì„¤ì • (M3 Max + conda ìµœì í™”)
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 50 if self.is_m3_max else 20)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        self.min_model_size_mb = kwargs.get('min_model_size_mb', 50)  # 50MB ìš°ì„ ìˆœìœ„
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # í•µì‹¬ ë°ì´í„° êµ¬ì¡°
        self.loaded_models: Dict[str, Any] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        self.model_cache: Dict[str, SafeModelCacheEntry] = {}
        self.available_models: Dict[str, Any] = {}
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # ì„±ëŠ¥ ì¶”ì 
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'validation_count': 0,
            'validation_success': 0,
            'checkpoint_loads': 0,
            'large_models_loaded': 0  # 50MB ì´ìƒ ëª¨ë¸ ìˆ˜
        }
        
        # ë™ê¸°í™” ë° ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=6, thread_name_prefix="model_loader_v22")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ê¸°
        self.validator = CheckpointValidator()
        
        # ì´ˆê¸°í™” í”Œë˜ê·¸
        self._is_initialized = False
        
        # ì•ˆì „í•œ ì´ˆê¸°í™” ì‹¤í–‰
        self._safe_initialize()
        
        self.logger.info(f"ğŸ¯ ì™„ì „ ê°œì„ ëœ ModelLoader v22.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
        self.logger.info(f"ğŸ’¾ Memory: {self.memory_gb:.1f}GB")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬: {self.model_cache_dir}")
        
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²° (M3 Max ìµœì í™”)"""
        if device == "auto":
            if self.is_m3_max and MPS_AVAILABLE:
                self.logger.info("ğŸ M3 Max MPS ë””ë°”ì´ìŠ¤ ì„ íƒ")
                return "mps"
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def _resolve_model_cache_dir(self, model_cache_dir_raw) -> Path:
        """ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ í•´ê²° (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)"""
        try:
            if model_cache_dir_raw is None:
                # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìë™ ê³„ì‚°
                current_file = Path(__file__).resolve()
                current_path = current_file.parent
                
                # backend/ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                for i in range(10):
                    if current_path.name == 'backend':
                        ai_models_path = current_path / "ai_models"
                        self.logger.info(f"ğŸ“ ìë™ ê°ì§€ëœ AI ëª¨ë¸ ê²½ë¡œ: {ai_models_path}")
                        return ai_models_path
                    if current_path.parent == current_path:
                        break
                    current_path = current_path.parent
                
                # í´ë°± ê²½ë¡œ
                fallback_path = Path.cwd() / "ai_models"
                self.logger.warning(f"âš ï¸ í´ë°± AI ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {fallback_path}")
                return fallback_path
            else:
                path = Path(model_cache_dir_raw)
                # backend/backend íŒ¨í„´ ì œê±°
                path_str = str(path)
                if "backend/backend" in path_str:
                    path = Path(path_str.replace("backend/backend", "backend"))
                return path.resolve()
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ í•´ê²° ì‹¤íŒ¨: {e}")
            return Path.cwd() / "ai_models"
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            total_gb = memory.total / (1024**3)
            
            # M3 Max íŠ¹ë³„ ì²˜ë¦¬
            if self.is_m3_max:
                self.logger.info(f"ğŸ M3 Max 128GB í†µí•© ë©”ëª¨ë¦¬ ê°ì§€: {total_gb:.1f}GB")
            
            return total_gb
        except ImportError:
            return 128.0 if self.is_m3_max else 16.0
    
    def _safe_initialize(self):
        """ì•ˆì „í•œ ì´ˆê¸°í™” (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            if not self.model_cache_dir.exists():
                self.model_cache_dir.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.model_cache_dir}")
            
            # ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
            self._initialize_model_registry()
            
            # ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ ëª¨ë¸ ìŠ¤ìº” (229GB í™œìš©)
            self._comprehensive_model_scan()
            
            # ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max íŠ¹í™”)
            if self.optimization_enabled:
                self._safe_memory_cleanup()
            
            self._is_initialized = True
            self.logger.info(f"ğŸ“¦ ModelLoader ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œ ê¸°ëŠ¥ì´ë¼ë„ ë³´ì¥
            self._emergency_fallback_init()
    
    def _initialize_model_registry(self):
        """ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)"""
        try:
            # ì‹¤ì œ 229GB íŒŒì¼ êµ¬ì¡°ë¥¼ ë°˜ì˜í•œ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
            default_models = {
                # Human Parsing Models (4GB)
                "human_parsing_schp_atr": ModelConfig(
                    name="human_parsing_schp_atr",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="HumanParsingModel",
                    device="auto",
                    precision="fp16",
                    input_size=(512, 512),
                    num_classes=20,
                    priority_score=90.0  # ë†’ì€ ìš°ì„ ìˆœìœ„
                ),
                
                # Cloth Segmentation Models (5.5GB)
                "cloth_segmentation_sam": ModelConfig(
                    name="cloth_segmentation_sam",
                    model_type=ModelType.CLOTH_SEGMENTATION,
                    model_class="SAMModel",
                    device="auto",
                    precision="fp16",
                    input_size=(1024, 1024),
                    num_classes=1,
                    priority_score=95.0  # ë§¤ìš° ë†’ì€ ìš°ì„ ìˆœìœ„
                ),
                
                # Virtual Fitting Models (14GB)
                "virtual_fitting_ootd": ModelConfig(
                    name="virtual_fitting_ootd",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="OOTDiffusionModel",
                    device="auto",
                    precision="fp16",
                    input_size=(512, 512),
                    priority_score=100.0  # ìµœê³  ìš°ì„ ìˆœìœ„
                ),
                
                # Cloth Warping Models (7GB)
                "cloth_warping_realvis": ModelConfig(
                    name="cloth_warping_realvis",
                    model_type=ModelType.CLOTH_WARPING,
                    model_class="RealVisXLModel",
                    device="auto",
                    precision="fp16",
                    input_size=(512, 512),
                    priority_score=85.0
                ),
                
                # Quality Assessment Models (7GB)
                "quality_assessment_clip": ModelConfig(
                    name="quality_assessment_clip",
                    model_type=ModelType.QUALITY_ASSESSMENT,
                    model_class="CLIPModel",
                    device="auto",
                    precision="fp16",
                    input_size=(224, 224),
                    priority_score=80.0
                )
            }
            
            for name, config in default_models.items():
                self.model_configs[name] = config
            
            self.logger.info(f"ğŸ“ ê¸°ë³¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”: {len(default_models)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _comprehensive_model_scan(self):
        """ì¢…í•©ì ì¸ ëª¨ë¸ ìŠ¤ìº” (229GB ì‹¤ì œ íŒŒì¼ í™œìš©)"""
        try:
            if not self.model_cache_dir.exists():
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_cache_dir}")
                return
            
            # ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ê²€ìƒ‰ ê²½ë¡œë“¤
            search_paths = [
                self.model_cache_dir,
                self.model_cache_dir / "step_01_human_parsing",
                self.model_cache_dir / "step_02_pose_estimation", 
                self.model_cache_dir / "step_03_cloth_segmentation",
                self.model_cache_dir / "step_04_geometric_matching",
                self.model_cache_dir / "step_05_cloth_warping",
                self.model_cache_dir / "step_06_virtual_fitting",
                self.model_cache_dir / "step_07_post_processing",
                self.model_cache_dir / "step_08_quality_assessment",
                self.model_cache_dir / "checkpoints",
                self.model_cache_dir / "Self-Correction-Human-Parsing",
                self.model_cache_dir / "Graphonomy"
            ]
            
            # ultra_models í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ í¬í•¨
            for step_dir in [f"step_{i:02d}_*" for i in range(1, 9)]:
                for path in self.model_cache_dir.glob(step_dir):
                    if path.is_dir():
                        search_paths.append(path)
                        ultra_path = path / "ultra_models"
                        if ultra_path.exists():
                            search_paths.append(ultra_path)
            
            # ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ í•„í„°ë§
            existing_paths = [p for p in search_paths if p.exists()]
            
            scanned_count = 0
            large_model_count = 0
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            
            self.logger.info(f"ğŸ” {len(existing_paths)}ê°œ ê²½ë¡œì—ì„œ ëª¨ë¸ ìŠ¤ìº” ì‹œì‘...")
            
            for search_path in existing_paths:
                try:
                    for ext in extensions:
                        for model_file in search_path.rglob(f"*{ext}"):
                            try:
                                if not model_file.is_file():
                                    continue
                                
                                size_mb = model_file.stat().st_size / (1024 * 1024)
                                
                                # í¬ê¸° í•„í„°ë§ (50MB ì´ìƒ ìš°ì„ ìˆœìœ„)
                                if size_mb < self.min_model_size_mb:
                                    continue
                                
                                # ë¹ ë¥¸ ê²€ì¦
                                if not self._quick_validate_file(model_file):
                                    continue
                                
                                relative_path = model_file.relative_to(self.model_cache_dir)
                                model_type, step_class = self._detect_model_info(model_file)
                                
                                # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
                                priority_score = self._calculate_priority_score(size_mb, model_file)
                                
                                model_info = {
                                    "name": model_file.stem,
                                    "path": str(relative_path),
                                    "size_mb": round(size_mb, 2),
                                    "model_type": model_type,
                                    "step_class": step_class,
                                    "loaded": False,
                                    "device": self.device,
                                    "is_valid": True,
                                    "priority_score": priority_score,
                                    "is_large_model": size_mb >= 1000,  # 1GB ì´ìƒ
                                    "metadata": {
                                        "extension": ext,
                                        "parent_dir": model_file.parent.name,
                                        "full_path": str(model_file),
                                        "detected_from": str(search_path.name),
                                        "scan_time": time.time()
                                    }
                                }
                                
                                self.available_models[model_file.stem] = model_info
                                scanned_count += 1
                                
                                if size_mb >= 1000:  # 1GB ì´ìƒ ëŒ€í˜• ëª¨ë¸
                                    large_model_count += 1
                                    self.logger.info(f"ğŸ† ëŒ€í˜• ëª¨ë¸ ë°œê²¬: {model_file.stem} ({size_mb:.1f}MB)")
                                
                            except Exception as e:
                                self.logger.debug(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {model_file}: {e}")
                                continue
                                
                except Exception as path_error:
                    self.logger.debug(f"âš ï¸ ê²½ë¡œ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {path_error}")
                    continue
            
            # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ (í¬ê¸° ìš°ì„ )
            self._sort_models_by_priority()
            
            self.logger.info(f"âœ… ì¢…í•© ëª¨ë¸ ìŠ¤ìº” ì™„ë£Œ: {scanned_count}ê°œ ë“±ë¡ (ëŒ€í˜•: {large_model_count}ê°œ)")
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
    
    def _quick_validate_file(self, file_path: Path) -> bool:
        """ë¹ ë¥¸ íŒŒì¼ ê²€ì¦"""
        try:
            if not file_path.exists():
                return False
            
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb < 1:  # 1MB ë¯¸ë§Œ ì œì™¸
                return False
                
            valid_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.ckpt'}
            if file_path.suffix.lower() not in valid_extensions:
                return False
                
            return True
        except Exception:
            return False
    
    def _detect_model_info(self, model_file: Path) -> tuple:
        """ëª¨ë¸ íƒ€ì… ë° Step í´ë˜ìŠ¤ ê°ì§€ (ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜)"""
        filename = model_file.name.lower()
        path_str = str(model_file).lower()
        
        # ì‹¤ì œ íŒŒì¼ëª… ê¸°ë°˜ ì •í™•í•œ ê°ì§€
        if "schp" in filename or "atr" in filename or "human" in filename or "parsing" in filename:
            return "human_parsing", "HumanParsingStep"
        elif "openpose" in filename or "pose" in filename or "yolo" in filename:
            return "pose_estimation", "PoseEstimationStep"
        elif "sam_vit" in filename or "u2net" in filename or "segment" in filename:
            return "cloth_segmentation", "ClothSegmentationStep"
        elif "gmm" in filename or "tps" in filename or "geometric" in filename:
            return "geometric_matching", "GeometricMatchingStep"
        elif "realvis" in filename or "warping" in filename or "vgg" in filename:
            return "cloth_warping", "ClothWarpingStep"
        elif "diffusion" in filename or "ootd" in filename or "pytorch_model" in filename:
            return "virtual_fitting", "VirtualFittingStep"
        elif "esrgan" in filename or "gfpgan" in filename or "enhancement" in filename:
            return "post_processing", "PostProcessingStep"
        elif "clip" in filename or "vit" in filename or "quality" in filename:
            return "quality_assessment", "QualityAssessmentStep"
        
        # ê²½ë¡œ ê¸°ë°˜ ê°ì§€
        if "step_01" in path_str or "human_parsing" in path_str:
            return "human_parsing", "HumanParsingStep"
        elif "step_02" in path_str or "pose" in path_str:
            return "pose_estimation", "PoseEstimationStep"
        elif "step_03" in path_str or "cloth_segmentation" in path_str:
            return "cloth_segmentation", "ClothSegmentationStep"
        elif "step_04" in path_str or "geometric" in path_str:
            return "geometric_matching", "GeometricMatchingStep"
        elif "step_05" in path_str or "warping" in path_str:
            return "cloth_warping", "ClothWarpingStep"
        elif "step_06" in path_str or "virtual" in path_str:
            return "virtual_fitting", "VirtualFittingStep"
        elif "step_07" in path_str or "post_processing" in path_str:
            return "post_processing", "PostProcessingStep"
        elif "step_08" in path_str or "quality" in path_str:
            return "quality_assessment", "QualityAssessmentStep"
        
        return "unknown", "UnknownStep"
    
    def _calculate_priority_score(self, size_mb: float, model_file: Path) -> float:
        """ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚° (í¬ê¸° ê¸°ë°˜)"""
        score = 0.0
        
        # í¬ê¸° ì ìˆ˜ (60% ê°€ì¤‘ì¹˜)
        if size_mb >= 5000:  # 5GB+
            score += 60.0
        elif size_mb >= 2000:  # 2GB+
            score += 50.0
        elif size_mb >= 1000:  # 1GB+
            score += 40.0
        elif size_mb >= 500:   # 500MB+
            score += 30.0
        elif size_mb >= 100:   # 100MB+
            score += 20.0
        else:  # 50MB+
            score += 10.0
        
        # íŒŒì¼ íƒ€ì… ì ìˆ˜ (20% ê°€ì¤‘ì¹˜)
        if model_file.suffix == ".safetensors":
            score += 20.0
        elif model_file.suffix in [".pth", ".pt"]:
            score += 15.0
        elif model_file.suffix == ".bin":
            score += 10.0
        
        # íŒŒì¼ëª… ì¤‘ìš”ë„ ì ìˆ˜ (20% ê°€ì¤‘ì¹˜)
        filename = model_file.name.lower()
        if "diffusion" in filename or "ootd" in filename:
            score += 20.0  # Virtual Fitting ìµœìš°ì„ 
        elif "sam_vit" in filename or "realvis" in filename:
            score += 18.0  # ëŒ€í˜• í•µì‹¬ ëª¨ë¸
        elif "clip" in filename or "schp" in filename:
            score += 15.0  # ì¤‘ìš” ëª¨ë¸
        else:
            score += 5.0   # ê¸°íƒ€
        
        return round(score, 2)
    
    def _sort_models_by_priority(self):
        """ëª¨ë¸ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬"""
        try:
            sorted_models = dict(sorted(
                self.available_models.items(),
                key=lambda x: x[1].get('priority_score', 0),
                reverse=True
            ))
            self.available_models = sorted_models
            self.logger.info(f"âœ… ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì •ë ¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë ¬ ì‹¤íŒ¨: {e}")
    
    def _emergency_fallback_init(self):
        """ë¹„ìƒ í´ë°± ì´ˆê¸°í™”"""
        try:
            if not hasattr(self, 'model_configs'):
                self.model_configs = {}
            if not hasattr(self, 'available_models'):
                self.available_models = {}
            if not hasattr(self, 'step_requirements'):
                self.step_requirements = {}
            
            self.logger.warning("âš ï¸ ë¹„ìƒ í´ë°± ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ìƒ í´ë°± ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ ê´€ë¦¬ (BaseStepMixin í˜¸í™˜)
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (BaseStepMixin ì™„ì „ í˜¸í™˜)"""
        try:
            with self._interface_lock:
                # Step ìš”êµ¬ì‚¬í•­ ë“±ë¡
                if step_requirements:
                    self.register_step_requirements(step_name, step_requirements)
                
                # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                if step_name in self.step_interfaces:
                    self.logger.info(f"âœ… ê¸°ì¡´ {step_name} ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜")
                    return self.step_interfaces[step_name]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                interface = StepModelInterface(self, step_name)
                self.step_interfaces[step_name] = interface
                
                self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return StepModelInterface(self, step_name)
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Union[Dict[str, Any], List[Dict[str, Any]]]
    ) -> bool:
        """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹œì‘...")
                
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # requirementsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(requirements, list):
                    processed_requirements = {}
                    for i, req in enumerate(requirements):
                        if isinstance(req, dict):
                            model_name = req.get("model_name", f"{step_name}_model_{i}")
                            processed_requirements[model_name] = req
                    requirements = processed_requirements
                
                # ìš”êµ¬ì‚¬í•­ ì—…ë°ì´íŠ¸
                if isinstance(requirements, dict):
                    self.step_requirements[step_name].update(requirements)
                
                self.logger.info(f"âœ… {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            with self._lock:
                model_config = ModelConfig(
                    name=model_name,
                    model_type=model_type,
                    model_class=kwargs.get("model_class", model_type),
                    device=kwargs.get("device", "auto"),
                    precision=kwargs.get("precision", "fp16"),
                    input_size=tuple(kwargs.get("input_size", (512, 512))),
                    num_classes=kwargs.get("num_classes"),
                    file_size_mb=kwargs.get("file_size_mb", 0.0),
                    priority_score=kwargs.get("priority_score", 50.0),
                    metadata=kwargs.get("metadata", {})
                )
                
                self.model_configs[model_name] = model_config
                
                self.available_models[model_name] = {
                    "name": model_name,
                    "path": f"requirements/{model_name}",
                    "size_mb": kwargs.get("file_size_mb", 0.0),
                    "model_type": model_type,
                    "step_class": kwargs.get("model_class", model_type),
                    "loaded": False,
                    "device": kwargs.get("device", "auto"),
                    "priority_score": kwargs.get("priority_score", 50.0),
                    "metadata": {
                        "source": "requirement_registration",
                        "registered_at": time.time(),
                        "step_name": kwargs.get("step_name", "unknown"),
                        **kwargs.get("metadata", {})
                    }
                }
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ì™„ì „ ê°œì„ )"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                if cache_entry.is_healthy:
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                    return cache_entry.model
            
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
            
            # ë¹„ë™ê¸°ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤í–‰
            loop = asyncio.get_event_loop()
            checkpoint = await loop.run_in_executor(
                self._executor, 
                self._safe_load_checkpoint_sync,
                model_name,
                kwargs
            )
            
            if checkpoint is not None:
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                cache_entry = SafeModelCacheEntry(
                    model=checkpoint,
                    load_time=time.time(),
                    last_access=time.time(),
                    access_count=1,
                    memory_usage_mb=self._estimate_checkpoint_size(checkpoint),
                    device=getattr(checkpoint, 'device', self.device) if hasattr(checkpoint, 'device') else self.device,
                    step_name=kwargs.get('step_name'),
                    is_healthy=True,
                    error_count=0
                )
                
                self.model_cache[model_name] = cache_entry
                self.loaded_models[model_name] = checkpoint
                
                if model_name in self.available_models:
                    self.available_models[model_name]["loaded"] = True
                
                self.performance_stats['models_loaded'] += 1
                self.performance_stats['checkpoint_loads'] += 1
                
                # ëŒ€í˜• ëª¨ë¸ ì¹´ìš´í„° ì¦ê°€
                if cache_entry.memory_usage_mb >= self.min_model_size_mb:
                    self.performance_stats['large_models_loaded'] += 1
                
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {model_name}")
                
            return checkpoint
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                if cache_entry.is_healthy:
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜: {model_name}")
                    return cache_entry.model
            
            if model_name not in self.available_models and model_name not in self.model_configs:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {model_name}")
                return None
            
            return self._safe_load_checkpoint_sync(model_name, kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _safe_load_checkpoint_sync(self, model_name: str, kwargs: Dict[str, Any]) -> Optional[Any]:
        """ì•ˆì „í•œ ë™ê¸° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            start_time = time.time()
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            checkpoint_path = self._find_checkpoint_file(model_name)
            if not checkpoint_path or not checkpoint_path.exists():
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {model_name}")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            validation = self.validator.validate_checkpoint_file(checkpoint_path)
            if not validation.is_valid:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {model_name} - {validation.error_message}")
                return None
            
            self.performance_stats['validation_count'] += 1
            if validation.pytorch_loadable:
                self.performance_stats['validation_success'] += 1
            
            # PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (M3 Max ìµœì í™”)
            if TORCH_AVAILABLE:
                try:
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max MPS ìµœì í™”)
                    if self.device in ["mps", "cuda"]:
                        self._safe_memory_cleanup()
                    
                    self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë”©: {checkpoint_path}")
                    
                    # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    checkpoint = None
                    
                    try:
                        # M3 Max MPS ìµœì í™”ëœ ë¡œë”©
                        map_location = self.device if self.device != "mps" else "cpu"
                        checkpoint = torch.load(checkpoint_path, map_location=map_location, weights_only=True)
                        
                        # MPSë¡œ ì´ë™ (M3 Max ìµœì í™”)
                        if self.device == "mps" and hasattr(checkpoint, 'to'):
                            checkpoint = checkpoint.to('mps')
                        
                        self.logger.debug(f"âœ… ì•ˆì „í•œ ë¡œë”© ì„±ê³µ (weights_only=True): {model_name}")
                    except Exception:
                        try:
                            checkpoint = torch.load(checkpoint_path, map_location=map_location, weights_only=False)
                            
                            # MPSë¡œ ì´ë™ (M3 Max ìµœì í™”)
                            if self.device == "mps" and hasattr(checkpoint, 'to'):
                                checkpoint = checkpoint.to('mps')
                                
                            self.logger.debug(f"âœ… ì¼ë°˜ ë¡œë”© ì„±ê³µ (weights_only=False): {model_name}")
                        except Exception as load_error:
                            self.logger.error(f"âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {load_error}")
                            return None
                    
                    if checkpoint is None:
                        self.logger.error(f"âŒ ë¡œë”©ëœ ì²´í¬í¬ì¸íŠ¸ê°€ None: {model_name}")
                        return None
                    
                    # ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬
                    processed_checkpoint = self._post_process_checkpoint(checkpoint, model_name)
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    load_time = time.time() - start_time
                    self.load_times[model_name] = load_time
                    self.last_access[model_name] = time.time()
                    self.access_counts[model_name] = self.access_counts.get(model_name, 0) + 1
                    
                    self.performance_stats['models_loaded'] += 1
                    self.performance_stats['checkpoint_loads'] += 1
                    
                    self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_name} ({load_time:.2f}ì´ˆ)")
                    return processed_checkpoint
                    
                except Exception as e:
                    self.logger.error(f"âŒ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
                    return None
            
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë¶ˆê°€: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_checkpoint_file(self, model_name: str) -> Optional[Path]:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° (ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜)"""
        try:
            # ëª¨ë¸ ì„¤ì •ì—ì„œ ì§ì ‘ ê²½ë¡œ í™•ì¸
            if model_name in self.model_configs:
                config = self.model_configs[model_name]
                if hasattr(config, 'checkpoint_path') and config.checkpoint_path:
                    checkpoint_path = Path(config.checkpoint_path)
                    if checkpoint_path.exists():
                        return checkpoint_path
            
            # available_modelsì—ì„œ ì°¾ê¸° (ì‹¤ì œ ìŠ¤ìº”ëœ íŒŒì¼ë“¤)
            if model_name in self.available_models:
                model_info = self.available_models[model_name]
                if "full_path" in model_info["metadata"]:
                    full_path = Path(model_info["metadata"]["full_path"])
                    if full_path.exists():
                        return full_path
                
                # ìƒëŒ€ ê²½ë¡œë¡œ ì°¾ê¸°
                if "path" in model_info:
                    relative_path = self.model_cache_dir / model_info["path"]
                    if relative_path.exists():
                        return relative_path
            
            # ì§ì ‘ íŒŒì¼ëª… ë§¤ì¹­ (í™•ì¥ìë³„)
            extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt"]
            for ext in extensions:
                direct_path = self.model_cache_dir / f"{model_name}{ext}"
                if direct_path.exists():
                    return direct_path
            
            # ìŠ¤ë§ˆíŠ¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
            return self._smart_find_checkpoint(model_name, extensions)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _smart_find_checkpoint(self, model_name: str, extensions: List[str]) -> Optional[Path]:
        """ìŠ¤ë§ˆíŠ¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
        try:
            candidates = []
            
            # ì‹¤ì œ 229GB íŒŒì¼ êµ¬ì¡°ì—ì„œ ê²€ìƒ‰
            search_patterns = [
                f"**/*{model_name}*",
                f"**/*{model_name.replace('_', '*')}*",
                f"**/*{model_name.split('_')[-1]}*" if '_' in model_name else f"**/*{model_name}*"
            ]
            
            for pattern in search_patterns:
                for model_file in self.model_cache_dir.rglob(pattern):
                    if model_file.is_file() and model_file.suffix.lower() in extensions:
                        try:
                            size_mb = model_file.stat().st_size / (1024 * 1024)
                            if size_mb >= self.min_model_size_mb:  # 50MB ì´ìƒ
                                candidates.append((model_file, size_mb))
                        except:
                            continue
            
            if candidates:
                # í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°) - ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ
                candidates.sort(key=lambda x: x[1], reverse=True)
                best_candidate = candidates[0][0]
                self.logger.info(f"ğŸ¯ ìŠ¤ë§ˆíŠ¸ ë§¤ì¹­ ì„±ê³µ: {model_name} -> {best_candidate.name} ({candidates[0][1]:.1f}MB)")
                return best_candidate
            
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return None
    
    def _post_process_checkpoint(self, checkpoint: Any, model_name: str) -> Any:
        """ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)"""
        try:
            if isinstance(checkpoint, dict):
                # ì¼ë°˜ì ì¸ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ì²˜ë¦¬
                if 'model' in checkpoint:
                    return checkpoint['model']
                elif 'state_dict' in checkpoint:
                    return checkpoint['state_dict']
                elif 'model_state_dict' in checkpoint:
                    return checkpoint['model_state_dict']
                else:
                    return checkpoint
            return checkpoint
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return checkpoint
    
    def _estimate_checkpoint_size(self, checkpoint) -> float:
        """ì²´í¬í¬ì¸íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        try:
            if TORCH_AVAILABLE and checkpoint is not None:
                if isinstance(checkpoint, dict):
                    total_params = 0
                    for param in checkpoint.values():
                        if hasattr(param, 'numel'):
                            total_params += param.numel()
                    return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
                elif hasattr(checkpoint, 'parameters'):
                    total_params = sum(p.numel() for p in checkpoint.parameters())
                    return total_params * 4 / (1024 * 1024)
            return 0.0
        except:
            return 0.0
    
    def _safe_memory_cleanup(self):
        """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max ìµœì í™”)"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as mps_error:
                        self.logger.debug(f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {mps_error}")
        except Exception as e:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ê´€ë¦¬ ë° ì •ë³´ ì¡°íšŒ
    # ==============================================
    
    def list_available_models(self, step_class: Optional[str] = None, 
                            model_type: Optional[str] = None,
                            large_only: bool = False) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (í¬ê¸° ìš°ì„ ìˆœìœ„ ì ìš©)"""
        try:
            models = []
            
            for model_name, model_info in self.available_models.items():
                # í¬ê¸° í•„í„°ë§
                if large_only and model_info.get("size_mb", 0) < self.min_model_size_mb:
                    continue
                
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                    
                models.append({
                    "name": model_info["name"],
                    "path": model_info["path"],
                    "size_mb": model_info["size_mb"],
                    "model_type": model_info["model_type"],
                    "step_class": model_info["step_class"],
                    "loaded": model_info["loaded"],
                    "device": model_info["device"],
                    "is_valid": model_info.get("is_valid", True),
                    "priority_score": model_info.get("priority_score", 0),
                    "is_large_model": model_info.get("is_large_model", False),
                    "metadata": model_info["metadata"]
                })
            
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°)
            models.sort(key=lambda x: x.get("priority_score", 0), reverse=True)
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if model_name in self.model_cache:
                cache_entry = self.model_cache[model_name]
                return {
                    "status": "loaded",
                    "device": cache_entry.device,
                    "memory_usage_mb": cache_entry.memory_usage_mb,
                    "last_used": cache_entry.last_access,
                    "load_time": cache_entry.load_time,
                    "access_count": cache_entry.access_count,
                    "model_type": type(cache_entry.model).__name__,
                    "loaded": True,
                    "is_healthy": cache_entry.is_healthy,
                    "error_count": cache_entry.error_count,
                    "step_name": cache_entry.step_name
                }
            elif model_name in self.model_configs:
                return {
                    "status": "registered",
                    "device": self.device,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": "Not Loaded",
                    "loaded": False,
                    "is_healthy": True,
                    "error_count": 0,
                    "step_name": None
                }
            elif model_name in self.available_models:
                model_info = self.available_models[model_name]
                return {
                    "status": "available",
                    "device": model_info.get("device", "auto"),
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": model_info.get("model_type", "unknown"),
                    "loaded": False,
                    "is_healthy": True,
                    "error_count": 0,
                    "step_name": model_info.get("step_class"),
                    "file_size_mb": model_info.get("size_mb", 0),
                    "priority_score": model_info.get("priority_score", 0)
                }
            else:
                return {
                    "status": "not_found",
                    "device": None,
                    "memory_usage_mb": 0,
                    "last_used": 0,
                    "load_time": 0,
                    "access_count": 0,
                    "model_type": None,
                    "loaded": False,
                    "is_healthy": False,
                    "error_count": 0,
                    "step_name": None
                }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {"status": "error", "error": str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (ì™„ì „ ê°œì„ )"""
        try:
            total_memory = sum(
                entry.memory_usage_mb for entry in self.model_cache.values()
            )
            
            load_times = list(self.load_times.values())
            avg_load_time = sum(load_times) / len(load_times) if load_times else 0
            
            validation_rate = (
                self.performance_stats['validation_success'] / 
                max(1, self.performance_stats['validation_count'])
            )
            
            cache_hit_rate = (
                self.performance_stats['cache_hits'] / 
                max(1, self.performance_stats['models_loaded'])
            )
            
            return {
                "model_counts": {
                    "loaded": len(self.model_cache),
                    "registered": len(self.model_configs),
                    "available": len(self.available_models),
                    "large_models_loaded": self.performance_stats.get('large_models_loaded', 0)
                },
                "memory_usage": {
                    "total_mb": total_memory,
                    "average_per_model_mb": total_memory / len(self.model_cache) if self.model_cache else 0,
                    "device": self.device,
                    "available_memory_gb": self.memory_gb,
                    "is_m3_max": self.is_m3_max
                },
                "performance_stats": {
                    "cache_hit_rate": cache_hit_rate,
                    "average_load_time_sec": avg_load_time,
                    "total_models_loaded": self.performance_stats['models_loaded'],
                    "checkpoint_loads": self.performance_stats.get('checkpoint_loads', 0),
                    "validation_rate": validation_rate,
                    "validation_count": self.performance_stats['validation_count'],
                    "validation_success": self.performance_stats['validation_success']
                },
                "step_interfaces": len(self.step_interfaces),
                "system_info": {
                    "conda_env": self.conda_env,
                    "is_mycloset_env": self.is_mycloset_env,
                    "is_m3_max": self.is_m3_max,
                    "torch_available": TORCH_AVAILABLE,
                    "mps_available": MPS_AVAILABLE,
                    "min_model_size_mb": self.min_model_size_mb
                },
                "optimization": {
                    "use_fp16": self.use_fp16,
                    "max_cached_models": self.max_cached_models,
                    "lazy_loading": self.lazy_loading,
                    "optimization_enabled": self.optimization_enabled
                },
                "version": "22.0"
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            if model_name in self.model_cache:
                del self.model_cache[model_name]
                
            if model_name in self.loaded_models:
                del self.loaded_models[model_name]
                
            if model_name in self.available_models:
                self.available_models[model_name]["loaded"] = False
                
            self._safe_memory_cleanup()
            
            self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {model_name} - {e}")
            return True  # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.model_cache.keys()):
                self.unload_model(model_name)
                
            # ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            self.step_interfaces.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            self._safe_memory_cleanup()
            
            self.logger.info("âœ… ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def initialize(self, **kwargs) -> bool:
        """ModelLoader ì´ˆê¸°í™”"""
        try:
            if self._is_initialized:
                self.logger.info("âœ… ì´ë¯¸ ì´ˆê¸°í™”ë¨")
                return True
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            if kwargs:
                for key, value in kwargs.items():
                    if hasattr(self, key):
                        setattr(self, key, value)
                        self.logger.debug(f"ğŸ”§ ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
            
            # ì¬ì´ˆê¸°í™” ì‹¤í–‰
            self._safe_initialize()
            
            self._is_initialized = True
            self.logger.info(f"âœ… ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            result = self.initialize(**kwargs)
            if result:
                self.logger.info("âœ… ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return result
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸"""
        return getattr(self, '_is_initialized', False)
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ ì „ì—­ ModelLoader ê´€ë¦¬ (BaseStepMixin í˜¸í™˜)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            # ì˜¬ë°”ë¥¸ AI ëª¨ë¸ ê²½ë¡œ ê³„ì‚°
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            ai_models_path = backend_root / "ai_models"
            
            try:
                _global_model_loader = ModelLoader(
                    config=config,
                    device="auto",
                    model_cache_dir=str(ai_models_path),
                    use_fp16=True,
                    optimization_enabled=True,
                    enable_fallback=True,
                    min_model_size_mb=50  # 50MB ì´ìƒ ìš°ì„ ìˆœìœ„
                )
                logger.info("âœ… ì „ì—­ ModelLoader ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader

def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info(f"âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜)
# ==============================================

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        loader = get_global_model_loader()
        if step_requirements:
            loader.register_step_requirements(step_name, step_requirements)
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        return StepModelInterface(get_global_model_loader(), step_name)

def validate_checkpoint_file(checkpoint_path: Union[str, Path]) -> CheckpointValidation:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ í•¨ìˆ˜"""
    return CheckpointValidator.validate_checkpoint_file(checkpoint_path)

def safe_load_checkpoint(checkpoint_path: Union[str, Path], device: str = "auto") -> Optional[Any]:
    """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í•¨ìˆ˜"""
    try:
        validation = validate_checkpoint_file(checkpoint_path)
        if not validation.is_valid:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {validation.error_message}")
            return None
        
        if TORCH_AVAILABLE:
            try:
                # M3 Max ìµœì í™”
                map_location = device if device != "auto" else DEFAULT_DEVICE
                if map_location == "mps":
                    map_location = "cpu"  # ë¡œë”© ì‹œì—ëŠ” CPU, ì´í›„ MPSë¡œ ì´ë™
                
                checkpoint = torch.load(checkpoint_path, map_location=map_location, weights_only=True)
                
                # MPSë¡œ ì´ë™ (M3 Max ìµœì í™”)
                if device == "auto" and DEFAULT_DEVICE == "mps" and hasattr(checkpoint, 'to'):
                    checkpoint = checkpoint.to('mps')
                
                logger.debug(f"âœ… ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=map_location, weights_only=False)
                    
                    # MPSë¡œ ì´ë™ (M3 Max ìµœì í™”)
                    if device == "auto" and DEFAULT_DEVICE == "mps" and hasattr(checkpoint, 'to'):
                        checkpoint = checkpoint.to('mps')
                        
                    logger.debug(f"âœ… ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    return checkpoint
                except Exception as e:
                    logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    return None
        
        return None
    except Exception as e:
        logger.error(f"âŒ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜)
def get_model(model_name: str) -> Optional[Any]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        if model_loader_instance is None:
            model_loader_instance = get_global_model_loader()
        
        return model_loader_instance.create_step_interface(step_name)
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return StepModelInterface(model_loader_instance or get_global_model_loader(), step_name)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (M3 Max ìµœì í™”)
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max ìµœì í™”)"""
    try:
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            if hasattr(torch, 'mps'):
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                if hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                return True
        return False
    except Exception:
        return False

def safe_torch_cleanup():
    """ì•ˆì „í•œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        gc.collect()
        
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            if MPS_AVAILABLE:
                safe_mps_empty_cache()
        
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def get_enhanced_memory_info() -> Dict[str, Any]:
    """í–¥ìƒëœ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        
        memory_info = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "percent": memory.percent,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV,
            "is_mycloset_env": IS_MYCLOSET_ENV
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                memory_info["gpu"] = {
                    "allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                    "reserved_mb": torch.cuda.memory_reserved() / (1024**2),
                    "max_allocated_mb": torch.cuda.max_memory_allocated() / (1024**2)
                }
            elif MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'current_allocated_memory'):
                        memory_info["mps"] = {
                            "allocated_mb": torch.mps.current_allocated_memory() / (1024**2)
                        }
                except:
                    memory_info["mps"] = {"status": "available"}
        
        return memory_info
        
    except ImportError:
        return {
            "total_gb": 128.0 if IS_M3_MAX else 16.0,
            "available_gb": 100.0 if IS_M3_MAX else 12.0,
            "used_gb": 28.0 if IS_M3_MAX else 4.0,
            "percent": 22.0 if IS_M3_MAX else 25.0,
            "is_m3_max": IS_M3_MAX,
            "conda_env": CONDA_ENV,
            "is_mycloset_env": IS_MYCLOSET_ENV
        }

# ==============================================
# ğŸ”¥ conda í™˜ê²½ ìµœì í™” í•¨ìˆ˜ë“¤
# ==============================================

def optimize_for_conda_env():
    """conda í™˜ê²½ íŠ¹í™” ìµœì í™” (mycloset-ai-clean)"""
    try:
        if IS_MYCLOSET_ENV:
            logger.info(f"ğŸ MyCloset conda í™˜ê²½ ìµœì í™” ì ìš©: {CONDA_ENV}")
            
            # M3 Max + conda íŠ¹í™” ì„¤ì •
            if IS_M3_MAX and TORCH_AVAILABLE:
                # PyTorch ì„¤ì • ìµœì í™”
                torch.set_num_threads(8)  # M3 Max 8ì½”ì–´ í™œìš©
                logger.info("ğŸ M3 Max + conda í™˜ê²½ ìµœì í™” ì ìš©")
                
                # MPS ë°±ì—”ë“œ ìµœì í™”
                if MPS_AVAILABLE:
                    logger.info("ğŸ”¥ MPS ë°±ì—”ë“œ ìµœì í™” ì ìš©")
                
        return {"conda_env": CONDA_ENV, "optimized": IS_MYCLOSET_ENV, "m3_max": IS_M3_MAX}
    except Exception as e:
        logger.warning(f"âš ï¸ conda í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {"conda_env": CONDA_ENV, "optimized": False, "m3_max": IS_M3_MAX}

def setup_m3_max_optimization():
    """M3 Max íŠ¹í™” ìµœì í™” ì„¤ì •"""
    if IS_M3_MAX and TORCH_AVAILABLE:
        try:
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            os.environ.update({
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'MPS_DISABLE_METAL_PERFORMANCE_SHADERS': '0',
                'PYTORCH_MPS_PREFER_DEVICE_PLACEMENT': '1'
            })
            
            # ìŠ¤ë ˆë“œ ìµœì í™”
            torch.set_num_threads(8)
            
            logger.info("ğŸ M3 Max íŠ¹í™” ìµœì í™” ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    return False

# ==============================================
# ğŸ”¥ Export (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'ModelLoader',
    'StepModelInterface', 
    'CheckpointValidator',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelFormat',
    'ModelType', 
    'ModelConfig',
    'SafeModelCacheEntry',
    'CheckpointValidation',
    'LoadingStatus',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
    'create_step_interface',
    'validate_checkpoint_file',
    'safe_load_checkpoint',
    'get_step_model_interface',
    
    # ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
    'get_model',
    'get_model_async',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'safe_mps_empty_cache',
    'safe_torch_cleanup', 
    'get_enhanced_memory_info',
    
    # conda í™˜ê²½ ìµœì í™”
    'optimize_for_conda_env',
    'setup_m3_max_optimization',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'IS_MYCLOSET_ENV'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” (conda í™˜ê²½ ìë™ ìµœì í™”)
# ==============================================

# conda í™˜ê²½ ë° M3 Max ìë™ ìµœì í™”
try:
    optimize_for_conda_env()
    setup_m3_max_optimization()
except Exception:
    pass

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
logger.info("=" * 80)
logger.info("âœ… ì™„ì „ ê°œì„ ëœ ModelLoader v22.0 - ì‹¤ì œ 229GB AI ëª¨ë¸ ì™„ì „ í™œìš©")
logger.info("=" * 80)
logger.info("ğŸ”¥ 2ë²ˆ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ 1ë²ˆ íŒŒì¼ê³¼ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ ì°¸ì¡°í•˜ì—¬ ì™„ì „ ê°œì„ ")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + BaseStepMixin 100% í˜¸í™˜")
logger.info("âœ… ì‹¤ì œ 229GB AI ëª¨ë¸ íŒŒì¼ 100% í™œìš©")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… conda í™˜ê²½ mycloset-ai-clean ì™„ì „ ì§€ì›")
logger.info("âœ… í¬ê¸° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ (50MB ì´ìƒ ìš°ì„ )")
logger.info("âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/ë©”ì„œë“œëª… 100% ìœ ì§€")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° ì„±ëŠ¥")
logger.info("=" * 80)