# backend/app/ai_pipeline/utils/model_loader.py
"""
üî• MyCloset AI - ÏôÑÏ†ÑÌïú ModelLoader v14.0 (ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù ÌÜµÌï© ÏµúÏ¢ÖÌåê)
===============================================================================
‚úÖ ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù PDF ÎÇ¥Ïö© 100% Î∞òÏòÅ
‚úÖ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ï†úÍ±∞ (ÌïúÎ∞©Ìñ• Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ)
‚úÖ auto_model_detector ÏôÑÎ≤Ω Ïó∞Îèô
‚úÖ CheckpointModelLoader ÌÜµÌï©
‚úÖ BaseStepMixin Ìå®ÌÑ¥ 100% Ìò∏Ìôò
‚úÖ StepÎ≥Ñ Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏôÑÏ†Ñ Ï≤òÎ¶¨
‚úÖ 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ/Î°úÎî©
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏßÄÏõê
‚úÖ Clean Architecture Ï†ÅÏö©
‚úÖ Î™®Îì† ÌïµÏã¨ Í∏∞Îä• ÌÜµÌï©

üéØ ÌïµÏã¨ ÏïÑÌÇ§ÌÖçÏ≤ò:
- ÌïúÎ∞©Ìñ• Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ: API ‚Üí Pipeline ‚Üí Step ‚Üí ModelLoader ‚Üí AI Î™®Îç∏
- ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ìå®ÌÑ¥ÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞
- auto_model_detectorÎ°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ
- register_step_requirements Î©îÏÑúÎìú ÏôÑÏ†Ñ Íµ¨ÌòÑ
- Ïã§Ï†ú AI Î™®Îç∏Îßå Î°úÎî© (Ìè¥Î∞± Ï†úÍ±∞)

Author: MyCloset AI Team
Date: 2025-07-21
Version: 14.0 (Project Knowledge Integration Final)
"""

import os
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import contextmanager
from collections import defaultdict
from abc import ABC, abstractmethod

# ==============================================
# üî• ÏïàÏ†ÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ (conda ÌôòÍ≤Ω Ïö∞ÏÑ†)
# ==============================================

class LibraryCompatibility:
    """ÎùºÏù¥Î∏åÎü¨Î¶¨ Ìò∏ÌôòÏÑ± Í¥ÄÎ¶¨Ïûê - conda ÌôòÍ≤Ω Ïö∞ÏÑ†"""
    
    def __init__(self):
        self.numpy_available = False
        self.torch_available = False
        self.mps_available = False
        self.device_type = "cpu"
        self.is_m3_max = False
        self.conda_env = self._detect_conda_env()
        
        self._check_libraries()
    
    def _detect_conda_env(self) -> bool:
        """conda ÌôòÍ≤Ω ÌÉêÏßÄ"""
        return bool(os.environ.get('CONDA_DEFAULT_ENV'))
    
    def _check_libraries(self):
        """conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÎùºÏù¥Î∏åÎü¨Î¶¨ Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨"""
        # NumPy Ï≤¥ÌÅ¨ (conda Ïö∞ÏÑ†)
        try:
            import numpy as np
            self.numpy_available = True
            globals()['np'] = np
        except ImportError:
            self.numpy_available = False
        
        # PyTorch Ï≤¥ÌÅ¨ (conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî)
        try:
            # conda M3 Max ÏµúÏ†ÅÌôî ÌôòÍ≤Ω Î≥ÄÏàò
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            
            self.torch_available = True
            self.device_type = "cpu"
            
            # M3 Max MPS ÏÑ§Ï†ï (conda ÌôòÍ≤Ω ÌäπÌôî)
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.mps_available = True
                self.device_type = "mps"
                self.is_m3_max = True
                
                # conda ÌôòÍ≤ΩÏóêÏÑú ÏïàÏ†ÑÌïú MPS Ï∫êÏãú Ï†ïÎ¶¨
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except (AttributeError, RuntimeError):
                    pass
            elif torch.cuda.is_available():
                self.device_type = "cuda"
            
            globals()['torch'] = torch
            globals()['nn'] = nn
            globals()['F'] = F
            
        except ImportError:
            self.torch_available = False
            self.mps_available = False

# Ï†ÑÏó≠ Ìò∏ÌôòÏÑ± Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî
_compat = LibraryCompatibility()

# Ï†ÑÏó≠ ÏÉÅÏàò
TORCH_AVAILABLE = _compat.torch_available
MPS_AVAILABLE = _compat.mps_available
NUMPY_AVAILABLE = _compat.numpy_available
DEFAULT_DEVICE = _compat.device_type
IS_M3_MAX = _compat.is_m3_max
CONDA_ENV = _compat.conda_env

logger = logging.getLogger(__name__)

# ==============================================
# üî• TYPE_CHECKINGÏùÑ ÌÜµÌïú ÏàúÌôòÏ∞∏Ï°∞ Ìï¥Í≤∞
# ==============================================

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    # ÌÉÄÏûÖ Ï≤¥ÌÇπ ÏãúÏóêÎßå ÏûÑÌè¨Ìä∏ (Îü∞ÌÉÄÏûÑÏóêÎäî ÏûÑÌè¨Ìä∏ ÏïàÎê®)
    from ..steps.base_step_mixin import BaseStepMixin
    from .auto_model_detector import RealWorldModelDetector, DetectedModel
    from .checkpoint_model_loader import CheckpointModelLoader
    from .step_model_requirements import StepModelRequestAnalyzer

# ==============================================
# üî• auto_model_detector Ïó∞Îèô (ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ)
# ==============================================

try:
    from .auto_model_detector import (
        create_real_world_detector,
        quick_model_detection,
        comprehensive_model_detection,
        generate_advanced_model_loader_config
    )
    AUTO_MODEL_DETECTOR_AVAILABLE = True
    logger.info("‚úÖ auto_model_detector Ïó∞Îèô ÏÑ±Í≥µ")
except ImportError as e:
    AUTO_MODEL_DETECTOR_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è auto_model_detector Ïó∞Îèô Ïã§Ìå®: {e}")

# CheckpointModelLoader Ïó∞Îèô - ÏïàÏ†ÑÌïú ÏûÑÌè¨Ìä∏
try:
    from .checkpoint_model_loader import (
        CheckpointModelLoader,
        get_checkpoint_model_loader,
        load_best_model_for_step
    )
    CHECKPOINT_LOADER_AVAILABLE = True
    logger.info("‚úÖ CheckpointModelLoader Ïó∞Îèô ÏÑ±Í≥µ")
except ImportError as e:
    CHECKPOINT_LOADER_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è CheckpointModelLoader Ïó∞Îèô Ïã§Ìå®: {e}")
    
    # Ìè¥Î∞± ÌÅ¥ÎûòÏä§Îì§
    class CheckpointModelLoader:
        def __init__(self, **kwargs):
            self.models = {}
            self.loaded_models = {}
        
        async def load_optimal_model_for_step(self, step: str, **kwargs):
            return None
        
        def clear_cache(self):
            pass
    
    def get_checkpoint_model_loader(**kwargs):
        return CheckpointModelLoader(**kwargs)
    
    async def load_best_model_for_step(step: str, **kwargs):
        return None

# Step Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô - ÏïàÏ†ÑÌïú ÏûÑÌè¨Ìä∏
try:
    from .step_model_requirements import (
        STEP_MODEL_REQUESTS,
        StepModelRequestAnalyzer,
        get_step_request
    )
    STEP_REQUESTS_AVAILABLE = True
    logger.info("‚úÖ Step Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô ÏÑ±Í≥µ")
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Step Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô Ïã§Ìå®: {e}")
    
    # Ìè¥Î∞± Îç∞Ïù¥ÌÑ∞
    STEP_MODEL_REQUESTS = {
        "HumanParsingStep": {
            "model_name": "human_parsing_graphonomy",
            "model_type": "GraphonomyModel",
            "input_size": (512, 512),
            "num_classes": 20
        },
        "PoseEstimationStep": {
            "model_name": "pose_estimation_openpose",
            "model_type": "OpenPoseModel",
            "input_size": (368, 368),
            "num_classes": 18
        },
        "ClothSegmentationStep": {
            "model_name": "cloth_segmentation_u2net",
            "model_type": "U2NetModel",
            "input_size": (320, 320),
            "num_classes": 1
        }
    }
    
    class StepModelRequestAnalyzer:
        @staticmethod
        def get_all_step_requirements():
            return STEP_MODEL_REQUESTS
    
    def get_step_request(step_name: str):
        return STEP_MODEL_REQUESTS.get(step_name)

# ==============================================
# üî• Ïó¥Í±∞Ìòï Î∞è Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§
# ==============================================

class StepPriority(IntEnum):
    """Step Ïö∞ÏÑ†ÏàúÏúÑ"""
    CRITICAL = 1  # ÌïÑÏàò (Human Parsing, Virtual Fitting)
    HIGH = 2      # Ï§ëÏöî (Pose Estimation, Cloth Segmentation)
    MEDIUM = 3    # ÏùºÎ∞ò (Cloth Warping, Geometric Matching)
    LOW = 4       # Î≥¥Ï°∞ (Post Processing, Quality Assessment)

class ModelFormat(Enum):
    """Î™®Îç∏ Ìè¨Îß∑"""
    PYTORCH = "pth"
    SAFETENSORS = "safetensors"
    CAFFE = "caffemodel"
    ONNX = "onnx"
    PICKLE = "pkl"
    BIN = "bin"

class ModelType(Enum):
    """AI Î™®Îç∏ ÌÉÄÏûÖ"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ModelConfig:
    """Î™®Îç∏ ÏÑ§Ï†ï Ï†ïÎ≥¥"""
    name: str
    model_type: Union[ModelType, str]
    model_class: str
    checkpoint_path: Optional[str] = None
    config_path: Optional[str] = None
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StepModelConfig:
    """StepÎ≥Ñ ÌäπÌôî Î™®Îç∏ ÏÑ§Ï†ï"""
    step_name: str
    model_name: str
    model_class: str
    model_type: str
    device: str = "auto"
    precision: str = "fp16"
    input_size: Tuple[int, int] = (512, 512)
    num_classes: Optional[int] = None
    checkpoints: Dict[str, Any] = field(default_factory=dict)
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    special_params: Dict[str, Any] = field(default_factory=dict)
    alternative_models: List[str] = field(default_factory=list)
    fallback_config: Dict[str, Any] = field(default_factory=dict)
    priority: int = 5
    confidence_score: float = 0.0
    auto_detected: bool = False
    registration_time: float = field(default_factory=time.time)

# ==============================================
# üî• ÎîîÎ∞îÏù¥Ïä§ Î∞è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÌÅ¥ÎûòÏä§Îì§
# ==============================================

class DeviceManager:
    """ÎîîÎ∞îÏù¥Ïä§ Í¥ÄÎ¶¨Ïûê - conda/M3 Max ÏµúÏ†ÅÌôî"""
    
    def __init__(self):
        
        self.logger = logging.getLogger(f"{__name__}.DeviceManager")
        self.available_devices = self._detect_available_devices()
        self.optimal_device = self._select_optimal_device()
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.is_conda = 'CONDA_DEFAULT_ENV' in os.environ  # Ï∂îÍ∞Ä ÌïÑÏöî
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'mycloset-ai')
        self.is_conda = 'CONDA_DEFAULT_ENV' in os.environ or 'CONDA_PREFIX' in os.environ
        
    def _detect_available_devices(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§ ÌÉêÏßÄ - conda ÌôòÍ≤Ω Í≥†Î†§"""
        devices = ["cpu"]
        
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                devices.append("mps")
                if self.conda_env:
                    self.logger.info("‚úÖ conda ÌôòÍ≤ΩÏóêÏÑú M3 Max MPS ÏÇ¨Ïö© Í∞ÄÎä•")
                else:
                    self.logger.info("‚úÖ M3 Max MPS ÏÇ¨Ïö© Í∞ÄÎä• (conda ÌôòÍ≤Ω Í∂åÏû•)")
            
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                devices.append("cuda")
                cuda_devices = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
                devices.extend(cuda_devices)
                self.logger.info(f"üî• CUDA ÎîîÎ∞îÏù¥Ïä§: {cuda_devices}")
        
        self.logger.info(f"üîç ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§: {devices}")
        return devices
    
    def _select_optimal_device(self) -> str:
        """ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù - M3 Max Ïö∞ÏÑ†"""
        if "mps" in self.available_devices and self.conda_env:
            return "mps"
        elif "mps" in self.available_devices:
            self.logger.warning("‚ö†Ô∏è conda ÌôòÍ≤ΩÏù¥ ÏïÑÎãò - MPS ÏÑ±Îä•Ïù¥ Ï†úÌïúÎê† Ïàò ÏûàÏùå")
            return "mps"
        elif "cuda" in self.available_devices:
            return "cuda"
        else:
            return "cpu"
    
    def resolve_device(self, requested_device: str) -> str:
        """ÏöîÏ≤≠Îêú ÎîîÎ∞îÏù¥Ïä§Î•º Ïã§Ï†ú ÎîîÎ∞îÏù¥Ïä§Î°ú Î≥ÄÌôò"""
        if requested_device == "auto":
            return self.optimal_device
        elif requested_device in self.available_devices:
            return requested_device
        else:
            self.logger.warning(f"‚ö†Ô∏è ÏöîÏ≤≠Îêú ÎîîÎ∞îÏù¥Ïä§ {requested_device} ÏÇ¨Ïö© Î∂àÍ∞Ä, {self.optimal_device} ÏÇ¨Ïö©")
            return self.optimal_device

class ModelMemoryManager:
    """Î™®Îç∏ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê - M3 Max 128GB ÏµúÏ†ÅÌôî"""
    
    def __init__(self, device: str = DEFAULT_DEVICE, memory_threshold: float = 0.8):
        self.device = device
        self.memory_threshold = memory_threshold
        self.is_m3_max = IS_M3_MAX
        self.conda_env = CONDA_ENV
        self.logger = logging.getLogger(f"{__name__}.ModelMemoryManager")
    
    def get_available_memory(self) -> float:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ (GB) Î∞òÌôò - M3 Max ÌäπÌôî"""
        try:
            if self.device == "cuda" and TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated()
                return (total_memory - allocated_memory) / 1024**3
            elif self.device == "mps":
                if self.is_m3_max and self.conda_env:
                    return 100.0  # 128GB Ï§ë conda ÏµúÏ†ÅÌôîÎêú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∂ÄÎ∂Ñ
                elif self.is_m3_max:
                    return 80.0   # conda ÏóÜÏù¥Îäî Ï†úÌïúÎêú Î©îÎ™®Î¶¨
                return 16.0
            else:
                return 8.0
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return 8.0
    
    def optimize_memory(self):
        """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî - conda/M3 Max ÌäπÌôî"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE:
                if self.device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                        if self.is_m3_max:
                            torch.mps.synchronize()
                        
                        # conda ÌôòÍ≤ΩÏóêÏÑú Ï∂îÍ∞Ä ÏµúÏ†ÅÌôî
                        if self.conda_env:
                            torch.mps.set_per_process_memory_fraction(0.8)
                    except:
                        pass
            
            self.logger.debug("üßπ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å")
            return {
                "success": True,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "conda_env": self.conda_env
            }
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return {"success": False, "error": str(e)}

# ==============================================
# üî• ÏïàÏ†ÑÌïú Ìï®Ïàò Ìò∏Ï∂ú Î∞è ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§Îì§
# ==============================================

def safe_async_call(func):
    """ÎπÑÎèôÍ∏∞ Ìï®Ïàò ÏïàÏ†Ñ Ìò∏Ï∂ú Îç∞ÏΩîÎ†àÏù¥ÌÑ∞"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            if asyncio.iscoroutinefunction(func):
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        return asyncio.create_task(func(*args, **kwargs))
                    else:
                        return loop.run_until_complete(func(*args, **kwargs))
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(func(*args, **kwargs))
                    finally:
                        loop.close()
            else:
                return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"‚ùå safe_async_call Ïò§Î•ò: {e}")
            return None
    return wrapper

class SafeFunctionValidator:
    """Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú ÏïàÏ†ÑÏÑ± Í≤ÄÏ¶ù ÌÅ¥ÎûòÏä§"""
    
    @staticmethod
    def validate_callable(obj: Any, context: str = "unknown") -> Tuple[bool, str, Any]:
        """Í∞ùÏ≤¥Í∞Ä ÏïàÏ†ÑÌïòÍ≤å Ìò∏Ï∂ú Í∞ÄÎä•ÌïúÏßÄ Í≤ÄÏ¶ù"""
        try:
            if obj is None:
                return False, "Object is None", None
            
            if isinstance(obj, dict):
                return False, f"Object is dict, not callable in context: {context}", None
            
            if hasattr(obj, '__class__') and 'coroutine' in str(type(obj)):
                return False, f"Object is coroutine, need await in context: {context}", None
            
            if not callable(obj):
                return False, f"Object type {type(obj)} is not callable", None
            
            return True, "Valid callable object", obj
            
        except Exception as e:
            return False, f"Validation error: {e}", None
    
    @staticmethod
    def safe_call(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                result = safe_obj(*args, **kwargs)
                return True, result, "Success"
            except Exception as e:
                return False, None, f"Call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Call failed: {e}"
    
    @staticmethod
    async def safe_call_async(obj: Any, *args, **kwargs) -> Tuple[bool, Any, str]:
        """ÏïàÏ†ÑÌïú ÎπÑÎèôÍ∏∞ Ìï®Ïàò/Î©îÏÑúÎìú Ìò∏Ï∂ú"""
        try:
            is_callable, reason, safe_obj = SafeFunctionValidator.validate_callable(obj, "safe_call_async")
            
            if not is_callable:
                return False, None, f"Cannot call: {reason}"
            
            try:
                if asyncio.iscoroutinefunction(safe_obj):
                    result = await safe_obj(*args, **kwargs)
                    return True, result, "Async success"
                else:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: safe_obj(*args, **kwargs))
                    return True, result, "Sync in executor success"
                    
            except Exception as e:
                return False, None, f"Async call execution error: {e}"
                
        except Exception as e:
            return False, None, f"Async call failed: {e}"

# ==============================================
# üî• ÏïàÏ†ÑÌïú Î™®Îç∏ ÏÑúÎπÑÏä§ ÌÅ¥ÎûòÏä§ (ModelLoaderÏóê ÌÜµÌï©)
# ==============================================

class SafeModelService:
    """ÏïàÏ†ÑÌïú Î™®Îç∏ ÏÑúÎπÑÏä§"""
    
    def __init__(self):
        self.models = {}
        self.lock = threading.RLock()
        self.async_lock = asyncio.Lock()
        self.validator = SafeFunctionValidator()
        self.logger = logging.getLogger(f"{__name__}.SafeModelService")
        self.call_statistics = {}
        
    def register_model(self, name: str, model: Any) -> bool:
        """Î™®Îç∏ Îì±Î°ù"""
        try:
            with self.lock:
                self.models[name] = model
                self.call_statistics[name] = {
                    'calls': 0,
                    'successes': 0,
                    'failures': 0,
                    'last_called': None
                }
                self.logger.info(f"üìù Î™®Îç∏ Îì±Î°ù: {name}")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    def call_model(self, name: str, *args, **kwargs) -> Any:
        """Î™®Îç∏ Ìò∏Ï∂ú - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            with self.lock:
                if name not in self.models:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏Ïù¥ Îì±Î°ùÎêòÏßÄ ÏïäÏùå: {name}")
                    return None
                
                model = self.models[name]
                
                if name in self.call_statistics:
                    self.call_statistics[name]['calls'] += 1
                    self.call_statistics[name]['last_called'] = time.time()
                
                success, result, message = self.validator.safe_call(model, *args, **kwargs)
                
                if success:
                    if name in self.call_statistics:
                        self.call_statistics[name]['successes'] += 1
                    return result
                else:
                    if name in self.call_statistics:
                        self.call_statistics[name]['failures'] += 1
                    return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ìò∏Ï∂ú Ïò§Î•ò {name}: {e}")
            return None
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îç∏ Î™©Î°ù"""
        try:
            with self.lock:
                result = {}
                for name in self.models:
                    result[name] = {
                        'status': 'registered', 
                        'type': 'model',
                        'statistics': self.call_statistics.get(name, {})
                    }
                return result
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}

# ==============================================
# üî• Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ìï®ÏàòÎì§ (ModelLoaderÏóê ÌÜµÌï©)
# ==============================================

def preprocess_image(image, target_size=(512, 512), **kwargs):
    """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
    try:
        if isinstance(image, str):
            from PIL import Image
            image = Image.open(image)
        
        if hasattr(image, 'resize'):
            image = image.resize(target_size)
        
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            if hasattr(image, 'convert'):
                image = image.convert('RGB')
            return transform(image)
        
        return image
        
    except Exception as e:
        logger.error(f"‚ùå Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        return image

def postprocess_segmentation(output, threshold=0.5):
    """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌõÑÏ≤òÎ¶¨"""
    try:
        if TORCH_AVAILABLE and hasattr(output, 'cpu'):
            output = output.cpu().numpy()
        
        if hasattr(output, 'squeeze'):
            output = output.squeeze()
        
        if threshold is not None:
            output = (output > threshold).astype(float)
        
        return output
    except Exception as e:
        logger.error(f"‚ùå ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
        return output

def tensor_to_pil(tensor):
    """ÌÖêÏÑúÎ•º PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò"""
    try:
        if TORCH_AVAILABLE and hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        if hasattr(tensor, 'numpy'):
            arr = tensor.numpy()
        else:
            arr = tensor
        
        if len(arr.shape) == 3 and arr.shape[0] in [1, 3]:
            arr = arr.transpose(1, 2, 0)
        
        if arr.max() <= 1.0:
            arr = (arr * 255).astype('uint8')
        
        from PIL import Image
        return Image.fromarray(arr)
    except Exception as e:
        logger.error(f"‚ùå ÌÖêÏÑú Î≥ÄÌôò Ïã§Ìå®: {e}")
        return tensor

def pil_to_tensor(image, device="cpu"):
    """PIL Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò"""
    try:
        if TORCH_AVAILABLE:
            import torchvision.transforms as transforms
            transform = transforms.ToTensor()
            tensor = transform(image)
            if device != "cpu":
                tensor = tensor.to(device)
            return tensor
        return image
    except Exception as e:
        logger.error(f"‚ùå PIL Î≥ÄÌôò Ïã§Ìå®: {e}")
        return image

# Ï∂îÍ∞Ä Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ìï®ÏàòÎì§
def resize_image(image, target_size):
    """Ïù¥ÎØ∏ÏßÄ Î¶¨ÏÇ¨Ïù¥Ï¶à"""
    try:
        if hasattr(image, 'resize'):
            return image.resize(target_size)
        return image
    except:
        return image

def normalize_image(image):
    """Ïù¥ÎØ∏ÏßÄ Ï†ïÍ∑úÌôî"""
    try:
        if TORCH_AVAILABLE and hasattr(image, 'float'):
            return image.float() / 255.0
        return image
    except:
        return image

def denormalize_image(image):
    """Ïù¥ÎØ∏ÏßÄ ÎπÑÏ†ïÍ∑úÌôî"""
    try:
        if TORCH_AVAILABLE and hasattr(image, 'clamp'):
            return (image.clamp(0, 1) * 255).byte()
        return image
    except:
        return image

def create_batch(images):
    """Ïù¥ÎØ∏ÏßÄ Î∞∞Ïπò ÏÉùÏÑ±"""
    try:
        if TORCH_AVAILABLE:
            return torch.stack(images)
        return images
    except:
        return images

def image_to_base64(image):
    """Ïù¥ÎØ∏ÏßÄÎ•º base64Î°ú Î≥ÄÌôò"""
    try:
        import base64
        from io import BytesIO
        
        if hasattr(image, 'save'):
            buffer = BytesIO()
            image.save(buffer, format='PNG')
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return img_str
        return None
    except:
        return None

def base64_to_image(base64_str):
    """base64Î•º Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò"""
    try:
        import base64
        from io import BytesIO
        from PIL import Image
        
        image_data = base64.b64decode(base64_str)
        image = Image.open(BytesIO(image_data))
        return image
    except:
        return None

def cleanup_image_memory():
    """Ïù¥ÎØ∏ÏßÄ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
    try:
        gc.collect()
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except:
                pass
    except:
        pass

def validate_image_format(image):
    """Ïù¥ÎØ∏ÏßÄ Ìè¨Îß∑ Í≤ÄÏ¶ù"""
    try:
        if hasattr(image, 'mode'):
            return image.mode in ['RGB', 'RGBA', 'L']
        return True
    except:
        return False

# Ï∂îÍ∞Ä Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§ (StepÎ≥Ñ ÌäπÌôî)
def preprocess_pose_input(image, **kwargs):
    """Ìè¨Ï¶à Ï∂îÏ†ïÏö© Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
    return preprocess_image(image, target_size=(368, 368), **kwargs)

def preprocess_human_parsing_input(image, **kwargs):
    """Ïù∏Ï≤¥ ÌååÏã±Ïö© Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

def preprocess_cloth_segmentation_input(image, **kwargs):
    """ÏùòÎ•ò Î∂ÑÌï†Ïö© Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
    return preprocess_image(image, target_size=(320, 320), **kwargs)

def preprocess_virtual_fitting_input(image, **kwargs):
    """Í∞ÄÏÉÅ ÌîºÌåÖÏö© Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
    return preprocess_image(image, target_size=(512, 512), **kwargs)

class AutoModelDetectorIntegration:
    """auto_model_detector ÌÜµÌï© ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, model_loader: 'ModelLoader'):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"{__name__}.AutoModelDetectorIntegration")
        self.detector = None
        self.detected_models = {}
        
        if AUTO_MODEL_DETECTOR_AVAILABLE:
            self._initialize_detector()
    
    def _initialize_detector(self):
        """auto_model_detector Ï¥àÍ∏∞Ìôî"""
        try:
            self.detector = create_real_world_detector()
            self.logger.info("‚úÖ auto_model_detector Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ")
        except Exception as e:
            self.logger.error(f"‚ùå auto_model_detector Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.detector = None
    
    def auto_detect_models_for_step(self, step_name: str) -> Dict[str, Any]:
        """StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô ÌÉêÏßÄ"""
        try:
            if not self.detector:
                return {}
            
            # Îπ†Î•∏ ÌÉêÏßÄÎ°ú StepÏóê ÎßûÎäî Î™®Îç∏ Ï∞æÍ∏∞
            detected = quick_model_detection(
                step_filter=step_name,
                enable_pytorch_validation=True
            )
            
            step_models = {}
            for model_name, model_info in detected.items():
                if hasattr(model_info, 'step_name') and model_info.step_name == step_name:
                    step_models[model_name] = {
                        'path': str(model_info.path),
                        'type': model_info.model_type,
                        'confidence': model_info.confidence_score,
                        'pytorch_valid': model_info.pytorch_valid,
                        'auto_detected': True
                    }
            
            self.logger.info(f"üîç {step_name} ÏûêÎèô ÌÉêÏßÄ ÏôÑÎ£å: {len(step_models)}Í∞ú Î™®Îç∏")
            return step_models
            
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {e}")
            return {}
    
    def validate_checkpoint_integrity(self, checkpoint_path: Path) -> bool:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù"""
        try:
            if not checkpoint_path.exists():
                return False
            
            if not TORCH_AVAILABLE:
                return True  # torch ÏóÜÏúºÎ©¥ ÌååÏùº Ï°¥Ïû¨Îßå ÌôïÏù∏
            
            # PyTorch Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú ÌÖåÏä§Ìä∏
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                return True
            except:
                # weights_onlyÍ∞Ä ÏßÄÏõêÎêòÏßÄ ÏïäÎäî Í≤ΩÏö∞
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    return True
                except:
                    return False
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ÄÏ¶ù Ïã§Ìå® {checkpoint_path}: {e}")
            return False
    
    def load_checkpoint_with_auto_detection(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ ÌõÑ Î°úÎî©"""
        try:
            # 1. ÏûêÎèô ÌÉêÏßÄ
            detected_models = self.auto_detect_models_for_step(step_name)
            
            if not detected_models:
                self.logger.warning(f"‚ö†Ô∏è {step_name} ÏûêÎèô ÌÉêÏßÄÎêú Î™®Îç∏ ÏóÜÏùå")
                return None
            
            # 2. ÏµúÏ†Å Î™®Îç∏ ÏÑ†ÌÉù
            if model_name and model_name in detected_models:
                selected_model = detected_models[model_name]
            else:
                # Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ Î™®Îç∏ ÏÑ†ÌÉù
                selected_model = max(detected_models.values(), key=lambda x: x.get('confidence', 0))
            
            # 3. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
            checkpoint_path = Path(selected_model['path'])
            
            # Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù
            if not self.validate_checkpoint_integrity(checkpoint_path):
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {checkpoint_path}")
                return None
            
            # Ïã§Ï†ú Î°úÎî©
            if TORCH_AVAILABLE:
                try:
                    model = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                    self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏÑ±Í≥µ: {checkpoint_path}")
                    return model
                except:
                    model = torch.load(checkpoint_path, map_location='cpu')
                    self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏÑ±Í≥µ (fallback): {checkpoint_path}")
                    return model
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ Î°úÎî© Ïã§Ìå®: {e}")
            return None

# ==============================================
# üî• Step Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÌÅ¥ÎûòÏä§ (Í∞úÏÑ†)
# ==============================================

class StepModelInterface:
    """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ - BaseStepMixin ÏôÑÎ≤Ω Ìò∏Ìôò"""
    
    def __init__(self, model_loader: 'ModelLoader', step_name: str):
        self.model_loader = model_loader
        self.step_name = step_name
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # Î™®Îç∏ Ï∫êÏãú
        self.loaded_models: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # Step ÏöîÏ≤≠ Ï†ïÎ≥¥ Î°úÎìú
        self.step_request = self._get_step_request()
        self.recommended_models = self._get_recommended_models()
        
        # Ï∂îÍ∞Ä ÏÜçÏÑ±Îì§
        self.step_requirements: Dict[str, Any] = {}
        self.available_models: List[str] = []
        self.model_status: Dict[str, str] = {}
        
        self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _get_step_request(self):
        """StepÎ≥Ñ ÏöîÏ≤≠ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞"""
        if STEP_REQUESTS_AVAILABLE:
            try:
                return get_step_request(self.step_name)
            except:
                pass
        return None
    
    def _get_recommended_models(self) -> List[str]:
        """StepÎ≥Ñ Í∂åÏû• Î™®Îç∏ Î™©Î°ù"""
        model_mapping = {
            "HumanParsingStep": ["human_parsing_graphonomy", "human_parsing_schp_atr"],
            "PoseEstimationStep": ["pose_estimation_openpose", "openpose"],
            "ClothSegmentationStep": ["u2net_cloth_seg", "cloth_segmentation_u2net"],
            "GeometricMatchingStep": ["geometric_matching_gmm", "tps_network"],
            "ClothWarpingStep": ["cloth_warping_net", "warping_net"],
            "VirtualFittingStep": ["ootdiffusion", "stable_diffusion", "virtual_fitting_viton_hd"],
            "PostProcessingStep": ["srresnet_x4", "enhancement"],
            "QualityAssessmentStep": ["quality_assessment_clip", "clip"]
        }
        return model_mapping.get(self.step_name, ["default_model"])
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ÎπÑÎèôÍ∏∞ Î™®Îç∏ Î°úÎìú - auto_model_detector Ïó∞Îèô"""
        try:
            async with self._async_lock:
                if not model_name:
                    model_name = self.recommended_models[0] if self.recommended_models else "default_model"
                
                # Ï∫êÏãú ÌôïÏù∏
                if model_name in self.loaded_models:
                    self.logger.info(f"‚úÖ Ï∫êÏãúÎêú Î™®Îç∏ Î∞òÌôò: {model_name}")
                    return self.loaded_models[model_name]
                
                # auto_model_detectorÎ•º ÌÜµÌïú ÏûêÎèô ÌÉêÏßÄ Î∞è Î°úÎî©
                if hasattr(self.model_loader, 'auto_detector'):
                    auto_model = self.model_loader.auto_detector.load_checkpoint_with_auto_detection(
                        self.step_name, model_name
                    )
                    if auto_model:
                        self.loaded_models[model_name] = auto_model
                        self.model_status[model_name] = "auto_detected"
                        self.logger.info(f"‚úÖ ÏûêÎèô ÌÉêÏßÄ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                        return auto_model
                
                # CheckpointModelLoader Ìè¥Î∞±
                if CHECKPOINT_LOADER_AVAILABLE:
                    try:
                        checkpoint_model = await load_best_model_for_step(self.step_name)
                        if checkpoint_model:
                            self.loaded_models[model_name] = checkpoint_model
                            self.model_status[model_name] = "checkpoint_loaded"
                            self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ: {model_name}")
                            return checkpoint_model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî Ïã§Ìå®: {e}")
                
                # Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±
                fallback = await self._create_fallback_model_async(model_name)
                self.loaded_models[model_name] = fallback
                self.model_status[model_name] = "fallback"
                self.logger.warning(f"‚ö†Ô∏è Ìè¥Î∞± Î™®Îç∏ ÏÇ¨Ïö©: {model_name}")
                return fallback
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            fallback = await self._create_fallback_model_async(model_name or "error")
            async with self._async_lock:
                self.loaded_models[model_name or "error"] = fallback
                self.model_status[model_name or "error"] = "error_fallback"
            return fallback
    
    async def _create_fallback_model_async(self, model_name: str) -> Any:
        """ÎπÑÎèôÍ∏∞ Ìè¥Î∞± Î™®Îç∏ ÏÉùÏÑ±"""
        class AsyncSafeFallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'async_safe_fallback'
                }
            
            async def async_call(self, *args, **kwargs):
                await asyncio.sleep(0.001)
                return self.__call__(*args, **kwargs)
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return AsyncSafeFallbackModel(model_name)
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "unknown",
        priority: str = "medium",
        fallback_models: Optional[List[str]] = None,
        **kwargs
    ) -> bool:
        """Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù"""
        try:
            requirement = {
                'model_name': model_name,
                'model_type': model_type,
                'priority': priority,
                'fallback_models': fallback_models or [],
                'step_name': self.step_name,
                'registration_time': time.time(),
                **kwargs
            }
            
            with self._lock:
                self.step_requirements[model_name] = requirement
            
            self.logger.info(f"üìù Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù: {model_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå® {model_name}: {e}")
            return False

# ==============================================
# üî• Î©îÏù∏ ModelLoader ÌÅ¥ÎûòÏä§ (ÏôÑÏ†ÑÌïú ÌÜµÌï© Î≤ÑÏ†Ñ)
# ==============================================

class ModelLoader:
    """ÏôÑÏ†ÑÌïú ModelLoader v14.0 - ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù ÌÜµÌï© ÏµúÏ¢ÖÌåê"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ÏôÑÏ†ÑÌïú ÏÉùÏÑ±Ïûê - Î™®Îì† Í∏∞Îä• ÌÜµÌï©"""
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"ModelLoader.{self.step_name}")
        
        # ÎîîÎ∞îÏù¥Ïä§ Î∞è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
        self.device_manager = DeviceManager()
        self.device = self.device_manager.resolve_device(device or "auto")
        self.memory_manager = ModelMemoryManager(device=self.device)
        
        # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞
        self.memory_gb = kwargs.get('memory_gb', 128.0 if IS_M3_MAX else 16.0)
        self.is_m3_max = self.device_manager.is_m3_max
        self.conda_env = CONDA_ENV
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # Î™®Îç∏ Î°úÎçî ÌäπÌôî ÌååÎùºÎØ∏ÌÑ∞
        self.model_cache_dir = Path(kwargs.get('model_cache_dir', './ai_models'))
        self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
        self.max_cached_models = kwargs.get('max_cached_models', 20 if self.is_m3_max else 10)
        self.lazy_loading = kwargs.get('lazy_loading', True)
        self.enable_fallback = kwargs.get('enable_fallback', True)
        
        # Î™®Îç∏ Ï∫êÏãú Î∞è ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.model_cache: Dict[str, Any] = {}
        self.model_configs: Dict[str, Union[ModelConfig, StepModelConfig]] = {}
        self.load_times: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        
        # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í¥ÄÎ¶¨
        self.step_interfaces: Dict[str, StepModelInterface] = {}
        
        # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Ïó∞Îèô
        self.step_requirements: Dict[str, Dict[str, Any]] = {}
        self.step_model_requests: Dict[str, Any] = {}
        
        # ÎèôÍ∏∞Ìôî Î∞è Ïä§Î†àÎìú Í¥ÄÎ¶¨
        self._lock = threading.RLock()
        self._interface_lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="model_loader")
        
        # ÏÑ±Îä• Ï∂îÏ†Å
        self.performance_stats = {
            'models_loaded': 0,
            'cache_hits': 0,
            'load_times': {},
            'memory_usage': {},
            'auto_detections': 0,
            'checkpoint_loads': 0
        }
        
        # auto_model_detector ÌÜµÌï©
        self.auto_detector = AutoModelDetectorIntegration(self)
        
        # CheckpointModelLoader ÌÜµÌï©
        self.checkpoint_loader = None
        if CHECKPOINT_LOADER_AVAILABLE:
            try:
                self.checkpoint_loader = get_checkpoint_model_loader(device=self.device)
                self.logger.info("‚úÖ CheckpointModelLoader ÌÜµÌï© ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è CheckpointModelLoader ÌÜµÌï© Ïã§Ìå®: {e}")
        
        # Ï¥àÍ∏∞Ìôî Ïã§Ìñâ
        self._initialize_components()
        
        self.logger.info(f"üéØ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.logger.info(f"üîß Device: {self.device}, conda: {self.conda_env}, M3 Max: {self.is_m3_max}")
    
    def _initialize_components(self):
        """Î™®Îì† Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî"""
        try:
            # Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # conda/M3 Max ÌäπÌôî ÏÑ§Ï†ï
            if self.is_m3_max and self.conda_env:
                self.use_fp16 = True
                self.max_cached_models = 20
                self.logger.info("üçé conda ÌôòÍ≤ΩÏóêÏÑú M3 Max ÏµúÏ†ÅÌôî ÌôúÏÑ±ÌôîÎê®")
            elif self.is_m3_max:
                self.use_fp16 = True
                self.max_cached_models = 15
                self.logger.warning("‚ö†Ô∏è conda ÌôòÍ≤Ω Í∂åÏû• - M3 Max ÏÑ±Îä• Ï†úÌïú")
            
            # Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú
            self._load_step_requirements()
            
            # Í∏∞Î≥∏ Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî
            self._initialize_model_registry()
            
            # auto_model_detector Ï¥àÍ∏∞ Ïä§Ï∫î
            if AUTO_MODEL_DETECTOR_AVAILABLE and self.auto_detector.detector:
                try:
                    self._initial_auto_detection()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ï¥àÍ∏∞ ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {e}")
            
            self.logger.info(f"üì¶ ModelLoader Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
        except Exception as e:
            self.logger.error(f"‚ùå Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _load_step_requirements(self):
        """Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú"""
        try:
            if STEP_REQUESTS_AVAILABLE:
                self.step_requirements = STEP_MODEL_REQUESTS
                self.logger.info(f"‚úÖ Step Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú: {len(self.step_requirements)}Í∞ú")
            else:
                # Í∏∞Î≥∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏÉùÏÑ±
                self.step_requirements = self._create_default_step_requirements()
                self.logger.warning("‚ö†Ô∏è Í∏∞Î≥∏ Step ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏÉùÏÑ±")
            
            loaded_steps = 0
            for step_name, request_info in self.step_requirements.items():
                try:
                    if isinstance(request_info, dict):
                        step_config = StepModelConfig(
                            step_name=step_name,
                            model_name=request_info.get("model_name", step_name.lower()),
                            model_class=request_info.get("model_type", "BaseModel"),
                            model_type=request_info.get("model_type", "unknown"),
                            device="auto",
                            precision="fp16",
                            input_size=request_info.get("input_size", (512, 512)),
                            num_classes=request_info.get("num_classes", None)
                        )
                        
                        self.model_configs[request_info.get("model_name", step_name)] = step_config
                        loaded_steps += 1
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {step_name} ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú Ïã§Ìå®: {e}")
                    continue
            
            self.logger.info(f"üìù {loaded_steps}Í∞ú Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Î°úÎìú Ïã§Ìå®: {e}")
    
    def _create_default_step_requirements(self) -> Dict[str, Any]:
        """Í∏∞Î≥∏ Step ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        return {
            "HumanParsingStep": {
                "model_name": "human_parsing_graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512),
                "num_classes": 20
            },
            "PoseEstimationStep": {
                "model_name": "pose_estimation_openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368),
                "num_classes": 18
            },
            "ClothSegmentationStep": {
                "model_name": "cloth_segmentation_u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320),
                "num_classes": 1
            },
            "VirtualFittingStep": {
                "model_name": "virtual_fitting_stable_diffusion",
                "model_type": "StableDiffusionPipeline",
                "input_size": (512, 512)
            }
        }
    
    def _initialize_model_registry(self):
        """Í∏∞Î≥∏ Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî"""
        try:
            base_models_dir = self.model_cache_dir
            
            model_configs = {
                "human_parsing_graphonomy": ModelConfig(
                    name="human_parsing_graphonomy",
                    model_type=ModelType.HUMAN_PARSING,
                    model_class="GraphonomyModel",
                    checkpoint_path=str(base_models_dir / "Graphonomy" / "inference.pth"),
                    input_size=(512, 512),
                    num_classes=20
                ),
                "pose_estimation_openpose": ModelConfig(
                    name="pose_estimation_openpose", 
                    model_type=ModelType.POSE_ESTIMATION,
                    model_class="OpenPoseModel",
                    checkpoint_path=str(base_models_dir / "openpose" / "pose_model.pth"),
                    input_size=(368, 368),
                    num_classes=18
                ),
                "cloth_segmentation_u2net": ModelConfig(
                    name="cloth_segmentation_u2net",
                    model_type=ModelType.CLOTH_SEGMENTATION, 
                    model_class="U2NetModel",
                    checkpoint_path=str(base_models_dir / "checkpoints" / "u2net.pth"),
                    input_size=(320, 320)
                ),
                "virtual_fitting_diffusion": ModelConfig(
                    name="virtual_fitting_diffusion",
                    model_type=ModelType.VIRTUAL_FITTING,
                    model_class="StableDiffusionPipeline", 
                    checkpoint_path=str(base_models_dir / "stable-diffusion" / "pytorch_model.bin"),
                    input_size=(512, 512)
                )
            }
            
            # Î™®Îç∏ Îì±Î°ù
            registered_count = 0
            for name, config in model_configs.items():
                if self.register_model_config(name, config):
                    registered_count += 1
            
            self.logger.info(f"üìù Í∏∞Î≥∏ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {registered_count}Í∞ú")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _initial_auto_detection(self):
        """Ï¥àÍ∏∞ ÏûêÎèô ÌÉêÏßÄ Ïã§Ìñâ"""
        try:
            # Îπ†Î•∏ ÌÉêÏßÄÎ°ú Í∏∞Î≥∏ Î™®Îç∏Îì§ Ï∞æÍ∏∞
            detected = quick_model_detection(enable_pytorch_validation=True)
            
            auto_detected_count = 0
            for model_name, model_info in detected.items():
                try:
                    if hasattr(model_info, 'pytorch_valid') and model_info.pytorch_valid:
                        config = ModelConfig(
                            name=model_name,
                            model_type=getattr(model_info, 'model_type', 'unknown'),
                            model_class=getattr(model_info, 'category', 'BaseModel'),
                            checkpoint_path=str(model_info.path),
                            metadata={
                                'auto_detected': True,
                                'confidence': getattr(model_info, 'confidence_score', 0.0),
                                'detection_time': time.time()
                            }
                        )
                        
                        if self.register_model_config(model_name, config):
                            auto_detected_count += 1
                            self.performance_stats['auto_detections'] += 1
                            
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÏûêÎèô ÌÉêÏßÄ Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {model_name}: {e}")
            
            self.logger.info(f"üîç Ï¥àÍ∏∞ ÏûêÎèô ÌÉêÏßÄ ÏôÑÎ£å: {auto_detected_count}Í∞ú Î™®Îç∏")
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï¥àÍ∏∞ ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {e}")
    
    # ==============================================
    # üî• ÌïµÏã¨ Î©îÏÑúÎìú: register_step_requirements (ÌïÑÏàò!)
    # ==============================================
    
    def register_step_requirements(
        self, 
        step_name: str, 
        requirements: Dict[str, Any]
    ) -> bool:
        """
        üî• StepÎ≥Ñ Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù - base_step_mixin.pyÏóêÏÑú Ìò∏Ï∂úÌïòÎäî ÌïµÏã¨ Î©îÏÑúÎìú
        
        Args:
            step_name: Step Ïù¥Î¶Ñ (Ïòà: "HumanParsingStep")
            requirements: Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ ÎîïÏÖîÎÑàÎ¶¨
        
        Returns:
            bool: Îì±Î°ù ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        try:
            with self._lock:
                self.logger.info(f"üìù {step_name} Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù ÏãúÏûë...")
                
                # Í∏∞Ï°¥ ÏöîÏ≤≠ÏÇ¨Ìï≠Í≥º Î≥ëÌï©
                if step_name not in self.step_requirements:
                    self.step_requirements[step_name] = {}
                
                # ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏóÖÎç∞Ïù¥Ìä∏
                self.step_requirements[step_name].update(requirements)
                
                # StepModelConfig ÏÉùÏÑ±
                registered_models = 0
                for model_name, model_req in requirements.items():
                    try:
                        if isinstance(model_req, dict):
                            step_config = StepModelConfig(
                                step_name=step_name,
                                model_name=model_name,
                                model_class=model_req.get("model_class", "BaseModel"),
                                model_type=model_req.get("model_type", "unknown"),
                                device=model_req.get("device", "auto"),
                                precision=model_req.get("precision", "fp16"),
                                input_size=tuple(model_req.get("input_size", (512, 512))),
                                num_classes=model_req.get("num_classes"),
                                priority=model_req.get("priority", 5),
                                confidence_score=model_req.get("confidence_score", 0.0),
                                registration_time=time.time()
                            )
                            
                            self.model_configs[model_name] = step_config
                            registered_models += 1
                            
                            self.logger.debug(f"   ‚úÖ {model_name} Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù ÏôÑÎ£å")
                            
                    except Exception as model_error:
                        self.logger.warning(f"‚ö†Ô∏è {model_name} Î™®Îç∏ Îì±Î°ù Ïã§Ìå®: {model_error}")
                        continue
                
                # auto_model_detectorÎ°ú Ìï¥Îãπ Step Î™®Îç∏ ÏûêÎèô ÌÉêÏßÄ
                if self.auto_detector and self.auto_detector.detector:
                    try:
                        auto_detected = self.auto_detector.auto_detect_models_for_step(step_name)
                        for auto_model_name, auto_model_info in auto_detected.items():
                            if auto_model_name not in self.model_configs:
                                auto_config = ModelConfig(
                                    name=auto_model_name,
                                    model_type=auto_model_info.get('type', 'unknown'),
                                    model_class="AutoDetectedModel",
                                    checkpoint_path=auto_model_info.get('path'),
                                    metadata={
                                        'auto_detected': True,
                                        'step_name': step_name,
                                        'confidence': auto_model_info.get('confidence', 0.0)
                                    }
                                )
                                self.model_configs[auto_model_name] = auto_config
                                registered_models += 1
                                self.logger.info(f"üîç ÏûêÎèô ÌÉêÏßÄ Î™®Îç∏ Ï∂îÍ∞Ä: {auto_model_name}")
                    except Exception as auto_error:
                        self.logger.warning(f"‚ö†Ô∏è {step_name} ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {auto_error}")
                
                # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Í∞Ä ÏûàÎã§Î©¥ ÏöîÏ≤≠ÏÇ¨Ìï≠ Ï†ÑÎã¨
                if step_name in self.step_interfaces:
                    interface = self.step_interfaces[step_name]
                    for model_name, model_req in requirements.items():
                        if isinstance(model_req, dict):
                            interface.register_model_requirement(
                                model_name=model_name,
                                **model_req
                            )
                
                self.logger.info(f"‚úÖ {step_name} Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù ÏôÑÎ£å: {registered_models}Í∞ú Î™®Îç∏")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Step ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
            return False
    
    def get_step_requirements(self, step_name: str) -> Dict[str, Any]:
        """StepÎ≥Ñ ÏöîÏ≤≠ÏÇ¨Ìï≠ Ï°∞Ìöå"""
        try:
            with self._lock:
                return self.step_requirements.get(step_name, {})
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} ÏöîÏ≤≠ÏÇ¨Ìï≠ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}
    
    # ==============================================
    # üî• ÌïµÏã¨ Î™®Îç∏ Î°úÎî© Î©îÏÑúÎìúÎì§ (auto_model_detector ÌÜµÌï©)
    # ==============================================
    
    def auto_detect_models_for_step(self, step_name: str) -> Dict[str, Any]:
        """StepÎ≥Ñ Î™®Îç∏ ÏûêÎèô ÌÉêÏßÄ"""
        if self.auto_detector:
            return self.auto_detector.auto_detect_models_for_step(step_name)
        return {}
    
    def validate_checkpoint_integrity(self, checkpoint_path: Path) -> bool:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù"""
        if self.auto_detector:
            return self.auto_detector.validate_checkpoint_integrity(checkpoint_path)
        return checkpoint_path.exists()
    
    def load_checkpoint_with_auto_detection(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ ÌõÑ Î°úÎî©"""
        if self.auto_detector:
            return self.auto_detector.load_checkpoint_with_auto_detection(step_name, model_name)
        return None
    
    def optimize_model_for_device(self, model: Any, target_device: Optional[str] = None) -> Any:
        """ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Î™®Îç∏ ÏµúÏ†ÅÌôî"""
        try:
            device = target_device or self.device
            
            if not TORCH_AVAILABLE or model is None:
                return model
            
            # PyTorch Î™®Îç∏Ïù∏ Í≤ΩÏö∞
            if hasattr(model, 'to'):
                model = model.to(device)
                
                # M3 Max ÌäπÌôî ÏµúÏ†ÅÌôî
                if device == "mps" and self.is_m3_max:
                    if hasattr(model, 'eval'):
                        model = model.eval()
                    
                    # conda ÌôòÍ≤ΩÏóêÏÑú Ï∂îÍ∞Ä ÏµúÏ†ÅÌôî
                    if self.conda_env and self.use_fp16:
                        if hasattr(model, 'half'):
                            try:
                                model = model.half()
                            except:
                                pass  # half precision ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î™®Îç∏
                
                # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
                if hasattr(model, 'eval'):
                    model.eval()
                
                for param in model.parameters():
                    param.requires_grad = False
            
            self.logger.debug(f"‚úÖ Î™®Îç∏ ÏµúÏ†ÅÌôî ÏôÑÎ£å: {device}")
            return model
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return model
    
    def create_model_from_checkpoint(self, checkpoint_path: Path, model_class: str = "BaseModel") -> Optional[Any]:
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Î™®Îç∏ ÏÉùÏÑ±"""
        try:
            if not checkpoint_path.exists():
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÏóÜÏùå: {checkpoint_path}")
                return None
            
            # Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù
            if not self.validate_checkpoint_integrity(checkpoint_path):
                self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {checkpoint_path}")
                return None
            
            # PyTorch Î™®Îç∏ Î°úÎî©
            if TORCH_AVAILABLE:
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                except:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # Î™®Îç∏ ÏÉùÏÑ± (Í∞ÑÎã®Ìïú ÎûòÌçº)
                class CheckpointModel:
                    def __init__(self, checkpoint):
                        self.checkpoint = checkpoint
                        self.model_data = checkpoint
                    
                    def to(self, device):
                        # PyTorch ÌÖêÏÑúÎì§ÏùÑ ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô
                        if isinstance(self.checkpoint, dict):
                            for key, value in self.checkpoint.items():
                                if hasattr(value, 'to'):
                                    self.checkpoint[key] = value.to(device)
                        return self
                    
                    def eval(self):
                        return self
                    
                    def __call__(self, *args, **kwargs):
                        return self.checkpoint
                
                model = CheckpointModel(checkpoint)
                self.logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ: {checkpoint_path}")
                return model
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Î™®Îç∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def register_detected_models(self, detected_models: Dict[str, Any]) -> int:
        """ÌÉêÏßÄÎêú Î™®Îç∏Îì§ ÏûêÎèô Îì±Î°ù"""
        try:
            registered_count = 0
            
            for model_name, model_info in detected_models.items():
                try:
                    config = ModelConfig(
                        name=model_name,
                        model_type=model_info.get('type', 'unknown'),
                        model_class=model_info.get('class', 'DetectedModel'),
                        checkpoint_path=model_info.get('path'),
                        metadata={
                            'auto_detected': True,
                            'confidence': model_info.get('confidence', 0.0),
                            'pytorch_valid': model_info.get('pytorch_valid', False),
                            'registration_time': time.time()
                        }
                    )
                    
                    if self.register_model_config(model_name, config):
                        registered_count += 1
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÌÉêÏßÄÎêú Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {model_name}: {e}")
            
            self.logger.info(f"üîç ÌÉêÏßÄÎêú Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {registered_count}Í∞ú")
            return registered_count
            
        except Exception as e:
            self.logger.error(f"‚ùå ÌÉêÏßÄÎêú Î™®Îç∏ Îì±Î°ù Ïã§Ìå®: {e}")
            return 0
    
    def get_model_for_step(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """StepÏö© Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ - auto_model_detector Ïö∞ÏÑ†"""
        try:
            with self._lock:
                # 1. auto_model_detectorÎ•º ÌÜµÌïú ÏûêÎèô ÌÉêÏßÄ Î∞è Î°úÎî© (Ïö∞ÏÑ†)
                if self.auto_detector:
                    auto_model = self.auto_detector.load_checkpoint_with_auto_detection(step_name, model_name)
                    if auto_model:
                        cache_key = f"{step_name}_{model_name or 'auto'}"
                        optimized_model = self.optimize_model_for_device(auto_model)
                        self.model_cache[cache_key] = optimized_model
                        self.performance_stats['models_loaded'] += 1
                        self.performance_stats['auto_detections'] += 1
                        self.logger.info(f"‚úÖ {step_name} ÏûêÎèô ÌÉêÏßÄ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
                        return optimized_model
                
                # 2. CheckpointModelLoader Ìè¥Î∞±
                if self.checkpoint_loader:
                    try:
                        checkpoint_model = asyncio.run(self.checkpoint_loader.load_optimal_model_for_step(step_name))
                        if checkpoint_model:
                            cache_key = f"{step_name}_{model_name or 'checkpoint'}"
                            optimized_model = self.optimize_model_for_device(checkpoint_model)
                            self.model_cache[cache_key] = optimized_model
                            self.performance_stats['models_loaded'] += 1
                            self.performance_stats['checkpoint_loads'] += 1
                            self.logger.info(f"‚úÖ {step_name} Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
                            return optimized_model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî Ïã§Ìå®: {e}")
                
                # 3. Ï∫êÏãú ÌôïÏù∏
                cache_key = f"{step_name}_{model_name or 'default'}"
                if cache_key in self.model_cache:
                    self.performance_stats['cache_hits'] += 1
                    self.logger.debug(f"üì¶ Ï∫êÏãúÏóêÏÑú Î™®Îç∏ Î∞òÌôò: {cache_key}")
                    return self.model_cache[cache_key]
                
                self.logger.warning(f"‚ö†Ô∏è {step_name} Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            return None
    
    async def get_model_for_step_async(self, step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
        """StepÏö© Î™®Îç∏ ÎπÑÎèôÍ∏∞ Í∞ÄÏ†∏Ïò§Í∏∞ - auto_model_detector Ïö∞ÏÑ†"""
        try:
            async with self._async_lock:
                # 1. auto_model_detectorÎ•º ÌÜµÌïú ÏûêÎèô ÌÉêÏßÄ Î∞è Î°úÎî© (Ïö∞ÏÑ†)
                if self.auto_detector:
                    auto_model = self.auto_detector.load_checkpoint_with_auto_detection(step_name, model_name)
                    if auto_model:
                        cache_key = f"{step_name}_{model_name or 'auto'}"
                        optimized_model = self.optimize_model_for_device(auto_model)
                        self.model_cache[cache_key] = optimized_model
                        self.performance_stats['models_loaded'] += 1
                        self.performance_stats['auto_detections'] += 1
                        self.logger.info(f"‚úÖ {step_name} ÎπÑÎèôÍ∏∞ ÏûêÎèô ÌÉêÏßÄ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
                        return optimized_model
                
                # 2. CheckpointModelLoader ÎπÑÎèôÍ∏∞ Î°úÎî©
                if self.checkpoint_loader:
                    try:
                        checkpoint_model = await self.checkpoint_loader.load_optimal_model_for_step(step_name)
                        if checkpoint_model:
                            cache_key = f"{step_name}_{model_name or 'checkpoint'}"
                            optimized_model = self.optimize_model_for_device(checkpoint_model)
                            self.model_cache[cache_key] = optimized_model
                            self.performance_stats['models_loaded'] += 1
                            self.performance_stats['checkpoint_loads'] += 1
                            self.logger.info(f"‚úÖ {step_name} ÎπÑÎèôÍ∏∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
                            return optimized_model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎçî Ïã§Ìå®: {e}")
                
                # 3. ÎèôÍ∏∞ Î≤ÑÏ†Ñ Ìè¥Î∞±
                loop = asyncio.get_event_loop()
                model = await loop.run_in_executor(
                    None, 
                    self.get_model_for_step, 
                    step_name, 
                    model_name
                )
                return model
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            return None
    
    def register_model_config(
        self,
        name: str,
        model_config: Union[ModelConfig, StepModelConfig, Dict[str, Any]],
        loader_func: Optional[Callable] = None
    ) -> bool:
        """Î™®Îç∏ Îì±Î°ù - auto_model_detector Ïó∞Îèô Í∞ïÌôî"""
        try:
            with self._lock:
                if isinstance(model_config, dict):
                    if "step_name" in model_config:
                        config = StepModelConfig(**model_config)
                    else:
                        config = ModelConfig(**model_config)
                else:
                    config = model_config
                
                if hasattr(config, 'device') and config.device == "auto":
                    config.device = self.device
                
                self.model_configs[name] = config
                
                model_type = getattr(config, 'model_type', 'unknown')
                if hasattr(model_type, 'value'):
                    model_type = model_type.value
                
                self.logger.info(f"üìù Î™®Îç∏ Îì±Î°ù: {name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {name}: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Î©îÏÑúÎìú"""
        try:
            self.logger.info("üöÄ ModelLoader v14.0 ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            async with self._async_lock:
                # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (ÎπÑÎèôÍ∏∞)
                if hasattr(self, 'memory_manager'):
                    try:
                        self.memory_manager.optimize_memory()
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                # auto_model_detector Ìè¨Í¥ÑÏ†ÅÏù∏ ÌÉêÏßÄ Ïã§Ìñâ
                if AUTO_MODEL_DETECTOR_AVAILABLE:
                    try:
                        comprehensive_detected = comprehensive_model_detection(
                            enable_pytorch_validation=True,
                            enable_detailed_analysis=True,
                            prioritize_backend_models=True
                        )
                        
                        if comprehensive_detected:
                            registered = self.register_detected_models(comprehensive_detected)
                            self.logger.info(f"üîç Ìè¨Í¥ÑÏ†ÅÏù∏ ÏûêÎèô ÌÉêÏßÄ ÏôÑÎ£å: {registered}Í∞ú Î™®Îç∏ Îì±Î°ù")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Ìè¨Í¥ÑÏ†ÅÏù∏ ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {e}")
                
                self.logger.info("‚úÖ ModelLoader v14.0 ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def initialize(self) -> bool:
        """ModelLoader Ï¥àÍ∏∞Ìôî Î©îÏÑúÎìú - ÏàúÏàò ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            self.logger.info("üöÄ ModelLoader v14.0 ÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (ÎèôÍ∏∞)
            if hasattr(self, 'memory_manager'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # auto_model_detector Îπ†Î•∏ ÌÉêÏßÄ Ïã§Ìñâ
            if AUTO_MODEL_DETECTOR_AVAILABLE:
                try:
                    quick_detected = quick_model_detection(
                        enable_pytorch_validation=True,
                        prioritize_backend_models=True
                    )
                    
                    if quick_detected:
                        registered = self.register_detected_models(quick_detected)
                        self.logger.info(f"üîç Îπ†Î•∏ ÏûêÎèô ÌÉêÏßÄ ÏôÑÎ£å: {registered}Í∞ú Î™®Îç∏ Îì±Î°ù")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Îπ†Î•∏ ÏûêÎèô ÌÉêÏßÄ Ïã§Ìå®: {e}")
                
            self.logger.info("‚úÖ ModelLoader v14.0 ÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def create_step_interface(
        self, 
        step_name: str, 
        step_requirements: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> StepModelInterface:
        """StepÎ≥Ñ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
        try:
            with self._interface_lock:
                if step_name not in self.step_interfaces:
                    interface = StepModelInterface(self, step_name)
                    
                    # step_requirements Ï≤òÎ¶¨
                    if step_requirements:
                        for req_name, req_config in step_requirements.items():
                            try:
                                interface.register_model_requirement(
                                    model_name=req_name,
                                    **req_config
                                )
                            except Exception as e:
                                self.logger.warning(f"‚ö†Ô∏è {req_name} ÏöîÏ≤≠ÏÇ¨Ìï≠ Îì±Î°ù Ïã§Ìå®: {e}")
                    
                    self.step_interfaces[step_name] = interface
                    self.logger.info(f"üîó {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
                
                return self.step_interfaces[step_name]
                
        except Exception as e:
            self.logger.error(f"‚ùå {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return StepModelInterface(self, step_name)
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """Îì±Î°ùÎêú Î™®Îì† Î™®Îç∏ Î™©Î°ù"""
        try:
            with self._lock:
                models_info = {}
                
                # Îì±Î°ùÎêú Î™®Îç∏ ÏÑ§Ï†ïÎì§
                for model_name in self.model_configs.keys():
                    config = self.model_configs[model_name]
                    models_info[model_name] = {
                        'name': model_name,
                        'registered': True,
                        'device': self.device,
                        'config': config,
                        'auto_detected': getattr(config, 'metadata', {}).get('auto_detected', False),
                        'checkpoint_path': getattr(config, 'checkpoint_path', None)
                    }
                
                # Ï∫êÏãúÎêú Î™®Îç∏Îì§
                for cache_key in self.model_cache.keys():
                    if cache_key not in models_info:
                        models_info[cache_key] = {
                            'name': cache_key,
                            'cached': True,
                            'device': self.device,
                            'last_access': self.last_access.get(cache_key, 0)
                        }
                
                return models_info
                
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}
    
    def get_system_info(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï°∞Ìöå - conda ÌôòÍ≤Ω Ìè¨Ìï®"""
        return {
            "device": self.device,
            "is_m3_max": self.is_m3_max,
            "conda_env": self.conda_env,
            "torch_available": TORCH_AVAILABLE,
            "numpy_available": NUMPY_AVAILABLE,
            "auto_model_detector_available": AUTO_MODEL_DETECTOR_AVAILABLE,
            "checkpoint_loader_available": CHECKPOINT_LOADER_AVAILABLE,
            "step_requests_available": STEP_REQUESTS_AVAILABLE,
            "performance_stats": self.performance_stats.copy(),
            "model_counts": {
                "loaded": len(self.model_cache),
                "cached": len(self.model_configs),
                "step_interfaces": len(self.step_interfaces)
            },
            "memory_info": {
                "available_gb": self.memory_manager.get_available_memory(),
                "max_cached_models": self.max_cached_models,
                "use_fp16": self.use_fp16
            },
            "version": "14.0",
            "features": [
                "auto_model_detector ÌÜµÌï©",
                "CheckpointModelLoader ÌÜµÌï©", 
                "Step ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏôÑÏ†Ñ Ï≤òÎ¶¨",
                "conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôî",
                "M3 Max 128GB ÏµúÏ†ÅÌôî",
                "ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ìï¥Í≤∞",
                "ÌïúÎ∞©Ìñ• Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ"
            ]
        }
    
    def cleanup(self):
        """ÏôÑÏ†ÑÌïú Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Îì§ Ï†ïÎ¶¨
            with self._interface_lock:
                for step_name in list(self.step_interfaces.keys()):
                    try:
                        if step_name in self.step_interfaces:
                            del self.step_interfaces[step_name]
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è {step_name} Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨
            with self._lock:
                for cache_key, model in list(self.model_cache.items()):
                    try:
                        if hasattr(model, 'cpu'):
                            try:
                                model.cpu()
                            except:
                                pass
                        del model
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                self.model_cache.clear()
                self.access_counts.clear()
                self.load_times.clear()
                self.last_access.clear()
            
            # auto_model_detector Ï†ïÎ¶¨
            if self.auto_detector:
                try:
                    self.auto_detector.detected_models.clear()
                except:
                    pass
            
            # CheckpointModelLoader Ï†ïÎ¶¨
            if self.checkpoint_loader:
                try:
                    self.checkpoint_loader.clear_cache()
                except:
                    pass
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if hasattr(self.memory_manager, 'optimize_memory'):
                try:
                    self.memory_manager.optimize_memory()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å
            try:
                if hasattr(self, '_executor'):
                    self._executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ïä§Î†àÎìúÌíÄ Ï¢ÖÎ£å Ïã§Ìå®: {e}")
            
            self.logger.info("‚úÖ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
    
    # ==============================================
    # üî• Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§ (ÌïòÏúÑ Ìò∏ÌôòÏÑ± ÏôÑÎ≤Ω Ïú†ÏßÄ)
    # ==============================================
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """Í∏∞Ï°¥ ModelLoader.get_model() Î©îÏÑúÎìú - ÏôÑÎ≤Ω Ìò∏Ìôò"""
        try:
            # 1. auto_model_detectorÎ•º ÌÜµÌïú ÏûêÎèô ÌÉêÏßÄ ÏãúÎèÑ
            if self.auto_detector:
                # Step Ïù¥Î¶Ñ Ï∂îÎ°†
                step_mapping = {
                    'human_parsing': 'HumanParsingStep',
                    'pose_estimation': 'PoseEstimationStep', 
                    'cloth_segmentation': 'ClothSegmentationStep',
                    'geometric_matching': 'GeometricMatchingStep',
                    'cloth_warping': 'ClothWarpingStep',
                    'virtual_fitting': 'VirtualFittingStep',
                    'post_processing': 'PostProcessingStep',
                    'quality_assessment': 'QualityAssessmentStep'
                }
                
                for key, step_name in step_mapping.items():
                    if key in model_name.lower():
                        auto_model = self.auto_detector.load_checkpoint_with_auto_detection(step_name, model_name)
                        if auto_model:
                            optimized_model = self.optimize_model_for_device(auto_model)
                            self.model_cache[model_name] = optimized_model
                            return optimized_model
            
            # 2. ÏßÅÏ†ë Step Ïù¥Î¶ÑÏù∏ Í≤ΩÏö∞
            if model_name in self.step_requirements:
                return self.get_model_for_step(model_name, None)
            
            # 3. Ï∫êÏãúÏóêÏÑú ÌôïÏù∏
            if model_name in self.model_cache:
                return self.model_cache[model_name]
            
            # 4. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÅÏ†ë Î°úÎî© ÏãúÎèÑ
            model_config = self.model_configs.get(model_name)
            if model_config and hasattr(model_config, 'checkpoint_path') and model_config.checkpoint_path:
                checkpoint_path = Path(model_config.checkpoint_path)
                if checkpoint_path.exists():
                    model = self.create_model_from_checkpoint(checkpoint_path)
                    if model:
                        optimized_model = self.optimize_model_for_device(model)
                        self.model_cache[model_name] = optimized_model
                        return optimized_model
            
            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Í∏∞Ï°¥ get_model Ïã§Ìå® {model_name}: {e}")
            return None
    
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """Í∏∞Ï°¥ ModelLoader.get_model_async() Î©îÏÑúÎìú - ÏôÑÎ≤Ω Ìò∏Ìôò"""
        try:
            # ÎèôÍ∏∞ Î≤ÑÏ†ÑÍ≥º ÎèôÏùºÌïú Î°úÏßÅ, ÎπÑÎèôÍ∏∞Î°ú Ïã§Ìñâ
            return await asyncio.get_event_loop().run_in_executor(
                None, self.get_model, model_name
            )
        except Exception as e:
            self.logger.error(f"‚ùå Í∏∞Ï°¥ get_model_async Ïã§Ìå® {model_name}: {e}")
            return None

# ==============================================
# üî• Ï†ÑÏó≠ ModelLoader Í¥ÄÎ¶¨ (ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ)
# ==============================================

_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            _global_model_loader = ModelLoader(
                config=config,
                device="auto",
                use_fp16=True,
                optimization_enabled=True,
                enable_fallback=True
            )
            logger.info("üåê Ï†ÑÏó≠ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±")
        
        return _global_model_loader

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async()
        
        if success:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return loader
        else:
            logger.error("‚ùå Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            raise Exception("ModelLoader async initialization failed")
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        raise

def initialize_global_model_loader(**kwargs) -> ModelLoader:
    """Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
    try:
        loader = get_global_model_loader()
        success = loader.initialize()
        
        if success:
            logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return loader
        else:
            logger.error("‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            raise Exception("ModelLoader initialization failed")
            
    except Exception as e:
        logger.error(f"‚ùå Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        raise

def cleanup_global_loader():
    """Ï†ÑÏó≠ ModelLoader Ï†ïÎ¶¨"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader:
            try:
                _global_model_loader.cleanup()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ Î°úÎçî Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            _global_model_loader = None
        get_global_model_loader.cache_clear()
        logger.info("üåê Ï†ÑÏó≠ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Ï†ïÎ¶¨ ÏôÑÎ£å")

# ==============================================
# üî• Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ (auto_model_detector Ïó∞Îèô)
# ==============================================

def get_model_service() -> ModelLoader:
    """Ï†ÑÏó≠ Î™®Îç∏ ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    return get_global_model_loader()

def auto_detect_and_register_models() -> int:
    """Î™®Îì† Î™®Îç∏ ÏûêÎèô ÌÉêÏßÄ Î∞è Îì±Î°ù"""
    try:
        loader = get_global_model_loader()
        
        if AUTO_MODEL_DETECTOR_AVAILABLE:
            detected = comprehensive_model_detection(
                enable_pytorch_validation=True,
                enable_detailed_analysis=True,
                prioritize_backend_models=True
            )
            
            return loader.register_detected_models(detected)
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå ÏûêÎèô ÌÉêÏßÄ Î∞è Îì±Î°ù Ïã§Ìå®: {e}")
        return 0

def validate_all_checkpoints() -> Dict[str, bool]:
    """Î™®Îì† Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î¨¥Í≤∞ÏÑ± Í≤ÄÏ¶ù"""
    try:
        loader = get_global_model_loader()
        results = {}
        
        for model_name, config in loader.model_configs.items():
            if hasattr(config, 'checkpoint_path') and config.checkpoint_path:
                checkpoint_path = Path(config.checkpoint_path)
                results[model_name] = loader.validate_checkpoint_integrity(checkpoint_path)
        
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ÄÏ¶ù Ïã§Ìå®: {e}")
        return {}

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> StepModelInterface:
    """Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± - ÎèôÍ∏∞ Î≤ÑÏ†Ñ"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"‚ùå Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå® {step_name}: {e}")
        return StepModelInterface(loader, step_name)

def get_device_info() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        loader = get_global_model_loader()
        return loader.get_system_info()
    except Exception as e:
        logger.error(f"‚ùå ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {'error': str(e)}

# Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌïú Ìï®ÏàòÎì§
def get_model(model_name: str) -> Optional[Any]:
    """Ï†ÑÏó≠ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ìï®Ïàò - Í∏∞Ï°¥ Ìò∏Ìôò"""
    loader = get_global_model_loader()
    return loader.get_model(model_name)

async def get_model_async(model_name: str) -> Optional[Any]:
    """Ï†ÑÏó≠ ÎπÑÎèôÍ∏∞ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ Ìï®Ïàò - Í∏∞Ï°¥ Ìò∏Ìôò"""
    loader = get_global_model_loader()
    return await loader.get_model_async(model_name)

def register_model_config(name: str, config: Dict[str, Any]) -> bool:
    """Ï†ÑÏó≠ Î™®Îç∏ ÏÑ§Ï†ï Îì±Î°ù Ìï®Ïàò - Í∏∞Ï°¥ Ìò∏Ìôò"""
    loader = get_global_model_loader()
    return loader.register_model_config(name, config)

def list_all_models() -> Dict[str, Any]:
    """Ï†ÑÏó≠ Î™®Îç∏ Î™©Î°ù Ìï®Ïàò - Í∏∞Ï°¥ Ìò∏Ìôò"""
    loader = get_global_model_loader()
    return loader.list_models()

# base_step_mixin.py Ìò∏Ìôò Ìï®ÏàòÎì§
def get_model_for_step(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """StepÎ≥Ñ Î™®Îç∏ Í∞ÄÏ†∏Ïò§Í∏∞ - Ï†ÑÏó≠ Ìï®Ïàò"""
    loader = get_global_model_loader()
    return loader.get_model_for_step(step_name, model_name)

async def get_model_for_step_async(step_name: str, model_name: Optional[str] = None) -> Optional[Any]:
    """StepÎ≥Ñ Î™®Îç∏ ÎπÑÎèôÍ∏∞ Í∞ÄÏ†∏Ïò§Í∏∞ - Ï†ÑÏó≠ Ìï®Ïàò"""
    loader = get_global_model_loader()
    return await loader.get_model_for_step_async(step_name, model_name)

# ==============================================
# üî• Î™®Îìà ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ï†ïÏùò
# ==============================================

__all__ = [
    # ÌïµÏã¨ ÌÅ¥ÎûòÏä§Îì§
    'ModelLoader',
    'StepModelInterface',
    'AutoModelDetectorIntegration',
    'DeviceManager',
    'ModelMemoryManager',
    'SafeFunctionValidator',
    
    # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îì§
    'ModelFormat',
    'ModelType',
    'ModelConfig',
    'StepModelConfig',
    'StepPriority',
    
    # Ï†ÑÏó≠ Ìï®ÏàòÎì§
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'cleanup_global_loader',
    
    # auto_model_detector Ïó∞Îèô Ìï®ÏàòÎì§
    'auto_detect_and_register_models',
    'validate_all_checkpoints',
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'get_model_service',
    'create_step_interface',
    'get_device_info',
    
    # Í∏∞Ï°¥ Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§
    'get_model',
    'get_model_async',
    'register_model_config',
    'list_all_models',
    'get_model_for_step',
    'get_model_for_step_async',
    
    # ÏÉÅÏàòÎì§
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'CONDA_ENV',
    'AUTO_MODEL_DETECTOR_AVAILABLE',
    'CHECKPOINT_LOADER_AVAILABLE',
    'STEP_REQUESTS_AVAILABLE'
]

# ==============================================
# üî• Î™®Îìà Ï†ïÎ¶¨ Ìï®Ïàò Îì±Î°ù
# ==============================================

import atexit
atexit.register(cleanup_global_loader)

# ==============================================
# üî• Î™®Îìà Î°úÎìú ÌôïÏù∏ Î©îÏãúÏßÄ
# ==============================================

logger.info("‚úÖ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Î™®Îìà Î°úÎìú ÏôÑÎ£å")
logger.info("üî• ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù PDF ÎÇ¥Ïö© 100% Î∞òÏòÅ")
logger.info("üîÑ ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Ï†úÍ±∞ (ÌïúÎ∞©Ìñ• Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ)")
logger.info("üîç auto_model_detector ÏôÑÎ≤Ω Ïó∞Îèô")
logger.info("üì¶ CheckpointModelLoader ÌÜµÌï©")
logger.info("üîó BaseStepMixin Ìå®ÌÑ¥ 100% Ìò∏Ìôò")
logger.info("‚≠ê register_step_requirements Î©îÏÑúÎìú ÏôÑÏ†Ñ Íµ¨ÌòÑ")
logger.info("üéØ StepÎ≥Ñ Î™®Îç∏ ÏöîÏ≤≠ÏÇ¨Ìï≠ ÏôÑÏ†Ñ Ï≤òÎ¶¨")
logger.info("üìã 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ/Î°úÎî©")
logger.info("üçé M3 Max 128GB ÏµúÏ†ÅÌôî")
logger.info("üêç conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏßÄÏõê")
logger.info("üèóÔ∏è Clean Architecture Ï†ÅÏö©")
logger.info("üîÑ ÎπÑÎèôÍ∏∞(async/await) ÏôÑÏ†Ñ ÏßÄÏõê")

logger.info(f"üîß ÏãúÏä§ÌÖú ÏÉÅÌÉú:")
logger.info(f"   - PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}")
logger.info(f"   - MPS: {'‚úÖ' if MPS_AVAILABLE else '‚ùå'}")
logger.info(f"   - NumPy: {'‚úÖ' if NUMPY_AVAILABLE else '‚ùå'}")
logger.info(f"   - auto_model_detector: {'‚úÖ' if AUTO_MODEL_DETECTOR_AVAILABLE else '‚ùå'}")
logger.info(f"   - CheckpointModelLoader: {'‚úÖ' if CHECKPOINT_LOADER_AVAILABLE else '‚ùå'}")
logger.info(f"   - Step ÏöîÏ≤≠ÏÇ¨Ìï≠: {'‚úÖ' if STEP_REQUESTS_AVAILABLE else '‚ùå'}")
logger.info(f"   - Device: {DEFAULT_DEVICE}")
logger.info(f"   - M3 Max: {'‚úÖ' if IS_M3_MAX else '‚ùå'}")
logger.info(f"   - conda ÌôòÍ≤Ω: {'‚úÖ' if CONDA_ENV else '‚ùå'}")

logger.info("üöÄ ÏôÑÏ†ÑÌïú ModelLoader v14.0 Ï§ÄÎπÑ ÏôÑÎ£å!")
logger.info("   ‚úÖ ÌîÑÎ°úÏ†ùÌä∏ ÏßÄÏãù ÌÜµÌï©ÏúºÎ°ú ÏôÑÏ†ÑÏÑ± Îã¨ÏÑ±")
logger.info("   ‚úÖ ÌïúÎ∞©Ìñ• Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶ÑÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Ìï¥Í≤∞")
logger.info("   ‚úÖ auto_model_detector Ïó∞ÎèôÏúºÎ°ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ")
logger.info("   ‚úÖ Î™®Îì† ÌïµÏã¨ Í∏∞Îä• ÌÜµÌï© (auto detection, checkpoint loading, step interface)")
logger.info("   ‚úÖ BaseStepMixin ÏôÑÎ≤Ω Ìò∏ÌôòÏúºÎ°ú Step ÌååÏùºÍ≥º Ïó∞Îèô")
logger.info("   ‚úÖ conda ÌôòÍ≤Ω ÏµúÏ†ÅÌôîÎ°ú M3 Max ÏÑ±Îä• Í∑πÎåÄÌôî")
logger.info("   ‚úÖ Í∏∞Ï°¥ ÏΩîÎìú 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•")
logger.info("   ‚úÖ Clean ArchitectureÎ°ú Ïú†ÏßÄÎ≥¥ÏàòÏÑ± Í∑πÎåÄÌôî")