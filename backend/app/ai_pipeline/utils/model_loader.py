# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ ModelLoader v5.1 â†’ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©
âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ - DI Containerë§Œì„ í†µí•œ ì˜ì¡´ì„± ì£¼ì…
âœ… inject_to_step() ë©”ì„œë“œ êµ¬í˜„ - Stepì— ModelLoader ìë™ ì£¼ì…
âœ… create_step_interface() ë©”ì„œë“œ ê°œì„  - Central Hub ê¸°ë°˜ í†µí•© ì¸í„°í˜ì´ìŠ¤
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ - validate_di_container_integration() ì™„ì „ ê°œì„ 
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ ì§€ì› - fix_checkpoints.py ê²€ì¦ ê²°ê³¼ ë°˜ì˜
âœ… Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡ - register_step_requirements() ì¶”ê°€
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” - Central Hub MemoryManager ì—°ë™
âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥ - ëª¨ë“  ë©”ì„œë“œëª…/í´ë˜ìŠ¤ëª… ìœ ì§€

í•µì‹¬ ì„¤ê³„ ì›ì¹™:
1. Single Source of Truth - ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” Central Hub DI Containerë¥¼ ê±°ì¹¨
2. Central Hub Pattern - DI Containerê°€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ì‹¬
3. Dependency Inversion - ìƒìœ„ ëª¨ë“ˆì´ í•˜ìœ„ ëª¨ë“ˆì„ ì œì–´
4. Zero Circular Reference - ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨

Author: MyCloset AI Team
Date: 2025-07-31
Version: 5.1 (Central Hub DI Container v7.0 Integration)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
import mmap
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod
from io import BytesIO

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================
# ğŸ”¥ ê°œì„ ëœ ìˆœí™˜ì°¸ì¡° ë°©ì§€ íŒ¨í„´

_central_hub_cache = None
_dependencies_cache = {}

def _get_central_hub_container():
    """ê°œì„ ëœ Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    global _central_hub_cache
    
    if _central_hub_cache is not None:
        return _central_hub_cache
    
    try:
        # ğŸ”¥ ê°œì„ : ìºì‹œëœ ëª¨ë“ˆ ìš°ì„  í™•ì¸
        if 'app.core.di_container' in sys.modules:
            module = sys.modules['app.core.di_container']
        else:
            import importlib
            module = importlib.import_module('app.core.di_container')
        
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn and callable(get_global_fn):
            _central_hub_cache = get_global_fn()
            return _central_hub_cache
        
        return None
    except (ImportError, AttributeError, RuntimeError):
        return None
    except Exception:
        return None

def _get_service_from_central_hub(service_key: str):
    """ê°œì„ ëœ Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    if service_key in _dependencies_cache:
        return _dependencies_cache[service_key]
    
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'get'):
            service = container.get(service_key)
            if service:
                _dependencies_cache[service_key] = service
            return service
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """ê°œì„ ëœ Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì…"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

# ğŸ”¥ ê°œì„ : ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
def _clear_dependency_cache():
    """ì˜ì¡´ì„± ìºì‹œ ì •ë¦¬"""
    global _central_hub_cache, _dependencies_cache
    _central_hub_cache = None
    _dependencies_cache.clear()




# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin
    from ..factories.step_factory import StepFactory
    from app.core.di_container import CentralHubDIContainer

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

logger = logging.getLogger(__name__)

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    import platform
    if platform.system() == 'Darwin':
        import subprocess
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# PyTorch ì•ˆì „ import (weights_only ë¬¸ì œ ì™„ì „ í•´ê²°)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    # ğŸ”¥ YOLOv8 ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ì„ Falseë¡œ ì„¤ì •
    os.environ['PYTORCH_DISABLE_WEIGHTS_ONLY_LOAD'] = '1'
    
    # ğŸ”¥ PyTorch 2.7 weights_only ë¬¸ì œ ì™„ì „ í•´ê²°
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            """PyTorch 2.7 í˜¸í™˜ ì•ˆì „ ë¡œë”"""
            # weights_onlyê°€ Noneì´ë©´ Falseë¡œ ì„¤ì • (Legacy í˜¸í™˜)
            if weights_only is None:
                weights_only = False
            
            try:
                # 1ë‹¨ê³„: weights_only=True ì‹œë„ (ê°€ì¥ ì•ˆì „)
                if weights_only:
                    # ğŸ”¥ YOLOv8/Ultralytics ëª¨ë¸ìš© ì•ˆì „ ê¸€ë¡œë²Œ ì¶”ê°€
                    try:
                        if hasattr(torch, 'serialization') and hasattr(torch.serialization, 'add_safe_globals'):
                            torch.serialization.add_safe_globals([
                                'ultralytics.nn.tasks.PoseModel',
                                'ultralytics.nn.tasks.DetectionModel',
                                'ultralytics.nn.tasks.SegmentationModel',
                                'ultralytics.nn.modules.head.Pose',
                                'ultralytics.nn.modules.block.C2f',
                                'ultralytics.nn.modules.conv.Conv'
                            ])
                    except Exception:
                        pass
                                
                    checkpoint = original_torch_load(f, map_location=map_location, 
                                                pickle_module=pickle_module, 
                                                weights_only=True, **kwargs)
                else:
                    # 2ë‹¨ê³„: weights_only=False ì‹œë„ (í˜¸í™˜ì„±)
                    checkpoint = original_torch_load(f, map_location=map_location, 
                                                pickle_module=pickle_module, 
                                                weights_only=False, **kwargs)            
                        # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                if map_location == 'mps' or (isinstance(map_location, torch.device) and map_location.type == 'mps'):
                    checkpoint = _convert_checkpoint_mps_float64_to_float32(checkpoint)
                
                return checkpoint
                    
            except RuntimeError as e:

                error_msg = str(e).lower()
                
                # Legacy .tar í¬ë§· ì—ëŸ¬ ê°ì§€
                if "legacy .tar format" in error_msg or "weights_only" in error_msg:
                    try:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # TorchScript ì•„ì¹´ì´ë¸Œ ì—ëŸ¬ ê°ì§€
                if "torchscript" in error_msg or "zip file" in error_msg:
                    try:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  íŒŒë¼ë¯¸í„° ì—†ì´
                try:
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore")
                        return original_torch_load(f, map_location=map_location)
                except Exception:
                    pass
                
                # ì›ë³¸ ì—ëŸ¬ ë‹¤ì‹œ ë°œìƒ
                raise e
            except Exception as e:
                # UnpicklingError ë“± ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
                error_msg = str(e).lower()
                
                if "unpicklingerror" in error_msg or "unsupported global" in error_msg:
                    try:
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            return original_torch_load(f, map_location=map_location, 
                                                     pickle_module=pickle_module, 
                                                     weights_only=False, **kwargs)
                    except Exception:
                        pass
                
                # ì›ë³¸ ì—ëŸ¬ ë‹¤ì‹œ ë°œìƒ
                raise e
        
        # torch.load ëŒ€ì²´
        torch.load = safe_torch_load
        
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
except ImportError:
    torch = None
    
def _convert_checkpoint_mps_float64_to_float32(checkpoint: Any) -> Any:
    """MPSìš© ì²´í¬í¬ì¸íŠ¸ float64 â†’ float32 ë³€í™˜ (model_loader ì „ìš©)"""
    if not TORCH_AVAILABLE:
        return checkpoint
    
    def convert_tensor(tensor):
        if hasattr(tensor, 'dtype') and tensor.dtype == torch.float64:
            return tensor.to(torch.float32)
        return tensor
    
    def recursive_convert(obj):
        if torch.is_tensor(obj):
            return convert_tensor(obj)
        elif isinstance(obj, dict):
            return {key: recursive_convert(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return type(obj)(recursive_convert(item) for item in obj)
        else:
            return obj
    
    try:
        converted_checkpoint = recursive_convert(checkpoint)
        logger.debug("âœ… ModelLoader MPS float64 â†’ float32 ë³€í™˜ ì™„ë£Œ")
        return converted_checkpoint
    except Exception as e:
        logger.warning(f"âš ï¸ ModelLoader MPS float64 ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
        return checkpoint
    
# ë””ë°”ì´ìŠ¤ ë° ì‹œìŠ¤í…œ ì •ë³´
DEFAULT_DEVICE = "cpu"
if IS_M3_MAX and MPS_AVAILABLE:
    DEFAULT_DEVICE = "mps"
elif TORCH_AVAILABLE and torch.cuda.is_available():
    DEFAULT_DEVICE = "cuda"

# auto_model_detector import (ì•ˆì „ ì²˜ë¦¬)
AUTO_DETECTOR_AVAILABLE = False
try:
    from .auto_model_detector import get_global_detector
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False

# ==============================================
# ğŸ”¥ Central Hub í˜¸í™˜ ë°ì´í„° êµ¬ì¡°
# ==============================================

class RealStepModelType(Enum):
    """ì‹¤ì œ AI Stepì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ íƒ€ì… (Central Hub í˜¸í™˜)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class RealModelStatus(Enum):
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ (Central Hub í˜¸í™˜)"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class RealModelPriority(Enum):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ (Central Hub í˜¸í™˜)"""
    PRIMARY = 1
    SECONDARY = 2
    FALLBACK = 3
    OPTIONAL = 4

@dataclass
class RealStepModelInfo:
    """ì‹¤ì œ AI Step ëª¨ë¸ ì •ë³´ (Central Hub í˜¸í™˜)"""
    name: str
    path: str
    step_type: RealStepModelType
    priority: RealModelPriority
    device: str
    
    # ì‹¤ì œ ë¡œë”© ì •ë³´
    memory_mb: float = 0.0
    loaded: bool = False
    load_time: float = 0.0
    checkpoint_data: Optional[Any] = None
    
    # Central Hub í˜¸í™˜ì„± ì •ë³´
    model_class: Optional[str] = None
    config_path: Optional[str] = None
    preprocessing_params: Dict[str, Any] = field(default_factory=dict)
    
    # Central Hub ì—°ë™ í•„ë“œ
    model_type: str = "BaseModel"
    size_gb: float = 0.0
    requires_checkpoint: bool = True
    checkpoint_key: Optional[str] = None
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    access_count: int = 0
    last_access: float = 0.0
    inference_count: int = 0
    avg_inference_time: float = 0.0
    
    # ì—ëŸ¬ ì •ë³´
    error: Optional[str] = None
    validation_passed: bool = False

@dataclass 
class RealStepModelRequirement:
    """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ (Central Hub í˜¸í™˜)"""
    step_name: str
    step_id: int
    step_type: RealStepModelType
    
    # ëª¨ë¸ ìš”êµ¬ì‚¬í•­
    required_models: List[str] = field(default_factory=list)
    optional_models: List[str] = field(default_factory=list)
    primary_model: Optional[str] = None
    
    # Central Hub DetailedDataSpec ì—°ë™
    model_configs: Dict[str, Any] = field(default_factory=dict)
    input_data_specs: Dict[str, Any] = field(default_factory=dict)
    output_data_specs: Dict[str, Any] = field(default_factory=dict)
    
    # AI ì¶”ë¡  ìš”êµ¬ì‚¬í•­
    batch_size: int = 1
    precision: str = "fp32"
    memory_limit_mb: Optional[float] = None
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)

# ==============================================
# ğŸ”¥ Central Hub ê¸°ë°˜ AI ëª¨ë¸ í´ë˜ìŠ¤
# ==============================================

class RealAIModel:
    """ì‹¤ì œ AI ì¶”ë¡ ì— ì‚¬ìš©í•  ëª¨ë¸ í´ë˜ìŠ¤ (Central Hub í˜¸í™˜)"""
    
    def __init__(self, model_name: str, model_path: str, step_type: RealStepModelType, device: str = "auto"):
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.step_type = step_type
        self.device = device if device != "auto" else DEFAULT_DEVICE
        
        # ë¡œë”© ìƒíƒœ
        self.loaded = False
        self.load_time = 0.0
        self.memory_usage_mb = 0.0
        self.checkpoint_data = None
        self.model_instance = None

        self.access_count = 0
        self.last_access = 0.0
        self.inference_count = 0
        self.avg_inference_time = 0.0
        # Central Hub í˜¸í™˜ì„ ìœ„í•œ ì†ì„±ë“¤
        self.preprocessing_params = {}
        self.model_class = None
        self.config_path = None
        
        # ê²€ì¦ ìƒíƒœ
        self.validation_passed = False
        self.compatibility_checked = False
        
        # Logger
        self.logger = logging.getLogger(f"RealAIModel.{model_name}")
        
        # Stepë³„ íŠ¹í™” ë¡œë” ë§¤í•‘ (Central Hub ê¸°ë°˜)
        self.step_loaders = {
            RealStepModelType.HUMAN_PARSING: self._load_human_parsing_model,
            RealStepModelType.POSE_ESTIMATION: self._load_pose_model,
            RealStepModelType.CLOTH_SEGMENTATION: self._load_segmentation_model,
            RealStepModelType.GEOMETRIC_MATCHING: self._load_geometric_model,
            RealStepModelType.CLOTH_WARPING: self._load_warping_model,
            RealStepModelType.VIRTUAL_FITTING: self._load_diffusion_model,
            RealStepModelType.POST_PROCESSING: self._load_enhancement_model,
            RealStepModelType.QUALITY_ASSESSMENT: self._load_quality_model
        }
        
    # ê¸°ì¡´ RealAIModelì˜ load ë©”ì„œë“œë§Œ ê°œì„ 
# (ë‹¤ë¥¸ ë©”ì„œë“œë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)

    def load(self, validate: bool = True) -> bool:
        """ëª¨ë¸ ë¡œë”© (ê°œì„ ëœ ì „ëµ ì ìš©, ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        try:
            start_time = time.time()
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.model_path.exists():
                self.logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = self.model_path.stat().st_size
            self.memory_usage_mb = file_size / (1024 * 1024)
            
            self.logger.info(f"ğŸ”„ {self.step_type.value} ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name} ({self.memory_usage_mb:.1f}MB)")
            
            # ğŸ”¥ ê°œì„ : ìŠ¤ë§ˆíŠ¸ ë¡œë”© ì „ëµ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            success = self._smart_load_with_strategy()
            
            if success:
                self.load_time = time.time() - start_time
                self.loaded = True
                
                # ê²€ì¦ ìˆ˜í–‰
                if validate:
                    self.validation_passed = self._validate_model()
                else:
                    self.validation_passed = True
                
                self.logger.info(f"âœ… {self.step_type.value} ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name} ({self.load_time:.2f}ì´ˆ)")
                return True
            else:
                self.logger.error(f"âŒ {self.step_type.value} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {self.model_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _detect_file_format(self) -> str:
        """íŒŒì¼ í˜•ì‹ ì‚¬ì „ ê°ì§€ë¡œ ì˜¬ë°”ë¥¸ ë¡œë” ì„ íƒ"""
        try:
            file_ext = self.model_path.suffix.lower()
            filename = self.model_path.name.lower()
            
            # Safetensors íŒŒì¼ í™•ì‹¤íˆ êµ¬ë¶„
            if file_ext == '.safetensors':
                return 'safetensors'
            
            # YOLO íŒŒì¼ êµ¬ë¶„
            if 'yolo' in filename or filename.endswith('-pose.pt'):
                return 'yolo'
            
            # CLIP/ViT íŒŒì¼ êµ¬ë¶„
            if 'clip' in filename or 'vit' in filename:
                return 'clip'
            
            # Diffusion ëª¨ë¸ êµ¬ë¶„
            if 'diffusion' in filename:
                return 'diffusion'
            
            # ê¸°ë³¸ PyTorch íŒŒì¼
            if file_ext in ['.pth', '.pt', '.bin']:
                return 'pytorch'
            
            return 'unknown'
            
        except Exception:
            return 'unknown'


    def _smart_load_with_strategy(self) -> bool:
        """ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ ë¡œë”© ì „ëµ (íŒŒì¼ í˜•ì‹ ê¸°ë°˜)"""
        try:
            # íŒŒì¼ í˜•ì‹ ì‚¬ì „ ê°ì§€
            file_format = self._detect_file_format()
            
            # í˜•ì‹ë³„ ìµœì í™”ëœ ë¡œë” ë§¤í•‘
            format_loaders = {
                'safetensors': self._load_safetensors,
                'yolo': self._load_yolo_optimized,
                'clip': self._load_clip_model,
                'diffusion': self._load_diffusion_checkpoint,
                'pytorch': self._load_pytorch_checkpoint
            }
            
            # 1ì°¨: í˜•ì‹ë³„ ìµœì í™” ë¡œë” ì‹œë„
            if file_format in format_loaders:
                self.logger.debug(f"íŒŒì¼ í˜•ì‹ ê°ì§€: {file_format}")
                loader_func = format_loaders[file_format]
                
                if file_format == 'safetensors':
                    self.checkpoint_data = loader_func()
                    return self.checkpoint_data is not None
                else:
                    return loader_func()
            
            # 2ì°¨: Stepë³„ íŠ¹í™” ë¡œë” ì‹œë„
            if self.step_type in self.step_loaders:
                self.logger.debug(f"Stepë³„ íŠ¹í™” ë¡œë” ì‹œë„: {self.step_type}")
                return self.step_loaders[self.step_type]()
            
            # 3ì°¨: ì¼ë°˜ ë¡œë”
            self.logger.debug("ì¼ë°˜ PyTorch ë¡œë” ì‹œë„")
            return self._load_generic_model()
            
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤ë§ˆíŠ¸ ë¡œë”© ì „ëµ ì‹¤íŒ¨: {e}")
            return False

    def _load_pytorch_checkpoint(self) -> Optional[Any]:
        """ğŸ”¥ ê°œì„ ëœ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        if not TORCH_AVAILABLE:
            self.logger.error("âŒ PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
            return None
        
        loading_methods = [
            ('safe_mode', {'weights_only': True}),
            ('compat_mode', {'weights_only': False}),
            ('legacy_mode', {})
        ]
        
        for method_name, kwargs in loading_methods:
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    checkpoint = torch.load(
                        self.model_path, 
                        map_location='cpu',
                        **kwargs
                    )
                self.logger.debug(f"âœ… {method_name} ë¡œë”© ì„±ê³µ: {self.model_name}")
                return checkpoint
            except Exception as e:
                self.logger.debug(f"{method_name} ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.error(f"âŒ ëª¨ë“  PyTorch ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {self.model_name}")
        return None

    def _load_yolo_optimized(self) -> bool:
        """YOLO ëª¨ë¸ ìµœì í™” ë¡œë”© (Ultralytics ì˜ì¡´ì„± í•´ê²°)"""
        try:
            # 1. Ultralytics ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜
            try:
                from ultralytics import YOLO
            except ImportError:
                self.logger.warning("âš ï¸ Ultralytics ë¯¸ì„¤ì¹˜, ìë™ ì„¤ì¹˜ ì‹œë„")
                try:
                    import subprocess
                    subprocess.check_call(['pip', 'install', 'ultralytics'])
                    from ultralytics import YOLO
                    self.logger.info("âœ… Ultralytics ìë™ ì„¤ì¹˜ ì™„ë£Œ")
                except Exception as install_error:
                    self.logger.error(f"âŒ Ultralytics ì„¤ì¹˜ ì‹¤íŒ¨: {install_error}")
                    return False
            
            # 2. YOLO ëª¨ë¸ ë¡œë”©
            try:
                model = YOLO(str(self.model_path))
                self.model_instance = model
                self.checkpoint_data = {"ultralytics_model": model}
                self.logger.debug(f"âœ… YOLO Ultralytics ë¡œë”© ì„±ê³µ: {self.model_name}")
                return True
            except Exception as yolo_error:
                self.logger.error(f"âŒ YOLO ë¡œë”© ì‹¤íŒ¨: {yolo_error}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ YOLO ìµœì í™” ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _load_pytorch_checkpoint(self) -> Optional[Any]:
        """PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (MPS float64 ë¬¸ì œ í•´ê²°)"""
        if not TORCH_AVAILABLE:
            self.logger.error("âŒ PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
            return None
        
        try:
            filename = self.model_path.name.lower()
            
            # MPS float64 ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ CPU ìš°ì„  ë¡œë”©
            loading_methods = [
                ('safe_mode', {'weights_only': True, 'map_location': 'cpu'}),
                ('compat_mode', {'weights_only': False, 'map_location': 'cpu'}),
                ('legacy_mode', {'map_location': 'cpu'})
            ]
            
            for method_name, kwargs in loading_methods:
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        checkpoint = torch.load(self.model_path, **kwargs)
                    
                    # MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        checkpoint = self._convert_float64_to_float32(checkpoint)
                    
                    self.logger.debug(f"âœ… {method_name} ë¡œë”© ì„±ê³µ: {self.model_name}")
                    return checkpoint
                    
                except Exception as e:
                    self.logger.debug(f"{method_name} ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.error(f"âŒ ëª¨ë“  PyTorch ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {self.model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _convert_float64_to_float32(self, checkpoint: Any) -> Any:
        """MPSìš© float64 â†’ float32 ë³€í™˜"""
        if isinstance(checkpoint, dict):
            return {k: self._convert_float64_to_float32(v) for k, v in checkpoint.items()}
        elif isinstance(checkpoint, torch.Tensor) and checkpoint.dtype == torch.float64:
            return checkpoint.float()
        else:
            return checkpoint
        
    # ğŸ”¥ ê¸°ì¡´ Stepë³„ ë¡œë”ë“¤ë„ ì•½ê°„ ê°œì„ 
    def _load_warping_model(self) -> bool:
        """Cloth Warping ëª¨ë¸ ë¡œë”© (ìˆœì„œ ê°œì„ )"""
        try:
            # ğŸ”¥ ê°œì„ : Safetensors íŒŒì¼ì€ ë°”ë¡œ Safetensors ë¡œë”©
            if self.model_path.suffix.lower() == '.safetensors':
                self.logger.debug(f"Safetensors íŒŒì¼ ê°ì§€: {self.model_name}")
                self.checkpoint_data = self._load_safetensors()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Cloth Warping ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _load_diffusion_model(self) -> bool:
        """Virtual Fitting ëª¨ë¸ ë¡œë”© (ìˆœì„œ ê°œì„ )"""
        try:
            # ğŸ”¥ ê°œì„ : Safetensors íŒŒì¼ì€ ë°”ë¡œ Safetensors ë¡œë”©
            if self.model_path.suffix.lower() == '.safetensors':
                self.logger.debug(f"Safetensors íŒŒì¼ ê°ì§€: {self.model_name}")
                self.checkpoint_data = self._load_safetensors()
            elif "diffusion" in self.model_name.lower():
                self.checkpoint_data = self._load_diffusion_checkpoint()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Virtual Fitting ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False


    def _load_human_parsing_model(self) -> bool:
        """Human Parsing ëª¨ë¸ ë¡œë”© (Graphonomy, ATR ë“±) - Central Hub í˜¸í™˜"""
        try:
            # Graphonomy íŠ¹ë³„ ì²˜ë¦¬ (170.5MB - fix_checkpoints.py ê²€ì¦ë¨)
            if "graphonomy" in self.model_name.lower():
                return self._load_graphonomy_ultra_safe()
            
            # ATR ëª¨ë¸ ì²˜ë¦¬
            if "atr" in self.model_name.lower() or "schp" in self.model_name.lower():
                return self._load_atr_model()
            
            # ì¼ë°˜ PyTorch ëª¨ë¸
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Human Parsing ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_pose_model(self) -> bool:
        """Pose Estimation ëª¨ë¸ ë¡œë”© (YOLO, OpenPose ë“±) - Central Hub í˜¸í™˜"""
        try:
            # YOLO ëª¨ë¸ ì²˜ë¦¬
            if "yolo" in self.model_name.lower():
                self.checkpoint_data = self._load_yolo_model()
            # OpenPose ëª¨ë¸ ì²˜ë¦¬
            elif "openpose" in self.model_name.lower() or "pose" in self.model_name.lower():
                self.checkpoint_data = self._load_openpose_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Pose Estimation ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_segmentation_model(self) -> bool:
        """Segmentation ëª¨ë¸ ë¡œë”© (SAM, U2Net ë“±) - Central Hub í˜¸í™˜"""
        try:
            # SAM ëª¨ë¸ ì²˜ë¦¬ (2445.7MB - fix_checkpoints.py ê²€ì¦ë¨)
            if "sam" in self.model_name.lower():
                self.checkpoint_data = self._load_sam_model()
            # U2Net ëª¨ë¸ ì²˜ë¦¬ (38.8MB - fix_checkpoints.py ê²€ì¦ë¨)
            elif "u2net" in self.model_name.lower():
                self.checkpoint_data = self._load_u2net_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Segmentation ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_geometric_model(self) -> bool:
        """Geometric Matching ëª¨ë¸ ë¡œë”© - Central Hub í˜¸í™˜"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Geometric Matching ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_enhancement_model(self) -> bool:
        """Post Processing ëª¨ë¸ ë¡œë”© (Real-ESRGAN ë“±) - Central Hub í˜¸í™˜"""
        try:
            # Real-ESRGAN íŠ¹ë³„ ì²˜ë¦¬
            if "esrgan" in self.model_name.lower():
                self.checkpoint_data = self._load_esrgan_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Post Processing ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_quality_model(self) -> bool:
        """Quality Assessment ëª¨ë¸ ë¡œë”© (CLIP, ViT ë“±) - Central Hub í˜¸í™˜"""
        try:
            # CLIP ëª¨ë¸ ì²˜ë¦¬ (5213.7MB - fix_checkpoints.py ê²€ì¦ë¨)
            if "clip" in self.model_name.lower() or "vit" in self.model_name.lower():
                self.checkpoint_data = self._load_clip_model()
            else:
                self.checkpoint_data = self._load_pytorch_checkpoint()
            
            return self.checkpoint_data is not None
            
        except Exception as e:
            self.logger.error(f"âŒ Quality Assessment ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_generic_model(self) -> bool:
        """ì¼ë°˜ ëª¨ë¸ ë¡œë”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"âŒ ì¼ë°˜ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ íŠ¹í™” ë¡œë”ë“¤ (fix_checkpoints.py ê²€ì¦ ê²°ê³¼ ê¸°ë°˜)
    # ==============================================
    
    def _load_safetensors(self) -> Optional[Any]:
        """Safetensors íŒŒì¼ ë¡œë”© (PyTorch ì‹œë„ ë°©ì§€)"""
        try:
            import safetensors.torch
            
            # Safetensors ì „ìš© ë¡œë”© (PyTorch ì‹œë„ ì•ˆí•¨)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = safetensors.torch.load_file(
                    str(self.model_path),
                    device='cpu'  # CPUì—ì„œ ì•ˆì „í•˜ê²Œ ë¡œë”©
                )
            
            self.logger.debug(f"âœ… Safetensors ì „ìš© ë¡œë”© ì„±ê³µ: {self.model_name}")
            return checkpoint
            
        except ImportError:
            self.logger.error("âŒ Safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìˆ˜ ì„¤ì¹˜ í•„ìš”")
            return None
        except Exception as e:
            self.logger.error(f"âŒ Safetensors ë¡œë”© ì‹¤íŒ¨: {e}")
            return None


    def _load_graphonomy_ultra_safe(self) -> bool:
        """Graphonomy 170.5MB ëª¨ë¸ ì´ˆì•ˆì „ ë¡œë”© (Central Hub ê¸°ë°˜)"""
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # ë©”ëª¨ë¦¬ ë§¤í•‘ ë°©ë²•
                try:
                    with open(self.model_path, 'rb') as f:
                        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
                            checkpoint = torch.load(
                                BytesIO(mmapped_file[:]), 
                                map_location='cpu',
                                weights_only=False
                            )
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("âœ… Graphonomy ë©”ëª¨ë¦¬ ë§¤í•‘ ë¡œë”© ì„±ê³µ")
                    return True
                    
                except Exception:
                    pass
                
                # ì§ì ‘ pickle ë¡œë”©
                try:
                    with open(self.model_path, 'rb') as f:
                        checkpoint = pickle.load(f)
                    
                    self.checkpoint_data = checkpoint
                    self.logger.info("âœ… Graphonomy ì§ì ‘ pickle ë¡œë”© ì„±ê³µ")
                    return True
                    
                except Exception:
                    pass
                
                # í´ë°±: ì¼ë°˜ PyTorch ë¡œë”©
                self.checkpoint_data = self._load_pytorch_checkpoint()
                return self.checkpoint_data is not None
                
        except Exception as e:
            self.logger.error(f"âŒ Graphonomy ì´ˆì•ˆì „ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_atr_model(self) -> bool:
        """ATR/SCHP ëª¨ë¸ ë¡œë”©"""
        try:
            self.checkpoint_data = self._load_pytorch_checkpoint()
            return self.checkpoint_data is not None
        except Exception as e:
            self.logger.error(f"âŒ ATR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_yolo_model(self) -> Optional[Any]:
        """YOLO ëª¨ë¸ ë¡œë”©"""
        try:
            # YOLOv8 ëª¨ë¸ì¸ ê²½ìš°
            if "v8" in self.model_name.lower():
                try:
                    from ultralytics import YOLO
                    model = YOLO(str(self.model_path))
                    self.model_instance = model
                    return {"model": model, "type": "yolov8"}
                except ImportError:
                    pass
            
            # ì¼ë°˜ PyTorch ëª¨ë¸ë¡œ ë¡œë”©
            return self._load_pytorch_checkpoint()
            
        except Exception as e:
            self.logger.error(f"âŒ YOLO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_openpose_model(self) -> Optional[Any]:
        """OpenPose ëª¨ë¸ ë¡œë”©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_sam_model(self) -> Optional[Any]:
        """SAM ëª¨ë¸ ë¡œë”© (2445.7MB - fix_checkpoints.py ê²€ì¦ë¨)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            if checkpoint and isinstance(checkpoint, dict):
                if "model" in checkpoint:
                    return checkpoint
                elif "state_dict" in checkpoint:
                    return checkpoint
                else:
                    return {"model": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_u2net_model(self) -> Optional[Any]:
        """U2Net ëª¨ë¸ ë¡œë”© (38.8MB - fix_checkpoints.py ê²€ì¦ë¨)"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_diffusion_checkpoint(self) -> Optional[Any]:
        """Diffusion ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (3278.9MB - fix_checkpoints.py ê²€ì¦ë¨)"""
        try:
            checkpoint = self._load_pytorch_checkpoint()
            
            # Diffusion ëª¨ë¸ êµ¬ì¡° ì •ê·œí™”
            if checkpoint and isinstance(checkpoint, dict):
                if "state_dict" in checkpoint:
                    return checkpoint
                elif "model" in checkpoint:
                    return checkpoint
                else:
                    return {"state_dict": checkpoint}
            
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_esrgan_model(self) -> Optional[Any]:
        """Real-ESRGAN ëª¨ë¸ ë¡œë”©"""
        try:
            return self._load_pytorch_checkpoint()
        except Exception as e:
            self.logger.error(f"âŒ Real-ESRGAN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _load_clip_model(self) -> Optional[Any]:
        """CLIP ëª¨ë¸ ë¡œë”© (MPS float64 ì˜¤ë¥˜ í•´ê²°)"""
        try:
            # MPS float64 ë¬¸ì œ í•´ê²°: CPUë¡œ ë¨¼ì € ë¡œë”© í›„ ë³€í™˜
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # CPUì—ì„œ ë¡œë”©
                checkpoint = torch.load(
                    self.model_path, 
                    map_location='cpu',  # ê°•ì œë¡œ CPU ì‚¬ìš©
                    weights_only=False   # CLIP ëª¨ë¸ì€ ë³µì¡í•œ êµ¬ì¡°ì´ë¯€ë¡œ False
                )
                
                # MPS ë””ë°”ì´ìŠ¤ë¼ë©´ float32ë¡œ ë³€í™˜
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    checkpoint = self._convert_float64_to_float32(checkpoint)
            
            self.logger.debug(f"âœ… CLIP ëª¨ë¸ MPS í˜¸í™˜ ë¡œë”© ì„±ê³µ: {self.model_name}")
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"âŒ CLIP ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _convert_float64_to_float32(self, checkpoint: Any) -> Any:
        """MPSìš© float64 â†’ float32 ë³€í™˜ (ì¬ê·€ì  ì²˜ë¦¬)"""
        if isinstance(checkpoint, dict):
            converted = {}
            for key, value in checkpoint.items():
                converted[key] = self._convert_float64_to_float32(value)
            return converted
        elif isinstance(checkpoint, torch.Tensor) and checkpoint.dtype == torch.float64:
            return checkpoint.float()  # float64 â†’ float32
        elif isinstance(checkpoint, list):
            return [self._convert_float64_to_float32(item) for item in checkpoint]
        elif isinstance(checkpoint, tuple):
            return tuple(self._convert_float64_to_float32(item) for item in checkpoint)
        else:
            return checkpoint


    def _validate_model(self) -> bool:
        """ëª¨ë¸ ê²€ì¦"""
        try:
            if self.checkpoint_data is None:
                return False
            
            # ê¸°ë³¸ ê²€ì¦
            if not isinstance(self.checkpoint_data, (dict, torch.nn.Module)) and self.checkpoint_data is not None:
                self.logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì²´í¬í¬ì¸íŠ¸ íƒ€ì…: {type(self.checkpoint_data)}")
            
            # Stepë³„ íŠ¹í™” ê²€ì¦
            if self.step_type == RealStepModelType.HUMAN_PARSING:
                return self._validate_human_parsing_model()
            elif self.step_type == RealStepModelType.VIRTUAL_FITTING:
                return self._validate_diffusion_model()
            else:
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _validate_human_parsing_model(self) -> bool:
        """Human Parsing ëª¨ë¸ ê²€ì¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    expected_keys = ["backbone", "decoder", "classifier"]
                    for key in expected_keys:
                        if any(key in k for k in state_dict.keys()):
                            return True
                
                if any("conv" in k or "bn" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Human Parsing ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return True
    
    def _validate_diffusion_model(self) -> bool:
        """Diffusion ëª¨ë¸ ê²€ì¦"""
        try:
            if isinstance(self.checkpoint_data, dict):
                if "state_dict" in self.checkpoint_data:
                    state_dict = self.checkpoint_data["state_dict"]
                    if any("down_blocks" in k or "up_blocks" in k for k in state_dict.keys()):
                        return True
                
                if any("time_embed" in k or "input_blocks" in k for k in self.checkpoint_data.keys()):
                    return True
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Diffusion ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return True
    
    # ==============================================
    # ğŸ”¥ Central Hub í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_checkpoint_data(self) -> Optional[Any]:
        """ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë°˜í™˜ (Central Hub í˜¸í™˜)"""
        return self.checkpoint_data
    
    def get_model_instance(self) -> Optional[Any]:
        """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Central Hub í˜¸í™˜)"""
        return self.model_instance
    
    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (Central Hub í˜¸í™˜)"""
        self.loaded = False
        self.checkpoint_data = None
        self.model_instance = None
        gc.collect()
        
        # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (Central Hub MemoryManager ì—°ë™)
        if MPS_AVAILABLE and TORCH_AVAILABLE:
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except:
                pass
    
    def get_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (Central Hub í˜¸í™˜)"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "step_type": self.step_type.value,
            "device": self.device,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "memory_usage_mb": self.memory_usage_mb,
            "file_exists": self.model_path.exists(),
            "file_size_mb": self.model_path.stat().st_size / (1024 * 1024) if self.model_path.exists() else 0,
            "has_checkpoint_data": self.checkpoint_data is not None,
            "has_model_instance": self.model_instance is not None,
            "validation_passed": self.validation_passed,
            "compatibility_checked": self.compatibility_checked,
            
            # Central Hub í˜¸í™˜ ì¶”ê°€ í•„ë“œ
            "model_type": getattr(self, 'model_type', 'BaseModel'),
            "size_gb": self.memory_usage_mb / 1024 if self.memory_usage_mb > 0 else 0,
            "requires_checkpoint": True,
            "preprocessing_required": getattr(self, 'preprocessing_required', []),
            "postprocessing_required": getattr(self, 'postprocessing_required', [])
        }

# ==============================================
# ğŸ”¥ Central Hub ê¸°ë°˜ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
# ==============================================

class RealStepModelInterface:
    """Central Hub ì™„ì „ í˜¸í™˜ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, model_loader, step_name: str, step_type: RealStepModelType):
        self.model_loader = model_loader
        self.step_name = step_name
        self.step_type = step_type
        self.logger = logging.getLogger(f"RealStepInterface.{step_name}")
        
        # Stepë³„ ëª¨ë¸ë“¤ (Central Hub í˜¸í™˜)
        self.step_models: Dict[str, RealAIModel] = {}
        self.primary_model: Optional[RealAIModel] = None
        self.fallback_models: List[RealAIModel] = []
        
        # Central Hub ìš”êµ¬ì‚¬í•­ ì—°ë™
        self.requirements: Optional[RealStepModelRequirement] = None
        self.data_specs_loaded: bool = False
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (Central Hub í˜¸í™˜)
        self.creation_time = time.time()
        self.access_count = 0
        self.error_count = 0
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        # ìºì‹œ (Central Hub í˜¸í™˜)
        self.model_cache: Dict[str, Any] = {}
        self.preprocessing_cache: Dict[str, Any] = {}
        
        # Central Hub í†µê³„ í˜¸í™˜
        self.real_statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'real_checkpoints_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'creation_time': time.time(),
            'central_hub_integrated': True
        }
    
    def register_requirements(self, requirements: Dict[str, Any]):
        """Central Hub DetailedDataSpec ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            self.requirements = RealStepModelRequirement(
                step_name=self.step_name,
                step_id=requirements.get('step_id', 0),
                step_type=self.step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.data_specs_loaded = True
            self.logger.info(f"âœ… Central Hub í˜¸í™˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {len(self.requirements.required_models)}ê°œ í•„ìˆ˜ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ì‹¤ì œ AI ëª¨ë¸ ë°˜í™˜ (Central Hub í˜¸í™˜)"""
        try:
            self.access_count += 1
            
            # íŠ¹ì • ëª¨ë¸ ìš”ì²­
            if model_name:
                if model_name in self.step_models:
                    model = self.step_models[model_name]
                    model.access_count += 1
                    model.last_access = time.time()
                    self.real_statistics['cache_hits'] += 1
                    return model
                
                # ìƒˆ ëª¨ë¸ ë¡œë”©
                return self._load_new_model(model_name)
            
            # ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜ (Central Hub í˜¸í™˜)
            if self.primary_model and self.primary_model.loaded:
                return self.primary_model
            
            # ë¡œë“œëœ ëª¨ë¸ ì¤‘ ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒ
            for model in sorted(self.step_models.values(), key=lambda m: getattr(m, 'priority', 999)):
                if model.loaded:
                    return model
            
            # ì²« ë²ˆì§¸ ëª¨ë¸ ë¡œë”© ì‹œë„
            if self.requirements and self.requirements.required_models:
                return self._load_new_model(self.requirements.required_models[0])
            
            return None
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_new_model(self, model_name: str) -> Optional[RealAIModel]:
        """ìƒˆ ëª¨ë¸ ë¡œë”© (Central Hub í˜¸í™˜)"""
        try:
            # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
            base_model = self.model_loader.load_model(model_name, step_name=self.step_name, step_type=self.step_type)
            
            if base_model and isinstance(base_model, RealAIModel):
                self.step_models[model_name] = base_model
                
                # Primary ëª¨ë¸ ì„¤ì •
                if not self.primary_model or (self.requirements and model_name == self.requirements.primary_model):
                    self.primary_model = base_model
                
                # í†µê³„ ì—…ë°ì´íŠ¸ (Central Hub í˜¸í™˜)
                self.real_statistics['models_loaded'] += 1
                self.real_statistics['real_ai_calls'] += 1
                if base_model.checkpoint_data is not None:
                    self.real_statistics['real_checkpoints_loaded'] += 1
                
                return base_model
            else:
                self.real_statistics['cache_misses'] += 1
                self.real_statistics['loading_failures'] += 1
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.real_statistics['loading_failures'] += 1
            return None
    
    def get_model_sync(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ë™ê¸° ëª¨ë¸ ì¡°íšŒ - Central Hub BaseStepMixin í˜¸í™˜"""
        return self.get_model(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ (Central Hub í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - Central Hub BaseStepMixin í˜¸í™˜"""
        try:
            if not hasattr(self, 'model_requirements'):
                self.model_requirements = {}
            
            self.model_requirements[model_name] = {
                'model_type': model_type,
                'step_type': self.step_type.value,
                'required': kwargs.get('required', True),
                'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                'device': kwargs.get('device', DEFAULT_DEVICE),
                'preprocessing_params': kwargs.get('preprocessing_params', {}),
                **kwargs
            }
            
            self.real_statistics['models_registered'] += 1
            self.logger.info(f"âœ… Central Hub í˜¸í™˜ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (Central Hub í˜¸í™˜)"""
        try:
            return self.model_loader.list_available_models(step_class, model_type)
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (Central Hub í˜¸í™˜)"""
        try:
            # ë©”ëª¨ë¦¬ í•´ì œ
            for model_name, model in self.step_models.items():
                if hasattr(model, 'unload'):
                    model.unload()
            
            self.step_models.clear()
            self.model_cache.clear()
            
            self.logger.info(f"âœ… Central Hub í˜¸í™˜ {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
EnhancedStepModelInterface = RealStepModelInterface
StepModelInterface = RealStepModelInterface

# ==============================================
# ğŸ”¥ ModelLoader v5.1 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
# ==============================================

from functools import wraps
from typing import Callable, Any, Optional

def safe_execution(fallback_value: Any = None, log_error: bool = True):
    """ì•ˆì „í•œ ì‹¤í–‰ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            try:
                return func(self, *args, **kwargs)
            except Exception as e:
                if log_error and hasattr(self, 'logger'):
                    self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if hasattr(self, 'performance_metrics'):
                    self.performance_metrics['error_count'] += 1
                
                return fallback_value
        return wrapper
    return decorator

def safe_async_execution(fallback_value: Any = None, log_error: bool = True):
    """ë¹„ë™ê¸° ì•ˆì „í•œ ì‹¤í–‰ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            try:
                return await func(self, *args, **kwargs)
            except Exception as e:
                if log_error and hasattr(self, 'logger'):
                    self.logger.error(f"âŒ {func.__name__} ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if hasattr(self, 'performance_metrics'):
                    self.performance_metrics['error_count'] += 1
                
                return fallback_value
        return wrapper
    return decorator


class ModelLoader:
    """
    ModelLoader v5.1 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import
    âœ… inject_to_step() ë©”ì„œë“œ êµ¬í˜„ - Stepì— ModelLoader ìë™ ì£¼ì…
    âœ… create_step_interface() ë©”ì„œë“œ ê°œì„  - Central Hub ê¸°ë°˜
    âœ… register_step_requirements() ë©”ì„œë“œ ì¶”ê°€ - Step ìš”êµ¬ì‚¬í•­ ë“±ë¡
    âœ… validate_di_container_integration() ì™„ì „ ê°œì„  - ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” - Central Hub MemoryManager ì—°ë™
    âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥
    """
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 enable_optimization: bool = True,
                 **kwargs):
        """ModelLoader ì´ˆê¸°í™” (Central Hub DI Container v7.0 ì™„ì „ ì—°ë™)"""
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # ğŸ”¥ Central Hub DI Container ì§€ì—° ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        self._central_hub_container = None
        self._container_initialized = False
        
        # ğŸ”¥ ì˜ì¡´ì„±ë“¤ (Central Hubë¥¼ í†µí•´ ì£¼ì…ë°›ìŒ)
        self.memory_manager = None
        self.data_converter = None
        
        # ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (Central Hub AI_MODELS_ROOT í˜¸í™˜)
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            # Central Hub AI_MODELS_ROOT ê²½ë¡œ ë§¤í•‘
            current_file = Path(__file__)
            backend_root = current_file.parents[3]  # backend/
            self.model_cache_dir = backend_root / "ai_models"
            
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤ì œ AI ëª¨ë¸ ê´€ë¦¬ (Central Hub í˜¸í™˜)
        self.loaded_models: Dict[str, RealAIModel] = {}
        self.model_info: Dict[str, RealStepModelInfo] = {}
        self.model_status: Dict[str, RealModelStatus] = {}
        
        # Step ìš”êµ¬ì‚¬í•­ (Central Hub í˜¸í™˜)
        self.step_requirements: Dict[str, RealStepModelRequirement] = {}
        self.step_interfaces: Dict[str, RealStepModelInterface] = {}
        
        # auto_model_detector ì—°ë™
        self.auto_detector = None
        self._available_models_cache: Dict[str, Any] = {}
        self._integration_successful = False
        self._initialize_auto_detector()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (Central Hub í˜¸í™˜)
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'total_memory_mb': 0.0,
            'error_count': 0,
            'inference_count': 0,
            'total_inference_time': 0.0,
            'central_hub_injections': 0,
            'step_requirements_registered': 0
        }
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ModelLoader")
        
        # Central Hub Step ë§¤í•‘ ë¡œë”©
        self._load_central_hub_step_mappings()
        
        # ğŸ”¥ Central Hub DI Container ì—°ë™ ì´ˆê¸°í™”
        self._initialize_central_hub_integration()
        
        self.logger.info(f"ğŸš€ ModelLoader v5.1 Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“± Device: {self.device} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
        self.logger.info(f"ğŸ“ ëª¨ë¸ ìºì‹œ: {self.model_cache_dir}")
        self.logger.info(f"ğŸ¯ Central Hub AI Step í˜¸í™˜ ëª¨ë“œ")
    
    def _initialize_central_hub_integration(self):
        """ğŸ”¥ Central Hub DI Container ì—°ë™ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # Central Hub Container ì§€ì—° ì´ˆê¸°í™”
            self._central_hub_container = _get_central_hub_container()
            self._container_initialized = True
            
            if self._central_hub_container:
                self.logger.info("âœ… Central Hub DI Container ì—°ê²° ì„±ê³µ")
                
                # ğŸ”¥ ìê¸° ìì‹ ì„ Central Hubì— ë“±ë¡
                try:
                    if hasattr(self._central_hub_container, 'register'):
                        self._central_hub_container.register('model_loader', self)
                        self.logger.info("âœ… ModelLoader Central Hub ë“±ë¡ ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"ModelLoader Central Hub ë“±ë¡ ì‹¤íŒ¨: {e}")
                
                # ğŸ”¥ Central Hubë¡œë¶€í„° ì˜ì¡´ì„±ë“¤ ì¡°íšŒ
                self._resolve_dependencies_from_central_hub()
                
            else:
                self.logger.warning("âš ï¸ Central Hub DI Container ì—°ê²° ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.error(f"âŒ Central Hub ì—°ë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _resolve_dependencies_from_central_hub(self):
        """ğŸ”¥ Central Hubë¡œë¶€í„° ì˜ì¡´ì„±ë“¤ ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            if self._central_hub_container:
                # MemoryManager ì¡°íšŒ
                self.memory_manager = _get_service_from_central_hub('memory_manager')
                if self.memory_manager:
                    self.logger.debug("âœ… Central Hubë¡œë¶€í„° MemoryManager ì¡°íšŒ ì„±ê³µ")
                
                # DataConverter ì¡°íšŒ
                self.data_converter = _get_service_from_central_hub('data_converter')
                if self.data_converter:
                    self.logger.debug("âœ… Central Hubë¡œë¶€í„° DataConverter ì¡°íšŒ ì„±ê³µ")
                
                # ì‹œìŠ¤í…œ ì •ë³´ë„ Central Hubë¡œë¶€í„°
                self.device_info = _get_service_from_central_hub('device') or self.device
                self.memory_gb = _get_service_from_central_hub('memory_gb') or MEMORY_GB
                self.is_m3_max = _get_service_from_central_hub('is_m3_max') or IS_M3_MAX
                
                self.logger.debug("âœ… Central Hub ì˜ì¡´ì„± í•´ê²° ì™„ë£Œ")
                
        except Exception as e:
            self.logger.debug(f"âš ï¸ Central Hub ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ Central Hub í•µì‹¬ ë©”ì„œë“œë“¤ (ìƒˆë¡œ ì¶”ê°€)
    # ==============================================
    
    def inject_to_step(self, step_instance) -> int:
        """ğŸ”¥ Stepì— ModelLoader ì£¼ì… (Central Hub ì§€ì›)"""
        try:
            injections_made = 0
            
            # ModelLoader ìì²´ ì£¼ì…
            if hasattr(step_instance, 'model_loader'):
                step_instance.model_loader = self
                injections_made += 1
                self.logger.debug(f"âœ… ModelLoader ì£¼ì…: {step_instance.__class__.__name__}")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì…
            if hasattr(step_instance, 'step_name'):
                try:
                    step_interface = self.create_step_interface(step_instance.step_name)
                    if hasattr(step_instance, 'model_interface'):
                        step_instance.model_interface = step_interface
                        injections_made += 1
                        self.logger.debug(f"âœ… Step ì¸í„°í˜ì´ìŠ¤ ì£¼ì…: {step_instance.step_name}")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡
            if hasattr(step_instance, 'step_name') and hasattr(step_instance, 'step_id'):
                step_requirements = self._get_step_requirements_from_instance(step_instance)
                if step_requirements:
                    success = self.register_step_requirements(step_instance.step_name, step_requirements)
                    if success:
                        injections_made += 1
                        self.logger.debug(f"âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_instance.step_name}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if injections_made > 0:
                self.performance_metrics['central_hub_injections'] += injections_made
            
            self.logger.info(f"âœ… Step ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injections_made}ê°œ")
            return injections_made
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return 0
    
    def register_step_requirements(self, step_name: str, requirements: Dict[str, Any]) -> bool:
        """ğŸ”¥ Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡ (Central Hub ì§€ì›)"""
        try:
            step_type = requirements.get('step_type')
            if isinstance(step_type, str):
                step_type = RealStepModelType(step_type)
            elif not step_type:
                step_type = self._infer_step_type_from_name(step_name)
            
            self.step_requirements[step_name] = RealStepModelRequirement(
                step_name=step_name,
                step_id=requirements.get('step_id', self._get_step_id(step_name)),
                step_type=step_type,
                required_models=requirements.get('required_models', []),
                optional_models=requirements.get('optional_models', []),
                primary_model=requirements.get('primary_model'),
                model_configs=requirements.get('model_configs', {}),
                input_data_specs=requirements.get('input_data_specs', {}),
                output_data_specs=requirements.get('output_data_specs', {}),
                batch_size=requirements.get('batch_size', 1),
                precision=requirements.get('precision', 'fp32'),
                memory_limit_mb=requirements.get('memory_limit_mb'),
                preprocessing_required=requirements.get('preprocessing_required', []),
                postprocessing_required=requirements.get('postprocessing_required', [])
            )
            
            self.performance_metrics['step_requirements_registered'] += 1
            self.logger.info(f"âœ… Central Hub Step ìš”êµ¬ì‚¬í•­ ë“±ë¡: {step_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨ {step_name}: {e}")
            return False
    
    def validate_di_container_integration(self) -> Dict[str, Any]:
        """ğŸ”¥ Central Hub DI Container ì—°ë™ ìƒíƒœ ê²€ì¦ (ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ í¬í•¨)"""
        try:
            validation_result = {
                'di_container_available': self._central_hub_container is not None,
                'registered_in_container': False,
                'can_inject_to_steps': hasattr(self, 'inject_to_step'),
                'container_stats': {},
                'checkpoint_loading_ready': False,
                'central_hub_integrated': True,
                'memory_optimization_available': False,
                'step_requirements_support': True
            }
            
            if self._central_hub_container:
                # Containerì— ë“±ë¡ í™•ì¸
                model_loader_from_container = _get_service_from_central_hub('model_loader')
                validation_result['registered_in_container'] = model_loader_from_container is not None
                
                # Container í†µê³„
                if hasattr(self._central_hub_container, 'get_stats'):
                    validation_result['container_stats'] = self._central_hub_container.get_stats()
                
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ (ì‹¤ì œ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸)
                validation_result['checkpoint_loading_ready'] = self._validate_checkpoint_loading()
                
                # MemoryManager ì—°ë™ í™•ì¸
                validation_result['memory_optimization_available'] = self.memory_manager is not None
            
            # ì¶”ê°€ Central Hub ê¸°ëŠ¥ ê²€ì¦
            validation_result.update({
                'loaded_models_count': len(self.loaded_models),
                'step_interfaces_count': len(self.step_interfaces),
                'step_requirements_count': len(self.step_requirements),
                'auto_detector_integrated': self._integration_successful,
                'available_models_count': len(self._available_models_cache),
                'central_hub_injections': self.performance_metrics['central_hub_injections'],
                'device_optimized': self.device in ['mps', 'cuda'] if TORCH_AVAILABLE else False,
                'm3_max_optimized': IS_M3_MAX and MPS_AVAILABLE,
                'conda_environment': CONDA_INFO['conda_env'],
                'target_environment': CONDA_INFO['is_target_env']
            })
            
            return validation_result
            
        except Exception as e:
            return {
                'error': str(e), 
                'di_container_available': False,
                'central_hub_integrated': True,
                'checkpoint_loading_ready': False
            }
    
    def _validate_checkpoint_loading(self) -> bool:
        """ğŸ”¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ (fix_checkpoints.py ê¸°ë°˜)"""
        try:
            # ê²€ì¦ëœ ëª¨ë¸ ê²½ë¡œë“¤ í…ŒìŠ¤íŠ¸ (fix_checkpoints.py ê²€ì¦ ê²°ê³¼)
            test_models = [
                ('graphonomy.pth', 'checkpoints/step_01_human_parsing/graphonomy.pth'),
                ('sam_vit_h_4b8939.pth', 'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth'),
                ('u2net_alternative.pth', 'checkpoints/step_03_cloth_segmentation/u2net_alternative.pth')
            ]
            
            validated_count = 0
            for model_name, relative_path in test_models:
                full_path = self.model_cache_dir / relative_path
                if full_path.exists():
                    # ê°„ë‹¨í•œ ë¡œë”© í…ŒìŠ¤íŠ¸
                    try:
                        if TORCH_AVAILABLE:
                            # ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë”© (ë¹ ë¥¸ ê²€ì¦)
                            with warnings.catch_warnings():
                                warnings.simplefilter("ignore")
                                checkpoint = torch.load(full_path, map_location='cpu', weights_only=True)
                            if checkpoint is not None:
                                validated_count += 1
                                self.logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ: {model_name}")
                    except:
                        # weights_only=True ì‹¤íŒ¨ ì‹œ weights_only=Falseë¡œ ì¬ì‹œë„
                        try:
                            with warnings.catch_warnings():
                                warnings.simplefilter("ignore")
                                checkpoint = torch.load(full_path, map_location='cpu', weights_only=False)
                            if checkpoint is not None:
                                validated_count += 1
                                self.logger.debug(f"âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì„±ê³µ (í˜¸í™˜ëª¨ë“œ): {model_name}")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {model_name} - {e}")
            
            # ìµœì†Œ 1ê°œ ì´ìƒ ê²€ì¦ë˜ë©´ ì„±ê³µ
            success = validated_count > 0
            self.logger.info(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦: {validated_count}/3ê°œ ì„±ê³µ, ê²°ê³¼: {'âœ…' if success else 'âŒ'}")
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_memory_via_central_hub(self) -> Dict[str, Any]:
        """ğŸ”¥ ê°œì„ ëœ Central Hub ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            optimization_result = {
                'models_unloaded': 0,
                'memory_freed_mb': 0.0,
                'cache_cleared': False,
                'mps_cache_cleared': False,
                'central_hub_optimization': False,
                'gc_collected': 0
            }
            
            # ğŸ”¥ ê°œì„ : ì²´ê³„ì ì¸ ìµœì í™” ìˆœì„œ
            optimization_steps = [
                ('central_hub_memory_manager', self._optimize_via_central_hub),
                ('unused_models_cleanup', self._cleanup_unused_models),
                ('cache_cleanup', self._cleanup_caches),
                ('system_memory_cleanup', self._cleanup_system_memory)
            ]
            
            for step_name, step_func in optimization_steps:
                try:
                    step_result = step_func()
                    if isinstance(step_result, dict):
                        for key, value in step_result.items():
                            if key in optimization_result:
                                if isinstance(value, (int, float)):
                                    optimization_result[key] += value
                                else:
                                    optimization_result[key] = value
                    self.logger.debug(f"âœ… {step_name} ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ {step_name} ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ê°œì„ : ìµœì¢… ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            optimization_result['gc_collected'] = collected
            
            self.logger.info(f"âœ… ì²´ê³„ì  ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {optimization_result}")
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def _optimize_via_central_hub(self) -> Dict[str, Any]:
        """Central Hub MemoryManagerë¥¼ í†µí•œ ìµœì í™”"""
        result = {'central_hub_optimization': False}
        
        if self.memory_manager and hasattr(self.memory_manager, 'optimize_memory'):
            try:
                memory_stats = self.memory_manager.optimize_memory(aggressive=True)
                result.update(memory_stats)
                result['central_hub_optimization'] = True
            except Exception as e:
                self.logger.debug(f"Central Hub MemoryManager ìµœì í™” ì‹¤íŒ¨: {e}")
        
        return result

    def _cleanup_unused_models(self) -> Dict[str, Any]:
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ ì •ë¦¬"""
        result = {'models_unloaded': 0, 'memory_freed_mb': 0.0}
        
        current_time = time.time()
        unused_threshold = 3600  # 1ì‹œê°„
        
        models_to_unload = []
        for model_name, model in self.loaded_models.items():
            if current_time - getattr(model, 'last_access', 0) > unused_threshold:
                models_to_unload.append(model_name)
        
        for model_name in models_to_unload:
            if self.unload_model(model_name):
                result['models_unloaded'] += 1
                result['memory_freed_mb'] += self.model_info.get(model_name, {}).get('memory_mb', 0)
        
        return result

    def _cleanup_caches(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë¦¬"""
        result = {'cache_cleared': False}
        
        # ëª¨ë¸ ìºì‹œ ì •ë¦¬
        self._available_models_cache.clear()
        
        # ì˜ì¡´ì„± ìºì‹œ ì •ë¦¬
        _clear_dependency_cache()
        
        result['cache_cleared'] = True
        return result

    def _cleanup_system_memory(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        result = {'mps_cache_cleared': False}
        
        # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max)
        if MPS_AVAILABLE and TORCH_AVAILABLE:
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    result['mps_cache_cleared'] = True
            except Exception as e:
                self.logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        return result

    def get_central_hub_stats(self) -> Dict[str, Any]:
        """ğŸ”¥ Central Hub í†µê³„ ì—°ë™"""
        try:
            stats = {
                'model_loader_stats': self.get_performance_metrics(),
                'central_hub_connection': self._central_hub_container is not None,
                'dependency_resolution': {
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None
                },
                'step_integration': {
                    'registered_step_requirements': len(self.step_requirements),
                    'active_step_interfaces': len(self.step_interfaces),
                    'total_injections': self.performance_metrics['central_hub_injections']
                }
            }
            
            # Central Hub Container í†µê³„ ì¶”ê°€
            if self._central_hub_container and hasattr(self._central_hub_container, 'get_stats'):
                stats['container_stats'] = self._central_hub_container.get_stats()
            
            return stats
            
        except Exception as e:
            return {'error': str(e)}

    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ ë©”ì„œë“œë“¤ (Central Hub í˜¸í™˜ìœ¼ë¡œ ê°œì„ )
    # ==============================================
    
    def _initialize_auto_detector(self):
        """auto_model_detector ì´ˆê¸°í™” (Central Hub í˜¸í™˜)"""
        try:
            if AUTO_DETECTOR_AVAILABLE:
                self.auto_detector = get_global_detector()
                if self.auto_detector is not None:
                    self.logger.info("âœ… auto_model_detector ì—°ë™ ì™„ë£Œ")
                    self.integrate_auto_detector()
                else:
                    self.logger.warning("âš ï¸ auto_detector ì¸ìŠ¤í„´ìŠ¤ê°€ None")
            else:
                self.logger.warning("âš ï¸ AUTO_DETECTOR_AVAILABLE = False")
                self.auto_detector = None
        except Exception as e:
            self.logger.error(f"âŒ auto_model_detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.auto_detector = None
    
    def integrate_auto_detector(self) -> bool:
        """AutoDetector í†µí•© (Central Hub í˜¸í™˜)"""
        try:
            if not AUTO_DETECTOR_AVAILABLE or not self.auto_detector:
                return False
            
            if hasattr(self.auto_detector, 'detect_all_models'):
                detected_models = self.auto_detector.detect_all_models()
                if detected_models:
                    integrated_count = 0
                    for model_name, detected_model in detected_models.items():
                        try:
                            model_path = getattr(detected_model, 'path', '')
                            if model_path and Path(model_path).exists():
                                # Step íƒ€ì… ì¶”ë¡ 
                                step_type = self._infer_step_type(model_name, model_path)
                                
                                self._available_models_cache[model_name] = {
                                    "name": model_name,
                                    "path": str(model_path),
                                    "size_mb": getattr(detected_model, 'file_size_mb', 0),
                                    "step_class": getattr(detected_model, 'step_name', 'UnknownStep'),
                                    "step_type": step_type.value if step_type else 'unknown',
                                    "model_type": self._infer_model_type(model_name),
                                    "auto_detected": True,
                                    "priority": self._infer_model_priority(model_name),
                                    # Central Hub í˜¸í™˜ í•„ë“œ
                                    "loaded": False,
                                    "step_id": self._get_step_id_from_step_type(step_type),
                                    "device": self.device,
                                    "real_ai_model": True,
                                    "central_hub_integrated": True
                                }
                                integrated_count += 1
                        except:
                            continue
                    
                    if integrated_count > 0:
                        self._integration_successful = True
                        self.logger.info(f"âœ… AutoDetector Central Hub í†µí•© ì™„ë£Œ: {integrated_count}ê°œ ëª¨ë¸")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ AutoDetector í†µí•© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_central_hub_step_mappings(self):
        """Central Hub Step ë§¤í•‘ ë¡œë”©"""
        try:
            # Central Hub Step ë§¤í•‘ êµ¬ì¡° ë°˜ì˜
            self.central_hub_step_mappings = {
                'HumanParsingStep': {
                    'step_type': RealStepModelType.HUMAN_PARSING,
                    'step_id': 1,
                    'ai_models': [
                        'graphonomy.pth',  # 170.5MB - fix_checkpoints.py ê²€ì¦ë¨
                        'exp-schp-201908301523-atr.pth'
                    ],
                    'primary_model': 'graphonomy.pth',
                    'local_paths': [
                        'checkpoints/step_01_human_parsing/graphonomy.pth'
                    ]
                },
                'PoseEstimationStep': {
                    'step_type': RealStepModelType.POSE_ESTIMATION,
                    'step_id': 2,
                    'ai_models': [
                        'diffusion_pytorch_model.safetensors'  # 1378.2MB - fix_checkpoints.py ê²€ì¦ë¨
                    ],
                    'primary_model': 'diffusion_pytorch_model.safetensors',
                    'local_paths': [
                        'step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors'
                    ]
                },
                'ClothSegmentationStep': {
                    'step_type': RealStepModelType.CLOTH_SEGMENTATION,
                    'step_id': 3,
                    'ai_models': [
                        'sam_vit_h_4b8939.pth',  # 2445.7MB - fix_checkpoints.py ê²€ì¦ë¨
                        'u2net_alternative.pth'  # 38.8MB - fix_checkpoints.py ê²€ì¦ë¨
                    ],
                    'primary_model': 'sam_vit_h_4b8939.pth',
                    'local_paths': [
                        'checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                        'checkpoints/step_03_cloth_segmentation/u2net_alternative.pth'
                    ]
                },
                'GeometricMatchingStep': {
                    'step_type': RealStepModelType.GEOMETRIC_MATCHING,
                    'step_id': 4,
                    'ai_models': [
                        'gmm_final.pth'
                    ],
                    'primary_model': 'gmm_final.pth',
                    'local_paths': [
                        'step_04_geometric_matching/gmm_final.pth'
                    ]
                },
                'ClothWarpingStep': {
                    'step_type': RealStepModelType.CLOTH_WARPING,
                    'step_id': 5,
                    'ai_models': [
                        'RealVisXL_V4.0.safetensors'  # 6616.6MB - fix_checkpoints.py ê²€ì¦ë¨
                    ],
                    'primary_model': 'RealVisXL_V4.0.safetensors',
                    'local_paths': [
                        'checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors'
                    ]
                },
                'VirtualFittingStep': {
                    'step_type': RealStepModelType.VIRTUAL_FITTING,
                    'step_id': 6,
                    'ai_models': [
                        'diffusion_pytorch_model.safetensors'  # 3278.9MB - fix_checkpoints.py ê²€ì¦ë¨ (4ê°œ íŒŒì¼)
                    ],
                    'primary_model': 'diffusion_pytorch_model.safetensors',
                    'local_paths': [
                        'step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors',
                        'step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors'
                    ]
                },
                'PostProcessingStep': {
                    'step_type': RealStepModelType.POST_PROCESSING,
                    'step_id': 7,
                    'ai_models': [
                        'Real-ESRGAN_x4plus.pth'
                    ],
                    'primary_model': 'Real-ESRGAN_x4plus.pth',
                    'local_paths': [
                        'step_07_post_processing/Real-ESRGAN_x4plus.pth'
                    ]
                },
                'QualityAssessmentStep': {
                    'step_type': RealStepModelType.QUALITY_ASSESSMENT,
                    'step_id': 8,
                    'ai_models': [
                        'open_clip_pytorch_model.bin'  # 5213.7MB - fix_checkpoints.py ê²€ì¦ë¨  
                    ],
                    'primary_model': 'open_clip_pytorch_model.bin',
                    'local_paths': [
                        'step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin'
                    ]
                }
            }
            
            self.logger.info(f"âœ… Central Hub Step ë§¤í•‘ ë¡œë”© ì™„ë£Œ: {len(self.central_hub_step_mappings)}ê°œ Step")
            
        except Exception as e:
            self.logger.error(f"âŒ Central Hub ë§¤í•‘ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.central_hub_step_mappings = {}
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤ (Central Hub í˜¸í™˜)
    # ==============================================
    
    def load_model(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (Central Hub ì™„ì „ í˜¸í™˜)"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.loaded:
                        self.performance_metrics['cache_hits'] += 1
                        model.access_count += 1
                        model.last_access = time.time()
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ì‹¤ì œ AI ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return model
                
                # ìƒˆ ëª¨ë¸ ë¡œë”©
                self.model_status[model_name] = RealModelStatus.LOADING
                
                # ëª¨ë¸ ê²½ë¡œ ë° Step íƒ€ì… ê²°ì • (Central Hub ê²½ë¡œ ê¸°ë°˜)
                model_path = self._find_model_path(model_name, **kwargs)
                if not model_path:
                    self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                    self.model_status[model_name] = RealModelStatus.ERROR
                    return None
                
                # Step íƒ€ì… ì¶”ë¡  (Central Hub í˜¸í™˜)
                step_type = kwargs.get('step_type')
                if not step_type:
                    step_type = self._infer_step_type(model_name, model_path)
                
                if not step_type:
                    step_type = RealStepModelType.HUMAN_PARSING  # ê¸°ë³¸ê°’
                
                # RealAIModel ìƒì„± ë° ë¡œë”©
                model = RealAIModel(
                    model_name=model_name,
                    model_path=model_path,
                    step_type=step_type,
                    device=self.device
                )
                
                # ëª¨ë¸ ë¡œë”© ìˆ˜í–‰
                if model.load(validate=kwargs.get('validate', True)):
                    # ìºì‹œì— ì €ì¥
                    self.loaded_models[model_name] = model
                    
                    # ëª¨ë¸ ì •ë³´ ì €ì¥ (Central Hub í˜¸í™˜)
                    priority = RealModelPriority(kwargs.get('priority', RealModelPriority.SECONDARY.value))
                    self.model_info[model_name] = RealStepModelInfo(
                        name=model_name,
                        path=model_path,
                        step_type=step_type,
                        priority=priority,
                        device=self.device,
                        memory_mb=model.memory_usage_mb,
                        loaded=True,
                        load_time=model.load_time,
                        checkpoint_data=model.checkpoint_data,
                        validation_passed=model.validation_passed,
                        access_count=1,
                        last_access=time.time(),
                        # Central Hub í˜¸í™˜ í•„ë“œ
                        model_type=kwargs.get('model_type', 'BaseModel'),
                        size_gb=model.memory_usage_mb / 1024 if model.memory_usage_mb > 0 else 0,
                        requires_checkpoint=True,
                        preprocessing_required=kwargs.get('preprocessing_required', []),
                        postprocessing_required=kwargs.get('postprocessing_required', [])
                    )
                    
                    self.model_status[model_name] = RealModelStatus.LOADED
                    self.performance_metrics['models_loaded'] += 1
                    self.performance_metrics['total_memory_mb'] += model.memory_usage_mb
                    
                    self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} ({step_type.value}, {model.memory_usage_mb:.1f}MB)")
                    
                    # ìºì‹œ í¬ê¸° ê´€ë¦¬
                    self._manage_cache()
                    
                    return model
                else:
                    self.model_status[model_name] = RealModelStatus.ERROR
                    self.performance_metrics['error_count'] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            self.model_status[model_name] = RealModelStatus.ERROR
            self.performance_metrics['error_count'] += 1
            return None
    
    async def load_model_async(self, model_name: str, **kwargs) -> Optional[RealAIModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© (Central Hub í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _find_model_path(self, model_name: str, **kwargs) -> Optional[str]:
        """
        ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° - fix_checkpoints.pyì—ì„œ ê²€ì¦ëœ ê²½ë¡œë“¤ ìš°ì„  ì‚¬ìš©
        """
        try:
            # ì§ì ‘ ê²½ë¡œ ì§€ì •ëœ ê²½ìš°
            if 'model_path' in kwargs:
                path = Path(kwargs['model_path'])
                if path.exists():
                    return str(path)
            
            # ğŸ”¥ fix_checkpoints.pyì—ì„œ ê²€ì¦ëœ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë“¤
            VERIFIED_MODEL_PATHS = {
                # Human Parsing (âœ… 170.5MB ê²€ì¦ë¨)
                "graphonomy": "checkpoints/step_01_human_parsing/graphonomy.pth",
                "graphonomy.pth": "checkpoints/step_01_human_parsing/graphonomy.pth",
                
                # Cloth Segmentation (âœ… 2445.7MB ê²€ì¦ë¨)
                "sam": "checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                "sam_vit_h_4b8939": "checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                "sam_vit_h_4b8939.pth": "checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                
                # U2Net alternative (âœ… 38.8MB ê²€ì¦ë¨)
                "u2net": "checkpoints/step_03_cloth_segmentation/u2net_alternative.pth",
                "u2net_alternative": "checkpoints/step_03_cloth_segmentation/u2net_alternative.pth",
                "u2net_alternative.pth": "checkpoints/step_03_cloth_segmentation/u2net_alternative.pth",
                
                # Cloth Warping (âœ… 6616.6MB ê²€ì¦ë¨)
                "realvis": "checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                "realvisxl": "checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                "RealVisXL_V4.0": "checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                "RealVisXL_V4.0.safetensors": "checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                
                # Virtual Fitting (âœ… 3278.9MB ê²€ì¦ë¨ - 4ê°œ íŒŒì¼)
                "diffusion_unet_vton": "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
                "diffusion_unet_garm": "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors",
                "diffusion_unet_vton_dc": "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
                "diffusion_unet_garm_dc": "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors",
                "diffusion_main": "step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors",
                
                # Quality Assessment (âœ… 5213.7MB ê²€ì¦ë¨)
                "clip": "step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin",
                "open_clip": "step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin",
                "open_clip_pytorch_model": "step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin",
                "open_clip_pytorch_model.bin": "step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin",
                
                # Stable Diffusion (âœ… 4067.6MB ê²€ì¦ë¨)
                "stable_diffusion": "checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors",
                "v1-5-pruned": "checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors",
                "v1-5-pruned-emaonly": "checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors",
                "v1-5-pruned-emaonly.safetensors": "checkpoints/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors",
                
                # Pose Estimation (âœ… 1378.2MB ê²€ì¦ë¨)
                "diffusion_pose": "step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors",
                "diffusion_pytorch_model": "step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors"
            }
            
            # ğŸ”¥ ê²€ì¦ëœ ê²½ë¡œì—ì„œ ë¨¼ì € ì°¾ê¸°
            if model_name in VERIFIED_MODEL_PATHS:
                verified_path = self.model_cache_dir / VERIFIED_MODEL_PATHS[model_name]
                if verified_path.exists():
                    self.logger.info(f"âœ… ê²€ì¦ëœ ê²½ë¡œì—ì„œ ëª¨ë¸ ë°œê²¬: {model_name} â†’ {verified_path}")
                    return str(verified_path)
            
            # ìºì‹œëœ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
            if hasattr(self, '_model_path_cache') and model_name in self._model_path_cache:
                cached_path = Path(self._model_path_cache[model_name])
                if cached_path.exists():
                    return str(cached_path)
            
            # ìºì‹œ ì´ˆê¸°í™”
            if not hasattr(self, '_model_path_cache'):
                self._model_path_cache = {}
            
            # íŒ¨í„´ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë¸ ì°¾ê¸°
            search_patterns = [
                f"**/{model_name}.pth",
                f"**/{model_name}.pt", 
                f"**/{model_name}.safetensors",
                f"**/{model_name}.bin",
                f"**/*{model_name}*.pth",
                f"**/*{model_name}*.pt"
            ]
            
            for pattern in search_patterns:
                try:
                    for found_path in self.model_cache_dir.glob(pattern):
                        if found_path.is_file() and found_path.stat().st_size > 1024:  # 1KB ì´ìƒ
                            self._model_path_cache[model_name] = str(found_path)
                            self.logger.info(f"ğŸ” íŒ¨í„´ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë¸ ë°œê²¬: {model_name} â†’ {found_path}")
                            return str(found_path)
                except Exception as e:
                    self.logger.debug(f"íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨ {pattern}: {e}")
                    continue
            
            # ëª» ì°¾ì€ ê²½ìš°
            self.logger.warning(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _manage_cache(self):
        """ğŸ”¥ ê°œì„ ëœ ì‹¤ì œ AI ëª¨ë¸ ìºì‹œ ê´€ë¦¬"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # ğŸ”¥ ê°œì„ : ë³´í˜¸í•  ëª¨ë¸ë“¤ ì‹ë³„
            protected_models = set()
            
            # Primary ëª¨ë¸ë“¤ ë³´í˜¸
            for mapping in self.central_hub_step_mappings.values():
                primary_model = mapping.get('primary_model')
                if primary_model:
                    protected_models.add(primary_model)
            
            # ìµœê·¼ ì‚¬ìš©ëœ ëª¨ë¸ë“¤ ë³´í˜¸ (1ì‹œê°„ ì´ë‚´)
            current_time = time.time()
            recent_threshold = 3600  # 1ì‹œê°„
            
            for model_name, model_info in self.model_info.items():
                if current_time - model_info.last_access < recent_threshold:
                    protected_models.add(model_name)
            
            # ğŸ”¥ ê°œì„ : ìŠ¤ë§ˆíŠ¸ ì œê±° ì „ëµ
            models_by_score = []
            for model_name, model_info in self.model_info.items():
                if model_name in protected_models:
                    continue
                    
                # ì ìˆ˜ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì œê±° ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                score = self._calculate_model_retention_score(model_info)
                models_by_score.append((model_name, score, model_info))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ì ìˆ˜ë¶€í„°)
            models_by_score.sort(key=lambda x: x[1])
            
            # ì œê±°í•  ëª¨ë¸ ìˆ˜ ê³„ì‚°
            models_to_remove_count = len(self.loaded_models) - self.max_cached_models
            models_to_remove = models_by_score[:models_to_remove_count]
            
            # ëª¨ë¸ ì œê±° ì‹¤í–‰
            removed_count = 0
            for model_name, score, model_info in models_to_remove:
                if self.unload_model(model_name):
                    removed_count += 1
                    self.logger.debug(f"ğŸ’½ ìºì‹œì—ì„œ ì œê±°: {model_name} (ì ìˆ˜: {score:.2f})")
            
            self.logger.info(f"ğŸ’½ ìºì‹œ ê´€ë¦¬ ì™„ë£Œ: {removed_count}ê°œ ëª¨ë¸ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")

    def _calculate_model_retention_score(self, model_info: RealStepModelInfo) -> float:
        """ğŸ”¥ ëª¨ë¸ ë³´ì¡´ ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ë³´ì¡´ ìš°ì„ ìˆœìœ„ ë†’ìŒ)"""
        try:
            current_time = time.time()
            
            # ê¸°ë³¸ ì ìˆ˜ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
            priority_scores = {
                RealModelPriority.PRIMARY: 100.0,
                RealModelPriority.SECONDARY: 50.0,
                RealModelPriority.FALLBACK: 25.0,
                RealModelPriority.OPTIONAL: 10.0
            }
            score = priority_scores.get(model_info.priority, 10.0)
            
            # ìµœê·¼ ì ‘ê·¼ ì‹œê°„ ë³´ë„ˆìŠ¤ (24ì‹œê°„ ì´ë‚´)
            time_since_access = current_time - model_info.last_access
            if time_since_access < 86400:  # 24ì‹œê°„
                time_bonus = max(0, 50 * (1 - time_since_access / 86400))
                score += time_bonus
            
            # ì‚¬ìš© ë¹ˆë„ ë³´ë„ˆìŠ¤
            if model_info.access_count > 0:
                frequency_bonus = min(30, model_info.access_count * 2)
                score += frequency_bonus
            
            # ì¶”ë¡  ì„±ëŠ¥ ë³´ë„ˆìŠ¤
            if model_info.inference_count > 0 and model_info.avg_inference_time > 0:
                # ë¹ ë¥¸ ì¶”ë¡ ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                performance_bonus = min(20, 100 / max(1, model_info.avg_inference_time))
                score += performance_bonus
            
            # ê²€ì¦ í†µê³¼ ë³´ë„ˆìŠ¤
            if model_info.validation_passed:
                score += 15
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í˜ë„í‹° (í° ëª¨ë¸ì¼ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ)
            memory_penalty = min(20, model_info.memory_mb / 1000)  # GBë‹¹ ì ìˆ˜ ê°ì†Œ
            score -= memory_penalty
            
            return max(0, score)
            
        except Exception as e:
            self.logger.debug(f"ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 10.0  # ê¸°ë³¸ ì ìˆ˜


    def unload_model(self, model_name: str) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ (Central Hub í˜¸í™˜)"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    model.unload()
                    
                    # ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
                    if model_name in self.model_info:
                        self.performance_metrics['total_memory_mb'] -= self.model_info[model_name].memory_mb
                        del self.model_info[model_name]
                    
                    del self.loaded_models[model_name]
                    self.model_status[model_name] = RealModelStatus.NOT_LOADED
                    
                    self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                    return True
                    
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ Central Hub ì™„ì „ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ì§€ì›
    # ==============================================
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
        """ğŸ”¥ Central Hub ê¸°ë°˜ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê°œì„ ë¨)"""
        try:
            if step_name in self.step_interfaces:
                return self.step_interfaces[step_name]
            
            # Step íƒ€ì… ê²°ì • (Central Hub ê¸°ë°˜)
            step_type = None
            if step_name in self.central_hub_step_mappings:
                step_type = self.central_hub_step_mappings[step_name].get('step_type')
            
            if not step_type:
                # ì´ë¦„ìœ¼ë¡œ ì¶”ë¡  (Central Hub í˜¸í™˜)
                step_type = self._infer_step_type_from_name(step_name)
            
            interface = RealStepModelInterface(self, step_name, step_type)
            
            # Central Hub DetailedDataSpec ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ë“±ë¡
            if step_requirements:
                interface.register_requirements(step_requirements)
            elif step_name in self.central_hub_step_mappings:
                # ê¸°ë³¸ ë§¤í•‘ì—ì„œ ìš”êµ¬ì‚¬í•­ ìƒì„± (Central Hub í˜¸í™˜)
                mapping = self.central_hub_step_mappings[step_name]
                default_requirements = {
                    'step_id': mapping.get('step_id', 0),
                    'required_models': mapping.get('ai_models', []),
                    'primary_model': mapping.get('primary_model'),
                    'model_configs': {},
                    'batch_size': 1,
                    'precision': 'fp16' if self.device == 'mps' else 'fp32'
                }
                interface.register_requirements(default_requirements)
            
            self.step_interfaces[step_name] = interface
            self.logger.info(f"âœ… Central Hub í˜¸í™˜ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {step_name} ({step_type.value})")
            
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
            return RealStepModelInterface(self, step_name, RealStepModelType.HUMAN_PARSING)
    
    def create_step_model_interface(self, step_name: str) -> RealStepModelInterface:
        """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (Central Hub í˜¸í™˜ ë³„ì¹­)"""
        return self.create_step_interface(step_name)
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (Central Hub í˜¸í™˜)
    # ==============================================
    
    def _get_step_requirements_from_instance(self, step_instance) -> Optional[Dict[str, Any]]:
        """Step ì¸ìŠ¤í„´ìŠ¤ë¡œë¶€í„° ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        try:
            requirements = {}
            
            # Step ê¸°ë³¸ ì •ë³´
            requirements['step_id'] = getattr(step_instance, 'step_id', 0)
            requirements['step_type'] = self._infer_step_type_from_name(step_instance.step_name)
            
            # DetailedDataSpecì—ì„œ ì •ë³´ ì¶”ì¶œ (Central Hub í˜¸í™˜)
            if hasattr(step_instance, 'detailed_data_spec') and step_instance.detailed_data_spec:
                spec = step_instance.detailed_data_spec
                requirements.update({
                    'input_data_specs': getattr(spec, 'input_data_types', {}),
                    'output_data_specs': getattr(spec, 'output_data_types', {}),
                    'preprocessing_required': getattr(spec, 'preprocessing_steps', []),
                    'postprocessing_required': getattr(spec, 'postprocessing_steps', [])
                })
            
            # Central Hub ë§¤í•‘ì—ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            step_name = step_instance.step_name
            if step_name in self.central_hub_step_mappings:
                mapping = self.central_hub_step_mappings[step_name]
                requirements.update({
                    'required_models': mapping.get('ai_models', []),
                    'primary_model': mapping.get('primary_model'),
                    'model_configs': {}
                })
            
            return requirements if requirements else None
            
        except Exception as e:
            self.logger.debug(f"Step ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _infer_step_type_from_name(self, step_name: str) -> RealStepModelType:
        """Step ì´ë¦„ìœ¼ë¡œ íƒ€ì… ì¶”ë¡  (Central Hub í˜¸í™˜)"""
        step_type_map = {
            'HumanParsingStep': RealStepModelType.HUMAN_PARSING,
            'PoseEstimationStep': RealStepModelType.POSE_ESTIMATION,
            'ClothSegmentationStep': RealStepModelType.CLOTH_SEGMENTATION,
            'GeometricMatchingStep': RealStepModelType.GEOMETRIC_MATCHING,
            'ClothWarpingStep': RealStepModelType.CLOTH_WARPING,
            'VirtualFittingStep': RealStepModelType.VIRTUAL_FITTING,
            'PostProcessingStep': RealStepModelType.POST_PROCESSING,
            'QualityAssessmentStep': RealStepModelType.QUALITY_ASSESSMENT
        }
        return step_type_map.get(step_name, RealStepModelType.HUMAN_PARSING)
    
    def _infer_step_type(self, model_name: str, model_path: str) -> Optional[RealStepModelType]:
        """ëª¨ë¸ëª…ê³¼ ê²½ë¡œë¡œ Step íƒ€ì… ì¶”ë¡  (Central Hub í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        model_path_lower = model_path.lower()
        
        # ê²½ë¡œ ê¸°ë°˜ ì¶”ë¡  (Central Hub êµ¬ì¡°)
        if "step_01" in model_path_lower or "human_parsing" in model_path_lower:
            return RealStepModelType.HUMAN_PARSING
        elif "step_02" in model_path_lower or "pose" in model_path_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif "step_03" in model_path_lower or "segmentation" in model_path_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif "step_04" in model_path_lower or "geometric" in model_path_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif "step_05" in model_path_lower or "warping" in model_path_lower:
            return RealStepModelType.CLOTH_WARPING
        elif "step_06" in model_path_lower or "virtual" in model_path_lower or "fitting" in model_path_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif "step_07" in model_path_lower or "post" in model_path_lower:
            return RealStepModelType.POST_PROCESSING
        elif "step_08" in model_path_lower or "quality" in model_path_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        # ëª¨ë¸ëª… ê¸°ë°˜ ì¶”ë¡  (Central Hub ë§¤í•‘ ê¸°ë°˜)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "atr", "schp"]):
            return RealStepModelType.HUMAN_PARSING
        elif any(keyword in model_name_lower for keyword in ["yolo", "openpose", "pose"]):
            return RealStepModelType.POSE_ESTIMATION
        elif any(keyword in model_name_lower for keyword in ["sam", "u2net", "segment"]):
            return RealStepModelType.CLOTH_SEGMENTATION
        elif any(keyword in model_name_lower for keyword in ["gmm", "tps", "geometric"]):
            return RealStepModelType.GEOMETRIC_MATCHING
        elif any(keyword in model_name_lower for keyword in ["realvis", "vgg", "warping"]):
            return RealStepModelType.CLOTH_WARPING
        elif any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet", "unet", "vae"]):
            return RealStepModelType.VIRTUAL_FITTING
        elif any(keyword in model_name_lower for keyword in ["esrgan", "sr", "enhancement"]):
            return RealStepModelType.POST_PROCESSING
        elif any(keyword in model_name_lower for keyword in ["clip", "vit", "quality"]):
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return None
    
    def _infer_model_type(self, model_name: str) -> str:
        """ëª¨ë¸ íƒ€ì… ì¶”ë¡  (Central Hub í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        
        if any(keyword in model_name_lower for keyword in ["diffusion", "stable", "controlnet"]):
            return "DiffusionModel"
        elif any(keyword in model_name_lower for keyword in ["yolo", "detection"]):
            return "DetectionModel"
        elif any(keyword in model_name_lower for keyword in ["segment", "sam", "u2net"]):
            return "SegmentationModel"
        elif any(keyword in model_name_lower for keyword in ["pose", "openpose"]):
            return "PoseModel"
        elif any(keyword in model_name_lower for keyword in ["clip", "vit"]):
            return "ClassificationModel"
        else:
            return "BaseModel"
    
    def _infer_model_priority(self, model_name: str) -> int:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì¶”ë¡  (Central Hub í˜¸í™˜)"""
        model_name_lower = model_name.lower()
        
        # Primary ëª¨ë¸ë“¤ (Central Hub ë§¤í•‘ ê¸°ë°˜)
        if any(keyword in model_name_lower for keyword in ["graphonomy", "yolo", "sam", "diffusion", "esrgan", "clip"]):
            return RealModelPriority.PRIMARY.value
        elif any(keyword in model_name_lower for keyword in ["atr", "openpose", "u2net", "vgg"]):
            return RealModelPriority.SECONDARY.value
        else:
            return RealModelPriority.OPTIONAL.value
    
    def _get_step_id_from_step_type(self, step_type: Optional[RealStepModelType]) -> int:
        """Step íƒ€ì…ì—ì„œ ID ì¶”ì¶œ (Central Hub í˜¸í™˜)"""
        if not step_type:
            return 0
        
        step_id_map = {
            RealStepModelType.HUMAN_PARSING: 1,
            RealStepModelType.POSE_ESTIMATION: 2,
            RealStepModelType.CLOTH_SEGMENTATION: 3,
            RealStepModelType.GEOMETRIC_MATCHING: 4,
            RealStepModelType.CLOTH_WARPING: 5,
            RealStepModelType.VIRTUAL_FITTING: 6,
            RealStepModelType.POST_PROCESSING: 7,
            RealStepModelType.QUALITY_ASSESSMENT: 8
        }
        return step_id_map.get(step_type, 0)
    
    def _get_step_id(self, step_name: str) -> int:
        """Step ì´ë¦„ìœ¼ë¡œ ID ë°˜í™˜ (Central Hub í˜¸í™˜)"""
        step_id_map = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return step_id_map.get(step_name, 0)
    
    # ==============================================
    # ğŸ”¥ Central Hub BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    @property
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ (Central Hub í˜¸í™˜)"""
        return hasattr(self, 'loaded_models') and hasattr(self, 'model_info')
    
    def initialize(self, **kwargs) -> bool:
        """ì´ˆê¸°í™” (Central Hub í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
            
            self.logger.info("âœ… Central Hub í˜¸í™˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self, **kwargs) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” (Central Hub í˜¸í™˜)"""
        return self.initialize(**kwargs)
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - Central Hub BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if not hasattr(self, 'model_requirements'):
                    self.model_requirements = {}
                
                # Step íƒ€ì… ì¶”ë¡ 
                step_type = kwargs.get('step_type')
                if isinstance(step_type, str):
                    step_type = RealStepModelType(step_type)
                elif not step_type:
                    step_type = self._infer_step_type(model_name, kwargs.get('model_path', ''))
                
                self.model_requirements[model_name] = {
                    'model_type': model_type,
                    'step_type': step_type.value if step_type else 'unknown',
                    'required': kwargs.get('required', True),
                    'priority': kwargs.get('priority', RealModelPriority.SECONDARY.value),
                    'device': kwargs.get('device', self.device),
                    'preprocessing_params': kwargs.get('preprocessing_params', {}),
                    **kwargs
                }
                
                self.logger.info(f"âœ… Central Hub í˜¸í™˜ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_model_compatibility(self, model_name: str, step_name: str) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ (Central Hub í˜¸í™˜)"""
        try:
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            if model_name not in self.model_info and model_name not in self._available_models_cache:
                return False
            
            # Step ìš”êµ¬ì‚¬í•­ í™•ì¸
            if step_name in self.step_requirements:
                step_req = self.step_requirements[step_name]
                if model_name in step_req.required_models or model_name in step_req.optional_models:
                    return True
            
            # Central Hub ë§¤í•‘ í™•ì¸
            if step_name in self.central_hub_step_mappings:
                mapping = self.central_hub_step_mappings[step_name]
                if model_name in mapping.get('ai_models', []):
                    return True
                for local_path in mapping.get('local_paths', []):
                    if model_name in local_path or Path(local_path).name == model_name:
                        return True
            
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í˜¸í™˜ ê°€ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def has_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (Central Hub í˜¸í™˜)"""
        return (model_name in self.loaded_models or 
                model_name in self._available_models_cache or
                model_name in self.model_info)
    
    def is_model_loaded(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ (Central Hub í˜¸í™˜)"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name].loaded
        return False
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ AI ëª¨ë¸ ëª©ë¡ (Central Hub ì™„ì „ í˜¸í™˜)"""
        try:
            models = []
            
            # available_modelsì—ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            for model_name, model_info in self._available_models_cache.items():
                # í•„í„°ë§
                if step_class and model_info.get("step_class") != step_class:
                    continue
                if model_type and model_info.get("model_type") != model_type:
                    continue
                
                # ë¡œë”© ìƒíƒœ ì¶”ê°€ (Central Hub í˜¸í™˜)
                is_loaded = model_name in self.loaded_models
                model_info_copy = model_info.copy()
                model_info_copy["loaded"] = is_loaded
                
                # Central Hub í˜¸í™˜ í•„ë“œ ì¶”ê°€
                model_info_copy.update({
                    "real_ai_model": True,
                    "checkpoint_loaded": is_loaded and self.loaded_models.get(model_name, {}).get('checkpoint_data') is not None if is_loaded else False,
                    "step_loadable": True,
                    "device_compatible": True,
                    "requires_checkpoint": True,
                    "central_hub_integrated": True
                })
                
                models.append(model_info_copy)
            
            # Central Hub ë§¤í•‘ì—ì„œ ì¶”ê°€
            for step_name, mapping in self.central_hub_step_mappings.items():
                if step_class and step_class != step_name:
                    continue
                
                step_type = mapping.get('step_type', RealStepModelType.HUMAN_PARSING)
                for model_name in mapping.get('ai_models', []):
                    if model_name not in [m['name'] for m in models]:
                        # Central Hub í˜¸í™˜ ëª¨ë¸ ì •ë³´
                        models.append({
                            'name': model_name,
                            'path': f"ai_models/step_{mapping.get('step_id', 0):02d}_{step_name.lower()}/{model_name}",
                            'type': self._infer_model_type(model_name),
                            'step_type': step_type.value,
                            'loaded': model_name in self.loaded_models,
                            'step_class': step_name,
                            'step_id': mapping.get('step_id', 0),
                            'size_mb': 0.0,  # ì‹¤ì œ íŒŒì¼ í¬ê¸°ëŠ” ë¡œë”© ì‹œ ê³„ì‚°
                            'priority': self._infer_model_priority(model_name),
                            'is_primary': model_name == mapping.get('primary_model'),
                            'real_ai_model': True,
                            'device_compatible': True,
                            'requires_checkpoint': True,
                            'step_loadable': True,
                            'central_hub_integrated': True
                        })
            
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì •ë³´ ì¡°íšŒ (Central Hub ì™„ì „ í˜¸í™˜)"""
        try:
            if model_name in self.model_info:
                info = self.model_info[model_name]
                return {
                    'name': info.name,
                    'path': info.path,
                    'step_type': info.step_type.value,
                    'priority': info.priority.value,
                    'device': info.device,
                    'memory_mb': info.memory_mb,
                    'loaded': info.loaded,
                    'load_time': info.load_time,
                    'access_count': info.access_count,
                    'last_access': info.last_access,
                    'inference_count': info.inference_count,
                    'avg_inference_time': info.avg_inference_time,
                    'validation_passed': info.validation_passed,
                    'has_checkpoint_data': info.checkpoint_data is not None,
                    'error': info.error,
                    
                    # Central Hub í˜¸í™˜ í•„ë“œ
                    'model_type': info.model_type,
                    'size_gb': info.size_gb,
                    'requires_checkpoint': info.requires_checkpoint,
                    'preprocessing_required': info.preprocessing_required,
                    'postprocessing_required': info.postprocessing_required,
                    'real_ai_model': True,
                    'device_compatible': True,
                    'step_loadable': True,
                    'central_hub_integrated': True
                }
            else:
                return {'name': model_name, 'exists': False}
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'name': model_name, 'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (Central Hub í˜¸í™˜)"""
        return {
            **self.performance_metrics,
            "device": self.device,
            "is_m3_max": IS_M3_MAX,
            "mps_available": MPS_AVAILABLE,
            "loaded_models_count": len(self.loaded_models),
            "cached_models": list(self.loaded_models.keys()),
            "auto_detector_integration": self._integration_successful,
            "available_models_count": len(self._available_models_cache),
            "step_interfaces_count": len(self.step_interfaces),
            "avg_inference_time": self.performance_metrics['total_inference_time'] / max(1, self.performance_metrics['inference_count']),
            "memory_efficiency": self.performance_metrics['total_memory_mb'] / max(1, len(self.loaded_models)),
            
            # Central Hub í˜¸í™˜ í•„ë“œ
            "central_hub_integrated": True,
            "central_hub_injections": self.performance_metrics['central_hub_injections'],
            "step_requirements_registered": self.performance_metrics['step_requirements_registered'],
            "central_hub_container_connected": self._central_hub_container is not None,
            "dependency_resolution_active": self.memory_manager is not None or self.data_converter is not None,
            "github_step_mapping_loaded": len(self.central_hub_step_mappings) > 0,
            "real_ai_models_only": True,
            "mock_removed": True,
            "checkpoint_loading_optimized": True
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (Central Hub í˜¸í™˜)"""
        try:
            self.logger.info("ğŸ§¹ Central Hub í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # ìºì‹œ ì •ë¦¬
            self.model_info.clear()
            self.model_status.clear()
            self.step_interfaces.clear()
            self.step_requirements.clear()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # Central Hub MemoryManagerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
            if self.memory_manager and hasattr(self.memory_manager, 'optimize_memory'):
                try:
                    self.memory_manager.optimize_memory(aggressive=True)
                except Exception as e:
                    self.logger.debug(f"Central Hub MemoryManager ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE and TORCH_AVAILABLE:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except:
                    pass
            
            self.logger.info("âœ… Central Hub í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (Central Hub ì™„ì „ í˜¸í™˜)
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_model_loader: Optional[ModelLoader] = None
_loader_lock = threading.Lock()

def get_global_model_loader(config: Optional[Dict[str, Any]] = None) -> ModelLoader:
    """ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê°œì„ ëœ TypeError ë°©ì§€)"""
    global _global_model_loader
    
    with _loader_lock:
        if _global_model_loader is None:
            try:
                # ğŸ”¥ ê°œì„ : ì•ˆì „í•œ config ì²˜ë¦¬
                safe_config = {}
                if config:
                    # di_container í‚¤ë§Œ ì œì™¸í•˜ê³  ë³µì‚¬
                    safe_config = {k: v for k, v in config.items() if k != 'di_container'}
                
                # ğŸ”¥ ê°œì„ : ë‹¨ìˆœí•œ ìƒì„± ë¡œì§
                _global_model_loader = ModelLoader(**safe_config)
                
                # ğŸ”¥ ê°œì„ : ìƒì„± í›„ Central Hub ì—°ê²°
                try:
                    central_hub_container = _get_central_hub_container()
                    if central_hub_container:
                        _global_model_loader._central_hub_container = central_hub_container
                        _global_model_loader._resolve_dependencies_from_central_hub()
                        logger.debug("âœ… Central Hub Container ì—°ê²° ì„±ê³µ")
                except Exception as hub_error:
                    logger.debug(f"âš ï¸ Central Hub ì—°ê²° ì‹¤íŒ¨: {hub_error}")
                
                logger.info("âœ… ì „ì—­ ModelLoader v5.1 ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ModelLoader ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}")
                # ğŸ”¥ ê°œì„ : ë‹¨ìˆœí•œ í´ë°±
                _global_model_loader = ModelLoader(device="cpu")
                
        return _global_model_loader
    
def initialize_global_model_loader(**kwargs) -> bool:
    """ì „ì—­ ModelLoader ì´ˆê¸°í™” (Central Hub í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        return loader.initialize(**kwargs)
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def initialize_global_model_loader_async(**kwargs) -> ModelLoader:
    """ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” (Central Hub í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        success = await loader.initialize_async(**kwargs)
        
        if success:
            logger.info("âœ… ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì¼ë¶€ ì‹¤íŒ¨")
            
        return loader
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (Central Hub í˜¸í™˜)"""
    try:
        loader = get_global_model_loader()
        return loader.create_step_interface(step_name, step_requirements)
    except Exception as e:
        logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        step_type = RealStepModelType.HUMAN_PARSING
        return RealStepModelInterface(get_global_model_loader(), step_name, step_type)

def get_model(model_name: str) -> Optional[RealAIModel]:
    """ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (Central Hub í˜¸í™˜)"""
    loader = get_global_model_loader()
    return loader.load_model(model_name)

async def get_model_async(model_name: str) -> Optional[RealAIModel]:
    """ì „ì—­ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (Central Hub í˜¸í™˜)"""
    loader = get_global_model_loader()
    return await loader.load_model_async(model_name)

def get_step_model_interface(step_name: str, model_loader_instance=None) -> RealStepModelInterface:
    """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (Central Hub í˜¸í™˜)"""
    if model_loader_instance is None:
        model_loader_instance = get_global_model_loader()
    
    return model_loader_instance.create_step_interface(step_name)

# ==============================================
# ğŸ”¥ Central Hub ì „ìš© í¸ì˜ í•¨ìˆ˜ë“¤ (ìƒˆë¡œ ì¶”ê°€)
# ==============================================

def inject_to_step(step_instance) -> int:
    """ğŸ”¥ Stepì— ModelLoader ë° ì˜ì¡´ì„± ì£¼ì… (Central Hub ì§€ì›)"""
    try:
        loader = get_global_model_loader()
        if hasattr(loader, 'inject_to_step'):
            return loader.inject_to_step(step_instance)
        
        # í´ë°±: ì§ì ‘ ì£¼ì…
        injections_made = 0
        if hasattr(step_instance, 'model_loader'):
            step_instance.model_loader = loader
            injections_made += 1
            
        return injections_made
        
    except Exception as e:
        logger.error(f"âŒ Step ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        return 0

def register_step_requirements(step_name: str, requirements: Dict[str, Any]) -> bool:
    """ğŸ”¥ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ (Central Hub ì§€ì›)"""
    try:
        loader = get_global_model_loader()
        if hasattr(loader, 'register_step_requirements'):
            return loader.register_step_requirements(step_name, requirements)
        return False
    except Exception as e:
        logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def validate_di_container_integration() -> Dict[str, Any]:
    """ğŸ”¥ Central Hub DI Container ì—°ë™ ìƒíƒœ ê²€ì¦"""
    try:
        loader = get_global_model_loader()
        if hasattr(loader, 'validate_di_container_integration'):
            return loader.validate_di_container_integration()
        
        # ê¸°ë³¸ ê²€ì¦
        return {
            'di_container_available': _get_central_hub_container() is not None,
            'model_loader_available': loader is not None,
            'central_hub_integrated': True
        }
        
    except Exception as e:
        return {'error': str(e), 'central_hub_integrated': False}

def optimize_memory_via_central_hub() -> Dict[str, Any]:
    """ğŸ”¥ Central Hub ë©”ëª¨ë¦¬ ìµœì í™”"""
    try:
        loader = get_global_model_loader()
        if hasattr(loader, 'optimize_memory_via_central_hub'):
            return loader.optimize_memory_via_central_hub()
        
        # ê¸°ë³¸ ìµœì í™”
        gc.collect()
        return {'gc_collected': True, 'central_hub_optimization': False}
        
    except Exception as e:
        return {'error': str(e)}

def get_central_hub_stats() -> Dict[str, Any]:
    """ğŸ”¥ Central Hub í†µê³„ ì—°ë™"""
    try:
        loader = get_global_model_loader()
        if hasattr(loader, 'get_central_hub_stats'):
            return loader.get_central_hub_stats()
        
        # ê¸°ë³¸ í†µê³„
        return {
            'model_loader_available': loader is not None,
            'central_hub_connected': _get_central_hub_container() is not None
        }
        
    except Exception as e:
        return {'error': str(e)}

# step_interface.py í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
BaseModel = RealAIModel
StepModelInterface = RealStepModelInterface

# ==============================================
# ğŸ”¥ Export ë° ì´ˆê¸°í™”
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤ (Central Hub ì™„ì „ í˜¸í™˜)
    'ModelLoader',
    'RealStepModelInterface',
    'EnhancedStepModelInterface',  # í˜¸í™˜ì„± ë³„ì¹­
    'StepModelInterface',  # í˜¸í™˜ì„± ë³„ì¹­
    'RealAIModel',
    'BaseModel',  # í˜¸í™˜ì„± ë³„ì¹­
    
    # Central Hub ì™„ì „ í˜¸í™˜ ë°ì´í„° êµ¬ì¡°ë“¤
    'RealStepModelType',
    'RealModelStatus',
    'RealModelPriority',
    'RealStepModelInfo',
    'RealStepModelRequirement',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤ (Central Hub ì™„ì „ í˜¸í™˜)
    'get_global_model_loader',
    'initialize_global_model_loader',
    'initialize_global_model_loader_async',
    'create_step_interface',
    'get_model',
    'get_model_async',
    'get_step_model_interface',
    
    # ğŸ”¥ Central Hub ì „ìš© í•¨ìˆ˜ë“¤ (ìƒˆë¡œ ì¶”ê°€)
    'inject_to_step',
    'register_step_requirements',
    'validate_di_container_integration',
    'optimize_memory_via_central_hub',
    'get_central_hub_stats',
    
    # ìƒìˆ˜ë“¤
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'TORCH_AVAILABLE',
    'AUTO_DETECTOR_AVAILABLE',
    'IS_M3_MAX',
    'MPS_AVAILABLE',
    'CONDA_INFO',
    'DEFAULT_DEVICE'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("ğŸš€ ModelLoader v5.1 â†’ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("=" * 80)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©")
logger.info("âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ - DI Containerë§Œì„ í†µí•œ ì˜ì¡´ì„± ì£¼ì…")
logger.info("âœ… inject_to_step() ë©”ì„œë“œ êµ¬í˜„ - Stepì— ModelLoader ìë™ ì£¼ì…")
logger.info("âœ… create_step_interface() ë©”ì„œë“œ ê°œì„  - Central Hub ê¸°ë°˜ í†µí•© ì¸í„°í˜ì´ìŠ¤")
logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ - validate_di_container_integration() ì™„ì „ ê°œì„ ")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ ì§€ì› - fix_checkpoints.py ê²€ì¦ ê²°ê³¼ ë°˜ì˜")
logger.info("âœ… Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡ - register_step_requirements() ì¶”ê°€")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” - Central Hub MemoryManager ì—°ë™")
logger.info("âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥ - ëª¨ë“  ë©”ì„œë“œëª…/í´ë˜ìŠ¤ëª… ìœ ì§€")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   Device: {DEFAULT_DEVICE} (M3 Max: {IS_M3_MAX}, MPS: {MPS_AVAILABLE})")
logger.info(f"   PyTorch: {TORCH_AVAILABLE}, NumPy: {NUMPY_AVAILABLE}, PIL: {PIL_AVAILABLE}")
logger.info(f"   AutoDetector: {AUTO_DETECTOR_AVAILABLE}")
logger.info(f"   conda í™˜ê²½: {CONDA_INFO['conda_env']} (target: {CONDA_INFO['is_target_env']})")

logger.info("ğŸ¯ ì§€ì› ì‹¤ì œ AI Step íƒ€ì… (Central Hub ì™„ì „ í˜¸í™˜):")
for step_type in RealStepModelType:
    logger.info(f"   - {step_type.value}: íŠ¹í™” ë¡œë” ì§€ì›")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ Central Hub Pattern: DI Containerê°€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ì‹¬")
logger.info("   â€¢ Single Source of Truth: ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” Central Hubë¥¼ ê±°ì¹¨")
logger.info("   â€¢ Dependency Inversion: ìƒìœ„ ëª¨ë“ˆì´ í•˜ìœ„ ëª¨ë“ˆì„ ì œì–´")  
logger.info("   â€¢ Zero Circular Reference: ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨")
logger.info("   â€¢ inject_to_step(): Stepì— ModelLoader ìë™ ì£¼ì…")
logger.info("   â€¢ register_step_requirements(): Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡")
logger.info("   â€¢ validate_di_container_integration(): ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ")
logger.info("   â€¢ optimize_memory_via_central_hub(): Central Hub MemoryManager ì—°ë™")
logger.info("   â€¢ get_central_hub_stats(): Central Hub í†µê³„ ì—°ë™")

logger.info("ğŸš€ Central Hub ì§€ì› íë¦„:")
logger.info("   CentralHubDIContainer (v7.0)")
logger.info("     â†“ (ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ - ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì¬)")
logger.info("   ModelLoader (v5.1) â† ğŸ”¥ Central Hub ì™„ì „ ì—°ë™!")
logger.info("     â†“ (inject_to_step() ìë™ ì£¼ì…)")
logger.info("   BaseStepMixin (v20.0)")
logger.info("     â†“ (Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡)")
logger.info("   Step Classes (GitHub í”„ë¡œì íŠ¸)")
logger.info("     â†“ (ì‹¤ì œ AI ì¶”ë¡ )")
logger.info("   ì‹¤ì œ AI ëª¨ë¸ë“¤ (229GB)")

logger.info("ğŸ‰ ModelLoader v5.1 Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì™„ë£Œ!")
logger.info("ğŸ‰ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ ë‹¬ì„±!")
logger.info("ğŸ‰ Step ìë™ ì£¼ì… + ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡ ì§€ì›!")
logger.info("ğŸ‰ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ + ë©”ëª¨ë¦¬ ìµœì í™” ì—°ë™!")
logger.info("ğŸ‰ ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥!")
logger.info("=" * 80)

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_loader = get_global_model_loader()
    
    # Central Hub ì—°ë™ ê²€ì¦
    integration_status = validate_di_container_integration()
    logger.info(f"ğŸ”— Central Hub ì—°ë™ ìƒíƒœ: {integration_status.get('di_container_available', False)}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤€ë¹„ ìƒíƒœ í™•ì¸
    checkpoint_ready = integration_status.get('checkpoint_loading_ready', False)
    logger.info(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤€ë¹„: {'âœ…' if checkpoint_ready else 'âš ï¸'}")
    
    logger.info(f"ğŸ‰ Central Hub ì™„ì „ ì—°ë™ ModelLoader v5.1 ì¤€ë¹„ ì™„ë£Œ!")
    logger.info(f"   ë””ë°”ì´ìŠ¤: {_test_loader.device}")
    logger.info(f"   ëª¨ë¸ ìºì‹œ: {_test_loader.model_cache_dir}")
    logger.info(f"   Central Hub ë§¤í•‘: {len(_test_loader.central_hub_step_mappings)}ê°œ Step")
    logger.info(f"   AutoDetector í†µí•©: {_test_loader._integration_successful}")
    logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(_test_loader._available_models_cache)}ê°œ")
    logger.info(f"   ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©: âœ…")
    logger.info(f"   Central Hub v7.0 í˜¸í™˜: âœ…")
    logger.info(f"   ìˆœí™˜ì°¸ì¡° í•´ê²°: âœ…")
    logger.info(f"   Step ìë™ ì£¼ì…: âœ…")
    
except Exception as e:
    logger.error(f"âŒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•˜ì§€ë§Œ ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

logger.info("ğŸ”¥ ModelLoader v5.1 Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")