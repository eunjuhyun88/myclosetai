"""
π”¥ Graphonomy μ²΄ν¬ν¬μΈνΈ ν†µν•© μ‹μ¤ν…
λ¨λ“  μ²΄ν¬ν¬μΈνΈ κ΄€λ ¨ λ΅μ§μ„ Graphonomy κΈ°μ¤€μΌλ΅ ν†µν•©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import logging

logger = logging.getLogger(__name__)


class ResNetGraphonomyModel(nn.Module):
    """ResNet κΈ°λ° Graphonomy λ¨λΈ"""
    def __init__(self, num_classes=20):
        super().__init__()
        from .graphonomy_models import ResNet101Backbone
        self.backbone = ResNet101Backbone()
        self.classifier = nn.Conv2d(2048, num_classes, kernel_size=1)
        self.edge_head = nn.Conv2d(2048, 1, kernel_size=1)
    
    def forward(self, x):
        features = self.backbone(x)
        parsing = self.classifier(features['layer4'])
        edge = self.edge_head(features['layer4'])
        return {'parsing': parsing, 'edge': edge}


class SimpleGraphonomyModel(nn.Module):
    """λ‹¨μν• Graphonomy λ¨λΈ"""
    def __init__(self, num_classes=20):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1)
        )
        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)
    
    def forward(self, x):
        features = self.features(x)
        parsing = self.classifier(features)
        return {'parsing': parsing}


class FallbackGraphonomyModel(nn.Module):
    """ν΄λ°± λ¨λΈ"""
    def __init__(self, num_classes=20):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)
    
    def forward(self, x):
        features = self.features(x)
        parsing = self.classifier(features)
        return {'parsing': parsing}


class GraphonomyCheckpointAnalyzer:
    """Graphonomy μ²΄ν¬ν¬μΈνΈ λ¶„μ„κΈ°"""
    
    def __init__(self):
        self.logger = logger
    
    def analyze_checkpoint(self, checkpoint_data: Dict[str, Any]) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° λ¶„μ„"""
        try:
            checkpoint_keys = list(checkpoint_data.keys())
            
            # μ²΄ν¬ν¬μΈνΈ νƒ€μ… λ¶„μ„
            checkpoint_type = self._detect_checkpoint_type(checkpoint_keys)
            
            # ν‚¤ λ§¤ν•‘ κ·μΉ™ μƒμ„±
            key_mappings = self._generate_key_mappings(checkpoint_type)
            
            return {
                'type': checkpoint_type,
                'keys': checkpoint_keys,
                'key_count': len(checkpoint_keys),
                'mappings': key_mappings,
                'has_resnet': any('resnet' in key.lower() or 'layer' in key.lower() for key in checkpoint_keys),
                'has_aspp': any('aspp' in key.lower() for key in checkpoint_keys),
                'has_classifier': any('classifier' in key.lower() for key in checkpoint_keys),
                'has_self_correction': any('self_correction' in key.lower() for key in checkpoint_keys),
                'has_progressive': any('progressive' in key.lower() for key in checkpoint_keys)
            }
            
        except Exception as e:
            self.logger.warning(f"β οΈ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹¤ν¨: {e}")
            return {'type': 'unknown', 'keys': [], 'mappings': {}}
    
    def _detect_checkpoint_type(self, checkpoint_keys: List[str]) -> str:
        """μ²΄ν¬ν¬μΈνΈ νƒ€μ… κ°μ§€"""
        if any('module.' in key for key in checkpoint_keys):
            return 'graphonomy_schp'  # Self-Correction Human Parsing
        elif any('backbone.' in key for key in checkpoint_keys):
            return 'graphonomy_resnet'
        elif any('aspp.' in key for key in checkpoint_keys):
            return 'graphonomy_aspp'
        elif any('self_correction' in key for key in checkpoint_keys):
            return 'graphonomy_advanced'
        else:
            return 'graphonomy_standard'
    
    def _generate_key_mappings(self, checkpoint_type: str) -> Dict[str, str]:
        """ν‚¤ λ§¤ν•‘ κ·μΉ™ μƒμ„±"""
        base_mappings = {
            # ResNet λ°±λ³Έ λ§¤ν•‘
            'module.conv1.weight': 'conv1.weight',
            'module.bn1.weight': 'bn1.weight',
            'module.bn1.bias': 'bn1.bias',
            'module.bn1.running_mean': 'bn1.running_mean',
            'module.bn1.running_var': 'bn1.running_var',
            
            # Layer λ§¤ν•‘
            'module.layer1.': 'layer1.',
            'module.layer2.': 'layer2.',
            'module.layer3.': 'layer3.',
            'module.layer4.': 'layer4.',
            
            # ASPP λ§¤ν•‘
            'module.aspp.convs.0.weight': 'aspp.conv1x1.0.weight',
            'module.aspp.convs.0.bias': 'aspp.conv1x1.0.bias',
            'module.aspp.convs.1.weight': 'aspp.atrous_convs.0.0.weight',
            'module.aspp.convs.1.bias': 'aspp.atrous_convs.0.0.bias',
            'module.aspp.convs.2.weight': 'aspp.atrous_convs.1.0.weight',
            'module.aspp.convs.2.bias': 'aspp.atrous_convs.1.0.bias',
            'module.aspp.convs.3.weight': 'aspp.atrous_convs.2.0.weight',
            'module.aspp.convs.3.bias': 'aspp.atrous_convs.2.0.bias',
            'module.aspp.convs.4.weight': 'aspp.conv_global.weight',
            'module.aspp.convs.4.bias': 'aspp.conv_global.bias',
            'module.aspp.convs.5.weight': 'aspp.conv_out.weight',
            'module.aspp.convs.5.bias': 'aspp.conv_out.bias',
            
            # λ¶„λ¥κΈ° λ§¤ν•‘
            'module.classifier.weight': 'classifier.weight',
            'module.classifier.bias': 'classifier.bias',
            
            # Edge Detection λ§¤ν•‘
            'module.edge_head.weight': 'edge_head.weight',
            'module.edge_head.bias': 'edge_head.bias'
        }
        
        # μ²΄ν¬ν¬μΈνΈ νƒ€μ…λ³„ μ¶”κ°€ λ§¤ν•‘
        if checkpoint_type == 'graphonomy_schp':
            base_mappings.update({
                'module.self_correction.': 'self_correction.',
                'module.progressive_parsing.': 'progressive_parsing.',
                'module.iterative_refinement.': 'iterative_refinement.'
            })
        
        return base_mappings


class GraphonomyModelFactory:
    """Graphonomy λ¨λΈ ν©ν† λ¦¬"""
    
    def __init__(self):
        self.logger = logger
    
    def create_model(self, checkpoint_info: Dict[str, Any]) -> nn.Module:
        """μ²΄ν¬ν¬μΈνΈ μ •λ³΄μ— λ”°λΌ μ μ ν• λ¨λΈ μƒμ„±"""
        try:
            if checkpoint_info['has_resnet'] and checkpoint_info['has_aspp']:
                return self._create_full_graphonomy_model()
            elif checkpoint_info['has_resnet']:
                return self._create_resnet_graphonomy_model()
            else:
                return self._create_simple_graphonomy_model()
                
        except Exception as e:
            self.logger.warning(f"β οΈ λ¨λΈ μƒμ„± μ‹¤ν¨: {e}")
            return self._create_fallback_model()
    
    def _create_full_graphonomy_model(self) -> nn.Module:
        """μ™„μ „ν• Graphonomy λ¨λΈ (ResNet + ASPP + λ¨λ“  λ¨λ“)"""
        from .graphonomy_models import AdvancedGraphonomyResNetASPP
        return AdvancedGraphonomyResNetASPP(num_classes=20)
    
    def _create_resnet_graphonomy_model(self) -> nn.Module:
        """ResNet κΈ°λ° Graphonomy λ¨λΈ"""
        return ResNetGraphonomyModel(num_classes=20)
    
    def _create_simple_graphonomy_model(self) -> nn.Module:
        """λ‹¨μν• Graphonomy λ¨λΈ"""
        return SimpleGraphonomyModel(num_classes=20)
    
    def _create_fallback_model(self) -> nn.Module:
        """ν΄λ°± λ¨λΈ"""
        return FallbackGraphonomyModel(num_classes=20)


class GraphonomyCheckpointLoader:
    """Graphonomy μ²΄ν¬ν¬μΈνΈ λ΅λ”"""
    
    def __init__(self):
        self.logger = logger
        self.analyzer = GraphonomyCheckpointAnalyzer()
        self.model_factory = GraphonomyModelFactory()
    
    def load_checkpoint(self, checkpoint_path: Path, device: str = 'cpu') -> Optional[nn.Module]:
        """μ²΄ν¬ν¬μΈνΈ λ΅λ”©"""
        try:
            self.logger.info(f"π”„ Graphonomy μ²΄ν¬ν¬μΈνΈ λ΅λ”©: {checkpoint_path}")
            
            # 1. μ²΄ν¬ν¬μΈνΈ νμΌ λ΅λ”©
            checkpoint_data = self._load_checkpoint_file(checkpoint_path)
            if checkpoint_data is None:
                return None
            
            # 2. μ²΄ν¬ν¬μΈνΈ λ¶„μ„
            checkpoint_info = self.analyzer.analyze_checkpoint(checkpoint_data)
            
            # 3. λ¨λΈ μƒμ„±
            model = self.model_factory.create_model(checkpoint_info)
            
            # 4. κ°€μ¤‘μΉ λ΅λ”©
            self._load_weights(checkpoint_data, model, checkpoint_info)
            
            # 5. λ¨λΈ μ„¤μ •
            model.to(device)
            model.eval()
            
            self.logger.info("β… Graphonomy μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ™„λ£")
            return model
            
        except Exception as e:
            self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_checkpoint_file(self, checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """μ²΄ν¬ν¬μΈνΈ νμΌ λ΅λ”©"""
        try:
            if not checkpoint_path.exists():
                self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ νμΌ μ—†μ: {checkpoint_path}")
                return None
            
            # PyTorch μ²΄ν¬ν¬μΈνΈ λ΅λ”©
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° μ •κ·ν™”
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    return checkpoint['state_dict']
                elif 'model' in checkpoint:
                    return checkpoint['model']
                else:
                    return checkpoint
            else:
                return checkpoint.state_dict()
                
        except Exception as e:
            self.logger.error(f"β μ²΄ν¬ν¬μΈνΈ νμΌ λ΅λ”© μ‹¤ν¨: {e}")
            return None
    
    def _load_weights(self, checkpoint_data: Dict[str, Any], model: nn.Module, checkpoint_info: Dict[str, Any]):
        """κ°€μ¤‘μΉ λ΅λ”©"""
        try:
            model_state_dict = model.state_dict()
            mapped_dict = {}
            
            key_mappings = checkpoint_info['mappings']
            
            # ν‚¤ λ§¤ν•‘ μ‹¤ν–‰
            for checkpoint_key, value in checkpoint_data.items():
                mapped_key = self._map_key(checkpoint_key, model_state_dict, key_mappings)
                if mapped_key:
                    mapped_dict[mapped_key] = value
            
            # λ§¤ν•‘λ κ°€μ¤‘μΉ λ΅λ“
            if mapped_dict:
                missing_keys, unexpected_keys = model.load_state_dict(mapped_dict, strict=False)
                self.logger.info(f"β… κ°€μ¤‘μΉ λ΅λ”© μ„±κ³µ: {len(mapped_dict)}κ° ν‚¤")
                
                if missing_keys:
                    self.logger.warning(f"β οΈ λ„λ½λ ν‚¤: {len(missing_keys)}κ°")
                if unexpected_keys:
                    self.logger.warning(f"β οΈ μμƒμΉ λ»ν• ν‚¤: {len(unexpected_keys)}κ°")
            else:
                self.logger.warning("β οΈ λ§¤ν•‘λ ν‚¤κ°€ μ—†μ - λλ¤ μ΄κΈ°ν™” μ‚¬μ©")
                
        except Exception as e:
            self.logger.error(f"β κ°€μ¤‘μΉ λ΅λ”© μ‹¤ν¨: {e}")
    
    def _map_key(self, checkpoint_key: str, model_state_dict: Dict[str, Any], key_mappings: Dict[str, str]) -> Optional[str]:
        """ν‚¤ λ§¤ν•‘"""
        # 1. μ§μ ‘ λ§¤ν•‘
        if checkpoint_key in key_mappings:
            model_key = key_mappings[checkpoint_key]
            if model_key in model_state_dict:
                return model_key
        
        # 2. module. μ ‘λ‘μ‚¬ μ κ±°
        clean_key = checkpoint_key.replace('module.', '')
        if clean_key in model_state_dict:
            return clean_key
        
        # 3. ν¨ν„΄ λ§¤ν•‘
        for pattern, replacement in key_mappings.items():
            if pattern in checkpoint_key:
                mapped_key = checkpoint_key.replace(pattern, replacement)
                if mapped_key in model_state_dict:
                    return mapped_key
        
        # 4. μ§μ ‘ λ§¤μΉ­
        if checkpoint_key in model_state_dict:
            return checkpoint_key
        
        return None


class UnifiedGraphonomyCheckpointSystem:
    """ν†µν•©λ Graphonomy μ²΄ν¬ν¬μΈνΈ μ‹μ¤ν…"""
    
    def __init__(self):
        self.logger = logger
        self.loader = GraphonomyCheckpointLoader()
    
    def create_model_from_checkpoint(self, checkpoint_data: Dict[str, Any], device: str = 'cpu') -> Optional[nn.Module]:
        """μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈ μƒμ„± (κΈ°μ΅΄ λ©”μ„λ“ νΈν™μ„±)"""
        try:
            # μ²΄ν¬ν¬μΈνΈ λ¶„μ„
            analyzer = GraphonomyCheckpointAnalyzer()
            checkpoint_info = analyzer.analyze_checkpoint(checkpoint_data)
            
            # λ¨λΈ μƒμ„±
            model_factory = GraphonomyModelFactory()
            model = model_factory.create_model(checkpoint_info)
            
            # κ°€μ¤‘μΉ λ΅λ”©
            self.loader._load_weights(checkpoint_data, model, checkpoint_info)
            
            # λ¨λΈ μ„¤μ •
            model.to(device)
            model.eval()
            
            return model
            
        except Exception as e:
            self.logger.error(f"β μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈ μƒμ„± μ‹¤ν¨: {e}")
            return None
    
    def load_model_from_file(self, checkpoint_path: Path, device: str = 'cpu') -> Optional[nn.Module]:
        """νμΌμ—μ„ λ¨λΈ λ΅λ”©"""
        return self.loader.load_checkpoint(checkpoint_path, device)
    
    def get_checkpoint_info(self, checkpoint_data: Dict[str, Any]) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ μ •λ³΄ λ°ν™"""
        analyzer = GraphonomyCheckpointAnalyzer()
        return analyzer.analyze_checkpoint(checkpoint_data) 