#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Metrics Utility
================================

ë©”íŠ¸ë¦­ ê³„ì‚° ê´€ë ¨ í•¨ìˆ˜ë“¤
- OOTD, VITON-HD, Diffusion ë©”íŠ¸ë¦­ ê³„ì‚°
- SSIM, LPIPS, í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜ë“¤

Author: MyCloset AI Team
Date: 2025-07-31
Version: 1.0
"""

# Common imports
from app.ai_pipeline.utils.common_imports import (
    torch, nn, F, TORCH_AVAILABLE,
    logging, Dict, Any, Optional, Tuple
)

if not TORCH_AVAILABLE:
    raise ImportError("PyTorch is required for metrics calculation")

def calculate_ootd_metrics(fitted_tensor: torch.Tensor, person_tensor: torch.Tensor, 
                          cloth_tensor: torch.Tensor) -> Dict[str, float]:
    """OOTD ëª¨ë¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    try:
        metrics = {}
        
        # SSIM ê³„ì‚°
        metrics['ssim'] = calculate_ssim(fitted_tensor, person_tensor)
        
        # LPIPS ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
        metrics['lpips'] = calculate_lpips_simple(fitted_tensor, person_tensor)
        
        # í”¼íŒ… í’ˆì§ˆ í‰ê°€
        metrics['fitting_quality'] = assess_fitting_quality_tensor(fitted_tensor, person_tensor, cloth_tensor)
        
        # í…ìŠ¤ì²˜ ë³´ì¡´ë„
        metrics['texture_preservation'] = calculate_texture_preservation(fitted_tensor, cloth_tensor)
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ
        metrics['image_quality'] = assess_image_quality(fitted_tensor)
        
        # ì˜ë¥˜ ìœ ì‚¬ë„
        metrics['cloth_similarity'] = calculate_cloth_similarity(fitted_tensor, cloth_tensor)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        metrics['overall_quality'] = (
            metrics['ssim'] * 0.3 +
            metrics['fitting_quality'] * 0.3 +
            metrics['texture_preservation'] * 0.2 +
            metrics['image_quality'] * 0.2
        )
        
        return metrics
        
    except Exception as e:
        logging.error(f"OOTD ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'ssim': 0.5, 'lpips': 0.5, 'fitting_quality': 0.5,
            'texture_preservation': 0.5, 'image_quality': 0.5,
            'cloth_similarity': 0.5, 'overall_quality': 0.5
        }

def calculate_viton_metrics(fitted_tensor: torch.Tensor, person_tensor: torch.Tensor,
                           cloth_tensor: torch.Tensor, result: Dict[str, Any]) -> Dict[str, float]:
    """VITON-HD ëª¨ë¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    try:
        metrics = {}
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ë“¤
        metrics['ssim'] = calculate_ssim(fitted_tensor, person_tensor)
        metrics['lpips'] = calculate_lpips_simple(fitted_tensor, person_tensor)
        metrics['fitting_quality'] = assess_fitting_quality_tensor(fitted_tensor, person_tensor, cloth_tensor)
        
        # VITON-HD íŠ¹í™” ë©”íŠ¸ë¦­
        if 'flow' in result:
            metrics['flow_consistency'] = calculate_flow_consistency(result['flow'])
            metrics['warping_quality'] = assess_warping_quality(fitted_tensor, cloth_tensor, result['flow'])
        
        # í…ìŠ¤ì²˜ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­
        metrics['texture_preservation'] = calculate_texture_preservation(fitted_tensor, cloth_tensor)
        metrics['image_quality'] = assess_image_quality(fitted_tensor)
        metrics['cloth_similarity'] = calculate_cloth_similarity(fitted_tensor, cloth_tensor)
        metrics['realism'] = assess_realism(fitted_tensor)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        metrics['overall_quality'] = (
            metrics['ssim'] * 0.25 +
            metrics['fitting_quality'] * 0.25 +
            metrics.get('flow_consistency', 0.5) * 0.15 +
            metrics['texture_preservation'] * 0.15 +
            metrics['image_quality'] * 0.1 +
            metrics['realism'] * 0.1
        )
        
        return metrics
        
    except Exception as e:
        logging.error(f"VITON-HD ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'ssim': 0.5, 'lpips': 0.5, 'fitting_quality': 0.5,
            'flow_consistency': 0.5, 'warping_quality': 0.5,
            'texture_preservation': 0.5, 'image_quality': 0.5,
            'cloth_similarity': 0.5, 'realism': 0.5, 'overall_quality': 0.5
        }

def calculate_diffusion_metrics(fitted_tensor: torch.Tensor, person_tensor: torch.Tensor,
                               cloth_tensor: torch.Tensor) -> Dict[str, float]:
    """Diffusion ëª¨ë¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    try:
        metrics = {}
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ë“¤
        metrics['ssim'] = calculate_ssim(fitted_tensor, person_tensor)
        metrics['lpips'] = calculate_lpips_simple(fitted_tensor, person_tensor)
        metrics['fitting_quality'] = assess_fitting_quality_tensor(fitted_tensor, person_tensor, cloth_tensor)
        
        # Diffusion íŠ¹í™” ë©”íŠ¸ë¦­
        metrics['realism'] = assess_realism(fitted_tensor)
        metrics['texture_preservation'] = calculate_texture_preservation(fitted_tensor, cloth_tensor)
        metrics['image_quality'] = assess_image_quality(fitted_tensor)
        metrics['cloth_similarity'] = calculate_cloth_similarity(fitted_tensor, cloth_tensor)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        metrics['overall_quality'] = (
            metrics['ssim'] * 0.2 +
            metrics['fitting_quality'] * 0.2 +
            metrics['realism'] * 0.25 +
            metrics['texture_preservation'] * 0.15 +
            metrics['image_quality'] * 0.2
        )
        
        return metrics
        
    except Exception as e:
        logging.error(f"Diffusion ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'ssim': 0.5, 'lpips': 0.5, 'fitting_quality': 0.5,
            'realism': 0.5, 'texture_preservation': 0.5,
            'image_quality': 0.5, 'cloth_similarity': 0.5, 'overall_quality': 0.5
        }

def calculate_ssim(img1: torch.Tensor, img2: torch.Tensor, window_size: int = 11) -> float:
    """SSIM (Structural Similarity Index) ê³„ì‚°"""
    try:
        if img1.shape != img2.shape:
            return 0.5
        
        # ì •ê·œí™”
        img1 = (img1 - img1.min()) / (img1.max() - img1.min() + 1e-8)
        img2 = (img2 - img2.min()) / (img2.max() - img2.min() + 1e-8)
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if img1.shape[1] == 3:
            img1 = 0.299 * img1[:, 0] + 0.587 * img1[:, 1] + 0.114 * img1[:, 2]
        if img2.shape[1] == 3:
            img2 = 0.299 * img2[:, 0] + 0.587 * img2[:, 1] + 0.114 * img2[:, 2]
        
        # ê°„ë‹¨í•œ SSIM ê³„ì‚°
        mu1 = F.avg_pool2d(img1, window_size, stride=1, padding=window_size//2)
        mu2 = F.avg_pool2d(img2, window_size, stride=1, padding=window_size//2)
        
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.avg_pool2d(img1 * img1, window_size, stride=1, padding=window_size//2) - mu1_sq
        sigma2_sq = F.avg_pool2d(img2 * img2, window_size, stride=1, padding=window_size//2) - mu2_sq
        sigma12 = F.avg_pool2d(img1 * img2, window_size, stride=1, padding=window_size//2) - mu1_mu2
        
        C1 = 0.01**2
        C2 = 0.03**2
        
        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        
        return ssim_map.mean().item()
        
    except Exception as e:
        logging.error(f"SSIM ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def calculate_lpips_simple(img1: torch.Tensor, img2: torch.Tensor) -> float:
    """ê°„ë‹¨í•œ LPIPS ê³„ì‚° (ì‹¤ì œ LPIPS ëŒ€ì‹  MSE ê¸°ë°˜)"""
    try:
        if img1.shape != img2.shape:
            return 0.5
        
        # ì •ê·œí™”
        img1 = (img1 - img1.min()) / (img1.max() - img1.min() + 1e-8)
        img2 = (img2 - img2.min()) / (img2.max() - img2.min() + 1e-8)
        
        # MSE ê³„ì‚°
        mse = F.mse_loss(img1, img2)
        
        # LPIPS ìŠ¤íƒ€ì¼ ì ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        lpips_score = 1.0 / (1.0 + mse.item())
        
        return lpips_score
        
    except Exception as e:
        logging.error(f"LPIPS ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def assess_fitting_quality_tensor(fitted_tensor: torch.Tensor, person_tensor: torch.Tensor,
                                 cloth_tensor: torch.Tensor) -> float:
    """í…ì„œ ê¸°ë°˜ í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
    try:
        # êµ¬ì¡°ì  ìœ ì‚¬ì„±
        structural_similarity = calculate_ssim(fitted_tensor, person_tensor)
        
        # ì˜ë¥˜ ë³´ì¡´ë„
        cloth_preservation = calculate_ssim(fitted_tensor, cloth_tensor)
        
        # ìƒ‰ìƒ ì¼ê´€ì„±
        color_consistency = calculate_color_consistency(fitted_tensor, person_tensor)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        quality_score = (
            structural_similarity * 0.4 +
            cloth_preservation * 0.3 +
            color_consistency * 0.3
        )
        
        return quality_score
        
    except Exception as e:
        logging.error(f"í”¼íŒ… í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5

def calculate_flow_consistency(flow: torch.Tensor) -> float:
    """Flow ì¼ê´€ì„± ê³„ì‚°"""
    try:
        # Flowì˜ ë¶€ë“œëŸ¬ì›€ ê³„ì‚°
        flow_grad_x = torch.gradient(flow[:, 0], dim=1)[0]
        flow_grad_y = torch.gradient(flow[:, 1], dim=2)[0]
        
        # Flow ë³€í™”ëŸ‰
        flow_magnitude = torch.sqrt(flow_grad_x**2 + flow_grad_y**2)
        
        # ì¼ê´€ì„± ì ìˆ˜ (ë³€í™”ëŸ‰ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        consistency = 1.0 / (1.0 + flow_magnitude.mean().item())
        
        return consistency
        
    except Exception as e:
        logging.error(f"Flow ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def assess_warping_quality(fitted_tensor: torch.Tensor, cloth_tensor: torch.Tensor,
                          flow: torch.Tensor) -> float:
    """ì›Œí•‘ í’ˆì§ˆ í‰ê°€"""
    try:
        # ì˜ë¥˜ ë³´ì¡´ë„
        cloth_preservation = calculate_ssim(fitted_tensor, cloth_tensor)
        
        # Flow ì¼ê´€ì„±
        flow_consistency = calculate_flow_consistency(flow)
        
        # êµ¬ì¡°ì  ë¬´ê²°ì„±
        structural_integrity = assess_structural_integrity(fitted_tensor)
        
        # ì „ì²´ ì›Œí•‘ í’ˆì§ˆ
        warping_quality = (
            cloth_preservation * 0.4 +
            flow_consistency * 0.3 +
            structural_integrity * 0.3
        )
        
        return warping_quality
        
    except Exception as e:
        logging.error(f"ì›Œí•‘ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5

def calculate_texture_preservation(fitted_tensor: torch.Tensor, cloth_tensor: torch.Tensor) -> float:
    """í…ìŠ¤ì²˜ ë³´ì¡´ë„ ê³„ì‚°"""
    try:
        # ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ (í…ìŠ¤ì²˜)
        kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        fitted_texture = F.conv2d(fitted_tensor.mean(dim=1, keepdim=True), kernel, padding=1)
        cloth_texture = F.conv2d(cloth_tensor.mean(dim=1, keepdim=True), kernel, padding=1)
        
        # í…ìŠ¤ì²˜ ìœ ì‚¬ë„
        texture_similarity = calculate_ssim(fitted_texture, cloth_texture)
        
        return texture_similarity
        
    except Exception as e:
        logging.error(f"í…ìŠ¤ì²˜ ë³´ì¡´ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def assess_image_quality(fitted_tensor: torch.Tensor) -> float:
    """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
    try:
        # ì„ ëª…ë„ ê³„ì‚° (Laplacian ë¶„ì‚°)
        gray = fitted_tensor.mean(dim=1, keepdim=True)
        laplacian = F.conv2d(gray, torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0), padding=1)
        sharpness = laplacian.var().item()
        
        # ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •
        noise_level = estimate_noise_level(fitted_tensor)
        
        # í’ˆì§ˆ ì ìˆ˜
        quality_score = min(1.0, sharpness / 100.0) * (1.0 - noise_level)
        
        return quality_score
        
    except Exception as e:
        logging.error(f"ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5

def calculate_cloth_similarity(fitted_tensor: torch.Tensor, cloth_tensor: torch.Tensor) -> float:
    """ì˜ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        # ìƒ‰ìƒ ìœ ì‚¬ë„
        color_similarity = calculate_color_consistency(fitted_tensor, cloth_tensor)
        
        # í…ìŠ¤ì²˜ ìœ ì‚¬ë„
        texture_similarity = calculate_texture_preservation(fitted_tensor, cloth_tensor)
        
        # ì „ì²´ ìœ ì‚¬ë„
        similarity = (color_similarity + texture_similarity) / 2.0
        
        return similarity
        
    except Exception as e:
        logging.error(f"ì˜ë¥˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def assess_realism(fitted_tensor: torch.Tensor) -> float:
    """í˜„ì‹¤ê° í‰ê°€"""
    try:
        # ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ìƒ ë¶„í¬
        color_naturalness = assess_color_naturalness(fitted_tensor)
        
        # êµ¬ì¡°ì  ì¼ê´€ì„±
        structural_consistency = assess_structural_integrity(fitted_tensor)
        
        # ì „ì²´ í˜„ì‹¤ê°
        realism = (color_naturalness + structural_consistency) / 2.0
        
        return realism
        
    except Exception as e:
        logging.error(f"í˜„ì‹¤ê° í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5

def calculate_color_consistency(img1: torch.Tensor, img2: torch.Tensor) -> float:
    """ìƒ‰ìƒ ì¼ê´€ì„± ê³„ì‚°"""
    try:
        # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
        hist1 = torch.histc(img1, bins=256, min=0, max=1)
        hist2 = torch.histc(img2, bins=256, min=0, max=1)
        
        # ì •ê·œí™”
        hist1 = hist1 / hist1.sum()
        hist2 = hist2 / hist2.sum()
        
        # íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„
        similarity = 1.0 - torch.abs(hist1 - hist2).sum() / 2.0
        
        return similarity.item()
        
    except Exception as e:
        logging.error(f"ìƒ‰ìƒ ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.5

def assess_structural_integrity(fitted_tensor: torch.Tensor) -> float:
    """êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€"""
    try:
        # ì—£ì§€ ì¼ê´€ì„±
        gray = fitted_tensor.mean(dim=1, keepdim=True)
        edges = F.conv2d(gray, torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0), padding=1)
        
        # ì—£ì§€ ê°•ë„ì˜ ì¼ê´€ì„±
        edge_consistency = 1.0 / (1.0 + edges.std().item())
        
        return edge_consistency
        
    except Exception as e:
        logging.error(f"êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5

def estimate_noise_level(fitted_tensor: torch.Tensor) -> float:
    """ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •"""
    try:
        # ê³ ì£¼íŒŒ ì„±ë¶„ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
        kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        high_freq = F.conv2d(fitted_tensor.mean(dim=1, keepdim=True), kernel, padding=1)
        
        # ë…¸ì´ì¦ˆ ë ˆë²¨
        noise_level = high_freq.abs().mean().item()
        
        return min(1.0, noise_level)
        
    except Exception as e:
        logging.error(f"ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì • ì‹¤íŒ¨: {e}")
        return 0.5

def assess_color_naturalness(fitted_tensor: torch.Tensor) -> float:
    """ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
    try:
        # ìƒ‰ìƒ ë¶„í¬ì˜ ìì—°ìŠ¤ëŸ¬ì›€ (ê°„ë‹¨í•œ ë²„ì „)
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìƒ‰ìƒ ëª¨ë¸ ì‚¬ìš©
        
        # ìƒ‰ìƒ ë²”ìœ„ ì²´í¬
        color_range = fitted_tensor.max() - fitted_tensor.min()
        naturalness = min(1.0, color_range.item())
        
        return naturalness
        
    except Exception as e:
        logging.error(f"ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ì‹¤íŒ¨: {e}")
        return 0.5 