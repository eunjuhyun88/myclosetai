# backend/app/ai_pipeline/utils/neural_architecture_manager.py
"""
ğŸ§  Neural Architecture Manager - ì‹ ê²½ë§ ìˆ˜ì¤€ ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
================================================================================
âœ… Neural Architecture Pattern - ëª¨ë“  ëª¨ë¸ì„ nn.Moduleë¡œ í†µí•©
âœ… Memory-Efficient Loading - ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”©
âœ… Dynamic Computation Graph - AutoGrad ì™„ì „ í™œìš©
âœ… Hardware-Aware Optimization - M3 Max MPS ìµœì í™”
âœ… Gradient-Free Inference - ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ìµœì í™”

í•µì‹¬ ì‹ ê²½ë§ ì„¤ê³„ ì›ì¹™:
1. ëª¨ë“  ëª¨ë¸ì„ nn.Moduleë¡œ í†µí•©
2. ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
3. AutoGrad ê¸°ë°˜ ë™ì  ê·¸ë˜í”„ ì™„ì „ í™œìš©
4. M3 Max í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™”
5. ì¶”ë¡  ì‹œ gradient-free ëª¨ë“œë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
================================================================================
"""

import os
import time
import logging
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import gc

# PyTorch imports
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ íŒ¨í„´ ì •ì˜
# ==============================================

class NeuralArchitectureType(Enum):
    """ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ íƒ€ì…"""
    RESNET = "resnet"
    VIT = "vision_transformer"
    CONV_NEXT = "convnext"
    EFFICIENT_NET = "efficientnet"
    SWIN_TRANSFORMER = "swin_transformer"
    CUSTOM = "custom"

@dataclass
class LayerConfig:
    """ë ˆì´ì–´ ì„¤ì •"""
    layer_type: str
    in_channels: int
    out_channels: int
    kernel_size: int = 3
    stride: int = 1
    padding: int = 1
    activation: str = "relu"
    normalization: str = "batch_norm"
    dropout: float = 0.0
    memory_priority: int = 1  # 1-10, ë†’ì„ìˆ˜ë¡ ìš°ì„  ë¡œë”©

@dataclass
class NeuralArchitectureConfig:
    """ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì„¤ì •"""
    architecture_type: NeuralArchitectureType
    input_size: Tuple[int, int] = (512, 512)
    num_classes: int = 20
    layers: List[LayerConfig] = field(default_factory=list)
    memory_limit_gb: float = 8.0
    enable_gradient_checkpointing: bool = True
    precision: str = "float32"  # float16, float32
    device: str = "auto"

# ==============================================
# ğŸ”¥ 2. ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”© ì‹œìŠ¤í…œ
# ==============================================

class ProgressiveLayerLoader:
    """ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: NeuralArchitectureConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.ProgressiveLayerLoader")
        self.loaded_layers = {}
        self.layer_memory_usage = {}
        self.loading_queue = []
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_threshold = config.memory_limit_gb * 0.8
        self.current_memory_usage = 0.0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
    def add_layer(self, layer_name: str, layer_config: LayerConfig, 
                  layer_module: nn.Module) -> bool:
        """ë ˆì´ì–´ ì¶”ê°€ (ë©”ëª¨ë¦¬ ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                estimated_memory = self._estimate_layer_memory(layer_config)
                
                # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ íì— ì¶”ê°€
                priority_item = {
                    'name': layer_name,
                    'config': layer_config,
                    'module': layer_module,
                    'memory': estimated_memory,
                    'priority': layer_config.memory_priority
                }
                
                self.loading_queue.append(priority_item)
                
                # ìš°ì„ ìˆœìœ„ ì •ë ¬
                self.loading_queue.sort(key=lambda x: x['priority'], reverse=True)
                
                self.logger.debug(f"âœ… ë ˆì´ì–´ ì¶”ê°€: {layer_name} (ë©”ëª¨ë¦¬: {estimated_memory:.2f}MB)")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ë ˆì´ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def load_layers_progressively(self, target_memory_gb: float = None) -> Dict[str, bool]:
        """ì ì§„ì  ë ˆì´ì–´ ë¡œë”©"""
        if target_memory_gb is None:
            target_memory_gb = self.memory_threshold
            
        try:
            with self._lock:
                results = {}
                current_memory = self.current_memory_usage
                
                for item in self.loading_queue:
                    layer_name = item['name']
                    
                    # ì´ë¯¸ ë¡œë“œëœ ë ˆì´ì–´ ìŠ¤í‚µ
                    if layer_name in self.loaded_layers:
                        results[layer_name] = True
                        continue
                    
                    # ë©”ëª¨ë¦¬ í•œê³„ í™•ì¸
                    if current_memory + item['memory'] > target_memory_gb * 1024:  # GB to MB
                        self.logger.debug(f"âš ï¸ ë©”ëª¨ë¦¬ í•œê³„ ë„ë‹¬: {layer_name} ë¡œë”© ê±´ë„ˆëœ€")
                        results[layer_name] = False
                        continue
                    
                    # ë ˆì´ì–´ ë¡œë”©
                    try:
                        self._load_single_layer(item)
                        self.loaded_layers[layer_name] = item['module']
                        self.layer_memory_usage[layer_name] = item['memory']
                        current_memory += item['memory']
                        results[layer_name] = True
                        
                        self.logger.info(f"âœ… ë ˆì´ì–´ ë¡œë”© ì™„ë£Œ: {layer_name} ({item['memory']:.2f}MB)")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ ë ˆì´ì–´ ë¡œë”© ì‹¤íŒ¨: {layer_name} - {e}")
                        results[layer_name] = False
                
                self.current_memory_usage = current_memory
                return results
                
        except Exception as e:
            self.logger.error(f"âŒ ì ì§„ì  ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_single_layer(self, layer_item: Dict[str, Any]):
        """ë‹¨ì¼ ë ˆì´ì–´ ë¡œë”©"""
        layer_name = layer_item['name']
        layer_module = layer_item['module']
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        device = self._get_optimal_device()
        layer_module.to(device)
        
        # ì •ë°€ë„ ì„¤ì •
        if self.config.precision == "float16":
            layer_module.half()
        
        # í‰ê°€ ëª¨ë“œ ì„¤ì •
        layer_module.eval()
        
        # Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
        if self.config.enable_gradient_checkpointing:
            layer_module = torch.utils.checkpoint.checkpoint_wrapper(layer_module)
        
        self.logger.debug(f"âœ… ë‹¨ì¼ ë ˆì´ì–´ ë¡œë”© ì™„ë£Œ: {layer_name}")
    
    def _estimate_layer_memory(self, layer_config: LayerConfig) -> float:
        """ë ˆì´ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        try:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ê³„ì‚°
            base_memory = layer_config.in_channels * layer_config.out_channels * layer_config.kernel_size ** 2
            
            # ì •ë°€ë„ë³„ ë©”ëª¨ë¦¬ ê³„ì‚°
            if self.config.precision == "float16":
                memory_bytes = base_memory * 2  # 2 bytes per parameter
            else:
                memory_bytes = base_memory * 4  # 4 bytes per parameter
            
            # ì¶”ê°€ ë©”ëª¨ë¦¬ (activation, gradients ë“±)
            total_memory = memory_bytes * 3  # 3x for activations and gradients
            
            return total_memory / (1024 * 1024)  # Convert to MB
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì¶”ì • ì‹¤íŒ¨: {e}")
            return 100.0  # ê¸°ë³¸ê°’ 100MB
    
    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if self.config.device == "auto":
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
                else:
                    return "cpu"
            else:
                return "cpu"
        return self.config.device

# ==============================================
# ğŸ”¥ 3. ë™ì  ê³„ì‚° ê·¸ë˜í”„ ê´€ë¦¬
# ==============================================

class DynamicComputationGraph:
    """AutoGrad ê¸°ë°˜ ë™ì  ê³„ì‚° ê·¸ë˜í”„ ê´€ë¦¬"""
    
    def __init__(self, enable_autograd: bool = True):
        self.enable_autograd = enable_autograd
        self.logger = logging.getLogger(f"{__name__}.DynamicComputationGraph")
        self.graph_nodes = {}
        self.computation_paths = []
        
    def register_node(self, node_name: str, node_module: nn.Module, 
                     dependencies: List[str] = None) -> bool:
        """ê³„ì‚° ê·¸ë˜í”„ ë…¸ë“œ ë“±ë¡"""
        try:
            self.graph_nodes[node_name] = {
                'module': node_module,
                'dependencies': dependencies or [],
                'output_cache': None,
                'gradient_mode': self.enable_autograd
            }
            
            self.logger.debug(f"âœ… ê·¸ë˜í”„ ë…¸ë“œ ë“±ë¡: {node_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¸ë˜í”„ ë…¸ë“œ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def execute_forward_pass(self, input_tensor: torch.Tensor, 
                           target_nodes: List[str] = None) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ ì‹¤í–‰ (ë™ì  ê·¸ë˜í”„)"""
        try:
            if target_nodes is None:
                target_nodes = list(self.graph_nodes.keys())
            
            results = {}
            
            for node_name in target_nodes:
                if node_name not in self.graph_nodes:
                    continue
                
                node_info = self.graph_nodes[node_name]
                module = node_info['module']
                
                # ì˜ì¡´ì„± í™•ì¸
                if not self._check_dependencies(node_name, results):
                    continue
                
                # ì…ë ¥ ì¤€ë¹„
                node_input = self._prepare_node_input(node_name, input_tensor, results)
                
                # ìˆœì „íŒŒ ì‹¤í–‰
                with torch.set_grad_enabled(node_info['gradient_mode']):
                    if node_info['gradient_mode']:
                        # Gradient ëª¨ë“œ
                        output = module(node_input)
                    else:
                        # Gradient-free ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        with torch.no_grad():
                            output = module(node_input)
                
                # ê²°ê³¼ ìºì‹±
                node_info['output_cache'] = output
                results[node_name] = output
                
                self.logger.debug(f"âœ… ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ: {node_name}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì „íŒŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def execute_backward_pass(self, loss: torch.Tensor, 
                            target_nodes: List[str] = None) -> Dict[str, torch.Tensor]:
        """ì—­ì „íŒŒ ì‹¤í–‰ (AutoGrad)"""
        if not self.enable_autograd:
            self.logger.warning("âš ï¸ AutoGrad ë¹„í™œì„±í™” - ì—­ì „íŒŒ ê±´ë„ˆëœ€")
            return {}
        
        try:
            if target_nodes is None:
                target_nodes = list(self.graph_nodes.keys())
            
            gradients = {}
            
            # ì—­ì „íŒŒ ì‹¤í–‰
            loss.backward()
            
            for node_name in target_nodes:
                if node_name not in self.graph_nodes:
                    continue
                
                module = self.graph_nodes[node_name]['module']
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ìˆ˜ì§‘
                node_gradients = []
                for param in module.parameters():
                    if param.grad is not None:
                        node_gradients.append(param.grad.clone())
                
                if node_gradients:
                    gradients[node_name] = node_gradients
                
                self.logger.debug(f"âœ… ê·¸ë˜ë””ì–¸íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {node_name}")
            
            return gradients
            
        except Exception as e:
            self.logger.error(f"âŒ ì—­ì „íŒŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    def _check_dependencies(self, node_name: str, 
                          available_results: Dict[str, torch.Tensor]) -> bool:
        """ì˜ì¡´ì„± í™•ì¸"""
        node_info = self.graph_nodes[node_name]
        dependencies = node_info['dependencies']
        
        for dep in dependencies:
            if dep not in available_results:
                self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ëˆ„ë½: {node_name} -> {dep}")
                return False
        
        return True
    
    def _prepare_node_input(self, node_name: str, base_input: torch.Tensor,
                           available_results: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ë…¸ë“œ ì…ë ¥ ì¤€ë¹„"""
        node_info = self.graph_nodes[node_name]
        dependencies = node_info['dependencies']
        
        if not dependencies:
            return base_input
        
        # ì˜ì¡´ì„± ì¶œë ¥ë“¤ì„ ê²°í•©
        dependency_outputs = [available_results[dep] for dep in dependencies]
        
        if len(dependency_outputs) == 1:
            return dependency_outputs[0]
        else:
            # ì—¬ëŸ¬ ì˜ì¡´ì„±ì´ ìˆëŠ” ê²½ìš° concatenate
            return torch.cat(dependency_outputs, dim=1)

# ==============================================
# ğŸ”¥ 4. í•˜ë“œì›¨ì–´ ì¸ì‹ ìµœì í™”
# ==============================================

class HardwareAwareOptimizer:
    """M3 Max í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™”"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._detect_optimal_device(device)
        self.logger = logging.getLogger(f"{__name__}.HardwareAwareOptimizer")
        self.is_m3_max = self._detect_m3_max()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self._setup_m3_max_optimizations()
    
    def _detect_optimal_device(self, device: str) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        if device == "auto":
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
                else:
                    return "cpu"
            else:
                return "cpu"
        return device
    
    def _detect_m3_max(self) -> bool:
        """M3 Max í™˜ê²½ ê°ì§€"""
        try:
            import platform
            import psutil
            
            # Apple Silicon í™•ì¸
            is_apple_silicon = platform.system() == "Darwin" and platform.machine() == "arm64"
            
            # ë©”ëª¨ë¦¬ ìš©ëŸ‰ í™•ì¸ (M3 MaxëŠ” 128GB)
            memory_gb = psutil.virtual_memory().total / (1024**3)
            
            # CPU ì½”ì–´ ìˆ˜ í™•ì¸ (M3 MaxëŠ” 12ì½”ì–´ ì´ìƒ)
            cpu_cores = psutil.cpu_count(logical=False) or 4
            
            return is_apple_silicon and memory_gb >= 120 and cpu_cores >= 12
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def _setup_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì„¤ì •"""
        try:
            if not self.is_m3_max:
                return
            
            self.logger.info("ğŸ M3 Max íŠ¹í™” ìµœì í™” ì„¤ì •")
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ.update({
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                'METAL_DEVICE_WRAPPER_TYPE': '1',
                'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                'PYTORCH_MPS_PREFER_METAL': '1',
                'PYTORCH_ENABLE_MPS_FALLBACK': '1'
            })
            
            # PyTorch ìŠ¤ë ˆë“œ ìµœì í™”
            if TORCH_AVAILABLE:
                torch.set_num_threads(min(16, os.cpu_count() or 8))
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def optimize_model_for_device(self, model: nn.Module) -> nn.Module:
        """ë””ë°”ì´ìŠ¤ë³„ ëª¨ë¸ ìµœì í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì´ë™
            model.to(self.device)
            
            # M3 Max íŠ¹í™” ìµœì í™”
            if self.is_m3_max and self.device == "mps":
                model = self._apply_m3_max_optimizations(model)
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            model.eval()
            
            self.logger.info(f"âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ (device: {self.device})")
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    def _apply_m3_max_optimizations(self, model: nn.Module) -> nn.Module:
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            # Float32 ê°•ì œ ì‚¬ìš© (M3 Maxì—ì„œ ì•ˆì •ì„±)
            model = model.float()
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì—°ì‚° ì„¤ì •
            for module in model.modules():
                if isinstance(module, nn.Conv2d):
                    # M3 Maxì—ì„œ ìµœì í™”ëœ ì»¨ë³¼ë£¨ì…˜ ì„¤ì •
                    module.padding_mode = 'zeros'
                elif isinstance(module, nn.BatchNorm2d):
                    # ë°°ì¹˜ ì •ê·œí™” ìµœì í™”
                    module.track_running_stats = True
            
            self.logger.debug("âœ… M3 Max íŠ¹í™” ìµœì í™” ì ìš© ì™„ë£Œ")
            return model
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
            return model

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ë§¤ë‹ˆì €
# ==============================================

class NeuralArchitectureManager:
    """ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, config: NeuralArchitectureConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.NeuralArchitectureManager")
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.layer_loader = ProgressiveLayerLoader(config)
        self.computation_graph = DynamicComputationGraph(enable_autograd=True)
        self.hardware_optimizer = HardwareAwareOptimizer(config.device)
        
        # ëª¨ë¸ ìƒíƒœ
        self.model = None
        self.is_initialized = False
        self.memory_usage = 0.0
        
        # ì´ˆê¸°í™”
        self._initialize_architecture()
    
    def _initialize_architecture(self):
        """ì•„í‚¤í…ì²˜ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ§  ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì´ˆê¸°í™” ì‹œì‘")
            
            # ì•„í‚¤í…ì²˜ íƒ€ì…ë³„ ëª¨ë¸ ìƒì„±
            self.model = self._create_architecture()
            
            # í•˜ë“œì›¨ì–´ ìµœì í™” ì ìš©
            self.model = self.hardware_optimizer.optimize_model_for_device(self.model)
            
            # ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”© ì„¤ì •
            self._setup_progressive_loading()
            
            # ê³„ì‚° ê·¸ë˜í”„ ë“±ë¡
            self._register_computation_graph()
            
            self.is_initialized = True
            self.logger.info("âœ… ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì•„í‚¤í…ì²˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
    
    def _create_architecture(self) -> nn.Module:
        """ì•„í‚¤í…ì²˜ íƒ€ì…ë³„ ëª¨ë¸ ìƒì„±"""
        try:
            if self.config.architecture_type == NeuralArchitectureType.RESNET:
                return self._create_resnet_architecture()
            elif self.config.architecture_type == NeuralArchitectureType.VIT:
                return self._create_vision_transformer_architecture()
            elif self.config.architecture_type == NeuralArchitectureType.CONV_NEXT:
                return self._create_convnext_architecture()
            elif self.config.architecture_type == NeuralArchitectureType.EFFICIENT_NET:
                return self._create_efficientnet_architecture()
            elif self.config.architecture_type == NeuralArchitectureType.SWIN_TRANSFORMER:
                return self._create_swin_transformer_architecture()
            else:
                return self._create_custom_architecture()
                
        except Exception as e:
            self.logger.error(f"âŒ ì•„í‚¤í…ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_fallback_architecture()
    
    def _create_resnet_architecture(self) -> nn.Module:
        """ResNet ì•„í‚¤í…ì²˜ ìƒì„±"""
        class ResNetArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # ì…ë ¥ ë ˆì´ì–´
                self.input_conv = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
                self.bn1 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                
                # ResNet ë¸”ë¡ë“¤
                self.layer1 = self._make_layer(64, 64, 2)
                self.layer2 = self._make_layer(64, 128, 2, stride=2)
                self.layer3 = self._make_layer(128, 256, 2, stride=2)
                self.layer4 = self._make_layer(256, 512, 2, stride=2)
                
                # ë¶„ë¥˜ í—¤ë“œ
                self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
                self.fc = nn.Linear(512, config.num_classes)
                
            def _make_layer(self, in_channels, out_channels, blocks, stride=1):
                layers = []
                layers.append(self._make_block(in_channels, out_channels, stride))
                for _ in range(1, blocks):
                    layers.append(self._make_block(out_channels, out_channels))
                return nn.Sequential(*layers)
            
            def _make_block(self, in_channels, out_channels, stride=1):
                return nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, 3, stride, 1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, 3, 1, 1),
                    nn.BatchNorm2d(out_channels)
                )
            
            def forward(self, x):
                x = self.input_conv(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.maxpool(x)
                
                x = self.layer1(x)
                x = self.layer2(x)
                x = self.layer3(x)
                x = self.layer4(x)
                
                x = self.avgpool(x)
                x = torch.flatten(x, 1)
                x = self.fc(x)
                
                return x
        
        return ResNetArchitecture(self.config)
    
    def _create_vision_transformer_architecture(self) -> nn.Module:
        """Vision Transformer ì•„í‚¤í…ì²˜ ìƒì„±"""
        class VisionTransformerArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # íŒ¨ì¹˜ ì„ë² ë”©
                patch_size = 16
                embed_dim = 768
                self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)
                
                # ìœ„ì¹˜ ì„ë² ë”©
                num_patches = (config.input_size[0] // patch_size) ** 2
                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
                
                # Transformer ë¸”ë¡ë“¤
                self.transformer_blocks = nn.ModuleList([
                    self._make_transformer_block(embed_dim) for _ in range(12)
                ])
                
                # ë¶„ë¥˜ í—¤ë“œ
                self.norm = nn.LayerNorm(embed_dim)
                self.head = nn.Linear(embed_dim, config.num_classes)
                
            def _make_transformer_block(self, embed_dim):
                return nn.Sequential(
                    nn.LayerNorm(embed_dim),
                    nn.MultiheadAttention(embed_dim, num_heads=12, batch_first=True),
                    nn.LayerNorm(embed_dim),
                    nn.Linear(embed_dim, embed_dim * 4),
                    nn.GELU(),
                    nn.Linear(embed_dim * 4, embed_dim)
                )
            
            def forward(self, x):
                # íŒ¨ì¹˜ ì„ë² ë”©
                x = self.patch_embed(x)
                b, c, h, w = x.shape
                x = x.flatten(2).transpose(1, 2)  # (B, N, C)
                
                # ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
                x = x + self.pos_embed
                
                # Transformer ë¸”ë¡ë“¤
                for block in self.transformer_blocks:
                    x = x + block(x)
                
                # ë¶„ë¥˜
                x = self.norm(x)
                x = x.mean(dim=1)  # Global average pooling
                x = self.head(x)
                
                return x
        
        return VisionTransformerArchitecture(self.config)
    
    def _create_convnext_architecture(self) -> nn.Module:
        """ConvNeXt ì•„í‚¤í…ì²˜ ìƒì„±"""
        class ConvNeXtArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # Stem
                self.stem = nn.Sequential(
                    nn.Conv2d(3, 96, kernel_size=4, stride=4),
                    nn.LayerNorm([96, config.input_size[0]//4, config.input_size[1]//4])
                )
                
                # ConvNeXt ë¸”ë¡ë“¤
                self.stages = nn.ModuleList([
                    self._make_convnext_stage(96, 192, 3),
                    self._make_convnext_stage(192, 384, 3),
                    self._make_convnext_stage(384, 768, 9),
                    self._make_convnext_stage(768, 768, 3)
                ])
                
                # ë¶„ë¥˜ í—¤ë“œ
                self.norm = nn.LayerNorm(768)
                self.head = nn.Linear(768, config.num_classes)
                
            def _make_convnext_stage(self, in_channels, out_channels, num_blocks):
                layers = []
                if in_channels != out_channels:
                    layers.append(nn.Conv2d(in_channels, out_channels, 2, 2))
                
                for _ in range(num_blocks):
                    layers.append(self._make_convnext_block(out_channels))
                
                return nn.Sequential(*layers)
            
            def _make_convnext_block(self, channels):
                return nn.Sequential(
                    nn.Conv2d(channels, channels, 7, 1, 3, groups=channels),
                    nn.LayerNorm([channels, 1, 1]),
                    nn.Conv2d(channels, channels * 4, 1),
                    nn.GELU(),
                    nn.Conv2d(channels * 4, channels, 1),
                    nn.Dropout(0.1)
                )
            
            def forward(self, x):
                x = self.stem(x)
                
                for stage in self.stages:
                    x = stage(x)
                
                x = self.norm(x)
                x = F.adaptive_avg_pool2d(x, (1, 1))
                x = torch.flatten(x, 1)
                x = self.head(x)
                
                return x
        
        return ConvNeXtArchitecture(self.config)
    
    def _create_efficientnet_architecture(self) -> nn.Module:
        """EfficientNet ì•„í‚¤í…ì²˜ ìƒì„±"""
        class EfficientNetArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # Stem
                self.stem = nn.Sequential(
                    nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),
                    nn.BatchNorm2d(32),
                    nn.ReLU(inplace=True)
                )
                
                # MBConv ë¸”ë¡ë“¤
                self.blocks = nn.ModuleList([
                    self._make_mbconv_block(32, 16, 1, 1),
                    self._make_mbconv_block(16, 24, 6, 2),
                    self._make_mbconv_block(24, 40, 6, 2),
                    self._make_mbconv_block(40, 80, 6, 2),
                    self._make_mbconv_block(80, 112, 6, 1),
                    self._make_mbconv_block(112, 192, 6, 2),
                    self._make_mbconv_block(192, 320, 6, 1),
                ])
                
                # Head
                self.head = nn.Sequential(
                    nn.Conv2d(320, 1280, kernel_size=1),
                    nn.BatchNorm2d(1280),
                    nn.ReLU(inplace=True),
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(1280, config.num_classes)
                )
                
            def _make_mbconv_block(self, in_channels, out_channels, expansion, stride):
                return nn.Sequential(
                    nn.Conv2d(in_channels, in_channels * expansion, 1),
                    nn.BatchNorm2d(in_channels * expansion),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(in_channels * expansion, in_channels * expansion, 3, stride, 1, groups=in_channels * expansion),
                    nn.BatchNorm2d(in_channels * expansion),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(in_channels * expansion, out_channels, 1),
                    nn.BatchNorm2d(out_channels)
                )
            
            def forward(self, x):
                x = self.stem(x)
                
                for block in self.blocks:
                    x = block(x)
                
                x = self.head(x)
                return x
        
        return EfficientNetArchitecture(self.config)
    
    def _create_swin_transformer_architecture(self) -> nn.Module:
        """Swin Transformer ì•„í‚¤í…ì²˜ ìƒì„±"""
        class SwinTransformerArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # Patch embedding
                self.patch_embed = nn.Conv2d(3, 96, kernel_size=4, stride=4)
                
                # Swin Transformer ë¸”ë¡ë“¤
                self.stages = nn.ModuleList([
                    self._make_swin_stage(96, 96, 2, 7),
                    self._make_swin_stage(96, 192, 2, 7),
                    self._make_swin_stage(192, 384, 2, 7),
                    self._make_swin_stage(384, 768, 2, 7)
                ])
                
                # Head
                self.norm = nn.LayerNorm(768)
                self.head = nn.Linear(768, config.num_classes)
                
            def _make_swin_stage(self, in_channels, out_channels, num_blocks, window_size):
                layers = []
                if in_channels != out_channels:
                    layers.append(nn.Conv2d(in_channels, out_channels, 2, 2))
                
                for _ in range(num_blocks):
                    layers.append(self._make_swin_block(out_channels, window_size))
                
                return nn.Sequential(*layers)
            
            def _make_swin_block(self, channels, window_size):
                return nn.Sequential(
                    nn.LayerNorm(channels),
                    nn.MultiheadAttention(channels, num_heads=8, batch_first=True),
                    nn.LayerNorm(channels),
                    nn.Linear(channels, channels * 4),
                    nn.GELU(),
                    nn.Linear(channels * 4, channels)
                )
            
            def forward(self, x):
                x = self.patch_embed(x)
                
                for stage in self.stages:
                    x = stage(x)
                
                x = self.norm(x)
                x = F.adaptive_avg_pool2d(x, (1, 1))
                x = torch.flatten(x, 1)
                x = self.head(x)
                
                return x
        
        return SwinTransformerArchitecture(self.config)
    
    def _create_custom_architecture(self) -> nn.Module:
        """ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ìƒì„±"""
        class CustomArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                # ì‚¬ìš©ì ì •ì˜ ë ˆì´ì–´ë“¤
                layers = []
                in_channels = 3
                
                for layer_config in config.layers:
                    if layer_config.layer_type == "conv":
                        layer = nn.Conv2d(
                            in_channels, layer_config.out_channels,
                            kernel_size=layer_config.kernel_size,
                            stride=layer_config.stride,
                            padding=layer_config.padding
                        )
                        layers.append(layer)
                        
                        if layer_config.normalization == "batch_norm":
                            layers.append(nn.BatchNorm2d(layer_config.out_channels))
                        
                        if layer_config.activation == "relu":
                            layers.append(nn.ReLU(inplace=True))
                        elif layer_config.activation == "gelu":
                            layers.append(nn.GELU())
                        
                        if layer_config.dropout > 0:
                            layers.append(nn.Dropout(layer_config.dropout))
                        
                        in_channels = layer_config.out_channels
                
                self.features = nn.Sequential(*layers)
                self.classifier = nn.Linear(in_channels, config.num_classes)
                
            def forward(self, x):
                x = self.features(x)
                x = F.adaptive_avg_pool2d(x, (1, 1))
                x = torch.flatten(x, 1)
                x = self.classifier(x)
                return x
        
        return CustomArchitecture(self.config)
    
    def _create_fallback_architecture(self) -> nn.Module:
        """í´ë°± ì•„í‚¤í…ì²˜ ìƒì„±"""
        class FallbackArchitecture(nn.Module):
            def __init__(self, config: NeuralArchitectureConfig):
                super().__init__()
                self.config = config
                
                self.features = nn.Sequential(
                    nn.Conv2d(3, 64, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(128, config.num_classes)
                )
                
            def forward(self, x):
                return self.features(x)
        
        return FallbackArchitecture(self.config)
    
    def _setup_progressive_loading(self):
        """ì ì§„ì  ë¡œë”© ì„¤ì •"""
        try:
            if self.model is None:
                return
            
            # ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´ë¥¼ ì ì§„ì  ë¡œë”ì— ë“±ë¡
            for name, module in self.model.named_modules():
                if len(list(module.children())) == 0:  # Leaf module
                    layer_config = LayerConfig(
                        layer_type=type(module).__name__,
                        in_channels=getattr(module, 'in_channels', 0),
                        out_channels=getattr(module, 'out_channels', 0),
                        memory_priority=5  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
                    )
                    
                    self.layer_loader.add_layer(name, layer_config, module)
            
            self.logger.info("âœ… ì ì§„ì  ë¡œë”© ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì ì§„ì  ë¡œë”© ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _register_computation_graph(self):
        """ê³„ì‚° ê·¸ë˜í”„ ë“±ë¡"""
        try:
            if self.model is None:
                return
            
            # ëª¨ë¸ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë“¤ì„ ê³„ì‚° ê·¸ë˜í”„ì— ë“±ë¡
            for name, module in self.model.named_children():
                self.computation_graph.register_node(name, module)
            
            self.logger.info("âœ… ê³„ì‚° ê·¸ë˜í”„ ë“±ë¡ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê³„ì‚° ê·¸ë˜í”„ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def load_layers_progressively(self, target_memory_gb: float = None) -> Dict[str, bool]:
        """ì ì§„ì  ë ˆì´ì–´ ë¡œë”© ì‹¤í–‰"""
        return self.layer_loader.load_layers_progressively(target_memory_gb)
    
    def forward(self, input_tensor: torch.Tensor, 
               enable_gradients: bool = False) -> torch.Tensor:
        """ìˆœì „íŒŒ ì‹¤í–‰"""
        try:
            if not self.is_initialized:
                raise RuntimeError("ì•„í‚¤í…ì²˜ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Gradient ëª¨ë“œ ì„¤ì •
            self.computation_graph.enable_autograd = enable_gradients
            
            # ê³„ì‚° ê·¸ë˜í”„ë¥¼ í†µí•œ ìˆœì „íŒŒ
            results = self.computation_graph.execute_forward_pass(input_tensor)
            
            # ìµœì¢… ì¶œë ¥ ë°˜í™˜
            if results:
                return list(results.values())[-1]  # ë§ˆì§€ë§‰ ë…¸ë“œì˜ ì¶œë ¥
            else:
                # í´ë°±: ì§ì ‘ ëª¨ë¸ ì‹¤í–‰
                with torch.set_grad_enabled(enable_gradients):
                    return self.model(input_tensor)
                
        except Exception as e:
            self.logger.error(f"âŒ ìˆœì „íŒŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
    
    def backward(self, loss: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ì—­ì „íŒŒ ì‹¤í–‰"""
        return self.computation_graph.execute_backward_pass(loss)
    
    def get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (GB)"""
        return self.layer_loader.current_memory_usage / 1024
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # PyTorch ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.config.device == "mps" and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
                elif self.config.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
            self.memory_usage = self.get_memory_usage()
            
            return {
                "success": True,
                "memory_usage_gb": self.memory_usage,
                "optimization_time": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 6. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_neural_architecture_manager(
    architecture_type: str = "resnet",
    input_size: Tuple[int, int] = (512, 512),
    num_classes: int = 20,
    memory_limit_gb: float = 8.0,
    device: str = "auto"
) -> NeuralArchitectureManager:
    """ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ë§¤ë‹ˆì € ìƒì„±"""
    try:
        # ì•„í‚¤í…ì²˜ íƒ€ì… ë³€í™˜
        arch_type = NeuralArchitectureType(architecture_type.lower())
        
        # ì„¤ì • ìƒì„±
        config = NeuralArchitectureConfig(
            architecture_type=arch_type,
            input_size=input_size,
            num_classes=num_classes,
            memory_limit_gb=memory_limit_gb,
            device=device
        )
        
        # ë§¤ë‹ˆì € ìƒì„±
        manager = NeuralArchitectureManager(config)
        
        logger.info(f"âœ… ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ë§¤ë‹ˆì € ìƒì„± ì™„ë£Œ: {architecture_type}")
        return manager
        
    except Exception as e:
        logger.error(f"âŒ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def get_optimal_architecture_for_task(
    task_type: str,
    input_size: Tuple[int, int] = (512, 512),
    num_classes: int = 20,
    memory_limit_gb: float = 8.0
) -> NeuralArchitectureManager:
    """íƒœìŠ¤í¬ë³„ ìµœì  ì•„í‚¤í…ì²˜ ì„ íƒ"""
    try:
        # íƒœìŠ¤í¬ë³„ ìµœì  ì•„í‚¤í…ì²˜ ë§¤í•‘
        task_architecture_map = {
            "human_parsing": "resnet",
            "pose_estimation": "convnext",
            "cloth_segmentation": "efficientnet",
            "geometric_matching": "vision_transformer",
            "virtual_fitting": "swin_transformer",
            "quality_assessment": "resnet"
        }
        
        architecture_type = task_architecture_map.get(task_type, "resnet")
        
        return create_neural_architecture_manager(
            architecture_type=architecture_type,
            input_size=input_size,
            num_classes=num_classes,
            memory_limit_gb=memory_limit_gb
        )
        
    except Exception as e:
        logger.error(f"âŒ ìµœì  ì•„í‚¤í…ì²˜ ì„ íƒ ì‹¤íŒ¨: {e}")
        raise
