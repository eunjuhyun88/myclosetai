"""
MyCloset AI GPU ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” ë° ê´€ë¦¬
"""

import gc
import logging
import psutil
import torch
import threading
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from contextlib import contextmanager
import time
from collections import defaultdict

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    total_system_gb: float
    used_system_gb: float
    available_system_gb: float
    mps_allocated_gb: float
    mps_cached_gb: float
    process_memory_gb: float
    timestamp: float

class GPUMemoryManager:
    """M3 Max í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, device: torch.device, memory_limit_gb: float = 16.0):
        self.device = device
        self.memory_limit_gb = memory_limit_gb
        self.logger = logging.getLogger(__name__)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_history: List[MemoryStats] = []
        self.peak_memory_gb = 0.0
        
        # ëª¨ë¸ ìºì‹œ ê´€ë¦¬
        self.model_cache: Dict[str, torch.nn.Module] = {}
        self.cache_order: List[str] = []  # LRU ìºì‹œë¥¼ ìœ„í•œ ìˆœì„œ
        self.max_cached_models = 3
        
        # ë©”ëª¨ë¦¬ ì„ê³„ì¹˜ ì„¤ì •
        self.warning_threshold = 0.8  # 80% ì‚¬ìš©ì‹œ ê²½ê³ 
        self.critical_threshold = 0.9  # 90% ì‚¬ìš©ì‹œ ê¸´ê¸‰ ì •ë¦¬
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.Lock()
        
        self.logger.info(f"ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” - í•œê³„: {memory_limit_gb}GB")

    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        system_memory = psutil.virtual_memory()
        total_system_gb = system_memory.total / (1024**3)
        used_system_gb = system_memory.used / (1024**3)
        available_system_gb = system_memory.available / (1024**3)
        
        # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
        process = psutil.Process()
        process_memory_gb = process.memory_info().rss / (1024**3)
        
        # MPS ë©”ëª¨ë¦¬ (M3 Max)
        mps_allocated_gb = 0.0
        mps_cached_gb = 0.0
        
        if self.device.type == "mps":
            try:
                mps_allocated_gb = torch.mps.current_allocated_memory() / (1024**3)
                # MPSëŠ” cached memory APIê°€ ì œí•œì ì´ë¯€ë¡œ ì¶”ì •ê°’ ì‚¬ìš©
                mps_cached_gb = max(0, process_memory_gb - mps_allocated_gb)
            except Exception as e:
                self.logger.warning(f"MPS ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        stats = MemoryStats(
            total_system_gb=total_system_gb,
            used_system_gb=used_system_gb,
            available_system_gb=available_system_gb,
            mps_allocated_gb=mps_allocated_gb,
            mps_cached_gb=mps_cached_gb,
            process_memory_gb=process_memory_gb,
            timestamp=time.time()
        )
        
        # í”¼í¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        if mps_allocated_gb > self.peak_memory_gb:
            self.peak_memory_gb = mps_allocated_gb
        
        return stats

    def check_memory_usage(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ë° í•„ìš”ì‹œ ì •ë¦¬"""
        with self.lock:
            stats = self.get_memory_stats()
            self.memory_history.append(stats)
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œ)
            if len(self.memory_history) > 100:
                self.memory_history = self.memory_history[-100:]
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚°
            usage_ratio = stats.process_memory_gb / self.memory_limit_gb
            
            if usage_ratio > self.critical_threshold:
                self.logger.warning(f"ì„ê³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {usage_ratio:.2%}")
                self._emergency_cleanup()
                return False
            elif usage_ratio > self.warning_threshold:
                self.logger.info(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {usage_ratio:.2%}")
                self._smart_cleanup()
                return True
            
            return True

    def _emergency_cleanup(self):
        """ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.logger.warning("ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
        
        # ëª¨ë“  ëª¨ë¸ ìºì‹œ ì œê±°
        self.clear_model_cache()
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # MPS ìºì‹œ ì •ë¦¬
        if self.device.type == "mps":
            torch.mps.empty_cache()
        elif self.device.type == "cuda":
            torch.cuda.empty_cache()
        
        self.logger.info("ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

    def _smart_cleanup(self):
        """ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”í•œ ê²ƒë§Œ)"""
        # LRU ê¸°ë°˜ ëª¨ë¸ ìºì‹œ ì •ë¦¬
        if len(self.model_cache) > 1:
            oldest_model = self.cache_order[0]
            self.remove_from_cache(oldest_model)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        if self.device.type == "mps":
            torch.mps.empty_cache()

    @contextmanager
    def memory_context(self, operation_name: str):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        start_stats = self.get_memory_stats()
        start_time = time.time()
        
        try:
            yield
        finally:
            end_stats = self.get_memory_stats()
            end_time = time.time()
            
            memory_delta = end_stats.mps_allocated_gb - start_stats.mps_allocated_gb
            time_delta = end_time - start_time
            
            self.logger.info(
                f"{operation_name} - ë©”ëª¨ë¦¬ ë³€í™”: {memory_delta:+.2f}GB, "
                f"ì†Œìš”ì‹œê°„: {time_delta:.2f}ì´ˆ"
            )

    def cache_model(self, model_name: str, model: torch.nn.Module):
        """ëª¨ë¸ ìºì‹± (LRU ë°©ì‹)"""
        with self.lock:
            # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ìˆœì„œ ì—…ë°ì´íŠ¸
            if model_name in self.model_cache:
                self.cache_order.remove(model_name)
            
            # ìºì‹œ í¬ê¸° ì œí•œ ì²´í¬
            while len(self.model_cache) >= self.max_cached_models:
                oldest_model = self.cache_order.pop(0)
                del self.model_cache[oldest_model]
                self.logger.info(f"ëª¨ë¸ ìºì‹œì—ì„œ ì œê±°: {oldest_model}")
            
            # ìƒˆ ëª¨ë¸ ìºì‹±
            self.model_cache[model_name] = model
            self.cache_order.append(model_name)
            
            self.logger.info(f"ëª¨ë¸ ìºì‹œì— ì¶”ê°€: {model_name}")

    def get_cached_model(self, model_name: str) -> Optional[torch.nn.Module]:
        """ìºì‹œëœ ëª¨ë¸ ì¡°íšŒ (LRU ì—…ë°ì´íŠ¸)"""
        with self.lock:
            if model_name in self.model_cache:
                # LRU ìˆœì„œ ì—…ë°ì´íŠ¸
                self.cache_order.remove(model_name)
                self.cache_order.append(model_name)
                return self.model_cache[model_name]
            return None

    def remove_from_cache(self, model_name: str):
        """íŠ¹ì • ëª¨ë¸ì„ ìºì‹œì—ì„œ ì œê±°"""
        with self.lock:
            if model_name in self.model_cache:
                del self.model_cache[model_name]
                self.cache_order.remove(model_name)
                self.logger.info(f"ëª¨ë¸ ìºì‹œì—ì„œ ì œê±°: {model_name}")

    def clear_model_cache(self):
        """ëª¨ë¸ ìºì‹œ ì „ì²´ ì •ë¦¬"""
        with self.lock:
            self.model_cache.clear()
            self.cache_order.clear()
            self.logger.info("ëª¨ë¸ ìºì‹œ ì „ì²´ ì •ë¦¬ ì™„ë£Œ")

    def clear_cache(self):
        """ì „ì²´ ìºì‹œ ì •ë¦¬"""
        self.clear_model_cache()
        gc.collect()
        
        if self.device.type == "mps":
            torch.mps.empty_cache()
        elif self.device.type == "cuda":
            torch.cuda.empty_cache()

    def get_memory_report(self) -> Dict[str, Any]:
        """ìƒì„¸ ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸"""
        stats = self.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚°
        usage_ratio = stats.process_memory_gb / self.memory_limit_gb
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
        efficiency_score = 1.0 - (stats.mps_cached_gb / max(stats.mps_allocated_gb, 0.1))
        
        return {
            "current_stats": {
                "total_system_gb": stats.total_system_gb,
                "used_system_gb": stats.used_system_gb,
                "available_system_gb": stats.available_system_gb,
                "mps_allocated_gb": stats.mps_allocated_gb,
                "mps_cached_gb": stats.mps_cached_gb,
                "process_memory_gb": stats.process_memory_gb,
                "usage_ratio": usage_ratio,
                "efficiency_score": efficiency_score
            },
            "limits": {
                "memory_limit_gb": self.memory_limit_gb,
                "warning_threshold": self.warning_threshold,
                "critical_threshold": self.critical_threshold
            },
            "peak_memory_gb": self.peak_memory_gb,
            "cached_models": list(self.model_cache.keys()),
            "cache_utilization": len(self.model_cache) / self.max_cached_models,
            "recommendations": self._get_recommendations(stats, usage_ratio)
        }

    def _get_recommendations(self, stats: MemoryStats, usage_ratio: float) -> List[str]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì¶”ì²œì‚¬í•­"""
        recommendations = []
        
        if usage_ratio > 0.9:
            recommendations.append("ê¸´ê¸‰: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 90%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë‚®ì¶”ì„¸ìš”.")
        elif usage_ratio > 0.8:
            recommendations.append("ê²½ê³ : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ëª¨ë¸ ìºì‹œë¥¼ ì •ë¦¬í•˜ì„¸ìš”.")
        
        if stats.mps_cached_gb > 2.0:
            recommendations.append("MPS ìºì‹œê°€ 2GBë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. torch.mps.empty_cache()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if len(self.model_cache) == self.max_cached_models:
            recommendations.append("ëª¨ë¸ ìºì‹œê°€ ê°€ë“ ì°¼ìŠµë‹ˆë‹¤. ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ ì œê±°í•˜ì„¸ìš”.")
        
        if stats.available_system_gb < 2.0:
            recommendations.append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•˜ì„¸ìš”.")
        
        return recommendations

    def optimize_for_batch_size(self, base_batch_size: int = 1) -> int:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒí™©ì— ë§ëŠ” ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        stats = self.get_memory_stats()
        usage_ratio = stats.process_memory_gb / self.memory_limit_gb
        
        if usage_ratio > 0.8:
            return max(1, base_batch_size // 2)
        elif usage_ratio < 0.5:
            return min(4, base_batch_size * 2)
        else:
            return base_batch_size

    def profile_memory_usage(self, duration_seconds: int = 60):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§"""
        self.logger.info(f"ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹œì‘ - {duration_seconds}ì´ˆ")
        
        start_time = time.time()
        profile_data = []
        
        while time.time() - start_time < duration_seconds:
            stats = self.get_memory_stats()
            profile_data.append({
                "timestamp": stats.timestamp,
                "mps_allocated_gb": stats.mps_allocated_gb,
                "process_memory_gb": stats.process_memory_gb
            })
            time.sleep(1)
        
        # í”„ë¡œíŒŒì¼ ê²°ê³¼ ë¶„ì„
        avg_mps = sum(d["mps_allocated_gb"] for d in profile_data) / len(profile_data)
        max_mps = max(d["mps_allocated_gb"] for d in profile_data)
        
        self.logger.info(f"í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ - í‰ê· : {avg_mps:.2f}GB, ìµœëŒ€: {max_mps:.2f}GB")
        
        return {
            "duration_seconds": duration_seconds,
            "average_mps_gb": avg_mps,
            "peak_mps_gb": max_mps,
            "data_points": len(profile_data),
            "raw_data": profile_data
        }

# M3 Max íŠ¹í™” ìµœì í™” ë„êµ¬
class M3MaxOptimizer:
    """M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    @staticmethod
    def optimize_tensor_operations():
        """í…ì„œ ì—°ì‚° ìµœì í™” ì„¤ì •"""
        # MPS ìµœì í™” ì„¤ì •
        if torch.backends.mps.is_available():
            # MPS ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ìµœì í™”
            torch.mps.set_per_process_memory_fraction(0.8)
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  attention ì‚¬ìš©
        torch.backends.cuda.enable_flash_sdp(False)  # MPSì—ì„œëŠ” ë¹„í™œì„±í™”
        
        # ìë™ mixed precision ì„¤ì •
        torch.backends.cudnn.benchmark = False  # MPSì—ì„œëŠ” ë¶ˆí•„ìš”
        
        return True

    @staticmethod
    def get_optimal_image_size(available_memory_gb: float) -> int:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ì´ë¯¸ì§€ í¬ê¸°"""
        if available_memory_gb > 12:
            return 1024
        elif available_memory_gb > 8:
            return 768
        elif available_memory_gb > 4:
            return 512
        else:
            return 256

    @staticmethod
    def estimate_memory_usage(
        batch_size: int,
        image_size: int,
        model_complexity: str = "medium"
    ) -> float:
        """ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (GB)"""
        
        # ê¸°ë³¸ ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ (RGB, float32)
        image_memory = (batch_size * 3 * image_size * image_size * 4) / (1024**3)
        
        # ëª¨ë¸ ë³µì¡ë„ë³„ ê°€ì¤‘ì¹˜
        complexity_multiplier = {
            "simple": 2.0,
            "medium": 4.0,
            "complex": 8.0
        }.get(model_complexity, 4.0)
        
        # ì´ ì˜ˆìƒ ë©”ëª¨ë¦¬ (ì´ë¯¸ì§€ + ëª¨ë¸ + ì¤‘ê°„ ê²°ê³¼)
        total_memory = image_memory * complexity_multiplier
        
        return total_memory

# ì‚¬ìš© ì˜ˆì‹œ
def example_usage():
    """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    memory_manager = GPUMemoryManager(device, memory_limit_gb=16.0)
    
    # M3 Max ìµœì í™” ì ìš©
    M3MaxOptimizer.optimize_tensor_operations()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
    memory_manager.check_memory_usage()
    
    # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
    with memory_manager.memory_context("ëª¨ë¸ ì¶”ë¡ "):
        # ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì½”ë“œ
        dummy_tensor = torch.randn(1, 3, 512, 512).to(device)
        result = dummy_tensor * 2
    
    # ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸
    report = memory_manager.get_memory_report()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {report['current_stats']['usage_ratio']:.2%}")
    
    # ì¶”ì²œì‚¬í•­ ì¶œë ¥
    for rec in report['recommendations']:
        print(f"ğŸ’¡ {rec}")

if __name__ == "__main__":
    example_usage()