# app/ai_pipeline/utils/memory_manager.py
"""
MyCloset AI - ÏßÄÎä•Ìòï Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú (M3 Max ÏµúÏ†ÅÌôî)
‚úÖ ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Ï†ÅÏö© + create_memory_manager Ìï®Ïàò Ï∂îÍ∞Ä
üî• ÌïµÏã¨: ÎàÑÎùΩÎêú Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§ Î™®Îëê Ï∂îÍ∞Ä
"""
import os
import gc
import threading
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Callable, List, Union
from dataclasses import dataclass
from contextlib import contextmanager, asynccontextmanager
import weakref
from functools import wraps
import numpy as np

# psutil ÏÑ†ÌÉùÏ†Å ÏûÑÌè¨Ìä∏
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ÏÑ†ÌÉùÏ†Å ÏûÑÌè¨Ìä∏
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ Ï†ïÎ≥¥"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0

class MemoryManager:
    """
    ÏßÄÎä•Ìòï GPU/CPU Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê - Apple Silicon M3 Max ÏµúÏ†ÅÌôî
    ‚úÖ ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Ï†ÅÏö©
    """
    
    def __init__(
        self,
        device: Optional[str] = None,  # üî• ÏµúÏ†Å Ìå®ÌÑ¥: NoneÏúºÎ°ú ÏûêÎèô Í∞êÏßÄ
        config: Optional[Dict[str, Any]] = None,
        **kwargs  # üöÄ ÌôïÏû•ÏÑ±: Î¨¥Ï†úÌïú Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞
    ):
        """
        ‚úÖ ÏµúÏ†Å ÏÉùÏÑ±Ïûê - Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÌäπÌôî

        Args:
            device: ÏÇ¨Ïö©Ìï† ÎîîÎ∞îÏù¥Ïä§ (None=ÏûêÎèôÍ∞êÏßÄ, 'cpu', 'cuda', 'mps')
            config: Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
            **kwargs: ÌôïÏû• ÌååÎùºÎØ∏ÌÑ∞Îì§
                - device_type: str = "auto"
                - memory_gb: float = 16.0  
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - memory_limit_gb: float = None  # Î©îÎ™®Î¶¨ Ï†úÌïú
                - warning_threshold: float = 0.75  # Í≤ΩÍ≥† ÏûÑÍ≥ÑÏπò
                - critical_threshold: float = 0.9  # ÏúÑÌóò ÏûÑÍ≥ÑÏπò
                - auto_cleanup: bool = True  # ÏûêÎèô Ï†ïÎ¶¨
                - monitoring_interval: float = 30.0  # Î™®ÎãàÌÑ∞ÎßÅ Ï£ºÍ∏∞
                - enable_caching: bool = True  # Ï∫êÏã± ÌôúÏÑ±Ìôî
        """
        # 1. üí° ÏßÄÎä•Ï†Å ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ
        self.device = self._auto_detect_device(device)

        # 2. üìã Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")

        # 3. üîß ÌëúÏ§Ä ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÏ∂ú (ÏùºÍ¥ÄÏÑ±)
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')

        # 4. ‚öôÔ∏è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÌäπÌôî ÌååÎùºÎØ∏ÌÑ∞
        memory_limit_gb = kwargs.get('memory_limit_gb', None)
        if memory_limit_gb is None:
            if PSUTIL_AVAILABLE:
                total_memory = psutil.virtual_memory().total / 1024**3
                self.memory_limit_gb = total_memory * 0.8  # 80% ÏÇ¨Ïö©
            else:
                self.memory_limit_gb = 16.0  # Í∏∞Î≥∏Í∞í
        else:
            self.memory_limit_gb = memory_limit_gb
            
        self.warning_threshold = kwargs.get('warning_threshold', 0.75)
        self.critical_threshold = kwargs.get('critical_threshold', 0.9)
        self.auto_cleanup = kwargs.get('auto_cleanup', True)
        self.monitoring_interval = kwargs.get('monitoring_interval', 30.0)
        self.enable_caching = kwargs.get('enable_caching', True)

        # 5. ‚öôÔ∏è Ïä§ÌÖùÎ≥Ñ ÌäπÌôî ÌååÎùºÎØ∏ÌÑ∞Î•º configÏóê Î≥ëÌï©
        self._merge_step_specific_config(kwargs)

        # 6. ‚úÖ ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
        self.is_initialized = False

        # 7. üéØ Í∏∞Ï°¥ ÌÅ¥ÎûòÏä§Î≥Ñ Í≥†Ïú† Ï¥àÍ∏∞Ìôî Î°úÏßÅ Ïã§Ìñâ
        self._initialize_step_specific()

        self.logger.info(f"üéØ {self.step_name} Ï¥àÍ∏∞Ìôî - ÎîîÎ∞îÏù¥Ïä§: {self.device}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """üí° ÏßÄÎä•Ï†Å ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            return 'cpu'

        try:
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max Ïö∞ÏÑ†
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # Ìè¥Î∞±
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """üçé M3 Max Ïπ© ÏûêÎèô Í∞êÏßÄ"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                # M3 Max Í∞êÏßÄ Î°úÏßÅ
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """‚öôÔ∏è Ïä§ÌÖùÎ≥Ñ ÌäπÌôî ÏÑ§Ï†ï Î≥ëÌï©"""
        # ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞ Ï†úÏô∏ÌïòÍ≥† Î™®Îì† kwargsÎ•º configÏóê Î≥ëÌï©
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'memory_limit_gb', 'warning_threshold', 'critical_threshold',
            'auto_cleanup', 'monitoring_interval', 'enable_caching'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value

    def _initialize_step_specific(self):
        """üéØ Í∏∞Ï°¥ Ï¥àÍ∏∞Ìôî Î°úÏßÅ ÏôÑÏ†Ñ Ïú†ÏßÄ"""
        # Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ
        self.stats_history: List[MemoryStats] = []
        self.max_history_length = 100
        
        # Ï∫êÏãú ÏãúÏä§ÌÖú
        self.tensor_cache: Dict[str, Any] = {}
        self.image_cache: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self.cache_priority: Dict[str, float] = {}
        
        # Î™®ÎãàÌÑ∞ÎßÅ
        self.monitoring_active = False
        self.monitoring_thread = None
        self._lock = threading.Lock()
        
        logger.info(f"üß† MemoryManager Ï¥àÍ∏∞Ìôî - ÎîîÎ∞îÏù¥Ïä§: {self.device}, Î©îÎ™®Î¶¨ Ï†úÌïú: {self.memory_limit_gb:.1f}GB")
        
        # M3 Max ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
        if self.device == "mps":
            logger.info("üçé M3 Max ÏµúÏ†ÅÌôî Î™®Îìú ÌôúÏÑ±Ìôî")
        
        # Ï¥àÍ∏∞Ìôî ÏôÑÎ£å
        self.is_initialized = True
    
    def get_memory_stats(self) -> MemoryStats:
        """ÌòÑÏû¨ Î©îÎ™®Î¶¨ ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            # CPU Î©îÎ™®Î¶¨
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                cpu_percent = memory.percent
                cpu_total_gb = memory.total / 1024**3
                cpu_used_gb = memory.used / 1024**3
                cpu_available_gb = memory.available / 1024**3
                swap_used_gb = psutil.swap_memory().used / 1024**3
                
                # ÌîÑÎ°úÏÑ∏Ïä§ Î©îÎ™®Î¶¨
                process = psutil.Process()
                process_memory_mb = process.memory_info().rss / 1024**2
            else:
                cpu_percent = 0.0
                cpu_total_gb = 16.0
                cpu_used_gb = 8.0
                cpu_available_gb = 8.0
                swap_used_gb = 0.0
                process_memory_mb = 0.0
            
            # GPU Î©îÎ™®Î¶¨
            gpu_allocated_gb = 0.0
            gpu_reserved_gb = 0.0
            gpu_total_gb = 0.0
            
            if TORCH_AVAILABLE:
                try:
                    if self.device == "cuda" and torch.cuda.is_available():
                        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        gpu_total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPS Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ (Ï∂îÏ†ï)
                        gpu_allocated_gb = 2.0  # ÏûÑÏãúÍ∞í
                        gpu_total_gb = 128.0  # M3 Max 128GB
                except Exception:
                    pass
            
            # Ï∫êÏãú ÌÅ¨Í∏∞
            cache_size_mb = 0.0
            if self.enable_caching:
                cache_size_mb = sum(
                    len(str(v)) / 1024**2 for v in 
                    [*self.tensor_cache.values(), *self.image_cache.values(), *self.model_cache.values()]
                )
            
            return MemoryStats(
                cpu_percent=cpu_percent,
                cpu_available_gb=cpu_available_gb,
                cpu_used_gb=cpu_used_gb,
                cpu_total_gb=cpu_total_gb,
                gpu_allocated_gb=gpu_allocated_gb,
                gpu_reserved_gb=gpu_reserved_gb,
                gpu_total_gb=gpu_total_gb,
                swap_used_gb=swap_used_gb,
                cache_size_mb=cache_size_mb,
                process_memory_mb=process_memory_mb
            )
            
        except Exception as e:
            logger.error(f"Î©îÎ™®Î¶¨ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return MemoryStats(
                cpu_percent=0.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )
    
    def check_memory_pressure(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏïïÎ∞ï ÏÉÅÌÉú Ï≤¥ÌÅ¨"""
        stats = self.get_memory_stats()
        
        cpu_usage_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        gpu_usage_ratio = stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb)
        
        status = "normal"
        if cpu_usage_ratio > self.critical_threshold or gpu_usage_ratio > self.critical_threshold:
            status = "critical"
        elif cpu_usage_ratio > self.warning_threshold or gpu_usage_ratio > self.warning_threshold:
            status = "warning"
        
        return {
            "status": status,
            "cpu_usage_ratio": cpu_usage_ratio,
            "gpu_usage_ratio": gpu_usage_ratio,
            "cache_size_mb": stats.cache_size_mb,
            "recommendations": self._get_cleanup_recommendations(stats)
        }
    
    def _get_cleanup_recommendations(self, stats: MemoryStats) -> List[str]:
        """Ï†ïÎ¶¨ Í∂åÏû•ÏÇ¨Ìï≠"""
        recommendations = []
        
        cpu_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        if cpu_ratio > 0.8:
            recommendations.append("CPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Í∂åÏû•")
        
        if stats.gpu_allocated_gb > 10.0:
            recommendations.append("GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Í∂åÏû•")
        
        if stats.cache_size_mb > 1000:
            recommendations.append("Ï∫êÏãú Ï†ïÎ¶¨ Í∂åÏû•")
        
        return recommendations
    
    def clear_cache(self, aggressive: bool = False):
        """Ï∫êÏãú Ï†ïÎ¶¨"""
        try:
            if not self.enable_caching:
                return
                
            with self._lock:
                if aggressive:
                    # Ï†ÑÏ≤¥ Ï∫êÏãú ÏÇ≠Ï†ú
                    self.tensor_cache.clear()
                    self.image_cache.clear()
                    self.model_cache.clear()
                    self.cache_priority.clear()
                    logger.info("üßπ Ï†ÑÏ≤¥ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")
                else:
                    # ÏÑ†ÌÉùÏ†Å Ï∫êÏãú Ï†ïÎ¶¨
                    self._evict_low_priority_cache()
                    logger.debug("üßπ ÏÑ†ÌÉùÏ†Å Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")
        except Exception as e:
            logger.error(f"Ï∫êÏãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def smart_cleanup(self):
        """ÏßÄÎä•Ìòï Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            pressure = self.check_memory_pressure()
            
            if pressure["status"] == "critical":
                self.clear_cache(aggressive=True)
                if TORCH_AVAILABLE:
                    gc.collect()
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPSÎäî empty_cacheÍ∞Ä ÏóÜÏúºÎØÄÎ°ú ÎåÄÏ≤¥ Î∞©Î≤ï
                        pass
                logger.info("üö® Í∏¥Í∏â Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìñâ")
            elif pressure["status"] == "warning":
                self.clear_cache(aggressive=False)
                logger.debug("‚ö†Ô∏è ÏòàÎ∞©Ï†Å Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìñâ")
        except Exception as e:
            logger.error(f"ÏßÄÎä•Ìòï Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    async def cleanup(self):
        """ÎπÑÎèôÍ∏∞ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            # ÎèôÍ∏∞ Ï†ïÎ¶¨ ÏûëÏóÖÏùÑ ÎπÑÎèôÍ∏∞Î°ú Ïã§Ìñâ
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.smart_cleanup)
        except Exception as e:
            logger.error(f"ÎπÑÎèôÍ∏∞ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def cache_tensor(self, key: str, tensor: Any, priority: float = 0.5):
        """ÌÖêÏÑú Ï∫êÏã±"""
        if not self.enable_caching:
            return
            
        try:
            with self._lock:
                self.tensor_cache[key] = tensor
                self.cache_priority[key] = priority
        except Exception as e:
            logger.error(f"ÌÖêÏÑú Ï∫êÏã± Ïã§Ìå®: {e}")
    
    def get_cached_tensor(self, key: str, cache_type: str = "tensor") -> Optional[Any]:
        """Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
        if not self.enable_caching:
            return None
            
        try:
            with self._lock:
                if cache_type == "image":
                    return self.image_cache.get(key)
                else:
                    data = self.tensor_cache.get(key)
                    if data is not None:
                        # ÏÇ¨Ïö© Ïãú Ïö∞ÏÑ†ÏàúÏúÑ Ï¶ùÍ∞Ä
                        self.cache_priority[key] = min(1.0, self.cache_priority.get(key, 0.5) + 0.1)
                    return data
        except Exception as e:
            logger.error(f"Ï∫êÏãú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return None
    
    def _evict_low_priority_cache(self):
        """ÎÇÆÏùÄ Ïö∞ÏÑ†ÏàúÏúÑ Ï∫êÏãú Ï†úÍ±∞"""
        if not self.cache_priority:
            return
        
        try:
            # Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Ï§Ä Ï†ïÎ†¨
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # ÌïòÏúÑ 20% Ï†úÍ±∞
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                self.tensor_cache.pop(key, None)
                self.cache_priority.pop(key, None)
            
            logger.debug(f"ÎÇÆÏùÄ Ïö∞ÏÑ†ÏàúÏúÑ Ï∫êÏãú {num_to_remove}Í∞ú Ï†úÍ±∞")
            
        except Exception as e:
            logger.error(f"Ï∫êÏãú Ï†úÍ±∞ Ïã§Ìå®: {e}")
    
    # Ïª®ÌÖçÏä§Ìä∏ Îß§ÎãàÏ†Ä
    @asynccontextmanager
    async def memory_efficient_context(self, clear_before: bool = True, clear_after: bool = True):
        """ÎπÑÎèôÍ∏∞ Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å Ïª®ÌÖçÏä§Ìä∏ Îß§ÎãàÏ†Ä"""
        if clear_before:
            await self.cleanup()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                await self.cleanup()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:  # 100MB Ïù¥ÏÉÅ Ï¶ùÍ∞Ä
                logger.info(f"üìä Ïª®ÌÖçÏä§Ìä∏ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: +{memory_diff:.2f}GB")
    
    @contextmanager
    def memory_efficient_sync_context(self, clear_before: bool = True, clear_after: bool = True):
        """ÎèôÍ∏∞ Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å Ïª®ÌÖçÏä§Ìä∏ Îß§ÎãàÏ†Ä"""
        if clear_before:
            self.clear_cache()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                self.clear_cache()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:
                logger.info(f"üìä Ïª®ÌÖçÏä§Ìä∏ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: +{memory_diff:.2f}GB")
    
    # Î™®ÎãàÌÑ∞ÎßÅ
    def start_monitoring(self):
        """Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitoring_thread.start()
        logger.info("üìä Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë")
    
    def stop_monitoring(self):
        """Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ Ï§ëÏßÄ"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        logger.info("üìä Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ Ï§ëÏßÄ")
    
    def _monitor_loop(self):
        """Î™®ÎãàÌÑ∞ÎßÅ Î£®ÌîÑ"""
        while self.monitoring_active:
            try:
                stats = self.get_memory_stats()
                
                with self._lock:
                    self.stats_history.append(stats)
                    
                    # ÌûàÏä§ÌÜ†Î¶¨ ÌÅ¨Í∏∞ Ï†úÌïú
                    if len(self.stats_history) > self.max_history_length:
                        self.stats_history.pop(0)
                
                # ÏûêÎèô Ï†ïÎ¶¨ Ïã§Ìñâ
                if self.auto_cleanup:
                    self.smart_cleanup()
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ Ïò§Î•ò: {e}")
                time.sleep(10)
    
    # ÏÑ±Îä• ÏµúÏ†ÅÌôî
    def optimize_for_inference(self):
        """Ï∂îÎ°† ÏµúÏ†ÅÌôî ÏÑ§Ï†ï"""
        if not TORCH_AVAILABLE:
            return
            
        try:
            # Ï∂îÎ°† Î™®Îìú ÏÑ§Ï†ï
            torch.set_grad_enabled(False)
            
            # Î∞±ÏóîÎìú ÏµúÏ†ÅÌôî
            if self.device == 'cuda':
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            elif self.device == 'mps':
                # MPS ÏµúÏ†ÅÌôî (M3 Max)
                torch.backends.mps.is_available()  # MPS ÌôúÏÑ±Ìôî ÌôïÏù∏
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            self.clear_cache(aggressive=True)
            
            logger.info(f"üöÄ {self.device.upper()} Ï∂îÎ°† ÏµúÏ†ÅÌôî ÏôÑÎ£å")
            
        except Exception as e:
            logger.error(f"Ï∂îÎ°† ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìúÎì§
    async def get_usage_stats(self) -> Dict[str, Any]:
        """ÏÇ¨Ïö© ÌÜµÍ≥Ñ (Í∏∞Ï°¥ Ìò∏ÌôòÏÑ±)"""
        stats = self.get_memory_stats()
        pressure_info = self.check_memory_pressure()
        
        return {
            "memory_usage": {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb
            },
            "pressure": pressure_info,
            "cache_info": {
                "tensor_cache_size": len(self.tensor_cache),
                "image_cache_size": len(self.image_cache),
                "model_cache_size": len(self.model_cache)
            }
        }

    async def initialize(self) -> bool:
        """Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî"""
        try:
            # M3 Max ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
            if self.is_m3_max and self.optimization_enabled:
                self.optimize_for_inference()
            
            # Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë (ÏòµÏÖò)
            if self.config.get('start_monitoring', False):
                self.start_monitoring()
            
            self.logger.info(f"‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False

    async def get_step_info(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "step_name": self.step_name,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "specialized_features": {
                "memory_limit_gb": self.memory_limit_gb,
                "warning_threshold": self.warning_threshold,
                "critical_threshold": self.critical_threshold,
                "auto_cleanup": self.auto_cleanup,
                "monitoring_interval": self.monitoring_interval,
                "enable_caching": self.enable_caching
            },
            "current_stats": self.get_memory_stats().__dict__,
            "pressure_info": self.check_memory_pressure()
        }
    
    def __del__(self):
        """ÏÜåÎ©∏Ïûê"""
        try:
            self.stop_monitoring()
            if self.enable_caching:
                self.clear_cache(aggressive=True)
        except:
            pass

# ============================================
# üî• ÌïµÏã¨: ÎàÑÎùΩÎêú Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§ Î™®Îëê Ï∂îÍ∞Ä
# ============================================

# Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ïù∏Ïä§ÌÑ¥Ïä§ (Ïã±Í∏ÄÌÜ§)
_global_memory_manager: Optional[MemoryManager] = None

def get_memory_manager(**kwargs) -> MemoryManager:
    """Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager(**kwargs)
    return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """Ï†ÑÏó≠ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (Î≥ÑÏπ≠)"""
    return get_memory_manager(**kwargs)

# üî• ÌïµÏã¨: main.pyÏóêÏÑú Ï∞æÎäî Ìï®Ïàò Ï∂îÍ∞Ä
def create_memory_manager(device: str = "auto", **kwargs) -> MemoryManager:
    """
    üî• Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ìå©ÌÜ†Î¶¨ Ìï®Ïàò - main.pyÏóêÏÑú ÏÇ¨Ïö©
    
    Args:
        device: ÏÇ¨Ïö©Ìï† ÎîîÎ∞îÏù¥Ïä§
        **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
    
    Returns:
        MemoryManager Ïù∏Ïä§ÌÑ¥Ïä§
    """
    try:
        logger.info(f"üì¶ MemoryManager ÏÉùÏÑ± - ÎîîÎ∞îÏù¥Ïä§: {device}")
        manager = MemoryManager(device=device, **kwargs)
        return manager
    except Exception as e:
        logger.error(f"‚ùå MemoryManager ÏÉùÏÑ± Ïã§Ìå®: {e}")
        # Ïã§Ìå® ÏãúÏóêÎèÑ Í∏∞Î≥∏ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò
        return MemoryManager(device="cpu")

# Ï∂îÍ∞Ä Ìå©ÌÜ†Î¶¨ Ìï®ÏàòÎì§
def create_optimized_memory_manager(
    device: str = "auto",
    memory_gb: float = 16.0,
    is_m3_max: bool = None,
    optimization_enabled: bool = True
) -> MemoryManager:
    """ÏµúÏ†ÅÌôîÎêú Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê ÏÉùÏÑ±"""
    if is_m3_max is None:
        is_m3_max = _detect_m3_max()
    
    return MemoryManager(
        device=device,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled,
        auto_cleanup=True,
        enable_caching=True
    )

def _detect_m3_max() -> bool:
    """M3 Max Í∞êÏßÄ Ìó¨Ìçº"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True)
            return 'M3' in result.stdout
    except:
        pass
    return False

# ============================================
# üî• ÌïµÏã¨: optimize_memory_usage Ìï®Ïàò - ÎèôÍ∏∞Î°ú ÏàòÏ†ï
# ============================================

def optimize_memory_usage(device: str = None, aggressive: bool = False) -> Dict[str, Any]:
    """
    üî• Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÏµúÏ†ÅÌôî - ÎèôÍ∏∞ Ìï®ÏàòÎ°ú ÏàòÏ†ï
    
    Args:
        device: ÎåÄÏÉÅ ÎîîÎ∞îÏù¥Ïä§ ('mps', 'cuda', 'cpu')
        aggressive: Í≥µÍ≤©Ï†Å Ï†ïÎ¶¨ Ïó¨Î∂Ä
    
    Returns:
        ÏµúÏ†ÅÌôî Í≤∞Í≥º Ï†ïÎ≥¥
    """
    try:
        manager = get_memory_manager(device=device or "auto")
        
        # ÏµúÏ†ÅÌôî Ï†Ñ ÏÉÅÌÉú
        before_stats = manager.get_memory_stats()
        
        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        manager.clear_cache(aggressive=aggressive)
        
        # PyTorch Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        if TORCH_AVAILABLE:
            gc.collect()
            if manager.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif manager.device == "mps" and torch.backends.mps.is_available():
                # MPSÎäî empty_cache ÏóÜÏúºÎØÄÎ°ú ÎåÄÏ≤¥ Î∞©Î≤ï
                torch.mps.empty_cache() if hasattr(torch.mps, 'empty_cache') else None
        
        # ÏµúÏ†ÅÌôî ÌõÑ ÏÉÅÌÉú
        after_stats = manager.get_memory_stats()
        
        # Í≤∞Í≥º Í≥ÑÏÇ∞
        freed_cpu = before_stats.cpu_used_gb - after_stats.cpu_used_gb
        freed_gpu = before_stats.gpu_allocated_gb - after_stats.gpu_allocated_gb
        freed_cache = before_stats.cache_size_mb - after_stats.cache_size_mb
        
        result = {
            "success": True,
            "device": manager.device,
            "freed_memory": {
                "cpu_gb": max(0, freed_cpu),
                "gpu_gb": max(0, freed_gpu),
                "cache_mb": max(0, freed_cache)
            },
            "before": {
                "cpu_used_gb": before_stats.cpu_used_gb,
                "gpu_allocated_gb": before_stats.gpu_allocated_gb,
                "cache_size_mb": before_stats.cache_size_mb
            },
            "after": {
                "cpu_used_gb": after_stats.cpu_used_gb,
                "gpu_allocated_gb": after_stats.gpu_allocated_gb,
                "cache_size_mb": after_stats.cache_size_mb
            }
        }
        
        logger.info(f"üßπ Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏôÑÎ£å - CPU: {freed_cpu:.2f}GB, GPU: {freed_gpu:.2f}GB, Ï∫êÏãú: {freed_cache:.1f}MB")
        return result
        
    except Exception as e:
        logger.error(f"Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

# Ìé∏Ïùò Ìï®ÏàòÎì§
async def optimize_memory():
    """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî (ÎπÑÎèôÍ∏∞)"""
    manager = get_memory_manager()
    await manager.cleanup()

def check_memory():
    """Î©îÎ™®Î¶¨ ÏÉÅÌÉú ÌôïÏù∏"""
    manager = get_memory_manager()
    return manager.check_memory_pressure()

def check_memory_available(min_gb: float = 1.0) -> bool:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ ÌôïÏù∏"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return stats.cpu_available_gb >= min_gb
    except Exception:
        return True  # ÌôïÏù∏ Ïã§Ìå® Ïãú true Î∞òÌôò

def get_memory_info() -> Dict[str, Any]:
    """Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return {
            "device": manager.device,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "gpu_total_gb": stats.gpu_total_gb,
            "gpu_allocated_gb": stats.gpu_allocated_gb
        }
    except Exception as e:
        return {"error": str(e)}

# Îç∞ÏΩîÎ†àÏù¥ÌÑ∞
def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å Ïã§Ìñâ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            async with manager.memory_efficient_context(clear_before, clear_after):
                return await func(*args, **kwargs)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            with manager.memory_efficient_sync_context(clear_before, clear_after):
                return func(*args, **kwargs)
        
        # Ìï®ÏàòÍ∞Ä ÏΩîÎ£®Ìã¥Ïù∏ÏßÄ ÌôïÏù∏
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# Î™®Îìà ÏùµÏä§Ìè¨Ìä∏
__all__ = [
    'MemoryManager',
    'MemoryStats',
    'get_memory_manager',
    'get_global_memory_manager',
    'create_memory_manager',  # üî• ÌïµÏã¨ Ï∂îÍ∞Ä
    'create_optimized_memory_manager',
    'optimize_memory_usage',
    'optimize_memory',
    'check_memory',
    'check_memory_available',  # üî• ÌïµÏã¨ Ï∂îÍ∞Ä
    'get_memory_info',
    'memory_efficient'
]

# Î™®Îìà Î°úÎìú ÌôïÏù∏
logger.info("‚úÖ MemoryManager Î™®Îìà Î°úÎìú ÏôÑÎ£å - Î™®Îì† Ìå©ÌÜ†Î¶¨ Ìï®Ïàò Ìè¨Ìï®")