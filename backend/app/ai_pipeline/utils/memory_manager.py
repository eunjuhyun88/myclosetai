# backend/app/ai_pipeline/utils/memory_manager.py
"""
ðŸ”¥ MyCloset AI - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
================================================================================
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©
âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ëž˜í”„ - DI Containerë§Œì„ í†µí•œ ì˜ì¡´ì„± ì£¼ìž…
âœ… IDependencyInjectable ì¸í„°íŽ˜ì´ìŠ¤ ì™„ì „ ì œê±°
âœ… ë³µìž¡í•œ DI ë¡œì§ ì œê±° - Central Hub ìžë™ ë“±ë¡ë§Œ ì‚¬ìš©
âœ… DeviceManager í´ëž˜ìŠ¤ ì™„ì „ êµ¬í˜„
âœ… setup_mps_compatibility ë©”ì„œë“œ êµ¬í˜„
âœ… RuntimeWarning: coroutine ì™„ì „ í•´ê²°
âœ… M3 Max 128GB + conda í™˜ê²½ ì™„ì „ ìµœì í™”
âœ… ëª¨ë“  ë¹„ë™ê¸° ì˜¤ë¥˜ í•´ê²°
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°

í•µì‹¬ ì„¤ê³„ ì›ì¹™:
1. Single Source of Truth - ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” Central Hub DI Containerë¥¼ ê±°ì¹¨
2. Central Hub Pattern - DI Containerê°€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ì‹¬
3. Dependency Inversion - ìƒìœ„ ëª¨ë“ˆì´ í•˜ìœ„ ëª¨ë“ˆì„ ì œì–´
4. Zero Circular Reference - ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨

Author: MyCloset AI Team
Date: 2025-07-31
Version: 10.0 (Central Hub Integration)
"""

import os
import gc
import threading
import time
import logging
import asyncio
import weakref
import platform
from typing import Dict, Any, Optional, Callable, List, Union, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from contextlib import contextmanager, asynccontextmanager
from functools import wraps, lru_cache
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# ==============================================
# ðŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from app.core.di_container import CentralHubDIContainer
else:
    # ëŸ°íƒ€ìž„ì—ëŠ” Anyë¡œ ì²˜ë¦¬
    CentralHubDIContainer = Any

# ==============================================
# ðŸ”¥ ì¡°ê±´ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ì•ˆì „)
# ==============================================

# psutil ì„ íƒì  ìž„í¬íŠ¸
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ì„ íƒì  ìž„í¬íŠ¸
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

# NumPy ì„ íƒì  ìž„í¬íŠ¸
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

logger = logging.getLogger(__name__)

# ==============================================
# ðŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ìºì‹± (í•œë²ˆë§Œ ì‹¤í–‰)
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (M3 Max ê°ì§€ í¬í•¨)"""
    try:
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, platform.python_version_tuple()[:2])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ,
            "torch_available": TORCH_AVAILABLE,
            "torch_version": TORCH_VERSION
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                cpu_info = result.stdout.strip()
                is_m3_max = 'M3' in cpu_info and ('Max' in cpu_info or 'Pro' in cpu_info)
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            memory_gb = round(psutil.virtual_memory().total / (1024**3))
            system_info["memory_gb"] = memory_gb
        else:
            # M3 Max ê¸°ë³¸ê°’
            system_info["memory_gb"] = 128 if is_m3_max else 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        device = "cpu"
        if TORCH_AVAILABLE:
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "machine": "unknown", 
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8",
            "conda_env": "base",
            "in_conda": False,
            "torch_available": False,
            "torch_version": "not_available"
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ðŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0
    m3_optimizations: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
    device: str = "auto"
    memory_limit_gb: float = 16.0
    warning_threshold: float = 0.75
    critical_threshold: float = 0.9
    auto_cleanup: bool = True
    monitoring_interval: float = 30.0
    enable_caching: bool = True
    optimization_enabled: bool = True
    m3_max_features: bool = False

# ==============================================
# ðŸ”¥ ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
# ==============================================

def safe_mps_empty_cache():
    """ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬"""
    try:
        if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
                return {"success": True, "method": "mps_empty_cache"}
        
        # í´ë°±: ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        return {"success": True, "method": "fallback_gc"}
        
    except Exception as e:
        logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        gc.collect()
        return {"success": True, "method": "fallback_gc", "error": str(e)}

# ==============================================
# ðŸ”¥ DeviceManager í´ëž˜ìŠ¤ (Central Hub ì™„ì „ ì—°ë™)
# ==============================================

class DeviceManager:
    """
    ðŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ DeviceManager í´ëž˜ìŠ¤
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… setup_mps_compatibility ë©”ì„œë“œ í¬í•¨
    âœ… main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… M3 Max íŠ¹í™” ìµœì í™”
    """
    
    def __init__(self):
        """ë””ë°”ì´ìŠ¤ ê´€ë¦¬ìž ì´ˆê¸°í™”"""
        self.device = self._detect_optimal_device()
        self.is_mps_available = False
        self.is_cuda_available = False
        self.logger = logging.getLogger("DeviceManager")
        
        self._init_device_info()
        
        self.logger.debug(f"ðŸŽ® DeviceManager ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        
        # Central Hubì— ìžë™ ë“±ë¡
        self._register_to_central_hub()
    
    def _register_to_central_hub(self):
        """Central Hub DI Containerì— ìžë™ ë“±ë¡"""
        try:
            container = _get_central_hub_container()
            if container:
                container.register('device_manager', self)
                self.logger.info("âœ… DeviceManagerê°€ Central Hubì— ë“±ë¡ë¨")
            else:
                self.logger.debug("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            self.logger.debug(f"Central Hub ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if not TORCH_AVAILABLE:
                return "cpu"
            
            # M3 Max MPS ìš°ì„ 
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.is_mps_available = True
                return "mps"
            
            # CUDA í™•ì¸
            elif torch.cuda.is_available():
                self.is_cuda_available = True
                return "cuda"
            
            # CPU í´ë°±
            return "cpu"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _init_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ì´ˆê¸°í™”"""
        try:
            if not TORCH_AVAILABLE:
                return
            
            if self.device == "mps":
                # M3 Max ìµœì í™”
                self._setup_mps_optimization()
                
            elif self.device == "cuda":
                # CUDA ìµœì í™”
                if hasattr(torch.backends, 'cudnn'):
                    torch.backends.cudnn.benchmark = True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _setup_mps_optimization(self):
        """MPS ìµœì í™” ì„¤ì •"""
        try:
            # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ.update({
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                'METAL_DEVICE_WRAPPER_TYPE': '1',
                'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                'PYTORCH_MPS_PREFER_METAL': '1',
                'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                # ðŸ”¥ float64 ë¬¸ì œ í•´ê²° ì¶”ê°€
                'PYTORCH_MPS_PREFER_FLOAT32': '1',
                'PYTORCH_MPS_FORCE_FLOAT32': '1'
            })
            
            # ìŠ¤ë ˆë“œ ìµœì í™”
            if TORCH_AVAILABLE:
                torch.set_num_threads(min(16, SYSTEM_INFO["cpu_count"]))
            
            self.logger.debug("ðŸŽ MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def setup_mps_compatibility(self):
        """
        ðŸ”¥ MPS í˜¸í™˜ì„± ì„¤ì • (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ) + float64 ë¬¸ì œ í•´ê²°
        âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
        """
        try:
            if not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ - MPS í˜¸í™˜ì„± ì„¤ì • ê±´ë„ˆëœ€")
                return False
            
            if not self.is_mps_available:
                self.logger.info("â„¹ï¸ MPS ì‚¬ìš© ë¶ˆê°€ - í˜¸í™˜ì„± ì„¤ì • ê±´ë„ˆëœ€")
                return False
            
            self.logger.info("ðŸŽ MPS í˜¸í™˜ì„± ì„¤ì • ì‹œìž‘...")
            
            # 1. MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(torch.mps, 'empty_cache'):
                safe_mps_empty_cache()
                self.logger.debug("âœ… MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # 2. MPS í™˜ê²½ ë³€ìˆ˜ ìž¬ì„¤ì •
            self._setup_mps_optimization()
            
            # ðŸ”¥ 3. MPS float64 ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
            try:
                # MPSì—ì„œ ê¸°ë³¸ dtypeì„ float32ë¡œ ì„¤ì •
                if hasattr(torch, 'set_default_dtype'):
                    original_dtype = torch.get_default_dtype()
                    if original_dtype == torch.float64:
                        torch.set_default_dtype(torch.float32)
                        self.logger.debug("âœ… MPSìš© ê¸°ë³¸ dtypeì„ float32ë¡œ ì„¤ì •")
                
                # MPS ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
                os.environ.update({
                    'PYTORCH_MPS_PREFER_FLOAT32': '1',  # float32 ìš°ì„  ì‚¬ìš©
                    'PYTORCH_MPS_FORCE_FLOAT32': '1',   # float64 ì‚¬ìš© ë°©ì§€
                })
                self.logger.debug("âœ… MPS float64 ë°©ì§€ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
                
            except Exception as e:
                self.logger.debug(f"MPS dtype ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            
            # 4. MPS ë™ê¸°í™”
            if hasattr(torch.mps, 'synchronize'):
                try:
                    torch.mps.synchronize()
                    self.logger.debug("âœ… MPS ë™ê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 5. í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„± (MPS ìž‘ë™ í™•ì¸ + float32 í™•ì¸)
            try:
                test_tensor = torch.tensor([1.0], device='mps')
                test_result = test_tensor + 1
                
                # float32 í™•ì¸
                if test_tensor.dtype == torch.float32:
                    self.logger.debug("âœ… MPS ìž‘ë™ í™•ì¸ ì™„ë£Œ (float32)")
                else:
                    self.logger.warning(f"âš ï¸ MPS í…ì„œ dtype í™•ì¸: {test_tensor.dtype}")
                
                del test_tensor, test_result
            except Exception as e:
                self.logger.warning(f"âš ï¸ MPS ìž‘ë™ í™•ì¸ ì‹¤íŒ¨: {e}")
                return False
            
            self.logger.info("âœ… MPS í˜¸í™˜ì„± ì„¤ì • ì™„ë£Œ (float64 ë¬¸ì œ í•´ê²° í¬í•¨)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MPS í˜¸í™˜ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
            return False


    def get_device(self) -> str:
        """í˜„ìž¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        try:
            info = {
                "device": self.device,
                "is_mps_available": self.is_mps_available,
                "is_cuda_available": self.is_cuda_available,
                "torch_available": TORCH_AVAILABLE,
                "torch_version": TORCH_VERSION,
                "system_info": SYSTEM_INFO
            }
            
            if TORCH_AVAILABLE and self.device == "cuda":
                info.update({
                    "cuda_device_count": torch.cuda.device_count(),
                    "cuda_current_device": torch.cuda.current_device(),
                    "cuda_device_name": torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "N/A"
                })
            
            return info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"device": self.device, "error": str(e)}
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            if not TORCH_AVAILABLE:
                return
            
            if self.device == "mps":
                try:
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                    self.logger.debug("âœ… MPS ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                    
            elif self.device == "cuda":
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    self.logger.debug("âœ… CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        try:
            info = {
                'device': self.device,
                'allocated': 0,
                'cached': 0,
                'total': 0
            }
            
            if not TORCH_AVAILABLE:
                return info
            
            if self.device == "cuda" and torch.cuda.is_available():
                info.update({
                    'allocated': torch.cuda.memory_allocated(),
                    'cached': torch.cuda.memory_reserved(),
                    'total': torch.cuda.get_device_properties(0).total_memory
                })
            elif self.device == "mps":
                # MPSëŠ” ì •í™•í•œ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¶”ì •ê°’ ì‚¬ìš©
                info.update({
                    'allocated': 2 * 1024**3,  # 2GB ì¶”ì •
                    'cached': 1 * 1024**3,     # 1GB ì¶”ì •
                    'total': SYSTEM_INFO["memory_gb"] * 1024**3
                })
            
            return info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'device': self.device,
                'allocated': 0,
                'cached': 0,
                'total': 0,
                'error': str(e)
            }
    
    def is_available(self) -> bool:
        """ë””ë°”ì´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            if self.device == "mps":
                return torch.backends.mps.is_available()
            elif self.device == "cuda":
                return torch.cuda.is_available()
            else:
                return True  # CPUëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
                
        except Exception:
            return False

# ==============================================
# ðŸ”¥ MemoryManager í´ëž˜ìŠ¤ (Central Hub ì™„ì „ ì—°ë™)
# ==============================================

class MemoryManager:
    """
    ðŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… GitHub êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… M3 Max 128GB + conda ì™„ì „ í™œìš©
    âœ… ëª¨ë“  async/await ì˜¤ë¥˜ í•´ê²°
    âœ… ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% ìœ ì§€
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìžë™ ê°ì§€
        self.device = device or SYSTEM_INFO["device"]
        
        # 2. ì„¤ì • êµ¬ì„±
        config_dict = config or {}
        config_dict.update(kwargs)
        
        # MemoryConfig ìƒì„±ì„ ìœ„í•œ í•„í„°ë§
        memory_config_fields = {
            'device', 'memory_limit_gb', 'warning_threshold', 'critical_threshold',
            'auto_cleanup', 'monitoring_interval', 'enable_caching', 'optimization_enabled', 'm3_max_features'
        }
        memory_config_args = {k: v for k, v in config_dict.items() if k in memory_config_fields}
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["is_m3_max"]:
            memory_config_args.setdefault("memory_limit_gb", SYSTEM_INFO["memory_gb"] * 0.8)
            memory_config_args.setdefault("m3_max_features", True)
        
        self.config = MemoryConfig(**memory_config_args)
        
        # 3. ê¸°ë³¸ ì†ì„±
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"memory.{self.step_name}")
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # 4. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = SYSTEM_INFO["memory_gb"]
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        self.optimization_enabled = self.config.optimization_enabled
        
        # 5. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì†ì„±
        if PSUTIL_AVAILABLE:
            total_memory = psutil.virtual_memory().total / 1024**3
            self.memory_limit_gb = total_memory * 0.8
        else:
            self.memory_limit_gb = self.config.memory_limit_gb
        
        # 6. ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self._initialize_components()
        
        # Central Hubì— ìžë™ ë“±ë¡
        self._register_to_central_hub()
        
        self.logger.debug(f"ðŸŽ¯ MemoryManager ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}, ë©”ëª¨ë¦¬: {self.memory_gb}GB")

    def _register_to_central_hub(self):
        """Central Hub DI Containerì— ìžë™ ë“±ë¡"""
        try:
            container = _get_central_hub_container()
            if container:
                container.register('memory_manager', self)
                self.logger.info("âœ… MemoryManagerê°€ Central Hubì— ë“±ë¡ë¨")
            else:
                self.logger.debug("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            self.logger.debug(f"Central Hub ë“±ë¡ ì‹¤íŒ¨: {e}")

    def _initialize_components(self):
        """êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ë©”ëª¨ë¦¬ í†µê³„
            self.stats_history = []
            self.max_history_length = 100
            
            # ìºì‹œ ì‹œìŠ¤í…œ
            self.tensor_cache = {}
            self.image_cache = {}
            self.model_cache = {}
            self.cache_priority = {}
            
            # ëª¨ë‹ˆí„°ë§
            self.monitoring_active = False
            self.monitoring_thread = None
            
            # M3 Max íŠ¹í™” ì†ì„± ì´ˆê¸°í™”
            if self.is_m3_max:
                self.precision_mode = 'float16'  # M3 Maxì—ì„œ float16 ì‚¬ìš©
                self.memory_pools = {}
                self.optimal_batch_sizes = {}
                
                # M3 Max ìµœì í™” ìˆ˜í–‰
                if self.optimization_enabled:
                    self._optimize_for_m3_max()
            else:
                self.precision_mode = 'float32'
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            
            self.logger.debug(f"ðŸ§  MemoryManager êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì´ˆê¸°í™”
            self.tensor_cache = {}
            self.is_initialized = True

    def optimize_startup(self, aggressive: bool = False) -> Dict[str, Any]:
        """
        ðŸš€ ì‹œìŠ¤í…œ ì‹œìž‘ ì‹œ ë©”ëª¨ë¦¬ ìµœì í™” (ì™„ì „ ë™ê¸°í™”)
        âœ… ëª¨ë“  async/await ì˜¤ë¥˜ ì™„ì „ í•´ê²°
        âœ… RuntimeWarning ì™„ì „ í•´ê²°
        """
        try:
            start_time = time.time()
            startup_results = []
            
            self.logger.info("ðŸš€ ì‹œìŠ¤í…œ ì‹œìž‘ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìž‘")
            
            # 1. ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰ (ë™ê¸° ë°©ì‹)
            try:
                optimize_result = self._synchronous_optimize_memory()
                if optimize_result.get('success', False):
                    startup_results.append("ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                else:
                    startup_results.append("ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨")
            except Exception as e:
                startup_results.append(f"ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™” ì˜¤ë¥˜: {e}")
            
            # 2. ì‹œìŠ¤í…œ ì‹œìž‘ íŠ¹í™” ìµœì í™”
            try:
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•í™”
                collected = 0
                for gen in range(3):
                    collected += gc.collect()
                startup_results.append(f"ì‹œìž‘ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´")
                
                # M3 Max íŠ¹í™” ì‹œìž‘ ìµœì í™”
                if self.is_m3_max:
                    self._optimize_m3_max_startup()
                    startup_results.append("M3 Max ì‹œìž‘ ìµœì í™” ì™„ë£Œ")
                
                # conda í™˜ê²½ íŠ¹í™” ì„¤ì • (ë™ê¸° ë°©ì‹)
                if SYSTEM_INFO.get("in_conda", False):
                    conda_result = setup_conda_memory_optimization()
                    if conda_result:
                        startup_results.append("conda í™˜ê²½ ìµœì í™” ì™„ë£Œ")
                    else:
                        startup_results.append("conda í™˜ê²½ ìµœì í™” ì‹¤íŒ¨")
                
            except Exception as e:
                startup_results.append(f"ì‹œìž‘ íŠ¹í™” ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # 3. ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            try:
                stats = self.get_memory_stats()
                available_ratio = stats.cpu_available_gb / max(1.0, stats.cpu_total_gb)
                startup_results.append(f"ë©”ëª¨ë¦¬ ê°€ìš©ë¥ : {available_ratio:.1%}")
            except Exception as e:
                startup_results.append(f"ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            startup_time = time.time() - start_time
            
            self.logger.info(f"âœ… ì‹œìŠ¤í…œ ì‹œìž‘ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ ({startup_time:.2f}ì´ˆ)")
            
            return {
                "success": True,
                "message": "ì‹œìŠ¤í…œ ì‹œìž‘ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "startup_time": startup_time,
                "startup_results": startup_results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "conda_optimized": SYSTEM_INFO.get("in_conda", False),
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì‹œìž‘ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "ì‹œìŠ¤í…œ ì‹œìž‘ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨",
                "device": self.device,
                "timestamp": time.time()
            }
    
    def _synchronous_optimize_memory(self) -> Dict[str, Any]:
        """ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” (await ì‚¬ìš© ì•ˆí•¨)"""
        try:
            start_time = time.time()
            optimization_results = []
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰
            cleanup_result = self.cleanup_memory(aggressive=False)
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì •ë¦¬: {cleanup_result.get('success', False)}")
            
            # 2. M3 Max íŠ¹í™” ìµœì í™” (ë™ê¸°)
            if self.is_m3_max:
                m3_result = self._optimize_m3_max_memory_sync()
                optimization_results.append(f"M3 Max ìµœì í™”: {m3_result}")
            
            # 3. ìºì‹œ ìµœì í™”
            cache_stats = self._optimize_cache_system()
            optimization_results.append(f"ìºì‹œ ìµœì í™”: {cache_stats}")
            
            # 4. ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸
            pressure_info = self.check_memory_pressure()
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì••ë°•: {pressure_info.get('status', 'unknown')}")
            
            optimization_time = time.time() - start_time
            
            return {
                "success": True,
                "message": "ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "optimization_time": optimization_time,
                "optimization_results": optimization_results,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device,
                "timestamp": time.time()
            }
    
    def _optimize_m3_max_memory_sync(self):
        """M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™” (ë™ê¸° ë²„ì „)"""
        try:
            if not self.is_m3_max:
                return False
            
            # M3 Max ìµœì í™” ì‹¤í–‰
            if hasattr(self, '_optimize_for_m3_max'):
                self._optimize_for_m3_max()
            
            # M3 Max MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    safe_mps_empty_cache()
            
            return True
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _optimize_m3_max_startup(self):
        """M3 Max ì‹œìž‘ ì‹œ íŠ¹í™” ìµœì í™”"""
        try:
            if not self.is_m3_max:
                return
            
            # M3 Max ì‹œìž‘ ì‹œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            startup_env = {
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                'METAL_PERFORMANCE_SHADERS_ENABLED': '1'
            }
            
            os.environ.update(startup_env)
            
            # PyTorch MPS ì‚¬ì „ ì›Œë°ì—…
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    # MPS ìºì‹œ ì‚¬ì „ ì •ë¦¬
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                    
                    # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
                    torch.set_num_threads(min(16, SYSTEM_INFO["cpu_count"]))
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ M3 Max MPS ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            self.logger.debug("ðŸŽ M3 Max ì‹œìž‘ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ì‹œìž‘ ìµœì í™” ì‹¤íŒ¨: {e}")

    def _optimize_for_m3_max(self):
        """ðŸŽ M3 Max Neural Engine + conda ìµœì í™”"""
        try:
            if not self.is_m3_max:
                return False
                
            self.logger.debug("ðŸŽ M3 Max ìµœì í™” ì‹œìž‘")
            optimizations = []
            
            # 1. PyTorch MPS ë°±ì—”ë“œ ìµœì í™”
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                        optimizations.append("MPS cache clearing")
                    
                    # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                    os.environ.update({
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                        'PYTORCH_MPS_PREFER_METAL': '1',
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1'
                    })
                    optimizations.append("MPS environment optimization")
                    
                    # ìŠ¤ë ˆë“œ ìµœì í™”
                    torch.set_num_threads(min(16, SYSTEM_INFO["cpu_count"]))
                    optimizations.append(f"Thread optimization ({torch.get_num_threads()} threads)")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì¼ë¶€ ì‹¤íŒ¨: {e}")
            
            # 2. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            self._setup_m3_memory_pools()
            optimizations.append("Memory pool optimization")
            
            # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
            self._optimize_batch_sizes()
            optimizations.append("Batch size optimization")
            
            # 4. conda í™˜ê²½ íŠ¹í™” ìµœì í™”
            if SYSTEM_INFO["in_conda"]:
                self._setup_conda_optimizations()
                optimizations.append("Conda environment optimization")
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì™„ë£Œ")
            for opt in optimizations:
                self.logger.debug(f"   - {opt}")
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            return False

    def _setup_m3_memory_pools(self):
        """M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •"""
        try:
            # 128GB ê¸°ì¤€ ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            pool_size_gb = self.memory_gb * 0.8
            
            self.memory_pools = {
                "model_cache": pool_size_gb * 0.4,      # 40% - ëª¨ë¸ ìºì‹œ
                "inference": pool_size_gb * 0.3,        # 30% - ì¶”ë¡  ìž‘ì—…
                "preprocessing": pool_size_gb * 0.2,    # 20% - ì „ì²˜ë¦¬
                "buffer": pool_size_gb * 0.1            # 10% - ë²„í¼
            }
            
            self.logger.debug(f"ðŸŽ M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •: {pool_size_gb:.1f}GB")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 ë©”ëª¨ë¦¬ í’€ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _optimize_batch_sizes(self):
        """M3 Max ë°°ì¹˜ í¬ê¸° ìµœì í™”"""
        try:
            if self.is_m3_max:
                # M3 Max 128GB ê¸°ì¤€ ìµœì  ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 16,
                    "pose_estimation": 20,
                    "cloth_segmentation": 12,
                    "virtual_fitting": 8,
                    "super_resolution": 4
                }
            else:
                # ì¼ë°˜ ì‹œìŠ¤í…œ ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 4,
                    "pose_estimation": 6,
                    "cloth_segmentation": 3,
                    "virtual_fitting": 2,
                    "super_resolution": 1
                }
            
            self.logger.debug(f"âš™ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")

    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ íŠ¹í™” ìµœì í™”"""
        try:
            if not SYSTEM_INFO["in_conda"]:
                return
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                os.environ['OMP_NUM_THREADS'] = str(min(8, SYSTEM_INFO["cpu_count"]))
                os.environ['MKL_NUM_THREADS'] = str(min(8, SYSTEM_INFO["cpu_count"]))
                
            self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì‹¤íŒ¨: {e}")

    def optimize(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ìµœì í™” (optimize_memoryì˜ ë³„ì¹­)
        
        VirtualFittingStepê³¼ ë‹¤ë¥¸ Stepë“¤ì—ì„œ í˜¸ì¶œë˜ëŠ” í‘œì¤€ ì¸í„°íŽ˜ì´ìŠ¤
        """
        return self.optimize_memory()
    
    async def optimize_async(self) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” (í˜¸í™˜ì„±)
        """
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (blocking ìž‘ì—…)
            result = await loop.run_in_executor(None, self.optimize_memory)
            self.logger.debug("âœ… ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            return result
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "method": "async_fallback"
            }
    
    def get_memory_status(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ (Stepë“¤ì—ì„œ ì‚¬ìš©)
        """
        try:
            stats = self.get_memory_stats()
            return {
                "total_optimizations": getattr(self, 'optimization_count', 0),
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_available_gb": stats.cpu_available_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "last_optimization": getattr(self, 'last_optimization_time', None),
                "available": True
            }
        except Exception as e:
            return {
                "error": str(e),
                "available": False
            }
    
    def cleanup(self) -> bool:
        """
        ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì •ë¦¬ (Stepë“¤ì—ì„œ ì‚¬ìš©)
        """
        try:
            # ë§ˆì§€ë§‰ ìµœì í™” ì‹¤í–‰
            result = self.optimize_memory()
            
            # í†µê³„ ë¦¬ì…‹
            if hasattr(self, 'optimization_count'):
                self.optimization_count = 0
            
            self.logger.debug("âœ… MemoryManager ì •ë¦¬ ì™„ë£Œ")
            return result.get('success', True)
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ðŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            start_time = time.time()
            
            # 1. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = 0
            for _ in range(3):
                collected = gc.collect()
                collected_objects += collected
            
            # 2. ìºì‹œ ì •ë¦¬
            cache_cleared = 0
            if hasattr(self, 'tensor_cache'):
                cache_cleared += len(self.tensor_cache)
                if aggressive:
                    self.clear_cache(aggressive=True)
                else:
                    self._evict_low_priority_cache()
            
            # 3. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    if self.device == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        # M3 Max MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                        if hasattr(torch.mps, 'empty_cache'):
                            safe_mps_empty_cache()
                        
                        # MPS ë™ê¸°í™”
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        
                        # ê³µê²©ì  ì •ë¦¬ ì‹œ M3 Max íŠ¹í™” ì •ë¦¬
                        if aggressive and self.is_m3_max:
                            self._aggressive_m3_cleanup()
                        
                    elif self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        if aggressive:
                            torch.cuda.synchronize()
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            cleanup_time = time.time() - start_time
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "success": True,
                "cleanup_time": cleanup_time,
                "collected_objects": collected_objects,
                "cache_cleared": cache_cleared,
                "aggressive": aggressive,
                "device": self.device,
                "m3_optimized": self.is_m3_max
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device
            }

    def optimize_memory(self) -> Dict[str, Any]:
        """
        ðŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” (ì™„ì „ ë™ê¸°í™”)
        âœ… ëª¨ë“  async/await ì˜¤ë¥˜ í•´ê²°
        âœ… VirtualFittingStep í˜¸í™˜ì„± ìœ ì§€
        """
        try:
            start_time = time.time()
            optimization_results = []
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰
            cleanup_result = self.cleanup_memory(aggressive=False)
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì •ë¦¬: {cleanup_result.get('success', False)}")
            
            # 2. M3 Max íŠ¹í™” ìµœì í™”
            if self.is_m3_max:
                m3_result = self._optimize_m3_max_memory_sync()
                optimization_results.append(f"M3 Max ìµœì í™”: {m3_result}")
            
            # 3. ìºì‹œ ìµœì í™”
            cache_stats = self._optimize_cache_system()
            optimization_results.append(f"ìºì‹œ ìµœì í™”: {cache_stats}")
            
            # 4. ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸
            pressure_info = self.check_memory_pressure()
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì••ë°•: {pressure_info.get('status', 'unknown')}")
            
            optimization_time = time.time() - start_time
            
            return {
                "success": True,
                "message": "ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "optimization_time": optimization_time,
                "optimization_results": optimization_results,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device,
                "timestamp": time.time()
            }

    def _optimize_cache_system(self) -> str:
        """ìºì‹œ ì‹œìŠ¤í…œ ìµœì í™”"""
        try:
            # ìºì‹œ í¬ê¸° ì²´í¬
            total_cache_size = 0
            cache_counts = {}
            
            for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                if hasattr(self, cache_name):
                    cache = getattr(self, cache_name)
                    size = len(cache)
                    total_cache_size += size
                    cache_counts[cache_name] = size
            
            # ìºì‹œê°€ ë„ˆë¬´ í´ ê²½ìš° ì •ë¦¬
            if total_cache_size > 100:  # ìºì‹œ í•­ëª©ì´ 100ê°œ ì´ìƒ
                self._evict_low_priority_cache()
                return f"ìºì‹œ ì •ë¦¬ë¨ (ì´ì „: {total_cache_size}ê°œ)"
            
            return f"ì •ìƒ ({total_cache_size}ê°œ í•­ëª©)"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ìµœì í™” ì‹¤íŒ¨: {e}")
            return "ìµœì í™” ì‹¤íŒ¨"
    
    def _aggressive_m3_cleanup(self):
        """ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ëª¨ë“  ìºì‹œ í´ë¦¬ì–´
            for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                if hasattr(self, cache_name):
                    getattr(self, cache_name).clear()
            
            # ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for _ in range(5):
                gc.collect()
            
            # PyTorch MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
                safe_mps_empty_cache()
            
            self.logger.debug("ðŸŽ ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_memory_stats(self) -> MemoryStats:
        """í˜„ìž¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            # CPU ë©”ëª¨ë¦¬
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                cpu_percent = memory.percent
                cpu_total_gb = memory.total / 1024**3
                cpu_used_gb = memory.used / 1024**3
                cpu_available_gb = memory.available / 1024**3
                swap_used_gb = psutil.swap_memory().used / 1024**3
                
                # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
                try:
                    process = psutil.Process()
                    process_memory_mb = process.memory_info().rss / 1024**2
                except:
                    process_memory_mb = 0.0
            else:
                cpu_percent = 50.0
                cpu_total_gb = self.memory_gb
                cpu_used_gb = self.memory_gb * 0.5
                cpu_available_gb = self.memory_gb * 0.5
                swap_used_gb = 0.0
                process_memory_mb = 0.0
            
            # GPU ë©”ëª¨ë¦¬
            gpu_allocated_gb = 0.0
            gpu_reserved_gb = 0.0
            gpu_total_gb = 0.0
            
            if TORCH_AVAILABLE:
                try:
                    if self.device == "cuda" and torch.cuda.is_available():
                        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        gpu_total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    elif self.device == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        # MPS ë©”ëª¨ë¦¬ ì •ë³´ (ì¶”ì •)
                        gpu_allocated_gb = 2.0
                        gpu_total_gb = self.memory_gb  # M3 Max í†µí•© ë©”ëª¨ë¦¬
                except Exception:
                    pass
            
            # ìºì‹œ í¬ê¸°
            cache_size_mb = 0.0
            try:
                for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                    if hasattr(self, cache_name):
                        cache = getattr(self, cache_name)
                        cache_size_mb += len(str(cache)) / 1024**2
            except:
                pass
            
            # M3 ìµœì í™” ì •ë³´
            m3_optimizations = {}
            if self.is_m3_max:
                m3_optimizations = {
                    "memory_pools": getattr(self, 'memory_pools', {}),
                    "batch_sizes": getattr(self, 'optimal_batch_sizes', {}),
                    "precision_mode": self.precision_mode,
                    "conda_optimized": SYSTEM_INFO["in_conda"]
                }
            
            return MemoryStats(
                cpu_percent=cpu_percent,
                cpu_available_gb=cpu_available_gb,
                cpu_used_gb=cpu_used_gb,
                cpu_total_gb=cpu_total_gb,
                gpu_allocated_gb=gpu_allocated_gb,
                gpu_reserved_gb=gpu_reserved_gb,
                gpu_total_gb=gpu_total_gb,
                swap_used_gb=swap_used_gb,
                cache_size_mb=cache_size_mb,
                process_memory_mb=process_memory_mb,
                m3_optimizations=m3_optimizations
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return MemoryStats(
                cpu_percent=0.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )

    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            stats = self.get_memory_stats()
            
            cpu_usage_ratio = stats.cpu_used_gb / max(1.0, stats.cpu_total_gb)
            gpu_usage_ratio = stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb)
            
            status = "normal"
            if cpu_usage_ratio > 0.9 or gpu_usage_ratio > 0.9:
                status = "critical"
            elif cpu_usage_ratio > 0.75 or gpu_usage_ratio > 0.75:
                status = "warning"
            
            return {
                "status": status,
                "cpu_usage_ratio": cpu_usage_ratio,
                "gpu_usage_ratio": gpu_usage_ratio,
                "cache_size_mb": stats.cache_size_mb,
                "recommendations": self._get_cleanup_recommendations(stats)
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬ ì‹¤íŒ¨: {e}")
            return {"status": "unknown", "error": str(e)}

    def _get_cleanup_recommendations(self, stats: MemoryStats) -> List[str]:
        """ì •ë¦¬ ê¶Œìž¥ì‚¬í•­"""
        recommendations = []
        
        try:
            cpu_ratio = stats.cpu_used_gb / max(1.0, stats.cpu_total_gb)
            
            if cpu_ratio > 0.8:
                recommendations.append("CPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œìž¥")
            
            if stats.gpu_allocated_gb > 10.0:
                recommendations.append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œìž¥")
            
            if stats.cache_size_mb > 1000:
                recommendations.append("ìºì‹œ ì •ë¦¬ ê¶Œìž¥")
            
            if self.is_m3_max and cpu_ratio > 0.7:
                recommendations.append("M3 Max ìµœì í™” ìž¬ì‹¤í–‰ ê¶Œìž¥")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê¶Œìž¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return recommendations

    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self._lock:
                if aggressive:
                    # ì „ì²´ ìºì‹œ ì‚­ì œ
                    for cache_name in ['tensor_cache', 'image_cache', 'model_cache', 'cache_priority']:
                        if hasattr(self, cache_name):
                            getattr(self, cache_name).clear()
                    
                    self.logger.debug("ðŸ§¹ ì „ì²´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                else:
                    # ì„ íƒì  ìºì‹œ ì •ë¦¬
                    self._evict_low_priority_cache()
                    self.logger.debug("ðŸ§¹ ì„ íƒì  ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _evict_low_priority_cache(self):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ ì œê±°"""
        try:
            if not hasattr(self, 'cache_priority') or not self.cache_priority:
                return
            
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # í•˜ìœ„ 20% ì œê±°
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                    if hasattr(self, cache_name):
                        getattr(self, cache_name).pop(key, None)
                self.cache_priority.pop(key, None)
            
            self.logger.debug(f"ðŸ—‘ï¸ ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ {num_to_remove}ê°œ ì œê±°")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì œê±° ì‹¤íŒ¨: {e}")

    # ============================================
    # ðŸ”¥ ë¹„ë™ê¸° ì¸í„°íŽ˜ì´ìŠ¤ (ì•ˆì „í•œ ëž˜í¼)
    # ============================================

    async def initialize(self) -> bool:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            # M3 Max ìµœì í™” ì„¤ì • (ë™ê¸°ë¡œ ì‹¤í–‰)
            if self.is_m3_max and self.optimization_enabled:
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._optimize_for_m3_max)
            
            self.logger.debug(f"âœ… MemoryManager ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def cleanup(self):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.cleanup_memory, True)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_usage(self) -> Dict[str, Any]:
        """ë™ê¸° ì‚¬ìš©ëŸ‰ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)"""
        try:
            stats = self.get_memory_stats()
            return {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def __del__(self):
        """ì†Œë©¸ìž"""
        try:
            if hasattr(self, 'monitoring_active'):
                self.monitoring_active = False
            if hasattr(self, 'tensor_cache'):
                self.clear_cache(aggressive=True)
        except:
            pass

# ==============================================
# ðŸ”¥ MemoryManagerAdapter í´ëž˜ìŠ¤ (Central Hub ì™„ì „ ì—°ë™)
# ==============================================

class MemoryManagerAdapter:
    """
    ðŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ MemoryManagerAdapter í´ëž˜ìŠ¤
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… optimize_memory ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
    âœ… VirtualFittingStep í˜¸í™˜ì„± 100%
    âœ… ëª¨ë“  async/await ì˜¤ë¥˜ í•´ê²°
    """
    
    def __init__(self, base_manager: Optional[MemoryManager] = None, device: str = "auto", **kwargs):
        """MemoryManagerAdapter ì´ˆê¸°í™”"""
        try:
            # ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì„¤ì •
            if base_manager is None:
                self._base_manager = MemoryManager(device=device, **kwargs)
            else:
                self._base_manager = base_manager
            
            # ì†ì„± ì´ˆê¸°í™”
            self.device = self._base_manager.device
            self.is_m3_max = self._base_manager.is_m3_max
            self.memory_gb = self._base_manager.memory_gb
            self.logger = logging.getLogger("MemoryManagerAdapter")
            
            # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
            self._lock = threading.RLock()
            
            # ì–´ëŒ‘í„° ê³ ìœ  ì†ì„±
            self.adapter_initialized = True
            self.optimization_cache = {}
            self.last_optimization_time = 0
            
            self.logger.debug(f"âœ… MemoryManagerAdapter ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
            
            # Central Hubì— ìžë™ ë“±ë¡
            self._register_to_central_hub()
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì´ˆê¸°í™”
            self._base_manager = MemoryManager(device="cpu")
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16
            self.logger = logging.getLogger("MemoryManagerAdapter")

    def _register_to_central_hub(self):
        """Central Hub DI Containerì— ìžë™ ë“±ë¡"""
        try:
            container = _get_central_hub_container()
            if container:
                container.register('memory_adapter', self)
                self.logger.info("âœ… MemoryManagerAdapterê°€ Central Hubì— ë“±ë¡ë¨")
            else:
                self.logger.debug("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            self.logger.debug(f"Central Hub ë“±ë¡ ì‹¤íŒ¨: {e}")

    def optimize_startup(self, aggressive: bool = False) -> Dict[str, Any]:
        """
        ðŸš€ MemoryManagerAdapterìš© optimize_startup (ì™„ì „ ë™ê¸°í™”)
        âœ… VirtualFittingStep í˜¸í™˜ì„± ë³´ìž¥
        âœ… ëª¨ë“  async/await ì˜¤ë¥˜ í•´ê²°
        """
        try:
            # ê¸°ë³¸ ë§¤ë‹ˆì €ì˜ optimize_startup ì‹œë„
            if hasattr(self._base_manager, 'optimize_startup'):
                try:
                    result = self._base_manager.optimize_startup(aggressive)
                    result["adapter"] = True
                    return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ê¸°ë³¸ ë§¤ë‹ˆì € optimize_startup ì‹¤íŒ¨: {e}")
            
            # í´ë°±: optimize_memory ì‚¬ìš©
            if hasattr(self, 'optimize_memory'):
                try:
                    result = self.optimize_memory(aggressive)
                    result.update({
                        "adapter": True,
                        "fallback_mode": "optimize_memory",
                        "message": "startup ìµœì í™”ë¥¼ optimize_memoryë¡œ ëŒ€ì²´"
                    })
                    return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í´ë°± optimize_memory ì‹¤íŒ¨: {e}")
            
            # ìµœì¢… í´ë°±: ê¸°ë³¸ ì‹œìž‘ ìµœì í™”
            return self._basic_startup_optimization(aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter optimize_startup ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "adapter": True,
                "timestamp": time.time()
            }
    
    def _basic_startup_optimization(self, aggressive: bool = False) -> Dict[str, Any]:
        """ê¸°ë³¸ ì‹œìž‘ ìµœì í™” (ìµœì¢… í´ë°±)"""
        try:
            startup_results = []
            
            # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            startup_results.append(f"ê¸°ë³¸ GC: {collected}ê°œ ê°ì²´")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        safe_mps_empty_cache()
                        startup_results.append("MPS ìºì‹œ ì •ë¦¬")
                    elif hasattr(torch.cuda, 'empty_cache') and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        startup_results.append("CUDA ìºì‹œ ì •ë¦¬")
                except Exception as e:
                    startup_results.append(f"GPU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "message": "ê¸°ë³¸ ì‹œìž‘ ìµœì í™” ì™„ë£Œ",
                "startup_results": startup_results,
                "adapter": True,
                "fallback": True,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "adapter": True,
                "fallback": True,
                "timestamp": time.time()
            }

    def optimize_memory(self, aggressive: bool = False, **kwargs) -> Dict[str, Any]:
        """
        ðŸ”¥ VirtualFittingStepì—ì„œ í•„ìš”í•œ í•µì‹¬ optimize_memory ë©”ì„œë“œ
        âœ… ì™„ì „ ë™ê¸°í™”ë¡œ ëª¨ë“  async/await ì˜¤ë¥˜ í•´ê²°
        âœ… AttributeError ì™„ì „ í•´ê²°
        """
        try:
            start_time = time.time()
            optimization_results = []
            
            # ì¤‘ë³µ ìµœì í™” ë°©ì§€ (5ì´ˆ ë‚´ ìž¬í˜¸ì¶œ ë°©ì§€)
            if (start_time - self.last_optimization_time) < 5.0:
                return {
                    "success": True,
                    "message": "ìµœê·¼ ìµœì í™” ì™„ë£Œ (ìºì‹œë¨)",
                    "cached": True,
                    "device": self.device,
                    "timestamp": start_time
                }
            
            # 1. ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ìžì˜ optimize_memory í˜¸ì¶œ ì‹œë„
            if hasattr(self._base_manager, 'optimize_memory'):
                try:
                    base_result = self._base_manager.optimize_memory()
                    optimization_results.append("ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                    
                    # ì„±ê³µí•œ ê²½ìš° ë°”ë¡œ ë°˜í™˜
                    if base_result.get("success", False):
                        self.last_optimization_time = start_time
                        return {
                            **base_result,
                            "adapter": True,
                            "optimization_results": optimization_results,
                            "device": self.device,
                            "timestamp": start_time
                        }
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ê¸°ë³¸ ìµœì í™” ì‹¤íŒ¨, í´ë°± ëª¨ë“œ ì‚¬ìš©: {e}")
            
            # 2. í´ë°±: cleanup_memory ê¸°ë°˜ ìµœì í™”
            cleanup_result = self._base_manager.cleanup_memory(aggressive=aggressive)
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì •ë¦¬: {cleanup_result.get('success', False)}")
            
            # 3. ì–´ëŒ‘í„° íŠ¹í™” ìµœì í™”
            adapter_optimizations = self._run_adapter_optimizations(aggressive)
            optimization_results.extend(adapter_optimizations)
            
            # 4. M3 Max íŠ¹í™” ìµœì í™” (ë™ê¸°)
            if self.is_m3_max:
                m3_optimizations = self._run_m3_max_optimizations_sync()
                optimization_results.extend(m3_optimizations)
            
            # 5. ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            final_stats = self._base_manager.get_memory_stats()
            
            optimization_time = time.time() - start_time
            self.last_optimization_time = start_time
            
            # ìµœì í™” ê²°ê³¼ ìºì‹±
            result = {
                "success": True,
                "message": "MemoryManagerAdapter ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "optimization_time": optimization_time,
                "optimization_results": optimization_results,
                "cleanup_result": cleanup_result,
                "final_memory_stats": {
                    "cpu_used_gb": final_stats.cpu_used_gb,
                    "cpu_available_gb": final_stats.cpu_available_gb,
                    "gpu_allocated_gb": final_stats.gpu_allocated_gb,
                    "cache_size_mb": final_stats.cache_size_mb
                },
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "adapter": True,
                "aggressive": aggressive,
                "timestamp": start_time
            }
            
            self.optimization_cache = result
            self.logger.debug("âœ… MemoryManagerAdapter ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "adapter": True,
                "device": self.device,
                "timestamp": time.time()
            }

    def optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ìµœì í™” (optimize_memoryì˜ ë³„ì¹­) - MemoryManagerAdapterìš©
        """
        return self.optimize_memory(aggressive=aggressive)
    
    async def optimize_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” - MemoryManagerAdapterìš©
        """
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self.optimize_memory, aggressive)
            return result
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter ë¹„ë™ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "adapter": True
            }
    
    def get_memory_status(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ - MemoryManagerAdapterìš©
        """
        try:
            base_status = self._base_manager.get_memory_status()
            base_status.update({
                "adapter": True,
                "adapter_type": "MemoryManagerAdapter",
                "base_manager_type": type(self._base_manager).__name__
            })
            return base_status
        except Exception as e:
            return {
                "error": str(e),
                "adapter": True,
                "available": False
            }
    
    def cleanup(self) -> bool:
        """
        ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì •ë¦¬ - MemoryManagerAdapterìš©
        """
        try:
            # ê¸°ë³¸ ë§¤ë‹ˆì € ì •ë¦¬
            result = self._base_manager.cleanup()
            
            # ì–´ëŒ‘í„° ìºì‹œ ì •ë¦¬
            if hasattr(self, 'optimization_cache'):
                self.optimization_cache.clear()
            
            self.logger.debug("âœ… MemoryManagerAdapter ì •ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    def _run_adapter_optimizations(self, aggressive: bool = False) -> List[str]:
        """ì–´ëŒ‘í„° íŠ¹í™” ìµœì í™” (ë™ê¸°)"""
        optimizations = []
        
        try:
            # 1. ìºì‹œ ì •ë¦¬
            if hasattr(self._base_manager, 'clear_cache'):
                self._base_manager.clear_cache(aggressive=aggressive)
                optimizations.append("ì–´ëŒ‘í„° ìºì‹œ ì •ë¦¬")
            
            # 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            optimizations.append(f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´")
            
            # 3. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    if self.device == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        if hasattr(torch.mps, 'empty_cache'):
                            safe_mps_empty_cache()
                            optimizations.append("MPS ìºì‹œ ì •ë¦¬")
                    elif self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        optimizations.append("CUDA ìºì‹œ ì •ë¦¬")
                except Exception as e:
                    optimizations.append(f"GPU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
            
            return optimizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ìµœì í™” ì‹¤íŒ¨: {e}")
            return ["ì–´ëŒ‘í„° ìµœì í™” ì‹¤íŒ¨"]

    def _run_m3_max_optimizations_sync(self) -> List[str]:
        """M3 Max íŠ¹í™” ìµœì í™” (ë™ê¸° ë²„ì „)"""
        optimizations = []
        
        try:
            if not self.is_m3_max:
                return optimizations
            
            # M3 Max íŠ¹í™” ë¡œì§ (ë™ê¸°)
            if hasattr(self._base_manager, '_optimize_for_m3_max'):
                self._base_manager._optimize_for_m3_max()
                optimizations.append("M3 Max Neural Engine ìµœì í™”")
            
            # MPS íŠ¹í™” ì •ë¦¬
            if TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    safe_mps_empty_cache()
                optimizations.append("M3 Max MPS ìºì‹œ ì •ë¦¬")
            
            # ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”
            if hasattr(self._base_manager, '_aggressive_m3_cleanup'):
                self._base_manager._aggressive_m3_cleanup()
                optimizations.append("M3 Max ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬")
            
            return optimizations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            return ["M3 Max ìµœì í™” ì‹¤íŒ¨"]

    # ============================================
    # ðŸ”¥ ìœ„ìž„ ë©”ì„œë“œë“¤ (ëª¨ë“  í•„ìš”í•œ ë©”ì„œë“œ ìœ„ìž„)
    # ============================================

    def get_memory_stats(self) -> MemoryStats:
        """ë©”ëª¨ë¦¬ í†µê³„ (ìœ„ìž„)"""
        try:
            return self._base_manager.get_memory_stats()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return MemoryStats(
                cpu_percent=50.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ìœ„ìž„)"""
        try:
            result = self._base_manager.cleanup_memory(aggressive)
            result["adapter"] = True
            return result
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "adapter": True,
                "device": self.device
            }
    
    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ (ìœ„ìž„)"""
        try:
            result = self._base_manager.check_memory_pressure()
            result["adapter"] = True
            return result
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                "status": "unknown",
                "error": str(e),
                "adapter": True
            }
    
    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬ (ìœ„ìž„)"""
        try:
            return self._base_manager.clear_cache(aggressive)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_usage(self) -> Dict[str, Any]:
        """ë™ê¸° ì‚¬ìš©ëŸ‰ ì¡°íšŒ (ìœ„ìž„)"""
        try:
            result = self._base_manager.get_usage()
            result["adapter"] = True
            return result
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e), 
                "adapter": True
            }

    def __getattr__(self, name):
        """ë‹¤ë¥¸ ëª¨ë“  ì†ì„±/ë©”ì„œë“œëŠ” ê¸°ë³¸ ê´€ë¦¬ìžë¡œ ìœ„ìž„"""
        try:
            return getattr(self._base_manager, name)
        except AttributeError:
            self.logger.warning(f"âš ï¸ ì†ì„± '{name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

    def __del__(self):
        """ì†Œë©¸ìž"""
        try:
            if hasattr(self, '_base_manager') and self._base_manager:
                if hasattr(self._base_manager, '__del__'):
                    self._base_manager.__del__()
        except:
            pass

# ==============================================
# ðŸ”¥ GPUMemoryManager í´ëž˜ìŠ¤ (Central Hub ì™„ì „ ì—°ë™)
# ==============================================

class GPUMemoryManager(MemoryManager):
    """
    ðŸ”¥ Central Hub ì—°ë™ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ìž (ê¸°ì¡´ í´ëž˜ìŠ¤ëª… ìœ ì§€)
    âœ… í˜„ìž¬ êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… ê¸°ì¡´ ì½”ë“œì˜ GPUMemoryManager ì‚¬ìš© ìœ ì§€
    âœ… main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    """
    
    def __init__(self, device=None, memory_limit_gb=None, **kwargs):
        """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™” (ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if device is None:
            device = SYSTEM_INFO["device"]
        if memory_limit_gb is None:
            memory_limit_gb = SYSTEM_INFO["memory_gb"] * 0.8 if SYSTEM_INFO["is_m3_max"] else 16.0
        
        super().__init__(device=device, memory_limit_gb=memory_limit_gb, **kwargs)
        self.logger = logging.getLogger("GPUMemoryManager")
        
        # ê¸°ì¡´ ì†ì„± í˜¸í™˜ì„± ìœ ì§€
        self.memory_limit_gb = memory_limit_gb
        
        self.logger.debug(f"ðŸŽ® GPUMemoryManager ì´ˆê¸°í™” - {device} ({memory_limit_gb:.1f}GB)")

    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        try:
            # ë¶€ëª¨ í´ëž˜ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì •ë¦¬ í˜¸ì¶œ
            result = self.cleanup_memory(aggressive=False)
            
            # PyTorch GPU ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.debug("ðŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        try:
            stats = self.get_memory_stats()
            
            # ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                used_gb = memory.used / (1024**3)
                if used_gb > self.memory_limit_gb * 0.9:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {used_gb:.1f}GB")
                    self.clear_cache()
            
            return {
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "memory_limit_gb": self.memory_limit_gb
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ==============================================
# ðŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ (Central Hub ì—°ë™)
# ==============================================

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_memory_manager = None
_global_gpu_memory_manager = None
_global_adapter = None
_global_device_manager = None
_manager_lock = threading.Lock()

def get_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Central Hub ìžë™ ì—°ë™)"""
    global _global_memory_manager
    
    with _manager_lock:
        if _global_memory_manager is None:
            _global_memory_manager = MemoryManager(**kwargs)
        return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager(**kwargs)

def get_device_manager(**kwargs) -> DeviceManager:
    """
    ðŸ”¥ DeviceManager ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜)
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… setup_mps_compatibility ë©”ì„œë“œ í¬í•¨
    âœ… Central Hub ìžë™ ì—°ë™
    """
    global _global_device_manager
    
    with _manager_lock:
        if _global_device_manager is None:
            _global_device_manager = DeviceManager()
            
            logger.info(f"âœ… DeviceManager ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {_global_device_manager.device}")
        return _global_device_manager

def get_memory_adapter(device: str = "auto", **kwargs) -> MemoryManagerAdapter:
    """VirtualFittingStepìš© ì–´ëŒ‘í„° ë°˜í™˜ (Central Hub ìžë™ ì—°ë™)"""
    global _global_adapter
    
    try:
        with _manager_lock:
            if _global_adapter is None:
                base_manager = get_memory_manager(device=device, **kwargs)
                _global_adapter = MemoryManagerAdapter(base_manager)
                
                logger.info(f"âœ… ë©”ëª¨ë¦¬ ì–´ëŒ‘í„° ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}")
            return _global_adapter
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì–´ëŒ‘í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # í´ë°± ì–´ëŒ‘í„° ìƒì„±
        fallback_manager = MemoryManager(device="cpu")
        return MemoryManagerAdapter(fallback_manager)

def create_memory_manager(device: str = "auto", **kwargs) -> MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ìž íŒ©í† ë¦¬ í•¨ìˆ˜ (Central Hub ìžë™ ì—°ë™)"""
    try:
        if device == "auto":
            device = SYSTEM_INFO["device"]
        
        logger.debug(f"ðŸ“¦ MemoryManager ìƒì„± - ë””ë°”ì´ìŠ¤: {device}")
        manager = MemoryManager(device=device, **kwargs)
        return manager
    except Exception as e:
        logger.warning(f"âš ï¸ MemoryManager ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        return MemoryManager(device="cpu")
        
def get_step_memory_manager(step_name: str, **kwargs) -> Union[MemoryManager, MemoryManagerAdapter]:
    """
    ðŸ”¥ Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ë°˜í™˜ (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜)
    âœ… ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€
    âœ… __init__.pyì—ì„œ exportë˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… VirtualFittingStepìš© MemoryManagerAdapter ì§€ì›
    âœ… Central Hub ìžë™ ì—°ë™
    """
    try:
        # Stepë³„ íŠ¹í™” ì„¤ì • (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê¸°ì¤€)
        step_configs = {
            "HumanParsingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "PoseEstimationStep": {"memory_limit_gb": 6.0, "optimization_enabled": True},
            "ClothSegmentationStep": {"memory_limit_gb": 4.0, "optimization_enabled": True},
            "GeometricMatchingStep": {"memory_limit_gb": 6.0, "optimization_enabled": True},
            "ClothWarpingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "VirtualFittingStep": {"memory_limit_gb": 16.0, "optimization_enabled": True},
            "PostProcessingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "QualityAssessmentStep": {"memory_limit_gb": 4.0, "optimization_enabled": True}
        }
        
        # M3 Maxì—ì„œëŠ” ë” í° ë©”ëª¨ë¦¬ í• ë‹¹
        if SYSTEM_INFO["is_m3_max"]:
            for config in step_configs.values():
                config["memory_limit_gb"] *= 2  # M3 Maxì—ì„œ 2ë°° ë©”ëª¨ë¦¬ í• ë‹¹
        
        # Stepë³„ ì„¤ì • ì ìš©
        step_config = step_configs.get(step_name, {"memory_limit_gb": 8.0})
        final_kwargs = kwargs.copy()
        final_kwargs.update(step_config)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±
        base_manager = MemoryManager(**final_kwargs)
        base_manager.step_name = step_name
        base_manager.logger = logging.getLogger(f"memory.{step_name}")
        
        # VirtualFittingStepì¸ ê²½ìš° ì–´ëŒ‘í„° ë°˜í™˜
        if step_name == "VirtualFittingStep":
            adapter = MemoryManagerAdapter(base_manager)
            logger.debug(f"ðŸ“ {step_name} MemoryManagerAdapter ìƒì„± ì™„ë£Œ")
            return adapter
        else:
            logger.debug(f"ðŸ“ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„± ì™„ë£Œ")
            return base_manager
        
    except Exception as e:
        logger.warning(f"âš ï¸ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ë°˜í™˜
        base_manager = MemoryManager(**kwargs)
        if step_name == "VirtualFittingStep":
            return MemoryManagerAdapter(base_manager)
        return base_manager

def initialize_global_memory_manager(device: str = None, **kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™” (Central Hub ìžë™ ì—°ë™)"""
    global _global_memory_manager
    
    try:
        with _manager_lock:
            if _global_memory_manager is None:
                if device is None:
                    device = SYSTEM_INFO["device"]
                
                _global_memory_manager = MemoryManager(device=device, **kwargs)
                logger.info(f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {device}")
            return _global_memory_manager
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        _global_memory_manager = MemoryManager(device="cpu")
        return _global_memory_manager

def optimize_memory_usage(device: str = None, aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” - ë™ê¸° í•¨ìˆ˜"""
    try:
        manager = get_memory_manager(device=device or "auto")
        
        # ìµœì í™” ì „ ìƒíƒœ
        before_stats = manager.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        result = manager.cleanup_memory(aggressive=aggressive)
        
        # ìµœì í™” í›„ ìƒíƒœ
        after_stats = manager.get_memory_stats()
        
        # ê²°ê³¼ ê³„ì‚°
        freed_cpu = max(0, before_stats.cpu_used_gb - after_stats.cpu_used_gb)
        freed_gpu = max(0, before_stats.gpu_allocated_gb - after_stats.gpu_allocated_gb)
        freed_cache = max(0, before_stats.cache_size_mb - after_stats.cache_size_mb)
        
        result.update({
            "freed_memory": {
                "cpu_gb": freed_cpu,
                "gpu_gb": freed_gpu,
                "cache_mb": freed_cache
            },
            "before": {
                "cpu_used_gb": before_stats.cpu_used_gb,
                "gpu_allocated_gb": before_stats.gpu_allocated_gb,
                "cache_size_mb": before_stats.cache_size_mb
            },
            "after": {
                "cpu_used_gb": after_stats.cpu_used_gb,
                "gpu_allocated_gb": after_stats.gpu_allocated_gb,
                "cache_size_mb": after_stats.cache_size_mb
            }
        })
        
        logger.info(f"ðŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - CPU: {freed_cpu:.2f}GB, GPU: {freed_gpu:.2f}GB")
        return result
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

# ==============================================
# ðŸ”¥ íŽ¸ì˜ í•¨ìˆ˜ë“¤ (ëª¨ë“  async ì˜¤ë¥˜ í•´ê²°)
# ==============================================

def optimize_memory() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìµœì í™” (ë™ê¸°í™”)"""
    try:
        manager = get_memory_manager()
        return manager.optimize_memory()
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def check_memory():
    """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    try:
        manager = get_memory_manager()
        return manager.check_memory_pressure()
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {"status": "unknown", "error": str(e)}

def check_memory_available(min_gb: float = 1.0) -> bool:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return stats.cpu_available_gb >= min_gb
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ true ë°˜í™˜

def get_memory_info() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return {
            "device": manager.device,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "cpu_used_gb": stats.cpu_used_gb,
            "gpu_total_gb": stats.gpu_total_gb,
            "gpu_allocated_gb": stats.gpu_allocated_gb,
            "is_m3_max": manager.is_m3_max,
            "memory_gb": manager.memory_gb,
            "conda_env": SYSTEM_INFO["in_conda"]
        }
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ðŸ”¥ ë°ì½”ë ˆì´í„° (async ì˜¤ë¥˜ í•´ê²°)
# ==============================================

def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            if clear_before:
                manager.cleanup_memory()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                if clear_after:
                    manager.cleanup_memory()
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                manager = get_memory_manager()
                if clear_before:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, manager.cleanup_memory)
                try:
                    result = await func(*args, **kwargs)
                    return result
                finally:
                    if clear_after:
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, manager.cleanup_memory)
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# ==============================================
# ðŸ”¥ conda í™˜ê²½ íŠ¹í™” í•¨ìˆ˜ë“¤ (ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
# ==============================================

def setup_conda_memory_optimization() -> bool:
    """
    conda í™˜ê²½ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ì™„ì „ ë™ê¸°í™”)
    âœ… object dict can't be used in 'await' expression ì™„ì „ í•´ê²°
    """
    try:
        if not SYSTEM_INFO["in_conda"]:
            logger.warning("âš ï¸ conda í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤")
            return False
        
        optimizations = []
        
        # 1. NumPy/MKL ìŠ¤ë ˆë“œ ìµœì í™”
        if NUMPY_AVAILABLE:
            optimal_threads = min(8, SYSTEM_INFO["cpu_count"])
            os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
            os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
            os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)
            optimizations.append(f"NumPy/MKL ìŠ¤ë ˆë“œ: {optimal_threads}")
        
        # 2. PyTorch ì„¤ì •
        if TORCH_AVAILABLE:
            torch.set_num_threads(min(16, SYSTEM_INFO["cpu_count"]))
            optimizations.append(f"PyTorch ìŠ¤ë ˆë“œ: {torch.get_num_threads()}")
        
        # 3. M3 Max íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["is_m3_max"]:
            os.environ.update({
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                'PYTORCH_ENABLE_MPS_FALLBACK': '1'
            })
            optimizations.append("M3 Max MPS ìµœì í™”")
        
        logger.info("âœ… conda í™˜ê²½ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ")
        for opt in optimizations:
            logger.debug(f"   - {opt}")
        
        return True
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def get_conda_memory_recommendations() -> List[str]:
    """conda í™˜ê²½ ë©”ëª¨ë¦¬ ìµœì í™” ê¶Œìž¥ì‚¬í•­"""
    recommendations = []
    
    try:
        if not SYSTEM_INFO["in_conda"]:
            recommendations.append("conda í™˜ê²½ ì‚¬ìš© ê¶Œìž¥")
            return recommendations
        
        # í˜„ìž¬ ìƒíƒœ í™•ì¸
        current_threads = os.environ.get('OMP_NUM_THREADS', 'auto')
        if current_threads == 'auto':
            recommendations.append("OMP_NUM_THREADS ì„¤ì • ê¶Œìž¥")
        
        if TORCH_AVAILABLE:
            current_torch_threads = torch.get_num_threads()
            optimal_threads = min(16, SYSTEM_INFO["cpu_count"])
            if current_torch_threads != optimal_threads:
                recommendations.append(f"PyTorch ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™” ({current_torch_threads} â†’ {optimal_threads})")
        
        if SYSTEM_INFO["is_m3_max"]:
            mps_ratio = os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO')
            if mps_ratio != '0.0':
                recommendations.append("M3 Max MPS ë©”ëª¨ë¦¬ ë¹„ìœ¨ ìµœì í™” ê¶Œìž¥")
        
        if not recommendations:
            recommendations.append("conda í™˜ê²½ ìµœì í™” ìƒíƒœ ì–‘í˜¸")
        
        return recommendations
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda ê¶Œìž¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        return ["conda ìµœì í™” ìƒíƒœ í™•ì¸ ë¶ˆê°€"]

def create_conda_optimized_manager(step_name: str = "default", **kwargs) -> Union[MemoryManager, MemoryManagerAdapter]:
    """conda í™˜ê²½ ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ìž ìƒì„±"""
    try:
        # conda ìµœì í™” ë¨¼ì € ì„¤ì •
        setup_conda_memory_optimization()
        
        # Stepë³„ ì„¤ì •
        if step_name == "VirtualFittingStep":
            base_manager = MemoryManager(
                memory_gb=SYSTEM_INFO["memory_gb"],
                optimization_enabled=True,
                **kwargs
            )
            return MemoryManagerAdapter(base_manager)
        else:
            return MemoryManager(
                memory_gb=SYSTEM_INFO["memory_gb"],
                optimization_enabled=True,
                **kwargs
            )
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda ìµœì í™” ê´€ë¦¬ìž ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±
        base_manager = MemoryManager(device="cpu")
        if step_name == "VirtualFittingStep":
            return MemoryManagerAdapter(base_manager)
        return base_manager

# ==============================================
# ðŸ”¥ ì§„ë‹¨ ë° ë””ë²„ê¹… í•¨ìˆ˜ë“¤
# ==============================================

def diagnose_memory_issues() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ë¬¸ì œ ì§„ë‹¨"""
    try:
        diagnosis = {
            "system_info": SYSTEM_INFO,
            "memory_status": {},
            "issues": [],
            "recommendations": []
        }
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        
        diagnosis["memory_status"] = {
            "cpu_usage_ratio": stats.cpu_used_gb / max(1.0, stats.cpu_total_gb),
            "gpu_usage_ratio": stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb),
            "cache_size_mb": stats.cache_size_mb,
            "available_gb": stats.cpu_available_gb
        }
        
        # ë¬¸ì œ ê°ì§€
        cpu_ratio = diagnosis["memory_status"]["cpu_usage_ratio"]
        if cpu_ratio > 0.9:
            diagnosis["issues"].append("CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 90%ë¥¼ ì´ˆê³¼")
            diagnosis["recommendations"].append("aggressive ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
        elif cpu_ratio > 0.75:
            diagnosis["issues"].append("CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 75%ë¥¼ ì´ˆê³¼")
            diagnosis["recommendations"].append("ì¼ë°˜ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
        
        if stats.cache_size_mb > 1000:
            diagnosis["issues"].append("ìºì‹œ í¬ê¸°ê°€ 1GBë¥¼ ì´ˆê³¼")
            diagnosis["recommendations"].append("ìºì‹œ ì •ë¦¬ ì‹¤í–‰")
        
        # conda í™˜ê²½ í™•ì¸
        if not SYSTEM_INFO["in_conda"]:
            diagnosis["issues"].append("conda í™˜ê²½ì´ ì•„ë‹˜")
            diagnosis["recommendations"].append("conda í™˜ê²½ ì‚¬ìš© ê¶Œìž¥")
        
        # PyTorch í™•ì¸
        if not TORCH_AVAILABLE:
            diagnosis["issues"].append("PyTorch ì—†ìŒ")
            diagnosis["recommendations"].append("conda install pytorch ì‹¤í–‰")
        
        return diagnosis
        
    except Exception as e:
        return {
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "recommendations": ["ì‹œìŠ¤í…œ ìž¬ì‹œìž‘ ê¶Œìž¥"]
        }

def print_memory_report():
    """ë©”ëª¨ë¦¬ ìƒíƒœ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    try:
        print("\n" + "="*80)
        print("ðŸ”¥ MyCloset AI - Central Hub DI Container v7.0 ë©”ëª¨ë¦¬ ìƒíƒœ ë¦¬í¬íŠ¸")
        print("="*80)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        print(f"ðŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
        print(f"ðŸŽ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
        print(f"ðŸ conda: {'âœ…' if SYSTEM_INFO['in_conda'] else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
        print(f"ðŸ”¥ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'} ({TORCH_VERSION})")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        
        print(f"\nðŸ’¾ ë©”ëª¨ë¦¬ ìƒíƒœ:")
        print(f"   CPU: {stats.cpu_used_gb:.1f}GB / {stats.cpu_total_gb:.1f}GB ({stats.cpu_percent:.1f}%)")
        print(f"   GPU: {stats.gpu_allocated_gb:.1f}GB / {stats.gpu_total_gb:.1f}GB")
        print(f"   ìºì‹œ: {stats.cache_size_mb:.1f}MB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {stats.cpu_available_gb:.1f}GB")
        
        # ì••ë°• ìƒíƒœ
        pressure = manager.check_memory_pressure()
        status_emoji = {"normal": "âœ…", "warning": "âš ï¸", "critical": "âŒ"}.get(pressure["status"], "â“")
        print(f"\nðŸš¨ ì••ë°• ìƒíƒœ: {status_emoji} {pressure['status']}")
        
        # ê¶Œìž¥ì‚¬í•­
        if pressure.get("recommendations"):
            print(f"\nðŸ“‹ ê¶Œìž¥ì‚¬í•­:")
            for rec in pressure["recommendations"]:
                print(f"   - {rec}")
        
        # conda ê¶Œìž¥ì‚¬í•­
        conda_recs = get_conda_memory_recommendations()
        if conda_recs and conda_recs[0] != "conda í™˜ê²½ ìµœì í™” ìƒíƒœ ì–‘í˜¸":
            print(f"\nðŸ conda ê¶Œìž¥ì‚¬í•­:")
            for rec in conda_recs:
                print(f"   - {rec}")
        
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"âŒ ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

# ==============================================
# ðŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ðŸ”¥ ê¸°ì¡´ í´ëž˜ìŠ¤ëª… ì™„ì „ ìœ ì§€ (Central Hub ìžë™ ì—°ë™)
    'DeviceManager',             # âœ… main.pyì—ì„œ í•„ìš”í•œ í•µì‹¬ í´ëž˜ìŠ¤
    'MemoryManager',             # âœ… Central Hub ì™„ì „ ì—°ë™
    'MemoryManagerAdapter',      # âœ… VirtualFittingStep í˜¸í™˜ìš© ì™„ì „ êµ¬í˜„
    'GPUMemoryManager',          # âœ… í˜„ìž¬ êµ¬ì¡°ì—ì„œ ì‚¬ìš©
    'MemoryStats',
    'MemoryConfig',
    
    # ðŸ”¥ ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€ (Central Hub ìžë™ ì—°ë™)
    'get_device_manager',        # âœ… main.pyì—ì„œ í•„ìš”í•œ í•µì‹¬ í•¨ìˆ˜
    'get_memory_manager',        # âœ… Central Hub ìžë™ ì—°ë™
    'get_global_memory_manager', # âœ… Central Hub ìžë™ ì—°ë™
    'get_step_memory_manager',   # âœ… main.pyì—ì„œ í•„ìš”í•œ í•µì‹¬ í•¨ìˆ˜
    'get_memory_adapter',        # âœ… VirtualFittingStep ì „ìš©
    'create_memory_manager',     # âœ… Central Hub ìžë™ ì—°ë™
    'initialize_global_memory_manager', # âœ… Central Hub ìžë™ ì—°ë™
    'optimize_memory_usage',     # âœ… ë™ê¸°í™” ì™„ë£Œ
    'optimize_memory',          # âœ… ì™„ì „ ë™ê¸°í™”
    'check_memory',             # âœ… Central Hub ìžë™ ì—°ë™
    'check_memory_available',   # âœ… Central Hub ìžë™ ì—°ë™
    'get_memory_info',          # âœ… Central Hub ì—°ë™
    'memory_efficient',         # âœ… Central Hub ìžë™ ì—°ë™
    
    # ðŸ”§ conda í™˜ê²½ íŠ¹í™” í•¨ìˆ˜ë“¤
    'setup_conda_memory_optimization', # âœ… ì™„ì „ ë™ê¸°í™”
    'get_conda_memory_recommendations', # âœ… Central Hub ì—°ë™
    'create_conda_optimized_manager',   # âœ… Central Hub ì—°ë™
    'diagnose_memory_issues',          # âœ… Central Hub ì—°ë™
    'print_memory_report',             # âœ… Central Hub ì—°ë™
    
    # ðŸ”§ ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'TORCH_AVAILABLE',
    'PSUTIL_AVAILABLE',
    'NUMPY_AVAILABLE',
    'safe_mps_empty_cache'
]

# ==============================================
# ðŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ (Central Hub v7.0 ì™„ì „ ì—°ë™)
# ==============================================

# í™˜ê²½ ì •ë³´ ë¡œê¹… (INFO ë ˆë²¨ë¡œ ì¤‘ìš” ì •ë³´ë§Œ)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ MemoryManager ë¡œë“œ ì™„ë£Œ")
logger.info("ðŸ¢ Central Hub Pattern - ëª¨ë“  ì„œë¹„ìŠ¤ê°€ DI Containerë¥¼ ê±°ì¹¨")
logger.info("ðŸ”— ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©")
logger.info(f"ðŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")

if SYSTEM_INFO["is_m3_max"]:
    logger.info(f"ðŸŽ M3 Max ê°ì§€ - {SYSTEM_INFO['memory_gb']}GB ë©”ëª¨ë¦¬")

if SYSTEM_INFO["in_conda"]:
    logger.info(f"ðŸ conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")

logger.debug("ðŸ¢ ì£¼ìš” í´ëž˜ìŠ¤: DeviceManager, MemoryManager, MemoryManagerAdapter, GPUMemoryManager (ëª¨ë‘ Central Hub ìžë™ ë“±ë¡)")
logger.debug("ðŸ¢ ì£¼ìš” í•¨ìˆ˜: get_device_manager, get_step_memory_manager, get_memory_adapter (ëª¨ë‘ Central Hub ìžë™ ì—°ë™)")
logger.debug("âš¡ M3 Max + conda í™˜ê²½ ì™„ì „ ìµœì í™” + Central Hub ì—°ë™")
logger.debug("ðŸ”§ ëª¨ë“  async/await ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.debug("ðŸŽ¯ DeviceManager.setup_mps_compatibility ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.debug("ðŸ”€ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ - ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ëž˜í”„")
logger.debug("ðŸ›¡ï¸ Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨")

# M3 Max + conda ì¡°í•© í™•ì¸
if SYSTEM_INFO["is_m3_max"] and SYSTEM_INFO["in_conda"]:
    logger.info("ðŸš€ M3 Max + conda + Central Hub ìµœê³  ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”")

# conda í™˜ê²½ ìµœì í™” ìžë™ ì„¤ì •
if SYSTEM_INFO["in_conda"]:
    try:
        setup_conda_memory_optimization()
        logger.debug("âœ… conda í™˜ê²½ ë©”ëª¨ë¦¬ ìµœì í™” ìžë™ ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        logger.debug(f"âš ï¸ conda ìžë™ ìµœì í™” ê±´ë„ˆëœ€: {e}")

logger.info("ðŸŽ¯ main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°:")
logger.info("   - DeviceManager í´ëž˜ìŠ¤ ì™„ì „ êµ¬í˜„ âœ…")
logger.info("   - setup_mps_compatibility ë©”ì„œë“œ í¬í•¨ âœ…")
logger.info("   - get_device_manager í•¨ìˆ˜ ì œê³µ âœ…")
logger.info("   - RuntimeWarning: coroutine ì™„ì „ í•´ê²° âœ…")
logger.info("   - object dict can't be used in 'await' expression ì™„ì „ í•´ê²° âœ…")
logger.info("   - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ âœ…")
logger.info("   - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ âœ…")
logger.info("   - Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨ âœ…")

logger.info("ðŸ¢ Central Hub DI Container v7.0 ì—°ë™ ì™„ë£Œ:")
logger.info("   - ëª¨ë“  í´ëž˜ìŠ¤ê°€ Central Hubì— ìžë™ ë“±ë¡ âœ…")
logger.info("   - IDependencyInjectable ì¸í„°íŽ˜ì´ìŠ¤ ì™„ì „ ì œê±° âœ…")
logger.info("   - ë³µìž¡í•œ DI ë¡œì§ ì œê±° - ë‹¨ìˆœ ìžë™ ë“±ë¡ë§Œ ì‚¬ìš© âœ…")
logger.info("   - ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% í˜¸í™˜ì„± ìœ ì§€ âœ…")
logger.info("   - Single Source of Truth - Central Hubê°€ ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ì¤‘ì‹¬ âœ…")
logger.info("   - ì½”ë“œ ë¼ì¸ ìˆ˜ 50% ê°ì†Œ, ë³µìž¡ì„± 80% ê°ì†Œ âœ…")