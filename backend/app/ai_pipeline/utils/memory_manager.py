# app/ai_pipeline/utils/memory_manager.py
"""
ğŸ MyCloset AI - ì™„ì „ ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ v8.0
================================================================================
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ìµœì í™” ì™„ë£Œ
âœ… get_step_memory_manager í•¨ìˆ˜ ì™„ë²½ êµ¬í˜„
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… ì™„ì „ ìœ ì§€ (GPUMemoryManager)
âœ… Python 3.8+ ì™„ë²½ í˜¸í™˜
âœ… M3 Max 128GB + conda í™˜ê²½ ì™„ì „ ìµœì í™”
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (í•œë°©í–¥ ì˜ì¡´ì„±)
âœ… main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
âœ… MemoryManagerAdapter optimize_memory ì˜¤ë¥˜ í•´ê²°
================================================================================
Author: MyCloset AI Team
Date: 2025-07-20
Version: 8.0 (GitHub Project Optimized)
"""

import os
import gc
import threading
import time
import logging
import asyncio
import weakref
import platform
from typing import Dict, Any, Optional, Callable, List, Union, Tuple
from dataclasses import dataclass, field
from contextlib import contextmanager, asynccontextmanager
from functools import wraps, lru_cache
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# ==============================================
# ğŸ”¥ ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# psutil ì„ íƒì  ì„í¬íŠ¸
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ì„ íƒì  ì„í¬íŠ¸
try:
    import torch
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    TORCH_VERSION = "not_available"

# NumPy ì„ íƒì  ì„í¬íŠ¸
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ìºì‹± (í•œë²ˆë§Œ ì‹¤í–‰)
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (M3 Max ê°ì§€ í¬í•¨)"""
    try:
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, platform.python_version_tuple()[:2])),
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'base'),
            "in_conda": 'CONDA_PREFIX' in os.environ,
            "torch_available": TORCH_AVAILABLE,
            "torch_version": TORCH_VERSION
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                cpu_info = result.stdout.strip()
                is_m3_max = 'M3' in cpu_info and ('Max' in cpu_info or 'Pro' in cpu_info)
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            memory_gb = round(psutil.virtual_memory().total / (1024**3))
            system_info["memory_gb"] = memory_gb
        else:
            # M3 Max ê¸°ë³¸ê°’
            system_info["memory_gb"] = 128 if is_m3_max else 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        device = "cpu"
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "machine": "unknown", 
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8",
            "conda_env": "base",
            "in_conda": False,
            "torch_available": False,
            "torch_version": "not_available"
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0
    m3_optimizations: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
    device: str = "auto"
    memory_limit_gb: float = 16.0
    warning_threshold: float = 0.75
    critical_threshold: float = 0.9
    auto_cleanup: bool = True
    monitoring_interval: float = 30.0
    enable_caching: bool = True
    optimization_enabled: bool = True
    m3_max_features: bool = False

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ê¸°ë³¸ í´ë˜ìŠ¤
# ==============================================

class MemoryManager:
    """
    ğŸ í”„ë¡œì íŠ¸ ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ì
    âœ… GitHub êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… M3 Max 128GB + conda ì™„ì „ í™œìš©
    âœ… ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ì•ˆì „í•œ êµ¬ì¡°
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = device or SYSTEM_INFO["device"]
        
        # 2. ì„¤ì • êµ¬ì„±
        config_dict = config or {}
        config_dict.update(kwargs)
        
        # MemoryConfig ìƒì„±ì„ ìœ„í•œ í•„í„°ë§
        memory_config_fields = {
            'device', 'memory_limit_gb', 'warning_threshold', 'critical_threshold',
            'auto_cleanup', 'monitoring_interval', 'enable_caching', 'optimization_enabled', 'm3_max_features'
        }
        memory_config_args = {k: v for k, v in config_dict.items() if k in memory_config_fields}
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if SYSTEM_INFO["is_m3_max"]:
            memory_config_args.setdefault("memory_limit_gb", SYSTEM_INFO["memory_gb"] * 0.8)
            memory_config_args.setdefault("m3_max_features", True)
        
        self.config = MemoryConfig(**memory_config_args)
        
        # 3. ê¸°ë³¸ ì†ì„±
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"memory.{self.step_name}")
        
        # 4. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = SYSTEM_INFO["memory_gb"]
        self.is_m3_max = SYSTEM_INFO["is_m3_max"]
        self.optimization_enabled = self.config.optimization_enabled
        
        # 5. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì†ì„±
        if PSUTIL_AVAILABLE:
            total_memory = psutil.virtual_memory().total / 1024**3
            self.memory_limit_gb = total_memory * 0.8
        else:
            self.memory_limit_gb = self.config.memory_limit_gb
        
        # 6. ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self._initialize_components()
        
        self.logger.debug(f"ğŸ¯ MemoryManager ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}, ë©”ëª¨ë¦¬: {self.memory_gb}GB")

    def _initialize_components(self):
        """êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ë©”ëª¨ë¦¬ í†µê³„
            self.stats_history = []
            self.max_history_length = 100
            
            # ìºì‹œ ì‹œìŠ¤í…œ
            self.tensor_cache = {}
            self.image_cache = {}
            self.model_cache = {}
            self.cache_priority = {}
            
            # ëª¨ë‹ˆí„°ë§
            self.monitoring_active = False
            self.monitoring_thread = None
            self._lock = threading.RLock()
            
            # M3 Max íŠ¹í™” ì†ì„± ì´ˆê¸°í™”
            if self.is_m3_max:
                self.precision_mode = 'float16'  # M3 Maxì—ì„œ float16 ì‚¬ìš©
                self.memory_pools = {}
                self.optimal_batch_sizes = {}
                
                # M3 Max ìµœì í™” ìˆ˜í–‰
                if self.optimization_enabled:
                    self._optimize_for_m3_max()
            else:
                self.precision_mode = 'float32'
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            
            self.logger.debug(f"ğŸ§  MemoryManager êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì´ˆê¸°í™”
            self.tensor_cache = {}
            self.is_initialized = True

    def _optimize_for_m3_max(self):
        """ğŸ M3 Max Neural Engine + conda ìµœì í™”"""
        try:
            if not self.is_m3_max:
                return False
                
            self.logger.debug("ğŸ M3 Max ìµœì í™” ì‹œì‘")
            optimizations = []
            
            # 1. PyTorch MPS ë°±ì—”ë“œ ìµœì í™”
            if TORCH_AVAILABLE and torch.backends.mps.is_available():
                try:
                    # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        optimizations.append("MPS cache clearing")
                    
                    # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                    os.environ.update({
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                        'PYTORCH_MPS_PREFER_METAL': '1',
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1'
                    })
                    optimizations.append("MPS environment optimization")
                    
                    # ìŠ¤ë ˆë“œ ìµœì í™”
                    torch.set_num_threads(min(16, SYSTEM_INFO["cpu_count"]))
                    optimizations.append(f"Thread optimization ({torch.get_num_threads()} threads)")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì¼ë¶€ ì‹¤íŒ¨: {e}")
            
            # 2. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            self._setup_m3_memory_pools()
            optimizations.append("Memory pool optimization")
            
            # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
            self._optimize_batch_sizes()
            optimizations.append("Batch size optimization")
            
            # 4. conda í™˜ê²½ íŠ¹í™” ìµœì í™”
            if SYSTEM_INFO["in_conda"]:
                self._setup_conda_optimizations()
                optimizations.append("Conda environment optimization")
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì™„ë£Œ")
            for opt in optimizations:
                self.logger.debug(f"   - {opt}")
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            return False

    def _setup_m3_memory_pools(self):
        """M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •"""
        try:
            # 128GB ê¸°ì¤€ ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            pool_size_gb = self.memory_gb * 0.8
            
            self.memory_pools = {
                "model_cache": pool_size_gb * 0.4,      # 40% - ëª¨ë¸ ìºì‹œ
                "inference": pool_size_gb * 0.3,        # 30% - ì¶”ë¡  ì‘ì—…
                "preprocessing": pool_size_gb * 0.2,    # 20% - ì „ì²˜ë¦¬
                "buffer": pool_size_gb * 0.1            # 10% - ë²„í¼
            }
            
            self.logger.debug(f"ğŸ M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •: {pool_size_gb:.1f}GB")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 ë©”ëª¨ë¦¬ í’€ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _optimize_batch_sizes(self):
        """M3 Max ë°°ì¹˜ í¬ê¸° ìµœì í™”"""
        try:
            if self.is_m3_max:
                # M3 Max 128GB ê¸°ì¤€ ìµœì  ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 16,
                    "pose_estimation": 20,
                    "cloth_segmentation": 12,
                    "virtual_fitting": 8,
                    "super_resolution": 4
                }
            else:
                # ì¼ë°˜ ì‹œìŠ¤í…œ ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 4,
                    "pose_estimation": 6,
                    "cloth_segmentation": 3,
                    "virtual_fitting": 2,
                    "super_resolution": 1
                }
            
            self.logger.debug(f"âš™ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")

    def _setup_conda_optimizations(self):
        """conda í™˜ê²½ íŠ¹í™” ìµœì í™”"""
        try:
            if not SYSTEM_INFO["in_conda"]:
                return
            
            # NumPy ìµœì í™”
            if NUMPY_AVAILABLE:
                os.environ['OMP_NUM_THREADS'] = str(min(8, SYSTEM_INFO["cpu_count"]))
                os.environ['MKL_NUM_THREADS'] = str(min(8, SYSTEM_INFO["cpu_count"]))
                
            self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì‹¤íŒ¨: {e}")

    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            start_time = time.time()
            
            # 1. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = 0
            for _ in range(3):
                collected = gc.collect()
                collected_objects += collected
            
            # 2. ìºì‹œ ì •ë¦¬
            cache_cleared = 0
            if hasattr(self, 'tensor_cache'):
                cache_cleared += len(self.tensor_cache)
                if aggressive:
                    self.clear_cache(aggressive=True)
                else:
                    self._evict_low_priority_cache()
            
            # 3. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    if self.device == "mps" and torch.backends.mps.is_available():
                        # M3 Max MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        
                        # MPS ë™ê¸°í™”
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        
                        # ê³µê²©ì  ì •ë¦¬ ì‹œ M3 Max íŠ¹í™” ì •ë¦¬
                        if aggressive and self.is_m3_max:
                            self._aggressive_m3_cleanup()
                        
                    elif self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        if aggressive:
                            torch.cuda.synchronize()
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            cleanup_time = time.time() - start_time
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "success": True,
                "cleanup_time": cleanup_time,
                "collected_objects": collected_objects,
                "cache_cleared": cache_cleared,
                "aggressive": aggressive,
                "device": self.device,
                "m3_optimized": self.is_m3_max
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device
            }

    async def optimize_memory(self) -> Dict[str, Any]:
        """ğŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” (VirtualFittingStepì—ì„œ í•„ìš”í•œ ë©”ì„œë“œ)"""
        try:
            start_time = time.time()
            optimization_results = []
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰
            cleanup_result = self.cleanup_memory(aggressive=False)
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì •ë¦¬: {cleanup_result.get('success', False)}")
            
            # 2. M3 Max íŠ¹í™” ìµœì í™”
            if self.is_m3_max:
                m3_result = await self._optimize_m3_max_memory()
                optimization_results.append(f"M3 Max ìµœì í™”: {m3_result}")
            
            # 3. ìºì‹œ ìµœì í™”
            cache_stats = self._optimize_cache_system()
            optimization_results.append(f"ìºì‹œ ìµœì í™”: {cache_stats}")
            
            # 4. ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸
            pressure_info = self.check_memory_pressure()
            optimization_results.append(f"ë©”ëª¨ë¦¬ ì••ë°•: {pressure_info.get('status', 'unknown')}")
            
            optimization_time = time.time() - start_time
            
            return {
                "success": True,
                "message": "ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "optimization_time": optimization_time,
                "optimization_results": optimization_results,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device,
                "timestamp": time.time()
            }

    async def _optimize_m3_max_memory(self):
        """M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # ì¶”ê°€ M3 Max ìµœì í™” ë¡œì§
            if hasattr(self, '_optimize_for_m3_max'):
                await asyncio.get_event_loop().run_in_executor(
                    None, self._optimize_for_m3_max
                )
            
            # M3 Max MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            
            return True
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return False

    def _optimize_cache_system(self) -> str:
        """ìºì‹œ ì‹œìŠ¤í…œ ìµœì í™”"""
        try:
            # ìºì‹œ í¬ê¸° ì²´í¬
            total_cache_size = 0
            cache_counts = {}
            
            for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                if hasattr(self, cache_name):
                    cache = getattr(self, cache_name)
                    size = len(cache)
                    total_cache_size += size
                    cache_counts[cache_name] = size
            
            # ìºì‹œê°€ ë„ˆë¬´ í´ ê²½ìš° ì •ë¦¬
            if total_cache_size > 100:  # ìºì‹œ í•­ëª©ì´ 100ê°œ ì´ìƒ
                self._evict_low_priority_cache()
                return f"ìºì‹œ ì •ë¦¬ë¨ (ì´ì „: {total_cache_size}ê°œ)"
            
            return f"ì •ìƒ ({total_cache_size}ê°œ í•­ëª©)"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ìµœì í™” ì‹¤íŒ¨: {e}")
            return "ìµœì í™” ì‹¤íŒ¨"
    
    def _aggressive_m3_cleanup(self):
        """ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ëª¨ë“  ìºì‹œ í´ë¦¬ì–´
            for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                if hasattr(self, cache_name):
                    getattr(self, cache_name).clear()
            
            # ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for _ in range(5):
                gc.collect()
            
            # PyTorch MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            
            self.logger.debug("ğŸ ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            # CPU ë©”ëª¨ë¦¬
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                cpu_percent = memory.percent
                cpu_total_gb = memory.total / 1024**3
                cpu_used_gb = memory.used / 1024**3
                cpu_available_gb = memory.available / 1024**3
                swap_used_gb = psutil.swap_memory().used / 1024**3
                
                # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
                try:
                    process = psutil.Process()
                    process_memory_mb = process.memory_info().rss / 1024**2
                except:
                    process_memory_mb = 0.0
            else:
                cpu_percent = 50.0
                cpu_total_gb = self.memory_gb
                cpu_used_gb = self.memory_gb * 0.5
                cpu_available_gb = self.memory_gb * 0.5
                swap_used_gb = 0.0
                process_memory_mb = 0.0
            
            # GPU ë©”ëª¨ë¦¬
            gpu_allocated_gb = 0.0
            gpu_reserved_gb = 0.0
            gpu_total_gb = 0.0
            
            if TORCH_AVAILABLE:
                try:
                    if self.device == "cuda" and torch.cuda.is_available():
                        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        gpu_total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPS ë©”ëª¨ë¦¬ ì •ë³´ (ì¶”ì •)
                        gpu_allocated_gb = 2.0
                        gpu_total_gb = self.memory_gb  # M3 Max í†µí•© ë©”ëª¨ë¦¬
                except Exception:
                    pass
            
            # ìºì‹œ í¬ê¸°
            cache_size_mb = 0.0
            try:
                for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                    if hasattr(self, cache_name):
                        cache = getattr(self, cache_name)
                        cache_size_mb += len(str(cache)) / 1024**2
            except:
                pass
            
            # M3 ìµœì í™” ì •ë³´
            m3_optimizations = {}
            if self.is_m3_max:
                m3_optimizations = {
                    "memory_pools": getattr(self, 'memory_pools', {}),
                    "batch_sizes": getattr(self, 'optimal_batch_sizes', {}),
                    "precision_mode": self.precision_mode,
                    "conda_optimized": SYSTEM_INFO["in_conda"]
                }
            
            return MemoryStats(
                cpu_percent=cpu_percent,
                cpu_available_gb=cpu_available_gb,
                cpu_used_gb=cpu_used_gb,
                cpu_total_gb=cpu_total_gb,
                gpu_allocated_gb=gpu_allocated_gb,
                gpu_reserved_gb=gpu_reserved_gb,
                gpu_total_gb=gpu_total_gb,
                swap_used_gb=swap_used_gb,
                cache_size_mb=cache_size_mb,
                process_memory_mb=process_memory_mb,
                m3_optimizations=m3_optimizations
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return MemoryStats(
                cpu_percent=0.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )

    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        try:
            stats = self.get_memory_stats()
            
            cpu_usage_ratio = stats.cpu_used_gb / max(1.0, stats.cpu_total_gb)
            gpu_usage_ratio = stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb)
            
            status = "normal"
            if cpu_usage_ratio > 0.9 or gpu_usage_ratio > 0.9:
                status = "critical"
            elif cpu_usage_ratio > 0.75 or gpu_usage_ratio > 0.75:
                status = "warning"
            
            return {
                "status": status,
                "cpu_usage_ratio": cpu_usage_ratio,
                "gpu_usage_ratio": gpu_usage_ratio,
                "cache_size_mb": stats.cache_size_mb,
                "recommendations": self._get_cleanup_recommendations(stats)
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬ ì‹¤íŒ¨: {e}")
            return {"status": "unknown", "error": str(e)}

    def _get_cleanup_recommendations(self, stats: MemoryStats) -> List[str]:
        """ì •ë¦¬ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        try:
            cpu_ratio = stats.cpu_used_gb / max(1.0, stats.cpu_total_gb)
            
            if cpu_ratio > 0.8:
                recommendations.append("CPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
            
            if stats.gpu_allocated_gb > 10.0:
                recommendations.append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
            
            if stats.cache_size_mb > 1000:
                recommendations.append("ìºì‹œ ì •ë¦¬ ê¶Œì¥")
            
            if self.is_m3_max and cpu_ratio > 0.7:
                recommendations.append("M3 Max ìµœì í™” ì¬ì‹¤í–‰ ê¶Œì¥")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return recommendations

    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self._lock:
                if aggressive:
                    # ì „ì²´ ìºì‹œ ì‚­ì œ
                    for cache_name in ['tensor_cache', 'image_cache', 'model_cache', 'cache_priority']:
                        if hasattr(self, cache_name):
                            getattr(self, cache_name).clear()
                    self.logger.debug("ğŸ§¹ ì „ì²´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                else:
                    # ì„ íƒì  ìºì‹œ ì •ë¦¬
                    self._evict_low_priority_cache()
                    self.logger.debug("ğŸ§¹ ì„ íƒì  ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _evict_low_priority_cache(self):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ ì œê±°"""
        try:
            if not hasattr(self, 'cache_priority') or not self.cache_priority:
                return
            
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # í•˜ìœ„ 20% ì œê±°
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                for cache_name in ['tensor_cache', 'image_cache', 'model_cache']:
                    if hasattr(self, cache_name):
                        getattr(self, cache_name).pop(key, None)
                self.cache_priority.pop(key, None)
            
            self.logger.debug(f"ğŸ—‘ï¸ ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ {num_to_remove}ê°œ ì œê±°")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì œê±° ì‹¤íŒ¨: {e}")

    # ============================================
    # ğŸ”¥ ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤
    # ============================================

    async def initialize(self) -> bool:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            # M3 Max ìµœì í™” ì„¤ì •
            if self.is_m3_max and self.optimization_enabled:
                await asyncio.get_event_loop().run_in_executor(
                    None, self._optimize_for_m3_max
                )
            
            self.logger.debug(f"âœ… MemoryManager ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def cleanup(self):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.cleanup_memory, True)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_usage(self) -> Dict[str, Any]:
        """ë™ê¸° ì‚¬ìš©ëŸ‰ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)"""
        try:
            stats = self.get_memory_stats()
            return {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'monitoring_active'):
                self.monitoring_active = False
            if hasattr(self, 'tensor_cache'):
                self.clear_cache(aggressive=True)
        except:
            pass

# ==============================================
# ğŸ”¥ MemoryManagerAdapter í´ë˜ìŠ¤ (VirtualFittingStepìš©)
# ==============================================

class MemoryManagerAdapter:
    """
    ğŸ”¥ MemoryManagerAdapter - VirtualFittingStep í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„°
    âœ… optimize_memory() ë©”ì„œë“œ êµ¬í˜„
    âœ… ê¸°ì¡´ MemoryManager ê¸°ëŠ¥ ìœ„ì„
    âœ… VirtualFittingStep ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    """
    
    def __init__(self, base_manager: MemoryManager):
        """ì–´ëŒ‘í„° ì´ˆê¸°í™”"""
        self._base_manager = base_manager
        self.logger = logging.getLogger("MemoryManagerAdapter")
        
    async def optimize_memory(self) -> Dict[str, Any]:
        """ğŸ”¥ VirtualFittingStepì—ì„œ í•„ìš”í•œ optimize_memory ë©”ì„œë“œ"""
        try:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ìì˜ optimize_memory í˜¸ì¶œ
            result = await self._base_manager.optimize_memory()
            
            self.logger.debug("âœ… MemoryManagerAdapter ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManagerAdapter ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "adapter": True,
                "timestamp": time.time()
            }
    
    async def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ìœ„ì„)"""
        await self._base_manager.cleanup()
    
    def get_memory_stats(self) -> MemoryStats:
        """ë©”ëª¨ë¦¬ í†µê³„ (ìœ„ì„)"""
        return self._base_manager.get_memory_stats()
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ í†µê³„ (ë¹„ë™ê¸° ë˜í¼)"""
        return self._base_manager.get_usage()
    
    def __getattr__(self, name):
        """ë‹¤ë¥¸ ëª¨ë“  ì†ì„±/ë©”ì„œë“œëŠ” ê¸°ë³¸ ê´€ë¦¬ìë¡œ ìœ„ì„"""
        return getattr(self._base_manager, name)

# ==============================================
# ğŸ”¥ GPUMemoryManager í´ë˜ìŠ¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
# ==============================================

class GPUMemoryManager(MemoryManager):
    """
    ğŸ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ê¸°ì¡´ í´ë˜ìŠ¤ëª… ìœ ì§€)
    âœ… í˜„ì¬ êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… ê¸°ì¡´ ì½”ë“œì˜ GPUMemoryManager ì‚¬ìš© ìœ ì§€
    âœ… main.py import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    """
    
    def __init__(self, device=None, memory_limit_gb=None, **kwargs):
        """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” (ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if device is None:
            device = SYSTEM_INFO["device"]
        if memory_limit_gb is None:
            memory_limit_gb = SYSTEM_INFO["memory_gb"] * 0.8 if SYSTEM_INFO["is_m3_max"] else 16.0
        
        super().__init__(device=device, memory_limit_gb=memory_limit_gb, **kwargs)
        self.logger = logging.getLogger("GPUMemoryManager")
        
        # ê¸°ì¡´ ì†ì„± í˜¸í™˜ì„± ìœ ì§€
        self.memory_limit_gb = memory_limit_gb
        
        self.logger.debug(f"ğŸ® GPUMemoryManager ì´ˆê¸°í™” - {device} ({memory_limit_gb:.1f}GB)")

    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        try:
            # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì •ë¦¬ í˜¸ì¶œ
            result = self.cleanup_memory(aggressive=False)
            
            # PyTorch GPU ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.debug("ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        try:
            stats = self.get_memory_stats()
            
            # ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                used_gb = memory.used / (1024**3)
                if used_gb > self.memory_limit_gb * 0.9:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {used_gb:.1f}GB")
                    self.clear_cache()
            
            return {
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "memory_limit_gb": self.memory_limit_gb
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ ì™„ì „ ìœ ì§€)
# ==============================================

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_memory_manager = None
_global_gpu_memory_manager = None
_manager_lock = threading.Lock()

def get_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_memory_manager
    
    with _manager_lock:
        if _global_memory_manager is None:
            _global_memory_manager = MemoryManager(**kwargs)
        return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager(**kwargs)

def get_step_memory_manager(step_name: str, **kwargs) -> Union[MemoryManager, MemoryManagerAdapter]:
    """
    ğŸ”¥ Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜ (main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜)
    âœ… ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€
    âœ… __init__.pyì—ì„œ exportë˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    âœ… import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… VirtualFittingStepìš© MemoryManagerAdapter ì§€ì›
    """
    try:
        # Stepë³„ íŠ¹í™” ì„¤ì • (GitHub 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê¸°ì¤€)
        step_configs = {
            "HumanParsingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "PoseEstimationStep": {"memory_limit_gb": 6.0, "optimization_enabled": True},
            "ClothSegmentationStep": {"memory_limit_gb": 4.0, "optimization_enabled": True},
            "GeometricMatchingStep": {"memory_limit_gb": 6.0, "optimization_enabled": True},
            "ClothWarpingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "VirtualFittingStep": {"memory_limit_gb": 16.0, "optimization_enabled": True},
            "PostProcessingStep": {"memory_limit_gb": 8.0, "optimization_enabled": True},
            "QualityAssessmentStep": {"memory_limit_gb": 4.0, "optimization_enabled": True}
        }
        
        # M3 Maxì—ì„œëŠ” ë” í° ë©”ëª¨ë¦¬ í• ë‹¹
        if SYSTEM_INFO["is_m3_max"]:
            for config in step_configs.values():
                config["memory_limit_gb"] *= 2  # M3 Maxì—ì„œ 2ë°° ë©”ëª¨ë¦¬ í• ë‹¹
        
        # Stepë³„ ì„¤ì • ì ìš©
        step_config = step_configs.get(step_name, {"memory_limit_gb": 8.0})
        final_kwargs = kwargs.copy()
        final_kwargs.update(step_config)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±
        base_manager = MemoryManager(**final_kwargs)
        base_manager.step_name = step_name
        base_manager.logger = logging.getLogger(f"memory.{step_name}")
        
        # VirtualFittingStepì¸ ê²½ìš° ì–´ëŒ‘í„° ë°˜í™˜
        if step_name == "VirtualFittingStep":
            adapter = MemoryManagerAdapter(base_manager)
            logger.debug(f"ğŸ“ {step_name} MemoryManagerAdapter ìƒì„± ì™„ë£Œ")
            return adapter
        else:
            logger.debug(f"ğŸ“ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì™„ë£Œ")
            return base_manager
        
    except Exception as e:
        logger.warning(f"âš ï¸ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜
        base_manager = MemoryManager(**kwargs)
        if step_name == "VirtualFittingStep":
            return MemoryManagerAdapter(base_manager)
        return base_manager

def create_memory_manager(device: str = "auto", **kwargs) -> MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì íŒ©í† ë¦¬ í•¨ìˆ˜"""
    try:
        if device == "auto":
            device = SYSTEM_INFO["device"]
        
        logger.debug(f"ğŸ“¦ MemoryManager ìƒì„± - ë””ë°”ì´ìŠ¤: {device}")
        manager = MemoryManager(device=device, **kwargs)
        return manager
    except Exception as e:
        logger.warning(f"âš ï¸ MemoryManager ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        return MemoryManager(device="cpu")

def create_optimized_memory_manager(
    device: str = "auto",
    memory_gb: float = None,
    is_m3_max: bool = None,
    optimization_enabled: bool = True
) -> MemoryManager:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if device == "auto":
        device = SYSTEM_INFO["device"]
    if memory_gb is None:
        memory_gb = SYSTEM_INFO["memory_gb"]
    if is_m3_max is None:
        is_m3_max = SYSTEM_INFO["is_m3_max"]
    
    return MemoryManager(
        device=device,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled,
        auto_cleanup=True,
        enable_caching=True
    )

def initialize_global_memory_manager(device: str = None, **kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
    global _global_memory_manager
    
    try:
        with _manager_lock:
            if _global_memory_manager is None:
                if device is None:
                    device = SYSTEM_INFO["device"]
                
                _global_memory_manager = MemoryManager(device=device, **kwargs)
                logger.info(f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {device}")
            return _global_memory_manager
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        _global_memory_manager = MemoryManager(device="cpu")
        return _global_memory_manager

def optimize_memory_usage(device: str = None, aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” - ë™ê¸° í•¨ìˆ˜"""
    try:
        manager = get_memory_manager(device=device or "auto")
        
        # ìµœì í™” ì „ ìƒíƒœ
        before_stats = manager.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        result = manager.cleanup_memory(aggressive=aggressive)
        
        # ìµœì í™” í›„ ìƒíƒœ
        after_stats = manager.get_memory_stats()
        
        # ê²°ê³¼ ê³„ì‚°
        freed_cpu = max(0, before_stats.cpu_used_gb - after_stats.cpu_used_gb)
        freed_gpu = max(0, before_stats.gpu_allocated_gb - after_stats.gpu_allocated_gb)
        freed_cache = max(0, before_stats.cache_size_mb - after_stats.cache_size_mb)
        
        result.update({
            "freed_memory": {
                "cpu_gb": freed_cpu,
                "gpu_gb": freed_gpu,
                "cache_mb": freed_cache
            },
            "before": {
                "cpu_used_gb": before_stats.cpu_used_gb,
                "gpu_allocated_gb": before_stats.gpu_allocated_gb,
                "cache_size_mb": before_stats.cache_size_mb
            },
            "after": {
                "cpu_used_gb": after_stats.cpu_used_gb,
                "gpu_allocated_gb": after_stats.gpu_allocated_gb,
                "cache_size_mb": after_stats.cache_size_mb
            }
        })
        
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - CPU: {freed_cpu:.2f}GB, GPU: {freed_gpu:.2f}GB")
        return result
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

async def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°)"""
    try:
        manager = get_memory_manager()
        await manager.cleanup()
    except Exception as e:
        logger.warning(f"âš ï¸ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

def check_memory():
    """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    try:
        manager = get_memory_manager()
        return manager.check_memory_pressure()
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {"status": "unknown", "error": str(e)}

def check_memory_available(min_gb: float = 1.0) -> bool:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return stats.cpu_available_gb >= min_gb
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ true ë°˜í™˜

def get_memory_info() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return {
            "device": manager.device,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "cpu_used_gb": stats.cpu_used_gb,
            "gpu_total_gb": stats.gpu_total_gb,
            "gpu_allocated_gb": stats.gpu_allocated_gb,
            "is_m3_max": manager.is_m3_max,
            "memory_gb": manager.memory_gb,
            "conda_env": SYSTEM_INFO["in_conda"]
        }
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

# ==============================================
# ğŸ”¥ ë°ì½”ë ˆì´í„°
# ==============================================

def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            if clear_before:
                await manager.cleanup()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                if clear_after:
                    await manager.cleanup()
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            if clear_before:
                manager.cleanup_memory()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                if clear_after:
                    manager.cleanup_memory()
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ êµ¬ì¡° ì™„ì „ ìœ ì§€)
# ==============================================

__all__ = [
    # ğŸ”¥ ê¸°ì¡´ í´ë˜ìŠ¤ëª… ì™„ì „ ìœ ì§€
    'MemoryManager',
    'MemoryManagerAdapter',      # âœ… VirtualFittingStep í˜¸í™˜ìš© ì¶”ê°€
    'GPUMemoryManager',          # âœ… í˜„ì¬ êµ¬ì¡°ì—ì„œ ì‚¬ìš©
    'MemoryStats',
    'MemoryConfig',
    
    # ğŸ”¥ ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€
    'get_memory_manager',
    'get_global_memory_manager',
    'get_step_memory_manager',   # âœ… main.pyì—ì„œ í•„ìš”í•œ í•µì‹¬ í•¨ìˆ˜
    'create_memory_manager',
    'create_optimized_memory_manager',
    'initialize_global_memory_manager',
    'optimize_memory_usage',
    'optimize_memory',
    'check_memory',
    'check_memory_available',
    'get_memory_info',
    'memory_efficient',
    
    # ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'TORCH_AVAILABLE',
    'PSUTIL_AVAILABLE',
    'NUMPY_AVAILABLE'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ (GitHub í”„ë¡œì íŠ¸ ìµœì í™”)
# ==============================================

# í™˜ê²½ ì •ë³´ ë¡œê¹… (INFO ë ˆë²¨ë¡œ ì¤‘ìš” ì •ë³´ë§Œ)
logger.info("âœ… MemoryManager v8.0 ë¡œë“œ ì™„ë£Œ (GitHub Project Optimized)")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
if SYSTEM_INFO["is_m3_max"]:
    logger.info(f"ğŸ M3 Max ê°ì§€ - {SYSTEM_INFO['memory_gb']}GB ë©”ëª¨ë¦¬")
if SYSTEM_INFO["in_conda"]:
    logger.info(f"ğŸ conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")
logger.debug("ğŸ”— ì£¼ìš” í•¨ìˆ˜: get_step_memory_manager, GPUMemoryManager")
logger.debug("âš¡ M3 Max + conda í™˜ê²½ ì™„ì „ ìµœì í™”")
logger.debug("ğŸ”§ MemoryManagerAdapter VirtualFittingStep í˜¸í™˜ ì™„ë£Œ")

# M3 Max + conda ì¡°í•© í™•ì¸
if SYSTEM_INFO["is_m3_max"] and SYSTEM_INFO["in_conda"]:
    logger.info("ğŸš€ M3 Max + conda ìµœê³  ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”")