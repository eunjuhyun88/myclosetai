# app/ai_pipeline/utils/memory_manager.py
"""
M3 Max ìµœì í™” ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € (PyTorch 2.5.1 í˜¸í™˜ì„± ë³´ì¥)
- ëˆ„ë½ëœ export í•¨ìˆ˜ë“¤ ì¶”ê°€
- main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í•¨ìˆ˜ êµ¬í˜„
"""

import logging
import torch
import psutil
import gc
import platform
from typing import Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_total_gb: float
    cpu_available_gb: float
    cpu_used_percent: float
    gpu_allocated_gb: float = 0.0
    gpu_cached_gb: float = 0.0

class MemoryManager:
    """M3 Max ìµœì í™” ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €"""
    
    def __init__(self, device: str = "mps", memory_limit_gb: Optional[float] = None):
        self.device = device
        self.is_mps = device == "mps"
        self.is_m3_max = self._detect_m3_max()
        self.memory_limit_gb = memory_limit_gb or self._get_optimal_memory_limit()
        
        # PyTorch ë²„ì „ë³„ í˜¸í™˜ì„± ì²´í¬
        self.pytorch_version = torch.__version__
        self.mps_empty_cache_available = self._check_mps_empty_cache()
        
        logger.info(f"ğŸ§  MemoryManager ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}, ë©”ëª¨ë¦¬ ì œí•œ: {self.memory_limit_gb}GB")
        
        if self.is_m3_max:
            logger.info("ğŸ M3 Max ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            
        if not self.mps_empty_cache_available and self.is_mps:
            logger.warning(f"âš ï¸ PyTorch {self.pytorch_version}: MPS empty_cache ë¯¸ì§€ì› - ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‚¬ìš©")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() != "Darwin" or platform.machine() != "arm64":
                return False
            
            # ë©”ëª¨ë¦¬ í¬ê¸°ë¡œ M3 Max ì¶”ì • (36GB+ = M3 Max)
            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            return total_memory_gb >= 32.0
        except:
            return False
    
    def _check_mps_empty_cache(self) -> bool:
        """MPS empty_cache ë©”ì„œë“œ ì‚¬ìš© ê°€ëŠ¥ì„± ì²´í¬"""
        if not self.is_mps:
            return False
        
        try:
            # PyTorch 2.5.1ì—ì„œëŠ” empty_cacheê°€ ì œê±°ë¨
            return hasattr(torch.backends.mps, 'empty_cache')
        except:
            return False
    
    def _get_optimal_memory_limit(self) -> float:
        """ìµœì  ë©”ëª¨ë¦¬ ì œí•œ ê³„ì‚°"""
        total_memory = psutil.virtual_memory().total / (1024**3)
        
        if self.is_m3_max:
            # M3 Max: í†µí•© ë©”ëª¨ë¦¬ 80% í™œìš©
            return total_memory * 0.8
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ: ë³´ìˆ˜ì  ì ‘ê·¼
            return min(total_memory * 0.6, 16.0)
    
    def clear_cache(self) -> None:
        """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (ë²„ì „ í˜¸í™˜ì„± ë³´ì¥)"""
        try:
            # CPU ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            if self.is_mps:
                if self.mps_empty_cache_available:
                    # PyTorch 2.4 ì´í•˜
                    torch.backends.mps.empty_cache()
                    logger.debug("âœ… MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                else:
                    # PyTorch 2.5+ ëŒ€ì²´ ë°©ë²•
                    self._alternative_mps_cleanup()
                    logger.debug("âœ… MPS ëŒ€ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("âœ… CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _alternative_mps_cleanup(self) -> None:
        """MPS ëŒ€ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ë°©ë²•"""
        try:
            # 1. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            for _ in range(3):
                gc.collect()
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            import os
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            
            # 3. ì„ì‹œ í…ì„œ ìƒì„±/ì‚­ì œë¡œ ë©”ëª¨ë¦¬ í• ë‹¹ íŒ¨í„´ ë¦¬ì…‹
            if torch.backends.mps.is_available():
                temp_tensor = torch.zeros(1, device='mps')
                del temp_tensor
                
        except Exception as e:
            logger.debug(f"ëŒ€ì²´ MPS ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_total_gb = cpu_memory.total / (1024**3)
        cpu_available_gb = cpu_memory.available / (1024**3)
        cpu_used_percent = cpu_memory.percent
        
        # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_allocated_gb = 0.0
        gpu_cached_gb = 0.0
        
        if torch.cuda.is_available():
            try:
                gpu_allocated_gb = torch.cuda.memory_allocated() / (1024**3)
                gpu_cached_gb = torch.cuda.memory_reserved() / (1024**3)
            except:
                pass
        
        return MemoryStats(
            cpu_total_gb=cpu_total_gb,
            cpu_available_gb=cpu_available_gb,
            cpu_used_percent=cpu_used_percent,
            gpu_allocated_gb=gpu_allocated_gb,
            gpu_cached_gb=gpu_cached_gb
        )
    
    def optimize_memory(self) -> None:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            stats_before = self.get_memory_stats()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.clear_cache()
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max:
                self._optimize_for_m3_max()
            
            stats_after = self.get_memory_stats()
            
            # ìµœì í™” ê²°ê³¼ ë¡œê¹…
            freed_memory = stats_before.cpu_total_gb - stats_before.cpu_available_gb - \
                          (stats_after.cpu_total_gb - stats_after.cpu_available_gb)
            
            logger.info(
                f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - "
                f"CPU: {stats_after.cpu_available_gb:.2f}GB, "
                f"GPU: {stats_after.gpu_allocated_gb:.2f}GB, "
                f"í•´ì œë¨: {freed_memory:.2f}GB"
            )
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_for_m3_max(self) -> None:
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            import os
            
            # M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            os.environ.update({
                "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.0",
                "PYTORCH_MPS_ALLOCATOR_POLICY": "garbage_collection",
                "OMP_NUM_THREADS": "8",  # M3 Max ì„±ëŠ¥ ì½”ì–´ ìˆ˜
                "MKL_NUM_THREADS": "8"
            })
            
            logger.debug("ğŸ M3 Max í™˜ê²½ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def check_memory_available(self, required_gb: float) -> bool:
        """í•„ìš” ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ì„± ì²´í¬"""
        stats = self.get_memory_stats()
        available = stats.cpu_available_gb
        
        if self.is_m3_max:
            # M3 MaxëŠ” í†µí•© ë©”ëª¨ë¦¬ë¡œ ë” ìœ ì—°í•¨
            available *= 1.2
        
        return available >= required_gb
    
    def start_monitoring(self) -> None:
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        # ì‹¤ì œ ëª¨ë‹ˆí„°ë§ ë¡œì§ì€ í•„ìš”ì‹œ êµ¬í˜„
    
    def stop_monitoring(self) -> None:
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")

# ==========================================
# MAIN.PYì—ì„œ ìš”êµ¬í•˜ëŠ” EXPORT í•¨ìˆ˜ë“¤ ì¶”ê°€
# ==========================================

# ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_global_memory_manager: Optional[MemoryManager] = None

def get_memory_manager() -> Optional[MemoryManager]:
    """ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager()
    return _global_memory_manager

def get_global_memory_manager() -> Optional[MemoryManager]:
    """ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager()

def create_memory_manager(device: str = "mps", memory_limit_gb: Optional[float] = None) -> MemoryManager:
    """ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„±"""
    return MemoryManager(device=device, memory_limit_gb=memory_limit_gb)

def get_default_memory_manager() -> MemoryManager:
    """ê¸°ë³¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë°˜í™˜"""
    manager = get_memory_manager()
    if manager is None:
        manager = create_memory_manager()
    return manager

def optimize_memory_usage(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” (main.py í˜¸í™˜)"""
    try:
        manager = get_memory_manager()
        if manager is None:
            manager = create_memory_manager(device=device or "mps")
        
        stats_before = manager.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        manager.clear_cache()
        
        # ì ê·¹ì  ì •ë¦¬
        if aggressive:
            manager.optimize_memory()
            
            # ì¶”ê°€ ì •ë¦¬
            import gc
            for _ in range(3):
                gc.collect()
        
        stats_after = manager.get_memory_stats()
        
        return {
            "success": True,
            "device": device or manager.device,
            "aggressive": aggressive,
            "memory_before": {
                "cpu_available_gb": stats_before.cpu_available_gb,
                "cpu_used_percent": stats_before.cpu_used_percent,
                "gpu_allocated_gb": stats_before.gpu_allocated_gb
            },
            "memory_after": {
                "cpu_available_gb": stats_after.cpu_available_gb,
                "cpu_used_percent": stats_after.cpu_used_percent,
                "gpu_allocated_gb": stats_after.gpu_allocated_gb
            },
            "freed_memory_gb": stats_before.cpu_used_percent - stats_after.cpu_used_percent,
            "optimization_time": 0.1  # ë”ë¯¸ ì‹œê°„
        }
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

def check_memory() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬ (main.py í˜¸í™˜)"""
    try:
        manager = get_memory_manager()
        if manager is None:
            return {
                "status": "unknown",
                "error": "Memory manager not available"
            }
        
        stats = manager.get_memory_stats()
        
        # ìƒíƒœ íŒë‹¨
        if stats.cpu_used_percent > 90:
            status = "critical"
        elif stats.cpu_used_percent > 75:
            status = "warning"
        elif stats.cpu_used_percent > 50:
            status = "normal"
        else:
            status = "good"
        
        return {
            "status": status,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "cpu_used_percent": stats.cpu_used_percent,
            "gpu_allocated_gb": stats.gpu_allocated_gb,
            "gpu_cached_gb": stats.gpu_cached_gb,
            "is_m3_max": manager.is_m3_max,
            "device": manager.device,
            "memory_limit_gb": manager.memory_limit_gb
        }
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "status": "error",
            "error": str(e)
        }