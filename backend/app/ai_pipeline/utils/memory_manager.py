# app/ai_pipeline/utils/memory_manager.py
"""
MyCloset AI - ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)
âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš© + ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ëª¨ë‘ ì¶”ê°€
ğŸ”¥ í•µì‹¬: main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í•¨ìˆ˜ í¬í•¨
ğŸ M3 Max Neural Engine ìµœì í™” ë©”ì„œë“œ ì™„ì „ ì¶”ê°€
"""
import os
import gc
import threading
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Callable, List, Union
from dataclasses import dataclass
from contextlib import contextmanager, asynccontextmanager
import weakref
from functools import wraps
import numpy as np

# psutil ì„ íƒì  ì„í¬íŠ¸
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ì„ íƒì  ì„í¬íŠ¸
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0

class MemoryManager:
    """
    ì§€ëŠ¥í˜• GPU/CPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì - Apple Silicon M3 Max ìµœì í™”
    âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
    ğŸ M3 Max Neural Engine ìµœì í™” ë©”ì„œë“œ ì™„ì „ ì¶”ê°€
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… ìµœì  ìƒì„±ì - ë©”ëª¨ë¦¬ ê´€ë¦¬ íŠ¹í™”

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
        """
        # 1. ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)

        # 2. ğŸ“‹ ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"utils.{self.step_name}")

        # 3. ğŸ”§ í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì¼ê´€ì„±)
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')

        # 4. âš™ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ íŠ¹í™” íŒŒë¼ë¯¸í„°
        memory_limit_gb = kwargs.get('memory_limit_gb', None)
        if memory_limit_gb is None:
            if PSUTIL_AVAILABLE:
                total_memory = psutil.virtual_memory().total / 1024**3
                self.memory_limit_gb = total_memory * 0.8  # 80% ì‚¬ìš©
            else:
                self.memory_limit_gb = 16.0  # ê¸°ë³¸ê°’
        else:
            self.memory_limit_gb = memory_limit_gb
            
        self.warning_threshold = kwargs.get('warning_threshold', 0.75)
        self.critical_threshold = kwargs.get('critical_threshold', 0.9)
        self.auto_cleanup = kwargs.get('auto_cleanup', True)
        self.monitoring_interval = kwargs.get('monitoring_interval', 30.0)
        self.enable_caching = kwargs.get('enable_caching', True)

        # 5. âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ configì— ë³‘í•©
        self._merge_step_specific_config(kwargs)

        # 6. âœ… ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False

        # 7. ğŸ¯ ê¸°ì¡´ í´ë˜ìŠ¤ë³„ ê³ ìœ  ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
        self._initialize_step_specific()

        # 8. ğŸ M3 Max íŠ¹í™” ì†ì„± ì´ˆê¸°í™”
        self.precision_mode = 'float32'
        self.memory_pools = {}
        self.optimal_batch_sizes = {}

        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            return 'cpu'

        try:
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                # M3 Max ê°ì§€ ë¡œì§
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” ì„¤ì • ë³‘í•©"""
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì œì™¸í•˜ê³  ëª¨ë“  kwargsë¥¼ configì— ë³‘í•©
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'memory_limit_gb', 'warning_threshold', 'critical_threshold',
            'auto_cleanup', 'monitoring_interval', 'enable_caching'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value

    def _initialize_step_specific(self):
        """ğŸ¯ ê¸°ì¡´ ì´ˆê¸°í™” ë¡œì§ ì™„ì „ ìœ ì§€"""
        # ë©”ëª¨ë¦¬ í†µê³„
        self.stats_history: List[MemoryStats] = []
        self.max_history_length = 100
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.tensor_cache: Dict[str, Any] = {}
        self.image_cache: Dict[str, Any] = {}
        self.model_cache: Dict[str, Any] = {}
        self.cache_priority: Dict[str, float] = {}
        
        # ëª¨ë‹ˆí„°ë§
        self.monitoring_active = False
        self.monitoring_thread = None
        self._lock = threading.Lock()
        
        logger.info(f"ğŸ§  MemoryManager ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}, ë©”ëª¨ë¦¬ ì œí•œ: {self.memory_limit_gb:.1f}GB")
        
        # M3 Max ìµœì í™” ì„¤ì •
        if self.device == "mps":
            logger.info("ğŸ M3 Max ìµœì í™” ëª¨ë“œ í™œì„±í™”")
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True

    # ============================================
    # ğŸ M3 Max ìµœì í™” ë©”ì„œë“œë“¤ (ìƒˆë¡œ ì¶”ê°€)
    # ============================================

    def optimize_for_m3_max(self):
        """
        ğŸ M3 Max ìµœì í™” (ëˆ„ë½ëœ ë©”ì„œë“œ)
        M3 Max Neural Engine í™œìš© ë° ë©”ëª¨ë¦¬ ìµœì í™”
        """
        try:
            if not TORCH_AVAILABLE:
                logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ìµœì í™”")
                return False
                
            # M3 Max ê°ì§€ í™•ì¸
            if not self.is_m3_max:
                logger.info("ğŸ”§ ì¼ë°˜ ì‹œìŠ¤í…œ ìµœì í™” ì ìš©")
                torch.set_num_threads(4)
                return True
            
            # M3 Max íŠ¹í™” ìµœì í™”
            logger.info("ğŸ M3 Max Neural Engine ìµœì í™” ì‹œì‘")
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if torch.backends.mps.is_available():
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                
                # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ.update({
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                    'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                    'METAL_DEVICE_WRAPPER_TYPE': '1',
                    'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                    'PYTORCH_MPS_PREFER_METAL': '1',
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1'
                })
                
                # ìŠ¤ë ˆë“œ ìµœì í™” (M3 Max 16ì½”ì–´ í™œìš©)
                torch.set_num_threads(16)
                
                # Neural Engine ìµœì í™” ì„¤ì •
                if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                    torch.backends.mps.set_per_process_memory_fraction(0.8)
            
            # 2. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            self._setup_m3_memory_pools()
            
            # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
            self._optimize_batch_sizes()
            
            # 4. ì •ë°€ë„ ìµœì í™” (Float32 ì•ˆì •ì„± ìš°ì„ )
            self.precision_mode = 'float32'
            
            logger.info("âœ… M3 Max Neural Engine ìµœì í™” ì™„ë£Œ")
            logger.info(f"   - í™œìš© ì½”ì–´: 16ê°œ")
            logger.info(f"   - ë©”ëª¨ë¦¬ í•œê³„: {self.memory_limit_gb:.1f}GB")
            logger.info(f"   - ì •ë°€ë„ ëª¨ë“œ: {self.precision_mode}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ìµœì í™”
            try:
                torch.set_num_threads(8)
                return True
            except:
                return False

    def cleanup_memory(self, aggressive: bool = False):
        """
        ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ (ëˆ„ë½ëœ ë©”ì„œë“œ)
        M3 Max 128GB ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì •ë¦¬
        """
        try:
            start_time = time.time()
            
            # 1. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = 0
            for _ in range(3):  # 3íšŒ ë°˜ë³µ (ìˆœí™˜ ì°¸ì¡° í•´ê²°)
                collected = gc.collect()
                collected_objects += collected
            
            # 2. ìºì‹œ ì •ë¦¬
            if self.enable_caching:
                cache_cleared = len(self.tensor_cache) + len(self.image_cache) + len(self.model_cache)
                if aggressive:
                    self.clear_cache(aggressive=True)
                else:
                    self._evict_low_priority_cache()
            else:
                cache_cleared = 0
            
            # 3. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            gpu_freed = 0
            if TORCH_AVAILABLE:
                try:
                    if self.device == "mps" and torch.backends.mps.is_available():
                        # M3 Max MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                        initial_memory = self._get_mps_memory_usage()
                        
                        # ë°©ë²• 1: torch.mps.empty_cache() (PyTorch 2.1+)
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        
                        # ë°©ë²• 2: MPS ë™ê¸°í™”
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        
                        # ë°©ë²• 3: í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                        if aggressive and self.is_m3_max:
                            self._aggressive_m3_cleanup()
                        
                        final_memory = self._get_mps_memory_usage()
                        gpu_freed = max(0, initial_memory - final_memory)
                        
                    elif self.device == "cuda" and torch.cuda.is_available():
                        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                        initial_memory = torch.cuda.memory_allocated() / 1024**3
                        torch.cuda.empty_cache()
                        if aggressive:
                            torch.cuda.synchronize()
                        final_memory = torch.cuda.memory_allocated() / 1024**3
                        gpu_freed = max(0, initial_memory - final_memory)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ê³µê²©ì  ëª¨ë“œ)
            system_freed = 0
            if aggressive:
                try:
                    # ì‹œìŠ¤í…œ ìºì‹œ ì •ë¦¬
                    if PSUTIL_AVAILABLE:
                        process = psutil.Process()
                        initial_rss = process.memory_info().rss / 1024**3
                    
                    # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ (Unix ê³„ì—´)
                    try:
                        import ctypes
                        if hasattr(ctypes.CDLL, "libc.so.6"):
                            ctypes.CDLL("libc.so.6").malloc_trim(0)
                    except:
                        pass
                    
                    if PSUTIL_AVAILABLE:
                        final_rss = process.memory_info().rss / 1024**3
                        system_freed = max(0, initial_rss - final_rss)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
            self._update_memory_stats()
            
            cleanup_time = time.time() - start_time
            
            # ê²°ê³¼ ë¡œê¹…
            logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ ({cleanup_time:.2f}ì´ˆ)")
            logger.info(f"   - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected_objects}ê°œ ê°ì²´")
            logger.info(f"   - ìºì‹œ ì •ë¦¬: {cache_cleared}ê°œ í•­ëª©")
            if gpu_freed > 0:
                logger.info(f"   - GPU ë©”ëª¨ë¦¬ í•´ì œ: {gpu_freed:.2f}GB")
            if system_freed > 0:
                logger.info(f"   - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í•´ì œ: {system_freed:.2f}GB")
            
            return {
                "success": True,
                "cleanup_time": cleanup_time,
                "collected_objects": collected_objects,
                "cache_cleared": cache_cleared,
                "gpu_freed_gb": gpu_freed,
                "system_freed_gb": system_freed,
                "aggressive": aggressive,
                "device": self.device
            }
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device
            }

    def _setup_m3_memory_pools(self):
        """M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •"""
        try:
            if not self.is_m3_max:
                return
            
            # ë©”ëª¨ë¦¬ í’€ í¬ê¸° ê³„ì‚° (ì „ì²´ ë©”ëª¨ë¦¬ì˜ 80%)
            pool_size_gb = self.memory_gb * 0.8
            
            # ìš©ë„ë³„ ë©”ëª¨ë¦¬ í• ë‹¹
            self.memory_pools = {
                "model_cache": pool_size_gb * 0.4,      # 40% - ëª¨ë¸ ìºì‹œ
                "inference": pool_size_gb * 0.3,        # 30% - ì¶”ë¡  ì‘ì—…
                "preprocessing": pool_size_gb * 0.2,    # 20% - ì „ì²˜ë¦¬
                "buffer": pool_size_gb * 0.1            # 10% - ë²„í¼
            }
            
            logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •: {pool_size_gb:.1f}GB")
            for pool_name, size in self.memory_pools.items():
                logger.info(f"   - {pool_name}: {size:.1f}GB")
                
        except Exception as e:
            logger.error(f"âŒ M3 ë©”ëª¨ë¦¬ í’€ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _optimize_batch_sizes(self):
        """ë°°ì¹˜ í¬ê¸° ìµœì í™”"""
        try:
            if self.is_m3_max:
                # M3 Max 128GB ê¸°ì¤€ ìµœì  ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 8,
                    "pose_estimation": 12,
                    "cloth_segmentation": 6,
                    "virtual_fitting": 4,
                    "super_resolution": 2
                }
            else:
                # ì¼ë°˜ ì‹œìŠ¤í…œ ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 4,
                    "pose_estimation": 6,
                    "cloth_segmentation": 3,
                    "virtual_fitting": 2,
                    "super_resolution": 1
                }
            
            logger.info(f"âš™ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")

    def _get_mps_memory_usage(self) -> float:
        """MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            if PSUTIL_AVAILABLE:
                # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ì¶”ì •
                process = psutil.Process()
                return process.memory_info().rss / 1024**3
            else:
                return 0.0
        except:
            return 0.0

    def _aggressive_m3_cleanup(self):
        """ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # 1. ëª¨ë“  ìºì‹œ í´ë¦¬ì–´
            if hasattr(self, 'tensor_cache'):
                self.tensor_cache.clear()
            if hasattr(self, 'image_cache'):
                self.image_cache.clear()
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            # 2. ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for _ in range(5):
                gc.collect()
            
            # 3. PyTorch ìºì‹œ ì •ë¦¬
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            
            # 4. ë©”ëª¨ë¦¬ í’€ ì¬ì„¤ì •
            if hasattr(self, 'memory_pools'):
                self._setup_m3_memory_pools()
            
            logger.info("ğŸ ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_optimization_recommendations(self) -> List[str]:
        """
        ğŸ¯ ë©”ëª¨ë¦¬ ìµœì í™” ê¶Œì¥ì‚¬í•­ (ìƒˆë¡œìš´ ë©”ì„œë“œ)
        í˜„ì¬ ìƒíƒœ ê¸°ë°˜ ìµœì í™” ì œì•ˆ
        """
        try:
            recommendations = []
            stats = self.get_memory_stats()
            
            # CPU ë©”ëª¨ë¦¬ ê²€ì‚¬
            cpu_usage_ratio = stats.cpu_used_gb / stats.cpu_total_gb
            if cpu_usage_ratio > 0.9:
                recommendations.append("ğŸš¨ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìœ„í—˜ (90% ì´ˆê³¼)")
                recommendations.append("   â†’ cleanup_memory(aggressive=True) ì‹¤í–‰ ê¶Œì¥")
            elif cpu_usage_ratio > 0.8:
                recommendations.append("âš ï¸ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ (80% ì´ˆê³¼)")
                recommendations.append("   â†’ ìºì‹œ ì •ë¦¬ ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ê°ì†Œ ê¶Œì¥")
            
            # GPU ë©”ëª¨ë¦¬ ê²€ì‚¬
            if stats.gpu_total_gb > 0:
                gpu_usage_ratio = stats.gpu_allocated_gb / stats.gpu_total_gb
                if gpu_usage_ratio > 0.9:
                    recommendations.append("ğŸš¨ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìœ„í—˜ (90% ì´ˆê³¼)")
                    if self.device == "mps":
                        recommendations.append("   â†’ torch.mps.empty_cache() ì‹¤í–‰ ê¶Œì¥")
                    else:
                        recommendations.append("   â†’ torch.cuda.empty_cache() ì‹¤í–‰ ê¶Œì¥")
            
            # ìºì‹œ í¬ê¸° ê²€ì‚¬
            if stats.cache_size_mb > 1000:  # 1GB ì´ìƒ
                recommendations.append(f"ğŸ“¦ ìºì‹œ í¬ê¸° í¼ ({stats.cache_size_mb:.0f}MB)")
                recommendations.append("   â†’ clear_cache() ì‹¤í–‰ ê¶Œì¥")
            
            # M3 Max íŠ¹í™” ê¶Œì¥ì‚¬í•­
            if self.is_m3_max:
                if cpu_usage_ratio > 0.7:
                    recommendations.append("ğŸ M3 Max ìµœì í™” ê¶Œì¥:")
                    recommendations.append("   â†’ optimize_for_m3_max() ì¬ì‹¤í–‰")
                    recommendations.append("   â†’ Neural Engine í™œìš©ë„ ì¦ëŒ€")
            
            # ì‹œìŠ¤í…œë³„ ìµœì í™”
            if len(recommendations) == 0:
                recommendations.append("âœ… ë©”ëª¨ë¦¬ ìƒíƒœ ì–‘í˜¸")
                if self.is_m3_max:
                    recommendations.append("ğŸ M3 Max Neural Engine ìµœì  í™œìš© ì¤‘")
                recommendations.append("   â†’ í˜„ì¬ ì„¤ì • ìœ ì§€ ê¶Œì¥")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["âŒ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨"]

    def get_memory_efficiency_score(self) -> float:
        """
        ğŸ“Š ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (ìƒˆë¡œìš´ ë©”ì„œë“œ)
        0.0 ~ 1.0 ì ìˆ˜ë¡œ í˜„ì¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í‰ê°€
        """
        try:
            stats = self.get_memory_stats()
            score_factors = []
            
            # 1. CPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (40%)
            cpu_ratio = stats.cpu_used_gb / stats.cpu_total_gb
            cpu_score = max(0, 1.0 - cpu_ratio) if cpu_ratio < 0.9 else 0.1
            score_factors.append(("cpu_efficiency", cpu_score, 0.4))
            
            # 2. GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (30%)
            if stats.gpu_total_gb > 0:
                gpu_ratio = stats.gpu_allocated_gb / stats.gpu_total_gb
                gpu_score = max(0, 1.0 - gpu_ratio) if gpu_ratio < 0.9 else 0.1
            else:
                gpu_score = 1.0  # GPU ì—†ìœ¼ë©´ ë§Œì 
            score_factors.append(("gpu_efficiency", gpu_score, 0.3))
            
            # 3. ìºì‹œ íš¨ìœ¨ì„± (20%)
            cache_ratio = min(1.0, stats.cache_size_mb / 1000)  # 1GB ê¸°ì¤€
            cache_score = max(0.2, 1.0 - cache_ratio)
            score_factors.append(("cache_efficiency", cache_score, 0.2))
            
            # 4. M3 Max ë³´ë„ˆìŠ¤ (10%)
            m3_bonus = 0.1 if self.is_m3_max and self.optimization_enabled else 0.0
            score_factors.append(("m3_bonus", m3_bonus, 0.1))
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = sum(score * weight for _, score, weight in score_factors)
            
            # ì¶”ê°€ íŒ¨ë„í‹°
            if stats.cpu_percent > 95:
                total_score *= 0.5  # ê³¼ë¶€í•˜ ì‹œ 50% ê°ì 
            
            return max(0.0, min(1.0, total_score))
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’

    def _update_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            stats = self.get_memory_stats()
            with self._lock:
                self.stats_history.append(stats)
                if len(self.stats_history) > self.max_history_length:
                    self.stats_history.pop(0)
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    # ============================================
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (ëª¨ë‘ ìœ ì§€)
    # ============================================
    
    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            # CPU ë©”ëª¨ë¦¬
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                cpu_percent = memory.percent
                cpu_total_gb = memory.total / 1024**3
                cpu_used_gb = memory.used / 1024**3
                cpu_available_gb = memory.available / 1024**3
                swap_used_gb = psutil.swap_memory().used / 1024**3
                
                # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
                process = psutil.Process()
                process_memory_mb = process.memory_info().rss / 1024**2
            else:
                cpu_percent = 0.0
                cpu_total_gb = 16.0
                cpu_used_gb = 8.0
                cpu_available_gb = 8.0
                swap_used_gb = 0.0
                process_memory_mb = 0.0
            
            # GPU ë©”ëª¨ë¦¬
            gpu_allocated_gb = 0.0
            gpu_reserved_gb = 0.0
            gpu_total_gb = 0.0
            
            if TORCH_AVAILABLE:
                try:
                    if self.device == "cuda" and torch.cuda.is_available():
                        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        gpu_total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPS ë©”ëª¨ë¦¬ ì •ë³´ (ì¶”ì •)
                        gpu_allocated_gb = 2.0  # ì„ì‹œê°’
                        gpu_total_gb = 128.0  # M3 Max 128GB
                except Exception:
                    pass
            
            # ìºì‹œ í¬ê¸°
            cache_size_mb = 0.0
            if self.enable_caching:
                cache_size_mb = sum(
                    len(str(v)) / 1024**2 for v in 
                    [*self.tensor_cache.values(), *self.image_cache.values(), *self.model_cache.values()]
                )
            
            return MemoryStats(
                cpu_percent=cpu_percent,
                cpu_available_gb=cpu_available_gb,
                cpu_used_gb=cpu_used_gb,
                cpu_total_gb=cpu_total_gb,
                gpu_allocated_gb=gpu_allocated_gb,
                gpu_reserved_gb=gpu_reserved_gb,
                gpu_total_gb=gpu_total_gb,
                swap_used_gb=swap_used_gb,
                cache_size_mb=cache_size_mb,
                process_memory_mb=process_memory_mb
            )
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return MemoryStats(
                cpu_percent=0.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )
    
    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        stats = self.get_memory_stats()
        
        cpu_usage_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        gpu_usage_ratio = stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb)
        
        status = "normal"
        if cpu_usage_ratio > self.critical_threshold or gpu_usage_ratio > self.critical_threshold:
            status = "critical"
        elif cpu_usage_ratio > self.warning_threshold or gpu_usage_ratio > self.warning_threshold:
            status = "warning"
        
        return {
            "status": status,
            "cpu_usage_ratio": cpu_usage_ratio,
            "gpu_usage_ratio": gpu_usage_ratio,
            "cache_size_mb": stats.cache_size_mb,
            "recommendations": self._get_cleanup_recommendations(stats)
        }
    
    def _get_cleanup_recommendations(self, stats: MemoryStats) -> List[str]:
        """ì •ë¦¬ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        cpu_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        if cpu_ratio > 0.8:
            recommendations.append("CPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
        
        if stats.gpu_allocated_gb > 10.0:
            recommendations.append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
        
        if stats.cache_size_mb > 1000:
            recommendations.append("ìºì‹œ ì •ë¦¬ ê¶Œì¥")
        
        return recommendations
    
    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬"""
        try:
            if not self.enable_caching:
                return
                
            with self._lock:
                if aggressive:
                    # ì „ì²´ ìºì‹œ ì‚­ì œ
                    self.tensor_cache.clear()
                    self.image_cache.clear()
                    self.model_cache.clear()
                    self.cache_priority.clear()
                    logger.info("ğŸ§¹ ì „ì²´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                else:
                    # ì„ íƒì  ìºì‹œ ì •ë¦¬
                    self._evict_low_priority_cache()
                    logger.debug("ğŸ§¹ ì„ íƒì  ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def smart_cleanup(self):
        """ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            pressure = self.check_memory_pressure()
            
            if pressure["status"] == "critical":
                self.clear_cache(aggressive=True)
                if TORCH_AVAILABLE:
                    gc.collect()
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPSëŠ” empty_cacheê°€ ì—†ìœ¼ë¯€ë¡œ ëŒ€ì²´ ë°©ë²•
                        pass
                logger.info("ğŸš¨ ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
            elif pressure["status"] == "warning":
                self.clear_cache(aggressive=False)
                logger.debug("âš ï¸ ì˜ˆë°©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
        except Exception as e:
            logger.error(f"ì§€ëŠ¥í˜• ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ë™ê¸° ì •ë¦¬ ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.smart_cleanup)
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cache_tensor(self, key: str, tensor: Any, priority: float = 0.5):
        """í…ì„œ ìºì‹±"""
        if not self.enable_caching:
            return
            
        try:
            with self._lock:
                self.tensor_cache[key] = tensor
                self.cache_priority[key] = priority
        except Exception as e:
            logger.error(f"í…ì„œ ìºì‹± ì‹¤íŒ¨: {e}")
    
    def get_cached_tensor(self, key: str, cache_type: str = "tensor") -> Optional[Any]:
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        if not self.enable_caching:
            return None
            
        try:
            with self._lock:
                if cache_type == "image":
                    return self.image_cache.get(key)
                else:
                    data = self.tensor_cache.get(key)
                    if data is not None:
                        # ì‚¬ìš© ì‹œ ìš°ì„ ìˆœìœ„ ì¦ê°€
                        self.cache_priority[key] = min(1.0, self.cache_priority.get(key, 0.5) + 0.1)
                    return data
        except Exception as e:
            logger.error(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _evict_low_priority_cache(self):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ ì œê±°"""
        if not self.cache_priority:
            return
        
        try:
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # í•˜ìœ„ 20% ì œê±°
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                self.tensor_cache.pop(key, None)
                self.cache_priority.pop(key, None)
            
            logger.debug(f"ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ {num_to_remove}ê°œ ì œê±°")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì œê±° ì‹¤íŒ¨: {e}")
    
    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    @asynccontextmanager
    async def memory_efficient_context(self, clear_before: bool = True, clear_after: bool = True):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if clear_before:
            await self.cleanup()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                await self.cleanup()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:  # 100MB ì´ìƒ ì¦ê°€
                logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: +{memory_diff:.2f}GB")
    
    @contextmanager
    def memory_efficient_sync_context(self, clear_before: bool = True, clear_after: bool = True):
        """ë™ê¸° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if clear_before:
            self.clear_cache()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                self.clear_cache()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:
                logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: +{memory_diff:.2f}GB")
    
    # ëª¨ë‹ˆí„°ë§
    def start_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitoring_thread.start()
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                stats = self.get_memory_stats()
                
                with self._lock:
                    self.stats_history.append(stats)
                    
                    # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                    if len(self.stats_history) > self.max_history_length:
                        self.stats_history.pop(0)
                
                # ìë™ ì •ë¦¬ ì‹¤í–‰
                if self.auto_cleanup:
                    self.smart_cleanup()
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(10)
    
    # ì„±ëŠ¥ ìµœì í™”
    def optimize_for_inference(self):
        """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
        if not TORCH_AVAILABLE:
            return
            
        try:
            # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
            torch.set_grad_enabled(False)
            
            # ë°±ì—”ë“œ ìµœì í™”
            if self.device == 'cuda':
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            elif self.device == 'mps':
                # MPS ìµœì í™” (M3 Max)
                torch.backends.mps.is_available()  # MPS í™œì„±í™” í™•ì¸
            
            # ìºì‹œ ì •ë¦¬
            self.clear_cache(aggressive=True)
            
            logger.info(f"ğŸš€ {self.device.upper()} ì¶”ë¡  ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì¶”ë¡  ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    async def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        stats = self.get_memory_stats()
        pressure_info = self.check_memory_pressure()
        
        return {
            "memory_usage": {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb
            },
            "pressure": pressure_info,
            "cache_info": {
                "tensor_cache_size": len(self.tensor_cache),
                "image_cache_size": len(self.image_cache),
                "model_cache_size": len(self.model_cache)
            }
        }

    async def initialize(self) -> bool:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        try:
            # M3 Max ìµœì í™” ì„¤ì •
            if self.is_m3_max and self.optimization_enabled:
                self.optimize_for_inference()
            
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì˜µì…˜)
            if self.config.get('start_monitoring', False):
                self.start_monitoring()
            
            self.logger.info(f"âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def get_step_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "specialized_features": {
                "memory_limit_gb": self.memory_limit_gb,
                "warning_threshold": self.warning_threshold,
                "critical_threshold": self.critical_threshold,
                "auto_cleanup": self.auto_cleanup,
                "monitoring_interval": self.monitoring_interval,
                "enable_caching": self.enable_caching
            },
            "current_stats": self.get_memory_stats().__dict__,
            "pressure_info": self.check_memory_pressure(),
            "m3_max_features": {
                "precision_mode": getattr(self, 'precision_mode', 'float32'),
                "memory_pools": getattr(self, 'memory_pools', {}),
                "optimal_batch_sizes": getattr(self, 'optimal_batch_sizes', {})
            }
        }
    
    def get_usage(self) -> Dict[str, Any]:
        """ë™ê¸° ì‚¬ìš©ëŸ‰ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)"""
        try:
            stats = self.get_memory_stats()
            return {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb
            }
        except Exception as e:
            logger.error(f"ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.stop_monitoring()
            if self.enable_caching:
                self.clear_cache(aggressive=True)
        except:
            pass

# ============================================
# ğŸ”¥ í•µì‹¬: ëˆ„ë½ëœ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ ëª¨ë‘ ì¶”ê°€
# ============================================

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_memory_manager: Optional[MemoryManager] = None

def get_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager(**kwargs)
    return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager(**kwargs)

# ğŸ”¥ í•µì‹¬: main.pyì—ì„œ ì°¾ëŠ” í•¨ìˆ˜ ì¶”ê°€
def create_memory_manager(device: str = "auto", **kwargs) -> MemoryManager:
    """
    ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì íŒ©í† ë¦¬ í•¨ìˆ˜ - main.pyì—ì„œ ì‚¬ìš©
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        MemoryManager ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        logger.info(f"ğŸ“¦ MemoryManager ìƒì„± - ë””ë°”ì´ìŠ¤: {device}")
        manager = MemoryManager(device=device, **kwargs)
        return manager
    except Exception as e:
        logger.error(f"âŒ MemoryManager ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        return MemoryManager(device="cpu")

# ì¶”ê°€ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_optimized_memory_manager(
    device: str = "auto",
    memory_gb: float = 16.0,
    is_m3_max: bool = None,
    optimization_enabled: bool = True
) -> MemoryManager:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
    if is_m3_max is None:
        is_m3_max = _detect_m3_max()
    
    return MemoryManager(
        device=device,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled,
        auto_cleanup=True,
        enable_caching=True
    )

def _detect_m3_max() -> bool:
    """M3 Max ê°ì§€ í—¬í¼"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True)
            return 'M3' in result.stdout
    except:
        pass
    return False

# ============================================
# ğŸ”¥ í•µì‹¬: main.pyì—ì„œ ì°¾ëŠ” í•¨ìˆ˜ë“¤ ì¶”ê°€
# ============================================

def initialize_global_memory_manager(device: str = "mps", **kwargs) -> MemoryManager:
    """
    ğŸ”¥ ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” - main.pyì—ì„œ ì‚¬ìš©
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        **kwargs: ì¶”ê°€ ì„¤ì •
    
    Returns:
        ì´ˆê¸°í™”ëœ MemoryManager ì¸ìŠ¤í„´ìŠ¤
    """
    global _global_memory_manager
    
    try:
        if _global_memory_manager is None:
            _global_memory_manager = MemoryManager(device=device, **kwargs)
            logger.info(f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {device}")
        return _global_memory_manager
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        _global_memory_manager = MemoryManager(device="cpu")
        return _global_memory_manager

def optimize_memory_usage(device: str = None, aggressive: bool = False) -> Dict[str, Any]:
    """
    ğŸ”¥ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” - ë™ê¸° í•¨ìˆ˜ (main.pyì—ì„œ ì‚¬ìš©)
    
    Args:
        device: ëŒ€ìƒ ë””ë°”ì´ìŠ¤ ('mps', 'cuda', 'cpu')
        aggressive: ê³µê²©ì  ì •ë¦¬ ì—¬ë¶€
    
    Returns:
        ìµœì í™” ê²°ê³¼ ì •ë³´
    """
    try:
        manager = get_memory_manager(device=device or "auto")
        
        # ìµœì í™” ì „ ìƒíƒœ
        before_stats = manager.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        manager.clear_cache(aggressive=aggressive)
        
        # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
        if TORCH_AVAILABLE:
            gc.collect()
            if manager.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif manager.device == "mps" and torch.backends.mps.is_available():
                # MPSëŠ” empty_cache ì—†ìœ¼ë¯€ë¡œ ëŒ€ì²´ ë°©ë²•
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
        
        # ìµœì í™” í›„ ìƒíƒœ
        after_stats = manager.get_memory_stats()
        
        # ê²°ê³¼ ê³„ì‚°
        freed_cpu = before_stats.cpu_used_gb - after_stats.cpu_used_gb
        freed_gpu = before_stats.gpu_allocated_gb - after_stats.gpu_allocated_gb
        freed_cache = before_stats.cache_size_mb - after_stats.cache_size_mb
        
        result = {
            "success": True,
            "device": manager.device,
            "freed_memory": {
                "cpu_gb": max(0, freed_cpu),
                "gpu_gb": max(0, freed_gpu),
                "cache_mb": max(0, freed_cache)
            },
            "before": {
                "cpu_used_gb": before_stats.cpu_used_gb,
                "gpu_allocated_gb": before_stats.gpu_allocated_gb,
                "cache_size_mb": before_stats.cache_size_mb
            },
            "after": {
                "cpu_used_gb": after_stats.cpu_used_gb,
                "gpu_allocated_gb": after_stats.gpu_allocated_gb,
                "cache_size_mb": after_stats.cache_size_mb
            }
        }
        
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - CPU: {freed_cpu:.2f}GB, GPU: {freed_gpu:.2f}GB, ìºì‹œ: {freed_cache:.1f}MB")
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

# í¸ì˜ í•¨ìˆ˜ë“¤
async def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°)"""
    manager = get_memory_manager()
    await manager.cleanup()

def check_memory():
    """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    manager = get_memory_manager()
    return manager.check_memory_pressure()

def check_memory_available(min_gb: float = 1.0) -> bool:
    """ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸ - main.pyì—ì„œ ì‚¬ìš©"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return stats.cpu_available_gb >= min_gb
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ true ë°˜í™˜

def get_memory_info() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return {
            "device": manager.device,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "gpu_total_gb": stats.gpu_total_gb,
            "gpu_allocated_gb": stats.gpu_allocated_gb
        }
    except Exception as e:
        return {"error": str(e)}

# ë°ì½”ë ˆì´í„°
def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            async with manager.memory_efficient_context(clear_before, clear_after):
                return await func(*args, **kwargs)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            with manager.memory_efficient_sync_context(clear_before, clear_after):
                return func(*args, **kwargs)
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'MemoryManager',
    'MemoryStats',
    'get_memory_manager',
    'get_global_memory_manager',
    'create_memory_manager',  # ğŸ”¥ í•µì‹¬ ì¶”ê°€
    'create_optimized_memory_manager',
    'initialize_global_memory_manager',  # ğŸ”¥ í•µì‹¬ ì¶”ê°€
    'optimize_memory_usage',
    'optimize_memory',
    'check_memory',
    'check_memory_available',  # ğŸ”¥ í•µì‹¬ ì¶”ê°€
    'get_memory_info',
    'memory_efficient'
]

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… MemoryManager ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ëª¨ë“  íŒ©í† ë¦¬ í•¨ìˆ˜ + M3 Max ìµœì í™” ë©”ì„œë“œ í¬í•¨")