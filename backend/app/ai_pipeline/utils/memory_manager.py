# app/ai_pipeline/utils/memory_manager.py
"""
M3 Max μµμ ν™” λ©”λ¨λ¦¬ λ§¤λ‹μ € (PyTorch 2.5.1 νΈν™μ„± λ³΄μ¥)
"""

import logging
import torch
import psutil
import gc
import platform
from typing import Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """λ©”λ¨λ¦¬ ν†µκ³„ μ •λ³΄"""
    cpu_total_gb: float
    cpu_available_gb: float
    cpu_used_percent: float
    gpu_allocated_gb: float = 0.0
    gpu_cached_gb: float = 0.0

class MemoryManager:
    """M3 Max μµμ ν™” λ©”λ¨λ¦¬ λ§¤λ‹μ €"""
    
    def __init__(self, device: str = "mps", memory_limit_gb: Optional[float] = None):
        self.device = device
        self.is_mps = device == "mps"
        self.is_m3_max = self._detect_m3_max()
        self.memory_limit_gb = memory_limit_gb or self._get_optimal_memory_limit()
        
        # PyTorch λ²„μ „λ³„ νΈν™μ„± μ²΄ν¬
        self.pytorch_version = torch.__version__
        self.mps_empty_cache_available = self._check_mps_empty_cache()
        
        logger.info(f"π§  MemoryManager μ΄κΈ°ν™” - λ””λ°”μ΄μ¤: {device}, λ©”λ¨λ¦¬ μ ν•: {self.memory_limit_gb}GB")
        
        if self.is_m3_max:
            logger.info("π M3 Max μµμ ν™” λ¨λ“ ν™μ„±ν™”")
            
        if not self.mps_empty_cache_available and self.is_mps:
            logger.warning(f"β οΈ PyTorch {self.pytorch_version}: MPS empty_cache λ―Έμ§€μ› - λ€μ²΄ λ©”λ¨λ¦¬ κ΄€λ¦¬ μ‚¬μ©")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max κ°μ§€"""
        try:
            if platform.system() != "Darwin" or platform.machine() != "arm64":
                return False
            
            # λ©”λ¨λ¦¬ ν¬κΈ°λ΅ M3 Max μ¶”μ • (36GB+ = M3 Max)
            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            return total_memory_gb >= 32.0
        except:
            return False
    
    def _check_mps_empty_cache(self) -> bool:
        """MPS empty_cache λ©”μ„λ“ μ‚¬μ© κ°€λ¥μ„± μ²΄ν¬"""
        if not self.is_mps:
            return False
        
        try:
            # PyTorch 2.5.1μ—μ„λ” empty_cacheκ°€ μ κ±°λ¨
            return hasattr(torch.backends.mps, 'empty_cache')
        except:
            return False
    
    def _get_optimal_memory_limit(self) -> float:
        """μµμ  λ©”λ¨λ¦¬ μ ν• κ³„μ‚°"""
        total_memory = psutil.virtual_memory().total / (1024**3)
        
        if self.is_m3_max:
            # M3 Max: ν†µν•© λ©”λ¨λ¦¬ 80% ν™μ©
            return total_memory * 0.8
        else:
            # μΌλ° μ‹μ¤ν…: λ³΄μμ  μ ‘κ·Ό
            return min(total_memory * 0.6, 16.0)
    
    def clear_cache(self) -> None:
        """λ©”λ¨λ¦¬ μΊμ‹ μ •λ¦¬ (λ²„μ „ νΈν™μ„± λ³΄μ¥)"""
        try:
            # CPU λ©”λ¨λ¦¬ μ •λ¦¬
            gc.collect()
            
            if self.is_mps:
                if self.mps_empty_cache_available:
                    # PyTorch 2.4 μ΄ν•
                    torch.backends.mps.empty_cache()
                    logger.debug("β… MPS μΊμ‹ μ •λ¦¬ μ™„λ£")
                else:
                    # PyTorch 2.5+ λ€μ²΄ λ°©λ²•
                    self._alternative_mps_cleanup()
                    logger.debug("β… MPS λ€μ²΄ λ©”λ¨λ¦¬ μ •λ¦¬ μ™„λ£")
            
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("β… CUDA μΊμ‹ μ •λ¦¬ μ™„λ£")
                
        except Exception as e:
            logger.warning(f"β οΈ λ©”λ¨λ¦¬ μ •λ¦¬ μ‹¤ν¨: {e}")
    
    def _alternative_mps_cleanup(self) -> None:
        """MPS λ€μ²΄ λ©”λ¨λ¦¬ μ •λ¦¬ λ°©λ²•"""
        try:
            # 1. Python κ°€λΉ„μ§€ μ»¬λ ‰μ… κ°•μ  μ‹¤ν–‰
            for _ in range(3):
                gc.collect()
            
            # 2. λ©”λ¨λ¦¬ μµμ ν™” ν™κ²½λ³€μ μ„¤μ •
            import os
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            
            # 3. μ„μ‹ ν…μ„ μƒμ„±/μ‚­μ λ΅ λ©”λ¨λ¦¬ ν• λ‹Ή ν¨ν„΄ λ¦¬μ…‹
            if torch.backends.mps.is_available():
                temp_tensor = torch.zeros(1, device='mps')
                del temp_tensor
                
        except Exception as e:
            logger.debug(f"λ€μ²΄ MPS μ •λ¦¬ μ‹¤ν¨: {e}")
    
    def get_memory_stats(self) -> MemoryStats:
        """ν„μ¬ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ΅°ν"""
        # CPU λ©”λ¨λ¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_total_gb = cpu_memory.total / (1024**3)
        cpu_available_gb = cpu_memory.available / (1024**3)
        cpu_used_percent = cpu_memory.percent
        
        # GPU λ©”λ¨λ¦¬ (κ°€λ¥ν• κ²½μ°)
        gpu_allocated_gb = 0.0
        gpu_cached_gb = 0.0
        
        if torch.cuda.is_available():
            try:
                gpu_allocated_gb = torch.cuda.memory_allocated() / (1024**3)
                gpu_cached_gb = torch.cuda.memory_reserved() / (1024**3)
            except:
                pass
        
        return MemoryStats(
            cpu_total_gb=cpu_total_gb,
            cpu_available_gb=cpu_available_gb,
            cpu_used_percent=cpu_used_percent,
            gpu_allocated_gb=gpu_allocated_gb,
            gpu_cached_gb=gpu_cached_gb
        )
    
    def optimize_memory(self) -> None:
        """λ©”λ¨λ¦¬ μµμ ν™” μ‹¤ν–‰"""
        try:
            stats_before = self.get_memory_stats()
            
            # λ©”λ¨λ¦¬ μ •λ¦¬
            self.clear_cache()
            
            # M3 Max νΉλ³„ μµμ ν™”
            if self.is_m3_max:
                self._optimize_for_m3_max()
            
            stats_after = self.get_memory_stats()
            
            # μµμ ν™” κ²°κ³Ό λ΅κΉ…
            freed_memory = stats_before.cpu_total_gb - stats_before.cpu_available_gb - \
                          (stats_after.cpu_total_gb - stats_after.cpu_available_gb)
            
            logger.info(
                f"π§Ή λ©”λ¨λ¦¬ μµμ ν™” μ™„λ£ - "
                f"CPU: {stats_after.cpu_available_gb:.2f}GB, "
                f"GPU: {stats_after.gpu_allocated_gb:.2f}GB, "
                f"ν•΄μ λ¨: {freed_memory:.2f}GB"
            )
            
        except Exception as e:
            logger.error(f"β λ©”λ¨λ¦¬ μµμ ν™” μ‹¤ν¨: {e}")
    
    def _optimize_for_m3_max(self) -> None:
        """M3 Max νΉν™” μµμ ν™”"""
        try:
            import os
            
            # M3 Max ν†µν•© λ©”λ¨λ¦¬ μµμ ν™” μ„¤μ •
            os.environ.update({
                "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.0",
                "PYTORCH_MPS_ALLOCATOR_POLICY": "garbage_collection",
                "OMP_NUM_THREADS": "8",  # M3 Max μ„±λ¥ μ½”μ–΄ μ
                "MKL_NUM_THREADS": "8"
            })
            
            logger.debug("π M3 Max ν™κ²½ μµμ ν™” μ™„λ£")
            
        except Exception as e:
            logger.warning(f"M3 Max μµμ ν™” μ‹¤ν¨: {e}")
    
    def check_memory_available(self, required_gb: float) -> bool:
        """ν•„μ” λ©”λ¨λ¦¬ μ‚¬μ© κ°€λ¥μ„± μ²΄ν¬"""
        stats = self.get_memory_stats()
        available = stats.cpu_available_gb
        
        if self.is_m3_max:
            # M3 Maxλ” ν†µν•© λ©”λ¨λ¦¬λ΅ λ” μ μ—°ν•¨
            available *= 1.2
        
        return available >= required_gb
    
    def start_monitoring(self) -> None:
        """λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§ μ‹μ‘"""
        logger.info("π“ λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§ μ‹μ‘")
        # μ‹¤μ  λ¨λ‹ν„°λ§ λ΅μ§μ€ ν•„μ”μ‹ κµ¬ν„
    
    def stop_monitoring(self) -> None:
        """λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§ μ¤‘μ§€"""
        logger.info("π“ λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§ μ¤‘μ§€")