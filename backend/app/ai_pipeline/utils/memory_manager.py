# app/ai_pipeline/utils/memory_manager.py
"""
ğŸ MyCloset AI - ì™„ì „ ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
================================================================================
âœ… í˜„ì¬ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ìµœì í™”
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… ì™„ì „ ìœ ì§€ (GPUMemoryManager, get_step_memory_manager)
âœ… Python 3.8+ ì™„ë²½ í˜¸í™˜
âœ… ModelLoader ì‹œìŠ¤í…œê³¼ ì™„ë²½ ì—°ë™
âœ… BaseStepMixin logger ì†ì„± ì™„ë²½ ë³´ì¥  
âœ… M3 Max Neural Engine ìµœì í™” ì™„ì „ êµ¬í˜„
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (í•œë°©í–¥ ì˜ì¡´ì„±)
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
================================================================================
Author: MyCloset AI Team
Date: 2025-07-20
Version: 7.1 (Python 3.8+ Compatible)
"""

import os
import gc
import threading
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Callable, List, Union, Tuple
from dataclasses import dataclass, field
from contextlib import contextmanager, asynccontextmanager
import weakref
from functools import wraps
from pathlib import Path

# psutil ì„ íƒì  ì„í¬íŠ¸
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ì„ íƒì  ì„í¬íŠ¸
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# NumPy ì„ íƒì  ì„í¬íŠ¸
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0
    m3_optimizations: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
    device: str = "auto"
    memory_limit_gb: float = 16.0
    warning_threshold: float = 0.75
    critical_threshold: float = 0.9
    auto_cleanup: bool = True
    monitoring_interval: float = 30.0
    enable_caching: bool = True
    optimization_enabled: bool = True
    m3_max_features: bool = False

# ==============================================
# ğŸ”¥ í•µì‹¬ ë©”ëª¨ë¦¬ ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤
# ==============================================

class MemoryManager:
    """
    ğŸ í”„ë¡œì íŠ¸ ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ê¸°ë³¸ í´ë˜ìŠ¤)
    âœ… í˜„ì¬ êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… M3 Max Neural Engine ì™„ì „ í™œìš©
    âœ… ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ì•ˆì „í•œ êµ¬ì¡°
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)

        # 2. ê¸°ë³¸ ì„¤ì • (Python 3.8 í˜¸í™˜)
        config_dict = config or {}
        config_dict.update(kwargs)
        
        # MemoryConfig ìƒì„±ì„ ìœ„í•œ í•„í„°ë§
        memory_config_fields = {
            'device', 'memory_limit_gb', 'warning_threshold', 'critical_threshold',
            'auto_cleanup', 'monitoring_interval', 'enable_caching', 'optimization_enabled', 'm3_max_features'
        }
        memory_config_args = {k: v for k, v in config_dict.items() if k in memory_config_fields}
        self.config = MemoryConfig(**memory_config_args)
        
        self.step_name = self.__class__.__name__
        
        # ğŸ”¥ logger ì†ì„± ë³´ì¥ (í˜„ì¬ êµ¬ì¡° í˜¸í™˜)
        self.logger = logging.getLogger(f"utils.{self.step_name}")

        # 3. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)

        # 4. ë©”ëª¨ë¦¬ ê´€ë¦¬ íŠ¹í™” íŒŒë¼ë¯¸í„°
        if PSUTIL_AVAILABLE:
            total_memory = psutil.virtual_memory().total / 1024**3
            self.memory_limit_gb = total_memory * 0.8  # 80% ì‚¬ìš©
        else:
            self.memory_limit_gb = kwargs.get('memory_limit_gb', 16.0)
            
        # 5. ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self._initialize_components()

        self.logger.info(f"ğŸ¯ MemoryManager ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if not TORCH_AVAILABLE:
            return 'cpu'

        try:
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False

    def _initialize_components(self):
        """êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        # ë©”ëª¨ë¦¬ í†µê³„
        self.stats_history = []
        self.max_history_length = 100
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.tensor_cache = {}
        self.image_cache = {}
        self.model_cache = {}
        self.cache_priority = {}
        
        # ëª¨ë‹ˆí„°ë§
        self.monitoring_active = False
        self.monitoring_thread = None
        self._lock = threading.RLock()
        
        # M3 Max íŠ¹í™” ì†ì„± ì´ˆê¸°í™”
        self.precision_mode = 'float32'
        self.memory_pools = {}
        self.optimal_batch_sizes = {}
        
        self.logger.info(f"ğŸ§  MemoryManager êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # M3 Max ìµœì í™” ì„¤ì •
        if self.device == "mps" and self.is_m3_max:
            self.logger.info("ğŸ M3 Max ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            self.optimize_for_m3_max()
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True

    # ============================================
    # ğŸ M3 Max ìµœì í™” ë©”ì„œë“œë“¤
    # ============================================

    def optimize_for_m3_max(self):
        """ğŸ M3 Max Neural Engine ìµœì í™”"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ìµœì í™”")
                return False
                
            # M3 Max ê°ì§€ í™•ì¸
            if not self.is_m3_max:
                self.logger.info("ğŸ”§ ì¼ë°˜ ì‹œìŠ¤í…œ ìµœì í™” ì ìš©")
                return True
            
            self.logger.info("ğŸ M3 Max Neural Engine ìµœì í™” ì‹œì‘")
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if torch.backends.mps.is_available():
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    optimizations.append("MPS cache clearing")
                
                # M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ.update({
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                    'PYTORCH_MPS_LOW_WATERMARK_RATIO': '0.0',
                    'METAL_DEVICE_WRAPPER_TYPE': '1',
                    'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                    'PYTORCH_MPS_PREFER_METAL': '1',
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1'
                })
                optimizations.append("MPS environment optimization")
                
                # ìŠ¤ë ˆë“œ ìµœì í™” (M3 Max 16ì½”ì–´ í™œìš©)
                torch.set_num_threads(16)
                optimizations.append("Thread optimization (16 cores)")
            
            # 2. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
            self._setup_m3_memory_pools()
            optimizations.append("Memory pool optimization")
            
            # 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
            self._optimize_batch_sizes()
            optimizations.append("Batch size optimization")
            
            # 4. ì •ë°€ë„ ìµœì í™” (Float32 ì•ˆì •ì„± ìš°ì„ )
            self.precision_mode = 'float32'
            optimizations.append("Float32 precision mode")
            
            self.logger.info("âœ… M3 Max Neural Engine ìµœì í™” ì™„ë£Œ")
            for opt in optimizations:
                self.logger.info(f"   - {opt}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            return False

    def _setup_m3_memory_pools(self):
        """M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •"""
        try:
            if not self.is_m3_max:
                return
            
            # ë©”ëª¨ë¦¬ í’€ í¬ê¸° ê³„ì‚° (ì „ì²´ ë©”ëª¨ë¦¬ì˜ 80%)
            pool_size_gb = self.memory_gb * 0.8
            
            # ìš©ë„ë³„ ë©”ëª¨ë¦¬ í• ë‹¹
            self.memory_pools = {
                "model_cache": pool_size_gb * 0.4,      # 40% - ëª¨ë¸ ìºì‹œ
                "inference": pool_size_gb * 0.3,        # 30% - ì¶”ë¡  ì‘ì—…
                "preprocessing": pool_size_gb * 0.2,    # 20% - ì „ì²˜ë¦¬
                "buffer": pool_size_gb * 0.1            # 10% - ë²„í¼
            }
            
            self.logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ í’€ ì„¤ì •: {pool_size_gb:.1f}GB")
                
        except Exception as e:
            self.logger.error(f"âŒ M3 ë©”ëª¨ë¦¬ í’€ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _optimize_batch_sizes(self):
        """ë°°ì¹˜ í¬ê¸° ìµœì í™”"""
        try:
            if self.is_m3_max:
                # M3 Max 128GB ê¸°ì¤€ ìµœì  ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 8,
                    "pose_estimation": 12,
                    "cloth_segmentation": 6,
                    "virtual_fitting": 4,
                    "super_resolution": 2
                }
            else:
                # ì¼ë°˜ ì‹œìŠ¤í…œ ë°°ì¹˜ í¬ê¸°
                self.optimal_batch_sizes = {
                    "human_parsing": 4,
                    "pose_estimation": 6,
                    "cloth_segmentation": 3,
                    "virtual_fitting": 2,
                    "super_resolution": 1
                }
            
            self.logger.info(f"âš™ï¸ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")

    def cleanup_memory(self, aggressive: bool = False):
        """ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            start_time = time.time()
            
            # 1. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected_objects = 0
            for _ in range(3):  # 3íšŒ ë°˜ë³µ (ìˆœí™˜ ì°¸ì¡° í•´ê²°)
                collected = gc.collect()
                collected_objects += collected
            
            # 2. ìºì‹œ ì •ë¦¬
            cache_cleared = 0
            if hasattr(self, 'tensor_cache'):
                cache_cleared += len(self.tensor_cache)
                if aggressive:
                    self.clear_cache(aggressive=True)
                else:
                    self._evict_low_priority_cache()
            
            # 3. PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            gpu_freed = 0
            if TORCH_AVAILABLE:
                try:
                    if self.device == "mps" and torch.backends.mps.is_available():
                        # M3 Max MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        
                        # MPS ë™ê¸°í™”
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        
                        # ê³µê²©ì  ì •ë¦¬ ì‹œ M3 Max íŠ¹í™” ì •ë¦¬
                        if aggressive and self.is_m3_max:
                            self._aggressive_m3_cleanup()
                        
                    elif self.device == "cuda" and torch.cuda.is_available():
                        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                        torch.cuda.empty_cache()
                        if aggressive:
                            torch.cuda.synchronize()
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            cleanup_time = time.time() - start_time
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ ({cleanup_time:.2f}ì´ˆ)")
            self.logger.info(f"   - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected_objects}ê°œ ê°ì²´")
            self.logger.info(f"   - ìºì‹œ ì •ë¦¬: {cache_cleared}ê°œ í•­ëª©")
            
            return {
                "success": True,
                "cleanup_time": cleanup_time,
                "collected_objects": collected_objects,
                "cache_cleared": cache_cleared,
                "aggressive": aggressive,
                "device": self.device
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device
            }

    def _aggressive_m3_cleanup(self):
        """ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # 1. ëª¨ë“  ìºì‹œ í´ë¦¬ì–´
            if hasattr(self, 'tensor_cache'):
                self.tensor_cache.clear()
            if hasattr(self, 'image_cache'):
                self.image_cache.clear()
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            # 2. ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for _ in range(5):
                gc.collect()
            
            # 3. PyTorch ìºì‹œ ì •ë¦¬
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            
            self.logger.info("ğŸ ê³µê²©ì  M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            # CPU ë©”ëª¨ë¦¬
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                cpu_percent = memory.percent
                cpu_total_gb = memory.total / 1024**3
                cpu_used_gb = memory.used / 1024**3
                cpu_available_gb = memory.available / 1024**3
                swap_used_gb = psutil.swap_memory().used / 1024**3
                
                # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
                process = psutil.Process()
                process_memory_mb = process.memory_info().rss / 1024**2
            else:
                cpu_percent = 0.0
                cpu_total_gb = self.memory_gb
                cpu_used_gb = self.memory_gb * 0.5
                cpu_available_gb = self.memory_gb * 0.5
                swap_used_gb = 0.0
                process_memory_mb = 0.0
            
            # GPU ë©”ëª¨ë¦¬
            gpu_allocated_gb = 0.0
            gpu_reserved_gb = 0.0
            gpu_total_gb = 0.0
            
            if TORCH_AVAILABLE:
                try:
                    if self.device == "cuda" and torch.cuda.is_available():
                        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        gpu_total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    elif self.device == "mps" and torch.backends.mps.is_available():
                        # MPS ë©”ëª¨ë¦¬ ì •ë³´ (ì¶”ì •)
                        gpu_allocated_gb = 2.0  # ì„ì‹œê°’
                        gpu_total_gb = self.memory_gb  # M3 Max ì „ì²´ ë©”ëª¨ë¦¬
                except Exception:
                    pass
            
            # ìºì‹œ í¬ê¸°
            cache_size_mb = 0.0
            try:
                if hasattr(self, 'tensor_cache'):
                    cache_size_mb = len(str(self.tensor_cache)) / 1024**2
            except:
                pass
            
            # M3 ìµœì í™” ì •ë³´
            m3_optimizations = {}
            if self.is_m3_max:
                m3_optimizations = {
                    "memory_pools": self.memory_pools,
                    "batch_sizes": self.optimal_batch_sizes,
                    "precision_mode": self.precision_mode
                }
            
            return MemoryStats(
                cpu_percent=cpu_percent,
                cpu_available_gb=cpu_available_gb,
                cpu_used_gb=cpu_used_gb,
                cpu_total_gb=cpu_total_gb,
                gpu_allocated_gb=gpu_allocated_gb,
                gpu_reserved_gb=gpu_reserved_gb,
                gpu_total_gb=gpu_total_gb,
                swap_used_gb=swap_used_gb,
                cache_size_mb=cache_size_mb,
                process_memory_mb=process_memory_mb,
                m3_optimizations=m3_optimizations
            )
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return MemoryStats(
                cpu_percent=0.0,
                cpu_available_gb=8.0,
                cpu_used_gb=8.0,
                cpu_total_gb=16.0
            )

    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ ì²´í¬"""
        stats = self.get_memory_stats()
        
        cpu_usage_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        gpu_usage_ratio = stats.gpu_allocated_gb / max(1.0, stats.gpu_total_gb)
        
        status = "normal"
        if cpu_usage_ratio > 0.9 or gpu_usage_ratio > 0.9:
            status = "critical"
        elif cpu_usage_ratio > 0.75 or gpu_usage_ratio > 0.75:
            status = "warning"
        
        return {
            "status": status,
            "cpu_usage_ratio": cpu_usage_ratio,
            "gpu_usage_ratio": gpu_usage_ratio,
            "cache_size_mb": stats.cache_size_mb,
            "recommendations": self._get_cleanup_recommendations(stats)
        }

    def _get_cleanup_recommendations(self, stats: MemoryStats) -> List[str]:
        """ì •ë¦¬ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        cpu_ratio = stats.cpu_used_gb / stats.cpu_total_gb
        if cpu_ratio > 0.8:
            recommendations.append("CPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
        
        if stats.gpu_allocated_gb > 10.0:
            recommendations.append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê¶Œì¥")
        
        if stats.cache_size_mb > 1000:
            recommendations.append("ìºì‹œ ì •ë¦¬ ê¶Œì¥")
        
        if self.is_m3_max and cpu_ratio > 0.7:
            recommendations.append("M3 Max ìµœì í™” ì¬ì‹¤í–‰ ê¶Œì¥")
        
        return recommendations

    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self._lock:
                if aggressive:
                    # ì „ì²´ ìºì‹œ ì‚­ì œ
                    if hasattr(self, 'tensor_cache'):
                        self.tensor_cache.clear()
                    if hasattr(self, 'image_cache'):
                        self.image_cache.clear()
                    if hasattr(self, 'model_cache'):
                        self.model_cache.clear()
                    if hasattr(self, 'cache_priority'):
                        self.cache_priority.clear()
                    self.logger.info("ğŸ§¹ ì „ì²´ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                else:
                    # ì„ íƒì  ìºì‹œ ì •ë¦¬
                    self._evict_low_priority_cache()
                    self.logger.debug("ğŸ§¹ ì„ íƒì  ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _evict_low_priority_cache(self):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ ì œê±°"""
        try:
            if not hasattr(self, 'cache_priority') or not self.cache_priority:
                return
            
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # í•˜ìœ„ 20% ì œê±°
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                if hasattr(self, 'tensor_cache'):
                    self.tensor_cache.pop(key, None)
                if hasattr(self, 'cache_priority'):
                    self.cache_priority.pop(key, None)
            
            self.logger.debug(f"ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ {num_to_remove}ê°œ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì œê±° ì‹¤íŒ¨: {e}")

    # ============================================
    # ğŸ”¥ í˜„ì¬ êµ¬ì¡° í˜¸í™˜ ë©”ì„œë“œë“¤
    # ============================================

    async def initialize(self) -> bool:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        try:
            # M3 Max ìµœì í™” ì„¤ì •
            if self.is_m3_max and self.optimization_enabled:
                self.optimize_for_m3_max()
            
            self.logger.info(f"âœ… MemoryManager ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def cleanup(self):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ë™ê¸° ì •ë¦¬ ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.cleanup_memory, True)
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_usage(self) -> Dict[str, Any]:
        """ë™ê¸° ì‚¬ìš©ëŸ‰ ì¡°íšŒ (í•˜ìœ„ í˜¸í™˜)"""
        try:
            stats = self.get_memory_stats()
            return {
                "cpu_percent": stats.cpu_percent,
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "gpu_total_gb": stats.gpu_total_gb,
                "cache_size_mb": stats.cache_size_mb
            }
        except Exception as e:
            self.logger.error(f"ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'monitoring_active'):
                self.monitoring_active = False
            if hasattr(self, 'tensor_cache'):
                self.clear_cache(aggressive=True)
        except:
            pass

# ==============================================
# ğŸ”¥ GPUMemoryManager í´ë˜ìŠ¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
# ==============================================

class GPUMemoryManager(MemoryManager):
    """
    ğŸ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ê¸°ì¡´ í´ë˜ìŠ¤ëª… ìœ ì§€)
    âœ… í˜„ì¬ êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
    âœ… ê¸°ì¡´ ì½”ë“œì˜ GPUMemoryManager ì‚¬ìš© ìœ ì§€
    """
    
    def __init__(self, device="mps", memory_limit_gb=16.0, **kwargs):
        """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” (ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        super().__init__(device=device, memory_limit_gb=memory_limit_gb, **kwargs)
        self.logger = logging.getLogger("GPUMemoryManager")
        
        # ê¸°ì¡´ ì†ì„± í˜¸í™˜ì„± ìœ ì§€
        self.memory_limit_gb = memory_limit_gb
        
        self.logger.info(f"ğŸ® GPUMemoryManager ì´ˆê¸°í™” - {device} ({memory_limit_gb}GB)")

    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)"""
        try:
            # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì •ë¦¬ í˜¸ì¶œ
            result = self.cleanup_memory(aggressive=False)
            
            # PyTorch GPU ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                elif self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            self.logger.info("ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ GPU ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        try:
            stats = self.get_memory_stats()
            
            # ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                used_gb = memory.used / (1024**3)
                if used_gb > self.memory_limit_gb * 0.9:
                    self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {used_gb:.1f}GB")
                    self.clear_cache()
            
            return {
                "cpu_used_gb": stats.cpu_used_gb,
                "cpu_total_gb": stats.cpu_total_gb,
                "gpu_allocated_gb": stats.gpu_allocated_gb,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ ì™„ì „ ìœ ì§€)
# ==============================================

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_memory_manager = None
_global_gpu_memory_manager = None

def get_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager(**kwargs)
    return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager(**kwargs)

def get_step_memory_manager(step_name: str, **kwargs) -> MemoryManager:
    """
    ğŸ”¥ Stepë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜ (í˜„ì¬ êµ¬ì¡°ì—ì„œ ìš”êµ¬)
    âœ… ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€
    âœ… í˜„ì¬ utils/__init__.pyì—ì„œ ì‚¬ìš©
    """
    try:
        # Stepë³„ íŠ¹í™” ì„¤ì •
        step_configs = {
            "HumanParsingStep": {"memory_limit_gb": 8.0},
            "PoseEstimationStep": {"memory_limit_gb": 6.0},
            "ClothSegmentationStep": {"memory_limit_gb": 4.0},
            "VirtualFittingStep": {"memory_limit_gb": 16.0},
            "PostProcessingStep": {"memory_limit_gb": 8.0},
            "QualityAssessmentStep": {"memory_limit_gb": 4.0}
        }
        
        # Stepë³„ ì„¤ì • ì ìš©
        step_config = step_configs.get(step_name, {})
        final_kwargs = kwargs.copy()
        final_kwargs.update(step_config)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±
        manager = MemoryManager(**final_kwargs)
        manager.step_name = step_name
        manager.logger = logging.getLogger(f"MemoryManager.{step_name}")
        
        logger.info(f"ğŸ“ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì™„ë£Œ")
        return manager
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë°˜í™˜
        return MemoryManager(**kwargs)

def create_memory_manager(device: str = "auto", **kwargs) -> MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì íŒ©í† ë¦¬ í•¨ìˆ˜"""
    try:
        logger.info(f"ğŸ“¦ MemoryManager ìƒì„± - ë””ë°”ì´ìŠ¤: {device}")
        manager = MemoryManager(device=device, **kwargs)
        return manager
    except Exception as e:
        logger.error(f"âŒ MemoryManager ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        return MemoryManager(device="cpu")

def create_optimized_memory_manager(
    device: str = "auto",
    memory_gb: float = 16.0,
    is_m3_max: bool = None,
    optimization_enabled: bool = True
) -> MemoryManager:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
    if is_m3_max is None:
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
        except:
            is_m3_max = False
    
    return MemoryManager(
        device=device,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled,
        auto_cleanup=True,
        enable_caching=True
    )

def initialize_global_memory_manager(device: str = "mps", **kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
    global _global_memory_manager
    
    try:
        if _global_memory_manager is None:
            _global_memory_manager = MemoryManager(device=device, **kwargs)
            logger.info(f"âœ… ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {device}")
        return _global_memory_manager
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        _global_memory_manager = MemoryManager(device="cpu")
        return _global_memory_manager

def optimize_memory_usage(device: str = None, aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” - ë™ê¸° í•¨ìˆ˜"""
    try:
        manager = get_memory_manager(device=device or "auto")
        
        # ìµœì í™” ì „ ìƒíƒœ
        before_stats = manager.get_memory_stats()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        result = manager.cleanup_memory(aggressive=aggressive)
        
        # ìµœì í™” í›„ ìƒíƒœ
        after_stats = manager.get_memory_stats()
        
        # ê²°ê³¼ ê³„ì‚°
        freed_cpu = before_stats.cpu_used_gb - after_stats.cpu_used_gb
        freed_gpu = before_stats.gpu_allocated_gb - after_stats.gpu_allocated_gb
        freed_cache = before_stats.cache_size_mb - after_stats.cache_size_mb
        
        result.update({
            "freed_memory": {
                "cpu_gb": max(0, freed_cpu),
                "gpu_gb": max(0, freed_gpu),
                "cache_mb": max(0, freed_cache)
            },
            "before": {
                "cpu_used_gb": before_stats.cpu_used_gb,
                "gpu_allocated_gb": before_stats.gpu_allocated_gb,
                "cache_size_mb": before_stats.cache_size_mb
            },
            "after": {
                "cpu_used_gb": after_stats.cpu_used_gb,
                "gpu_allocated_gb": after_stats.gpu_allocated_gb,
                "cache_size_mb": after_stats.cache_size_mb
            }
        })
        
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - CPU: {freed_cpu:.2f}GB, GPU: {freed_gpu:.2f}GB")
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

# í¸ì˜ í•¨ìˆ˜ë“¤
async def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°)"""
    manager = get_memory_manager()
    await manager.cleanup()

def check_memory():
    """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    manager = get_memory_manager()
    return manager.check_memory_pressure()

def check_memory_available(min_gb: float = 1.0) -> bool:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return stats.cpu_available_gb >= min_gb
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ true ë°˜í™˜

def get_memory_info() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        manager = get_memory_manager()
        stats = manager.get_memory_stats()
        return {
            "device": manager.device,
            "cpu_total_gb": stats.cpu_total_gb,
            "cpu_available_gb": stats.cpu_available_gb,
            "gpu_total_gb": stats.gpu_total_gb,
            "gpu_allocated_gb": stats.gpu_allocated_gb,
            "is_m3_max": manager.is_m3_max
        }
    except Exception as e:
        return {"error": str(e)}

# ë°ì½”ë ˆì´í„°
def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            if clear_before:
                await manager.cleanup()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                if clear_after:
                    await manager.cleanup()
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            if clear_before:
                manager.cleanup_memory()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                if clear_after:
                    manager.cleanup_memory()
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ê¸°ì¡´ êµ¬ì¡° ì™„ì „ ìœ ì§€)
__all__ = [
    # ğŸ”¥ ê¸°ì¡´ í´ë˜ìŠ¤ëª… ì™„ì „ ìœ ì§€
    'MemoryManager',
    'GPUMemoryManager',          # âœ… í˜„ì¬ êµ¬ì¡°ì—ì„œ ì‚¬ìš©
    'MemoryStats',
    'MemoryConfig',
    
    # ğŸ”¥ ê¸°ì¡´ í•¨ìˆ˜ëª… ì™„ì „ ìœ ì§€
    'get_memory_manager',
    'get_global_memory_manager',
    'get_step_memory_manager',   # âœ… í˜„ì¬ êµ¬ì¡°ì—ì„œ ì¤‘ìš”
    'create_memory_manager',
    'create_optimized_memory_manager',
    'initialize_global_memory_manager',
    'optimize_memory_usage',
    'optimize_memory',
    'check_memory',
    'check_memory_available',
    'get_memory_info',
    'memory_efficient'
]

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("âœ… Python 3.8+ í˜¸í™˜ MemoryManager ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”§ ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€ (GPUMemoryManager, get_step_memory_manager)")
logger.info("ğŸ M3 Max Neural Engine ìµœì í™” ì™„ì „ êµ¬í˜„")
logger.info("ğŸ”— í˜„ì¬ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜")
logger.info("âš¡ conda í™˜ê²½ ì™„ë²½ ì§€ì›")