# app/ai_pipeline/utils/memory_manager.py
"""
MyCloset AI - ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)
- ë™ì  ë©”ëª¨ë¦¬ í• ë‹¹
- ìºì‹œ ìµœì í™”  
- GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (MPS/CUDA)
- ìë™ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
- OOM ë°©ì§€
- Apple Silicon ìµœì í™”
"""
import os
import gc
import threading
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Callable, List, Union
from dataclasses import dataclass
from contextlib import contextmanager, asynccontextmanager
import weakref
from functools import wraps
import numpy as np

# psutil ì„ íƒì  ì„í¬íŠ¸
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

# PyTorch ì„ íƒì  ì„í¬íŠ¸
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""
    cpu_percent: float
    cpu_available_gb: float
    cpu_used_gb: float
    cpu_total_gb: float
    gpu_allocated_gb: float = 0.0
    gpu_reserved_gb: float = 0.0
    gpu_total_gb: float = 0.0
    swap_used_gb: float = 0.0
    cache_size_mb: float = 0.0
    process_memory_mb: float = 0.0

class MemoryManager:
    """ì§€ëŠ¥í˜• GPU/CPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì - Apple Silicon M3 Max ìµœì í™”"""
    
    def __init__(self, 
                 device: str = "auto", 
                 memory_limit_gb: float = None,
                 warning_threshold: float = 0.75,
                 critical_threshold: float = 0.9,
                 auto_cleanup: bool = True,
                 monitoring_interval: float = 30.0,
                 enable_caching: bool = True):
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._detect_optimal_device(device)
        
        # ë©”ëª¨ë¦¬ ì œí•œ ìë™ ì„¤ì •
        if memory_limit_gb is None:
            if PSUTIL_AVAILABLE:
                total_memory = psutil.virtual_memory().total / 1024**3
                self.memory_limit_gb = total_memory * 0.8  # 80% ì‚¬ìš©
            else:
                self.memory_limit_gb = 16.0  # ê¸°ë³¸ê°’
        else:
            self.memory_limit_gb = memory_limit_gb
            
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.auto_cleanup = auto_cleanup
        self.monitoring_interval = monitoring_interval
        self.enable_caching = enable_caching
        
        # ë©”ëª¨ë¦¬ í†µê³„
        self.stats_history: List[MemoryStats] = []
        self.max_history_length = 100
        
        # ìºì‹œ ê´€ë¦¬ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self.model_cache = weakref.WeakValueDictionary() if enable_caching else {}
        self.tensor_cache = {} if enable_caching else {}
        self.cache_priority = {} if enable_caching else {}
        self.image_cache = {} if enable_caching else {}
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.warning_callbacks: List[Callable] = []
        self.critical_callbacks: List[Callable] = []
        
        # GPU ì •ë³´ ê°ì§€
        self.gpu_info = self._detect_gpu_info()
        
        # ì´ˆê¸°í™”
        if auto_cleanup:
            self.start_monitoring()
        
        logger.info(f"MemoryManager ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"- Device: {self.device}")
        logger.info(f"- Memory Limit: {self.memory_limit_gb:.1f}GB")
        logger.info(f"- GPU Info: {self.gpu_info}")
    
    def _detect_optimal_device(self, preferred: str) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred != "auto":
            return preferred
            
        if not TORCH_AVAILABLE:
            return "cpu"
            
        try:
            # M3 Max MPS ìš°ì„ 
            if torch.backends.mps.is_available():
                return "mps"
            # CUDA ë‹¤ìŒ
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except Exception as e:
            logger.warning(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _detect_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ê°ì§€"""
        info = {
            "available": False,
            "type": "none",
            "memory_gb": 0.0,
            "name": "CPU Only"
        }
        
        if not TORCH_AVAILABLE:
            return info
            
        try:
            if self.device == 'cuda' and torch.cuda.is_available():
                props = torch.cuda.get_device_properties(0)
                info.update({
                    "available": True,
                    "type": "cuda",
                    "memory_gb": props.total_memory / 1024**3,
                    "name": props.name
                })
            elif self.device == 'mps' and torch.backends.mps.is_available():
                # MPSëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê³µìœ  (M3 Max ìµœì í™”)
                if PSUTIL_AVAILABLE:
                    system_memory = psutil.virtual_memory().total / 1024**3
                else:
                    system_memory = 128.0  # M3 Max ê¸°ë³¸ê°’
                info.update({
                    "available": True,
                    "type": "mps",
                    "memory_gb": system_memory * 0.7,  # 70% í• ë‹¹
                    "name": "Apple Silicon MPS"
                })
        except Exception as e:
            logger.error(f"GPU ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        return info
    
    async def get_memory_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ (ë¹„ë™ê¸°)"""
        return self.get_memory_stats().__dict__
    
    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            if PSUTIL_AVAILABLE:
                # CPU ë©”ëª¨ë¦¬
                memory = psutil.virtual_memory()
                swap = psutil.swap_memory()
                process = psutil.Process()
                
                stats = MemoryStats(
                    cpu_percent=memory.percent,
                    cpu_available_gb=memory.available / 1024**3,
                    cpu_used_gb=memory.used / 1024**3,
                    cpu_total_gb=memory.total / 1024**3,
                    swap_used_gb=swap.used / 1024**3,
                    cache_size_mb=self._get_cache_size_mb(),
                    process_memory_mb=process.memory_info().rss / 1024**2
                )
            else:
                # psutil ì—†ì´ ê¸°ë³¸ê°’
                stats = MemoryStats(
                    cpu_percent=50.0,
                    cpu_available_gb=64.0,
                    cpu_used_gb=64.0,
                    cpu_total_gb=128.0,
                    cache_size_mb=self._get_cache_size_mb(),
                    process_memory_mb=1024.0
                )
            
            # GPU ë©”ëª¨ë¦¬
            if TORCH_AVAILABLE and self.gpu_info["available"]:
                try:
                    if self.device == 'cuda':
                        stats.gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
                        stats.gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
                        stats.gpu_total_gb = self.gpu_info["memory_gb"]
                    elif self.device == 'mps':
                        # MPS ë©”ëª¨ë¦¬ ìƒíƒœ (M3 Max)
                        stats.gpu_allocated_gb = torch.mps.current_allocated_memory() / 1024**3
                        stats.gpu_total_gb = self.gpu_info["memory_gb"]
                except Exception as e:
                    logger.debug(f"GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            return stats
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return MemoryStats(
                cpu_percent=50.0,
                cpu_available_gb=64.0,
                cpu_used_gb=64.0,
                cpu_total_gb=128.0
            )
    
    def _get_cache_size_mb(self) -> float:
        """ìºì‹œ í¬ê¸° ê³„ì‚° (MB)"""
        if not self.enable_caching:
            return 0.0
            
        total_size = 0
        
        try:
            # í…ì„œ ìºì‹œ
            for tensor in self.tensor_cache.values():
                if TORCH_AVAILABLE and isinstance(tensor, torch.Tensor):
                    total_size += tensor.numel() * tensor.element_size()
                elif isinstance(tensor, np.ndarray):
                    total_size += tensor.nbytes
            
            # ì´ë¯¸ì§€ ìºì‹œ
            for img_data in self.image_cache.values():
                if isinstance(img_data, (bytes, bytearray)):
                    total_size += len(img_data)
                elif isinstance(img_data, np.ndarray):
                    total_size += img_data.nbytes
                    
        except Exception as e:
            logger.debug(f"ìºì‹œ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return total_size / 1024**2
    
    def check_memory_pressure(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸"""
        stats = self.get_memory_stats()
        
        pressure_info = {
            'cpu_pressure': 'none',
            'gpu_pressure': 'none',
            'process_pressure': 'none',
            'overall_pressure': 'none',
            'recommendations': [],
            'stats': stats.__dict__
        }
        
        # CPU ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸
        if stats.cpu_percent > self.critical_threshold * 100:
            pressure_info['cpu_pressure'] = 'critical'
            pressure_info['recommendations'].append('ğŸ’¥ CPU ë©”ëª¨ë¦¬ ì„ê³„: ì¦‰ì‹œ ì •ë¦¬ í•„ìš”')
        elif stats.cpu_percent > self.warning_threshold * 100:
            pressure_info['cpu_pressure'] = 'warning'
            pressure_info['recommendations'].append('âš ï¸ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ')
        
        # GPU ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸
        if stats.gpu_total_gb > 0:
            gpu_usage_ratio = stats.gpu_allocated_gb / stats.gpu_total_gb
            if gpu_usage_ratio > self.critical_threshold:
                pressure_info['gpu_pressure'] = 'critical'
                pressure_info['recommendations'].append('ğŸ’¥ GPU ë©”ëª¨ë¦¬ ì„ê³„: ëª¨ë¸ ì–¸ë¡œë“œ í•„ìš”')
            elif gpu_usage_ratio > self.warning_threshold:
                pressure_info['gpu_pressure'] = 'warning'
                pressure_info['recommendations'].append('âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ')
        
        # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ í™•ì¸
        if stats.process_memory_mb > self.memory_limit_gb * 1024 * 0.9:
            pressure_info['process_pressure'] = 'critical'
            pressure_info['recommendations'].append('ğŸ’¥ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ í•œê³„ ê·¼ì ‘')
        
        # ì „ì²´ ì••ë°• ìˆ˜ì¤€ ê²°ì •
        pressures = [pressure_info['cpu_pressure'], pressure_info['gpu_pressure'], pressure_info['process_pressure']]
        if 'critical' in pressures:
            pressure_info['overall_pressure'] = 'critical'
        elif 'warning' in pressures:
            pressure_info['overall_pressure'] = 'warning'
        
        return pressure_info
    
    async def cleanup(self):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        await asyncio.get_event_loop().run_in_executor(None, self.clear_cache, True)
    
    def clear_cache(self, aggressive: bool = False):
        """ìºì‹œ ì •ë¦¬"""
        if not self.enable_caching:
            return
            
        try:
            with self._lock:
                cleared_items = 0
                
                if aggressive:
                    # ëª¨ë“  ìºì‹œ ì •ë¦¬
                    cleared_items += len(self.tensor_cache)
                    self.tensor_cache.clear()
                    self.cache_priority.clear()
                    self.image_cache.clear()
                    logger.info(f"ğŸ§¹ ì „ì²´ ìºì‹œ ì •ë¦¬: {cleared_items}ê°œ í•­ëª©")
                else:
                    # ìš°ì„ ìˆœìœ„ê°€ ë‚®ì€ ìºì‹œë§Œ ì •ë¦¬
                    to_remove = [k for k, v in self.cache_priority.items() if v < 0.5]
                    for key in to_remove:
                        self.tensor_cache.pop(key, None)
                        self.cache_priority.pop(key, None)
                        cleared_items += 1
                    
                    # ì´ë¯¸ì§€ ìºì‹œ ë¶€ë¶„ ì •ë¦¬ (LRU ë°©ì‹)
                    if len(self.image_cache) > 50:
                        items_to_remove = len(self.image_cache) - 30
                        for _ in range(items_to_remove):
                            if self.image_cache:
                                self.image_cache.popitem()
                                cleared_items += 1
                    
                    logger.info(f"ğŸ§¹ ì„ íƒì  ìºì‹œ ì •ë¦¬: {cleared_items}ê°œ í•­ëª©")
                
                # GPU ìºì‹œ ì •ë¦¬
                if TORCH_AVAILABLE:
                    if self.device == 'cuda' and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    elif self.device == 'mps' and torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected = gc.collect()
                logger.debug(f"ğŸ—‘ï¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´ ì •ë¦¬")
                
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def smart_cleanup(self):
        """ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì •ë¦¬"""
        pressure = self.check_memory_pressure()
        
        if pressure['overall_pressure'] == 'critical':
            logger.warning("ğŸ’¥ ë©”ëª¨ë¦¬ ì„ê³„ ìƒíƒœ - ì ê·¹ì  ì •ë¦¬ ì‹¤í–‰")
            self.clear_cache(aggressive=True)
            
            # ì¶”ê°€ ì •ë¦¬ ì‘ì—…
            self._emergency_cleanup()
            
            # ì½œë°± ì‹¤í–‰
            for callback in self.critical_callbacks:
                try:
                    callback(pressure)
                except Exception as e:
                    logger.error(f"Critical callback ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    
        elif pressure['overall_pressure'] == 'warning':
            logger.info("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ - ë¶€ë¶„ ì •ë¦¬ ì‹¤í–‰")
            self.clear_cache(aggressive=False)
            
            # ì½œë°± ì‹¤í–‰
            for callback in self.warning_callbacks:
                try:
                    callback(pressure)
                except Exception as e:
                    logger.error(f"Warning callback ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def _emergency_cleanup(self):
        """ë¹„ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ëª¨ë“  ì•½í•œ ì°¸ì¡° ì •ë¦¬
            if self.enable_caching:
                self.model_cache.clear()
            
            # ì‹œìŠ¤í…œ ë ˆë²¨ ì •ë¦¬
            if hasattr(gc, 'set_threshold'):
                gc.set_threshold(100, 10, 10)
                gc.collect()
                gc.set_threshold(700, 10, 10)  # ê¸°ë³¸ê°’ ë³µì›
            
            logger.info("ğŸš¨ ë¹„ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¹„ìƒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œ ê´€ë¦¬ ë©”ì„œë“œë“¤
    def add_to_cache(self, key: str, data: Any, priority: float = 0.5, cache_type: str = "tensor") -> bool:
        """ìºì‹œì— ë°ì´í„° ì¶”ê°€"""
        if not self.enable_caching:
            return False
            
        try:
            # ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ìºì‹œ ì¶”ê°€ ê±°ë¶€
            pressure = self.check_memory_pressure()
            if pressure['overall_pressure'] == 'critical':
                logger.warning("ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìºì‹œ ì¶”ê°€ ê±°ë¶€")
                return False
            
            with self._lock:
                if cache_type == "image":
                    self.image_cache[key] = data
                else:
                    self.tensor_cache[key] = data
                    self.cache_priority[key] = priority
                
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.tensor_cache) > 100:
                    self._evict_low_priority_cache()
                
                if len(self.image_cache) > 50:
                    excess = len(self.image_cache) - 30
                    for _ in range(excess):
                        if self.image_cache:
                            self.image_cache.popitem()
            
            return True
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_from_cache(self, key: str, cache_type: str = "tensor") -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if not self.enable_caching:
            return None
            
        try:
            with self._lock:
                if cache_type == "image":
                    return self.image_cache.get(key)
                else:
                    data = self.tensor_cache.get(key)
                    if data is not None:
                        # ì‚¬ìš© ì‹œ ìš°ì„ ìˆœìœ„ ì¦ê°€
                        self.cache_priority[key] = min(1.0, self.cache_priority.get(key, 0.5) + 0.1)
                    return data
        except Exception as e:
            logger.error(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _evict_low_priority_cache(self):
        """ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ ì œê±°"""
        if not self.cache_priority:
            return
        
        try:
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(self.cache_priority.items(), key=lambda x: x[1])
            
            # í•˜ìœ„ 20% ì œê±°
            num_to_remove = max(1, len(sorted_items) // 5)
            for key, _ in sorted_items[:num_to_remove]:
                self.tensor_cache.pop(key, None)
                self.cache_priority.pop(key, None)
            
            logger.debug(f"ë‚®ì€ ìš°ì„ ìˆœìœ„ ìºì‹œ {num_to_remove}ê°œ ì œê±°")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì œê±° ì‹¤íŒ¨: {e}")
    
    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    @asynccontextmanager
    async def memory_efficient_context(self, clear_before: bool = True, clear_after: bool = True):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if clear_before:
            await self.cleanup()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                await self.cleanup()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:  # 100MB ì´ìƒ ì¦ê°€
                logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: +{memory_diff:.2f}GB")
    
    @contextmanager
    def memory_efficient_sync_context(self, clear_before: bool = True, clear_after: bool = True):
        """ë™ê¸° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if clear_before:
            self.clear_cache()
        
        initial_stats = self.get_memory_stats()
        
        try:
            yield
        finally:
            if clear_after:
                self.clear_cache()
            
            final_stats = self.get_memory_stats()
            memory_diff = final_stats.gpu_allocated_gb - initial_stats.gpu_allocated_gb
            
            if memory_diff > 0.1:
                logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: +{memory_diff:.2f}GB")
    
    # ëª¨ë‹ˆí„°ë§
    def start_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitoring_thread.start()
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                stats = self.get_memory_stats()
                
                with self._lock:
                    self.stats_history.append(stats)
                    
                    # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                    if len(self.stats_history) > self.max_history_length:
                        self.stats_history.pop(0)
                
                # ìë™ ì •ë¦¬ ì‹¤í–‰
                if self.auto_cleanup:
                    self.smart_cleanup()
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(10)
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    async def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        stats = self.get_memory_stats()
        return {
            "memory_usage_mb": stats.cpu_used_gb * 1024,
            "memory_free_mb": stats.cpu_available_gb * 1024,
            "memory_percentage": stats.cpu_percent,
            "process_memory_mb": stats.process_memory_mb,
            "gpu_memory_gb": stats.gpu_allocated_gb,
            "cache_size_mb": stats.cache_size_mb
        }
    
    def add_warning_callback(self, callback: Callable):
        """ê²½ê³  ì½œë°± ì¶”ê°€"""
        self.warning_callbacks.append(callback)
    
    def add_critical_callback(self, callback: Callable):
        """ìœ„í—˜ ì½œë°± ì¶”ê°€"""
        self.critical_callbacks.append(callback)
    
    def get_memory_report(self) -> Dict[str, Any]:
        """ìƒì„¸ ë©”ëª¨ë¦¬ ë³´ê³ ì„œ"""
        current_stats = self.get_memory_stats()
        pressure = self.check_memory_pressure()
        
        report = {
            'timestamp': time.time(),
            'device_info': self.gpu_info,
            'current_stats': current_stats.__dict__,
            'pressure_analysis': pressure,
            'cache_info': {
                'enabled': self.enable_caching,
                'tensor_cache_size': len(self.tensor_cache) if self.enable_caching else 0,
                'image_cache_size': len(self.image_cache) if self.enable_caching else 0,
                'cache_size_mb': current_stats.cache_size_mb,
                'model_cache_size': len(self.model_cache) if self.enable_caching else 0
            },
            'monitoring': {
                'active': self.monitoring_active,
                'history_length': len(self.stats_history),
                'interval_seconds': self.monitoring_interval
            },
            'configuration': {
                'memory_limit_gb': self.memory_limit_gb,
                'warning_threshold': self.warning_threshold,
                'critical_threshold': self.critical_threshold,
                'auto_cleanup': self.auto_cleanup
            }
        }
        
        # ìµœê·¼ ì¶”ì„¸ ë¶„ì„
        if len(self.stats_history) >= 5:
            recent_stats = self.stats_history[-5:]
            report['trends'] = {
                'cpu_trend_percent': recent_stats[-1].cpu_percent - recent_stats[0].cpu_percent,
                'gpu_trend_gb': recent_stats[-1].gpu_allocated_gb - recent_stats[0].gpu_allocated_gb,
                'process_trend_mb': recent_stats[-1].process_memory_mb - recent_stats[0].process_memory_mb
            }
        
        return report
    
    def optimize_for_inference(self):
        """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
        if not TORCH_AVAILABLE:
            return
            
        try:
            # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
            torch.set_grad_enabled(False)
            
            # ë°±ì—”ë“œ ìµœì í™”
            if self.device == 'cuda':
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            elif self.device == 'mps':
                # MPS ìµœì í™” (M3 Max)
                torch.backends.mps.is_available()  # MPS í™œì„±í™” í™•ì¸
            
            # ìºì‹œ ì •ë¦¬
            self.clear_cache(aggressive=True)
            
            logger.info(f"ğŸš€ {self.device.upper()} ì¶”ë¡  ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì¶”ë¡  ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.stop_monitoring()
            if self.enable_caching:
                self.clear_cache(aggressive=True)
        except:
            pass

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_memory_manager: Optional[MemoryManager] = None

def get_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager(**kwargs)
    return _global_memory_manager

def get_global_memory_manager(**kwargs) -> MemoryManager:
    """ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë³„ì¹­)"""
    return get_memory_manager(**kwargs)

# í¸ì˜ í•¨ìˆ˜ë“¤
async def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°)"""
    manager = get_memory_manager()
    await manager.cleanup()

def check_memory():
    """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    manager = get_memory_manager()
    return manager.check_memory_pressure()

# ë°ì½”ë ˆì´í„°
def memory_efficient(clear_before: bool = True, clear_after: bool = True):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            async with manager.memory_efficient_context(clear_before, clear_after):
                return await func(*args, **kwargs)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            manager = get_memory_manager()
            with manager.memory_efficient_sync_context(clear_before, clear_after):
                return func(*args, **kwargs)
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator