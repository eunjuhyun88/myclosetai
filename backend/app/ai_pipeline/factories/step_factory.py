# backend/app/ai_pipeline/factories/step_factory.py
"""
ğŸ”¥ MyCloset AI StepFactory v9.0 - BaseStepMixin ì™„ì „ í˜¸í™˜ (Option A êµ¬í˜„)
================================================================================

âœ… í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
âœ… BaseStepMixin v18.0 í‘œì¤€ ì™„ì „ í˜¸í™˜
âœ… ìƒì„±ì ì‹œê·¸ë‹ˆì²˜ í†µì¼ (**kwargs ê¸°ë°˜)
âœ… ì˜ì¡´ì„± ì£¼ì… ìƒì„±ì ì‹œì  ì§€ì›
âœ… UnifiedDependencyManager í†µí•©
âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í‘œì¤€í™”
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Step ìƒì„±ìì— ì˜ì¡´ì„±ì„ ì§ì ‘ ì „ë‹¬ (ìƒì„±ì ì£¼ì…)
2. BaseStepMixin í‘œì¤€ kwargs íŒ¨í„´ ì™„ì „ ì§€ì›
3. process() ë©”ì„œë“œ í†µì¼ëœ ì‹œê·¸ë‹ˆì²˜ ë³´ì¥
4. UnifiedDependencyManager ì™„ì „ í™œìš©
5. ì‹¤ì œ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜

Author: MyCloset AI Team
Date: 2025-07-26
Version: 9.0 (BaseStepMixin Complete Compatibility)
"""

import os
import sys
import logging
import threading
import time
import weakref
import gc
import traceback
import uuid
import asyncio
import concurrent.futures  # ğŸ”¥ ì¶”ê°€ëœ import
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Type, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor

# ì•ˆì „í•œ íƒ€ì… íŒíŒ… (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin
    from ..utils.model_loader import ModelLoader
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ...core.di_container import DIContainer

# ==============================================
# ğŸ”¥ ë¡œê¹… ë° í™˜ê²½ ì„¤ì •
# ==============================================

logger = logging.getLogger(__name__)

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# M3 Max ê°ì§€
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    import platform
    if platform.system() == 'Darwin' and platform.machine() == 'arm64':
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=3)
            IS_M3_MAX = 'M3' in result.stdout
            
            memory_result = subprocess.run(['sysctl', '-n', 'hw.memsize'], 
                                         capture_output=True, text=True, timeout=3)
            if memory_result.stdout.strip():
                MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
        except:
            pass
except:
    pass

logger.info(f"ğŸ”§ StepFactory v9.0 í™˜ê²½: conda={CONDA_INFO['conda_env']}, M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")

# ==============================================
# ğŸ”¥ í•µì‹¬ ë°ì´í„° êµ¬ì¡° (BaseStepMixin í˜¸í™˜)
# ==============================================

class StepType(Enum):
    """Step íƒ€ì… (8ë‹¨ê³„)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class StepPriority(IntEnum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1    # Virtual Fitting, Human Parsing
    HIGH = 2        # Pose Estimation, Cloth Segmentation
    NORMAL = 3      # Geometric Matching, Cloth Warping
    LOW = 4         # Post Processing, Quality Assessment

@dataclass
class BaseStepMixinConfig:
    """BaseStepMixin v18.0 í˜¸í™˜ ì„¤ì • êµ¬ì¡°"""
    # ê¸°ë³¸ Step ì •ë³´
    step_name: str
    step_id: int
    step_type: StepType
    class_name: str
    module_path: str
    priority: StepPriority = StepPriority.NORMAL
    
    # BaseStepMixin í‘œì¤€ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    
    # ìµœì í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ì„¤ì • (BaseStepMixin í‘œì¤€)
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    require_di_container: bool = False
    require_unified_dependency_manager: bool = True
    
    # AI ëª¨ë¸ ì •ë³´
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 0.0
    
    # conda/M3 Max ìµœì í™”
    conda_optimized: bool = True
    m3_max_optimized: bool = True

@dataclass
class StepCreationResult:
    """Step ìƒì„± ê²°ê³¼ (ê°•í™”ë¨)"""
    success: bool
    step_instance: Optional['BaseStepMixin'] = None
    step_name: str = ""
    step_type: Optional[StepType] = None
    class_name: str = ""
    module_path: str = ""
    creation_time: float = 0.0
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ì£¼ì… ê²°ê³¼
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    ai_models_loaded: List[str] = field(default_factory=list)
    
    # BaseStepMixin í˜¸í™˜ì„± ê²€ì¦
    basestepmixin_compatible: bool = True
    process_method_validated: bool = False
    dependency_injection_success: bool = False

# ==============================================
# ğŸ”¥ BaseStepMixin í˜¸í™˜ Step ë§¤í•‘
# ==============================================

class BaseStepMixinMapping:
    """BaseStepMixin v18.0 í‘œì¤€ í˜¸í™˜ Step ë§¤í•‘"""
    
    STEP_CONFIGS = {
        StepType.HUMAN_PARSING: BaseStepMixinConfig(
            step_name="HumanParsingStep",
            step_id=1,
            step_type=StepType.HUMAN_PARSING,
            class_name="HumanParsingStep",
            module_path="app.ai_pipeline.steps.step_01_human_parsing",
            priority=StepPriority.CRITICAL,
            ai_models=["graphonomy", "atr_model", "human_parsing_schp"],
            model_size_gb=4.0,
            require_model_loader=True,
            require_memory_manager=True
        ),
        StepType.POSE_ESTIMATION: BaseStepMixinConfig(
            step_name="PoseEstimationStep",
            step_id=2,
            step_type=StepType.POSE_ESTIMATION,
            class_name="PoseEstimationStep",
            module_path="app.ai_pipeline.steps.step_02_pose_estimation",
            priority=StepPriority.HIGH,
            ai_models=["openpose", "yolov8_pose", "diffusion_pose"],
            model_size_gb=3.4,
            require_model_loader=True,
            require_memory_manager=True
        ),
        StepType.CLOTH_SEGMENTATION: BaseStepMixinConfig(
            step_name="ClothSegmentationStep",
            step_id=3,
            step_type=StepType.CLOTH_SEGMENTATION,
            class_name="ClothSegmentationStep",
            module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
            priority=StepPriority.HIGH,
            ai_models=["u2net", "sam_huge", "cloth_segmentation"],
            model_size_gb=5.5,
            require_model_loader=True,
            require_memory_manager=True
        ),
        StepType.GEOMETRIC_MATCHING: BaseStepMixinConfig(
            step_name="GeometricMatchingStep",
            step_id=4,
            step_type=StepType.GEOMETRIC_MATCHING,
            class_name="GeometricMatchingStep",
            module_path="app.ai_pipeline.steps.step_04_geometric_matching",
            priority=StepPriority.NORMAL,
            ai_models=["gmm", "tps_network", "geometric_matching"],
            model_size_gb=1.3,
            require_model_loader=True
        ),
        StepType.CLOTH_WARPING: BaseStepMixinConfig(
            step_name="ClothWarpingStep",
            step_id=5,
            step_type=StepType.CLOTH_WARPING,
            class_name="ClothWarpingStep",
            module_path="app.ai_pipeline.steps.step_05_cloth_warping",
            priority=StepPriority.NORMAL,
            ai_models=["cloth_warping", "stable_diffusion", "hrviton"],
            model_size_gb=7.0,
            require_model_loader=True,
            require_memory_manager=True
        ),
        StepType.VIRTUAL_FITTING: BaseStepMixinConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            step_type=StepType.VIRTUAL_FITTING,
            class_name="VirtualFittingStep",
            module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
            priority=StepPriority.CRITICAL,
            ai_models=["ootdiffusion", "hr_viton", "virtual_fitting"],
            model_size_gb=14.0,
            require_model_loader=True,
            require_memory_manager=True,
            require_data_converter=True
        ),
        StepType.POST_PROCESSING: BaseStepMixinConfig(
            step_name="PostProcessingStep",
            step_id=7,
            step_type=StepType.POST_PROCESSING,
            class_name="PostProcessingStep",
            module_path="app.ai_pipeline.steps.step_07_post_processing",
            priority=StepPriority.LOW,
            ai_models=["super_resolution", "realesrgan", "enhancement"],
            model_size_gb=1.3,
            require_model_loader=True
        ),
        StepType.QUALITY_ASSESSMENT: BaseStepMixinConfig(
            step_name="QualityAssessmentStep",
            step_id=8,
            step_type=StepType.QUALITY_ASSESSMENT,
            class_name="QualityAssessmentStep",
            module_path="app.ai_pipeline.steps.step_08_quality_assessment",
            priority=StepPriority.LOW,
            ai_models=["clip", "quality_assessment", "perceptual_loss"],
            model_size_gb=7.0,
            require_model_loader=True,
            require_data_converter=True
        )
    }
    
    @classmethod
    def get_config(cls, step_type: StepType, **overrides) -> BaseStepMixinConfig:
        """BaseStepMixin í˜¸í™˜ ì„¤ì • ë°˜í™˜"""
        base_config = cls.STEP_CONFIGS[step_type]
        
        if overrides:
            # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì˜¤ë²„ë¼ì´ë“œ ì ìš©
            config_dict = {
                'step_name': base_config.step_name,
                'step_id': base_config.step_id,
                'step_type': base_config.step_type,
                'class_name': base_config.class_name,
                'module_path': base_config.module_path,
                'priority': base_config.priority,
                'device': base_config.device,
                'use_fp16': base_config.use_fp16,
                'batch_size': base_config.batch_size,
                'confidence_threshold': base_config.confidence_threshold,
                'auto_memory_cleanup': base_config.auto_memory_cleanup,
                'auto_warmup': base_config.auto_warmup,
                'optimization_enabled': base_config.optimization_enabled,
                'strict_mode': base_config.strict_mode,
                'require_model_loader': base_config.require_model_loader,
                'require_memory_manager': base_config.require_memory_manager,
                'require_data_converter': base_config.require_data_converter,
                'require_di_container': base_config.require_di_container,
                'require_unified_dependency_manager': base_config.require_unified_dependency_manager,
                'ai_models': base_config.ai_models.copy(),
                'model_size_gb': base_config.model_size_gb,
                'conda_optimized': base_config.conda_optimized,
                'm3_max_optimized': base_config.m3_max_optimized
            }
            config_dict.update(overrides)
            return BaseStepMixinConfig(**config_dict)
        
        return base_config

# ==============================================
# ğŸ”¥ BaseStepMixin í˜¸í™˜ ì˜ì¡´ì„± í•´ê²°ê¸°
# ==============================================

class BaseStepMixinDependencyResolver:
    """BaseStepMixin v18.0 í˜¸í™˜ ì˜ì¡´ì„± í•´ê²°ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.BaseStepMixinDependencyResolver")
        self._resolved_cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        
        # í•´ê²° ì‹œë„ ì¹´ìš´í„°
        self._resolution_attempts: Dict[str, int] = {}
        self._max_attempts = 3
    
    def resolve_dependencies_for_constructor(self, config: BaseStepMixinConfig) -> Dict[str, Any]:
        """BaseStepMixin ìƒì„±ììš© ì˜ì¡´ì„± í•´ê²° (í•µì‹¬ ë©”ì„œë“œ)"""
        try:
            self.logger.info(f"ğŸ”„ {config.step_name} ìƒì„±ììš© ì˜ì¡´ì„± í•´ê²° ì‹œì‘...")
            
            dependencies = {}
            
            # ê¸°ë³¸ Step ì„¤ì •ë“¤ (BaseStepMixin í‘œì¤€)
            dependencies.update({
                'step_name': config.step_name,
                'step_id': config.step_id,
                'device': self._resolve_device(config.device),
                'use_fp16': config.use_fp16,
                'batch_size': config.batch_size,
                'confidence_threshold': config.confidence_threshold,
                'auto_memory_cleanup': config.auto_memory_cleanup,
                'auto_warmup': config.auto_warmup,
                'optimization_enabled': config.optimization_enabled,
                'strict_mode': config.strict_mode
            })
            
            # conda í™˜ê²½ ì„¤ì •
            if config.conda_optimized and CONDA_INFO['is_target_env']:
                dependencies.update({
                    'conda_optimized': True,
                    'conda_env': CONDA_INFO['conda_env']
                })
            
            # M3 Max ì„¤ì •
            if config.m3_max_optimized and IS_M3_MAX:
                dependencies.update({
                    'm3_max_optimized': True,
                    'memory_gb': MEMORY_GB,
                    'use_unified_memory': True,
                    'is_m3_max': True
                })
            
            # ì˜ì¡´ì„± ì»´í¬ë„ŒíŠ¸ë“¤ í•´ê²°
            if config.require_model_loader:
                model_loader = self._resolve_model_loader()
                if model_loader:
                    dependencies['model_loader'] = model_loader
                    self.logger.info(f"âœ… {config.step_name} ModelLoader ìƒì„±ì ì£¼ì… ì¤€ë¹„")
                else:
                    self.logger.warning(f"âš ï¸ {config.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨")
                    if config.strict_mode:
                        raise RuntimeError("Strict Mode: ModelLoader í•„ìˆ˜ì´ì§€ë§Œ í•´ê²° ì‹¤íŒ¨")
            
            if config.require_memory_manager:
                memory_manager = self._resolve_memory_manager()
                if memory_manager:
                    dependencies['memory_manager'] = memory_manager
                    self.logger.info(f"âœ… {config.step_name} MemoryManager ìƒì„±ì ì£¼ì… ì¤€ë¹„")
            
            if config.require_data_converter:
                data_converter = self._resolve_data_converter()
                if data_converter:
                    dependencies['data_converter'] = data_converter
                    self.logger.info(f"âœ… {config.step_name} DataConverter ìƒì„±ì ì£¼ì… ì¤€ë¹„")
            
            if config.require_di_container:
                di_container = self._resolve_di_container()
                if di_container:
                    dependencies['di_container'] = di_container
                    self.logger.info(f"âœ… {config.step_name} DIContainer ìƒì„±ì ì£¼ì… ì¤€ë¹„")
            
            if config.require_unified_dependency_manager:
                unified_dep_manager = self._resolve_unified_dependency_manager()
                if unified_dep_manager:
                    dependencies['unified_dependency_manager'] = unified_dep_manager
                    self.logger.info(f"âœ… {config.step_name} UnifiedDependencyManager ìƒì„±ì ì£¼ì… ì¤€ë¹„")
            
            # AI ëª¨ë¸ ì„¤ì •
            dependencies['ai_models'] = config.ai_models
            dependencies['model_size_gb'] = config.model_size_gb
            
            resolved_count = len([v for v in dependencies.values() if v is not None])
            self.logger.info(f"âœ… {config.step_name} ìƒì„±ììš© ì˜ì¡´ì„± í•´ê²° ì™„ë£Œ: {resolved_count}ê°œ í•­ëª©")
            
            return dependencies
            
        except Exception as e:
            self.logger.error(f"âŒ {config.step_name} ìƒì„±ììš© ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
            return {}
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²°"""
        if device != "auto":
            return device
        
        if IS_M3_MAX:
            return "mps"
        
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
        except ImportError:
            pass
        
        return "cpu"
    
    def _resolve_model_loader(self) -> Optional['ModelLoader']:
        """ModelLoader í•´ê²°"""
        try:
            with self._lock:
                cache_key = "model_loader"
                if cache_key in self._resolved_cache:
                    return self._resolved_cache[cache_key]
                
                # í•´ê²° ì‹œë„ ì œí•œ
                attempts = self._resolution_attempts.get(cache_key, 0)
                if attempts >= self._max_attempts:
                    self.logger.warning(f"ModelLoader í•´ê²° ì‹œë„ í•œê³„ ì´ˆê³¼: {attempts}")
                    return None
                
                self._resolution_attempts[cache_key] = attempts + 1
                
                try:
                    # ì ˆëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    from app.ai_pipeline.utils.model_loader import get_global_model_loader
                    model_loader = get_global_model_loader()
                    
                    if model_loader:
                        # conda í™˜ê²½ ìµœì í™” ì„¤ì •
                        if CONDA_INFO['is_target_env'] and hasattr(model_loader, 'configure'):
                            config = {
                                'conda_optimized': True,
                                'conda_env': CONDA_INFO['conda_env'],
                                'm3_max_optimized': IS_M3_MAX,
                                'memory_gb': MEMORY_GB
                            }
                            model_loader.configure(config)
                        
                        self._resolved_cache[cache_key] = model_loader
                        self.logger.info("âœ… ModelLoader í•´ê²° ì™„ë£Œ")
                        return model_loader
                    
                except ImportError as e:
                    # ìƒëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        from ..utils.model_loader import get_global_model_loader
                        model_loader = get_global_model_loader()
                        if model_loader:
                            self._resolved_cache[cache_key] = model_loader
                            self.logger.info("âœ… ModelLoader í•´ê²° ì™„ë£Œ (ìƒëŒ€ ê²½ë¡œ)")
                            return model_loader
                    except ImportError as e2:
                        self.logger.debug(f"ModelLoader import ì‹¤íŒ¨ (ì ˆëŒ€/ìƒëŒ€): {e}, {e2}")
                        return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_memory_manager(self) -> Optional['MemoryManager']:
        """MemoryManager í•´ê²°"""
        try:
            with self._lock:
                cache_key = "memory_manager"
                if cache_key in self._resolved_cache:
                    return self._resolved_cache[cache_key]
                
                try:
                    # ì ˆëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    from app.ai_pipeline.utils.memory_manager import get_global_memory_manager
                    memory_manager = get_global_memory_manager()
                    
                    if memory_manager:
                        # M3 Max ìµœì í™”
                        if IS_M3_MAX and hasattr(memory_manager, 'configure_m3_max'):
                            memory_manager.configure_m3_max(memory_gb=MEMORY_GB)
                        
                        self._resolved_cache[cache_key] = memory_manager
                        self.logger.info("âœ… MemoryManager í•´ê²° ì™„ë£Œ")
                        return memory_manager
                        
                except ImportError:
                    # ìƒëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        from ..utils.memory_manager import get_global_memory_manager
                        memory_manager = get_global_memory_manager()
                        if memory_manager:
                            self._resolved_cache[cache_key] = memory_manager
                            self.logger.info("âœ… MemoryManager í•´ê²° ì™„ë£Œ (ìƒëŒ€ ê²½ë¡œ)")
                            return memory_manager
                    except ImportError:
                        return None
                    
        except Exception as e:
            self.logger.debug(f"MemoryManager í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_data_converter(self) -> Optional['DataConverter']:
        """DataConverter í•´ê²°"""
        try:
            with self._lock:
                cache_key = "data_converter"
                if cache_key in self._resolved_cache:
                    return self._resolved_cache[cache_key]
                
                try:
                    # ì ˆëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    from app.ai_pipeline.utils.data_converter import get_global_data_converter
                    data_converter = get_global_data_converter()
                    if data_converter:
                        self._resolved_cache[cache_key] = data_converter
                        self.logger.info("âœ… DataConverter í•´ê²° ì™„ë£Œ")
                        return data_converter
                        
                except ImportError:
                    # ìƒëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        from ..utils.data_converter import get_global_data_converter
                        data_converter = get_global_data_converter()
                        if data_converter:
                            self._resolved_cache[cache_key] = data_converter
                            self.logger.info("âœ… DataConverter í•´ê²° ì™„ë£Œ (ìƒëŒ€ ê²½ë¡œ)")
                            return data_converter
                    except ImportError:
                        return None
                    
        except Exception as e:
            self.logger.debug(f"DataConverter í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_di_container(self) -> Optional['DIContainer']:
        """DI Container í•´ê²°"""
        try:
            with self._lock:
                cache_key = "di_container"
                if cache_key in self._resolved_cache:
                    return self._resolved_cache[cache_key]
                
                try:
                    # ì ˆëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    from app.core.di_container import get_global_di_container
                    di_container = get_global_di_container()
                    if di_container:
                        self._resolved_cache[cache_key] = di_container
                        self.logger.info("âœ… DIContainer í•´ê²° ì™„ë£Œ")
                        return di_container
                        
                except ImportError:
                    # ìƒëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        from ...core.di_container import get_global_di_container
                        di_container = get_global_di_container()
                        if di_container:
                            self._resolved_cache[cache_key] = di_container
                            self.logger.info("âœ… DIContainer í•´ê²° ì™„ë£Œ (ìƒëŒ€ ê²½ë¡œ)")
                            return di_container
                    except ImportError:
                        return None
                    
        except Exception as e:
            self.logger.debug(f"DIContainer í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_unified_dependency_manager(self) -> Optional[Any]:
        """UnifiedDependencyManager í•´ê²°"""
        try:
            with self._lock:
                cache_key = "unified_dependency_manager"
                if cache_key in self._resolved_cache:
                    return self._resolved_cache[cache_key]
                
                try:
                    # BaseStepMixinì—ì„œ ì‚¬ìš©í•˜ëŠ” UnifiedDependencyManager ìƒì„±
                    # ğŸ”¥ ì ˆëŒ€ ê²½ë¡œ ì‹œë„
                    try:
                        from app.ai_pipeline.steps.base_step_mixin import UnifiedDependencyManager
                    except ImportError:
                        # ìƒëŒ€ ê²½ë¡œ ì‹œë„
                        from ..steps.base_step_mixin import UnifiedDependencyManager
                    
                    # í˜„ì¬ í•´ê²°ëœ ì˜ì¡´ì„±ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ë‹ˆì € ìƒì„±
                    unified_manager = UnifiedDependencyManager(
                        step_name="GlobalStepFactory",
                        is_m3_max=IS_M3_MAX,
                        memory_gb=MEMORY_GB,
                        conda_info=CONDA_INFO
                    )
                    
                    self._resolved_cache[cache_key] = unified_manager
                    self.logger.info("âœ… UnifiedDependencyManager í•´ê²° ì™„ë£Œ")
                    return unified_manager
                    
                except ImportError:
                    self.logger.debug("UnifiedDependencyManager import ì‹¤íŒ¨")
                    # í´ë°±: ê°„ë‹¨í•œ Mock ê°ì²´ ìƒì„±
                    class MockUnifiedDependencyManager:
                        def __init__(self, **kwargs):
                            for key, value in kwargs.items():
                                setattr(self, key, value)
                    
                    mock_manager = MockUnifiedDependencyManager(
                        step_name="GlobalStepFactory",
                        is_m3_max=IS_M3_MAX,
                        memory_gb=MEMORY_GB,
                        conda_info=CONDA_INFO
                    )
                    self._resolved_cache[cache_key] = mock_manager
                    self.logger.info("âœ… UnifiedDependencyManager í•´ê²° ì™„ë£Œ (Mock)")
                    return mock_manager
                    
        except Exception as e:
            self.logger.debug(f"UnifiedDependencyManager í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        with self._lock:
            self._resolved_cache.clear()
            self._resolution_attempts.clear()
            gc.collect()

# ==============================================
# ğŸ”¥ ë™ì  Step í´ë˜ìŠ¤ ë¡œë” (ê°œì„ ë¨)
# ==============================================

class BaseStepMixinClassLoader:
    """BaseStepMixin í˜¸í™˜ ë™ì  Step í´ë˜ìŠ¤ ë¡œë”"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.BaseStepMixinClassLoader")
        self._loaded_classes: Dict[str, Type] = {}
        self._import_attempts: Dict[str, int] = {}
        self._lock = threading.RLock()
        self._max_attempts = 5
    
    def load_step_class(self, config: BaseStepMixinConfig) -> Optional[Type]:
        """BaseStepMixin í˜¸í™˜ Step í´ë˜ìŠ¤ ë¡œë”©"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                cache_key = config.class_name
                if cache_key in self._loaded_classes:
                    return self._loaded_classes[cache_key]
                
                # ì¬ì‹œë„ ì œí•œ
                attempts = self._import_attempts.get(cache_key, 0)
                if attempts >= self._max_attempts:
                    self.logger.error(f"âŒ {config.class_name} import ì¬ì‹œë„ í•œê³„ ì´ˆê³¼")
                    return None
                
                self._import_attempts[cache_key] = attempts + 1
                
                self.logger.info(f"ğŸ”„ {config.class_name} ë™ì  ë¡œë”© ì‹œì‘ (ì‹œë„ {attempts + 1}/{self._max_attempts})...")
                
                # ë™ì  import ì‹¤í–‰
                step_class = self._dynamic_import_step_class(config)
                
                if step_class:
                    # BaseStepMixin í˜¸í™˜ì„± ê²€ì¦
                    if self._validate_basestepmixin_compatibility(step_class, config):
                        self._loaded_classes[cache_key] = step_class
                        self.logger.info(f"âœ… {config.class_name} ë™ì  ë¡œë”© ì„±ê³µ (BaseStepMixin í˜¸í™˜)")
                        return step_class
                    else:
                        self.logger.error(f"âŒ {config.class_name} BaseStepMixin í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨")
                        return None
                else:
                    self.logger.error(f"âŒ {config.class_name} ë™ì  import ì‹¤íŒ¨")
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ {config.class_name} ë™ì  ë¡œë”© ì˜ˆì™¸: {e}")
            return None
    
    def _dynamic_import_step_class(self, config: BaseStepMixinConfig) -> Optional[Type]:
        """ë™ì  import ì‹¤í–‰"""
        import importlib
        
        # ê¸°ë³¸ ëª¨ë“ˆ ê²½ë¡œ
        base_module = config.module_path
        
        # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
        import_paths = [
            base_module,
            f"app.ai_pipeline.steps.{config.module_path.split('.')[-1]}",  # ì ˆëŒ€ ê²½ë¡œ ìš°ì„ 
            f"ai_pipeline.steps.{config.module_path.split('.')[-1]}",
            f"backend.{base_module}",
            f"..steps.{config.module_path.split('.')[-1]}",
            # ğŸ”¥ ì¶”ê°€ ëŒ€ì•ˆ ê²½ë¡œë“¤
            f"backend.app.ai_pipeline.steps.{config.module_path.split('.')[-1]}",
            f"app.ai_pipeline.steps.step_{config.step_id:02d}_{config.step_type.value}",
            f"steps.{config.class_name.lower()}"
        ]
        
        for import_path in import_paths:
            try:
                self.logger.debug(f"ğŸ” {config.class_name} import ì‹œë„: {import_path}")
                
                # ë™ì  ëª¨ë“ˆ import
                module = importlib.import_module(import_path)
                
                # í´ë˜ìŠ¤ ì¶”ì¶œ
                if hasattr(module, config.class_name):
                    step_class = getattr(module, config.class_name)
                    self.logger.info(f"âœ… {config.class_name} ë™ì  import ì„±ê³µ: {import_path}")
                    return step_class
                else:
                    self.logger.debug(f"âš ï¸ {import_path}ì— {config.class_name} í´ë˜ìŠ¤ ì—†ìŒ")
                    continue
                    
            except ImportError as e:
                self.logger.debug(f"âš ï¸ {import_path} import ì‹¤íŒ¨: {e}")
                continue
            except Exception as e:
                self.logger.warning(f"âš ï¸ {import_path} import ì˜ˆì™¸: {e}")
                continue
        
        self.logger.error(f"âŒ {config.class_name} ëª¨ë“  ê²½ë¡œì—ì„œ import ì‹¤íŒ¨")
        return None
    
    def _validate_basestepmixin_compatibility(self, step_class: Type, config: BaseStepMixinConfig) -> bool:
        """BaseStepMixin v18.0 í˜¸í™˜ì„± ê²€ì¦"""
        try:
            # ê¸°ë³¸ í´ë˜ìŠ¤ ê²€ì¦
            if not step_class or step_class.__name__ != config.class_name:
                return False
            
            # BaseStepMixin ìƒì† í™•ì¸
            mro_names = [cls.__name__ for cls in step_class.__mro__]
            if 'BaseStepMixin' not in mro_names:
                self.logger.warning(f"âš ï¸ {config.class_name}ì´ BaseStepMixinì„ ìƒì†í•˜ì§€ ì•ŠìŒ")
                # BaseStepMixin ë¯¸ìƒì†ë„ í—ˆìš© (í´ë°± ì§€ì›)
            
            # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            required_methods = ['process', 'initialize']
            missing_methods = []
            for method in required_methods:
                if not hasattr(step_class, method):
                    missing_methods.append(method)
            
            if missing_methods:
                self.logger.error(f"âŒ {config.class_name}ì— í•„ìˆ˜ ë©”ì„œë“œ ì—†ìŒ: {missing_methods}")
                return False
            
            # process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ê²€ì¦
            process_method = getattr(step_class, 'process')
            if not self._validate_process_method_signature(process_method, config):
                self.logger.warning(f"âš ï¸ {config.class_name} process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¹„í‘œì¤€")
                # ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
            
            # ìƒì„±ì í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (BaseStepMixin í‘œì¤€ kwargs)
            try:
                test_kwargs = {
                    'step_name': 'test',
                    'step_id': config.step_id,
                    'device': 'cpu'
                }
                test_instance = step_class(**test_kwargs)
                if test_instance:
                    self.logger.debug(f"âœ… {config.class_name} BaseStepMixin ìƒì„±ì í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                    # ì •ë¦¬
                    if hasattr(test_instance, 'cleanup'):
                        try:
                            if asyncio.iscoroutinefunction(test_instance.cleanup):
                                # ë¹„ë™ê¸° cleanupì€ ìŠ¤í‚µ (í…ŒìŠ¤íŠ¸ì—ì„œ)
                                pass
                            else:
                                test_instance.cleanup()
                        except:
                            pass
                    del test_instance
                    return True
            except Exception as e:
                self.logger.warning(f"âš ï¸ {config.class_name} ìƒì„±ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                # ğŸ”¥ ëŒ€ì•ˆ í…ŒìŠ¤íŠ¸: ë§¤ê°œë³€ìˆ˜ ì—†ì´ ì‹œë„
                try:
                    test_instance = step_class()
                    if test_instance:
                        self.logger.debug(f"âœ… {config.class_name} ê¸°ë³¸ ìƒì„±ì í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                        del test_instance
                        return True
                except Exception as e2:
                    self.logger.debug(f"ê¸°ë³¸ ìƒì„±ìë„ ì‹¤íŒ¨: {e2}")
                # ìƒì„±ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ëŸ°íƒ€ì„ì—ì„œ ì¬ì‹œë„)
                return True
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {config.class_name} BaseStepMixin í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _validate_process_method_signature(self, process_method, config: BaseStepMixinConfig) -> bool:
        """process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ê²€ì¦"""
        try:
            import inspect
            
            signature = inspect.signature(process_method)
            params = list(signature.parameters.keys())
            
            # ê¸°ë³¸ì ìœ¼ë¡œ self, input_data íŒŒë¼ë¯¸í„°ê°€ ìˆì–´ì•¼ í•¨
            expected_params = ['self', 'input_data']
            for expected in expected_params:
                if expected not in params:
                    self.logger.debug(f"process ë©”ì„œë“œì— {expected} íŒŒë¼ë¯¸í„° ì—†ìŒ")
                    return False
            
            # async í•¨ìˆ˜ì¸ì§€ í™•ì¸
            if not inspect.iscoroutinefunction(process_method):
                self.logger.debug(f"{config.class_name} process ë©”ì„œë“œê°€ asyncê°€ ì•„ë‹˜")
                # sync í•¨ìˆ˜ë„ í—ˆìš©
            
            return True
            
        except Exception as e:
            self.logger.debug(f"process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ ë©”ì¸ StepFactory v9.0 (BaseStepMixin ì™„ì „ í˜¸í™˜)
# ==============================================

class StepFactory:
    """
    ğŸ”¥ StepFactory v9.0 - BaseStepMixin ì™„ì „ í˜¸í™˜ (Option A êµ¬í˜„)
    
    í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
    - BaseStepMixin v18.0 í‘œì¤€ ì™„ì „ í˜¸í™˜
    - ìƒì„±ì ì‹œì  ì˜ì¡´ì„± ì£¼ì… (constructor injection)
    - process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í‘œì¤€í™”
    - UnifiedDependencyManager ì™„ì „ í™œìš©
    - conda í™˜ê²½ ìš°ì„  ìµœì í™”
    """
    
    def __init__(self):
        self.logger = logging.getLogger("StepFactory.v9")
        
        # BaseStepMixin í˜¸í™˜ ì»´í¬ë„ŒíŠ¸ë“¤
        self.class_loader = BaseStepMixinClassLoader()
        self.dependency_resolver = BaseStepMixinDependencyResolver()
        
        # ìºì‹œ ê´€ë¦¬
        self._step_cache: Dict[str, weakref.ref] = {}
        self._lock = threading.RLock()
        
        # í†µê³„
        self._stats = {
            'total_created': 0,
            'successful_creations': 0,
            'failed_creations': 0,
            'cache_hits': 0,
            'basestepmixin_compatible_creations': 0,
            'dependency_injection_successes': 0,
            'conda_optimized': CONDA_INFO['is_target_env'],
            'm3_max_optimized': IS_M3_MAX
        }
        
        self.logger.info("ğŸ­ StepFactory v9.0 ì´ˆê¸°í™” ì™„ë£Œ (BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜)")
    
    def create_step(
        self,
        step_type: Union[StepType, str],
        use_cache: bool = True,
        **kwargs
    ) -> StepCreationResult:
        """Step ìƒì„± ë©”ì¸ ë©”ì„œë“œ (BaseStepMixin í˜¸í™˜)"""
        start_time = time.time()
        
        try:
            # Step íƒ€ì… ì •ê·œí™”
            if isinstance(step_type, str):
                try:
                    step_type = StepType(step_type.lower())
                except ValueError:
                    return StepCreationResult(
                        success=False,
                        error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Step íƒ€ì…: {step_type}",
                        creation_time=time.time() - start_time
                    )
            
            # BaseStepMixin í˜¸í™˜ ì„¤ì • ìƒì„±
            config = BaseStepMixinMapping.get_config(step_type, **kwargs)
            
            self.logger.info(f"ğŸ¯ {config.step_name} ìƒì„± ì‹œì‘ (BaseStepMixin v18.0 í˜¸í™˜)...")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            with self._lock:
                self._stats['total_created'] += 1
            
            # ìºì‹œ í™•ì¸
            if use_cache:
                cached_step = self._get_cached_step(config.step_name)
                if cached_step:
                    with self._lock:
                        self._stats['cache_hits'] += 1
                    self.logger.info(f"â™»ï¸ {config.step_name} ìºì‹œì—ì„œ ë°˜í™˜")
                    return StepCreationResult(
                        success=True,
                        step_instance=cached_step,
                        step_name=config.step_name,
                        step_type=step_type,
                        class_name=config.class_name,
                        module_path=config.module_path,
                        creation_time=time.time() - start_time,
                        basestepmixin_compatible=True
                    )
            
            # ì‹¤ì œ Step ìƒì„± (BaseStepMixin í˜¸í™˜)
            result = self._create_basestepmixin_step_instance(config)
            
            # ì„±ê³µ ì‹œ ìºì‹œì— ì €ì¥
            if result.success and result.step_instance and use_cache:
                self._cache_step(config.step_name, result.step_instance)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            with self._lock:
                if result.success:
                    self._stats['successful_creations'] += 1
                    if result.basestepmixin_compatible:
                        self._stats['basestepmixin_compatible_creations'] += 1
                    if result.dependency_injection_success:
                        self._stats['dependency_injection_successes'] += 1
                else:
                    self._stats['failed_creations'] += 1
            
            result.creation_time = time.time() - start_time
            return result
            
        except Exception as e:
            with self._lock:
                self._stats['failed_creations'] += 1
            
            self.logger.error(f"âŒ Step ìƒì„± ì‹¤íŒ¨: {e}")
            return StepCreationResult(
                success=False,
                error_message=f"Step ìƒì„± ì˜ˆì™¸: {str(e)}",
                creation_time=time.time() - start_time
            )
    
    def _create_basestepmixin_step_instance(self, config: BaseStepMixinConfig) -> StepCreationResult:
        """BaseStepMixin í˜¸í™˜ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í•µì‹¬ ë©”ì„œë“œ)"""
        try:
            self.logger.info(f"ğŸ”„ {config.step_name} BaseStepMixin í˜¸í™˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
            
            # 1. Step í´ë˜ìŠ¤ ë¡œë”©
            StepClass = self.class_loader.load_step_class(config)
            if not StepClass:
                return StepCreationResult(
                    success=False,
                    step_name=config.step_name,
                    step_type=config.step_type,
                    class_name=config.class_name,
                    module_path=config.module_path,
                    error_message=f"{config.class_name} í´ë˜ìŠ¤ ë¡œë”© ì‹¤íŒ¨"
                )
            
            self.logger.info(f"âœ… {config.class_name} í´ë˜ìŠ¤ ë¡œë”© ì™„ë£Œ")
            
            # 2. ìƒì„±ììš© ì˜ì¡´ì„± í•´ê²° (í•µì‹¬: ìƒì„±ì ì‹œì  ì£¼ì…)
            constructor_dependencies = self.dependency_resolver.resolve_dependencies_for_constructor(config)
            
            # 3. BaseStepMixin í‘œì¤€ ìƒì„±ì í˜¸ì¶œ (**kwargs íŒ¨í„´)
            self.logger.info(f"ğŸ”„ {config.class_name} BaseStepMixin ìƒì„±ì í˜¸ì¶œ ì¤‘...")
            step_instance = StepClass(**constructor_dependencies)
            self.logger.info(f"âœ… {config.class_name} ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (ìƒì„±ì ì˜ì¡´ì„± ì£¼ì…)")
            
            # 4. ì´ˆê¸°í™” ì‹¤í–‰ (ë™ê¸°/ë¹„ë™ê¸° ìë™ ê°ì§€)
            initialization_success = self._initialize_basestepmixin_step(step_instance, config)
            
            # 5. BaseStepMixin í˜¸í™˜ì„± ìµœì¢… ê²€ì¦
            compatibility_result = self._verify_basestepmixin_compatibility(step_instance, config)
            
            # 6. AI ëª¨ë¸ ë¡œë”© í™•ì¸
            ai_models_loaded = self._check_ai_models_basestepmixin(step_instance, config)
            
            self.logger.info(f"âœ… {config.step_name} BaseStepMixin í˜¸í™˜ ìƒì„± ì™„ë£Œ")
            
            return StepCreationResult(
                success=True,
                step_instance=step_instance,
                step_name=config.step_name,
                step_type=config.step_type,
                class_name=config.class_name,
                module_path=config.module_path,
                dependencies_injected={'constructor_injection': True},
                initialization_success=initialization_success,
                ai_models_loaded=ai_models_loaded,
                basestepmixin_compatible=compatibility_result['compatible'],
                process_method_validated=compatibility_result['process_method_valid'],
                dependency_injection_success=True
            )
            
        except Exception as e:
            self.logger.error(f"âŒ {config.step_name} BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return StepCreationResult(
                success=False,
                step_name=config.step_name,
                step_type=config.step_type,
                class_name=config.class_name,
                module_path=config.module_path,
                error_message=f"BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                basestepmixin_compatible=False
            )
    
    def _initialize_basestepmixin_step(self, step_instance: 'BaseStepMixin', config: BaseStepMixinConfig) -> bool:
        """BaseStepMixin Step ì´ˆê¸°í™” (ë™ê¸°/ë¹„ë™ê¸° ìë™ ê°ì§€)"""
        try:
            # BaseStepMixin initialize ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(step_instance, 'initialize'):
                initialize_method = step_instance.initialize
                
                # ğŸ”¥ ë™ê¸°/ë¹„ë™ê¸° ìë™ ê°ì§€ ë° ì²˜ë¦¬
                if asyncio.iscoroutinefunction(initialize_method):
                    # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš°
                    try:
                        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                        loop = asyncio.get_running_loop()
                        
                        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„± í›„ ë¸”ë¡œí‚¹ ëŒ€ê¸°
                        if loop.is_running():
                            # ìƒˆë¡œìš´ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
                            import concurrent.futures
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(asyncio.run, initialize_method())
                                success = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                        else:
                            # ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë©´ ì§ì ‘ ì‹¤í–‰
                            success = asyncio.run(initialize_method())
                    except RuntimeError:
                        # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
                        success = asyncio.run(initialize_method())
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {config.step_name} ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨, ë™ê¸° ë°©ì‹ ì‹œë„: {e}")
                        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í´ë°± (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„)
                        success = self._fallback_sync_initialize(step_instance, config)
                else:
                    # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš°
                    success = initialize_method()
                
                if success:
                    self.logger.info(f"âœ… {config.step_name} BaseStepMixin ì´ˆê¸°í™” ì™„ë£Œ")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {config.step_name} BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return False
            else:
                self.logger.debug(f"â„¹ï¸ {config.step_name} initialize ë©”ì„œë“œ ì—†ìŒ")
                return True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {config.step_name} ì´ˆê¸°í™” ì˜ˆì™¸: {e}")
            # ì˜ˆì™¸ ë°œìƒ ì‹œ í´ë°± ì´ˆê¸°í™” ì‹œë„
            return self._fallback_sync_initialize(step_instance, config)
    
    def _fallback_sync_initialize(self, step_instance: 'BaseStepMixin', config: BaseStepMixinConfig) -> bool:
        """í´ë°± ë™ê¸° ì´ˆê¸°í™” (ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)"""
        try:
            self.logger.info(f"ğŸ”„ {config.step_name} í´ë°± ë™ê¸° ì´ˆê¸°í™” ì‹œë„...")
            
            # ê¸°ë³¸ ì†ì„±ë“¤ ìˆ˜ë™ ì„¤ì •
            if hasattr(step_instance, 'is_initialized'):
                step_instance.is_initialized = True
            
            if hasattr(step_instance, 'is_ready'):
                step_instance.is_ready = True
                
            # ì˜ì¡´ì„±ì´ ì œëŒ€ë¡œ ì£¼ì…ë˜ì—ˆëŠ”ì§€ í™•ì¸
            dependencies_ok = True
            if config.require_model_loader and not hasattr(step_instance, 'model_loader'):
                dependencies_ok = False
                
            if dependencies_ok:
                self.logger.info(f"âœ… {config.step_name} í´ë°± ë™ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                return True
            else:
                self.logger.warning(f"âš ï¸ {config.step_name} í´ë°± ì´ˆê¸°í™”: ì˜ì¡´ì„± ë¬¸ì œ ìˆìŒ")
                return not config.strict_mode  # strict_modeê°€ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                
        except Exception as e:
            self.logger.error(f"âŒ {config.step_name} í´ë°± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _verify_basestepmixin_compatibility(self, step_instance: 'BaseStepMixin', config: BaseStepMixinConfig) -> Dict[str, Any]:
        """BaseStepMixin í˜¸í™˜ì„± ìµœì¢… ê²€ì¦"""
        try:
            result = {
                'compatible': True,
                'process_method_valid': False,
                'issues': []
            }
            
            # process ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
            if not hasattr(step_instance, 'process'):
                result['compatible'] = False
                result['issues'].append('process ë©”ì„œë“œ ì—†ìŒ')
            else:
                result['process_method_valid'] = True
            
            # BaseStepMixin ì†ì„± í™•ì¸
            expected_attrs = ['step_name', 'step_id', 'device', 'is_initialized']
            for attr in expected_attrs:
                if not hasattr(step_instance, attr):
                    result['issues'].append(f'{attr} ì†ì„± ì—†ìŒ')
            
            # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
            if hasattr(step_instance, 'model_loader') and step_instance.model_loader:
                self.logger.debug(f"âœ… {config.step_name} ModelLoader ì£¼ì… í™•ì¸ë¨")
            
            if result['issues']:
                self.logger.warning(f"âš ï¸ {config.step_name} BaseStepMixin í˜¸í™˜ì„± ì´ìŠˆ: {result['issues']}")
            else:
                self.logger.info(f"âœ… {config.step_name} BaseStepMixin í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {config.step_name} BaseStepMixin í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'compatible': False, 'process_method_valid': False, 'issues': [str(e)]}
    
    def _check_ai_models_basestepmixin(self, step_instance: 'BaseStepMixin', config: BaseStepMixinConfig) -> List[str]:
        """AI ëª¨ë¸ ë¡œë”© í™•ì¸ (BaseStepMixin í˜¸í™˜)"""
        loaded_models = []
        
        try:
            # ModelLoader ë¥¼ í†µí•œ ëª¨ë¸ í™•ì¸
            if hasattr(step_instance, 'model_loader') and step_instance.model_loader:
                for model_name in config.ai_models:
                    try:
                        if hasattr(step_instance.model_loader, 'is_model_loaded'):
                            if step_instance.model_loader.is_model_loaded(model_name):
                                loaded_models.append(model_name)
                    except Exception:
                        pass
            
            # model_interface ë¥¼ í†µí•œ ëª¨ë¸ í™•ì¸
            if hasattr(step_instance, 'model_interface') and step_instance.model_interface:
                for model_name in config.ai_models:
                    try:
                        if hasattr(step_instance.model_interface, 'is_model_available'):
                            if step_instance.model_interface.is_model_available(model_name):
                                loaded_models.append(model_name)
                    except Exception:
                        pass
            
            if loaded_models:
                self.logger.info(f"ğŸ¤– {config.step_name} AI ëª¨ë¸ ë¡œë”© í™•ì¸: {loaded_models}")
            
            return loaded_models
            
        except Exception as e:
            self.logger.debug(f"AI ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_cached_step(self, step_name: str) -> Optional['BaseStepMixin']:
        """ìºì‹œëœ Step ë°˜í™˜"""
        try:
            with self._lock:
                if step_name in self._step_cache:
                    weak_ref = self._step_cache[step_name]
                    step_instance = weak_ref()
                    if step_instance is not None:
                        return step_instance
                    else:
                        del self._step_cache[step_name]
                return None
        except Exception:
            return None
    
    def _cache_step(self, step_name: str, step_instance: 'BaseStepMixin'):
        """Step ìºì‹œì— ì €ì¥"""
        try:
            with self._lock:
                self._step_cache[step_name] = weakref.ref(step_instance)
        except Exception:
            pass
    
    # ==============================================
    # ğŸ”¥ í¸ì˜ ë©”ì„œë“œë“¤ (BaseStepMixin í˜¸í™˜)
    # ==============================================
    
    def create_human_parsing_step(self, **kwargs) -> StepCreationResult:
        """Human Parsing Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.HUMAN_PARSING, **kwargs)
    
    def create_pose_estimation_step(self, **kwargs) -> StepCreationResult:
        """Pose Estimation Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.POSE_ESTIMATION, **kwargs)
    
    def create_cloth_segmentation_step(self, **kwargs) -> StepCreationResult:
        """Cloth Segmentation Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.CLOTH_SEGMENTATION, **kwargs)
    
    def create_geometric_matching_step(self, **kwargs) -> StepCreationResult:
        """Geometric Matching Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.GEOMETRIC_MATCHING, **kwargs)
    
    def create_cloth_warping_step(self, **kwargs) -> StepCreationResult:
        """Cloth Warping Step ìƒì„± (BaseStepMixin í˜¸compat)"""
        return self.create_step(StepType.CLOTH_WARPING, **kwargs)
    
    def create_virtual_fitting_step(self, **kwargs) -> StepCreationResult:
        """Virtual Fitting Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.VIRTUAL_FITTING, **kwargs)
    
    def create_post_processing_step(self, **kwargs) -> StepCreationResult:
        """Post Processing Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.POST_PROCESSING, **kwargs)
    
    def create_quality_assessment_step(self, **kwargs) -> StepCreationResult:
        """Quality Assessment Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
        return self.create_step(StepType.QUALITY_ASSESSMENT, **kwargs)
    
    def create_full_pipeline(self, device: str = "auto", **kwargs) -> Dict[str, StepCreationResult]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒì„± (BaseStepMixin í˜¸í™˜) - ë™ê¸° ë©”ì„œë“œ"""
        try:
            self.logger.info("ğŸš€ ì „ì²´ AI íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œì‘ (BaseStepMixin v18.0 í˜¸í™˜)...")
            
            pipeline_results = {}
            total_model_size = 0.0
            
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ Step ìƒì„±
            sorted_steps = sorted(
                StepType,
                key=lambda x: BaseStepMixinMapping.STEP_CONFIGS[x].priority.value
            )
            
            for step_type in sorted_steps:
                try:
                    result = self.create_step(step_type, device=device, **kwargs)
                    pipeline_results[step_type.value] = result
                    
                    if result.success:
                        config = BaseStepMixinMapping.get_config(step_type)
                        total_model_size += config.model_size_gb
                        self.logger.info(f"âœ… {result.step_name} íŒŒì´í”„ë¼ì¸ ìƒì„± ì„±ê³µ (BaseStepMixin í˜¸í™˜)")
                    else:
                        self.logger.warning(f"âš ï¸ {step_type.value} íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    self.logger.error(f"âŒ {step_type.value} Step ìƒì„± ì˜ˆì™¸: {e}")
                    pipeline_results[step_type.value] = StepCreationResult(
                        success=False,
                        step_name=f"{step_type.value}Step",
                        step_type=step_type,
                        error_message=str(e)
                    )
            
            success_count = sum(1 for result in pipeline_results.values() if result.success)
            total_count = len(pipeline_results)
            
            self.logger.info(f"ğŸ BaseStepMixin í˜¸í™˜ íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")
            self.logger.info(f"ğŸ¤– ì´ AI ëª¨ë¸ í¬ê¸°: {total_model_size:.1f}GB")
            
            return pipeline_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜ (BaseStepMixin í˜¸í™˜ì„± í¬í•¨)"""
        with self._lock:
            total = self._stats['total_created']
            success_rate = (self._stats['successful_creations'] / max(1, total)) * 100
            basestepmixin_compatibility_rate = (self._stats['basestepmixin_compatible_creations'] / max(1, self._stats['successful_creations'])) * 100
            
            return {
                'version': 'StepFactory v9.0 (BaseStepMixin Complete Compatibility)',
                'total_created': total,
                'successful_creations': self._stats['successful_creations'],
                'failed_creations': self._stats['failed_creations'],
                'success_rate': round(success_rate, 2),
                'cache_hits': self._stats['cache_hits'],
                'cached_steps': len(self._step_cache),
                'active_cache_entries': len([
                    ref for ref in self._step_cache.values() if ref() is not None
                ]),
                'basestepmixin_compatibility': {
                    'compatible_creations': self._stats['basestepmixin_compatible_creations'],
                    'compatibility_rate': round(basestepmixin_compatibility_rate, 2),
                    'dependency_injection_successes': self._stats['dependency_injection_successes']
                },
                'environment': {
                    'conda_env': CONDA_INFO['conda_env'],
                    'conda_optimized': self._stats['conda_optimized'],
                    'is_m3_max': IS_M3_MAX,
                    'm3_max_optimized': self._stats['m3_max_optimized'],
                    'memory_gb': MEMORY_GB
                },
                'loaded_classes': self.class_loader._loaded_classes.keys()
            }
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self._lock:
                self._step_cache.clear()
                self.dependency_resolver.clear_cache()
                
                # M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬
                if IS_M3_MAX:
                    try:
                        import torch
                        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                            if hasattr(torch.backends.mps, 'empty_cache'):
                                torch.backends.mps.empty_cache()
                    except:
                        pass
                
                gc.collect()
                self.logger.info("ğŸ§¹ StepFactory v9.0 ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ StepFactory ê´€ë¦¬ (BaseStepMixin í˜¸í™˜)
# ==============================================

_global_step_factory: Optional[StepFactory] = None
_factory_lock = threading.Lock()

def get_global_step_factory() -> StepFactory:
    """ì „ì—­ StepFactory v9.0 ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_step_factory
    
    with _factory_lock:
        if _global_step_factory is None:
            _global_step_factory = StepFactory()
            logger.info("âœ… ì „ì—­ StepFactory v9.0 (BaseStepMixin í˜¸í™˜) ìƒì„± ì™„ë£Œ")
        
        return _global_step_factory

def reset_global_step_factory():
    """ì „ì—­ StepFactory ë¦¬ì…‹"""
    global _global_step_factory
    
    with _factory_lock:
        if _global_step_factory:
            _global_step_factory.clear_cache()
        _global_step_factory = None
        logger.info("ğŸ”„ ì „ì—­ StepFactory v9.0 ë¦¬ì…‹ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜)
# ==============================================

def create_step(step_type: Union[StepType, str], **kwargs) -> StepCreationResult:
    """ì „ì—­ Step ìƒì„± í•¨ìˆ˜ (BaseStepMixin í˜¸í™˜)"""
    factory = get_global_step_factory()
    return factory.create_step(step_type, **kwargs)

def create_human_parsing_step(**kwargs) -> StepCreationResult:
    """Human Parsing Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.HUMAN_PARSING, **kwargs)

def create_pose_estimation_step(**kwargs) -> StepCreationResult:
    """Pose Estimation Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.POSE_ESTIMATION, **kwargs)

def create_cloth_segmentation_step(**kwargs) -> StepCreationResult:
    """Cloth Segmentation Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.CLOTH_SEGMENTATION, **kwargs)

def create_geometric_matching_step(**kwargs) -> StepCreationResult:
    """Geometric Matching Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.GEOMETRIC_MATCHING, **kwargs)

def create_cloth_warping_step(**kwargs) -> StepCreationResult:
    """Cloth Warping Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.CLOTH_WARPING, **kwargs)

def create_virtual_fitting_step(**kwargs) -> StepCreationResult:
    """Virtual Fitting Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.VIRTUAL_FITTING, **kwargs)

def create_post_processing_step(**kwargs) -> StepCreationResult:
    """Post Processing Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.POST_PROCESSING, **kwargs)

def create_quality_assessment_step(**kwargs) -> StepCreationResult:
    """Quality Assessment Step ìƒì„± (BaseStepMixin í˜¸í™˜)"""
    return create_step(StepType.QUALITY_ASSESSMENT, **kwargs)

def create_full_pipeline(device: str = "auto", **kwargs) -> Dict[str, StepCreationResult]:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒì„± (BaseStepMixin í˜¸í™˜) - ë™ê¸° í•¨ìˆ˜"""
    factory = get_global_step_factory()
    return factory.create_full_pipeline(device, **kwargs)

def get_step_factory_statistics() -> Dict[str, Any]:
    """StepFactory í†µê³„ ì¡°íšŒ (BaseStepMixin í˜¸í™˜ì„± í¬í•¨)"""
    factory = get_global_step_factory()
    return factory.get_statistics()

def clear_step_factory_cache():
    """StepFactory ìºì‹œ ì •ë¦¬"""
    factory = get_global_step_factory()
    factory.clear_cache()

# ==============================================
# ğŸ”¥ conda í™˜ê²½ ìµœì í™” (BaseStepMixin í˜¸í™˜)
# ==============================================

def optimize_conda_environment_for_basestepmixin():
    """conda í™˜ê²½ ìµœì í™” (BaseStepMixin í˜¸í™˜)"""
    try:
        if not CONDA_INFO['is_target_env']:
            logger.warning(f"âš ï¸ ê¶Œì¥ conda í™˜ê²½ì´ ì•„ë‹˜: {CONDA_INFO['conda_env']} (ê¶Œì¥: mycloset-ai-clean)")
            return False
        
        # PyTorch conda ìµœì í™”
        try:
            import torch
            if IS_M3_MAX and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                # MPS ìºì‹œ ì •ë¦¬
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                logger.info("ğŸ M3 Max MPS ìµœì í™” í™œì„±í™” (BaseStepMixin í˜¸í™˜)")
            
            # CPU ìŠ¤ë ˆë“œ ìµœì í™”
            cpu_count = os.cpu_count()
            torch.set_num_threads(max(1, cpu_count // 2))
            logger.info(f"ğŸ§µ PyTorch ìŠ¤ë ˆë“œ ìµœì í™”: {torch.get_num_threads()}/{cpu_count}")
            
        except ImportError:
            pass
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['OMP_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        os.environ['MKL_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        
        logger.info("ğŸ conda í™˜ê²½ ìµœì í™” ì™„ë£Œ (BaseStepMixin v18.0 í˜¸í™˜)")
        return True
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# ğŸ”¥ BaseStepMixin í˜¸í™˜ì„± ê²€ì¦ ë„êµ¬
# ==============================================

def validate_basestepmixin_step_compatibility(step_instance: 'BaseStepMixin') -> Dict[str, Any]:
    """BaseStepMixin Step í˜¸í™˜ì„± ê²€ì¦"""
    try:
        result = {
            'compatible': True,
            'version': 'StepFactory v9.0',
            'issues': [],
            'recommendations': []
        }
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attrs = ['step_name', 'step_id', 'device', 'is_initialized']
        for attr in required_attrs:
            if not hasattr(step_instance, attr):
                result['compatible'] = False
                result['issues'].append(f'í•„ìˆ˜ ì†ì„± {attr} ì—†ìŒ')
        
        # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
        required_methods = ['process', 'initialize']
        for method in required_methods:
            if not hasattr(step_instance, method):
                result['compatible'] = False
                result['issues'].append(f'í•„ìˆ˜ ë©”ì„œë“œ {method} ì—†ìŒ')
        
        # BaseStepMixin ìƒì† í™•ì¸
        mro_names = [cls.__name__ for cls in step_instance.__class__.__mro__]
        if 'BaseStepMixin' not in mro_names:
            result['recommendations'].append('BaseStepMixin ìƒì† ê¶Œì¥')
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
        dependency_attrs = ['model_loader', 'memory_manager', 'data_converter']
        injected_deps = []
        for attr in dependency_attrs:
            if hasattr(step_instance, attr) and getattr(step_instance, attr) is not None:
                injected_deps.append(attr)
        
        result['injected_dependencies'] = injected_deps
        result['dependency_injection_score'] = len(injected_deps) / len(dependency_attrs)
        
        return result
        
    except Exception as e:
        return {
            'compatible': False,
            'error': str(e),
            'version': 'StepFactory v9.0'
        }

def get_basestepmixin_step_info(step_instance: 'BaseStepMixin') -> Dict[str, Any]:
    """BaseStepMixin Step ì •ë³´ ì¡°íšŒ"""
    try:
        info = {
            'step_name': getattr(step_instance, 'step_name', 'Unknown'),
            'step_id': getattr(step_instance, 'step_id', 0),
            'class_name': step_instance.__class__.__name__,
            'module': step_instance.__class__.__module__,
            'device': getattr(step_instance, 'device', 'Unknown'),
            'is_initialized': getattr(step_instance, 'is_initialized', False),
            'has_model': getattr(step_instance, 'has_model', False),
            'model_loaded': getattr(step_instance, 'model_loaded', False)
        }
        
        # ì˜ì¡´ì„± ìƒíƒœ
        dependencies = {}
        for dep_name in ['model_loader', 'memory_manager', 'data_converter', 'di_container']:
            dependencies[dep_name] = hasattr(step_instance, dep_name) and getattr(step_instance, dep_name) is not None
        
        info['dependencies'] = dependencies
        
        # BaseStepMixin íŠ¹ì • ì†ì„±ë“¤
        if hasattr(step_instance, 'dependency_manager'):
            dep_manager = step_instance.dependency_manager
            if hasattr(dep_manager, 'get_status'):
                try:
                    info['dependency_manager_status'] = dep_manager.get_status()
                except:
                    info['dependency_manager_status'] = 'error'
        
        return info
        
    except Exception as e:
        return {'error': str(e)}

# ==============================================
# ğŸ”¥ ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸ ë„êµ¬
# ==============================================

async def test_step_creation_flow(step_type: StepType, **kwargs) -> Dict[str, Any]:
    """Step ìƒì„± í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (ë™ê¸°/ë¹„ë™ê¸° í˜¸í™˜)"""
    try:
        test_result = {
            'step_type': step_type.value,
            'test_start_time': time.time(),
            'phases': {}
        }
        
        factory = get_global_step_factory()
        
        # Phase 1: ì„¤ì • ìƒì„± í…ŒìŠ¤íŠ¸
        phase1_start = time.time()
        try:
            config = BaseStepMixinMapping.get_config(step_type, **kwargs)
            test_result['phases']['config_creation'] = {
                'success': True,
                'time': time.time() - phase1_start,
                'config_class': config.class_name
            }
        except Exception as e:
            test_result['phases']['config_creation'] = {
                'success': False,
                'time': time.time() - phase1_start,
                'error': str(e)
            }
            return test_result
        
        # Phase 2: í´ë˜ìŠ¤ ë¡œë”© í…ŒìŠ¤íŠ¸
        phase2_start = time.time()
        try:
            step_class = factory.class_loader.load_step_class(config)
            test_result['phases']['class_loading'] = {
                'success': step_class is not None,
                'time': time.time() - phase2_start,
                'class_found': step_class.__name__ if step_class else None
            }
        except Exception as e:
            test_result['phases']['class_loading'] = {
                'success': False,
                'time': time.time() - phase2_start,
                'error': str(e)
            }
            if not step_class:
                return test_result
        
        # Phase 3: ì˜ì¡´ì„± í•´ê²° í…ŒìŠ¤íŠ¸
        phase3_start = time.time()
        try:
            dependencies = factory.dependency_resolver.resolve_dependencies_for_constructor(config)
            test_result['phases']['dependency_resolution'] = {
                'success': len(dependencies) > 0,
                'time': time.time() - phase3_start,
                'resolved_count': len(dependencies),
                'resolved_dependencies': list(dependencies.keys())
            }
        except Exception as e:
            test_result['phases']['dependency_resolution'] = {
                'success': False,
                'time': time.time() - phase3_start,
                'error': str(e)
            }
        
        # Phase 4: ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ (ë™ê¸°)
        phase4_start = time.time()
        try:
            result = factory.create_step(step_type, **kwargs)
            test_result['phases']['instance_creation'] = {
                'success': result.success,
                'time': time.time() - phase4_start,
                'step_name': result.step_name,
                'basestepmixin_compatible': result.basestepmixin_compatible,
                'error': result.error_message if not result.success else None
            }
        except Exception as e:
            test_result['phases']['instance_creation'] = {
                'success': False,
                'time': time.time() - phase4_start,
                'error': str(e)
            }
        
        test_result['total_time'] = time.time() - test_result['test_start_time']
        test_result['overall_success'] = all(
            phase.get('success', False) for phase in test_result['phases'].values()
        )
        
        return test_result
        
    except Exception as e:
        return {
            'step_type': step_type.value if step_type else 'unknown',
            'overall_success': False,
            'error': str(e)
        }

def diagnose_step_factory_health() -> Dict[str, Any]:
    """StepFactory ìƒíƒœ ì§„ë‹¨"""
    try:
        factory = get_global_step_factory()
        health_report = {
            'factory_version': 'v9.0 (BaseStepMixin Complete Compatibility)',
            'timestamp': time.time(),
            'environment': {
                'conda_env': CONDA_INFO['conda_env'],
                'is_target_env': CONDA_INFO['is_target_env'],
                'is_m3_max': IS_M3_MAX,
                'memory_gb': MEMORY_GB
            },
            'statistics': factory.get_statistics(),
            'cache_status': {
                'cached_steps': len(factory._step_cache),
                'active_references': len([
                    ref for ref in factory._step_cache.values() if ref() is not None
                ])
            },
            'component_status': {
                'class_loader': 'operational',
                'dependency_resolver': 'operational'
            },
            'recommendations': []
        }
        
        # í™˜ê²½ ì²´í¬
        if not CONDA_INFO['is_target_env']:
            health_report['recommendations'].append(
                f"conda í™˜ê²½ì„ mycloset-ai-cleanìœ¼ë¡œ ë³€ê²½ ê¶Œì¥ (í˜„ì¬: {CONDA_INFO['conda_env']})"
            )
        
        # ë©”ëª¨ë¦¬ ì²´í¬
        if MEMORY_GB < 16:
            health_report['recommendations'].append(
                f"ë©”ëª¨ë¦¬ ë¶€ì¡± ì£¼ì˜: {MEMORY_GB:.1f}GB (ê¶Œì¥: 16GB+)"
            )
        
        # ìºì‹œ ì²´í¬
        if len(factory._step_cache) > 10:
            health_report['recommendations'].append(
                "ìºì‹œëœ Stepì´ ë§ìŠµë‹ˆë‹¤. clear_cache() í˜¸ì¶œ ê³ ë ¤"
            )
        
        health_report['overall_health'] = 'good' if len(health_report['recommendations']) == 0 else 'warning'
        
        return health_report
        
    except Exception as e:
        return {
            'overall_health': 'error',
            'error': str(e)
        }

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'StepFactory',
    'BaseStepMixinClassLoader', 
    'BaseStepMixinDependencyResolver',
    'BaseStepMixinMapping',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'StepType',
    'StepPriority', 
    'BaseStepMixinConfig',
    'StepCreationResult',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_step_factory',
    'reset_global_step_factory',
    
    # Step ìƒì„± í•¨ìˆ˜ë“¤ (BaseStepMixin í˜¸í™˜)
    'create_step',
    'create_human_parsing_step',
    'create_pose_estimation_step', 
    'create_cloth_segmentation_step',
    'create_geometric_matching_step',
    'create_cloth_warping_step',
    'create_virtual_fitting_step',
    'create_post_processing_step',
    'create_quality_assessment_step',
    'create_full_pipeline',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_step_factory_statistics',
    'clear_step_factory_cache',
    'optimize_conda_environment_for_basestepmixin',
    
    # BaseStepMixin í˜¸í™˜ì„± ë„êµ¬ë“¤
    'validate_basestepmixin_step_compatibility',
    'get_basestepmixin_step_info',
    'test_step_creation_flow',
    'diagnose_step_factory_health',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX', 
    'MEMORY_GB'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” (BaseStepMixin v18.0 í˜¸í™˜)
# ==============================================

logger.info("ğŸ”¥ StepFactory v9.0 - BaseStepMixin ì™„ì „ í˜¸í™˜ (Option A êµ¬í˜„) ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” ê°œì„ ì‚¬í•­:")
logger.info("   - BaseStepMixin v18.0 í‘œì¤€ ì™„ì „ í˜¸í™˜")
logger.info("   - ìƒì„±ì ì‹œì  ì˜ì¡´ì„± ì£¼ì… (constructor injection)")
logger.info("   - process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í‘œì¤€í™”")
logger.info("   - UnifiedDependencyManager ì™„ì „ í™œìš©")
logger.info("   - **kwargs íŒ¨í„´ ì™„ì „ ì§€ì›")
logger.info("   - conda í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("   - M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")

logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œì¥: mycloset-ai-clean'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")

logger.info("ğŸ¯ ì§€ì› Step í´ë˜ìŠ¤ (BaseStepMixin í˜¸í™˜):")
for step_type in StepType:
    config = BaseStepMixinMapping.STEP_CONFIGS[step_type]
    logger.info(f"   - {config.class_name} (Step {config.step_id:02d}) - {config.model_size_gb}GB")

# conda í™˜ê²½ ìë™ ìµœì í™” (BaseStepMixin í˜¸í™˜)
if CONDA_INFO['is_target_env']:
    optimize_conda_environment_for_basestepmixin()
    logger.info("ğŸ conda í™˜ê²½ ìë™ ìµœì í™” ì™„ë£Œ! (BaseStepMixin v18.0 í˜¸í™˜)")
else:
    logger.warning(f"âš ï¸ conda í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”: conda activate mycloset-ai-clean")

# ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™”
if IS_M3_MAX:
    try:
        import torch
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ! (BaseStepMixin í˜¸í™˜)")
    except:
        pass

logger.info("ğŸš€ StepFactory v9.0 ì™„ì „ ì¤€ë¹„ ì™„ë£Œ! (BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜) ğŸš€")
logger.info("ğŸ’¡ ì´ì œ ì‹¤ì œ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ìƒì„±ì ì‹œì  ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥!")
logger.info("ğŸ’¡ process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í‘œì¤€í™” ì™„ë£Œ!")