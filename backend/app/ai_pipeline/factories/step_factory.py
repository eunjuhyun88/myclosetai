# backend/app/ai_pipeline/factories/step_factory.py
"""
ğŸ”¥ StepFactory v1.0 - ì˜ì¡´ì„± ì£¼ì… ì „ìš© íŒ©í† ë¦¬ (ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°) - ìˆ˜ì •ëœ ë²„ì „
=======================================================================
âœ… StepFactoryConfig ë§¤ê°œë³€ìˆ˜ ë¶ˆì¼ì¹˜ ìˆ˜ì • (device_type â†’ device)
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ - í•œë°©í–¥ ì˜ì¡´ì„± êµ¬ì¡°
âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ êµ¬í˜„
âœ… BaseStepMixinê³¼ ModelLoader ì•ˆì „í•œ ì¡°ë¦½
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

êµ¬ì¡°:
StepFactory â†’ ModelLoader (ìƒì„±) â†’ BaseStepMixin (ìƒì„±) â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step

í•µì‹¬ ì² í•™:
- StepFactoryê°€ ëª¨ë“  ê²ƒì„ ì¡°ë¦½
- ModelLoaderì™€ BaseStepMixinì€ ì„œë¡œ ëª¨ë¦„
- ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì—°ê²°
- ë‹¨ë°©í–¥ ì˜ì¡´ì„±ë§Œ í—ˆìš©

Author: MyCloset AI Team
Date: 2025-07-23
Version: 1.0 (Fixed Parameter Compatibility)
"""

import os
import gc
import time
import logging
import asyncio
import threading
import traceback
import weakref
import platform
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from functools import lru_cache, wraps
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..utils.model_loader import ModelLoader
    from ..steps.base_step_mixin import BaseStepMixin, HumanParsingMixin, PoseEstimationMixin
    from ..steps.base_step_mixin import ClothSegmentationMixin, GeometricMatchingMixin
    from ..steps.base_step_mixin import ClothWarpingMixin, VirtualFittingMixin
    from ..steps.base_step_mixin import PostProcessingMixin, QualityAssessmentMixin

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° ì‹œìŠ¤í…œ ì²´í¬
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± ì²´í¬
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    pass

# ==============================================
# ğŸ”¥ 3. ë¡œê¹… ì„¤ì •
# ==============================================

logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# ==============================================
# ğŸ”¥ 4. íŒ©í† ë¦¬ ì„¤ì • ë° ë°ì´í„° êµ¬ì¡° (ìˆ˜ì •ëœ ë²„ì „)
# ==============================================

class StepType(Enum):
    """Step íƒ€ì… ì •ì˜ (8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class OptimizationLevel(IntEnum):
    """ìµœì í™” ë ˆë²¨"""
    BASIC = 1
    STANDARD = 2
    HIGH = 3
    M3_MAX = 4
    PRODUCTION = 5

@dataclass
class StepFactoryConfig:
    """
    ğŸ”¥ StepFactory ì„¤ì • (ìˆ˜ì •ëœ ë²„ì „ - device_type ëŒ€ì‹  device ì‚¬ìš©)
    """
    # ì‹œìŠ¤í…œ ì„¤ì • (ğŸ”¥ device_type ì œê±°, deviceë§Œ ì‚¬ìš©)
    device: str = "auto"
    optimization_level: OptimizationLevel = OptimizationLevel.STANDARD
    use_conda_optimization: bool = True
    
    # ModelLoader ì„¤ì •
    model_cache_dir: Optional[str] = None
    use_fp16: bool = True
    max_cached_models: int = 30
    lazy_loading: bool = True
    
    # BaseStepMixin ì„¤ì •
    auto_warmup: bool = True
    auto_memory_cleanup: bool = True
    
    # Stepë³„ ì„¤ì •
    step_configs: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    # ì˜ì¡´ì„± ì£¼ì… ì„¤ì •
    enable_dependency_injection: bool = True
    dependency_injection_mode: str = "runtime"  # "runtime" or "creation"
    
    # ë””ë²„ê¹… ì„¤ì •
    enable_debug_logging: bool = False
    validate_dependencies: bool = True
    
    # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ property (device_typeì„ deviceë¡œ ìë™ ë§¤í•‘)
    @property
    def device_type(self) -> str:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ device_type ì†ì„± (deviceë¡œ ë§¤í•‘)"""
        return self.device
    
    @device_type.setter
    def device_type(self, value: str):
        """device_type ì„¤ì • ì‹œ deviceë¡œ ë§¤í•‘"""
        self.device = value

@dataclass
class StepFactoryResult:
    """StepFactory ê²°ê³¼"""
    step_instance: Any
    model_loader: Any
    step_config: Dict[str, Any]
    creation_time: float
    success: bool
    error_message: Optional[str] = None
    dependencies_injected: bool = False
    optimization_applied: bool = False

# ==============================================
# ğŸ”¥ 5. ì˜ì¡´ì„± í•´ê²° ìœ í‹¸ë¦¬í‹°
# ==============================================

class DependencyResolver:
    """ì˜ì¡´ì„± í•´ê²° ë„ìš°ë¯¸ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DependencyResolver")
        self._model_loader_cache = None
        self._step_mixin_classes = {}
        
    def resolve_model_loader(self, config: StepFactoryConfig) -> Optional[Any]:
        """ModelLoader ì•ˆì „í•˜ê²Œ í•´ê²° (ë™ì  import)"""
        try:
            if self._model_loader_cache is not None:
                return self._model_loader_cache
            
            # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            import importlib
            loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            
            # ModelLoader í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            ModelLoaderClass = getattr(loader_module, 'ModelLoader', None)
            if not ModelLoaderClass:
                self.logger.error("ModelLoader í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            model_loader = ModelLoaderClass(
                device=config.device,
                config={
                    'model_cache_dir': config.model_cache_dir or './ai_models',
                    'use_fp16': config.use_fp16,
                    'max_cached_models': config.max_cached_models,
                    'lazy_loading': config.lazy_loading,
                    'optimization_enabled': config.optimization_level >= OptimizationLevel.STANDARD
                }
            )
            
            # ì´ˆê¸°í™”
            if hasattr(model_loader, 'initialize'):
                success = model_loader.initialize()
                if not success:
                    self.logger.warning("ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return None
            
            self._model_loader_cache = model_loader
            self.logger.info("âœ… ModelLoader í•´ê²° ì™„ë£Œ")
            return model_loader
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def resolve_step_mixin_class(self, step_type: StepType) -> Optional[Type]:
        """BaseStepMixin í´ë˜ìŠ¤ ì•ˆì „í•˜ê²Œ í•´ê²° (ë™ì  import)"""
        try:
            if step_type.value in self._step_mixin_classes:
                return self._step_mixin_classes[step_type.value]
            
            # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            import importlib
            mixin_module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
            
            # Step íƒ€ì…ë³„ í´ë˜ìŠ¤ ë§¤í•‘
            class_mapping = {
                StepType.HUMAN_PARSING: 'HumanParsingMixin',
                StepType.POSE_ESTIMATION: 'PoseEstimationMixin',
                StepType.CLOTH_SEGMENTATION: 'ClothSegmentationMixin',
                StepType.GEOMETRIC_MATCHING: 'GeometricMatchingMixin',
                StepType.CLOTH_WARPING: 'ClothWarpingMixin',
                StepType.VIRTUAL_FITTING: 'VirtualFittingMixin',
                StepType.POST_PROCESSING: 'PostProcessingMixin',
                StepType.QUALITY_ASSESSMENT: 'QualityAssessmentMixin'
            }
            
            class_name = class_mapping.get(step_type, 'BaseStepMixin')
            StepClass = getattr(mixin_module, class_name, None)
            
            if not StepClass:
                self.logger.error(f"{class_name} í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            self._step_mixin_classes[step_type.value] = StepClass
            self.logger.info(f"âœ… {class_name} í´ë˜ìŠ¤ í•´ê²° ì™„ë£Œ")
            return StepClass
            
        except Exception as e:
            self.logger.error(f"âŒ Step í´ë˜ìŠ¤ í•´ê²° ì‹¤íŒ¨ {step_type}: {e}")
            return None
    
    def resolve_memory_manager(self) -> Optional[Any]:
        """MemoryManager í•´ê²° (ì˜µì…˜)"""
        try:
            import importlib
            memory_module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
            MemoryManagerClass = getattr(memory_module, 'MemoryManager', None)
            
            if MemoryManagerClass:
                return MemoryManagerClass()
            return None
            
        except ImportError:
            self.logger.debug("MemoryManager ëª¨ë“ˆ ì—†ìŒ (ì˜µì…˜)")
            return None
        except Exception as e:
            self.logger.warning(f"MemoryManager í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def resolve_data_converter(self) -> Optional[Any]:
        """DataConverter í•´ê²° (ì˜µì…˜)"""
        try:
            import importlib
            converter_module = importlib.import_module('app.ai_pipeline.utils.data_converter')
            DataConverterClass = getattr(converter_module, 'DataConverter', None)
            
            if DataConverterClass:
                return DataConverterClass()
            return None
            
        except ImportError:
            self.logger.debug("DataConverter ëª¨ë“ˆ ì—†ìŒ (ì˜µì…˜)")
            return None
        except Exception as e:
            self.logger.warning(f"DataConverter í•´ê²° ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ì˜ì¡´ì„± í•´ê²°ê¸°
_global_resolver = DependencyResolver()

# ==============================================
# ğŸ”¥ 6. ì‹œìŠ¤í…œ ìµœì í™” ê´€ë¦¬ì
# ==============================================

class SystemOptimizer:
    """ì‹œìŠ¤í…œ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, config: StepFactoryConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.SystemOptimizer")
        
    def apply_conda_optimization(self):
        """conda í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            if not self.config.use_conda_optimization:
                return False
                
            if CONDA_INFO['conda_env'] == 'none':
                self.logger.warning("conda í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ì¼ë°˜ ìµœì í™” ì ìš©")
                return False
            
            # conda í™˜ê²½ë³„ ìµœì í™” ì„¤ì •
            conda_env = CONDA_INFO['conda_env']
            
            if 'mycloset' in conda_env.lower() or 'ai' in conda_env.lower():
                # MyCloset AI ì „ìš© í™˜ê²½ ìµœì í™”
                if TORCH_AVAILABLE:
                    torch.set_num_threads(8 if IS_M3_MAX else 4)
                    
                self.logger.info(f"âœ… MyCloset AI conda í™˜ê²½ ìµœì í™” ì ìš©: {conda_env}")
                return True
            else:
                # ì¼ë°˜ conda í™˜ê²½ ìµœì í™”
                if TORCH_AVAILABLE:
                    torch.set_num_threads(4)
                    
                self.logger.info(f"âœ… ì¼ë°˜ conda í™˜ê²½ ìµœì í™” ì ìš©: {conda_env}")
                return True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ conda ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
            return False
    
    def apply_m3_max_optimization(self):
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            if not IS_M3_MAX:
                return False
            
            if TORCH_AVAILABLE:
                # M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                
                # ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™” (M3 Max 12ì½”ì–´ í™œìš©)
                torch.set_num_threads(12)
                
                # MPS ìºì‹œ ì •ë¦¬
                if MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            self.logger.info("âœ… M3 Max íŠ¹í™” ìµœì í™” ì ìš© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            return False
    
    def apply_optimization_level(self):
        """ìµœì í™” ë ˆë²¨ë³„ ì„¤ì • ì ìš©"""
        try:
            level = self.config.optimization_level
            
            if level >= OptimizationLevel.M3_MAX and IS_M3_MAX:
                self.apply_m3_max_optimization()
            
            if level >= OptimizationLevel.STANDARD:
                self.apply_conda_optimization()
            
            if level >= OptimizationLevel.HIGH:
                # ê³ ì„±ëŠ¥ ìµœì í™”
                if TORCH_AVAILABLE:
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
            
            if level >= OptimizationLevel.PRODUCTION:
                # í”„ë¡œë•ì…˜ ìµœì í™”
                if TORCH_AVAILABLE:
                    torch.set_float32_matmul_precision('high')
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                gc.set_threshold(100, 10, 10)
            
            self.logger.info(f"âœ… ìµœì í™” ë ˆë²¨ {level.name} ì ìš© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì í™” ë ˆë²¨ ì ìš© ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 7. ë©”ì¸ StepFactory í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)
# ==============================================

class StepFactory:
    """
    ğŸ”¥ StepFactory v1.0 - ì˜ì¡´ì„± ì£¼ì… ì „ìš© íŒ©í† ë¦¬ (ìˆ˜ì •ëœ ë²„ì „)
    
    í•µì‹¬ ì—­í• :
    1. ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    2. BaseStepMixin ê¸°ë°˜ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    3. ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ë‘ ê°œì²´ ì—°ê²°
    4. ì‹œìŠ¤í…œ ìµœì í™” ì ìš©
    
    ìˆœí™˜ì°¸ì¡° ë°©ì§€ êµ¬ì¡°:
    StepFactory â†’ ModelLoader (ìƒì„±) â†’ BaseStepMixin (ìƒì„±) â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step
    """
    
    def __init__(self, config: Optional[StepFactoryConfig] = None):
        """StepFactory ì´ˆê¸°í™” (ìˆ˜ì •ëœ ë²„ì „)"""
        self.config = config or StepFactoryConfig()
        self.logger = logging.getLogger(f"{__name__}.StepFactory")
        
        # ì˜ì¡´ì„± í•´ê²°ê¸°
        self.resolver = _global_resolver
        
        # ì‹œìŠ¤í…œ ìµœì í™”ê¸°
        self.optimizer = SystemOptimizer(self.config)
        
        # ìƒì„± ìºì‹œ
        self.creation_cache: Dict[str, StepFactoryResult] = {}
        self._cache_lock = threading.RLock()
        
        # í†µê³„
        self.creation_stats = {
            'total_created': 0,
            'successful_creations': 0,
            'failed_creations': 0,
            'cache_hits': 0,
            'dependency_injection_success': 0,
            'optimization_applied': 0
        }
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """íŒ©í† ë¦¬ ì´ˆê¸°í™”"""
        try:
            # ì‹œìŠ¤í…œ ìµœì í™” ì ìš©
            optimization_success = self.optimizer.apply_optimization_level()
            if optimization_success:
                self.creation_stats['optimization_applied'] += 1
            
            # ë””ë°”ì´ìŠ¤ í•´ê²°
            if self.config.device == "auto":
                self.config.device = self._detect_optimal_device()
            
            self.logger.info(f"âœ… StepFactory v1.0 ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"ğŸ”§ Device: {self.config.device}")
            self.logger.info(f"ğŸ”§ Optimization: {self.config.optimization_level.name}")
            self.logger.info(f"ğŸ”§ conda í™˜ê²½: {CONDA_INFO['conda_env']}")
            self.logger.info(f"ğŸ”§ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
            
        except Exception as e:
            self.logger.error(f"âŒ StepFactory ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE and IS_M3_MAX:
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    # ==============================================
    # ğŸ”¥ 8. í•µì‹¬ ìƒì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    def create_step(
        self, 
        step_type: Union[StepType, str], 
        step_config: Optional[Dict[str, Any]] = None,
        use_cache: bool = True
    ) -> StepFactoryResult:
        """
        Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë™ê¸° ë²„ì „)
        
        Args:
            step_type: Step íƒ€ì… (StepType enum ë˜ëŠ” ë¬¸ìì—´)
            step_config: Stepë³„ ì„¤ì • (ì˜µì…˜)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            StepFactoryResult: ìƒì„± ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # Step íƒ€ì… ì •ê·œí™”
            if isinstance(step_type, str):
                try:
                    step_type = StepType(step_type)
                except ValueError:
                    return StepFactoryResult(
                        step_instance=None,
                        model_loader=None,
                        step_config={},
                        creation_time=0,
                        success=False,
                        error_message=f"ì•Œ ìˆ˜ ì—†ëŠ” Step íƒ€ì…: {step_type}"
                    )
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(step_type, step_config)
            if use_cache:
                cached_result = self._get_from_cache(cache_key)
                if cached_result:
                    self.creation_stats['cache_hits'] += 1
                    return cached_result
            
            # Step ì„¤ì • ì¤€ë¹„
            final_step_config = self._prepare_step_config(step_type, step_config)
            
            # 1ë‹¨ê³„: ModelLoader ìƒì„±
            model_loader = self.resolver.resolve_model_loader(self.config)
            if not model_loader:
                return StepFactoryResult(
                    step_instance=None,
                    model_loader=None,
                    step_config=final_step_config,
                    creation_time=time.time() - start_time,
                    success=False,
                    error_message="ModelLoader ìƒì„± ì‹¤íŒ¨"
                )
            
            # 2ë‹¨ê³„: BaseStepMixin ê¸°ë°˜ Step í´ë˜ìŠ¤ í•´ê²°
            StepClass = self.resolver.resolve_step_mixin_class(step_type)
            if not StepClass:
                return StepFactoryResult(
                    step_instance=None,
                    model_loader=model_loader,
                    step_config=final_step_config,
                    creation_time=time.time() - start_time,
                    success=False,
                    error_message=f"{step_type.value} Step í´ë˜ìŠ¤ í•´ê²° ì‹¤íŒ¨"
                )
            
            # 3ë‹¨ê³„: Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_instance = StepClass(**final_step_config)
            
            # 4ë‹¨ê³„: ì˜ì¡´ì„± ì£¼ì…
            dependencies_injected = False
            if self.config.enable_dependency_injection:
                dependencies_injected = self._inject_dependencies(
                    step_instance, 
                    model_loader, 
                    step_type
                )
                
                if dependencies_injected:
                    self.creation_stats['dependency_injection_success'] += 1
            
            # 5ë‹¨ê³„: ì´ˆê¸°í™”
            if hasattr(step_instance, 'initialize'):
                try:
                    step_instance.initialize()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ìƒì„±
            result = StepFactoryResult(
                step_instance=step_instance,
                model_loader=model_loader,
                step_config=final_step_config,
                creation_time=time.time() - start_time,
                success=True,
                dependencies_injected=dependencies_injected,
                optimization_applied=True
            )
            
            # ìºì‹œ ì €ì¥
            if use_cache:
                self._save_to_cache(cache_key, result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.creation_stats['total_created'] += 1
            self.creation_stats['successful_creations'] += 1
            
            self.logger.info(f"âœ… {step_type.value} Step ìƒì„± ì™„ë£Œ ({result.creation_time:.3f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.creation_stats['total_created'] += 1
            self.creation_stats['failed_creations'] += 1
            
            self.logger.error(f"âŒ {step_type} Step ìƒì„± ì‹¤íŒ¨: {e}")
            return StepFactoryResult(
                step_instance=None,
                model_loader=None,
                step_config=step_config or {},
                creation_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    async def create_step_async(
        self, 
        step_type: Union[StepType, str], 
        step_config: Optional[Dict[str, Any]] = None,
        use_cache: bool = True
    ) -> StepFactoryResult:
        """
        Step ì¸ìŠ¤í„´ìŠ¤ ë¹„ë™ê¸° ìƒì„±
        
        Args:
            step_type: Step íƒ€ì…
            step_config: Stepë³„ ì„¤ì •
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            StepFactoryResult: ìƒì„± ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ ìƒì„±ì€ ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.create_step,
                step_type,
                step_config,
                use_cache
            )
            
            # ë¹„ë™ê¸° ì´ˆê¸°í™”ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
            if result.success and result.step_instance:
                if hasattr(result.step_instance, 'initialize_async'):
                    try:
                        await result.step_instance.initialize_async()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
                # ë¹„ë™ê¸° ì›Œë°ì—…
                if hasattr(result.step_instance, 'warmup_async') and self.config.auto_warmup:
                    try:
                        await result.step_instance.warmup_async()
                        self.logger.info(f"ğŸ”¥ {step_type} Step ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {step_type} Step ë¹„ë™ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return StepFactoryResult(
                step_instance=None,
                model_loader=None,
                step_config=step_config or {},
                creation_time=0,
                success=False,
                error_message=str(e)
            )
    
    # ==============================================
    # ğŸ”¥ 9. ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def _inject_dependencies(
        self, 
        step_instance: Any, 
        model_loader: Any, 
        step_type: StepType
    ) -> bool:
        """ì˜ì¡´ì„± ì£¼ì… ì‹¤í–‰"""
        try:
            injections_made = 0
            
            # 1. ModelLoader ì£¼ì… (í•„ìˆ˜)
            if hasattr(step_instance, 'set_model_loader'):
                step_instance.set_model_loader(model_loader)
                injections_made += 1
                self.logger.debug(f"âœ… ModelLoader ì£¼ì… ì™„ë£Œ")
            elif hasattr(step_instance, 'model_loader'):
                step_instance.model_loader = model_loader
                injections_made += 1
                self.logger.debug(f"âœ… ModelLoader ì†ì„± ì„¤ì • ì™„ë£Œ")
            
            # 2. MemoryManager ì£¼ì… (ì˜µì…˜)
            memory_manager = self.resolver.resolve_memory_manager()
            if memory_manager:
                if hasattr(step_instance, 'set_memory_manager'):
                    step_instance.set_memory_manager(memory_manager)
                    injections_made += 1
                    self.logger.debug(f"âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
                elif hasattr(step_instance, 'memory_manager'):
                    step_instance.memory_manager = memory_manager
                    injections_made += 1
                    self.logger.debug(f"âœ… MemoryManager ì†ì„± ì„¤ì • ì™„ë£Œ")
            
            # 3. DataConverter ì£¼ì… (ì˜µì…˜)
            data_converter = self.resolver.resolve_data_converter()
            if data_converter:
                if hasattr(step_instance, 'set_data_converter'):
                    step_instance.set_data_converter(data_converter)
                    injections_made += 1
                    self.logger.debug(f"âœ… DataConverter ì£¼ì… ì™„ë£Œ")
                elif hasattr(step_instance, 'data_converter'):
                    step_instance.data_converter = data_converter
                    injections_made += 1
                    self.logger.debug(f"âœ… DataConverter ì†ì„± ì„¤ì • ì™„ë£Œ")
            
            # 4. Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ModelLoaderë¥¼ í†µí•´)
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    step_name = self._get_step_name(step_type)
                    step_interface = model_loader.create_step_interface(step_name)
                    
                    if step_interface and hasattr(step_instance, 'set_step_interface'):
                        step_instance.set_step_interface(step_interface)
                        injections_made += 1
                        self.logger.debug(f"âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                        
                except Exception as e:
                    self.logger.debug(f"Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ê²€ì¦
            if self.config.validate_dependencies:
                self._validate_injected_dependencies(step_instance)
            
            success = injections_made > 0
            if success:
                self.logger.info(f"âœ… {step_type.value} ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injections_made}ê°œ")
            else:
                self.logger.warning(f"âš ï¸ {step_type.value} ì˜ì¡´ì„± ì£¼ì… ì—†ìŒ")
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ {step_type.value} ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def _validate_injected_dependencies(self, step_instance: Any):
        """ì£¼ì…ëœ ì˜ì¡´ì„± ê²€ì¦"""
        try:
            validation_results = []
            
            # ModelLoader ê²€ì¦
            if hasattr(step_instance, 'model_loader') or hasattr(step_instance, 'get_model'):
                validation_results.append("ModelLoader: âœ…")
            else:
                validation_results.append("ModelLoader: âŒ")
            
            # í•„ìˆ˜ ë©”ì„œë“œ ê²€ì¦
            required_methods = ['initialize', 'get_status']
            for method in required_methods:
                if hasattr(step_instance, method):
                    validation_results.append(f"{method}: âœ…")
                else:
                    validation_results.append(f"{method}: âŒ")
            
            self.logger.debug(f"ì˜ì¡´ì„± ê²€ì¦ ê²°ê³¼: {', '.join(validation_results)}")
            
        except Exception as e:
            self.logger.debug(f"ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 10. í¸ì˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def create_human_parsing_step(self, **kwargs) -> StepFactoryResult:
        """Human Parsing Step ìƒì„±"""
        return self.create_step(StepType.HUMAN_PARSING, kwargs)
    
    def create_pose_estimation_step(self, **kwargs) -> StepFactoryResult:
        """Pose Estimation Step ìƒì„±"""
        return self.create_step(StepType.POSE_ESTIMATION, kwargs)
    
    def create_cloth_segmentation_step(self, **kwargs) -> StepFactoryResult:
        """Cloth Segmentation Step ìƒì„±"""
        return self.create_step(StepType.CLOTH_SEGMENTATION, kwargs)
    
    def create_geometric_matching_step(self, **kwargs) -> StepFactoryResult:
        """Geometric Matching Step ìƒì„±"""
        return self.create_step(StepType.GEOMETRIC_MATCHING, kwargs)
    
    def create_cloth_warping_step(self, **kwargs) -> StepFactoryResult:
        """Cloth Warping Step ìƒì„±"""
        return self.create_step(StepType.CLOTH_WARPING, kwargs)
    
    def create_virtual_fitting_step(self, **kwargs) -> StepFactoryResult:
        """Virtual Fitting Step ìƒì„± (í•µì‹¬)"""
        return self.create_step(StepType.VIRTUAL_FITTING, kwargs)
    
    def create_post_processing_step(self, **kwargs) -> StepFactoryResult:
        """Post Processing Step ìƒì„±"""
        return self.create_step(StepType.POST_PROCESSING, kwargs)
    
    def create_quality_assessment_step(self, **kwargs) -> StepFactoryResult:
        """Quality Assessment Step ìƒì„±"""
        return self.create_step(StepType.QUALITY_ASSESSMENT, kwargs)
    
    # ë¹„ë™ê¸° ë²„ì „ë“¤
    async def create_human_parsing_step_async(self, **kwargs) -> StepFactoryResult:
        """Human Parsing Step ë¹„ë™ê¸° ìƒì„±"""
        return await self.create_step_async(StepType.HUMAN_PARSING, kwargs)
    
    async def create_virtual_fitting_step_async(self, **kwargs) -> StepFactoryResult:
        """Virtual Fitting Step ë¹„ë™ê¸° ìƒì„± (í•µì‹¬)"""
        return await self.create_step_async(StepType.VIRTUAL_FITTING, kwargs)
    
    # M3 Max ìµœì í™” ë²„ì „ë“¤
    def create_m3_max_optimized_step(
        self, 
        step_type: Union[StepType, str], 
        **kwargs
    ) -> StepFactoryResult:
        """M3 Max ìµœì í™” Step ìƒì„±"""
        # M3 Max íŠ¹í™” ì„¤ì • ì ìš©
        m3_max_config = {
            'device': 'mps' if MPS_AVAILABLE else 'cpu',
            'use_fp16': True,
            'auto_memory_cleanup': True,
            'optimization_level': OptimizationLevel.M3_MAX,
            **kwargs
        }
        
        return self.create_step(step_type, m3_max_config)
    
    # ==============================================
    # ğŸ”¥ 11. ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _prepare_step_config(
        self, 
        step_type: StepType, 
        step_config: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Step ì„¤ì • ì¤€ë¹„"""
        # ê¸°ë³¸ ì„¤ì •
        final_config = {
            'step_name': self._get_step_name(step_type),
            'step_id': self._get_step_id(step_type),
            'device': self.config.device,
            'use_fp16': self.config.use_fp16,
            'auto_warmup': self.config.auto_warmup,
            'auto_memory_cleanup': self.config.auto_memory_cleanup
        }
        
        # ì „ì—­ Step ì„¤ì • ë³‘í•©
        if step_type.value in self.config.step_configs:
            final_config.update(self.config.step_configs[step_type.value])
        
        # ê°œë³„ ì„¤ì • ë³‘í•©
        if step_config:
            final_config.update(step_config)
        
        return final_config
    
    def _get_step_name(self, step_type: StepType) -> str:
        """Step ì´ë¦„ ë°˜í™˜"""
        name_mapping = {
            StepType.HUMAN_PARSING: "HumanParsingStep",
            StepType.POSE_ESTIMATION: "PoseEstimationStep",
            StepType.CLOTH_SEGMENTATION: "ClothSegmentationStep",
            StepType.GEOMETRIC_MATCHING: "GeometricMatchingStep",
            StepType.CLOTH_WARPING: "ClothWarpingStep",
            StepType.VIRTUAL_FITTING: "VirtualFittingStep",
            StepType.POST_PROCESSING: "PostProcessingStep",
            StepType.QUALITY_ASSESSMENT: "QualityAssessmentStep"
        }
        return name_mapping.get(step_type, f"{step_type.value.title()}Step")
    
    def _get_step_id(self, step_type: StepType) -> int:
        """Step ID ë°˜í™˜"""
        id_mapping = {
            StepType.HUMAN_PARSING: 1,
            StepType.POSE_ESTIMATION: 2,
            StepType.CLOTH_SEGMENTATION: 3,
            StepType.GEOMETRIC_MATCHING: 4,
            StepType.CLOTH_WARPING: 5,
            StepType.VIRTUAL_FITTING: 6,
            StepType.POST_PROCESSING: 7,
            StepType.QUALITY_ASSESSMENT: 8
        }
        return id_mapping.get(step_type, 0)
    
    def _generate_cache_key(
        self, 
        step_type: StepType, 
        step_config: Optional[Dict[str, Any]]
    ) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        key_data = {
            'step_type': step_type.value,
            'device': self.config.device,
            'optimization_level': self.config.optimization_level.value,
            'step_config': step_config or {}
        }
        
        key_str = str(key_data)  # json.dumps ëŒ€ì‹  str ì‚¬ìš©ìœ¼ë¡œ í˜¸í™˜ì„± í–¥ìƒ
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[StepFactoryResult]:
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            with self._cache_lock:
                if cache_key in self.creation_cache:
                    cached_result = self.creation_cache[cache_key]
                    
                    # ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—¬ì „íˆ ìœ íš¨í•œì§€ í™•ì¸
                    if (cached_result.step_instance and 
                        hasattr(cached_result.step_instance, 'is_initialized')):
                        return cached_result
                
                return None
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_to_cache(self, cache_key: str, result: StepFactoryResult):
        """ìºì‹œì— ì €ì¥"""
        try:
            with self._cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 50ê°œ)
                if len(self.creation_cache) >= 50:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = next(iter(self.creation_cache))
                    del self.creation_cache[oldest_key]
                
                self.creation_cache[cache_key] = result
        except Exception as e:
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_creation_stats(self) -> Dict[str, Any]:
        """ìƒì„± í†µê³„ ë°˜í™˜"""
        return {
            **self.creation_stats,
            'cache_size': len(self.creation_cache),
            'success_rate': (
                self.creation_stats['successful_creations'] / 
                max(1, self.creation_stats['total_created'])
            ),
            'dependency_injection_rate': (
                self.creation_stats['dependency_injection_success'] / 
                max(1, self.creation_stats['successful_creations'])
            )
        }
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self._cache_lock:
                self.creation_cache.clear()
            self.logger.info("âœ… StepFactory ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """íŒ©í† ë¦¬ ì •ë¦¬"""
        try:
            self.clear_cache()
            
            # í†µê³„ ë¦¬ì…‹
            for key in self.creation_stats:
                self.creation_stats[key] = 0
            
            self.logger.info("âœ… StepFactory ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ StepFactory ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 12. ì „ì—­ íŒ©í† ë¦¬ ê´€ë¦¬ (ìˆ˜ì •ëœ ë²„ì „)
# ==============================================

_global_step_factory: Optional[StepFactory] = None
_factory_lock = threading.Lock()

@lru_cache(maxsize=1)
def get_global_step_factory(config: Optional[StepFactoryConfig] = None) -> StepFactory:
    """ì „ì—­ StepFactory ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_step_factory
    
    with _factory_lock:
        if _global_step_factory is None:
            factory_config = config or StepFactoryConfig(
                optimization_level=OptimizationLevel.M3_MAX if IS_M3_MAX else OptimizationLevel.STANDARD,
                use_conda_optimization=True,
                enable_dependency_injection=True,
                auto_warmup=True,
                auto_memory_cleanup=True
            )
            
            _global_step_factory = StepFactory(factory_config)
            logger.info("ğŸŒ ì „ì—­ StepFactory v1.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        
        return _global_step_factory

def create_m3_max_step_factory() -> StepFactory:
    """M3 Max ìµœì í™” StepFactory ìƒì„±"""
    config = StepFactoryConfig(
        device="mps" if MPS_AVAILABLE else "cpu",
        optimization_level=OptimizationLevel.M3_MAX,
        use_conda_optimization=True,
        use_fp16=True,
        max_cached_models=50,  # M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
        auto_warmup=True,
        auto_memory_cleanup=True,
        enable_dependency_injection=True,
        dependency_injection_mode="runtime"
    )
    return StepFactory(config)

def create_production_step_factory() -> StepFactory:
    """í”„ë¡œë•ì…˜ StepFactory ìƒì„±"""
    config = StepFactoryConfig(
        optimization_level=OptimizationLevel.PRODUCTION,
        use_conda_optimization=True,
        use_fp16=True,
        lazy_loading=True,
        auto_warmup=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìˆ˜ë™ ì›Œë°ì—…
        auto_memory_cleanup=True,
        enable_dependency_injection=True,
        validate_dependencies=True,
        enable_debug_logging=False
    )
    return StepFactory(config)

def cleanup_global_step_factory():
    """ì „ì—­ StepFactory ì •ë¦¬"""
    global _global_step_factory
    
    with _factory_lock:
        if _global_step_factory:
            _global_step_factory.cleanup()
            _global_step_factory = None
        
        get_global_step_factory.cache_clear()
        logger.info("ğŸŒ ì „ì—­ StepFactory v1.0 ì •ë¦¬ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ 13. í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ API í˜¸í™˜)
# ==============================================

def create_step(
    step_type: Union[StepType, str], 
    step_config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> StepFactoryResult:
    """Step ìƒì„± (ì „ì—­ íŒ©í† ë¦¬ ì‚¬ìš©)"""
    factory = get_global_step_factory()
    final_config = {**(step_config or {}), **kwargs}
    return factory.create_step(step_type, final_config)

async def create_step_async(
    step_type: Union[StepType, str], 
    step_config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> StepFactoryResult:
    """Step ë¹„ë™ê¸° ìƒì„± (ì „ì—­ íŒ©í† ë¦¬ ì‚¬ìš©)"""
    factory = get_global_step_factory()
    final_config = {**(step_config or {}), **kwargs}
    return await factory.create_step_async(step_type, final_config)

# Stepë³„ í¸ì˜ í•¨ìˆ˜ë“¤
def create_human_parsing_step(**kwargs) -> StepFactoryResult:
    """Human Parsing Step ìƒì„±"""
    return create_step(StepType.HUMAN_PARSING, kwargs)

def create_virtual_fitting_step(**kwargs) -> StepFactoryResult:
    """Virtual Fitting Step ìƒì„± (í•µì‹¬)"""
    return create_step(StepType.VIRTUAL_FITTING, kwargs)

async def create_virtual_fitting_step_async(**kwargs) -> StepFactoryResult:
    """Virtual Fitting Step ë¹„ë™ê¸° ìƒì„± (í•µì‹¬)"""
    return await create_step_async(StepType.VIRTUAL_FITTING, kwargs)

def create_m3_max_optimized_step(step_type: Union[StepType, str], **kwargs) -> StepFactoryResult:
    """M3 Max ìµœì í™” Step ìƒì„±"""
    factory = get_global_step_factory()
    return factory.create_m3_max_optimized_step(step_type, **kwargs)

# íŒŒì´í”„ë¼ì¸ ì „ì²´ ìƒì„±
def create_complete_pipeline(**kwargs) -> Dict[str, StepFactoryResult]:
    """8ë‹¨ê³„ ì™„ì „ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    factory = get_global_step_factory()
    
    pipeline_results = {}
    for step_type in StepType:
        try:
            result = factory.create_step(step_type, kwargs)
            pipeline_results[step_type.value] = result
            
            if result.success:
                logger.info(f"âœ… {step_type.value} Step ìƒì„± ì™„ë£Œ")
            else:
                logger.error(f"âŒ {step_type.value} Step ìƒì„± ì‹¤íŒ¨: {result.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ {step_type.value} Step ìƒì„± ì˜¤ë¥˜: {e}")
            pipeline_results[step_type.value] = StepFactoryResult(
                step_instance=None,
                model_loader=None,
                step_config={},
                creation_time=0,
                success=False,
                error_message=str(e)
            )
    
    return pipeline_results

async def create_complete_pipeline_async(**kwargs) -> Dict[str, StepFactoryResult]:
    """8ë‹¨ê³„ ì™„ì „ íŒŒì´í”„ë¼ì¸ ë¹„ë™ê¸° ìƒì„±"""
    factory = get_global_step_factory()
    
    # ë³‘ë ¬ ìƒì„±ì„ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
    tasks = []
    for step_type in StepType:
        task = factory.create_step_async(step_type, kwargs)
        tasks.append((step_type, task))
    
    # ë³‘ë ¬ ì‹¤í–‰
    pipeline_results = {}
    for step_type, task in tasks:
        try:
            result = await task
            pipeline_results[step_type.value] = result
            
            if result.success:
                logger.info(f"âœ… {step_type.value} Step ë¹„ë™ê¸° ìƒì„± ì™„ë£Œ")
            else:
                logger.error(f"âŒ {step_type.value} Step ë¹„ë™ê¸° ìƒì„± ì‹¤íŒ¨: {result.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ {step_type.value} Step ë¹„ë™ê¸° ìƒì„± ì˜¤ë¥˜: {e}")
            pipeline_results[step_type.value] = StepFactoryResult(
                step_instance=None,
                model_loader=None,
                step_config={},
                creation_time=0,
                success=False,
                error_message=str(e)
            )
    
    return pipeline_results

# ==============================================
# ğŸ”¥ 14. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'StepFactory',
    'DependencyResolver',
    'SystemOptimizer',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'StepType',
    'OptimizationLevel',
    'StepFactoryConfig',
    'StepFactoryResult',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_step_factory',
    'create_m3_max_step_factory',
    'create_production_step_factory',
    'cleanup_global_step_factory',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_step',
    'create_step_async',
    'create_human_parsing_step',
    'create_virtual_fitting_step',
    'create_virtual_fitting_step_async',
    'create_m3_max_optimized_step',
    'create_complete_pipeline',
    'create_complete_pipeline_async',
    
    # ìƒìˆ˜ë“¤
    'IS_M3_MAX',
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
# ==============================================

import atexit
atexit.register(cleanup_global_step_factory)

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("âœ… StepFactory v1.0 - ë§¤ê°œë³€ìˆ˜ í˜¸í™˜ì„± ìˆ˜ì • ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ”¥ í•µì‹¬ ìˆ˜ì •ì‚¬í•­:")
logger.info("   âœ… device_type â†’ device ë§¤ê°œë³€ìˆ˜ í†µì¼")
logger.info("   âœ… device_type property í˜¸í™˜ì„± ì§€ì›")
logger.info("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ - í•œë°©í–¥ ì˜ì¡´ì„± êµ¬ì¡°")
logger.info("   âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… BaseStepMixinê³¼ ModelLoader ì•ˆì „í•œ ì¡°ë¦½")
logger.info("   âœ… M3 Max 128GB ìµœì í™”")
logger.info("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›")
logger.info("   âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
logger.info("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("")
logger.info("ğŸ—ï¸ êµ¬ì¡°:")
logger.info("   StepFactory â†’ ModelLoader (ìƒì„±) â†’ BaseStepMixin (ìƒì„±) â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step")
logger.info("")
logger.info("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›:")
logger.info("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
logger.info("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€") 
logger.info("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
logger.info("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
logger.info("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
logger.info("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
logger.info("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
logger.info("   8ï¸âƒ£ QualityAssessmentMixin - í’ˆì§ˆ í‰ê°€")
logger.info("")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']}")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info("")
logger.info("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
logger.info("   # ê¸°ë³¸ ì‚¬ìš©")
logger.info("   result = create_virtual_fitting_step()")
logger.info("   if result.success:")
logger.info("       step = result.step_instance")
logger.info("   ")
logger.info("   # ë¹„ë™ê¸° ì‚¬ìš©")
logger.info("   result = await create_virtual_fitting_step_async()")
logger.info("   ")
logger.info("   # M3 Max ìµœì í™”")
logger.info("   result = create_m3_max_optimized_step('virtual_fitting')")
logger.info("   ")
logger.info("   # ì™„ì „ íŒŒì´í”„ë¼ì¸")
logger.info("   pipeline = await create_complete_pipeline_async()")
logger.info("")
logger.info("=" * 80)
logger.info("ğŸš€ StepFactory v1.0 ë§¤ê°œë³€ìˆ˜ í˜¸í™˜ì„± ìˆ˜ì • ì™„ë£Œ!")
logger.info("   âœ… device_type ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("   âœ… ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€")
logger.info("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("   âœ… ê¹”ë”í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
logger.info("   âœ… BaseStepMixin + ModelLoader ì™„ë²½ ì¡°ë¦½")
logger.info("   âœ… M3 Max 128GB ìµœì í™”")
logger.info("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("=" * 80)