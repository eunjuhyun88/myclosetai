# app/ai_pipeline/adapters/__init__.py
"""ì–´ëŒ‘í„° íŒ¨í‚¤ì§€"""

from .model_adapter import ModelLoaderAdapter, StepInterfaceAdapter
from .memory_adapter import MemoryManagerAdapter
from .data_adapter import DataConverterAdapter

__all__ = [
    'ModelLoaderAdapter',
    'StepInterfaceAdapter', 
    'MemoryManagerAdapter',
    'DataConverterAdapter'
]

# ==============================================
# app/ai_pipeline/adapters/model_adapter.py
# ==============================================
"""
ğŸ”¥ ëª¨ë¸ ì–´ëŒ‘í„° êµ¬í˜„
==================

âœ… ê¸°ì¡´ ModelLoaderë¥¼ IModelLoader ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘
âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… ì•ˆì „í•œ ì§€ì—° ë¡œë”©
"""

import logging
from typing import Any, Dict, Optional, List
from ..interface.model_interface import IModelLoader, IStepInterface

logger = logging.getLogger(__name__)

class StepInterfaceAdapter(IStepInterface):
    """Step ì¸í„°í˜ì´ìŠ¤ ì–´ëŒ‘í„°"""
    
    def __init__(self, real_interface: Any = None, step_name: str = "unknown"):
        self.real_interface = real_interface
        self.step_name = step_name
        self.logger = logging.getLogger(f"{__name__}.StepInterfaceAdapter.{step_name}")
        
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ì¡°íšŒ"""
        try:
            if self.real_interface and hasattr(self.real_interface, 'get_model_sync'):
                return self.real_interface.get_model_sync(model_name)
            elif self.real_interface and hasattr(self.real_interface, 'get_model'):
                result = self.real_interface.get_model(model_name)
                # coroutine ê°ì²´ ì²˜ë¦¬
                if hasattr(result, '__await__'):
                    self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ê²°ê³¼ë¥¼ ë™ê¸°ë¡œ ë³€í™˜: {model_name}")
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” ìƒˆ íƒœìŠ¤í¬ ìƒì„±
                            import concurrent.futures
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(asyncio.run, result)
                                return future.result(timeout=30)
                        else:
                            return asyncio.run(result)
                    except Exception as e:
                        self.logger.error(f"âŒ ë¹„ë™ê¸° ë³€í™˜ ì‹¤íŒ¨: {e}")
                        return self._create_mock_model(model_name or "default")
                return result
            else:
                return self._create_mock_model(model_name or "default")
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_mock_model(model_name or "error")
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ"""
        try:
            if self.real_interface and hasattr(self.real_interface, 'get_model'):
                result = self.real_interface.get_model(model_name)
                if hasattr(result, '__await__'):
                    return await result
                else:
                    return result
            else:
                return self._create_mock_model(model_name or "default")
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return self._create_mock_model(model_name or "error")
    
    def list_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            if self.real_interface and hasattr(self.real_interface, 'list_available_models'):
                return self.real_interface.list_available_models()
            else:
                return [f"{self.step_name}_default", f"{self.step_name}_backup"]
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_mock_model(self, model_name: str) -> Any:
        """Mock ëª¨ë¸ ìƒì„±"""
        class MockModel:
            def __init__(self, name: str, step: str):
                self.name = name
                self.step = step
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'step_name': self.step,
                    'result': f'mock_result_for_{self.name}',
                    'type': 'mock_adapter'
                }
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return MockModel(model_name, self.step_name)

class ModelLoaderAdapter(IModelLoader):
    """ModelLoader ì–´ëŒ‘í„°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.ModelLoaderAdapter")
        self._model_loader = None
        self._initialized = False
        self._initialization_attempted = False
        
    def _ensure_initialized(self):
        """ì§€ì—° ì´ˆê¸°í™”"""
        if self._initialization_attempted:
            return self._model_loader is not None
            
        self._initialization_attempted = True
        
        try:
            # ì‹¤ì œ ModelLoader ê°€ì ¸ì˜¤ê¸°
            from ..utils.model_loader import get_global_model_loader
            self._model_loader = get_global_model_loader()
            self._initialized = True
            self.logger.info("âœ… ì‹¤ì œ ModelLoader ì—°ê²° ì„±ê³µ")
            return True
        except ImportError as e:
            self.logger.warning(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        if self._ensure_initialized() and self._model_loader:
            try:
                if hasattr(self._model_loader, 'load_model_sync'):
                    return self._model_loader.load_model_sync(model_name, **kwargs)
                elif hasattr(self._model_loader, 'get_model'):
                    return self._model_loader.get_model(model_name, **kwargs)
                else:
                    self.logger.warning(f"âš ï¸ ModelLoaderì— ì ì ˆí•œ ë©”ì„œë“œ ì—†ìŒ")
                    return None
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return None
        
        # í´ë°± ì²˜ë¦¬
        self.logger.warning(f"âš ï¸ í´ë°± ëª¨ë¸ ìƒì„±: {model_name}")
        return self._create_fallback_model(model_name)
    
    async def get_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        if self._ensure_initialized() and self._model_loader:
            try:
                if hasattr(self._model_loader, 'load_model_async'):
                    return await self._model_loader.load_model_async(model_name, **kwargs)
                elif hasattr(self._model_loader, 'get_model_async'):
                    return await self._model_loader.get_model_async(model_name, **kwargs)
                elif hasattr(self._model_loader, 'get_model'):
                    # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                    import asyncio
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        None, 
                        lambda: self._model_loader.get_model(model_name, **kwargs)
                    )
            except Exception as e:
                self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                return None
        
        # í´ë°± ì²˜ë¦¬
        return self._create_fallback_model(model_name)
    
    def create_step_interface(self, step_name: str, **kwargs) -> IStepInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        if self._ensure_initialized() and self._model_loader:
            try:
                if hasattr(self._model_loader, 'create_step_interface'):
                    real_interface = self._model_loader.create_step_interface(step_name, **kwargs)
                    return StepInterfaceAdapter(real_interface, step_name)
                else:
                    self.logger.warning(f"âš ï¸ create_step_interface ë©”ì„œë“œ ì—†ìŒ")
            except Exception as e:
                self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨ {step_name}: {e}")
        
        # í´ë°±: Mock ì¸í„°í˜ì´ìŠ¤
        return StepInterfaceAdapter(None, step_name)
    
    def list_models(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        if self._ensure_initialized() and self._model_loader:
            try:
                if hasattr(self._model_loader, 'list_models'):
                    return self._model_loader.list_models()
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # í´ë°±
        return {
            'default_model': {
                'name': 'default_model',
                'status': 'fallback',
                'source': 'adapter'
            }
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._model_loader and hasattr(self._model_loader, 'cleanup'):
            try:
                self._model_loader.cleanup()
                self.logger.info("âœ… ModelLoader ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        self._model_loader = None
        self._initialized = False
        self._initialization_attempted = False
    
    def _create_fallback_model(self, model_name: str) -> Any:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        class FallbackModel:
            def __init__(self, name: str):
                self.name = name
                self.device = "cpu"
                
            def __call__(self, *args, **kwargs):
                return {
                    'status': 'success',
                    'model_name': self.name,
                    'result': f'fallback_result_for_{self.name}',
                    'type': 'fallback_adapter'
                }
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
        
        return FallbackModel(model_name)

# ==============================================
# app/ai_pipeline/adapters/memory_adapter.py
# ==============================================
"""
ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì–´ëŒ‘í„°
======================
"""

import logging
import gc
import time
from typing import Dict, Any
from ..interface.memory_interface import IMemoryManager

class MemoryManagerAdapter(IMemoryManager):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì–´ëŒ‘í„°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.MemoryManagerAdapter")
        self._memory_manager = None
        self._initialized = False
    
    def _ensure_initialized(self):
        """ì§€ì—° ì´ˆê¸°í™”"""
        if self._initialized:
            return self._memory_manager is not None
            
        try:
            from ..utils.memory_manager import get_global_memory_manager
            self._memory_manager = get_global_memory_manager()
            self._initialized = True
            self.logger.info("âœ… ì‹¤ì œ MemoryManager ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._initialized = True
            return False
    
    def optimize_memory(self, **kwargs) -> Dict[str, Any]:
        """ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self._ensure_initialized() and self._memory_manager:
            try:
                if hasattr(self._memory_manager, 'optimize_memory'):
                    return self._memory_manager.optimize_memory(**kwargs)
            except Exception as e:
                self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ìµœì í™”
        return self._basic_memory_optimization()
    
    async def optimize_memory_async(self, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self._ensure_initialized() and self._memory_manager:
            try:
                if hasattr(self._memory_manager, 'optimize_memory_async'):
                    return await self._memory_manager.optimize_memory_async(**kwargs)
                else:
                    # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                    import asyncio
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        None, 
                        lambda: self.optimize_memory(**kwargs)
                    )
            except Exception as e:
                self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        
        # í´ë°±
        return self._basic_memory_optimization()
    
    def get_memory_status(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                'total_gb': memory.total / 1024**3,
                'available_gb': memory.available / 1024**3,
                'percent_used': memory.percent,
                'adapter': True
            }
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'adapter': True}
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                import torch
                if torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                    elif hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
            
            return {
                'success': True,
                'objects_freed': before_objects - after_objects,
                'aggressive': aggressive,
                'adapter': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e), 'adapter': True}
    
    def _basic_memory_optimization(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            start_time = time.time()
            
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            
            duration = time.time() - start_time
            
            return {
                'success': True,
                'duration': duration,
                'objects_freed': before_objects - after_objects,
                'method': 'basic_fallback',
                'adapter': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e), 'adapter': True}

# ==============================================
# app/ai_pipeline/adapters/data_adapter.py
# ==============================================
"""
ğŸ”¥ ë°ì´í„° ë³€í™˜ê¸° ì–´ëŒ‘í„°
======================
"""

import logging
from typing import Any, Tuple, Union
from PIL import Image
import numpy as np
from ..interface.data_interface import IDataConverter

class DataConverterAdapter(IDataConverter):
    """ë°ì´í„° ë³€í™˜ê¸° ì–´ëŒ‘í„°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DataConverterAdapter")
        self._data_converter = None
        self._initialized = False
    
    def _ensure_initialized(self):
        """ì§€ì—° ì´ˆê¸°í™”"""
        if self._initialized:
            return self._data_converter is not None
            
        try:
            from ..utils.data_converter import DataConverter
            self._data_converter = DataConverter()
            self._initialized = True
            self.logger.info("âœ… ì‹¤ì œ DataConverter ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._initialized = True
            return False
    
    def convert_image(self, image: Any, target_format: str = "tensor", **kwargs) -> Any:
        """ì´ë¯¸ì§€ ë³€í™˜"""
        if self._ensure_initialized() and self._data_converter:
            try:
                if hasattr(self._data_converter, 'convert_image'):
                    return self._data_converter.convert_image(image, target_format, **kwargs)
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        # í´ë°± ì²˜ë¦¬
        return self._basic_image_conversion(image, target_format, **kwargs)
    
    def preprocess_image(self, image: Any, size: Tuple[int, int] = (512, 512), **kwargs) -> Any:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if self._ensure_initialized() and self._data_converter:
            try:
                if hasattr(self._data_converter, 'preprocess_image'):
                    return self._data_converter.preprocess_image(image, size, **kwargs)
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # í´ë°± ì²˜ë¦¬
        return self._basic_image_preprocessing(image, size, **kwargs)
    
    def postprocess_result(self, result: Any, output_format: str = "image", **kwargs) -> Any:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        if self._ensure_initialized() and self._data_converter:
            try:
                if hasattr(self._data_converter, 'postprocess_result'):
                    return self._data_converter.postprocess_result(result, output_format, **kwargs)
            except Exception as e:
                self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # í´ë°± ì²˜ë¦¬
        return result
    
    def tensor_to_image(self, tensor: Any, **kwargs) -> Image.Image:
        """í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ì ì¸ í…ì„œ-ì´ë¯¸ì§€ ë³€í™˜
            if hasattr(tensor, 'cpu'):
                tensor = tensor.cpu().numpy()
            
            if isinstance(tensor, np.ndarray):
                # ì •ê·œí™” (0-1 â†’ 0-255)
                if tensor.max() <= 1.0:
                    tensor = (tensor * 255).astype(np.uint8)
                
                # ì°¨ì› ì¡°ì •
                if tensor.ndim == 4:  # (B, C, H, W)
                    tensor = tensor[0]
                if tensor.ndim == 3 and tensor.shape[0] in [1, 3]:  # (C, H, W)
                    tensor = tensor.transpose(1, 2, 0)
                if tensor.ndim == 3 and tensor.shape[2] == 1:  # (H, W, 1)
                    tensor = tensor.squeeze(2)
                
                return Image.fromarray(tensor)
            
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', (512, 512), (128, 128, 128))
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ-ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), (128, 128, 128))
    
    def image_to_tensor(self, image: Union[Image.Image, np.ndarray], **kwargs) -> Any:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            # PIL Image â†’ NumPy
            if isinstance(image, Image.Image):
                image_array = np.array(image)
            else:
                image_array = image
            
            # ì •ê·œí™” (0-255 â†’ 0-1)
            if image_array.max() > 1.0:
                image_array = image_array.astype(np.float32) / 255.0
            
            # ì°¨ì› ì¡°ì • (H, W, C) â†’ (C, H, W)
            if image_array.ndim == 3:
                image_array = image_array.transpose(2, 0, 1)
            elif image_array.ndim == 2:
                image_array = image_array[np.newaxis, :, :]
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (C, H, W) â†’ (1, C, H, W)
            image_array = image_array[np.newaxis, :, :, :]
            
            # PyTorch í…ì„œë¡œ ë³€í™˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                import torch
                return torch.from_numpy(image_array)
            except ImportError:
                return image_array
                
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€-í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ í…ì„œ
            try:
                import torch
                return torch.zeros(1, 3, 512, 512)
            except ImportError:
                return np.zeros((1, 3, 512, 512), dtype=np.float32)
    
    def _basic_image_conversion(self, image: Any, target_format: str, **kwargs) -> Any:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜ (í´ë°±)"""
        try:
            if target_format == "tensor":
                return self.image_to_tensor(image, **kwargs)
            elif target_format == "image":
                if isinstance(image, Image.Image):
                    return image
                else:
                    return self.tensor_to_image(image, **kwargs)
            elif target_format == "numpy":
                if isinstance(image, Image.Image):
                    return np.array(image)
                elif hasattr(image, 'numpy'):
                    return image.numpy()
                else:
                    return image
            else:
                return image
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    def _basic_image_preprocessing(self, image: Any, size: Tuple[int, int], **kwargs) -> Any:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í´ë°±)"""
        try:
            # PIL Imageë¡œ ë³€í™˜
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    # í…ì„œì¸ ê²½ìš°
                    image = self.tensor_to_image(image)
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            image = image.resize(size, Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)
            
            # ìš”ì²­ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            target_format = kwargs.get('output_format', 'image')
            if target_format == 'tensor':
                return self.image_to_tensor(image)
            elif target_format == 'numpy':
                return np.array(image)
            else:
                return image
                
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image