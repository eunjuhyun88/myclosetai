# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ Step Interface v5.0 - Logger ë¬¸ì œ ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„
=======================================================================

âœ… Logger ì¤‘ë³µ ì •ì˜ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ ì™„ì „ í•´ê²°  
âœ… ëª¨ë“ˆ import ìˆœì„œ ì™„ì „ ìµœì í™”
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… ë¹ ì§„ ê¸°ëŠ¥ ëª¨ë‘ ë³µì› (GitHubMemoryManager, GitHubDependencyManager ë“±)
âœ… PyTorch weights_only ë¬¸ì œ í•´ê²°
âœ… rembg ì„¸ì…˜ ë¬¸ì œ í•´ê²°
âœ… Safetensors í˜¸í™˜ì„± í™•ì¸
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜
âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜

Author: MyCloset AI Team
Date: 2025-07-28
Version: 5.0 (Complete Logger Fix + All Features)
"""

# =============================================================================
# ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (Logger ì „)
# =============================================================================

import os
import gc
import sys
import time
import warnings
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache
from contextlib import asynccontextmanager
import json
import hashlib

# =============================================================================
# ğŸ”¥ 2ë‹¨ê³„: Logger ì•ˆì „ ì´ˆê¸°í™” (ìµœìš°ì„ )
# =============================================================================

import logging

# Logger ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì „ì—­ ì„¤ì •
_LOGGER_INITIALIZED = False
_MODULE_LOGGER = None

def get_safe_logger():
    """Thread-safe Logger ì´ˆê¸°í™” (ì¤‘ë³µ ë°©ì§€)"""
    global _LOGGER_INITIALIZED, _MODULE_LOGGER
    
    if _LOGGER_INITIALIZED and _MODULE_LOGGER is not None:
        return _MODULE_LOGGER
    
    try:
        # í˜„ì¬ ëª¨ë“ˆì˜ Logger ìƒì„±
        logger_name = __name__
        _MODULE_LOGGER = logging.getLogger(logger_name)
        
        # í•¸ë“¤ëŸ¬ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
        if not _MODULE_LOGGER.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            _MODULE_LOGGER.addHandler(handler)
            _MODULE_LOGGER.setLevel(logging.INFO)
        
        _LOGGER_INITIALIZED = True
        return _MODULE_LOGGER
        
    except Exception as e:
        # ìµœí›„ í´ë°±: print ì‚¬ìš©
        print(f"âš ï¸ Logger ì´ˆê¸°í™” ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
        
        class FallbackLogger:
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def debug(self, msg): print(f"DEBUG: {msg}")
        
        return FallbackLogger()

# ëª¨ë“ˆ ë ˆë²¨ Logger (ë‹¨ í•œ ë²ˆë§Œ ì´ˆê¸°í™”)
logger = get_safe_logger()

# =============================================================================
# ğŸ”¥ 3ë‹¨ê³„: ê²½ê³  ë° ì—ëŸ¬ ì²˜ë¦¬ (Logger ì •ì˜ í›„)
# =============================================================================

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*deprecated.*')
warnings.filterwarnings('ignore', category=ImportWarning)

# =============================================================================
# ğŸ”¥ 4ë‹¨ê³„: TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# =============================================================================

if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core import DIContainer
    from ..steps.base_step_mixin import BaseStepMixin

# =============================================================================
# ğŸ”¥ 5ë‹¨ê³„: ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ í•´ê²°
# =============================================================================

# 1. PyTorch weights_only ë¬¸ì œ í•´ê²°
PYTORCH_FIXED = False
try:
    import torch
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            if weights_only is None:
                weights_only = False
            return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=weights_only, **kwargs)
        torch.load = safe_torch_load
        PYTORCH_FIXED = True
        logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# 2. rembg ì„¸ì…˜ ë¬¸ì œ í•´ê²°
REMBG_AVAILABLE = False
try:
    import rembg
    if hasattr(rembg, 'sessions'):
        REMBG_AVAILABLE = True
        logger.info("âœ… rembg ì„¸ì…˜ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ rembg ì„¸ì…˜ ëª¨ë“ˆ í˜¸í™˜ì„± ë¬¸ì œ - í´ë°± ëª¨ë“œ ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ rembg ëª¨ë“ˆ ë¬¸ì œ: {e}")

# 3. Safetensors í˜¸í™˜ì„± í™•ì¸  
SAFETENSORS_AVAILABLE = False
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
    logger.info("âœ… Safetensors ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.warning("âš ï¸ Safetensors ì‚¬ìš© ë¶ˆê°€ - .pth íŒŒì¼ ìš°ì„  ì‚¬ìš©")

# =============================================================================
# ğŸ”¥ 6ë‹¨ê³„: GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ê°ì§€
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ ì •ë³´
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
BACKEND_ROOT = PROJECT_ROOT / "backend"
AI_PIPELINE_ROOT = BACKEND_ROOT / "app" / "ai_pipeline"

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
    'project_path': str(PROJECT_ROOT)
}

# í•˜ë“œì›¨ì–´ ê°ì§€
IS_M3_MAX = False
MEMORY_GB = 16.0
MPS_AVAILABLE = False

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
    
    # MPS ê°ì§€
    if PYTORCH_FIXED:
        MPS_AVAILABLE = (
            IS_M3_MAX and 
            hasattr(torch.backends, 'mps') and 
            torch.backends.mps.is_available()
        )
except Exception:
    pass

logger.info(f"ğŸ”§ í™˜ê²½ ì •ë³´: conda={CONDA_INFO['conda_env']}, M3_Max={IS_M3_MAX}, MPS={MPS_AVAILABLE}")

# =============================================================================
# ğŸ”¥ 7ë‹¨ê³„: GitHub Step íƒ€ì… ë° ìƒìˆ˜ ì •ì˜
# =============================================================================

class GitHubStepType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class GitHubStepPriority(Enum):
    """GitHub Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class GitHubDeviceType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"

class GitHubProcessingStatus(Enum):
    """GitHub Step ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"
    MOCK_MODE = "mock_mode"

# =============================================================================
# ğŸ”¥ 8ë‹¨ê³„: GitHub Step ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

@dataclass
class GitHubStepConfig:
    """GitHub BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜ ì„¤ì •"""
    # ê¸°ë³¸ Step ì •ë³´
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    module_path: str = ""
    
    # GitHub Step íƒ€ì…
    step_type: GitHubStepType = GitHubStepType.HUMAN_PARSING
    priority: GitHubStepPriority = GitHubStepPriority.MEDIUM
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # GitHub BaseStepMixin í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_support: bool = True
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = False
    require_step_interface: bool = True
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 1.0
    primary_model_file: str = ""
    checkpoint_patterns: List[str] = field(default_factory=list)
    
    # GitHub í™˜ê²½ ìµœì í™”
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    
    # DetailedDataSpec ì„¤ì •
    enable_detailed_data_spec: bool = True
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # 7ë‹¨ê³„ Mock ë¬¸ì œ í•´ê²° ì„¤ì •
    force_real_ai_processing: bool = True
    mock_mode_disabled: bool = True
    fallback_on_ai_failure: bool = False
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ GitHub í™˜ê²½ ìµœì í™”"""
        # conda í™˜ê²½ ìë™ ìµœì í™”
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            # M3 Max + conda ì¡°í•© ìµœì í™”
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.step_name == "VirtualFittingStep" or self.step_id == 7:
            self.force_real_ai_processing = True
            self.mock_mode_disabled = True
            self.fallback_on_ai_failure = False
        
        # AI ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        if not isinstance(self.ai_models, list):
            self.ai_models = []

# =============================================================================
# ğŸ”¥ 9ë‹¨ê³„: GitHub Step ë§¤í•‘ ì‹œìŠ¤í…œ
# =============================================================================

class GitHubStepMapping:
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step ë§¤í•‘"""
    
    GITHUB_STEP_CONFIGS = {
        GitHubStepType.HUMAN_PARSING: GitHubStepConfig(
            step_name="HumanParsingStep",
            step_id=1,
            class_name="HumanParsingStep",
            module_path="app.ai_pipeline.steps.step_01_human_parsing",
            step_type=GitHubStepType.HUMAN_PARSING,
            priority=GitHubStepPriority.HIGH,
            ai_models=["graphonomy.pth", "atr_model.pth"],
            model_size_gb=4.0,
            primary_model_file="graphonomy.pth"
        ),
        
        GitHubStepType.POSE_ESTIMATION: GitHubStepConfig(
            step_name="PoseEstimationStep",
            step_id=2,
            class_name="PoseEstimationStep",
            module_path="app.ai_pipeline.steps.step_02_pose_estimation",
            step_type=GitHubStepType.POSE_ESTIMATION,
            priority=GitHubStepPriority.MEDIUM,
            ai_models=["pose_model.pth", "openpose_model.pth"],
            model_size_gb=3.4,
            primary_model_file="pose_model.pth"
        ),
        
        GitHubStepType.CLOTH_SEGMENTATION: GitHubStepConfig(
            step_name="ClothSegmentationStep",
            step_id=3,
            class_name="ClothSegmentationStep",
            module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
            step_type=GitHubStepType.CLOTH_SEGMENTATION,
            priority=GitHubStepPriority.MEDIUM,
            ai_models=["sam_vit_h_4b8939.pth", "u2net.pth"],
            model_size_gb=5.5,
            primary_model_file="sam_vit_h_4b8939.pth"
        ),
        
        GitHubStepType.GEOMETRIC_MATCHING: GitHubStepConfig(
            step_name="GeometricMatchingStep",
            step_id=4,
            class_name="GeometricMatchingStep",
            module_path="app.ai_pipeline.steps.step_04_geometric_matching",
            step_type=GitHubStepType.GEOMETRIC_MATCHING,
            priority=GitHubStepPriority.LOW,
            ai_models=["geometric_matching.pth", "tps_model.pth"],
            model_size_gb=1.3,
            primary_model_file="geometric_matching.pth"
        ),
        
        GitHubStepType.CLOTH_WARPING: GitHubStepConfig(
            step_name="ClothWarpingStep",
            step_id=5,
            class_name="ClothWarpingStep",
            module_path="app.ai_pipeline.steps.step_05_cloth_warping",
            step_type=GitHubStepType.CLOTH_WARPING,
            priority=GitHubStepPriority.HIGH,
            ai_models=["RealVisXL_V4.0.safetensors", "cloth_warping.pth"],
            model_size_gb=7.0,
            primary_model_file="RealVisXL_V4.0.safetensors"
        ),
        
        GitHubStepType.VIRTUAL_FITTING: GitHubStepConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            class_name="VirtualFittingStep",
            module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
            step_type=GitHubStepType.VIRTUAL_FITTING,
            priority=GitHubStepPriority.CRITICAL,
            ai_models=[
                "v1-5-pruned.safetensors",
                "v1-5-pruned-emaonly.safetensors",
                "controlnet_openpose",
                "vae_decoder"
            ],
            model_size_gb=14.0,
            primary_model_file="v1-5-pruned.safetensors",
            force_real_ai_processing=True,
            mock_mode_disabled=True,
            fallback_on_ai_failure=False
        ),
        
        GitHubStepType.POST_PROCESSING: GitHubStepConfig(
            step_name="PostProcessingStep",
            step_id=7,
            class_name="PostProcessingStep",
            module_path="app.ai_pipeline.steps.step_07_post_processing",
            step_type=GitHubStepType.POST_PROCESSING,
            priority=GitHubStepPriority.LOW,
            ai_models=["super_resolution.pth", "enhancement.pth"],
            model_size_gb=1.3,
            primary_model_file="super_resolution.pth"
        ),
        
        GitHubStepType.QUALITY_ASSESSMENT: GitHubStepConfig(
            step_name="QualityAssessmentStep",
            step_id=8,
            class_name="QualityAssessmentStep",
            module_path="app.ai_pipeline.steps.step_08_quality_assessment",
            step_type=GitHubStepType.QUALITY_ASSESSMENT,
            priority=GitHubStepPriority.CRITICAL,
            ai_models=["open_clip_pytorch_model.bin", "ViT-L-14.pt"],
            model_size_gb=7.0,
            primary_model_file="open_clip_pytorch_model.bin"
        )
    }
    
    @classmethod
    def get_config(cls, step_type: GitHubStepType) -> GitHubStepConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        return cls.GITHUB_STEP_CONFIGS.get(step_type, GitHubStepConfig())
    
    @classmethod
    def get_config_by_name(cls, step_name: str) -> Optional[GitHubStepConfig]:
        """Step ì´ë¦„ìœ¼ë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_name == step_name or config.class_name == step_name:
                return config
        return None
    
    @classmethod
    def get_config_by_id(cls, step_id: int) -> Optional[GitHubStepConfig]:
        """Step IDë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_id == step_id:
                return config
        return None

# =============================================================================
# ğŸ”¥ 10ë‹¨ê³„: GitHub ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (ë³µì›)
# =============================================================================

class GitHubMemoryManager:
    """GitHub í”„ë¡œì íŠ¸ìš© ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = get_safe_logger()
        
        # M3 Max ìë™ ìµœì í™”
        if max_memory_gb is None:
            if IS_M3_MAX and MEMORY_GB >= 64:
                self.max_memory_gb = MEMORY_GB * 0.85
            elif IS_M3_MAX:
                self.max_memory_gb = MEMORY_GB * 0.8
            elif CONDA_INFO['is_target_env']:
                self.max_memory_gb = 12.0
            else:
                self.max_memory_gb = 8.0
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        
        self.logger.info(f"ğŸ§  GitHub ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner}")
                return True
            else:
                available = self.max_memory_gb - self.current_memory_gb
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {available:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner}")
                return size_gb
            return 0.0
    
    def optimize_for_github_project(self):
        """GitHub í”„ë¡œì íŠ¸ íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max)
            if self.mps_enabled and PYTORCH_FIXED:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    self.logger.debug("ğŸ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬")
            
            # Python GC
            gc.collect()
            
            # 128GB M3 Max íŠ¹ë³„ ìµœì í™”
            if IS_M3_MAX and MEMORY_GB >= 128:
                self.max_memory_gb = min(MEMORY_GB * 0.9, 115.0)
                self.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'memory_pool': self.memory_pool.copy(),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'total_system_gb': MEMORY_GB,
                'github_optimized': True
            }

# =============================================================================
# ğŸ”¥ 11ë‹¨ê³„: GitHub ì˜ì¡´ì„± ê´€ë¦¬ì (ë³µì›)
# =============================================================================

@dataclass
class GitHubDependencyStatus:
    """GitHub í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ìƒíƒœ"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    
    # BaseStepMixin v19.1 ìƒíƒœ
    base_initialized: bool = False
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ
    github_compatible: bool = False
    real_ai_models_loaded: bool = False
    mock_mode_disabled: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)

class GitHubDependencyManager:
    """GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ì"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = get_safe_logger()
        
        # ì˜ì¡´ì„± ìƒíƒœ
        self.dependency_status = GitHubDependencyStatus()
        
        # ì˜ì¡´ì„± ì €ì¥ì†Œ
        self.dependencies: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = GitHubMemoryManager()
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        self.logger.debug(f"ğŸ”§ GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì´ˆê¸°í™”: {step_name}")
    
    def inject_dependency(self, name: str, dependency: Any, required: bool = False) -> bool:
        """ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                if dependency is not None:
                    self.dependencies[name] = dependency
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    if name == 'model_loader':
                        self.dependency_status.model_loader = True
                    elif name == 'memory_manager':
                        self.dependency_status.memory_manager = True
                    elif name == 'data_converter':
                        self.dependency_status.data_converter = True
                    elif name == 'di_container':
                        self.dependency_status.di_container = True
                    elif name == 'step_interface':
                        self.dependency_status.step_interface = True
                    
                    self.logger.debug(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ: {name}")
                    return True
                else:
                    if required:
                        self.logger.error(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± {name}ì´ None")
                        return False
                    else:
                        self.logger.warning(f"âš ï¸ ì„ íƒì  ì˜ì¡´ì„± {name}ì´ None")
                        return True
        
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {name} - {e}")
            return False
    
    def auto_inject_github_dependencies(self) -> bool:
        """GitHub í”„ë¡œì íŠ¸ ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # conda í™˜ê²½ ìµœì í™”
            if CONDA_INFO['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™”")
            
            # M3 Max ìµœì í™”
            if IS_M3_MAX:
                self.dependency_status.m3_max_optimized = True
                self.memory_manager.optimize_for_github_project()
                self.logger.debug("âœ… M3 Max ìµœì í™”")
            
            # GitHub í˜¸í™˜ì„± í™œì„±í™”
            self.dependency_status.github_compatible = True
            self.dependency_status.base_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                self.dependencies.clear()
                self.dependency_status = GitHubDependencyStatus()
                self.logger.debug(f"ğŸ§¹ {self.step_name} GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 12ë‹¨ê³„: GitHub Step Model Interface (í•µì‹¬ í´ë˜ìŠ¤)
# =============================================================================

class GitHubStepModelInterface:
    """
    ğŸ”¥ GitHub Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ v5.0 - Logger ë¬¸ì œ ì™„ì „ í•´ê²°
    
    âœ… Logger ì¤‘ë³µ ì •ì˜ ë¬¸ì œ í•´ê²°
    âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models ì •í™• êµ¬í˜„
    âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ í•´ê²°
    âœ… PyTorch weights_only ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        
        # GitHub ì„¤ì • ìë™ ë¡œë”©
        self.config = GitHubStepMapping.get_config_by_name(step_name)
        if not self.config:
            self.config = GitHubStepConfig(step_name=step_name)
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = GitHubMemoryManager()
        
        # ì˜ì¡´ì„± ê´€ë¦¬
        self.dependency_manager = GitHubDependencyManager(step_name)
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'mock_calls_blocked': 0,
            'creation_time': time.time()
        }
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.config.step_id == 6:  # VirtualFittingStep
            self.statistics['force_real_ai'] = True
            self.logger.info(f"ğŸ”¥ {step_name}: ì‹¤ì œ AI ëª¨ë¸ ê°•ì œ ì‚¬ìš© ëª¨ë“œ í™œì„±í™”")
        
        self.logger.info(f"ğŸ”— GitHub {step_name} Interface v5.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # GitHub ì„¤ì • ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'step_id': self.config.step_id,
                    'device': kwargs.get('device', self.config.device),
                    'precision': 'fp16' if self.config.use_fp16 else 'fp32',
                    'github_compatible': True,
                    'force_real_ai': self.config.force_real_ai_processing,
                    'mock_disabled': self.config.mock_mode_disabled,
                    'registered_at': time.time(),
                    'pytorch_fixed': PYTORCH_FIXED,
                    'rembg_available': REMBG_AVAILABLE,
                    'safetensors_available': SAFETENSORS_AVAILABLE,
                    'metadata': {
                        'module_path': self.config.module_path,
                        'class_name': self.config.class_name,
                        'primary_model_file': self.config.primary_model_file,
                        **kwargs.get('metadata', {})
                    }
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'step_id': self.config.step_id,
                    'loaded': False,
                    'size_mb': self.config.model_size_gb * 1024,
                    'device': requirement['device'],
                    'status': 'registered',
                    'github_compatible': True,
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - GitHub êµ¬ì¡° ê¸°ë°˜"""
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    requirement = registry_entry.get('requirement', {})
                    
                    # GitHub í‘œì¤€ ëª¨ë¸ ì •ë³´
                    model_info = {
                        'name': model_name,
                        'path': f"ai_models/step_{requirement.get('step_id', self.config.step_id):02d}_{self.step_name.lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'size_gb': round(registry_entry['size_mb'] / 1024, 2),
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'step_id': registry_entry['step_id'],
                        'loaded': registry_entry['loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'github_compatible': registry_entry.get('github_compatible', True),
                        'force_real_ai': requirement.get('force_real_ai', False),
                        'mock_disabled': requirement.get('mock_disabled', False),
                        'pytorch_fixed': requirement.get('pytorch_fixed', PYTORCH_FIXED),
                        'rembg_available': requirement.get('rembg_available', REMBG_AVAILABLE),
                        'safetensors_available': requirement.get('safetensors_available', SAFETENSORS_AVAILABLE),
                        'metadata': {
                            'step_name': self.step_name,
                            'conda_env': CONDA_INFO['conda_env'],
                            'registered_at': requirement.get('registered_at', 0),
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ì¡°íšŒ
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'size_gb': round(model.get('size_mb', 0.0) / 1024, 2),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'step_id': self.config.step_id,
                                    'loaded': model.get('loaded', False),
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'github_compatible': False,
                                    'force_real_ai': False,
                                    'mock_disabled': False,
                                    'pytorch_fixed': PYTORCH_FIXED,
                                    'rembg_available': REMBG_AVAILABLE,
                                    'safetensors_available': SAFETENSORS_AVAILABLE,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                elif sort_by == "step_id":
                    models.sort(key=lambda x: x['step_id'])
                else:
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ GitHub ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - PyTorch weights_only ë¬¸ì œ í•´ê²°"""
        try:
            with self._lock:
                # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬: Mock ë°ì´í„° ì°¨ë‹¨
                if self.config.step_id == 6 and ('mock' in model_name.lower() or 'test' in model_name.lower()):
                    self.statistics['mock_calls_blocked'] += 1
                    self.logger.warning(f"ğŸ”¥ {self.step_name}: Mock ëª¨ë¸ í˜¸ì¶œ ì°¨ë‹¨ - {model_name}")
                    return None
                
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.statistics['real_ai_calls'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # PyTorch ë¡œë”© ë¬¸ì œ í•´ê²°
                loading_kwargs = kwargs.copy()
                if PYTORCH_FIXED and 'weights_only' not in loading_kwargs:
                    loading_kwargs['weights_only'] = False
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    try:
                        model = self.model_loader.load_model(model_name, **loading_kwargs)
                    except Exception as load_error:
                        # PyTorch ë¡œë”© ì˜¤ë¥˜ ì¬ì‹œë„
                        if PYTORCH_FIXED and ('weights_only' in str(load_error) or 'WeightsUnpickler' in str(load_error)):
                            self.logger.warning(f"âš ï¸ PyTorch weights_only ì˜¤ë¥˜ ê°ì§€, ì¬ì‹œë„: {model_name}")
                            loading_kwargs['weights_only'] = False
                            loading_kwargs['map_location'] = 'cpu'
                            model = self.model_loader.load_model(model_name, **loading_kwargs)
                        else:
                            raise load_error
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        self.statistics['real_ai_calls'] += 1
                        
                        self.logger.info(f"âœ… GitHub ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            
            # ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ íŠ¹ì • ì˜¤ë¥˜ ì²˜ë¦¬
            if 'constants.pkl' in str(e):
                self.logger.warning(f"ğŸ”§ Mobile SAM ëª¨ë¸ íŒŒì¼ ì†ìƒ ê°ì§€: {model_name}")
            elif 'Expected hasRecord' in str(e):
                self.logger.warning(f"ğŸ”§ Graphonomy ëª¨ë¸ ë²„ì „ ë¬¸ì œ ê°ì§€: {model_name}")
            elif 'Unsupported operand' in str(e):
                self.logger.warning(f"ğŸ”§ U2Net ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€: {model_name}")
            
            return None
    
    # BaseStepMixin v19.1 í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜ ë³„ì¹­"""
        return self.get_model_sync(model_name, **kwargs)
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        if model_name:
            return self.get_model_sync(model_name, **kwargs)
        return None
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ í•´ì œ
            for model_name in self._model_cache:
                self.memory_manager.deallocate_memory(model_name)
            
            self._model_cache.clear()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.dependency_manager.cleanup()
            
            self.logger.info(f"âœ… GitHub {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ GitHub Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 13ë‹¨ê³„: Step ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡° (ë³µì›)
# =============================================================================

@dataclass
class GitHubStepCreationResult:
    """GitHub Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    step_type: Optional[GitHubStepType] = None
    class_name: str = ""
    module_path: str = ""
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # GitHub ì˜ì¡´ì„± ì£¼ì… ê²°ê³¼
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    ai_models_loaded: List[str] = field(default_factory=list)
    
    # GitHub BaseStepMixin v19.1 í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    dependency_injection_success: bool = False
    
    # 7ë‹¨ê³„ ë¬¸ì œ í•´ê²° ìƒíƒœ
    mock_mode_disabled: bool = False
    real_ai_processing_enabled: bool = False
    fallback_disabled: bool = False
    
    # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 14ë‹¨ê³„: Step íŒŒì¼ë“¤ì„ ìœ„í•œ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ (í˜¸í™˜ì„±)
# =============================================================================

class StepInterface:
    """Step íŒŒì¼ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, step_name: str, model_loader=None, **kwargs):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        self.config = kwargs
        
        # ê¸°ë³¸ ì†ì„±ë“¤
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.initialized = False
        
        self.logger.debug(f"âœ… StepInterface ìƒì„±: {step_name}")
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (í˜¸í™˜ì„±)"""
        try:
            self.logger.debug(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, **kwargs) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í˜¸í™˜ì„±)"""
        return []
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# =============================================================================
# ğŸ”¥ 15ë‹¨ê³„: ë‹¨ìˆœí•œ í´ë°± í´ë˜ìŠ¤ë“¤ (Step íŒŒì¼ í˜¸í™˜ì„±ìš©)
# =============================================================================

class SimpleStepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì • (í´ë°±ìš©)"""
    def __init__(self, **kwargs):
        self.step_name = kwargs.get('step_name', 'Unknown')
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.model_size_gb = kwargs.get('model_size_gb', 1.0)
        self.ai_models = kwargs.get('ai_models', [])
        
        # ëª¨ë“  kwargsë¥¼ ì†ì„±ìœ¼ë¡œ ì„¤ì •
        for key, value in kwargs.items():
            if not hasattr(self, key):
                setattr(self, key, value)

# =============================================================================
# ğŸ”¥ 16ë‹¨ê³„: íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ëª¨ë“  í•¨ìˆ˜ ë³µì›)
# =============================================================================

def create_github_step_interface_with_diagnostics(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    step_type: Optional[GitHubStepType] = None
) -> GitHubStepModelInterface:
    """ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°˜ì˜í•œ GitHub Step Interface ìƒì„±"""
    try:
        interface = GitHubStepModelInterface(step_name, model_loader)
        
        # Step íƒ€ì…ë³„ ì¶”ê°€ ì„¤ì •
        if step_type:
            config = GitHubStepMapping.get_config(step_type)
            interface.config = config
        
        # ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ í•´ê²° ì ìš©
        if IS_M3_MAX and MEMORY_GB >= 128:
            interface.memory_manager = GitHubMemoryManager(115.0)
            interface.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
        
        # 7ë‹¨ê³„ Mock ì°¨ë‹¨ ê°•í™”
        if step_name == "VirtualFittingStep" or (interface.config and interface.config.step_id == 6):
            interface.statistics['force_real_ai'] = True
            interface.statistics['diagnostic_fixes_applied'] = True
            interface.logger.info(f"ğŸ”¥ Step 06 VirtualFittingStep ì§„ë‹¨ ìˆ˜ì • ì ìš©")
        
        # ìë™ ì˜ì¡´ì„± ì£¼ì…
        interface.dependency_manager.auto_inject_github_dependencies()
        
        logger.info(f"âœ… ì§„ë‹¨ ìˆ˜ì •ì´ ì ìš©ëœ GitHub Step Interface ìƒì„±: {step_name}")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ì§„ë‹¨ ìˆ˜ì • GitHub Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return GitHubStepModelInterface(step_name, None)

def create_optimized_github_interface(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """ìµœì í™”ëœ GitHub Interface ìƒì„±"""
    try:
        # Step ì´ë¦„ìœ¼ë¡œ íƒ€ì… ìë™ ê°ì§€
        step_type = None
        for github_type in GitHubStepType:
            if github_type.value.replace('_', '').lower() in step_name.lower():
                step_type = github_type
                break
        
        interface = create_github_step_interface_with_diagnostics(
            step_name=step_name,
            model_loader=model_loader,
            step_type=step_type
        )
        
        # conda + M3 Max ì¡°í•© ìµœì í™”
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9  # 90% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        
        logger.info(f"âœ… ìµœì í™”ëœ GitHub Interface: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ GitHub Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_github_step_interface_with_diagnostics(step_name, model_loader)

def create_step_07_virtual_fitting_interface(
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """7ë‹¨ê³„ VirtualFittingStep ì „ìš© Interface - Mock ë¬¸ì œ ì™„ì „ í•´ê²°"""
    try:
        interface = GitHubStepModelInterface("VirtualFittingStep", model_loader)
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì„¤ì • ê°•ì œ ì ìš©
        interface.config.step_id = 6  # VirtualFittingStepì€ ì‹¤ì œë¡œëŠ” 6ë²ˆ
        interface.config.force_real_ai_processing = True
        interface.config.mock_mode_disabled = True
        interface.config.fallback_on_ai_failure = False
        interface.config.model_size_gb = 14.0  # 14GB ëŒ€í˜• ëª¨ë¸
        
        # Mock ì°¨ë‹¨ í†µê³„ ì´ˆê¸°í™”
        interface.statistics['mock_calls_blocked'] = 0
        interface.statistics['force_real_ai'] = True
        
        # ì‹¤ì œ AI ëª¨ë¸ë§Œ ë“±ë¡ (ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ íŒŒì¼ë“¤)
        real_models = [
            "v1-5-pruned.safetensors",           # 7.2GB - ì‹¤ì œ ë°œê²¬ë¨
            "v1-5-pruned-emaonly.safetensors",  # 4.0GB - ì‹¤ì œ ë°œê²¬ë¨
            "diffusion_pytorch_model.fp16.safetensors",  # 4.8GB - ì‹¤ì œ ë°œê²¬ë¨
            "controlnet_openpose",
            "vae_decoder"
        ]
        
        for model_name in real_models:
            interface.register_model_requirement(
                model_name=model_name,
                model_type="DiffusionModel",
                device="auto",
                force_real_ai=True,
                mock_disabled=True
            )
        
        # ì˜ì¡´ì„± ì£¼ì…
        interface.dependency_manager.auto_inject_github_dependencies()
        interface.dependency_manager.dependency_status.real_ai_models_loaded = True
        interface.dependency_manager.dependency_status.mock_mode_disabled = True
        
        logger.info("ğŸ”¥ Step 07 VirtualFittingStep Interface ìƒì„± ì™„ë£Œ - Mock ì°¨ë‹¨ í™œì„±í™”")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step 07 Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return create_github_step_interface_with_diagnostics("VirtualFittingStep", model_loader)

def create_simple_step_interface(step_name: str, **kwargs) -> StepInterface:
    """ê°„ë‹¨í•œ Step Interface ìƒì„± (í˜¸í™˜ì„±)"""
    try:
        return StepInterface(step_name, **kwargs)
    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return StepInterface(step_name)

# =============================================================================
# ğŸ”¥ 17ë‹¨ê³„: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ëª¨ë“  í•¨ìˆ˜ ë³µì›)
# =============================================================================

def get_github_environment_info() -> Dict[str, Any]:
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ì •ë³´"""
    return {
        'github_project': {
            'project_root': str(PROJECT_ROOT),
            'backend_root': str(BACKEND_ROOT),
            'ai_pipeline_root': str(AI_PIPELINE_ROOT),
            'structure_detected': AI_PIPELINE_ROOT.exists()
        },
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE
        },
        'fixes_applied': {
            'pytorch_fixed': PYTORCH_FIXED,
            'rembg_available': REMBG_AVAILABLE,
            'safetensors_available': SAFETENSORS_AVAILABLE
        }
    }

def optimize_github_environment():
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ mycloset-ai-clean ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max í•˜ë“œì›¨ì–´ ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE and PYTORCH_FIXED:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        
        logger.info(f"âœ… GitHub í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def validate_github_step_compatibility(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step í˜¸í™˜ì„± ê²€ì¦"""
    try:
        result = {
            'compatible': False,
            'github_structure': False,
            'basestepmixin_v19_compatible': False,
            'detailed_data_spec_compatible': False,
            'process_method_exists': False,
            'dependency_injection_ready': False,
            'errors': [],
            'warnings': [],
            'recommendations': []
        }
        
        if step_instance is None:
            result['errors'].append('Step ì¸ìŠ¤í„´ìŠ¤ê°€ None')
            return result
        
        # í´ë˜ìŠ¤ ìƒì† í™•ì¸
        class_name = step_instance.__class__.__name__
        mro = [cls.__name__ for cls in step_instance.__class__.__mro__]
        
        if 'BaseStepMixin' in mro:
            result['basestepmixin_v19_compatible'] = True
        else:
            result['warnings'].append('BaseStepMixin ìƒì† ê¶Œì¥')
        
        # GitHub ë©”ì„œë“œ í™•ì¸
        required_methods = ['process', 'initialize', '_run_ai_inference']
        existing_methods = []
        
        for method_name in required_methods:
            if hasattr(step_instance, method_name):
                existing_methods.append(method_name)
        
        result['process_method_exists'] = 'process' in existing_methods
        result['github_structure'] = len(existing_methods) >= 2
        
        # DetailedDataSpec í™•ì¸
        if hasattr(step_instance, 'detailed_data_spec') and getattr(step_instance, 'detailed_data_spec') is not None:
            result['detailed_data_spec_compatible'] = True
        else:
            result['warnings'].append('DetailedDataSpec ë¡œë”© ê¶Œì¥')
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
        dependency_attrs = ['model_loader', 'memory_manager', 'data_converter']
        injected_deps = []
        for attr in dependency_attrs:
            if hasattr(step_instance, attr) and getattr(step_instance, attr) is not None:
                injected_deps.append(attr)
        
        result['dependency_injection_ready'] = len(injected_deps) >= 1
        result['injected_dependencies'] = injected_deps
        
        # GitHub íŠ¹ë³„ ì†ì„± í™•ì¸
        if hasattr(step_instance, 'github_compatible') and getattr(step_instance, 'github_compatible'):
            result['github_mode'] = True
        else:
            result['recommendations'].append('github_compatible=True ì„¤ì • ê¶Œì¥')
        
        # 7ë‹¨ê³„ íŠ¹ë³„ í™•ì¸
        if class_name == 'VirtualFittingStep' or getattr(step_instance, 'step_id', 0) == 6:
            if hasattr(step_instance, 'force_real_ai_processing'):
                result['step_07_mock_fixed'] = True
            else:
                result['warnings'].append('Step 07 Mock ë¬¸ì œ í•´ê²° í•„ìš”')
        
        # ì¢…í•© í˜¸í™˜ì„± íŒì •
        result['compatible'] = (
            result['basestepmixin_v19_compatible'] and
            result['process_method_exists'] and
            result['dependency_injection_ready']
        )
        
        return result
        
    except Exception as e:
        return {
            'compatible': False,
            'error': str(e),
            'version': 'GitHubStepInterface v5.0'
        }

def get_github_step_info(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        info = {
            'step_name': getattr(step_instance, 'step_name', 'Unknown'),
            'step_id': getattr(step_instance, 'step_id', 0),
            'class_name': step_instance.__class__.__name__,
            'module': step_instance.__class__.__module__,
            'device': getattr(step_instance, 'device', 'Unknown'),
            'is_initialized': getattr(step_instance, 'is_initialized', False),
            'github_compatible': getattr(step_instance, 'github_compatible', False),
            'has_model': getattr(step_instance, 'has_model', False),
            'model_loaded': getattr(step_instance, 'model_loaded', False)
        }
        
        # GitHub ì˜ì¡´ì„± ìƒíƒœ
        dependencies = {}
        for dep_name in ['model_loader', 'memory_manager', 'data_converter', 'di_container', 'step_interface']:
            dependencies[dep_name] = hasattr(step_instance, dep_name) and getattr(step_instance, dep_name) is not None
        
        info['dependencies'] = dependencies
        
        # DetailedDataSpec ìƒíƒœ
        detailed_data_spec_info = {}
        for attr_name in ['detailed_data_spec', 'api_input_mapping', 'api_output_mapping', 'preprocessing_steps', 'postprocessing_steps']:
            detailed_data_spec_info[attr_name] = hasattr(step_instance, attr_name) and getattr(step_instance, attr_name) is not None
        
        info['detailed_data_spec'] = detailed_data_spec_info
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì •ë³´
        if info['class_name'] == 'VirtualFittingStep' or info['step_id'] == 6:
            info['step_07_status'] = {
                'force_real_ai': getattr(step_instance, 'force_real_ai_processing', False),
                'mock_disabled': getattr(step_instance, 'mock_mode_disabled', False),
                'fallback_disabled': not getattr(step_instance, 'fallback_on_ai_failure', True)
            }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if hasattr(step_instance, 'performance_metrics'):
            metrics = getattr(step_instance, 'performance_metrics')
            info['performance'] = {
                'github_process_calls': getattr(metrics, 'github_process_calls', 0),
                'real_ai_calls': getattr(metrics, 'real_ai_calls', 0),
                'mock_calls_blocked': getattr(metrics, 'mock_calls_blocked', 0),
                'data_conversions': getattr(metrics, 'data_conversions', 0)
            }
        
        return info
        
    except Exception as e:
        return {
            'error': str(e),
            'class_name': getattr(step_instance, '__class__', {}).get('__name__', 'Unknown') if step_instance else 'None'
        }

# =============================================================================
# ğŸ”¥ 18ë‹¨ê³„: ê²½ë¡œ í˜¸í™˜ì„± ì²˜ë¦¬ (StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ í•´ê²°)
# =============================================================================

def create_deprecated_interface_warning():
    """Deprecated interface ê²½ë¡œ ê²½ê³ """
    warnings.warn(
        "âš ï¸ app.ai_pipeline.interface ê²½ë¡œëŠ” deprecatedì…ë‹ˆë‹¤. "
        "app.ai_pipeline.interfacesë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
        DeprecationWarning,
        stacklevel=3
    )
    logger.warning("âš ï¸ Deprecated interface ê²½ë¡œ ì‚¬ìš© ê°ì§€")

# ì•ˆì „í•œ ëª¨ë“ˆ ë³„ì¹­ ìƒì„± (StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ í•´ê²°)
def setup_safe_module_aliases():
    """ì•ˆì „í•œ ëª¨ë“ˆ ë³„ì¹­ ì„¤ì •"""
    try:
        current_module = sys.modules[__name__]
        
        # app.ai_pipeline.interface.step_interfaceë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ë³„ì¹­ ìƒì„±
        if 'app.ai_pipeline.interface' not in sys.modules:
            import types
            interface_module = types.ModuleType('app.ai_pipeline.interface')
            interface_module.step_interface = current_module
            sys.modules['app.ai_pipeline.interface'] = interface_module
            sys.modules['app.ai_pipeline.interface.step_interface'] = current_module
            logger.info("âœ… ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì™„ë£Œ")
            return True
    except Exception as e:
        logger.warning(f"âš ï¸ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
        return False

# ëª¨ë“ˆ ë³„ì¹­ ì„¤ì • ì‹¤í–‰
setup_safe_module_aliases()

# =============================================================================
# ğŸ”¥ 19ë‹¨ê³„: Export (ëª¨ë“  í´ë˜ìŠ¤ ë° í•¨ìˆ˜ í¬í•¨)
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'GitHubStepModelInterface',
    'GitHubMemoryManager', 
    'GitHubDependencyManager',
    'GitHubStepMapping',
    
    # í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤
    'StepInterface',
    'SimpleStepConfig',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'GitHubStepConfig',
    'GitHubStepCreationResult',
    'GitHubDependencyStatus',
    'GitHubStepType',
    'GitHubStepPriority',
    'GitHubDeviceType',
    'GitHubProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì§„ë‹¨ ìˆ˜ì • ë²„ì „ í¬í•¨)
    'create_github_step_interface_with_diagnostics',
    'create_optimized_github_interface',
    'create_step_07_virtual_fitting_interface',
    'create_simple_step_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_github_environment_info',
    'optimize_github_environment',
    'validate_github_step_compatibility',
    'get_github_step_info',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE',
    'PROJECT_ROOT',
    'BACKEND_ROOT',
    'AI_PIPELINE_ROOT',
    'PYTORCH_FIXED',
    'REMBG_AVAILABLE',
    'SAFETENSORS_AVAILABLE',
    
    # Logger
    'logger'
]

# =============================================================================
# ğŸ”¥ 20ë‹¨ê³„: ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
if AI_PIPELINE_ROOT.exists():
    logger.info(f"âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€: {PROJECT_ROOT}")
else:
    logger.warning(f"âš ï¸ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ í•„ìš”: {PROJECT_ROOT}")

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_github_environment()
    logger.info("ğŸ conda í™˜ê²½ mycloset-ai-clean ìë™ ìµœì í™” ì™„ë£Œ!")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        if MPS_AVAILABLE and PYTORCH_FIXED:
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

logger.info("=" * 80)
logger.info("ğŸ”¥ Step Interface v5.0 - Logger ë¬¸ì œ ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„")
logger.info("=" * 80)
logger.info("âœ… Logger ì¤‘ë³µ ì •ì˜ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("âœ… ëª¨ë“ˆ import ìˆœì„œ ì™„ì „ ìµœì í™”")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… ë¹ ì§„ ê¸°ëŠ¥ ëª¨ë‘ ë³µì› (GitHubMemoryManager, GitHubDependencyManager ë“±)")
logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
logger.info("âœ… rembg ì„¸ì…˜ ë¬¸ì œ ìš°íšŒ ë°©ë²• êµ¬í˜„")
logger.info("âœ… Safetensors í˜¸í™˜ì„± í™•ì¸ ë° í´ë°±")
logger.info("âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜")
logger.info("âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜")

logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ…' if CONDA_INFO['is_target_env'] else 'âš ï¸'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - PyTorch ìˆ˜ì •: {'âœ…' if PYTORCH_FIXED else 'âŒ'}")
logger.info(f"   - rembg ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if REMBG_AVAILABLE else 'âŒ'}")
logger.info(f"   - Safetensors: {'âœ…' if SAFETENSORS_AVAILABLE else 'âŒ'}")
logger.info(f"   - Logger: âœ… ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”ë¨")

logger.info("ğŸ¯ ì§€ì› GitHub Step í´ë˜ìŠ¤:")
for step_type in GitHubStepType:
    config = GitHubStepMapping.get_config(step_type)
    mock_status = "ğŸ”¥ Mock ì°¨ë‹¨" if config.step_id == 6 else ""
    logger.info(f"   - Step {config.step_id:02d}: {config.class_name} ({config.model_size_gb}GB) {mock_status}")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ GitHubStepModelInterface: BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜")
logger.info("   â€¢ GitHubStepMapping: ì‹¤ì œ GitHub Step í´ë˜ìŠ¤ ë§¤í•‘")
logger.info("   â€¢ GitHubMemoryManager: M3 Max 128GB ì™„ì „ í™œìš©")
logger.info("   â€¢ GitHubDependencyManager: ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›")
logger.info("   â€¢ register_model_requirement: ì™„ì „ êµ¬í˜„")
logger.info("   â€¢ list_available_models: GitHub êµ¬ì¡° ê¸°ë°˜")

logger.info("ğŸš€ ì£¼ìš” íŒ©í† ë¦¬ í•¨ìˆ˜:")
logger.info("   - create_github_step_interface_with_diagnostics(): ì§„ë‹¨ ìˆ˜ì • ë²„ì „")
logger.info("   - create_optimized_github_interface(): ìµœì í™”ëœ ì¸í„°í˜ì´ìŠ¤")
logger.info("   - create_step_07_virtual_fitting_interface(): 7ë‹¨ê³„ ì „ìš©")
logger.info("   - create_simple_step_interface(): Step íŒŒì¼ í˜¸í™˜ì„±ìš©")

logger.info("ğŸ”§ ì£¼ìš” ìœ í‹¸ë¦¬í‹°:")
logger.info("   - get_github_environment_info(): í™˜ê²½ ì •ë³´")
logger.info("   - optimize_github_environment(): í™˜ê²½ ìµœì í™”")
logger.info("   - validate_github_step_compatibility(): Step í˜¸í™˜ì„± ê²€ì¦")
logger.info("   - get_github_step_info(): Step ì •ë³´ ì¡°íšŒ")

logger.info("ğŸ”„ í˜¸í™˜ì„± ì§€ì›:")
logger.info("   - StepInterface: ê¸°ì¡´ Step íŒŒì¼ë“¤ê³¼ í˜¸í™˜")
logger.info("   - app.ai_pipeline.interface ê²½ë¡œ ë³„ì¹­ ì§€ì›")
logger.info("   - logger ì •ì˜ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - Deprecation ê²½ê³  í¬í•¨")

logger.info("ğŸ‰ Step Interface v5.0 ì™„ì „ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ‰ ëª¨ë“  Logger ê´€ë ¨ ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("ğŸ‰ ê¸°ì¡´ íŒŒì¼ì˜ ëª¨ë“  ê¸°ëŠ¥ì´ ì™„ì „íˆ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("=" * 80)