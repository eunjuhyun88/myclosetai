# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ Step Interface v5.1 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€
================================================================

âœ… BaseStepMixin ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨ (ì§€ì—° import)
âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€
âœ… Logger ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜
âœ… PyTorch weights_only ë¬¸ì œ í•´ê²°
âœ… M3 Max ìµœì í™” ìœ ì§€

Author: MyCloset AI Team
Date: 2025-07-30
Version: 5.1 (Circular Reference Fix)
"""

# =============================================================================
# ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (Logger ì „)
# =============================================================================

import os
import gc
import sys
import time
import warnings
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache
from contextlib import asynccontextmanager
import json
import hashlib

# =============================================================================
# ğŸ”¥ 2ë‹¨ê³„: Logger ì•ˆì „ ì´ˆê¸°í™” (ìµœìš°ì„ )
# =============================================================================

import logging

# Logger ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì „ì—­ ì„¤ì •
_LOGGER_INITIALIZED = False
_MODULE_LOGGER = None

def get_safe_logger():
    """Thread-safe Logger ì´ˆê¸°í™” (ì¤‘ë³µ ë°©ì§€)"""
    global _LOGGER_INITIALIZED, _MODULE_LOGGER
    
    if _LOGGER_INITIALIZED and _MODULE_LOGGER is not None:
        return _MODULE_LOGGER
    
    try:
        # í˜„ì¬ ëª¨ë“ˆì˜ Logger ìƒì„±
        logger_name = __name__
        _MODULE_LOGGER = logging.getLogger(logger_name)
        
        # í•¸ë“¤ëŸ¬ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
        if not _MODULE_LOGGER.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            _MODULE_LOGGER.addHandler(handler)
            _MODULE_LOGGER.setLevel(logging.INFO)
        
        _LOGGER_INITIALIZED = True
        return _MODULE_LOGGER
        
    except Exception as e:
        # ìµœí›„ í´ë°±: print ì‚¬ìš©
        print(f"âš ï¸ Logger ì´ˆê¸°í™” ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
        
        class FallbackLogger:
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def debug(self, msg): print(f"DEBUG: {msg}")
        
        return FallbackLogger()

# ëª¨ë“ˆ ë ˆë²¨ Logger (ë‹¨ í•œ ë²ˆë§Œ ì´ˆê¸°í™”)
logger = get_safe_logger()

# =============================================================================
# ğŸ”¥ 3ë‹¨ê³„: ê²½ê³  ë° ì—ëŸ¬ ì²˜ë¦¬ (Logger ì •ì˜ í›„)
# =============================================================================

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*deprecated.*')
warnings.filterwarnings('ignore', category=ImportWarning)

# =============================================================================
# ğŸ”¥ 4ë‹¨ê³„: TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# =============================================================================

if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core import DIContainer
    from ..steps.base_step_mixin import BaseStepMixin

# =============================================================================
# ğŸ”¥ 5ë‹¨ê³„: ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ í•´ê²°
# =============================================================================

# 1. PyTorch weights_only ë¬¸ì œ í•´ê²°
PYTORCH_FIXED = False
try:
    import torch
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            if weights_only is None:
                weights_only = False
            return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=weights_only, **kwargs)
        torch.load = safe_torch_load
        PYTORCH_FIXED = True
        logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# 2. rembg ì„¸ì…˜ ë¬¸ì œ í•´ê²°
REMBG_AVAILABLE = False
try:
    import rembg
    if hasattr(rembg, 'sessions'):
        REMBG_AVAILABLE = True
        logger.info("âœ… rembg ì„¸ì…˜ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ rembg ì„¸ì…˜ ëª¨ë“ˆ í˜¸í™˜ì„± ë¬¸ì œ - í´ë°± ëª¨ë“œ ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ rembg ëª¨ë“ˆ ë¬¸ì œ: {e}")

# 3. Safetensors í˜¸í™˜ì„± í™•ì¸  
SAFETENSORS_AVAILABLE = False
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
    logger.info("âœ… Safetensors ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    logger.warning("âš ï¸ Safetensors ì‚¬ìš© ë¶ˆê°€ - .pth íŒŒì¼ ìš°ì„  ì‚¬ìš©")

# =============================================================================
# ğŸ”¥ 6ë‹¨ê³„: GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ê°ì§€
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ ì •ë³´
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
BACKEND_ROOT = PROJECT_ROOT / "backend"
AI_PIPELINE_ROOT = BACKEND_ROOT / "app" / "ai_pipeline"

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
    'project_path': str(PROJECT_ROOT)
}

# í•˜ë“œì›¨ì–´ ê°ì§€
IS_M3_MAX = False
MEMORY_GB = 16.0
MPS_AVAILABLE = False

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
    
    # MPS ê°ì§€
    if PYTORCH_FIXED:
        MPS_AVAILABLE = (
            IS_M3_MAX and 
            hasattr(torch.backends, 'mps') and 
            torch.backends.mps.is_available()
        )
except Exception:
    pass

logger.info(f"ğŸ”§ í™˜ê²½ ì •ë³´: conda={CONDA_INFO['conda_env']}, M3_Max={IS_M3_MAX}, MPS={MPS_AVAILABLE}")

# =============================================================================
# ğŸ”¥ 7ë‹¨ê³„: GitHub Step íƒ€ì… ë° ìƒìˆ˜ ì •ì˜
# =============================================================================

class GitHubStepType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class GitHubStepPriority(Enum):
    """GitHub Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class GitHubDeviceType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"

class GitHubProcessingStatus(Enum):
    """GitHub Step ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"
    MOCK_MODE = "mock_mode"

# =============================================================================
# ğŸ”¥ 8ë‹¨ê³„: ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì (ìˆœí™˜ì°¸ì¡° í•´ê²°)
# =============================================================================

class EmbeddedDependencyManager:
    """ğŸ”¥ ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì v5.1 - ìˆœí™˜ì°¸ì¡° ì°¨ë‹¨"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = get_safe_logger()
        
        # í•µì‹¬ ì†ì„±ë“¤
        self.step_instance = None
        self.injected_dependencies = {}
        self.dependencies = {}
        
        # ì‹œê°„ ì¶”ì 
        self.last_injection_time = time.time()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.dependencies_injected = 0
        self.injection_failures = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        self.logger.debug(f"âœ… EmbeddedDependencyManager v5.1 ì´ˆê¸°í™” ì™„ë£Œ: {step_name}")
    
    def set_step_instance(self, step_instance):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
        try:
            with self._lock:
                self.step_instance = step_instance
                self.logger.debug(f"âœ… {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì… (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} EmbeddedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                success_count = 0
                total_dependencies = 0
                
                # ModelLoader ìë™ ì£¼ì… (ì§€ì—° import)
                if not hasattr(self.step_instance, 'model_loader') or self.step_instance.model_loader is None:
                    total_dependencies += 1
                    try:
                        model_loader = self._resolve_model_loader()
                        if model_loader:
                            self.step_instance.model_loader = model_loader
                            self.injected_dependencies['model_loader'] = model_loader
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} ModelLoader ìë™ ì£¼ì… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # MemoryManager ìë™ ì£¼ì… (ì§€ì—° import)
                if not hasattr(self.step_instance, 'memory_manager') or self.step_instance.memory_manager is None:
                    total_dependencies += 1
                    try:
                        memory_manager = self._resolve_memory_manager()
                        if memory_manager:
                            self.step_instance.memory_manager = memory_manager
                            self.injected_dependencies['memory_manager'] = memory_manager
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} MemoryManager ìë™ ì£¼ì… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # DataConverter ìë™ ì£¼ì… (ì§€ì—° import)
                if not hasattr(self.step_instance, 'data_converter') or self.step_instance.data_converter is None:
                    total_dependencies += 1
                    try:
                        data_converter = self._resolve_data_converter()
                        if data_converter:
                            self.step_instance.data_converter = data_converter
                            self.injected_dependencies['data_converter'] = data_converter
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} DataConverter ìë™ ì£¼ì… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} DataConverter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                if total_dependencies == 0:
                    self.logger.info(f"âœ… {self.step_name} ëª¨ë“  ì˜ì¡´ì„±ì´ ì´ë¯¸ ì£¼ì…ë˜ì–´ ìˆìŒ")
                    return True
                
                success_rate = success_count / total_dependencies if total_dependencies > 0 else 1.0
                
                if success_count > 0:
                    self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}/{total_dependencies} ({success_rate*100:.1f}%)")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} ì‹¤ì œ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {success_count}/{total_dependencies}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì¤‘ ì˜¤ë¥˜: {e}")
            self.injection_failures += 1
            return False
    
    def _resolve_model_loader(self):
        """ModelLoader í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.model_loader')
                if hasattr(module, 'get_global_model_loader'):
                    loader = module.get_global_model_loader()
                    if loader and not isinstance(loader, bool):
                        return loader
            except ImportError:
                self.logger.debug(f"{self.step_name} ModelLoader ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_memory_manager(self):
        """MemoryManager í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
                if hasattr(module, 'get_global_memory_manager'):
                    manager = module.get_global_memory_manager()
                    if manager:
                        return manager
            except ImportError:
                self.logger.debug(f"{self.step_name} MemoryManager ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_data_converter(self):
        """DataConverter í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.data_converter')
                if hasattr(module, 'get_global_data_converter'):
                    converter = module.get_global_data_converter()
                    if converter:
                        return converter
            except ImportError:
                self.logger.debug(f"{self.step_name} DataConverter ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def inject_model_loader(self, model_loader):
        """ModelLoader ì£¼ì…"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if model_loader is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} ModelLoaderê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                if isinstance(model_loader, bool):
                    self.logger.error(f"âŒ {self.step_name} ModelLoaderëŠ” boolì´ ì•„ë‹Œ ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤")
                    return False
                
                # ì£¼ì… ì‹¤í–‰
                self.step_instance.model_loader = model_loader
                self.injected_dependencies['model_loader'] = model_loader
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_memory_manager(self, memory_manager):
        """MemoryManager ì£¼ì…"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if memory_manager is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} MemoryManagerê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                # ì£¼ì… ì‹¤í–‰
                self.step_instance.memory_manager = memory_manager
                self.injected_dependencies['memory_manager'] = memory_manager
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} MemoryManager ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} EmbeddedDependencyManager ì •ë¦¬ ì‹œì‘...")
                
                # ì£¼ì…ëœ ì˜ì¡´ì„±ë“¤ ì •ë¦¬
                for dep_name, dep_instance in self.injected_dependencies.items():
                    try:
                        if hasattr(dep_instance, 'cleanup'):
                            dep_instance.cleanup()
                        elif hasattr(dep_instance, 'close'):
                            dep_instance.close()
                    except Exception as e:
                        self.logger.debug(f"ì˜ì¡´ì„± ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({dep_name}): {e}")
                
                # ìƒíƒœ ì´ˆê¸°í™”
                self.injected_dependencies.clear()
                self.dependencies.clear()
                self.step_instance = None
                
                self.logger.info(f"âœ… {self.step_name} EmbeddedDependencyManager ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} EmbeddedDependencyManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 9ë‹¨ê³„: GitHub Step ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

@dataclass
class GitHubStepConfig:
    """GitHub BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜ ì„¤ì •"""
    # ê¸°ë³¸ Step ì •ë³´
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    module_path: str = ""
    
    # GitHub Step íƒ€ì…
    step_type: GitHubStepType = GitHubStepType.HUMAN_PARSING
    priority: GitHubStepPriority = GitHubStepPriority.MEDIUM
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # GitHub BaseStepMixin í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_support: bool = True
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = False
    require_step_interface: bool = True
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 1.0
    primary_model_file: str = ""
    checkpoint_patterns: List[str] = field(default_factory=list)
    
    # GitHub í™˜ê²½ ìµœì í™”
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    
    # DetailedDataSpec ì„¤ì •
    enable_detailed_data_spec: bool = True
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # 7ë‹¨ê³„ Mock ë¬¸ì œ í•´ê²° ì„¤ì •
    force_real_ai_processing: bool = True
    mock_mode_disabled: bool = True
    fallback_on_ai_failure: bool = False
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ GitHub í™˜ê²½ ìµœì í™”"""
        # conda í™˜ê²½ ìë™ ìµœì í™”
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            # M3 Max + conda ì¡°í•© ìµœì í™”
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.step_name == "VirtualFittingStep" or self.step_id == 7:
            self.force_real_ai_processing = True
            self.mock_mode_disabled = True
            self.fallback_on_ai_failure = False
        
        # AI ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        if not isinstance(self.ai_models, list):
            self.ai_models = []

# =============================================================================
# ğŸ”¥ 10ë‹¨ê³„: GitHub Step ë§¤í•‘ ì‹œìŠ¤í…œ
# =============================================================================

class GitHubStepMapping:
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step ë§¤í•‘"""
    
    GITHUB_STEP_CONFIGS = {
        GitHubStepType.HUMAN_PARSING: GitHubStepConfig(
            step_name="HumanParsingStep",
            step_id=1,
            class_name="HumanParsingStep",
            module_path="app.ai_pipeline.steps.step_01_human_parsing",
            step_type=GitHubStepType.HUMAN_PARSING,
            priority=GitHubStepPriority.HIGH,
            ai_models=["graphonomy.pth", "atr_model.pth"],
            model_size_gb=4.0,
            primary_model_file="graphonomy.pth"
        ),
        
        GitHubStepType.POSE_ESTIMATION: GitHubStepConfig(
            step_name="PoseEstimationStep",
            step_id=2,
            class_name="PoseEstimationStep",
            module_path="app.ai_pipeline.steps.step_02_pose_estimation",
            step_type=GitHubStepType.POSE_ESTIMATION,
            priority=GitHubStepPriority.MEDIUM,
            ai_models=["pose_model.pth", "openpose_model.pth"],
            model_size_gb=3.4,
            primary_model_file="pose_model.pth"
        ),
        
        GitHubStepType.CLOTH_SEGMENTATION: GitHubStepConfig(
            step_name="ClothSegmentationStep",
            step_id=3,
            class_name="ClothSegmentationStep",
            module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
            step_type=GitHubStepType.CLOTH_SEGMENTATION,
            priority=GitHubStepPriority.MEDIUM,
            ai_models=["sam_vit_h_4b8939.pth", "u2net.pth"],
            model_size_gb=5.5,
            primary_model_file="sam_vit_h_4b8939.pth"
        ),
        
        GitHubStepType.GEOMETRIC_MATCHING: GitHubStepConfig(
            step_name="GeometricMatchingStep",
            step_id=4,
            class_name="GeometricMatchingStep",
            module_path="app.ai_pipeline.steps.step_04_geometric_matching",
            step_type=GitHubStepType.GEOMETRIC_MATCHING,
            priority=GitHubStepPriority.LOW,
            ai_models=["geometric_matching.pth", "tps_model.pth"],
            model_size_gb=1.3,
            primary_model_file="geometric_matching.pth"
        ),
        
        GitHubStepType.CLOTH_WARPING: GitHubStepConfig(
            step_name="ClothWarpingStep",
            step_id=5,
            class_name="ClothWarpingStep",
            module_path="app.ai_pipeline.steps.step_05_cloth_warping",
            step_type=GitHubStepType.CLOTH_WARPING,
            priority=GitHubStepPriority.HIGH,
            ai_models=["RealVisXL_V4.0.safetensors", "cloth_warping.pth"],
            model_size_gb=7.0,
            primary_model_file="RealVisXL_V4.0.safetensors"
        ),
        
        GitHubStepType.VIRTUAL_FITTING: GitHubStepConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            class_name="VirtualFittingStep",
            module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
            step_type=GitHubStepType.VIRTUAL_FITTING,
            priority=GitHubStepPriority.CRITICAL,
            ai_models=[
                "v1-5-pruned.safetensors",
                "v1-5-pruned-emaonly.safetensors",
                "controlnet_openpose",
                "vae_decoder"
            ],
            model_size_gb=14.0,
            primary_model_file="v1-5-pruned.safetensors",
            force_real_ai_processing=True,
            mock_mode_disabled=True,
            fallback_on_ai_failure=False
        ),
        
        GitHubStepType.POST_PROCESSING: GitHubStepConfig(
            step_name="PostProcessingStep",
            step_id=7,
            class_name="PostProcessingStep",
            module_path="app.ai_pipeline.steps.step_07_post_processing",
            step_type=GitHubStepType.POST_PROCESSING,
            priority=GitHubStepPriority.LOW,
            ai_models=["super_resolution.pth", "enhancement.pth"],
            model_size_gb=1.3,
            primary_model_file="super_resolution.pth"
        ),
        
        GitHubStepType.QUALITY_ASSESSMENT: GitHubStepConfig(
            step_name="QualityAssessmentStep",
            step_id=8,
            class_name="QualityAssessmentStep",
            module_path="app.ai_pipeline.steps.step_08_quality_assessment",
            step_type=GitHubStepType.QUALITY_ASSESSMENT,
            priority=GitHubStepPriority.CRITICAL,
            ai_models=["open_clip_pytorch_model.bin", "ViT-L-14.pt"],
            model_size_gb=7.0,
            primary_model_file="open_clip_pytorch_model.bin"
        )
    }
    
    @classmethod
    def get_config(cls, step_type: GitHubStepType) -> GitHubStepConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        return cls.GITHUB_STEP_CONFIGS.get(step_type, GitHubStepConfig())
    
    @classmethod
    def get_config_by_name(cls, step_name: str) -> Optional[GitHubStepConfig]:
        """Step ì´ë¦„ìœ¼ë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_name == step_name or config.class_name == step_name:
                return config
        return None
    
    @classmethod
    def get_config_by_id(cls, step_id: int) -> Optional[GitHubStepConfig]:
        """Step IDë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_id == step_id:
                return config
        return None

# =============================================================================
# ğŸ”¥ 11ë‹¨ê³„: GitHub ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

class GitHubMemoryManager:
    """GitHub í”„ë¡œì íŠ¸ìš© ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = get_safe_logger()
        
        # M3 Max ìë™ ìµœì í™”
        if max_memory_gb is None:
            if IS_M3_MAX and MEMORY_GB >= 64:
                self.max_memory_gb = MEMORY_GB * 0.85
            elif IS_M3_MAX:
                self.max_memory_gb = MEMORY_GB * 0.8
            elif CONDA_INFO['is_target_env']:
                self.max_memory_gb = 12.0
            else:
                self.max_memory_gb = 8.0
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        
        self.logger.info(f"ğŸ§  GitHub ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner}")
                return True
            else:
                available = self.max_memory_gb - self.current_memory_gb
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {available:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner}")
                return size_gb
            return 0.0
    
    def optimize_for_github_project(self):
        """GitHub í”„ë¡œì íŠ¸ íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max)
            if self.mps_enabled and PYTORCH_FIXED:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    self.logger.debug("ğŸ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬")
            
            # Python GC
            gc.collect()
            
            # 128GB M3 Max íŠ¹ë³„ ìµœì í™”
            if IS_M3_MAX and MEMORY_GB >= 128:
                self.max_memory_gb = min(MEMORY_GB * 0.9, 115.0)
                self.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'memory_pool': self.memory_pool.copy(),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'total_system_gb': MEMORY_GB,
                'github_optimized': True
            }

# =============================================================================
# ğŸ”¥ 12ë‹¨ê³„: GitHub Step Model Interface (í•µì‹¬ í´ë˜ìŠ¤)
# =============================================================================

class GitHubStepModelInterface:
    """
    ğŸ”¥ GitHub Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ v5.1 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì ì‚¬ìš©)
    âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models ì •í™• êµ¬í˜„
    âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ í•´ê²°
    âœ… PyTorch weights_only ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        
        # GitHub ì„¤ì • ìë™ ë¡œë”©
        self.config = GitHubStepMapping.get_config_by_name(step_name)
        if not self.config:
            self.config = GitHubStepConfig(step_name=step_name)
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = GitHubMemoryManager()
        
        # ğŸ”¥ ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì (ìˆœí™˜ì°¸ì¡° í•´ê²°)
        self.dependency_manager = EmbeddedDependencyManager(step_name)
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'mock_calls_blocked': 0,
            'creation_time': time.time()
        }
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.config.step_id == 6:  # VirtualFittingStep
            self.statistics['force_real_ai'] = True
            self.logger.info(f"ğŸ”¥ {step_name}: ì‹¤ì œ AI ëª¨ë¸ ê°•ì œ ì‚¬ìš© ëª¨ë“œ í™œì„±í™”")
        
        self.logger.info(f"ğŸ”— GitHub {step_name} Interface v5.1 ìˆœí™˜ì°¸ì¡° í•´ê²° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # GitHub ì„¤ì • ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'step_id': self.config.step_id,
                    'device': kwargs.get('device', self.config.device),
                    'precision': 'fp16' if self.config.use_fp16 else 'fp32',
                    'github_compatible': True,
                    'force_real_ai': self.config.force_real_ai_processing,
                    'mock_disabled': self.config.mock_mode_disabled,
                    'registered_at': time.time(),
                    'pytorch_fixed': PYTORCH_FIXED,
                    'rembg_available': REMBG_AVAILABLE,
                    'safetensors_available': SAFETENSORS_AVAILABLE,
                    'metadata': {
                        'module_path': self.config.module_path,
                        'class_name': self.config.class_name,
                        'primary_model_file': self.config.primary_model_file,
                        **kwargs.get('metadata', {})
                    }
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'step_id': self.config.step_id,
                    'loaded': False,
                    'size_mb': self.config.model_size_gb * 1024,
                    'device': requirement['device'],
                    'status': 'registered',
                    'github_compatible': True,
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - GitHub êµ¬ì¡° ê¸°ë°˜"""
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    requirement = registry_entry.get('requirement', {})
                    
                    # GitHub í‘œì¤€ ëª¨ë¸ ì •ë³´
                    model_info = {
                        'name': model_name,
                        'path': f"ai_models/step_{requirement.get('step_id', self.config.step_id):02d}_{self.step_name.lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'size_gb': round(registry_entry['size_mb'] / 1024, 2),
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'step_id': registry_entry['step_id'],
                        'loaded': registry_entry['loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'github_compatible': registry_entry.get('github_compatible', True),
                        'force_real_ai': requirement.get('force_real_ai', False),
                        'mock_disabled': requirement.get('mock_disabled', False),
                        'pytorch_fixed': requirement.get('pytorch_fixed', PYTORCH_FIXED),
                        'rembg_available': requirement.get('rembg_available', REMBG_AVAILABLE),
                        'safetensors_available': requirement.get('safetensors_available', SAFETENSORS_AVAILABLE),
                        'metadata': {
                            'step_name': self.step_name,
                            'conda_env': CONDA_INFO['conda_env'],
                            'registered_at': requirement.get('registered_at', 0),
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ì¡°íšŒ
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'size_gb': round(model.get('size_mb', 0.0) / 1024, 2),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'step_id': self.config.step_id,
                                    'loaded': model.get('loaded', False),
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'github_compatible': False,
                                    'force_real_ai': False,
                                    'mock_disabled': False,
                                    'pytorch_fixed': PYTORCH_FIXED,
                                    'rembg_available': REMBG_AVAILABLE,
                                    'safetensors_available': SAFETENSORS_AVAILABLE,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                elif sort_by == "step_id":
                    models.sort(key=lambda x: x['step_id'])
                else:
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ GitHub ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - PyTorch weights_only ë¬¸ì œ í•´ê²°"""
        try:
            with self._lock:
                # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬: Mock ë°ì´í„° ì°¨ë‹¨
                if self.config.step_id == 6 and ('mock' in model_name.lower() or 'test' in model_name.lower()):
                    self.statistics['mock_calls_blocked'] += 1
                    self.logger.warning(f"ğŸ”¥ {self.step_name}: Mock ëª¨ë¸ í˜¸ì¶œ ì°¨ë‹¨ - {model_name}")
                    return None
                
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.statistics['real_ai_calls'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # PyTorch ë¡œë”© ë¬¸ì œ í•´ê²°
                loading_kwargs = kwargs.copy()
                if PYTORCH_FIXED and 'weights_only' not in loading_kwargs:
                    loading_kwargs['weights_only'] = False
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    try:
                        model = self.model_loader.load_model(model_name, **loading_kwargs)
                    except Exception as load_error:
                        # PyTorch ë¡œë”© ì˜¤ë¥˜ ì¬ì‹œë„
                        if PYTORCH_FIXED and ('weights_only' in str(load_error) or 'WeightsUnpickler' in str(load_error)):
                            self.logger.warning(f"âš ï¸ PyTorch weights_only ì˜¤ë¥˜ ê°ì§€, ì¬ì‹œë„: {model_name}")
                            loading_kwargs['weights_only'] = False
                            loading_kwargs['map_location'] = 'cpu'
                            model = self.model_loader.load_model(model_name, **loading_kwargs)
                        else:
                            raise load_error
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        self.statistics['real_ai_calls'] += 1
                        
                        self.logger.info(f"âœ… GitHub ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            
            # ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ íŠ¹ì • ì˜¤ë¥˜ ì²˜ë¦¬
            if 'constants.pkl' in str(e):
                self.logger.warning(f"ğŸ”§ Mobile SAM ëª¨ë¸ íŒŒì¼ ì†ìƒ ê°ì§€: {model_name}")
            elif 'Expected hasRecord' in str(e):
                self.logger.warning(f"ğŸ”§ Graphonomy ëª¨ë¸ ë²„ì „ ë¬¸ì œ ê°ì§€: {model_name}")
            elif 'Unsupported operand' in str(e):
                self.logger.warning(f"ğŸ”§ U2Net ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€: {model_name}")
            
            return None
    
    # BaseStepMixin v19.1 í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜ ë³„ì¹­"""
        return self.get_model_sync(model_name, **kwargs)
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        if model_name:
            return self.get_model_sync(model_name, **kwargs)
        return None
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ í•´ì œ
            for model_name in self._model_cache:
                self.memory_manager.deallocate_memory(model_name)
            
            self._model_cache.clear()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.dependency_manager.cleanup()
            
            self.logger.info(f"âœ… GitHub {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ GitHub Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 13ë‹¨ê³„: Step ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
# =============================================================================

@dataclass
class GitHubStepCreationResult:
    """GitHub Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    step_type: Optional[GitHubStepType] = None
    class_name: str = ""
    module_path: str = ""
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # GitHub ì˜ì¡´ì„± ì£¼ì… ê²°ê³¼
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    ai_models_loaded: List[str] = field(default_factory=list)
    
    # GitHub BaseStepMixin v19.1 í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    dependency_injection_success: bool = False
    
    # ìˆœí™˜ì°¸ì¡° í•´ê²° ìƒíƒœ
    circular_reference_free: bool = True
    embedded_dependency_manager: bool = True
    real_ai_processing_enabled: bool = False
    
    # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 14ë‹¨ê³„: Step íŒŒì¼ë“¤ì„ ìœ„í•œ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ (í˜¸í™˜ì„±)
# =============================================================================

class StepInterface:
    """Step íŒŒì¼ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, step_name: str, model_loader=None, **kwargs):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        self.config = kwargs
        
        # ê¸°ë³¸ ì†ì„±ë“¤
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.initialized = False
        
        self.logger.debug(f"âœ… StepInterface ìƒì„±: {step_name}")
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (í˜¸í™˜ì„±)"""
        try:
            self.logger.debug(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, **kwargs) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í˜¸í™˜ì„±)"""
        return []
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# =============================================================================
# ğŸ”¥ 15ë‹¨ê³„: ë‹¨ìˆœí•œ í´ë°± í´ë˜ìŠ¤ë“¤ (Step íŒŒì¼ í˜¸í™˜ì„±ìš©)
# =============================================================================

class SimpleStepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì • (í´ë°±ìš©)"""
    def __init__(self, **kwargs):
        self.step_name = kwargs.get('step_name', 'Unknown')
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.model_size_gb = kwargs.get('model_size_gb', 1.0)
        self.ai_models = kwargs.get('ai_models', [])
        
        # ëª¨ë“  kwargsë¥¼ ì†ì„±ìœ¼ë¡œ ì„¤ì •
        for key, value in kwargs.items():
            if not hasattr(self, key):
                setattr(self, key, value)

# =============================================================================
# ğŸ”¥ 16ë‹¨ê³„: íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „)
# =============================================================================

def create_github_step_interface_circular_reference_free(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    step_type: Optional[GitHubStepType] = None
) -> GitHubStepModelInterface:
    """ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°ëœ GitHub Step Interface ìƒì„±"""
    try:
        interface = GitHubStepModelInterface(step_name, model_loader)
        
        # Step íƒ€ì…ë³„ ì¶”ê°€ ì„¤ì •
        if step_type:
            config = GitHubStepMapping.get_config(step_type)
            interface.config = config
        
        # ìˆœí™˜ì°¸ì¡° í•´ê²° ì ìš©
        if IS_M3_MAX and MEMORY_GB >= 128:
            interface.memory_manager = GitHubMemoryManager(115.0)
            interface.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
        
        # 7ë‹¨ê³„ Mock ì°¨ë‹¨ ê°•í™”
        if step_name == "VirtualFittingStep" or (interface.config and interface.config.step_id == 6):
            interface.statistics['force_real_ai'] = True
            interface.statistics['circular_reference_free'] = True
            interface.logger.info(f"ğŸ”¥ Step 06 VirtualFittingStep ìˆœí™˜ì°¸ì¡° í•´ê²° ì ìš©")
        
        # ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì ìë™ ì£¼ì…
        interface.dependency_manager.auto_inject_dependencies()
        
        logger.info(f"âœ… ìˆœí™˜ì°¸ì¡° í•´ê²°ëœ GitHub Step Interface ìƒì„±: {step_name}")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìˆœí™˜ì°¸ì¡° í•´ê²° GitHub Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return GitHubStepModelInterface(step_name, None)

def create_optimized_github_interface_v51(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """ìµœì í™”ëœ GitHub Interface ìƒì„± v5.1"""
    try:
        # Step ì´ë¦„ìœ¼ë¡œ íƒ€ì… ìë™ ê°ì§€
        step_type = None
        for github_type in GitHubStepType:
            if github_type.value.replace('_', '').lower() in step_name.lower():
                step_type = github_type
                break
        
        interface = create_github_step_interface_circular_reference_free(
            step_name=step_name,
            model_loader=model_loader,
            step_type=step_type
        )
        
        # conda + M3 Max ì¡°í•© ìµœì í™”
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9  # 90% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        
        logger.info(f"âœ… ìµœì í™”ëœ GitHub Interface v5.1: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ GitHub Interface v5.1 ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_github_step_interface_circular_reference_free(step_name, model_loader)

def create_step_07_virtual_fitting_interface_v51(
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """7ë‹¨ê³„ VirtualFittingStep ì „ìš© Interface v5.1 - ìˆœí™˜ì°¸ì¡° í•´ê²°"""
    try:
        interface = GitHubStepModelInterface("VirtualFittingStep", model_loader)
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì„¤ì • ê°•ì œ ì ìš©
        interface.config.step_id = 6  # VirtualFittingStepì€ ì‹¤ì œë¡œëŠ” 6ë²ˆ
        interface.config.force_real_ai_processing = True
        interface.config.mock_mode_disabled = True
        interface.config.fallback_on_ai_failure = False
        interface.config.model_size_gb = 14.0  # 14GB ëŒ€í˜• ëª¨ë¸
        
        # Mock ì°¨ë‹¨ í†µê³„ ì´ˆê¸°í™”
        interface.statistics['mock_calls_blocked'] = 0
        interface.statistics['force_real_ai'] = True
        interface.statistics['circular_reference_free'] = True
        
        # ì‹¤ì œ AI ëª¨ë¸ë§Œ ë“±ë¡
        real_models = [
            "v1-5-pruned.safetensors",
            "v1-5-pruned-emaonly.safetensors",
            "diffusion_pytorch_model.fp16.safetensors",
            "controlnet_openpose",
            "vae_decoder"
        ]
        
        for model_name in real_models:
            interface.register_model_requirement(
                model_name=model_name,
                model_type="DiffusionModel",
                device="auto",
                force_real_ai=True,
                mock_disabled=True
            )
        
        # ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì í™œìš©
        interface.dependency_manager.auto_inject_dependencies()
        
        logger.info("ğŸ”¥ Step 07 VirtualFittingStep Interface v5.1 ìƒì„± ì™„ë£Œ - ìˆœí™˜ì°¸ì¡° í•´ê²°")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step 07 Interface v5.1 ìƒì„± ì‹¤íŒ¨: {e}")
        return create_github_step_interface_circular_reference_free("VirtualFittingStep", model_loader)

def create_simple_step_interface(step_name: str, **kwargs) -> StepInterface:
    """ê°„ë‹¨í•œ Step Interface ìƒì„± (í˜¸í™˜ì„±)"""
    try:
        return StepInterface(step_name, **kwargs)
    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return StepInterface(step_name)

# =============================================================================
# ğŸ”¥ 17ë‹¨ê³„: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „)
# =============================================================================

def get_github_environment_info() -> Dict[str, Any]:
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ì •ë³´"""
    return {
        'github_project': {
            'project_root': str(PROJECT_ROOT),
            'backend_root': str(BACKEND_ROOT),
            'ai_pipeline_root': str(AI_PIPELINE_ROOT),
            'structure_detected': AI_PIPELINE_ROOT.exists()
        },
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE
        },
        'fixes_applied': {
            'pytorch_fixed': PYTORCH_FIXED,
            'rembg_available': REMBG_AVAILABLE,
            'safetensors_available': SAFETENSORS_AVAILABLE,
            'circular_reference_resolved': True
        }
    }

def optimize_github_environment():
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ mycloset-ai-clean ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max í•˜ë“œì›¨ì–´ ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE and PYTORCH_FIXED:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        optimizations.append("ìˆœí™˜ì°¸ì¡° í•´ê²° ì ìš©")
        
        logger.info(f"âœ… GitHub í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def validate_github_step_compatibility(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step í˜¸í™˜ì„± ê²€ì¦ (ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „)"""
    try:
        result = {
            'compatible': False,
            'github_structure': False,
            'basestepmixin_v19_compatible': False,
            'detailed_data_spec_compatible': False,
            'process_method_exists': False,
            'dependency_injection_ready': False,
            'circular_reference_free': True,
            'embedded_dependency_manager': False,
            'errors': [],
            'warnings': [],
            'recommendations': []
        }
        
        if step_instance is None:
            result['errors'].append('Step ì¸ìŠ¤í„´ìŠ¤ê°€ None')
            return result
        
        # í´ë˜ìŠ¤ ìƒì† í™•ì¸
        class_name = step_instance.__class__.__name__
        mro = [cls.__name__ for cls in step_instance.__class__.__mro__]
        
        if 'BaseStepMixin' in mro:
            result['basestepmixin_v19_compatible'] = True
        else:
            result['warnings'].append('BaseStepMixin ìƒì† ê¶Œì¥')
        
        # GitHub ë©”ì„œë“œ í™•ì¸
        required_methods = ['process', 'initialize', '_run_ai_inference']
        existing_methods = []
        
        for method_name in required_methods:
            if hasattr(step_instance, method_name):
                existing_methods.append(method_name)
        
        result['process_method_exists'] = 'process' in existing_methods
        result['github_structure'] = len(existing_methods) >= 2
        
        # DetailedDataSpec í™•ì¸
        if hasattr(step_instance, 'detailed_data_spec') and getattr(step_instance, 'detailed_data_spec') is not None:
            result['detailed_data_spec_compatible'] = True
        else:
            result['warnings'].append('DetailedDataSpec ë¡œë”© ê¶Œì¥')
        
        # ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì í™•ì¸
        if hasattr(step_instance, 'dependency_manager'):
            if isinstance(getattr(step_instance, 'dependency_manager'), EmbeddedDependencyManager):
                result['embedded_dependency_manager'] = True
                result['circular_reference_free'] = True
            else:
                result['warnings'].append('EmbeddedDependencyManager ì‚¬ìš© ê¶Œì¥')
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
        dependency_attrs = ['model_loader', 'memory_manager', 'data_converter']
        injected_deps = []
        for attr in dependency_attrs:
            if hasattr(step_instance, attr) and getattr(step_instance, attr) is not None:
                injected_deps.append(attr)
        
        result['dependency_injection_ready'] = len(injected_deps) >= 1
        result['injected_dependencies'] = injected_deps
        
        # GitHub íŠ¹ë³„ ì†ì„± í™•ì¸
        if hasattr(step_instance, 'github_compatible') and getattr(step_instance, 'github_compatible'):
            result['github_mode'] = True
        else:
            result['recommendations'].append('github_compatible=True ì„¤ì • ê¶Œì¥')
        
        # 7ë‹¨ê³„ íŠ¹ë³„ í™•ì¸
        if class_name == 'VirtualFittingStep' or getattr(step_instance, 'step_id', 0) == 6:
            if hasattr(step_instance, 'force_real_ai_processing'):
                result['step_07_mock_fixed'] = True
            else:
                result['warnings'].append('Step 07 Mock ë¬¸ì œ í•´ê²° í•„ìš”')
        
        # ì¢…í•© í˜¸í™˜ì„± íŒì •
        result['compatible'] = (
            result['basestepmixin_v19_compatible'] and
            result['process_method_exists'] and
            result['dependency_injection_ready'] and
            result['circular_reference_free']
        )
        
        return result
        
    except Exception as e:
        return {
            'compatible': False,
            'error': str(e),
            'version': 'GitHubStepInterface v5.1 Circular Reference Free'
        }

def get_github_step_info(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „)"""
    try:
        info = {
            'step_name': getattr(step_instance, 'step_name', 'Unknown'),
            'step_id': getattr(step_instance, 'step_id', 0),
            'class_name': step_instance.__class__.__name__,
            'module': step_instance.__class__.__module__,
            'device': getattr(step_instance, 'device', 'Unknown'),
            'is_initialized': getattr(step_instance, 'is_initialized', False),
            'github_compatible': getattr(step_instance, 'github_compatible', False),
            'has_model': getattr(step_instance, 'has_model', False),
            'model_loaded': getattr(step_instance, 'model_loaded', False),
            'circular_reference_free': True
        }
        
        # GitHub ì˜ì¡´ì„± ìƒíƒœ
        dependencies = {}
        for dep_name in ['model_loader', 'memory_manager', 'data_converter', 'di_container', 'step_interface']:
            dependencies[dep_name] = hasattr(step_instance, dep_name) and getattr(step_instance, dep_name) is not None
        
        info['dependencies'] = dependencies
        
        # ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì ìƒíƒœ
        if hasattr(step_instance, 'dependency_manager'):
            dep_manager = getattr(step_instance, 'dependency_manager')
            if isinstance(dep_manager, EmbeddedDependencyManager):
                info['embedded_dependency_manager'] = {
                    'type': 'EmbeddedDependencyManager',
                    'injected_count': dep_manager.dependencies_injected,
                    'injection_failures': dep_manager.injection_failures,
                    'circular_reference_safe': True
                }
            else:
                info['embedded_dependency_manager'] = {
                    'type': type(dep_manager).__name__,
                    'circular_reference_safe': False
                }
        
        # DetailedDataSpec ìƒíƒœ
        detailed_data_spec_info = {}
        for attr_name in ['detailed_data_spec', 'api_input_mapping', 'api_output_mapping', 'preprocessing_steps', 'postprocessing_steps']:
            detailed_data_spec_info[attr_name] = hasattr(step_instance, attr_name) and getattr(step_instance, attr_name) is not None
        
        info['detailed_data_spec'] = detailed_data_spec_info
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì •ë³´
        if info['class_name'] == 'VirtualFittingStep' or info['step_id'] == 6:
            info['step_07_status'] = {
                'force_real_ai': getattr(step_instance, 'force_real_ai_processing', False),
                'mock_disabled': getattr(step_instance, 'mock_mode_disabled', False),
                'fallback_disabled': not getattr(step_instance, 'fallback_on_ai_failure', True),
                'circular_reference_free': True
            }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if hasattr(step_instance, 'performance_metrics'):
            metrics = getattr(step_instance, 'performance_metrics')
            info['performance'] = {
                'github_process_calls': getattr(metrics, 'github_process_calls', 0),
                'real_ai_calls': getattr(metrics, 'real_ai_calls', 0),
                'mock_calls_blocked': getattr(metrics, 'mock_calls_blocked', 0),
                'data_conversions': getattr(metrics, 'data_conversions', 0),
                'circular_reference_optimized': True
            }
        
        return info
        
    except Exception as e:
        return {
            'error': str(e),
            'class_name': getattr(step_instance, '__class__', {}).get('__name__', 'Unknown') if step_instance else 'None',
            'circular_reference_free': True
        }

# =============================================================================
# ğŸ”¥ 18ë‹¨ê³„: ê²½ë¡œ í˜¸í™˜ì„± ì²˜ë¦¬ (StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ í•´ê²°)
# =============================================================================

def create_deprecated_interface_warning():
    """Deprecated interface ê²½ë¡œ ê²½ê³ """
    warnings.warn(
        "âš ï¸ app.ai_pipeline.interface ê²½ë¡œëŠ” deprecatedì…ë‹ˆë‹¤. "
        "app.ai_pipeline.interfacesë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
        DeprecationWarning,
        stacklevel=3
    )
    logger.warning("âš ï¸ Deprecated interface ê²½ë¡œ ì‚¬ìš© ê°ì§€")

# ì•ˆì „í•œ ëª¨ë“ˆ ë³„ì¹­ ìƒì„± (StepInterface ë³„ì¹­ ì„¤ì • ì˜¤ë¥˜ í•´ê²°)
def setup_safe_module_aliases():
    """ì•ˆì „í•œ ëª¨ë“ˆ ë³„ì¹­ ì„¤ì •"""
    try:
        current_module = sys.modules[__name__]
        
        # app.ai_pipeline.interface.step_interfaceë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ë³„ì¹­ ìƒì„±
        if 'app.ai_pipeline.interface' not in sys.modules:
            import types
            interface_module = types.ModuleType('app.ai_pipeline.interface')
            interface_module.step_interface = current_module
            sys.modules['app.ai_pipeline.interface'] = interface_module
            sys.modules['app.ai_pipeline.interface.step_interface'] = current_module
            logger.info("âœ… ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì™„ë£Œ")
            return True
    except Exception as e:
        logger.warning(f"âš ï¸ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
        return False

# ëª¨ë“ˆ ë³„ì¹­ ì„¤ì • ì‹¤í–‰
setup_safe_module_aliases()

# =============================================================================
# ğŸ”¥ 19ë‹¨ê³„: Export (ëª¨ë“  í´ë˜ìŠ¤ ë° í•¨ìˆ˜ í¬í•¨)
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'GitHubStepModelInterface',
    'GitHubMemoryManager', 
    'EmbeddedDependencyManager',  # ğŸ”¥ ìˆœí™˜ì°¸ì¡° í•´ê²° í•µì‹¬
    'GitHubStepMapping',
    
    # í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤
    'StepInterface',
    'SimpleStepConfig',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'GitHubStepConfig',
    'GitHubStepCreationResult',
    'GitHubStepType',
    'GitHubStepPriority',
    'GitHubDeviceType',
    'GitHubProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „)
    'create_github_step_interface_circular_reference_free',
    'create_optimized_github_interface_v51',
    'create_step_07_virtual_fitting_interface_v51',
    'create_simple_step_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_github_environment_info',
    'optimize_github_environment',
    'validate_github_step_compatibility',
    'get_github_step_info',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE',
    'PROJECT_ROOT',
    'BACKEND_ROOT',
    'AI_PIPELINE_ROOT',
    'PYTORCH_FIXED',
    'REMBG_AVAILABLE',
    'SAFETENSORS_AVAILABLE',
    
    # Logger
    'logger'
]

# =============================================================================
# ğŸ”¥ 20ë‹¨ê³„: ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
if AI_PIPELINE_ROOT.exists():
    logger.info(f"âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€: {PROJECT_ROOT}")
else:
    logger.warning(f"âš ï¸ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ í•„ìš”: {PROJECT_ROOT}")

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_github_environment()
    logger.info("ğŸ conda í™˜ê²½ mycloset-ai-clean ìë™ ìµœì í™” ì™„ë£Œ!")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        if MPS_AVAILABLE and PYTORCH_FIXED:
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

logger.info("=" * 80)
logger.info("ğŸ”¥ Step Interface v5.1 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€")
logger.info("=" * 80)
logger.info("âœ… BaseStepMixin ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨ (ì§€ì—° import)")
logger.info("âœ… EmbeddedDependencyManager ë‚´ì¥ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° í•´ê²°")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€")
logger.info("âœ… Logger ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜")
logger.info("âœ… M3 Max ìµœì í™” ìœ ì§€")

logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ…' if CONDA_INFO['is_target_env'] else 'âš ï¸'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - PyTorch ìˆ˜ì •: {'âœ…' if PYTORCH_FIXED else 'âŒ'}")
logger.info(f"   - ìˆœí™˜ì°¸ì¡° í•´ê²°: âœ…")
logger.info(f"   - ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì: âœ…")

logger.info("ğŸ¯ ì§€ì› GitHub Step í´ë˜ìŠ¤:")
for step_type in GitHubStepType:
    config = GitHubStepMapping.get_config(step_type)
    circular_ref_status = "ğŸ”— ìˆœí™˜ì°¸ì¡° í•´ê²°" if config.step_id <= 8 else ""
    logger.info(f"   - Step {config.step_id:02d}: {config.class_name} ({config.model_size_gb}GB) {circular_ref_status}")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ EmbeddedDependencyManager: ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨")
logger.info("   â€¢ ì§€ì—° import: TYPE_CHECKINGìœ¼ë¡œ import ìˆœí™˜ ë°©ì§€")
logger.info("   â€¢ GitHubStepModelInterface: BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜")
logger.info("   â€¢ GitHubStepMapping: ì‹¤ì œ GitHub Step í´ë˜ìŠ¤ ë§¤í•‘")
logger.info("   â€¢ GitHubMemoryManager: M3 Max 128GB ì™„ì „ í™œìš©")

logger.info("ğŸš€ ì£¼ìš” íŒ©í† ë¦¬ í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° í•´ê²°):")
logger.info("   - create_github_step_interface_circular_reference_free(): ìˆœí™˜ì°¸ì¡° í•´ê²° ë²„ì „")
logger.info("   - create_optimized_github_interface_v51(): ìµœì í™”ëœ ì¸í„°í˜ì´ìŠ¤ v5.1")
logger.info("   - create_step_07_virtual_fitting_interface_v51(): 7ë‹¨ê³„ ì „ìš© v5.1")
logger.info("   - create_simple_step_interface(): Step íŒŒì¼ í˜¸í™˜ì„±ìš©")

logger.info("ğŸ”§ ì£¼ìš” ìœ í‹¸ë¦¬í‹° (ìˆœí™˜ì°¸ì¡° í•´ê²°):")
logger.info("   - get_github_environment_info(): í™˜ê²½ ì •ë³´ + ìˆœí™˜ì°¸ì¡° ìƒíƒœ")
logger.info("   - validate_github_step_compatibility(): Step í˜¸í™˜ì„± + ìˆœí™˜ì°¸ì¡° ê²€ì¦")
logger.info("   - get_github_step_info(): Step ì •ë³´ + ë‚´ì¥ ì˜ì¡´ì„± ê´€ë¦¬ì ìƒíƒœ")

logger.info("ğŸ”„ í˜¸í™˜ì„± ì§€ì›:")
logger.info("   - StepInterface: ê¸°ì¡´ Step íŒŒì¼ë“¤ê³¼ í˜¸í™˜")
logger.info("   - app.ai_pipeline.interface ê²½ë¡œ ë³„ì¹­ ì§€ì›")
logger.info("   - logger ì •ì˜ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°ë¡œ ì•ˆì •ì„± í™•ë³´")

logger.info("ğŸ‰ Step Interface v5.1 ìˆœí™˜ì°¸ì¡° í•´ê²° ì™„ë£Œ!")
logger.info("ğŸ‰ BaseStepMixinê³¼ì˜ ìˆœí™˜ì°¸ì¡°ê°€ ì™„ì „íˆ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("ğŸ‰ EmbeddedDependencyManagerë¡œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… ì§€ì›!")
logger.info("ğŸ‰ ëª¨ë“  ê¸°ëŠ¥ì´ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ë©´ì„œ ìˆœí™˜ì°¸ì¡°ë§Œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("=" * 80)