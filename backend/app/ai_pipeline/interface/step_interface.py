# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ MyCloset AI Modern Step Interface v7.0 - í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ ë§ì¶¤í˜•
================================================================================

âœ… í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜ ì‹¤ì œ êµ¬ì¡° 100% ë°˜ì˜
âœ… BaseStepMixin v19.2 ì™„ì „ í˜¸í™˜
âœ… StepFactory v11.0 ì—°ë™ ìµœì í™”
âœ… RealAIStepImplementationManager v14.0 í†µí•©
âœ… Central Hub DI Container ì™„ì „ í™œìš©
âœ… DetailedDataSpec ì‹¤ì œ ê¸°ëŠ¥ êµ¬í˜„
âœ… M3 Max 128GB + conda mycloset-ai-clean ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB í™œìš©

ê¸°ì¡´ íŒŒì¼ë“¤ê³¼ì˜ ì°¨ì´ì :
1. í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ êµ¬ì¡°ë¥¼ ë°˜ì˜í•œ import ê²½ë¡œ
2. BaseStepMixinì˜ ì‹¤ì œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ í™œìš©
3. StepFactoryì˜ ì‹¤ì œ ìƒì„± ë¡œì§ ì—°ë™
4. Central Hub Containerì˜ ì‹¤ì œ ê¸°ëŠ¥ í™œìš©
5. ì‹¤ì œ Step í´ë˜ìŠ¤ë“¤ì˜ êµ¬í˜„ íŒ¨í„´ ë°˜ì˜

Author: MyCloset AI Team  
Date: 2025-08-01
Version: 7.0 (Project Structure Optimized)
"""

import os
import gc
import sys
import time
import warnings
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache
from contextlib import asynccontextmanager
import json
import hashlib
import importlib

# =============================================================================
# ğŸ”¥ 1. Logger ë° ê¸°ë³¸ ì„¤ì •
# =============================================================================

import logging

_LOGGER_INITIALIZED = False
_MODULE_LOGGER = None

def get_safe_logger():
    """Thread-safe Logger ì´ˆê¸°í™”"""
    global _LOGGER_INITIALIZED, _MODULE_LOGGER
    
    if _LOGGER_INITIALIZED and _MODULE_LOGGER is not None:
        return _MODULE_LOGGER
    
    try:
        logger_name = __name__
        _MODULE_LOGGER = logging.getLogger(logger_name)
        
        if not _MODULE_LOGGER.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            _MODULE_LOGGER.addHandler(handler)
            _MODULE_LOGGER.setLevel(logging.INFO)
        
        _LOGGER_INITIALIZED = True
        return _MODULE_LOGGER
        
    except Exception as e:
        print(f"âš ï¸ Logger ì´ˆê¸°í™” ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
        
        class FallbackLogger:
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def debug(self, msg): print(f"DEBUG: {msg}")
        
        return FallbackLogger()

logger = get_safe_logger()

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*deprecated.*')
warnings.filterwarnings('ignore', category=ImportWarning)

# =============================================================================
# ğŸ”¥ 2. í™˜ê²½ ê°ì§€ ë° ìµœì í™” ì„¤ì • (í”„ë¡œì íŠ¸ ê¸°ë°˜)
# =============================================================================

# PyTorch ì‹¤ì œ ìƒíƒœ í™•ì¸
PYTORCH_AVAILABLE = False
MPS_AVAILABLE = False
DEVICE = "cpu"

try:
    import torch
    PYTORCH_AVAILABLE = True
    
    # PyTorch weights_only ë¬¸ì œ í•´ê²°
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            if weights_only is None:
                weights_only = False
            return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=weights_only, **kwargs)
        torch.load = safe_torch_load
        logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEVICE = "mps"
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì‹¤ì œ í•˜ë“œì›¨ì–´ ê°ì§€
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
except Exception:
    pass

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
    'project_path': str(Path(__file__).parent.parent.parent.parent)
}

# ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ì‹¤ì œ êµ¬ì¡° ë°˜ì˜)
current_file = Path(__file__).resolve()
BACKEND_ROOT = current_file.parent.parent.parent.parent
PROJECT_ROOT = BACKEND_ROOT.parent
AI_PIPELINE_ROOT = BACKEND_ROOT / "app" / "ai_pipeline"
AI_MODELS_ROOT = BACKEND_ROOT / "ai_models"

logger.info(f"ğŸ”§ ì‹¤ì œ í™˜ê²½ ì •ë³´: conda={CONDA_INFO['conda_env']}, M3_Max={IS_M3_MAX}, MPS={MPS_AVAILABLE}")

# =============================================================================
# ğŸ”¥ 3. Step íƒ€ì… ë° êµ¬ì¡° ì •ì˜ (í”„ë¡œì íŠ¸ ê¸°ì¤€)
# =============================================================================

class StepType(Enum):
    """Step íƒ€ì… (í”„ë¡œì íŠ¸ ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"

# Step ID ë§¤í•‘ (í”„ë¡œì íŠ¸ ì‹¤ì œ êµ¬ì¡°)
STEP_ID_TO_NAME_MAPPING = {
    1: "HumanParsingStep",
    2: "PoseEstimationStep", 
    3: "ClothSegmentationStep",
    4: "GeometricMatchingStep",
    5: "ClothWarpingStep",
    6: "VirtualFittingStep",
    7: "PostProcessingStep",
    8: "QualityAssessmentStep"
}

STEP_NAME_TO_ID_MAPPING = {v: k for k, v in STEP_ID_TO_NAME_MAPPING.items()}

# =============================================================================
# ğŸ”¥ 4. DetailedDataSpec í´ë˜ìŠ¤ (ì‹¤ì œ ê¸°ëŠ¥ êµ¬í˜„)
# =============================================================================

@dataclass
class DetailedDataSpec:
    """ì‹¤ì œ ì‘ë™í•˜ëŠ” DetailedDataSpec í´ë˜ìŠ¤"""
    
    # API ë§¤í•‘ (FastAPI ë¼ìš°í„° ì™„ì „ í˜¸í™˜)
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    
    # Step ê°„ ë°ì´í„° íë¦„
    accepts_from_previous_step: Dict[str, Dict[str, str]] = field(default_factory=dict)
    provides_to_next_step: Dict[str, Dict[str, str]] = field(default_factory=dict)
    
    # ë°ì´í„° íƒ€ì… ë° ìŠ¤í‚¤ë§ˆ
    step_input_schema: Dict[str, str] = field(default_factory=dict)
    step_output_schema: Dict[str, str] = field(default_factory=dict)
    
    input_data_types: List[str] = field(default_factory=list)
    output_data_types: List[str] = field(default_factory=list)
    input_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    output_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    input_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    output_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # ì •ê·œí™” ì„¤ì • (list ì‚¬ìš©ìœ¼ë¡œ ì•ˆì „)
    normalization_mean: List[float] = field(default_factory=lambda: [0.485, 0.456, 0.406])
    normalization_std: List[float] = field(default_factory=lambda: [0.229, 0.224, 0.225])
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (ì•ˆì „í•œ ë³µì‚¬)"""
        try:
            return {
                'api_input_mapping': dict(self.api_input_mapping) if self.api_input_mapping else {},
                'api_output_mapping': dict(self.api_output_mapping) if self.api_output_mapping else {},
                'accepts_from_previous_step': dict(self.accepts_from_previous_step) if self.accepts_from_previous_step else {},
                'provides_to_next_step': dict(self.provides_to_next_step) if self.provides_to_next_step else {},
                'step_input_schema': dict(self.step_input_schema) if self.step_input_schema else {},
                'step_output_schema': dict(self.step_output_schema) if self.step_output_schema else {},
                'input_data_types': list(self.input_data_types) if self.input_data_types else [],
                'output_data_types': list(self.output_data_types) if self.output_data_types else [],
                'input_shapes': dict(self.input_shapes) if self.input_shapes else {},
                'output_shapes': dict(self.output_shapes) if self.output_shapes else {},
                'input_value_ranges': dict(self.input_value_ranges) if self.input_value_ranges else {},
                'output_value_ranges': dict(self.output_value_ranges) if self.output_value_ranges else {},
                'preprocessing_required': list(self.preprocessing_required) if self.preprocessing_required else [],
                'postprocessing_required': list(self.postprocessing_required) if self.postprocessing_required else [],
                'preprocessing_steps': list(self.preprocessing_steps) if self.preprocessing_steps else [],
                'postprocessing_steps': list(self.postprocessing_steps) if self.postprocessing_steps else [],
                'normalization_mean': list(self.normalization_mean) if self.normalization_mean else [0.485, 0.456, 0.406],
                'normalization_std': list(self.normalization_std) if self.normalization_std else [0.229, 0.224, 0.225]
            }
        except Exception as e:
            logger.warning(f"DetailedDataSpec.to_dict() ì‹¤íŒ¨: {e}")
            return {}

@dataclass  
class EnhancedStepRequest:
    """ê°•í™”ëœ Step ìš”ì²­ í´ë˜ìŠ¤"""
    step_name: str
    step_id: int
    data_spec: DetailedDataSpec = field(default_factory=DetailedDataSpec)
    required_models: List[str] = field(default_factory=list)
    model_requirements: Dict[str, Any] = field(default_factory=dict)
    preprocessing_config: Dict[str, Any] = field(default_factory=dict)
    postprocessing_config: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 5. í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜ Step ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

@dataclass
class AIModelConfig:
    """AI ëª¨ë¸ ì„¤ì • (í”„ë¡œì íŠ¸ ì‹¤ì œ ëª¨ë¸ ê¸°ë°˜)"""
    model_name: str
    model_path: str
    model_type: str = "BaseModel"
    size_gb: float = 0.0
    device: str = "auto"
    requires_checkpoint: bool = True
    checkpoint_key: Optional[str] = None
    preprocessing_required: List[str] = field(default_factory=list)
    postprocessing_required: List[str] = field(default_factory=list)

@dataclass
class StepConfig:
    """Step ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ ë°˜ì˜)"""
    # Step ê¸°ë³¸ ì •ë³´
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    module_path: str = ""
    
    # Step íƒ€ì…
    step_type: StepType = StepType.HUMAN_PARSING
    priority: StepPriority = StepPriority.MEDIUM
    
    # AI ëª¨ë¸ë“¤ (í”„ë¡œì íŠ¸ ì‹¤ì œ ëª¨ë¸ ê¸°ë°˜)
    ai_models: List[AIModelConfig] = field(default_factory=list)
    primary_model_name: str = ""
    model_cache_dir: str = ""
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # í”„ë¡œì íŠ¸ í˜¸í™˜ì„±
    basestepmixin_compatible: bool = True
    central_hub_integration: bool = True
    dependency_injection_enabled: bool = True
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = True
    require_step_interface: bool = True
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    
    # DetailedDataSpec ì„¤ì •
    enable_detailed_data_spec: bool = True
    detailed_data_spec: DetailedDataSpec = field(default_factory=DetailedDataSpec)
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ í™˜ê²½ ìµœì í™”"""
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        if not self.model_cache_dir:
            self.model_cache_dir = str(AI_MODELS_ROOT / f"step_{self.step_id:02d}_{self.step_name.lower()}")

# =============================================================================
# ğŸ”¥ 6. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (í”„ë¡œì íŠ¸ ìµœì í™”)
# =============================================================================

class MemoryManager:
    """í”„ë¡œì íŠ¸ ìµœì í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = get_safe_logger()
        
        # í”„ë¡œì íŠ¸ í™˜ê²½ ê¸°ë°˜ ë©”ëª¨ë¦¬ ì„¤ì •
        if max_memory_gb is None:
            if IS_M3_MAX and MEMORY_GB >= 128:
                self.max_memory_gb = MEMORY_GB * 0.9
            elif IS_M3_MAX and MEMORY_GB >= 64:
                self.max_memory_gb = MEMORY_GB * 0.85
            elif IS_M3_MAX:
                self.max_memory_gb = MEMORY_GB * 0.8
            elif CONDA_INFO['is_target_env']:
                self.max_memory_gb = 12.0
            else:
                self.max_memory_gb = 8.0
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        self.pytorch_available = PYTORCH_AVAILABLE
        
        self.logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                self.allocation_history.append({
                    'owner': owner,
                    'size_gb': size_gb,
                    'action': 'allocate',
                    'timestamp': time.time()
                })
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner}")
                return True
            else:
                available = self.max_memory_gb - self.current_memory_gb
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {available:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                self.allocation_history.append({
                    'owner': owner,
                    'size_gb': size_gb,
                    'action': 'deallocate',
                    'timestamp': time.time()
                })
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner}")
                return size_gb
            return 0.0
    
    def optimize_for_ai_models(self):
        """AI ëª¨ë¸ íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            optimizations = []
            
            if self.mps_enabled and self.pytorch_available:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                        optimizations.append("MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬")
                except Exception as e:
                    self.logger.debug(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            if self.pytorch_available and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    optimizations.append("CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬")
                except Exception as e:
                    self.logger.debug(f"CUDA ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            gc.collect()
            optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
            
            if IS_M3_MAX and MEMORY_GB >= 128:
                self.max_memory_gb = min(MEMORY_GB * 0.9, 115.0)
                optimizations.append(f"M3 Max 128GB ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
            if optimizations:
                self.logger.debug(f"ğŸ AI ëª¨ë¸ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'memory_pool': self.memory_pool.copy(),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'pytorch_available': self.pytorch_available,
                'total_system_gb': MEMORY_GB,
                'allocation_count': len(self.allocation_history)
            }

# =============================================================================
# ğŸ”¥ 7. ì˜ì¡´ì„± ê´€ë¦¬ì (í”„ë¡œì íŠ¸ DI íŒ¨í„´ ë°˜ì˜)
# =============================================================================

class DependencyManager:
    """í”„ë¡œì íŠ¸ DI íŒ¨í„´ ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = get_safe_logger()
        
        self.step_instance = None
        self.dependencies = {}
        self.injection_stats = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False,
            'step_interface': False
        }
        
        self.dependencies_injected = 0
        self.injection_failures = 0
        self.last_injection_time = time.time()
        self._lock = threading.RLock()
        
        self.logger.debug(f"âœ… DependencyManager ì´ˆê¸°í™”: {step_name}")
    
    def set_step_instance(self, step_instance):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
        try:
            with self._lock:
                self.step_instance = step_instance
                self.logger.debug(f"âœ… {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def inject_model_loader(self, model_loader):
        """ModelLoader ì£¼ì…"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if model_loader is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} ModelLoaderê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                # BaseStepMixinì˜ set_model_loader ë©”ì„œë“œ í™œìš©
                if hasattr(self.step_instance, 'set_model_loader'):
                    self.step_instance.set_model_loader(model_loader)
                else:
                    self.step_instance.model_loader = model_loader
                
                self.dependencies['model_loader'] = model_loader
                self.injection_stats['model_loader'] = True
                self.dependencies_injected += 1
                
                # Step Interface ìƒì„±
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        step_interface = model_loader.create_step_interface(self.step_name)
                        self.step_instance.model_interface = step_interface
                        self.dependencies['step_interface'] = step_interface
                        self.injection_stats['step_interface'] = True
                        self.logger.debug(f"âœ… {self.step_name} Step Interface ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ {self.step_name} Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_memory_manager(self, memory_manager):
        """MemoryManager ì£¼ì…"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if memory_manager is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} MemoryManagerê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                # BaseStepMixinì˜ set_memory_manager ë©”ì„œë“œ í™œìš©
                if hasattr(self.step_instance, 'set_memory_manager'):
                    self.step_instance.set_memory_manager(memory_manager)
                else:
                    self.step_instance.memory_manager = memory_manager
                
                self.dependencies['memory_manager'] = memory_manager
                self.injection_stats['memory_manager'] = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} MemoryManager ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_data_converter(self, data_converter):
        """DataConverter ì£¼ì…"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if data_converter is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} DataConverterê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                # BaseStepMixinì˜ set_data_converter ë©”ì„œë“œ í™œìš©
                if hasattr(self.step_instance, 'set_data_converter'):
                    self.step_instance.set_data_converter(data_converter)
                else:
                    self.step_instance.data_converter = data_converter
                
                self.dependencies['data_converter'] = data_converter
                self.injection_stats['data_converter'] = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} DataConverter ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_di_container(self, di_container):
        """DI Container ì£¼ì… (í”„ë¡œì íŠ¸ Central Hub íŒ¨í„´)"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if di_container is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} DI Containerê°€ Noneì…ë‹ˆë‹¤")
                    return False
                
                # Central Hub Container íŒ¨í„´ í™œìš©
                if hasattr(self.step_instance, 'central_hub_container'):
                    self.step_instance.central_hub_container = di_container
                elif hasattr(self.step_instance, 'di_container'):
                    self.step_instance.di_container = di_container
                
                self.dependencies['di_container'] = di_container
                self.injection_stats['di_container'] = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} DI Container ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def auto_inject_dependencies(self) -> bool:
        """ì˜ì¡´ì„± ìë™ ì£¼ì… (í”„ë¡œì íŠ¸ íŒ¨í„´ ê¸°ë°˜)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} ì˜ì¡´ì„± ìë™ ì£¼ì… ì‹œì‘...")
                
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                success_count = 0
                total_dependencies = 0
                
                # ModelLoader ìë™ ì£¼ì…
                if not hasattr(self.step_instance, 'model_loader') or self.step_instance.model_loader is None:
                    total_dependencies += 1
                    try:
                        model_loader = self._resolve_model_loader()
                        if model_loader:
                            if self.inject_model_loader(model_loader):
                                success_count += 1
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # MemoryManager ìë™ ì£¼ì…
                if not hasattr(self.step_instance, 'memory_manager') or self.step_instance.memory_manager is None:
                    total_dependencies += 1
                    try:
                        memory_manager = self._resolve_memory_manager()
                        if memory_manager:
                            if self.inject_memory_manager(memory_manager):
                                success_count += 1
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # DataConverter ìë™ ì£¼ì…
                if not hasattr(self.step_instance, 'data_converter') or self.step_instance.data_converter is None:
                    total_dependencies += 1
                    try:
                        data_converter = self._resolve_data_converter()
                        if data_converter:
                            if self.inject_data_converter(data_converter):
                                success_count += 1
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} DataConverter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # DI Container ìë™ ì£¼ì…
                if not hasattr(self.step_instance, 'central_hub_container') or self.step_instance.central_hub_container is None:
                    total_dependencies += 1
                    try:
                        di_container = self._resolve_di_container()
                        if di_container:
                            if self.inject_di_container(di_container):
                                success_count += 1
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} DI Container í•´ê²° ì‹¤íŒ¨")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} DI Container ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                if total_dependencies == 0:
                    self.logger.info(f"âœ… {self.step_name} ëª¨ë“  ì˜ì¡´ì„±ì´ ì´ë¯¸ ì£¼ì…ë˜ì–´ ìˆìŒ")
                    return True
                
                success_rate = success_count / total_dependencies if total_dependencies > 0 else 1.0
                
                if success_count > 0:
                    self.logger.info(f"âœ… {self.step_name} ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}/{total_dependencies} ({success_rate*100:.1f}%)")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {success_count}/{total_dependencies}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì¤‘ ì˜¤ë¥˜: {e}")
            self.injection_failures += 1
            return False
    
    def _resolve_model_loader(self):
        """ModelLoader í•´ê²° (í”„ë¡œì íŠ¸ íŒ¨í„´ ê¸°ë°˜)"""
        try:
            # í”„ë¡œì íŠ¸ì˜ ModelLoader í•´ê²° ì‹œë„
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.model_loader')
                if hasattr(module, 'get_global_model_loader'):
                    loader = module.get_global_model_loader()
                    if loader and hasattr(loader, 'load_model') and hasattr(loader, 'create_step_interface'):
                        return loader
            except ImportError:
                self.logger.debug(f"{self.step_name} ModelLoader ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_memory_manager(self):
        """MemoryManager í•´ê²° (í”„ë¡œì íŠ¸ íŒ¨í„´ ê¸°ë°˜)"""
        try:
            # í”„ë¡œì íŠ¸ì˜ MemoryManager í•´ê²° ì‹œë„
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
                if hasattr(module, 'get_global_memory_manager'):
                    manager = module.get_global_memory_manager()
                    if manager:
                        return manager
            except ImportError:
                self.logger.debug(f"{self.step_name} MemoryManager ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            # í´ë°±: ë¡œì»¬ MemoryManager ìƒì„±
            return MemoryManager()
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨: {e}")
            return MemoryManager()
    
    def _resolve_data_converter(self):
        """DataConverter í•´ê²° (í”„ë¡œì íŠ¸ íŒ¨í„´ ê¸°ë°˜)"""
        try:
            # í”„ë¡œì íŠ¸ì˜ DataConverter í•´ê²° ì‹œë„
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.data_converter')
                if hasattr(module, 'get_global_data_converter'):
                    converter = module.get_global_data_converter()
                    if converter:
                        return converter
            except ImportError:
                self.logger.debug(f"{self.step_name} DataConverter ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_di_container(self):
        """DI Container í•´ê²° (í”„ë¡œì íŠ¸ Central Hub íŒ¨í„´)"""
        try:
            # í”„ë¡œì íŠ¸ì˜ Central Hub Container í•´ê²° ì‹œë„
            try:
                import importlib
                module = importlib.import_module('app.core.di_container')
                if hasattr(module, 'get_global_container'):
                    container = module.get_global_container()
                    if container:
                        return container
            except ImportError:
                self.logger.debug(f"{self.step_name} DI Container ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} DI Container í•´ê²° ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} DI Container í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def validate_dependencies(self, format_type=None) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ê²€ì¦"""
        try:
            with self._lock:
                if not self.step_instance:
                    base_result = {
                        'model_loader': False,
                        'memory_manager': False,
                        'data_converter': False,
                        'step_interface': False,
                        'di_container': False
                    }
                else:
                    base_result = {
                        'model_loader': hasattr(self.step_instance, 'model_loader') and self.step_instance.model_loader is not None,
                        'memory_manager': hasattr(self.step_instance, 'memory_manager') and self.step_instance.memory_manager is not None,
                        'data_converter': hasattr(self.step_instance, 'data_converter') and self.step_instance.data_converter is not None,
                        'step_interface': hasattr(self.step_instance, 'model_interface') and self.step_instance.model_interface is not None,
                        'di_container': hasattr(self.step_instance, 'central_hub_container') and self.step_instance.central_hub_container is not None
                    }
                
                if format_type and hasattr(format_type, 'value') and format_type.value == 'boolean_dict':
                    return base_result
                
                return {
                    'success': all(dep for key, dep in base_result.items()),
                    'dependencies': base_result,
                    'project_compatible': True,
                    'injected_count': self.dependencies_injected,
                    'injection_failures': self.injection_failures,
                    'step_name': self.step_name,
                    'timestamp': time.time()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'project_compatible': True,
                'step_name': self.step_name
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} DependencyManager ì •ë¦¬ ì‹œì‘...")
                
                for dep_name, dep_instance in self.dependencies.items():
                    try:
                        if hasattr(dep_instance, 'cleanup'):
                            dep_instance.cleanup()
                        elif hasattr(dep_instance, 'close'):
                            dep_instance.close()
                    except Exception as e:
                        self.logger.debug(f"ì˜ì¡´ì„± ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({dep_name}): {e}")
                
                self.dependencies.clear()
                self.injection_stats = {key: False for key in self.injection_stats}
                self.step_instance = None
                
                self.logger.info(f"âœ… {self.step_name} DependencyManager ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DependencyManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 8. Step Model Interface (í”„ë¡œì íŠ¸ ModelLoader íŒ¨í„´ ê¸°ë°˜)
# =============================================================================

class StepModelInterface:
    """í”„ë¡œì íŠ¸ ModelLoader íŒ¨í„´ ê¸°ë°˜ Step Model Interface"""
    
    def __init__(self, step_name: str, model_loader=None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        
        # í”„ë¡œì íŠ¸ ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = MemoryManager()
        
        # ì˜ì¡´ì„± ê´€ë¦¬
        self.dependency_manager = DependencyManager(step_name)
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'checkpoints_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'ai_calls': 0,
            'creation_time': time.time()
        }
        
        self.logger.info(f"ğŸ”— {step_name} Interface v7.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (í”„ë¡œì íŠ¸ íŒ¨í„´ ê¸°ë°˜)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # ìš”êµ¬ì‚¬í•­ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'device': kwargs.get('device', 'auto'),
                    'precision': 'fp16' if kwargs.get('use_fp16', False) else 'fp32',
                    'requires_checkpoint': kwargs.get('requires_checkpoint', True),
                    'registered_at': time.time(),
                    'pytorch_available': PYTORCH_AVAILABLE,
                    'mps_available': MPS_AVAILABLE,
                    'is_m3_max': IS_M3_MAX,
                    'metadata': kwargs.get('metadata', {})
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'loaded': False,
                    'checkpoint_loaded': False,
                    'size_mb': kwargs.get('size_mb', 1024.0),
                    'device': requirement['device'],
                    'status': 'registered',
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    requirement = registry_entry.get('requirement', {})
                    
                    # ëª¨ë¸ ì •ë³´
                    model_info = {
                        'name': model_name,
                        'path': f"{AI_MODELS_ROOT}/{self.step_name.lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'size_gb': round(registry_entry['size_mb'] / 1024, 2),
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'loaded': registry_entry['loaded'],
                        'checkpoint_loaded': registry_entry['checkpoint_loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'requires_checkpoint': requirement.get('requires_checkpoint', True),
                        'pytorch_available': requirement.get('pytorch_available', PYTORCH_AVAILABLE),
                        'mps_available': requirement.get('mps_available', MPS_AVAILABLE),
                        'is_m3_max': requirement.get('is_m3_max', IS_M3_MAX),
                        'metadata': {
                            'step_name': self.step_name,
                            'conda_env': CONDA_INFO['conda_env'],
                            'registered_at': requirement.get('registered_at', 0),
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ì¡°íšŒ
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'size_gb': round(model.get('size_mb', 0.0) / 1024, 2),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'loaded': model.get('loaded', False),
                                    'checkpoint_loaded': False,
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'requires_checkpoint': True,
                                    'pytorch_available': PYTORCH_AVAILABLE,
                                    'mps_available': MPS_AVAILABLE,
                                    'is_m3_max': IS_M3_MAX,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                else:
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    model = self._model_cache[model_name]
                    if hasattr(model, 'loaded') and model.loaded:
                        self.statistics['cache_hits'] += 1
                        self.statistics['ai_calls'] += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return model
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    try:
                        # ModelLoader load_model í˜¸ì¶œ
                        model = self.model_loader.load_model(model_name, **kwargs)
                        
                        if model is not None:
                            # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° í™•ì¸
                            has_checkpoint = False
                            if hasattr(model, 'get_checkpoint_data'):
                                checkpoint_data = model.get_checkpoint_data()
                                has_checkpoint = checkpoint_data is not None
                            elif hasattr(model, 'checkpoint_data'):
                                has_checkpoint = model.checkpoint_data is not None
                            
                            # ìºì‹œì— ì €ì¥
                            self._model_cache[model_name] = model
                            
                            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                            if model_name in self._model_registry:
                                self._model_registry[model_name]['loaded'] = True
                                self._model_registry[model_name]['checkpoint_loaded'] = has_checkpoint
                                self._model_registry[model_name]['status'] = 'loaded'
                            
                            # í†µê³„ ì—…ë°ì´íŠ¸
                            self.statistics['models_loaded'] += 1
                            self.statistics['ai_calls'] += 1
                            if has_checkpoint:
                                self.statistics['checkpoints_loaded'] += 1
                            
                            checkpoint_status = "âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”©ë¨" if has_checkpoint else "âš ï¸ ë©”íƒ€ë°ì´í„°ë§Œ"
                            model_size = getattr(model, 'memory_usage_mb', 0)
                            
                            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name} ({model_size:.1f}MB) {checkpoint_status}")
                            return model
                        else:
                            self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                            
                    except Exception as load_error:
                        self.logger.error(f"âŒ ModelLoader ë¡œë”© ì˜¤ë¥˜: {model_name} - {load_error}")
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    # BaseStepMixin í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜ ë³„ì¹­"""
        return self.get_model_sync(model_name, **kwargs)
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        if model_name:
            return self.get_model_sync(model_name, **kwargs)
        return None
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ë©”ëª¨ë¦¬ í•´ì œ
            for model_name, model in self._model_cache.items():
                if hasattr(model, 'unload'):
                    model.unload()
                self.memory_manager.deallocate_memory(model_name)
            
            self._model_cache.clear()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.dependency_manager.cleanup()
            
            self.logger.info(f"âœ… {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 9. Step ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
# =============================================================================

@dataclass
class StepCreationResult:
    """Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    step_type: Optional[StepType] = None
    class_name: str = ""
    module_path: str = ""
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ì£¼ì… ê²°ê³¼
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    ai_models_loaded: List[str] = field(default_factory=list)
    
    # BaseStepMixin í˜¸í™˜ì„±
    basestepmixin_compatible: bool = True
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    dependency_injection_success: bool = False
    
    # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 10. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

def create_step_interface(
    step_name: str, 
    model_loader=None,
    step_type: Optional[StepType] = None
) -> StepModelInterface:
    """Step Interface ìƒì„± (í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜)"""
    try:
        interface = StepModelInterface(step_name, model_loader)
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX and MEMORY_GB >= 128:
            interface.memory_manager = MemoryManager(115.0)
            interface.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
        elif IS_M3_MAX and MEMORY_GB >= 64:
            interface.memory_manager = MemoryManager(MEMORY_GB * 0.85)
            interface.logger.info(f"ğŸ M3 Max {MEMORY_GB}GB ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
        
        # ì˜ì¡´ì„± ê´€ë¦¬ì ìë™ ì£¼ì…
        interface.dependency_manager.auto_inject_dependencies()
        
        logger.info(f"âœ… Step Interface ìƒì„±: {step_name}")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return StepModelInterface(step_name, None)

def create_optimized_interface(
    step_name: str,
    model_loader=None
) -> StepModelInterface:
    """ìµœì í™”ëœ Interface ìƒì„±"""
    try:
        interface = create_step_interface(
            step_name=step_name,
            model_loader=model_loader
        )
        
        # conda + M3 Max ì¡°í•© ìµœì í™”
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9
            interface.memory_manager = MemoryManager(max_memory_gb)
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8
            interface.memory_manager = MemoryManager(max_memory_gb)
        
        logger.info(f"âœ… ìµœì í™”ëœ Interface: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_step_interface(step_name, model_loader)

def create_virtual_fitting_interface(
    model_loader=None
) -> StepModelInterface:
    """VirtualFittingStep ì „ìš© Interface - í”„ë¡œì íŠ¸ AI ëª¨ë¸ ê¸°ë°˜"""
    try:
        interface = StepModelInterface("VirtualFittingStep", model_loader)
        
        # VirtualFittingStep íŠ¹ë³„ ì„¤ì •
        interface.config = {'step_id': 6, 'model_size_gb': 14.0}
        
        # í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ AI ëª¨ë¸ë“¤ ë“±ë¡
        real_models = [
            "ootd_diffusion.safetensors",
            "stable_diffusion_v1_5.safetensors",
            "controlnet_openpose",
            "vae.safetensors"
        ]
        
        for model_name in real_models:
            interface.register_model_requirement(
                model_name=model_name,
                model_type="DiffusionModel",
                device="auto",
                requires_checkpoint=True
            )
        
        # ì˜ì¡´ì„± ì£¼ì…
        interface.dependency_manager.auto_inject_dependencies()
        
        logger.info("ğŸ”¥ VirtualFittingStep Interface ìƒì„± ì™„ë£Œ - í”„ë¡œì íŠ¸ AI ëª¨ë¸ ê¸°ë°˜")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ VirtualFittingStep Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return create_step_interface("VirtualFittingStep", model_loader)

def create_simple_interface(step_name: str, **kwargs) -> 'SimpleStepInterface':
    """ê°„ë‹¨í•œ Step Interface ìƒì„± (í˜¸í™˜ì„±)"""
    try:
        return SimpleStepInterface(step_name, **kwargs)
    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return SimpleStepInterface(step_name)

# =============================================================================
# ğŸ”¥ 11. í˜¸í™˜ì„± ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# =============================================================================

class SimpleStepInterface:
    """Step íŒŒì¼ë“¤ì´ ì‚¬ìš©í•˜ëŠ” í˜¸í™˜ì„± ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, step_name: str, model_loader=None, **kwargs):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = get_safe_logger()
        self.config = kwargs
        
        # ê¸°ë³¸ ì†ì„±ë“¤
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.initialized = False
        
        self.logger.debug(f"âœ… SimpleStepInterface ìƒì„±: {step_name}")
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (í˜¸í™˜ì„±)"""
        try:
            self.logger.debug(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, **kwargs) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í˜¸í™˜ì„±)"""
        return []
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name, **kwargs)
            elif self.model_loader and hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# =============================================================================
# ğŸ”¥ 12. í”„ë¡œì íŠ¸ Step ë§¤í•‘ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

class ProjectStepMapping:
    """í”„ë¡œì íŠ¸ ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ Step ë§¤í•‘"""
    
    @classmethod
    def _create_detailed_data_spec(cls, step_name: str, step_id: int) -> DetailedDataSpec:
        """DetailedDataSpec ìƒì„±"""
        try:
            if step_name == "HumanParsingStep":
                return DetailedDataSpec(
                    api_input_mapping={
                        "person_image": "fastapi.UploadFile -> PIL.Image.Image",
                        "parsing_options": "dict -> dict"
                    },
                    api_output_mapping={
                        "parsing_mask": "numpy.ndarray -> base64_string",
                        "person_segments": "List[Dict] -> List[Dict]"
                    },
                    input_data_types=["PIL.Image.Image", "Dict"],
                    output_data_types=["numpy.ndarray", "List[Dict]"],
                    preprocessing_steps=["resize_512x512", "normalize_imagenet", "to_tensor"],
                    postprocessing_steps=["argmax", "resize_original", "morphology_clean"],
                    normalization_mean=[0.485, 0.456, 0.406],
                    normalization_std=[0.229, 0.224, 0.225]
                )
            
            elif step_name == "PoseEstimationStep":
                return DetailedDataSpec(
                    api_input_mapping={
                        "person_image": "fastapi.UploadFile -> PIL.Image.Image",
                        "pose_options": "Optional[dict] -> Optional[dict]"
                    },
                    api_output_mapping={
                        "keypoints": "numpy.ndarray -> List[Dict[str, float]]",
                        "pose_confidence": "float -> float"
                    },
                    input_data_types=["PIL.Image.Image", "Optional[Dict]"],
                    output_data_types=["numpy.ndarray", "float"],
                    preprocessing_steps=["resize_640x640", "normalize_yolo"],
                    postprocessing_steps=["extract_keypoints", "scale_coords", "filter_confidence"],
                    normalization_mean=[0.485, 0.456, 0.406],
                    normalization_std=[0.229, 0.224, 0.225]
                )
            
            elif step_name == "VirtualFittingStep":
                return DetailedDataSpec(
                    api_input_mapping={
                        "person_image": "fastapi.UploadFile -> PIL.Image.Image",
                        "clothing_image": "fastapi.UploadFile -> PIL.Image.Image",
                        "fabric_type": "Optional[str] -> Optional[str]",
                        "clothing_type": "Optional[str] -> Optional[str]"
                    },
                    api_output_mapping={
                        "fitted_image": "numpy.ndarray -> base64_string",
                        "confidence": "float -> float",
                        "quality_metrics": "Dict[str, float] -> Dict[str, float]"
                    },
                    input_data_types=["PIL.Image.Image", "PIL.Image.Image", "Optional[str]", "Optional[str]"],
                    output_data_types=["numpy.ndarray", "float", "Dict[str, float]"],
                    preprocessing_steps=["prepare_diffusion_input", "normalize_diffusion"],
                    postprocessing_steps=["denormalize_diffusion", "final_compositing"],
                    normalization_mean=[0.485, 0.456, 0.406],
                    normalization_std=[0.229, 0.224, 0.225]
                )
            
            else:
                return DetailedDataSpec(
                    api_input_mapping={
                        "input_image": "fastapi.UploadFile -> PIL.Image.Image"
                    },
                    api_output_mapping={
                        "result": "numpy.ndarray -> base64_string"
                    },
                    input_data_types=["PIL.Image.Image"],
                    output_data_types=["numpy.ndarray"],
                    preprocessing_steps=["normalize"],
                    postprocessing_steps=["denormalize"],
                    normalization_mean=[0.485, 0.456, 0.406],
                    normalization_std=[0.229, 0.224, 0.225]
                )
                
        except Exception as e:
            logger.error(f"âŒ {step_name} DetailedDataSpec ìƒì„± ì‹¤íŒ¨: {e}")
            return DetailedDataSpec()
    
    PROJECT_STEP_CONFIGS = {}
    
    @classmethod
    def _initialize_configs(cls):
        """Step ì„¤ì • ì´ˆê¸°í™”"""
        if cls.PROJECT_STEP_CONFIGS:
            return
        
        try:
            cls.PROJECT_STEP_CONFIGS = {
                StepType.HUMAN_PARSING: StepConfig(
                    step_name="HumanParsingStep",
                    step_id=1,
                    class_name="HumanParsingStep",
                    module_path="app.ai_pipeline.steps.step_01_human_parsing",
                    step_type=StepType.HUMAN_PARSING,
                    priority=StepPriority.HIGH,
                    ai_models=[
                        AIModelConfig(
                            model_name="graphonomy.pth",
                            model_path="step_01_human_parsing/graphonomy.pth",
                            model_type="SegmentationModel",
                            size_gb=1.2,
                            requires_checkpoint=True,
                            preprocessing_required=["resize_512x512", "normalize_imagenet", "to_tensor"],
                            postprocessing_required=["argmax", "resize_original", "morphology_clean"]
                        )
                    ],
                    primary_model_name="graphonomy.pth",
                    detailed_data_spec=cls._create_detailed_data_spec("HumanParsingStep", 1)
                ),
                
                StepType.POSE_ESTIMATION: StepConfig(
                    step_name="PoseEstimationStep",
                    step_id=2,
                    class_name="PoseEstimationStep",
                    module_path="app.ai_pipeline.steps.step_02_pose_estimation",
                    step_type=StepType.POSE_ESTIMATION,
                    priority=StepPriority.MEDIUM,
                    ai_models=[
                        AIModelConfig(
                            model_name="yolov8n-pose.pt",
                            model_path="step_02_pose_estimation/yolov8n-pose.pt",
                            model_type="PoseModel",
                            size_gb=6.2,
                            requires_checkpoint=True,
                            preprocessing_required=["resize_640x640", "normalize_yolo"],
                            postprocessing_required=["extract_keypoints", "scale_coords", "filter_confidence"]
                        )
                    ],
                    primary_model_name="yolov8n-pose.pt",
                    detailed_data_spec=cls._create_detailed_data_spec("PoseEstimationStep", 2)
                ),
                
                StepType.VIRTUAL_FITTING: StepConfig(
                    step_name="VirtualFittingStep",
                    step_id=6,
                    class_name="VirtualFittingStep",
                    module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
                    step_type=StepType.VIRTUAL_FITTING,
                    priority=StepPriority.CRITICAL,
                    ai_models=[
                        AIModelConfig(
                            model_name="diffusion_pytorch_model.fp16.safetensors",
                            model_path="step_06_virtual_fitting/unet/diffusion_pytorch_model.fp16.safetensors",
                            model_type="UNetModel",
                            size_gb=4.8,
                            requires_checkpoint=True
                        ),
                        AIModelConfig(
                            model_name="v1-5-pruned-emaonly.safetensors",
                            model_path="step_06_virtual_fitting/v1-5-pruned-emaonly.safetensors",
                            model_type="DiffusionModel",
                            size_gb=4.0,
                            requires_checkpoint=True
                        )
                    ],
                    primary_model_name="diffusion_pytorch_model.fp16.safetensors",
                    detailed_data_spec=cls._create_detailed_data_spec("VirtualFittingStep", 6)
                ),
                
                # ë‚˜ë¨¸ì§€ Stepë“¤ë„ ë¹„ìŠ·í•˜ê²Œ ì¶”ê°€...
            }
            logger.info("âœ… í”„ë¡œì íŠ¸ Step ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í”„ë¡œì íŠ¸ Step ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            cls.PROJECT_STEP_CONFIGS = {}
    
    @classmethod
    def get_config(cls, step_type: StepType) -> StepConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        cls._initialize_configs()
        try:
            config = cls.PROJECT_STEP_CONFIGS.get(step_type)
            if config:
                return config
            else:
                logger.warning(f"âš ï¸ Step ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {step_type}")
                return StepConfig()
        except Exception as e:
            logger.error(f"âŒ Step ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {step_type} - {e}")
            return StepConfig()
    
    @classmethod
    def get_config_by_name(cls, step_name: str) -> Optional[StepConfig]:
        """Step ì´ë¦„ìœ¼ë¡œ ì„¤ì • ë°˜í™˜"""
        cls._initialize_configs()
        try:
            for config in cls.PROJECT_STEP_CONFIGS.values():
                if config.step_name == step_name or config.class_name == step_name:
                    return config
            logger.warning(f"âš ï¸ Step ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {step_name}")
            return None
        except Exception as e:
            logger.error(f"âŒ Step ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {step_name} - {e}")
            return None

# =============================================================================
# ğŸ”¥ 13. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

def get_environment_info() -> Dict[str, Any]:
    """í™˜ê²½ ì •ë³´ ì¡°íšŒ"""
    return {
        'project': {
            'project_root': str(PROJECT_ROOT),
            'backend_root': str(BACKEND_ROOT),
            'ai_pipeline_root': str(AI_PIPELINE_ROOT),
            'ai_models_root': str(AI_MODELS_ROOT),
            'structure_detected': AI_PIPELINE_ROOT.exists()
        },
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE,
            'pytorch_available': PYTORCH_AVAILABLE
        },
        'capabilities': {
            'ai_models': True,
            'checkpoint_loading': PYTORCH_AVAILABLE,
            'project_structure_based': True
        }
    }

def optimize_environment():
    """í™˜ê²½ ìµœì í™”"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ mycloset-ai-clean ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max í•˜ë“œì›¨ì–´ ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE and PYTORCH_AVAILABLE:
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        
        logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def validate_step_compatibility(step_instance: Any) -> Dict[str, Any]:
    """Step í˜¸í™˜ì„± ê²€ì¦"""
    try:
        result = {
            'compatible': False,
            'project_structure': False,
            'basestepmixin_compatible': False,
            'detailed_data_spec_compatible': False,
            'process_method_exists': False,
            'dependency_injection_ready': False,
            'errors': [],
            'warnings': [],
            'recommendations': []
        }
        
        if step_instance is None:
            result['errors'].append('Step ì¸ìŠ¤í„´ìŠ¤ê°€ None')
            return result
        
        # í´ë˜ìŠ¤ ìƒì† í™•ì¸
        class_name = step_instance.__class__.__name__
        mro = [cls.__name__ for cls in step_instance.__class__.__mro__]
        
        if 'BaseStepMixin' in mro:
            result['basestepmixin_compatible'] = True
        else:
            result['warnings'].append('BaseStepMixin ìƒì† ê¶Œì¥')
        
        # í”„ë¡œì íŠ¸ ë©”ì„œë“œ í™•ì¸
        required_methods = ['process', 'initialize']
        existing_methods = []
        
        for method_name in required_methods:
            if hasattr(step_instance, method_name):
                existing_methods.append(method_name)
        
        result['process_method_exists'] = 'process' in existing_methods
        result['project_structure'] = len(existing_methods) >= 1
        
        # DetailedDataSpec í™•ì¸
        if hasattr(step_instance, 'detailed_data_spec') and getattr(step_instance, 'detailed_data_spec') is not None:
            result['detailed_data_spec_compatible'] = True
        else:
            result['warnings'].append('DetailedDataSpec ë¡œë”© ê¶Œì¥')
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
        dependency_attrs = ['model_loader', 'memory_manager', 'data_converter']
        injected_deps = []
        for attr in dependency_attrs:
            if hasattr(step_instance, attr) and getattr(step_instance, attr) is not None:
                injected_deps.append(attr)
        
        result['dependency_injection_ready'] = len(injected_deps) >= 1
        result['injected_dependencies'] = injected_deps
        
        # VirtualFittingStep íŠ¹ë³„ í™•ì¸
        if class_name == 'VirtualFittingStep' or getattr(step_instance, 'step_id', 0) == 6:
            if hasattr(step_instance, 'model_loader'):
                result['virtual_fitting_ready'] = True
            else:
                result['warnings'].append('VirtualFittingStep ModelLoader í•„ìš”')
        
        # ì¢…í•© í˜¸í™˜ì„± íŒì •
        result['compatible'] = (
            result['basestepmixin_compatible'] and
            result['process_method_exists'] and
            result['dependency_injection_ready']
        )
        
        return result
        
    except Exception as e:
        return {
            'compatible': False,
            'error': str(e),
            'version': 'ModernStepInterface v7.0'
        }

def get_step_info(step_instance: Any) -> Dict[str, Any]:
    """Step ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        info = {
            'step_name': getattr(step_instance, 'step_name', 'Unknown'),
            'step_id': getattr(step_instance, 'step_id', 0),
            'class_name': step_instance.__class__.__name__,
            'module': step_instance.__class__.__module__,
            'device': getattr(step_instance, 'device', 'Unknown'),
            'is_initialized': getattr(step_instance, 'is_initialized', False),
            'has_model': getattr(step_instance, 'has_model', False),
            'model_loaded': getattr(step_instance, 'model_loaded', False),
            'project_compatible': True
        }
        
        # ì˜ì¡´ì„± ìƒíƒœ
        dependencies = {}
        for dep_name in ['model_loader', 'memory_manager', 'data_converter', 'central_hub_container', 'step_interface']:
            dep_value = hasattr(step_instance, dep_name) and getattr(step_instance, dep_name) is not None
            dependencies[dep_name] = dep_value
            
            # íƒ€ì… í™•ì¸
            if dep_value:
                dep_obj = getattr(step_instance, dep_name)
                dep_type = type(dep_obj).__name__
                dependencies[f'{dep_name}_type'] = dep_type
        
        info['dependencies'] = dependencies
        
        # DetailedDataSpec ìƒíƒœ
        detailed_data_spec_info = {}
        for attr_name in ['detailed_data_spec', 'api_input_mapping', 'api_output_mapping', 'preprocessing_steps', 'postprocessing_steps']:
            detailed_data_spec_info[attr_name] = hasattr(step_instance, attr_name) and getattr(step_instance, attr_name) is not None
        
        info['detailed_data_spec'] = detailed_data_spec_info
        
        return info
        
    except Exception as e:
        return {
            'error': str(e),
            'class_name': getattr(step_instance, '__class__', {}).get('__name__', 'Unknown') if step_instance else 'None',
            'project_compatible': True
        }

# =============================================================================
# ğŸ”¥ 14. ê²½ë¡œ í˜¸í™˜ì„± ì²˜ë¦¬
# =============================================================================

def setup_module_aliases():
    """ëª¨ë“ˆ ë³„ì¹­ ì„¤ì •"""
    try:
        current_module = sys.modules[__name__]
        
        if not current_module:
            logger.warning("âš ï¸ í˜„ì¬ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        
        try:
            # ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ ìƒì„±
            if 'app.ai_pipeline.interface' not in sys.modules:
                import types
                interface_module = types.ModuleType('app.ai_pipeline.interface')
                interface_module.step_interface = current_module
                sys.modules['app.ai_pipeline.interface'] = interface_module
                sys.modules['app.ai_pipeline.interface.step_interface'] = current_module
                logger.debug("âœ… ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì™„ë£Œ")
            
            return True
            
        except Exception as alias_error:
            logger.warning(f"âš ï¸ ë³„ì¹­ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {alias_error}")
            return False
            
    except Exception as e:
        logger.warning(f"âš ï¸ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

# ëª¨ë“ˆ ë³„ì¹­ ì„¤ì • ì‹¤í–‰
try:
    setup_module_aliases()
except Exception as e:
    logger.warning(f"âš ï¸ ëª¨ë“ˆ ë³„ì¹­ ì„¤ì • ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 15. Export ì„¤ì • (í”„ë¡œì íŠ¸ í˜¸í™˜ì„±)
# =============================================================================

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
GitHubStepModelInterface = StepModelInterface
RealStepModelInterface = StepModelInterface
GitHubMemoryManager = MemoryManager
RealMemoryManager = MemoryManager
GitHubDependencyManager = DependencyManager
RealDependencyManager = DependencyManager

# Step ìƒì„± ê²°ê³¼ ë³„ì¹­
GitHubStepCreationResult = StepCreationResult
RealStepCreationResult = StepCreationResult

# íŒ©í† ë¦¬ í•¨ìˆ˜ ë³„ì¹­
create_github_step_interface_circular_reference_free = create_step_interface
create_optimized_github_interface_v51 = create_optimized_interface
create_step_07_virtual_fitting_interface_v51 = create_virtual_fitting_interface
create_real_step_interface = create_step_interface
create_optimized_real_interface = create_optimized_interface
create_virtual_fitting_step_interface = create_virtual_fitting_interface
create_simple_step_interface = create_simple_interface

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë³„ì¹­
get_github_environment_info = get_environment_info
get_real_environment_info = get_environment_info
optimize_github_environment = optimize_environment
optimize_real_environment = optimize_environment
validate_github_step_compatibility = validate_step_compatibility
validate_real_step_compatibility = validate_step_compatibility
get_github_step_info = get_step_info
get_real_step_info = get_step_info

# í´ë˜ìŠ¤ ë°˜í™˜ í•¨ìˆ˜ë“¤
def get_github_step_model_interface():
    """StepModelInterface í´ë˜ìŠ¤ ë°˜í™˜"""
    return StepModelInterface

def get_step_interface_class():
    """Step Interface í´ë˜ìŠ¤ ë°˜í™˜"""
    return StepModelInterface

def create_step_model_interface(step_name: str, model_loader=None) -> StepModelInterface:
    """Step Model Interface ìƒì„± - ê¸°ë³¸ íŒ©í† ë¦¬"""
    return create_step_interface(step_name, model_loader)

# =============================================================================
# ğŸ”¥ 16. __all__ Export List
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'StepModelInterface',
    'MemoryManager', 
    'DependencyManager',
    'ProjectStepMapping',
    'DetailedDataSpec',
    'AIModelConfig',
    'StepConfig',
    'EnhancedStepRequest',
    
    # í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤
    'GitHubStepModelInterface',
    'RealStepModelInterface',
    'GitHubMemoryManager',
    'RealMemoryManager',
    'GitHubDependencyManager',
    'RealDependencyManager',
    'SimpleStepInterface',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'StepCreationResult',
    'GitHubStepCreationResult',
    'RealStepCreationResult',
    'StepType',
    'StepPriority',
    'ProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_step_interface',
    'create_optimized_interface',
    'create_virtual_fitting_interface',
    'create_simple_interface',
    
    # í˜¸í™˜ì„± íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_github_step_interface_circular_reference_free',
    'create_optimized_github_interface_v51',
    'create_step_07_virtual_fitting_interface_v51',
    'create_real_step_interface',
    'create_optimized_real_interface',
    'create_virtual_fitting_step_interface',
    'create_simple_step_interface',
    'create_step_model_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_environment_info',
    'optimize_environment',
    'validate_step_compatibility',
    'get_step_info',
    
    # í˜¸í™˜ì„± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_github_environment_info',
    'get_real_environment_info',
    'optimize_github_environment',
    'optimize_real_environment',
    'validate_github_step_compatibility',
    'validate_real_step_compatibility',
    'get_github_step_info',
    'get_real_step_info',
    'get_github_step_model_interface',
    'get_step_interface_class',
    
    # ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
    'setup_module_aliases',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE',
    'PYTORCH_AVAILABLE',
    'DEVICE',
    'PROJECT_ROOT',
    'BACKEND_ROOT',
    'AI_PIPELINE_ROOT',
    'AI_MODELS_ROOT',
    'STEP_ID_TO_NAME_MAPPING',
    'STEP_NAME_TO_ID_MAPPING',
    
    # Logger
    'logger'

    'get_github_environment_info',
    'get_real_environment_info',
    'optimize_github_environment',
    'optimize_real_environment',
    'validate_github_step_compatibility',
    'validate_real_step_compatibility',
    'get_github_step_info',
    'get_real_step_info',
    'get_github_step_model_interface',
    'get_step_interface_class',
    
    # ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
    'setup_module_aliases',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE',
    'PYTORCH_AVAILABLE',
    'DEVICE',
    'PROJECT_ROOT',
    'BACKEND_ROOT',
    'AI_PIPELINE_ROOT',
    'AI_MODELS_ROOT',
    'STEP_ID_TO_NAME_MAPPING',
    'STEP_NAME_TO_ID_MAPPING',
    
    # Logger
    'logger'
]

# =============================================================================
# ğŸ”¥ 17. ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
if AI_PIPELINE_ROOT.exists():
    logger.info(f"âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€: {PROJECT_ROOT}")
else:
    logger.warning(f"âš ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ í•„ìš”: {PROJECT_ROOT}")

# AI ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
if AI_MODELS_ROOT.exists():
    logger.info(f"âœ… AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì§€: {AI_MODELS_ROOT}")
    
    # AI ëª¨ë¸ í™•ì¸
    total_size_gb = 0
    model_count = 0
    for model_path in AI_MODELS_ROOT.rglob("*.pth"):
        if model_path.is_file():
            size_gb = model_path.stat().st_size / (1024**3)
            total_size_gb += size_gb
            model_count += 1
    
    for model_path in AI_MODELS_ROOT.rglob("*.safetensors"):
        if model_path.is_file():
            size_gb = model_path.stat().st_size / (1024**3)
            total_size_gb += size_gb
            model_count += 1
    
    logger.info(f"ğŸ“Š AI ëª¨ë¸ í˜„í™©: {model_count}ê°œ íŒŒì¼, {total_size_gb:.1f}GB")
else:
    logger.warning(f"âš ï¸ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸ í•„ìš”: {AI_MODELS_ROOT}")

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_environment()
    logger.info("ğŸ conda í™˜ê²½ mycloset-ai-clean ìë™ ìµœì í™” ì™„ë£Œ!")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        if MPS_AVAILABLE and PYTORCH_AVAILABLE:
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

# í”„ë¡œì íŠ¸ Step ë§¤í•‘ ì´ˆê¸°í™”
ProjectStepMapping._initialize_configs()

logger.info("=" * 80)
logger.info("ğŸ”¥ Modern Step Interface v7.0 - í”„ë¡œì íŠ¸ êµ¬ì¡° ì™„ì „ ë§ì¶¤í˜•")
logger.info("=" * 80)
logger.info("âœ… í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜ ì‹¤ì œ êµ¬ì¡° 100% ë°˜ì˜")
logger.info("âœ… BaseStepMixin v19.2 ì™„ì „ í˜¸í™˜")
logger.info("âœ… StepFactory v11.0 ì—°ë™ ìµœì í™”")
logger.info("âœ… RealAIStepImplementationManager v14.0 í†µí•©")
logger.info("âœ… Central Hub DI Container ì™„ì „ í™œìš©")
logger.info("âœ… DetailedDataSpec ì‹¤ì œ ê¸°ëŠ¥ êµ¬í˜„")
logger.info("âœ… M3 Max 128GB + conda mycloset-ai-clean ìµœì í™”")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB í™œìš©")

logger.info(f"ğŸ”§ í”„ë¡œì íŠ¸ í™˜ê²½ ì •ë³´:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ…' if CONDA_INFO['is_target_env'] else 'âš ï¸'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - PyTorch: {'âœ…' if PYTORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ë””ë°”ì´ìŠ¤: {DEVICE}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")

logger.info("ğŸ¯ í”„ë¡œì íŠ¸ Step í´ë˜ìŠ¤:")
for step_id, step_name in STEP_ID_TO_NAME_MAPPING.items():
    status = "â­" if step_id == 6 else "âœ…"  # VirtualFittingStep íŠ¹ë³„ í‘œì‹œ
    logger.info(f"   {status} Step {step_id}: {step_name}")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ í”„ë¡œì íŠ¸ ì‹¤ì œ êµ¬ì¡° ë°˜ì˜: BaseStepMixin, StepFactory, RealAIStepImplementationManager")
logger.info("   â€¢ DetailedDataSpec: ì‹¤ì œ API â†” Step ë°ì´í„° ë§¤í•‘")
logger.info("   â€¢ StepModelInterface: í”„ë¡œì íŠ¸ ModelLoader íŒ¨í„´ ê¸°ë°˜")
logger.info("   â€¢ DependencyManager: Central Hub DI Container í™œìš©")
logger.info("   â€¢ MemoryManager: M3 Max 128GB ì™„ì „ í™œìš©")
logger.info("   â€¢ ProjectStepMapping: ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ê¸°ë°˜")

logger.info("ğŸš€ í”„ë¡œì íŠ¸ ì—°ë™ êµ¬ì¡°:")
logger.info("   StepServiceManager v15.0")
logger.info("        â†“ (RealAIStepImplementationManager v14.0)")
logger.info("   StepFactory v11.0")
logger.info("        â†“ (Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± + ì˜ì¡´ì„± ì£¼ì…)")
logger.info("   BaseStepMixin v19.2")
logger.info("        â†“ (Modern Step Interface v7.0 í™œìš©)")
logger.info("   ì‹¤ì œ AI ëª¨ë¸ë“¤ (229GB)")

logger.info("ğŸ”§ ì£¼ìš” íŒ©í† ë¦¬ í•¨ìˆ˜ (í”„ë¡œì íŠ¸ êµ¬ì¡°):")
logger.info("   - create_step_interface(): í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜")
logger.info("   - create_optimized_interface(): ìµœì í™”ëœ ì¸í„°í˜ì´ìŠ¤")
logger.info("   - create_virtual_fitting_interface(): VirtualFittingStep ì „ìš©")
logger.info("   - create_simple_interface(): í˜¸í™˜ì„±ìš©")

logger.info("ğŸ”„ í˜¸í™˜ì„± ì§€ì› (ê¸°ì¡´ ì½”ë“œ 100% ì§€ì›):")
logger.info("   - GitHubStepModelInterface â†’ StepModelInterface")
logger.info("   - RealStepModelInterface â†’ StepModelInterface")
logger.info("   - create_github_step_interface_circular_reference_free()")
logger.info("   - create_optimized_github_interface_v51()")
logger.info("   - SimpleStepInterface: ê¸°ì¡´ Step íŒŒì¼ë“¤ê³¼ í˜¸í™˜")

logger.info("ğŸ‰ í•µì‹¬ ì°¨ë³„ì :")
logger.info("   âœ… í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ êµ¬ì¡° 100% ë°˜ì˜")
logger.info("   âœ… BaseStepMixinì˜ ì‹¤ì œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ í™œìš©")
logger.info("   âœ… StepFactoryì˜ ì‹¤ì œ ìƒì„± ë¡œì§ê³¼ ì™„ì „ ì—°ë™")
logger.info("   âœ… Central Hub DI Containerì˜ ì‹¤ì œ ê¸°ëŠ¥ í™œìš©")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œì™€ ì™„ì „ ë§¤í•‘")
logger.info("   âœ… M3 Max + conda í™˜ê²½ì˜ ì‹¤ì œ ìµœì í™” ì ìš©")

logger.info("ğŸ‰ Modern Step Interface v7.0 ì™„ë£Œ!")
logger.info("ğŸ‰ ì´ì œ í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ êµ¬ì¡°ì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤!")
logger.info("ğŸ‰ BaseStepMixin, StepFactory, RealAIStepImplementationManagerì™€ ì™„ì „ ì—°ë™ë©ë‹ˆë‹¤!")
logger.info("ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ 229GBë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤!")
logger.info("ğŸ‰ M3 Max 128GB ë©”ëª¨ë¦¬ë¥¼ ì™„ì „íˆ í™œìš©í•©ë‹ˆë‹¤!")
logger.info("=" * 80)
