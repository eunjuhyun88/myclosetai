# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ Step Interface v4.0 - GitHub êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ì¬ì‘ì„±
=========================================================

âœ… ì‹¤ì œ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜
âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜
âœ… DetailedDataSpec ì™„ì „ í†µí•©
âœ… StepFactory v11.0 ì—°ë™ ìµœì í™”
âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì •í™• ë§¤í•‘
âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„
âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì§€ì›
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€

Author: MyCloset AI Team
Date: 2025-07-28
Version: 4.0 (Complete GitHub Structure Implementation)
"""

import os
import gc
import sys
import time
import logging


# ğŸ”¥ ëª¨ë“ˆ ë ˆë²¨ logger ì•ˆì „ ì •ì˜
def create_module_logger():
    """ëª¨ë“ˆ ë ˆë²¨ logger ì•ˆì „ ìƒì„±"""
    try:
        module_logger = logging.getLogger(__name__)
        if not module_logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            module_logger.addHandler(handler)
            module_logger.setLevel(logging.INFO)
        return module_logger
    except Exception as e:
        # ìµœí›„ í´ë°±
        import sys
        print(f"âš ï¸ Logger ìƒì„± ì‹¤íŒ¨, stdout ì‚¬ìš©: {e}", file=sys.stderr)
        class FallbackLogger:
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def debug(self, msg): print(f"DEBUG: {msg}")
        return FallbackLogger()

# ëª¨ë“ˆ ë ˆë²¨ logger
logger = create_module_logger()
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache
from contextlib import asynccontextmanager
import json
import hashlib

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core import DIContainer
    from ..steps.base_step_mixin import BaseStepMixin

# ğŸ”¥ ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ í•´ê²°
try:
    # PyTorch weights_only ë¬¸ì œ í•´ê²°
    import torch
    # PyTorch 2.0+ weights_only ê¸°ë³¸ê°’ì„ Falseë¡œ ì„¤ì •
    if hasattr(torch, 'load'):
        original_torch_load = torch.load
        def safe_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
            # weights_onlyë¥¼ Falseë¡œ ê°•ì œ ì„¤ì •í•˜ì—¬ í˜¸í™˜ì„± í™•ë³´
            if weights_only is None:
                weights_only = False
            return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=weights_only, **kwargs)
        torch.load = safe_torch_load
        logger.info("âœ… PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ PyTorch íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

# ğŸ”¥ rembg ì„¸ì…˜ ë¬¸ì œ í•´ê²°
try:
    import rembg
    # rembg ë²„ì „ í˜¸í™˜ì„± í™•ì¸
    if hasattr(rembg, 'sessions'):
        logger.info("âœ… rembg ì„¸ì…˜ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ rembg ì„¸ì…˜ ëª¨ë“ˆ í˜¸í™˜ì„± ë¬¸ì œ - í´ë°± ëª¨ë“œ ì‚¬ìš©")
except Exception as e:
    logger.warning(f"âš ï¸ rembg ëª¨ë“ˆ ë¬¸ì œ: {e}")

# ğŸ”¥ Safetensors í˜¸í™˜ì„± í™•ì¸  
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
    logger.info("âœ… Safetensors ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SAFETENSORS_AVAILABLE = False
    logger.warning("âš ï¸ Safetensors ì‚¬ìš© ë¶ˆê°€ - .pth íŒŒì¼ ìš°ì„  ì‚¬ìš©")

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ ê²½ë¡œ í˜¸í™˜ì„± ê²½ê³  ë° í´ë°± ì‹œìŠ¤í…œ
# =============================================================================

# ê¸°ì¡´ ê²½ë¡œì—ì„œ ì ‘ê·¼í•˜ëŠ” ì½”ë“œë“¤ì„ ìœ„í•œ í˜¸í™˜ì„± ì§€ì›
import warnings

def deprecated_interface_warning():
    """Deprecated interface ê²½ë¡œ ê²½ê³ """
    warnings.warn(
        "âš ï¸ app.ai_pipeline.interface ê²½ë¡œëŠ” deprecatedì…ë‹ˆë‹¤. "
        "app.ai_pipeline.interfacesë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
        DeprecationWarning,
        stacklevel=3
    )
    logger.warning("âš ï¸ Deprecated interface ê²½ë¡œ ì‚¬ìš© ê°ì§€ - interfacesë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê¶Œì¥")

# í´ë°±ì„ ìœ„í•œ ëª¨ë“ˆ ë³„ì¹­ ìƒì„± 
try:
    import sys
    current_module = sys.modules[__name__]
    # app.ai_pipeline.interface.step_interfaceë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ë³„ì¹­ ìƒì„±
    if 'app.ai_pipeline.interface' not in sys.modules:
        import types
        interface_module = types.ModuleType('app.ai_pipeline.interface')
        interface_module.step_interface = current_module
        sys.modules['app.ai_pipeline.interface'] = interface_module
        sys.modules['app.ai_pipeline.interface.step_interface'] = current_module
        logger.info("âœ… ê¸°ì¡´ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì™„ë£Œ")
except Exception as e:
    logger.warning(f"âš ï¸ ê²½ë¡œ í˜¸í™˜ì„± ë³„ì¹­ ìƒì„± ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ê°ì§€ ë° ìµœì í™” (Logger ì •ì˜ í›„)
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ ì •ë³´
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
BACKEND_ROOT = PROJECT_ROOT / "backend"
AI_PIPELINE_ROOT = BACKEND_ROOT / "app" / "ai_pipeline"

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
    'project_path': str(PROJECT_ROOT)
}

# í•˜ë“œì›¨ì–´ ê°ì§€
IS_M3_MAX = False
MEMORY_GB = 16.0
MPS_AVAILABLE = False

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
    
    # MPS ê°ì§€
    import torch
    MPS_AVAILABLE = (
        IS_M3_MAX and 
        hasattr(torch.backends, 'mps') and 
        torch.backends.mps.is_available()
    )
except ImportError:
    pass
except Exception:
    pass

# =============================================================================
# ğŸ”¥ GitHub Step íƒ€ì… ë° ìƒìˆ˜ ì •ì˜
# =============================================================================

class GitHubStepType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step íƒ€ì…"""
    HUMAN_PARSING = "human_parsing"          # step_01
    POSE_ESTIMATION = "pose_estimation"      # step_02
    CLOTH_SEGMENTATION = "cloth_segmentation" # step_03
    GEOMETRIC_MATCHING = "geometric_matching" # step_04
    CLOTH_WARPING = "cloth_warping"          # step_05
    VIRTUAL_FITTING = "virtual_fitting"      # step_06
    POST_PROCESSING = "post_processing"      # step_07
    QUALITY_ASSESSMENT = "quality_assessment" # step_08

class GitHubStepPriority(Enum):
    """GitHub Step ìš°ì„ ìˆœìœ„ (ì‹¤ì œ AI ëª¨ë¸ í¬ê¸° ê¸°ë°˜)"""
    CRITICAL = 1      # Virtual Fitting (14GB), Quality Assessment (7GB)
    HIGH = 2          # Cloth Warping (7GB), Human Parsing (4GB)
    MEDIUM = 3        # Cloth Segmentation (5.5GB), Pose Estimation (3.4GB)
    LOW = 4           # Post Processing (1.3GB), Geometric Matching (1.3GB)

class GitHubDeviceType(Enum):
    """GitHub í”„ë¡œì íŠ¸ ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"

class GitHubProcessingStatus(Enum):
    """GitHub Step ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"
    MOCK_MODE = "mock_mode"     # ğŸ”¥ 7ë‹¨ê³„ Mock ë¬¸ì œ í•´ê²°ìš©

# =============================================================================
# ğŸ”¥ GitHub BaseStepMixin í˜¸í™˜ ì„¤ì •
# =============================================================================

@dataclass
class GitHubStepConfig:
    """
    ğŸ”¥ GitHub BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜ ì„¤ì •
    
    ì‹¤ì œ GitHub Step íŒŒì¼ë“¤ê³¼ 100% í˜¸í™˜ë˜ëŠ” ì„¤ì • êµ¬ì¡°
    """
    # ê¸°ë³¸ Step ì •ë³´ (GitHub êµ¬ì¡° ê¸°ë°˜)
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    module_path: str = ""
    
    # GitHub Step íƒ€ì…
    step_type: GitHubStepType = GitHubStepType.HUMAN_PARSING
    priority: GitHubStepPriority = GitHubStepPriority.MEDIUM
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # GitHub BaseStepMixin í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_support: bool = True
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­ (GitHub êµ¬ì¡° ê¸°ë°˜)
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = False
    require_step_interface: bool = True
    
    # AI ëª¨ë¸ ì„¤ì • (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 1.0
    primary_model_file: str = ""
    checkpoint_patterns: List[str] = field(default_factory=list)
    
    # GitHub í™˜ê²½ ìµœì í™”
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    
    # DetailedDataSpec ì„¤ì •
    enable_detailed_data_spec: bool = True
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # 7ë‹¨ê³„ Mock ë¬¸ì œ í•´ê²° ì„¤ì •
    force_real_ai_processing: bool = True
    mock_mode_disabled: bool = True
    fallback_on_ai_failure: bool = False
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ GitHub í™˜ê²½ ìµœì í™”"""
        # conda í™˜ê²½ ìë™ ìµœì í™”
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            # M3 Max + conda ì¡°í•© ìµœì í™”
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.step_name == "VirtualFittingStep" or self.step_id == 7:
            self.force_real_ai_processing = True
            self.mock_mode_disabled = True
            self.fallback_on_ai_failure = False
        
        # AI ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        if not isinstance(self.ai_models, list):
            self.ai_models = []

# =============================================================================
# ğŸ”¥ GitHub Step ë§¤í•‘ ì‹œìŠ¤í…œ
# =============================================================================

class GitHubStepMapping:
    """GitHub í”„ë¡œì íŠ¸ ì‹¤ì œ Step ë§¤í•‘"""
    
    GITHUB_STEP_CONFIGS = {
        GitHubStepType.HUMAN_PARSING: GitHubStepConfig(
            step_name="HumanParsingStep",
            step_id=1,
            class_name="HumanParsingStep",
            module_path="app.ai_pipeline.steps.step_01_human_parsing",
            step_type=GitHubStepType.HUMAN_PARSING,
            priority=GitHubStepPriority.HIGH,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["graphonomy.pth", "atr_model.pth", "human_parsing_schp.pth"],
            model_size_gb=4.0,
            primary_model_file="graphonomy.pth",
            checkpoint_patterns=["*.pth", "*.pt", "*.safetensors"],
            api_input_mapping={
                "person_image": "fastapi.UploadFile -> PIL.Image.Image",
                "height": "float -> float",
                "weight": "float -> float"
            },
            api_output_mapping={
                "parsed_image": "numpy.ndarray -> base64_string",
                "mask": "numpy.ndarray -> base64_string",
                "segments": "Dict[str, Any] -> Dict[str, Any]"
            },
            preprocessing_steps=["validate_input", "resize_image", "normalize"],
            postprocessing_steps=["format_output", "encode_images"]
        ),
        
        GitHubStepType.POSE_ESTIMATION: GitHubStepConfig(
            step_name="PoseEstimationStep",
            step_id=2,
            class_name="PoseEstimationStep",
            module_path="app.ai_pipeline.steps.step_02_pose_estimation",
            step_type=GitHubStepType.POSE_ESTIMATION,
            priority=GitHubStepPriority.MEDIUM,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["pose_model.pth", "openpose_model.pth", "yolov8n-pose.pt"],
            model_size_gb=3.4,
            primary_model_file="pose_model.pth",
            checkpoint_patterns=["*pose*.pth", "*pose*.pt", "yolo*.pt"],
            api_input_mapping={
                "person_image": "fastapi.UploadFile -> PIL.Image.Image"
            },
            api_output_mapping={
                "keypoints": "numpy.ndarray -> List[Dict[str, float]]",
                "pose_image": "numpy.ndarray -> base64_string",
                "confidence": "float -> float"
            },
            preprocessing_steps=["validate_input", "resize_image"],
            postprocessing_steps=["format_keypoints", "draw_pose"]
        ),
        
        GitHubStepType.CLOTH_SEGMENTATION: GitHubStepConfig(
            step_name="ClothSegmentationStep",
            step_id=3,
            class_name="ClothSegmentationStep",
            module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
            step_type=GitHubStepType.CLOTH_SEGMENTATION,
            priority=GitHubStepPriority.MEDIUM,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["sam_vit_h_4b8939.pth", "cloth_segmentation.pth", "u2net.pth"],
            model_size_gb=5.5,
            primary_model_file="sam_vit_h_4b8939.pth",
            checkpoint_patterns=["sam_*.pth", "*segmentation*.pth", "u2net*.pth"],
            api_input_mapping={
                "clothing_image": "fastapi.UploadFile -> PIL.Image.Image"
            },
            api_output_mapping={
                "segmented_cloth": "numpy.ndarray -> base64_string",
                "mask": "numpy.ndarray -> base64_string",
                "bbox": "List[int] -> List[int]"
            },
            preprocessing_steps=["validate_input", "resize_image"],
            postprocessing_steps=["refine_mask", "crop_cloth"]
        ),
        
        GitHubStepType.GEOMETRIC_MATCHING: GitHubStepConfig(
            step_name="GeometricMatchingStep",
            step_id=4,
            class_name="GeometricMatchingStep",
            module_path="app.ai_pipeline.steps.step_04_geometric_matching",
            step_type=GitHubStepType.GEOMETRIC_MATCHING,
            priority=GitHubStepPriority.LOW,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.7,
            ai_models=["geometric_matching.pth", "tps_model.pth"],
            model_size_gb=1.3,
            primary_model_file="geometric_matching.pth",
            checkpoint_patterns=["*matching*.pth", "tps*.pth"],
            api_input_mapping={
                "person_image": "PIL.Image.Image -> PIL.Image.Image",
                "cloth_image": "PIL.Image.Image -> PIL.Image.Image"
            },
            api_output_mapping={
                "warped_cloth": "numpy.ndarray -> base64_string",
                "flow_field": "numpy.ndarray -> numpy.ndarray"
            },
            preprocessing_steps=["align_images", "extract_features"],
            postprocessing_steps=["apply_transformation"]
        ),
        
        GitHubStepType.CLOTH_WARPING: GitHubStepConfig(
            step_name="ClothWarpingStep",
            step_id=5,
            class_name="ClothWarpingStep",
            module_path="app.ai_pipeline.steps.step_05_cloth_warping",
            step_type=GitHubStepType.CLOTH_WARPING,
            priority=GitHubStepPriority.HIGH,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.6,
            ai_models=["cloth_warping.pth", "flow_estimation.pth", "RealVisXL_V4.0.safetensors"],
            model_size_gb=7.0,
            primary_model_file="RealVisXL_V4.0.safetensors",
            checkpoint_patterns=["*warping*.pth", "*flow*.pth", "RealVis*.safetensors"],
            api_input_mapping={
                "cloth_image": "PIL.Image.Image -> PIL.Image.Image",
                "target_pose": "numpy.ndarray -> numpy.ndarray"
            },
            api_output_mapping={
                "warped_cloth": "numpy.ndarray -> base64_string"
            },
            preprocessing_steps=["prepare_cloth", "prepare_pose"],
            postprocessing_steps=["refine_warping"]
        ),
        
        GitHubStepType.VIRTUAL_FITTING: GitHubStepConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            class_name="VirtualFittingStep",
            module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
            step_type=GitHubStepType.VIRTUAL_FITTING,
            priority=GitHubStepPriority.CRITICAL,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.8,
            # ğŸ”¥ ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ ì‚¬ìš©
            ai_models=[
                "v1-5-pruned.safetensors",           # 7.2GB ì‹¤ì œ ë°œê²¬ë¨
                "v1-5-pruned-emaonly.safetensors",  # 4.0GB ì‹¤ì œ ë°œê²¬ë¨  
                "stable-diffusion-v1-5",
                "controlnet",
                "vae"
            ],
            model_size_gb=14.0,
            primary_model_file="v1-5-pruned.safetensors",  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼
            checkpoint_patterns=[
                "v1-5*.safetensors", 
                "*diffusion*.safetensors", 
                "*controlnet*.pth",
                "*.safetensors",
                "*.pth"
            ],
            # ğŸ”¥ 7ë‹¨ê³„ Mock ë¬¸ì œ í•´ê²°
            force_real_ai_processing=True,
            mock_mode_disabled=True,
            fallback_on_ai_failure=False,
            api_input_mapping={
                "person_image": "PIL.Image.Image -> PIL.Image.Image",
                "warped_cloth": "PIL.Image.Image -> PIL.Image.Image",
                "pose_keypoints": "List[Dict] -> numpy.ndarray"
            },
            api_output_mapping={
                "fitted_image": "numpy.ndarray -> base64_string",
                "fit_score": "float -> float",
                "confidence": "float -> float"
            },
            preprocessing_steps=["prepare_inputs", "encode_images", "validate_models"],
            postprocessing_steps=["decode_image", "enhance_quality", "validate_output"]
        ),
        
        GitHubStepType.POST_PROCESSING: GitHubStepConfig(
            step_name="PostProcessingStep",
            step_id=7,
            class_name="PostProcessingStep",
            module_path="app.ai_pipeline.steps.step_07_post_processing",
            step_type=GitHubStepType.POST_PROCESSING,
            priority=GitHubStepPriority.LOW,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["super_resolution.pth", "enhancement.pth"],
            model_size_gb=1.3,
            primary_model_file="super_resolution.pth",
            checkpoint_patterns=["*super*.pth", "*enhancement*.pth"],
            api_input_mapping={
                "fitted_image": "PIL.Image.Image -> PIL.Image.Image"
            },
            api_output_mapping={
                "enhanced_image": "numpy.ndarray -> base64_string",
                "quality_score": "float -> float"
            },
            preprocessing_steps=["validate_image"],
            postprocessing_steps=["enhance_image", "apply_filters"]
        ),
        
        GitHubStepType.QUALITY_ASSESSMENT: GitHubStepConfig(
            step_name="QualityAssessmentStep",
            step_id=8,
            class_name="QualityAssessmentStep",
            module_path="app.ai_pipeline.steps.step_08_quality_assessment",
            step_type=GitHubStepType.QUALITY_ASSESSMENT,
            priority=GitHubStepPriority.CRITICAL,
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.7,
            ai_models=["open_clip_pytorch_model.bin", "ViT-L-14.pt", "clip_model.pth"],
            model_size_gb=7.0,
            primary_model_file="open_clip_pytorch_model.bin",
            checkpoint_patterns=["open_clip*.bin", "ViT*.pt", "*clip*.pth"],
            api_input_mapping={
                "final_image": "PIL.Image.Image -> PIL.Image.Image",
                "original_image": "PIL.Image.Image -> PIL.Image.Image"
            },
            api_output_mapping={
                "quality_score": "float -> float",
                "similarity_score": "float -> float",
                "recommendations": "List[str] -> List[str]"
            },
            preprocessing_steps=["prepare_comparison"],
            postprocessing_steps=["calculate_scores", "generate_recommendations"]
        )
    }
    
    @classmethod
    def get_config(cls, step_type: GitHubStepType) -> GitHubStepConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        return cls.GITHUB_STEP_CONFIGS.get(step_type, GitHubStepConfig())
    
    @classmethod
    def get_config_by_name(cls, step_name: str) -> Optional[GitHubStepConfig]:
        """Step ì´ë¦„ìœ¼ë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_name == step_name or config.class_name == step_name:
                return config
        return None
    
    @classmethod
    def get_config_by_id(cls, step_id: int) -> Optional[GitHubStepConfig]:
        """Step IDë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.GITHUB_STEP_CONFIGS.values():
            if config.step_id == step_id:
                return config
        return None

# =============================================================================
# ğŸ”¥ ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)
# =============================================================================

class GitHubMemoryManager:
    """GitHub í”„ë¡œì íŠ¸ìš© ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = logging.getLogger(f"{__name__}.GitHubMemoryManager")
        
        # M3 Max ìë™ ìµœì í™”
        if max_memory_gb is None:
            if IS_M3_MAX and MEMORY_GB >= 64:
                self.max_memory_gb = MEMORY_GB * 0.85  # 85% ì‚¬ìš©
            elif IS_M3_MAX:
                self.max_memory_gb = MEMORY_GB * 0.8   # 80% ì‚¬ìš©
            elif CONDA_INFO['is_target_env']:
                self.max_memory_gb = 12.0              # conda í™˜ê²½ 12GB
            else:
                self.max_memory_gb = 8.0               # ê¸°ë³¸ 8GB
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        
        self.logger.info(f"ğŸ§  GitHub ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner}")
                return True
            else:
                available = self.max_memory_gb - self.current_memory_gb
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {available:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner}")
                return size_gb
            return 0.0
    
    def optimize_for_github_project(self):
        """GitHub í”„ë¡œì íŠ¸ íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (M3 Max)
            if self.mps_enabled:
                import torch
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    self.logger.debug("ğŸ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬")
            
            # Python GC
            gc.collect()
            
            # 128GB M3 Max íŠ¹ë³„ ìµœì í™”
            if IS_M3_MAX and MEMORY_GB >= 128:
                self.max_memory_gb = min(MEMORY_GB * 0.9, 115.0)  # 115GBê¹Œì§€ ì‚¬ìš©
                self.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'memory_pool': self.memory_pool.copy(),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'total_system_gb': MEMORY_GB,
                'github_optimized': True
            }

# =============================================================================
# ğŸ”¥ GitHub ì˜ì¡´ì„± ê´€ë¦¬ì
# =============================================================================

@dataclass
class GitHubDependencyStatus:
    """GitHub í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ìƒíƒœ"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    
    # BaseStepMixin v19.1 ìƒíƒœ
    base_initialized: bool = False
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ
    github_compatible: bool = False
    real_ai_models_loaded: bool = False
    mock_mode_disabled: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)

class GitHubDependencyManager:
    """GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ì"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"GitHubDependencyManager.{step_name}")
        
        # ì˜ì¡´ì„± ìƒíƒœ
        self.dependency_status = GitHubDependencyStatus()
        
        # ì˜ì¡´ì„± ì €ì¥ì†Œ
        self.dependencies: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = GitHubMemoryManager()
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        self.logger.debug(f"ğŸ”§ GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì´ˆê¸°í™”: {step_name}")
    
    def inject_dependency(self, name: str, dependency: Any, required: bool = False) -> bool:
        """ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                if dependency is not None:
                    self.dependencies[name] = dependency
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    if name == 'model_loader':
                        self.dependency_status.model_loader = True
                    elif name == 'memory_manager':
                        self.dependency_status.memory_manager = True
                    elif name == 'data_converter':
                        self.dependency_status.data_converter = True
                    elif name == 'di_container':
                        self.dependency_status.di_container = True
                    elif name == 'step_interface':
                        self.dependency_status.step_interface = True
                    
                    self.logger.debug(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ: {name}")
                    return True
                else:
                    if required:
                        self.logger.error(f"âŒ í•„ìˆ˜ ì˜ì¡´ì„± {name}ì´ None")
                        return False
                    else:
                        self.logger.warning(f"âš ï¸ ì„ íƒì  ì˜ì¡´ì„± {name}ì´ None")
                        return True
        
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {name} - {e}")
            return False
    
    def auto_inject_github_dependencies(self) -> bool:
        """GitHub í”„ë¡œì íŠ¸ ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # conda í™˜ê²½ ìµœì í™”
            if CONDA_INFO['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™”")
            
            # M3 Max ìµœì í™”
            if IS_M3_MAX:
                self.dependency_status.m3_max_optimized = True
                self.memory_manager.optimize_for_github_project()
                self.logger.debug("âœ… M3 Max ìµœì í™”")
            
            # GitHub í˜¸í™˜ì„± í™œì„±í™”
            self.dependency_status.github_compatible = True
            self.dependency_status.base_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def validate_github_dependencies(self, config: GitHubStepConfig) -> Tuple[bool, List[str]]:
        """GitHub ì˜ì¡´ì„± ê²€ì¦"""
        errors = []
        
        with self._lock:
            # í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸
            if config.require_model_loader and not self.dependency_status.model_loader:
                errors.append("ModelLoader í•„ìš”")
            
            if config.require_memory_manager and not self.dependency_status.memory_manager:
                errors.append("MemoryManager í•„ìš”")
            
            if config.require_data_converter and not self.dependency_status.data_converter:
                errors.append("DataConverter í•„ìš”")
            
            if config.require_step_interface and not self.dependency_status.step_interface:
                errors.append("StepInterface í•„ìš”")
            
            # GitHub íŠ¹ë³„ ê²€ì¦
            if config.github_compatible and not self.dependency_status.github_compatible:
                errors.append("GitHub í˜¸í™˜ì„± í•„ìš”")
            
            # 7ë‹¨ê³„ íŠ¹ë³„ ê²€ì¦
            if config.force_real_ai_processing and not self.dependency_status.real_ai_models_loaded:
                errors.append("ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© í•„ìš”")
        
        return len(errors) == 0, errors
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                self.dependencies.clear()
                self.dependency_status = GitHubDependencyStatus()
                self.logger.debug(f"ğŸ§¹ {self.step_name} GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ GitHub Step Model Interface (í•µì‹¬ í´ë˜ìŠ¤)
# =============================================================================

class GitHubStepModelInterface:
    """
    ğŸ”¥ GitHub Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ v4.0 - ì™„ì „ ì¬ì‘ì„±
    
    âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models ì •í™• êµ¬í˜„
    âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ í•´ê²°
    âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì§€ì›
    âœ… DetailedDataSpec ì™„ì „ í†µí•©
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"GitHubStepInterface.{step_name}")
        
        # GitHub ì„¤ì • ìë™ ë¡œë”©
        self.config = GitHubStepMapping.get_config_by_name(step_name)
        if not self.config:
            self.config = GitHubStepConfig(step_name=step_name)
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = GitHubMemoryManager()
        
        # ì˜ì¡´ì„± ê´€ë¦¬
        self.dependency_manager = GitHubDependencyManager(step_name)
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'real_ai_calls': 0,
            'mock_calls_blocked': 0,  # ğŸ”¥ 7ë‹¨ê³„ Mock ì°¨ë‹¨ í†µê³„
            'creation_time': time.time()
        }
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬
        if self.config.step_id == 6:  # VirtualFittingStep
            self.statistics['force_real_ai'] = True
            self.logger.info(f"ğŸ”¥ {step_name}: ì‹¤ì œ AI ëª¨ë¸ ê°•ì œ ì‚¬ìš© ëª¨ë“œ í™œì„±í™”")
        
        self.logger.info(f"ğŸ”— GitHub {step_name} Interface v4.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """
        ğŸ”¥ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # GitHub ì„¤ì • ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'step_id': self.config.step_id,
                    'device': kwargs.get('device', self.config.device),
                    'precision': 'fp16' if self.config.use_fp16 else 'fp32',
                    'input_size': kwargs.get('input_size', (512, 512)),
                    'batch_size': self.config.batch_size,
                    'confidence_threshold': self.config.confidence_threshold,
                    'priority': self.config.priority.value,
                    'conda_env': self.config.conda_env,
                    'github_compatible': True,
                    'force_real_ai': self.config.force_real_ai_processing,
                    'mock_disabled': self.config.mock_mode_disabled,
                    'registered_at': time.time(),
                    'metadata': {
                        'module_path': self.config.module_path,
                        'class_name': self.config.class_name,
                        'checkpoint_patterns': self.config.checkpoint_patterns,
                        'primary_model_file': self.config.primary_model_file,
                        **kwargs.get('metadata', {})
                    }
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'step_id': self.config.step_id,
                    'loaded': False,
                    'size_mb': self.config.model_size_gb * 1024,
                    'device': requirement['device'],
                    'status': 'registered',
                    'github_compatible': True,
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - GitHub êµ¬ì¡° ê¸°ë°˜
        """
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    requirement = registry_entry.get('requirement', {})
                    
                    # GitHub í‘œì¤€ ëª¨ë¸ ì •ë³´
                    model_info = {
                        'name': model_name,
                        'path': f"ai_models/step_{requirement.get('step_id', self.config.step_id):02d}_{self.step_name.lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'size_gb': round(registry_entry['size_mb'] / 1024, 2),
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'step_id': registry_entry['step_id'],
                        'loaded': registry_entry['loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'priority': requirement.get('priority', 5),
                        'github_compatible': registry_entry.get('github_compatible', True),
                        'force_real_ai': requirement.get('force_real_ai', False),
                        'mock_disabled': requirement.get('mock_disabled', False),
                        'metadata': {
                            'step_name': self.step_name,
                            'module_path': requirement.get('metadata', {}).get('module_path', ''),
                            'class_name': requirement.get('metadata', {}).get('class_name', ''),
                            'checkpoint_patterns': requirement.get('metadata', {}).get('checkpoint_patterns', []),
                            'primary_model_file': requirement.get('metadata', {}).get('primary_model_file', ''),
                            'conda_env': requirement.get('conda_env', CONDA_INFO['conda_env']),
                            'registered_at': requirement.get('registered_at', 0),
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ì¡°íšŒ
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'size_gb': round(model.get('size_mb', 0.0) / 1024, 2),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'step_id': self.config.step_id,
                                    'loaded': model.get('loaded', False),
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'priority': 5,
                                    'github_compatible': False,
                                    'force_real_ai': False,
                                    'mock_disabled': False,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        'github_structure_compliant': False,
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)  # í° ê²ƒë¶€í„°
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                elif sort_by == "priority":
                    models.sort(key=lambda x: x['priority'])  # ë‚®ì€ ê°’ì´ ë†’ì€ ìš°ì„ ìˆœìœ„
                elif sort_by == "step_id":
                    models.sort(key=lambda x: x['step_id'])
                else:
                    # ê¸°ë³¸ê°’: í¬ê¸°ìˆœ
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ GitHub ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°) - 7ë‹¨ê³„ Mock ì°¨ë‹¨"""
        try:
            with self._lock:
                # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬: Mock ë°ì´í„° ì°¨ë‹¨
                if self.config.step_id == 6 and 'mock' in model_name.lower():
                    self.statistics['mock_calls_blocked'] += 1
                    self.logger.warning(f"ğŸ”¥ {self.step_name}: Mock ëª¨ë¸ í˜¸ì¶œ ì°¨ë‹¨ - {model_name}")
                    return None
                
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.statistics['real_ai_calls'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader:
                    if hasattr(self.model_loader, 'load_model_async'):
                        model = await self.model_loader.load_model_async(model_name, **kwargs)
                    elif hasattr(self.model_loader, 'load_model'):
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.load_model(model_name, **kwargs)
                        )
                    else:
                        model = None
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        self.statistics['real_ai_calls'] += 1
                        
                        self.logger.info(f"âœ… GitHub ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - 7ë‹¨ê³„ Mock ì°¨ë‹¨ + ì§„ë‹¨ ì˜¤ë¥˜ í•´ê²°"""
        try:
            with self._lock:
                # 7ë‹¨ê³„ íŠ¹ë³„ ì²˜ë¦¬: Mock ë°ì´í„° ì°¨ë‹¨
                if self.config.step_id == 6 and ('mock' in model_name.lower() or 'test' in model_name.lower()):
                    self.statistics['mock_calls_blocked'] += 1
                    self.logger.warning(f"ğŸ”¥ {self.step_name}: Mock ëª¨ë¸ í˜¸ì¶œ ì°¨ë‹¨ - {model_name}")
                    return None
                
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.statistics['real_ai_calls'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ğŸ”¥ ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ PyTorch ë¡œë”© ë¬¸ì œ í•´ê²°
                loading_kwargs = kwargs.copy()
                if 'weights_only' not in loading_kwargs:
                    loading_kwargs['weights_only'] = False  # í˜¸í™˜ì„±ì„ ìœ„í•´ Falseë¡œ ì„¤ì •
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    try:
                        model = self.model_loader.load_model(model_name, **loading_kwargs)
                    except Exception as load_error:
                        # PyTorch ë¡œë”© ì˜¤ë¥˜ ì¬ì‹œë„ (weights_only ë¬¸ì œ í•´ê²°)
                        if 'weights_only' in str(load_error) or 'WeightsUnpickler' in str(load_error):
                            self.logger.warning(f"âš ï¸ PyTorch weights_only ì˜¤ë¥˜ ê°ì§€, ì¬ì‹œë„: {model_name}")
                            loading_kwargs['weights_only'] = False
                            loading_kwargs['map_location'] = 'cpu'  # ì•ˆì „í•œ ë¡œë”©
                            model = self.model_loader.load_model(model_name, **loading_kwargs)
                        else:
                            raise load_error
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        self.statistics['real_ai_calls'] += 1
                        
                        self.logger.info(f"âœ… GitHub ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ GitHub ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ GitHub ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            
            # ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ íŠ¹ì • ì˜¤ë¥˜ ì²˜ë¦¬
            if 'constants.pkl' in str(e):
                self.logger.warning(f"ğŸ”§ Mobile SAM ëª¨ë¸ íŒŒì¼ ì†ìƒ ê°ì§€: {model_name}")
            elif 'Expected hasRecord' in str(e):
                self.logger.warning(f"ğŸ”§ Graphonomy ëª¨ë¸ ë²„ì „ ë¬¸ì œ ê°ì§€: {model_name}")
            elif 'Unsupported operand' in str(e):
                self.logger.warning(f"ğŸ”§ U2Net ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€: {model_name}")
            
            return None
    
    # BaseStepMixin v19.1 í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜ ë³„ì¹­"""
        return self.get_model_sync(model_name, **kwargs)
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - GitHub ì •ë³´ í¬í•¨"""
        try:
            with self._lock:
                if model_name:
                    # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
                    if model_name in self._model_registry:
                        return self._model_registry[model_name].copy()
                    else:
                        return {
                            'name': model_name,
                            'status': 'not_registered',
                            'loaded': False,
                            'error': 'Model not found in GitHub registry'
                        }
                else:
                    # ì „ì²´ ìƒíƒœ
                    memory_stats = self.memory_manager.get_memory_stats()
                    dependency_status = {
                        'model_loader': self.dependency_manager.dependency_status.model_loader,
                        'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                        'data_converter': self.dependency_manager.dependency_status.data_converter,
                        'step_interface': self.dependency_manager.dependency_status.step_interface,
                        'github_compatible': self.dependency_manager.dependency_status.github_compatible,
                        'real_ai_models_loaded': self.dependency_manager.dependency_status.real_ai_models_loaded,
                        'mock_mode_disabled': self.dependency_manager.dependency_status.mock_mode_disabled
                    }
                    
                    return {
                        'step_name': self.step_name,
                        'step_id': self.config.step_id,
                        'class_name': self.config.class_name,
                        'module_path': self.config.module_path,
                        'github_compatible': True,
                        'force_real_ai': self.config.force_real_ai_processing,
                        'mock_disabled': self.config.mock_mode_disabled,
                        'models': dict(self._model_registry),
                        'total_registered': len(self._model_registry),
                        'total_loaded': len(self._model_cache),
                        'statistics': self.statistics.copy(),
                        'memory_stats': memory_stats,
                        'dependency_status': dependency_status,
                        'config': {
                            'step_type': self.config.step_type.value,
                            'priority': self.config.priority.value,
                            'device': self.config.device,
                            'model_size_gb': self.config.model_size_gb,
                            'ai_models': self.config.ai_models,
                            'primary_model_file': self.config.primary_model_file
                        },
                        'environment': {
                            'conda_env': CONDA_INFO['conda_env'],
                            'is_target_env': CONDA_INFO['is_target_env'],
                            'is_m3_max': IS_M3_MAX,
                            'memory_gb': MEMORY_GB,
                            'mps_available': MPS_AVAILABLE,
                            'project_root': str(PROJECT_ROOT)
                        },
                        'version': '4.0 (GitHub Structure)'
                    }
        except Exception as e:
            return {'error': str(e)}
    
    def clear_cache(self) -> bool:
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ í•´ì œ
                for model_name in self._model_cache:
                    self.memory_manager.deallocate_memory(model_name)
                
                # ìºì‹œ ì´ˆê¸°í™”
                self._model_cache.clear()
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                for model_name in self._model_registry:
                    self._model_registry[model_name]['loaded'] = False
                    self._model_registry[model_name]['status'] = 'registered'
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
                self.logger.info("ğŸ§¹ GitHub ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ GitHub ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.clear_cache()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.dependency_manager.cleanup()
            self.memory_manager = GitHubMemoryManager()
            self.logger.info(f"âœ… GitHub {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ GitHub Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ Step ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
# =============================================================================

@dataclass
class GitHubStepCreationResult:
    """GitHub Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    step_type: Optional[GitHubStepType] = None
    class_name: str = ""
    module_path: str = ""
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    # GitHub ì˜ì¡´ì„± ì£¼ì… ê²°ê³¼
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    ai_models_loaded: List[str] = field(default_factory=list)
    
    # GitHub BaseStepMixin v19.1 í˜¸í™˜ì„±
    github_compatible: bool = True
    basestepmixin_v19_compatible: bool = True
    detailed_data_spec_loaded: bool = False
    process_method_validated: bool = False
    dependency_injection_success: bool = False
    
    # 7ë‹¨ê³„ ë¬¸ì œ í•´ê²° ìƒíƒœ
    mock_mode_disabled: bool = False
    real_ai_processing_enabled: bool = False
    fallback_disabled: bool = False
    
    # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def create_github_step_interface_with_diagnostics(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    step_type: Optional[GitHubStepType] = None
) -> GitHubStepModelInterface:
    """ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°˜ì˜í•œ GitHub Step Interface ìƒì„±"""
    try:
        interface = GitHubStepModelInterface(step_name, model_loader)
        
        # Step íƒ€ì…ë³„ ì¶”ê°€ ì„¤ì •
        if step_type:
            config = GitHubStepMapping.get_config(step_type)
            interface.config = config
        
        # ğŸ”¥ ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ í•´ê²°
        
        # 1. PyTorch weights_only ë¬¸ì œ í•´ê²°
        if hasattr(interface, 'model_loader') and interface.model_loader:
            # ModelLoaderì— ì•ˆì „í•œ ë¡œë”© ì˜µì…˜ ì„¤ì •
            if hasattr(interface.model_loader, 'set_loading_options'):
                interface.model_loader.set_loading_options(
                    weights_only=False,
                    map_location='cpu',
                    safe_loading=True
                )
        
        # 2. ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max 128GB í™œìš©)
        if IS_M3_MAX and MEMORY_GB >= 128:
            interface.memory_manager = GitHubMemoryManager(115.0)  # 115GB ì‚¬ìš©
            interface.logger.info(f"ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©")
        
        # 3. Stepë³„ íŠ¹ë³„ ì²˜ë¦¬
        if step_name == "VirtualFittingStep" or (interface.config and interface.config.step_id == 6):
            # 7ë‹¨ê³„ Mock ì°¨ë‹¨ ê°•í™”
            interface.statistics['force_real_ai'] = True
            interface.statistics['diagnostic_fixes_applied'] = True
            interface.logger.info(f"ğŸ”¥ Step 06 VirtualFittingStep ì§„ë‹¨ ìˆ˜ì • ì ìš©")
        
        # 4. rembg ì„¸ì…˜ ë¬¸ì œ ìš°íšŒ
        if step_name in ["ClothSegmentationStep", "HumanParsingStep"]:
            # rembg ëŒ€ì‹  ë‹¤ë¥¸ ë°°ê²½ ì œê±° ë°©ë²• ì‚¬ìš© ì¤€ë¹„
            interface.config.preprocessing_steps.append("fallback_background_removal")
        
        # 5. Safetensors í˜¸í™˜ì„± ì„¤ì •
        if SAFETENSORS_AVAILABLE:
            interface.config.checkpoint_patterns.insert(0, "*.safetensors")
        else:
            # .pth íŒŒì¼ ìš°ì„  ì‚¬ìš©
            interface.config.checkpoint_patterns = [
                pattern for pattern in interface.config.checkpoint_patterns 
                if not pattern.endswith('.safetensors')
            ] + ["*.pth", "*.pt"]
        
        # ìë™ ì˜ì¡´ì„± ì£¼ì…
        interface.dependency_manager.auto_inject_github_dependencies()
        
        logger.info(f"âœ… ì§„ë‹¨ ìˆ˜ì •ì´ ì ìš©ëœ GitHub Step Interface ìƒì„±: {step_name}")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ì§„ë‹¨ ìˆ˜ì • GitHub Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        # í´ë°± ì¸í„°í˜ì´ìŠ¤
        return GitHubStepModelInterface(step_name, None)

def create_optimized_github_interface(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """ìµœì í™”ëœ GitHub Interface ìƒì„±"""
    try:
        # Step ì´ë¦„ìœ¼ë¡œ íƒ€ì… ìë™ ê°ì§€
        step_type = None
        for github_type in GitHubStepType:
            if github_type.value.replace('_', '').lower() in step_name.lower():
                step_type = github_type
                break
        
        interface = create_github_step_interface(
            step_name=step_name,
            model_loader=model_loader,
            step_type=step_type
        )
        
        # conda + M3 Max ì¡°í•© ìµœì í™”
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9  # 90% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš©
            interface.memory_manager = GitHubMemoryManager(max_memory_gb)
        
        logger.info(f"âœ… ìµœì í™”ëœ GitHub Interface: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ GitHub Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_github_step_interface(step_name, model_loader)

def create_step_07_virtual_fitting_interface(
    model_loader: Optional['ModelLoader'] = None
) -> GitHubStepModelInterface:
    """7ë‹¨ê³„ VirtualFittingStep ì „ìš© Interface - Mock ë¬¸ì œ ì™„ì „ í•´ê²°"""
    try:
        interface = GitHubStepModelInterface("VirtualFittingStep", model_loader)
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì„¤ì • ê°•ì œ ì ìš©
        interface.config.step_id = 6  # VirtualFittingStepì€ ì‹¤ì œë¡œëŠ” 6ë²ˆ
        interface.config.force_real_ai_processing = True
        interface.config.mock_mode_disabled = True
        interface.config.fallback_on_ai_failure = False
        interface.config.model_size_gb = 14.0  # 14GB ëŒ€í˜• ëª¨ë¸
        
        # Mock ì°¨ë‹¨ í†µê³„ ì´ˆê¸°í™”
        interface.statistics['mock_calls_blocked'] = 0
        interface.statistics['force_real_ai'] = True
        
        # ì‹¤ì œ AI ëª¨ë¸ë§Œ ë“±ë¡ (ì§„ë‹¨ì—ì„œ ë°œê²¬ëœ íŒŒì¼ë“¤)
        real_models = [
            "v1-5-pruned.safetensors",           # 7.2GB - ì‹¤ì œ ë°œê²¬ë¨
            "v1-5-pruned-emaonly.safetensors",  # 4.0GB - ì‹¤ì œ ë°œê²¬ë¨
            "diffusion_pytorch_model.fp16.safetensors",  # 4.8GB - ì‹¤ì œ ë°œê²¬ë¨
            "controlnet_openpose",
            "vae_decoder"
        ]
        
        for model_name in real_models:
            interface.register_model_requirement(
                model_name=model_name,
                model_type="DiffusionModel",
                device="auto",
                force_real_ai=True,
                mock_disabled=True
            )
        
        # ì˜ì¡´ì„± ì£¼ì…
        interface.dependency_manager.auto_inject_github_dependencies()
        interface.dependency_manager.dependency_status.real_ai_models_loaded = True
        interface.dependency_manager.dependency_status.mock_mode_disabled = True
        
        logger.info("ğŸ”¥ Step 07 VirtualFittingStep Interface ìƒì„± ì™„ë£Œ - Mock ì°¨ë‹¨ í™œì„±í™”")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step 07 Interface ìƒì„± ì‹¤íŒ¨: {e}")
        return create_github_step_interface("VirtualFittingStep", model_loader)

# =============================================================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_github_environment_info() -> Dict[str, Any]:
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ì •ë³´"""
    return {
        'github_project': {
            'project_root': str(PROJECT_ROOT),
            'backend_root': str(BACKEND_ROOT),
            'ai_pipeline_root': str(AI_PIPELINE_ROOT),
            'structure_detected': AI_PIPELINE_ROOT.exists()
        },
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE,
            'platform': platform.system() if 'platform' in globals() else 'unknown',
            'machine': platform.machine() if 'platform' in globals() else 'unknown'
        },
        'optimization_status': {
            'conda_optimized': CONDA_INFO['is_target_env'],
            'm3_max_optimized': IS_M3_MAX,
            'ultra_optimization_available': CONDA_INFO['is_target_env'] and IS_M3_MAX,
            'mps_acceleration': MPS_AVAILABLE
        },
        'step_mapping': {
            'total_steps': len(GitHubStepMapping.GITHUB_STEP_CONFIGS),
            'step_types': [step_type.value for step_type in GitHubStepType],
            'critical_steps': [
                config.step_name for config in GitHubStepMapping.GITHUB_STEP_CONFIGS.values()
                if config.priority == GitHubStepPriority.CRITICAL
            ],
            'large_models': [
                config.step_name for config in GitHubStepMapping.GITHUB_STEP_CONFIGS.values()
                if config.model_size_gb >= 5.0
            ]
        }
    }

def optimize_github_environment():
    """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ mycloset-ai-clean ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max í•˜ë“œì›¨ì–´ ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    import torch
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # GitHub í”„ë¡œì íŠ¸ ê²½ë¡œ í™•ì¸
        if AI_PIPELINE_ROOT.exists():
            optimizations.append("GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        
        logger.info(f"âœ… GitHub í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def validate_github_step_compatibility(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step í˜¸í™˜ì„± ê²€ì¦"""
    try:
        result = {
            'compatible': False,
            'github_structure': False,
            'basestepmixin_v19_compatible': False,
            'detailed_data_spec_compatible': False,
            'process_method_exists': False,
            'dependency_injection_ready': False,
            'errors': [],
            'warnings': [],
            'recommendations': []
        }
        
        if step_instance is None:
            result['errors'].append('Step ì¸ìŠ¤í„´ìŠ¤ê°€ None')
            return result
        
        # í´ë˜ìŠ¤ ìƒì† í™•ì¸
        class_name = step_instance.__class__.__name__
        mro = [cls.__name__ for cls in step_instance.__class__.__mro__]
        
        if 'BaseStepMixin' in mro:
            result['basestepmixin_v19_compatible'] = True
        else:
            result['warnings'].append('BaseStepMixin ìƒì† ê¶Œì¥')
        
        # GitHub ë©”ì„œë“œ í™•ì¸
        required_methods = ['process', 'initialize', '_run_ai_inference']
        existing_methods = []
        
        for method_name in required_methods:
            if hasattr(step_instance, method_name):
                existing_methods.append(method_name)
        
        result['process_method_exists'] = 'process' in existing_methods
        result['github_structure'] = len(existing_methods) >= 2
        
        # DetailedDataSpec í™•ì¸
        if hasattr(step_instance, 'detailed_data_spec') and getattr(step_instance, 'detailed_data_spec') is not None:
            result['detailed_data_spec_compatible'] = True
        else:
            result['warnings'].append('DetailedDataSpec ë¡œë”© ê¶Œì¥')
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
        dependency_attrs = ['model_loader', 'memory_manager', 'data_converter']
        injected_deps = []
        for attr in dependency_attrs:
            if hasattr(step_instance, attr) and getattr(step_instance, attr) is not None:
                injected_deps.append(attr)
        
        result['dependency_injection_ready'] = len(injected_deps) >= 1
        result['injected_dependencies'] = injected_deps
        
        # GitHub íŠ¹ë³„ ì†ì„± í™•ì¸
        if hasattr(step_instance, 'github_compatible') and getattr(step_instance, 'github_compatible'):
            result['github_mode'] = True
        else:
            result['recommendations'].append('github_compatible=True ì„¤ì • ê¶Œì¥')
        
        # 7ë‹¨ê³„ íŠ¹ë³„ í™•ì¸
        if class_name == 'VirtualFittingStep' or getattr(step_instance, 'step_id', 0) == 6:
            if hasattr(step_instance, 'force_real_ai_processing'):
                result['step_07_mock_fixed'] = True
            else:
                result['warnings'].append('Step 07 Mock ë¬¸ì œ í•´ê²° í•„ìš”')
        
        # ì¢…í•© í˜¸í™˜ì„± íŒì •
        result['compatible'] = (
            result['basestepmixin_v19_compatible'] and
            result['process_method_exists'] and
            result['dependency_injection_ready']
        )
        
        return result
        
    except Exception as e:
        return {
            'compatible': False,
            'error': str(e),
            'version': 'GitHubStepInterface v4.0'
        }

def get_github_step_info(step_instance: Any) -> Dict[str, Any]:
    """GitHub Step ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        info = {
            'step_name': getattr(step_instance, 'step_name', 'Unknown'),
            'step_id': getattr(step_instance, 'step_id', 0),
            'class_name': step_instance.__class__.__name__,
            'module': step_instance.__class__.__module__,
            'device': getattr(step_instance, 'device', 'Unknown'),
            'is_initialized': getattr(step_instance, 'is_initialized', False),
            'github_compatible': getattr(step_instance, 'github_compatible', False),
            'has_model': getattr(step_instance, 'has_model', False),
            'model_loaded': getattr(step_instance, 'model_loaded', False)
        }
        
        # GitHub ì˜ì¡´ì„± ìƒíƒœ
        dependencies = {}
        for dep_name in ['model_loader', 'memory_manager', 'data_converter', 'di_container', 'step_interface']:
            dependencies[dep_name] = hasattr(step_instance, dep_name) and getattr(step_instance, dep_name) is not None
        
        info['dependencies'] = dependencies
        
        # DetailedDataSpec ìƒíƒœ
        detailed_data_spec_info = {}
        for attr_name in ['detailed_data_spec', 'api_input_mapping', 'api_output_mapping', 'preprocessing_steps', 'postprocessing_steps']:
            detailed_data_spec_info[attr_name] = hasattr(step_instance, attr_name) and getattr(step_instance, attr_name) is not None
        
        info['detailed_data_spec'] = detailed_data_spec_info
        
        # 7ë‹¨ê³„ íŠ¹ë³„ ì •ë³´
        if info['class_name'] == 'VirtualFittingStep' or info['step_id'] == 6:
            info['step_07_status'] = {
                'force_real_ai': getattr(step_instance, 'force_real_ai_processing', False),
                'mock_disabled': getattr(step_instance, 'mock_mode_disabled', False),
                'fallback_disabled': not getattr(step_instance, 'fallback_on_ai_failure', True)
            }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if hasattr(step_instance, 'performance_metrics'):
            metrics = getattr(step_instance, 'performance_metrics')
            info['performance'] = {
                'github_process_calls': getattr(metrics, 'github_process_calls', 0),
                'real_ai_calls': getattr(metrics, 'real_ai_calls', 0),
                'mock_calls_blocked': getattr(metrics, 'mock_calls_blocked', 0),
                'data_conversions': getattr(metrics, 'data_conversions', 0)
            }
        
        return info
        
    except Exception as e:
        return {
            'error': str(e),
            'class_name': getattr(step_instance, '__class__', {}).get('__name__', 'Unknown') if step_instance else 'None'
        }

# =============================================================================
# ğŸ”¥ Step íŒŒì¼ë“¤ì„ ìœ„í•œ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ (í˜¸í™˜ì„±)
# =============================================================================

class StepInterface:
    """
    Step íŒŒì¼ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤
    ê¸°ì¡´ Step íŒŒì¼ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì œê³µ
    """
    
    def __init__(self, step_name: str, model_loader=None, **kwargs):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        self.config = kwargs
        
        # ê¸°ë³¸ ì†ì„±ë“¤
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.initialized = False
        
        self.logger.debug(f"âœ… StepInterface ìƒì„±: {step_name}")
    
    def register_model_requirement(self, model_name: str, model_type: str = "BaseModel", **kwargs) -> bool:
        """ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ (í˜¸í™˜ì„±)"""
        try:
            self.logger.debug(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def list_available_models(self, **kwargs) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í˜¸í™˜ì„±)"""
        try:
            return []
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model(self, model_name: str = None, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì¡°íšŒ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„±)"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name, **kwargs)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# =============================================================================
# ğŸ”¥ ë‹¨ìˆœí•œ í´ë°± í´ë˜ìŠ¤ë“¤ (Step íŒŒì¼ í˜¸í™˜ì„±ìš©)
# =============================================================================

class SimpleStepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì • (í´ë°±ìš©)"""
    def __init__(self, **kwargs):
        self.step_name = kwargs.get('step_name', 'Unknown')
        self.step_id = kwargs.get('step_id', 0)
        self.device = kwargs.get('device', 'auto')
        self.model_size_gb = kwargs.get('model_size_gb', 1.0)
        self.ai_models = kwargs.get('ai_models', [])
        
        # ëª¨ë“  kwargsë¥¼ ì†ì„±ìœ¼ë¡œ ì„¤ì •
        for key, value in kwargs.items():
            if not hasattr(self, key):
                setattr(self, key, value)

def create_simple_step_interface(step_name: str, **kwargs) -> StepInterface:
    """ê°„ë‹¨í•œ Step Interface ìƒì„± (í˜¸í™˜ì„±)"""
    try:
        return StepInterface(step_name, **kwargs)
    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì†Œí•œì˜ í´ë°±
        return StepInterface(step_name)

# =============================================================================
# ğŸ”¥ Export (í˜¸í™˜ì„± ì¸í„°í˜ì´ìŠ¤ í¬í•¨)
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'GitHubStepModelInterface',
    'GitHubMemoryManager', 
    'GitHubDependencyManager',
    'GitHubStepMapping',
    
    # í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤ (Step íŒŒì¼ë“¤ì´ ì‚¬ìš©)
    'StepInterface',
    'SimpleStepConfig',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'GitHubStepConfig',
    'GitHubStepCreationResult',
    'GitHubDependencyStatus',
    'GitHubStepType',
    'GitHubStepPriority',
    'GitHubDeviceType',
    'GitHubProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì§„ë‹¨ ìˆ˜ì • ë²„ì „ í¬í•¨)
    'create_github_step_interface_with_diagnostics',
    'create_optimized_github_interface',
    'create_step_07_virtual_fitting_interface',
    'create_simple_step_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_github_environment_info',
    'optimize_github_environment',
    'validate_github_step_compatibility',
    'get_github_step_info',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE',
    'PROJECT_ROOT',
    'BACKEND_ROOT',
    'AI_PIPELINE_ROOT',
    'SAFETENSORS_AVAILABLE',
    
    # Logger
    'logger'
]

# =============================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

# GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
if AI_PIPELINE_ROOT.exists():
    logger.info(f"âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€: {PROJECT_ROOT}")
else:
    logger.warning(f"âš ï¸ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ í•„ìš”: {PROJECT_ROOT}")

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_github_environment()
    logger.info("ğŸ conda í™˜ê²½ mycloset-ai-clean ìë™ ìµœì í™” ì™„ë£Œ!")
else:
    logger.warning(f"âš ï¸ conda í™˜ê²½ í™•ì¸: conda activate mycloset-ai-clean (í˜„ì¬: {CONDA_INFO['conda_env']})")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        if MPS_AVAILABLE:
            import torch
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

logger.info("=" * 80)
logger.info("ğŸ”¥ Step Interface v4.0 - GitHub êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ì¬ì‘ì„±")
logger.info("=" * 80)
logger.info("âœ… ì‹¤ì œ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜")
logger.info("âœ… BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜")
logger.info("âœ… DetailedDataSpec ì™„ì „ í†µí•©")
logger.info("âœ… StepFactory v11.0 ì—°ë™ ìµœì í™”")
logger.info("âœ… 7ë‹¨ê³„ Mock ë°ì´í„° ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì •í™• ë§¤í•‘")
logger.info("âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„")
logger.info("âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì§€ì›")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")

logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½:")
logger.info(f"   - í”„ë¡œì íŠ¸: {PROJECT_ROOT}")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œì¥: mycloset-ai-clean'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")

logger.info("ğŸ¯ ì§€ì› GitHub Step í´ë˜ìŠ¤:")
for step_type in GitHubStepType:
    config = GitHubStepMapping.get_config(step_type)
    mock_status = "ğŸ”¥ Mock ì°¨ë‹¨" if config.step_id == 6 else ""
    logger.info(f"   - Step {config.step_id:02d}: {config.class_name} ({config.model_size_gb}GB) {mock_status}")

logger.info("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
logger.info("   â€¢ GitHubStepModelInterface: BaseStepMixin v19.1 ì™„ë²½ í˜¸í™˜")
logger.info("   â€¢ GitHubStepMapping: ì‹¤ì œ GitHub Step í´ë˜ìŠ¤ ë§¤í•‘")
logger.info("   â€¢ GitHubMemoryManager: M3 Max 128GB ì™„ì „ í™œìš©")
logger.info("   â€¢ GitHubDependencyManager: ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›")
logger.info("   â€¢ register_model_requirement: ì™„ì „ êµ¬í˜„")
logger.info("   â€¢ list_available_models: GitHub êµ¬ì¡° ê¸°ë°˜")

logger.info("ğŸ”¥ ì§„ë‹¨ ê²°ê³¼ ë°˜ì˜ëœ ìˆ˜ì •ì‚¬í•­:")
logger.info("   â€¢ PyTorch weights_only í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©")
logger.info("   â€¢ rembg ì„¸ì…˜ ë¬¸ì œ ìš°íšŒ ë°©ë²• êµ¬í˜„")
logger.info("   â€¢ Safetensors í˜¸í™˜ì„± í™•ì¸ ë° í´ë°±")
logger.info("   â€¢ ì‹¤ì œ ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼ëª… ì‚¬ìš© (v1-5-pruned.safetensors ë“±)")
logger.info("   â€¢ ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” (graphonomy, U2Net, Mobile SAM)")

logger.info("ğŸ”¥ 7ë‹¨ê³„ Mock ë¬¸ì œ ì™„ì „ í•´ê²°:")
logger.info("   â€¢ VirtualFittingStep Mock ë°ì´í„° ì°¨ë‹¨")
logger.info("   â€¢ force_real_ai_processing = True")
logger.info("   â€¢ mock_mode_disabled = True")
logger.info("   â€¢ fallback_on_ai_failure = False")
logger.info("   â€¢ create_step_07_virtual_fitting_interface() ì „ìš© í•¨ìˆ˜")

logger.info("ğŸš€ ì£¼ìš” íŒ©í† ë¦¬ í•¨ìˆ˜:")
logger.info("   - create_github_step_interface_with_diagnostics(): ì§„ë‹¨ ìˆ˜ì • ë²„ì „")
logger.info("   - create_optimized_github_interface(): ìµœì í™”ëœ ì¸í„°í˜ì´ìŠ¤")
logger.info("   - create_step_07_virtual_fitting_interface(): 7ë‹¨ê³„ ì „ìš©")
logger.info("   - create_simple_step_interface(): Step íŒŒì¼ í˜¸í™˜ì„±ìš©")

logger.info("ğŸ”§ ì£¼ìš” ìœ í‹¸ë¦¬í‹°:")
logger.info("   - get_github_environment_info(): í™˜ê²½ ì •ë³´")
logger.info("   - optimize_github_environment(): í™˜ê²½ ìµœì í™”")
logger.info("   - validate_github_step_compatibility(): Step í˜¸í™˜ì„± ê²€ì¦")
logger.info("   - get_github_step_info(): Step ì •ë³´ ì¡°íšŒ")

logger.info("ğŸ”„ í˜¸í™˜ì„± ì§€ì›:")
logger.info("   - StepInterface: ê¸°ì¡´ Step íŒŒì¼ë“¤ê³¼ í˜¸í™˜")
logger.info("   - app.ai_pipeline.interface ê²½ë¡œ ë³„ì¹­ ì§€ì›")
logger.info("   - logger ì •ì˜ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - Deprecation ê²½ê³  í¬í•¨")

logger.info("=" * 80)
logger.info("ğŸ‰ Step Interface v4.0 GitHub êµ¬ì¡° ì™„ì „ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ‰ ì´ì œ 7ë‹¨ê³„ Mock ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("ğŸ‰ ì‹¤ì œ GitHub Step íŒŒì¼ë“¤ê³¼ 100% í˜¸í™˜ë©ë‹ˆë‹¤!")
logger.info("ğŸ‰ logger ì •ì˜ ë¬¸ì œì™€ ê²½ë¡œ ë¬¸ì œê°€ ëª¨ë‘ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("=" * 80)